Grey-box fuzz testing has revealed thousands of vulner-abilities in real-world software owing to its lightweight instrumentation, fast coverage feedback, and dynamic adjusting strategies.
However, directly applying grey-box fuzzing to input-dependent multithreaded programs can be extremely inefficient.
In practice, multithreading-relevant bugs are usually buried in the sophisticated program flows.
Meanwhile, existing grey-box fuzzing techniques do not stress thread-interleavings that affect execution states in multithreaded programs.
Therefore, mainstream grey-box fuzzers cannot adequately test problematic segments in multithreaded software, although they might obtain high code coverage statistics.
To this end, we propose MUZZ, a new grey-box fuzzing technique that hunts for bugs in multithreaded programs.
MUZZ owns three novel thread-aware instrumentations, namely coverage-oriented instrumentation, thread-context instrumentation, and schedule-intervention instrumentation.
During fuzzing, these instrumentations engender runtime feedback to accentuate execution states caused by thread inter-leavings.
By leveraging such feedback in the dynamic seed selection and execution strategies, MUZZ preserves more valuable seeds that expose bugs under a multithreading context.
We evaluate MUZZ on twelve real-world multithreaded programs.
Experiments show that MUZZ outperforms AFL in both multithreading-relevant seed generation and concurrency-vulnerability detection.
Further, by replaying the target programs against the generated seeds, MUZZ also reveals more concurrency-bugs (e.g., data-races, thread-leaks) than AFL.
In total, MUZZ detected eight new concurrency-vulnerabilities and nineteen new concurrency-bugs.
At the time of writing, four reported issues have received CVE IDs.
Multithreading has been popular in modern software systems since it substantially utilizes the hardware resources to boost * Corresponding Author.
software performance.
A typical computing paradigm of multithreaded programs is to accept a set of inputs, distribute computing jobs to threads, and orchestrate their progress accordingly.
Compared to sequential programs, however, multithreaded programs are more prone to severe software faults.
On the one hand, the non-deterministic thread-interleavings give rise to concurrency-bugs like data-races, deadlocks, etc [32].
These bugs may cause the program to end up with abnormal results or unexpected hangs.
On the other hand, bugs that appear under specific inputs and interleavings may lead to concurrency-vulnerabilities [5,30], resulting in memory corruptions, information leakage, etc.There exist a line of works on detecting bugs and vulnerabilities inmultithreaded programs.
Static concurrency-bug predictors [2,40,45,50] aim to approximate the runtime behaviors of a program without actual concurrent execution.
However, they typically serve as a complementary solution due to the high percentage of false alarms [19].
Dynamic detectors detect concurrency-violations by reasoning memory read/write and synchronization events in a particular execution trace [5,12,21,41,42,49,58].
Several techniques like ThreadSanitizer (a.k.a., TSan) [42] and Helgrind [49] have been widely used in practice.
However, these approaches by themselves do not automatically generate new test inputs to exercise different paths in multithreaded programs.Meanwhile, grey-box fuzzing is effective in generating test inputs to expose vulnerabilities [34,36].
It is reported that grey-box fuzzers (GBFs) such as AFL [63] and libFuzzer [31] have detected more than 16,000 vulnerabilities in hundreds of real-world software projects [16,31,63].
Despite the great success of GBFs in detecting vulnerabilities, there are few efforts on fuzzing user-space multithreaded programs.
General-purpose GBFs usually cannot explore thread-interleaving introduced execution states due to their unawareness of multithreading.
Therefore, they cannot effectively detect concurrency-vulnerabilities inherently buried in sophisticated program flows [30].
In a discussion in 2015 [64], the author of AFL, Michal Zalewski, even suggests that "it's generally better to have a single thread".
In fact, due to the difficulty and inefficiency, the fuzzing driver programs in Google's continuous fuzzing platform OSS-fuzz are all tested in single-threaded mode [15].
Also, by matching unions of keyword patterns "race*", "concurren*" and "thread*" in the MITRE CVE database [48], we found that only 202 CVE records are relevant to concurrency-vulnerabilities out of the 70438 assigned CVE IDs ranging from CVE-2014-* to CVE-2018-*.
In particular, we observed that, theoretically, at most 4 CVE records could be detected by grey-box fuzzers that work on user-space programs.As a result, there are no practical fuzzing techniques to test input-dependent user-space multithreaded programs and detect bugs or vulnerabilities inside them.
To this end, we present a dedicated grey-box fuzzing technique, MUZZ, to reveal bugs by exercising input-dependent and interleavingdependent paths.
We categorize the targeted multithreadingrelevant bugs into two major groups:• concurrency-vulnerabilities (V m ): they correspond to memory corruption vulnerabilities that occur in a multithreading context.
These vulnerabilities can be detected during the fuzzing phase.
• concurrency-bugs (B m ): they correspond to the bugs like data-races, atomicity-violations, deadlocks, etc.
We detect them by replaying the seeds generated by MUZZ with stateof-the-art concurrency-bug detectors such as TSan.
Note that B m may not be revealed during fuzzing since they do not necessarily result in memory corruption crashes.
In the remaining sections, when referring to multithreading-relevant bugs, we always mean the combination of concurrency-bugs and concurrency-vulnerabilities, i.e., V m ∪ B m .
We summarize the contributions of our work as follows: 1) We develop three novel thread-aware instrumentations for grey-box fuzzing that can distinguish the execution states caused by thread-interleavings.
2) We optimize seed selection and execution strategies based on the runtime feedback provided by the instrumentations, which help generate more effective seeds concerning the multithreading context.
3) We integrate these analyses into MUZZ for an effective bug hunting in multithreaded programs.
Experiments on 12 realworld programs show that MUZZ outperforms other fuzzers like AFL and MOPT in detecting concurrency-vulnerabilities and revealing concurrency-bugs.
4) MUZZ detected 8 new concurrency-vulnerabilities and 19 new concurrency-bugs, with 4 CVE IDs assigned.
Considering the small portion of concurrency-vulnerabilities recorded in the CVE database, the results are promising.
Algorithm 1 presents the typical workflow of a grey-box fuzzer [3,34,63].
Given a target program P o and the input else if cov_new_trace(t', res) then 12 Q S ← Q S ⊕ t ; // preserve "effective" seeds seeds Q S , a GBF first utilizes instrumentation to track the coverage information in P o .
Then it enters the fuzzing loop: 1) Seed selection decides which seed to be selected next; 2) Seed scheduling decides how many mutations M will be applied on the selected seed t; 3) Seed mutation applies mutations on seed t to generate a new seed t ; 4) During repeated execution, for each new seed t , the fuzzer executes against it N c times to get its execution statistics; 5) Seed triaging evaluates t based on the statistics and the coverage feedback from instrumentation, to determine whether the seed leads to a vulnerability, or whether it is "effective" and should be preserved in the seed queue for subsequent fuzzing.
Here, steps 3), 4), 5) are continuously processed M times.
Notably, N c times of repeated executions are necessary since a GBF needs to collect statistics such as average execution time for t , which will be used to calculate mutation times M for seed scheduling in the next iteration.
In essence, the effectiveness of grey-box fuzzing relies on the feedback collected from the instrumentation.
Specifically, the result of cov_new_trace (line 11) is determined by the coverage feedback.
Programs and Our Solution Figure 1 is an abstracted multithreaded program that accepts a certain input file and distributes computing jobs to threads.
Practically it may behave like compressors/decompressors (e.g., lbzip2, pbzip2), image processors (e.g., ImageMagick, GraphicsMagick), encoders/decoders (e.g., WebM, libvpx), etc.
After reading the input content buf, it does an initial validity check inside the function check.
It exits immediately if the buffer does not satisfy certain properties.
The multithreading context starts from function compute (via pthread_create at lines 24-25).
It contains shared variables s_var (passed from main) and g_var (global variables), as well as the mutex primitive m to exclusively read/write shared variables (via pthread_mutex_lock and pthread_mutex_unlock).
With different inputs, the program may execute different segments.
For example, based on the condition of statement 3 , which is purely dependent on the input content (i.e., different results of buf provided by seed files), it may or may not execute 4 .
Therefore, different seed files need to be generated to exercise different paths in multithreading context -in fact, this is the starting point that we use fuzzing to generate seed files to test multithreaded programs.
Meanwhile, in the presence of thread-interleavings, g_var (initialized with -1) may also have different values.
Let us focus on different seeds' executions at two statements:1 :"g_var+=1", and 2 : "g_var*=2".
Suppose there are two threads: T1, T2; and T1: 1 is executed first.
Then there are at least three interleavings: i) T1: 1 →T2: 1 →T2: 2 →T1: 2 g_var=4 ii) T1: 1 →T2: 1 →T1: 2 →T2: 2 g_var=4 iii) T1: 1 →T1: 2 →T2: 1 →T2: 2 g_var=2 After the second 2 is executed, the values of g_var may be different (4 and 2, respectively).
Worse still, since neither 1 nor 2 is an atomic operation in the representation of the actual program binary, many more interleavings can be observed and g_var will be assigned to other values.The challenge.
To reveal multithreading-relevant bugs, a GBF needs to generate diverse seeds that execute different paths in multithreading context (e.g., paths inside compute).
However, existing GBFs even have difficulties in generating seeds to reach multithreading segments.
For example, if check is complicated enough, most of the seeds may fail the check and exit before entering compute -this is quite common due to the low quality of fuzzer-generated seeds [34,61].
Meanwhile, even if a seed indeed executes multithreading code, it may still fail to satisfy certain preconditions to reach the problematic context.
For example, suppose modify contains a vulnerability that can only be triggered when g_var is 2.
If the fuzzer has occasionally generated a seed that executes compute and the condition of 3 is true, with no awareness of thread-interleavings, it will not distinguish different schedules between i), ii) and iii).
As a result, subsequent mutations on this seed will miss important feedback regarding g_var, making it difficult to generate seeds that trigger the vulnerability.To summarize, the challenge of fuzzing multithreaded programs is, existing GBFs have difficulties in generating seeds that execute multithreading context and keep threadinterleaving execution states.
Our solution.
We provide fine-grained thread-aware feedback for seed files that execute multithreading context and distinguish more such execution states.
According to §2.1, the preservation of seeds is based on the feedback; then we can expect that the fuzzer will preserve more distinct seeds that execute multithreading code segments in the seed queue.
This means that the multithreading-relevant seeds are implicitly prioritized.
Since these seeds have already passed the validity checking, the overall quality of the generated seeds is higher.
The "Matthew Effect" helps keep the quality of seed generations for subsequent fuzzing.
Essentially, this provides a biased coverage feedback on multithreading code segments (more explanations on this are available in §5.3.
Now let us investigate what instrumentations can be improved to existing fuzzers for thread-aware feedback.
Thread-contextThe state-of-the-art GBFs, such as AFL, instrument the entry instruction of each basicblock evenly as the basicblock's deputy.
We refer to this selection strategy over deputy instructions as AFL-Ins.
AFL-Ins provides coverage feedback during the dynamic fuzzing phase to explore more paths.
During repeated execution (line 8 in Algorithm 1), AFL labels a value to each transition that connects the deputies of two consecutively executed basicblocks [63].
By maintaining a set of transitions for queued seeds, AFL-Ins tracks the "coverage" of the target program.
cov_new_trace (line 11 in Algorithm 1) checks whether a transition indicates a new path/state.
Figure 2b depicts the transitions upon executing the functions compute and modify in Figure 1.
For brevity, we use source code to illustrate the problem and use statements to represent instructions in assembly or LLVM IR [28].
AFL-Ins works perfectly on single-threaded programs: the kept transitions can reflect both branching conditions (e.g., 3 → 4 and 3 → 5 ) and function calls (e.g., 4 → 9 and 6 → 9 ).
However, AFL-Ins cannot capture these differences among schedules i), ii) and iii) (c.f. §2.2).
In fact, it can only observe there is a transition 1 → 1 ; thus it will not prioritize this path for subsequent mutations, compared to other paths that do not even execute compute.
The root cause of this defect lies in that AFL only tracks entry statements of basicblocks evenly, and does not record thread identities.
Therefore, we can add more deputy instructions within multithreadingrelevant basicblocks to provide more interleaving feedback, and add thread-context information to distinguish different threads.
During a GBF's repeated execution procedure (line 8 in Algorithm 1), a seed may exhibit non-deterministic behaviors: it executes different paths of the target program across executions due to randomness.
In this scenario, AFL (and other GBFs) will execute against such a seed more times than a seed with deterministic behaviors [63].
For the non-deterministic behaviors caused by scheduling-interleaving in multithreaded programs, since the execution is continuously repeated N c times, the system level environment (e.g., CPU usage, memory consumption, I/O status) is prone to be similar [23,26].
This will decrease the diversities of schedules, and consequently reduce the overall effectiveness.
For example, during a repeated execution with N c = 40, schedules i) and iii) might occur 10 and 30 times respectively, while schedule ii) do not occur at all; in this scenario, the execution states corresponding to ii) will not be observed by the fuzzer.
Ideally, we would like the fuzzer to observe as many distinct interleavings as possible during repeated execution since that marks the potential states a seed can exercise.
In the case of statements 1 and 2 , we hope schedules i), ii), iii) can all occur.
Therefore, it is favorable to provide schedule interventions to diversify the actual schedules.
Figure 3 depicts the system overview of MUZZ.
It contains four major components: A static thread-aware analysis guided instrumentations, B dynamic fuzzing, C vulnerability analysis, D concurrency-bug revealing.
During A :instrumentation ( §4), for a multithreaded program P o , MUZZ firstly computes thread-aware interprocedural control flow graph (ICFG) and the code segments that are likely to interleave with others during execution [11,45], namely suspicious interleaving scope, in §4.1.
Based on these results, it performs three instrumentations inspired by §2.3.
1) Coverage-oriented instrumentation ( §4.2) is one kind of stratified instrumentation that assigns more deputies to suspicious interleaving scope.
It is the major instrumentation to track thread-interleaving induced coverage.
2) Thread-context instrumentation ( §4.3) is a type of lightweight instrumentation that distinguishes different thread identities by tracking the context of threading functions for thread-forks, locks, unlocks, joins, etc. 3) Schedule-intervention instrumentation ( §4.4) is a type of lightweight instrumentation at the entry of a thread-fork routine that dynamically adjusts each thread's priority.
This complementary instrumentation aims to diversify interleavings by intervening in the thread schedules.
During B :dynamic fuzzing ( §5), MUZZ optimizes seed selection and repeated execution to generate more multithreading-relevant seeds.
For seed selection ( §5.1), in addition to the new coverage information provided by coverageoriented instrumentation, MUZZ also prioritizes those seeds that cover new thread-context based on the feedback provided by thread-context instrumentation.
For repeated execution ( §5.2), owing to the schedule-intervention instrumentation, MUZZ adjusts the repeating times N c , to maximize the benefit of repetitions and track the interleaved execution states.C :Vulnerability analysis is applied to the crashing seeds found by dynamic fuzzing, which reveals vulnerabilities (including V m ).
D :concurrency-bug revealing component reveals B m with the help of concurrency-bug detectors (e.g., TSan [42], Helgrind [49]).
These two components will be explained in the evaluation section ( §6).
This component includes the thread-aware static analysis and the instrumentations based on it.
[34] are the same as typical GBF flows, thus are marked dashed); C (right-bottom) denotes the vulnerability analysis applied on vulnerable seeds; and D (right-top) is the replaying component used to reveal concurrency-bugs from the seed queue.
The static analysis aims to provide lightweight thread-aware information for instrumentation and runtime feedback.
We firstly apply an inclusion-based pointer analysis [1] on the target program.
The points-to results are used to resolve the def-use flow of thread-sharing variables and indirect calls to reconstruct the ICFG.
By taking into account the semantics of threading APIs (e.g., POSIX standard Pthread, the OpenMP library), we get an ICFG that is aware of the following multithreading information: 1) TFork is the set of program sites that call thread-fork functions.
This includes the explicit call to pthread_create, the std::thread constructor that internally uses pthread_create, or the "parallel pragma" in OpenMP.
The called functions, denoted as F f ork , are extracted from the semantics of these forking sites.
2) TJoin contains call sites for functions that mark the end of a multithreading context.
It includes the call sites of the pthread APIs such as pthread_join, pthread_exit, etc. 3) TLock is the set of sites that call thread-lock functions such as pthread_mutex_lock, omp_set_lock, etc. 4) TUnLock is the set of sites that call thread-unlock functions like pthread_mutex_unlock, omp_unset_lock, etc. 5) TShareVar is the set of variables shared among different threads.
This includes global variables and those variables that are passed from thread-fork sites (e.g., TFork).
Given a program that may run simultaneously with multiple threads, we hope the instrumentation to collect execution states to reflect the interleavings.
However, instrumentation introduces considerable overhead to the original program, especially when it is applied intensively throughout the whole program.
Fortunately, with the static information provided by the thread-aware ICFG, we know that thread-interleavings may only happen on some specific program statements; therefore, the instrumentation can stress these statements.
We hereby use L m to denote the set of these statements and term it as suspicious interleaving scope.
L m is determined according to the following three conditions.
C1 The statements should be executed after one of TFork, while TJoin is not encountered yet.
C2 The statements can only be executed before the invocation of TLock and after the invocation of TUnLock.
C3 The statements should read or write at least one of the shared variables by different threads.
C1 excludes the statements irrelevant to multithreading.
These statements can be prologue code that does the validity check (e.g., check in Figure 1), or the epilogue that postprocesses the inputs or deals with error handlings.
C2 prevents the statements that are protected by certain locks from being put into L m .
C3 is necessary since the interleavings will not affect the shared states if the segment involves no shared variables.
This condition is determined by observing whether the investigated statement contains a variable data dependent on TShareVar (based on pointer analysis).
We provide a separate preprocessing procedure to exclude cases where there are only read operations on shared variables.Note that L m is used to emphasize multithreading-relevant paths via instrumentations for state exploration during fuzzing.
Therefore the conditions are different from the constraints required by static models (e.g., may-happen-in-parallel [11,45]) or dynamic concurrency-bug detection algorithms (e.g., happens-before [12] or lockset [41]).
In Figure 1, according to the call pthread_create at Lines 24 and 25, F f ork = {compute}.
MUZZ then gets all the functions that may be called by functions inside F f ork , i.e., {modify,compute} and according to C1 the scope L m comes from Lines 1, 2, 10−17.
Inside these functions, we check the statements that are outside pthread_mutex_lock and pthread_mutex_unlock based on C2: Line 15 should be excluded from L m .
According to C3, we exclude the statements that do not access or modify the shared variables g_var, s_var, which means Lines 14 and 16 should also be excluded.
In the end, the scope is determined as L m = {1, 2, 10, 11, 12, 13, 17}.
Note that although modify can be called in a single-threading site inside check (Line 6), we still conservatively include it in L m .
The reason is that it might be called within multithreading contexts (Line 13 and Line 15) -modify is protected by mutex m at Line 15 while unprotected at Line 13.
It is worth noting that line 15, although protected by m, may still happen-in-parallel [11,45] with lines 10 and 11.
However, since lines 10 and 11 have already been put in L m , we consider it sufficient to help provide more feedback to track thread-interleavings, with line 15 excluded from L m .
Overall, the static analysis is lightweight.
For example, the pointer analysis is flow-and context-insensitive; extraction of thread-aware results such as F f ork (in C1) and TShareVar (in C3) are over-approximated in that the statically calculated sets may be larger than the actual sets; C2 may aggressively exclude several statements that involve interleavings.
The benefit, however, is that it makes our analysis scalable to large-scale real-world programs.
With the knowledge of L m , we can instrument more deputy instructions (corresponding to statements in source code) inside the scope than the others, for exploring new transitions.
However, it is still costly to instrument on each instruction inside L m since this may significantly reduce the overall execution speed of the target programs.
It is also unnecessary to do so -although theoretically, interleavings may happen everywhere inside L m , many interleavings are not important because they do not change the values of shared variables in practice.
This means that we can skip some instructions for instrumentation, or equivalently instrument them with a probability.
We still instrument, despite less, on segments outside L m for exploration purposes [34].
For example, in Figure 1, we apply instrumentation on check, just in case the initial seeds are all rejected by the validity check and no intermediate feedback are available at all, making the executions extremely difficult to even enter compute.
Similarly, we can also selectively instrument some instructions outside L m .
The goal of calculating instrumentation probabilities is to strike a balance between execution overhead and feedback effectiveness by investigating code segments' complexity of the target programs.
First of all, MUZZ calculates a base instrumentation probability according to cyclomatic complexity [35], based on the fact that bugs or vulnerabilities usually come from functions with higher cyclomatic complexity [9,43].
For each function f , we calculate the complexity value: M c ( f ) = E( f ) − N( f ) + 2 where N( f ) is the number of nodes (basicblocks) and E( f ) is the number of edges in the function's control flow graph.
Intuitively, this value determines the complexity of the function across its basicblocks.
As 10 is considered to be the preferred upper bound of M c [35], we determine the base probability as:P e ( f ) = min E( f ) − N( f ) + 2 10 , 1.0 (1)We use P s as the probability to selectively instrument on the entry instruction of a basicblock that is entirely outside suspicious interleaving scope, i.e., none of the instructions inside the basicblock belong to L m .
Here, P s is calculated as:P s ( f ) = min P e ( f ), P s0 (2)where 0 < P s0 < 1.
Empirically, MUZZ sets P s0 = 0.5.
Further, for each basicblock b inside the given function f , we calculate the total number of instructions N(b), and the total number of memory operation instructions N m (b) (e.g., load/store, memcpy, free).
Then for the instructions within L m , the instrumentation probability is calculated as:P m ( f , b) = min P e ( f ) · N m (b) N(b) , P m0(3)where P m0 is a factor satisfying 0 < P m0 < 1 and defaults to 0.33.
The rationale of N m (b) N(b) is that vulnerabilities usually result from memory operation instructions [34], and executions on more such operations deserve more attention.
The coverage-oriented instrumentation algorithm is described in Algorithm 2.
It traverses functions in the target program P. For each basicblock b in function f , MUZZ firstly gets the intersection of the instructions inside both b and L m .
If this intersection L m (b) is empty, it instruments the entry instruction of b with a probability of P s ( f ).
Otherwise, 1) for the entry instruction in b, MUZZ always instruments it (i.e., with probability 1.0); 2) for the other instructions, if they are inside L m , MUZZ instruments them with a probability of P m ( f , b).
We will refer to our selection strategy over deputy instructions as M-Ins.
As a comparison, AFL-Ins always instruments evenly at the entry instructions of all the basicblocks.For the example in Figure 1, since the lines 21-25 and line 5 are out of L m , we can expect M-Ins to instrument fewer entry statements on their corresponding basicblocks.
Meanwhile, for the statements inside L m , M-Ins may instrument other statements besides the entry statements.
For example, 1 is the entry statement thus it must be instrumented; statement 2 may also be instrumented (with a probability) -if so, transition 1 → 2 can be tracked.
input :target program P, and suspicious interleaving scope L m output :program P instrumented with M-Insdeputies 1 for f ∈ P do 2 for b ∈ f do 3 L m (b) = L m ∩ b; 4 if L m (b) !
= / 0 then 5 for i ∈ b do 6 if is_entry_instr(i, b) then 7 P ← instrument_cov(P, i, 1.0); 8 else if i ∈ L m then 9 P ← instrument_cov P, i, P m ( f , b) ; 10 else 11 for b ∈ f do 12 i = get_entry_instr(b); 13 P ← instrument_cov P, i, P s ( f ) ; We apply threading-context instrumentation to distinguish thread identities for additional feedback.
This complements coverage-oriented instrumentation since the latter is unaware of thread IDs.
The context is collected at the call sites of F ctx = {TLock, TUnLock, TJoin}, each of which has the form TC = Loc, N ctx , where Loc is the labeling value of deputy instruction executed before this call site, and N ctx is obtained by getting the value of the key identified by current thread ID from the "thread ID map" collected by the instrumented function F S (to be explained in §4.4 is a context-signature that determines the overall thread-context of a specific execution.
Essentially, this is a sampling on threading-relevant APIs to track the thread-context of a specific execution.
As we shall see in §5.1, the occurrence of S ctx determines the results of cov_new_mt_ctx during seed selection.In Figure 1, each time when pthread_mutex_lock∈ TLock is called, MUZZ collects the deputy instruction prior to the corresponding call site (e.g., 3 ) and the thread ID label (e.g., T1) to form the tuple (e.g., 3 , T 1); these tuples form a sequence for TLock, and a hash value H(TLock) will be calculated eventually.
Similar calculations are applied for pthread_mutex_unlock and pthread_join.
When a user-space program does not specify any scheduling policy or priority, the operating system determines the actual schedule dynamically [23,26].
Schedule-intervention instrumentation aims to diversify the thread-interleavings to Algorithm 3: select_next_seed Strategy input :seed queue Q S , seed t at queue front output :whether t will be selected in this round 1 collaborate with coverage-oriented and thread-context instrumentations.
This instrumentation should be general enough to work for different multithreaded programs and extremely lightweight to keep runtime overhead minimal.POSIX compliant systems such as Linux, FreeBSD usually provide APIs to control the low-level process or thread schedules [23,26].
In order to intervene in the interleavings during the execution of the multithreading segments, we resort to the POSIX API pthread_setschedparam to adjust the thread priorities with an instrumented function named F S that will be invoked during fuzzing.
This function does two tasks: a) During repeated execution ( §5.2), whenever the thread calls F S , it updates the scheduling policy to SCHED_RR, and assigns a ranged random value to its priority.
This value is uniformly distributed random and diversifies the actual schedules across different threads.
With this intervention, we try to approximate the goal in §2.3.2.
b) For each newly mutated seed file, it calls pthread_self in the entry of F f ork to collect the thread IDs.
It has two purposes: 1) it informs the fuzzer that the current seed is multithreading-relevant; 2) based on the invocation order of F S , each thread can be associated with a unique ID N ctx starting from 1, 2, . . ., which composes "thread ID map" and calculates thread-context in §4.3.
The dynamic fuzzing loop follows the workflow of a typical GBF described in Algorithm 1.
To improve the feedback on multithreading context, we optimize seed selection ( §5.1) and repeated execution ( §5.2) for fuzzing multithreaded programs, based on the aforementioned instrumentations.
Seed selection decides which seeds to be mutated next.
In practice, this problem is reduced to: when traversing seed queue Q S , whether the seed t at the queue front will be selected for mutation.
Algorithm 3 depicts our solution.
The intuition is that we prioritize those seeds with new (normal) coverage or covering new thread-context.
In addition to following AFL's strategy by using has_new_trace(Q S ) to check whether there exists a seed, s, in Q S that covers a new transition (i.e., cov_new_trace(s)==true), MUZZ also uses has_new_mt_ctx(Q S ) to check whether there exists a seed in Q S with a new thread-context (S ctx ).
If either is satisfied, it means there exist some "interesting seeds" in the queue.
Specifically, if the current seed t covers a new threadcontext, the algorithm directly returns true.
If it covers a new trace, it has a probability of P ynt to be selected; otherwise, the probability is P ynn .
On the contrary, if no seeds in Q S are interesting, the algorithm selects t with a probability of P nnn .
Analogous to AFL's seed selection strategy [63], MUZZ sets P ynt = 0.95, P ynn = 0.01, P nnn = 0.15.
As to the implementation of cov_new_mt_ctx(t), we track the thread-context of calling a multithreading API in F ctx = {TJoin, TLock, TUnLock} (c.f. §4.3) and check whether the context-signature S ctx has been met before -when S ctx is new, cov_new_mt_ctx(t)=true; otherwise, cov_new_mt_ctx(t)=false.
Note that cov_new_trace(t)==true does not imply cov_new_mt_ctx(t)==true.
The reason is that (1) we cannot instrument inside the body of threading API functions (as they are "external functions") inside F ctx , hence cov_new_trace cannot track the transitions; (2) cov_new_mt_ctx also facilitates the thread IDs that cov_new_trace is unaware of.
Multithreaded programs introduce non-deterministic behaviors when different interleavings are involved.
As mentioned in §2.3.2, for a seed with non-deterministic behaviors, a GBF typically repeats the execution on the target program against the seed for more times.
With the help of F S (c.f. §4.4), we are able to tell whether or not the exhibited non-deterministic behaviors result from thread-interleavings.
In fact, since we focus on multithreading only, based on the thread-fork information kept by F S , the fuzzer can distinguish the seeds with non-deterministic behaviors purely by checking whether the executions exercise multithreading context.
Further, if previous executions on a seed induce more distinct values of S ctx (the number of these values for a provided seed t is denoted as C m (t)), we know that there must exist more threadinterleavings.
To determine the repeating times N c applied on t, we rely on C m (t).
In AFL, the repeating times on t is:N c (t) = N 0 + N v · B v , B v ∈ {0, 1}(4)where N 0 is the initial repeating times, N v is a constant as the "bonus" times for non-deterministic runs.
B v =0 if none of the N 0 executions exhibit non-deterministic behaviors; otherwise B v =1.
We augment this to fit for multithreading setting.N c (t) = N 0 + min N v , N 0 ·C m (t)(5)In both AFL and MUZZ, N 0 = 8, N v = 32.
For all the N c executions, we track their execution traces and count how many different states it exhibits.
The rationale of adjusting N c is that, in real-world programs the possibilities of threadinterleavings can vary greatly for different seeds.
For example, a seed may exhibit non-deterministic behaviors when executing compute in Figure 1 (e.g., races in g_var), but it exits soon after failing an extra check inside compute (typically, exit code >0).
For sure, it will exhibit fewer non-deterministic behaviors than a seed that is concurrently processed and the program exits normally (typically, exit code =0).
Here we provide some explanations to show why MUZZ's static and dynamic thread-aware strategies help to improve the overall fuzzing effectiveness.
1) Mutations on multithreading-relevant seeds are more valuable for seed mutation/generation.
Multithreading-relevant seeds themselves have already passed validity checks of the target program.
Compared to a seed that cannot even enter the thread-fork routines, it is usually much easier to generate a multithreading-relevant seed mutant from an existing multithreading-relevant seed.
This is because the mutation operations (e.g., bitwise/bytewise flips, arithmetic adds/subs) in grey-box fuzzers are rather random and it is rather difficult to turn an invalid seed to be valid.
Therefore, from the mutation's perspective, we prefer multithreading-relevant seeds to be mutated.2) MUZZ can distinguish more multithreadingrelevant states.
For example, in Figure 1, it can distinguish transitions 1 → 1 → 2 → 2 and 1 → 2 → 1 → 2 .
Then when two different seeds exercise the two transitions, MUZZ is able to preserve both seeds.
However, other GBFs such as AFL cannot observe the difference.
Conversely, when we provide less feedback for seeds that do not involve multithreading, MUZZ can distinguish less of these states and put less multithreading-irrelevant seeds in the seed queue.3) Large portions of multithreading-relevant seeds in the seed queue benefit subsequent mutations.
Suppose at some time of fuzzing, both MUZZ and AFL preserve 10 seeds (N all =10), and MUZZ keeps 8 multithreading-relevant seeds (N mt =8) while AFL keeps 6 (N mt =6).
Obviously, the probability of picking MUZZ generated multithreading-relevant seeds (80%) is higher than AFL's (60%).
After this iteration of mutation, more seed mutants in MUZZ are likely multithreadingrelevant.
in MUZZ are likely to be bigger than those in AFL.
Owing to more multithreading-relevant seeds in the queue and property 1), we can expect that: a) concurrency-vulnerabilities are more likely to be detected with the new proof-of-crash files mutated from multithreading-relevant files from the seed queue.
b) concurrency-bugs are more likely to be revealed with the (seemingly normal) files in the seed queue that violate certain concurrency conditions.
Providing more feedback for multithreading-relevant segments essentially provides a biased coverage criterion to specialize fuzzing on multithreaded programs.
Other specialization techniques, such as the context-sensitive instrumentation used by Angora [7], or the typestate-guided instrumentation in UAFL [52], provide similar solutions and achieve inspiring results.
The novelty of MUZZ lies in that we facilitate the multithreading-specific features as feedback to innovatively improve the seed generation quality.
It is worth noting that our solution only needs lightweight thread-aware analyses rather than deep knowledge of multithreading/concurrency; thus, it can scale to real-world software.
We implemented MUZZ upon SVF [46], AFL [63] , and ClusterFuzz [16].
The thread-aware ICFG construction leverages SVF's inter-procedural value-flow analysis.
The instrumentation and dynamic fuzzing strategies lay inside AFL's LLVM module.
The vulnerability analysis and concurrency-bug replaying components rely on ClusterFuzz's crash analysis module.
We archive our supporting materials at https://sites.google.com/view/mtfuzz.
The archive includes initial seeds for fuzzing, the detected concurrencyvulnerabilities and concurrency-bugs, the implementation details, and other findings during evaluation.Our evaluation targets the following questions: RQ1 Can MUZZ generate more effective seeds that execute multithreading-relevant program states?
RQ2 What is the capability of MUZZ in detecting concurrency-vulnerabilities (V m )?
RQ3 What is the effect of using MUZZ generated seeds to reveal concurrency-bugs (B m ) with bug detectors?
All these projects' single-thread functionalities have been intensively tested by mainstream GBFs such as AFL.
We try to use their latest versions at the time of our evaluation; the only exception is libvpx, which we use version v1.3.0-5589 to reproduce the ground-truth vulnerabilities and concurrencybugs.
Among the 12 multithreaded programs, pxz, GraphicsMagick, and ImageMagick use OpenMP library, while the others use native PThread.
Table 1 lists the statistics of the benchmarks.
The first two columns show the benchmark IDs and their host projects.
The next column specifies the command-line options.
In particular, four working threads are specified to enforce the program to run in multithreading mode.The rest of the columns are the static statistics.
Column "Binary Size" calculates the sizes of the instrumented binaries.
Column T pp records the preprocessing time of static analysis (c.f. §4.1).
Among the 12 benchmarks, vpxdec takes the longest time of approximately 30 minutes.
Columns N b , N i , and N ii depict the number of basicblocks, the number of total instructions, and the number of deputy instructions for M-Ins (c.f. §4.2), respectively.
Recall that AFL-Ins instruments evenly over entry instructions of all basicblocks, hence N b also denotes the number of deputy instructions in AFL, MAFL, and MOPT.
The last column,N ii −N b N b, is the ratio of more instructions MUZZ instrumented versus AFL (or MAFL, MOPT).
This ratio ranges from 6.0% (pbzip2-c or pbzip2-d) to 288.9% (x265).
Fortunately, in practice, this does not proportionally increase the runtime overhead.
Many aspects can affect this metric, including the characteristics of the target programs, the precision of the applied static analysis, and the empirically specified thresholds P s0 and P m0 .
Fuzzing Configuration The experiments are conducted on four Intel(R) Xeon(R) Platinum 8151 CPU@3.40GHz workstations with 28 cores, each of which runs a 64-bit Ubuntu 18.04 LTS; the evaluation upon a specific benchmark is conducted on one machine.
To make fair comparisons, MUZZ, MAFL and AFL are executed in their "fidgety mode" [65], while MOPT is specified with -L 0 to facilitate its "pacemaker mode" [33].
The CPU affinity is turned off during fuzzing to avoid multiple threads being bound to a single CPU core.
During fuzzing, we run each of the aforementioned fuzzers six times against all the 12 benchmark programs, with a time budget of 24 hours.
Since all the evaluated programs are set to run with four working threads and the threads are mapped to different cores, it takes each fuzzer approximately 12 × 6 × 24 × 4 = 6912 CPU hours.
Table 2 shows the overall fuzzing results in terms of newly generated seeds.
We collect the total number of generated seeds (N all ) and the number of seeds that exercise the multithreading context (N mt ).
In AFL's jargon, N all corresponds to the distinct paths that the fuzzer observes [63].
The multithreading-relevant seeds are collected with a separate procedure, based on the observations that they at least invoke one element in TFork.
Therefore, N mt tracks the different multithreading execution states during fuzzing -a larger value of this metric suggests the fuzzer can keep more effective thread-interleaving seeds.
We sum up those seed files across all six fuzzing runs to form N all and N mt in in all the benchmarks.
For example, in pxz-c, the number of generated multithreadingrelevant seeds in MAFL is 3401, which is more than AFL (2470) and MOPT (2634).
Correspondingly, the percentage of multithreading-relevant seeds in MAFL is 60.3%; for AFL and MOPT, they are 46.1% and 47.2%, respectively.
Considering MAFL, AFL, MOPT apply coverage-oriented instrumentation (M-Ins), we can conclude that other strategies in MAFL, including thread-context instrumentation, schedule-intervention instrumentation, and the optimized dynamic strategies, also contribute to effective seed generation.
Answer to RQ1: MUZZ has advantages in increasing the number and percentages of multithreading-relevant seeds for multithreaded programs.
The proposed three thread-aware instrumentations and dynamic fuzzing strategies benefit the seed generation.
For vulnerability detection, we denote the total number of proof-of-crash (POC) files generated during fuzzing as N c .
The vulnerability analysis component (right-bottom area as C in Figure 3) analyzes the POC files and categorizes them into different vulnerabilities.
This basically follows ClusterFuzz's practice [16]: if two POC files have the same last N lines of backtraces and the root cause is the same (e.g., both Table 3 to evaluate MUZZ's concurrency-vulnerability detection capability.The number of multithreading-relevant POC files, N m c , is important since it corresponds to different crashing states when executing multithreading context [27,34].
It is apparent that MUZZ has the best results of N m c in all the benchmarks that have V m vulnerabilities (e.g., for im-cnvt, MUZZ: 63, MAFL: 23, AFL: 6, MOPT: 6).
Moreover, MAFL also exhibits better results than AFL and MOPT (e.g., for pbzip2-c, MUZZ: 6, MAFL: 6, AFL: 0, MOPT: 0).
This suggests that MUZZ's and MAFL's emphasis on multithreading-relevant seed generation indeed helps to exercise more erroneous multithreading-relevant execution states.The most important metric is N m v since our focus is to detect concurrency-vulnerabilities (V m ).
Table 3 shows that MUZZ has the best results: MUZZ detects 9 concurrencyvulnerabilities, while MAFL, AFL and MOPT detects 5, 4, 4, respectively.
Detected V m can be divided into three groups.
1) V m caused by concurrency-bugs.
We term this group of vulnerabilities as V cb .
The 4 vulnerabilities in im-cnvt all belong to this group -the misuses of caches shared among threads cause the data races.
The generated seeds may exhibit various symptoms such as buffer-overflow and memcpyparam-overlap.
MUZZ found all the 4 vulnerabilities, while the others only found 2.
We also observed that for the 2 vulnerabilities that are detected by all these fuzzers, MAFL's detection capability appears more stable since it detects both in all its six fuzzing runs, while the others can only detect them at most in five runs (not depicted in the table).
2) V m triggered in multithreading only but not induced by concurrencybugs.
For example, the vulnerability in pbzip2-d stems from a stack-overflow error when executing a function concurrently.
This crash can never happen when pbzip2-d works in singlethread mode since it does not even invoke that erroneous function.
In our evaluation, MUZZ detected this vulnerability while the other fuzzers failed.
Another case is the vulnerability in pbzip2-c, which was detected by MUZZ and MAFL, but not by AFL or MOPT.
3) Other concurrency-vulnerabilities.
The characteristics of these V m are that their crashing backtrace contains multithreading context (i.e., TFork is invoked), however, the crashing condition might also be occasionally triggered when only one thread is specified.
The V m vulnerabilities detected in vpxdec and x264 belong to this category.
In particular, MUZZ detects 2 vulnerabilities in vpxdec while MAFL, AFL, and MOPT only find 1.
We consider the reason behind the differences w.r.t. N m c and N m v among the fuzzers to be that, MUZZ keeps more "deeper" multithreading-relevant seeds that witness different execution states, and mutations on some of them are more prone to trigger the crashing conditions.Columns N c , N s c , N s v are metrics less focused.
But we can still observe that 1) according to N c , MUZZ (and MAFL) can exercise more total crashing states; 2) despite that the values of N s c from MUZZ are usually smaller, MUZZ can still find all the (categorized) V s detected by other fuzzers.From the 12 evaluated benchmarks, we reported the 10 new vulnerabilities (sum of MUZZ's results in columns N m v and N s v except for row vpxdec; 7 of them belong to V m ), all of them have been confirmed or fixed, 3 of which have already been assigned CVE IDs.
Besides, we also conducted a similar evaluation on libvpx v1.8.0-178 (the git HEAD version at the time of evaluation).
MUZZ detected a 0-day concurrencyvulnerability within 24 hours (among six fuzzing runs, two of them detected the vulnerability in 5h38min and 16h07min, Table 3: Fuzzing results on MUZZ, MAFL, AFL and MOPT, in terms of crashes and vulnerabilities.
Some projects (e.g., lbzip2-c) are excluded since there were no crashes/vulnerabilities detected by any of the fuzzers.
6 0(+6) 1(0) 0 0 0 0(+6) 0(+1) 0 0 0 0(+6) 0(+1) 0 0 pbzip2-d 15 15 1 0 0 0 0(+15) 0(+1) 0 0 0 0(+15) 0(+1) 0 0 0 0(+15) 0(+1) 0 0 im-1(0) 0 0 78 78(+25) 1(0) 0 0 66 66(+37) 1(0) 0 0 x265 43 0 0 43 1 52 0(0) 0(0) 52 1 62 0(0) 0(0) 62 1 59 0(0) 0(0) 59 1respectively), while MAFL, AFL and MOPT failed to detect it in 15 days (360 hours) in all their six fuzzing runs.
The newly detected vulnerability has been assigned with another CVE ID.
The vulnerability details are available in Table 5.
Given the fact that there are extremely few CVE records caused by concurrency-vulnerabilities (e.g., 202 among 70438, based on records from CVE-2014-* to CVE-2018-*) [48], MUZZ demonstrates the high capability in detecting concurrency-vulnerabilities.
Answer to RQ2: MUZZ demonstrates superiority in exercising more multithreading-relevant crashing states and detecting concurrency-vulnerabilities.
The fuzzing phase only detects the vulnerabilities caused by crashes, but the seemingly normal seed files generated during fuzzing may still execute paths that trigger concurrencyviolation conditions like data-races, deadlocks, etc.
We detect concurrency-bugs in concurrency-bug revealing component ( D , right-top in Figure 3).
It is worth noting that our goal is not to improve the capabilities of concurrency-bug detection over existing techniques such as TSan [42], Helgrind [49], or UFO [21].
Instead, we aim to reveal as many bugs as possible within a time budget, by replaying against fuzzergenerated seeds with the help of these techniques.
In practice, this component feeds the target program with the seeds that were generated during fuzzing as its inputs, and facilitate detectors such as TSan to reveal concurrency-bugs.
During this evaluation, we compiled the target programs with TSan and replayed them against the fuzzer-generated multithreadingrelevant seeds (corresponding to N mt in Table 2).
We did not replay with all the generated seeds (corresponding to N all in Table 2) since seeds not exercising multithreading context will not reveal concurrency-bugs.
We limit our replay time budget to two hours; in §6.5.4 we discuss the rationale of this configuration.
The next is to determine the replay pattern per seed to reveal more concurrencybugs within this budget.
This is necessary since TSan may fail to detect concurrency-bugs in a few runs when it does not observe concurrency violation conditions [12,42,49].
Meanwhile, as the time budget is limited, we cannot exhaustively replay against a given seed to see whether it may trigger concurrency-violations -in the worst case, we may waste time in executing against a seed that never violates the conditions.
We provide two replay patterns.
It is fair to compare replay results w.r.t. P1 and P2 in that the time budget is fixed.
The difference between the two patterns is that seeds' execution orders and accumulated execution time spent on them can be rather different.
Table 4 depicts the results for concurrency-bug revealing with P1 and P2.
N m e is the number of observed concurrencyviolation executions and N m B is the number of concurrencybugs (B m ) according to their root causes.
For example, it only counts one concurrency-bug (N m B =1) even when the replaying process observes 10 data-race pairs across executions (N m e =10), as long as the root cause of the races is unique.
We analyze this table from two perspectives.First, MUZZ demonstrates superiority in concurrency-bug detection regardless of replay patterns.
This is observed based on the "best results" for each metric in each pattern.
MUZZ achieves the best results for most projects.
better than MUZZ.
Second, as to MUZZ and MAFL, P2 is probably better than P1.
It is concluded based on the fact that P2's "best results" are all better than P1's.
For example, as to N m e in x264, the best result of N m e is achieved with P2 (P1: 68, P2: 91); similarly, the best result of N m B also comes from P2 (P1: 8, P2: 9).
In the meantime, there seems to be no such implication inside AFL or MOPT.
Besides the numbers of concurrencyviolations or concurrency-bugs, §6.5.3 provides a case study on gm-cnvt that demonstrates P2's advantages over P1 w.r.t. time-to-exposure of the concurrency-bugs.
We have reported all the newly detected 19 concurrencybugs (excluding the 3 concurrency-bugs in vpxdec-v1.3.0-5589) to their project maintainers (c.f., Table 5 for the details).
Answer to RQ3: MUZZ outperforms competitors in detecting concurrency-bugs; the value N c calculated during fuzzing additionally contributes to revealing these bugs.
This section discusses miscellaneous concerns, issues and observations for MUZZ's design and evaluation.
Using empirical constant parameters for grey-box fuzzing is practiced by many fuzzing techniques [6,33,63].
For example, AFL itself has many hard-coded configurations used by default; MOPT additionally has the suggested configuration to control the time to move on to pacemaker mode (i.e., -L 0).
In MUZZ, constant parameters are used in two places.
(1) The upper bounds for coverage-oriented instruction: P s0 (defaults to 0.5) and P m0 (defaults to 0.33).
These default values inspire from AFL's "selective deputy instruction instrumentation" strategy to make the instrumentation ratio to be 0.33 when AddressSanitizer is involved during instrumentation.
Larger values of P s0 and P m0 increases the instrumentation ratio only if the thresholds are frequently reached.
Subsequently, the instrumented program has these symptoms: a) the program size after instrumentation increases; b) the execution state feedback is potentially better; c) the instrumentation-introduced execution speed slowdown is more evident.
Therefore, increasing the values of P s0 and P m0 reflects a tradeoff between precise feedback and its overhead.
In our benchmarks, when we assign P s0 =0.5, P m0 =0.33,• For im-cnvt, the speed slowdown is about 15% compared to default settings, while the capability of detecting concurrency-vulnerabilities and concurrency-bugs are similar; meanwhile, there are a few more multithreadingrelevant seeds (N mt ) but N mt N all is slightly smaller.
• For pbzip2-c, the differences brought by changes of P s0and P m0 from the default settings are all neglectable.
We believe there are no optimal instrumentation thresholds that work for all the projects; therefore MUZZ provides the empirical values as the defaults.
(2) The seed selection probabilities P ynt = 0.95, P ynn = 0.01, P nnn = 0.15 in Algorithm 3.
These constants are not introduced by MUZZ, but based on AFL's "skipping probability" to conditionally favor seeds with new coverage [63].
Since the 12 benchmarks that we chose are quite diversified (c.f., §6.1.2), it is considered fair to use default settings for these parameters, when comparing MUZZ, MAFL with other fuzzers such as AFL, MOPT.
In practice, we suggest keeping MUZZ's default settings to test other multithreaded programs.
The goal of MUZZ's schedule-intervention is to diversify interleavings during repeated executions in the fuzzing phase.
During the evaluation, we did not separately evaluate the effects of schedule-intervention instrumentation.
However, based on our observation, this instrumentation is important to achieve more stable fuzzing results.
Two case studies can support this statement.
a) We turned off schedule-intervention instrumentation in MUZZ and fuzzed lbzip2-c six times on the same machine.
The calculated value of N mt N all is 54.5% (= 4533/8310), which is lower than the result in Table 2 (63.6% = 5127/8056).
Since 54.5% is still greater than the results of AFL (42.9%) and MOPT (41.8%), this also indicates MUZZ's other two strategies indeed benefit the multithreading-relevant seed generation for fuzzing.
Table 5: Newly detected vulnerabilities and concurrency-bugs.
This summarizes the new vulnerabilities and concurrency-bugs evaluated in Table 3 and Table 4 over the 11 benchmarks (libvpx-v1.3.0-5589 results are all excluded), and includes one concurrency-vulnerability in vpxdec-v1.8.0-178 which was mentioned in §6.3.
b) We turned off schedule-intervention instrumentation in MUZZ and fuzzed im-cnvt on a different machine.
In all the six fuzzing runs it only detects three concurrencyvulnerabilities which is less than the result in Table 3 (N m v =4).
Meanwhile, when the schedule-intervention instrumentation is re-enabled, MUZZ can still detect four concurrency-vulnerabilities in that machine.
In §6.4, we demonstrate P2's advantage over P1 in terms of occurrences of concurrency-violations (N m e ) and the number of categorized concurrency-bugs (N m B ).
Another interesting metric is the time-to-exposure capability of these two replay patterns -given the ground truth that the target programs contain certain concurrency-bugs, the minimal time cost for each pattern to reveal all the known bugs.
This metric can further distinguish the two replay patterns' capabilities in terms of revealing concurrency-bug.
We conducted a case study on gm-cnvt.
From Table 4, it is observable that with both P1 and P2, TSan detected four concurrency-bugs (N m B ) by replaying the MAFL generated multithreading-relevant seeds (totally 10784) from Table 2; besides, their N m e results are also similar (P1: 79, P2: 83).
We repeated six times against the 10784 seeds by applying P1 and P2.
When a replaying process detects all the four different ground-truth concurrency-bugs, we record the total execution time (in minutes).
Table 6 shows the results.In Table 6, compared to P1, we can observe that P2 reduces the average time-to-exposure from 66.5 minutes to 34.1 minutes.
This fact means, for example, given a tighter replay time budget (say, 60 minutes), P1 has a high chance to miss some of the four concurrency-bugs.
Moreover, P2 is more stable since the timing variance is much smaller than that of Table 4, for the concurrency-bug revealing capability of MAFL, the P2's result in gm-cnvt is likely to be much better than P1's.
The evaluation of time-to-exposure suggests that, given a set of seeds, P2 is prone to expose concurrency-bugs faster and more stable.
Since P2 is closely relevant to scheduleintervention instrumentation ( §4.4) and repeated execution ( §5.2), this also indicates that these strategies are helpful for concurrency-bug revealing.
We chose two hours (2h) as the time budget in the reply phase during evaluation.
Unlike the fuzzing phase, which aims to generate new seed files that exercise multithreading context, the replay phase runs the target program against existing seeds (generated during fuzzing).
Therefore, the criterion is to 1) minimize the time for replay; 2) ensure that replay phase traverses all the generated seeds.
For projects with less generated (multithreading-relevant) seeds (e.g., N mt =126 for pbzip2-c when applying MUZZ), traversing the seeds (with both P1 and P2) once are quite fast; however for projects with more generated seeds (e.g., N mt =13774 for gm-cnvt when applying MUZZ), this requires more time.
To make the evaluation fair, we use the fixed time budget for all the 12 benchmarks, where seeds in projects like pbzip2-c will be traversed repeatedly until timeout.
During the evaluation, we found 2h to be moderate since it can traverse all the generated multithreading-relevant seeds at least once for all the projects.Less time budget, e.g., 1h, may make the replay phase to miss certain generated seeds triggering concurrency violation conditions.
In fact, from Table 6, we see that time-to-exposure for the concurrency-bugs may take 101.5 minutes.
Meanwhile, more time budget, e.g., 4h, might be a waste of time for the exercised 12 benchmarks.
In fact, in a case study for gm-cnvt, when time budget is 4h, despite that N m e is nearly doubled, the number of revealed B m (i.e., N m B ) is still the same as the results in Table 4, regardless of P1 or P2.
Specific to the nature of multithreaded programs and our evaluation strategy to determine seeds' relevance with multithreading, we decide not to provide some commonly-used statistical results [27].
First, it is unfair to track coverage over time when comparing MUZZ, MAFL with AFL or MOPT due to the different meanings of "coverage".
In fact, owing to coverageoriented instrumentation (in MUZZ) and threading-context instrumentation (in MUZZ and MAFL), MUZZ and MAFL cover more execution states (corresponding to N all ), therefore naturally preserve more seeds.
That is also the reason that in §6.2 the values of N mt and N mt N all are more important than N all .
Second, we cannot compare the multithreading-relevant paths over time among MUZZ, MAFL, AFL, and MOPT.
This reason is simple: we resort to a separate procedure after fuzzing to determine whether it covers thread-forking routines.
We have to do so since AFL and MOPT do not provide a builtin solution to discovering seeds' relevance with multithreading.
Consequently, we cannot plot multithreadingrelevant crashing states over time.Third, despite that the statistical variance is important, it is not easy to be calculated comprehensively.
During evaluation, to reduce the variance among individuals, we apply an ensemble strategy by sharing seeds among the six runs, for each of the specific fuzzers [63].
However, for multithreaded target programs, another variance comes from the thread scheduling for different threads (in our experiments, four working threads were specified).
MUZZ and MAFL have the schedule-intervention instrumentation to help diversify the effects, while it is absent in AFL and MOPT.
In fact, from the case studies in §6.5.2, we envision that the variance may be huge for different machines under different workloads.
Due to this, providing fair statistical results w.r.t. the variance may still be impractical.
Therefore, we tend to exclude variance metrics and only choose those that exhibit the "overall results", i.e., N mt , N mt N all , N m c , N m v , N m e , and N m B .
Similarly, the case studies or comparisons in §6.2, §6.3, §6.4 are all based on "overall results".
During the evaluation, we indeed observed that the results of MUZZ and MAFL are more stable than those of AFL and MOPT.
The most relevant is the fuzzing techniques on concurrencyvulnerability detection.
ConAFL [30] is a thread-aware GBF that focuses on user-space multithreaded programs.
Much different from MUZZ's goal to reveal both V m and B m , ConAFL only detects a subset of concurrency-bug induced vulnerabilities (V cb ) that cause buffer-overflow, double-free, or useafter-free.
ConAFL also utilizes heavy thread-aware static and dynamic analyses, making it suffer from scalability issues.
The other difference is that MUZZ's thread-aware analyses aim to provide runtime feedback to distinguish more execution states in multithreading contexts, to bring more multithreading-relevant seeds; meanwhile, ConAFL relies on the discovery of sensitive concurrency operations to capture pairs that may introduce the aforementioned three kinds of vulnerabilities.
Further, since the static and dynamic analyses aim to capture and intervene "sensitive concurrency operation pairs", ConAFL suffers from the scalability issue.
In fact, the biggest binary it evaluated was 196K (bzip2smp), while MUZZ can handle programs scaling to 19.4M (im-cnvt).
In the evaluation, we did not evaluate ConAFL -the GitHub version of ConAFL (https://github.com/Lawliar/ConAFL) does not work since its static analysis is not publicly available and it is not trivial to implement that technique ourselves; further, we have not obtained the runnable tool after we requested from the authors.
RAZZER [24] utilizes a customized hypervisor to control thread-interleaving deterministically to trigger data races in Linux kernel.
It is a kernel fuzzer that cannot reveal multithreading-relevant bugs in user-space programs.
As a matter of fact, the proof-of-crashes are essentially sequences of system calls that could trigger race conditions, and the fix of the detected vulnerabilities requires patches to the kernel code.
Consequently, the guidance of fuzzing is also different.
RAZZER spots the over-approximated racing segments and tames non-deterministic behavior of the kernel such that it can deterministically trigger a race.
While MUZZ's solution is to distinguish more thread-interleaving states to trap the fuzzing to reveal more multithreading-relevant paths.
Practically, it is not easy to effectively sequentialize the thread-interleavings to fuzz the user-space programs [64].
Multithreading-relevant bugs are inherently deep.
To reveal deep bugs in the target programs, some GBFs facilitate other feedback [7,14,29,44,52,55,56,61].
Angora [7] distinguishes different calling context when calculating deputy instruction transitions to keep more valuable seeds.
Driller [44], QSYM [61], and Savior [8] integrate symbolic execution to provide additional coverage information to exercise deeper paths.
MUZZ inspires from these techniques in that it provides more feedback for multithreading context with stratified coverage-oriented and thread-context instrumentations, as well as schedule-intervention instrumentation.
Other fuzzing techniques utilize the domain knowledge of the target pro-gram to generate more effective seeds [39,53,54].
Skyfire [53] and Superion [54] provide customized seed generation and mutation strategies on the programs that feed grammar-based inputs.
SGF [39] relies on the specifications of the structured input to improve seed quality.
These techniques are orthogonal to MUZZ and can be integrated into seed mutation (c.f. B in Figure 3).
Static concurrency-bug (B m ) predictors aim to approximate the runtime behaviors of a concurrent program without actual execution.
Several static approaches have been proposed for analyzing Pthread and Java programs [40,45,50].
LOCK-SMITH [40] uses existential types to correlate locks and data in dynamic heap structures for race detection.
Goblint [50] relies on a thread-modular constant propagation and points-to analysis for detecting concurrent bugs by considering conditional locking schemes.
[51] scales its detection to large codebases by sacrificing soundness and suppressing false alarms using heuristic filters.
FSAM [45,46] proposes a sparse flowsensitive pointer analysis for C/C++ programs using contextsensitive thread-interleaving analysis.
Currently, MUZZ relies on flow-and context-insensitive results of FSAM for thread-aware instrumentations.
We are seeking solutions to integrating other bug prediction techniques to further improve MUZZ's effectiveness.
There are a large number of dynamic analyses on concurrencybugs.
They can be divided into two categories: modeling concurrency-bugs and strategies to trigger these bugs.The techniques in the first category [12,41,42,59] typically monitor the memory and synchronization events [19].
The two fundamentals are happens-before model [12] and lockset model [41].
Happens-before model reports a race condition when two threads read/write a shared memory arena in a causally unordered way, while at least one of the threads write this arena.
Lockset model conservatively considers a potential race if two threads read/write a shared memory arena without locking.
Modern detectors such as TSan [42], Helgrind [49] usually apply a hybrid strategy to combine these two models.
MUZZ does not aim to improve existing concurrency violation models; instead, it relies on these models to detect concurrency-bugs with our fuzzer-generated seeds.The second category of dynamic analyses focuses on how to trigger concurrency violation conditions.
This includes random testings that mimic non-deterministic program executions [4,25,38], regression testings [47,60] that target interleavings from code changes, model checking [13,57,62] and hybrid constraint solving [20][21][22] approaches that systematically check or execute possible thread schedules, heuristically avoid fruitless executions [10,17,18,66], or utilizing multicore to accelerate bug detection [37].
Our work differs from all the above, as our focus is not to test schedules with a given seed file, but to generate seed files that execute multithreadingrelevant paths.
In particular, our goal of schedule-intervention instrumentation is to diversify the actual schedules to help provide feedback during fuzzing.
This paper presented MUZZ, a novel technique that empowers thread-aware seed generation to GBFs for fuzzing multithreaded programs.
Our approach performs three novel instrumentations that can distinguish execution states introduced by thread-interleavings.
Based on the feedback provided by these instrumentations, MUZZ optimizes the dynamic strategies to stress different kinds of multithreading context.
Experiments on 12 real-world programs demonstrate that MUZZ outperforms other grey-box fuzzers such as AFL and MOPT in generating valuable seeds, detecting concurrency-vulnerabilities, as well as revealing concurrency-bugs.
This research was supported
