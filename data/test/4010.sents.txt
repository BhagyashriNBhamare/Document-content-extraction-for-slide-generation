The cut-and-choose technique plays a fundamental role in cryptographic-protocol design, especially for secure two-party computation in the malicious model.
The basic idea is that one party constructs n versions of a message in a protocol (e.g., garbled circuits); the other party randomly checks some of them and uses the rest of them in the protocol.
Most existing uses of cut-and-choose fix in advance the number of objects to be checked and in optimizing this parameter they fail to recognize the fact that checking and evaluating may have dramatically different costs.
In this paper, we consider a refined cost model and formalize the cut-and-choose parameter selection problem as a constrained optimization problem.
We analyze "cut-and-choose games" and show equilibrium strategies for the parties in these games.
We then show how our methodology can be applied to improve the efficiency of three representative categories of secure-computation protocols based on cut-and-choose.
We show improvements of up to an-order-of-magnitude in terms of band-width, and 12-106% in terms of total time.
Source code of our game solvers is available to download at https://github.com/cut-n-choose.
Most efficient implementations for secure two-party computation in the semi-honest setting rely on garbled circuits.
One party, acting as circuit generator, prepares a garbled circuit for the function of interest and sends it to the other party along with garbled values corresponding to its input.
The second party, who will serve as the circuit evaluator, obtains garbled values for its own inputs using oblivious transfer, and then evaluates the garbled circuit to obtain the result.The primary challenge in handling malicious adversaries is to ensure that the garbled circuit sent by the first party is constructed correctly.
The cut-and-choose paradigm is a popular and efficient mechanism for doing so.
The basic idea is that the circuit generator produces and sends several garbled circuits; the circuit evaluator checks a random subset of these, and evaluates the rest to determine the final result.
Since its formal treatment by Lindell and Pinkas [16], numerous works have improved various aspects of the cut-and-choose methodology and used it to design secure protocols [28,18,24,25,26,17,14,6,15,10,27,4,2,19,11,1,20].
These prior works fall roughly into three categories: 1.
MajorityCut.
Here the circuit evaluator determines its output by taking the majority value among the evaluated garbled circuits.
Thus, security holds as long as a majority of the evaluated circuits are correct.
This is the classic approach adopted by many papers [16,28,18,17] and implementations [25,26,14].
2.
SingleCut.
Here the circuit evaluator is able to obtain the correct output as long as at least one of the evaluated circuits are correctly generated.
Schemes adopting this approach include [15,10,4,2].
3.
BatchedCut.
This considers a slightly different setting in which the parties repeatedly evaluate some function, and the goal is to obtain good amortized efficiency by batching the cut-and-choose procedure either across multiple instances of secure computation [19,11], or at the gate level [24,6].
Although SingleCut is asymptotically better than MajorityCut, some SingleCut protocols [15] require using MajorityCut on a smaller circuit as a sub-routine, and therefore optimizations to MajorityCut can result in efficiency improvements to SingleCut.
In addition, MajorityCut works better for applications with long outputs as its cost does not grow with output length.When setting parameters for cut-and-choose protocols, in order to optimize efficiency for some target level of security, state-of-the-art approaches treat circuit checking roughly as expensive as circuit evaluation, and hence strive to optimize the total number of garbled cir- To be conservative in estimating r, figures assume 128-bit labels.
a One-block AES128 with 128-bit wire labels.
(Non-free gate counts, 6800, reported in [3,31], hence 6800 × 256/(256 + 128) ≈ 4533).
b A single multiplication of two 64-bit IEEE-754 floating-point numbers [21].
c Securely compute an oblivious access to an ORAM of one million 32-bit numbers [21].
d Sorting one million 32-bit numbers [21].
cuits involved.
Although some researchers [15,7,2] observed the asymmetry in the cost of checking and evaluation, they did not explore the cost asymmetry further, and did not investigate the possibility of optimizing cutand-choose parameters based on this asymmetry.As evidenced by many recent prototypes [21,29,20,5,9,26,25,14,13], network communication has become the most prominent bottleneck of garbled-circuit protocols, especially when exploiting dedicated hardware [3,8] or parallelism [5,23,13] for faster garbling/evaluation.
However, the bandwidth costs are markedly different for checking and evaluating circuits: garbled circuits that are evaluated must be transmitted in their entirety, but checking garbled circuits can be done by generating the circuit from a short seed and committing to the circuit using a succinct commitment [7,14,2].
Table 1 presents, in the context of a few example applications, the bandwidth costs for sending an entire circuit (i.e. the costs for an evaluated circuit) versus the cost for committing to the circuit (which, for simplicity, requires only one SHA256 hash), and thus a sample ratio r that we use in this paper as a variable.Based on these observations, we propose a new approach to optimizing parameters in cut-and-choose protocols.
Our approach casts the interaction between the circuit generator and circuit evaluator as a game, computes the optimal strategies in this game (which, interestingly, turn out to be mixed strategies), and then sets parameters while explicitly taking into account the relative costs of circuit checking and circuit evaluation.
Our optimizations result in cut-and-choose approaches that can be easily integrated into prior protocols, and can reduce the bandwidth in these protocols by an order-ofmagnitude in some settings.
The protocol of Lindell and Pinkas [16] checks exactly half the circuits, an idea followed in many subsequent works [25,17].
They showed that by generating n circuits and checking a random subset of size n/2, a cheating generator succeeds in convincing the evaluator to accept an incorrect output with probability at most 2 −0.311n .
Thus, to achieve (statistical) security level 2 −40 , their protocol requires 128 garbled circuits.
Shen and Shelat [26] slightly improved the bound to 2 −0.32s by opening roughly 60% (instead of one half) of the circuits; this reduces the number of garbled circuits needed to 125 for 2 −40 security.
These protocols belong to the MajorityCut category in our terminology.The idea of using SingleCut protocols was subsequently introduced [15,4,2].
Here, the evaluator chooses whether to check each circuit with independent probability 1/2; now n circuits suffice to achieve security level 2 −n .
Most recently, several works [19,11,20] have proposed to amortize the cost of cut-and-choose across multiple evaluations of the same function.
Along with the LEGO family of protocols [6,24] that amortize checks at the gate level (rather than the circuit level), they all fall in the class of BatchedCut protocols.
These works show that cut-and-choose can be very efficient in an amortized sense, requiring fewer than 8 circuits per execution to achieve 2 −40 security when amortizing over 1000 executions.
A brief explanation of the BatchedCut idea is given at the beginning of Section 3.3.
We introduce a game-theoretic approach to study cutand-choose in the context of secure-protocol design.
The simplest version of cut-and-choose can be treated as a zero-sum game (where the utilities are 0/1 for the loser/winner) between the evaluator and the generator in which the generator wins if it can produce enough incorrect circuits to skew the protocol without being detected.
Finding an optimal strategy for the evaluator can be cast as solving a linear-program and results in a randomized strategy for choosing the number of circuits to check.
This linear program can be further refined to take into consideration the different cost of checking vs evaluating (i.e., the ratio r).
Analyzing the equilibrium of this game leads to a constrained optimization problem that can be used to derive more efficient protocols meeting a targeted security bound (e.g. ε = 2 −40 as per many published implementations).
Our techniques enable optimization based on the precise relative costs of checking and evaluating, which in turn may depend on the function being computed as well as characteristics of specific deployment settings, such as software, hardware configuration and network condition, etc.
This provides the ability to "tune" protocols to specific applications in a much more fine-grained way than before.
We demonstrate that doing so can lead to bandwidth savings of 1.2-10×.
We concretely apply our methodology to three representative types of cut-and-choose-based securecomputation protocols, and show a significant overall improvement in the bandwidth usage.
For example, we are able to reduce the network traffic by up to an orderof-magnitude in comparison with the state-of-the-art SingleCut (see Figure 5) and MajorityCut (see Figure 2) protocols, and savings of 20% ∼ 80% for state-of-the-art (already highly optimized) BatchedCut protocols (see Fig- ure 8).
Our improvements do not require any additional cryptographic assumptions and come with little development overhead.
Notation.
Throughout this paper, we implicitly fix the semantic meaning for a few frequently-used variables (unless explicitly noted otherwise) as in Table 2.
Let e and k be the numbers of evaluate-circuits and check-circuits, respectively.
Let r be the ratio between the costs of evaluating and checking a circuit.
In the case when the parameters e, k are set deterministically and public, the cut-and-choose parameter optimization problem can be expressed as the following non-linear programming problem:arg min e,k r · e + k subject to max b Pr a (e, k, b) ≤ ε,where ε, r are known input constants; Pr a (e, k, b) is the probability of a successful attack; and b is the total number of bad circuits generated by the malicious generator.In the case when at least one of the two parameters (e and k) is randomly picked by the circuit evaluator from some public distributions (but sampled values remain secret to the circuit generator at the time of circuit generation), the optimization problem takes a more general form arg minS E E[cost(r, S E )] subject to E [Pr a (S E , S G )] ≤ ε, ∀ S Gwhere S E and S G are the circuit evaluator's and the circuit generator's strategies, respectively; cost is the cut-andchoose cost function, and E[·] denotes the expectation function.
Note that the cost function does not need to account for pre-maturely terminated protocol executions (due to detected cheating activity).
Our goal is to identify the best S E for the evaluator.
We leave the notion of S E and S G abstract for now but will give more concrete representations when analyzing specific protocols in Section 3.
We stress that, in contrast to the common belief used in the state-of-the-art cost analysis of cut-and-choose protocols, the cost of cut-and-choose is usually not best represented by n-the total number of circuits generated, but rather by a cost ratio r between checking and evaluation which depends on many factors such as (1) the kind of cost (e.g., bandwidth or computation); (2) the deployment environment (e.g., network condition, distribution of computation power on the players, buffering, etc.) (3) the specific cryptographic primitives and optimization techniques (e.g., the garbling scheme) used in a protocol.
Therefore, the best practice would be always micro-benchmarking the ratio between the per circuit cost of evaluation and checking before running the protocol, and then select the best cut-and-choose strategies accordingly.
The main thesis of this work is, Cut-and-choose protocols should be appropriately configured based on the security requirement (ε) and the cost ratio (r) benchmarked at run-time.
Such practice can bring significant cost savings to many cut-and-choose based cryptographic protocols.To support our thesis, we have formalized the cut-andchoose-based protocol configuration problem into a constrained optimization problem over a refined cost model.
Our solutions to the constrained optimization problem imply randomized strategies are optimal.
We show how to support randomized strategies in the state-of-the-art cut-and-choose-based cryptographic protocols with only small changes.
We applied this methodology to analyze three major types of cut-and-choose schemes and the experimental results corroborate our thesis.
We have implemented a search tool for each category of schemes to output the optimal parameters.
The tool is available at https://github.com/cut-n-choose.
In this section, we show how our general idea can be applied to three main types of two-party secure computation protocols that are based on the cut-and-choose method to substantially improve their performance.
We assume that n is fixed and public, while e will be selected from some distribution and remain hidden to the generator until all circuits are generated and committed.
MajorityCut strategy stems from an intuitive folklore idea: the circuit evaluator randomly selects k (out of a total n circuits) to check for correctness, evaluates the remaining e = n−k circuits, and outputs the majority of the e evaluation results.
All previous work assumed the use of fixed and public n, e, k parameter values, which grants a malicious generator unnecessary advantages.
For example, knowing e, a malicious generator can choose to generate 񮽙e/2񮽙 bad circuits to maximize the chance that an honest evaluator outputs a wrong result.
Thanks to its simplicity, it is the scheme the most widely adopted by implementations thus far.In the following, we show how to apply our observations to MajorityCut protocols, which involves delaying the revelation of cut-and-choose parameters and employing a mixed strategy (instead of a pure one) to minimize the total cost of cut-and-choose.
Analysis.
We represent the evaluator's strategy by a vector x x x = (x 0 , x 1 ,.
.
.
,x n ) where x i is the probability that the evaluator evaluates i uniform-randomly chosen circuits and checks the remaining n − i.
The expected cost of MajorityCut isn ∑ i=0 [x i · (i · r + (n − i))] = n + (r − 1) n ∑ i=0 x i · iIf the generator produces b incorrect circuits and the evaluator evaluates i circuits, the probability that the evaluator's check passes is񮽙 n−b n−i n n−i񮽙 .
After a successful check, the evaluator loses the security game if and only if 2b ≥ i, i.e., there is no majority of correct evaluation circuits.
Hence, when the evaluator uses strategy x x x, the expected failure probability of the MajorityCut scheme is∑ i≤2b x i · 񮽙 n − b n − i n n − i 񮽙 Since i ≤ n and 񮽙 n−b n−i 񮽙 = 0 for all i < b, this sum can be further reduced to ∑ min(n,2b) i=b x i · 񮽙 n−b n−i n n−i񮽙 .
The security requirement stipulates that for every choice of b by the malicious generator, the resulting cut-and-choose failure probability should be less than ε.
In other words, the goal of picking optimal cut-and-choose parameters can be achieved by solving the following linear program:min x x x n + (r − 1) n ∑ i=0 x i · isubject tox i ≥ 0 n ∑ i=0 x i = 1, min(n,2b) ∑ i=b x i · 񮽙 n − b i − b n i 񮽙 < ε, ∀b ∈ {1,.
.
.
,n}.
Solving this linear program provides us an equilibrium strategy for every fixed n, ε, r. Using standard LP solvers, such programs can be solved exactly for n that ranges into the thousands (i.e., all practical settings).
With this capability, we can identify, for a given target ε and ratio r, the optimal n (that leads to the least overall cost) by solving the linear programs for all feasible n values.
While this leads to the search algorithm described in Figure 1, we note several important observations that speedup the search: 1.
We begin our search at n 0 = 񮽙ε񮽙 and consider n = n 0 , n 0 + 1,.
... After solving each LP, we identify a current best cost c * .
Observe that c * − (r − 1) is an upper-bound of the best n (noted n * ), since any feasible strategy with n > c * − (r − 1) will cost at least c * (the evaluator need to evaluate at least one circuit except with at most ε probability).
Thus, as our search continues, we update c * , and terminate the search as soon as all values of n between n 0 and c * − (r − 1) are examined.
2.
When the value of r is beyond moderate (i.e., r > t r for some constant t r like 128 with our laptop), searching for the optimal cost becomes time-consuming as it involves solving the above linear programming problem for many relatively large n values (e.g., n > 500).
In these settings, however, we opt to live with a suboptimal pure strategy, based on the observation that the standard deviation of e is already so small (less than 0.6 and only keeps decreasing as r grows) that the cost of a sub-optimal pure strategy (i.e. a combination of n and e) approximates the theoretical optimal pretty well ( Figure 3b).
3.
We note that when r > t r (step 2), it suffices to search all e less than e 0 (recall (e 0 , n 0 , c 0 ) is the starting point of our search, simply derived from the traditional setup with MajorityCut) instead of the infinite range because any strategy with e = e 0 + 1 that is more efficient than one with e = e 0 has to use at least r − 1 fewer check circuits.
Since t r = 128, such strategy would have used at least 127 fewer checkcircuits, which will contradict with n 0 = 128 (assuming ε = 2 −40 ).
Results.
We have implemented the search algorithm of Figure 1 and run it with a wide range of practically possible r values (see Figure 2).
For r values ranging from 5000 to 10 9 , which are typical regarding the cost in network traffic, we can achieve 6 to 16 times savings compared to traditional MajorityCut protocols.
Even when r is small, such as 8 ∼ 128 which are representative when considering only the timing cost, our approach brings about 1.45 to 3 times savings.
When circuit-level parallelism is exploited like in the work of [14,13,5], where r typically ranges from 50 to 500 (see Table 5), we are able to speedup the best existing works by 2.3 ∼ 3.9 times.
Table 3 gives two example optimal strategies for achieving ε = 2 −40 security when r = 10 and r = 100, respectively.
We observe that the solution mixes fewer pure strategies (which is consistent to the decrease of variance) as r grows.
Also note that all pure strategies with even es are dominated by ones with odd es.
In contrast to current implementations, these strategies suggest that the generator produce a few hundred circuits, but only send roughly 13-21 of them.
(Such a scheme is for example quite feasible when using the GPU to produce and commit to garbled circuits.)
In comparison, the best protocols that use BatchedCut need to amortize 1000s of protocol executions to achieve security when sending roughly 10 circuits.In Figure 2, the cross-marked solid curve delineates the optimal cost of mixed strategies (among all strategies with public fixed n), while the dot-marked dashed curve delineates pure-strategy approximation of the optimal mixed strategies (efficiently computed as a result of step 2 of Figure 1 search algorithm).
We observe that the pure-strategy approximation actually improves as r get bigger.
But when r is relatively small (100 ≥ r ≥ 1), our optimization-based approach can indeed bring about 1% ∼ 11% extra improvement (Figure 2).
Last, the curves for two different ε values have similar shape but the improvement as a result of our approach is significantly larger for smaller ε values.We also observe from Figure 2 that the performance boost seems to be upper-bounded by some value related to ε, no matter how big r becomes.
This makes some intuitive sense because the cost of our optimized protocols will be upper-bounded by a linear function of r (as the solution comes out of solving the linear programming problem where r constitutes the coefficients of the unknowns).
We leave the formal proof of this intuition as an interesting future work.To examine the characteristic of our solution more closely for 1 ≤ r ≤ 128, we have plotted the comparison of overall cost of the optimal strategy with respect to best prior works (Figure 3a), the standard deviation of the overall cost (Figure 3b, recall the optimal strategy is a randomized strategy), and the best n used in every optimal strategy (Figure 3c).
Note that the standard deviation of the overall cost is also exactly the standard deviation of e because the randomness in cost all comes from the randomness in selecting e.
The fact that the standard deviation quickly drops to less than 0.3 (when r ≥ 100) and strictly decreases justifies the accuracy of pure strategy approximation for large r.Changes to Existing Protocols.
Our approach to MajorityCut applies to many published cut-and-choose based two-party computation protocols; in particular, it applies directly to those protocols in which the generator first commits to n garbled circuits, and later, after a cointossing protocol between generator and evaluator, opens each check circuit by sending either (a) both the 0 and 1 labels for all of its input wires, or (b) the random coins used to construct the circuit.
Goyal, Smith and Mohassel [7] were the first to use this technique and the secondInput: ε, r. Output: c * , n * , x x x * .
1.
If r ≤ t r ,(a) Initialize n 0 := 񮽙log ε񮽙 and c * := +∞.
(b) For n := n 0 to c * − (r − 1)(1 − ε), i. Solve the MajorityCut linear programming problem for (n, ε, r) to obtain (c, x x x) where c is the minimal cost of LP(n, ε, r) and x x x represents the corresponding strategy to achieve c. ii.
If (c, x x x) is a feasible solution and c * > c, then (c * , x x x * , n * ) := (c, x x x, n).
(c) Output (c, n * , x x x * ).
.
ii.
If c * > c i , then (c * , n * , e * ) := (c i , n i , e i ).
(c) Output(c, n * , {x 0 ,...,x n }) where x i = 1 if i = e * , and x i = 0 otherwise.
Figure 1: Search the most efficient strategy (n, x x x) for MajorityCut protocols.
log(·) is base-2.
c is the minimal cost, n is the fixed total number of circuits and x x x = (x 0 ,.
.
.
,x n ) stands for the evaluator's best strategy to sample e.
While the value of t r depends on hardware and users' tolerance of performance.
t r = 128 works well on a MacBook Air for ε = 2 −40 .
[14] also operates in this way.
In these cases, no modifications to the security analysis are needed.
For every specific ε and r, our solver outputs a particular n and a distribution x x x for picking e. Roughly speaking, the only changes needed in the protocol are straightforward: the evaluator announces this n beforehand and the result of the coin-tossing protocol ρ is used to sample e according to x x x using standard methods (instead of the 1/2 or 3/5 fractions as before).
The simulation of a malicious evaluator proceeds as in the original security proof with the exception that the simulator first samples e according to x x x using random tape ρ and then (as before), uses a simulated coin-tossing to ensure the outcome of the toss induces ρ.
(The cointossing method is a simple and effective method to prove security; other proofs may also exist.)
Similarly, the first protocol of Lindell and Pinkas [16] can be modified to adopt this idea: step (3) should send commitments to garbled circuits, modify step (4) to use the random tape from coin-tossing to sample e, modify step (8) so that the garbler sends the entire garbled circuits for the evaluation specimens as well as openings to the commitments so that the evaluator can check consistency.The idea seems applicable to many protocols which have the property that the set of checked circuits becomes publicly verifiable.
For example, Mohassel and Riva [22] use a different idea in their protocol to allow the same output labels to be used across all n copies of the garbled circuit.
In their original protocol, the evaluator and generator then use coin-tossing to select the open circuits, but then proceed to evaluate the remaining circuits first, perform some checks, and then the evaluator commits to the output labels.
Finally, the generator opens the check circuits for the evaluator to check, and if all succeed, the evaluator opens a commitment to the output.
Although a different order, the modifications noted above seem to apply without the need to modify the security proof.
The protocols of Lindell and Pinkas [17] and shelat and Shen [27], however, seem to require more subtle modifications and new security arguments to use our technique.
In both cases, the protocols use a special oblivious transfer (instead of coin-tossing) to allow the evaluator to independently choose the set of check circuits.
In the case of [17], the fact that the size of the set of checked circuits is fixed, and therefore verifiable by the garbler, is needed in the security proof.
This restriction can be lifted with a variant of cut-and-choose oblivious transfer proposed and used in Lindell's SingleCut protocol [15].
For a different reason, a new security argument will also be needed for shelat and Shen [27].
With SingleCut protocols, extra cryptographic mechanisms (e.g., a second-stage fully secure computation as in [15] or an additive homomorphic commitment as in [2]) are employed in order to weaken the requirement for the soundness property.
In particular, in such protocols, it suffices to ensure that at least one evaluation circuit selected by the evaluator is not corrupted.
If one evaluation circuit is properly formed, then the evaluator will either receive the same output from all of the evaluated circuits (in which case it can accept the output since one circuit is good), or it receives two different outputs.
In the latter case, the evaluator uses the two different authenticated output labels to recover the garbler's input, and then evaluate the function itself.The state-of-the-art SingleCut protocols implicitly assume r = 1, in which case an honest evaluator's best strategy is to evaluate each garbled circuit with probability 1/2, as there is only a single way for the malicious generator to win the cut-and-choose game.
In reality, however, r is not necessarily equal to 1.
In order to achieve ε statistical security, this strategy will lead to an expected 񮽙log ε񮽙 · (r + 1)/2 units of cost.Analysis.
As before, let i be the number of evaluation circuits and x i is the probability that the evaluator chooses to evaluate i circuits.
An evaluator's strategy is denoted by x x x = {x 0 ,.
.
.
,x n }.
Then the cost of the cutand-choose scheme is n + (r − 1) ∑ n i=0 x i · i. 񮽙 .
As a result, the security requirement can be dramatically simplified in comparison to MajorityCut.
In particular, we need that every pure strategy for the generator, i.e., every choice of b, wins with probability at most ε: x b / 񮽙 n b 񮽙 < ε.
Therefore, fixing n, r and ε, the original cut-andchoose game configuration problem can be translated into the following linear programming problem:min x x x n + (r − 1) n ∑ i=0 x i · i Input: ε, r. Output: c * , n * , x x x *1.
Initialize n 0 := 񮽙log ε񮽙 and c * := n 0 · (r + 1)/2.
2.
For n := n 0 to c * − (r − 1)(1 − ε), (a) Solve the SingleCut linear programming problem for (n, ε, r) to obtain (c, x x x) where c is the minimal cost of LP(n, ε, r) and x x x represents the corresponding strategy to achieve c.(b) If (c, x x x) 񮽙 = ⊥ and c * > c, then (c * , x x x * , n * ) := (c, x x x, n).
3.
Output (c, n * , x x x * ).
Figure 4: Search the optimal strategy (n, x x x) for SingleCut protocols.
log(·) is base-2.
c is the minimal cost, n is the fixed total number of circuits and x x x = (x 0 ,.
.
.
,x n ) stands for the evaluator's best strategy to sample e. subject tox i ≥ 0, ∀i ∈ {0,.
.
.
,n} n ∑ i=0 x i = 1, x b 񮽙񮽙 n b 񮽙 < ε, ∀b ∈ {0,.
.
.
,n}.
Next, we show that the linear programming problem above can actually be solved highly efficiently thanks to its special form.
The key observation is that this linear programming problem is in essence a special continuous knapsack program (where the weight w i = i).
In order to minimize ∑ n i=0 x i · i, we aim to maximize x i (which is upper-bounded by ε · 񮽙 n i 񮽙 and collectively constrained by ∑ n i=0 x i = 1) for all small i's.
This leads to the following simple greedy algorithm that solves the problem in linear time (of n).
As with the MajorityCut setting, we scan all possible values of n to identify the best n leading to the smallest overall cost (Figure 4).
Fortunately, thanks to this highly efficient special solver, we are always able to identify the best n within seconds for r as large as 10 10 .
Results.
Using the search algorithm described above, we are able to compute the fixed-n, variating e optimal randomized strategies for every ε and r.
We summarize the performance gains in Figure 5a.
The savings due to our approach rise steadily for r < 10 4 and can get to about 10X for reasonably large r (e.g., r = 7 × 10 7 , which roughly corresponds to the bandwidth-based costratio for privately computing the edit distance between two 1000-character strings).
Generally, it appears that the improvement-curves ( Figure 5) for different ε share some similarity in their shape but smaller ε results in bigger improvements.
Table 4 shows two example optimal strategies of SingleCut protocols for r = 10 and r = 100, respectively.
We observe that the optimal strategy exhibit some pattern: the number of evaluation circuits with positive support falls within [0, g] where g < n and g shrinks as r grows.
An interesting note is that e = 0 (i.e., checking all n circuits) has positive support, albeit with probability less than 2 −40 (note the "%" sign), hence preserving security.
Instead of artificially preventing e = 0 as Lindell did [15], our solution indicates that a rational evaluator should set e = 0 with some negligible probability to maximize its chance to win (while keeping the expected cost low).
Figure 6 presents a closer look at various characteristics of the optimal strategies, including expected costs (Figure 6a), the standard deviation of the costs (which also applies to e, Figure 6b), and the best ns associated with those optimal strategies (Figure 6c).
We note in Fig- ure 6b that the standard deviation for SingleCut optimal strategies are generally smaller (about half) than that for MajorityCut strategies (Figure 3b).
In addition, the ns for the optimal strategies also exhibit a staircase effect like in MajorityCut.
This is because for any fixed ε, it does not make sense to trade in a larger n for a smaller e, • Add a step 0 to [15, Protocol 2], where an n is fixed in advance based on ε and r.• Change step 2(b) of [15, Protocol 2] to: P 2 picks the check-set J at random so that |J| = k, where k is randomly sampled from the distribution computable (from n, r, ε) by the 2-step algorithm given above.Afshar et al.[2] present a conceptually simple and elegant non-interactive secure computation protocol; it uses a cut-and-choose technique and achieves security 2 −40 by sending 40 garbled circuits.
Surprisingly, this can be done in just one round.
Like Lindell, they create a trapdoor which allows an evaluator to recover the garbler's inputs with high probability if inconsistent but valid output wire-labels are obtained.
However, since their protocol is only 1 round, the cut-and-choose is implemented through oblivious transfer; specifically, the evaluator recovers a seed for all of the check circuits through OT, and the garbler sends all circuits in its one message.
In order to apply our technique, we need to add extra rounds (in order to save substantial communication costs).
Instead of sending the full circuits in the first message, we change the protocol to send succinct commitments of the circuits (thereby committing the sender) which keeps the first message short.
In the next message, the evaluator asks the garbler to send the evaluation circuits only; and the evaluator uses its previous messages to continue running the original protocol.
We believe these modifications are consistent with the security proof implicitly given in [2].
As a result, we can run this protocol with significantly less communication when r > 1.
The basic idea of BatchedCut is to amortize the cost of cut-and-choose across either many protocol executions (of the same circuit) [11,19] or many basic gates [12,6,24] of a big circuit.
Without loss of generality, we focus on the setting of batched execution of a single functionality.
Roughly speaking, the evaluator randomly selects and checks k out of T circuits in total and randomly groups the remaining circuits in buckets of size B.
The state-of-the-art can ensure correctness as long as at least one good circuit is included in every bucket.
This can effectively reduce the number of circuit copies to less than 8 (c.f. the optimal 40 without amortization [2,15]) per execution to ensure 2 −40 security.
However, optimality of this result holds only if r = 1.
In this section, we present our approach to optimize BatchedCut protocols for general r values.We note one technical complication in this setting: in the checking stage, a bad circuit (or gate) might only be detected by the evaluator with probability τ.
Although τ = 1 for most protocols, it can be less than 1 for some other protocols, e.g., τ = 1/2 for [12] and τ = 1/4 for MiniLEGO [6].
Our analysis below is generalized to account for any 0 ≤ τ ≤ 1.
State-of-the-art BatchedCut protocols [19,11,12] only require one object in each bucket to evaluate being correct, hence the focus of our analysis.Analysis.
Let N be the number of times a particular functionality will be executed, T be the total number of circuits generated to realize the N executions, and let B denote the bucket size.With any positive r, we want to identify parameters (T, B) such that cost(T, B) is minimized over all (T, B) configurations that satisfy the security constraint.
That is, Pr fail , the overall failure probability of cut-andchoose, should be no more than ε.
Therefore, the problem reduces to the following constrained optimization Cost ratio rOptimal n (c) Figure 6: Characteristics of optimal mixed-strategy solutions for SingleCut protocols (ε = 2 −40 .
The bandwidth overhead is measured in units.
A unit cost is that of evaluating a evaluation-circuit.
The standard deviation chart applies to both the overall cost and e.)problem: min T + (r − 1)BN subject to Pr fail (N, T, B, τ, b) < ε, ∀b ∈ {0,.
.
.
,T }where b is the number of bad circuits a malicious generator chooses to inject.
Note that Pr fail describes the failure across all N executions as follows: In the first move of the game, the evaluator picks T − BN circuits to open and verifies all are correct.
In the second move, the evaluator randomly partitions the BN unopened circuits into buckets of B circuits.
A failure occurs if (i) the adversary is able to corrupt b circuits such that the first check passes, and (ii) there is some bucket containing only corrupted circuits.
We let Pr c (N, B, T, τ, b, i) denote the probability of (i) and Pr e (N, B, b) denote that of (ii).
First, as before, when i circuits are opened in the first phase, the garbler succeeds with probabilityPr c (N, B, T, τ, b, i) = (1 − τ) i 񮽙 b i 񮽙񮽙 T − b T − BN − i T T − BN 񮽙 .
Here the extra (1 − τ) term reflects the case when checking a circuit can succeed with some chance even if it is corrupt.
There are T total circuits, and T − BN − i of them can be checked.
The next term, Pr e reflects the probability that conditioned on the first phase passing, the evaluator randomly assigns the remaining circuits to buckets, and one bucket of size B contains all corrupted circuits.Pr e (N, B, b) = 0, ∀0 ≤ b ≤ B(1)Pr e (N, B, b) = 񮽙 b B BN B 񮽙 + b−1 ∑ i=0 Pr e (N − 1, B, b − i) · 񮽙 b B−i 񮽙񮽙 BN−b i 񮽙 񮽙 BN B 񮽙(2)The first equation holds because a garbler who corrupts fewer than B circuits never succeeds.
Finally, since phase 1 and phase 2 are independent, we conclude thatPr fail (N, B, T, τ, b) = b ∑ i=0 Pr c (N, B, T, τ, b, i)Pr e (N, B, b − i)The summation over i occurs because every check only succeeds with probability τ, and thus even after i checks on corrupted circuits, b − i corrupted circuits may remain in the second phase.Having explained the constraint, Figure 7 describes our search algorithm to solve the BatchedCut parameter optimization problem.
The basic idea is simple-for every B = 2, 3,.
.
.
, find the least T such that the security constraint is satisfied for every b ∈ {0,.
.
.
,T }.
Our main contribution here is to make the search efficient enough for realistic r, N, and ε, which is achieved based on a new efficient and accurate way to calculate Pr fail and the following observations to ensure efficiency and completeness of the search:1.
For every B, the cost cost(T, B) strictly increases with T while the failure rate Pr fail strictly decreases with T .
So the best T for a given B can be identified efficiently using a combination of exponential backoff and binary search.
2.
The constraint that Pr fail < ε regardless of the attacker's strategy can be verified by computing Pr fail for every b ∈ {1,.
.
.
,T } (where b is the number of corrupted circuits generated by the attacker), which, if naively implemented, would require computing Pr fail T · T times for every B.
We can leverage the idea of generating functions to reduce it to T + T inexpensive operations (we will detail this in a bit).
identified during the search stipulates an upper-bound on the Bs that needs to be examined.
Compute Pr fail Efficiently.
For every N, B, T, τ, b, the probability of a malicious generator's successful attack can be described by equations described above.
However, for most N, T values (e.g., N > 2 15 ), it is infeasible to compute Pr c (which involves calculating large binomial coefficients) and Pr e (which involves exponential number of slow recursions) accurately based on (3.3) and (2).
Hence, we propose an efficient way to compute Pr e and Pr c with provable accuracy.
1.
Compute Pr e (N, B, b).
The idea is to use generating functions to efficiently calculate Pr e as the ratio between the number of ways to group garbled circuits into buckets that will result a failure (i.e., at least one bucket is filled with all B bad circuits) and the total number of ways to group the garbled circuits.
First, we use function g(x, y) = (1 + x) B + (y − 1)x B to model the circuit assignment process for a single bucket, where 'x' denotes a "bad" gate and '1' denotes a "good" gate, thus the coefficient of x i in g(x, y) equals to the number of ways to assign i bad gates to a bucket.
Note that the symbol 'y' we intentionally introduce as the coefficient of x B term of g(x, y) denotes the event that "all B gates in a bucket are bad".
Next, we use the generating function G(x, y) = g(x, y) N to model the circuit assignment process over all of the N buckets: the coefficient of x i (which is a polynomial in y, hence written as f i (y)) in G(x, y) denotes the number of assignments involving i bad gates.
Let f i (y) = ∑ ∞ j=0 c j y j (where c j are constants efficiently computable from G(x, y)), then f b (1) = ∑ ∞ j=0 c j is the total number of assignments with b bad gates used in the evaluation stage; and f b (1) − f b (0) = ∑ ∞ j=1 c j is the number of assignments (among all with b bad circuits) that result in at least one broken bucket.
Hence, we computePr e (N, B, b) = ( f b (1) − f b (0))/ f b (1).
Further, we can dramatically reduce the cost of computing the coefficients of G(x, y) by not distinguishing any terms y j 1 and y j 2 for any j 1 , j 2 ≥ 1.
That is, multiplying (u+vy) and (w + ty) yields uw + (ut + vw + vt)y, hence, however big N and B are, f i (y)s are linear formulas in y. To illustrate the precision of the above calculation, e.g., when s = 40, if N = 50, 000, the overall error in our calculation of log Pr fail (N, B, T, τ, b) is less than 1.
Note the error only decreases as N grows (following Claim 1 and Claim 2).
Claim 1 Let T, B, N, b, i be defined as above.
Thenlim N→∞ 񮽙 T −b T −BN−i 񮽙 񮽙 T T −BN 񮽙 = 񮽙 T − BN T 񮽙 i 񮽙 BN T − i 񮽙 b−i .
Claim 2 Let Pr fail (N, B, T, τ, b) be the probability that the cut-and-choose game fails in a BatchedCut scheme (with b bad circuits up-front).
For every ε, c 0 =(T − BN)/T, τ > 0, if N > i 0 񮽙񮽙 B 1−c 0 − B 1−(1−τ)c 0 񮽙 and b > −(񮽙log ε񮽙 + 1) 񮽙 log 񮽙 (1 − τ)c 0 + B B/(1−c 0 )−i 0 /N 񮽙 , then Pr fail (N, B, T, τ, b) < ε.Last, we also considered employing mixed strategies for BatchedCut protocols (i.e., fixing T to some public value up-front while randomizing the selection of B) to further reduce the cost.
However, our analysis show that the extra improvement brought by randomized strategies is very small in this setting.
This is consistent with our intuition: (1) It only makes sense to alternate B between two consecutive integers, which can be derived as a corollary of [12, Lemma 9]; (2) The strategy with smaller B is almost dominated by the one with larger B such that mixing them brings little extra benefit.
Therefore, we opt to avoid using randomized strategies for BatchedCut protocols.Results.
Figure 8 depicts the improvements induced by the refined cost model for cut-and-choose.
In this scenario, our search algorithm is able to identify the optimal pure strategies for r up to 10 5 , assuming the check rate c is always larger than 0.02.
We note that the optimal strategy (characterized by (T, B) pairs) does not change much for 10 5 < r 񮽙 ∞.
Experimental results show that roughly 20 ∼ 80% performance gain can be achieved (while the exact improvement depends on r, ε and N).
Note the effects of the bad circuits detection rate τ on the benefits of our approach (through comparing Figure 8a and 8b).
Changes to Existing Protocols.
In this case, because we do not use randomized strategies, our proposal applied to the BatchedCut scenario requires no protocol changes other than setup the public parameters to the suggested value output by our search algorithm.
Recall that circuit checking will result in negligible network traffic because only a short circuit seed and a circuit (a) τ = 1 (applicable to [15,4] hash needs to be transferred.
This gap in bandwidth overhead also leads to a substantial gap in execution speeds, due to the significant difference in the throughputs of garbling/checking (about 50 ns/gate on a single-core processor) and that of network transmission (typically a wide range of 50∼3000 ns/gate).
To evaluate the benefit of our technique in terms of time, we modified OblivC [29] to measure the ratios of speed for circuit evaluation and circuit checking tasks in various network settings.
Our test implementation utilizes Intel AES-NI instructions and the half-gate garbling technique [30] to minimize bandwidth usage, and SHA256 implementation provided by Libgcrypt for circuit hashing.
In circuit checking tasks, the circuit generator garbles a number of circuits but sends only a (Seed, Hash) pair for each garbled circuit, while the circuit verifier re-computes the hash from the seed for each circuit.
We record the per circuit time cost for this task as T c .
For circuit evaluation tasks, the circuit generator garbles a number of circuits and sends the garbled gates to the evaluator, who not only evaluates, but also computes the hash of the received circuit.
We record the per circuit time cost for this task as T e .
(In both tasks, the two ends work in a pipelined fashion.)
Thus, the time cost ratios between evaluation and checking can be calculated as (T c + T e )/(2T c ) (recall that every circuits will be generated twice, once for commiting their hashes and once for check/evaluate to avoid storing the typically gigantic circuits).
We used a benchmark circuit (provided by OblivC) with 31 × 10 6 non-free gates.
Our exepriments were run on two Amazon EC2 boxes (instance type: c4.large, $0.105/hour, Intel Xeon E5-2666, 2.9GHz, 3.75GB memory) with Ubuntu 14.04 Server edition in the VA region.We detail our experimental results in Table 5.
The r values (in terms of wall-clock time) range from a little over 1 (with high speed connection) to 30 (with ordinary home-to-home connections).
Such time gaps can be well-explained by the difference in the throughputs of computation and communication.
The observed speedups of the proposed cut-and-choose technique can range from 12% up to 106%.
Note that our approach yields no noticeable time savings for the settings of running SingleCut or BatchedCut protocols in a 1 Gbps LAN with single-core processors (compared to the their state-of-the-art couterparts), because the cost ratio r is already very close to 1.
We note that due to the use of SHA256 in computing circuit hashes, we observe only 2.23 × 10 6 gates/second for circuit verification while 1.30 × 10 6 gates/second for circuit evaluation.
It would be interesting to replace SHA256 with some hashing algorithm that leverages AES-NI instructions to match up with the speed of AES-NI based garbling (more than 10 9 gates/second, as was reported in [3]).
That will imply a time ratio up to 100× larger than we observe in our experiments.
The state-of-the-art design of cut-and-choose protocols considers an overly simplified cost model, and does not exploit the opportunity of dynamically variating e to thwart cheating adversaries.
We have shown, through experiments, the dramatic gap in the bandwidth costs between circuit evaluation and circuit verification.
We revisit the cut-and-choose protocol design problem in a refined cost model and give three highly efficient solvers, one for each class of cut-and-choose protocols, that output the best strategy for a particular cost ratio in our model.
Simulation results show that our approach bring significant savings in bandwidth cost, as well as substantial speedups (especially when running secure computation protocols outside idealized laboratory environments).
Most importantly, the benefits require very small changes to existing protocols and come completely from formal proofs that do not depend on any unprovable assumptions.
Similarly, we have, there exists N 1 such that if N > N 1 , 񮽙 T −b T −BN−i 񮽙 񮽙 T T −BN 񮽙 ≥ 񮽙 T − BN − i + 1 T − i + 1 񮽙 i 񮽙 BN − b + i + 1 T − b + 1 񮽙 b−i 񮽙 L.So, we know that, for sufficiently large N, U 񮽙 񮽙 T −b T −BN−i 񮽙 񮽙 T T −BN 񮽙 ≤ U L = 񮽙 T − BN T − BN − i + 1 · T − i + 1 T 񮽙 i 񮽙 BN BN − b + i + 1 · T − b + 1 T − i 񮽙 b−i = 񮽙 (≤ 񮽙 1 + i − 1 T − BN − i + 1 񮽙 i 񮽙 1 + b − i − 1 BN − b + i + 1 񮽙 b−i (3) ≤ 񮽙 1 + i − 1 T − BN − i + 1 񮽙 i 񮽙 1 + b − 1 BN − b + 1 񮽙 b (4)Note that the inequality (3) , then Pr fail (N, B, T, τ, b) < ε.Proof Let 0 < τ ≤ 1 be the probability that P 2 detects the abnormality in checking garbled gate g conditioned on g is indeed bad.
We have We thank Yuan Zhou (MIT) and Zhilei Xu (MIT) for inspiring discussions on cut-and-choose.
We also appreciate Xiao Wang (Maryland)'s advice on benchmarking state-of-the-art garbled circuit implementations.
Work of Ruiyu Zhu and Yan Huang was supported by NSF award #1464113.
Work of Jonathan Katz was supported in part by NSF award #1111599.
Work of shelat was supported in part by NSF grants TC-0939718, TC-1111781, and 1618559, a Microsoft Faculty Fellowship, and a Google Faculty Research Award.
n
