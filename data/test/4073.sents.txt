Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user's privacy, without relying on a trusted third party.
LDP protocols (such as Google's RAPPOR) have been deployed in real-world scenarios.
In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population.
In this paper, we introduce a framework that generalizes several LDP protocols proposed in the literature.
Our framework yields a simple and fast aggre-gation algorithm, whose accuracy can be precisely analyzed.
Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed.
We present precise conditions for when each proposed protocol should be used, and perform experiments that demonstrate the advantage of our proposed protocols .
Differential privacy [10,11] has been increasingly accepted as the de facto standard for data privacy in the research community.
While many differentially private algorithms have been developed for data publishing and analysis [12,19], there have been few deployments of such techniques.
Recently, techniques for satisfying differential privacy (DP) in the local setting, which we call LDP, have been deployed.
Such techniques enable gathering of statistics while preserving privacy of every user, without relying on trust in a single data curator.
For example, researchers from Google developed RAP-POR [13,16], which is included as part of Chrome.
It enables Google to collect users' answers to questions such as the default homepage of the browser, the default search engine, and so on, to understand the unwanted or malicious hijacking of user settings.
Apple [1] also uses similar methods to help with predictions of spelling and other things, but the details of the algorithm are not public yet.
Samsung proposed a similar system [21] which enables collection of not only categorical answers (e.g., screen resolution) but also numerical answers (e.g., time of usage, battery volume), although it is not clear whether this has been deployed by Samsung.A basic goal in the LDP setting is frequency estimation.
A protocol for doing this can be broken down into following steps: For each question, each user encodes his or her answer (called input) into a specific format, randomizes the encoded value to get an output, and then sends the output to the aggregator, who then aggregates and decodes the reported values to obtain, for each value of interest, an estimate of how many users have that value.
With improvement on the basic task of frequency estimation, solutions to more complex problems that rely on it, such as heavy hitter identification, frequent itemset mining, can also be improved.We introduce a framework for what we call "pure" LDP protocols, which has a nice symmetric property.
We introduce a simple, generic aggregation and decoding technique that works for all pure LDP protocols, and prove that this technique results in an unbiased estimate.
We also present a formula for the variance of the estimate.
Most existing protocols fit our proposed framework.
The framework also enables us to precisely analyze and compare the accuracy of different protocols, and generalize and optimize them.
For example, we show that the Basic RAPPOR protocol [13], which essentially uses unary encoding of input, chooses sub-optimal parameters for the randomization step.
Optimizing the parameters results in what we call the Optimized Unary Encoding (OUE) protocol, which has significantly better accuracy.Protocols based on unary encoding require Θ(d) com-munication cost, where d is the number of possible input values, and can be very large (or even unbounded) for some applications.
The RAPPOR protocol uses a Bloom filter encoding to reduce the communication cost; however, this comes with a cost of decreasing accuracy as well as increasing computation cost for aggregation and decoding.
The random matrix projection-based approach introduced in [6] has Θ(log n) communication cost (where n is the number of users); however, its accuracy is unsatisfactory.
We observe that in our framework this protocol can be interpreted as binary local hashing.
Generalizing this and optimizing the parameters results in a new Optimized Local Hashing (OLH) protocol, which provides much better accuracy while still requiring Θ(log n) communication cost.
The variance of OLH is orders of magnitude smaller than the previous methods, for ε values used in RAPPOR's implementation.
Interestingly, OLH has the same error variance as OUE; thus it reduces communication cost at no cost of utility.
With LDP, it is possible to collect data that was inaccessible because of privacy issues.
Moreover, the increased amount of data will significantly improve the performance of some learning tasks.
Understanding customer statistics help cloud server and software platform operators to better understand the needs of populations and offer more effective and reliable services.
Such privacy-preserving crowd-sourced statistics are also useful for providing better security while maintaining a level of privacy.
For example, in [13], it is demonstrated that such techniques can be applied to collecting windows process names and Chrome homepages to discover malware processes and unexpected default homepages (which could be malicious).
Our paper makes the following contributions:• We introduce a framework for "pure" LDP protocols, and develop a simple, generic aggregation and decoding technique that works for all such protocols.
This framework enables us to analyze, generalize, and optimize different LDP protocols.
• We introduce the Optimized Local Hashing (OLH) protocol, which has low communication cost and provides much better accuracy than existing protocols.
For ε ≈ 4, which was used in the RAPPOR implementation, the variance of OLH's estimation is 1/2 that of RAPPOR, and close to 1/14 that of Random Matrix Projection [6].
Systems using LDP as a primitive could benefit significantly by adopting improved LDP protocols like OLH.
Roadmap.
In Section 2, we describe existing protocols from [13,6].
We then present our framework for pure LDP protocols in Section 3, apply it to study LDP protocols in Section 4, and compare different LDP protocols in Section 5.
We show experimental results in Section 6.
We review related work in Section 7, discuss in Section 8, and conclude in Section 9.
The notion of differential privacy was originally introduced for the setting where there is a trusted data curator, who gathers data from individual users, processes the data in a way that satisfies DP, and then publishes the results.
Intuitively, the DP notion requires that any single element in a dataset has only a limited impact on the output.Definition 1 (Differential Privacy) An algorithm A satisfies ε-differential privacy (ε-DP), where ε ≥ 0, if and only if for any datasets D and D ′ that differ in one element, we have∀t ∈ Range(A) : Pr [A(D) = t] ≤ e ε Pr [ A(D ′ ) = t ] ,where Range(A) denotes the set of all possible outputs of the algorithm A.
In the local setting, there is no trusted third party.
An aggregator wants to gather information from users.
Users are willing to help the aggregator, but do not fully trust the aggregator for privacy.
For the sake of privacy, each user perturbs her own data before sending it to the aggregator (via a secure channel).
For this paper, we consider that each user has a single value v, which can be viewed as the user's answer to a given question.
The aggregator aims to find out the frequencies of values among the population.
Such a data collection protocol consists of the following algorithms:• Encode is executed by each user.
The algorithm takes an input value v and outputs an encoded value x.• Perturb, which takes an encoded value x and outputs y. Each user with value v reports y = Perturb(Encode(v)).
For compactness, we use PE(·) to denote the composition of the encoding and perturbation algorithms, i.e., PE(·) = Perturb(Encode(·)).
PE(·) should satisfy ε-local differential privacy, as defined below.
• Aggregate is executed by the aggregator; it takes all the reported values, and outputs aggregated information.Definition 2 (Local Differential Privacy) An algorithm A satisfies ε-local differential privacy (ε-LDP), where ε ≥ 0, if and only if for any input v 1 and v 2 , we have∀y ∈ Range(A) : Pr [A(v 1 ) = y] ≤ e ε Pr [A(v 2 ) = y] ,where Range(A) denotes the set of all possible outputs of the algorithm A.This notion is related to randomized response [24], which is a decades-old technique in social science to collect statistical information about embarrassing or illegal behavior.
To report a single bit by random response, one reports the true value with probability p and the flip of the true value with probability 1− p.
This satisfies( ln p 1−p ) - LDP.Comparing to the setting that requires a trusted data curator, the local setting offers a stronger level of protection, because the aggregator sees only perturbed data.
Even if the aggregator is malicious and colludes with all other participants, one individual's private data is still protected according to the guarantee of LDP.Problem Definition and Notations.
There are n users.
Each user j has one value v j and reports once.
We use d to denote the size of the domain of the values the users have, and [d] to denote the set {1, 2, . . . , d}.
Without loss of generality, we assume the input domain is [d].
The most basic goal of Aggregate is frequency estimation, i.e., estimate, for a given value i ∈ [d], how many users have the value i.
Other goals have also been considered in the literature.
One goal is, when d is very large, identify values in [d] that are frequent, without going through every value in [d] [16,6].
In this paper, we focus on frequency estimation.
This is the most basic primitive and is a necessary building block for all other goals.
Improving this will improve effectiveness of other protocols.
RAPPOR [13] is designed to enable longitudinal collections, where the collection happens multiple times.
Indeed, Chrome's implementation of RAPPOR [3] collects answers to some questions once every 30 minutes.
Two protocols, Basic RAPPOR and RAPPOR, are proposed in [13].
We first describe Basic RAPPOR.
Perturbation.
Perturb(B 0 ) consists of two steps:Step 1: Permanent randomized response: Generate B 1 such that:Pr [B 1 [i] = 1] = { 1 − 1 2 f , if B 0 [i] = 1, 1 2 f , if B 0 [i] = 0.
RAPPOR's implementation uses f = 1/2 and f = 1/4.
Note that this randomization is symmetric in the sense thatPr [B 1 [i] = 1|B 0 [i] = 1] = Pr [B 1 [i] = 0|B 0 [i] = 0] = 1 − 1 2 f; that is, the probability that a bit of 1 is preserved equals the probability that a bit of 0 is preserved.
This step is carried out only once for each value v that the user has.Step 2: Instantaneous randomized response: Report B 2 such that:Pr [B 2 [i] = 1] = { p, if B 1 [i] = 1, q, if B 1 [i] = 0.
This step is carried out each time a user reports the value.
That is, B 1 will be perturbed to generate different B 2 's for each reporting.
RAPPOR's implementation [5] uses p = 0.75 and q = 0.25, and is hence also symmetric because p + q = 1.
We note that as both steps are symmetric, their combined effect can also be modeled by a symmetric randomization.
Moreover, we study the problem where each user only reports once.
Thus without loss of generality, we ignore the instantaneous randomized response step and consider only the permanent randomized response when trying to identify effective protocols.Aggregation.
Let B j be the reported vector of the j-th user.
Ignoring the Instantaneous randomized response step, to estimate the number of times i occurs, the aggregator computes:˜ c(i) = ∑ j 1 {i|B j [i]=1} (i) − 1 2 f n 1 − fThat is, the aggregator first counts how many time i is reported by computing ∑ j 1 {i|B j [i]=1} (i), which counts how many reported vectors have the i'th bit being 1, and then corrects for the effect of randomization.
We use 1 X (i) to denote the indicator function such that:1 X (i) = { 1, if i ∈ X, 0, if i / ∈ X.Cost.
The communication and computing cost is Θ(d) for each user, and Θ(nd) for the aggregator.Privacy.
Against an adversary who may observe multiple transmissions, this achieves ε-LDP for ε = ln ( (1− 1 2 f 1 2 f ) 2 ), which is ln 9 for f = 1/2 and ln 49 for f = 1/4.
Basic RAPPOR uses unary encoding, and does not scale when d is large.
To address this problem, RAPPOR uses Bloom filters [7].
While Bloom filters are typically used to encode a set for membership testing, in RAPPOR it is used to encode a single element.Encoding.
Encoding uses a set of m hash functionsH = {H 1 , H 2 , . . . , H m }, each of which outputs an integer in [k] = {0, 1, . . . , k − 1}.
Encode(v) = B 0 , which is k-bit binary vector such that B 0 [i] = { 1, if ∃H ∈ H, s.t., H(v) = i, 0, otherwise.Perturbation.
The perturbation process is identical to that of Basic RAPPOR.Aggregation.
The use of shared hashing creates challenges due to potential collisions.
If two values happen to be hashed to the same set of indices, it becomes impossible to distinguish them.
To deal with this problem, RAPPOR introduces the concept of cohorts.
The users are divided into a number of cohorts.
Each cohort uses a different set of hash functions, so that the effect of collisions is limited to within one cohort.
However, partial collisions, i.e., two values are hashed to overlapping (though not identical) sets of indices, can still occur and interfere with estimation.
These complexities make the aggregation algorithm more complicated.
RAPPOR uses LASSO and linear regression to estimate frequencies of values.Cost.
The communication and computing cost is Θ(k) for each user.
The aggregator's computation cost is higher than Basic RAPPOR due to the usage of LASSO and regression.
RAPPOR achieves ε-LDP for ε = ln ( (1− 1 2 f 1 2 f ) 2m ) .
The RAPPOR implementation uses m = 2; thus this is ln 81 ≈ 4.39 for f = 1/2 and ln 7 4 ≈ 7.78 for f = 1/4.
Bassily and Smith [6] proposed a protocol that uses random matrix projection.
This protocol has an additional Setup step.Setup.
The aggregator generates a public matrixΦ ∈ {− 1 √ m , 1 √ m} m×d uniformly at random.
Here m is a parameter determined by the error bound, where the "error" is defined as the maximal distance between the estimation and true frequency of any domain.Encoding.
Encode(v) = ⟨r, x⟩, where r is selected uniformly at random from [m], and x is the v's element of the r's row of Φ, i.e., x = Φ [r, v].
Perturbation.
Perturb(⟨r, x⟩) = ⟨r, b · c · m · x⟩, where b = { 1 with probability p = e ε e ε +1 , −1with probability q = 1 e ε +1 , c = (e ε + 1)/(e ε − 1).
Aggregation.
Given reports ⟨r j , y j ⟩'s, the estimate fori ∈ [d] is given by ˜ c(i) = ∑ j y j · Φ[r j , i].
The effect is that each user with input value i contributes c tõ c(i) with probability p, and −c with probability q; thus the expected contribution is(p − q) · c = ( e ε e ε + 1 − 1 e ε + 1 ) · e ε + 1 e ε − 1 = 1.
Because of the randomness in Φ, each user with value ̸ = i contributes tõ c(i) either c or −c, each with probability 1/2; thus the expected contribution from all such users is 0.
Note that each row in the matrix is essentially a random hashing function mapping each value in [d] to a single bit.
Each user selects such a hash function, uses it to hash her value into one bit, and then perturbs this bit using random response.Cost.
A straightforward implementation of the protocol is expensive.
However, the public random matrix Φ does not need to be explicitly computed.
For example, using a common pseudo-random number generator, each user can randomly choose a seed to generate a row in the matrix and send the seed in her report.
With this technique, the communication cost is Θ(log m) for each user, and the computation cost is O(d) for computing one row of the Φ.
The aggregator needs Θ(dm) to generate Φ, and Θ(md) to compute the estimations.
Multiple protocols have been proposed for estimating frequencies under LDP, and one can envision other protocols.
A natural research question is how do they compare with each other?
Under the same level of privacy, which protocol provides better accuracy in aggregation, with lower cost?
Can we come up with even better ones?
To answer these questions, we define a class of LDP protocols that we call "pure".
For a protocol to be pure, we require the specification of an additional function Support, which maps each possible output y to a set of input values that y "supports".
For example, in the basic RAPPOR protocol, an output binary vector B is interpreted as supporting each input whose corresponding bit is 1, i.e.,Support(B) = {i | B[i] = 1}.
Definition 3 (Pure LDP Protocols) A protocol given by PE and Support is pure if and only if there exist two probability values p * > q * such that for all v 1 ,Pr [PE(v 1 ) ∈ {y | v 1 ∈ Support(y)}] = p * , ∀ v 2 ̸ =v 1 Pr [PE(v 2 ) ∈ {y | v 1 ∈ Support(y)}] = q * .
A pure protocol is in some sense "pure and simple".
For each input v 1 , the set {y | v 1 ∈ Support(y)} identifies all outputs y that "support" v 1 , and can be called the support set of v 1 .
A pure protocol requires the probability that any value v 1 is mapped to its own support set be the same for all values.
We use p * to denote this probability.
In order to satisfy LDP, it must be possible for a value v 2 ̸ = v 1 to be mapped to v 1 's support set.
It is required that this probability, which we use q * to denote, must be the same for all pairs of v 1 and v 2 .
Intuitively, we want p * to be as large as possible, and q * to be as small as possible.
However, satisfying ε-LDP requires that p * q * ≤ e ε .
Basic RAPPOR is pure with p * = 1 − f 2 and q * = f 2 .
RAPPOR is not pure because there does not exist a suitable q * due to collisions in mapping values to bit vectors.
Assuming the use of two hash functions, ifv 1 is mapped to [1, 1, 0, 0], v 2 is mapped to [1, 0, 1, 0], and v 3 is mapped to [0, 0, 1, 1], then because [1, 1, 0, 0] differs from [1, 0, 1, 0] by only two bits, and from [0, 0, 1, 1] by four bits, the probability that v 2 is mapped to v 1 's support set is higher than that of v 3 being mapped to v 1 's support set.For a pure protocol, let y j denote the submitted value by user j, a simple aggregation technique to estimate the number of times that i occurs is as follows:˜ c(i) = ∑ j 1 Support(y j ) (i) − nq * p * − q * (1)The intuition is that each output that supports i gives an count of 1 for i. However, this needs to be normalized, because even if every input is i, we only expect to see n · p * outputs that support i, and even if input i never occurs, we expect to see n · q * supports for it.
Thus the original range of 0 to n is "compressed" into an expected range of nq * to np * .
The linear transformation in (1) corrects this effect.Theorem 1 For a pure LDP protocol PE and Support, (1) is unbiased, i.e., ∀ i E [ ˜ c(i) ] = n f i , where f i is the fraction of times that the value i occurs.Proof 1E [ ˜ c(i) ] =E ⎡ ⎣ ( ∑ j 1 Support(y j ) (i) ) − nq * p * − q * ⎤ ⎦ = n f i p * + n(1 − f i )q * − nq * p * − q * =n · f i p * + q * − f i q * − q * p * − q * =n f iThe variance of the estimator in 1 is a valuable indicator of an LDP protocol's accuracy:Theorem 2 For a pure LDP protocol PE and Support, the variance of the estimatioñ c(i) in (1) is:Var[ ˜ c(i)] = nq * (1 − q * ) (p * − q * ) 2 + n f i (1 − p * − q * ) p * − q * (2)Proof 2 The random variable˜cvariable˜ variable˜c(i) is the (scaled) summation of n independent random variables drawn from the Bernoulli distribution.
More specifically, n f i (resp.
(1 − f i )n) of these random variables are drawn from the Bernoulli distribution with parameter p * (resp.
q * ).
Thus,Var[ ˜ c(i)] = Var ⎡ ⎣ ( ∑ j 1 Support(y j ) (i) ) − nq * p * − q * ⎤ ⎦ = ∑ j Var[1 Support(y j ) (i)] (p * − q * ) 2 = n f i p * (1 − p * ) + n(1 − f i )q * (1 − q * ) (p * − q * ) 2 = nq * (1 − q * ) (p * − q * ) 2 + n f i (1 − p * − q * ) p * − q * (3)In many application domains, the vast majority of values appear very infrequently, and one wants to identify the more frequent ones.
The key to avoid having lots of false positives is to have low estimation variances for the infrequent values.
When f i is small, the variance in (2) is dominated by the first term.
We use Var * to denote this approximation of the variance, that is:Var * [ ˜ c(i)] = nq * (1 − q * ) (p * − q * ) 2(4)We also note that some protocols have the property that p * + q * = 1, in which case Var * = Var.
As the estimatioñ c(i) is the sum of many independent random variables, its distribution is very close to a normal distribution.
Thus, the mean and variance of˜cof˜ of˜c(i) fully characterizes the distribution of˜cof˜ of˜c(i) for all practical purposes.
When comparing different methods, we observe that fixing ε, the differences are reflected in the constants for the variance, which is where we focus our attention.
We now cast many protocols that have been proposed into our framework of "pure" LDP protocols.
Casting these protocols into the framework of pure protocols enables us to derive their variances and understand how each method's accuracy is affected by parameters such as domain size, ε, etc.
This also enables us to generalize and optimize these protocols and propose two new protocols that improve upon existing ones.
More specifically, we will consider the following protocols, which we organize by their encoding methods.
• Direct Encoding (DE).
There is no encoding.
It is a generalization of the Random Response technique.
• Histogram Encoding (HE).
An input v is encoded as a histogram for the d possible values.
The perturbation step adds noise from the Laplace distribution to each number in the histogram.
We consider two aggregation techniques, SHE and THE.-Summation with Histogram Encoding (SHE) simply sums up the reported noisy histograms from all users.-Thresholding with Histogram Encoding (THE) is parameterized by a value θ ; it interprets each noisy count above a threshold θ as a 1, and each count below θ as a 0.
• Unary Encoding (UE).
An input v is encoded as a length-d bit vector, with only the bit corresponding to v set to 1.
Here two key parameters in perturbation are p, the probability that 1 remains 1 after perturbation, and q, the probability that 0 is perturbed into 1.
Depending on their choices, we have two protocols, SUE and OUE.-Symmetric Unary Encoding (SUE) uses p + q = 1; this is the Basic RAPPOR protocol [13].
-Optimized Unary Encoding (OUE) uses optimized choices of p and q; this is newly proposed in this paper.
• Local Hashing (LH).
An input v is encoded by choosing at random H from a universal hash function family H, and then outputting (H, H(v)).
This is called Local Hashing because each user chooses independently the hash function to use.
Here a key parameter is the range of these hash functions.
Depending on this range, we have two protocols, BLH and OLH.-Binary Local Hashing (BLH) uses hash functions that outputs a single bit.
This is equivalent to the random matrix projection technique in [6].
-Optimized Local Hashing (OLH) uses optimized choices for the range of hash functions; this is newly proposed in this paper.
One natural method is to extend the binary response method to the case where the number of input values is more than 2.
This is used in [23].
Encoding and Perturbing.
Encode DE (v) = v, and Perturb is defined as follows.Pr [Perturb DE (x) = i] = { p = e ε e ε +d−1 , if i = x q = 1−p d−1 = 1 e ε +d−1 , if i ̸ = x Theorem 3 (Privacy of DE) The Direct Encoding (DE) Protocol satisfies ε-LDP.
Proof 3 For any inputs v 1 , v 2 and output y, we have:Pr [PE DE (v 1 ) = y] Pr [PE DE (v 2 ) = y] ≤ p q = e ε /(e ε + d − 1) 1/(e ε + d − 1) = e εAggregation.
Let the Support function for DE be Support DE (i) = {i}, i.e., each output value i supports the input i.
Then this protocol is pure, with p * = p and q * = q. Plugging these values into (4), we haveVar * [ ˜ c DE (i)] = n · d − 2 + e ε (e ε − 1) 2Note that the variance given above is linear in nd.
As d increases, the accuracy of DE suffers.
This is because, as d increases, p = e ε e ε +d−1 , the probability that a value is transmitted correctly, becomes smaller.
For example, when e ε = 49 and d = 2 16 , we have p = 49 65584 ≈ 0.00075.
In Histogram Encoding (HE), an input x ∈ [d] is encoded using a length-d histogram.Encoding.Encode HE (v) = [0.0, 0.0, · · · , 1.0, · · · , 0.0],where only the v-th component is 1.0.
Two different input v values will result in two vectors that have L1 distance of 2.0.
Perturbing.
Perturb HE (B) outputs B ′ such that B ′ [i] = B[i] + Lap ( 2 ε ), where Lap (β ) is the Laplace distribution wherePr [Lap (β ) = x] = 1 2β e −|x|/β .
Theorem 4 (Privacy of HE) The Histogram Encoding protocol satisfies ε-LDP.
Proof 4 For any inputs v 1 , v 2 , and output B, we have Pr[B|v 1 ] Pr[B|v 2 ] = ∏ i∈[d] Pr[B[i]|v 1 ] ∏ i∈[d] Pr[B[i]|v 2 ] = Pr[B[v 1 ]|v 1 ]Pr[B[v 2 ]|v 1 ] Pr[B[v 1 ]|v 2 ]Pr[B[v 2 ]|v 2 ] ≤ e ε/2 · e ε/2 = e ε Aggregation:Support THE (B) = {v | B[v] > θ }That is, each noise count that is > θ supports the corresponding value.
This thresholding step can be performed either by the user or by the aggregator.
It does not access the original value, and thus does not affect the privacy guarantee.
Using thresholding to provide a Support function makes the protocol pure.
The probability p * and q * are given byp * = 1 − F(θ − 1); q * = 1 − F(θ ),whereF(x) = { 1 2 e ε 2 x , if x < 0 1 − 1 2 e − ε 2 x , if x ≥ 0Here, F(·) is the cumulative distribution function of Laplace distribution.
If 0 ≤ θ ≤ 1, then we havep * = 1 − 1 2 e ε 2 (θ −1) ; q * = 1 2 e − ε 2 θ .
Plugging these values into (4), we haveVar * [ ˜ c HET (i)] = n · 2e εθ /2 − 1 (1 + e ε(θ −1/2) − 2e εθ /2 ) 2Comparing SHE and THE.
Fixing ε, one can choose a θ value to minimize the variance.
Numerical analysis shows that the optimal θ is in ( 1 2 , 1), and depends on ε.
When ε is large, θ → 1.
Furthermore, Var[ ˜ c THE ] < Var[ ˜ c SHE ] is always true.
This means that by thresholding, one improves upon directly summing up noisy counts, likely because thresholding limits the impact of noises of large magnitude.
In Section 5, we illustrate the differences between them using actual numbers.
Basic RAPPOR, which we described in Section 2.2, takes the approach of directly perturbing a bit vector.
We now explore this method further.
Perturbing.
Perturb(B) outputs B ′ as follows:Pr [ B ′ [i] = 1 ] = { p, if B[i] = 1 q, if B[i] = 0Theorem 6 (Privacy of UE) The Unary Encoding protocol satisfies ε-LDP forε = ln ( p(1 − q) (1 − p)q )(5)Proof 6 For any inputs v 1 , v 2 , and output B, we havePr [B|v 1 ] Pr [B|v 2 ] = ∏ i∈[d] Pr [B[i]|v 1 ] ∏ i∈[d] Pr [B[i]|v 2 ](6)≤ Pr [B[v 1 ] = 1|v 1 ] Pr [B[v 2 ] = 0|v 1 ] Pr [B[v 1 ] = 1|v 2 ] Pr [B[v 2 ] = 0|v 2 ](7)= p q · 1 − q 1 − p = e ε (6)is because each bit is flipped independently, and (7) is because v 1 and v 2 result in bit vectors that differ only in locations v 1 and v 2 , and a vector with position v 1 being 1 and position v 2 being 0 maximizes the ratio.
(5) into (4), we haveVar * [ ˜ c UE (i)] = nq(1 − q) (p − q) 2 = nq(1 − q) ( e ε q 1−q+e ε q − q) 2 = n · ((e ε − 1)q + 1) 2 (e ε − 1) 2 (1 − q)q .
(8)Symmetric UE (SUE).
RAPPOR's implementation chooses p and q such that p + q = 1; making the treatment of 1 and 0 symmetric.
Combining this with (5), we havep = e ε/2 e ε/2 + 1 , q = 1 e ε/2 + 1Plugging these into (8), we haveVar * [ ˜ c SUE (i)] = n · e ε/2 (e ε/2 − 1) 2Optimized UE (OUE).
Instead of making p and q symmetric, we can choose them to minimize (8).
Take the partial derivative of (8) with respect to q, and solving q to make the result 0, we get:∂ [ ((e ε −1)q+1) 2 (e ε −1) 2 (1−q)q ] ∂ q = ∂ [ 1 (e ε −1) 2 · ( (e ε −1) 2 q 1−q + 2(e ε −1) 1−q + 1 q(1−q) )] ∂ q = ∂ [ 1 (e ε −1) 2 · ( −(e ε − 1) 2 + e 2ε 1−q + 1 q )] ∂ q = 1 (e ε − 1) 2 ( e 2ε (1 − q) 2 − 1 q 2 ) = 0 =⇒ 1 − q q = e ε , i.e., q = 1 e ε + 1 and p = 1 2 Plugging p = 1 2 and q= 1 e ε +1 into (8), we get Var * [ ˜ c OUE (i)] = n 4e ε (e ε − 1) 2(9)The reason why setting p = 1 2 and q = 1 e ε +1 is optimal when the true frequencies are small may be unclear at first glance; however, there is an intuition behind it.
When the true frequencies are small, d is large.
Recall that e ε = p 1−p 1−q q .
Setting p and q can be viewed as splitting ε into ε 1 + ε 2 such that p 1−p = e ε 1 and 1−q q = e ε 2 .
That is, ε 1 is the privacy budget for transmitting the 1 bit, and ε 2 is the privacy budget for transmitting each 0 bit.
Since there are many 0 bits and a single 1 bit, it is better to allocate as much privacy budget for transmitting the 0 bits as possible.
In the extreme, setting ε 1 = 0 and ε 2 = ε means that setting p = 1 2 .
Both HE and UE use unary encoding and have Θ(d) communication cost, which is too large for some applications.
To reduce the communication cost, a natural idea is to first hash the input value into a domain of size k < d, and then apply the UE method to the hashed value.
This is the basic idea underlying the RAPPOR method.
However, a problem with this approach is that two values may be hashed to the same output, making them indistinguishable from each other during decoding.
RAP-POR tries to address this in several ways.
One is to use more than one hash functions; this reduces the chance of a collision.
The other is to use cohorts, so that different cohorts use different sets of hash functions.
These remedies, however, do not fully eliminate the potential effect of collisions.
Using more than one hash functions also means that every individual bit needs to be perturbed more to satisfy ε-LDP for the same ε.
A better approach is to make each user belong to a cohort by herself.
We call this the local hashing approach.The random matrix-base protocol in [6] (described in Section 2.4), in its very essence, uses a local hashing encoding that maps an input value to a single bit, which is then transmitted using randomized response.
Below is the Binary Local Hashing (BLH) protocol, which is logically equivalent to the one in Section 2.4, but is simpler and, we hope, better illustrates the essence of the idea.Let H be a universal hash function family, such that each hash function H ∈ H hashes an input in [d] into one bit.
The universal property requires that∀x, y ∈ [d], x ̸ = y : Pr H∈H [H(x) = H(y)] ≤ 1 2 .
Encoding.
Encode BLH (v) = ⟨H, b⟩, where H ← R H is chosen uniformly at random from H, and b = H(v).
Note that the hash function H can be encoded using an index for the family H and takes only O(log n) bits.
Perturbing.
Perturb BLH (⟨H, b⟩) = ⟨H, b ′ ⟩ such that Pr [ b ′ = 1 ] = { p = e ε e ε +1 , if b = 1 q = 1 e ε +1 , if b = 0 Aggregation.
Support BLH (⟨H, b⟩) = {v | H(v) = b}, that Once the random matrix projection protocol is cast as binary local hashing, we can clearly see that the encoding step loses information because the output is just one bit.
Even if that bit is transmitted correctly, we can get only one bit of information about the input, i.e., to which half of the input domain does the value belong.
When ε is large, the amount of information loss in the encoding step dominates that of the random response step.
Based on this insight, we generalize Binary Local Hashing so that each input value is hashed into a value in [g], where g ≥ 2.
A larger g value means that more information is being preserved in the encoding step.
This is done, however, at a cost of more information loss in the random response step.
As in our analysis of the Direct Encoding method, a large domain results in more information loss.
Let H be a universal hash function family such that each H ∈ H outputs a value in [g].
Encoding.
Encode(v) = ⟨H, x⟩, where H ∈ H is chosen uniformly at random, and x = H(v).
Perturbing.
Perturb(⟨H, x⟩) = (⟨H, y⟩), where∀ i∈[g] Pr [y = i] = { p = e ε e ε +g−1 , if x = i q = 1e ε +g−1 , if x ̸ = i Theorem 7 (Privacy of LH) The Local Hashing (LH) Protocol satisfies ε-LDP Proof 7 For any two possible input values v 1 , v 2 and any output ⟨H, y⟩, we have,Pr [⟨H, y⟩|v 1 ] Pr [⟨H, y⟩|v 2 ] = Pr [Perturb(H(v 1 )) = y] Pr [Perturb(H(v 2 )) = y] ≤ p q = e ε Aggregation.
Let Support LH (⟨H, y⟩) = {i | H(i) = y},i.e., the set of values that are hashed into the reported value.
This gives rise to a pure protocol withp * = p and q * = 1 g p + g − 1 g q = 1 g .
Plugging these values into (4), we have theVar * [ ˜ c LP (i)] = n · (e ε − 1 + g) 2 (e ε − 1) 2 (g − 1).
Optimized LH (OLH) Now we find the optimal g value, by taking the partial derivative of (10) with respect to g. (8), and∂ [ (e ε −1+g) 2 (e ε −1) 2 (g−1) ] ∂ g = ∂ [ g−1 (e ε −1) 2 + 1 g−1 · e 2ε (e ε −1) 2 + 2e ε (e ε −1) 2 ] ∂ g = 1 (e ε − 1) 2 − 1 (g − 1) 2 · e 2ε (e ε − 1) 2 = 0 =⇒ g = e ε + 1 When g = e ε + 1, we have p * = e ε e ε +g−1 = 1 2 , q * = 1 g = 1 e ε +1 intoVar * [ ˜ c OLH (i)] = n · 4e ε (e ε − 1) 2 .
(11)Comparing OLH with OUE.
It is interesting to observe that the variance we derived for optimized local hashing (OLH), i.e., (11) is exactly that we have for optimized unary encoding (OUE), i.e., (9).
Furthermore, the probability values p * and q * are also exactly the same.
This illustrates that OLH and OUE are in fact deeply connected.
OLH can be viewed as a compact way of implementing OUE.
Compared with OUE, OLH has communication cost O(log n) instead of O(d).
The fact that optimizing two apparently different encoding approaches, namely, unary encoding and local hashing, results in conceptually equivalent protocol, seems to suggest that this may be optimal (at least when d is large).
However, whether this is the best possible protocol remains an interesting open question.
We have cast most of the LDP protocols proposed in the literature into our framework of pure LDP protocols.
Doing so also enables us to generalize and optimize existing protocols.
Now we are able to answer the question: Which LDP protocol should one use in a given setting?Guideline.
Table 1 lists the major parameters for the different protocols.
Histogram encoding and unary encoding requires Θ(d) communication cost, and is expensive when d is large.
Direct encoding and local hashing require Θ(log d) or Θ(log n) communication cost, which amounts to a constant in practice.
All protocols other than DE have O(n · d) computation cost to estimate frequency of all values.Numerical values of the approximate variances using (4) for all protocols are given in Table 2 and Figure 1 (n = 10, 000).
Our analysis gives the following guidelines for choosing protocols.
• When d is small, more precisely, when d < 3e ε + 2, DE is the best among all approaches.
• Discussion.
In addition to the guidelines, we make the following observations.
Adding Laplacian noises to a histogram is typically used in a setting with a trusted data curator, who first computes the histogram from all users' data and then adds the noise.
SHE applies it to each user's data.
Intuitively, this should perform poorly relative to other protocols specifically designed for the local setting.
However, SHE performs very similarly to BLH, which was specifically designed for the local setting.
In fact, when ε > 2.5, SHE performs better than BLH.
While all protocols' variances depend on ε, the relationships are different.
BLH is least sensitive to change in ε because binary hashing loses too much information.
Indeed, while all other protocols have variance goes to 0 when ε goes to infinity, BLH has variance goes to n.
SHE is slightly more sensitive to change in ε.
DE is most sensitive to change in ε; however, when d is large, its variance is very high.
OLH and OUE are able to better benefit from an increase in ε, without suffering the poor performance for small ε values.Another interesting finding is that when d = 2, the variance of DE is e ε (e ε −1) 2 , which is exactly 1 4 of that of USENIX Association 26th USENIX Security Symposium 737 OUE and OLH, whose variances do not depend on d. Intuitively, it is easier to transmit a piece of information when it is binary, i.e., d = 2.
As d increases, one needs to "pay" for this increase in source entropy by having higher variance.
However, it seems that there is a cap on the "price" one must pay no matter how large d is, i.e., OLH's variance does not depend on d and is always 4 times that of DE with d = 2.
There may exist a deeper reason for this rooted in information theory.
Exploring these questions is beyond the scope of this paper.DE SHE THE (θ = 1) SUE OUE BLH OLH Communication Cost O(log d) O(d) O(d) O(d) O(d) O(log n) O(log n) Var[ ˜ c(i)]/n d−2+e ε (e ε −1) 2 8 ε 2 2e ε/2 −1 (e ε/2 −1) 2 e ε/2 (e ε/2 −1) 2 4e ε (e ε −1) 2 (e ε +1) 2 (e ε −1) 2 4e ε (e ε −1) 2 We empirically evaluate these protocols on both synthetic and real-world datasets.
All experiments are performed ten times and we plot the mean and standard deviation.
The conclusions we drew above are based on analytical variances.
We now show that our analytical results of variances match the empirically measured squared errors.
For the empirical data, we issue queries using the protocols and measure the average of the squared errors, namely, 1d ∑ i∈[d] [ ˜ c(i) − n f i ] 2 ,where f i is the fraction of users taking value i.
We run queries for all i values and repeat for ten times.
We then plot the average and standard deviation of the squared error.
We use synthetic data generated by following the Zipf's distribution (with distribution parameter s = 1.1 and n = 10, 000 users), similar to experiments in [13].
Figure 2 gives the empirical and analytical results for all methods.
In Figures 2(a) and 2(b), we fix ε = 4 and vary the domain size.
For sufficiently large d (e.g., d ≥ 2 6 ), the empirical results match very well with the analytical results.
When d < 2 6 , the analytical variance tends to underestimate the variance, because in (4) we ignore the f i terms.
Standard deviation of the measured squared error from different runs also decreases when the domain size increases.
In Figures 2(c) and 2(d), we fix the domain size to d = 2 10 and vary the privacy budget.
We can see that the analytical results match the empirical results for all ε values and all methods.In practice, since the group size g of OLH can only be integers, we round g = e ε + 1 to the nearest integer.
We run OLH, BLH, together with RAPPOR, on real datasets.
The goal is to understand how does each protocol perform in real world scenarios and how to interpret the result.
Note that RAPPOR does not fall into the pure framework of LDP protocols so we cannot use Theorem 2 to obtain the variance analytically.
Instead, we run experiments to examine its performance empirically.
Following the setting of Erlingsson et al. [13], we use a 128-bit Bloom filter, 2 hash functions and 8/16 cohorts in RAPPOR.
In order to vary ε, we tweak the f value.
The instantaneous randomization process is omitted.
We implement RAPPOR in Python.
The regression part, which RAPPOR introduces to handle the collisions in the Bloom filter, is implemented using Scikit-learn library [4].
Datasets.
We use the Kosarak dataset [2], which contains the click stream of a Hungarian news website.
There are around 8 million click events for 41, 270 different pages.
The goal is to estimate the popularity of each page, assuming all events are reported.
One goal of estimating a distribution is to find out the frequent values and accurately estimate them.
We run different methods to estimate the distribution of the Kosarak dataset.
After the estimation, we issue queries for the 30 most frequent values in the original dataset.
We then calculate the average squared error of the 30 estimations produced by different methods.
Figure 3 shows the result.
We try RAPPOR with both 8 cohorts (RAP(8)) and 16 cohorts (RAP(16)).
It can be seen that when ε > 1, OLH starts to show its advantage.
Moreover, variance of OLH decreases fastest among the four.
Due to the internal collision caused by Bloom filters, the accuracy of RAPPOR does not benefit from larger ε.
We also perform this experiment on different datasets, and the results are similar.
Although there are noises, infrequent values are still unlikely to be estimated to be frequent.
Statistically, the frequent estimates are more reliable, because the probability it is generated from an infrequent value is quite low.
However, for the infrequent estimates, we don't know whether it comes from an originally infrequent value or a zero-count value.
Therefore, after getting the estimation, we need to choose which estimate to use, and which to discard.Significance Threshold.
In [13], the authors propose to use the significance threshold.
After the estimation, all estimations above the threshold are kept, and those below the threshold T s are discarded.T s = Φ −1 ( 1 − α d ) √ Var * ,where d is the domain size, Φ −1 is the inverse of the cumulative density function of standard normal distribution, and the term inside the square root is the variance of the protocol.
Roughly speaking, the parameter α controls the number of values that originally have low frequencies but estimated to have frequencies above the threshold (also known as false positives).
We use α = 0.05 in our experiment.For the values whose estimations are discarded, we don't know for sure whether they have low or zero frequencies.
Thus, a common approach is to assign the remaining probability to each of them uniformly.Recall Var * is the term we are trying to minimize.
So a protocol with a smaller variance will have a lower threshold; thus more values can be detected reliably.Number of Reliable Estimation.
We run different protocols using the significance threshold T s on the Kosarak dataset.
Note that T s will change as ε changes.
We define a true (false) positive as a value that has frequency above (below) the threshold, and is estimated to have frequency above the threshold.
In Figure 4, we show the number of true positives versus ε.
As ε increases, the number of true positives increases.
When ε = 4, RAPPOR can output 75 true positives, BLH can only output 36 true positives, but OLH can output nearly 200 true positives.
We also notice that the output sizes are similar for RAPPOR and OLH, which indicates that OLH gives out very few false positives compared to RAPPOR.
The cohort size does not affect much in this setting.
Now we test both the number of true positives and false positives, varying the threshold.
We run OLH, BLH and RAPPOR on the Kosarak dataset.As we can see in Figure 5(a), fixing a threshold, OLH and BLH performs similarly in identifying true positives, which is as expected, because frequent values are rare, and variance does not change much the probability it is identified.
RAPPOR performs slightly worse because of the Bloom filter collision.As for the false positives, as shown in Figure 5(b), different protocols perform quite differently in eliminating false positives.
When fixing T s to be 5, 000, OLH produces tens of false positives, but BLH will produce thousands of false positives.
The reason behind this is that, for the majority of infrequent values, their estimations are directly related to the variance of the protocol.
A protocol with a high variance means that more infrequent values will become frequent during estimation.
As a result, because of its smallest Var * , OLH produces the least false positives while generating the most true positives.
The notion of differential privacy and the technique of adding noises sampled from the Laplace distribution were introduced in [11].
Many algorithms for the centralized setting have been proposed.
See [12] for a theoretical treatment of these techniques, and [19] for a treatment from a more practical perspective.
It appears that only algorithms for the LDP settings have seen real world deployment.
Google deployed RAPPOR [13] in Chrome, and Apple [1] also uses similar methods to help with predictions of spelling and other things.State of the art protocols for frequency estimation under LDP are RAPPOR by Erlingsson et al. [13] and Random Matrix Projection (BLH) by Bassily and Smith [6], which we have presented in Section 2 and compared with in detail in the paper.
These protocols use ideas from earlier work [20,9].
Our proposed Optimized Unary Encoding (OUE) protocol builds upon the Basic RAP-POR protocol in [13]; and our proposed Optimized Local Hashing (OLH) protocol is inspired by BLH in [6].
Wang There are other interesting problems in the LDP setting beyond frequency estimation.
In this paper we do not study them.
One problem is to identify frequent values when the domain of possible input values is very large or even unbounded, so that one cannot simply obtain estimations for all values to identify which ones are frequent.
This problem is studied in [17,6,16].
Another problem is estimating frequencies of itemsets [14,15].
Nguyên et al. [21] studied how to report numerical answers (e.g., time of usage, battery volume) under LDP.
When these protocols use frequency estimation as a building block (such as in [16]), they can directly benefit from results in this paper.
Applying insights gained in our paper to better solve these problems is interesting future work.
Kairouz et al. [18] study the problem of finding the optimal LDP protocol for two goals: (1) hypothesis testing, i.e., telling whether the users' inputs are drawn from distribution P 0 or P 1 , and (2) maximize mutual information between input and output.
We note that these goals are different from ours.
Hypothesis testing does not reflect dependency on d. Mutual information considers only a single user's encoding, and not aggregation accuracy.
For example, both global and local hashing have exactly the same mutual information characteristics, but they have very different accuracy for frequency estimation, because of collisions in global hashing.
Nevertheless, it is found that for very large ε's, Direct Encoding is optimal, and for very small ε's, BLH is optimal.
This is consistent with our findings.
However, analysis in [18] did not lead to generalization and optimization of binary local hashing, nor does it provide concrete suggestion on which method to use for a given ε and d value.
On answering multiple questions.
In the setting of traditional DP, the privacy budget is split when answering multiple queries.
In the local setting, previous work follow this tradition and let the users split privacy budget evenly and report on multiple questions.
Instead, we suggest partitioning the users randomly into groups, and letting each group of users answer a separate question.
Now we compare the utilities by these approaches.Suppose there are Q ≥ 2 questions.
We calculate variances on one question.
Since there are different number of users in the two cases (n versus n/Q), we normalize the estimations into the range from 0 to 1.
In OLH, the variance is σ 2 = Var * [ ˜ c OLH (i)/n] = 4e ε (e ε −1σ 2 2 − σ 2 1 = 4 n ( e ε/Q ( e ε/Q − 1 ) 2 − Qe ε (e ε − 1) 2 ) = 4e ε/Q n ( e ε/Q − 1 ) 2 (e ε − 1) 2 · [ (e ε − 1) 2 − Qe ε−ε/Q ( e ε/Q − 1 ) 2 ]The first term is always greater than zero since ε > 0.
For the second term, we define e ε/Q = z, and write it as:(z Q − 1) 2 − Qz Q−1 (z − 1) 2 =(z − 1) 2 · [ (z Q−1 + z Q−2 + . . . + 1) 2 − Qz Q−1 ] > 0Therefore, σ 2 1 is always smaller than σ 2 2 .
Thus utility of partitioning users is better than splitting privacy budget.Limitations.
The current work only considers the framework of pure LDP protocols.
It is not known whether a protocol that is not pure will produce more accurate result or not.
Moreover, current protocols can only handle the case where the domain is limited, or a dictionary is available.
Other techniques are needed when the domain size is very big.
In this paper, we study frequency estimation in the Local Differential Privacy (LDP) setting.
We have introduced a framework of pure LDP protocols together with a simple and generic aggregation and decoding technique.
This framework enables us to analyze, compare, generalize, and optimize different protocols, significantly improving our understanding of LDP protocols.
More concretely, we have introduced the Optimized Local Hashing (OLH) protocol, which has much better accuracy than previous frequency estimation protocols satisfying LDP.
We provide a guideline as to which protocol to choose in different scenarios.
Finally we demonstrate the advantage of the OLH in both synthetic and real-world datasets.
This paper is based on work supported by the United States National Science Foundation under Grant No. 1640374.
This section provides additional experimental evaluation results.
We first try to measure average squared variance on other datasets.
Although RAPPOR did not specify a particular optimal setting, we vary the number of cohorts and find differences.
In the end, we evaluate different methods on the Rockyou dataset.
In [13], the authors did not identify the best cohort size to use.
Intuitively, if there are too few cohorts, many values will be hashed to be the same in the Bloom filter, making it difficult to distinguish these values.
If there are more cohorts, each cohort cannot convey enough useful information.
Here we try to test what cohort size we should use.
We generate 10 million values following the Zipf's distribution (with parameter 1.5), but only use the first 128 most frequent values because of memory limitation caused by regression part of RAPPOR.
We then run RAPPOR using 8, 16, 32, and 64, and 128 cohorts.
We measure the average squared errors of queries about the top 10 values, and the results are shown in Figure 7.
As we can see, more cohorts does not necessarily help lower the squared error because the reduced probability of collision within each cohort.
But it also has the disadvantage that each cohort may have insufficient information.
It can be seen OLH still performs best.
In Figure 6, we test performance of different methods on synthetic datasets.
We generate 10 million points following a normal distribution (rounded to integers, with mean 500 and standard deviation 10) and a Zipf's distribution (with parameter 1.5).
The values range from 0 to 1000.
We then test the average squared errors on the most frequent 100 values.
It can be seen that different methods perform similarly in different distributions.
RAPPOR using 16 cohorts performs better than BLH.
This is because when the number of cohort is enough, each user in a sense has his own hash functions.
This can be viewed as a kind of local hashing function.
When we only test the top 10 values instead of top 50, RAP(16) and BLH perform similarly.
Note that OLH performs best among all distributions.
We run experiments on the Rockyou dataset, which contains 21 million users' password in plaintext.
We first hash the plaintext into 20 bits, and use OLH, BLH, and Basic RAPPOR (also known as SUE in our framework) to test all hashed values.
It can be seen that OLH performs best in all settings, and basic RAPPOR outperforms BLH consistently.
When ε = 4, and threshold is 6000, OLH can recover around 50 true frequent hashes and 10 of false positives, which is 4 and 2 magnitudes smaller than BLH and basic RAPPOR, respectively.
The advantage is not significant when ε is small, since the variance difference is small.
26th USENIX Security Symposium 743
