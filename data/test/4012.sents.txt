We propose a hybrid model of differential privacy that considers a combination of regular and opt-in users who desire the differential privacy guarantees of the local privacy model and the trusted curator model, respectively.
We demonstrate that within this model, it is possible to design a new type of blended algorithm for the task of privately computing the most popular records of a web search log.
This blended approach provides significant improvements in the utility of obtained data compared to related work while providing users with their desired privacy guarantees.
Specifically, on two large search click data sets comprising 4.8 million and 13.2 million unique queries respectively, our approach attains NDCG values exceeding 95% across a range of commonly used privacy budget values.
Now more than ever we are confronted with the tension between collecting mass-scale user data and the ability to release or share this data in a way that preserves the privacy of individual users.
Today, an organization that needs user data to improve the quality of service they provide often has no choice but to perform the data collection themselves.
However, the users may not want to share their data with the organization, especially if they consider the data to be sensitive or private.
Similarly, the organization assumes liability by collecting sensitive user data: private information may be directly leaked through security breaches or subpoenas, or indirectly leaked by the output of computations done on the data.
Thus, both organizations and users would benefit not only from strong, rigorous privacy guarantees regarding the data collection process, but also from the organization collecting the minimum amount of data necessary to achieve their goal.
Some of the philosophy behind our work stems from a desire to enable privacy-preserving decentralized data collection that aggregates data from multiple entities into high quality datasets.
In the last decade, we have witnessed scores of adhoc approaches that have turned out to be inadequate for protecting privacy [33,23].
The problem stems from the impossibility of foreseeing all attacks of adversaries capable of utilizing outside knowledge.
Differential privacy [10,9,11], which has become the gold standard privacy guarantee in the academic literature, and is gaining traction in industry and government [13,17,28], overcomes the prior issues by focusing on the privatization algorithm applied to the data, requiring that it preserves privacy in a mathematically rigorous sense under an assumption of an omnipotent adversary.There are two primary models in the differential privacy framework that define how data is to be handled by the users and data collectors: the trusted curator model and the local model.Trusted curator model: Most differentially private algorithms developed to date operate in the trusted curator model: all users' data is collected by the curator before privatization techniques are applied to it.
In this model, although users are guaranteed that the released data set protects their privacy, they must be willing to share their private, unperturbed data with the curator and trust that the curator properly performs a privacy-preserving perturbation.Local model: As was most recently argued by Apple [17], users may not trust the data collector with their data, and may prefer privatization to occur before their data reaches the collector.
Since privatization occurs locally, this is known as the local differential privacy (LDP) model, or local model.
Over the last several years, we have seen some examples of the local model beginning to be used for data collection in practice, most notably in the context of the Chrome web browser [13] and Apple's data collection [17].
In the LDP model, a data collector such as Google or Apple obtains insights into the data without observing the exact values of user's private data.
This is achieved by applying a privacy-preserving perturbation to each user's private data before it leaves the user's device.
Since most people do not trust web companies with maintaining the privacy and security of their data [29], the minimal trust required of users towards the data collector is a very attractive property of the LDP model.
This approach protects not only the individual users, but also the data collector from the possible privacy breaches.
For these reasons, the local model directly embodies the "data minimization" principle described in the White House's 2012 consumer data privacy report [41].
Although it may seem counter-intuitive, it is possible to obtain useful insights even when the data collector does not have access to the original data and receives only data that has already been locally privatized.
Suppose a data collector wants to determine the proportion of the population that is HIVpositive.
The local privatization algorithm works as follows: each person contributing data secretly flips a coin.
If the coin lands heads, they report their true HIV status; otherwise, they report a status at random.
This algorithm, known as randomized response [40], guarantees each person plausible deniability and is differentially private.
Since the randomness is incorporated into the algorithm in a precisely specified way, the data collector is able to recover an accurate estimate of the true proportion of HIV-positive people if enough people contribute their locally privatized data.
Differential privacy: Formally, an algorithm A is (, δ)-differentially private [11] if and only if for all neighboring databases D and D differing in precisely one user's data, the following inequality is satisfied for all possible sets of outputs Y ⊆ Range(A):Pr[A(D) ∈ Y ] ≤ e Pr[A(D ) ∈ Y ] + δ.The definition of what it means for an algorithm to preserve differential privacy is the same for both the trusted curator model and the local model.
The only distinction is in the timing of when the privacy perturbation needs to be applied -in the local model, the data needs to undergo a privacypreserving perturbation before it is sent to the aggregator, whereas in the trusted curator model the aggregator may first collect all the data, and then apply a privacy-preserving perturbation.
The timing distinction leads to differences in what is meant by "neighboring databases" in the definition and which algorithms are analyzed.
In the local model, D represents data of a single user and D represents data of the same user, with possibly changed values.
In the trusted curator model, D represents data of all users and D represents data of all users, except values of one of the user's data may be altered.Current differential privacy literature considers the trusted curator model and the local model entirely independently.
Our goal is to show that there is much to be gained by combining the two.Hybrid model: Much of the contribution in this paper stems from our observation that the two models can co-exist.
As others have observed [2,1,7], people's attitudes toward privacy vary widely.
Specifically, some users may be comfortable with sharing their data with a trusted curator.Many companies rely on a group of beta testers with whom they have higher levels of mutual trust.
It is not uncommon for such beta testers to voluntarily opt-in to a less privacy-preserving model than that of an average end-user [32].
For example, Mozilla warns potential beta users of its Firefox browser that "Pre-release versions automatically send Telemetry data to Mozilla to help us improve Firefox 1 "; Google has a similar provision for the beta testers of the Canary build of the Chrome browser 2 .
For the users who have higher trust in the company -we call them the opt-in group, the trusted curator privacy model is a natural match.
For all other users -we call them clients, the local privacy model is appropriate.
Our goal is to demonstrate that by separating the user pool into these two groups, according to their trust (or lack thereof) in the data aggregator, we can improve the utility of the collected data.
We dub this new model the hybrid differential privacy model.
Heavy hitter discovery and estimation is a wellstudied problem in the context of information retrieval, and is one of the canonical problems in privacy-preserving data analysis [6,27].
Moreover, recent work in the LDP model is focused on precisely that problem [13,34] or very closely related ones of histogram computations [5,21].
However, current privacy-preserving approaches in the LDP model lead to utility losses that are quite significant, sometimes to the point where results are no longer usable.
Clearly, if the privacy-preserving perturbation makes the data deviate too far from the original, the approach will not be widely adopted.
This is especially true in the context of search tasks, where users have been conditioned for years to expect high-quality results.We consider two specific applications in the space of heavy hitter estimation: local search and search trend computation.
Local search: Much of the work in this paper is motivated by local search, an application of heavy hitter estimation.
Local search revolves around the problem of how a browser maker can collect information about users' clicks as they interact with search engines in order to create the head of the search, i.e., the collection of the most popular queries and their corresponding URLs, and make it available to users locally, i.e., on their devices.
Specifically, it involves computing on query-URL pairs, where the URLs are those clicked as a result of submitting the query and receiving a set of answers.A browser maker may choose to combine the results obtained from user interactions that stem from several search engines depending on the context or surface results obtained from Baidu and not Bing depending on the user's current geographic location.With proper privacy measures in place, this data set can be deployed in the end-user browser to serve the most common queries with a very low latency or in situations when the user is disconnected from the network.
Local search can be thought of as a form of caching, where many queries are answered in a manner that does not require a round trip to the server.
Such caching of the most frequently used queries locally has a disproportionately positive impact on the expected query latency [36,3] as queries to a search engine follow a power-law distribution [4].
Furthermore, it would not be unusual or require a significantly novel infrastructure, as plenty of data is delivered to the browser today, such as SafeBrowsing malware databases in Chrome and Firefox, Microsoft SmartScreen data in Internet Explorer, blocking lists for extensions such as AdBlock Plus, etc.
Trend computation: Search trend computation is a typical example of heavy hitter estimation.
This problem entails finding the most popular queries and sorting them in order of popularity; think about it as the top-10 computation based on local search observations.
An example of this is the Google trends service 3 , which has an always up-to-date list of trending topics and queries.Although trend computation is interesting, local search is a great deal harder to do well on while preserving most of the utility.
Luckily, in the domain of search quality, there are established metrics to numerically assess the quality of search results; one of such metrics is NDCG, and we rely on it heavily in assessing the performance of our proposed system.
Our paper makes the following contributions:• We introduce and utilize a more realistic, hybrid trust model, which removes the need for all users to trust a central curator.
• We propose Blender, an algorithm that operates with the hybrid differential privacy model for computing heavy hitters.
Blender blends the data of opt-in and all other users in order to improve the resulting utility.
• We test Blender on two common applications:search trend computation and local search and find that it preserves high levels of utility while maintaining differential privacy for reasonable privacy parameter values.
• As part of Blender, we propose an approach for automatically balancing the data obtained from participation of opt-in users with that of other users to maximize the eventual utility.
• We perform a comprehensive utility evaluation of Blender on two large web search data sets, comprising 4.8 million and 13.2 million queries, demonstrating that Blender maintains very high level of utility (i.e., NDCG values in excess of 95% across a range of parameters).
We now discuss the high-level overview of our proposed system, Blender, that coordinates the privatization, collection, and aggregation of data in the hybrid model, as well as some of the specific choices we make in this system.
We use the task of enabling local search based on user histories while preserving differential privacy throughout, but, as will become clear from the discussion, our model and system can also be applied to other frequency-based estimation tasks.
As discussed in Section 1, we consider two groups of users: the opt-in group, who are comfortable with privacy as ensured by the trusted curator model, and the clients, who desire the privacy guarantees of the local model.
The core of our innovation is to take advantage of the privatized information obtained from the opt-in group in order to create a more efficient (in terms of utility) algorithm for data collection from the clients.
Furthermore, the privatized results obtained from the opt-in group and from the clients are then "blended" in a way that takes into account the privatization algorithms used for each group, and thus, again, achieving an improved utility over a lessinformed combination of data from the two groups.
The problem of enabling privacy-preserving local search using past search histories can be viewed as the task of identifying the most frequent search records among the population of users, and estimating their underlying probabilities (both in a differential privacy-preserving manner).
In this context, we call the data collected from the users search records, where each search record is a pair of strings of the form query, U RL, representing a query that a user posed followed by the URL that the user subsequently clicked.
We denote by p q,u the true underlying probability of the search record q, u in the population.
We assume that our system receives a sample of users from the population, each holding their own collection of private data drawn independently and identically from the distribution over all records p. Its goal is to output an estimatê p of probabilities of the most frequent search records, while preserving differential privacy (in the trusted curator model) for the opt-in users and (in the local model) for the clients.Informal Overview of Blender: Figure 1 presents an architectural diagram of Blender.Blender serves as the trusted curator for the optin group of users, and begins by aggregating data from them.
Using a portion of the data, it constructs a candidate head list of records in a differentially private manner that approximates the most common search records in the population.
It additionally includes a single "wildcard" record, , , which represents all records in the population that weren't previously included in the candidate head list.
It then uses the remainder of the opt-in data to estimate the probability of each record in the candidate head list in a differentially private manner, and (optionally) trims the candidate head list down to create the final head list.
The result of this component of Blender is the privatized trimmed head list of search records and their corresponding probability and variance estimates, which can be shared with each user in the client group, and with the world.Each member of the client group receives the privatized head list obtained from the opt-in group.
Each client then uses the head list to apply a differential privacy-preserving perturbation to their data, subsequently reporting their perturbed results to Blender.
Blender then aggregates all the clients' reports and, using a statistical denoising procedure, estimates both the probability for each record in the head list as well as the variance of each of the estimated probabilities based on the clients' data.For each record, Blender combines the record's probability estimates obtained from the two groups.
It does so by taking a convex combination of the groups' probability estimates for each record, carefully weighted based on the record's variance estimate in each group.
The combined result under this weighting scheme yields a better probability estimate than either group is able to achieve individually.
Finally, Blender outputs the obtained records and their combined probability estimates, which can be used to drive local search, determine trends, etc.A Formal Overview of Blender: Figure 2 presents the precise algorithmic overview of each step, including key parameters.Lines 1-3 of Blender describe the treatment of data from opt-in users, line 4 -the treatment of clients, and line 5 -the process for combining the probability estimates obtained from the two groups.
The only distinction between opt-in users and clients in terms of privacy guarantees provided is the curator model -trusted curator and local model, respectively.
Other than that, both types of users are assumed to desire the same level of (, δ)-differential privacy.We will detail our choices for the privatization sub-algorithms and discuss their privacy properties next.
A key feature of Blender, however, is that its privacy properties do not depend on the specific choices of the sub-algorithms.
That is, as long as CreateHeadList, EstimateOptinProbabilities, and EstimateClientProbabilities each satisfy (, δ)-differential privacy in its respective curator model, then so does Blender.
This allows changing the sub-algorithms if better versions (utility-wise or implementation-wise) are discovered in the future.
Among the parameters of Blender, the first four (the privacy parameters and the sets of opt-in and client users) can be viewed as given externally, whereas the following five (the number of records collected from each user and the distribution of the privacy budget among the sub-algorithms' sub-components) can be viewed as knobs the designer of Blender is at liberty to tweak in order to improve the overall utility of Blender's results.
We now present the specific choices we made for the sub-algorithms in Blender.
Detailed technical discussions of their properties follow in Section 3.
Algorithms for Head List Creation and Probability Estimation Based on Opt-in User Data ( Figures 3, 4): The opt-in users are partitioned into two sets -S, whose data will be used for initial head list creation, and T , whose data will be used to estimate the probabilities and variances of records from the formed initial head list.The initial head list creation algorithm, described in Figure 3, constructs the list in a differentially private manner using search record data from group S.
The goal of the algorithm is to approximate the true set of most frequently searched and clicked search records as closely as possible, while ensuring differential privacy.
The algorithm follows the strategy introduced in [26] by aggregating the records of theBlender (, δ, O, C, m O , m C , f O , f C , M )Parameters:• , δ: the differential privacy parameters.
• O, C: the set of opt-in users and clients, respectively.
• mO, mC : the max number of records to collect from each opt-in / client user, respectively.
• fO: the fraction of the opt-in users to use in head list creation (the remainder are used to estimate the record probabilities).
• fC : the fraction of the clients' privacy budget to allocate to queries (as opposed to URLs).
• M : the maximum size of the finalized head list.
• HLS, HL: a map from each query to its corresponding set of URLs.
•ˆ pO, ˆ σ 2 O , ˆ pC , ˆ σ 2 C :vectors indexed by records in HL (and, overloaded to be indexed by queries in HL as well) containing the probability estimates and variance estimates for each record (and query).
Body 1: Arbitrarily partition O into S and T = O \ S, such that |S| = f O |O| and |T | = (1 − f O )|O|.
2: let HL S = CreateHeadList(, δ, S, m O ) be the initial head list of records computed based on opt-in users' data.
3: let HL, ˆ p O , ˆ σ 2 O = EstimateOptinProbabili- ties(, δ, T, m O , HL S , M) be the refined head list of records, their estimated probabilities, and estimated variances based on opt-in users' data.
4: letˆpletˆletˆp C , ˆ σ 2 C = EstimateClientProbabilities(, δ, C, m C , f C , HL) be the estimated record probabilities and estimated variances based on client reports.5: letˆpletˆ letˆp = BlendProbabilities(ˆ p O , ˆ σ 2 O , ˆ p C , ˆ σ 2 C , HL)be the combined estimate of record probabilities.
6: return HL, ˆ p. opt-in users from S, and including in the head list those records whose noisy count exceeds a threshold.
The noise to add to the true counts and the threshold to use are calibrated to ensure differential privacy, using [24].
Our algorithm differs from previous work in two ways: 1) it replaces the collection and thresholding of queries with the collection and thresholding of records (i.e., query -URL pairs) and 2) its definition of neighboring databases is that of databases differing in values of one user's records, rather than CreateHeadList(, δ, S, m O )Parameters:• , δ: the differential privacy parameters.
• S: a set of opt-in users.
• mO: the maximum number of records to collect from each opt-in user.
1: let N (r, D) = number of times an arbitrary record r appears in the given dataset D.2: for each user i ∈ S do 3:let D S,i be the database aggregating at most m O arbitrary records from i. 4: let D S be the concatenation of all D S,i databases.
5: let HL S be an empty map.6: b S = 2m O .
7: τ = bs · ln(exp( 2 ) + m O − 1) − ln(δ) .
8: Assert τ ≥ 1.
9: for each distinct q, u ∈ D S do 10:let Y be an independent draw from Lap(b S ), i.e., Laplace distribution with scale b S centered at 0.
11:if N (q, u, D S ) + Y > τ then 12:Add q to HL S if q ∈ HL S .
13:Append u to HL S [q].
14: Add , to HL S .
15: return HL S .
in the addition or removal of records of one user.
These necessitate the choice of m O = 1, as well as higher values for noise and threshold than in [24].
We introduce a wildcard record , to represent records not included in the head list, for the subsequent task of estimating their aggregate probability.For each record included in the initial head list, the algorithm described in Figure 4 uses the remaining opt-in users' data (from set T ) to differentially privately estimate their probabilities, denoted byˆpbyˆ byˆp O .
This algorithm is the standard Laplace mechanism from the differential privacy literature [10], with scale of noise calibrated to output sensitivity due to our definition of neighboring datasets.
Our implementation ensures (, 0)-differential privacy, which is a more stringent privacy guarantee than for any non-zero δ.
We need to set m O = 1 for the privacy guarantees to hold, because we treat data at the search record rather than query level.We form the final head list from the M most frequent records inˆpinˆ inˆp O .
Finally, the head list is passed to the client group, and the head list and its probability and variance estimates are passed to the BlendProbabilities step of Blender.The choice of how to split opt-in users into the sub-groups of S and T and the choice of M are un-EstimateOptinProbabilities(, δ, T, m O , HL S , M )Parameters:• , δ: the differential privacy parameters.
In fact, this algorithm achieves (, 0)-differential privacy, which is a stricter privacy guarantee than (, δ)-differential privacy, for all δ > 0.
• T : a set of opt-in users.
• mO: the maximum number of records to collect from each opt-in user.
• HLS: the initial head list of records whose probabilities are to be estimated.
• M : the maximum size of the finalized head list.
let Y be an independent draw from Lap(b T ).
12: as in line 13.
related to privacy constraints, and can be made by Blender's developer to optimize utility goals, as will be discussed in Section 4.2.1.
The technical discussions of the algorithms' privacy properties and variance estimate computations follow in Section 3.1 and Section 3.3.
ˆ p O,q,u = 1 |D T | (N (q, u, D T ) + Y ).
13: ˆ σ 2 O,q,u = ˆ p O,q,u (1− ˆ p O,q,u ) |D T |−1 + 2b 2 T |D T |·(|D T |−1)16: return HL, ˆ p O , ˆ σ 2 O .
Algorithms for client data collection (Fig- ures 5, 6): For privatization of client data, the records are no longer treated as a single entity, but rather in a two-stage process: first privatizing the query, then privatizing the URL.
This choice is intended to benefit utility as the number of queries isEstimateClientProbabilities(, δ, C, m C , f C , HL)Parameters:• , δ: the differential privacy parameters.
• C: the set of clients.
• mC : the number of records to collect from the client.
• fC : the fraction of the privacy budget to allocate to reporting queries.
• HL: a map from each query to its corresponding set of URLs.
1: Append query q = to HL.
2: for each query q ∈ HL do 3:Append URL u = to HL[q].
4: for each client i ∈ C do 5:let D C,i = LocalAlg(, δ, m C , f C , HL) be the reports from i's local execution of LocalAlg.
6: let D C be the concatenation of all reported client datasets, D C,i .
7: Denote |D C | as the total number of records in D C .
8: let variables Q , U , δ Q , δ U , k, t, kq, tq(∀q ∈ HL) be defined as in lines 2-4 of LocalAlg.
9: letˆrletˆ letˆr C , ˆ p C , ˆ σ 2 C be vectors indexed by records in HL (and overloading its use, also indexed by queries).
10: for q ∈ HL do 11:letˆrletˆ letˆr C,q be the fraction of queries q in D C .
:ˆ p C,q = ˆ r C,q − 1−t k−1 t− 1−t k−1 13: ˆ σ 2 C,q = 1 t− 1−t k−1 2 ˆ r C,q (1−ˆ r C,q ) |D C |−1 14: for u ∈ HL[q] do 15:letˆrletˆ letˆr C,q,u be the fraction of records which are q, u in D C .
: significantly larger than the number of URLs associated with any query, and hence allocating a larger portion of the privacy budget to the query-reporting stage is a prudent choice.
The process of local privatization of each client's value ( Figure 6) follows the strategy of the Exponential mechanism introduced by [30].
The privatization algorithm reports the true value with a certain bounded probability, and otherwise, randomizes the answer uniformly among all the other feasible values.ˆ p C,q,u = ˆ r C,q,u − (1−tq )t ˆ p C,q kq −1 − (1−t)(1− ˆ p C,q ) (k−1)kq t(tq − 1−tq kq −1 ) 17: ˆ σ 2 C,q,u = ˆ r C,q,u (1−ˆ r C,q,u ) |D C |−1 + 2|D C | |D C |−1 1−t (k−1)kq − t−ttq kq −1 k−2+t kt−1 ˆ r C,q,u + 1−t (k−1)kq − t−ttq kq −1 2 ˆ σ 2 C,q · 1 t 2 tq − 1−tq kq −1 2 18: returnˆpreturnˆ returnˆp C , ˆ σ 2 C .
The fact that the head list (approximating the setLocalAlg(, δ, m C , f C , HL)Parameters:• , δ: the differential privacy parameters.
• mC : the number of records to collect from the client.
• fC : the fraction of the privacy budget to allocate to reporting queries.
• HL: the head list, represented as a map keyed by queries {q1, . . . , q k , }.
The value for each q ∈ HL is defined as HL[q] = {u1, . . . , u l , }, representing all URLs in the head list associated with q.Body 1: let D C,i be the database aggregating at most m C records from current client i. 2: = /m C , and δ = δ/m C .
3:Q = f C , U = − Q and δ Q = f C δ , δ U = δ − δ Q .
4: k = |HL|, and t = exp( Q )+(δ Q /2)(k−1) exp( Q )+k−1.
5: for each q ∈ HL do:6: kq = |HL[q]|, and tq = exp( U )+(δ U /2)(kq −1) exp( U )+kq −1 .
7: for each q, u ∈ D C,i do 8:if q ∈ HL then 9:Set q = .
if u ∈ HL[q] then 11:Set u = .
12:With probability (1 − t), 13:let q be a unif.
random query from HL \ q. let u be a unif.
random URL from HL[q ].
report q , u .
continue 17:With probability (1 − tq), 18:let u be a unif.
random URL from HL[q]\u. report q, u .
20: continue 21:report q, u. of the most frequent records) is available to each client plays a crucial role in improving the utility of the data produced by this privatization algorithm compared to the previously known algorithms operating in the local privacy model.
Knowledge of the head list allows dedicating the entire privacy budget to report the true value, rather than having to allocate some of it for estimating an analogue of the head list, as done in [15,34].
Another distinction from the Exponential mechanism designed to improve utility is utilization of δ.
The choices of m C and f C are not related to privacy constraints, and can be made by Blender's developer to optimize utility goals, as will be dis-cussed in Section 4.2.1.
The local nature of the privatization algorith, i.e., the use of a randomization procedure that can report any record with some probability, induces a predictable bias to the distribution of reported records.
The removal of this bias, which we refer to as denoising (discussed further in Section 3.2), results in the proper probability estimatesˆpestimatesˆ estimatesˆp C ( Figure 5).
These probability estimates along with the variance estimates are then passed to the BlendProbabilities part of Blender.The technical discussion of the algorithm's privacy properties, the denoising procedure and variance estimate computations follow in Sections 3.2 and 3.3.
Algorithm for Blending (Figure 7): The blending portion of Blender combines the estimates produced by the opt-in and client probability-estimation algorithms by taking into account the sizes of the groups and the amount of noise each sub-algorithm added.
This produces a blended probability estimatê p which, in expectation, is more accurate than either group produced individually.
The procedure for blending is not subject to privacy constraints, as it operates on the data whose privacy has already been ensured by previous steps of Blender.
The motivation and technical discussion of blending follows in Section 3.3.
BlendProbabilities(ˆ p O , ˆ σ 2 O , ˆ p C , ˆ σ 2 C , HL)Parameters:• ˆ p O , ˆ p C : the probability estimates from the opt-in and client algorithms.
• ˆ σ O , ˆ σ C : the variance estimates from the opt-in and client algorithms.
• HL: the head list of records.
1: letˆpletˆ letˆp be a vector indexed by records in HL.
2: for q, u ∈ HL do 3:w q,u = ˆ σ 2 C,q,u ˆ σ 2 O,q,u +ˆσ+ˆσ 2 C,q,u .
4: ˆ p q,u = w q,u · ˆ p O,q,u + (1 − w q,u ) · ˆ p C,q,u .5: Optional: ProjectˆpProjectˆ Projectˆp onto probability simplex (e.g., see [39]).
6: returnˆpreturnˆ returnˆp.
We now present further technical details related to the instantiations of the sub-algorithms for Blender, such as statements of privacy properties and the motivation for BlendProbabilities.
Differential privacy of the algorithms handling optin client data follows directly from previous work.
LocalAlg is responsible for the privacy-preserving perturbation of each client's data before it gets sent to the server, and EstimateClientProbabilities is responsible for aggregating the received privatized data into a meaningful statistic.
We present the privacy statement and explain the logic behind the aggregation procedure next and prove them in Appendix A.Theorem 3.
LocalAlg is (, δ)-differentially pri- vate.Denoising: The reports aggregated by the client mechanism form an empirical distribution over the records (and queries).
Relative to the true underlying record distribution, this distribution is biased in an explicit and publicly-known way, as described by the reporting process.
Thus, we seek to obtain an unbiased estimate of the true record distribution from this reported distribution.
Concretely, we refer to this as denoising the reported empirical distributionˆrtributionˆ tributionˆr C to obtain the final estimate from the client algorithm, ˆ p C .
The denoising procedure relies only on the publicly-known reporting process as well as the already-privatized reports.
Thus, this can be considered a post-processing step, which has no negative impact on the differential privacy guarantee [11] yet significantly improves utility.Observation 1.
ˆ p Cgives the unbiased estimate of record and query probabilities under EstimateClientProbabilities.
The opt-in algorithm and the client algorithm both output independent estimatesˆpestimatesˆ estimatesˆp O andˆpandˆ andˆp C of the record distribution p.
The question we address now is how to best combine these estimates using the information available.A standard way to measure the quality of an estimate is by its variance.
Although it may seem natural to choose the estimate with lower variance as the final estimatê p, it is possible to achieve a better estimate by jointly utilizing the information provided by both algorithms.
This is because the errors in these algorithms' estimates come from different, independent sources.
The error in the estimates obtained from the opt-in algorithm is due to the addition of noise, whereas the error in the estimates obtained from the client algorithm is due to randomization of the reports over the set of records in the head list.
Thus, if we determine the variances of the estimates obtained from the two algorithms, we can use these variances to blend the estimates in the best way.More formally, for each record q, u let σ 2 O,q,uand σ 2 C,q,u be the variances of the opt-in and client algorithm's estimates ofˆpofˆ ofˆp O,q,u andˆpandˆ andˆp C,q,u respectively.
Since these variances depend on the underlying distribution, which is unknown a priori, we will compute sample variancesˆσvariancesˆ variancesˆσ 2 O,q,u andˆσandˆ andˆσ 2 O,q,u instead.
For each record q, u, we will weigh the estimate from the opt-algorithm by w q,u and the estimate from the client algorithm by (1−w q,u ), where w q,u is defined as in line 3 of BlendProbabilities.
The optional step of projecting the blended estimates (e.g., as in [39]) ensures that the estimates sum to 1 and are non-negative.
Theorem 4 presents our computation of the sample variance of EstimateOptinProbabilities, Theorem 5 presents our computation of the sample variance of EstimateClientProbabilities, and Theorem 6 motivates the weighting scheme used in BlendProbabilities.
Their proofs are presented in Appendix B.For the variance derivations, we make an explicit assumption that each piece of reported data is drawn independently and identically from the same underlying distribution.
This is reasonable when comparing data across users.
By setting m O = m C = 1, we remove the need to assume iid data within each user's own data, while simplifying our variance computations.
We show in Section 4 that Blender achieves high utility even when m O = m C = 1.
Theorem 4.
When m O = 1 the unbiased variance estimate for EstimateOptinProbabilities can be computed as:ˆ σ 2 O,q,u = |D T | |D T |−1 ˆ p O,q,u (1− ˆ p O,q,u ) |D T | + 2 b T |D T | 2 .
Theorem 5.
When m C = 1 the unbiased variance estimate for EstimateClientProbabilities can be computed as:ˆ σ 2 C,q,u = 1 t 2 tq − 1−tq kq −1 2 · ˆ r C,q,u (1−ˆ r C,q,u ) |D C |−1 + 1−t (k−1)kq − t−ttq kq −1 2 ˆ σ 2 C,q + 2|D C | |D C |−1 1−t (k−1)kq − t−ttq kq −1 k−2+t kt−1 ˆ r C,q,u .
Theorem 6 (Sample Variance Optimal Weighting).
IfˆσIfˆ Ifˆσ 2 O,q,u andˆσandˆ andˆσ 2 C,q,u are sample variances ofˆpofˆ ofˆp O,q,u andˆpandˆ andˆp C,q,u respectively, thenw q,u = ˆ σ 2 C,q,u ˆ σ 2 O,q,u +ˆσ+ˆσ 2 C,q,uis the sample variance optimal weighting.
We designed Blender with an eye toward preserving the utility of the eventual results in the two applications we explore in this paper: trend computation and local search, as described in Section 1.2.
We use two established domain-specific utility metrics to assess the utility, the L1 metric and NDCG.
L1: L1 is the Manhattan distance between the estimate and actual probability vectors, in other words,L1 = i |ˆp|ˆp i − p i |.
The smaller the L1, the better.
NDCG: NDCG is a standard measure of search quality [20,38] that explicitly takes the ordering of the items in a results list into account.
This measure uses a relevance score for each item: given a list of items and their true frequencies, we define the relevance or gain of the i th most frequent item as rel i = ni j nj , where n j is the number of occurrences of the j th most frequent item.
The discounted cumulative gain for the top k items in an estimated list (that is, a list that estimates the top k items and their frequencies) is typically computed as DCG k = k i=1 2 rel i −1 log 2 (i+1) .
Here, the log 2 (i + 1) factor diminishes the contribution of items later in the list, hence the notion of discounting.
In particular, getting the ordering correct for higher-relevance items early in the list yields a higher DCG k value.The magnitude of the DCG k value doesn't mean much on its own.
For better interpretability, it is usually normalized by the Ideal DCG (IDCG k ), which is the DCG k value if the estimated list had the exact same ordering as the actual list.
Thus, the normalized discounted cumulative gain (N DCG k ), which ranges between 0 and 1, is defined asN DCG k = DCG k /IDCG k .
While NDCG is traditionally defined for lists, Blender outputs a list-of-lists: there is a URL list corresponding to each query, and the queries themselves form a list.
Thus, we introduce a generalization of the traditional NDCG measure.
Specifically, for each query q, we first compute the NDCG as described above of q's URL list, N DCG q k .
We then define the DCG of the query list as its URL list was estimated.
The DCG value for the query list as a whole is then normalized by the analogous Ideal DCG (IDCG Q k ) -the DCG Q k if the estimated query list had the exact same ordering as the actual query list.DCG Q k = k i=1 2 rel i −1 log 2 (i+1) · N DCG i k .
Compared to the traditional NDCG definition, the additional discounting within DCG Q k makes it even harder to attain high NDCG values than in the query-only case.
Contrasted with the L1 measure, this formulation takes both the ranking and probabilities from the data set into account.
Since changes to the probabilities may not result in ranking changes, L1 is an even less forgiving measure than NDCG.Since the purpose of Blender is to estimate probabilities of the top records, we discard the artificially added queries and URLs and rescale rel i prior to L1 and NDCG computations.
However, since we use the method of [39] in BlendProbabilities, the probability estimates involving have a minor implicit effect on the L1 and NDCG scores.
Data sets: For our experiments, we use the AOL search logs, first released in 2006 and an order of magnitude bigger Yandex search data set 4 , from 2013.
Figure 8 compares their characteristics.Data analysis: To familiarize the reader with the approach we used for assessing result quality, Fig- ure 9 shows the top-10 most frequent queries in the AOL data set, with the estimates given by the different "ingredients" of Blender.The table is sorted by column 2, which contains the non-private, empirical probabilities p q for each query q from the AOL data set with 1 random record sampled from each user.
We consider this as the baseline for the true, underlying probability of that query.
Column 3 contains the final query probability estimates outputted by Blender, ˆ p q , after combining the estimates from the opt-in group and clients.
The remaining columns show the estimates that are produced by the sub-components of Blender that are eventually combined to form the estimates in column 3.
As the opt-in and client sub-components compute probability estimates over the records in the head list, we obtain query probability estimates by aggregating the probabilities associated with each URL for a given query (columns 4 and 6).
The sample variance of these aggregated probabilities, used for blending, is naively computed as in Theorem 4.
In addition to estimating the record probabilities, the client algorithm estimates query probabilities directly, which are shown in column 5.
Regressions, i.e., estimates that appear out of order relative to column 2, are shown in red.Takeaways: The biggest takeaway is that the numbers in columns 2 and 3 are similar to each other, with only one regression after Blender's usage.Blender compensates for the weaknesses of both the opt-in and the client estimates.
Despite the subcomponents having several regressions, their combination has only one.
The table also provides intuition for the usefulness of a two-stage reporting process in the client algorithm (first report a query and then the URL), thus allowing for separate estimates of query and record probabilities.
Specifically, despite the high number of regressions for the client algorithm's aggregated record probability estimates (column 6), its query probability estimates (column 5) have only one.
We formulate questions for our evaluation as follows: how to choose Blender's parameters (Section 4.2.1), how does Blender perform compared to alternatives (Section 4.2.2), and how robust are our findings (Section 4.2.3)?
Blender has a handful of parameters, some of which can be viewed as given externally (by the laws of nature, so to speak), and others whose choice is purely up to the entity that's utilizing Blender.
We now describe and, whenever possible, motivate, our choices for these.Privacy parameters, and δ: Academic literature on differential privacy views the selection of the parameter as a "social question" [9] and thus uses in the range of 0.01 to 10 for evaluating algorithm performance (see Table 1 in [18]).
The two known industry deployments of differential privacy (by Google [13] and Apple [17]) do not explicitly reveal the parameters used.
[25,37] found via reverseengineering of Apple's differential privacy implementation that Apple uses = 1 or = 2 per item submitted, but allows submission of several dozen items per day from one device.
A typical user might experience an of 4 -6 per day, but = 20 per day has also been observed [37].
The work most similar to ours, [34], performs evaluations using in the range [1,10].
We use = 4, unless otherwise stated.
Similarly, a range of δs has been used for evaluations (e.g., 10 −6 , 10 −5 , 10 −4 in [26] and 0.05 in [6]).
We use δ = 10 −5 for AOL and δ = 10 −7 for Yandex data sets, with the smaller δ choice for the latter reflecting the larger number of users in the data set.We use the same and δ values for the opt-in and client users.
From a behavioral perspective, this reduces a user's opt-in decision down to one purely of trust towards the curator.Opt-in and client group sizes, |O| and |C|: The relative sizes of opt-in group and client group, |O| and |C|, respectively, can be viewed as exogenous variables which are dictated by the trust that users place in the search engine.
We choose 5% and 2.5% for the fraction of opt-in users as compared to total users as these seem reasonable for representing the fraction of "early adopters" who are willing to supply their data for the improvement of products and allow us to demonstrate the utility benefits of algorithms designed to operate in the hybrid privacy model.
Remaining parameter choices (m C , f C , f O , M ) are driven purely by utility considerations.The number of records to collect from each client, m C = 1: Across a range of experimental values, collecting 1 record per user always yielded greatest utility, motivating this parameter choice.
Apple makes an analogous choice in their implementation -they (temporarily) store all relevant items on a client's device, and then choose 1 item of each type to transmit at random each day [37].
How to split the privacy budget between query and url reporting for clients, f C = 0.85: Figure 11 shows the effects of the budget split on both the L1 and NDCG metrics.
Unsurprisingly, Figure 11a shows that the larger the fraction of client algorithm's budget dedicated to query estimation as opposed to URL estimation, the better the L1 score for the client and Blender results.
The NDCG metric in Figure 11b shows a trade-off that emerges as we assign more budget to the queries, de-emphasizing the URLs; before and after 0.85, we start seeing a drop in NDCG values for the client algorithm.
The orange opt-in line in Figure 11b is constant, as the opt-in group is not affected by the budget split.
Somewhat surprisingly with this parameter setting, the NDCG for Blender result is also consistently high (nearly equal to and hidden by the opt-in line) and is unaffected by the budget split, unlike the L1 metric.What fraction of opt-in data to use for creating the headlist, f O = 0.95: Our goal is to build a large candidate head list, and unless we allocate most of the opt-in user data to building such a head list (algorithm CreateHeadList), our subsequent results may be accurate but apply only to a small number of records.
Since our opt-in group's size is small relative to our client group size, and it is difficult to generate a head list in the local privacy model -it makes sense to utilize most of the opt-in group's data for the task that is most difficult in the local model.
Through experiment we observe that increasing f O past 95% gives diminishing returns for increasing the head list size; on the other hand, there is a significant utility gain (NDCG and L1) from the use of a small fraction of opt-in users for estimating probabilities of the head list.
Thus, rather than using the entire opt-in group for head list generation (i.e., f O = 1), we reserve 5% of the opt-in data for probability estimation.What should be the final size of the set for which we provide probability estimates, M : The choice of M is influenced by competing considerations.
The larger the head list for which we provide the probability estimates, the more effective the local search application (subject to those probability estimates being accurate).
However, as desired head list size increases, the accuracy of our estimates drops (most notably due to client data privatization).
We want to strike a balance that allows us to get a sensibly large record set with reasonably accurate probability estimates it.
We choose M = 50 and M = 500 for the AOL and Yandex datasets, to reflect their differing sizes.Subsequently, we use the parameters shown in Fig- ure 10 unless explicitly stated.
The closest related work is a recent paper by Qin et al. [34] for heavy hitter estimation with local differential privacy, in which they provide a utility evaluation of their algorithm on the AOL data set for the head list size of 10.
We perform a direct comparison of their NDCG results with Blender's across values in the range of 1-5, which we plot in Figure 12.
Across the entire range of the privacy parameter, our NDCG values are above 95%, whereas the reported NDCG values for Qin et al. are in the 30% range, at best.
We believe that given the intense focus on search optimization in the field of information retrieval, NDCG values as low as those of Qin et al. are generally unusable, especially for such a small head list size.
Overall, Blender significantly outperforms what we believe to be the closest related research project.
A caveat to these findings is that Qin et al.[34] and this work use slightly different scoring functions.
The former's relevance score is based on the rank of queries in the original AOL data set, which results in penalizing mis-ranked queries regardless of how similar their underlying probabilities may be.
Blender's relevance score relies on the underlying probabilities, so mis-ranked items with similar underlying probabilities have only a small negative impact on the overall NDCG score; we believe this choice is justified.
Although it yields increased NDCG scores, Blender operates on records (rather than queries, as Qin et al. does).
Because of this, the generalized NDCG score used to evaluate Blender (Section 4) is a strictly less forgiving metric than the traditional NDCG score.
Thus, although simultaneously compensating for both differences would yield the ideal comparison, the one in Figure 12 is reasonable.
We now discuss how the size of the opt-in group and the choice of affect Blender's utility.Evaluation of trend computation: Figure 13 shows the L1 values as a function of the opt-in percentage ranging between 1% and 10%.
We see slight differences in the two data sets and across the various head list sizes.
Some of the differences might be due to the fact that given the relatively small size of the AOL data set, we need to consider higher opt-in percentages to get reasonably sized head lists and L1 values.
In fact, when we increase the optin percentage to 10% for the AOL data set, we see a decline in L1 values similar to what is observed in Figure 13b for the Yandex data set.
If our goal is to have head lists of 500+, we see that with the larger Yandex data set, an opt-in percentage as small as 2.5% is sufficient to achieve high utility.
On the other hand, portions of lines do not appear on figures if the desired head list size was not reached; e.g., in Figure 13a, the line for a head list of size 50 does not begin until 4.5% because that size head list was not created with a smaller opt-in percentage.
Figure 15 shows the L1 values as a function of , ranging from 1 to 5.
For both data sets, we see a steady decline in the L1 metric, despite aggregating L1 values over longer estimate vectors.
With more data in the Yandex data set, we are able to hit small values of L1 (under 0.1) with ≥ 1.
Similar to the case with small opt-in percentages, having too small an makes it difficult to achieve head lists of their target size; e.g., in Figure 15a, the line for a head list of size 50 does not begin until = 3 because that size head list was not created with a smaller value.Evaluation of local search computation: Fig- ure 14 shows the NDCG measurements as a function of the opt-in percentage ranging between 1% and 10%.
The results are quite encouraging; for the smaller AOL data set, for instance, we need to have an opt-in level of ≈5% to achieve an NDCG level of 95%, which we regard as acceptable.
However, for the larger Yandex data set, we hit that NDCG level even sooner: the NDCG value for 1.5% is above 95% for all but the largest head list size.
Figure 16 shows how the NDCG values vary across the two data sets for a range of head list sizes and values.
We see a clear trend toward higher NDCG values for Yandex, which is not surprising given the sheer volume of data.
For the Yandex data set, we can keep as low as 1 and still achieve NDCG values of 95% and above for all but the two largest head list sizes.
For those, we must increase in order to generate larger head lists from the opt-in users.
Algorithms for the trusted curator model: Researchers have developed numerous differentially private algorithms operating in the trusted curator model that result in useful data for a variety of applications.
For example, [24,26,16,31] address the problem of publishing a subset of the data contained in a search log with differential privacy guarantees; [27] and [6] propose approaches for frequent item identification; [14] propose an approach for monitoring aggregated web browsing activities; and so on.
ticularly among practitioners [17,35], fewer such algorithms are known [40,19,8,13,5].
Furthermore, the utility of the resulting data obtained through these algorithms is significantly limited compared to what is possible in the trusted curator model, as shown experimentally [15,21] and theoretically [22].
The recent work of [34] also takes a two-stage approach: first, spend some part of the privacy budget to learn a candidate head list and then use the remaining privacy budget to refine the probability estimates of the candidates.
However, that's where the similarities with Blender end, as [34] focuses entirely on the local model (and thus has to use entirely different algorithms from ours for each stage) and addresses the problem of estimating probabilities of queries, rather than the more challenging problem of estimating probabilities of query-URL pairs.Our contribution: Our work significantly improves upon the known results by developing application-specific local privatization algorithms that work in combination with the trusted curator model algorithms.
Specifically, our insight of providing all users with differential privacy guarantees but achieving it differently depending on whether or not they trust the data curator, enables an efficient privacy-preserving head list construction.
The subsequent usage of this head list in the algorithm operating in the local model helps overcome one of the main challenges to utility of privacy-preserving algorithms in the local model [15].
Moreover, the weighted aggregation of probability estimates obtained from algorithms operating in the two models (that explicitly factors in the amount of noise each contributed), enabled remarkable utility gains compared to usage of one algorithm's estimates.
As discussed in Section 4.2.2, we significantly outperform the most recently introduced local algorithm of [34] on metrics of utility in the search context.
Operating in the hybrid model is most beneficial utility-wise if the opt-in user records and client user records come from the same distribution -i.e., the two groups have fairly similar observed search behavior.
If that is not the case, the differential privacy guarantees still hold, but the accuracy of Blender's estimates may decrease.
Improvement in utility over what can be achieved in the local model comes from two sources: the hybrid privacy model lets us develop a better algorithm for client data collection and the analysis of algorithms' variances lets us smartly combine the results.In practice, a system for local search or trend computation would be run at regular intervals in order to refresh the data as well as accommodate for users being added to, removed from, or moving between the opt-in and the client groups.
We have focused on the problem of obtaining local search or trend computation results for a single execution of the system.
While one could simply re-run Blender at regular intervals to obtain new results (with potentially different opt-in and client groups), this comes at a cost to privacy.
We leave the task of improving the temporal aspect of Blender beyond what is achievable with standard composition techniques of differential privacy [11] to future work.
We proposed a hybrid privacy model and a blended approach that operates within it that combines the upsides of two common models of differential privacy: the local model and the trusted curator model.
Using local search as a motivating application, we demonstrated that our proposed approach leads to a significant improvement in terms of utility, bridging the gap between theory and practicality.Future work: We plan to continue this work in two directions: first, to address any systems and engineering challenges to Blender's adoption in practice, including those that arise due to data changing over time; and second, to develop algorithms for other settings where the hybrid privacy model is appropriate, thus facilitating adoption of differential privacy in practice by minimizing the utility impact of privacy-preserving data collection.
the f C fraction of the privacy budget to report a query, and 2) Usage of the remainder of the privacy budget to report a URL (given the reported query).
This decomposes a simultaneous two-item ( , δ ) reporting problem into two single-item reporting problems with ( Q , δ Q ) and ( U , δ U ) respectively, whereQ = f , δ Q = f δ , U = (1 − f C ), and δ U = (1 − f C )δ .
Consider the query-reporting case first.
Overloading our use of L, let L(q) be the portion of L that makes use of q.
We first ensure thatPr[L(q) = q HL ] ≤ exp( Q ) Pr[L(q ) = q HL ] + δ Q 2 (1)holds for all q, q , and q HL ∈ HL.
This trivially holds when q HL = q = q or q HL ∈ {q, q }.
The remaining scenarios to consider are: 1) q = q HL , q = q HL and 2) q = q HL , q = q HL .
By the design of the algorithm, Pr[L(q HL ) = q HL ] = t and Pr[L(¯ q HL ) = q HL ] = (1−t)( 1 k−1 ), where ¯ q HL represents any query not equal to q HL .
With t = exp(Q )+(δ Q /2)(k−1) exp( Q )+k−1, it is simple to verify that inequality (1) holds.Consider an arbitrary set of head list queries Y .
Pr[L(q) ∈ Y ] = q HL ∈Y Pr[L(q) = q HL ] = q HL ∈Y \{q,q } Pr[L(q) = q HL ] + q HL ∈Y ∩{q,q } P r[L(q) = q HL ] = q HL ∈Y \{q,q } Pr[L(q ) = q HL ] + q HL ∈Y ∩q,q Pr[L(q) = q HL ] (2) ≤ q HL ∈Y \{q,q } Pr[L(q ) = q HL ] + q HL ∈Y ∩{q,q } e Q Pr[L(q ) = q HL ] + δ Q 2 (3) ≤ e Q q HL ∈Y Pr[L(q ) = q HL ] + 2 · δ Q 2 = e Q Pr[L(q ) ∈ Y ] + δ Q ,Equality (2) stems from the fact that the probability of reporting a false query is independent of the user's true query.
The inequality (3) is a direct application of inequality (1).
Thus, L is ( Q , δ Q )-differentially private for query-reporting.2.
Privacy of URL Reporting:With t q defined as t q = exp( U )+0.5δ U (kq−1) exp( U )+kq−1, an analogous argument shows that the ( U , δ U )-differential privacy constraints hold if the original q is kept.
On the other hand, if it is replaced with a random query, then they trivially hold as the algorithm reports a random element in the URL list of the reported query, without taking into consideration the client's true URL u.By composition [12], each of the at mostm C itera- tions of L is ( Q + U , δ Q +δ U ) = ( , δ )-differentially private.
Observation 1.
ˆ p C gives the unbiased estimate of record and query probabilities under EstimateClientProbabilities.Proof.
Reporting records is a two-stage process (first, decide which query to report, then report a record); similarly, denoising is also done in two stages.
Denoising of query probability estimates: Let r C,q denote the probability that the algorithm has received query q as a report, and let p q be the true probability of a user having query q.
We want to learn p q based on r C,q .
By the design of our algorithm,r C,q = t · p q + q =q p q (1 − t) 1 k−1 = t · p q + 1−t k−1 q =q p q = t · p q + 1−t k−1 (1 − p q ).
Solving for p q in terms of r C,q yields p q = r C,q − 1−t k−1 t− 1−t k−1.
Using the obtained data for the queryˆrqueryˆ queryˆr C,q , we estimate p C,q asˆpasˆ asˆpC,q = ˆ r C,q − 1−t k−1 t− 1−t k−1 .
Analogously, denote by r C,q,u the probability that the algorithm has received a record q, u as a report, and recall p q,u is the record's true probability in the data set.
Then r C,q,u = t · t q · p q,u + t (1 − p q ), recalling from the algorithm that k q is the number of URLs associated with query q and t q is the probability of truthfully reporting u given that query q was reported.
Solving for .
Using the obtained data for the empirical report estimatê r C,q,u together with the query estimatê p C,q , we estimate p q,u asˆpasˆ asˆp C,q,u = .
Theorem 4.
When m O = 1 the unbiased variance estimate for EstimateOptinProbabilities can be computed as:ˆ σ 2 O,q,u = |D T | |D T |−1 ˆ p O,q,u (1− ˆ p O,q,u ) |D T | + 2 b T |D T | 2 .
Proof.
Given the head list, the distribution of EstimateOptinProbabilities' estimate for a record q, u is given by r O,q,u = p q,u + Y |D T | , where Y ∼ Laplace(b T ) where b T is the scale parameter and |D T | is the total number of records from the opt-in users used to estimate probabilities.
The empirical estimator for r O,q,u isˆrisˆ isˆr O,q,u = 1 |D T | |D T | j=1 X j + Y , where X j ∼ Bernoulli(p q,u ) is the random variable indicating whether report j was record q, u.The expectation of this estimator is given by E[ˆ r O,q,u ] = p q,u .
Thus, ˆ r O,q,u is an unbiased estimator for p q,u .
We denotê p O,q,u = ˆ r O,q,u to explicitly reference it as the estimator of p q,u .
The variance for this estimator isσ 2 O,q,u = V [ˆ p O,q,u ] = V 1 |D T | |D T | j=1 X j + Y = 1 |D T | 2 V |D T | j=1 X j + V [Y ] (4) = 1 |D T | 2 |D T | j=1 V [X j ] + V [Y ] (5) = 1 |D T | 2 |D T | · p q,u (1 − p q,u ) + 2 b T |D T | 2 = p q,u (1 − p q,u ) |D T | + 2 b T |D T | 2 .
Equality 4 comes from the independence between Y and all X j .
Equality 5 relies on an assumption of independence between X j , X k for all j = k (i.e., the iid assumption discussed prior to the theorem statements in Section 3.3).
To actually compute this variance, we need to use the data in place of the unknown p q,u .
Usingˆp Usingˆ Usingˆp O,q,u directly in place of p q,u requires a |D T | |D T |−1 factor correction (known as "Bessel's correction 5 ") to generate an unbiased estimate.
Thus, the variance of each opt-in record probability estimate is: ˆ Note that in line 15 of EstimateOptinProbabilities, the use of this sample variance expression in re-computingˆσcomputingˆ computingˆσ 2 O,,, is not statistically valid, so our computation ofˆpofˆ ofˆp O,,, andˆpandˆ andˆp ,, is sub-optimal.
Despite that, our overall utility, which does not include , is good (see Section 4).
Proof.
From Section 3.2 on denoising, the distribution of the reported query q from the client mechanism is given by r C,q = t · p q + 1−t k−1 (1 − p q ), and so the true probability of query q is distributed as p q = , wherê r C,q is the empirical estimator of r C,q defined explicitly asˆrasˆ asˆr C,q = 1 |D C | |D C | j=1 X j , where X j ∼ Bernoulli(r C,q ) is the random variable indicating whether report j was query q and |D C | is the total number of records from the client users.The variance ofˆrofˆ ofˆr C,q isV [ˆ r C,q ] = V 1 |D C | |D C | j=1 X j = 1 |D C | 2 |D C | j=1 V [X j ] (6) = 1 |D C | 2 |D C | · r C,q (1 − r C,q ) = r C,q (1 − r C,q ) |D C | ,where equality 6 relies on an assumption of independence between X j , X k for all j = k (i.e., the iid assumption discussed prior to the theorem statements in Section 3.3).
Then, the variance ofˆpofˆ ofˆp C,q is To actually compute this variance, we need to use the data in place of the unknown r C,q .
Usingˆringˆ ingˆr C,q directly in place of r C,q requires including Bessel's |D C | |D C |−1 factor correction to yield an unbiased estimate.
Thus, the variance of the query probability estimates by the client algorithm is: ˆ σ 2 C,q = Proof.
With the variance estimates for each algorithm fully computed, a blended estimate of p q,u is given byˆpbyˆ byˆp q,u = w q,u · ˆ p O,q,u +(1−w q,u )· ˆ p C,q,u , which has sample variancê σ 2 q,u = w 2 q,u · ˆ σ 2 O,q,u + (1 − w q,u ) 2 · ˆ σ 2 C,q,u .
MinimizingˆσMinimizingˆ Minimizingˆσ 2 q,u with respect to w q,u yields the desired.
