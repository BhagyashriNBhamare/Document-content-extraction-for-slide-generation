Machine learning is being increasingly used by individuals , research institutions, and corporations.
This has resulted in the surge of Machine Learning-as-a-Service (MLaaS)-cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model.
However, such MLaaS systems raise concerns such as model extraction.
In model extraction attacks, adversaries maliciously exploit the query interface to steal the model.
More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface.
This attack was introduced by Tramèr et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown.
We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems.
To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning.
In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.
Advancements in various facets of machine learning has made it an integral part of our daily life.
However, most real-world machine learning tasks are resource intensive.
To that end, several cloud providers, such as Amazon, Google, Microsoft, and BigML offset the storage and computational requirements by providing Machine Learning-as-a-Service (MLaaS).
A MLaaS server offers support for both the training phase, and a query interface for accessing the trained model.
The trained model is then queried by other users on chosen instances (refer Fig. 1).
Often, this is implemented in a pay-per-query regime i.e. the server, or the model owner via the server, charges the the users for the queries to the model.
Pricing for popular MLaaS APIs is given in Table 1.
Current research is focused at improving the performance of training algorithms, while little emphasis is placed on the related security aspects.
For example, in many real-world applications, the trained models are privacy-sensitive -a model can (a) leak sensitive information about training data [5] during/after training, and (b) can itself have commercial value or can be used in security applications that assume its secrecy (e.g., spam filters, fraud detection etc. [29,38,53]).
To keep the models private, there has been a surge in the practice of oracle access, or black-box access.
Here, the trained model is made available for prediction but is kept secret.
MLaaS systems use oracle access to balance the trade-off between privacy and usability.
In this section, we give a brief overview of machine learning, and terminology we use throughout the paper.
In particular, we summarize the passive learning framework in § 2.1, and focus on active learning algorithms in § 2.2.
A review of the state-of-the-art of active learning algorithms is needed to explicitly link model extraction to active learning and is presented in § 3.
In the standard, passive machine learning setting, the learner has access to a large labeled dataset and uses it in its entirety to learn a predictive model from a given class.
Let X be an instance space, and Y be a set of labels.
For example, in object recognition, X can be the space of all images, and Y can be a set of objects that we wish to detect in these images.
We refer to a pair (x, y) ∈ X × Y as a data-point or labeled instance (x is the instance, y is the label).
Finally, there is a class of functions F from X to Y called the hypothesis space that is known in advance.
The learner's goal is to find a functionˆffunctionˆ functionˆf ∈ F that is a good predictor for the label y given the instance x, with (x, y) ∈ X × Y. To measure how welî f predicts the labels, a loss function ℓ is used.
Given a data-point z = (x, y) ∈ X × Y, ℓ( ˆ f , z) measures the difference betweenˆf betweenˆ betweenˆf (x) and the true label y.
When the label domain Y is finite (classification problem), the 0-1 loss function is frequently used:ℓ( ˆ f , z) = 0, ifˆfifˆ ifˆf (x) = y 1, otherwiseIf the label domain Y is continuous, one can use the square loss: ℓ( ˆ f , z) = ( ˆ f (x) − y) 2 .
In the passive setting, the PAC (probably approximately correct) learning [56] framework is predominantly used.
Here, we assume that there is an underlying distribution D on X × Y that describes the data; the learner has no direct knowledge of D but has access to a set of training data D drawn from it.The main goal in passive PAC learning is to use the labeled instances from D to produce a hypothesisˆfhypothesisˆ hypothesisˆf such that its expected loss with respect to the probability distribution D is low.
This is often measured through the generalization error of the hypothesisˆfhypothesisˆ hypothesisˆf , defined byErr D ( ˆ f ) = E z∼D [ℓ( ˆ f , z)](1)More precisely, we have the following definition.Definition 1 (PAC passive learning [56]).
An algorithm A is a PAC passive learning algorithm for the hypothesis class F if the following holds for any D on X × Y and any ε, δ ∈ (0, 1): If A is given s A (ε, δ) i.i.d. data-points generated by D, then A outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ min f ∈F Err D ( f ) + ε with probability at least 1 − δ.
We refer to s A (ε, δ) as the sample complexity of algorithm A.Remark 1 (Realizability assumption).
In the general case, the labels are given together with the instances, and the factor min f ∈F Err D ( f ) depends on the hypothesis class.
Machine learning literature refers to this as agnostic learning or the non-separable case of PAC learning.
However, in some applications, the labels themselves can be described using a labeling function f * ∈ F .
In this case (known as realizable learning), min f ∈F Err D ( f ) = 0 and the distribution D can be described by its marginal over X.
A PAC passive learning algorithm A in the realizable case takes s A (ε, δ) i.i.d. instances generated by D and the corresponding labels generated using f * , and outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ ε with probability at least 1 − δ.
In the passive setting, learning an accurate model (i.e. learningˆf learningˆ learningˆf with low generalization error) requires a large number of data-points.
Thus, the labeling effort required to produce an accurate predictive model may be prohibitive.
In other words, the sample complexity of many learning algorithms grows rapidly as ε → 0 (refer Example 1).
This has spurred interest in learning algorithms that can operate on a smaller set of labeled instances, leading to the emergence of active learning (AL).
In active learning, the learning algorithm is allowed to select a subset of unlabeled instances, query their corresponding labels from an annotator (i.e. oracle) and then use it to construct or update a model.
How the algorithm chooses the instances varies widely.
However, the common underlying idea is that by actively choosing the data-points used for training, the learning algorithm can drastically reduce sample complexity.Formally, an active learning algorithm is an interactive process between two parties -the oracle O and the learner L.
The only interaction allowed is through queries -L chooses x ∈ X and sends it to O, who responds with y ∈ Y (i.e., the oracle returns the label for the chosen unlabeled instance).
This value of (x, y) is then used by L to infer some information about the labeling procedure, and to choose the next instance to query.Over many such interactions, L outputsˆfoutputsˆ outputsˆf as a predictor for labels.
We can use the generalization error (1) to evaluate the accuracy of the outputˆfoutputˆ outputˆf .
However, depending on the query strategy chosen by L, other types of error can be used.There are two distinct scenarios for active learning: PAC active learning and Query Synthesis (QS) active learning.
In literature, QS active learning is also known as Membership Query Learning, and we will use the two terms synonymously.
This scenario was introduced by Dasgupta in 2005 [20] in the realizable context and then subsequently developed in following works (e.g., [4,19,26]).
In this scenario, the instances are sampled according to the marginal of D over X, and the learner, after seeing them, decides whether to query for their labels or not.
Since the data-points seen by L come from the actual underlying distribution D, the accuracy of the output hypothesisˆfhypothesisˆ hypothesisˆf is measured using the generalization error (1), as in the classic (i.e., passive) PAC learning.There are two options to consider for sampling data-points.
In stream-based sampling (also called selective sampling) , the instances are sampled one at a time, and the learner decides whether to query for the label or not on a per-instance basis.
Pool-based sampling assumes that all of the instances are collected in a static pool S ⊆ X and then the learner chooses specific instances in S and queries for their labels.
Typically, instances are chosen by L in a greedy fashion using a metric to evaluate all instances in the pool.
This is not possible in stream-based sampling, where L goes through the data sequentially, and has to therefore make decisions to query individually.
Pool-based sampling is extensively studied since it has applications in many real-world problems, such as text classification, information extraction, image classification and retrieval, etc. [39].
Stream-based sampling represents scenarios where obtaining unlabeled data-points is easy and cheap, but obtaining their labels is expensive (e.g., stream of data is collected by a sensor, but the labeling requires an expert).
Before describing query synthesis active learning, we wish to highlight the advantage of PAC active learning over passive PAC learning (i.e. the reduced sample complexity) for some hypothesis class through Example 1.
Recall that this advantage comes from the fact that an active learner is allowed to adaptively choose the data from which it learns, while a passive learning algorithm learns from a static set of data-points.
Example 1 (PAC learning for halfspaces).
Let F d,HS be the hypothesis class of d-dimensional halfspaces 1 , used for binary classification.
A function in f w ∈ F d,HS is described by a normal vector w ∈ R d (i.e., ||w|| 2 = 1) and is defined byf w (x) = sign(w, x) for any x ∈ R dwhere given two vectors a, b ∈ R d , then their product is defined as a, b = ∑ d i=1 a i b i .
Moreover, if x ∈ R, then sign(x) = 1 if x ≥ 0 and sign(x) = −1 otherwise.
A classic result in passive PAC learning states that O( d ε log( 1 ε ) + 1 ε log( 1 δ )) datapoints are needed to learn f w [56].
On the other hand, several works propose active learning algorithms for F d,HS with sample complexity 2 ˜ O(d log( 1 ε )) (under certain distributional assumptions).
For example, if the underlying distribution is log-concave, there exists an active learning algorithm with sample complexity˜Ocomplexity˜ complexity˜O(d log( 1 ε )) [9,10,63].
This general reduction in the sample complexity for F d,HS is easy to infer when d = 1.
In this case, the data-points lie on the real line and their labels are a sequence of −1's followed by a sequence of +1's.
The goal is to discover a point w where the change from −1 to +1 happens.
PAC learning theory states that this can be achieved with˜Owith˜ with˜O ( f w (x) = −1 if w, x < −1 +1 otherwise R −1 −1 −1 +1 +1 +1 +1 +1w * Figure 2: Halfspace classification in dimension 1.
the other hand, an active learning algorithm that uses a simple binary search can achieve the same task with O(log( 1 ε )) queries [20] (refer Figure 2).
In this scenario, the learner can request labels for any instance in the input space X, including points that the learner generates de novo, independent of the distribution D (e.g., Lcan ask for labels for those x that have zero-probability of being sampled according to D).
Query synthesis is reasonable for many problems, but labeling such arbitrary instances can be difficult if the oracle is a human annotator.
Thus, this scenario better represents real-world applications where the oracle is automated (e.g., results from synthetic experiments [32]).
Since the data-points are independent of the distribution, generalization error is not an appropriate measure of accuracy of the hypothesisˆfhypothesisˆ hypothesisˆf , and other types of error are typically used.
These new error formulations depend on the concrete hypothesis class F considered.
For example, if F is the class of boolean functions from {0, 1} n to {0, 1}, then the uniform error is used.
Assume that the oracle O knows f * ∈ F and uses it as labeling function (realizable case), then the uniform error of the hypothesisˆfhypothesisˆ hypothesisˆf is defined asErr u ( ˆ f ) = Pr x∼{0,1} n [ ˆ f (x) 񮽙 = f * (x)]where x is sampled uniformly at random from the instance space {0, 1} n .
Recent work [3,16], for the class of halfspaces Err 2 ( f w ) = ||w * − w|| 2where || · || 2 is the 2-norm.
In both active learning scenarios (PAC and QS), the learner needs to evaluate the usefulness of an unlabeled instance x, which can either be generated de novo or sampled from the given distribution, in order to decide whether to query the oracle for the corresponding label.
In the state of the art, we can find many ways of formulating such query strategies.
Most of existing literature presents strategies where efficient search through the hypothesis space is the goal (refer the survey by Settles [50]).
Another point of consideration for an active learner L is to decide when to stop.
This is essential as active learning is geared at improving accuracy while being sensitive to new data acquisition cost (i.e., reducing the query complexity).
While one school of thought relies on the stopping criteria based on the intrinsic measure of stability or self-confidence within the learner, another believes that it is based on economic or other external factors (refer [50, Section 6.7]).
Given this diversity within active learning, we enhance the standard definition of a learning algorithm and propose the definition of an active learning system, which is geared towards model extraction.
Our definition is informed by the MLaaS APIs that we investigated (more details in Table 1).
Definition 2 (Active learning system).
Let F be a hypothesis class with instance space X and label space Y.
An active learning system for F is given by two entities, the learner L and the oracle O, interacting via membership queries: L sends to O an instance x ∈ X; O answers with a label y ∈ Y.
We indicate via the notation O f * the realizable case where O uses a specific labeling function f * ∈ F , i.e. y = f * (x).
The behavior of L is described by the following parameters:1.
Scenario: this is the rule that describes the generation of the input for the querying process (i.e. which instances x ∈ X can be queried).
In the PAC scenario, the instances are sampled from the underlying distribution D.
In the query synthesis (QS) scenario, the instances are generated by the learner L;2.
Query strategy: given a specific scenario, the query strategy is the algorithm that adaptively decides if the label for a given instance x i is queried for, given that the queries x 1 , . . . , x i−1 have been answered already.
In the query synthesis scenario, the query strategy also describes the procedure for instance generation.3.
Stopping criteria: this is a set of considerations used by L to decide when it must stop querying.Any system (L, O) described as above is an active learning system for F if one of the following holds:-(PAC scenario) For any D on X × Y and any ε, δ ∈ (0, 1), if L is allowed to interact with O using q L (ε, δ) queries, then L outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ min f ∈F Err D ( f ) + ε with probability at least 1 − δ.
-(QS scenario) Fix an error measure Err for the functionsin F .
For any f * ∈ F , if L is allowed to interact with O f * using q L (ε, δ) queries, then L outputsˆfoutputsˆ outputsˆf ∈ F such that Err( ˆ f ) ≤ ε with probability at least 1 − δ.We refer to q L (ε, δ) as the query complexity of L.As we will show in the following section (in particular, refer § 3.2), the query synthesis scenario is more appropriate in casting model extraction attack as active learning when we make no assumptions about the adversary's prior knowledge.Note that, other types queries have been studied in literature.
This includes the equivalence query [4].
Here the learner can verify if a hypothesis is correct or not.
We do not consider equivalence queries in our definition because we did not see any of the MLaaS APIs support them.
In § 3.1, we begin by formalizing the process of model extraction.
We then draw parallels between model extraction and active learning in § 3.2.
We begin by describing the operational ecosystem of model extraction attacks in the context of MLaaS systems.
An entity learns a private model f * from a public class F , and provides it to the MLaaS server.
The server provides a client-facing query interface for accessing the model for prediction.
For example, in the case of logistic regression, the MLaaS server knows a model represented by parameters a 0 , a 1 , · · · , a d .
The client issues queries of the form x = (x[1], · · · , x[d]) ∈ R d ,(x) = a 0 + ∑ d i=1 a i x[i].
Model extraction is the process where an adversary exploits this interface to learn more about the proprietary model f * .
The adversary can be interested in defrauding the description of the model f * itself (i.e., stealing the parameters a i as in a reverse engineering attack), or in obtaining an approximation of the model, sayˆfsayˆ sayˆf ∈ F , that he can then use for free for the same task as the original f * was intended for.
To capture the different goals of an adversary, we say that the attack is successful if the extracted model is "close enough" to f * according to an error function on F that is context dependent.
Since many existing MLaaS providers operate in a pay-per-query regime, we use query complexity as a measure of efficiency of such model extraction attacks.Formally, consider the following experiment: an adversary A, who knows the hypothesis class F , has oracle access to a proprietary model f * from F .
This can be thought of as A interacting with a server S that safely stores f * .
The interaction has several rounds.
In each round, A chooses an instance x and sends it to S.
The latter responds with f * (x).
After a few rounds, A outputs a functionˆffunctionˆ functionˆf that is the adversary's candidate approximation of f * ; the experiment considersˆfconsidersˆ considersˆf a good approximation if its error with respect to the true function f * held by the server is less then a fixed threshold ε.
The error function Err is defined a priori and fixed for the extraction experiment on the hypothesis class F .
Experiment 1 (Extraction experiment).
Given a hypothesis class F = { f : X → Y}, fix an error function Err : F → R.Let S be a MLaaS server with the knowledge of a specific f * ∈ F , denoted by S( f * ).
Let A be an adversary interacting with S with a maximum budget of q queries.
The extraction experiment Exp ε F (S( f * ), A,q) proceeds as follows 1.
A is given a description of F and oracle access to f * through the query interface of S.
That is, if A sends x ∈ X to S, it gets back y = f * (x).
After at most q queries, A eventually outputsˆfoutputsˆ outputsˆf ;2.
The output of the experiment is 1 if Err( ˆ f ) ≤ ε.
Otherwise the output is 0.
Informally, an adversary A is successful if with high probability the output of the extraction experiment is 1 for a small value of ε and a fixed query budget q.
This means that A likely learns a good approximation of f * by only asking q queries to the server.
More precisely, we have the following definition.Definition 3 (Extraction attack).
Let F be a public hypothesis class and S an MLaaS server as explained before.
We say that an adversary A, which interacts with S, implements an ε-extraction attack of complexity q and confidence γ againstthe class F if Pr[Exp ε F (S( f * ), A,q) = 1] ≥ γ, for any f * ∈ F .
The probability is over the randomness of A.In other words, in Definition 3 the success probability of an adversary constrained by a fixed budget for queries is explicitly lower bounded by the quantity γ.Before discussing the connection between model extraction and active learning, we provide an example of a hypothesis class that is easy to extract.Example 2 (Equation-solving attack for linear regression).
Let F d,R be the hypothesis class of regression models from R d to R.
A function f a in this class is described by d + 1 parameters a 0 , a 1 , . . . , a d from R and defined by: for anyx ∈ R d , f a (x) = a 0 + ∑ d i=1 a i x i .
Consider the adversary A ES that queries x 1 , . . . , x d+1 (d + 1 instances from R d ) chosenin such a way that the set of vectors {(1, x i )} i=1,...,d+1 is linearly independent in R d+1 .
A ES receives the corresponding d + 1 labels, y 1 , . . . , y d+1 , and can therefore solve the linear system given by the equations f a (x i ) = y i .
Assume that f a * is the function known by the MLaaS server (i.e., While our model operates in the black-box setting, we discuss other attack models in more detail in Remark 2y i = f a * (x i )).
It is easy to see that if we fix Err( f a ) = ||a * − a|| 1 , then Pr[Exp 0 F d,R (S( f a * ), A ES , d + 1) = 1] = 1.
That is, A ES imple- From the description presented in the § 2, it is clear that model extraction in the MLaaS system context closely resembles active learning.
The survey of active learning in § 2.2 contains a variety of algorithms and scenarios which can be used to implement model extraction attacks (or to study its impossibility).
However, different scenarios of active learning impose different assumptions on the adversary's prior knowledge.
Here, we focus on the general case of model extraction with an adversary A that has no knowledge of the data distribution D.In particular, such an adversary is not restricted to only considering instances x ∼ D to query.
For this reason, we believe that query synthesis (QS) is the right active learning scenario to investigate in order to draw a meaningful parallelism with model extraction.
Recall that the query synthesis is the only framework where the query inputs can be generated de novo (i.e., they do not conform to a distribution).
Observation 1: Given a hypothesis class F and an error function Err, let (L, O) be an active learning system for F in the QS scenario (Definition 2).
If the query complexity of L is q L (ε, δ), then there exists an adversary A that implements ε-extraction with complexity q L (ε, δ) and confidence 1 − δ against the class F .
The reasoning for this observation is as follows: consider the adversary A that is the learner L (i.e., A deploys the query strategy procedure and the stopping criteria that describe L).
This is possible because (L, O) is in the QS scenario and L is independent of any underlying (unknown) distribution.
Let q = q L (ε, δ) and observe thatPr[Exp ε F (S( f * ), A,q) = 1] = Pr[A outputsˆfoutputsˆ outputsˆf and Err( ˆ f ) ≤ ε] = Pr[L outputsˆfoutputsˆ outputsˆf and Err( ˆ f ) ≤ ε] ≥ 1 − δOur observation states that any active learning algorithm in the QS scenario can be used to implement a model extraction attack.
Therefore, in order to study the security of a given hypothesis class in the MLaaS framework, we can use known techniques and results from the active learning literature.
Two examples of this follow.Example 3 (Decision tree extraction via QS active learning).
Let F n,BF denote the set of boolean functions with domain {0, 1} n and range {−1, 1}.
The reader can think of −1 as 0 and +1 as 1.
Using the range of {−1, +1} is very common in the literature on learning boolean functions.
An interesting subset of F n,BF is given by the functions that can be represented as a boolean decision tree.
A boolean decision tree T is a labeled binary tree, where each node v of the tree is labeled by L v ⊆ {1, · · · , n} and has two outgoing edges.
Every leaf in this tree is labeled either +1 or −1.
Given an n-bit string x = (b 1 , · · · , b n ), b i ∈ {0, 1} as input, the decision tree defines the following computation: the computation starts at the root of the tree T .
When the computation arrives at an internal node v, we calculate the parity of ∑ i∈L v b i and go left if the parity is 0 and go right otherwise.
The value of the leaf that the computation ends up in is the value of the function.
We denote by F m n,BT the class of boolean decision trees with n-bit input and m nodes.
Kushilevitz and Mansour [35] present an active learning algorithm for the class F n,BF that works in the QS scenario.
This algorithm utilizes the uniform error to determine the stopping condition (refer § 2.2).
The authors claim that this algorithm has practical efficiency when restricted to the classes F m n,BT ⊂ F n,BF for any m.
In particular, if the active learner L of [35] interacts with the oracle O T * where T * ∈ F m n,BT , then L learns g ∈ F n,BF such that Pr x∼{0,1} n [g(x) 񮽙 = T * (x)] ≤ ε with probability at least 1 − δ using a number of queries polynomial in n, m, 1 ε and log( 1 δ ).
Based on Observation 1, this directly translates to the existence of an adversary that implements ε-extraction with complexity polynomial in n, m, 1 ε and confidence 1 − δ against the class F m n,BT .
Moreover, the algorithm [35] can be extended to (a) boolean functions of the form f : {0, 1, . . . , k − 1} n → {−1, +1} that can be computed by a polynomial-size k-ary decision tree 4 , and (b) regression trees (i.e., the output is a real value from [0, M]).
In the second case, the running time of the learning algorithm is polynomial in M (refer § 6 of [35]).
Note that the attack model considered here is a stronger model than that considered by Tramèr et al. [55] because the attacker/learner does not get any information about the internal path of the decision tree (refer Remark 2).
queries, where f w * ∈ F d,HS is the labeling function used by O.It follows from Observation 1 that an adversary utilizing this algorithm implements ε-extraction against the class F d,HS with complexity O(d log( 1 ε )) and confidence 1.
We validate the practical efficacy of this attack in § 6.
Remark 2 (Extraction with auxiliary information).
Observe that we define model extraction for only those MLaaS servers that return only the label value y for a well-formed query x (i.e. in the oracle access setting).
A weaker model considers the case of MLaaS servers responding to a user's query x even when x is incomplete (i.e. with missing features), and returning the label y along with some auxiliary information.
The work of Tramèr et al. [55] proves that model extraction attacks in the presence of such "leaky servers" are feasible and efficient (i.e. low query complexity) for many hypothesis classes (e.g., logistic regression, multilayer perceptron, and decision trees).
In particular, they propose an equation solving attack [55, Section 4.1] that uses the confidence values returned by the MLaaS server together with the labels to steal the model parameters.
For example, in the case of logistic regression, the MLaaS server knows the parameters a 0 , a 1 , . . . , a d and responds to a query x with the label y (y = 0 if (1 + e −a(x) ) ≤ 0.5 and y = 1 otherwise) and the value a(x) as confidence value for y. Clearly, the knowledge of the con-fidence values allows an adversary to implement the same attack we describe in Example 2 for linear regression models.
In [55, §4.2], the authors describes a path-finding attack that use the leaf/node identifier returned by the server, even for incomplete queries, to steal a decision tree.
These attacks are very efficient (i.e., d + 1 queries are needed to steal a d-dimensional logistic regression model).
However, their efficiency heavily relies on the presence of the various forms of auxiliary information provided by the MLaaS server.
While the work in [55] performs preliminary exploration of attacks in the black-box setting [17,38], it does not consider more recent and efficient algorithms in the QS scenario.
Our work explores this direction through a formalization of the model extraction framework that enables understanding the possibility of extending/improving the active learning attacks presented in [55].
Furthermore, having a better understanding of model extraction attack and its unavoidable connection with active learning is paramount for designing MLaaS systems that are resilient to model extraction.
This section focuses on model extraction for two important non-linear classifiers: kernel SVMs and discrete models (i.e. decision trees and random forests).
For kernel SVMs our method is a combination of the adaptive-retraining algorithm introduced by Tramèr et al. and the active selection strategy from classic literature on active learning of kernel SVMs [12].
For discrete models our algorithm is based on the importance weighted active learning (IWAL) as described in [11].
Note that decision trees for general labels (i.e. non-binary case) and random forests was not discussed in [11].
In kernel SVMs (kSVMs), there is a kernel K : X × X → R associated with the SVM.
Some of the common kernels are polynomials and radial-basis functions (RBFs).
If the kernel function K(.
, .)
has some special properties (required by classic theorem of Mercer [40]), then K(.
, .)
can be replaced with Φ(.)
T Φ(.)
for a projection/feature function Φ.
In the feature space (the domain of Φ) the optimization problem is as follows 5 :min w,b 񮽙w񮽙 2 +C ∑ n i=1 η i such that for 1 ≤ i ≤ n y i ˆ y(x i ) ≥ 1 − η i η i ≥ 0In the formulation given above, ˆ y(x) is equal to w T Φ(x) + b. Recall that prediction of the kSVM is the sign ofˆyofˆ ofˆy(x), sô y(x) is the "pre sign" value of the prediction.
Note that for some kernels (e.g. RBF) Φ is infinite dimensional, so one generally uses the "kernel trick"i.e. one solves the dual of the above problem and obtains a kernel expansion, so thatˆ y(x) = n ∑ i=1 α i K(x, x i ) + bThe vectors x 1 , · · · , x n are called support vectors.
We assume that hyper-parameters of the kernel (C, η) are known; one can extract the hyper-parameters for the RBF kernel using the extract-and-test approach as Tramèr et al.
Note that if Φ is finite dimensional, we can use an algorithm (including active learning strategies) for linear classifier by simply working in the feature space (i.e. extracting the domain of Φ(·)).
However, there is a subtle issue here, which was not addressed by Tramèr et al.
We need to make sure that if a query y is made in the feature space, it is "realizable" (i.e. there exists a x such that Φ(x) = y).
Otherwise the learning algorithm is not sound.Next we describe our model-extraction algorithm for kSVMs with kernels whose feature space is infinite dimension (e.g. RBF or Laplace kernels).
Our algorithm is a modification of the adaptive training approach from Tramèr et al.
Our discussion is specialized to kSVMs with RBFs, but our ideas are general and are applicable in other contexts.Extended Adaptive Training (EAT): EAT proceeds in multiple rounds.
In each round we construct h labeled instances.
In the initial stage (t = 0) we draw r instances x 1 , · · · , x r from the uniform distribution, query their labels, and create an initial model M 0 .
Assume that we are at round t, where t > 0, and let M t−1 be model at time t − 1.
Round t works as follows: create h labeled instances using a strategy St T (M t−1 , h) (note that the strategy St is oracle access to the teacher, and takes as parameters model from the previous round and number of labeled instances to be generated).
Now we train M t−1 on the instances generated by St T (M t−1 , h) and obtain the updated model M t .
We keep iterating using the strategy St T (·, ·) until the query budget is satisfied.
Ideally, St T (M t−1 , h) should be instances that the model M t−1 is least confident about or closest to the decision boundary.Tramèr et al. use line search as their strategy St T (M t−1 , h), which can lead to several queries (each step in the binary search leads to a query).
We generate the initial model M 0 as in Tramèr et al. and then our strategy differs.
Our strategy St T (M t−1 , 1) (note that we only add one labeled sample at each iteration) works as follows: we generate k random points x 1 , · · · , x k and then computê y i (x i ) for each x i (recall thatˆythatˆ thatˆy i (x i ) is the "pre sign" prediction of x i on the SVM M t−1 .
We then pick x i with minimum | ˆ y i (x i ) | and query for its label and retrain the model M t−1 and obtain M t .
This strategy is called active selection and has been used for active learning of SVMs [12].
The argument for why this strategy finds the point closest to the boundary is given in [12, §4].
There are other strategies described in [12], but we found active selection to perform the best.
Next we will describe the idea of importance weighted active learning (IWAL) [11].
Our discussion will be specialized to decision trees and random forests, but the ideas that are described are general.Let H be the hypothesis class (i.e. space of decision trees or random forests), X is the space of data, and Y is the space of labels.
The active learner has a pool of unlabeled data x 1 , x 2 , · · · .
For i > 1, we denote by X 1:i−1 the sequence x 1 , · · · , x i−1 .
After having processed the sequence X 1:i−1 , a coin is flipped with probability p i ∈ [0, 1] and if it comes up heads, the label of x i is queried.
We also define a set S i (S 0 = / 0) recursively as follows: If the label for x i is not queried, then S i = S i−1 ; otherwise S i = S i−1 ∪ (x i , y i , p i ).
Essentially the set S i keeps the information (i.e. data, label, and probability of querying) for all the datapoints whose label was queried.
Given a hypothesis h ∈ H , we define err(h, S n ) as follows:err(h, S n ) = 1 n ∑ (x,y,p)∈S n 1 p 1 h(x)񮽙 =y(2)Next we define the following quantities (we assume n ≥ 1):h n = argmin{err(h, S n−1 ) : h ∈ H } h ′ n = argmin{err(h, S n−1 ) : h ∈ H ∧ h(X n ) 񮽙 = h n (X n )} G n = err(h ′ n , S n−1 ) − err(h n , S n−1 )Recall that p n is the probability of querying for the label for X n , which is defined as follows:p n = 񮽙 1 if G n ≤ µ(n) s(n) otherwiseWhere µ(n) = 񮽙 c 0 log n n−1 + c 0 log n n−1 , and s(n) ∈ (0, 1) is the positive solution to the following equation:G n = c 1 √ s − c 1 + 1 · 񮽙 c 0 log n n − 1 + c 2 √ s − c 2 + 1 · c 0 log n n − 1Note the dependence on constants/hyperparameters c 0 , c 1 and c 2 , which are tuned for a specific problem (e.g. in their experiments for decision trees [11, §6] the authors set c 0 = 8 and c 1 = c 2 = 1).
Decision Trees: Let DT be any algorithm to create a decision tree.
We start with an initial tree h 0 (this can constructed using a small, uniformly sampled dataset whose labels are queried).
Let h n be the tree at step n − 1.
The question is: how to construct h ′ n ?
Let x n be the n th datapoint and Y = {l 1 , · · · , l r } be the set of labels.
Let h n (x n ) = l j .
Let h n (l) be the modification of tree h n such that h n (l) produces label l 񮽙 = h n (x n ) on datapoint x n .
Let h ′ n be the tree in the set {h n (l) | l ∈ Y − {l j }} that has minimum err(·, S n−1 ).
Now we can compute G n and the algorithm can proceed as described before.
Random Forests: In this case we will restrict ourselves to binary classification, but the algorithm can readily extended to the case of multiple labels.
As before RF 0 is the random forest trained on a small initial dataset.
Since we are in the binary classification domain, the label set Y = {1, −1}.
Assume that we have a random forest RF = {RF [1] Let RF n be the random forest at time step n − 1.
The question again is: how to construct RF ′ n ?
Without loss of generality, let us say on x n RF n (x n ) = +1 (the case when the label is −1 is symmetric) and there are r trees in RF n (denoted by RF +1 n (x n )) such that their labels on x n are +1.
Note that r > ⌊ o 2 ⌋ because the majority label was +1.
Define j = r −⌊ o 2 ⌋+1.
Note that if j trees in RF +1n (x n ) will "flip" their decision to −1 on x n , then the decision on x n will be flipped to −1.
This is the intuition we use to compute RF ′ n .
There are r j choices of trees and we pick the one with minimum error on S n−1 , and that gives us RF ′ n .
Recall that r j is approximately r j , but we can be approximate by randomly picking j trees out of RF +1 n (x n ), and choosing the random draw with the minimum error to approximate RF ′ n .
Our main observation is that model extraction in the context of MLaaS systems described at the beginning of § 3 (i.e., oracle access) is equivalent to QS active learning.
Therefore, any advancement in the area of QS active learning directly translates to a new threat for MLaaS systems.
In this section, we discuss strategies that could be used to make the process of extraction more difficult.We investigate the link between ML in the noisy setting and model extraction.
The design of a good defense strategy is an open problem; we believe this is an interesting direction for future work where the ML and security communities can fruitfully collaborate.In this section, we assume that the MLaaS server S with the knowledge of f * , S( f * ), has the freedom to modify the prediction before forwarding it to the client.
More precisely, we assume that there exists a (possibly) randomized procedure D that the server uses to compute the answer˜yanswer˜ answer˜y to a query x, and returns that instead of f * (x).
We use the notation S D ( f * ) to indicate that the server S implements D to protect f * .
Clearly, the learner that interacts with S D ( f * ) can still try to learn a function f from the noisy answers from the server.
However, the added noise requires the learner to make more queries, or could produce a less accurate model than f .
We focus on the binary classification problem where F is an hypothesis class of functions of the form f : X → Y and Y is binary, but our argument can be easily generalized to the multi-class setting.First, in the following two remarks we recall two known results from the literature [27] that establish information theoretic bounds for the number of queries required to extract the model when any defense is implemented.
Let ν be the generalization error of the model f * known by the server S D and µ be the generalization error of the model f learned by an adversary interacting with S D ( f * ).
Assume that the hypothesis class F has VC dimension equal to d. Recall that the VC dimension of a hypothesis class F is the largest number d such that there exists a subset X ⊂ X of size d which can be shattered by F .
A set X = {x 1 , . . . ,x d } ⊂ X is said to be shattered by F if |{( f (x 1 ), f (x 2 ), . . . , f (x d )) : f ∈ F }| = 2 d .
Remark 3 (Passive learning).
Assume that the adversary uses a passive learning algorithm to compute f , such as the Empirical Risk Minimization (ERM) algorithm, where given a labeled training set {(X 1 ,Y 1 ), . . . (X n ,Y n )}, the ERM algo- rithm outputsˆfoutputsˆ outputsˆf = arg min f ∈F 1 n ∑ n i=1 1[ f (X i ) 񮽙 = Y i ].
Then, the adversary can learnˆflearnˆ learnˆf with excess error ε (i.e., µ ≤ ν + ε) with˜O with˜ with˜O( ν+ε ε 2 d) examples.
For any algorithm, there is a distribution such that the algorithm needs at least˜Ωleast˜ least˜Ω( ν+ε ε 2 d) samples to achieve an excess error of ε.Remark 4 (Active learning).
Assume that the adversary uses an active learning algorithm to compute f , such as the disagreement-based active learning algorithm [27].
Then, the adversary achieves excess error ε with˜Owith˜ with˜O( ν 2 ε 2 dθ) queries (where θ is the disagreement coefficient [27]).
For any active learning algorithm, there is a distribution such that it takes at least˜Ωleast˜ least˜Ω( ν 2 ε 2 d) queries to achieve an excess error of ε.
Observe that any defense strategy D used by a server S to prevent the extraction of a model f * can be seen as a randomized procedure that outputs˜youtputs˜ outputs˜y instead of f * (x) with a given probability over the random coins of D.
In the discrete case, we represent this with the notationρ D ( f * , x) = Pr[Y x 񮽙 = f * (x)],(3)where Y x is the random variable that represents the answer of the server S D ( f * ) to the query x (e.g., ˜ y ← Y x ).
When the function f * is fixed, we can consider the supremum of the function ρ D ( f * , x), which represents the upper bound for the probability that an answer from S D ( f * ) is wrong:ρ D ( f * ) = sup x∈X ρ D ( f * , x).
Before discussing potential defense approaches, we first present a general negative result.
The following proposition states that that any candidate defense D that correctly responds to a query with probability greater than or equal to 1 2 + c for some constant c > 0 for all instances can be easily broken.
Indeed, an adversary that repetitively queries the same instance x can figure out the correct label f * (x) by simply looking at the most frequent label that is returned from S D ( f * ).
We prove that with this extraction strategy, the number of queries required increases by only a logarithmic multiplicative factor.
Proposition 1.
Let F be an hypothesis class used for classification and (L, O) be an active learning system for F in the QS scenario with query complexity q(ε, δ).
For any D, randomized procedure for returning labels, such that there exists f * ∈ F with ρ D ( f * ) < 1 2 , there exists an adversary that, interacting with S D ( f * ), can implement an ε-extraction attack with confidence 1 − 2δ and complexity q =8 (1−2ρ D ( f * )) 2 q(ε, δ) ln q(ε,δ) δ .
The proof of Proposition 1 can be found in the appendix in [1].
Proposition 1 can be used to discuss the following two different defense strategies:1.
Data-independent randomization.
Let F denote a hypothesis class that is subject to an extraction attack using QS active learning.
An intuitive defense for F involves adding noise to the query output f * (x) independent of the labeling function f * and the input query x.
In other words, ρ D ( f , x) = ρ for any x ∈ X, f ∈ F , and ρ is a constant value in the interval (0, 1).
It is easy to see that this simple strategy cannot work.
It follows from Proposition 1 that if ρ < 1 2 , then D is not secure.
On the other hand, if ρ ≥ 1 2 , then the server is useless since it outputs an incorrect label with probability at least 1 2 .
Example 5 (Halfspace extraction under noise).
For example, we know that ε-extraction with any level of confidence can be implemented with complexity q = O(d log( 1 ε )) using QS active learning for the class F d,HS i.e. for binary classification via halfspaces (refer Example 4).
It follows from the earlier discussion that any defense that flips labels with a constant flipping probability ρ does not work.
This defense approach is similar to the case of "noisy oracles" studied extensively in the active learning literature [30,31,45].
For example, from the ML literature we know that if the flipping probability is exactly ρ (ρ ≤ 1 2 ), the AVERAGE algorithm (similar to our Algorithm 1, defined in Section 6) ε-extracts f * with˜O with˜ with˜O( d 2 (1−2ρ) 2 log 1 ε ) labels [33].
Under bounded noise where each label is flipped with probability at most ρ (ρ < 1 2 ), the AV-ERAGE algorithm does not work anymore, but a modified Perceptron algorithm can learn with˜Owith˜ with˜O( d (1−2ρ) 2 log 1 ε ) labels [61] in a stream-based active learning setting, and a QS active learning algorithm proposed by Chen et al. [16] can also learn with the same number of labels.
An adversary implementing the Chen et al. algorithm [16] is even more efficient than the adversary˜Aadversary˜ adversary˜A defined in the proof of Proposition 1 (i.e., the total number of queries only increases by a constant multiplicative factor instead of ln q(ε, δ)).
We validate the practical efficiency of this attack in § 6.2.
Data-dependent randomization.
Based on the outcome of the earlier discussion, we believe that a defense that aims to protect a hypothesis class against model extraction via QS active learning should implement data-dependent perturbation of the returned labels.
That is, we are interested in a defense D such that the probability ρ D ( f * , x) depends on the query input x and the labeling function f * .
For example, given a class F that can be extracted using an active learner L (in the QS scenario), if we consider a defense D such that ρ D ( f * , x) ≥ 1 2 for some instances, then the proof of Proposition 1 does not work (the argument only works if there is a constant c > 0 such that ρ D ( f * , x) ≤ 1 2 − c for all x) and the effectiveness of the adversary˜Aadversary˜ adversary˜A is not guaranteed anymore 6 .
Example 6 (Halfspace extraction under noise).
For the case of binary classification via halfspaces, Alabdulmohsin et al.[2] design a system that follows this strategy.
They consider the class F d,HS and design a learning rule that uses training data to infer a distribution of models, as opposed to learning a single model.
To elaborate, the algorithm learns the mean µ and the covariance Σ for a multivariate Gaussian distribution N (µ, Σ) on F d,HS such that any model drawn from N (µ, Σ) provides an accurate prediction.
The problem of learning such a distribution of classifiers is formulated as a convex-optimization problem, which can be solved quite efficiently using existing solvers.
During prediction, when the label for a instance x is queried, a new w is drawn at random from the learned distribution N (µ, Σ) and the label is computed as y = sign(w, x).
The authors show that this randomization method can mitigate the risk of reverse engineering without incurring any notable loss in predictive accuracy.
In particular, they use PAC active learning algorithms [9,17] (assuming that the underlying distribution D is Gaussian) to learn an approximationˆwapproximationˆ approximationˆw from queries answered in three different ways: (a) with their strategy, i.e. using a new model for each query, (b) using a fixed model to compute all labels, and (c) using a fixed model and adding independent noise to each label, i.e. y = sign(w, x + η) and η ← [−1, +1].
They show that the geometric error ofˆwofˆ ofˆw with respect to the true model is higher in the former setting (i.e. in (a)) than in the others.
On 15 different datasets, their strategy gives typically an order of magnitude larger error.
We empirically evaluate this defense in the context of model extraction using QS active learning algorithms in § 6.
Continuous case: Generalizing Proposition 1 to the continuous case does not seem straightforward, i.e. when the target model held by the MLaaS server is a real-valued function f * : X → R; a detailed discussion about the continuous case appears in the appendix in [1].
For all experiments described below, we use an Ubuntu 16.04 server with 32 GB RAM, and an Intel i5-6600 CPU clocking 3.30GHz.
We use a combination of datasets obtained from the scikit-learn library and the UCI machine learning repository 7 , as used by Tramèr et al..
6 Intuitively, in the binary case if ρ D ( f * , x i ) ≥ 1 2 then the definition of y i performed by˜Aby˜ by˜A in step 2 (majority vote) is likely to be wrong.
However, notice that this is not always the case in the multiclass setting: For example, consider the case when the answer to query x i is defined to be wrong with probability ≥ 1 2 and, when wrong, is sampled uniformly at random among the k − 1 classes that are different to the true class f * (x), then if k is large enough, y i defined via the majority vote is likely to be still correct.
that we train locally 8 , eliminating redundant queries to the oracle.
To compare the efficiency of our algorithm, we reexecute the adaptive retraining procedure, and present our results in Table 3.
It is clear that our approach is more query efficient in comparison to Tramèr et al. (between 5×-224×), with comparable test accuracy.
These advantages stem from (a) using a more informative metric of uncertainty than the distance from the decision boundary, and (b) querying labels of only those points which the local model is uncertain about.
Q2.
Decision Trees: Tramèr et al. propose a path finding algorithm to determine the structure of the server-hosted decision tree.
They rely on the server's response to incomplete queries, and the addition of node identifiers to the generated outputs to recreate the tree.
From our analysis presented in Table 1such flexibility is not readily available in most MLaaS providers.
As discussed earlier (refer § 4.2), we utilize the IWAL algorithm proposed by Beygelzimer et al. [11] that iteratively refines a learned hypothesis.
It is important to note that the IWAL algorithm is more general, and does not rely on the information needed by the path finding algorithm.
We present the results of extraction using the IWAL algorithm below in Table 4.
In each iteration, the algorithm learns a new hypothesis, but the efficiency of the approach relies on the hypothesis used preceding the first iteration.
To this end, we generate inputs uniformly at random.
Note that in such a uniform query generation scenario, we rely on zero auxiliary information.
We can see that while the number of queries required to launch such extraction attacks is greater than in the approach proposed 8 such a local model is seeded with uniformly random points labeled by the oracle by Tramèr et al., such an approach obtains comparable test error to the oracle.
While the authors rely on certain distributional assumptions to prove a label complexity result, we empirically observe success using the uniform strategy.
Such an approach is truly powerful; it makes limited assumptions about the MLaaS provider and any prior knowledge.
We begin our discussion by highlighting algorithms an adversary could use if the assumptions made about the operational ecosystem are relaxed.
Then, we discuss strategies that can potentially be used to make the process of extraction more difficult, and shortcomings in our approach.
The operational ecosystem in this work is one where the adversary is able to synthesize data-points de novo to extract a model through oracle access.
In this section, we discuss other algorithms an adversary could use if this assumption is relaxed.
We begin by discussing other models an adversary can learn in the query synthesis regime, and move on to discussing algorithms in other approaches.Equivalence queries.
In her seminal work, Angluin [4] proposes a learning algorithm, L * , to correctly learn a regular set from any minimally adequate teacher, in polynomial time.
For this to work, however, equivalence queries are also needed along with membership queries.
Should MLaaS servers provide responses to such equivalence queries, different extraction attacks could be devised.
To learn linear decision boundaries, Wang et al. [59] first synthesize an instance close to the decision boundary using labeled data, and then select the real instance closest to the synthesized one as a query.
Similarly, Awasthi et al.[7] study learning algorithms that make queries that are close to examples generated from the data distribution.
These attacks require the adversary to have access to some subset of the original training data.
In other domains, program synthesis using input-output example pairs (e.g., [25,58]) also follows a similar principle.If the adversary had access to a subset of the training data, or had prior knowledge of the distribution from which this data was drawn from, it could launch a different set of attacks based on the algorithms discussed below.
Stream-based selective sampling.
Atlas et al. [6] propose selective sampling as a form of directed search (similar to Mitchell [41]) that can greatly increase the ability of a connectionist network (i.e. power system security analysis in their paper) to generalize accurately.
Dagan et al. [18] propose a method for training probabilistic classifiers by choosing those examples from a stream that are more informative.
Linden- baum et al. [36] present a lookahead algorithm for selective sampling of examples for nearest neighbor classifiers.
The algorithm looks for the example with the highest utility, taking its effect on the resulting classifier into account.
Another important application of selective learning was for feature selection [37], an important preprocessing step.
Other applications of stream-based selective sampling include sensor scheduling [34], learning ranking functions for information retrieval [62], and in word sense disambiguation [24].
Pool-based sampling.
Dasgupta [21] surveys active learning in the non-separable case, with a special focus on statistical learning theory.
He claims that in this setting, AL algorithms usually follow one of the following two strategies -(i) Efficient search in the hypothesis spaces (as in the algorithm proposed by Chen et al. [16], or by Cohn et al. [17]), or (ii) Exploiting clusters in the data (as in the algorithm proposed by Dasgupta et al. [22]).
The latter option can be used to learn more complex models, such as decision trees.
As the ideal halving algorithm is difficult to implement in practice, pool-based approximations are used instead such as uncertainty sampling and the query-by-committee (QBC) algorithm (e.g., [14,54]).
Unfortunately, such approximation methods are only guaranteed to work well if the number of unlabeled examples (i.e. pool size) grows exponentially fast with each iteration.
Otherwise, such heuristics become crude approximations and they can perform quite poorly.
PAC active learning strategies have proven effective in learning DNNs.
The work of Sener et al. [49] selects the most representative points from a sample of the training distribution to learn the DNN.
Papernot et al. [46] employ substitute model training -a procedure where a small training subset is strategically augmented and used to train a shadow model that resembles the model being attacked.
Note that the prior approaches rely on some additional information, such as a subset of the training data.Active learning for complex models is challenging.
Active learning algorithms considered in this paper operate in an iterative manner.
Let H be the entire hypothesis class.
At time time t ≥ 0 let the set of possible hypothesis be H t ⊆ H .
Usually an active-learning algorithm issues a query at time t and updates the possible set of hypothesis to H t+1 , which is a subset of H t .
Once the size of H t is "small" the algorithm stops.
Analyzing the effect of a query on possible set of hypothesis is very complicated in the context of complex models, such as DNNs.
We believe this is a very important and interesting direction for future work.
Most work in active learning has assumed that the correct hypothesis space for the task is already known i.e. if the model being learned is for logistic regression, or is a neural network and so on.
In such situations, observe that the labeled data being used is biased, in that it is implicitly tied to the underlying hypothesis.
Thus, it can become problematic if one wishes to re-use the labeled data chosen to learn another, different hypothesis space.
This leads us to model transferability 9 , a 9 A special case of agnostic active learning [8].
less studied form of defense where the oracle responds to any query with the prediction output from an entirely different hypothesis class.
For example, imagine if a learner tries to learn a halfspace, but the teacher performs prediction using a boolean decision tree.
Initial work in this space includes that of Shi et al. [51], where an adversary can steal a linear separator by learning input-output relations using a deep neural network.
However, the performance of query synthesis active learning in such ecosystems is unclear.
We stress that these limitations are not a function of our specific approach, and stem from the theory of active learning.
Specifically: (1) As noted by Dasgupta [20], the label complexity of PAC active learning depends heavily on the specific target hypothesis, and can range from O(log 1 ε ) to Ω( 1 ε ).
Similar results have been obtained by others [28,43].
This suggests that for some hypotheses classes, the query complexity of active learning algorithms is as high as that in the passive setting.
(2) Some query synthesis algorithms assume that there is some labeled data to bootstrap the system.
However, this may not always be true, and randomly generating these labeled points may adversely impact the performance of the algorithm.
(3) For our particular implementation, the algorithms proposed rely on the geometric error between the optimal and learned halfspaces.
Sometimes, there is no direct correlation between this geometric error and the generalization error used to measure the model's goodness.
Machine learning algorithms and systems are optimized for performance.
Little attention is paid to the security and privacy risks of these systems and algorithms.
Our work is motivated by the following attacks against machine learning.1.
Causative Attacks: These attacks are primarily geared at poisoning the training data used for learning, such that the classifier produced performs erroneously during test time.
These include: (a) mislabeling the training data, (b) changing rewards in the case of reinforcement learning, or (c) modifying the sampling mechanism (to add some bias) such that it does not reflect the true underlying distribution in the case of unsupervised learning [48].
The work of Papernot et al. [47] modify input features resulting in misclassification by DNNs.
Once the algorithm has trained successfully, these forms of attacks provide tailored inputs such that the output is erroneous.
These noisy inputs often preserves the semantics of the original inputs, are human imperceptible, or are physically realizable.
The well studied area of adversarial examples is an instantiation of such an attack.
Moreover, evasion attacks can also be even black-box i.e. the attacker need not know the model.
This is because an adversarial example optimized for one model is highly likely to be effective for other models.
This concept, known as transferability, was introduced by Carlini et al. [15].3.
Exploratory Attacks: These forms of attacks are the primary focus of this work, and are geared at learning intrinsics about the algorithm used for training.
These intrinsics can include learning model parameters, hyperparameters, or training data.
Typically, these forms of attacks fall in two categories -model inversion, or model extraction.
In the first class, Fredrikson et al. [23] show that an attacker can learn sensitive information about the dataset used to train a model, given access to sidechannel information about the dataset.
In the second class, the work of Tramér et al. [55] provides attacks to learn parameters of a model hosted on the cloud, through a query interface.
Termed membership inference, Shokri et al. [52] learn the training data used for machine learning by training their own inference models.
Wang et al. [57] propose attacks to learn a model's hyperparameters.
In this paper, we formalize model extraction in the context of Machine-Learning-as-a-Service (MLaaS) servers that return only prediction values (i.e., oracle access setting), and we study its relation with query synthesis active learning (Observation 1).
Thus, we are able to implement efficient attacks to the class of halfspace models used for binary classification ( § 6).
While our experiments focus on the class of halfspace models, we believe that extraction via active learning can be extended to multiclass and non-linear models such as deep neural networks, random forests etc.
We also begin exploring possible defense approaches ( § 5).
To the best of our knowledge, this is the first work to formalize security in the context of MLaaS systems.
We believe this is a fundamental first step in designing more secure MLaaS systems.
Finally, we suggest that data-dependent randomization (e.g., model randomization as in [2]) is the most promising direction to follow in order to design effective defenses.
This material is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, SaTC-Frontiers-1804648, CCF-1652140, CNS-1838733, CNS-1719336, CNS-1647152, CNS-1629833 and ARO grant number W911NF-17-1-0405.
Kamalika Chaudhuri and Songbai Yan thank NSF under 1719133 and 1804829 for research support.
