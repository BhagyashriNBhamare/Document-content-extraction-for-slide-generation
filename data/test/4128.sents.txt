The proliferation of the Internet of Things has increased reliance on voice-controlled devices to perform everyday tasks.
Although these devices rely on accurate speech-recognition for correct functionality, many users experience frequent misinterpretations in normal use.
In this work, we conduct an empirical analysis of interpretation errors made by Amazon Alexa, the speech-recognition engine that powers the Amazon Echo family of devices.
We leverage a dataset of 11,460 speech samples containing English words spoken by American speakers and identify where Alexa misinterprets the audio inputs, how often, and why.
We find that certain misinterpretations appear consistently in repeated trials and are systematic.
Next, we present and validate a new attack, called skill squatting.
In skill squatting, an attacker leverages systematic errors to route a user to malicious application without their knowledge.
In a variant of the attack we call spear skill squatting, we further demonstrate that this attack can be targeted at specific demographic groups.
We conclude with a discussion of the security implications of speech interpretation errors, countermeasures, and future work.
The popularity of commercial Internet-of-Things (IoT) devices has sparked an interest in voice interfaces.
In 2017, more than 30 M smart speakers were sold [10], all of which use voice as their primary control interface [28].
Voice interfaces can be used to perform a wide array of tasks, such as calling a cab [11], initiating a bank transfer [2], or changing the temperature inside a home [8].
In spite of the growing importance of speechrecognition systems, little attention has been paid to their shortcomings.
While the accuracy of these systems is improving [37], many users still experience frequent misinterpretations in everyday use.
Those who speak with accents report especially high error rates [36] and other studies report differences in the accuracy of voice-recognition systems when operated by male or female voices [40,46].
Despite these reports, we are unaware of any independent, public effort to quantify the frequency of speechrecognition errors.In this work, we conduct an empirical analysis of interpretation errors in speech-recognition systems and investigate their security implications.
We focus on Amazon Alexa, the speech-recognition system that powers 70% of the smart speaker market [3], and begin by building a test harness that allows us to utilize Alexa as a black-box transcription service.
As test cases, we use the Nationwide Speech Project (NSP) corpus, a dataset of speech samples curated by linguists to study speech patterns [19].
The NSP corpus provides speech samples of 188 words from 60 speakers located in six distinct "dialect-regions" in the United States.We find that for this dataset of 11,460 utterances, Alexa has an aggregate accuracy rate of 68.9% on single-word queries.
Although 56.4% of the observed errors appear to occur unpredictably (i.e., Alexa makes diverse errors for a distinct input word), 12.7% of them are systematicthey appear consistently in repeated trials across multiple speakers.
As expected, some of these systematic errors (33.3%) are due to words that have the same pronunciation but different spellings (i.e., homophones).
However, other systematic errors (41.7%) can be modeled by differences in their underlying phonetic structure.Given our analysis of misinterpretations in Amazon Alexa, we consider how an adversary could leverage these systematic interpretation errors.
To this end, we introduce a new attack, called skill squatting, that exploits Alexa misinterpretations to surreptitiously cause users to trigger malicious, third-party skills.
Unlike existing work, which focuses on crafting adversarial audio input to inject voice commands [15,39,42,48,49], our attack exploits intrinsic error within the opaque natural language processing layer of speech-recognition systems and requires an adversary to only register a public skill.
We demonstrate Figure 1: Example of an Alexa skill -Alexa skills are applications that can perform useful tasks based on voice input.
For example, the Lyft skill [7] allows users to request a ride by saying "Alexa, ask Lyft for a ride."
this attack in a developer environment and show that we are able to successfully "squat" skills, meaning that Alexa invokes the malicious skill instead of a user-intended target skill at least once for 91.7% of the words that have systematic errors.
We then consider how an adversary may improve this attack.
To this end, we introduce a variant of skill squatting, called spear skill squatting, which exploits systematic errors that uniquely target individuals based on either their dialect-region or their gender.
We demonstrate that such an attack is feasible in 72.7% of cases by dialect-region and 83.3% of cases by gender.Ultimately, we find that an attacker can leverage systematic errors in Amazon Alexa speech-recognition to cause undue harm to users.
We conclude with a discussion of countermeasures to our presented attacks.
We hope our results will inform the security community about the potential security implications of interpretation errors in voice systems and will provide a foundation for future research in the area.
Voice interfaces are rooted in speech-recognition technology, which has been a topic of research since the 1970s [26].
In recent years, voice interfaces have become a general purpose means of interacting with computers, largely due to the proliferation of the Internet of Things.
In many cases, these interfaces entirely supplant traditional controls such as keyboards and touch screens.
Smart speakers, like the Amazon Echo and Google Home, use voice interfaces as their primary input source.
As of January 2018, an estimated 39 M Americans 18 years or older own a smart speaker [10], the most popular belonging to the Amazon Echo family.
Figure 2: User-skill interaction in Alexa -A typical user interaction with an Alexa skill, using an Echo device.
In this example, a user interacts with the Lyft skill to request a ride.
In this work, we focus on Amazon Alexa [14], the speechrecognition engine that powers the Amazon Echo family of devices, as a state-of-the-art commercial voice interface.
In order to add extensibility to the platform, Amazon allows the development of third-party applications, called "skills", that leverage Alexa voice services.
Many companies are actively developing Alexa skills to provide easy access to their services through voice.
For example, users can now request rides through the Lyft skill ( Figure 1) and conduct everyday banking tasks with the American Express skill [4].
Users interact with skills directly through their voice.
Figure 2 illustrates a typical interaction.
The user first invokes the skill by saying the skill name or its associated invocation phrase (x).
The user's request is then routed through Alexa cloud servers (y), which determine where to forward it based on the user input (z).
The invoked skill then replies with the desired output ({), which is finally routed from Alexa back to the user (|).
Up until April of 2017, Alexa required users to enable a skill to their account, in a manner similar to downloading a mobile application onto a personal device.
However, Alexa now offers the ability to interact with skills without enabling them [32].
In this work, we consider how the pronunciation of a word helps explain Alexa misinterpretations.
Word pronunciations are uniquely defined by their underlying phonemes.
Phonemes are a speaker-independent means of describing the units of sound that define the pronunciation of a particular word.
In order to enable text-based analysis of English speech, the Advanced Research Projects Agency (ARPA) developed ARPAbet, a set of phonetic transcription codes that represent phonemes of General American English using distinct sequences of ASCII characters [30].
For example, the phonetic representation of Figure 3: Speech-to-Text Test Harness Architecture -By building an experimental skill (called "Record This"), we are able to use the Amazon Alexa speech recognition system as a black box transcription service.
In this example, the client sends a speech sample of the word "apple" x, Alexa transcribes it for the skill server y, which then returns the transcription as a reply to Alexa z and back to the client {.
the word "pronounce" using the ARPAbet transcription codes is P R AH N AW N S. For the scope of this work, we define the phonetic spelling of a word as its ARPAbet phonetic representation, with each ARPAbet character representing a single phoneme.
There are 39 phonemes in the ARPAbet.
We rely on the CMU Pronunciation Dictionary [22] as our primary source for word to phonemes conversion.
In this section, we detail the architecture of our test harness, provide an overview of the speech corpora used in our analysis, and explain how we use both to investigate Alexa interpretation errors.
Alexa does not directly provide speech transcriptions of audio files.
It does, however, allow third-party skills to receive literal transcriptions of speech as a developer API feature.
In order to use Alexa as a transcription service, we built an Alexa skill (called "Record this") that records the raw transcript of input speech.
We then developed a client that takes audio files as input and sends them through the Alexa cloud to our skill server.
In order to start a session with our Alexa skill server, the client first sends an initialization command that contains the name of our custom skill.
Amazon then routes all future requests for that session directly to our "Record this" skill server.
Second, the client takes a collection of audio files as input, batches them, and sends them to our skill server, generating one query per file.
We limit queries to a maximum of 400 per minute in order to avoid overloading Amazon's production servers.
In addition, if a request is denied or no response is returned, we try up to five times before marking the query as a failure.
Figure 3 illustrates this architecture.
For each audio file sent from the client (x), Alexa sends a request to our skill server containing the understood text transcription (y).
The server then responds with that same transcription (z) through the Alexa service back to the client ({).
The client aggregates the transcriptions in a results file that maps input words to their output words for each audio sample.
In order to study interpretation errors in Alexa, we rely on two externally collected speech corpora.
A full breakdown of these datasets is provided in Table 1.
NSP The Nationwide Speech Project (NSP) is an effort led by Ohio State University to provide structured speech data from a range of speakers across the United States [19].
The NSP corpus provides speech from a total of 60 speakers from six geographical "dialect-regions", as defined by Labov et al. [31].
Figure 4 shows each of these speech regions -Mid-Atlantic, Midland, New England, North, South, and West -over a map of the United States.
In particular, five male and five female speakers from each region provide a set of 188 single-word recordings, 76 of which are single-syllable words (e.g. "mice", "dome", "bait") and 112 are multi-syllable words (e.g. "alfalfa", "nectarine").
These single-word files provide a total of 11,460 speech samples for further analysis and serve as our primary source of speech data.
In addition, NSP provides metadata on each speaker, including gender, age, race, and hometown.Forvo We also collect speech samples from the Forvo website [6], which is a crowdsourced collection of pronunciations of English words.
We crawled forvo.com for all audio files published by speakers in the United States, on November 22nd, 2017.
This dataset contains 91,843 speech samples covering 59,403 words from 4,991 speakers.
Unfortunately, the Forvo data is non-uniform and sparse.
40,582 (68.3%) of the words in the dataset are only spoken by a single speaker, which makes reasoning about interpretation errors in such words difficult.
In addition, the audio quality of each sample varies from speaker to speaker, which adds difficult-to-quantify noise in our measurements.
In light of these observations, we limit our use of these data to only cross-validation of our results drawn from NSP data.
We use our test harness to query Alexa for a transcription of each speech sample in the NSP dataset.
First, we observe that Alexa does not consistently return the same transcription when processing the same speech sample.
In other words, Alexa is non-deterministic, even when presented with identical audio files over reliable network communication (i.e., TCP).
This may be due to some combination of A/B testing, system load, or evolving models in the Alexa speech-recognition system.
Since we choose to treat Alexa as a black box, investigating this phenomenon is outside the scope of this work.
However, we note that this non-determinism will lead to unavoidable variance in our results.
To account for this variance, we query each audio sample 50 times.
This provides us with 573,000 data points across 60 speakers.
Over all these queries, Alexa did not return a response on 681 (0.1%) of the queries, which we exclude from our analysis.
We collected this dataset of 572,319 Alexa transcriptions on January 14th, 2018 over a period of 24 hours.
Part of our analysis includes investigating how interpretation errors relate to Alexa skill names.
We used a thirdparty aggregation database [1] to gather a list of all the skill names that were publicly available on the Alexa skills store.
This list contains 25,150 skill names, of which 23,368 are unique.
This list was collected on December 27th, 2017.
Accuracy by Word Figure 5: Word Accuracy -The accuracy of Alexa interpretations by word is shown as a cumulative distribution function.
9% of the words in our dataset are never interpreted correctly and 2% are always interpreted correctly.
This shows substantial variance in misinterpretation rate among words.
Although we use speech samples collected from human subjects, we never interact with subjects during the course of this work.
We use public datasets and ensure our usage is in line with their provider's terms of service.
All requests to Alexa are throttled so to not affect the availability of production services.
For all attacks presented in this paper, we test them only in a controlled, developer environment.
Furthermore, we do not attempt to publish a malicious skill to the public skill store.
We have disclosed these attacks to Amazon and will work with them through the standard disclosure process.
In this section, we conduct an empirical analysis of the Alexa speech-recognition system.
Specifically, we measure its accuracy, quantify the frequency of its interpretation errors, classify these errors, and explain why such errors occur.
We begin our analysis by investigating how well Alexa transcribes the words in our dataset.
We find that Alexa successfully interprets only 394,715 (68.9%) out of the 572,319 queries.In investigating where Alexa makes interpretation errors, we find that errors do not affect all words equally.
Figure 5 shows the interpretation accuracy by individual words in our dataset.
Only three words (2%) are always interpreted correctly.
In contrast, 9% of words are always interpreted incorrectly, indicating that Alexa is poor at correctly interpreting some classes of words.
Table 2 characterizes these extremes by showing the top 10 misinterpreted words as well as the top 10 correctly interpreted words in our dataset.
We find that words with the lowest accuracy tend to be small, single-syllable words, such as "bean", "calm", and "coal".
Words with the highest Table 2: Words with Highest and Lowest Accuracy -The best and worst interpretation accuracies for individual words are shown here.
We find that the words with the lowest accuracy seem to be small, single syllable words.
accuracy are mixed.
Many of the top words contain two or three syllables, such as "forecast" and "robin".
In one counter example, the word "good" was interpreted correctly 99.9% of the time.
Even among words that are poorly understood by Alexa, there is significant variance in the number of unique misinterpretations.
For example, the word "bean" has a 0% accuracy rate and is misinterpreted in 12 different ways, such as "been", "beam", and "bing".
In contrast, the word "unadvised" was also never interpreted correctly, but misinterpreted in 147 different ways, such as "an advised", "i devised", and "hundred biased".
Figure 6 shows the number of unique misinterpretations per word.
The median number of misinterpretations is 15, but with a heavy tail.In investigating the distributions of misinterpretations per word, we observe that, for each of the 188 words, there are one or two interpretations that Alexa outputs more frequently than the others.
Motivated by this ob- servation, we introduce the notion of the "most common error" (MCE) for a given word.
As an example, consider the word "boil", which is misinterpreted 100% of the time.
The MCE of "boil" is the word "boyle", which accounts for 94.3% (MCE Rate) of the errors.
In this sense, the rate at which the MCE occurs serves as a measure of how random the distribution of misinterpretations is.
Because "boyle" accounts for the majority of its interpretation errors, we thus claim that "boil" has a predictable misinterpretation distribution.
To visualize the rate and randomness of interpretation errors per word, we plot the error rate for each word along with its MCE rate (Figure 7).
This graphical representation provides us with a clearer picture of interpretation errors in Alexa.
We then split this plot into four quadrants -quadrant I (upper-right), II (upper-left), III (bottom-left), and IV (bottom-right).
The majority (56.4%) of words in our dataset fall into quadrant III (bottom-left).
These are words that are both interpreted correctly most of the time and do not have a prevalent MCE.
Instead, they have uncommon errors with no obvious pattern.
21.3% of words appear in quadrant IV (bottom-right).
These are words that are often interpreted correctly, but do have a prevalent MCE.
There are 9.6% of the words in our dataset that appear in quadrant II (topleft), meaning they are misinterpreted often, but do not feature a prevalent MCE.
These are likely to be words that Alexa is poor at understanding altogether.
As an example, the word "unadvised", which has 147 unique misinterpretations, appears in this quadrant.
The final class of words, in quadrant I (upper-right), are those that are misinterpreted more than 50% of the time and have an MCE that appears in more than 50% of the errors.
These are words that are Alexa misunderstands both frequently Word MCE Word Phonemes MCE Phonemes Table 3: Phonetic Structure of Systematic Errors -We show the underlying phonetic structure of the ten systematic errors that seem to appear due to Alexa confusing certain phonemes with others.
In each case, the resultant MCE is at an edit distance of just one phoneme from the intended word.
and in a consistent manner.
There are 24 (12.8%) such words in our dataset.rip rap R IH P R AE P lung lang L AH NG L AE NG wet what W EH T W AH T dime time D AY M T AY M bean been B IY N B IH N dull doll D AH L D AA L coal call K OW L K AO L luck lock L AH K L AA K loud louder L AW D L AW D ER sweeten Sweden S W IY T AH N S W IY D AH N We now have a classification for interpretation errors from our dataset.
Moreover, we identified 24 words for which Alexa consistently outputs one wrong interpretation.
We next investigate why these systematic errors occur.Homophones Unsurprisingly, eight (33.3%) of these errors, including "sail" to "sale", "calm" to "com", and "sell" to "cell" are attributable to the fact that these words are homophones, as they have the same pronunciation, but different spellings.
Of these, five are cases where Alexa returns a proper noun (of a person, state, band or company) that is a homophone with the spoken word, for example, "main" to "Maine", "boil" to "Boyle", and "outshine" to "Outshyne".
Compound Words Two (8.3%) other systematic errors occur due to compound words.
Alexa appears to break these into their constituent words, rather than return the continuous compound word.
For example, "superhighway" is split into "super highway" and "outdoors" is split into "out doors".
Phonetic Confusion Ten (41.7%) of the systematic errors can be explained by examining the underlying phonetic structures of the input words and their errors: in each case, the MCE differs from the spoken word by just a single phoneme.
For example, the MCE for the word "wet" is the word "what".
The phonetic spelling of "wet" is W EH T, whereas the phonetic spelling of "what" is W AH T.
These errors show that Alexa often misunderstands certain specific phonemes within words while correctly interpreting the rest of them.
A full list of the phonetic structures for these cases is shown in Table 3.
We could not easily explain three (12.5%) of the errors: "mill" to "no", "full" to "four" and "earthy" to "Fi".
Even in listening to each speech sample individually, we found no auditory reason why this interpretation error occurs.
One surprising error ("preferably" to "preferrably") occurred because Alexa returned a common misspelling of the intended word.
This may be caused by a bug in the Alexa system itself.
Our empirical analysis uncovers the existence of frequently occurring, predictable errors in Amazon Alexa.
We next investigate how an adversary can leverage these errors to cause harm to users in the Alexa ecosystem.
To this end, we introduce a new attack, called skill squatting, which exploits predictable errors to surreptitiously route users to a malicious Alexa skill.
The core idea is simple -given a systematic error from one word to another, an adversary constructs a malicious skill that has a high likelihood of confusion with a target skill on the Alexa skills store.
When a user attempts to access a desired skill using their voice, they are routed instead to the malicious skill, due to a systematic error in the interpretation of the input.
This attack is most similar in style to domain name typosquatting, where an attacker predicts a common "typo" in domain names and abuses it to hijack a request [35,43,44,45].
However, typosquatting relies on the user to make a mistake when typing a domain; in contrast, our attack is intrinsic to the speech-recognition service itself.
In this section, we evaluate the skill squatting attack and explore what it looks like in the wild.
Up to this point, our model of interpretation errors has been entirely constructed based on observations outside of a skill invocation environment.
We next investigate whether these errors can be exploited in a skill invocation environment, to redirect the processing of an Alexa query to an attacker-controlled skill server.Our testing process is as follows: given a model of predictable errors, we build pairs of skills with names that are frequently confused by Alexa.
For example, because "boil" is frequently confused with "boyle", we would build two skills: one with the name Boil and one with the name Boyle.
We call these skills the target skill (or squattable skill) and the squatted skill.
We refer to words with these predictable, frequently occurring errors as squattable.
If an attack is successful, Alexa will trigger the squatted skill when a request for the target skill is received.
For example, when a user says: "Alexa, ask Boil hello."
Table 4: Skill Squatting Validation -We show the results of testing 27 skill squatting attacks.
The pairs of target and squatted skills are built using the squattable words of our training set.
The success rates are computed by querying the speech samples of our test set.
We are able to successfully squat 25 (92.6%) of the skills at least one time, demonstrating the feasibility of the attack.They will instead be routed to the Boyle skill.In order to demonstrate that our attack will work on speakers we have not previously seen, we use two-fold cross validation over the 60 speakers in our dataset.
We divide the set randomly into two halves, with 30 speakers in each half.
We build an error model using the first half of the speakers (training set) and then use this model to build pairs of target and squatted skills.
The analysis of this training set results in 27 squattable words, all of which are detailed in Table 4.
For each speaker in the test set, we construct a request to each of the 27 target skills and measure how many times the squatted skill is triggered.
We repeat this process five times to address non-determinism in Alexa responses.
As an ethical consideration, we test our attack by registering our skills in a developer environment and not on the public Alexa skills store, to avoid the possibility of regular users inadvertently triggering them.
Table 4 shows the results of our validation experiment.
We are able to successfully squat skills at least once for 25 (92.6%) of the 27 squattable skills.
There are two cases in which our squatting attack never works.
In the first case, we expect the skill name loud to be incorrectly interpreted as the word louder.
However, because louder is a native Alexa command which causes Alexa to increase the volume on the end-user device, when the target is misinterpreted, it is instead used to perform a native Alexa function.
We found no clear explanation for the second pair of skills, Boil/Boyle.
In other cases, we find that testing the attack in a skill environment results in a very high rate of success.
In the Coal/Call and Sell/Cell pairs, the attack works 100% of the time.
We speculate that this is a result of a smaller solution space when Alexa is choosing between skills as opposed to when it is transcribing arbitrary speech within a skill.
Ultimately, Table 4 demonstrates that skill squatting attacks are feasible.
We next investigate how an adversary can craft maliciously named skills targeting existing skills in the Alexa skills store, by leveraging the squattable words we identified in Section 4.
To this goal, we utilize our dataset of Alexa skill names described in Section 3.
First, we split each skill name into its individual words.
If a word in a skill exists in our spoken dataset of 188 words, we check whether that word is squattable.
If it is, we exchange that word with its most common error to create a new skill name.
As an example, the word "calm" is systematically misinterpreted as "com" in our dataset.
Therefore, a skill with the word "calm" can be squatted by using the word "com" in its place (e.g. "quick com" squats the existing Alexa skill "quick calm").
Using the 24 squattable words we identified in Section 4, we find that we can target 31 skill names that currently exist on the Alexa Store.
Only 11 (45.8%) of the squattable words appear in Alexa skill names.
Table 5 shows one example of a squattable skill for each of these 11 words.
We note that the number of squattable skills we identify is primarily limited by the size of our dataset and it is not a ceiling for the pervasiveness of this vulnerability in the Amazon market.
To address this shortcoming, in the remainder of this section we demonstrate how an attacker with a limited speech corpus can predict squattable skills using previously-unobserved words.
An adversary that attempts this attack using the techniques described thus far would be severely restricted by the size and diversity of their speech corpus.
Without many recordings of a target word from a variety of speakers, they would be unable to reliably identify systematic misinterpretations of that word.
Considering that many popular skill names make use of novel words (e.g., WeMo) or words that appear less frequently in discourse (e.g., Uber), acquiring such a speech corpus may prove prohibitively costly and, in some cases, infeasible.
We now consider how an attacker could amplify the value of their speech corpus by reasoning about Alexa misinterpretations at the phonetic level.
To demonstrate this approach, we consider the misinterpretation of "luck" in Table 4.
"Luck" (L AH K) is frequently misinterpreted as "lock" (L AA K), suggesting that Alexa experiences confusion specifically between the phonemes AH and AA.
As such, an attacker might predict confusion in other words with the AH phoneme (e.g., "duck" to "dock", "cluck" to "clock") without having directly observed those words in their speech corpus.Unfortunately, mapping an input word's phonemes to a misinterpreted output word's phonemes is non-trivial.
The phonetic spelling of the input and output words may be of different lengths, creating ambiguity in the attribution of an error to each input phoneme.
Consider the following example from our tests, where the input word "absentee" (AE, B, S, AH, N, T, IY) is understood by Alexa as "apps and t." (AE, P, S, AH, N, D, T, IY).
Moving from left to right, AE is correctly interpreted and an input of B maps to an output of P. However, determining which input phoneme is at fault for the D of the output is less clear.
In order to attribute errors at the phonetic level, we thus propose a conservative approach that a) minimizes the total number of errors attributed and b) discards errors that cannot be attributed to a single input phoneme.
Our algorithm works in the following steps:1.
We begin by identifying the input-to-output mapping of correct phonemes whose alignment provides the smallest cost (i.e., fewest errors):AE B S AH N T IY 2.
Based on this alignment, we inspect any additional phonemes inserted into the output that do not correspond to a phoneme in the input.
We choose to attribute these output phonemes to a misinterpretation of the phoneme that immediately precedes them in the input.
We extend the mappings created in the previous step to include these errors.
In our example, we attribute the D output phoneme to the N input phoneme, mapping N to N D:AE B S AH N T IY 3.
Finally, we analyze the remaining unmatched phonemes of the input.
We consider unambiguous cases to be where a single phoneme of the input: a) occurs between two already mapped pairs of phonemes or is the first or the last phoneme of the input, and b) was either omitted (maps to an empty phoneme) or confused with one or two other phonemes in the output.
In the example above, we map the phoneme B of the input to its singlephoneme misinterpretation as P in the output.
We note that this step only attributes an error when its source is unambiguous.
There exist some cases where we cannot safely attribute errors and thus we choose to discard an apparent phoneme error.
Taking an example from our tests, when the input word "consume" (K AH N S UW M) is confused by Alexa as "film" (F IH L M), the word error may have happened for reasons unrelated to phoneme misinterpretations and it is not clear how to align input and output except for the final M phoneme in both of the words.
Since the other phonemes could instead be mapped in many ways, we discard them.We use this algorithm to create a phoneme error model which provides a mapping from input phonemes to many possible output phonemes.
We next evaluate whether such phoneme error model, built using the NSP dataset, can predict Alexa interpretation errors for words that do not appear in our dataset.
To accomplish this, we leverage the Forvo dataset, described in Section 3, as a test set.First, we exclude from our test set all the speech samples of words that are also in the NSP dataset, since we seek to predict errors for words that we have not used before.
Then, we decompose each remaining Forvo word, w, into its phonetic spelling.
For every phoneme p in each phonetic spelling we attempt to replace p with each of its possible misinterpretations p i present in our phoneme error model.
We then check if the resultant phoneme string represents an English word, w .
If it does, we mark w as a potential misinterpretation of w.
As an example, consider the word "should", whose phonetic representation is SH UH D.
The UH phoneme is confused with the OW phoneme in our phoneme error model, so we attempt a phoneme level swap and get the phoneme string SH OW D.
This phoneme string maps back to the English word "showed".
Thus, we predict that the word "should" will be misinterpreted by Alexa as "showed".
Using this technique, we are able to make error predictions for 12,869 unique Forvo words.
To validate the correctness of our predictions, we next collect the actual Alexa interpretations of this set of words.
We query each speech sample from this set 50 times using our test harness and record their interpretations.
We then check whether any observed interpretation errors in this set are in our predictions.
We observe that our predictions are correct for 3,606 (28.8%) of the words in our set.
This set is 17.5x larger than our seed of 188 words.
This indicates that by extending our word model with a phoneme model, we can successfully predict misinterpretations for a subset of words that we have not previously seen, thus improving the potency of this attack even with a small speech dataset.
We next apply our method of extending our seed-set of errors to identify already existing instances of confused skills in the Alexa skills store.
In total, we find 381 unique skill pairs that exhibit phoneme confusion.
The largest single contributor is the word "fact", which is commonly misinterpreted as "facts", and "fax".
Given the large number of fact-related skills available on the skill store, it is unsurprising that many of these exist in the wild.In order to determine whether these similarities are due to chance, we investigate each pair individually on the skill store.
We find eight examples of squatted skills that we mark as worth investigating more closely (Table 6).
We cannot speak to the intention of the skill creators.
However, we find it interesting that such examples cur- Table 6: Squatted Skills in the Alexa skills store -We show examples of squatted skills in the Alexa skills store that drew our attention during manual analysis.
Notably, a customer review of the "phish geek" skill noted they were unable to use the application due to common confusion with the "fish geek" skill.rently exist on the store.
For example, "cat facts" has a corresponding squatted skill, "cat fax", which seemingly performs the same function, though published by a different developer.
In another example, "Phish Geek" [9], which purports to give facts about the American rock band Phish, is squatted by "Fish Geek" [5], which gives facts about fish.
Anecdotally, one user of "Phish Geek" appears to have experienced squatting, writing in a review:I would love it if this actually gave facts about the band.
But instead, it tells you things like "Some fish have fangs!
"Ultimately, we have no clear evidence that any of these skills of interest were squatted intentionally.
However, this does provide interesting insight into some examples of what an attacker may do and further validates our assertion that our phoneme-based approach can prove useful in finding such examples in the wild.
We have thus far demonstrated skill squatting attacks that target speakers at an aggregate level.
We next ask the question, "Can an attacker use skill squatting to target specific groups of people?"
To accomplish this, we introduce a variant of the skill squatting attack, called spear skill squatting.
Spear skill squatting extends skill squatting attacks by leveraging words that only squattable in targeted users' demographic.
Spear skill squatting draws its name from the closely related spear phishing family of attacks, which are phishing attacks targeted at specific groups of individuals [25].
In this section, we identify and validate spear skill squatting attacks by targeting speakers based on their geographic region and their gender.
Regional Intersection of Squattable Words -We show the 6-way intersection of squattable words by region.
Squattable words that affect all regions are omitted.
Each region is denoted by a dot in the bottom half of the graph.
If a squattable word is shared between two or more regions, the region-dots are connected with a line.
The height of each bar corresponds to the number of squattable words per region-intersection.
There are 11 squattable words that target just one specific region.
The 60 speakers in the NSP corpus are separated both by dialect-region (10 speakers per region) and gender (30 speakers identify as male, 30 identify as female).
We first examine if user demographics play a factor in Alexa accuracy rates.In order to quantify the differences in accuracy between regions, we run a chi-squared "goodness-of-fit" test.
This test is used to determine whether a particular distribution follows an expected distribution.
To not over report this statistic given our sample size, we only consider the most common interpretation per speaker per word, rather than use 50 interpretations per speaker per word.
As we would like to measure whether interpretation errors happen across all regions with equal probability, our null hypothesis is that there is no significant difference in accuracy between the regions.
Our chi-squared test returns a p-value of 6.54 * 10 −139 , indicating strong evidence to reject the null hypothesis.
This demonstrates that at least one region has a significant difference in accuracy from the rest, with a confidence interval > 99%.
We next investigate whether Alexa has different accuracy rates when interpreting speakers of different genders.
We find that Alexa is more accurate when interpreting women (71.9%) than men (66.6%).
In addition, a two proportion z-test between the groups shows a statistically significant difference at a confidence interval of 99% (pvalue: 1.03 * 10 −9 ).
These results indicate that Alexa interprets speakers differently based on their region and their gender.
We next investigate whether the interpretation errors for each demographic are systematic and, as a result, can be used by an adversary to launch a spear skill squatting attack.To identify squattable words based on region, we first split our speakers into their respective dialect-region.
Using the techniques outlined in Section 4, we identify the systematic errors that affect each region in isolation.
This produces a total of 46 unique squattable words that are occur at least in one region.
However, this also includes squattable words that affect every region.
Because this attack focuses on targeting specific groups of individuals, we exclude squattable words that affect all regions.
After removing these, we are left with 22 squattable words that target a strict subset of all regions.
For example, the interpretation error from Pull/Pole, only affects systematically speakers from the West, New England, Midland, and Mid-Atlantic regions, but not speakers from the North or South.
In contrast, the error Pal/Pow only systematically impacts speakers from the Midland region.
Figure 8 shows the distribution of these squattable words per region-intersection.
Notably, there are 11 squattable words that each affect one region in isolation.
Table 7a further breaks down these specific squattable words and their systematic interpretation errors by region.
An attacker can leverage any of these in order to target speakers from one specific region.We then apply the same technique to find squattable words based on speaker gender and observe a similar result -there are squattable words that only affect speakers based on their gender.
Table 7: Validating the Spear Skill Squatting Attack -We test our spear skill squatting attacks in a developer environment.
The last column shows the p-value of a proportion z-test checking whether there is a statistically significant difference, at a confidence interval of 95%, between the success rates of the attack against the region/gender group and the overall population.
Our attacks are successful in impacting specific demographic groups 8 out of 11 times by region and 10 out of 12 times by gender.
We next turn to validating that our spear skill squatting attacks will work in a skill environment.
To test this, we use a methodology similar to that described in Section 5.1, where we build skills in a developer environment and observe the rate at which our squatted skill is favored over the target skill.
Table 7 shows the breakdown of our squatting attempts to target speakers based on both their region and gender.
For 8 out of the 11 region-based attacks, we observe a statistically different rate of success for our attack than when compared to the rate of success observed for the rest of the population.
Our attack works slightly better when targeting speakers by gender, with an attack working in 10 out of the 12 cases.Our results provide evidence that such an attack can be successful in a skill environment.
We acknowledge that our results are inherently limited in scope by the size of our dataset.
An adversary with better knowledge of squattable words can construct new attacks that are outside the purview of our analysis; thus, further scrutiny must be placed on these systems to ensure they do not inadvertently increase risk to the people that use them.
A core limitation of our analysis is the scope and scale of the dataset we use in our analysis.
The NSP dataset only provides 188 words from 60 speakers, which is inadequate for measuring the full scale of systematic misinterpretations of Amazon Alexa.
Although our phoneme model extends our observed misinterpretation results to new words, it is also confined by just the errors that appeared from querying the NSP dataset.Another limitation of our work is that we rely on the key assumption that triggering skills in a development environment works similarly to triggering publicly available skills.
However, do not attempt to publish skills or attack existing skills on the Alexa skills store due to ethical concerns.
A comprehensive validation of our attack would require that we work with Amazon to test the skill squatting technique safely in their public, production environment.
The skill squatting attack relies on an attacker registering squatted skills.
All skills must go through a certification process before they are published.
To prevent skill squatting, Amazon could add to the certification process both a word-based and a phoneme-based analysis of a new skill's invocation name in order to determine whether it may be confused with skills that are already registered.
As a similar example, domain name registrars commonly restrict the registration of homographs -domains which look very similar visually -of well known domains [34].
These checks seem not to be currently in place on Alexa, as we found 381 pairs of skills with different names, but likely to be squatted on the store (Section 5.4).
Short of pronunciation based attacks, there already exist public skills with identical invocation names on the Alexa skills store.
For example, there are currently more than 30 unique skills called "Cat Facts", and the way in which Amazon routes requests in these cases is unclear.
Although this is a benign example, it demonstrates that some best practices from other third-party app store environments have not made their way to Alexa yet.Attacks against targeted user populations based on their demographic information are harder to defend against, as they require a deeper understanding of why such errors occur and how they may appear in the future.
Amazon certainly has proprietary models of human speech, likely from many demographic groups.
Further analysis is required in order to identify cases in which systematic errors can be used to target a specific population.
While we have demonstrated the existence of systematic errors and the feasibility of skill squatting attacks, there remain several open challenges to quantifying the scope and scale of these results.Collecting Richer Datasets.
The conclusions we can draw about systematic errors are limited by the size of our speech corpus.
We find that, in theory, 16,836 of the 23,238 (72.5%) unique skills in the Alexa skills store could potentially be squatted using our phoneme model.
However, without additional speech samples, there is no way for us to validate these potential attacks.
In order to more thoroughly investigate systematic errors and their security implications, we must curate a larger, more diverse dataset for future analysis.
We suspect that with a larger set of words and speakers, we would not only be able to quantify other systematic errors in Alexa, but also draw stronger conclusions about the role of demographics in speech recognition systems.Measuring the Harms of Skill Squatting.
It remains unclear how effective our attack would be in the wild.
In order to observe this, we would need to submit public skills to Amazon for certification.
In addition, our work does not explore what an attacker may be able to accomplish once a target skill is successfully squatted.
In initial testing, we successfully built phishing attacks on top of skill squatting (for example, against the American Express skill) 1 .
However, investigating the scale of such attacks is beyond the scope of this work.
We hypothesize that the most significant risk comes from the possibility that an attacker could steal credentials to third party services, but this topic merits further investigation.Investigating IoT Trust Relationships.
On the web, many users have been conditioned to be security conscious, primarily through browser-warnings [13].
However, an outstanding question is whether that conditioning transfers to a voice-controlled IoT setting.
If an attacker realizes that users trust voice interfaces more than other forms of computation, they may build better, more targeted attacks on voice-interfaces.
Generalizing our Models.
An outstanding question is whether our models can be broadly generalized to other speech-recognition systems.
It is unlikely that our Alexaspecific model of systematic errors will translate directly to other systems.
However, the techniques we use to build these models will work as long as we can leverage a speech-recognition system as a black box.
Future work must be done in replicating our techniques to other speechrecognition systems.
Our work builds on research from a number of disciplines, including linguistics, the human aspects of security and targeted audio attacks on voice-controlled systems.Dialects in Speech.
Linguists have developed models of English speech since the 1970s, from intonation to rhythm patterns [23].
Recently, researchers have used phoneme and vowel data similar to that of the NSP dataset [19] to study the patterns of speech by region and gender [20,21,31].
Clopper has also investigated the effects of dialect variation within sentences on "semantic predictability" -this is the ability of a listener to discern words based on the context in which they appear [18].
Typosquatting and Human Factors.Our work broadly aligns with research about the human aspects of security, such as susceptibility to spam or phishing attacks [25,27].
Specifically, we focus on a long history of research into domain typosquatting [12,33,43,44].
Using ideas similar to our work, Nikiforakis et al. relied on homophone confusion to find vulnerable domain names [35].
Most recently, Tahir et al. investigated why some URLs are more susceptible to typosquatting than other URLs [45].
Our work also draws on analysis of attack vectors that are beyond simply making mistakes -Kintis et al. studied the longitudinal effects of "combosquatting" attacks, which are variants of typosquatting [29].
Other Skill Squatting Attacks.
We are not alone in highlighting the need to investigate the security of speech recognition systems.
In a recent preprint, Zhang et al. report a variant of the skill squatting attack based on the observation that Alexa favors the longest matching skill name when processing voice commands [50].
If a user embellished their voice command with naturalistic speech, e.g.,"Alexa, open Sleep Sounds please" instead of "Alexa, open Sleep Sounds," an attacker may be able to register a skill named Sleep Sounds please in order to squat on the user's intended skill.
Their attack demonstrates dangerous logic errors in the voice assistant's skills market.
In contrast, our work considers more broadly how the intrinsic error present in natural language processing algorithms can be weaponized to attack speech recognition systems.
Researchers have shown time after time that acoustic attacks are a viable vector causing harm in computing devices.
For example, shooting deliberate audio at a drone can cause it to malfunction and crash [41].
Audio attacks have been used to bias sensor input on Fitbit devices and, further, can manipulate sensor input to fully operate toy RC cars [47].
Audio has also been used as an effective side channel in stealing private key information during key generation [24] and leaking private data through the modification of vibration sensors [38].
Beyond such attacks, several researchers have developed a number of of adversarial examples of audio input to trick voice-based interfaces.
Carlini et al. demonstrated that audio can be synthesized in a way that is indiscernible to humans, but are actuated on by devices [15].
Further, a number of researchers independently developed adversarial audio attacks that are beyond the range of human hearing [39,42,49].
Houdini demonstrated that it is possible to construct adversarial audio files that are not distinguishable from the legitimate ones by a human, but lead to predicted invalid transcriptions by target automatic speech recognition systems [17].
Carlini et al. developed a technique for constructing adversarial audio against Mozilla DeepSpeech with a 100% success rate [16].
More recently, Yuan et al. showed that voice commands can be automatically embedded into songs, while not being detected by a human listener [48].
In this work, we investigated the interpretation errors made by Amazon Alexa for 11,460 speech samples taken from 60 speakers.
We found that some classes of interpretation errors are systematic, meaning they appear consistently in repeated trials.
We then showed how an attacker can leverage systematic errors to surreptitiously trigger malicious applications for users in the Alexa ecosystem.
Further, we demonstrated how this attack could be extended to target users based on their demographic information.
We hope our results inform the security community about the implications of interpretation errors in speech-recognition systems and provide the groundwork for future work in the area.
This work was supported in part by the National Science Foundation under contracts CNS 1750024, CNS 1657534, and CNS 1518741.
This work was additionally supported by the U.S. Department of Homeland Security contract HSHQDC-17-J-00170.
Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of their employers or the sponsors.
