Recently there has been much academic and industrial interest in practical implementations of zero knowledge proofs.
These techniques allow a party to prove to another party that a given statement is true without revealing any additional information.
In a Bitcoin-like system, this allows a payer to prove validity of a payment without disclosing the payment's details.
Unfortunately, the existing systems for generating such proofs are very expensive, especially in terms of memory overhead.
Worse yet, these systems are "monolithic", so they are limited by the memory resources of a single machine.
This severely limits their practical applicability.
We describe DIZK, a system that distributes the generation of a zero knowledge proof across machines in a compute cluster.
Using a set of new techniques, we show that DIZK scales to computations of up to billions of logical gates (100× larger than prior art) at a cost of 10 µs per gate (100× faster than prior art).
We then use DIZK to study various security applications.
Cryptographic proofs with strong privacy and efficiency properties, known as zkSNARKs (zero-knowledge Succinct Non-interactive ARgument of Knowledge) [52,38,19], have recently received much attention from academia and industry [13,9,41,51,20,37,55,11,15,48,78,31,33,10,75,31,46,47,53,36,22], and have seen industrial deployments [7,5,3,4].
For example, zkSNARKs are the core technology of Zcash [7,10], a popular cryptocurrency that, unlike Bitcoin, preserves a user's payment privacy.
Bitcoin requires users to broadcast their private payment details in the clear on the public blockchain, so other participants can check the validity of the payment.
In contrast, zkSNARKs enable users to broadcast encrypted transactions details and prove the validity of the payments without disclosing what the payments are.More formally, zkSNARKs allow a prover (e.g., a Zcash user making a payment) to convince a verifier (e.g., any other Zcash user) of a statement of the form "given a function F and input x, there is a secret w such that F(x, w) = true".
In the cryptocurrency example, w is the private payment details, x is the encryption of the payment details, and F is a predicate that checks that x is an encryption of w and w is a valid payment.
These proofs provide two useful properties: succinctness and zero knowledge.
The first property allows for extremely small proofs (128 B) and cheap verification (2 ms plus a few µs per byte in x), regardless of how long it takes to evaluate F (even if F takes years to compute).
The second property enables privacy preservation, which means that the proof reveals no information about the secret w (beyond what is already implied by the statement itself).
The remarkable power of zkSNARKs comes at a cost: the prover has a significant overhead.
zkSNARKs are based on probabilistically checkable proofs (PCPs) from Complexity Theory, which remained prohibitively slow for two decades until a line of recent work brought them closer to practical systems (see §12).
One of the main reasons for the prover's overhead is that the statement to be proved must be represented via a set of logical gates forming a circuit, and the prover's cost is quasi-linear in this circuit's size.
Unfortunately, this prover cost is not only in time but also in space.Thus, in existing systems, the zkSNARK prover is a monolithic process running on a single machine that quickly exceeds memory bounds as the circuit size increases.
State-of-the-art zkSNARK systems [59] can only support statements of up to 10-20 million gates, at a cost of more than 1 ms per gate.
Let us put this size in perspective via a simple example: the SHA-256 compression function, which maps a 512-bit input to a 256-bit output, has more than 25,000 gates [10]; no more than 400 evaluations of this function fit in a circuit of 10 million gates, and such a circuit can be used to hash files of up to a mere 13 kB.
In sum, 10 million gates is not many.
This bottleneck severely limits the applicability of SNARKs, and motivates a basic question: can zkSNARKs be used for circuits of much larger sizes, and at what cost?
DIZK.
We design and build DIZK (DIstributed Zero Knowledge), a zkSNARK system that far exceeds the scale of previous state-of-the-art solutions.
At its core, DIZK distributes the execution of a zkSNARK across a compute cluster, thus enabling it to leverage the aggregated cluster's memory and computation resources.
This allows DIZK to support circuits with billions of gates (100× larger than prior art) at a cost of 10 µs per gate (100× faster than prior art).
We evaluate DIZK on two applications: proving authenticity of edited photos (as proposed in [53]), and proving integrity of machine learning models.
DIZK enables applications on significantly larger instance sizes, e.g., image editing on photos of 2048 by 2048 pixels.DIZK makes a significant conceptual step forward, enlarging the class of applications feasible for zkSNARKs.
We implement DIZK via Apache Spark [2] and will release all source code under a permissive software license.DIZK does inherit important limitations of zkSNARKs (see §13).
First, while DIZK supports larger circuits than prior systems, its overhead is still prohibitive for many practical applications; improving the efficiency of zkSNARKs for both small and large circuits remains an important challenge.
Also, like other zkSNARKs, DIZK requires a trusted party to run a setup procedure that uses secret randomness to sample certain public parameters; the cost of this setup grows with circuit size, which means that this party must also use a cluster, which is harder to protect against attackers than a single machine.
Nevertheless, the recent progress on zkSNARKs has been nothing short of spectacular, which makes us optimistic that future advancements will address these challenges, and bring the power of zkSNARKs to many more practical applications.
Challenges and techniques.
Distributing a zkSNARK is challenging.
Protocols for zkSNARKs on large circuits involve solving multiple large instances of tasks about polynomial arithmetic over cryptographically-large prime fields and about multi-scalar multiplication over elliptic curve groups.
For example, generating proofs for billiongate circuits requires multiplying polynomials of a degree in the billions, and merely representing these polynomials necessitates terabit-size arrays.
Moreover, fast algorithms for solving these tasks, such as Fast Fourier Transforms (FFTs), are notoriously memory intensive, and rely on continuously accessing large pools of shared memory in complex patterns.
But each node in a compute cluster can store only a small fraction of the overall state, and thus memory is distributed and communication between nodes incurs network delays.
In addition, these heavy algorithmic tasks are all intertwined, which is problematic as reshuffling large amounts of data from the output of one task to give as input to the next task is expensive.We tackle the above challenges in two steps.
First, we single out basic computational tasks about field and group arithmetic and achieve efficient distributed realizations of these.
Specifically, for finite fields, DIZK provides distributed FFTs and distributed Lagrange interpolant evaluation ( §4.1); for finite groups, it provides distributed multi-scalar multiplication with fixed bases and with variable bases ( §4.2).
Throughout, we improve efficiency by leveraging characteristics of the zkSNARK setting instead of implementing agnostic solutions.
Second, we build on these components to achieve a distributed zkSNARK.
Merely assembling these components into a zkSNARK as in prior monolithic systems, however, does not yield good efficiency.
zkSNARKs transform the computation of a circuit into an equivalent representation called a Quadratic Arithmetic Program [37,55]: a circuit with N wires and M gates is transformed into a satisfaction problem about O(N) polynomials of degree O(M).
The evaluations of these polynomials yield matrices of size O(N) × O(M) that are sparse, with only O(N + M) non-zero entries.
While this sparsity gives rise to straightforward serial algorithms, the corresponding distributed computations suffer from stragglers with large overheads.The reason lies in how the foregoing transformation is used in a zkSNARK.
Different parts of a zkSNARK leverage the sparsity of the matrices above in different ways: the so-called QAP instance reduction relies on their column sparsity ( §5), while the corresponding QAP witness reduction relies on their row sparsity ( §6).
However, it turns out that the columns and rows are almost sparse: while most columns and rows are sparse, some are dense, and the dense ones create stragglers.We address this issue via a two-part solution.
First, we run a lightweight distributed computation to identify and annotate the circuit with which columns/rows are dense.
Second, we run a hybrid distributed computation that uses different approaches to process the sparse and dense columns/rows.
Overall we achieve efficient distributed realizations for these QAP routines.
In particular, this approach outperforms merely invoking generic approaches that correct for load imbalances such as skewjoin [6].
Finally, we emphasize that most of the technical work described above can be re-used as the starting point to distribute many other similar proof systems.
We have thus packaged these standalone components as a separate library, which we deem of independent interest.We also briefly mention that supporting billion-gate circuits required us to generate and use a pairing-friendly elliptic curve suitable for this task.
See §9 for details.Authenticity of photos & integrity of ML models.
We study the use of DIZK for two natural applications: (1) authenticity of edited photos [53] (see §7.1); and (2) integrity of machine learning models (see §7.2).
Our experiments show that DIZK enables such applications to scale to much larger instance sizes than what is possible via previous (monolithic) systems.An application uses DIZK by constructing a circuit for the desired computation, and by computing values for the circuit's wires from the application inputs.
We do this, for the above applications, via distributed algorithms that exploit the parallel nature of computations underlying editing photos and ML training algorithms.Cryptography at scale.
DIZK exemplifies a new paradigm.
Cryptographic tools are often executed as monolithic procedures, which hampers their applicability to large problem sizes.
We believe that explicitly designing such tools with distributed architectures in mind enables "cryptography at scale", and we view DIZK as a step in this direction for the case of zkSNARKs.
The notion of a zkSNARK, formulated in [52,38,19], has several definitions.
We consider one known as a publiclyverifiable preprocessing zkSNARK (see [20,37]).
We cover necessary background on zkSNARKs by providing a high-level description ( §2.1), an informal definition ( §2.2), and the protocol that we start from ( §2.3).
A zkSNARK can be used to prove/verify statements of the form "given a public predicate F and a public input x, I know a secret input w such that F(x, w) = true".
It has three components: setup, prover, and verifier (Fig. 1).
• The setup receives a predicate F (expressed in a certain way as discussed in §2.2) and outputs a proving key pk F and verification key vk F .
Both keys are published as public parameters and pk F /vk F can be used to prove/verify any number of statements about F.
In particular, the setup for F needs to be run only once.
While the setup outputs keys that are public information, its intermediate computation steps involve secret values that must remain secret.
Thus, the setup must be run by a trusted party -this requirement is challenging, however prior work has studied mitigations (see §13).
• The prover receives the proving key pk F , a public input x for F, and a secret input w for F, and outputs a proof π.
The proof attests to the statement "given F and x, I know a secret w such that F(x, w) = true", but reveals no information about w.
The generation of π involves randomness that imbues it with zero knowledge.
Anyone can run the prover.
• The verifier receives the verification key vk F , a public input x for F, and a proof π, and outputs a decision bit ('accept' or 'reject').
Anyone can run the verifier.
A zkSNARK's costs are determined by the 'execution time' T F of F (see §2.2) and the size k of the input x (which is at most T F ).
The execution time is at least the size of the input and, in many applications, much larger than it.
Thus, T F is seen to be significantly larger than k.The key efficiency feature of a zkSNARK is that the verifier running time is proportional to k alone (regardless of T F ) and the proof has constant size (regardless of k, T F ).
The size of vk F is proportional to k (regardless of T F ).
However, the setup and the prover are very expensive: their running times are (at least) proportional to T F .
The size of pk F is large, because it is proportional to T F .
Running the setup and prover is a severe bottleneck in prior zkSNARK systems since time and space usage grows in T F .
Our focus is to overcome these bottlenecks.
While one typically expresses a computation F via a high-level programming language, a zkSNARK requires expressing F via a set of quadratic constraints φ F , which is closely related to circuits of logical gates.
A zkSNARK proof then attests that such a set of constraints is satisfiable.
The size of φ F is related to the execution time of F.
There has been much research [55,11,15,22,48,78,31,75,14] devoted to techniques for encoding programs via sets of constraints, and in this paper, we consider φ F as given.
The zkSNARK language.
We describe the type of computation used in the interface of a zkSNARK.
Values are in a field F of a large prime order p.An R1CS instance φ over F is parameterized by the number of inputs k, number of variables N (with k ≤ N), and number of constraints M; φ is a tuple (k, N, M, a, b, c) where a, b, c are (1 + N) × M matrices over F.An input for φ is a vector x in F k , and a witness for φ is a vector w in F N−k .
An input-witness pair (x, w) satisfies φ if, letting z be the vector F 1+N composed of 1, x, and w, the following holds for all j ∈ [M]:∑ N i=0 a i, j z i · ∑ N i=0 b i, j z i = ∑ N i=0 c i, j z i .
One can treat each quadratic constraint above as representing a logical gate.
Boolean and arithmetic circuits are easily reducible to this form.
We view a, b, c as containing the 'left', 'right', and 'output' coefficients respectively; rows index variables and columns index constraints.
The zkSNARK interface.
A zkSNARK consists of three algorithms: setup S , prover P, and verifier V .
• Setup.
On input a R1CS instance φ = (k, N, M, a, b, c), S outputs a proving key pk and a verification key vk.
• Prover.
On input a proving key pk (for an R1CS instance φ ), input x in F k , and witness w in F N−k , P outputs a proof π that attests to the x-satisfiability of φ .
• Verifier.
On input a verification key vk (generated for φ ), input x in F k , and proof π, V outputs a decision bit.
Our system provides a distributed implementation of a zkSNARK protocol due to Groth [42].
We selected Groth's protocol because it is, to our knowledge, the Figure 2: A distributed zkSNARK.
The setup algorithm is run on a compute cluster, and generates a long proving key pk, held in distributed storage, and a short verification key vk.
The prover algorithm is also run on a compute cluster.most efficient zkSNARK protocol.
That said, our techniques are easily adapted to similar zkSNARK protocols [37,20,55,32,43].
We now describe only the parts of Groth's protocol that are needed to understand our techniques, and refer the reader to [42] for details (including correctness and security, which we inherit).
For reference, we include the full protocol in Fig. 10 (in the appendix) using the notation introduced in this section.QAPs.
Groth's zkSNARK protocol uses Quadratic Arithmetic Programs (QAPs) [37,55] to efficiently express the satisfiability of R1CS instances via certain lowdegree polynomials.
Essentially, the M constraints are 'bundled' into a single equation that involves univariate polynomials of degree O(M).
The prover's goal is then to convince the verifier that this equation holds.
In fact, it suffices for the verifier to know that this equation holds at a random point because distinct polynomials of small degree can only agree on a small number of points.In a little more detail, we now define what is a QAP instance, and what does satisfying such an instance mean.A QAP instance Φ over F has three parameters, the number of inputs k, number of variables N (with k ≤ N), and degree M; Φ is a tuple (k, N, M, A, B, C, D) where A, B, C are each a vector of 1 + N polynomials over F of degree < M, and D is a subset of F of size M.An input for Φ is a vector x in F k , and a witness for Φ is a pair (w, h) where w is a vector in F N−k and h is a vector in F M−1 .
An input-witness pair x, (w, h) satisfies Φ if, letting z ∈ F 1+N be the concatenation of 1, x, and w:∑ N i=0 A i (X)z i · ∑ N i=0 B i (X)z i = ∑ N i=0 C i (X)z i + ∑ M−2 i=0 h i X i · Z D (X) ,whereZ D (X) := ∏ α∈D (X − α).
One can efficiently reduce R1CS instances to QAP instances [37,55]: there is a QAP instance reduction qapI and a QAP witness reduction qapW, for which our system provides distributed implementations of both.QAP instance reduction.
For every R1CS instanceφ = (k, N, M, a, b, c), qapI(φ ) outputs a QAP instance Φ = (k, N, M, A, B, C, D) that preserves satisfiability: for every input x in F k , φ is x-satisfiable iff Φ is x-satisfiable.
It works as follows: let D be a subset of F of size M and then, for each i ∈ {0, 1, . . . , N}, let A i be the polynomial of degree < M that interpolates over D the i-th row of the matrix a; similar for each B i and C i in regards to b and c.QAP witness reduction.
For every witness w inF N−k s.t. (x, w) satisfies φ , qapW(φ , x, w) outputs h in F M−1 s.t. (x, (w, h)) satisfies Φ.It works as follows: let h be the coefficients of the polynomial H(X) of degree less than M − 1 that equals the quotient of(∑ N i=0 A i (X)z i ) · (∑ N i=0 B i (X)z i ) − ∑ N i=0 C i (X)z i and Z D (X).
Bilinear encodings.
Groth's protocol uses bilinear encodings, which enable hiding secrets while still allowing for anyone to homomorphically evaluate linear functions as well as zero-test quadratic functions.We denote by G a group, and consider only groups with a prime order p, which are generated by an element G .
We use additive notation for group arithmetic: P + Q denotes addition of the two elements P and Q. Thus, s · P denotes scalar multiplication of P by the scalar s ∈ Z.
Since p · P equals the identity element, we can equivalently think of a scalar s as in the field F of size p. Theencoding (relative to G ) of a scalar s ∈ F is [s] := s · G ; similarly, the encoding of a vector of scalars s ∈ F n is [s] := (s 1 · G , . . . , s n · G ).
The encoding of a scalar can be efficiently computed via the double-and-add algorithm; yet (for suitable choices of G) its inverse is conjecturally hard to compute, which means that [s] hides (some) information about s. Encodings are also linearly homomorphic:[αs + βt] = α[s] + β [t] for all α, β , s,t ∈ F.Bilinear encodings involve three groups of order p:G 1 , G 2 , G 3 generated by G 1 , G 2 , G 3 respectively.
The en- coding of a scalar s ∈ F in G i is [s] i := s · G i .
Moreover,there is an efficiently computable map e :G 1 × G 2 → G 3 , called pairing, that is bilinear: for every nonzero α, β ∈ F, it holds that e ([α] 1 , [β ] 2 ) = αβ · e (G 1 , G 2 ).
(Also, e is non-degenerate in that e ([1] 1 , [1] 2 ) = [0] 3 .)
Pairings al- low zero-testing quadratic polynomials evaluated on en- codings.
For example, given [s] 1 , [t] 2 , [u] 1 , one can test if st + u = 0 by testing if e ([s] 1 , [t] 2 ) + e ([u] 1 , [1] 2 ) = [0] 3 .3 Design overview of DIZK Fig. 2 shows the outline of DIZK's design.
The setup and the prover in DIZK are modified from monolithic procedures to distributed jobs on a cluster; F, pk F , and w are stored as data structures distributed across multiple machines instead of on a single machine.
The verifier remains unchanged from the vanilla protocol as it is inexpensive, enabling DIZK's proofs to be verified by existing implementations of the verifier.
Spark.
We implemented DIZK using Apache Spark [2], a popular cluster computing framework, though our design principles behind DIZK are applicable to other frameworks [1,35,44].
Spark consists of two components: the driver and executors.
Applications are created by the driver and assigned to executors, consisting of jobs split into stages that dictate a set of tasks.
Large datasets are stored as Resilient Distributed Datasets (RDDs).
System interface.
The interface of DIZK matches the interface of a zkSNARK for proving/verifying satisfiability of R1CS instances (see §2.2) except that large objects are represented via RDDs.
More precisely:• The setup receives an R1CS instance φ = (k, N, M, a, b, c) and outputs corresponding keys pk and vk.
As instance size grows (i.e., as the number of variables N and of constraints M grow), φ and pk grow in size (linearly in N and M), so both are represented as RDDs.
• The prover receives the proving key pk, input x in F k , and witness w in F N−k .
The prover outputs a proof π of constant size (128 B).
As typically the input size k is small and the witness size N − k is large, we represent the input as an array and the witness as an RDD.
When using DIZK in an application, the application setup needs to provide φ to the DIZK setup, and the application prover needs to provide x and w to the DIZK prover.
Since these items are big, they may also need to be generated in a distributed way; we do so for our applications in §7.
High-level approach.
The setup and prover in serial implementations of zkSNARKs run monolithic space- intensive computations that quickly exceed memory bounds.
Our approach for an efficient distributed implementation is as follows.First, we identify the heavy computational tasks that underlie the setup and prover.
In Groth's protocol these fall in three categories: (1) arithmetic (multiplication and division) for polynomials of large degree over large prime fields; (2) multi-scalar multiplication over large prime groups; (3) the QAP instance and witness reductions described in §2.3.
Such computations underlie other proof systems too (see full version).
Second, we design distributed implementations of these components.
While there are simple strawman designs that follow naive serial algorithms, these are too expensive (e.g., run in quadratic time); on the other hand, non-naive serial algorithms gain efficiency by leveraging large pools of memory.
We explain how to distribute these memoryintensive algorithms.Finally, we assemble the aforementioned distributed components into a distributed setup and distributed prover.
This assembly poses challenges as the dataflow from one component to another requires several large-scale re-shuffles that we resolve with tailored data structures.
Fig. 3 presents a diagram of the main parts of the design, and we describe them in the following sections: §4 discusses how to distribute polynomial arithmetic and multi-scalar multiplication; §5 discusses how to distribute the QAP instance reduction, and how to obtain the distributed setup from it; §6 discusses how to distribute the QAP witness reduction, and how to obtain the distributed prover from it.
We describe the computational tasks involving finite field and finite group arithmetic that arise in the zkSNARK, and how we distribute these tasks.
These form subroutines of the distributed setup and distributed prover computations (see §5 and §6).
27th USENIX Security Symposium 679 The reduction from an R1CS instance φ = (k, N, M, a, b, c) to a QAP instance Φ = (k, N, M, A, B, C, D) (in the setup) and its witness reduction (in the prover) involves arithmetic on Θ(N) polynomials of degree Θ(M); see §2.3.
(N is the number of variables and M is the number of constraints.)
We distribute the necessary polynomial arithmetic, allowing us to scale to N and M that are in the billions.
Fast polynomial arithmetic is well-known to rely on fast algorithms for two fundamental tasks: polynomial evaluation and interpolation.
In light of this, our approach is the following: (i) we achieve distributed fast implementations of evaluation and interpolation, and (ii) use these to achieve distributed fast polynomial arithmetic such as multiplication and division.Recall that (multi-point) polynomial evaluation is as follows: given a polynomial P(X) = ∑ n−1 j=0 c j X j over F and elements u 1 , . . . , u n in F, compute the elements P(u 1 ), . . . , P(u n ).
One can do this by evaluating P at each point, costing Θ(n 2 ) field operations overall.Conversely, polynomial interpolation is as follows: given elements u 1 , v 1 , . . . , u n , v n in F, compute the polynomial P(X) = ∑ n−1 j=0 c j X j over F such that v i = P(u i ) for every i ∈ {1, . . . , n}.
One can do this by using u 1 , . . . , u n to compute the Lagrange interpolants L 1 (X), . . . , L n (X), which costs Θ(n 2 log n) field operations [71], and then output ∑ n j=1 v j L j (X), which costs another Θ(n 2 ).
While both solutions are straightforward to distribute, they are too expensive due to the quadratic growth in n.
We describe distributed FFT in the next section, while leaving the details of Lag to the appendix ( §4.1.3).
Fast Fourier Transforms (FFTs) [71] provide much faster solutions, which run in time˜Otime˜ time˜O(n).
For instance, the Cooley-Tukey algorithm [29] solves both problems with O(n log n) field operations, provided that F has suitable algebraic structure (in our setting it does).
The algorithm requires storing an array of n field elements in working memory, and performing O(log n) 'passes' on this array, each costing O(n).
The structure of this algorithm can be viewed as a butterfly network since each pass requires shuffling the array according to certain memory patterns.While the Cooley-Tukey algorithm implies a fast parallel algorithm, its communication structure is not suitable for compute clusters.
At each layer of the butterfly network, half of the executors are left idle and the other half have their memory consumption doubled; moreover, each such layer requires a shuffle involving the entire array.We take a different approach, suggested by Sze [65], who studies the problem of computing the product of terabit-size integers on compute clusters, via MapReduce.Sze's approach requires only a single shuffle.
Roughly, an FFT computation with input size n is reduced to two batches of √ n FFT computations, each on input size √ n.
The first batch is computed by the mappers; after the shuffle, the second batch is computed by the reducers.
We use the same approach to implement a distributed FFT, but in the setting of finite fields.
An additional task that arises (in the setup, see §5) is a problem related to polynomial evaluation that we call Lag (from 'Lagrange'): given a domain {u 1 , . . . , u n } ⊆ F and an element t ∈ F, compute the evaluation at t of all Lagrange interpolants L 1 (X), . . . , L n (X) for the domain.A common approach to do so is via the barycentric Lagrange formula [17]: compute the barycentric weights r 1 , . . . , r n as r i := 1/ ∏ j =i (u i − u j ), and then computeL 1 (t), . . . , L n (t) as L i (t) := r i t−u i · L(t) where L(X) := ∏ n j=1 (X − u j ).
When the domain is a multiplicative subgroup of the field generated by some ω ∈ F (in our setting it is), this approach results in an expression, L i (X) = ω i /n X−ω i · (X n − 1), that is cheap to evaluate.
This suggests a simple but effective distributed strategy: each executor in the cluster receives the value t ∈ F and a chunk of the index space i, and uses the inexpensive formula to evaluate L i (t) for each index in that space.
In addition to the expensive finite field arithmetic discussed above, the setup and prover also perform expensive group arithmetic, which we must efficiently distribute.After obtaining the evaluations of Θ(N + M) polynomials, the setup encodes these values in the groups G 1 and G 2 , performing the operations s → [s] 1 and s → [s] 2 for Θ(N + M) values of s.
In contrast, the prover computes linear combinations of Θ(N + M) encodings.
Again, we seek to scale to N and M that are in the billions.These operations can be summarized as two basic computational problems within a group G of a prime order p (where scalars come from the field F of size p).
• Fixed-base multi-scalar multiplication (fixMSM).
Given a vector of scalars s in F n and element P in G, compute the vector of elements s · P in G n .
• Variable-base multi-scalar multiplication (varMSM).
Given a vector of scalars s in F n and a vector of elements(P i ) n i=1 in G n , compute ∑ n i=1 s i · P i in G.For small n, both problems have simple solutions: for fixMSM, compute each element s i · P and output it; for varMSM, compute each s i · P i and output their sum.In our setting, these solutions are expensive not only because n is huge, but also because the scalars are (essentially) random in F, whose cryptographically-large prime size p has k ≈ 256 bits.
This means that the (average) number of group operations in these simple solutions is ≈ 1.5kn, a prohibitive cost.Both problems can be solved via algorithms that, while being much faster, make an intensive use of memory.
We next discuss our approach to efficiently distribute varMSM.
We leave the discussion of distributing fixMSM to §4.2.2.
An efficient algorithm for varMSM is Pippenger's algorithm [57], which is within 1 + o(1) of optimal for nearly all scalar vectors [58].
In the setting of serial zkSNARKs this algorithm outperforms, by 20-30%, the popular BosCoster algorithm [34, §4].
(Other well-known algorithms like Straus' algorithm [64] and the Chang-Lou algorithm [25] are not as fast on large instances; see [16].)
Given scalars s 1 , . . . , s n and their bases P 1 , · · · , P n , Pippenger's algorithm chooses a radix 2 c , computes s 1 /2 c P 1 + · · · + s n /2 c P n , doubles it c times, and sums it to (s 1 mod 2 c )P 1 + · · · + (s n mod 2 c )P n .
For the last step, the algorithm sorts the base elements into 2 c buckets according to (s 1 mod 2 c ), . . . , (s n mod 2 c ) (discarding bucket 0), sums the base elements in the remaining buckets to obtain intermediate sums Q 1 , . . . , Q 2 c −1 , and computes Q 1 +2Q 2 +· · ·+(2 c −1)Q 2 c −1 = (s 1 mod 2 c )P 1 + · · · + (s n mod 2 c )P n .
For a suitable choice of 2 c , this last step saves computation because each bucket contains the sum of several input bases.A natural approach to distribute Pippenger's algorithm is to set the number of partitions to 2 c and use a custom partitioner that takes in a scalar s i as the key and maps its base element b i to partition (s i mod 2 c ).
While this approach is convenient, we find in practice that the cost of shuffling in this approach is too high.
Instead, we find it much faster to merely split the problem evenly across executors, run Pippenger's algorithm serially on each executor, and combine the computed results.
Efficient algorithms for fixMSM use time-space tradeoffs [23].
Essentially, one first computes a certain look-up table of multiples of P, and then uses it to compute each s i · P.
As a simple example, via log |F| group operations, one can compute the table (P, 2 · P, 4 · P, . . . , 2 log |F| · P), and then compute each s i · P with only log |F|/2 group operations (on average).
More generally one can increase the 'density' of the look-up table and further reduce the time to compute each s i · P.
As n increases, it is better for the look-up table to also grow, but larger tables require more memory to store them.A natural approach to distribute this workload across a cluster is to evenly divide the n scalars among the set of executors, have each executor build its own in-memory look-up table and perform all assigned scalar multiplications aided by that table, and then assemble the output from all executors.
However, this approach does not fit Spark because each executor receives many 'partitions' and these cannot hold shared references to local results previously computed by the executor.
Instead, we let a single executor (the driver) build the look-up table and broadcast it to all other executors.
Each executor receives this table and an even distribution of the scalars, and computes all its assigned scalar multiplications.
The zkSNARK setup receives as input an R1CS instance φ = (k, N, M, a, b, c) and produces a proving key pk and a verification key vk.Informally, the protocol has three stages: (i) evaluate the polynomials A, B, C at a random element t, where A, B, C are from the QAP instance Φ = (k, N, M, A, B, C, D) corresponding to φ ; (ii) compute certain random linear combinations of these; (iii) compute encodings of corresponding vectors.
The second stage is straightforward to distribute, and the third stage is an instance of fixMSM (see §4.2.2).
Thus here we discuss efficient distribution of the first stage only.Recall from the QAP instance reduction (in §2.3) that A = (A 0 , . . . , A N ) where A i is the polynomial of degree < M that interpolates over D the i-th row of the matrix a; similarly for each B and C with regard to b and c. Focusing on a for simplicity and letting L 1 , . . . , L M be the Lagrange interpolants for the set D (i.e., L j evaluates to 1 at the j-th element of D and to 0 everywhere else in D), the task we need to solve in a distributed way is:in: a ∈ F (1+N)×M and t ∈ F out: (A i (t)) N i=0 where A i (t) := ∑ M j=1 a i, j L j (t)The parameters N and M are big enough such that no single machine can store any vector of length N or M.In both serial zkSNARK systems and in our distributed system, the first step is to compute (L j (t)) M j=1 .
We do so via the distributed Lag protocol described in §4.1.3, which computes and stores (L j (t)) M j=1 in an RDD.
We now focus on the remainder of the task.A key property of the matrix a exploited in serial zk-SNARK systems is its sparsity; that is, a contains very few non-zero entries.
This enables the serial algorithm to iterate through every nonzero a i, j , look up the value L j (t), and add a i, j L j (t) to the i-th entry in A(t).
Distributing this approach in the natural way, however, results in a solution that is both inefficient in time and cannot scale to large N and M, as discussed next.Strawman.
Represent a = (a i, j ) i, j and (L j (t)) j as two RDDs and perform the following computations: 1.
Join the set (a i, j ) i, j with the set (L j (t)) j by index j. 2.
Map each pair (a i, j , L j (t)) to its product a i, j L j (t).
i to get (∑ M j=1 a i, j L j (t)) N i=0 .
When running this computation, we encounter notable issues at every step: the set of joined pairs (a i, j , L j (t)) is unevenly distributed among executors, the executors take drastically differing amounts of time to perform the pair evaluations, and a small set of executors quickly exceed memory bounds from insufficient heap space.Our problems lie in that, while the matrix a is sparse, its columns are merely almost sparse: most columns are sparse, but a few are dense.
This occurs when in an R1CS instance φ some constraints "touch" many variables.
This is not a rarity, but a common occurrence in typical constraint systems.
E.g., consider the basic linear-algebraic operation of computing the dot product between a large variable vector and a large constant vector.
The single constraint in φ that captures this dot product has as many variables as the number of non-zero constants in the constant vector, inducing a dense column.The default (hash-based) partitioner of the join algorithm maps all entries in a column to the same executor, resulting in executors for dense columns becoming stragglers.
While there exist alternative join algorithms to handle load imbalances, like blockjoin and skewjoin [6], these do not perform well, as we now explain.First, blockjoin replicates each entry in one RDD (the one for (L j (t)) j ) in the hopes that when joining with the other RDD (the one for (a i, j ) i, j ) the partitions will be more evenly distributed.
However, in our setting we cannot afford blowing up the size of the first RDD.Second, skewjoin takes a more fine-grained approach, by computing statistics of the second RDD and using it to calculate the replication factor for each entry in the first RDD.
While the memory footprint is smaller, it remains undesirable.A problem in both approaches is that replicating entries entails changing the keys of the two RDDs, by first adding counters to each key before joining and then removing these after joining.
Each of these changes requires expensive shuffles to relocate keys to the correct partitions based on their hash.
A second inefficiency comes from performing a single monolithic join on the two (modified) RDDs, costing significant working memory.We circumvent all these problems via systematic twopart solution tailored to our setting, as described below.
(And only briefly mention that the foregoing skewjoin approach does not scale beyond 50 million constraints on even 128 executors and is twice as slow as our solution.)
Part 1: identify dense vectors.
Before running the setup, DIZK runs a lightweight, distributed computation to identify the columns that have many non-zero elements and annotates them for Part 2.
Using a straightforward map and reduce computation would also result in stragglers because of the dense columns.
DIZK avoids stragglers as follows.
Suppose that the matrix a is stored as an RDD with partitions.
First, DIZK assigns each partition to a random executor.
Second, each executor computes, for every column j, the number of non-zero elements it receives.
Third, the executors run a shuffle, during which the elements for the same column go to the same executor.
Finally, each executor computes the final count for its assigned columns.
Thus even dense columns will have at most values to aggregate, avoiding stragglers.DIZK identifies which columns have more than a threshold of non-zero elements and annotates them for Part 2.
We heuristically set the threshold to be √ M.
As a is overall sparse, there are not many dense constraints.Let J a be the set of indices j identified as dense.
Part 2: employ a hybrid solution.
DIZK now executes two jobs: one for the few dense columns, and one for the many sparse columns.
The first computation filters each dense column into multiple partitions, so that no executor deals with an entire dense column but only with a part of it, and evaluates the joined pairs.
The second computation is the strawman above, limited to indices not in J a .
We do so without having to re-key RDDs or incur any replication.
In more detail, the computation is:1.
For all dense column indices j ∈ J a :(a) filter a by index j to obtain column a j as an RDD; (b) join the RDD (a i, j ) i, j with L j (t) for j;(c) map each pair (a i, j , L j (t)) to its product a i, j L j (t).
2.
Join the set (a i, j ) i, j / ∈J a with L j (t) by index j. 3.
Map each pair (a i, j , L j (t)) to its evaluation a i, j L j (t).
4.
Union (a i, j L j (t)) j∈J a with (a i, j L j (t)) j / ∈J a .
5.
Reduce all a i, j L j (t) by i to get (A i (t)) N i=0 .
The zkSNARK prover receives a proving key pk, input x in F k , and witness w in F N−k , and samples a proof π.
The protocol has two stages: (i) extend the x-witness w for the R1CS instance φ to a x-witness (w, h) for the QAP instance Φ; (ii) use x, w, h and additional randomness to compute certain linear combinations of pk.
The second stage is an instance of varMSM (see §4.2.1).
Thus here we discuss efficient distribution of the first stage only.Recall from the QAP witness reduction (in §2.3) that h is the vector of coefficients of the polynomial H(X) of degree less than M − 1 that equals the ratio(∑ N i=0 A i (X)z i ) · (∑ N i=0 B i (X)z i ) − ∑ N i=0 C i (X)z i Z D (X).
Let us focus for simplicity on computing the evaluation of the polynomial A z (X) := ∑ N i=0 A i (X)z i on D, which is one of the terms in the numerator.
Since the evaluation of A i on D equals the i-th row of a, the task that needs to be solved in a distributed way is the following.in: a ∈ F (1+N)×M and z ∈ F 1+N out: (∑ N i=0 a i, j z i ) M j=1Again, the parameters N and M are huge, so no single machine can store an array with N or M field elements.Strawman.
Encode a = (a i, j ) i, j and z = (z i ) i as two RDDs and perform the following distributed computation:1.
Join the set (a i, j ) i, j and the set (z i ) i by the index i.2.
Map each (a i, j , z i ) pair to their product a i, j z i .3.
Reduce the evaluations by index j to get(∑ N i=0 a i, j z i ) M j=1.
When running this computation, we ran into a stragglers problem that is the converse of that described in §5: while matrix a is sparse, its rows are almost sparse because, while most rows are sparse, some rows are dense.
The join overloaded the executors assigned to dense rows.
The reason underlying the problem is also the converse: some variables participate in many constraints.
This situation too is a common occurrence in R1CS instances.
For example, the constant value 1 is used often (e.g., every constraint capturing boolean negations) and this constant appears as an entry in z.Generic solutions for load imbalances like skewjoin [6] were not performant for the same reasons as in §5.
Our approach.
We solve this problem via a two-part solution analogous to that in §5, with the change that the computation is now for rows instead of columns.
The dense vectors depend on the constraints alone so they do not change during proving, even for different inputs x. Hence, Part 1 runs once during setup, and not again during proving (only Part 2 runs then).
We study two applications for our distributed zkSNARK: (1) authenticity of edited photos [53] (see §7.1); and (2) integrity of machine learning models (see §7.2).
In both cases the application consists of algorithms for two tasks.
One task is expressing the application predicate as an R1CS instance, which means generating a certain set of constraints (ideally, as small as possible) to pass as input to the setup.
The other task is mapping the application inputs to a satisfying assignment to the constraints, to pass as input to the prover.Recall that our distributed zkSNARK expects the R1CS instance (set of constraints) and witness (assignment) to be distributed data structures (see §3).
In both applications, we distribute the constraint generation and witness generation across multiple machines, which for sufficiently large instance sizes, confers greater efficiency.
Authenticity of photos is crucial for journalism and crime investigations but is difficult to ensure due to powerful digital editing tools.
A recent paper, PhotoProof [53], proposes an approach that relies on a combination of special signature signing cameras and zkSNARKs to prove, in zero knowledge, that an edited image was obtained from a signed (and thus valid) input image only according to a set of permissible transformations.
More precisely, the camera actually signs a commitment to the input image, and this commitment and signature also accompany the edited image, and thus can be verified separately.We benchmark our system on this application because the original PhotoProof relies on monolithic zkSNARK implementations and is thus limited to small photo sizes.
Our system's scalability allows for proofs of relatively large images (see §11).
Below we describe the three transformations that we implemented: crop, rotation, and blur; the first two are also implemented in [53], while the third one is from [49].
Throughout, we consider images of dimension r × c that are black and white, which means that each pixel is an integer between 0 and 255; we represent such an image as a list of rc field elements each storing a pixel.
Our algorithms can be extended to color images via RGB representation, but we do not do so in this work.Crop.
The crop transformation is specified by a r × c mask and maps an input r × c image into an output r × c image by keeping or zeroing out each pixel according to the corresponding bit in the mask.
This choice is realized via a MUX gadget controlled by the mask's bit.
We obtain that the number of constraints is rc and the number of variables is 3rc.
In our implementation, we distribute the generation of constraints and variable assignment by individually processing blocks of pixels.Rotation.
The rotation transformation is specified by an angle θ ∈ [0, π/4] and maps a pixel in position (x, y) to cos θ − sin θ sin θ cos θ (x, y); this rotates the image by angle θ around (0, 0).
Some pixels go outside the image and are thus lost, while new pixels appear and are set to zero.We follow the approach of [53], and use the method of rotation by shears [54], which uses the identity cos θ − sin θsin θ cos θ = 1 − tan(θ /2) 0 1 1 0 sin θ 1 1 − tan(θ /2) 0 1 .
The first is a shear by row, the second a shear by column, and the third again a shear by row.
Each shear is performed by individually invoking a barrel shifter to every row or column, with the correct offset.
For more details on how to compute the offsets and the shear transformations, please refer to the full version.In our implementation, we distribute the generation of constraints and variable assignment by distributing each shear, which can be done by generating each barrel shifter's constraints and variable assignment in parallel.
Blur.
The blur transformation is specified by a position (x, y), height u, and width v; it maps an input r × c image into an output r × c image in which Gaussian blur has been applied to the u × v rectangle whose bottomleft corner is at (x, y).
More precisely, we approximate Gaussian blur via three sequential box blurs, which are further reduced to six directional blurs [49].
To realize this transformation as constraints, we need to verify, for each of the uv positions in the selected region and for each of the 6 directional blurs, that the new pixel is the correct (rounded) average of the 2r + 1 pixels in the old image.
For more details on the algorithm, please refer to the full version.In our implementation, since the value of each new pixel only depends on several surrounding pixels, we distribute the generation of constraints and witnesses by pixel blocks in the selected region.
Suppose that a hospital owns sensitive patient data, and a researcher wishes to build a (public) model by running a (public) training algorithm on this sensitive data.
The hospital does not want (or legally cannot) release the data; on the other hand, the researcher wants others to be able to check the integrity of the model.
One way to resolve this tension is to have the hospital use a zkSNARK to prove that the model is the output obtained when running it on the sensitive data.
1 In this paper, we study two operations: linear regression and covariance matrix calculation (an important subroutine for classification).
Both rely on linear algebraic operations that are simple to express as constraints and to distribute across machines.
Linear regression.
Least-squares linear regression is a popular supervised machine learning training algorithm that models the relationship between variables as linear.
The input is a labeled dataset D = (X,Y ) where rows of X ∈ R n×d and Y ∈ R n×1 are the observations' independent and dependent variables.
Assuming that Xw ≈ Y for some w ∈ R d×1 , the algorithm's goal is to find such a w that minimizes the mean squared-error loss.
The solution to the optimization problem is w = (X T X) −1 X T Y .
While the formula to compute w uses a matrix inversion, one can easily check correctness of w by verifying that X T Xw = X T y.
The problem is thus reduced to checking matrix multiplications, which can be easily expressed and distributed as we now describe.We generate the constraints and variable assignments by following a distributed block-based algorithm for matrix multiplication [24,50,70].
Such an algorithm splits the output matrix into blocks, and assigns and shuffles the data needed to generate each block to the same machine.
Each block can independently generate its constraints and variable assignments after receiving the necessary values.
This simple approach works well for us because memory usage is dominated by the number of constraints and variables rather than the size of the input/output matrices.Covariance matrix.
Computing covariance matrices is an important subroutine in classification algorithms such as Gaussian naive Bayes and linear discriminant analysis [18].
These algorithms classify observations into discrete classes by constructing a probability distribution for each class.
This reduces to computing the mean and covariance matrix for each class of sample points.Suppose that{x i ∈ R d×1 } i=1.
.
n is an input data set from a single class.
Its covariance matrix is M := 1 n−1 ∑ n i=1 (x i − ¯ x)(x i − ¯ x) T ∈ R d×d , where ¯ x := ( 1 n ∑ n i=1 x i ) ∈ R d×1is the average of the n observations.To verify M, we first check the correctness of ¯ x by individually checking each of the d entries; for each entry we use the same approach as in the case of blur (in §7.1).
Then, we check correctness of each matrix multiplication (x i − ¯ x)(x i − ¯ x) T , using the same distribution technique from linear regression.
Finally, we check correctness of the 'average' of the n resulting matrices.
We implemented the distributed zkSNARK in ≈ 10K lines of Java code over Apache Spark [2], a popular cluster computing framework.
All data representations are designed to fit within the Spark computation model.
For example, we represent an R1CS instance φ = (k, N, M, a, b, c) via three RDDs, one for each of the three matrices a, b, c, and each record in an RDD is a tuple ( j, (i, v)) where v is the (i, j)-th entry of the matrix.
(Recall from §2.2 that a, b, c are coefficient matrices that determine all constraints of the instance.)
Since DIZK deals with large instances, we carefully adjust the RDD partition size such that each partition fits on an executor's heap space.
We evaluated DIZK on Amazon EC2 using r3.large instances (2 vCPUs, 15 GiB of memory) and r3.8xlarge instances (32 vCPUs, 244 GiB of memory).
For singlemachine experiments, we used one r3.large instance.For distributed experiments, we used a cluster of ten r3.8xlarge instances for up to 128 executors, and a cluster of twenty r3.8xlarge for 256 executors.We instantiate the zkSNARK via a 256-bit BarretoNaehrig curve [8], a standard choice in prior zkSNARK implementations.
This means that G 1 and G 2 are elliptic curve groups of a prime order p of 256 bits, and the scalar field F has this same size.An important technicality is that we cannot rely on curves used in prior zkSNARK works, because they do not support the large instance sizes in this work, as we now explain.
To allow for efficient implementations of the setup and the prover one needs a curve in which the group order p is such that p − 1 is divisible by 2 a , where 2 a is larger than the maximum instance size to be supported [11].
As the instance sizes that we support are in the billions (at least 2 30 ), we need, say, a ≥ 40.
We thus generated (by modifying the sampling algorithm in [8]) a 256-bit Barreto-Naehrig curve with a = 50, which suffices for our purposes.
The curve is E/F q : y 2 = x 3 + 13 with q = 178558083348049028502 609238317702557737797405798625193380108245358 56509878273, and its order is p = 17855808334804902 850260923831770255773646114952324966112694569 107431857586177.
We evaluated our distributed zkSNARK and show that: 1.
We support instances of more than a billion gates, a significant improvement over serial implementations, which exceed memory bounds at 10-20 million gates.
2.
Fixing a number of executors on the cluster and letting the instance size increase (from several millions to over a billion), the running time of the setup and prover increases close to linearly as expected, demonstrating scalability over this range of instance sizes.
3.
Fixing an input size and increasing the number of executors, the running time of the setup and prover decreases close to linearly as expected, demonstrating parallelization over this range of executors.
In the next few sub-sections we support these findings.
We evaluate our distributed implementation of the zk-SNARK setup and prover.
Below we use 'instance size' to denote the number of constraints M in a R1CS instance.
2 First, we measure the largest instance size (as a power of 2) that is supported by:• the serial implementation of Groth's protocol [59], a state-of-the-art zkSNARK library; and • our distributed implementation of the same protocol.
(Also, we plot the same for the serial implementation of PGHR [55]'s protocol in libsnark, a common zk-SNARK choice.)
Data from our experiments, reported in Fig. 4, shows that using more executors allows us to support larger instance sizes, in particular supporting billions of constraints with sufficiently many executors.
Instances of this size are much larger than what was previously possible via serial techniques.Next, we measure the running time of the setup and the prover on an increasing number of constraints and with an increasing number of executors.
Data from our experiments, reported in Fig. 5, shows that (a) for a given number of executors, running times increase nearly linearly as expected, demonstrating scalability over a wide range of instance sizes; (b) for a given instance size, running times decrease nearly linearly as expected, demonstrating parallelization over a wide range of number of executors.Finally, we again stress that we do not evaluate the zk-SNARK verifier because it is a simple and fast algorithm that can be run even on a smartphone.
Thus, we simply use libsnark's implementation of the verifier [59], whose running time is ≈ 2 ms + 0.5 µs · k where k is the number of field elements in the R1CS input (not a large number in typical applications).
We separately evaluate the performance and scalability of key components of our distributed SNARK implementation: the field algorithms for Lag and FFT ( §10.2.1) and group algorithms for fixMSM and varMSM ( §10.2.2).
We single out these components since they are starting points to distribute other similar proof systems.
We evaluate our implementation of distributed algorithms for Lag (used in the setup) and FFT (used in the prover).
For the scalar field F, we measure the running time, for an increasing instance size and increasing number of executors in the cluster.
Data from our experiments, reported in Fig. 6, shows that our implementation behaves as desired: for a given number of executors, running times increase close to linearly in the instance size; also, for a given instance size, running times decrease close to linearly as the number of executors grow.
We evaluate our implementation of distributed algorithms for fixMSM (used in the setup) and varMSM (used in the prover).
For each of the elliptic-curve groups G 1 and G 2 , we measure the total running time, for increasing instance size and number of executors in the cluster.
Data from our experiments, reported in Fig. 7, shows that our implementation behaves as desired: for a given number of executors, running times increase close to linearly in the instance size; also, for a given instance size, running times decrease close to linearly in the number of executors.
We ran experiments (32 and 64 executors for all feasible instances) comparing the performance of the setup and prover with two implementations: (1) the implementation that is part of DIZK, which has optimizations described in the design sections ( §4, §5, §6); and (2) an implementation that does not employ these optimizations (e.g., uses skewjoin instead of our solution, and so on).
Our data established that our techniques allow achieving instance sizes that are 10 times larger, at a cost that is 2-4 times faster in the setup and prover.
We evaluated the performance of constraint and witness generation for the applications described in §7.
Fig. 9 shows, for various instances of our applications, the number of constraints and the performance of constraint and witness generation.
In all cases, witness generation is markedly more expensive than constraint generation due to data shuffling.
Either way, both costs are insignificant when compared to the corresponding costs of the SNARK setup and prover.
Hence, we did not try to optimize this performance further.
Fig. 8 shows the scaling behavior of constraint and witness generation for one application, linear regression.
Fig. 8a and Fig. 8b show the time for constraint and witness generation when fixing the number of executors and increasing the instance size (as determined by the number of constraints); the graphs show that time scales nearly linearly, which means that the algorithm parallelizes well with respect to instance size.
Fig. 8c and Fig. 8d show the time for constraint and witness generation when fixing the instance size and increasing the number of executors; the graphs show that the system scales well as the number of executors are increased (at some point, a fixed overhead dominates, so the time flattens out).
Optimization and implementation of proof systems.
Recent years have seen beautiful works that optimize and implement information-theoretic and cryptographic proof systems.
These proof systems enable a weak verifier (e.g., a mobile device) to outsource an expensive computation to a powerful prover (e.g., a cloud provider).
For example, doubly-efficient interactive proofs for parallel computation [40] have been optimized and implemented in software [30,68,66,67,77] and hardware [73,74].
Also, batch arguments based on Linear PCPs [45] have attained remarkable efficiency [60,62,63,61,72,22].
Some proof systems, such as zkSNARKs, also provide zero knowledge, which is important for applications [33,10,75,31,46,47,53,36].
Approaches to construct zkSNARKs include using PCPs [52,13] or Linear PCPs [41,51,20,37].
An implementation following the first approach has been attained [9], but most other implementations follow the second approach [55,11,15,48,78,31].
The zkSNARK setup and prover in prior implementations run on a single machine.Some recent work explores zero knowledge proofs based not on probabilistic checking techniques and do not offer constant-size proofs, but whose provers are cheaper (and need no setup).
See [39] and references therein.Proof systems & distributed systems.
While prior work does not distribute the prover's computation across a cluster, some prior work did show how even monolithic provers can be used to prove correct execution of distributed computations.
For example, the system Pantry [22] transforms a proof system such as a batch argument or a zkSNARK into an interactive argument for outsourcing MapReduce computations (though it does not preserve zero knowledge).
Also, the framework of Proof-Carrying Data [26,27] allows reasoning, and proving the correctness of, certain distributed computations via the technique of recursive proof composition on SNARKs.
This technique can be used to attain zkSNARKs for MapReduce [28], and also for 'breaking up' generic computation into sub-computations while proving each correct [14,31].
Our work is complementary to the above approaches: prior work can leverage our distributed zkSNARK (instead of a 'monolithic' one) to feasibly support larger instance sizes.
For instance, Pantry can use our distributed zkSNARK as the starting point of their transformation.Trusted hardware.
If one assumes trusted hardware, achieving 'zero knowledge proofs', even ones that are short and cheap to verify, is easier.
For example, trusted hardware with attested execution (e.g. Intel SGX) suffices [69,56].
DIZK does not assume trusted hardware, and thus protects against a wider range of attackers at the prover than these approaches.While we are excited about scaling to larger circuits, zkSNARKs continue to suffer from important limitations.First, even if DIZK enables using zkSNARKs for much larger circuits than what was previously possible, doing so is still very expensive (we resort to using a compute cluster!)
and so scaling to even larger sizes (say, hundreds of billions of gates) requires resources that may even go beyond those of big clusters.
Making zkSNARKs more efficient overall (across all circuit sizes) remains a challenging open problem.Second, the zkSNARKs that we study, like most other 'practical' ones, require a trusted party to run a setup procedure that uses secret randomness to sample certain public parameters.
This setup is needed only once per circuit, but its time and space costs also grow with circuit size.
While DIZK does provide an efficient distributed setup (in addition to the same for the prover), performing this setup in practice is challenging due to many real-world security concerns.
Currently-deployed zkSNARKs have relied on Secure Multi-party Computation "ceremonies" for this [12,21], and it remains to be studied if those techniques can be distributed by building on our work.Our outlook is optimistic as the area of efficient proof systems sees tremendous progress [76], not only in terms of real-world deployment [7] but also for zkSNARK constructions that, while still somewhat expensive, rely only on public randomness (no setup is needed) [13,9].
We design and build DIZK, a distributed zkSNARK system.
While prior systems only support circuits of up to 10-20 million gates (at a cost of 1 ms per gate in the prover), DIZK leverages the combined CPU and memory resources in a cluster to support circuits of up to billions of gates (at a cost of 10 µs per gate in the prover).
This is a qualitative leap forward in the capabilities zkSNARKs, a recent cryptographic tool that has garnered much academic and industrial interest.Setup.
The setup S receives an R1CS instance φ = (k, N, M, a, b, c) and then samples a proving key pk and a verification key vk as follows.
First, S reduces the R1CS instance φ to a QAP instance Φ = (k, N, M, A, B, C, D) by running the algorithm qapI.
Then, S samples random elements t, α, β , γ, δ in F (this is the randomness that must remain secret).
After that, S evaluates the polynomials in A, B, C at the element t, and computes Finally, the setup algorithm computes encodings of these elements and outputs pk and vk defined as follows:pk := [α] 1 , [β ] 1 , [δ ] 1 [β ] 2 , [δ ] 2 , [A(t)] 1 , [B(t)] 1 [B(t)] 2 , [K pk (t)] 1 [Z(t)] 1 , vk :=(e (α, β ) , [γ] 2 , [δ ] 2 , [K vk (t)] 1 ) .
Prover.
The prover P receives a proving key pk, input x in F k , and witness w in F N−k , and then samples a proof π as follows.
First, P extends the x-witness w for the R1CS instance φ to a x-witness (w, h) for the QAP instance Φ by running the algorithm qapW.
Then, P samples random elements r, s in F (this is the randomness that imbues the proof with zero knowledge).
Next, letting z := 1xw, P computes three encodings obtained as follows[A r ] 1 :=[α] 1 + N ∑ i=0 z i [A i (t)] 1 + r[δ ] 1 , [B s ] 1 :=[β ] 1 + N ∑ i=0 z i [B i (t)] 1 + s[δ ] 1 [B s ] 2 :=[β ] 2 + N ∑ i=0 z i [B i (t)] 2 + s[δ ] 2 .
Then P uses these two compute a fourth encoding:[K r,s ] 1 := s[A r ] 1 + r[B s ] 1 − rs[δ ] 1 + N ∑ i=k+1 z i [K pk i (t)] 1 + M−2 ∑ j=0 h j [Z j (t)] 1 .
The output proof is π := ([A r ] 1 , [B s ] 2 , [K r,s ] 1 ).
Verifier.
The verifier V receives a verification key vk, input x in F k , and proof π, and, letting x 0 := 1, checks that the following holds: e ([A r ] 1 , [B s ] 2 ) = e (α, β ) +e k ∑ i=0 x i [K vk i (t)] 1 , [γ] 2 + e ([K r,s ] 1 , [δ ] 2 ) .
The authors are grateful to Jiahao Wang for participating in early stages of this work.
This work was supported by the Intel/NSF CPS-Security grants #1505773 and #20153754, the UC Berkeley Center for Long-Term Cybersecurity, and gifts to the RISELab from Amazon, Ant Financial, CapitalOne, Ericsson, GE, Google, Huawei, IBM, Intel, Microsoft, and VMware.
The authors thank Amazon for donating compute credits to RISELab, which were extensively used in this project.
