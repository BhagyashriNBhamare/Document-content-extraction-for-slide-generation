We present a novel learning framework for pipeline models aimed at improving the communication between consecutive stages in a pipeline.
Our method exploits the confidence scores associated with outputs at any given stage in a pipeline in order to compute prob-abilistic features used at other stages downstream.
We describe a simple method of integrating probabilistic features into the linear scoring functions used by state of the art machine learning algorithms.
Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models , especially when upstream stages in the pipeline exhibit low accuracy.
Machine learning algorithms are used extensively in natural language processing.
Applications range from fundamental language tasks such as part of speech (POS) tagging or syntactic parsing, to higher level applications such as information extraction (IE), semantic role labeling (SRL), or question answering (QA).
Learning a model for a particular language processing problem often requires the output from other natural language tasks.
Syntactic parsing and dependency parsing usually start with a textual input that is tokenized, split in sentences and POS tagged.
In information extraction, named entity recognition (NER), coreference resolution, and relation extraction (RE) have been shown to benefit from features that use POS tags and syntactic dependencies.
Similarly, most SRL approaches assume a parse tree representation of the input sentences.
The common practice in modeling such dependencies is to use a pipeline organization, in which the output of one task is fed as input to the next task in the sequence.
One advantage of this model is that it is very simple to implement; it also allows for a modular approach to natural language processing.
The key disadvantage is that errors propagate between stages in the pipeline, significantly affecting the quality of the final results.
One solution is to solve the tasks jointly, using the principled framework of probabilistic graphical models.
Sut- ton et al. (2004) use factorial Conditional Random Fields (CRFs) ( Lafferty et al., 2001) to jointly predict POS tags and segment noun phrases, improving on the cascaded models that perform the two tasks in sequence.
Wellner et al. (2004) describe a CRF model that integrates the tasks of citation segmentation and citation matching.
Their empirical results show the superiority of the integrated model over the pipeline approach.
While more accurate than their pipeline analogues, probabilistic graphical models that jointly solve multiple natural language tasks are generally more demanding in terms of finding the right representations, the associated inference algorithms and their computational complexity.
Recent negative results on the integration of syntactic parsing with SRL ( Sutton and McCallum, 2005) provide additional evidence for the difficulty of this general approach.
When dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is to solve the tasks separately and integrate the constraints in a linear programming formulation, as proposed by Roth and Yih (2004) for the simultaneous learning of named entities and relations between them.
More recently, Finkel et al. (2006) model the linguistic pipelines as Bayesian networks on which they perform Monte Carlo inference in order to find the most likely output for the final stage in the pipeline.In this paper, we present a new learning method for pipeline models that mitigates the problem of error propagation between the tasks.
Our method exploits the probabilities output by any given stage in the pipeline as weights for the features used at other stages downstream.
We show a simple method of integrating probabilistic features into linear scoring functions, which makes our approach applicable to state of the art machine learning algorithms such as CRFs and Support Vector Machines (Vapnik, 1998;Schölkopf and Smola, 2002).
Experimental results on dependency parsing and named entity recognition show useful improvements over the baseline pipeline models, especially when the basic pipeline components exhibit low accuracy.
We consider that the task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y(x).
Each input x is also associated with a different set of outputs z ∈ Z(x) for which we are given a probabilistic confidence measure p (z|x).
In a pipeline model, z would correspond to the annotations performed on the input x by all stages in the pipeline other than the stage that produces y. For example, in the case of dependency parsing, x is a sequence of words, y is a set of word-word dependencies, z is a sequence of POS tags, and p(z|x) is a measure of the confidence that the POS tagger has in the output z. Let φ be a representation function that maps an example (x, y, z) to a feature vector φ(x, y, z) ∈ R d , and w ∈ R d a parameter vector.
Equations (1) and (2) below show the traditional method for computing the optimal outputˆyoutputˆ outputˆy in a pipeline model, assuming a linear scoring function defined by w and φ.ˆ y(x) = argmax y∈Y(x) w · φ(x, y, ˆ z(x))(1)ˆ z(x) = argmax z∈Z(x) p(z|x)(2)The weight vector w is learned by optimizing a predefined objective function on a training dataset.In the model above, only the best annotationˆzannotationˆ annotationˆz produced by upstream stages is used for determining the optimal outputˆyoutputˆ outputˆy.
However, ˆ z may be an incorrect annotation, while the correct annotation may be ignored because it was assigned a lower confidence value.
We propose exploiting all possible annotations and their probabilities as illustrated in the new model below:ˆ y(x) = argmax y∈Y(x) w · ψ(x, y) (3) ψ(x, y) = z∈Z(x) p(z|x) · φ(x, y, z) (4)In most cases, directly computing ψ(x, y) is unfeasible, due to a large number of annotations in Z(x).
In our dependency parsing example, Z(x) contains all possible POS taggings of sentence x; consequently its cardinality is exponential in the length of the sentence.
A more efficient way of computing ψ(x, y) can be designed based on the observation that most components φ i of the original feature vector φ utilize only a limited amount of evidence from the example (x, y, z).
We define (˜ x, ˜ y, ˜ z) ∈ F i (x, y, z) to capture the actual evidence from (x, y, z) that is used by one instance of feature function φ i .
We call (˜ x, ˜ y, ˜ z) a feature instance of φ i in the example (x, y, z).
Correspondingly, F i (x, y, z) is the set of all feature instances of φ i in example (x, y, z).
Usually, φ i (x, y, z) is set to be equal with the number of instances of φ i in example (x, y, z), i.e. φ i (x, y, z) = |F i (x, y, z)|.
Table 1 illustrates three feature instances (˜ x, ˜ y, ˜ z) generated by three typical dependency parsing features in the example from Figure 1.
Because the same feature may be instantiated multi- ple times in the same example, the components of each feature instance are annotated with their positions relative to the example.
Given these definitions, the feature vector ψ(x, y) from (4) can be rewritten in a component-wise manner as follows:φ 1 : DT → NN φ 2 : NNS → thought φ 3 : be ← iñiñ y 10 → 11 2 → 4 7 ← 9 ˜ z DT 10 NN 11 NNS 2 ˜ x thought 4 be 7 in 9 |F i | O(|x| 2 ) O(|x|) O(1)ψ(x, y) = [ψ 1 (x, y) . . . ψ d (x, y)](5)ψ i (x, y) = z∈Z(x) p(z|x) · φ i (x, y, z) = z∈Z(x) p(z|x) · |F i (x, y, z)| = z∈Z(x) p(z|x) (˜ x,˜ y,˜ z)∈F i (x,y,z) 1 = z∈Z(x) (˜ x,˜ y,˜ z)∈F i (x,y,z) p(z|x) = (˜ x,˜ y,˜ z)∈F i (x,y,Z(x)) z∈Z(x),z⊇˜zz⊇˜z p(z|x)where F i (x, y, Z(x)) stands for:F i (x, y, Z(x)) = z∈Z(x) F i (x, y, z)We introduce p(˜ z|x) to denote the expectation:p(˜ z|x) = z∈Z(x),z⊇˜zz⊇˜z p(z|x)Then ψ i (x, y) can be written compactly as:ψ i (x, y) = (˜ x,˜ y,˜ z)∈F i (x,y,Z(x)) p(˜ z|x)(6)The total number of terms in (6) is equal with the number of instantiations of feature φ i in the example (x, y) across all possible annotations z ∈ Z(x), i.e. |F i (x, y, Z(x))|.
Usually this is significantly smaller than the exponential number of terms in (4).
The actual number of terms depends on the particular feature used to generate them, as illustrated in the last row of Table 1 for the three features used in dependency parsing.
The overall time complexity for calculating ψ(x, y) also depends on the time complexity needed to compute the expectations p(˜ z|x).
When z is a sequence, p(˜ z|x) can be computed efficiently using a constrained version of the forwardbackward algorithm (to be described in Section 3).
When z is a tree, p(˜ z|x) will be computed using a constrained version of the CYK algorithm (to be described in Section 4).
The time complexity can be further reduced if instead of ψ(x, y) we use its subcomponentˆψsubcomponentˆ subcomponentˆψ(x, y) that is calculated based only on instances that appear in the optimal annotationˆzannotationˆ annotationˆz:ˆ ψ(x, y) = [ ˆ ψ 1 (x, y) . . . ˆ ψ d (x, y)] (7) ˆ ψ i (x, y) = (˜ x,˜ y,˜ z)∈F i (x,y,ˆ z) p(˜ z|x)(8)The three models are summarized in Table 2 below.In the next two sections we illustrate their applica- tion to two common tasks in language processing: dependency parsing and named entity recognition.ˆ y(x) = argmax y∈Y(x) w · φ(x, y) M 1 φ(x, y) = φ(x, y, ˆ z(x)) ˆ z(x) = argmax z∈Z(x) p(z|x) ˆ y(x) = argmax y∈Y(x) w · ψ(x, y) M 2 ψ(x, y) = [ψ 1 (x, y) . . . ψ d (x, y)] ψ i (x, y) = (˜ x,˜ y,˜ z)∈F i (x,y,Z(x)) p(˜ z|x) ˆ y(x) = argmax y∈Y(x) w · ˆ ψ(x, y) M 3 ˆ ψ(x, y) = [ ˆ ψ 1 (x, y) . . . ˆ ψ d (x, y)] ˆ ψ i (x, y) = (˜ x,˜ y,˜ z)∈F i (x,y,ˆ z) p(˜ z|x) In a traditional dependency parsing pipeline (model M 1 in Table 2), an input sentence x is first augmented with a POS taggingˆztaggingˆ taggingˆz(x), and then processed by a dependency parser in order to obtain a dependency structurê y(x).
To evaluate the new pipeline models we use MSTPARSER 1 , a linearly scored dependency parser developed by McDonald et al. (2005).
Following the edge based factorization method of Eisner (1996), the score of a dependency tree in the first order version is defined as the sum of the scores of all edges in the tree.
Equivalently, the feature vector of a dependency tree is defined as the sum of the feature vectors of all edges in the tree:M 1 : φ(x, y) = u→v∈y φ(x, u → v, ˆ z(x)) M 2 : ψ(x, y) = u→v∈y ψ(x, u → v) M 3 : ˆ ψ(x, y) = u→v∈yˆψ u→v∈yˆ u→v∈yˆψ(x, u → v)For each edge u → v ∈ y, MSTPARSER generates features based on a set of feature templates that take into account the words and POS tags at positions u, v, and their left and right neighbors u ± 1, v ± 1.
For example, a particular feature template T used inside MSTPARSER generates the following POS bigram features:φ i (x, u → v, z) = 1, if z u , z v = t 1 , t 2 0, otherwisewhere t 1 , t 2 ∈ P are the two POS tags associated with feature index i. By replacing y with u → v in the feature expressions from Table 2, we obtain the following formulations:M 1 : φ i (x, u → v) = 1, ifˆzifˆifˆz u , ˆ z v = t 1 , t 2 0, otherwise M 2 : ψ i (x, u → v) = p(˜ z = t 1 , t 2 |x) M 3 : ˆ ψ i (x, u → v) = p(˜ z = t 1 , t 2 |x), ifˆzifˆifˆz u , ˆ z v = t 1 , t 2 0, otherwisewhere, following the notation from Section 2, ˜ z = z u , z v is the actual evidence from z that is used by feature i, andˆzandˆ andˆz is the top scoring annotation produced by the POS tagger.
The implementation in MSTPARSER corresponds to the traditional pipeline model M 1 .
Given a method for computing feature 1 URL: http://sourceforge.net/projects/mstparser probabilities p(˜ z = t 1 , t 2 |x), it is straightforward to modify MSTPARSER to implement models M 2 and M 3 -we simply replace the feature vectors φ with ψ andˆψandˆ andˆψ respectively.
As mentioned in Section 2, the time complexity of computing the feature vectors ψ in model M 2 depends on the complexity of the actual evidence˜zevidence˜ evidence˜z used by the features.
For example, the feature template T used above is based on the POS tags at both ends of a dependency edge, consequently it would generate |P| 2 features in model M 2 for any given edge u → v.There are however feature templates used in MST-PARSER that are based on the POS tags of up to 4 tokens in the input sentence, which means that for each edge they would generate |P| 4 ≈ 4.5M features.
Whether using all these probabilistic features is computationally feasible or not also depends on the time complexity of computing the confidence measure p(˜ z|x) associated with each feature.
The new pipeline models M 2 and M 3 require an annotation model that, at a minimum, facilitates the computation of probabilistic confidence values for each output.
We chose to use linear chain CRFs ( Lafferty et al., 2001) since CRFs can be easily modified to compute expectations of the type p(˜ z|x), as needed by M 2 and M 3 .
The CRF tagger was implemented in MAL-LET (McCallum, 2002) using the original feature templates from (Ratnaparkhi, 1996).
The model was trained on sections 2-21 from the English Penn Treebank ( Marcus et al., 1993).
When tested on section 23, the CRF tagger obtains 96.25% accuracy, which is competitive with more finely tuned systems such as Ratnaparkhi's MaxEnt tagger.We have also implemented in MALLET a constrained version of the forward-backward procedure that allows computing feature probabilities p(˜ z|x).
If˜zIf˜ If˜z = t i 1 t i 2 ...t i k specifies the tags at k positions in the sentence, then the procedure recomputes the α parameters for all positions between i 1 and i k by constraining the state transitions to pass through the specified tags at the k positions.
A similar approach was used by Culotta et al. in (2004) in order to associate confidence values with sequences of contiguous tokens identified by a CRF model as fields in an information extraction task.
The constrained procedure requires (i k − i 1 )|P| 2 = O(N |P| 2 ) multiplications in an order 1 Markov model, where N is the length of the sentence.
Because MSTPARSER uses an edge based factorization of the scoring function, the constrained forward procedure will need to be run for each feature template, for each pair of tokens in the input sentence x.
If the evidence˜zevidence˜ evidence˜z required by the feature template T constrains the tags at k positions, then the total time complexity for computing the probabilistic features p(˜ z|x) generated by T is:O(N 3 |P| k+2 ) = O(N |P| 2 ) · O(N 2 ) · O(|P| k ) (9)As mentioned earlier, some feature templates used in the dependency parser constrain the POS tags at 4 positions, leading to a O(N 3 |P| 6 ) time complexity for a length N sentence.
Experimental runs on the same machine that was used for CRF training show that such a time complexity is not yet feasible, especially because of the large size of P (46 POS tags).
In order to speed up the computation of probabilistic features, we made the following two approximations:1.
Instead of using the constrained forwardbackward procedure, we enforce an independence assumption between tags at different positions and rewrite p(˜ z = t i 1 t i 2 ...t i k |x) as:p(t i 1 t i 2 ...t i k |x) ≈ k j=1 p(t i j |x)The marginal probabilities p(t i j |x) are easily computed using the original forward and backward parameters as:p(t i j |x) = α i j (t i j |x)β i j (t i j |x) Z(x)This approximation eliminates the factor O(N |P| 2 ) from the time complexity in (9).2.
If any of the marginal probabilities p(t i j |x) is less than a predefined threshold (τ |P|) −1 , we set p(˜ z|x) to 0.
When τ ≥ 1, the method is guaranteed to consider at least the most probable state when computing the probabilistic features.
Looking back at Equation (4), this is equivalent with summing feature vectors only over the most probable annotations z ∈ Z(x).
The approximation effectively replaces the factor O(|P| k ) in (9) with a quasi-constant factor.The two approximations lead to an overall time complexity of O(N 2 ) for computing the probabilistic features associated with any feature template T , plus O(N |P| 2 ) for the unconstrained forward-backward procedure.
We will use M ′ 2 to refer to the model M 2 that incorporates the two approximations.
The independence assumption from the first approximation can be relaxed without increasing the asymptotic time complexity by considering as independent only chunks of contiguous POS tags that are at least a certain number of tokens apart.
Consequently, the probability of the tag sequence will be approximated with the product of the probabilities of the tag chunks, where the exact probability of each chunk is computed in constant time with the constrained forward-backward procedure.
We will use M ′′ 2 to refer to the resulting model.
MSTPARSER was trained on sections 2-21 from the WSJ Penn Treebank, using the gold standard POS tagging.
The parser was then evaluated on section 23, using the POS tagging output by the CRF tagger.
For model M 1 we need only the best output from the POS tagger.
For models M ′ 2 and M ′′ 2 we compute the probability associated with each feature using the corresponding approximations, as described in the previous section.
In model M ′′ 2 we consider as independent only chunks of POS tags that are 4 tokens or more apart.
If the distance between the chunks is less than 4 tokens, the probability for the entire tag sequence in the feature is computed exactly using the constrained forward-backward procedure.
Table 3 shows the accuracy obtained by models M 1 , M ′ 2 (τ ) and M ′′ 2 (τ ) for various values of the threshold parameter τ .
The accuracy is com- puted over unlabeled dependencies i.e. the percentage of words for which the parser has correctly identified the parent in the dependency tree.
The pipeline model M ′ 2 that uses probabilistic features outperforms the traditional pipeline model M 1 .
As expected, M ′′ 2 performs slightly better than M ′ 2 , due to a more exact computation of feature probabilities.
Overall, only by using the probabilities associated with the POS features, we achieve an absolute error reduction of 0.19%, in a context where the POS stage in the pipeline already has a very high accuracy of 96.25%.
We expect probabilistic features to yield a more substantial improvement in cases where the pipeline model contains less accurate upstream stages.
Such a case is that of NER based on a combination of POS and dependency parsing features.M 1 M ′ 2 (1) M ′ 2 (2) M ′ 2 (4) M ′′ 2(4 In Named Entity Recognition (NER), the task is to identify textual mentions of predefined types of entities.
Traditionally, NER is modeled as a sequence classification problem: each token in the input sentence is tagged as being either inside (I) or outside (O) of an entity mention.
Most sequence tagging approaches use the words and the POS tags in a limited neighborhood of the current sentence position in order to compute the corresponding features.
We augment these flat features with a set of tree features that are computed based on the words and POS tags found in the proximity of the current token in the dependency tree of the sentence.
We argue that such dependency tree features are better at capturing predicate-argument relationships, especially when they span long stretches of text.
Figure 2 shows a sentence x together with its POS tagging z 1 , dependency links z 2 , and an output tagging y. Assuming the task is to recognize mentions of people, the word sailors needs to be tagged as inside.
If we extracted only flat features using a symmetric window of size 3, the relationship between sailors and thought would be missed.
This relationship is useful, since an agent of the predicate thought is likely to be a person entity.
On the other hand, the nodes sailors and thought are adjacent in the dependency tree of the sentence.
Therefore, their relationship can be easily captured as a dependency tree feature using the same window size.For every token position, we generate flat features by considering all unigrams, bigrams and trigrams that start with the current token and extend either to the left or to the right.
Similarly, we generate tree features by considering all unigrams, bigrams and trigrams that start with the current token and extend in any direction in the undirected version of the dependency tree.
The tree features are also augmented with the actual direction of the dependency arcs between the tokens.
If we use only words to create n-gram features, the token sailors will be associated with the following features:• Flat: sailors, the sailors, S the sailors, sailors mistakenly, sailors mistakenly thought.
• Tree: sailors, sailors ← the, sailors → thought, sailors → thought ← must, sailors → thought ← mistakenly.We also allow n-grams to use word classes such as POS tags and any of the following five categories: 1C for tokens consisting of one capital letter, AC for tokens containing only capital letters, FC for tokens that start with a capital letter, followed by small letters, CD for tokens containing at least one digit, and CRT for the current token.
The set of features can then be defined as a Cartesian product over word classes, as illustrated in Fig- ure 3 for the original tree feature sailors → thought ← mistakenly.
In this case, instead of one completely lexicalized feature, the model will consider 12 different features such as sailors → VBD ← RB, NNS → thought ← RB, or NNS → VBD ← RB.
The pipeline model M 2 uses features that appear in all possible annotations z = z 1 , z 2 , where z 1 and z 2 are the POS tagging and the dependency parse respectively.
If the corresponding evidence is˜z is˜ is˜z = ˜ z 1 , ˜ z 2 , then:p(˜ z|x) = p(˜ z 2 |˜z|˜z 1 , x)p(˜ z 1 |x)For example, NNS 2 → thought 4 ← RB 3 is a feature instance for the token sailors in the annotations from Figure 2.
This can be construed as having been generated by a feature template T that outputs the POS tag t i at the current position, the word x j that is the parent of x i in the dependency tree, and the POS tagt k of another dependent of x j (i.e. t i → x j ← t k ).
The probability p(˜ z|x) for this type of features can then be written as:p(˜ z|x) = p(i → j ← k|t i , t k , x) · p(t i , t k |x)The two probability factors can be computed exactly as follows:1.
The M 2 model for dependency parsing from Section 3 is used to compute the probabilistic features ψ(x, u → v|t i , t k ) by constraining the POS annotations to pass through tags t i and t k at positions i and k.
The total time complexity for this step is O(N 3 |P| k+2 ).2.
Having access to ψ(x, u → v|t i , t k ), the factor p(i→j←k|t i , t k , x) can be computed in O(N 3 ) time using a constrained version of Eisner's algorithm, as will be explained in Section 4.1.3.
As described in Section 3.1, computing the expectation p(t i , t k |x) takes O(N |P 2 |) time using the constrained forward-backward algorithm.The current token position i can have a total of N values, while j and k can be any positions other than i. Also, t i and t k can be any POS tag from P. Consequently, the feature template T induces O(N 3 |P| 2 ) feature instances.
Overall, the time complexity for computing the feature instances generated by T is O(N 6 |P| k+4 ), as results from:O(N 3 |P| 2 ) · (O(N 3 |P| k+2 ) + O(N 3 ) + O(N |P| 2 ))While still polynomial, this time complexity is feasible only for small values of N .
In general, the time complexity for computing probabilistic features in the full model M 2 increases with both the number of stages in the pipeline and the complexity of the features.Motivated by efficiency, we decided to use the pipeline model M 3 in which probabilities are computed only over features that appear in the top scoring annotationˆzannotationˆ annotationˆz = ˆ z 1 , ˆ z 2 , wherê z 1 andˆzandˆ andˆz 2 represent the best POS tagging, and the best dependency parse respectively.
In order to further speed up the computation of probabilistic features, we made the following approximations:1.
We consider the POS tagging and the dependency parse independent and rewrite p(˜ z|x) as:p(˜ z|x) = p(˜ z 1 , ˜ z 2 |x) ≈ p(˜ z 1 |x)p(˜ z 2 |x)2.
We enforce an independence assumption between POS tags.
Thus, if˜zif˜ if˜z 1 = t i 1 t i 2 ...t i k specifies the tags at k positions in the sentence, then p(˜ z 1 |x) is rewritten as:p(t i 1 t i 2 ...t i k |x) ≈ k j=1 p(t i j |x)3.
We also enforce a similar independence assumption between dependency links.
Thus, if˜z if˜ if˜z 2 = u 1 → v 1 ...u k → v k specifies k dependency links, then p(˜ z 2 |x) is rewritten as:p(u 1 → v 1 ...u k → v k |x) ≈ k l=1 p(u l → v l |x)For example, the probability p(˜ z|x) of the feature instance NNS 2 → thought 4 ← RB 3 is approximated as:p(˜ z|x) ≈ p(˜ z 1 |x) · p(˜ z 2 |x) p(˜ z 1 |x) ≈ p(t 2 = NNS|x) · p(t 3 = RB|x) p(˜ z 2 |x) ≈ p(2 → 4|x) · p(3 → 4|x)We will use M ′ 3 to refer to the resulting model.
The probabilistic POS features p(t i |x) are computed using the forward-backward procedure in CRFs, as described in Section 3.1.
To completely specify the pipeline model for NER, we also need an efficient method for computing the probabilistic dependency features p(u → v|x), where u → v is a dependency edge between positions u and v in the sentence x. MSTPARSER is a large-margin method that computes an unbounded score s(x, y) for any given sentence x and dependency structure y ∈ Y(x) using the following edge-based factorization:s(x, y) = u→v∈y s(x, u → v) = w u→v∈y φ(x, u → v)The following three steps describe a general method for associating probabilities with output substructures.
The method can be applied whenever a structured output is associated a score value that is unbounded in R, assuming that the score of the entire output structure can be computed efficiently based on a factorization into smaller substructures.
S1.
Map the unbounded score s(x, y) from R into [0, 1] using the softmax function (Bishop, 1995):n(x, y) = e s(x,y) y∈Y(x) e s(x,y)The normalized score n(x, y) preserves the ranking given by the original score s(x, y).
The normalization constant at the denominator can be computed in O(N 3 ) time by replacing the max operator with the sum operator inside Eisner's chart parsing algorithm.S2.
Compute a normalized score for the substructure by summing up the normalized scores of all the complete structures that contain it.
In our model, dependency edges are substructures, while dependency trees are complete structures.
The normalized score will then be computed as:n(x, u → v) = y∈Y(x),u→v∈y n(x, y)The sum can be computed in O(N 3 ) time using a constrained version of the algorithm that computes the normalization constant in step S1.
This constrained version of Eisner's algorithm works in a similar manner with the constrained forward backward algorithm by restricting the dependency structures to contain a predefined edge or set of edges.S3.
Use the isotonic regression method of Zadrozny and Elkan (2002) to map the normalized scores n(x, u → v) into probabilities p(u → v|x).
A potential problem with the softmax function is that, depending on the distribution of scores, the exponential transform could dramatically overinflate the higher scores.
Isotonic regression, by redistributing the normalized scores inside [0,1], can alleviate this problem.
We test the pipeline model M ′ 3 versus the traditional model M 1 on the task of detecting mentions of person entities in the ACE dataset 2 .
We use the standard training -testing split of the ACE 2002 dataset in which the training dataset is also augmented with the documents from the ACE 2003 dataset.
The combined dataset contains 674 documents for training and 97 for testing.
We implemented the CRF model in MALLET using three different sets of features: Tree, Flat, and Full corresponding to the union of all flat and tree features.
The POS tagger and the dependency parser were trained on sections 2-21 of the Penn Treebank, followed by an isotonic regression step on section 23 for the dependency parser.
We compute precision recall (PR) graphs by varying a threshold on the token level confidence output by the CRF tagger, and summarize the tagger performance using the area under the curve.
tures consistently outperforms the traditional model, especially when only tree features are used.
Dependency parsing is significantly less accurate than POS tagging.
Consequently, the improvement for the tree based model is more substantial than for the flat model, confirming our expectation that probabilistic features are more useful when upstream stages in the pipeline are less accurate.
Figure 4 shows the PR curves obtained for the tree-based models, on which we see a significant 5% improvement in precision over a wide range of recall values.
In terms of the target task -improving the performance of linguistic pipelines -our research is most related to the work of Finkel et al. (2006).
In their approach, output samples are drawn at each stage in the pipeline conditioned on the samples drawn at previous stages, and the final output is determined by a majority vote over the samples from the final stage.
The method needs very few samples for tasks such as textual entailment, where the final outcome is binary, in agreement with a theoretical result on the rate of convergence of the voting Gibbs classifier due to Ng and Jordan (2001).
While their sampling method is inherently approximate, our full pipeline model M 2 is exact in the sense that feature expectations are computed exactly in polynomial time whenever the inference step at each stage can be done in polynomial time, irrespective of the cardinality of the final output space.
Also, the pipeline models M 2 and M 3 and their more efficient alternatives propagate uncertainty during both training and testing through the vector of probabilistic features, whereas the sampling method takes advantage of the probabilistic nature of the outputs only during testing.
Overall, the two approaches can be seen as complementary.
In order to be applicable with minimal engineering effort, the sampling method needs NLP researchers to write packages that can generate samples from the posterior.
Similarly, the new pipeline models could be easily applied in a diverse range of applications, assuming researchers develop packages that can efficiently compute marginals over output substructures.
We have presented a new, general method for improving the communication between consecutive stages in pipeline models.
The method relies on the computation of probabilities for count features, which translates in adding a polynomial factor to the overall time complexity of the pipeline whenever the inference step at each stage is done in polynomial time, which is the case for the vast majority of inference algorithms used in practical NLP applications.
We have also shown that additional independence assumptions can make the approach more practical by significantly reducing the time complexity.
Existing learning based models can implement the new method by replacing the original feature vector with a more dense vector of probabilistic features 3 .
It is essential that every stage in the pipeline produces probabilistic features, and to this end we have described an effective method for associating probabilities with output substructures.
We have shown for NER that simply using the probabilities associated with features that appear only in the top annotation can lead to useful improvements in performance, with minimal engineering effort.
In future work we plan to empirically evaluate NER with an approximate version of the full model M 2 which, while more demanding in terms of time complexity, could lead to even more significant gains in accuracy.
We also intend to comprehensively evaluate the proposed scheme for computing probabilities by experimenting with alternative normalization functions.
We would like to thank Rada Mihalcea and the anonymous reviewers for their insightful comments and suggestions.
