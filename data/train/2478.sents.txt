Deduplication decreases the physical occupancy of files in a storage volume by removing duplicate copies of data chunks, but creates data-sharing dependencies that complicate standard storage management tasks.
Specifically, data migration plans must consider the dependencies between files that are remapped to new volumes and files that are not.
Thus far, only greedy approaches have been suggested for constructing such plans, and it is unclear how they compare to one another and how much they can be improved.
We set to bridge this gap for seeding-migration in which the target volume is initially empty.
We present GoSeed, a formulation of seeding as an integer linear programming (ILP) problem, and three acceleration methods for applying it to real-sized storage volumes.
Our experimental evaluation shows that, while the greedy approaches perform well on "easy" problem instances, the cost of their solution can be significantly higher than that of GoSeed's solution on "hard" instances, for which they are sometimes unable to find a solution at all.
Data deduplication is one of the most effective ways to reduce the size of data stored in large scale systems.
Deduplication consists of identifying duplicate data chunks in different files, storing a single copy of each unique chunk, and replacing the duplicate chunks with pointers to this copy.
Deduplication reduces the total physical occupancy, but increases the complexity of management aspects of large-scale systems such as capacity planning, quality of service, and chargeback [53].
Another example, which is the focus of this study, is data migration-the task of moving a portion of a physical volume's data to another volume-typically performed for load balancing and resizing.
Deduplication complicates the task of determining which files to migrate: the physical capacity freed on the source volume, as well as the physical capacity occupied on the target volume, both depend on the amount of deduplication within the set of migrated files, as well as between them and files outside the set (i.e., files that remain on the source volume and files that initially reside on the target volume).
An efficient migration plan will free the required space on the source volume while minimizing the space occupied on the target.
However, as it turns out, even seeding, in which the target volume is initially empty, is a computationally hard problem.Data migration in deduplicated systems and seeding in particular are the subject of several recent studies, each focusing on a different aspect of the problem.
Harnik et al. [32] address capacity estimation for general migration between volumes, while Duggal et al. [25] describe seeding a cloud-tier for an existing system.
Rangoli [49] is designed for space reclamation-an equivalent problem to seeding.
These studies propose greedy algorithms for determining the set of migrated files, but the efficiency of their resulting migration plans has never been systematically compared.
Furthermore, in the absence of theoretical studies of this problem, it is unclear whether and to what extent they can be improved.We present GoSeed, a new approach that bridges this gap for the seeding and, consequently, space reclamation problems.
GoSeed consists of a formulation of seeding as an integer linear programming (ILP) problem, providing a theoretical framework for generating an optimal plan by minimizing its cost-the amount of data replicated.
Although ILP is known to be NP-Hard, commercial optimizers can solve it efficiently for instances with hundreds of thousands of variables [1][2][3][4]6].
At the same time, ILP instances representing real-world storage systems may consist of hundreds of millions of variables and constraints-too large even for the most efficient optimizers, that may require prohibitively long time to process these instances.
Thus, GoSeed also includes three practical acceleration methods, each presenting a different tradeoff between runtime and optimality.The first method, solver timeout, utilizes the optimizer's ability to return a feasible suboptimal solution when its runtime exceeds a predetermined threshold.
A larger timeout value allows the optimizer to continue its search for the optimal solution, but increasing the timeout may yield diminishing returns.
The second method, fingerprint sampling, is similar to the sketches used in [32], and generates an ILP instance from a probabilistically sampled subset of the system's chunks.
An optimal seeding plan generated on a sample will not necessarily be optimal for the original system.
Thus, increasing the sample size may reduce the plan's cost, but will necessarily increase the required processing time of the solver.Our third method, container aggregation, generates an ILP instance in terms of containers-the basic unit of storage and I/O in many deduplication systems.
Containers typically store several hundreds of chunks, where chunks in the same container likely belong to the same files.
When they do, containers represent the same data sharing constraints as their chunks.
In addition to reducing the problem size, migrating entire containers can be done without decompressing them, and without increasing the system's fragmentation.
At the same time, a container-based ILP instance may introduce "false" sharing between files, resulting in a suboptimal plan.We implement GoSeed with the Gurobi [2] commercial optimizer, and with the three acceleration methods.
We generate seeding plans for volumes based on deduplication snapshots from two public repositories [7,47].
Our evaluation reveals the limitations of the greedy algorithms proposed for seeding thus far-while they successfully generate good plans for "easy" problems (with modest deduplication), GoSeed generates better solutions for the harder problems, for which the greedy approaches sometimes return no solution.Our analysis further demonstrates the efficiency of the acceleration methods in GoSeed.
It shows that (1) the suboptimal solution returned by GoSeed after a timeout is often better than the greedy solutions, (2) fingerprint sampling "hides" some of the data sharing in volumes with modest deduplication, but provides an accurate representation of systems with substantial deduplication, and (3) GoSeed's container-based solutions are optimal if entire containers are migrated.
Our results suggest several rules of thumb for applying and combining these three methods in practical settings.The rest of this paper is organized as follows.
Section 2 provides background on deduplication and ILP, as well as related previous work.
We present the ILP formulation of GoSeed in Section 3, and its acceleration methods in Section 4.
Our experimental setup and evaluation are described in Section 5, with a discussion in Section 6.
Section 7 concludes this work.
The smallest unit of data in a deduplication system is a chunk, which typically consists of 8-64KB.
The incoming data is split into chunks of fixed or variable size, and the fingerprint of each chunk is used to identify duplicates and to replace them with pointers to existing copies.
In many systems, new chunks are written to durable storage in containers, which are the system's I/O unit, and typically consist of hundreds of chunks [20,29,38,41,68].
New chunks are added to containers in a log structure.
Thus, chunks belonging to the same file will likely reside in adjacent containers.
Designs that do not employ containers typically also persist the chunks in a log structure, and thus adjacent chunks will likely belong to the same files [16,18,24,54].
To recover a file, all the containers pointed to by the file recipe are fetched into memory, after which the file's chunks are collected.
The efficiency of this process, in terms of I/O and memory usage, strongly depends on the file's fragmentation: the physical location of the different containers and the portion of the container's chunks that belong to the requested file [50].
Some systems reduce the amount of fragmentation by limiting the number of containers a file may point to, or their age [28,40].
Over the last decade, numerous studies addressed the various aspects of deduplication system design, such as characterizing and estimating the system's deduplication potential [27,33,46,47,57,61], efficient chunking and fingerprinting [9,44,48,63,64], indexing and lookups [10,54,68], restore performance [28,36,40,67], compression [42,65], and security [14,34,39,55].
Their success (among others) has made it possible to use deduplication for primary storage and not just for archives.
Additional studies explored ways to adapt the concept of deduplication to related domains such as page caching [35,38], minimizing network bandwidth [9,48], management of memory resident VM pages [17,31,62], and minimizing flash writes [16,26,30,38,52,60].
Recently, Shilane et al.[53] described the "next" challenge in the design of deduplication systems: providing these systems with a set of management functions that are available in traditional enterprise storage systems, one of which is capacity management in deduplicated systems, and specifically, fast and effective data migration.
Data migration is typically performed in the background, according to a migration plan specifying which data is moved to which new location.
Typical objectives when generating a migration plan include minimizing the amount of data transferred, optimizing load balancing, or minimizing its effect on ongoing jobs.The effectiveness of data migration and the resources it consumes may greatly affect the system's performance.
Thus, efforts have been made to optimize its various aspects including service down-time, geolocation, provisioning, memory consumption, and system-specific performance requirements and constraints [43,45,56,59].
Hippodrome [12] and Ergastulum [13] formulated the storage allocation problem as an instance of bin-packing, while Anderson et al. [11] experimentally evaluated several theoretical algorithms, concluding that their theoretical bounds are overly pessimistic.The distinction between logical and physical capacity in deduplicated systems introduces additional complexity to the data migration problem.
For optimal read and restore performance, the physical copies of a file's chunks must reside on the same storage volume.
Thus, when migrating a file from one volume to another, this file's chunks that also belong to another file must be copied (duplicated), rather than moved.
As a result, migrating data from a full volume to an empty one is likely to increase the total physical capacity of the system.
Migrating data between two non-empty volumes can either increase or decrease the total physical capacity, depending on the degree of duplication between the migrated data and the data on the target volume.
Intuitively, to optimize the system's overall storage utilization, a migration plan should minimize the amount of data that is duplicated as a result.Deduplication complicates other related tasks in a similar manner.
Garbage collection must consider the logical as well as the physical relationships between chunks, files, and containers.
Unfortunately, specific approaches for optimizing garbage collection are not directly applicable to data migration [23,28,40].
As another example, online assignment of streams to servers in distributed systems must consider both content similarity and load balancing.
Current solutions distribute data to servers in the granularity of individual chunks [24], super-chunks [21], files [15], or users [22], considering server load as a secondary objective.
These online solutions are based on partial knowledge of the data in the system, and may result in suboptimal plans if applied directly to data migration.
A recent paper describes the Data Domain Cloud Tier, in which customers maintain two tightly connected deduplication domains, in an on-premises system and in a remote object store [25].
They dedicate special attention to the process of seeding the cloud-tier-migrating a portion of the on-premises system into an initially empty object store.
While the choice of the exact files to migrate is deferred to the client, the general use-case is to keep older backups in the cloud-tier and newer ones on-premises.
The authors refer to "many days or weeks possibly required to transfer a large dataset to the cloud", strongly motivating our goal to minimize the amount of data replicated during migration.Rangoli is a greedy algorithm for space reclamation in a deduplicated system [49].
Although it predates [25] by several years, its problem formulation is equivalent: choose a set of files for migration from an existing volume to a new empty volume.
Rangoli constructs a migration plan by greedily grouping files into roughly equal-sized bins according to the blocks they share, and then chooses for migration the bin whose files have the least amount of data shared with other bins.
The migration objective is specified as the number of bins.In another recent paper, Harnik et al. address migration in the broader context of load balancing [32].
Their system consists of several non-empty volumes, each operating as an independent deduplication domain.
The goal is to estimate the amount of deduplication between files on different volumes, to determine the potential occupancy reduction achieved by migrating files between volumes.
1 The focus of the study is a sketching technique that facilitates this estimation.
In their evaluation, the authors propose a greedy algorithm that iteratively migrates files from one volume to another, with the goal of minimizing the overall physical occupancy in the system.Capacity planning and space reclamation in deduplicated systems are relatively new challenges.
Current solutions are either naïve-migrating backups according to their age-or greedy.
At the same time, migration carries significant costs in terms of physical capacity and bandwidth consumption, and it is unclear whether and how much the greedy solutions can be improved upon.
This gap is the main motivation of our study.
Integer linear programming (ILP) is a well-known optimization problem.
The input to ILP is a set Ax of linear constraints, each of the form a 0 x 0 + a 1 x 1 · · · + a n−1 x n−1 ≤ c, where a 1 , . . . , a n , c ∈ Z, and an objective function of the form T x = t 0 x 0 + t 1 x 1 + · · · + t n−1 x n−1 .
The problem is finding, given Ax and T x, an integer assignment to x 0 , x 1 , . . . , x n that satisfies A x and maximizes T x.
There is no known efficient algorithm for solving ILP.
In particular, when the variables are restricted to Boolean assignments (0 or 1), then merely deciding whether Ax has a solution has been long known to be NP-Complete [37].
Nevertheless, ILP is used in various fields for modeling a wide range of problems [8,51,66,69].
This wide use has been made possible by efficient ILP solvers-designated heuristicbased tools that can handle and solve very large instances.
Thus, despite its theoretical hardness, ILP can in many cases be solved in practice for instances that contain hundreds of thousands and even millions of variables and constraints.Most ILP solvers are based on the Simplex algorithm [19], which efficiently solves linear programming where the variables are not necessarily integers.
They then search for an optimal integer solution, starting the search at the vicinity of the non-integer one.
The wide variety of ILP solvers includes open-source solvers such as SYMPHONY [6], lp_solve [4], and GNU LP Kit [3].
Industrial tools include IBM CPLEX [1] and the Gurobi optimizer [2].
In this research, we take advantage of these highly-optimized solvers for finding the optimal migration plan in a deduplicated storage system.
We formulate the goal of generating a migration plan as follows.
Move physical data of size M from one volume to another, while minimizing R, the total size of the physical data that must be copied (replicated) as a result.
In a seeding plan, the target volume is initially empty.
We refer to R as the cost of the migration.
Note that in a seeding plan, minimizing R minimizes the total capacity of the system, as well as the amount of data transferred between volumes during the migration.Problem definition.
For a storage volume V , let B V = {b 0 , b 1 , . . . , b m−1 } be the set of unique blocks stored on V , and let s(b) be the size of block b.
The storage cost of the volume is the total size of the blocks stored on it, i.e.,s(V ) = Σ b i ∈B V s(b i ).
Let F V = { f 0 , f 1 , .
.
.
, f n−1 } be the set of files mapped to V , and let I V ⊆ B V × F V be an inclusion relation, where (b, f ) ∈ I V means that block b is included in file f .
We intentionally disregard the order of blocks in a file, or blocks that appear several times in one file.
While this information is required for restoring the original file, it is irrelevant for the allocation of blocks to volumes.
(1) 0 ≤ x 0 , x 1 , x 2 , m 0 , m 1 , m 2 , r 0 , r 1 , r 2 ≤ 1 (2) m 0 ≤ x 0 , m 0 ≤ x 1 , m 1 ≤ x 1 , m 1 ≤ x 2 , m 2 ≤ x 2 (3) x 0 ≤ m 0 + r 0 , x 1 ≤ m 0 + r 0 , x 1 ≤ m 1 + r 1 , x 2 ≤ m 1 + r 1 , x 2 ≤ m 2 + r 2 (4) 4 · m 0 + 3 · m 1 + 3 · m 2 = 3 Goal: minimize 4 · r 0 + 3 · r 1 + 3 · r 2Figure 1: Example system and its formulation as an ILP problem, where the goal is to migrate 30% of the physical space (M = 3).
We require that all the blocks included in a file are stored on the volume this file is mapped to.
Thus, if a file f is remapped from V 1 to V 2 , then every block that is included in f must be either migrated to V 2 or replicated.
Similarly, if we migrate a block b from volume V 1 to volume V 2 , then every file f such that (b, f ) ∈ I V 1 must be remapped fromV 1 to V 2 .
The seeding problem is to decide, given a source volume V 1 with B V 1 , F V 1 , I V 1 , an empty destination volume V 2 , a target size M and a threshold size R, whether there exists a set B ⊆ B V 1 of blocks whose total size is M, that can be migrated from V 1 to V 2 , such that the total size of blocks that are replicated is at most R.
In practice, we are interested in the respective optimization problem.
Namely, the seeding optimization problem is to find such a set B while minimizing R.
A solution to the seeding optimization problem is a migration plan: the list of files that are remapped, the list of blocks that are replicated, and B -the list of blocks that are migrated from V 1 to V 2 .
We prove that the seeding problem is NP-hard using a reduction from the Clique problem (omitted due to space considerations).
Intuitively, the relationship between files and blocks influences the quality of the solution, because the decision whether to migrate a specific block depends on the decision regarding other blocks.
In this aspect, seeding is similar to many other set-selection problems such as Set Cover, Vertex Cover, and Hitting Set, that are known to be NP-hard [37].
ILP formulation.
We model the seeding optimization problem as an ILP problem as follows.
For every file f i ∈ F V 1 we allocate a Boolean variable x i .
Assigning 1 to x i means that f i is remapped from V 1 to V 2 .
For every block b i ∈ B V 1 we allocate two Boolean variables, m i , r i .
Assigning 1 to m i means that b i is migrated from V 1 to V 2 , and assigning 1 to r i means that b i is replicated and will be stored in both V 1 and V 2 .
We model the problem constraints as a set of linear inequalities, as follows.1.
All variables are Boolean: 0 ≤ x j ≤ 1, 0 ≤ m i ≤ 1, and 0 ≤ r i ≤ 1 for every f j ∈ F V 1 and b i ∈ B V 1 .2.
If a block b is migrated, then every file that b is included in is remapped:m i ≤ x j for every i, j such that (b i , f j ) ∈ I V 1 .
· Block b 2 is migrated: m 2 = 1 · File f 2 is remapped: x 2 = 1 · Block b 1 is replicated: r 1 = 1· The remaining files and blocks are untouched: 3.
If a file f is rempapped, then every block that is included in f is either migrated or replicated:x 0 = x 1 = m 0 = m 1 = r 0 = r 2 = 0 · The total cost is R = 3 · r 1 = 3x j ≤ m i + r i for every i, j such that (b i , f j ) ∈ I V 1 .
4.
The total size of migrated blocks is M:Σ b i ∈B V 1 s(b i ) · m i = M.The objective function minimizes the total size of blocks that are replicated: minimizeΣ b i ∈B V 1 s(b i ) · r i .
Another intuitive constraint is that a block cannot be migrated and replicated at the same time: m i + r i ≤ 1 for every b i ∈ B V 1 .
This constraint will be satisfied implicitly in any optimal solution-if a block is migrated (m i = 1) then replicating it will only increase the value of the objective function, and thus r i will remain 0.
This is also true for all the solutions in the space defined by the Simplex algorithm, and consequently for suboptimal solutions returned when the solver times out.A solution to the ILP instance is an assignment of values to the Boolean variables.
We note, however, that such an assignment does not necessarily exist.
If a solution does not exist, Simplex-based solvers will return quickly-we observed a few minutes in our evaluation.
If a solution to the ILP instance exists, we find B by returning every block b i such that m i = 1, and the list of replicated blocks by returning every block b i such that r i = 1.
The list of files to remap is given by every file f i such that x i = 1.
Figure 1 shows an example of a simple deduplicated system, and the formulation as an ILP instance of the respective seeding optimization problem with M = 3.
The optimal solution, depicted in Figure 2, is to migrate b 2 , replicate b 1 , and remap f 2 , which yields R = 3.
Another feasible solution is to migrate b 1 , whose size is also 3.
However, migrating b 1 results in replicating both b 0 and b 2 , which yields R = 7.
Refinements.
The requirement to migrate blocks whose total size is exactly M may severely limit the possibility of finding a solution.
Fortunately, in real settings, there is some range of acceptable migrated capacities.
For example, for the file system in Figure 1, a solution exists for M = 3 but not for M = 2.
In realistic systems, feasible solutions may be easier to find but their cost, R, might be unnecessarily high.
Thus, we redefine our problem by adding a slack value, ε, as follows.For a given B V 1 , F V 1 , I V 1 , target size M, and slack value ε, the seeding optimization problem with slack is to find B ⊆ B V 1 of blocks whose total size is M , M − ε ≤ M ≤ M + ε, that can be migrated from V 1 to V 2 .
In the formulation as an ILP problem, we require that the total size of migrated blocks isM ± ε: M − ε ≤ Σ b i ∈B V 1 s(b i ) · m i ≤ M + ε.For example, for the system in Figure 1, the optimal solution for M = 2 and ε = 1, is the solution given above for M = 3.
Another refinement in the problem formulation is required to prevent "leftovers" on the source volume V 1 .
An orphan block is copied because a file it is included in is remapped, but no other file that includes it remains in V 1 .
For example, consider the system in Figure 3(a), with a migration objective of M = 3.
For simplicity, assume that ε = 0.
The only feasible solution is depicted in Figure 3(b), where b 1 is migrated, f 1 and f 2 are remapped, and b 2 is replicated.
b 2 cannot be migrated because this would exceed the target migration size, M = 3.
Replicating b 2 leaves an extra copy of this block in V 1 , where it is not contained in any file.Although a migration plan with orphan blocks represents a feasible solution to the ILP problem, it is an inefficient one.
For example, b 2 in Figure 3(b) consists of 20% of the system's original capacity.
Orphans can be eliminated by garbage collection, or even as part of the migration process [25].
This is essentially equivalent to migrating the orphan blocks, rather than replicating them, resulting in a migrated capacity which exceeds the original objective.
For example, removing b 2 from volume V 2 in Figure 3(b) is equivalent to a migration plan with M = 5, rather than the intended M = 3.
We eliminate such solutions by adding the following constraint: if a block b is copied, then at least one file it is included in is not remapped:r i ≤ Σ { j|(b i , f j )∈I V 1 } (1 − x j ) for every b i ∈ B V 1 .
This additional constraint may result in the solver returning without a solution.
Such cases should be addressed by increasing ε or modifying M. Nevertheless, the decision whether to prevent orphan blocks in the migration plan or to eliminate them during its execution is a design choice that can easily be realized by adding or removing the above constraint.Complexity.
The number of constraints in the ILP formulation is linear in the size of I V -the number of pointers from files to blocks in the system.
Although the size of I V can be at most |B V | · |F V |, it is likely considerably smaller in practice: the majority of the files are small, and the majority of the blocks are included in a small number of files [47].
In general, the time required for an ILP solver to find an optimal solution depends on many factors, including the number of variables, the connections between them (represented by the constraints), and the number of feasible solutions.
In our context, the size of the problem is determined by the number of files and blocks, and its complexity depends on the deduplication ratio and on the pattern of data sharing between the files.
It is difficult to predict how each of these factors will affect the solving time in practice.
Furthermore, small changes in the target migration size and in the slack value may significantly affect the solver's performance.
We evaluate the sensitivity of GoSeed to these parameters in Section 5.
The challenge in applying ILP solvers to realistic migration problems is their size.
In a system with an average chunk size of 8KB, there will be approximately 130M chunks in each TB of physical capacity.
Thus, the runtime for generating a migration plan for a source volume with several TBs of data would be unacceptably long.
In this section, we present three methods for reducing this generation time.
We describe their advantages and limitations and the ways in which they may be combined, and evaluate their effectiveness in Section 5.
The runtime of an ILP solver can be limited by specifying a timeout value.
When a timeout is reached before the optimal solution is found, the solver will halt and return the best feasible solution found thus far.
This approach has the advantage of letting the solver process the unmodified problem.
It does not require any preprocessing, and, theoretically, the solver may succeed in finding the optimal solution.
The downside is that when the solver is timed out, we cannot necessarily tell how far the suboptimal solution is from the optimal one.
Sampling is a standard technique for handling large problems, and has been used in deduplication systems to increase the efficiency of the deduplication process [15,16,41], to route streams to servers [21], for estimating deduplication ratios [33], and for managing volume capacities [32].
We use sampling in the same way it is used in [32].
Given a sampling degree k, we include in our sample all the chunks whose fingerprint contains k leading zeroes, and all the files containing those chunks.
When the fingerprint values are uniformly distributed, the sample will include 1 2 k chunks.
Harnik et al. show in [32] that k = 13 guarantees small enough errors for estimating the capacity of deduplicated volumes larger than 100GB.Sampling reduces the size of the ILP instance by a predictable factor: incrementing the sampling degree k by one reduces the number of blocks by half.
Combining sampling and timeouts presents an interesting tradeoff: a smaller sampling factor results in a larger ILP instance that more accurately represents the sampled system.
However, solving a larger instance is more likely to time out and return a suboptimal solution.
It is not clear which combination will result in a better migration plan-a suboptimal solution on a large instance, or an optimal solution on a small instance.
Our analysis in Section 5 shows how the answer depends on the original (unsampled) instance and on the length of the timeout.
Aggregation is often employed as a first step in analysing large datasets.
In deduplication systems, containers are a natural basis for aggregation.
Containers are often compressed before being written to durable storage, and are decompressed when they are fetched into memory for retrieving individual chunks.
Thus, generating and executing a migration plan in the granularity of containers holds the advantage of avoiding decompression as well as an increase in the fragmentation in the system by migrating individual chunks from containers.To formulate the migration problem with containers we coalesce chunks that are stored in the same container into a single block, and remove parallel edges, i.e., pointers from the same file to different chunks in the same container.
Figure 4 shows the container view of the volume from Figure 1.
In a real system, formulating the migration problem with containers is more efficient than with chunks: when processing file recipes, we can ignore the chunk fingerprints and use only the container IDs for generating the variables and constraints.In a system that stores chunks in containers, the containerbased migration problem accurately represents the system's original constraints.
At the same time, we can further leverage container-based aggregation as an acceleration method by artificially increasing the container size beyond the size used by the system.
With aggregation degree K, we coalesce every K adjacent containers into one, like we do for chunks.
Thus, a system with 4MB containers can be represented as one with 4K-MB containers by coalescing every K original containers.
Containers typically store hundreds of chunks, which means that the size of the resulting ILP problem will be smaller by several orders of magnitude.
Furthermore, containers are allocated as fixed-size extents, which further reduces the ILP problem complexity: the optimization goal of minimizing the total size of migrated blocks becomes a simpler goal of minimizing their number.A container-based seeding plan can be obtained more quickly than a chunk-based one.
Thus, if aggregation is combined with solver timeouts, a container-based suboptimal solution will likely be closer to the optimal (container-based) solution than in an execution solving the chunk-based instance.
At the same time, container-based aggregation (like any aggregation method) reduces the granularity of the solution, which affects its efficiency as an acceleration method for the original chunk-based problem.
Namely, an optimal container-based migration plan is not necessarily optimal if the migration is executed in the granularity of chunks.Consider a migration plan generated with containers, and let F V 2 be the set of files that are remapped to V 2 as a result of that plan.
F V 1 is the set of files that remain on V 1 .
If a container is not part of the migration plan, this means that all of its chunks are contained only in files from F V 1 .
When a container is marked for migration, this means that all of its chunks are contained only in files from F V 2 .
When a container includes at least one chunk that is contained in a file from F V 1 as well as in a file from F V 2 , the entire container is marked for replication.
However, this container may also contain some "false positives"-chunks that are contained only in files from F V 1 (and should not be part of the migration), or only in files from F V 2 (and should be migrated rather than replicated).
These false positives increase the cost of the container-based solution, and can be eliminated by performing the actual migration in the granularity of chunks, as done in [25].
However, this would eliminate the advantages of migrating entire containers, and may cause the solver to "miss" the migration plan that would have been optimal for the chunk-based ILP instance.
We observe this effect in Section 5 The goal of our experimental evaluation is to answer the following questions:• What is the difference, in terms of cost, between the ILPbased migration plan and the greedy ones?
• How do the ILP instance parameters (its size, M, and ε) affect it's complexity, indicated by the solver's runtime?
• How does timing out the solver affect the quality (cost) of the returned solution?
• How do the sampling and aggregation degrees affect the solver's runtime and the cost of the migration plan?
Deduplication snapshots.
We use static file system snapshots from two publicly available repositories.
The UBC dataset [47] includes file systems of 857 Microsoft employees available via SNIA IOTTA [5].
The FSL dataset [7] includes daily snapshots of a Mac OS X Snow Leopard server and of student home directories at the File System and Storage Lab (FSL) at Stony Brook University [57,58].
The snapshots include, for each file, the fingerprints calculated for each of its chunks, as well as the chunk size in bytes.
Each snapshot file represents one entire file system, which is the migration unit in our model, and is represented as one file in our ILP instance.To obtain a mapping between files and unique chunks, we emulate the ingestion of each snapshot into a simplified deduplication system.
We assume that all duplicates are detected and eliminated.
We emulate the assignment of chunks to containers by assuming that unique chunks are added to containers in the order of their appearance in the original snapshot file.
We create snapshots of entire volumes by ingesting several file-system snapshots one after the other, thus eliminating duplicates across individual snapshots.
The resulting volume snapshot represents an independent deduplication domain.
The volume snapshots used in our experiments are detailed in Table 1.
The UBC-X volumes contain the first X file systems in the UBC dataset.
These snapshots were created with variable-sized chunks with Rabin fingerprints, whose specified average chunk size is 64KB.
In practice, however, many chunks are 4KB or less.
The FSL snapshots were also generated with Rabin fingerprints and average chunk size of 64KB.
2 The MacOS-Daily volume contains all available daily snapshots of the server between May 14, 2015 andMay 8, 2016, while the MacOS-Week volume contains weekly snapshots, which we emulate by ingesting the snapshots from all the Fridays in repository.
The Homes volume contains weekly snapshots of nine users between August 28 and October 23, 2014 (nine weeks in total).
GoSeed Implementation.
We use the commercial Gurobi optimizer [2] as our ILP solver, and use its C++ interface to define our problem instances.
The problem variables (x i , m i , r i ) are declared as Binary and represented by the GRBVar data type.
The constraints and objective are declared as linear expressions.
M and ε are given in units of percents of the physical capacity.
We specify three parameters for each execution: a timeout value, the parallelism degree (number of threads), and a random seed.
These parameters do not affect the optimality of the solution, but they do affect the solver's runtime.
Specifically, the starting point for the search for an integer solution is chosen at random, which may lead some executions to complete earlier than others.
If the solver times out, different executions might return solutions with slightly different costs.
In our evaluation, we solve each ILP instance in three separate executions, each with a different random seed, and present the average of their execution times and costs.
Our wrapper program for converting a volume snapshot into an ILP instance in Gurobi consists of approximately 400 lines of code.
3 We ran our experiments on a server running Ubuntu 18.04.3, equipped with 64GB DDR4 RAM (with 2666 MHz bus speed), Intel R Xeon R Silver 4114 CPU (with hyper-threading functionality) running at 2.20GHz, one Dell R T1WH8 240GB TLC SATA SSD, and one Micron 5200 Series 960GB 3D TLC NAND Flash SSD.
We let Gurobi use 38 CPUs, and specify a timeout of six hours, to allow for experiments with a wide range of setup and problem parameters.Comparison to existing approaches.
We use our volume snapshots to evaluate the quality of the migration plans generated by the existing approaches described in Section 2.3.
We implement Rangoli according to the original paper [49].
We convert our migration objective M into a number of bins B, such that B = 1 M .
We modified Rangoli to comply with the restriction that the migrated capacity is between M − ε and M + ε: when choosing one of B bins for migration, our version of Rangoli chooses only from those bins whose capacity is within the specified bounds.For evaluation purposes, we implemented a seeding version of the greedy load balancer that was used for evaluating the capacity sketches in [32].
We refer to this algorithm as SGreedy.In each iteration, SGreedy chooses one file from V 1 to remap to V 2 .
The remapped file is the one which yields the best spacesaving ratio, i.e., the ratio between the space freed from V 1 and that added to V 2 .
The iterations continue until the migrated capacity is at least M − ε, and if, at this point, it does not exceed M + ε, a solution is returned.
SGreedy returns a seeding plan in the form of a list of files that are remapped from V 1 to V 2 .
We then use a dedicated "cost calculator" to derive the cost of the migration plan on the original (unsampled) system.Our calculator creates an array of the volume's chunks and their sizes, and two bit indicators, V 1 and V 2 , that are initialized to FALSE for each chunk.
It then traverses the files in the volume snapshot and updates the indicators of their blocks as follows.
If a file is remapped, then the V 2 indicators of all its chunks are set to TRUE.
If a file is not remapped, then the V 1 indicators of all its chunks are set to TRUE.
A final pass over the chunk array calculates the replication cost by summing the sizes of all the chunks whose V 1 and V 2 indicators are both TRUE.
The migrated capacity is the sum of the sizes of all the chunks whose V 2 indicator is TRUE and V 1 indicator is FALSE.
Comparison of different algorithms.
We first analyze the migration cost incurred by the different algorithms on the various volume snapshots.
Figure 5 shows our results with three values of M (10,20,33) and ε = 2.
A missing bar of an algorithm indicates that it did not find a solution for that instance.
GoSeed-K and SGreedy-K depict the results obtained by these algorithms running on a snapshot created with sampling degree K (the cost was calculated on the original snapshot).
Rangoli does not perform well on most of the volume snapshots.
It incurs the highest replication cost on the UBC snapshots, except UBC-100 with M = 33, for which it does not find a solution.
On the FSL snapshots, it finds a good solution only for the Homes volume (with M = 10 and M = 33), but not for the remaining instances.
The backups on the MacOS volumes share most of their data, with a very low deduplication ratio.
In these circumstances, Rangoli fails because it is unable to partition the files into separate bins of the required size.
SGreedy returns a solution in all but two instances (UBC-50 with M = 10 and Homes with M = 33).
For the UBC snapshots, the cost of its solution is 37%-87% lower than the cost of Rangoli's solution.
When SGreedy is applied to a sampled snapshot, as it was originally intended, this cost increases by as much as 28% and 27%, for sample degrees 12 and 13, respectively.
This increase is expected, as the sampled snapshot "hides" some of the data sharing in the real system.
However, the increase is smaller in most instances.
It is also interesting to note a few cases where SGreedy returns a better solution (with a lower replication cost) on the sampled snapshot than on the original one, such as for UBC-50 with M = 33.
These situations can happen when "hiding" some of the sharing helps the greedy process find a solution that it wouldn't find otherwise.We can now classify our volumes into three rough categories.
We refer to the UBC volumes as easy-their data sharing is modest and the greedy algorithms find good solutions for them.
We refer to the Homes volume as hard-its data sharing is substantial and the greedy algorithms mostly return solutions with high costs (up to 29%), or don't find a solution at all.
We consider the MacOS volumes to be very hard because of their exceptionally high degree of sharing between files.
This sharing prevents Rangoli from finding any solution, and incurs very high costs (up to 60%) in the plan generated by SGreedy.GoSeed cannot find a solution for the full snapshots, which translate to ILP instances with hundreds of millions of constraints.
We thus use fingerprint sampling to apply GoSeed to the volume snapshots, with two sampling degrees, 12 and 13.
Our results show that GoSeed finds a solution for all the volumes and all values of M.
It generates slightly better plans with a smaller sampling degree, when more of the system's constraints are manifested in the ILP instance.In the easy (UBC) volumes, the cost of GoSeed's migration plan is similar to that of SGreedy's plan on the sampled snapshots.
It is higher for four instances (UBC-50 with M = 20, 33, and for UBC-100 and UBC-200 with M = 10) and equal or lower for the rest.
This shows that greedy solutions may suffice for volumes with modest data sharing between files.The picture is different for the hard volumes.
For Homes, GoSeed consistently finds a better migration plan, while each of the greedy algorithms finds a solution for some values of M but fails to find one for others.
The biggest gap between the greedy and optimal solutions occurs for M = 20: SGreedy (with and without sampling) replicates 27%-28% of the volume's capacity, while the replication cost of the plan generated by GoSeed is only 1%.
This demonstrates a known property of greedy algorithms-their solutions are good enough most of the time, but very bad in the worst case.Finally, for the very hard (MacOS) volumes, GoSeed finds similar or better solutions than SGreedy, with or without sampling.
Although more than 50% of the volume is replicated in all of the migration plans, the replication cost of GoSeed for MacOS-Weekly with M = 10 and M = 20 is 14% and 8% lower than that of SGreedy, respectively.
The exceptionally high degree of sharing in this volume indicates that better solutions likely do not exist.
This conclusion was supported in our attempt to apply the "user's" migration plan from [25], remapping the oldest backups (files, in our case) to a new tier.
In MacOS-Weekly and MacOS-Daily, remapping the single Homes M(%)-Epsilon(%) Figure 7: GoSeed plans generated with sampling degree K=12.
oldest backup to a new volume resulted in migrating 0.2% and 0.3% of the volume's capacity, and replicating 49% and 55% of it, respectively.
Figure 6 shows the runtime of the different algorithms.
The runtime of GoSeed is longer than that of SGreedy on the sampled snapshot, but shorter than that of SGreedy and Rangoli on the original snapshots.
GoSeed timed out at six hours only in one execution (UBC-500 and K = 12).
The rest of the instances were solved by GoSeed in less than one hour (UBC) or five minutes (FSL).
We note, though, that GoSeed utilizes 38 threads, while the greedy algorithms use only one.
For a migration plan transferring several TBs of data across a wide area network or a busy interconnect, these runtimes and resources are acceptable.Effect of ILP parameters.
We first investigate how M and ε affect the solver's ability to find a good solution.
We compare the cost of the plan generated by GoSeed with five values of M (used in [49]) and three values of ε on an easy (UBC-100) volume and on a hard one (Homes).
The results in Figure 7 show that in the easy volume, higher values of M result in a higher cost, and that this cost can be somewhat reduced by increasing ε, which increases the number of feasible solutions.
We observe a similar effect in Homes, but to a much smaller extent.
We note that this effect is also shared by the greedy algorithms (not shown for lack of space), for which differences in ε often make a difference between finding a feasible solution or returning without one.
Increasing M also exponentially increases the runtime of the solver-migrating more blocks results in more feasible solutions in the search space.
We omit the runtimes of this experiment, but the effect can be observed in Figure 6.
We next investigate how the size of the snapshot affects the time required to solve the ILP instance.
We compare problems with similar constraints and different sizes by generating sampled snapshots with K between 7 and 13 of the above two volumes.
Figure 8 shows the average runtime of GoSeed on these snapshots with M = 20 and ε = 2.
Error bars mark the minimum and maximum runtimes.
Note that both axes are log-scaled-incrementing K by one doubles the number of blocks in the ILP instance.
As we expected, the time increases exponentially with the number of blocks.
The figure also shows that the runtime of the same instance with one random seed can be as much as 1.45× longer than with another seed.
We discuss the implications of this difference below.Effect of solver timeout.
To evaluate the effect of timeouts on the cost of the generated plan, we generate a volume snapshot by sampling UBC-100 with K = 8, for which the solver's execution time is approximately four hours.
We repeatedly solve this instance (with the same random seed) with increasing timeout values.
We set the timeouts to fixed portions of the full runtime, after having measured the complete execution.
We repeat this process for three different seeds.
To eliminate the effect of sampling, we present the cost of migration assuming the sample represents the entire system.The results in Figure 9 show that the most substantial cost reduction occurs in the first half of the execution, after which the quality of the solution does not improve considerably.
The three processes converge to the same optimal solution at different speeds, corresponding to the different runtimes in Figure 8.
At the same time, we note that the largest differences in cost occur between suboptimal solutions returned in the first half of the execution, when the solver makes most of its progress.
The cost difference is relatively small and does not exceed 22% (at 6 16 )-a much smaller difference than the difference in time required to find the optimal solution.Gurobi provides an interface for querying the solver for intermediate results without halting its execution.
We did not use this interface because it might compromise the accuracy of our time measurements.
However, it can be used to periodically check the rate at which the intermediate solution improves.
When the rate decreases and begins to converge, continuing the execution yields diminishing returns, and it can be halted.Effect of fingerprint sampling.
We evaluate the effect of the sampling degree on the cost of the solution by calculating the costs of the plans generated for UBC-100 and Homes with M = 20, ε = 2, and K between 7 and 13.
Figure 10 shows that the difference between the cost of optimal solutions is very small.
However, when the solver times out, the cost of the suboptimal solution can be as much as 24× higher.Our results for varying the ILP instance parameters and sampling degrees suggest the following straightforward heuristic for obtaining the best seeding plan within a predetermined time frame.
Generate a sample of the system with degree between 10 and 13-smaller degrees are better for smaller systems.
If the solver times out, increase the sampling degree by one.
If the solver completes and there is still time, solve instances with increasing values of ε until the end of the time frame is reached.
This process results in a set of solutions that form a Pareto frontier-their cost decreases as their migrated capacity is farther from the original objective M.
The final solution should be chosen according to the design objectives of the system.Efficiency of container-based plans.
The container-based aggregation generates a reduced ILP instance which is an accurate representation of the connections between files and containers.
This representation can also be used to generate containerbased migration plans with Rangoli and SGreedy.
Thus, our next experiment compares the costs of GoSeed and the greedy algorithms on the same instances.
Our results in Figure 11 show that in these circumstances, GoSeed can reduce the migration cost obtained by Rangoli and SGreedy by as much as 87% and 66%, respectively.
These results are not surprising given the size of the ILP instances-they consist of several hundred thousand variables, well within Gurobi's capabilities.
As a result, even in experiments in which Gurobi times out (indicated by the small triangles in the figure), its suboptimal solutions are considerably better than the greedy ones.
The costs with aggregated containers (GoSeed-C×2) are higher because of the false dependencies described in Section 4.3.
We used our cost calculator to compare the chunk-level cost of the container-based migration plan to the greedy plans generated for the original system (figures omitted due to space considerations).
For the MacOS volumes and for UBC-50, GoSeed's container-based plan outperforms Rangoli and is comparable to SGreedy.
However, for the larger UBC volumes and for Homes, SGreedy and Rangoli find solutions with as much as 7.6× and 13.6× lower cost, respectively.
On these instances, Gurobi returned a suboptimal solution which was close to the container-based optimum, but far from the chunkbased optimum, due to the reasons described in Section 4.3.
We therefore recommend using GoSeed with container-based aggregation if the migration is to be performed with entire containers, and with fingerprint sampling otherwise.
Data migration within a large-scale deduplicated system can reallocate tens of terabytes of data.
This data is possibly transferred over a wide area network or a busy interconnect, and some of it might be replicated as a result.
The premise of our research is that the potentially high costs of data migration justify solving a complex optimization problem with the goal of minimizing these costs.Thus, in contrast to existing greedy heuristics to this hard problem, GoSeed attempts to solve it.
By formulating data migration as an ILP instance, GoSeed can "hide" its complexity by leveraging off-the-shelf highly optimized solvers.
This approach is independent of specific design and implementation details of the deduplication system or the ILP solver.
However, it introduces an inherent tradeoff between the time spent generating a seeding plan, and the cost of executing it.
As this cost depends on the system's characteristics, such as network speed, cost of storage, and read and restore workload, the potential for cost saving by GoSeed is system dependent as well.Our evaluation showed that the benefit of GoSeed is high in two scenarios.
The first is when the problem's size allows the solver to find the optimal (or near optimal) solution within the allocated time.
Container-based migration is an example of this case, where GoSeed significantly reduced the migration cost of the greedy algorithms.
The second case is when a high degree of data sharing in the system makes it hard for the greedy solutions to find a good migration plan, causing them to produce a costly solution or no solution at all.
At the same time, for systems with low or exceptionally high degrees of data sharing, the greedy solutions and that of GoSeed are comparable.Accurately identifying the large instances for which GoSeed would significantly improve on the greedy solution is not straightforward, and requires further research.
Fortunately, a simple hybrid approach can provide 'the best of both worlds': one can run the greedy algorithm, followed by GoSeed, and execute the migration plan whose cost is lower.Generalizations.
Seeding is the simplest form of data migration in large systems.
A natural next step to this work is to generalize our ILP-based approach to more complex migration scenarios, such as migration into a non-empty volume, and migration where both source and target volumes are chosen as part of the plan.
Each generalization introduces additional aspects, and might require reformulating not only the ILP constraints, but also its objective function.For example, when the destination volume is not empty, the optimal migration plan can be the one that minimizes the total storage capacity on the source and destination volumes combined.
An alternative formulation might minimize the total amount of data that must be transferred from the source volume to the destination.
In the most general case, generating the migration plan also entails determining either the source or the destination volume, or both, such that the migration goal is achieved and the objective is optimized.
Data migration in general introduces additional objectives, such load balancing between volumes, or optimizing the migration process under certain network conditions and limitations.
The problem can be further extended by allowing some files to be split between volumes, introducing a new tradeoff between the cost of migration and that of file access.The ILP formulation of these problems will result in considerably more complex instances than those of the seeding problem.
As a result, we might need to apply our acceleration methods more aggressively, e.g., by increasing the fingerprint sampling degree, or construct new methods.
Thus, each generalization of the seeding problem introduces non-trivial challenges as well as additional tradeoffs between the solving time and the cost of migration.
We presented GoSeed, an algorithm for generating theoretically optimal seeding plans in deduplicated systems, and three acceleration methods for applying it to realistic storage volumes.
Our evaluation demonstrated the effectiveness of the acceleration methods: GoSeed can produce an optimal seeding plan on a sample of the system in less than an hour, even in cases where the greedy solutions do not find a feasible solution to the problem.
When executed on the original system, GoSeed's solution is not theoretically optimal, but it can substantially reduce the cost of the greedy solutions.Finally, our formulation of data migration as an ILP problem, combined with the availability of numerous ILP solvers, opens up new opportunities for additional contributions in this domain, and for making data migration more efficient.
We thank our shepherd, Dalit Naor, and the anonymous reviewers, for their helpful comments.
