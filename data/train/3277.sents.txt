Cloud operators increasingly need more and more fine-grained rules to better control individual network flows for various traffic management policies.
In this paper, we explore automated rule management in the context of a system called vCRIB (a virtual Cloud Rule Information Base), which provides the abstraction of a centralized rule repository.
The challenge in our approach is the design of algorithms that automatically off-load rule processing to overcome resource constraints on hypervi-sors and/or switches, while minimizing redirection traffic overhead and responding to system dynamics.
vCRIB contains novel algorithms for finding feasible rule placements and adapting traffic overhead induced by rule placement in the face of traffic changes and VM migration.
We demonstrate that vCRIB can find feasible rule placements with less than 10% traffic overhead even in cases where the traffic-optimal rule placement may be in-feasible with respect to hypervisor CPU or memory constraints .
To improve network utilization, application performance, fairness and cloud security among tenants in multi-tenant data centers, recent research has proposed many novel traffic management policies [8,32,28,17].
These policies require fine-grained per-VM, per-VMpair, or per-flow rules.
Given the scale of today's data centers, the total number of rules within a data center can be hundreds of thousands or even millions (Section 2).
Given the expected scale in the number of rules, rule processing in future data centers can hit CPU or memory resource constraints at servers (resulting in fewer resources for revenue-generating tenant applications) and rule memory constraints at the cheap, energy-hungry switches.In this paper, we argue that future data centers will require automated rule management in order to ensure rule placement that respects resource constraints, minimizes traffic overhead, and automatically adapts to dynamics.
We describe the design and implementation of a virtual Cloud Rule Information Base (vCRIB), which provides the abstraction of a centralized rule repository, and automatically manages rule placement without operator or tenant intervention ( Figure 1).
vCRIB manages rules for different policies in an integrated fashion even in the presence of system dynamics such as traffic changes or VM migration, and is able to manage a variety of data center configurations in which rule processing may be constrained either to switches or servers or may be permitted on both types of devices, and where both CPU and memory constraints may co-exist.
vCRIB's rule placement algorithms achieve resourcefeasible, low-overhead rule placement by off-loading rule processing to nearby devices, thus trading off some traffic overhead to achieve resource feasibility.
This trade-off is managed through a combination of three novel features (Section 3).
• Rule offloading is complicated by dependencies between rules caused by overlaps in the rule hyperspace.
vCRIB uses per-source rule partitioning with replication, where the partitions encapsulate the dependencies, and replicating rules across partitions avoids rule inflation caused by splitting rules.
• vCRIB uses a resource-aware placement algorithm that offloads partitions to other devices in order to find a feasible placement of partitions, while also trying to co-locate partitions which share rules in order to optimize rule memory usage.
This algorithm can deal with data center configurations in which some devices are constrained by memory and others by CPU.
• vCRIB also uses a traffic-aware refinement algorithm that can, either online, or in batch mode, refine partition placements to reduce traffic overhead while still preserving feasibility.
This algorithm avoids local minima by defining novel benefit functions that perturb partitions allowing quicker convergence to feasi-ble low overhead placement.
We evaluate (Section 4) vCRIB through large-scale simulations, as well as experiments on a prototype built on Open vSwitch [4] and POX [1].
Our results demonstrate that vCRIB is able to find feasible placements with a few percent traffic overhead, even for a particularly adversarial setting in which the current practice needs more memory than the memory capacity of all the servers combined.
In this case, vCRIB is able to find a feasible placement, without relying on switch memory, albeit with about 20% traffic overhead; with modest amounts of switch memory, this overhead drops dramatically to less than 3%.
Finally, vCRIB correctly handles heterogeneous resource constraints, imposes minimal additional traffic on core links, and converges within 5 seconds after VM migration or traffic changes.
Today, tenants in data centers operated by Amazon [5] or whose servers run software from VMware place their rules at the servers that source traffic.
However, multiple tenants at a server may install too many rules at the same server causing unpredictable failures [2].
Rules consume resources at servers, which may otherwise be used for revenue-generating applications, while leaving many switch resources unused.Motivated by this, we propose to automatically manage rules by offloading rule processing to other devices in the data center.
The following paragraphs highlight the main design challenges in scalable automated rule management for data centers.The need for many fine-grained rules.
In this paper, we consider the class of data centers that provide computing as a service by allowing tenants to rent virtual machines (VMs).
In this setting, tenants and data center operators need fine-grained control on VMs and flows to achieve different management policies.
Access control policies either block unwanted traffic, or allocate resources to a group of traffic (e.g., rate limiting [32], fair sharing [29]).
For example, to ensure each tenant gets a fair share of the bandwidth, Seawall [32] installs rules that match the source VM address and performs rate limiting on the corresponding flows.
Measurement policies collect statistics of traffic at different places.
For example, to enable customized routing for traffic engineering [8,11] or energy efficiency [17], an operator may need to get traffic statistics using rules that match each flow (e.g., defined by five tuples) and count its number of bytes or packets.
Routing policies customize the routing for some types of traffic.
For example, Hedera [8] performs specific traffic engineering for large flows, while VLAN-based traffic management solutions [28] use different VLANs to route packets.
Most of these policies, expressed in high level languages [18,37], can be translated into virtual rules at switches 1 .
A simple policy can result in a large number of finegrained rules, especially when operators wish to control individual virtual machines and flows.
For example, bandwidth allocation policies require one rule per VM pair [29] or per VM [29], and access control policies might require one rule per VM pair [30].
Data center traffic measurement studies have shown that 11% of server pairs in the same rack and 0.5% of inter-rack server pairs exchange traffic [22], so in a data center with 100K servers and 20 VMs per server, there can, be 1G to 20G rules in total (200K per server) for access control or fair bandwidth allocation.
Furthermore, state-of-the-art solutions for traffic engineering in data centers [8,11,17] are most effective when per-flow statistics are available.
In today's data centers, switches routinely handle between 1K to 10K active flows within a one-second interval [10].
Assume a rack with 20 servers and if each server is the source of 50 to 500 active flows, then, for a data center with 100K servers, we can have up to 50M active flows, and need one measurement rule per-flow.
In addition, in a data center where multiple concurrent policies might co-exist, rules may have dependencies between them, so may require carefully designed offloading.
For example, a rate-limiting rule at a source VM A can overlap with the access control rule that blocks traffic to destination VM B, because the packets from A to B match both rules.
These rules cannot be offloaded to different devices.Resource constraints.
In modern data centers, rules can be processed either at servers (hypervisors) or programmable network switches (e.g., OpenFlow switches).
Our focus in this paper is on flow-based rules that match packets on one or more header fields (e.g., IP addresses, MAC addresses, ports, VLAN tags) and perform various actions on the matching packets (e.g., drop, rate limit, count).
Figure 2(a) shows a flow-space with source and destination IP dimensions (in practice, the flow space has 5 dimensions or more covering other packet header fields).
We show seven flow-based rules in the space; for example, A1 represents a rule that blocks traffic from source IP 2 (VM2) to destination IP 0-3 (VM 0-3).
While software-based hypervisors at servers can support complex rules and actions (e.g., dynamically calculating rates of each flow [32]), they may require committing an entire core or a substantial fraction of a core at each server in the data center.
Operators would prefer to allocate as much CPU/memory as possible to client VMs to maximize their revenue; e.g., RackSpace operators prefer not to dedicate even a portion of a server core for rule processing [6].
Some hypervisors offload rule processing to the NIC, which can only handle limited number of rules due to memory constraints.
As a result, the number of rules the hypervisor can support is limited by the available CPU/memory budget for rule processing at the server.We evaluate the numbers of rules and wildcard entries that can be supported by Open vSwitch, for different values of flow arrival rates and CPU budgets in Figure 3.
With 50% of a core dedicated for rule processing and a flow arrival rate of 1K flows per second, the hypervisor can only support about 2K rules when there are 600 wildcard entries.
This limit can easily be reached for some of the policies described above, so that manual placement of rules at sources can result in infeasible rule placement.To achieve feasible placement, it may be necessary to offload rules from source hypervisors to other devices and redirect traffic to these devices.
For instance, suppose VM2, and VM6 are located on S1 (Figure 2(b)).
If the hypervisor at S1 does not have enough resources to process the deny rule A3 in Figure 2(a), we can install the rule at ToR1, introducing more traffic overhead.
Indeed, some commercial products already support offloading rule processing from hypervisors to ToRs [7].
Similarly, if we were to install a measurement rule that counts traffic between S1 and S2 at Aggr1, it would cause the traffic between S1 and S2 to traverse through Aggr1 and then back.
The central challenge is to design a collection of algorithms that manages this tradeoff -keeps the traffic overhead induced by rule offloading low, while respecting the resource constraint.Offloading these rules to programmable switches, which leverage custom silicon to provide more scalable rule processing than hypervisors, is also subject to resource constraints.
Handling the rules using expensive power-hungry TCAMs limits the switch capacity to a few thousand rules [15], and even if this number increases in the future its power and silicon usage limits its applicability.
For example, the HP ProCurve 5406zl switch hardware can support about 1500 OpenFlow wildcard rules using TCAMs, and up to 64K Ethernet forwarding entries [15].
Heterogeneity and dynamics.
Rule management is further complicated by two other factors.
Due to the different design tradeoffs between switches and hypervisors, in the future different data centers may choose to support either programmable switches, hypervisors, or even, especially in data centers with large rule bases, a combination of the two.
Moreover, existing data centers may replace some existing devices with new models, resulting in device heterogeneity.
Finding feasible placements with low traffic overhead in a large data center with different types of devices and qualitatively different constraints is a significant challenge.
For example, in the topology of Figure 1, if rules were constrained by an operator to be only on servers, we would need to automatically determine whether to place a measurement rule for tenant traffic between S1 and S2 at one of those servers, but if the operator allowed rule placement at any device, we could choose between S1, ToR1, or S2; in either case, the tenant need not know the rule placement technology.
Today's data centers are highly dynamic environments with policy changes, VM migrations, and traffic changes.
For example, if VM2 moves from S1 to S3, the rules A0, A1, A2 and A4 should me moved to S3 if there are enough resources at S3's hypervisor.
(This decision is complicated by the fact that A4 overlaps with A3.)
When traffic changes, rules may need to be re-placed in order to satisfy resource constraints or reduce traffic overhead.
To address these challenges, we propose the design of a system called vCRIB (virtual Cloud Rule Information Base) (Figure 1).
vCRIB provides the abstraction of a centralized repository of rules for the cloud.
Tenants and operators simply install rules in this repository.
Then vCRIB uses network state information including network topology and the traffic information to proactively place rules in hypervisors and/or switches in a way that respects resource constraints and minimizes the redirection traffic.
Proactive rule placement incurs less controller overhead and lower data-path delays than a purely reac- tive approach, but needs sophisticated solutions to optimize placement and to quickly adapt to cloud dynamics (e.g., traffic changes and VM migrations), which is the subject of this paper.
A hybrid approach, where some rules can be inserted reactively, is left to future work.
vCRIB makes several carefully chosen design decisions ( Figure 4) that help address the diverse challenges discussed in Section 2 (Table 1).
It partitions the rule space to break dependencies between rules, where each partition contains rules that can be co-located with each other; thus, a partition is the unit of offloading decisions.
Rules that span multiple partitions are replicated, rather than split; this reduces rule inflation.
vCRIB uses persource partitions: within each partition, all rules have the same VM as the source so only a single rule is required to redirect traffic when that partition is offloaded.
When there is similarity between co-located partitions (i.e., when partitions share rules), vCRIB is careful not to double resource usage (CPU/memory) for these rules, thereby scaling rule processing better.
To accommodate device heterogeneity, vCRIB defines resource usage functions that deal with different constraints (CPU, memory etc.) in a uniform way.
Finally, vCRIB splits the task of finding "good" partition off-loading opportunities into two steps: a novel bin-packing heuristic for resource-aware partition placement identifies feasible partition placements that respect resource constraints, and leverage similarity; and a fast online traffic-aware refinement algorithm which migrates partitions between devices to explore only feasible solutions while reducing traffic overhead.
The split enables vCRIB to quickly adapt to small-scale dynamics (small traffic changes, or migration of a few VMs) without the need to recompute a feasible solution in some cases.
These design decisions are discussed below in greater detail.
The basic idea in vCRIB is to offload the rule processing from source hypervisors and allow more flexible and efficient placement of rules at both hypervisors and switches, while respecting resource constraints at devices and reducing the traffic overhead of offloading.
Different types of rules may be best placed at different places.
For instance, placing access control rules in the hypervisor (or at least at the ToR switches) can avoid injecting unwanted traffic into the network.
In contrast, operations on the aggregates of traffic (e.g., measuring the traffic traversing the same link) can be easily performed at switches inside the network.
Similarly, operations on inbound traffic from the Internet (e.g., load balancing) should be performed at the core/aggregate routers.
Rate control is a task that can require cooperation between the hypervisors and the switches.
Hypervisors can achieve end-to-end rate control by throttling individual flows or VMs [32], but in-network rate control can directly avoid buffer overflow at switches.
Such flexibility can be used to manage resource constraints by moving rules to other devices.However, rules cannot be moved unilaterally because there can be dependencies among them.
Rules can overlap with each other especially when they are derived from different policies.
For example, with respect to Figure 2, a flow from V M6 on server S1 to V M1 on server S2 matches both the rule A3 that blocks the source V M1 and the rule A4 that accepts traffic to destination V M1.
When rules overlap, operators specify priorities so only the rule with the highest priority takes effect.
For example, operators can set A4 to have higher priority.
Overlapping rules make automated rule management more challenging because they constrain rule placement.
For example, if we install A3 on S1 but A4 on ToR1, the traffic from V M6 to V M1, which should be accepted, matches A3 first and gets blocked.One way to handle overlapping rules is to divide the flow space into multiple partitions and split the rule that intersects multiple partitions into multiple independent rules, partition-with-splitting [38].
Aggressive rule splitting can create many small partitions making it flexible to place the partitions at different switches [26], but can increase the number of rules, resulting in inflation.
To minimize splitting, one can define a few large partitions, but these may reduce placement flexibility, since some partitions may not "fit" on some of the devices.
To achieve the flexibility of small partitions while limiting the effect of rule inflation, we propose a partitionwith-replication approach that replicates the rules across multiple partitions instead of splitting them.
Thus, in our approach, each partition contains the original rules that are covered partially or completely by that partition; these rules are not modified (e.g., by splitting).
For example, considering the rule set in Figure 5(a), we can form the three partitions shown in Figure 5(b).
We include both A1 and A3 in P1, the left one, in their original shape.
The problem is that there are other rules (e.g., A2, A7) that overlap with A1 and A3, so if a packet matches A1 at the device where P1 is installed, it may take the wrong action -A1's action instead of A7's or A2's action.
To address this problem, we leverage redirection rules R2 or R3 at the source of the packet to completely cover the flow space of P2 or P3, respectively.
In this way, any packets that are outside P1's scope will match the redirection rules and get directed to the current host of the right partition where the packet can match the right rule.
Notice that the other alternatives described above also require the same number of redirection rules, but we leverage high priority of the redirection rules to avoid incorrect matches.Partition-with-replication allows vCRIB to flexibly manage partitions without rule inflation.
For example, in Figure 5(c), we can place partitions P1 and P3 on one device; the same as in an approach that uses small partitions with rule splitting.
The difference is that since P1 and P3 both have rules A1, A3 and A0, we only need to store 7 rules using partition-with-replication instead of 10 rules using small partitions.
On the other hand, we can prove that the total number of rules using partitionwith-replication is the same as placing one large partition per device with rule splitting (proof omitted for brevity).
vCRIB generates per-source partitions by cutting the flow space based on the source field according to the source IP addresses of each virtual machine.
For example, Figure 6(a) presents eight per-source partitions P0, ·· · , P7 in the flow space separated by the dotted black lines.Per-source partitions contain rules for traffic sourced by a single VM.
Per-source partitions make the placement and refinement steps simpler.
vCRIB only needs one redirection rule installed at the source hypervisor to direct the traffic to the place where the partition is stored.
Unlike per-source partitions, a partition that spans multiple source may need to be replicated; vCRIB does not need to replicate partitions.
Partitions are ordered in the source dimension, making it easy to identify similar partitions to place on the same device.
The central challenge in vCRIB design is the assignment of partitions to devices.
In general, we can formulate this as an optimization problem, whose goal is to minimize the total traffic overhead subject to the resource constraints at each device.
2 This problem, even for partition-with-splitting, is equivalent to the generalized assignment problem, which is NP-hard and even APX-hard to approximate [14].
Moreover, existing approximation algorithms for this problem are inefficient.
We refer the reader to a technical report which discusses this in greater depth [27].
We propose a two-step heuristic algorithm to solve this problem.
First, we perform resource-aware placement of partitions, a step which only considers resource constraints; next, we perform traffic-aware refinement, a step in which partitions reassigned from one device to another to reduce traffic overhead.
An alternative approach might have mapped partitions to devices first to minimize traffic overhead (e.g., placing all the partitions at the source), and then refined the assignments to fit resource constraints.
With this approach, however, we cannot guarantee that we can find a feasible solution in the second stage.
Similar two-step approaches have also been used in the resource-aware placement of VMs across servers [20].
However, placing partitions is more difficult than placing VMs because it is important to colocate partitions which share rules, and placing partitions at different devices incurs different resource usage.Before discussing these algorithms, we describe how vCRIB models resource usage in hypervisors and switches in a uniform way.
As discussed in Section 2, CPU and memory constraints at hypervisors and switches can impact rule placement decisions.
We model resource constraints using a function F (P, d); specifically, F (P, d) is the percentage of the resource consumed by placing partition P on a device d. F determines how many rules a device can store, based on the rule patterns (i.e., exact match, prefix-based matching, and match based on wildcard ranges) and the resource constraints (i.e., CPU, memory).
For example, for a hardware OpenFlow switch d with s TCAM (d) TCAM entries and s SRAM (d) SRAM entries, the resource consumptionF (P, d) = r e (P)/s SRAM (d)+r w (P)/s TCAM (d),where r e and r w are the numbers of exact matching rules and wildcard rules in P respectively.The resource function for Open vSwitch is more complicated and depends upon the number of rules r(P) in the partition P, the number of wildcard patterns w(P) in P, and the rate k(d) of new flow arriving at switch d. Figure 3 shows the number of rules an Open vSwitch can support for different number of wild card patterns.
3 The number of rules it can support reduces exponentially with the increase of the number of wild card patterns (the y-axis in Figure 3 is in log-scale), because Open vSwitch creates a hash table for each wild card pattern and goes through these tables linearly.
For a fixed number of wild card patterns and the number of rules, to double the number of new flows that Open vSwitch can support, we must double the CPU allocation.We capture the CPU resource demand of Open vSwitch as a function of the number of new flows per second matching the rules in partition and the number of rules and wild card patterns handled by it.
Using nonlinear least squares regression, we achieved a good fit for Open vSwitch performance in Figure 3 with the function 4 R 2 is a measure of goodness of fit with a value of 1 denoting a perfect fit.F (P, d) = α(d) × k(d) × w(P) × log Resource-aware partition placement where partitions do not have rules in common can be formulated as a binpacking problem that minimizes the total number of devices to fit all the partitions.
This bin-packing problem is NP-hard, but there exist approximation algorithms for it [21].
However, resource-aware partition placement for vCRIB is more challenging since partitions may have rules in common and it is important to co-locate partitions with shared rules in order to save resources.
P= set of not placed partitions while |P| > 0 do Select a partition P i randomly Place P i on an empty device M k .
repeat Select P j ∈ P with maximum similarity to P i until Placing P j on M k Fails end whileWe use a heuristic algorithm for bin-packing similar partitions called First Fit Decreasing Similarity (FFDS) (Algorithm 1) which extends the traditional FFD algorithm [33] for bin packing to consider similarity between partitions.
One way to define similarity between two partitions is as the number of rules they share.
For example, the similarity between P4 and P5 is |P4 ∩ P5| = |P4| + |P5| − |P4 ∪ P5| = 4.
However, different devices may have different resource constraints (one may be constrained by CPU, and another by memory).
A more general definition of similarity between partitions P i and P k on device d is based on the resource consumption function F : our similarity function F (P i , d) + F (P k , d) − F (P i ∪ P k , d) compares the network resource usage of co-locating those partitions.Given this similarity definition, FFDS first picks a partition P i randomly and stores it in a new device.
5 Next, we pick partitions similar to P i until the device cannot fit more.
Finally, we repeat the first step till we go through all the partitions.For the memory usage model, since we use per-source partitions, we can quickly find partitions similar to a given partition, and improve the execution time of the algorithm from a few minutes to a second.
Since persource partitions are ordered in the source IP dimension and the rules are always contiguous blocks crossing only 5 As a greedy algorithm, one would expect to pick large partitions first.
However, since we have different resource functions for different devices, it is hard to pick the large partitions based on different metrics.
Fortunately, in theory, picking partitions randomly or greedily do not affect the approximation bound of the algorithm.
As an optimization, instead of picking a new device, we can pick the device whose existing rules are most similar to the new partition.
neighboring partitions, we can prove that the most similar partitions are always the ones adjacent to the partition [27]).
For example, P4 has 4 common rules with P5 but 3 common rules with P7 in Figure 6(a).
So in the third step of FFDS, we only need to compare left and right unassigned partitions.To illustrate the algorithm, suppose each server in the topology of Figure 1 has a capacity of four rules to place the partitions and switches have no capacity.
Considering the ruleset in Figure 2(a), we first pick a random partition P4 and place it on an empty device.
Then, we check P3 and P5 and pick P5 as it has more similar rules (4 vs 2).
Between P3 and P6, P6 is the most similar but the device has no additional capacity for A3, so we stop.
In the next round, we place P2 on an empty device and bring P1, P0 and P3 but stop at P6 again.
The last device will contain P6 and P7.We have proved that, FFDS algorithm is 2-approximation for resource-aware placement in networks with only memory-constrained devices [27].
Approximation bounds for CPU-constrained devices is left to future work.Our FFDS algorithm is inspired by the tree-based placement algorithm proposed in [33], which minimizes the number of servers to place VMs by putting VMs with more common memory pages together.
There are three key differences: (1) since we use per-source partitions, it is easier to find the most similar partitions than memory pages; (2) instead of placing sub-trees of VMs in the same device, we place a set of similar partitions in the same device since these similar partitions are not bounded by the boundaries of a sub-tree; and (3) we are able to achieve a tighter approximation bound (2, instead of 3).
(The construction of sub-trees is discussed in a technical report [27]).
Finally, it might seem that, because vCRIB uses persource partitions, it cannot efficiently handle a rule with a wildcard on the source IP dimension.
Such a rule would have to be placed in every partition in the source IP range specified by the wildcard.
Interestingly, in this case vCRIB works quite well: since all partitions on a machine will have this rule, our similarity-based placement will result in only one copy of this rule per device.
The resource-aware placement places partitions without heed to traffic overhead since a partition may be placed in a device other than the source, but the resulting assignment is feasible in the sense that it respects resource constraints.
We now describe an algorithm that refines this initial placement to reduce traffic overhead, while still maintaining feasibility.
Having thus separated placement and refinement, we can run the (usually) fast refinement after small-scale dynamics (some kinds of traffic changes, VM migration, or rule changes) that do not violate resource feasibility.
Because each per-source partition matches traffic from exactly one source, the refinement algorithm only stores each partition once in the entire network but tries to migrate it closer to its source.Given per-source partitions, an overhead-greedy heuristic would repeatedly pick the partition with the largest traffic overhead, and place it on the device which has enough resources to store the partition and the lowest traffic overhead.
However, this algorithm cannot handle dynamics, such as traffic changes or VM migration.
This is because in the steady state many partitions are already in their best locations, making it hard to rearrange other partitions to reduce their traffic overhead.
For example, in Figure 6(a), assume the traffic for each rule (excluding A0) is proportional to the area it covers and generated from servers in topology of Figure 6(b).
Suppose each server has a capacity of 5 rules and we put P4 on S4 which is the source of V M4, so it imposes no traffic overhead.
Now if V M2 migrates from S1 to S4, we cannot save both P2 and P4 on S4 as it will need space for 6 rules, so one of them must reside on ToR2.
As P2 has 3 units deny traffic overhead on A1 plus 2 units of accept traffic overhead from local flows of S4, we need to bring P4 out of its sweet spot and put P2 instead.
However, the overhead-greedy algorithm cannot move P4 as it is already in its best location.To get around this problem, it is important to choose a potential refinement step that not only considers the benefit of moving the selected partition, but also considers the other partitions that might take its place in future refinement steps.
We do this by calculating the benefit of moving a partition P i from its current device d(P i ) to a new device j, M(P i , j).
The benefit comes from two parts: (1) The reduction in traffic (the first term of Equation 1); (2) The potential benefit of moving other partitions to d(P i ) using the freed resources from P i , excluding the lost benefit of moving these partitions to j because P i takes the resources at j (the second term of Equation 1).
We define the potential benefit of moving other partitions to a device j as the maximum benefits of moving a partition P k from a device d to j, i.e.,Q j = max k,d (T (P k , d) − T (P k , j)).
We speed up the calculation of Q j by only considering the current device of P k and the best device b(P k ) for P k with the least traffic overhead.
(We omit the reasons for brevity.)
In summary, the benefit function is defined as:M(P i , j) = (T (P i , d(P i )) − T (P i , j)) + (Q d(P i ) − Q j ) (1)Our traffic-aware refinement algorithm is benefitgreedy, as described in Algorithm 2.
The algorithm is given a time budget (a "timeout") to run; in practice, we Update b(P i ) and Q(d) while not timeout do Update the benefit of moving every P i to its best feasible target device M(P i , b(P i )) Select P i with the largest benefit M(P i , b(P i )) Select the target device j for P i that maximizes the benefit M(P i , j) Update best feasible target devices for partitions and Q's end while return the best solution found have found time budgets of a few seconds to be sufficient to generate low traffic-overhead refinements.
At each step, it first picks that partition P i that would benefit the most by moving to its best feasible device b(P i ), and then picks the most beneficial and feasible device j to move P i to.
6 We now illustrate the benefit-greedy algorithm (Algorithm 2) using our running example in Figure 6(b).
The best feasible target device for both P2 and P4 are ToR2.
P2 maximizes Q S4 with value 5 because its deny traffic is 3 and has 1 unit of accept traffic to V M4 on S4.
Also we assume that Q j is zero for all other devices.
In the first step, the benefit of migrating P2 to ToR2 is larger than moving P4 to ToR2, while the benefits of all the other migration steps are negative.
After moving P2 to ToR2 the only beneficial step is moving P4 out of S4.
After moving P4 to ToR2, migrating P2 to S4 become feasible, so Q S4 will become 0 and as a result the benefit of this migration step will be 5.
So the last step is moving P2 to S4.An alternative to using a greedy approach would have been to devise a randomized algorithm for perturbing partitions.
For example, a Markov approximation method is used in [20] for VM placement.
In this approach, checking feasibility of a partition movement to create the links in the Markov chain turns out to be computationally expensive.
Moreover, a randomized iterative refinement takes much longer to converge after a traffic change or a VM migration.at the controller, and end-to-end delay between detecting traffic changes and re-installing the rules.
Topology: Our simulations use a three-level fat-tree topology with degree 16, containing 1024 servers in 128 racks connected by 320 switches.
Since current hypervisor implementations can support multiple concurrent VMs [31], we use 20 VMs per machine.
We consider two models of resource constraints at the servers: memory constraints (e.g., when rules are offloaded to a NIC), and CPU constraints (e.g., in Open vSwitch).
For switches, we only consider memory constraints.Rules: Since we do not have access to realistic data center rule bases, we use ClassBench [35] to create 200K synthetic rules each having 5 fields.
ClassBench has been shown to generates rules representative of real-world access control.VM IP address assignment: The IP address assigned to a VM determines the number of rules the VM matches.
A random address assignment that is oblivious to the rules generated in the previous set may cause most of the traffic to match the default rule.
Instead, we use a heuristic -we first segment the IP range with the boundaries of rules on the source and destination IP dimensions and pick random IP addresses from randomly chosen ranges.
We test two arrangements: Random allocation which assigns these IPs randomly to servers and Range allocation which assigns a block of IPs to each server so the IP addresses of VMs on a server are in the same range.
Following prior work, we use a staggered traffic distribution (ToRP=0.5, PodP=0.3, CoreP=0.2) [8].
We assume that each machine has an average of 1K flows that are uniformly distributed among hosted VMs; this represents larger traffic than has been reported [10], and allows us to stress vCRIB.
For each server, we select the source IP of a flow randomly from the VMs hosted on that machine and select the destination IP from one of the target machines matching the traffic distribution specified above.
The protocol and port fields of flows also affect the distribution of used rules.
The source port is wildcarded for ClassBench rules so we pick that randomly.
We pick the destination port based on the protocol fields and the port distributions for different protocols (This helps us cover more rules and do not dwell on different port values for ICMP protocol.)
.
Flow sizes are selected from a Pareto distribution [10].
Since CPU processing is impacted by newly arriving flows, we marked a subset of these flows as new flows in order to exercise the CPU resource constraint [10].
We run each experiment multiple times with different random seeds to get a stable mean and standard deviation.
The goal of vCRIB rule placement is to minimize the traffic overhead given the resource constraints.
To calibrate vCRIB's performance, we compare it against SourcePlacement, which stores the rules at the source hypervisor.
Our metric for the efficacy of vCRIB's performance is the ratio of traffic as a result of vCRIB's rule placement to the traffic incurred as a result of SourcePlacement (regardless of whether SourcePlacement is feasible or not).
When all the servers have enough capacity to process rules (i.e., SourcePlacement is feasible), it incurs lowest traffic overhead; in these cases, vCRIB automatically picks the same rule placement as SourcePlacement, so here we only evaluate cases that SourcePlacement is infeasible.
We begin with memory resource model at servers because of its simpler similarity model and later compare it with CPU-constrained servers.vCRIB uses similarity to find feasible solutions when SourcePlacement is infeasible.
With Range IP allocation, partitions in the Source IP dimension which are similar to each other are saved on one server, so the average load on machines is smaller for SourcePlacement.
However, there may still be a few overloaded machines that result in an infeasible SourcePlacement.
With Random IP allocation, the partitions on a server have low similarity and as a result the average load of machines is larger and there are many overloaded ones.
Having the maximum load of machines above 5K in all runs for both Range and Random cases, we set a capacity of 4K for servers and 0 for switches ("4K 0" setting) to make SourcePlacement infeasible.
vCRIB could successfully fit all the rules in the servers by leveraging the similarities of partitions and balancing the rules.
The power of leveraging similarity is evident when we observe that in the Random case the average number of rules per machine (4.2K) for SourcePlacement exceeds the server capacity, yet vCRIB finds a feasible placement by saving similar partitions on the same machine.
Moreover, vCRIB finds a feasible solution when we add switch capacity and uses this capacity to optimize traffic (see below), yet SourcePlacement is unable to offload the load.vCRIB finds a placement with low traffic overhead.
Figure 7(a) shows the traffic ratio between vCRIB and SourcePlacement for the Range and Random cases with error bars representing standard deviation for 10 runs.
For the Range IP assignment, vCRIB minimizes the traffic overhead under 0.1%.
The worst-case traffic overhead for vCRIB is 21% when vCRIB cannot leverage rule processing in switches to place rules and the VM IP address allocation is random, an adversarial setting for vCRIB.
The reason is that in the Random case the arrangement of the traffic sources is oblivious to the similarity of partitions.
So any feasible placement depending on similarity puts partitions far from their sources and incurs traffic overhead.
When it is possible to process rules on switches, vCRIB's traffic overhead decreases dramatically (6% (3%) for 4K (6K) rule capacity in internal switches); in these cases, to meet resource constraints, vCRIB places partitions on ToR switches on the path of traffic, incurring minimal overhead.
As an aside, these results illustrate the potential for using vCRIB's algorithms for provisioning: a data center operator might decide when, and how much, to add switch rule processing resources by exploring the trade-off between traffic and resource usage.vCRIB can also optimize placement given CPU constraints.We now consider the case where servers may be constrained by CPU allocated for rule processing (Figure 7(b)).
We vary the CPU budget allocated to rule processing (10%, 20%, 40%) in combination with zero, 4K or 6K memory at switches.
For example in case "40 0" (i.e., each server has 40% CPU budget, but there is no capacity at switches), SourcePlacement results in an infeasible solution, since the highest CPU usage is 56% for range IP allocation and 42% for random IP allocation.
In contrast, vCRIB can find feasible solutions in all the cases except "10 0" case.
When we have only 10% CPU budget at servers, vCRIB needs some memory space at the switches (e.g., 4K rules) to find a feasible solution.
With a 20% CPU budget, vCRIB can find a feasible solution even without any switch capacity ("20 0").
With higher CPU budgets, or with additional switch memory, vCRIB's traffic overhead becomes negligible.
Thus, vCRIB can effectively manage heterogeneous resource constraints and find low traffic-overhead placement in these settings.
Unlike with memory constraints, Range IP assignment with CPU constraints does not have a lower average load on servers for SourcePlacement, nor does it have a feasible solution with lower traffic overhead, since with the CPU resource usage function closer partitions in the source IP dimension are no longer the most similar.
We now study how resource usage and traffic overhead are spatially distributed across a data center for the Random case.
vCRIB is effective in leveraging on-path and nearby devices.
Figure 8(a) shows the case where servers have a capacity of 4K and switches have none.
We classify the rules into deny rules, accept rules whose traffic stays within the rack (labelled as "ToR"), within the Pod ("Pod"), or goes through the core routers ("Core").
In general, vCRIB may redirect traffic to other locations away from the original paths, causing traffic overhead.
We thus classify the traffic overhead based on the hops the traffic incurs, and then normalize the overhead based on the traffic volume in the SourcePlacement approach.
Adding the percentage of traffic that is handled in the same rack of the source for deny traffic (8.8%) and source or destination for accept traffic (1.8% ToR, 2.2% POD, and 1.6% Core), shows that out of 21% traffic overhead, about 14.4% is handled in nearby servers.Most traffic overhead vCRIB introduces is within the rack.
Figure 8(b) classifies the locations of the extra traffic vCRIB introduces.
vCRIB does not require additional bandwidth resources at the core links; this is advantageous, since core links can limit bisection bandwidth.
In part, this can be explained by the fact that only 20% of our traffic traverses core links.
However, it can also be explained by the fact that vCRIB places partitions only on ToRs or servers close to the source or destination.
For example, in the "4K 0" case, there is 29% traffic overhead in the rack, 11% in the Pod and 2% in the core routers, and based on Figure 8(c) all partitions are saved on servers.
However, if we add 4K capacity to internal switches, vCRIB will offload some partitions to switches close to the traffic path to lower the traffic overhead.
In this case, for accept rules, the ToR switch is on the path of traffic and does not increase traffic overhead.
Note that the servers are always full as they are the best place for saving partitions.
The IP assignment method, traffic locality and rules in partitions can affect vCRIB performance in finding a feasible solution with low traffic.
Our previous evaluations have explored uniform IP assignment for two extreme cases Range and Random above.
We have also evaluated a skewed distribution of the number of IPs/VMs per machine but have not seen major changes in the traffic overhead.
In this case, vCRIB was still able to find a nearby machine with lower load.
We also conducted another experiment with different traffic locality patterns, which showed that having more non-local flows gives vCRIB more choices to offload rule processing and reach feasible solutions with lower traffic overhead.
Finally, experiments on FFDS performance for different machine capacities [27] also validates its superior performance comparing to the tree-based placement [33].
Beyond these kinds of analyses, we have also explored the parameter space of similarity and partition size, which we discuss next.
vCRIB uses similarity to accommodate larger partitions.
We have explored two properties of the rules in partitions by changing the ruleset.
In Figure 9, we define a two dimensional space: one dimension measures the average similarity between partitions and the other the average size of partitions.
Intuitively, the size of partitions is a measure of the difficulty in finding a feasible solution and similarity is the property of a ruleset that vCRIB exploits to find solutions.
To generate this figure, we start from an infeasible setting for SourcePlacement with a maximum of 5.7K rules for "4k 0" setting and then change the ruleset without changing the load on the maximum loaded server.
We then explore the two dimensions as follows.
Starting from the ClassBench ruleset and Range IP assignment, we split rules into half in the source IP dimension to decrease similarity without changing partition sizes.
To increase similarity, we extend a rule in source IP dimension and remove rules in the extended area to maintain the same partition size.Adding or removing rules matching only one VM (micro rules), also help us change average partitions size without changing the similarity.
Unfortunately, removing just micro rules is not enough to explore the entire range of partition sizes, so we also remove rules randomly.
Figure 9(a) presents the feasibility region for vCRIB regardless of traffic overhead.
Since average similarity cannot be more than the average partition size, the interesting part of the space is below the 45 • .
Note that vCRIB is able to cover a large part of the space.
Moreover, the shape of the feasibility region shows that for a fixed average partition size, vCRIB works better for partitions with larger similarity.
This means that to handle larger partitions, vCRIB needs more similarity between partitions; however, this relation is not linear since vCRIB may not be able to utilize the available similarity given limits on server capacity.
When considering only solutions with less than 10% traffic overhead, vCRIB's feasibility region (Figure 9(b)) is only slightly smaller.
This figure demonstrates vCRIB's utility: for a small additional traffic overhead, vCRIB can find many additional operating points in a data center that, in many cases, might have otherwise been infeasible.We also tried a different method for exploring the space, by tuning the IP selection method on a fixed ruleset, and obtained qualitatively similar results [27].
Figure 10 compares benefit-greedy (with timeout 10 seconds) with overhead-greedy and a randomized algorithm 7 after a single VM migration for the 4K 0 case.
Each point in Figure 10 shows a step in which one partition is moved, and the horizontal axis is time in log scale.
At time A, we migrate a VM from its current server S old to a new one S new , but S new does not have any space for the partition of the VM, P.
As a result, P remains on S old and the traffic overhead increases by 40MBps.
Both benefit-greedy and overhead-greedy move the partition P for the migrated VM to a server in the rack containing S new at time B and reduce traffic by 20Mbps.
At time B, benefit-greedy brings out two partitions from their current host S new to free up the memory for P while imposing a little traffic overhead.
At time C, benefit-greedy moves P to S new and reduces traffic further by 15Mbps.
The entire process takes only 5 seconds.
In contrast, the randomized algorithm takes 100 seconds to find the right partitions and thus is not useful with these dynamics.We then run multiple VM migrations to study the average behavior of benefit-greedy with 5 and 10 seconds timeout.
In each 20 seconds interval, we randomly pick a VM and move it to another random server.
Our simulations last for 30 minutes.
The trend of data cen-7 Markov Approximation [20] with target switch selection probability ∝ exp(traffic reduction of migration step) ter traffic in Figure 11 shows that benefit-greedy maintains traffic levels, while overhead-greedy is unable to do so.
Over time, benefit-greedy (both configurations) reduces the average traffic overhead around 34 MBps, while overhead-greedy algorithm increases the overhead by 117.3 MBps.
Besides, this difference increases as the interval between two VM migration increases.
We built vCRIB prototype using Open vSwitch [4] as servers and switches, and POX [1] as the platform for vCRIB controller for micro-benchmarking.
Overhead of collecting traffic information: In our prototype, we send traffic information collected from each server's Open vSwitch kernel module to the controller.
Each piece of information requires 13 Bytes for 5 tuples 8 and 2 Bytes for the traffic change volume.Since we only need to detect traffic changes at the rulelevel, we can more aggressively filter the traffic information than traditional traffic engineering solutions [11].
The vCRIB controller sets a threshold δ (F) for traffic changes of a set of flows F and sends the threshold to the servers.
The servers then only report traffic changes above δ (F).
We set the threshold δ for two different granularities of flow sets F.
A larger set F makes vCRIB less sensitive to individual flow changes and leads to less reporting overhead but incurs less accuracy.
(1) We set F as the volume each rule for each destination server in each per-source partition.
(2) We assume all the rules in a partition have accept actions (as the worst case for traffic).
Thus, the vCRIB controller sets the threshold that affects the size of traffic to each destination server for each per-source partition (summing up all the rules).
If there are 20 flow changes above the threshold, we need to send 260B/s per server, which means 20Mbps for 10K servers in the data center.
For VM migrations and rule insertion/deletion, the vCRIB controller can be notified directly by the the data center management system.Controller overhead: We measure the delay of processing 200K ClassBench rules.
Initially, the vCRIB controller partitions these rules, runs the resource-aware placement algorithm and the traffic-aware refinement to derive an initial placement; this takes up to five minutes.
However, these recomputations are triggered only when a placement becomes infeasible; this can happen after a long sequence of rule changes or VM add/remove.
The traffic overhead of rule installation and removal depends on the number of refinement steps and the number of rules per partition.
The size of OpenFlow command for a rule entry is 100 Bytes, so if a partition has 1K rules, the overhead of removing it from one device and installing at another device is 200KB.
For each VM migration, which needs an average of 11 partitions, the bandwidth overhead of moving the rules is 11×200KB=2.2MB.Reaction to cloud dynamics: We evaluate the latency of handling traffic changes by deploying our prototype in a topology with five switches and six servers as shown in Figure 1.
We deploy a vCRIB controller that connects with all the devices with an RTT of 20 ms. We set the capacity of each server/switch as large enough to store at most one partition.
We then inject a traffic change pattern that causes vCRIB to swap two partitions and add a redirection rule at a VM.
It takes vCRIB 30ms to detect the traffic changes, and move the rules to the new locations.
Our work is inspired by several different strands of research, each of which we cover briefly.Policies and rules in the cloud: Recent proposals for new policies often propose customized systems to manage rules on either hypervisors [4,13,32,30]) or switches [3,8,29].
vCRIB proposes an abstraction of a centralized rule repository for all the policies, frees these systems from the complexity inherent in the rule management, and handles heterogeneous resource constraints at devices while minimizing the traffic overhead.Rule management in software-defined networks (SDNs): Recent work on SDNs provides rule repository abstractions and some rule management capabilities [12,23,38,13].
vCRIB focuses on data centers, which are more dynamic, more sensitive to traffic overhead, and face heterogeneous resource constraints.Distributed firewall: Distributed firewalls [9,19], often used in enterprises, leverage a centralized manager to deploy security policies on edge machines.
vCRIB manages more fine-grained rules on flows and VMs for various policies including firewalls in the cloud.
Rather than placing these rules at the edge, vCRIB places these rules taking into account the rule processing constraints, while minimizing traffic overhead.Rule partition and placement solutions: The problem of partitioning and placing multi-dimensional data at different locations also appears in other contexts.
Unlike traditional partitioning algorithms [36,34,16,25,24] which divide rules into partitions using a top-down approach, vCRIB uses per-source partitions to place the partitions close to the source with low traffic overhead.
Compared with DIFANE [38], which randomly places a single partition of rules at each switch, vCRIB takes the partitions-with-replication approach to flexibly place multiple per-source partitions at one device.
In preliminary work [26], we proposed an offline placement solution which works only for the TCAM resource model.
The paper has a top-down heuristic partition-with-split algorithm which cannot limit the overhead of redirection rules and is not optimized for CPU-based resource model.
Besides, having partitions with traffic from multiple sources requires complicated partition replication to minimize traffic overhead.
In contrast, vCRIB uses fast per-source partition-with-replication algorithm which reduces TCAM-usage by leveraging similarity of partitions and restricts the resource usage of redirection by using limited number of equal shaped redirection rules.
Our preliminary work used an unscalable DFS branch-andbound approach to find a feasible solution and optimized the traffic in one step.
vCRIB scales better using a twophase solution where the first phase has an approximation bound in finding a feasible solution and the second can be run separately when the placement is still feasible.
vCRIB, is a system for automatically managing the finegrained rules for various management policies in data centers.
It jointly optimizes resource usage at both switches and hypervisors while minimizing traffic overhead and quickly adapts to cloud dynamics such as traffic changes and VM migrations.
We have validated its design using simulations for large ClassBench rulesets and evaluation on a vCRIB prototype built on Open vSwitch.
Our results show that vCRIB can find feasible placements in most cases with very low additional traffic overhead, and its algorithms react quickly to dynamics.
