The Internet of Things (IoT) represents a new class of applications that can benefit from cloud infrastructure.
However, the current approach of directly connecting smart devices to the cloud has a number of disadvantages and is unlikely to keep up with either the growing speed of the IoT or the diverse needs of IoT applications.
In this paper we explore these disadvantages and argue that fundamental properties of the IoT prevent the current approach from scaling.
What is missing is a well-architected system that extends the functionality of the cloud and provides seamless interplay among the heterogeneous components in the IoT space.
We argue that raising the level of abstraction to a data-centric design-focused around the distribution, preservation and protection of information-provides a much better match to the IoT.
We present early work on such a distributed platform, called the Global Data Plane (GDP), and discuss how it addresses the problems with the cloud-centric architecture .
The market has seen an explosion in the number of smart devices.
These latest devices offer rich interactivity by connecting to computing platforms and services [45,50].
Featured by the growth of Internet connectivity and the augmentation of everyday things, this shift is commonly referred to as the Internet of Things (IoT) [25].
The IoT industry has benefited tremendously from the economic model of the cloud.
With little investment in the infrastructure, even novice users can start collecting sensor data and streaming it back to the cloud [24].
Several IoT cloud platforms [5,7,19,22] have gone further by offering easy-to-use APIs, data processing, visualization, and sample code for various hardware platforms.On the "thing" side, hardware platforms such as Arduino [1], Raspberry Pi [15], and BeagleBone Black [4] have allowed easy and cheap prototyping for customized IoT applications.
Companies [13,17,18] in this space offer complete solutions including hardware gateways, smartphone applications, web portals and cloud services.With this blizzard of activity, the current trend seems to be that all peripherals, including sensors and actuators, communicate directly with the cloud and interact with each other through web services [51].
At first glance, this seems to be a natural architecture for IoT applications.
However, several significant problems are revealed on closer inspection, including issues with privacy, security, scalability, latency, bandwidth, availability and durability control.
While these problems are not new to typical web applications, they are exacerbated in the IoT space because of the fundamental differences between IoT and web services (see Sec. 3).
Our analysis suggests a need for a higher layer of abstraction for the IoT-one that more naturally fits the requirements of IoT applications while exploiting the underlying computing platforms that enable the IoT (like the cloud, the Fog [30] and gateways [62]).
Our proposed abstraction is centered around data.
It is focused on the transport, replication, preservation, and integrity of streams of data while enabling transparent optimization for locality and quality of service.
We call the resulting infrastructure the Global Data Plane (GDP).
Its foundation is the concept of a single-writer append-only log, coupled with location-independent routing, overlay multicast and higher level interfaces such as common access APIs (see Sec. 4).
The contributions of this paper are as follows:• We present and summarize the state-of-the-art trends on IoT architecture design.
• We analyze the shortcomings of the existing architecture by explaining the fundamental differences between IoT applications and web services.
• We propose the design of a data-centric system for IoT applications.
Since this is an ongoing effort, we focus mainly on the design experience with GDP thus far.In this section, we review the state of the art in the distributed application space for IoT.
The cloud has become the de facto foundation around which distributed applications are constructed-a trend that, while understandable, is not the best long-term approach (see Section 3).
Over the last few years, cloud computing has shaped the software industry and made the development and deployment of web services easier than ever.
Public cloud providers such as Amazon, Microsoft, Google, Rackspace offer pay-as-you-go services for the general public.
Such a service model has reduced capital expenses, enabled elasticity for dynamic load adaption, and simplified resource management [24].
We have seen a huge recent trend of migrating computations and services to the cloud.
For example, over two trillion objects were reported stored in Amazon S3 as of April 2013 [28].
Riding on this popularity, IoT application developers have adopted the cloud as a universal computation resource and a storage backend.
This approach has been taken by industrial efforts (such as Carriots [5], GroveStreams [7], SAMI [22], Xively [19]), as well as academic research [44,62].
At the same time, we have seen a dizzying array of embedded platforms, from powerful computing units to lowpower microcontrollers (see Table 1 for some examples).
Below are three categories of embedded platforms: 1.
Smartphones: Many companies (like Fitbit [6] or Automatic [3]) use smartphones as sensing devices as well as gateways to connect other low-power devices to the network.
Researchers have explored how smartphones can be used for IoT including reusing discarded smartphones [32], writing new operating systems [10] and developing novel applications [47].
2.
Mini PC: Ranging from the powerful Mac Mini and Intel Next Unit of Computing (NUC) to inexpensive Raspberry Pi and BeagleBone Black, these devices typically run various versions of Linux to simplify application deployment.
Many companies [13,17,18] adopt these mini PCs as their gateway devices.
3.
Microcontroller platforms: Examples include Arduino [1], mbed [2], and Particle [14].
This is an emerging category; there are new open platforms on crowdfunding websites [11], good ecosystems, great support and libraries (e.g. Adafruit Online Tutorials [12]) and novel applications/products [9].
1 The original authors noted "Customer buyback price quoted by Sprint for a smartphone in good condition" [32].
Table 1: The world of IoT includes a wide spectrum of computing platforms.
Many of today's IoT solutions arise by connecting embedded platforms to the cloud.
For example, Bolt [44] provides data management for the Lab of Things (LoT) [31] and uses Amazon S3 or Azure for data storage.
2 Such direct connections often require an application-level gateway [62] to support low-power radios such as Z-Wave or Bluetooth Low Energy (BLE).
Companies tend to provide their own gateways (such as Ninja Sphere [13], SmartThings Hub [17], Wink Hub [18]); and researchers adopt a similar approach (e.g. HomeHub for the LoT [31]).
The fact that custom gateways are an integral part of IoT applications leads directly to "stovepipe" solutions or balkanization.
Data and services from one company cannot be shared or utilized by devices from another company: connection protocols, data formats, and security mechanisms (when present) are proprietary and often undocumented.
To date, IoT applications fall into two general categories:Ambient data collection and analytics: These applications involve sensors installed in buildings [36], at homes [46], in cities [16], and on humans themselves 3 [6,59].
Normally, data is not immediately inspected and the collected data is later processed for analytics [48].
The trend of data collection is constantly growing and many researchers have predicted a new big-data problem [37,63].
One thing to note is that many of the sensed data have serious privacy implications (for personal health, operational security, etc.).
Real-time applications with low-latency requirements: These applications could be reactive environments with humans in the loop [34].
An upper latency limit to avoid a notice by human participants is about 100 ms [53].
These applications could also be autonomous systems where humans are not involved (such as robots taking actions based on sensors).
In this case, a tight control over latency is important for deterministic applications [39].
Tight latency requirements are often incompatible with the unpredictable performance of cloud-based analytics or controllers.
In this section, we argue why the current approach of connecting IoT devices directly to the cloud is incompatible with the evolving world of IoT applications.
This incompatibility arises from the fundamental nature of IoT applications.
Our reasoning is as follows:1.
Privacy and Security.
Sensors implanted in our surrounding environment collect extremely sensitive information.
In a recent talk given by Wadlow [60], he described the IoT as "hundreds of computers that are aware of me, can talk about me, and are out of my control."
This is a strong call for intrinsic security and privacy.
This need is echoed in many critical posts and talks (such as "Internet of Crappy Things" [38], "The Internet of Fails" [56]).
The FTC's Technical Report [41] also emphasizes security in the IoT spaces.
As a centralized resource out of users' control, the cloud resents an ever-present opportunity to violate privacy.
Today, privacy has become a luxury [23], a situation that will be exacerbated in the IoT.2.
Scalability.
By 2020, Cisco estimates 50 billion [40] devices will be connected to the cloud, while Gartner estimates 26 billion [52].
Scalability in the IoT spaces will be more challenging than web-scale or Internet-scale applications; the amount of data generated will easily exceed the reported trillion objects in Amazon S3 [28].
The bisection bandwidth requirements for a centralized cloud solution are staggering, especially since most data acquired by IoT devices can or should be processed locally and immediately discarded.3.
Modeling: peripheral devices are physical.
Both sensors and actuators are physically present devices in our environment.
Although sensor data can be collected and replicated (similar to virtualizing sensors [61]), the data is still generated from the edge of the network.
Moreover, actuators cannot be virtualized and oftentimes the actuations cannot be rolled back.
This is significantly different from the model of web services today.4.
Latency: The cloud model differs from reality.
Application developers view the cloud as a component that interconnects the smart devices.
However, from a network point of view, the cloud is on the edge of the network (see Fig. 1).
Even simple IoT applications, such as those that turn on a fan in response to a rise of the local temperature, will experience unpredictable latencies from sensing, wireless transmission, gateway processing, Internet delivery, and cloud processing.
For example, a single Dropcam requires "a high speed internet connection with at least 0.5 Mbps" to use its service [8].
Even simple sensors, such as energy meters, can benefit from a higher sampling rate (the motivation of 1 kHz energy data with ground-truth from the Ubicomplab at the University of Washington [43] and 15 kHz sampling of energy from MIT REDD Dataset [48]).6.
Quality of Service (QoS) Guarantees.
Web users tolerate variable latency and occasional loss of web services.
In contrast, the temporary unavailability of sensors or actuators within IoT applications will directly impact the physical world.
While significant engineering effort has been put into improving the availability and latency profile of the cloud (allowing Service Level Agreements), such efforts are stymied by operator error, software bugs, DDoS attacks, and normal packet-to-packet variations from wide-area routing.
Further, the Internet connection to people's homes is far from perfect.
Over 10% of home networks in the developed world see connectivity interruptions more frequently than once every 10 days [42]; this situation is worse in developing countries.7.
Durability Management.
Some sensor data is ephemeral, while other data should be durable against global disasters.
For ephemeral data, there is no effective way of verifying the data has been completely destroyed because the cloud is out of the user's control.
For durable data, regardless of the promised guarantees [21], the reliability of cloud storage remains a major concern and there is active research in this direction [29].
Moreover, whatever durability is achieved by the cloud, it is typically done so without concern for application-specific privacy or export rules.
Note that control over durability is closely related to control in general: making sure that users retain the control and ownership over their data rather than providers.
The Global Data Plane (GDP) is a data-centric abstraction focused around the distribution, preservation, and protection of information.
It supports the same application model as the cloud, while better matching the needs and characteristics of the IoT by utilizing heterogeneous computing platforms, such as small gateway devices, moderately powerful nodes in the environment and the cloud, in a distributed manner.As shown in Fig. 2, the GDP interface provides a new "narrow waist" upon which applications are constructed.
The basic foundation of the GDP is the secure, singlewriter log.
Logs in the GDP are lightweight, durable, and they support multiple simultaneous readers-either through random access (pull-based) or subscription (pushbased).
Logs have no fixed location but rather are migrated as necessary to meet locality, privacy, or QoS needs of applications.Applications are built on top of the GDP by interconnecting log streams, rather than by addressing devices or services via IP.
Each sensor or computational element of an IoT application has its own unique output log in the GDP and writes timestamped entries to this log.
Actuators read from a unique input log.
The GDP masks the heterogeneity of underlying communication paradigms, network/storage devices, and physical connections; and on top, it supports a wide variety of Common Access Application Program Interfaces (CAAPIs) for applications.We detail a few key design decisions below:1.
Single-writer time-series logs: For each IoT device or application component that generates data, this data is represented as a log where the owner has the sole write permission.
This model is based on our observation that peripherals are physical devices in our environment.
We assume that devices have cryptographic keys for signing and encryption.
4 Logs are append-only; most data is readonly and can be securely replicated and validated through cryptographic hashes.
For each log, our current design exposes append, read and subscribe APIs.
The single-writer model allows the following properties:• Flexibility: The log interface is minimum but complete.
Aggregations of logs or CAAPIs (discussed below) can be built by composition.
In part (a) of Fig. 3, a new log is created by composing two existing ones and writing back to the GDP.
• Access Control: Since devices and services have associated public-key identities, each log has a single authorized writer.
An append operation is permitted only when signed by the appropriate writer's key.
For read operations, only those with an appropriate decryption 4 In case of extremely low power sensors, the cryptographic operations could be performed by a more powerful gateway device.
key can decrypt the data, providing for a way to implement read-access control policies; a variety of more complex access control policies can be constructed through hierarchical key management or selected use of trusted environments.
• Authenticity and integrity: Since only signed append operations are allowed, accidental or malicious corruption of the log won't occur and substitution attacks are easily detected.
A variety of traditional consistency problems are replaced with the simpler problem of finding the latest update.
5 • Encryption: We envision that all data written to the log is encrypted with the encryption key held by the writer.
A single writer with a single encryption key simplifies the key management challenges.
• Durability and replication: In contrast to the cloud where users rely on whatever durability the cloud providers offer, our model enables the choice of the level of durability and geographic span of replication on a per log basis.
The log model also simplifies replica consistency as previously mentioned.
2.
Location-independent Routing: Logs must be physically stored in the infrastructure.
As previously discussed, the current reliance of IoT on cloud storage provides few guarantees about the placement, latency of access, or durability of information.
Instead, to embrace heterogeneous platforms and support a variety of storage policies, the GDP employs location-independent routing in a large, 256-bit address space.
To meet the goal of flexible placement, controllable replication and easy migration, packets are routed through an overlay network that uses Distributed Hash Table (DHT) technology.
DHT addresses the challenges of scalability [54,58,65] with the sacrifice of an increased number of overlay hops.
GDP optimizes latency through log migration (see Fig. 3(d)) and dynamic changes to the routing topology.
Logs are named with a 256-bit identifier which may be derived from a cryptographic hash of the owner's public key and metadata.
Following a variety of placement and replication policies, 6 the GDP places logs within the infrastructure and advertises the location of these logs to the underlying routing layer.
Such placement and replication policies can optimize for latency, QoS, privacy, durability, and so forth.
Internally, logs are further split into chunks, and each chunk can be distributed for durability [49] and performance [44] (see Fig. 3(b)).
The publish/subscribe pattern has been shown to support a wide variety of fundamental communication services (for mobility, multicast, anycast [57]).
This fits nicely with our log abstraction and can support building interactive applications.
To alleviate the growth of sensor data bandwidth, when multiple subscribers exist, multicast trees can be built on top of the overlay network using techniques proposed earlier [27,65] so that effective bandwidth is reduced [33] (see Fig. 3(c)).
Although the singlewriter log abstraction shelters developers from low-level machine and communication primitives, many applications are likely to need more common APIs or data structures [26].
In fact, logs are sufficient to implement any convenient, mutable data storage repository.
Thus, Fig. 2 shows a CAAPI layer on top of the GDP.
A CAAPI can provide key-value store, file system or database operations.
Since logs serve as the ground truth, the benefit of consistency, durability, scalability and availability are carried over to CAAPIs for free.
However CAAPIs may need to replay the logs if the service fails; in this case, checkpointing can be employed to avoid expensive log replay.Our design for the GDP is not yet bullet-proof and our initial implementation has not withstood the test of widescale deployment.
Nonetheless, we believe that the core concepts of GDP overcome the pitfalls mentioned 6 How to specify a policy about where logs are placed is out of the scope of this paper.
We leave it as a future work.
in Sec. 3 in the following way: the single-writer, appendonly log models sensor data more accurately; integrity and authentication by design provides better privacy and security; the distributed nature with peer-to-peer technology makes scalability possible; explicit separation of policy from mechanism enables better control on level of durability for end users; and finally, latency, bandwidth and QoS guarantees are enabled by the integration of the cloud and the local infrastructure.
Other efforts exist to address the challenges of IoT.
Cisco's Fog Computing [30] provides computing resources closer to the edge of the network.
We believe that our arguments strengthen the need for fog-like computing platforms and our proposed GDP architecture can leverage such resources.
Also relevant are systems such as EdgeComputing from Akamai [35], Intel's Intelligent Edge [20], and Microsoft's Cloudlet [55].
The role of servers in these architectures seems to emphasize on being intelligent gateways or proxies for data flowing into and from the cloud.
Support for an entirely decentralized data storage and delivery platform is apparently absent.Our data-centric design hails from Oceanstore [49] and shares a number of goals with Named Data Networking [64], but our focus on the IoT application space leads to a number of important design differences.
A few design decisions are similar to Bolt [44]: single-writer time-series data, chunking for performance, efficient data sharing, policy-driven storage and data confidentiality/integrity.
However, Bolt takes the cloud approach where the pitfalls in Sec. 3 are unavoidable.
This work was supported in part by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by MARCO and DARPA.
