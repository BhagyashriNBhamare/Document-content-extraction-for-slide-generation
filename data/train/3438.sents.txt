Stateful middleboxes, such as intrusion detection systems and application-level firewalls, have provided key func-tionalities in operating modern IP networks.
However, designing an efficient middlebox is challenging due to the lack of networking stack abstraction for TCP flow processing.
Thus, middlebox developers often write the complex flow management logic from scratch, which is not only prone to errors, but also wastes efforts for similar functionalities across applications.
This paper presents the design and implementation of mOS, a reusable networking stack for stateful flow processing in middlebox applications.
Our API allows developers to focus on the core application logic instead of dealing with low-level packet/flow processing themselves.
Under the hood, it implements an efficient event system that scales to monitoring millions of concurrent flow events.
Our evaluation demonstrates that mOS enables modular development of stateful middleboxes, often significantly reducing development efforts represented by the source lines of code, while introducing little performance overhead in multi-10Gbps network environments.
Network appliances or "middleboxes", such as intrusion detection systems and application accelerators, are widely deployed in modern networks [59].
With the trend towards commodity server-based middleboxes [59] and network functions virtualization [38], these middlebox applications are commonly implemented in software.
Middlebox development, however, still remains an onerous task.
It often requires handling complex flow-level states and events at layer 4 or above, such as connection state management and flow reassembly.
The key challenge is that middlebox developers have to build these low-level flow management features from scratch, due to lack of common abstractions and well-defined APIs.
This is in stark contrast to end-host applications programming, where application programmers rely on a set of networking system calls, such as the Berkeley socket API, that hides the details.Existing socket APIs focus on end-to-end semantics and transferring application (layer 7) data.
Unfortunately, they are not flexible enough to monitor session state, packet loss or retransmission patterns at lower layers.
In contrast, popular packet processing frameworks, such as Click [46], DPDK [4], PacketShader IOEngine [40], and netmap [57], provide useful features for packet-level I/O processing, but lack flow-level abstraction required for stateful middlebox applications.
A huge semantic gap exists between the two commonly-used abstractions.
Thus, the state-of-the art middlebox programming remains that each application implements low-level flow-processing features in addition to the application-specific logic.
This practice prevents code reuse and makes it challenging to understand the details of implementation.
For example, we find that two popular NIDS implementations, Snort and Suricata, are drastically different, although they expose similar flow management features [19,58].
This work presents the design and implementation of mOS, a reusable networking stack and an API for modular development of flow-processing middlebox applications.
The design of mOS is based upon two principles.
First, the API should facilitate a clear separation between lowlevel packet/flow processing and application-specific logic.
While tight integration of the two layers might benefit performance, it easily becomes a source of complexity and a maintenance nightmare.
In contrast, a reusable middlebox networking stack allows developers to focus on core middlebox application logic.
Second, the middlebox networking API should provide programming constructs that natively support user-definable flow events for custom middlebox operations.
Most middlebox operations are triggered by a set of custom flow events-being able to express them via a well-defined API is the key to modular middlebox programming.
For example, a middlebox application that detects malicious payload in retransmission should be able to easily express the condition for the event and provide a custom action as its event handler.
Building middlebox applications as a synthesis of event processing significantly improves the code readability while hiding the details for tracking complex conditions.
mOS satisfies a number of practical demands for middlebox development.
First, it exposes a monitoring socket abstraction to precisely express the viewpoint of a middlebox on an individual TCP connection flowing through it.
Unlike an end-host stack, mOS simultaneously manages the flow states of both end-hosts, which allows developers to compose arbitrary conditions of a flow state on either side.
Second, mOS provides scalable monitoring.
In highspeed networks with hundreds of thousands of concurrent flows, monitoring individual flow events incurs high overhead.
Our event system significantly reduces the memory fooprint and memory bandwidth requirement for dynamic event registration and deregistration.
Third, the mOS API supports fine-grained resource management on a perflow basis.
Developers can dynamically enable/disable event generation for an active flow and turn off tracking of unnecessary features.
Tight controlling of computing resources leads to high performance as it avoids redundant cycle wastes.
Finally, the mOS implementation extends the mTCP [43] codebase to benefit from the scalable user-level TCP stack architecture that harnesses modern multicore systems.We make the following contributions.
First, we present a design and implementation of a reusable flowprocessing networking stack for modular development of high-performance middleboxes.
Second, we present key abstractions that hide the internals of complex middlebox flow management.
We find that the mOS monitoring socket and its flexible event composition provides an elegant separation between the low-level flow management and custom application logic.
Third, we demonstrate its benefits in a number of real-world middlebox applications including Snort, Halfback, and Abacus.
In this section, we explain the motivation for a unified middlebox networking stack, and present our approaches to its development.
mOS targets middleboxes that require L4-L7 processing but typically cannot benefit from existing socket APIs, including NIDS/NIPSes [3,6,19,41,58], L7 protocol analyzers [7,9], and stateful NATs [5,10].
These middleboxes track L4 flow states without terminating the TCP connections, often perform deep-packet inspection on flow-reassembled data, or detect anomalous behavior in TCP packet retransmission.Unfortunately, developing flow management features for every new middlebox is very tedious and highly errorprone.
As a result, one can find a long list of bugs related to flow management even in popular middleboxes [30- 32, 34-36].
What is worse, some middleboxes fail to implement critical flow management functions.
For example, PRADS [12] and nDPI [9] perform pattern matching, but they do not implement flow reassembly and would miss the patterns that span over multiple packets.
Similarly, Snort's HTTP parsing module had long been packet-based [61] and vulnerable to pattern evasion attacks.
While Snort has its own sophisticated flow management module, it is tightly coupled with other internal data structures, making it difficult to extend or to reuse.A reusable middlebox networking stack would significantly improve the situation, but no existing packet processing frameworks meet the requirements of generalpurpose flow management.
Click [27,37,46] encourages modular programming of packet processing but its abstraction level is restricted to layer 3 or lower layers.
iptables [5] along with its conntrack module supports TCP connection tracking, but its operation is based on individual packets instead of flows.
For example, it does not support flow reassembly nor allows monitoring fine-grained TCP state change or packet retransmission.
libnids [8] provides flow reassembly and monitors both server and client sides concurrently.
Unfortunately, its reassembly logic is not mature enough to properly handle multiple holes in a receive buffer [17], and it does not provide any control knob to adjust the level of flow management service.
Moreover, the performance of iptables and libnids depends on the internal kernel data structures that are known to be heavyweight and slow [43,57].
Bro [55] provides flow management and events similar to our work, but its built-in events are often too coarsegrained to catch arbitrary conditions for middleboxes other than NIDS.
While tailoring the event system to custom needs is possible through Bro's plugin framework [60], writing a plugin requires deep understanding of internal data structures and core stack implementation.
In addition, Bro scripting and the implementation of its flow management are not designed for high performance, making it challenging to support multi-10G network environments.
By analyzing various networking features of flowprocessing middleboxes, we identify four key requirements for a reusable networking stack.
R1: Middleboxes must be able to combine information from multiple layers using the stack.
For example, an NIDS must process each individual packet, but also be able to reconstruct bytestreams from TCP flows for precise analysis.
A NAT translates the address of each IP packet, but it should also monitor the TCP connection setup and teardown to block unsolicited packets.
R2: The networking stack must keep track of L4 states of both end-points, while adjusting to the level of state management service that applications require.
Tracking per-flow L4 state embodies multiple levels of services.
Some applications require full TCP processing including reassembling bi-directional bytestreams; others only need basic session management, such as sequence number tracking while a few require single-side monitoring without bytestream management (e.g. TCP/UDP port blockers).
Thus, we must dynamically adapt to application needs.
R3: The networking stack must provide intuitive abstactions to cater middlebox developers' diverse needs in a modular manner.
It must provide flow-level abstractions that enable developers to easily manipulate flows and packets that belong to the flow.
It should enable separation of its services from the application logic and allow developers to create higher-level abstraction that goes beyond the basic framework.
Unfortunately, current middlebox applications often do not decouple the two.
For instance, Snort heavily relies on its customized stream 1 preprocessing module for TCP processing, but its application logic (the detection engine) is tightly interlaced with the module.
A developer needs to understand the Snort-specific stream programming constructs before she can make any updates on flow management attributes inside the detection engine.
R4: The networking stack must deliver high performance.
Many middleboxes require throughputs of 10+ Gbps handling hundreds of thousands of concurrent TCP flows even on commodity hardware [2,41,42,44,62].
Our main approach is to provide a well-defined set of APIs and a unified networking stack that hides the implementation details of TCP flow management from custom application logic.
mOS consists of four components designed to meet all requirements:• mOS networking API is designed to provide packetand flow-level abstractions.
It hides the details of all TCP-level processing, but exposes information from multiple layers (R1, R2, and R3), encouraging developers to focus on the core logic and write a modular code that is easily reusable.
• Unified flow management: mOS delivers a composable and scalable flow management library (R2 and R4).
Users can adjust flow parameters (related to buffer management etc.) dynamically at run time.
Moreover, it provides a multi-core aware, high performing stack.
• User-defined events: Applications built on mOS are flexible since the framework provides not only a set of built-in events but also offers support for registering user-defined events (R1 and R3).
• User-level stack: mOS derives its high performance from mTCP [43] that is a parallelizable userspace TCP/IP stack (R4).
In this section, we provide the design and the usage of the mOS networking API and explain how it simplifies stateful middlebox development.
1 The stream module spans about 9, 800 lines of code in Snort-3.0.0a1 version.
The key abstraction that mOS exposes is a monitoring socket, which abstracts a middlebox's tap-point on a passing TCP flow or IP packets.
Conceptually, it is similar to a Berkeley socket, but they differ in the operating semantics.
First, a stream monitoring socket 2 represents a non-terminating midpoint of an active TCP connection.
With a stream monitoring socket, developers write only high-level actions for an individual connection while underlying networking stack automatically tracks low-level TCP flow states of both client and server 3 .
Second, a monitoring socket can monitor fine-grained TCP-layer operations while a Berkeley socket carries out coarse-grained, application-layer operations.
For example, a monitoring socket can detect TCP or packet-level events such as abnormal packet retransmission, packet arrival order, abrupt connection termination, employment of weird TCP/IP options, etc., while it simultaneously supports reading flow-reassembled data from server or client.Using the monitoring socket and its API functions (listed in Appendix A), one can write custom flow actions in a modular manner.
First, a developer creates a 'passive' monitoring socket (similar to a listening socket) and binds it to a traffic scope, specified in a Berkeley packet filter (BPF) syntax.
Only those flows/packets that fall into the scope are monitored.
Note that there is no notion of "accepting" a connection since a middlebox does not engage in a connection as an explicit endpoint.
Instead, one can express when custom action should be executed by setting up flow events as described in Section 3.2.
All one needs is to provide the event handlers that perform a custom middlebox logic, since the networking stack automatically detects and raises the events by managing the flow contexts.
When an event handler is invoked, it is passed an 'active' monitoring socket that represents the flow triggering the event.
Through the socket, one can probe further on the flow state or retrieve and modify the last packet that raised the event.
Figure 2 shows a code example that initializes a typical application with the mOS API.
The mOS API encourages modular middlebox programming by decomposing a complex application into a set of independent <event, event handler> pairs.
It supports two classes of events: built-in and user-defined events.
Built-in events represent pre-defined conditions of notable flow state that are automatically generated in the process of TCP flow management in mOS.
Developers can create their own user-defined events (UDEs) by extending existing built-in or user-defined events.
Built-in event: Built-in events are used to monitor common L4 events in an active TCP connection, such as start/termination of a connection, packet retransmission, or availability of new flow-reassembled data.
With a state transition event, one can even detect any state change in a TCP state transition diagram.
Table 1 lists eight built-in events that we have drawn from common functionalities of existing flow-processing middleboxes.
These pre-defined events are useful in many applications that require basic flow state tracking.
For example, developers can easily write a stateful NAT (or firewall) with only packet arrival and connection start/teardown events without explicitly tracking sequence/acknowledgment numbers or TCP state transitions.
Also, gathering flow-level statistics at a network vantage point can be trivially implemented in only a few lines of code as shown in Appendix B.User-defined event: For many non-trivial middlebox applications, built-in events are insufficient since they often require analyzing L7 content or composing multiple conditions into an event.
For example, an event that detects 3 duplicate ACKs or that monitors an HTTP request does not have built-in support.UDEs allow developers to systematically express such desired conditions.
A UDE is defined as a base event and a boolean filter function that specifies the event condition.
When the base event for an UDE is raised, mOS fires the UDE only if the filter function is evaluated to true.
mOS also supports a multi-event filter function that can dynamically determine an event type or raise multiple events simultaneously.
This feature is useful when it has to determine the event type or trigger multiple related events based on the same input data.UDEs bring three benefits to event-driven middlebox development.
First, new types of events can be created in a flexible manner because the filter function can evaluate arbitrary conditions of interest.
A good filter function, however, should run fast without producing unnecessary Figure 2: Initialization code of a typical mOS application.
Due to space limit, we omit error handling in this paper.
Figure 2 as a base event).
Third, it encourages code reuse.
One can share a well-designed set of event definitions as a UDE library, and 3rd party developers can implement their own event handlers.
For example, an open-source NIDS can declare all corner cases in flow management as a UDE library while 3rd party can provide custom actions to address each case.e 1 MOS_ON_REXMIT e 2 MOS_ON_CONN_NEW_DATA e 3 UDE_FAKE_REXMIT FT FAKE IsFakeRexmit() f a HandleFakeRexmitPacket() f b AccountDataUsage() e 1 e 2 FT FAKE e 3 f a f b Built-in We show how one can build a custom flow-processing middlebox application using the mOS API and events.
We pick Abacus [39] as an example here since it represents the needs of a real-world custom middlebox.
Abacus is a cellular data accounting system that detects a "free-riding" attack by TCP-level tunneling.
It has been reported that some cellular ISPs do not account for TCP retransmission packets [39], and this attack enables free-riding on cellular data by appending fake TCP headers that look like packet retransmission.
The attack detection requires comparing the payload of original and retransmitted packets either by buffering a sliding window or sampling some bytes [39].
Writing Abacus with the mOS API is straightforward.
First, we draw an event-action diagram that captures its main operations as shown in Figure 3.
It represents the normal data as a built-in event (e 2 , new data event) and registers a per-flow accounting function (f b ) for the event.To detect the free-riding attack, we extend a built-in event (retransmission) to define a fake retransmission event (e 3 ).
The filter function (FT FAKE ), as shown in Figure 4, determines whether the current packet retransmission is legal.
In the code, mtcp_getlastpkt() retrieves the metadata (pkt_info structure) and the payload of the packet that triggers the retransmission event.
mctx_t represents the thread context that the mOS stack is bound to and sock identifies the active flow that the packet belongs to.
Then, the code uses mtcp_ppeek() to fetch the original flow-reassembled data at a specified sequence number offset and compares it with the payload if the sequence number ranges match.
In case of partial retransmission, it calls mtcp_getsockopt() to retrieve non-contiguous TCP data fragments (frags) from a right flow buffer and compares the payload of the overlapping regions.
If any part is different from the original content, it returns true and e 3 is triggered, or otherwise it returns false.
When e 3 is triggered, f a is executed to report an attack, and stops any subsequent event processing for the flow.While Abacus is a conceptually simple middlebox, writing its flow management from scratch would require lots of programming effort.
Depending on a middlebox, at least thousands to tens of thousands of code lines are required to implement basic flow management and various corner cases.
The mOS API and its stack significantly save this effort while it allows the developer to write only the high-level actions in terms of events.
Drawing an eventaction diagram corresponds well to the application design process.
Also, it better supports modular programming since the developer only needs to define their own UDEs and convert filters and event handlers into functions.
In this section, we present the internals of mOS.
At a high-level, mOS takes a stream of packets from a network, classifies and processes them by the flow, and triggers matching flow events.
Inside event handlers, the application runs custom logic.
mOS supports TCP flow state management for end-hosts, scalable event monitoring, extended flow reassembly, and fine-grained resource management.
It is implemented by extending mTCP [43].
In total, it amounts to 27K lines of C code, which includes 11K lines of the mTCP code.
Automatic management of TCP contexts is the core functionality of mOS.
For flow management, mOS keeps track of the following L4 states of both end-points: (1) TCP connection parameters for tracking initiation, state transition, and termination of each connection, (2) a payload reassembly buffer and a list of fragmented packets for detecting new payload arrival and packet retransmission.
We further explain the payload reassembly buffer in Section 4.3.
We design the TCP context update in mOS to closely reflect the real TCP state machine by emulating the states of both end-hosts.
Tracking the flow states of both end-hosts is required as each side may take on a different TCP state.
Figure 5 illustrates our model.
When a packet arrives, mOS first updates its TCP context for the packet sender 4 and records all flow events that must be triggered.
Note that event handlers are executed as a batch after the TCP context update.
This is because intermixing them can produce an inconsistent state as some event handler may modify or drop the packet.
Also, processing sender-side events before the receiver side's is necessary to strictly enforce the temporal order of the events.
After sender-side stack update, mOS repeats the same process (update and trigger events) for the receiver-side stack.
Any events relating to packet payload (new data or retransmission) are triggered in the context of a receiver since application-level data is read by the receiver.
In addition, packet modification (or drop) is allowed only in the sender-side event handlers as mOS meticulously follows the middlebox semantics.
The only exception is the retransmission event, which is processed just before receiver context update.
This is to give its event handler a chance to modify or drop the packet if the retransmission turns out to be malicious.
While a receiver-side event handler cannot modify the packet, it can still reset a connection in case it detects malicious intrusion attempts in reassembled payload.
mOS applications operate by registering for flow events and providing custom actions for them.
For high performance, it is critical to have scalable event management with respect to the number of flows and user-defined events.
However, a naïve implementation introduces a serious scalability challenge because mOS allows events to be registered and de-registered on a per-flow basis for fine-grained control.
A busy middlebox that handles 200K concurrent flows and 1,000 UDEs per flow 5 amounts to managing a billion events 6 , which would require large amount of memory and consume huge memory bandwidth.
mOS provides an efficient implementation of internal data structures and algorithms designed to address the scalability challenge.
Data structures: Note UDEs form a tree hierarchy with its root being one of the eight built-in events.
Thus, mOS maintains all custom flow event definitions in a global event dependency forest (d-forest) with eight dependency trees (d-trees).
Figure 6(a) shows an example of a d-forest.
Each node in a d-tree represents a UDE as a base event (e.g., parent node) and its filter function.
Separate from the d-forest, mOS maintains, for each monitoring socket, its event invocation forest (i-forest) that records a set of flow events to wait on.
Similar to the d-forest, an i-forest consists of event invocation trees (i-trees) where each itree maintains the registered flow events derived from its root built-in event.
Only those events with an event handler are being monitored for the socket.
Addressing scalability challenge: If each socket maintains a separate i-forest, it would take up a large memory footprint and cause performance degradation due to redundant memory copying and releasing of the same i-forest.
mOS addresses the problem by sharing the same i-forest with different flows.
Our observation is that flows of the same traffic class (e.g., Web traffic) are likely to process the same set of events, and it is highly unlikely for all sockets to have a completely different i-forest.
This implies that we can reduce the memory footprint by sharing the same i-forest.
When an active socket (e.g., individual TCP connection) is created, it inherits the i-forest from its passive monitoring socket (e.g., listening socket) by keeping a pointer to it.
When an event is registered or de-registered for an active socket, mOS first checks if the resulting i-forest already exists in the system.
If it exists, the active socket simply shares the pointer to the i-forest.
Otherwise, mOS creates a new i-forest and adds it to the system.
In either case, the socket adjusts the reference count of both previous and new i-forests, and removes the i-forest whose reference count becomes zero.The key challenge lies in how to efficiently figure out if the same i-forest already exists in the system.
A naïve implementation would require traversing every event node in all i-forests, which does not scale.
Instead, we present an O(1) solution here.
First, we devise a novel i-forest identification scheme that produces a unique id given an i-forest.
We represent the id of an i-forest with m i-trees as: t 1 ⊕ t 2 ⊕ ... ⊕ t m , where t k indicates the id of the k-th i-tree and ⊕ is a bitwise exclusive-or (xor) operation.
Likewise, the id of an i-tree with n leaf event nodes is defined as:h(e 1 + f 1 ) ⊕ h(e 2 + f 2 ) ⊕ ... ⊕ h(e n + f n ),where h is a oneway hash function, + is simple memory concatenation, and e i and f i are the ids of the i-th event and its event handler, respectively.
Note that the ids of a distinct event and its event handler are generated as unique in the system, which makes a distinct i-forest have a unique id with a high probability with a proper choice of the hash function.
We include the id of an event handler in hash calculation since some event can be registered multiple times with a different handler.
Calculating the new id of an i-forest after adding or removing an event becomes trivial due to the xor operation.
Simply, new-id = old-id ⊕ h(e + f ) where e and f are the ids of the event and its event handler that need to be registered or deregistered.
The fast id operation enables efficient lookup in the invocation forest hash table.
Deterministic ordering of event handler execution: A single packet arrival can trigger multiple events for a flow.
Thus, the order of event handler execution must be predefined for deterministic operation.
For this, we assign a fixed priority for all built-in events.
First, packet arrival events (MOS_ON_PKT_IN and MOS_ON_ORPHAN) are processed because they convey the L3 semantics.
Then, MOS_ON_CONN_START is triggered followed by MOS_ON_-TCP_STATE_CHANGE and MOS_ON_CONN_NEW_DATA.
Finally, MOS_ON_CONN_END is scheduled to give a chance to other events to handle the flow data before connection termination.
Note, all built-in events are handled after TCP context update with an exception of MOS_ON_REXMIT, a special event triggered just before receive-side TCP context update.
All derived events inherit the priority of their root builtin event.
mOS first records all built-in events that are triggered, and 'executes' each invocation tree in a forest by the priority order of the root built-in events.
'Executing' an invocation tree means traversing the tree in the breadthfirst search order and executing each node by evaluating its event filter and running its event handler.
For example, events in F 2 in Figure 6(b) are traversed in the order of e 1 → e 2 → e 3 → e 4 → e 6 → e 5 .
Many middleboxes require L7 content scanning for detecting potential attack or application-specific patterns.
mOS supports such applications with robust payload reassembly that handles a number of sophisticated cases.
Basic operation: mOS exposes mtcp_peek() and mtcp_ppeek() to the application for reading L7 data in a flow.
Similar to recv(), mtcp_peek() allows the application to read the entire bytestream from an end-host.
Internally, mOS maintains and adjusts a current read pointer for each flow as the application reads the data.
mtcp_-ppeek() is useful for retrieving flow data or fragments at an arbitrary sequence number.
Reassembly buffer outrun: Since TCP flow control applies between end-hosts, the receive buffer managed by mOS can become full while new packets continue to arrive (see Figure 7 (a)).
Silent content overwriting is undesirable since the application may not notice the buffer overflow.
Instead, mOS raises an error event to explicitly notify the application about the buffer overflow.
The application can either drain the buffer by reading the data or enlarge the buffer.
Otherwise, mOS overwrites the buffer with the new data, and adjusts the internal read pointer(see Figure 7 (b)).
To notify the application about the overwriting, we make mtcp_peek() fail right after overwriting.
Subsequent function calls continue to read the data from the new position.
Notifying the application about buffer overflow and overwriting allows the developer to write correct operations even at corner cases.
Out-of-order packet arrival: Unlike the end-host TCP stack, some middlebox applications must read partiallyassembled data, especially when detecting attack scenarios with out-of-order or retransmitted packets.
mOS provides data fragment metadata by mtcp_getsockopt(), and the application can retrieve payload of data fragment by mtcp_ppeek() with a specific sequence number to read (see Figure 7 (c)).
Overlapping payload arrival: Another issue lies in how to handle a retransmitted packet whose payload overlaps with the previous content.
mOS allows to express a flexible policy on content overlap.
Given that the update policy differs by the end-host operating systems [53], mOS supports both policies (e.g., overwriting with the retransmitted payload or not) that can be configured on a per-flow basis.
Or a developer can register for a retransmission event and implement any custom policy of her choice.
A middlebox must handle a large number of concurrent flows with limited resources.
mOS is designed to adapt its resource consumption to the computing needs as follows.
Fine-grained control over reassembly: With many concurrent flows, the memory footprint and memory bandwidth consumption required for flow reassembly can be significant.
This is detrimental to those applications that do not require flow reassembly.
To support such applications, mOS allows disabling or resizing/limiting the TCP receive buffer at run-time on a per-flow basis.
For example, middleboxes that rely on IP whitelisting modules (e.g. Snort's IP reputation preprocessor [18]) can use this feature to dynamically disable buffers for those flows that arrive from whitelisted IP regions.
Figure 9: mOS application threading model Uni-directional monitoring: Some middleboxes may want to monitor only the client-side requests or others deployed before server farms may be interested only in the ingress traffic.
In such a case, developers can turn off TCP state management of one side.
Disabling the TCP stack of one side would ignore raising events, stack update, and flow reassembly.
Dynamic event management: The number of registered events affects the overall performance, as shown in Fig- ure 8.
When a large number of UDEs are naïvely set up for all flows, it degrades the performance due to frequent filter invocations.
To address this, the mOS API supports dynamic cancellation of registered events.
For example, one can stop scanning the flow data for attack signatures beyond a certain byte limit [51].
Alternatively, one can register for a new event or switch the event filter depending on the characteristics of each flow.
Such a selective application of flow events provides flexibility to the developers, while minimizing the overall resource consumption.
Threading model: mOS adopts the shared-nothing parallel-processing architecture that effectively harnesses modern multi-core CPUs.
Figure 9 shows the threading model of mOS.
At start, it spawns n independent threads, each of which is pinned to a CPU core and handles its share of TCP flows using symmetric receive-side scaling (S-RSS) [63].
S-RSS maps all packets in the same TCP connection to the same RX queue in a network interface card (NIC), by making the Toeplitz hash function [47] produce the same value even if the source and destination IP/port pairs on a packet are swapped.
This enables linerate delivery of packets to each thread as packet classification is done in NIC hardware.
Also, it allows each mOS thread to handle entire packets in a connection without sharing flow contexts with other threads, which avoids expensive inter-core locks and cache interference.
We adopt flow-based load balancing as it is reported to achieve a reasonably good performance with real traffic [63].
mOS reads multiple incoming packets as a batch but processes each packet by the run-to-completion model [28].
mOS currently supports Intel DPDK [4] and netmap [57] as scalable packet I/O, and supports the pcap library [20] for debugging and testing purposes.
Unlike mTCP, the application runs event handlers in the same context of the mOS thread.
This ensures fast event processing without context switching.
This section evaluates mOS by answering three key questions: (1) Does the mOS API support diverse use cases of middlebox applications?
(2) Does mOS provide high performance?
(3) Do mOS-based applications perform correct operations without introducing non-negligible overhead?
For over two years of mOS development, we have built a number of middlebox applications using the mOS API.
These include simple applications such as a stateful NAT, middlebox-netstat, and a stateful firewall as well as porting real middlebox applications, such as Snort [58], nDPI library [9], and PRADS [12] to use our API.
Using these case studies, we demonstrate that the mOS API supports diverse applications and enables modular development by allowing developers to focus on the core application logic.
As shown in Table 2, it requires only 2%-12% of code modification to adapt to the mOS framework.
Moreover, our porting experience shows that mOS applications have clear separation of the main logic from the flow management modules.
We add a prefix 'm', to the name of mOS-ported application (e.g., Snort → mSnort).
mSnort3: We demonstrate that the mOS API helps modularize a complex middlebox application by porting Snort3 [16] to using mOS.
A typical signature-based NIDS maintains a set of attack signatures (or rules) and examines whether a flow contains the attack patterns.
The signatures consist of a large number of rule options that express various attack patterns (e.g., content, pcre), payload type (e.g., http_header) and conditions (e.g., only_stream and to_-server).
To enhance the modularity of Snort, we leverage the monitoring socket abstraction and express the signatures using event-action pairs.
To transform the signatures into event-action pairs, we express each rule option type as a UDE filter, and synthesize each rule as a chain of UDEs.
We use three synthetic rules shown in Figure 10 as an example.
For example, the http_header rule option in rule (c) corresponds to the filter function FT HT T P that triggers an intermediate event e c1 .
e c1 checks FT AC2 for string pattern matching and triggers e c2 , which in turn runs PCRE pattern matching (FT PCRE ), triggers e c3 and finally executes its event handler ( f A ).
Note, Snort scans the traffic against these rules multiple times: (a) each time a packet arrives, (b) when-ever enough flow data is reassembled, and (c) whenever a flow finishes.
(b) and (c) are required to detect attack patterns that spread over multiple packets in a flow.
These naturally correspond to the four mOS built-in events (e 1 to e 4 ) in Figure 10.
One challenge in the process lies in how one represents the content rule option.
The content rule option specifies a string pattern in a payload, but for efficient pattern matching, it is critical that the payload should be scanned once to find all the string patterns specified by multiple rules.
To reflect this need, we use a multi-event filter function that performs Snort's Aho-Corasick algorithm [24].
Given a message, it scans the payload only once and raises distinct events for different string patterns.We have implemented 17 rule options (out of 45 options 7 ) that are most frequently used in Snort rules.
Our implementation covers HTTP attack signatures as well as general TCP content attack patterns.
The actual implementation required writing a Snort rule parser that converts each rule into a series of UDEs with filter functions so that mSnort3 can run with an arbitrary set of attack signatures.
In total, we have modified 2, 104 lines out of 79, 889 lines of code which replaces the Snort's stream5 and http-inspect modules that provide flow management and HTTP attack detection, respectively.mSnort3 benefits from mOS in a number of ways.
First, we find that each UDE is independent from the internal implementation of flow management, which makes the code easy to read and maintain.
Also, the same filter function is reused to define different intermediate UDEs since it can be used to extend a different base event.
This makes the rule-option evaluator easier to write, understand, and extend.
In contrast, Snort's current rule-option evaluator, which combines the evaluation results of multiple rule options in a rule, is a very complex recursive function that spans over 500 lines of code.
Second, since the attack signatures are evaluated in a modular manner, one can easily add new rule options or remove existing ones without understanding the rest of the code.
In contrast, such modification is highly challenging with existing stream5 and http-inspect modules since other Snort's modules heavily depend on internal structures and implementation of the two.
Third, rules that share the same prefix in the event chain would benefit from sharing the result of event evaluation.
Say, two rules are represented as e 1 → e 2 → e 3 → e 4 , and e 1 → e 2 → e 3 → e 5 → e 6 .
mOS ensures to evaluate up to e 3 only once and shares the result between the two rules.
Fourth, mSnort3 can now leverage more fine-grained monitoring features of mOS such as setting a different buffer size per flow or selective buffer management.
These features are difficult to implement in the existing code of Snort3.
7 Remaining options are mostly unrelated to HTTP/TCP protocols.e 1 MOS_ON_PKT_IN e 2 MOS_ON_ORPHAN e 3 MOS_ON_CONN_END e 4 MOS_ON_CONN_NEW_DATA FT STM FilterOnlyStream() FT HTTP FilterHTTPHeader() FT AC1 FilterACGlobalMatch() FT AC2 FilterACHTTPMatch() FT FDIR FilterFlowDirection() FT PCRE FilterPCREMatch() f A ACTION_LogMessage() alert tcp any any -> any any (msg:"B"; content: "pat1"; flow: to_server, only_stream, established; pcre:"/pcre1/") [event chain] e 4  e b1  e b2  e b3  e b4  f A Rule (c)alert tcp any any -> any any (msg:"C"; content: "pat2"; http_header; pcre:"/pcre2/")[event chain] e 4  e c1  e c2  e c3  f A e 1 e 2 e 3 e 4 FT STM FT HTTP e a1 e c1 e c2 e c3 f A FT AC1 FT AC2 FT FDIR e b1Rule (a) alert tcp any any -> any any (msg:"A"; content: "pat3"; flow: stateless, pcre:"/pcre1/")[event chain] e 2  e a1  e a2  e a3  f A e b3 e a3 e b4 e b2 e a2FT PCRE We write mAbacus from a clean slate and in a top-down approach.
mAbacus exploits the monitoring socket abstraction to monitor TCP packet retransmission, flow creation/termination and payload arrival events for accounting purpose.
Compared to the original version, mAbacus brings two extra benefits.
First, it correctly detects retransmission over fragmented segments from out-of-order packets in a receive buffer.
Second, one can disable a receive buffer of any side in a single line of code , while original Abacus requires commenting out 100+ lines of its flow processing code manually.
The new implementation requires only 561 lines of code.
This demonstrates that mOS hides the details of TCP flow management and allows application developers to focus on their own logic.
mHalfback: Halfback [50] is a transport-layer scheme designed for optimizing the flow completion time (FCT).
It relies on two techniques: (i) skipping the TCP slow start phase to pace up transmission rate at start, and (ii) performing proactive retransmission for fast packet loss recovery.
Inspired by this, we design mHalfback, a middlebox application that performs proactive retransmission over TCP flows.
mHalfback has no pacing phase, since a middlebox cannot force a TCP sender to skip the slow start phase.
Instead, mHalfback provides fast recovery of any packet loss, so that it transparently reduces the FCT without any modification of end-host stacks.
The main logic of mHalfback is as follows: (i) when a TCP data packet arrives, mHalfback holds a copy of the packet for future retransmission.
(ii) when a TCP ACK packet comes from the receiver, mHalfback will retransmit data packets (up to a certain threshold).
mHalfback calls mtcp_getlastpkt() to hold the packet, and mtcp_sendpkt() for proactive retransmission.
When the volume of per-flow data exceeds a retransmission threshold (e.g., [50] uses 141 KB), it deregisters the packet arrival event for the flow, so that any packet beyond the threshold would not be retransmitted.
Likewise, mHalfback stops monitoring a flow when its connection is closed.
Connection closure is easily detected with the state change built-in event.
With the help of mOS monitoring socket, mHalfback implementation requires only 128 lines of code.
mnDPI library: nDPI [9] is an open-source DPI library that detects more than 150 application protocols.
It scans each packet payload against a set of known string patterns or detects the protocol by TCP/IP headers.
Unfortunately, it neither performs flow reassembly nor properly handles out-of-order packets.
We have ported nDPI (libndpi) to use the mOS API by replacing their packet I/O and applying UDEs derived from 4 built-in events as in mSnort3.Our porting enables all applications that use libndpi to detect the patterns over flow-reassembled data.
This requires adding 765 lines of code to the existing 25, 483 lines of code.
mPRADS: PRADS [12] is a passive fingerprinting tool that detects the types of OSes and server/client applications based on their network traffic.
It relies on PCRE [13] pattern matching on TCP packets for this purpose.
Like nDPI, PRADS does not perform flow reassembly.
Furthermore, despite its comprehensive pattern set, the implementation is somewhat ad-hoc since it inspects only the first 10 (which is an arbitrarily set threshold) packets of a flow.
mPRADS employs 24 UDEs on MOS_ON_CONN_-NEW_DATA to detect different L7 protocols in separate event handlers.
This detects the patterns regardless of where the pattern appears in a TCP connection.
We modify only 615 lines out of 10, 848 lines to update mPRADS.
We now evaluate the performance of mOS, including the flow management and event system.
.
We observe that the performance almost linearly scales over the number of CPU cores, and both applications achieve high throughputs despite a large number of concurrent flows when multiple CPU cores are employed.
At 16 cores, packetcount produces 19.1 Gbps and 53.6 Gbps for 64 bytes and 8 KB objects, respectively while stringsearch achieves 18.3 Gbps and 44.7 Gbps for 64 bytes and 8 KB, respectively.
Figure 11(b) shows the flow completion time for the two mOS applications.
mOS applications add 41 ∼ 62 us of delay to that of a direct connection (without any middlebox) for 64-byte objects and 81 ∼ 170 us of latency for 8 KB objects.
We believe the latency stretch is reasonable even when a middlebox operates in the middle.
Figure 11(c) compares the performances of application packetcount under various levels of resource consumption.
Depending on the file size, selective resource configuration improves the performance by up to 25% to 34% compared with the full flow management of both sides.
Not surprisingly, disabling the entire state update of one side provides the biggest performance boost, but skipping flow buffering also brings non-trivial performance improvement.
This confirms that tailoring resource consumption to the needs of a specific middlebox application produces significant performance benefit.
Efficient i-forest management: Figure 12 compares the performance of stringsearch as it dynamically registers for an event.
Clients download 4KB objects with 192K concurrent flows.
When the application finds the ) would consume n * 512 KB, if the application ends up generating n distinct iforests for updating events for k times during its operation (typically, k >> n).
In contrast, a naïve implementation would consume k * 512 KB, which would use 98 GB if all 192K flows update their i-forest dynamically (k = 192K).
Handling abnormal traffic: We evaluate the behavior of mOS when it sees a large number of abnormal flows.
We use the same test environment, but have the server transmit the packets out of order (across the range of sender's transmission window) at wire rate.
We confirm that (a) mOS correctly buffers the bytestream in the right order, and (b) its application shows little performance degradation from when clients and servers directly communicate.
We verify the correctness of mOS-based middleboxes and evaluate their performance with real traffic traces.
libndpi) and mPRADS detect them.
We repeat the test for 100 times, and confirm that mnDPI and mPRADS successfully detect the flows while their original versions miss them.
We also inject 1K fake retransmission flows and find that mAbacus successfully detects all such attacks.
Performance under real traffic: We measure the performance of original applications and their mOS ports with a real network packet trace.
The trace is obtained from a large cellular ISP in South Korea and records 89 million TCP packets whose total size is 67.7 GB [63].
It contains 2.26 million TCP connections and the average packet size is 760 bytes.
We replay the traffic at the speed of 30 Gbps to gauge the maximum performance of mOS-ported applications.
We use the same machines as in Section 5.2.
Table 3 compares the performances of Snort, nDPIReader, PRADS, and Abacus.
Snort-DFC uses a more efficient multi-string matching algorithm, DFC [33], instead of the Aho-Corasick algorithm (Snort-AC).
Original applications (except Abacus) use the pcap library by default, which acts as the main performance barrier.
Porting them to use the DPDK library greatly improves the performance by a factor of 4.9 to 44.6 due to scalable packet I/O.
mOS-based applications deliver comparable performances to those of DPDK-ports while mOS ports provide code modularity and correct operation in pattern matching.
This confirms that mOS does not incur undesirable performance overhead over DPDK-ported applications.
mSnort is actually slightly faster than Snort+DPDK.
This is mainly due to the improved efficiency of mOS's flow management over Snort's stream5.
Our profiling finds that the stream5 module incurs more memory accesses per packet on average.
Compared to others, PRADS shows much lower performance because it naïvely performs expensive PCRE pattern matching on the traffic.
The flow management overhead in mnDPIReader and mPRADS is small, representing only about 1 to 4% of performance change.
Performance under packet loss: We evaluate mHalfback by injecting Web traffic into a lossy network link.
We test with the same topology used in the original paper [50] as shown in Figure 13.
Figure 14 compares average FCT of a direct connection ( Figure 13(A)) and of a connection via Halfback proxy (Figure 13(B)) under various packet loss rates.
We find that mHalfback significantly reduces the FCT with the help of fast loss recovery.
When testing with flows that download 100 KB under 5% packet loss, mHalfback brings 20% to 41% FCT reduction.
While this is lower than 58% FCT reduction reported by the original paper, it is promising as it does not require any modification on the server.
We discuss previous works that are related to mOS.
Flow management support: libnids [8] is a flow-level middlebox library that can be used for building middlebox applications but it does not provide comprehensive flow reassembly features [17,22].
Bro [55] provides an event-driven scripting language for network monitoring.
mOS differs from Bro in its programming model.
mOS is designed to write broader range of network applications and provides an API that allows more fine-grained control over live connections.
A mOS middlebox developer can dynamically register new events per flow at any stage of a connection life cycle, a flow's TCP receive buffer management can be disabled at run-time, and monitoring of any side (client or server) of the flow can be disabled dynamically.
Bro does not offer such features.
Modular middlebox development: Click [46] provides a modular packet processing platform, which allows development of complex packet forwarding applications by chaining elements.
Click has been a popular platform in research community to implement L3 network function prototypes [26,29,45,49,52].
mOS, on the other hand, provides comprehensive, flexible flow-level abstractions that allow mapping a custom flow-level event to a corresponding action, and is suitable for building L4-L7 monitoring applications.
CliMB [48] provides a modular TCP layer composed of Click elements, but its TCP-based elements are designed only for end-host stacks; whereas mOS facilitates programming middleboxes.xOMB [25] is a middlebox architecture that uses programmable pipelines that simplify the development of inline middleboxes.
Although xOMB shares the goal of simplifying development of flow-level middleboxes, its system is focused on an L7 proxy, which uses BSD sockets to initialize and terminate connections.
mOS focuses on exposing flow-level states and events for general-purpose monitoring applications without the ability to create new connections.
CoMb [59] aims to provide efficient resource utilization by consolidating common processing actions across multiple middleboxes; while mOS cuts down engineering effort by combining common flow-processing tasks on a single machine dataplane.
Scalable network programming libraries: Several highspeed packet I/O frameworks have been proposed [4,14,40,57].
However, extending these frameworks to support development of stateful middlebox applications requires significant software engineering effort.
mOS uses mTCP [43], a scalable user-level multicore TCP stack for stateful middleboxes.
The performance scalability of mOS comes from mTCP's per-thread socket abstraction, shared-nothing parallel architecture, and scalable packet I/O.
IX [28] and Arrakis [56] present new networking stack designs by separating the kernel control planes from data planes.
However, both models only provide endpoint networking stacks.
There are a few on-going efforts that provide fast-path networking stack solutions [1,11,15,21] for L2/L3 forwarding data planes.
We believe mOS is the first fast-path networking stack which provides comprehensive flow-level monitoring capabilities for L4-L7 stateful middleboxes.
Modnet [54] provides modular TCP stack customization for demanding applications, but only for end host stack.
Modular programming of stateful middleboxes has long been challenging due to complex low-level protocol management.
This works addresses the challenge with a general-purpose, reusable networking stack for stateful middleboxes.
mOS provides clear separation of interface and implementation in building complex stateful middleboxes.
Its flow management module provides accurate tracking of the end-host states, enabling the developer to interact with the system with a well-defined set of APIs.
Its flow event system flexibly expresses perflow conditions for custom actions.
The mOS source code is available at https://github.com/ndsl-kaist/ mos-networking-stack.
A/* monitoring socket creation/closure, scope setup (see Figure 2) */ int mtcp_socket(mctx_t mctx, int domain, int type, int protocol);-Create a socket.
The socket can be either regular TCP socket (type = MOS_SOCK_STREAM), -a TCP connection monitoring socket (type = MOS_SOCK_MONITOR_STREAM) or a raw packet monitoring socket (type = MOS_SOCK_MONITOR_RAW).
int mtcp_bind_monitor_filter(mctx_t mctx, int sock, monitor_filter_t ft);-Bind a monitoring socket(sock) to a traffic filter (ft).
ft limits the traffic monitoring scope in a BPF syntax.
int mtcp_getpeername(mctx_t mctx, int sock, struct sockaddr *addr, socklen_t *addrlen);-Retrieve the peer address information of a socket (sock).
-Depending on the size of addrlen, one can get either server-side or both server and client-side address information.
int mtcp_close(mctx_t mctx, int sock);-Close a socket.
Closing a monitoring socket does not terminate the connection but unregisters all flow events for the socket.
/* event manpulation (see Figure 2 and Section 3.2) */ event_t mtcp_define_event(event_t ev, filter_t filt, struct filter_arg *arg);-Define a new event with a base event (ev) and a filter function (filt) with a filter argument (arg).
event_t mtcp_alloc_event(event_t parent_event); / int mtcp_raise_event(mctx_t mctx, event_t child_event);-Define a follow-up event for a filter that can trigger multiple (child) events.
-Raise a child event for a multi-event filter.
-mtcp_getlastpkt() retrieves the information of the last packet of a flow (sock and side).
-mtcp_setlastpkt() updates the last packet with data at offset bytes from an anchor for datalen bytes.
-Read the data in a TCP receive buffer of sock.
side specifies either client or server side.
-mtcp_ppeek() is identical to mtcp_peek() except that it reads the data from a specific offset(seq_off) from the initial sequence number.
/* monitoring socket creation/closure, scope setup (see Figure 2) */ int mtcp_socket(mctx_t mctx, int domain, int type, int protocol);-Create a socket.
The socket can be either regular TCP socket (type = MOS_SOCK_STREAM), -a TCP connection monitoring socket (type = MOS_SOCK_MONITOR_STREAM) or a raw packet monitoring socket (type = MOS_SOCK_MONITOR_RAW).
int mtcp_bind_monitor_filter(mctx_t mctx, int sock, monitor_filter_t ft);-Bind a monitoring socket(sock) to a traffic filter (ft).
ft limits the traffic monitoring scope in a BPF syntax.
int mtcp_getpeername(mctx_t mctx, int sock, struct sockaddr *addr, socklen_t *addrlen);-Retrieve the peer address information of a socket (sock).
-Depending on the size of addrlen, one can get either server-side or both server and client-side address information.
int mtcp_close(mctx_t mctx, int sock);-Close a socket.
Closing a monitoring socket does not terminate the connection but unregisters all flow events for the socket.
/* event manpulation (see Figure 2 and Section 3.2) */ event_t mtcp_define_event(event_t ev, filter_t filt, struct filter_arg *arg);-Define a new event with a base event (ev) and a filter function (filt) with a filter argument (arg).
event_t mtcp_alloc_event(event_t parent_event); / int mtcp_raise_event(mctx_t mctx, event_t child_event);-Define a follow-up event for a filter that can trigger multiple (child) events.
-Raise a child event for a multi-event filter.
-mtcp_getlastpkt() retrieves the information of the last packet of a flow (sock and side).
-mtcp_setlastpkt() updates the last packet with data at offset bytes from an anchor for datalen bytes.
-Read the data in a TCP receive buffer of sock.
side specifies either client or server side.
-mtcp_ppeek() is identical to mtcp_peek() except that it reads the data from a specific offset(seq_off) from the initial sequence number.
