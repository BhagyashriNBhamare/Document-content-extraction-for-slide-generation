Residual gradient (RG) was proposed as an alternative to TD(0) for policy evaluation when function approximation is used, but there exists little formal analysis comparing them except in very limited cases.
This paper employs techniques from online learning of linear functions and provides a worst-case (non-probabilistic) analysis to compare these two types of algorithms when linear function approximation is used.
No statistical assumptions are made on the sequence of observations, so the analysis applies to non-Markovian and even adversarial domains as well.
In particular, our results suggest that RG may result in smaller temporal differences , while TD(0) is more likely to yield smaller prediction errors.
These phenomena can be observed even in two simple Markov chain examples that are non-adversarial.
Reinforcement learning (RL) is a learning paradigm for optimal sequential decision making (Bertsekas & Tsitsiklis, 1996;Sutton & Barto, 1998) and has been successfully applied to a number of challenging problems.
In the RL framework, the agent interacts with the environment in discrete timesteps by repeatedly observing its current state, taking an action, receiving a real-valued reward, and transitioning to a next state.
A policy is a function that maps states to actions; semantically, it specifies what action to take given the current state.
The goal of an agent is to optimize its policy in order to maximize the expected long-term return, namely, the discounted sum of rewards it receives by following the policy.
An important step in this optimization process is policy evaluation-the problem of evaluating expected returns of a fixed policy.
This problem is often the most challenging step in approximate policy-iteration algorithms (Bertsekas & Tsitsiklis, 1996;Lagoudakis & Parr, 2003).
Temporal difference (TD) is a family of algorithms for policy evaluation (Sutton, 1988) and has received a lot of attention from the community.
Unfortunately, it is observed (e.g., Baird (1995)) that TD methods may diverge when they are combined with function approximation.
An alternative algorithm known as residual gradient (RG) was proposed by Baird (1995) and enjoys guaranteed convergence to a local optimum.
Since RG is similar to TD(0), a particular instance of the TD family, we will focus on RG, TD(0), and a variant of TD(0) in this paper.Despite convergence issues, little is known that compares RG and TD(0).
Building on previous work on online learning of linear functions (Cesa-Bianchi et al., 1996) and a similar analysis by Schapire and Warmuth (1996), we provide a worst-case (nonprobabilistic) analysis of these algorithms and focus on two evaluation metrics: (i) total squared prediction error, and (ii) total squared temporal difference.
The former measures accuracy of the predictions, while the latter measures consistency and is closely related to the Bellman error (Sutton & Barto, 1998).
Either metric may be preferred over the other in different situations.
For instance, Lagoudakis and Parr (2003) argue that TD solutions tend to preserve the shape of the value function and is more suitable for approximate policy iteration, while there is evidence that minimizing squared Bellman errors is more robust in general (Munos, 2003).
Our analysis suggests that TD can make more accurate predictions, while RG can result in smaller temporal differences.
All terms will be made precise in the next section.
Although our theory focuses on worst-case upper bounds, we also provide numerical evidence and expect the resulting insights to give useful guidance to RL practitioners in deciding which algorithm best suits their purposes.
Fully observable environments in RL are often modelled as Markov decision processes (Puterman, 1994), which are equivalent to induced Markov chains when controlled by a fixed policy.
Here, however, we consider a different model that is suitable for worst-case analysis, as introduced in the next subsection.
This model makes no statistical assumption about the observations, and thus our results apply to much more general situations including partially observable or adversarial environments that subsume Markov chains.Some notation is in order.
We use bold-face, lower-case letters to denote real-valued column vectors such as v.
Their components are denoted by the corresponding letter with subscripts such as v t .
We use 񮽙·· to denote the Euclidean, or 񮽙 2 -norm: 񮽙v񮽙 = √ v 񮽙 v where v 񮽙 is the transpose of v. For a square matrix M , the set of eigenvalues of M , known as the spectrum of M , is denoted σ(M ).
If M is symmetric, its eigenvalues must be real, and its largest eigenvalue is denoted ρ(M ).
Our learning model is adopted from Schapire and War- muth (1996) and is an extension of the online-learning model to sequential prediction problems.
Let k be the dimension of input vectors.
The agent maintains a weight vector of the same dimension and uses it to make predictions.
In RL, input vectors are often feature vectors of states or state-action pairs, and are used to approximate value functions (Sutton & Barto, 1998).
Learning proceeds in discrete timesteps and terminates after T steps.
The agent starts with an initial input vector x 1 ∈ R k and an initial weight vectorw 1 ∈ R k .
At timestep t ∈ {1, 2, 3, · · · , T }:• The agent makes a predictionˆypredictionˆ predictionˆy t = w 񮽙 t x t ∈ R, where w t is the weight vector at time t. Throughout the paper, assume 񮽙x t 񮽙 ≤ X for some known constant X > 0.
• The agent then observes an immediate reward r t ∈ R and the next input vector x t+1 .
Based on this information, it updates its weight vector whose new value is denoted w t+1 .
The change in weight is ∆w t = w t+1 − w t .
By convention, r t = 0 and x t = 0 for t > T .
Define the return at time t by y t = 񮽙 ∞ τ =t γ τ −t r τ , where γ ∈ [0, 1) is the discount factor.
Since γ < 1, it effectively diminishes future rewards exponentially fast.
A quick observation is that y t = r t + γy t+1 , which is analogous to the Bellman equation for Markov chains (Sutton & Barto, 1998).
The agent attempts to mimic y t by its predictionˆypredictionˆ predictionˆy t , and the prediction error is e t = y t − ˆ y t .
Our first evaluation metric is the total squared prediction error :񮽙 P = 񮽙 T t=1 e 2 t = 񮽙e񮽙 2 .
Another useful metric in RL is the temporal differences (also known as TD errors), which measures how consistent the predictions are.
In particular, the temporal difference at time t is d t = r t + γw 񮽙 t x t+1 − w 񮽙 t x t , and the total squared temporal difference is񮽙 T D = 񮽙 T t=1 d 2 t = 񮽙d񮽙 2 .
Previous convergence results of TD and RG often rely heavily on certain stochastic assumptions of the environment such as the assumption that the sequence of observations, [(x t , r t )] t∈N , are generated by an irreducible and aperiodic Markov chain.
Tsitsiklis and Van Roy (1997) first proved convergence of TD with linear function approximation, while they also pointed out the potential divergence risk when nonlinear approximation is used.To resolve the instability issue of TD (0), Baird (1995) proposed the RG algorithm, but also noted that RG may converge more slowly than TD(0) in some problems.
Such an observation was later proved by Schoknecht and Merke (2003), who used spectral analysis to compare the asymptotic convergence rates of the two algorithms.
Although their results are interesting, they only apply to quite limited cases where, for example, a certain matrix associated with TD updates has real eigenvalues only (which does not hold in general).
More importantly, they study synchronous updates while TD and RG are often applied asynchronously in practice.
Furthermore, their results assume that the value function is represented by a lookup table, but the initial motivation of studying RG was to develop a provably convergent algorithm when function approximation is used.Schapire and Warmuth (1996) were also concerned with similar worst-case behavior of TD-like algorithms within the model described in Subsection 2.1.
They defined a new class of algorithms called TD * (λ), which is very similar to the TD(λ) algorithms of Sutton (1988).
They developed worst-case bounds for the total squared prediction error of TD * (λ), but not the total squared temporal difference.
The algorithms we consider all update the weight vector incrementally and differ only in the update rules.
TD(0) uses the following rule:∆w t = ηd t x t ,(1)where η ∈ (0, 1) is the step-size parameter controlling aggressiveness of the update.
Although TD(0) is widely used in practice, analysis turns out to be easier with a close relative of it, TD * (0).
This algorithm differs from TD(0) in that it adapts the step-size based on the input vectors ( Schapire and Warmuth (1996) defined TD * (0) in a different, but equivalent, form):∆w t = ηd t x t 1 − γηx 񮽙 t x t+1 .
(2)Due to space limitation, we only provide results for TD * (0), but similar results hold for TD(0).
It is expected, and also supported by the numerical evidence in Section 4, that TD(0) and TD * (0) have similar behavior and performance in practice.
For this reason, we refer to both algorithms as TD in the rest of the paper if there is no risk of confusion.
In contrast, RG uses the following update rule:∆w t = ηd t (x t − γx t+1 ) .
(3) This section contains the main theoretical results.
We will first describe how to evaluate an algorithm in the worst-case scenario.
For completeness, we also summarize the squared prediction error bounds for TD * (0) due to Schapire and Warmuth (1996).
Then, we analyze total squared temporal difference bounds and RG.Our analysis makes a few uses of matrix theory (see, e.g., Horn and Johnson (1986)), and several technical lemmas are found in the appendix.
Two basic facts about ρ(M ) will be used repeatedly: (i) if M is negative-definite, then ρ(M ) < 0; and (ii) the Rayleigh-Ritz theorem (Horn & Johnson, 1986, Theorem 4.2.2) states that ρ(M ) = max v񮽙 =0v 񮽙 M v v 񮽙 v .
Analogous to other online-learning analysis, we treat 񮽙 P and 񮽙 T D as total losses, and compare the total loss of an algorithm to that of an arbitrary weight vector, u.
We wish to prove that this difference is small for all u, including the optimal (in any well-defined sense) but unknown vector u * .
The prediction using vector u at time t is y u t = u 񮽙 x t .
Accordingly, the prediction error and temporal difference at time t are e u t = y t − y u t and d u t = r t + γu 񮽙 x t+1 − u 񮽙 x t , respectively.
The total squared prediction error and total squared temporal difference of u are񮽙 u P = 񮽙e u 񮽙 2 = 񮽙 T t=1 񮽙 y t − u 񮽙 x t 񮽙 2 and 񮽙 u T D = 񮽙d u 񮽙 2 = 񮽙 T t=1 񮽙 r t + γu 񮽙 x t+1 − u 񮽙 x t 񮽙 2 ,respectively.
Using step-size η = 1 X 2 +1 , Schapire and Warmuth (1996) showed a worst-case upper bound:񮽙 P ≤ 񮽙 1 + X 2 񮽙 񮽙 񮽙 u P + 񮽙w 1 − u񮽙 2 2 񮽙 1 − γ 2 .
Furthermore, if E and W are known beforehand such that 񮽙 u P ≤ E and 񮽙w 1 − u񮽙 ≤ W , then the step-size η can be optimized by η =W X √ E+X 2 Wto yield an asymptotically better bound:񮽙 P ≤ 񮽙 u P + 2W X √ E + X 2 W 2 1 − γ 2 .
(4) We will extend the analysis of Schapire and War- muth (1996) to the new loss function 񮽙 T D by examining how the potential function, 񮽙w t − u񮽙 2 , evolves when a single update is made at time t.
It can be shown (Schapire & Warmuth, 1996, Eqn 8) that− −w 1 − u񮽙 2 ≤ η 2 X 2 e 񮽙 D 񮽙 De + 2ηe 񮽙 D 񮽙 (e u − e),whereD = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 1 −γ 0 · · · 0 0 0 1 −γ · · · 0 0 . . . 0 0 · · · 1 −γ 0 0 0 · · · 0 1 −γ 0 0 · · · 0 0 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ .
(5)Define f = De.
According to Lemma A.1(1), d u = De u , and hence the inequality above is rewritten as:− −w 1 − u񮽙 2 ≤ η 2 X 2 f 񮽙 f − 2ηf 񮽙 D −1 f + 2ηf 񮽙 D −1 d u .
Using the fact that 2p 񮽙 q ≤ ≤p񮽙2 + 񮽙q񮽙 2 for p = η √ b D −− f , q = √ bd u ,and arbitrary b > 0, the inequal- ity becomes − −w 1 − u񮽙 2 ≤ f 񮽙 M 1 f + bb u T D , whereM 1 = η 2 X 2 I + η 2 b D −1 D −− − η(D −1 + D −− ) (6(1 + γ) 2 񮽙 X 2 + 1 b(1 − γ) 2 񮽙 񮽙 bb u T D + 񮽙w 1 − b񮽙 2 񮽙 ,when the step-size isη = 1 (1 + γ) 񮽙 X 2 + 1 b(1−γ) 2 񮽙 .
(7) Due to Lemma A.1 (2), we haved 2 t = 񮽙 1 − γηx 񮽙 t x t+1 񮽙 2 f 2 t ≤ 񮽙 1 + γηX 2 񮽙 2 f 2 t ≤ (1 + 2γ) 2 (1 + γ) 2 f 2 t .
Therefore, 񮽙 T D is at most(1 + 2γ) 2 񮽙 X 2 + 1 b(1 − γ) 2 񮽙 񮽙 bb u T D + 񮽙w 1 − u񮽙 2 񮽙 .
Using b = 1, we have thus proved the first main result.Theorem 3.1.
Let η be given by Eqn 7 using b = 1, then the following holds for TD * (0): 񮽙 T D ≤ (1+2γ) 2 񮽙 X 2 + 1 (1 − γ) 2 񮽙 񮽙 񮽙 u T D + 񮽙w 1 − u񮽙 2 񮽙 .񮽙
T D ≤ (1 + 2γ) 2 񮽙 񮽙 u T D (1 − γ) 2 + 2XW √ E 1 − γ + X 2 񮽙w 1 − u񮽙 2 񮽙 .
(8)Proof.
Previous analysis for Theorem 3.1 yields񮽙 T D ≤ (1 + 2γ) 2 񮽙񮽙 bX 2 񮽙 u T D + 񮽙w 1 − u񮽙 2 b(1 − γ) 2 񮽙 + 񮽙 񮽙 u T D (1 − γ) 2 + X 2 񮽙w 1 − u񮽙 2 񮽙񮽙 ≤ (1 + 2γ) 2 񮽙񮽙 bX 2 E + W 2 b(1 − γ) 2 񮽙 + 񮽙 񮽙 u T D (1 − γ) 2 + X 2 񮽙w 1 − u񮽙 2 񮽙񮽙 for any b > 0.
We may simply choose b = W X(1−γ) √ E, and the step-size in Eqn 7 becomesη = 1 (1 + γ) 񮽙 X 2 + X √ E W (1−γ)񮽙 .
By the update rule in Eqn 3 and simple algebra,∆w 񮽙 t (w t − u) = ηd t (x t − γx t+1 ) 񮽙 (w t − u) = ηd t 񮽙񮽙 w 񮽙 t x t − γw 񮽙 t x t+1 − r t 񮽙 − 񮽙 u 񮽙 x t − γu 񮽙 x t+1 − r t 񮽙񮽙 = ηd t (d u t − d t ), 񮽙∆w t 񮽙 2 = η 2 d 2 t 񮽙x t − γx t+1 񮽙 2 2 ≤ η 2 d 2 t X 2 (1 + γ) 2 .
Similar to the previous section, we use the potential function 񮽙w t − u񮽙 2 to measure progress of learning:− −w 1 − u񮽙 2 ≤ T 񮽙 t=1 񮽙 񮽙w t+1 − u񮽙 2 − −w t − u񮽙 2 񮽙 = T 񮽙 t=1 񮽙 2∆w 񮽙 t (w t − u) + ∆w 񮽙 t ∆w t 񮽙 ≤ T 񮽙 t=1 񮽙 2ηd t (d u t − d t ) + η 2 d 2 t X 2 (1 + γ) 2 񮽙 = 2ηd 񮽙 d u − 2ηd 񮽙 d + η 2 X 2 (1 + γ) 2 d 񮽙 d.According to Lemma A.1 (1) and using the fact that2p 񮽙 q ≤ ≤p񮽙 2 + 񮽙q񮽙 2 for p = η √ b D 񮽙 d, q = √be u , and arbitrary b > 0, the inequality above is written as:− −w 1 − u񮽙 2 ≤ b 񮽙e u 񮽙 2 + η 2 b d 񮽙 DD 񮽙 d + 񮽙 η 2 X 2 (1 + γ) 2 − 2η 񮽙 񮽙d񮽙 2Due to Lemma A.1 (3), d = ΣDe, whereΣ = diag 񮽙 1 1 + γη(x 1 − γx 2 ) 񮽙 x 2 , 1 1 + γη(x 2 − γx 3 ) 񮽙 x 3 , · · · , 1 1 + γη(x T −1 − γx T ) 񮽙 x T , 1 񮽙 .
(9)Then, the inequality above becomes:− −w 1 − u񮽙 2 ≤ b 񮽙e u 񮽙 2 + e 񮽙 M 2 e, where M2 = D 񮽙 Σ " η 2 b DD 񮽙 + ` η 2 X 2 (1 + γ) 2 − 2η´I 2η´2η´I « ΣD.
(10)Since e 񮽙 M 2 e ≤ ρ(M 2 ) 񮽙e񮽙 2 , Lemma A.5 implies the following theorems when the step-size isη = 1 (1 + γ) 2 񮽙 X 2 + 1 b 񮽙 .
(11)Theorem 3.3.
Let η be given by Eqn 11 using b = 1, then the following holds for RG:񮽙 P ≤ (1 + 2γ) 2 񮽙 X 2 + 1 񮽙 (1 − γ) 2 񮽙 񮽙 u P + 񮽙w 1 − u񮽙 2 񮽙 .
Theorem 3.4.
If E and W are known beforehand such that 񮽙 u P ≤ E and 񮽙w 1 − u񮽙 ≤ W , then η can be optimized in RG so that񮽙 P ≤ (1 + 2γ) 2 (1 − γ) 2 񮽙 񮽙 u P + 2XW √ E + X 2 񮽙w 1 − u񮽙 2 񮽙 .
(12)Proof.
Previous analysis in this subsection yields񮽙 P ≤ (1 + 2γ) 2 (1 − γ) 2 񮽙񮽙 񮽙 u P + X 2 񮽙w 1 − u񮽙 2 񮽙 + 񮽙 X 2 bb u P + 񮽙w 1 − u񮽙 2 b 񮽙񮽙 ≤ (1 + 2γ) 2 (1 − γ) 2 񮽙񮽙 񮽙 u P + X 2 񮽙w 1 − u񮽙 2 񮽙 + 񮽙 X 2 bE + W 2 b 񮽙񮽙 .
We simply choose b = W X √ Eand accordingly the stepsize in Eqn 11 becomesη = 1 (1 + γ) 2 񮽙 X 2 + X √ E W 񮽙 .
It is most convenient to turn this problem into one of analyzing the total squared prediction error in the original online-learning-of-linear-function framework ( Cesa-Bianchi et al., 1996).
In particular, define z t = x t − γx t+1 and thus 񮽙z t 񮽙 ≤ (1 + γ)X. Now, RG can be viewed as a gradient descent algorithm operating over the sequence of data [(z to obtain the following improved bound:񮽙 T D ≤ 񮽙 u T D + 2W X(1 + γ) √ E + (1 + γ) 2 W 2 X 2 .
(13) Based on Eqns 4, 8, 12, and 13, Table 1 summarizes the asymptotic upper bounds (when T → ∞) assuming E and W are known beforehand to optimize η.
1 Although our bounds are all upper bounds, results in the table suggest that, in worst cases, TD * (0) (and also TD(0)) tend to make smaller prediction errors, while RG tends to make smaller temporal differences.
The gaps between corresponding bounds increase as 1 Strictly speaking, the validity of these asymptotic results relies on the assumptions that (i) √ E = o(񮽙 u P ), and (ii) W and X remain constant as T → ∞.
Both assumptions are reasonable in practice.
(1) 1 + o (1) γ → 1.
On the other extreme where γ = 0, all these asymptotic bounds coincide, which is not surprising as TD (0), TD * (0), and RG are all identical when γ = 0.񮽙
P // u P 񮽙 T D // u T D TD * (0) 1 1−γ 2 + o(1) (1+2γ) 2 (1−γ) 2 + o(1) RG (1+2γ) 2 (1−γ) 2 + oSince it is unknown whether the leading constants in Table 1 are optimal, the next section will provide numerical evidence to support our claims about the relative strengths of these algorithms.It is worth mentioning that in sequential prediction or decision problems, the factor 1 1−γ often plays a role similar to the decision horizon (Puterman, 1994).
Therefore, in some sense, our bounds also characterize how prediction errors and temporal differences may scale with decision horizon, in the worst-case sense.When 񮽙 P or 񮽙 T D are relatively small, the asymptotic bounds in Table 1 Since our setting is quite different from that of Schoknecht and Merke (2003), our results are not comparable to theirs.
This section presents empirical evidence in two Markov chains that supports our claims in Section 3.6.
The first is the Ring Markov chain (Figure 1 (a)), a variant of the Hall problem introduced by Baird (1995) in which RG was observed to converge to the optimal weights more slowly than TD(0).
The state space is a ring consisting of 10 states numbered from 0 through 9.
Each state is associated with a randomly selected feature vector of dimension k = 5:x (0) , · · · , x (9) ∈ R k .
Transitions are deterministic and are indicated by arrows.
The reward in every state is stochastic and is distributed uniformly in [−0.1, 0.1].
As in Hall , the value of every state is exactly 0.
The second problem is a benchmark problem known as PuddleWorld (Boyan & Moore, 1995).
The state space is a unit square (Figure 1 (d The agent adopts a fixed policy that goes north or east with probability 0.5 each.
Every episode takes about 40 steps to terminate.
The reward is −1 unless the agent steps into the puddles and receives penalty for that; the smallest possible reward is −41.
We used 16 RBF features of width 0.3, whose centers were evenly distributed in the state space.
We also tried a degree-two polynomial feature: for a state s = (s 1 , s 2 ) 񮽙 , the feature vector had six components:x s = 񮽙 1, s 1 , s 2 , s 1 s 2 , s 2 1 , s 2 2񮽙 񮽙 .
Since the results are similar to those for RBF features, they are not included.We ran three algorithms in the experiments: TD(0), TD * (0), and RG.
For a fair comparison, all algorithms started with the all-one weight vector and were given the same sequence of (x t , r t ) for learning.
The procedure was repeated 500 times.
For Ring , each run used a different realization of feature x (s) and T = 500; for PuddleWorld , each run consisted of 50 episodes (yielding slightly less than 2000 steps in total).
A wide range of step-sizes were tried, and the best choices for each discount-factor-algorithm combination were used to evaluate 񮽙 P and 񮽙 T D , respectively.
Figure 1 (b,c,e,f) gives the average per-step squared prediction errors and squared temporal differences for these two problems, with 99% confidence intervals plotted.These results are consistent with our analysis: TD (0) and TD * (0) tended to make more accurate predictions, while RG did a better job at minimizing temporal differences; the differences between these algorithms were even larger as the discount factor γ approached 1.
2 Finally, as a side effect, it is verified that TD(0) and TD * (0) had essentially identical performance, although their best learning rates might differ.
We have carried out a worst-case analysis to compare two policy-evaluation algorithms, TD and RG, when linear function approximation is used.
Together with previously known results due to Schapire and War- muth (1996) and Cesa-Bianchi et al. (1996), our results suggest that, although the TD algorithms may make more accurate predictions, RG may be a better choice when small temporal differences are desired.
This claim is supported by empirical evidence in two simple Markov chains.
Although the analysis is purely mathematical, we expect the implications to deepen the understanding of these two types of algorithms and can provide useful insights to RL practitioners.There has been relatively little attention to this sort of online-learning analysis within the RL community.
Our analysis shows that this kind of analysis may be helpful and provide useful insights.
A few directions are worth pursuing.
First, we have focused on worst-case upper bounds, but it remains open whether matching lower bounds can be found.
More extensive empirical studies are also necessary to see if such worst-case behavior can be observed in realistic problems.
Second, we wish to generalize the analysis of total squared temporal difference from TD(0) and TD * (0) to TD(λ) and TD * (λ), respectively.
Finally, we would like to mention that, in their original forms, both TD and RG use additive updates.
Another class of updates known as multiplicative updates (Kivinen & Warmuth, 1997) has been useful when the number of features (i.e., the k in Subsection 2.1) is large but only a few of them are relevant for making predictions.
Such learning rules have potential uses in RL (Precup & Sutton, 1997), but it remains open whether these algorithms converge or whether worst-case error bounds similar to the ones given in this paper can be obtained.
Lemma A. 1.
In all three algorithms,d u = De u .
2.
In TD * (0), d t = (1 − γηx 񮽙 t x t+1 )(e t − γe t+1 ).
3.
In RG, d t = et−γet+1 1+γη(xt−γxt+1) 񮽙 xt+1 .
Proof.
1.
Since y t = r t + γy t+1 , we haved u t = r t + γu 񮽙 x t+1 − u 񮽙 x t = 񮽙 y t − u 񮽙 x t 񮽙 − 񮽙 y t − r t − γu 񮽙 x t+1 񮽙 = 񮽙 y t − u 񮽙 x t 񮽙 − γ 񮽙 y t+1 − u 񮽙 x t+1 񮽙 = e u t − γe u t+1 .
In matrix form, this is d u = De u .
2.
Since w t = w t+1 − ∆w t and y t = r t + γy t+1 ,d t = r t + γw 񮽙 t x t+1 − w 񮽙 t x t = r t + γ(w t+1 − ∆w t ) 񮽙 x t+1 − w 񮽙 t x t + (y t − r t − γy t+1 ) = (y t − w 񮽙 t x t ) − γ(y t+1 − w 񮽙 t+1 x t+1 ) − γ∆w 񮽙 t x t+1 = e t − γe t+1 − γηd t x 񮽙 t x t+1 1 − γηx 񮽙 t x t+1 .
Reorganizing terms will complete the proof.
3.
Similar to the proof for part (2) D −− or D −− D −1 .
Then, σ (A) ⊆ 񮽙 (1 − γ) 2 , (1 + γ) 2 񮽙 and σ (B) ⊆ 񮽙 (1 + γ) −2 , (1 − γ) −2 񮽙 .
Proof.
It can be verified that D 񮽙 D equals ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 1 −γ 0 · · · 0 −γ 1 + γ 2 −γ · · · 0 . . . 0 0 · · · 1 + γ 2 −γ 0 0 · · · −γ 1 + γ 2 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ .
Since D 񮽙 D is symmetric, σ 񮽙 D 񮽙 D 񮽙 ⊂ R.
It fol-lows from Geršgorin's theorem (Horn & Johnson, 1986, Theorem 6 and Proof.
By Weyl's theorem (Horn & Johnson, 1986, Theorem 4.3.1),.1.1) that σ 񮽙 D 񮽙 D 񮽙 ⊆ 񮽙 (1 − γ) 2 , (1 + γ) 2 񮽙 .
D −− D −1 = 񮽙 DD 񮽙 񮽙 −1 .
σ(D −1 + D −− ) ⊆ 񮽙 2 1 + γ , 2 1 − γ 񮽙 .
Proof.
It can be verified that D −1 + D −− equals G = 0 B B B B B B B @ 2 γ γ 2 · · · γ T −2 γ T −1 γ 2 γ · · · γ T −3 γ T −2 . . . γ T −3 γ T −4 · · · 2 γ γ 2 γ T −2 γ T −3 · · · γ 2 γ γ T −1 γ T −2 · · · γ 2 γ 2 1 C C C C C Cρ(M 1 ) ≤ ρ 񮽙 η 2 X 2 I 񮽙 + ρ 񮽙 η 2 b D −1 D −− 񮽙 + ρ 񮽙 −η 񮽙 D −1 + D −− 񮽙񮽙 .
The lemma then follows immediately from Lemmas A.2 and A.3.
Lemma A.5.
Let M 2 be defined by Eqn 10 and suppose the step-size is given by Eqn 11, thenρ(M 2 ) ≤ − (1 − γ) 2 (1 + 2γ) 2 񮽙 X 2 + 1 b 񮽙 .
Proof.
Let α = η 2 b and β = η 2 X 2 (1 + γ) 2 − 2η, then M 2 = D 񮽙 Σ 񮽙 αDD 񮽙 + βI 񮽙 ΣD.
It is known that ρ(M 2 ) = max v1񮽙 =0 v 񮽙 1 M 2 v 1 v 񮽙 1 v 1 .
Define v 2 = Dv 1 and we have:ρ(M 2 ) = max v2񮽙 =0 v 2 Σ 񮽙 αDD 񮽙 + βI 񮽙 Σv 2 v 񮽙 2 D −− D −1 v 2 ≤ max v2񮽙 =0 (1 − γ) 2 v 2 Σ 񮽙 αDD 񮽙 + βI 񮽙 Σv 2 v 񮽙 2 v 2 ,where the last step is due to Lemma A.2 and the fact that M 2 is negative-definite for η 񮽙 1.
Similarly, we define v 3 = Σv 2 and use the fact that0 ≤ v 񮽙 2 v 2 = v 񮽙 3 Σ −2 v 3 ≤ 񮽙 1 + γ(1 + γ)ηX 2 񮽙 2 񮽙v 3 񮽙 2to obtain:ρ(M 2 ) ≤ max v3񮽙 =0 (1 − γ) 2 v 񮽙 3 񮽙 αDD 񮽙 + βI 񮽙 v 񮽙 3 (1 + γ(1 + γ)ηX 2 ) 2 v 񮽙 3 v 3 = (1 − γ) 2 ρ 񮽙 αDD 񮽙 + βI 񮽙 (1 + γ(1 + γ)ηX 2 ) 2 ≤ (1 − γ) 2 񮽙 α(1 + γ) 2 + β 񮽙 (1 + γ(1 + γ)ηX 2 ) 2 .
If we choose η as in Eqn 11, then the lemma follows immediately from the fact that1 + γX 2 (1 + γ) 񮽙 X 2 + 1 b 񮽙 ≤ 1 + γ 1 + γ = 1 + 2γ 1 + γ .
We thank Michael Littman, Hengshuai Yao, and the anonymous reviewers for helpful comments that improved the presentation of the paper.
The author is supported by NSF under grant IIS-0325281.
