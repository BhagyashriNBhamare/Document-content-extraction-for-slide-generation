Online reviews in which users publish detailed commentary about their experiences and opinions with products, services, or events are extremely valuable to users who rely on them to make informed decisions.
However, reviews vary greatly in quality and are constantly increasing in number, therefore, automatic assessment of review helpfulness is of growing importance.
Previous work has addressed the problem by treating a review as a stand-alone document , extracting features from the review text, and learning a function based on these features for predicting the review quality.
In this work, we exploit contextual information about authors' identities and social networks for improving review quality prediction.
We propose a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor.
Our approach can effectively use the social context information available for large quantities of unlabeled reviews.
It also has the advantage that the resulting predictor is usable even when social context is unavailable.
We validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse.
Web 2.0 has empowered users to actively interact with each other, forming social networks around mutually interesting information and publishing large amounts of useful user-generated content online.
One popular and important type of such user-generated content is the review, where users post detailed commentary on online portals about their experiences and opinions on products, events, or services.
Reviews play a central role in the decision-making Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.WWW 2010, April 26-30, 2010, Raleigh, North Carolina, USA.ACM 978-1-60558-799-8/10/04.
process of online users for a variety of tasks including purchasing products, booking flights and hotels, selecting restaurants, and picking movies to watch.
Sites like Yelp.com and Epinions.
com have created a viable business as review portals, while part of the popularity and success of Amazon.com is attributed to their comprehensive user reviews.
As online commerce activity continues to grow [9], the role of online reviews is expected to become increasingly important.Unfortunately, the abundance of user-generated content comes at a price.
For every interesting opinion, or helpful review, there are also large amounts of spam content, unhelpful opinions, as well as highly subjective and misleading information.
Sifting through large quantities of reviews to identify high quality and useful information is a tedious, error-prone process.
It is thus highly desirable to develop reliable methods to assess the quality of reviews automatically.
Robust and reliable review quality prediction will enable sites to surface high-quality reviews to users while benefiting other important popular applications such as sentiment extraction and review summarization [8,7], by providing high-quality content on which to operate.Automatic review quality prediction is useful even for sites providing a mechanism where users can evaluate or rate the helpfulness of a review (e.g. Amazon.com and Epinions.com).
Not all reviews receive the same helpfulness evaluation [10].
There is a rich-get-richer effect [11] where the top reviews accumulate more and more ratings, while more recent reviews are rarely read and thus not rated.
Furthermore, such helpfulness evaluation is available only within a specific Web site, and is not comparable across different sources.
However, it would be more useful for users if reviews from different sources for the same item could be aggregated and rated automatically on the same scale.
This need is addressed by a number of increasingly popular aggregation sites such as Wise.com.
For these sites, automatic review rating is essential in order to meaningfully present the collected reviews.Most previous work [17,10,11,6,12,15] attempts to solve the problem of review evaluation by treating each review as a standalone text document, extracting features from the text and learning a function based on these features for predicting review quality.
However, in addition to textual content, there is much more information available that is useful for this task.
Online reviews are produced by identifiable authors (reviewers) who interact with one another to form social networks.
The history of reviewers and their social network interactions provide a social context for the reviews.
In our approach, we mine combined textual, and social context information to evaluate the quality of individual reviewers and to assess the quality of the reviews.In this paper, we investigate how the social context of reviews can help enhance the accuracy of a text-based quality predictor.
To the best of our knowledge, this is the first time that textual, author and social network information are combined for assessing review quality.
Expressed very generally, our idea is that social context reveals a lot about the quality of reviewers, which in turn affects the quality of the reviews.
We formulate hypotheses that capture this intuition and then mathematically model these hypotheses by developing regularization constraints which augment text-based review quality prediction.
The resulting quality predictor is formulated into a well-formed convex optimization problem with efficient solution.
The proposed regularization framework falls under the category of semi-supervised learning, making use of a small amount of labeled data as well as a large amount of unlabeled data.
It also has the advantage that the learned predictor is applicable to any review, even reviews from different sources or reviews for which the reviewer's social context is not available.
Finally, we experiment with real review data from an online commerce portal.
We test our hypotheses and show that they hold for all three categories of data we consider.
We then experimentally demonstrate that our novel regularization methods that combine social context with text information can lead to improved accuracy of review quality prediction, especially when the available training data is sparse.The remainder of our paper is structured as follows.
We first formally define the problem in Section 2.
In Section 3 we present a text-based quality predictor which we use as our baseline.
In Section 4, we outline our proposed methods for exploiting social context, formulate our hypotheses, and provide the mathematical modeling.
In Section 5 we experimentally validate our hypotheses, evaluate the prediction performance of our methods and compare against baselines.
Finally, we go over the related work in Section 6 and conclude in Section 7.
A review system consists of three sets of three different types of entities: a set I = {i1, ..., iN } of N items (products, events, or services); a set R = {r1, ..., rn} of n reviews over these items; and a set U = {u1, ..., um} of m reviewers (or users) that have authored these reviews.
Each entity has a set of attributes T associated with it.
For an item i or a user u, Ti and Tu are sets of attribute-value pairs describing the item and the user respectively while for a review r, Tr is the text of the review.
We are also given relationships between these sets of entities.
There is a function M : R → I that maps each review r to a unique item ir = M (r); an authorship function A : R → U , that maps each review r to a unique reviewer ur = A(r); and a relation S ⊂ U × U that defines the social network relationships between users.Since each review is associated with a unique item, we omit the set I, unless necessary, and assume all information about the item ir (item identifier and attributes) is included as part of the attributes Tr of review r.
We also model the social network relation as a directed graph GS = (U, S) with adjacency matrix S, where Suv = 1 if there is a link or edge from u to v and zero otherwise.
We assume that the links between users in the social network capture semantics of trust and friendship: the meaning of user u linking to user v is that u values the opinions of user v as a reviewer.The information about the authors of the reviews along with the social network of the reviewers places the reviews within a social context.
More formally we have the following definition.
DEFINITION 1 (SOCIAL CONTEXT).
Given a set of reviews R, we define the social context of the set R as the triple C(R) = 񮽙U, A, S񮽙, of the set of reviewers U , the authorship function A, and the social network relation S.The set of reviews R contains both labeled (RL) and unlabeled (RU ) reviews.
For each review ri ∈ RL in the labeled subset of reviews we observe a numeric value qi that captures the true quality and helpfulness of the review.
We use L = {(ri, qi)}, to denote the set of review-quality pairs.
Such quality values can be obtained through manual labeling or through feedback mechanisms in place for some online portals.Given the input data {RL ∪ RU , C(R), L}, we want to learn a quality predictor Q that, for a review r, predicts the quality of the review.
A review r is represented as an f -dimensional real vector r over a feature space F constructed from the information in R and C(R).
So the quality predictor is a function Q : R f → R that maps a review feature vector to a numerical quality value.Previous work has used the information in {RL, L} for learning a quality predictor, based mostly on different kinds of textual features.
In this paper, we investigate how to enhance the quality predictor function Q using the social context C(R) of the reviews in addition to the information in {RL, L}.
Our exploration for the prediction function Q takes the following steps.
First we construct a text-based baseline predictor that makes use of only the information in {RL, L}.
Then we enhance this predictor by adding social context features that we extract from C(RL).
In the last step, which is the focus of this paper, we propose a novel semi-supervised technique that makes use of the labeled data {RL, L}, the unlabeled data RU , and the social context information C(R) for both labeled and unlabeled data.
The text of a review provides rich information about its quality.
In this section, we build a baseline supervised predictor that makes use of a variety of textual features as detailed in the top part of With this feature set F , we can now represent each review r as an f -dimensional vector r. Given the labeled data in {RL, L}, we want to learn a function Q : R f → R that for a review ri it predicts a numerical valuê qi as its quality.
We formulate the problem as a linear regression problem, where the function Q is defined as a linear combination of the features in F .
More formally, the function Q is fully defined by an f -dimensional column weight vector w, such that Q(r) = w T r, where w T denotes the transpose of the vector.
In the following, since Q is uniquely determined the by weight vector w and vice versa, we will use Q and w interchangeably.
Our goal is to find the f -dimensional weight vectorˆwvectorˆ vectorˆw that minimizes the objective function:Ω(w) = 1 n 񮽙 n 񮽙 񮽙 i=1 L(w T ri, qi) + αw T w (1)where L is the loss function that measures distance of the predicted quality Q(ri) = w T ri of review ri ∈ RL with the true quality value qi, n 񮽙 is the number of training examples, and α ≥ 0 is regularization parameter for w.
In our work, we use squared error loss (or quadratic loss), and we minimize the functionΩ1(w) = 1 n 񮽙 n 񮽙 񮽙 i=1 (w T ri − qi) 2 + αw T w(2)The closed form solution forˆwforˆ forˆw is given byˆwbyˆ byˆw = arg max w Ω1(w) = ( n 񮽙 񮽙 i=1 rir T i + αn 񮽙 I) −1 n 񮽙 񮽙 i=1 qiriwhere I is the identity matrix of size f .
Once we have learned the weight vector w, we can apply it to any review feature vector and predict the quality of unlabeled reviews.
The solution we describe in Section 3 considers each review as a stand-alone text document.
As we have discussed, in many cases we also have available the social context of the reviews, that is, additional information about the authors of the reviews, and their social network.
In this section we discuss different ways of incorporating social context into the quality predictor we described in Section 3.
Our work is based on the following two premises:1.
The quality of a review depends on the quality of the reviewer.
Estimating the quality of the reviewer can help in estimating the quality of the review.2.
The quality of a reviewer depends on the quality of their peers in the social network.
We can obtain information about the quality of the reviewers using information from the quality of their friends in their social network.We investigate two different ways of incorporating the social context information into the linear quality predictor.
The first is a straightforward expansion of the feature space to include features extracted from the social context.
The second approach is novel in that it defines constraints between reviews, and between reviewers, and adds regularizers to the linear regression formulation to enforce these constraints.
We describe these two approaches in detail in the following sections.
A straightforward use of the social context information is by extracting additional features for the quality predictor function.
The social context features we consider are shown in the bottom part of Table 1.
The features capture the engagement of the author (ReviewNum), the historical quality of the reviewer (AvgRating), and the status of the author in the social network (In/Out-Degree, PageRank).
This approach of using social context is simple and it fits directly into our existing linear regression formulation.
We can still use Equation 2 for optimizing the function Q, which is now defined over the expanded feature set F .
The disadvantage is that such information is not always available for all reviews.
Consider for example, a review written anonymously, or a review by a new user with no history or social network information.
Predicting using social network features is no longer applicable.
Furthermore, as the dimension of features increases, the necessary amount of labeled training data to learn a good prediction function also increases.
We now present a novel alternative use of the social context that does not rely on explicit features, but instead defines a set of constraints for the text-based predictor.
These constraints define hypotheses about how reviewers behave individually or within the social network.
We require that the quality predictor respects these constraints, forcing our objective function to take into account relationships between reviews, and between different reviewers.
We now describe our hypotheses, and how these hypotheses can be used in enhancing the prediction of the review quality.
In Section 5 we validate them experimentally on real-world data, and we demonstrate that they hold for all the three data sets we consider.
Author Consistency Hypothesis: The hypothesis is that reviews from the same author will be of similar quality.
A reviewer that writes high quality reviews is likely to continue writing good reviews, while a reviewer with poor reviews is likely to continue writing poor reviews.
Trust Consistency Hypothesis: We make the assumption that a link from a user u1 to a user u2 is an explicit or implicit statement of trust.
The hypothesis is that the reviewers trust other reviewers in a rational way.
In this case, reviewer u1 trusts reviewer u2 only if the quality of reviewer u2 is at least as high as that of reviewer u1.
Intuitively, we claim that it does not make sense for users in the social network to trust someone with quality lower than themselves.
Co-Citation Consistency Hypothesis: The hypothesis is that people are consistent in how they trust other people.
So if two reviewers u1, and u2 are trusted by the same third reviewer u3, then their quality should be similar.
The hypothesis is that if two people are connected in the social network (u1 trusts u2, or u2 trusts u1, or both), then their quality should be similar.
The intuition is that two users that are linked to each other in some way, are more likely to share similar characteristics than two random users.
This is the weakest of the four hypotheses but we observed that it is still useful in practice.
We now describe how we enforce the hypotheses defined above by designing regularizing constraints to add into the text-based linear regression defined in Section 3.
Author Consistency: We enforce this hypothesis by adding a regularization term into the regression model where we require that the quality of reviews from the same author is similar.
Let Ru denote the set of reviews authored by reviewer u, including both labeled and unlabeled reviews.
Then the objective function becomes:Ω2(Q) = Ω1(Q) + β 񮽙 u∈U 񮽙 r i ,r j ∈Ru (Q(ri) − Q(rj)) 2(3)Minimizing the regularization constraint will force reviews of the same author u to receive similar quality values.
We can formulate this as a graph regularization.
The graph adjacency matrix A is defined as Aij = 1 if review ri and review rj are authored by the same reviewer, and zero otherwise.
Then, Equation 3 becomes:Ω2(w) = 1 n 񮽙 n 񮽙 񮽙 i=1 񮽙 w T ri − qi 񮽙 2 + αw T w + β 񮽙 i<j Aij 񮽙 w T ri − w T rj 񮽙 2(4)Let R = [r1, ..., rn] be an f × n feature-review matrix defined over all reviews (both labeled and unlabeled).
Then the last regularization constraint of Equation 4 can be written as 񮽙 i<j Aij 񮽙 w T ri − w T rj 񮽙 2 = w T RΔAR T w Δ A = D A − Aˆ w = ( n 񮽙 񮽙 i=1 rir T i + αn 񮽙 I + βn 񮽙 RΔ A R T ) −1 n 񮽙 񮽙 i=1 qiriTrust Consistency: Let u be a reviewer.
Given a review quality predictor function Q, we define the reviewer quality ¯ Q(u) as the average quality of all the reviews authored by this reviewer as it is estimated by our quality predictor.
That is,¯ Q(u) = 񮽙 r∈Ru Q(r) |Ru| = 񮽙 r∈Ru w T ri |Ru|(5)We enforce the trust consistency hypothesis by adding a regularization constraint to Equation 2.
Let Nu denote the set of reviewers that are linked to by reviewer u.
We haveΩ3(Q) = Ω1(Q) + β 񮽙 u 1 񮽙 u 2 ∈Nu 1 񮽙 max 񮽙 0, ¯ Q(u1) − ¯ Q(u2) 񮽙񮽙 2The regularization term is greater than zero for each pair of reviewers u1 and u2 where u1 trusts u2, but the estimated quality of u1 is greater than that of u2.
Minimizing function Ω3 will push such cases closer to zero, forcing the quality of a reviewer u1 to be no more than that of u 2, and thus enforcing the trust consistency hypothesis.
Formally, for a reviewer u, let hu be the n-dimensional normalized indicator vector where hu(i) = 1/|Ru| if user u has written review ri, and zero otherwise.
Then we have that ¯ Q(u) = w T Rhu.
We can thus write the objective function asΩ3(w) = 1 n 񮽙 n 񮽙 񮽙 i=1 (w T ri − qi) 2 + αw T w (6) + β 񮽙 u,v∈U Suv 񮽙 max 񮽙 0, w T Rhu − w T Rhv 񮽙񮽙 2where S is the social network matrix.
The optimization problem is still convex, but due to the max function, no nice closed form solution exists.
We can still solve it and find the global optimum by gradient descent, where the gradient of the objective function is∂Ω3(w) 2∂w = 1 n 񮽙 n 񮽙 񮽙 i=1 rir T i w − 1 n 񮽙 n 񮽙 񮽙 i=1 riqi + αw + β 񮽙 u,v, w T R(hu−hv )>0 SuvR(hu − hv)(hu − hv) T R T w Let H = [h1, ..., hm] be an n×m matrix defined over all reviewers and Z be a new matrix such thatZuv = ⎧ ⎨ ⎩ Suv if 񮽙 diag(w T RH)S − S diag(w T RH) 񮽙 uv > 0 0 otherwiseNow we can rewrite the gradient as Co-Citation Consistency: We enforce this hypothesis by adding a regularization term into the regression model, where we require that the quality of reviews authored by two co-cited reviewers is similar.
Then, the objective function (Equation 2) becomes:∂Ω3(w) 2∂w = 1 n 񮽙 n 񮽙 񮽙 i=1 rir T i w − 1 n 񮽙 n 񮽙 񮽙 i=1 riqi + αw + βRHΔ Z H T R T w where Δ Z = D Z + D Z T − Z − Z T canΩ4(Q) = Ω1(Q) + β 񮽙 u∈U 񮽙 x,y∈Nu 񮽙 ¯ Q(x) − ¯ Q(y) 񮽙 2Minimizing function Ω4 will cause the quality difference of reviewers x and y to be pushed closer to zero, making them more similar.We can again formulate these constraints as a graph regularizaton.
Let C be the co-citation graph adjacency matrix, where Cij = 1 if two reviewers ui and uj are both trusted by at least one other reviewer u. Using the same definition of matrix R and vector hu as for trust consistency, the objective function now becomesΩ4(w) = 1 n 񮽙 n 񮽙 񮽙 i=1 񮽙 w T ri − qi 񮽙 2 + αw T w + β 񮽙 i<j Cij 񮽙 w T Rhi − w T Rhj 񮽙 2(7)Let Δ C be the Laplacian of graph C.
The closed form solution isˆwisˆ isˆw = 񮽙 n 񮽙 񮽙 i=1 rir T i + αn 񮽙 I + βn 񮽙 RHΔ C H T R T 񮽙 −1 n 񮽙 񮽙 i=1 riqi Link Consistency:The regularization for this hypothesis is very similar to the one for the co-citation consistency.
We treat the trust network as an undirected graph.
Let B be the corresponding matrix, where Bij = 1 if Sij = 1 or Sji = 1.
Our objective function now becomesΩ5(w) = 1 n 񮽙 n 񮽙 񮽙 i=1 񮽙 w T ri − qi 񮽙 2 + αw T w + β 񮽙 i<j Bij 񮽙 w T Rhi − w T Rhj 񮽙 2(8)with a similar closed form solutionˆwsolutionˆ solutionˆw = 񮽙 n 񮽙 񮽙 i=1 rir T i + αn 񮽙 I + βn 񮽙 RHΔ B H T R T 񮽙 −1 n 񮽙 񮽙 i=1 riqiIn all these cases, β is a weight on the added regularization term which defines a trade-off between the mean squared error loss and the regularization constraint in the final objective function.Adding the regularization makes our problem a semi-supervised learning problem.
That is, our algorithms operate on both the labeled and the unlabeled data.
Although, only the labels of the labeled data are known to the algorithm, the unlabeled data are also used for optimizing the regularized regression functions.
This gives considerable more flexibility to the algorithm, since it is able to operate even with little labeled data by making use of the unlabeled data and the constraints defined by the social context.
Furthermore, through regularization the signal from the social context is incorporated into the textual features.
The resulting predictor function operates only on textual features, so it can be applied even in the case where there is no social context.
In this section, we present the experimental evaluation of our techniques.
For our experiments we use product reviews obtained from a real online commerce portal.
We begin by describing the characteristics and preprocessing of our data sets.
Then, we test the hypotheses we proposed in Section 4.2.2 on these real-world datasets.
Finally, we evaluate the prediction performance of different methods and conduct some analysis.
Our experiments employ the data from Ciao UK 1 , a community review web site.
In Ciao, people not only write critical reviews for all kinds of products and services, but also rate the reviews written by others.
Furthermore, people can add members to their network of trusted members or "Circle of Trust", if they find their reviews consistently interesting and helpful.We collected reviews, reviewers, and ratings up to May, 2009 for all products in three categories: Cellphones, Beauty, and Digital Cameras (DC).
We use the average rating of the reviews (a real value between 0 and 5) as our gold standard of review quality.
In order for the gold standard to be robust and resistant to outlier raters, we use only reviews with at least five ratings from different raters.
We then apply some further pruning by imposing the conditions shown in the top part of Table 2.
The purpose of the pruning is to obtain a dataset that is both large enough and has sufficient social context information.
Because we need some information about reviewers' history in order to test our Reviewer Consistency hypothesis, we require reviewers for Cellphone and Beauty to have at least two reviews each.
We also require reviewers to be part of the trust social network (with at least one link in the social network), in order to test our hypotheses and methods based on social networks.
Finally, we require for each product to have some representation in the dataset, that is, a sufficiently large number of reviews.
The pruning thresholds are selected per category, so as to obtain sufficient volume of data.
For the Digital Cameras category, this results in a minimum amount of pruning.
Although DC reviews do not contain much social context information, we still include them here for comparison and generality purposes.
From the statistics in Table 2, we can see that Cellphone and Beauty reviews contain more rich social context information than DC reviews in the sense that the average number of reviews per reviewer is more than twice that for Digital Cameras, and the link density (defined as D = for a graph with vertices V and edges E) is more than 10 times that of Digital Cameras.
We also plot the Kernel-smoothing density estimate (pdf) of the samples qi (the gold standard review quality) in Figure 1.
The distributions of qi for the three categories are quite different.
Beauty reviews are highly concentrated at rating 4, while Cellphone and DC reviews have a more balanced distribution of quality.
We summarize the characteristics of the three data sets in the bottom of Before evaluating the prediction performance of different algorithms, we first validate our four consistency hypotheses over our data sets.
For each dataset, we consider all n 2 pairs of reviews (ri, rj), and we divide them into two disjoint groups: Rel:DifferentReviewer if ri and rj are authored by different reviewers, i.e., ui 񮽙 = uj, and Rel:SameReviewer if ui = uj.
In each group, for each pair (ri, rj) we compute the difference in quality, dqij = qi − qj, of the two reviews.
Since for each value dqij we also include value dqji = −dqij the mean value of dqij for both groups is zero.
We are interested in the standard deviation, std(dqij), that captures how much variability there is in the difference of quality between reviews for the two groups.
Table 3 shows the results for the different datasets.
For a visual comparison, in Figure 2 we also plot the Kernel-smoothing density estimates of the two groups.We observe that the standard deviation of the quality difference of two reviews by the same author is much lower than that of two reviews from different authors.
This indicates that reviewers are, to some extent, consistent in the quality of reviews they write.
The figures also clearly indicate that the density curve for Rel:SameReviewer is more concentrated around zero than Rel:DifferentReviewer for all three categories.
Moreover, two-sample Kolmogorov-Smirnov (KS) test of the samples in the two groups indicates that the difference of the two groups is statistically significant.
The p-values are shown in the last row of Table 3.
The star next to the p-value means there is strong evidence (p < 0.01) that the two samples come from different distributions.
In order to test the three social network consistency hypotheses, namely Trust Consistency, Co-Citation Consistency and Link Consistency, we look at the empirical distribution of d ¯ Q * ij = ¯ Q * (ui) − ¯ Q * (uj), i.e., the difference in quality of two reviewers, where, similar to Equation 5¯ Q * (u) = 񮽙 r i ∈Ru qi |Ru|(9)is defined as the average quality of the reviews written by u in our dataset, but using gold standard quality.
Again, we group the pairs of reviewers (ui, uj) into the the following sets depending on the relationship between the two reviewers.Rel:None: User ui is not linked to user uj, i.e., Bij = 0.
Rel:Trust: User ui trusts user uj, i.e., Sij = 1.
Rel:Cocitation: Users ui and uj are trusted by at least one other reviewer u3, i.e., Cij = 1.
Rel:Link: User ui trusts user uj, or uj trusts ui, i.e., Bij = 1.
In Figure 3, we plot the Kernel-smoothing density estimate of the d ¯ Q * ij values for the four different sets of pairs, for the three categories.
We further show in Table 4 variance) of the four density estimates and p-values of the KS-test between pairs of density estimates.
The first observation is that the distribution of Rel:Trust is skewed towards the negative with a negative mean.
This supports the Trust Consistency Hypothesis that when ui trusts uj, the quality of ui is usually lower than that of uj, i.e., ¯ Q * (ui) − ¯ Q * (uj) < 0.
The remaining three distributions are all symmetric with mean zero.
However, Rel:Cocitation and Rel:Link have a much more concentrated peak around zero, i.e., smaller variance, compared with Rel:None.
This supports the Co-Citation and Link Consistency Hypotheses that reviewers are more similar in quality (quality difference closer to zero) if they are co-trusted by others, or linked in a trust graph regardless of direction.In the results of the KS-test, we have only one high p-value, for Rel:Link and Rel:Cocitation, while all the other pairs have pvalues close to zero.
This implies that Rel:Trust, Rel:Cocitation, or Rel:Link do not come from the same distribution as Rel:None.
This observation directly connects the quality of reviewers with their relations in the social network.
The correlation between Rel:Link and Rel:Cocitation could potentially be explained by the relatively high reciprocity ratio (the percentage of links in the Trust social network that are reciprocal), and the relatively high clustering coefficient [14] which measures the tendency of triples to form triangles.In summary, our experiments indicate that there exists correlation between review quality, reviewer quality, and social context.
For all the three data sets considered, the statistics support our hypotheses for designing the regularizers.
For all three datasets (Cellphones, Beauty, and Digital Cameras), we randomly split the data into training and testing sets: 50% of the products for training (Rtrain), and 50% for testing (Rtest).
We keep the test data fixed, while sub-sampling from the training data to generate training sets of different sizes (10%, 25%, 50% or 100% of the training data).
Our goal is to study the effect of different amount of training data on the prediction performance.
We draw 10 independent random splits, and we report test set mean and standard deviation for our evaluation metrics.
A polynomial kernel is used to enrich the feature representation for the linear model.
We fix the parameter α of Linear Regression to the value that gives the best performance for the text-based baseline.
Then, we report the best prediction performance by tuning the regularization weight β.
We will discuss the parameter sensitivity in Section 5.3.3, while leaving the automatic optimization of parameters as future work.
We evaluate the effectiveness of different prediction methods using Mean Squared Error (MSE) over the test set Rtest of size nt,MSE(Rtest) = 1 nt n t 񮽙 i=1 (Q(ri) − qi) 2MSE measures how much our predicted quality deviates from the true quality.
A smaller value indicates a more accurate prediction.
Since the graph statistics in Section 5.2 support our design of regularizers, we will examine a few text-free baselines (TBL) that are based solely on social context.
These baselines also serve as a sanity check for the experiments we report in the following section.
For the following, r denotes a test review written by reviewer ur, and ¯ Q * (u) is the quality of reviewer u as defined in Equation 9, when computed over the training data.
If reviewer u has no reviews in the training data, ¯ Q * (u) is undefined.
We consider the following baselines for predicting the quality of r. Simply predict as the mean review quality in the training data R train , i.e., Q(r) = 1 n t 񮽙 n t i=1 qi.
TBL:Reviewer: Predict as the quality ¯ Q * (ur) of the author ur in the training data.
If it is not defined, predict as TBL:Mean.TBL:Link: Predict as the mean quality of all the reviewers connected to ur in the link graph; if no such reviewer exists in the training set, or the value is undefined simply predict as TBL:Mean.
TBL:CoCitation: Similar to TBL:Link, predict as the mean quality of all reviewers connected to ur in the Co-Citation graph.
If this is not defined predict as TBL:Mean.We compare the four simple text-free baselines against BL:Text: the Linear Regression baseline that uses only text information.
Fig- ure 4 shows the MSE with standard deviation where the x-axis corresponds to the different percentages of the training data we used.
We observe that none of the text-free baselines works as well as Linear Regression with textual features, suggesting that social context by itself cannot accurately predict the quality of a review.
The MSE of the text-free baselines is lower for the Beauty category, where quality distribution is highly skewed at 4, but the text-based predictor is still significantly better.
Out of the three social-context based baselines, TBL:Reviewer appears to provide more accurate prediction than the other two when there is rich social context (Cellphones and Beauty), but it offers marginal improvements over TBL:Mean in the case where the social context is sparse (Digital Cameras).
TBL:CoCitation consistently outperforms TBL:Link, which is in line with our observation in Table 4 that the variance of Rel:Cocitation is smaller than that of Rel:Link.
We now compare the different techniques for review quality prediction that make use of text and social context of reviews.
We consider the following methods.
Linear Regression with a regularizer under Cociation Consistency Hypothesis (Equation 7).
REG:Trust: Linear Regression with a regularizer under Trust Consistency Hypothesis (Equation 6) It is possible to consider combinations of the different regularizers.
This would introduce multiple β parameters (one for each regularizer), and careful tuning is required to make the technique work.
We defer the exploration of this idea to future work.The results of our experiments are summarized in Table 5 where we show the mean MSE and the standard deviation for all techniques, over all categories, for different training data sizes.
In the parentheses we have the percentage of reduction over MSE of the text-based baseline BL:Text.
The best result (largest decrease of MSE) for each data set and each training size is emphasized in bold.The first observation is that adding social context as additional features BL:Text+Rvr can improve significantly over the text-only baseline when there is sufficient amount of training data.
The more training data available, the better the performance.
BL:Text+Rvr gives the best improvement for training percentage of 50% and 100% for all three categories.
We expect a similar trend for larger amounts of training data.
On the other hand, when there is little training data, the social context features are too sparse to be helpful, and it may be the case that the MSE actually increases, e.g., when training with 10% and 25% of the training data for Cellphone, and training with 10% for Digital Cameras.
There are techniques for dealing with sparse data, however, exploring such techniques is beyond the scope of this paper.Using social context as regularization (method names starting with REG) consistently improves over the text-only baseline.
The advantage of the regularization methods is most significant when the training size is small, e.g. using training percentage of 10% and 25% in all three data sets.
This is often the case in practice, where we have limited resources for obtaining labeled training data, while there are large amounts of unlabeled data available.Among the different regularization techniques, for both Cellphone and Beauty reviews, where there is relatively rich social context information, REG:Reviewer appears to be the most effective.
For the Cellphone dataset, REG:Reviewer outperforms BL:Text+Rvr even with 50% of training data, indicating that social context regularization can be helpful when we have rich social context and balanced data.
Among the regularization methods using the social network, REG:Trust, which is based on the most reasonable hypothesis, performs best in practice.
This means that the direction of the trust social network carries more useful information than the simplified undirected link graphs and co-citation graphs.Finally, for the Digital Camera reviews where the social context is very sparse there is still some improvement observed using regularization when the training data is small, but the improvement is not as significant as on the other two categories where the social context is richer; that is exactly what we expected.In addition to the experiments on our test data, we are interested in testing our algorithms on data for which we have no social context information.
Our premise is that using regularization can help to incorporate signals from the social network to the text-based predictor, thus improving accuracy prediction even if social context is not available.
We now validate this premise.
We use the Cellphone dataset, and we consider the case where we train on 10% of the training data.
Within the test data of Cellphone, there is a subset of data (144 reviews on average across splits) that has no social context information, i.e., the author has only one review, and is not in the social network.
2 Regularization methods only adjust weights on textual features and are thus applicable to those anonymous reviews too, even though these reviews do not contribute to the added regularization terms.
In Table 6, we report the percentage of improvement of four regularization methods over BL:Text.
We still observe some improvement on anonymous reviews with no social context, although as expected less than on reviews with social context.
This indicates the the generalizablity of the proposed regularization methods.To further support the generalizablity claim, we try an extra set of experiments testing our regularization methods on a held-out set of reviews which are not used in the optmization process and for which we use only the textual features and hide their social context.
More specifically, after learning a quality prediction function Q using 10% of the training data, we apply it to the remaining 90% of the training data, by multiplying the learned weight vector w with the text feature vectors of the held-out reviews.
From the last row in Table 6, we can clearly see that compared with the text-only baseline, all regularization methods can learn a better weight vector w that captures more accurately the importance of textual features for predicting the true quality on the held-out set.In summary, we make the following observations.
• Adding social context as features is effective only when there is enough training data to learn the importance of those additional features.
• On the other hand, regularization methods work best when there is little training data by exploiting the constraints defined by the social context and the large amount of unlabeled data.
• Since regularization techniques incorporate the social context information into the text-based predictor, they provide improvements even when applied to data without any social context.
Regularization methods have one parameter β to set: the tradeoff weight for the regularization term.
The value of the regularization weight defines our confidence in the regularizer: a higher value results in a higher penalty when violating the corresponding regularization hypothesis.
In the objective functions (Equations 4, 6, 7, and 8), the contribution from the regularization term depends on β as well as the number of non-zero edges in the regularization graph.
2 Although we prune the data by requiring that each reviewer has at least two reviews and a link in the social network, due to multiple consecutive pruning conditions some reviewers end up with only one review and no links in the final pruned subset.
We define the sum of regularization weight as σ = β 񮽙 ij Mij, where M can be the co-author matrix A, the directed trust matrix S, the co-citation matrix C, or the undirected link matrix B. Figure 5 shows how the prediction performance of regularization methods varies as we use different values of σ.
We only show the parameter sensitivity for Cellphone and Beauty reviews where the social context is relatively rich.
The training data size is fixed to be 10%.
As we can see, even though Cellphone and Beauty reviews carry different characteristics, the curves follow a very similar trend: as long as we set σ ≤ 0.1, all regularization methods achieve consistently better performance than the baseline.
As σ goes to zero, the performance converges to the text-based baseline.
In addition, the shape of the performance curve depends on the corresponding hypothesis.
For example, the optimum σ for REG:Trust is larger than that of REG:Link and REG:Cociation.
Also, even with a value of σ higher than the optimum, the error of the REG:Reviewer does not increase as quickly as for the other methods.
These observations are in line with the previous observations that the history of the reviewer (REG:Reviewer) and the Trust graph (REG:Trust) provide a better signal than the CoCitation graph, or the Link graph.
The problem of assessing the quality of user-generated content has recently attracted increasing attention.
Most previous work [17,10,11,6,12,15] has typically focused on automatically determining the quality (or helpfulness, or utility) of reviews by using textual features.
The problem of determining review quality is formulated as a classification or regression problem with users' votes serving as the ground-truth.
In this context, Zhang and Varadarajan [17] found that shallow syntactic features from the text of reviews are most useful, while review length seems weakly correlated with review quality.
In addition to textual features, Kim et al. [10] included metadata features including ratings given to an item under review and concluded that review length and the number of stars in product rating are most helpful within their SVM regression model.
Ghose and Ipeirotis [6] combined econometric models with textual subjectivity analysis and demonstrated evidence that extreme reviews are considered to be most helpful.
In [12], the authors incorporated reviewers' expertise and review timeliness in addition to the writing style of the review in a non-linear regression model.
In our work, we extend previous work by using author and social network information in order to assess review quality.Although user votes can be helpful as ground-truth data, Liu et al [11] identified a discrepancy between votes coming from Amazon.
com and votes coming from an independent study.
More specifically, they identified a "rich-get-richer" effect, where reviews accumulate votes more quickly depending on the number of votes they already have.
This observation further enhances our motivation to automatically determine the quality of reviews in order to avoid such biases.
Danescu-Niculescu-Mizil et al. [5] showed that the perceived helpfulness of a review depends not only on its content but also on the other reviews of the same product.
We include one of their hypotheses, i.e. conformity hypothesis, as a feature into our model.
A recent paper [15] took an un-supervised approach to finding the most helpful book reviews.
Although their method is shown to outperform users' votes, it is evaluated on only 12 books and thus is not clear whether it is robust and generalizable.
The problem of assessing the quality of user-generated data is also critical in domains other than reviews.
For example, previous works [2,4] focused on assessing the quality of postings within the community question/answering domain.
The work in [2] combines textual features with user and community meta-data features for assessing the quality of questions and answers.
In [4], the authors propose a co-training idea that jointly models the quality of the author and the review.
However, their work does not model user relationships, bur rather uses all community information for exacting features.Regularization using graphs has appeared as a type of effective method in the semi-supervised learning literature [19].
The interested reader may examine [18,20,3].
The resulting formulation is usually a well-formed convex optimization problem which has a unique and efficiently computable solution.
These types of graph regularization methods have been successfully applied in Web-page categorization [16] and Web spam detection [1].
In both cases, the link structure among Web pages is nicely exploited by the regularization which, in most cases, has improved the predictive accuracy within the problem at hand.
Recently, Mei et al. [13] propose to enhance topic models by regularizing on a contextual graph structure.
In our scenario, the social network of the reviewers defines the context, and we exploit it to enhance review quality prediction.
In this paper we studied the problem of automatically determining review quality using social context information.
We studied two methods for incorporating social context in the quality prediction: either as features, or as regularization constraints, based on a set of hypotheses that we validated experimentally.
We have demonstrated that prediction accuracy of a text-based classifier can greatly improve, when working with little training data, by using regularization on social context.
Importantly, our regularization techniques make the general approach applicable even when social context information is unavailable.
The method we propose is quite generalizable and applicable for quality (or attribute) estimation of other types of user-generated content.
This is a direction that we intend to explore further.As further future work, social context can be enhanced with additional information about items and authors.
Information about product attributes, for example, enables estimates of similarity between products, or categories of products which can be exploited as additional constraints.
Furthermore, although a portal may lack an explicit trust network, we plan to construct an implicit network using the ratings reviewers attach to each others' reviews and then apply our techniques to this case.
Finally, rather than predicting the quality of each review, it would be interesting to adapt our techniques for computing a ranking of a set of reviews.
