Multicast has long been a performance bottleneck for data centers.
Traditional solutions relying on IP mul-ticast suffer from poor congestion control and loss recovery on the data plane, as well as slow and complex group membership and multicast tree management on the control plane.
Some recent proposals have employed alternate optical circuit switched paths to enable loss-less multicast and a centralized control architecture to quickly configure multicast trees.
However, the high circuit reconfiguration delay of optical switches has substantially limited multicast performance.
In this paper, we propose to eliminate this reconfigu-ration delay by an unconventional optical multicast architecture called HyperOptics that directly interconnects top of rack switches by low cost optical splitters, thereby eliminating the need for optical switches.
The ToRs are organized to form the connectivity of a regular graph.
We analytically show that this architecture is scalable and efficient for multicasts.
Preliminary simulations show that running multicasts on HyperOptics can on average be 2.1× faster than on an optical circuit switched network.
As datacenters scale up, online services and data intensive computation jobs running on them have an increasing need for fast data replication from one source machine to multiple destination machines, or the multicast service.
Apart from traditional multicast applications such as simultaneous server OS installation and upgrade [9], data chunks replication in distributed file systems [4,13,29] and cache consistency check on a large number of nodes [14], recent distributed machine learning models also see a huge demand for multicast services.
The explosion of data allows the learning of powerful and complex models with 10 9 to 10 12 parameters [11,18], in which broadcasting the model parameters alone poses a challenge for the underlying network.
Some learning algorithms require the processed intermediate data to be duplicated across different nodes.
For example, the Latent Dirichlet Allocation algorithm for text mining needs to multicast the word distribution data in every iteration [10].
A few thousand iterations of LDA with 1 GB of data for each iteration would easily cause over 1 TB of multicast data transfer in today's datacenters.
Reducing the multicast delay would significantly accelerate the machine learning jobs.However, multicast services are still not natively supported by current datacenters.
The most established solution is IP multicast which is originally designed for the Internet.
Even though some efforts have been made to improve its scalability in the datacenter context [17,19,26], the complex dynamic multicast tree building and maintenance, the potentially high packet loss rate and costly loss recovery, and the lacking of satisfactory congestion control have caused most network operators to eschew its use.On the other hand, as data size continues to grow, there is an increasing trend towards deploying a high bandwidth (40/100 Gbps) network core for datacenters [1].
However, high data rate transmissions are not feasible for even modest-length electrical links.
For example, data transmissions on traditional twinax copper cable propagate at most 7 m at 40 Gbps due to power limitation [5].
Optical communication technologies are well suited to such high bandwidth networks.
The advantages of optical devices and links, such as data rate transparency, lower power consumption, less heat dissipation, lower bit-error rate and lower cost have been noted or already exploited by the industry [3].
As datacenters gradually evolve from electrical to optical, we believe a system design that fully leverages the key physical features of optical technologies is necessary for future datacenters.In this paper, we propose HyperOptics, a novel optical multicast architecture for datacenters.
HyperOptics follows the recent efforts such as [12,20,23,24,27,28] that augment the traditional electrical network with a high speed optical network, but HyperOptics dedicates the optical network to multicast transmissions.
The existing optical network proposals usually employ an Optical Circuit Switch (OCS) to provide configurable connectivity for ToRs.
The switching speed in today's large portcount OCSes is, however, orders of magnitude slower (about tens of millisecond) than packet switches.
In [23], the authors propose a specific implementation of OCS that is capable of switching in microseconds, but it is unscalable to support a large port-count due to the limited number of available optical wavelengths.
Also, OCSes are high cost devices.
According to our recent quote from a vendor, a 192-port OCS would cost 365 K USD.
All these problems of OCSes motivate us to design an optical network that gets rid of its use and directly interconnect the racks by low cost optical splitters.
The design of HyperOptics is inspired by Chord's [25] way of organizing peer nodes in traditional overlay networks.
Each ToR in HyperOptics can talk to multiple neighbor ToRs simultaneously via passive optical splitters, by which the ToRs form the connectivity of a regular graph.We identify two main advantages of HyperOptics over the OCS architecture.
First, HyperOptics can provide high bandwidth even at the packet granularity because the slow circuit switching delay is completely eliminated.
Second, unlike the existing OCS architecture, HyperOptics scales well in the number of ToRs because the constraint of the OCS port-count no longer exists in HyperOptics.
We show that HyperOptics is well suited for high throughput and low latency multicast transmissions.
Data from one ToR could be physically duplicated via an optical splitter to multiple ToRs at line speed.
For multicasts with large group sizes, data is relayed by some intermediate ToRs.
Due to the path flexibility of regular graphs, we show that the maximum path length for any multicast is bounded by log n, where n is the number of ToRs.
Another distinguishing property of HyperOptics is that it can support 2 simultaneously active multicasts with maximal group size.
To take full advantage of the underlying optical technologies, we propose a centralized control plane that manages the routing policy and multicast scheduling.
Preliminary simulations show that HyperOptics can on average be 2.1× faster than the OCS architecture for multicast services.
We first introduce the connectivity structure of ToRs and then discuss the routing strategy under the given network architecture.
Next, we analyze the multicast performance and the wiring complexity of HyperOptics.
And finally, we present an overview of the system.
We assume that all splitters have the same fanout k and the number of ToRs is n = 2 k .
In our model, optical signals can only pass through the splitters in one direction, i.e., from the input port to the output ports.
The ToRs are interconnected as a special k regular graph.
The only difference of HyperOptics from the general k regular graph is that a node (ToR) can only send the same data to its k neighbors simultaneously.
This limitation comes from the fact that the splitter just passively duplicates the input signal on its output ports.Assume that the ToRs are denoted as t 0 , t 1 ,..., t n−1 .
All ToRs are logically organized on a circle modulo 2 k .
Each ToR t i is connected to the input port of splitter s i .
The k output ports of s i are connected to t i+2 0 , t i+2 1 ,..., t i+2 k−1 , respectively.
Note that the gap between t i 's two consecutive neighbors increases exponentially, which is very similar to Chord [25] in organizing peer nodes in overlay networks.
Since all ToRs are on a logical circle, the operations above are all modulo 2 k .
For example, if k = 3 and n = 8, the third neighbor of t 4 is t 4+2 2 = t 0 .
An example of HyperOptics with k = 3 is given in Fig.1.
We only show the connectivity of t 0 , t 3 , t 4 and t 6 in the figure.
The other ToRs are connected in a similar way, e.g., t 1 is connected to t 2 , t 3 , t 5 .
The full connectivity of the architecture is shown in the table on the bottom.
Routing traffic to indirect destinations needs relays.
For example, in Fig 1, a possible path shown as dashed lines from t 0 to t 7 is t 0 − t 4 − t 6 − t 7 where t 4 and t 6 are relays.
There may exist multiple paths between each ToR pair.
The relay set of a multicast is mainly determined by the routing strategy of HyperOptics.
For simplicity, we propose a best-effort based routing strategy for HyperOptics.
We note that our routing strategy might not be optimal and there is room for improvement.
But it already provides satisfactory gains as we will show in Sec. 3.
For a single source-destination pair, best-effort routing will always designate the neighbor that is nearest to the destination as the next relay.
Also, we ensure that the index of the next relay is logically smaller than the destination.
Mathematically, given a destination t j , a relay ToR t i will specify t i+2 log( j−i) as the next relay.
The routing algorithm will recursively compute the remaining relays as if the next relay is the source.
For example, consider the traffic from t 0 to t 7 in Fig 1, the next relay for t 0 is t 4 because t 4 is one of t 0 's neighbor that is nearest to t 7 .
And the next relay for t 4 is t 6 .
Hence, the relay set for the path from t 0 to t 7 is {t 4 ,t 6 }.
Note that the next relay of t 4 is not t 0 because t 0 has logically passed the destination t 7 .
For multicasts, best-effort routing will compute the relay set for each individual destination and then return the union of all relay sets.
We now analyze the multicast performance under the design of HyperOptics and compare the cost of HyperOptics with the traditional OCS networks.
Multicast hop-count: The hop-count of a multicast characterizes the minimum latency of a packet traversing from the source to the destination.
The following theorem gives the worst case and average hop-count of a multicast in our architecture.Lemma 1.
The maximum hop-count of a multicast under best-effort routing is upper-bounded by log n and the average hop-count is log n 2 .
Proof.
All ToRs in HyperOptics are logically equal.
Without loss of generality, we consider a multicast originating from t 0 .
The k direct neighbors of t 0 is ToR 2 0 ,...,2 k−1 , these IDs differ from 0 by only one bit.
Similarly, the IDs of ToRs that are two hops away from t 0 differ by two bits.
The farthest ToR differs by k bits.
In best-effort routing, traversing a hop is equivalent to flipping the most significant bit of the source ToR's ID that is different with the corresponding bit of the destination's ID.
Therefore, the largest hop-count is k = log n.
The number of ToRs that are j hops away from t 0 is kj , (1 ≤ j ≤ k).
The average hop-count is ∑ k j=1 j( k j ) ∑ k j=1 ( k j ) = k2 k−1 2 k = k 2 = log n 2 .
For one hop, the signal decoding and packet processing can be done in sub-nanosecond [15].
Therefore, for a datacenter with 1 K racks, the average latency for a multicast is less than 0.5 * log 1000 * 1 ns ≈ 5.0 ns.
In the following, we simply assume that the multicast latency is negligible.
Simultaneously active multicasts: Each ToR in HyperOptics has k direct neighbors.
In an extreme case where all group members of a multicast are the source's direct neighbors, HyperOptics could support n active multicasts simultaneously.
In another extreme case where multicasts' group sizes are maximal and need the most number of relays, the number of simultaneous active multicasts would be much smaller.
However, the following theorem shows that HyperOptics still has the capability of servicing multiple multicasts simultaneously in the worst case.Lemma 2.
HyperOptics can simultaneously service two one-to-all multicasts.
ToR port-count: In HyperOptics, each ToR is connected to the input port of a 1 × k splitter.
One splitter would take up k + 1 ports across the ToRs.
The average number of of occupied ports on each ToR would be n * (k+1) n = 1 + log n. Cost: Even though HyperOptics does not use the OCS, it occupies more ToR ports than the OCS network.
The per-port OCS cost is 1.5K USD, derived from our recent vendor quote (365K USD for a 192-port switch) but factors in a 20% discount.
The per port cost of ToR and transceiver are 100 USD and 200 USD respectively, from [6,8].
Splitters are very inexpensive at 5 USD per port [7].
For a medium size datacenter with 128 ToRs where each ToR is connected to other ToRs via 40 Gbps links, the total networking cost for HyperOptics is approximately 0.31 M USD.
The total cost of the OCS network using a commercially available 192-port OCS is comparable at 0.33 M USD.
For a datacenter with 256 racks, the total costs for HyperOptics and the OCS network using a 320-port OCS become 0.69 M and 0.56 M, respectively.
HyperOptics is thus cost comparable with the OCS architecture under the current price of different network elements.
Wiring complexity: While the total number of fibers needed to interconnect the ToRs is n log n.
Many of them are short fibers that only go across a few racks.
In a datacenter with 2 k racks, the k fibers from each ToR will go across 2 0 , 2 1 ,..., 2 k−1 racks, respectively.
For instance, in a datacenter with 256 racks, only 2 fibers will go across over 50 racks for each ToR.
The total number of long fibers that go across over 50 racks is 2 * 256 = 512.
For large datacenters with thousands of racks, we envisage that the ToRs are packaged into Pods.
Pods can be wired in the same way as if one Pod is a single virtual ToR.
This hierarchical organization of ToRs would significantly reduce the number of global fibers.
A systematic study of this hierarchical design is our future work.
Fig. 3 shows an overview of the HyperOptics architecture.
Our current design of HyperOptics assumes that the network core bandwidth can be fully utilized.
This assumption holds when the link bandwidth between a server and its ToR is the same as the inter-rack bandwidth.
Or when a server's bandwidth is lower than the inter-rack bandwidth, multiple sources within a rack have the same destination set.
The work-flow of HyperOptics is as follows.
The manager first receives multicast requests from source servers.
A multicast request contains the request ID, the source server, the destination servers and the flow size.
The manager then computes the relay set for each request and send to each ToR i a list of IDs of multicasts that require ToR i as a relay.
All multicast data packets contain a multicast ID in their headers.
During the service period, when ToR i receives a packet, it will read the packet header and check whether it is a relay for the packet and relay the packet if it is.
Note that this rule installation process is conducted only once before each scheduling cycle.
Since relays are non-sharable resources for a multicast, multicasts that require common relays must be serviced sequentially.
The HyperOptics manager will compute a schedule for all requests, which we will discuss in the next section.
Every time a server finishes sending its multicast traffic, it will send a finish message to the manager, the manager then checks whether it is time to schedule the next batch of multicasts.
If yes, then the manager will send a start message to the source servers of the next batch.
Rules for the current scheduling cycle will be deleted on ToRs before the next cycle begins.i i+1 i+2 " … i+2 #$" … i+2 i+2 % +2 " i+2 % +2 &$" … … i+2 #$" -1 i+2 #$" i+2 &$" +2 #$" -2 = i-2 … i+2 #$" i+2 #$" +1 i+2 &$" +2 #$" -1= i-1 i+2 &-" i+ 2 #$" +1 i+2 #$" +2 " … i+2 #$" +2 #$" = i … i+2 #$" +2 i+2 #$" +2 " i+2 #$" +2 &$" +1 = i+1 … … i-1 i i+2 &$" -2 … i i+1 i+2 &$" -1 Given the input of n multicast requests, we now consider how to schedule these multicasts such that the overall delay is minimized.
We formulate this problem as a max vertex coloring problem [22] where a vertex corresponds to a multicast, the edges correspond to the conflict relations among multicasts, i.e., if two multicasts have common relays, there's an edge between them.
The weight of a vertex corresponds to the flow size of the multicast.
Max vertex coloring has been shown to be strongly NPhard [16].
We therefore focus on efficient heuristics.
HyperOptics adopts a heuristic called Weight based First Fit (WFF) in which the vertices are first sorted in a nonincreasing order of their weights.
WFF then scans the vertices and assign each vertex a least-index color that is consistent with its already colored neighbors.
The WFF heuristic is a specific version of the online coloring method for general graph coloring problems whose approximation ratio is analyzed in [21].
The time complexity of WFF is Θ(|V | 2 ).
In HyperOptics, the inter-rack link bandwidth is 40 Gbps.
We also simulate the following two networks to compare with HyperOptics.OCS network: Each ToR is connected to an OCS via a 40 Gbps link.
The OCS has 320 ports, among which some are occupied by the ToRs, and the remaining ports are reserved for optical splitters.
The number of splitters varies with the fanout of each splitter.
The maximum group size achieved by cascading m 1 × k splitters is k + (m − 1) * (k − 1).
We assume the OCS reconfiguration delay is 25 ms according to commercially available products [2].
As is discussed in Sec. 2.3, the total cost of this network is comparable to HyperOptics.
Conceptual OCS network: We assume the Conceptual OCS has zero reconfiguration delay and sufficient portcount to support arbitrary multicast group size.
The other configurations are the same as the OCS network.
This network is not feasible in practice; it only serves as a comparison baseline to isolate the effect of different design components of HyperOptics.
The control plane delay consists of the scheduling algorithm computation time, the rule installation time and the control message transmission time between the manager and the servers.
The computation time (measured at run-time) and the rule installation time (about 8.7 ms [28]) are one-time overheads for each scheduling cycle.
The control messages between the manager and the servers can be implemented using any existing RPC solutions and its delay has been shown to be less than 2 ms [24,28].
We assume in a scheduling cycle every rack has exactly one server that generates a multicast request (id, i, D, f ) with itself being the source, D being a random subset of servers in other racks as receivers (each rack has a 50% chance of having some receivers for each source), and f being a random flow size between 10 MB and 1 GB.
The number of requests is equal to the number of ToRs.
We repeat the experiment 500 times and report the average result.
This traffic pattern helps us evaluate the network core capacity of the HyperOptics architecture.
Note that the group size of a multicast is constrained in the OCS networks due to the limited number of ports available for splitters.
For better evaluations, we make sure that all multicast group sizes are no larger than the largest group size that the OCS network can support.
We apply the WFF scheduling algorithm on both HyperOptics and the OCS network and compare the total flow completion time (FCT) of multicasts.
The conflict relations of multicasts are only slightly different in the OCS network than in HyperOptics.
In the OCS network, multicasts conflict when they share some destinations or there are not enough splitter resources to service them simultaneously, or one multicast's source is another multicast's destination.
The overall multicast delay for the OCS network might vary as the splitter fanout changes.
Table 1 shows the average FCT of 256 random multicasts on the OCS network with varying splitter fanout.
It can be seen that the FCT remains quite constant.
Intuitively, smaller/larger splitter fanout would yield better result when the multicast group size is small/large.
In the following experiments, we always report the best result of the OCS network using various splitters.
an increasing number of ToRs.
We identify two reasons for HyperOptics' advantages.
First, HyperOptics does not use the OCS, the high reconfiguration delay (occurring every time the circuits need to change) is completely eliminated.
As can be seen, the overhead of OCS, mainly the OCS reconfiguration delay, is on average 24× larger than the overhead of HyperOptics, which contains only the 2 ms control message delay between the manager and the servers.
Second, in the OCS network, a ToR can only receive traffic from one other ToR at a time.
As a result, multicast requests that share some common destinations must be serviced sequentially.
However, ToRs are interconnected in a log n regular graph in HyperOptics, each ToR can receive traffic from log n other ToRs simultaneously.
We observe that the Conceptual OCS network is still 1.8× slower than HyperOptics.
This fact shows that the unique connectivity structure of HyperOptics alone can lead to a significant FCT improvement.
We run our C++ implementation of WFF scheduling on a 3.4 GHz, 4 GB RAM Linux machine.
As is shown in Fig. 5, the time cost is less than 80 ms with 600 requests and less than 18 ms with 256 requests.
In addition, this time cost is a one-time overhead for a scheduling cycle.
The manager does not need to recompute the schedule in the service period.
Results in Fig. 5 demonstrates that the HyperOptics manager is responsive in handling a large number of requests.
We have presented HyperOptics, a multicast architecture for datacenters.
A key contribution of HyperOptics is its novel connectivity design for the ToRs that leverages the physical layer optical splitting technology.
HyperOptics achieves high throughput and overcomes the high switching delay of the OCS.
We show that the overall cost of HyperOptics is comparable with the OCS network, but it is on average 2.1× faster than the OCS network for multicast services.
Our current routing and scheduling techniques in HyperOptics are quite basic and have much room for improvements.
Our next step is to explore alternate routing and scheduling techniques to fully exploit the HyperOptics architecture.
We would like to thank the anonymous reviewers for their thoughtful feedback.
This research was sponsored by the NSF under CNS-1422925, CNS-1305379 and CNS-1162270, an IBM Faculty Award, and by Microsoft Corp.
