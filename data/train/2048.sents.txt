As NAND flash technology continues to scale, flash-based SSDs have become key components in data-center servers.
One of the main design goals for data-center SSDs is low read tail latency, which is crucial for interactive online services as a single query can generate thousands of disk accesses.
Towards this goal, many prior works have focused on minimizing the effect of garbage collection on read tail latency.
Such advances have made the other, less explored source of long read tails, block erase operation, more important.
Prior work on erase suspension addresses this problem by allowing a read operation to interrupt an ongoing erase operation, to minimize its effect on read latency.
Unfortunately, the erase suspension technique attempts to suspend/resume an erase pulse at an arbitrary point, which incurs additional hardware cost for NAND peripherals and reduces the lifetime of the device.
Furthermore, we demonstrate this technique suffers a write starvation problem, using a real, production-grade SSD.
To overcome these limitations, we propose alternative practical erase suspension mechanisms, leveraging the iterative erase mechanism used in modern SSDs, to suspend/resume erase operation at well-aligned safe points.
The resulting design achieves a sub-200µs 99.999th percentile read tail latency for 4KB random I/O workload at queue depth 16 (70% reads and 30% writes).
Furthermore, it reduces the read tail latency by about 5× over the baseline for the two data-center work-loads that we evaluated with.
NAND flash-based SSDs offer superior throughput and average latency compared to those of hard disks and thus have become the de-facto standard for storage devices.
However, SSDs have much greater performance variability than that of hard disks [11].
While SSDs can achieve very low average read latency (e.g., under 15µs [5]), their tail latency (e.g., 99.999th percentile) can be very long (e.g., over 10ms).
One can mistakenly think that long tail read latency is a rare event that affects very few requests.
However, such tail latency can play a considerable role in data-center computing because a single query (e.g., web search) may require thousands of disk reads across the data-center [2,7].
In such a case, a single long-latency disk access can lead to an increase in the overall query response time.
Also, the chance of a query experiencing Latency (í µí¼s)Figure 1: Read latency distribution of a PCIe 3×4 NVMe low-latency SSD [5] running 4KB random reads (70%) and writes (30%) workload with a queue depth of 16.
long disk tail latency is continuously increasing with the trend of ever-increasing data size.To avoid such performance degradation in a data-center induced by SSD tail latency, minimizing the tail latency of a read operation is very important.
Naturally, this requires tackling two significant sources of long read tails: garbage collection (GC) and erase operations.
Recent prior works have already explored ways to minimize the effect of GC on read tail latency [6,14,37].
Furthermore, production-level low-latency SSDs -such as Samsung Z-SSD [38], which we base our work on -have already minimized the effect of GC on read operation [38] by allowing the user-initiated read to be processed between operations in a GC (i.e., read and program to copy valid pages from one block to another) [4,20].
In contrast, much less attention has been paid to the effect of erase operation on read tail latency, primarily because erase latency is relatively small (e.g., 5ms) compared to the effect of GC on tail latency (e.g., can exceed 100ms without any optimization).
However, with techniques minimizing this effect, erase latency is now becoming the most dominant component of read tail latency.To control the effect of erase on tail latency, Wu et al. [35] proposed an erase suspension technique, which suspends an ongoing erase (and verify) pulse when a read request is issued to the same flash die.
After processing the read request, the erase pulse resumes from the exact point at which it was suspended.
However, the commodity NAND flash business is known to be extremely cost-sensitive, and this technique may increase the cost of NAND peripherals to generate an erase pulse of an arbitrary length and track the exact state of every erase.
Furthermore, it can cause a serious NAND reliability problem and write starvation.To address these limitations, we present practical erase suspension schemes for modern low-latency SSDs.
Instead of suspending/resuming an erase pulse at an arbitrary point, our work focuses on suspending/resuming the erase operation at well-aligned safe points by either i) aborting an ongoing erase operation immediately and resuming from the last safe point or ii) deferring the suspension of erase operation until the next safe point.
Exploiting their trade-offs, we also introduce a timeout-based switching mechanism between the two mechanisms, to adapt to workload changes dynamically.
This scheme enables modern low-latency SSDs to offer extremely low read tail latency on a wide range of workloads without causing any NAND reliability problem or write starvation.Our contributions are summarized as follows:• We are the first to identify the problems of NAND reliability and write starvation in the existing erase suspension scheme and demonstrate the latter on a real, production-grade SSD.
• We propose two practical erase suspension mechanisms, immediate erase suspension and deferred erase suspension.
We also analyze the trade-offs between the two mechanisms and introduce a timeout-based switching policy between the two, to take the best of both as the workload changes.
• We demonstrate a significant reduction in read tail latency with the proposed erase suspension mechanisms on various workloads including Aerospike Certification Tool (ACT) [1] and TPC-C benchmark workloads [31].
To perform a NAND block erase, the incremental step pulse erasing scheme is a standard feature in modern SSDs [18].
Instead of utilizing a single, very high voltage pulse (e.g., 14V) for an erase, which has negative impact on NAND lifetime [12,25,29], this scheme performs an erase operation with several, discrete pulses (typically 5 or fewer), and each pulse has a higher nominal voltage than the previous one.
Figure 2 illustrates this scheme.
By verifying the set of erased cells between erase pulses and by applying higher voltage pulses to cells that are not erased yet, this scheme minimizes damage on NAND cells [12].
A single erase pulse consists of the following 3 stages: 1 voltage ramping stage in which the erase pulse reaches the desired voltage, 2 erase execution stage during which the voltage is stabilized and maintained, and 3 voltage recovery stage in which the erase voltage is reset for the erase-verify operation.The erase suspension mechanism has been proposed to provide tight read tail latency by suspending an ongoing erase pulse at the arrival of a read request to be resumed later [35].
While effective in reducing read tail latency, this mechanism poses several implementation challenges in terms of NAND reliability and cost.
First, an erase suspension adds to the cost of an erase an extra pair of voltage recovery ( 3 ) and ramping ( 1 ) stages for suspending and resuming the erase pulse.
These additional stresses caused by ramping up and down the voltage degrade the endurance of NAND [12,25,29].
For example, a recent case study using sub-20nm TLC NAND shows that the raw bit error rate (RBER) is increased by 14.4% with only 1000 erase suspensions [26].
The increased RBER leads to an increase in uncorrectable bit error rate (UBER) even with error correction, to eventually reduce the lifetime of SSD [3].
Another limitation of the existing erase suspension scheme is that it requires the capability to suspend and resume an erase pulse at an arbitrary point.
This increases the cost for NAND peripherals to generate a pulse of an arbitrary length, track the exact state of each suspended erase, and recover the peripheral state to resume.
As NAND cells continue to scale with the introduction of 3D NAND, NAND peripherals are becoming a scalability bottleneck to a greater extent [19,24,27].
Considering the extreme cost sensitivity of the commodity NAND business, this is a significant drawback.Instead, our erase suspension scheme allows erase suspension only at the beginning or the end of each erase step.
When a read request arrives while an erase pulse is still asserted, our scheme asserts an erase suspension command that either i) aborts the ongoing erase pulse and forfeits the current erase step progress to serve the read request immediately (named immediate erase suspension) or ii) finishes the ongoing erase step and serves the read request (named deferred erase suspension).
The next subsections discuss each option in detail and also present an adaptive switching scheme between our two proposed mechanisms.
One way to serve an incoming read request during the erase pulse is to immediately terminate the ongoing erase step and to forfeit the progress (Figure 2(a)).
Then, after serving the read request(s), the erase operation can resume from the beginning point of the current erase step.
We call this scheme Immediate Erase Suspension (I-ES) since it immediately cancels the ongoing erase step.In effect, I-ES is a practical variant of the original erase suspension [35] with the following two changes.
To improve NAND reliability, a verify pulse is applied before resuming a suspended erase pulse, exploiting the incremental step Figure 3: Write starvation experiment results.
This workload consists of two threads where one thread continuously generates 128KB read requests, while another thread continuously generates 128KB write requests (QD 1 for both).
Baseline and I-ES use real low-latency SSD [5], and Erase Suspension [35] uses an SSD simulator [33].
pulse erasing scheme in Section 2.1.
As each NAND cell in a NAND block has different erase timing, some NAND cells get erased more quickly than others [12,25,26,29].
Therefore, applying the same pulse to all NAND cells when resuming the erase pulse results in over-erasure of some cells, which puts unnecessary stress on already erased cells and harms the reliability of NAND.
A verify pulse, before resuming the erase pulse, detects already erased cells to not inflict unnecessary stress on these cells in a NAND block.
Furthermore, to keep the cost of NAND peripherals low, a whole step pulse is asserted at every resumption, instead of the remaining step pulse time.
To generate an erase pulse of an arbitrary length, a variable-length pulse generator with fine-grained control must be placed on a NAND device.
However, today's commodity NAND does not provide such a mechanism.
In contrast, I-ES does not require such changes keeping the cost of NAND low.
Advantages and Disadvantages.
Since the erase step is canceled immediately, the read command can always be guaranteed the highest priority.
The read command does not experience any other delay caused by an erase operation except for a fixed latency (e.g., ∼100µs) that tries to cancel the ongoing erase pulse ( 3 in Figure 2).
However, this scheme is problematic.
With the continuous incoming read requests, an erase step may be repeatedly canceled, preventing write requests (which are dependent on the erase requests) from completion.
To confirm this behavior, we modified the firmware of the real, production-grade lowlatency SSD [5] to implement this scheme.
Figure 3 shows the outcome of this experiment.
As expected, a few write operations cannot finish for a long time because the preceding erase operation is continuously canceled by incoming read requests.
As a result, its tail latency keeps increasing until the end of the workload.The erase (and write) starvation problem occurs because the latency of erase suspension/resuming operation is longer than the incoming rate of read requests on the NAND chip.
For example, if the throughput of PCIe Gen 3×4 NVMe interface is 3.2GB/s (i.e., the incoming read rate is 1.19µs/4KB) and the erase suspension/resuming overhead is 100µs, it is possible for a read request to arrive while the erase suspension/resumption is happening.
In such cases, the erase will immediately suspend again without making any forward progress, and thus the erase (and write) operation will starve.
Note that the same problem manifests with the original erase suspension scheme [35], which differs from I-ES only for the duration of the asserted pulse at resumption, as shown in Figure 3.
NVMe and PCIe specifications require the host to reset both hardware and software at write starvation (i.e., user's write requests timeout) [23,28], which adversely affects system performance [22] and may eventually lead to a fatal system failure if repeated.
Another way to serve an incoming read request during the erase pulse is to let the read request wait until the current erase step finishes (Figure 2(b)).
After that, in lieu of proceeding to the next erase step, the read request(s) is (are) served.
Then, the erase operation resumes and continues to the next erase step.
We call this Deferred Erase Suspension (D-ES) as it defers erase suspension until the end of the current erase step.
Advantages and Disadvantages.
This mechanism lets the erase operation finish without incurring the write starvation problem.
By guaranteeing a single erase step to be performed once the erase step is initiated, this mechanism guarantees forward progress for the erase operation.Although this mechanism does not incur erase (and write) starvation, it can harm read tail latency by making read requests wait until the end of the current erase step (i.e., 1ms in our low-latency NAND).
This scheme can also show worse read latency for bursty reads as D-ES adds extra latency to reads.
However, such situations could be avoided by batching backlogged erases after serving the bursty reads first.
This issue is addressed by T-ES, which switches adaptively between I-ES and D-ES.
Timeout-based Erase Suspension Policy.
If it is possible to know the request pattern of an application a priori, one should use I-ES when the application is expected to have a phase with sparse read requests as in this case erase (and write) starvation does not occur as an erase is likely to make progress during the upcoming sparse read period.
On the other hand, if an application is expected to have a steady stream of incoming read requests, the user should use D-ES as in this case, employing I-ES leads to an exponential increase in write tail latency (i.e., write starvation).
Unfortunately, in reality, it is not easy to predict future I/O access patterns without profiling runs.
Thus, we propose a Timeout-based Erase Suspension (T-ES) scheme, which performs I-ES until the erase operation is delayed for Nms.
If the erase operation is delayed for Nms (i.e., timeout happens), this scheme switches to D-ES mode to avoid potential erase (and write) starvation.
Hopefully, this scheme finds a period to perform an erase operation without being interrupted by a read operation.Choice of Erase Timeout Delay.
T-ES covers a spectrum of policies between I-ES and D-ES, controlled by the value of N.
If we set it to 0, this is equivalent to the base D-ES scheme.
In contrast, if we set it to infinite, this is equivalent to I-ES scheme prone to erase (and write) starvation problem.In general, choosing a higher value offers more chance to provide better read tail latency by delaying an erase operation, but this choice leads to an increase in maximum write tail latency.
On the other hand, choosing a smaller N makes it behave more like a D-ES, which provides smaller maximum write tail latency at the expense of an increased number of reads experiencing erase latency.
T-ES provides a knob for the user to choose N based on her willingness to trade-off the maximum write tail latency for potential improvement in read tail latency.
For example, if the user wants to achieve sub-100ms write tail latency and the maximum write delay occurring from a GC scheme (e.g., GC policy [6, 13-15, 17, 37], over-provisioning ratio [30]) is 35ms, the user should set N to be 64ms so that the total maximum write latency remains under 100ms.
By default, we set N to 64ms for our experiments.
Evaluation Framework.
Although we utilized a real, prototype low-latency SSD [5] with modified firmware to perform some experiments (Figure 1 and Figure 3), it is not possible to utilize the real device to evaluate our presented schemes such as D-ES and T-ES since implementations of such policy require an extension to the interface between the SSD controller and NAND flash chips (e.g., new commands Figure 4: Read/write tail latency on 4KB random reads (70%) and writes (30%) workload.reason, we use MQSim [33] for our experiments, which we extend so that it can accurately model low-latency NAND flash chips.
In particular, we i) allow data cache manager and FTL to use strict 8 plane program so that it can achieve higher write throughput, ii) enable I/O scheduler to process the user read request as the highest priority, and iii) modify NAND flash controller and memory logic to model our practical erase suspension mechanisms.
Table 1 summarizes the parameters used for our MQSim, while [1], and TPC-C [34]).
Also, we use for all experiments the steady state pre-condition [32] in which the space of an SSD including the over-provisioning (OP) [30] area is full; this pre-condition helps evaluate the latency behaviors that can happen under the worst-case condition of read and write requests.
Section 2 discusses three practical erase suspension mechanisms: I-ES, D-ES, and T-ES.
We add the following three configurations to our evaluation for comparison:• Baseline: Erase operations do not get preempted by an incoming read request.
• Erase Suspension (ES): The scheme can suspend and resume an erase pulse from an arbitrary point as proposed by Wu et al. [35].
• Ideal Erase Suspension (Ideal-ES): This scheme can suspend and resume an erase operation from any arbitrary point with zero erase suspension penalty.
Workload.
We first evaluate our erase suspension policies using a microbenchmark that generates a mixture of 4KB random reads (70%) and writes (30%) at queue depth 16.
We utilize Flexible I/O Tester (FIO) [8] to generate such disk access patterns.
This workload is widely used to evaluate an SSD's latency performance [10,38].
Figure 4 shows the results of this experiment.
Read Tail Latency.
In the baseline, when an erase happens, the subsequent read requests are simply delayed for the duration of the remaining erase time (up to 5ms).
As a result, baseline policy shows around 5ms read tail latency.
With ES and I-ES such an erase operation is repeatedly aborted by the incoming read requests.
As a result, incoming read requests do not experience an extra delay caused by the ongoing erase operation except for the 100µs latency required for erase suspension/resumption.
For this reason, both ES and I-ES policies have very low read tail latency.
Note that they are prone to experience erase (and write) starvation as pointed out in Section 2.2.
However, it does not happen here because this workload has a period when read requests are not sent out for some time (i.e., all 16 outstanding requests in the queue are write requests waiting for an erase to proceed).
During this period, erase operation successfully finishes.
The read tail latency of D-ES is around 1ms because it always lets the ongoing erase step (not the whole erase) finish, making a read request wait.
T-ES behaves similarly to I-ES in this workload as the timeout is rarely triggered.
Write Tail Latency.
Baseline, ES, D-ES, and Ideal-ES have a lower write tail latency because their erase operation is finished in a relatively short period of time without canceling an existing erase pulse.
Notably, the write tail latency of the baseline policy is the maximum GC latency (35ms) of our model without erase suspension.
On the other hand, both I-ES and T-ES abort the erase multiple times, and thus delays write operations significantly.
As a result, they tend to have a longer write tail latency.
Workload.
ACT models the Aerospike database servers' realtime I/O access patterns.
In essence, ACT consists of three threads: one issuing 2K small (1.5KB) read requests per second, another issuing 24 large (128KB) read requests per second, and the third one issuing 24 large (128KB) write requests per second.
ACT gradually increases this rate in integer multiples (e.g., ACT 4× workload means 8K small read requests/s, 96 large read requests/s, 96 large write requests/s) and considers that the device has passed the performance test if it meets the following conditions for a long time (e.g., 24hrs): i) 95% of transactions finish in 1ms, ii) 99% of transactions finish in 8ms, iii) 99.9% of transactions finish in 64ms, and iv) average transaction time for each type of requests is smaller than ACT workload's I/O request period.
If a device satisfies only the fourth condition but fails to satisfy one of the first three conditions, that device is said to pass the stress test but not the performance test.
The maximum ACT multiplier that an SSD can satisfy is that particular SSD's performance rating.ACT Results.
Table 3 shows the maximum multiplier in which each configuration passed the performance and stress tests.
Figure 5 shows the tail latency of 95%, 99%, and 99.9% accesses at 30× workload multiplier.
As shown in Table 3, the baseline has a good average response time and thus can successfully run ACT with 32× multiplier.
On the other hand, it has the worst read tail latency since the part of erase latency (up to 5ms) is exposed to read requests.
This leads to a relatively poor performance test result for the baseline.
ES and I-ES have good read tail latency behavior.
However, continuous read requests cause erase (and write) starvation, which leads to a failure in the stress test (fourth condition) at the workload multiplier whose value is above 22×.
On the other hand, both D-ES and T-ES demonstrate strong results for both stress and performance tests.
Both D-ES and T-ES can maintain low read tail latency while avoiding erase (and write) starvation.
Workload.
TPC-C [34] is a popular benchmark for online transaction processing frameworks.
We utilize a published disk trace of the system running TPC-C from SNIA [31], to evaluate our erase suspension mechanisms.
TPC-C Results.
Figure 6 shows the results from this experiment.
The baseline policy generally observes a noticeably higher read tail latency than the other schemes since latency is exposed to many read requests queued behind the erase operation.
Neither ES or I-ES runs to completion as they suffer from a severe erase (and write) starvation and cause the simulator to exit prematurely after failing to serve many write requests for a long time.
As in the ACT workload, this is because continuous read requests prevent the completion of an erase operation.
Both T-ES and D-ES achieve much lower tail latency than the baseline.
In particular, both achieve around 1ms max tail latency, which indicates that a read request only experiences about a single erase step delay at most.
The write tail latency of the baseline, D-ES, and Ideal-ES is similar to each other.
In contrast, ES and I-ES suffer write starvation, and T-ES records the longest write tail latency.
The T-ES scheme delays erase (and following writes) for up to the timeout value hoping to find a period in which it can perform an erase without blocking reads.
In this case, T-ES does not find such a period and hence ends up triggering the D-ES mechanism after delaying the write requests; as expected, the maximum write latency converges within 90ms (i.e., the sum of GC latency (24ms) and the T-ES timeout value (64ms)).
If an erase operation is delayed for Nms, T-ES switches from I-ES to D-ES to avoid erase (and hence write) starvation.
We perform a sensitivity study, using TPC-C with varying N to provide an insight into the tradeoff with selection of this parameter.
Figure 7(a) shows the read latency distribution with different values of N from 4ms to 4096ms.
At around the 80th percentile we start to observe a gradual transition of read latency from more of I-ES (about 200µs) to D-ES (about 1ms).
Increasing N generally i) lowers frequency of both high-latency read (i.e., over-200µs) and ii) average read latency, but iii) increases the maximum write latency.
As N increases, T-ES has a greater chance of running in I-ES mode to lower the chance for a read request to experience a 1ms delay for finishing an ongoing erase pulse.
Fewer long-latency reads lead to a lower average read latency as shown in Figure 7(b).
However, increasing N negatively affects the maximum write latency.
Table 4 summarizes the maximum write latency with varying N.
As discussed in Section 2.4, the maximum write latency of T-ES is the sum of GC latency and the T-ES timeout value.
This is because GC operations, which need an erase operation to produce a free block for user data write, may be interfered while running in I-ES mode.
Once T-ES timeout is triggered, it switches to D-ES to allow GC operations to produce the free blocks to write user's data.
The maximum GC latency of the TPC-C workload (with steady state pre-condition) is 24ms.
Thus, the measured maximum write latency is not far off from the estimated value (i.e., GC latency plus N).
Garbage Collection Optimization.
There are several prior works on alleviating the effect of GC on tail latency [6, 13- 15, 17, 37].
While these optimizations can effectively reduce the effect of coarse-grained GCs on read tail latency, they do not address the effect of long erase operation on read tail latency, which becomes more important with optimized GC.
Thus, these techniques are orthogonal or complementary to our presented erase suspension schemes.
I/O Scheduling Optimization.
Another way to optimize tail latency is to schedule a read/write request in an intelligent way [16,20,21,36,39,40].
These proposals are effective when there are multiple devices available.
On the other hand, our work focuses on tail latency reduction without requiring multiple devices.
Still, if multiple devices are available, our technique can be applied jointly with these optimizations.
This paper introduces practical erase suspension mechanisms to limit the impact of erase operation on long read tail latency.
Leveraging the iterative erase mechanism commonly employed by today's flash devices, the proposed mechanisms minimize the impact of erase operation on read tail latency, while requiring only minor extensions to the flash interface.
With the proposed erase suspension mechanisms, our proposal enables flash-based SSDs to achieve very low read tail latency, while avoiding erase (and write) starvation and endurance degradation of NAND.
For example, our results demonstrate the feasibility of a sub-200µs 99.999th percentile read tail latency for 4KB random access workloads, which is competitive with an emerging non-flash NVM-based SSD [10].
This will harness the full potential of flash-based SSDs as the primary storage platform for future data-centers that will be required to run a variety of latency-sensitive online services.
We thank Sam H. Noh for shepherding this paper and the anonymous reviewers for their feedback.
