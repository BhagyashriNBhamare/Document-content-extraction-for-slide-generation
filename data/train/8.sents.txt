We present a unified model of what was traditionally viewed as two separate tasks: data association and intensity tracking of multiple topics over time.
In the data association part, the task is to assign a topic (a class) to each data point, and the intensity tracking part models the bursts and changes in intensities of topics over time.
Our approach to this problem combines an extension of Factorial Hidden Markov models for topic intensity tracking with exponential order statistics for implicit data association.
Experiments on text and email datasets show that the interplay of classification and topic intensity tracking improves the accuracy of both classification and intensity tracking.
Even a little noise in topic assignments can mislead the traditional algorithms.
However, our approach detects correct topic intensities even with 30% topic noise.
When following a news event, the content and the temporal information are both important factors in understanding the evolution and the dynamics of the news topic over time.
When recognizing human activity, the observed person often performs a variety of tasks in parallel, each with a different intensity, and this intensity changes over time.
Both examples have in common a notion of classification: e.g., classifying documents into topics, and actions into activities.
Another common point is the temporal aspect: the intensity of each topic or activity changes over time.In a stream of incoming email for example, we want to associate each email with a topic, and then model bursts and changes in the frequency of emails of each topic.
A simple approach to this problem would be to first consider associating each email with a topic using some supervised, semi-supervised or unsupervised (clustering) method; thus segmenting the joint stream into a stream for each topic.
Then, using only data from each individual topic, we could identify bursts and changes in topic activity over time.
In this traditional view (Kleinberg, 2003), the data association (topic segmentation) problem and the burst detection (intensity estimation) problem are viewed as two distinct tasks.
However, this separation seems unnatural and introduces additional bias to the model.
We combine the tasks of data association and intensity tracking into a single model, where we allow the temporal information to influence classification.
The intuition is that by using temporal information the classification would improve, and by improved classification the topic intensity and topic content evolution tracking also benefit.Our approach combines an extension of Factorial Hidden Markov models (Ghahramani & Jordan, 1995) for topic intensity tracking with exponential order statistics for implicit data association.
Additionally, we demonstrate the use of a switching Kalman Filter to track content evolution of the topic over time.
Our approach is general in the sense that it can be combined with a variety of learning techniques; we demonstrate this flexibility by applying it in supervised and unsupervised settings.
Experimental results show that the interplay of classification and topic intensity tracking improves accuracy of both classification and intensity tracking.
More specifically, our contributions are:• A suite of models, EDA-IT, IDA-IT and IDA-ITT, for simultaneous reasoning about topic labels and topic intensities, and extensions to topic drift tracking.
• A modeling trick which uses exponential order statistics to achieve implicit data association.
This idea allows us to make an intractable data association problem tractable for exact inference, and is of independent interest.
• The extensive empirical evaluation in the supervised and unsupervised setting on synthetic as well as two real world datasets.In the following sections we will use email topic detection and tracking as our running example.
We also use the terms topic and class as synonyms.
Also note, that our approach is not limited to the text domain.
All our methods are general in a sense that they can be applied to any problem with simultaneous classification and class intensity tracking (e.g., activity recognition).
Traditionally, classification refers to the task of assigning a class label c to an unlabeled example x, given a set of training examples x i and corresponding classes c i .
Classification can be performed by calculating the probability distribution over the class assignments, P (c|x), using Bayes' rule, P (c|x) ∝ P (c)P (x|c), where the class prior P (c) and conditional probability of the data P (x|c) are estimated from the training set.Work in the areas of clustering, topic detection and tracking, e.g., (Allan et al., 1998;Yang et al., 2000), and text mining, e.g., (Swan & Allan, 2000;Blei et al., 2003), has explored techniques for identifying topics in document streams using a combination of content analysis and time-series modeling.
Most of these techniques are guided by the intuition that the appearance of a topic in a document stream is signaled by a burst, a sharp increase of intensity of document arrivals.
For example, in the problem of classifying emails into topics, the focus of attention might change from one topic to another and hence taking into account the topic intensity should help us in the classification task.To define the notion of intensity, consider a task where we are given a sequence of n email messages, x 1 , . . . , x n , and are asked to assign a topic c to each email.
We also observe the message arrival times t 1 , . . . , t n .
The intensity λ c of a topic c is defined as the rate at which documents of that topic appear, or equivalently as the inverse expected interarrival time E[∆ c ] −1 of the topic c, where ∆ c,i = t c,i − t c,i−1 is the time difference between two subsequent emails from the same topic c.
A natural model of interarrival times is the exponential distribution (Kleinberg, 2003), i.e., ∆ ∼ Exp(λ), with density p(∆ | λ) = λ exp(−λ∆).
Let us first consider the case of a single topic.
A na¨ıvena¨ıve solution to estimating intensity dynamics would be to compute average intensities over fixed time windows.Since the exponential distribution has very high variance, this procedure is likely not to be very robust.
Furthermore, it is not easy to select the appropriate length for the time window, since, depending on the topic intensity, the same time window will contain very different numbers of messages.
Also, from the perspective of identifying bursts in the data, a set of discrete levels of intensity is preferable ( Aizen et al., 2004).
To overcome these problems, Kleinberg (2003) proposed a weighted automaton model (WAM), an infinite-state automaton, where each state corresponds to a particular discrete level of intensity.
For each email, a transition is made in the automaton, whereby changes in intensities are penalized.
This can be interpreted as a Hidden Markov Model, where the search for the most likely parameters of the exponentially distributed topic deltas ∆ c,i reduces to the Viterbi algorithm.Since the WAM model operates on a single topic only, hard assignments of messages to topics have to be made in advance.
Although classification can be done using methods as described in (Blei & Lafferty, 2005;Segal & Kephart, 1999), these hard assignments imply that topic detection and identification of bursts are separated.
However, our intuition is that temporal information should help us assign the right topic and that the topic of an email will influence topic intensity.
For example, if we are working on a topic with a very high intensity and the next email arrives at the right moment, then this will influence our belief about the email's topic.
On the other hand, if an email arrives late and we are very sure about its topic, we will have to revise our belief about the intensity of the topic.In the following sections, we propose a suite of models which simultaneously reason about topic labels and topic intensities.
In Section 6 we show how a little class topic assignment noise can confuse WAM, while our model still identifies the true topic intensity level.
Given a stream of data points (we can think of them as emails) on K topics (classes) together with their arrival times, (x 1 , t 1 ), (x 2 , t 2 ), (x 3 , t 3 ), . . . , we want to simultaneously classify the emails into topics and detect bursts in the intensity of each of the topics.We have a data association problem: We observe the message deltas ∆ i = t i −t i−1 , the time between arrivals of consecutive emails.
One first needs to associate each email with a correct topic to find the topic deltas, the time between messages of the same topic.
Given the topic deltas one can then determine the topic intensity.For example, Figure 1( Figure 1.
Topic deltas and observed message deltas for email (a) and news data (b).
Note how observed deltas are dominated by bursts in a single topic.
Observed deltas in (b) look almost uniform, despite strong bursts in topic intensities.
Explicit (but intractable) data association model (c) capturing the intensity-driven generative process.
We observe t-th message from the distribution over N words and ∆t, the elapsed time from the last received email.
We have K topics, each with intensity L (k) t at time t. Ct is the topic indicator, τt stores the time of last email of each topic.L (k) t-1 L (k) t L (k) t+1 τ t-1 τ t τ t+1 K C t W t,n N ∆ t (c) EDA-ITtime between consecutive emails).
Not knowing the true topics, we only observe the black dotted curve in the middle and we need to associate each email with the correct topic (curves above and below the middle one).
Notice how bursts in activity of one topic dominate the observed deltas (middle dotted curve).
A na¨ıvena¨ıve approach to solving the data association problem described above is to explicitly keep track of when we last saw a message from a given topic.
Figure 1(c) presents a Dynamic Bayesian Network (DBN) for modeling such a process.
Each topic k is associated with an intensity process L (k) t , which is a Markov chain modeling the change of topic intensity over time.
The discrete states L (k) t = l are associated with a parameter λ(l) of an exponential distribution, modeling the message interarrival times for topic k.
We model the topic transition probabilities as P (L(k) t+1 = l | L (k) t = l) = 1 − θ, and P (|L (k) t+1 − l| = 1 | L (k) t = l) = θ/2,properly accounting for boundary cases.
So we allow intensity to increase or decrease with probability θ, which is a parameter of the model.
We can explicitly model τ (k) t , the time at which we last saw an email from topic k, as a vector τ t .
At each time index t, the topic c t of the t-th message isc t = argmax k τ (k)t , i.e., the last message on topic c t happened at the current time.
The transition from τ t to the next time step τ t+1 is a follows: for each topic k, a new arrival time τ (k) t+1 is generated by incrementing τ (k) t by the topic delta, which is exponentially distributed with parameter λ(L (k) t ).
Now, the smallest of τ t+1 determines the email arrival time and the index determines the topic,c t+1 = argmin k τ (k)t+1 .
For the topic c t+1 of this new email, we update the last topic access time τ(ct+1) t+1 = τ (ct+1) t+1 .
The remaining τ (k)t+1 for k = c t+1 are unchanged, and remain identical to τ (k) t .
In our problem the model observes message delta,∆ t = max k τ (k) t −max k τ (k)t−1 , which is the time between the current message and the previous one.
We also observe a representation w t of the message, e.g., a bag of words representation.
Unfortunately, the inference in this model is intractable -the state space grows as T K with the number T of documents.
Conceptually, the explicit data association model EDA-IT, as sketched in Figure 1(c), represents the generative process, underlying the intensity driven generation of document streams.
Instead of investigating heuristics for coping with the intractability of the presented model, we now introduce a simpler model, which elegantly avoids the intractability of explicit data association.
From Section 3 we have that the topic c t+1 of next message is the one with minimumτ (k) t+1 = τ (k) t + ∆ k .
Here ∆ k ∼ Exp[λ(L k )]is the topic delta, i.e., the time between consecutive emails from topic k.
So the probability that the next email is from topic k isP (c t+1 = k) = P (τ (k) t +∆ k ≤ τ (j) t +∆ j | τ (j) t +∆ j ≥ r),where j ranges over all topics, and r = max k τ (k) t is the arrival time of email at time index t. So, the probability that the topic of next arriving email is k, is the chance that τ (k) t+1 is the earliest of all "scheduled" arrivals, conditioned on how much time has passed.The key to making the EDA-IT model tractable is to exploit the memorylessness of the exponential distribution to avoid keeping track of the times τ t when we have last seen a message on each topic.
The memorylessness property states that, if X ∼ Exp(λ), then P (X > T + t | X > T ) = P (X > t).
Assuming the intensities of each topic are fixed, it follows that(C t |L t = l) ∼ argmin{Exp[λ(l 1 )], .
.
, Exp[λ(l k )])}, (1) (∆ t |L t = l) ∼ min{Exp[λ(l 1 )], .
.
, Exp[λ(l k )])}.
(2)Both conditional probability distributions (CPDs) rely on exponential order statistics: The observed message delta is the minimum of several exponential distributions (Eq.
2), whereas the selected topic is the corresponding index of the smallest variable (Eq.
1).
At first glance, since these CPDs represent complex order statistics, it is not obvious whether they can be represented compactly and evaluated efficiently.
The following result (Trivedi, 2002) gives simple closed form expressions for the CPDs 1 and 2:Proposition 1 Let λ 1 , . . . , λ n > 0 and Z 1 ∼ Exp(λ 1 ), . . . , Z n ∼ Exp(λ n ).
Then min{Z 1 , . . . , Z n } ∼ Exp( j λ j ) and P (Z i = min{Z 1 , . . . , Z n }) = λi P j λj .
Using these CPDs, we arrive at the model presented in Figure 2(a).
We retain the intensity processes L(k) t , but instead of keeping track of τ (k)t , the time of last email of each topic, and deriving the topic label c t from it, we use the intensities L t directly to model the topic prior.
In this model, the association of message deltas (time between consecutive emails) to topic deltas (time between consecutive emails of the same topic) is implicitly represented.
We refer to this model as IDA-IT, Implicit Data Association for Intensity Tracking.The order statistics simplification is an approximation, since in general the topic intensities are not constant during the interval between emails.
Our model makes the simplifying assumption that the topic is conditionally independent of the message delta given the topic intensities.
However, our experimental results indicate that this approximation is very powerful and performs very well in practice.
Moreover, the IDA-IT model now lends itself to exact inference (for a small number of topics).
IDA-IT is a simple extension of the Factorial Hidden Markov Model (Ghahramani & Jordan, 1995), for which a large variety of efficient approximate inference methods are readily available.
Note that the IDA-IT model is a special case of continuous time models such as continuous time Bayesian Networks (CTBNs) ( Nodelman et al., 2003).
Unlike our model, CTBNs are in general intractable, and one has to resort to approximate inference (c.f., Ng et al., 2005).
In a truly dynamic setting, such as a stream of documents, we do not only expect the topic intensities to change over time, but the vocabulary of the topic itself is also likely to change, an effect known as topic drift.
Next, we present an extension of IDA-IT model that Figure 2.
Proposed graphical models.
(a) Implicit (and tractable) data association and intensity tracking; (b) Implicit data association with intensity and topic tracking.L (k) t-1 L (k) t L (k) t+1 K C t W t,n N ∆ t ∆ t (a) IDA-IT L (k) t-1 L (k) t L (k) t+1 K C t W t,n N ∆ t ∆ t µ t µ t µ t-1 µ t-1 (b) IDA-ITTalso allows for tracking the evolution of the content of the topics.Here we use the Switching Kalman Filter to track the time evolution of the words associated with each topic.
We represent each topic with its centroid -a center of the documents in the topic.
As the topic content changes, the Kalman filter tracks the centroid of the topic over time.
Since representing documents in the bag-of-words fashion results in extremely high dimensional spaces, where modeling topic drift becomes difficult, we adopt the commonly used Latent Semantic Indexing ( Deerwester et al., 1990) to represent documents as vectors in a low dimensional space.Using the Gaussian Na¨ıveNa¨ıve Bayes model, the observation model for documents becomes P (W t,i | C t = k) ∼ N (µ i,k , σ 2 i,k ), where we represent each topic by its mean µ (k) and variance σ (k) .
For simplicity of presentation, we will assume that only the topic centers change over time, while variances remain constant.
Assuming a normal prior on the mean, and a normal drift, i.e., µ(k) t+1 = µ (k) t + ν for ν ∼ N (0, ε 2 ), we can model the topic drift µ (k) 1 , . . . , µ (k)T by plugging a Switching Kalman Filter (SKF) into our IDA-IT model.
We call this model Implicit Data Association for Intensity and Topic Tracking (IDA-ITT), presented in Figure 2(b).
The SKF model fits in the following way: The continuous state vector µ t = (µ(1) t , . . . , µ (K) t) describes the prior for the topic means.
The linear transition model is simply the identity, i.e., µ t+1 = µ t + ν.
This means that we expect the prior to stay constant, but allow a small Gaussian drift ν.
The observation model is a Gaussian distribution dependent on the topic:W t | [µ t , C t = c] ∼ N (H c · µ t , Σ c ).
Hereby, H c is a matrix selecting the mean µ (c) t from the state vector µ t .
For example, in the case of two classes, and the documents represented as points in R 2 , H 1 = (1, 1, 0, 0) and H 2 = (0, 0, 1, 1).
We can estimate Σ c from training data and keep it constant, or associate it with a Wishart prior.
In this paper, we select the first option for clarity of presentation.Unfortunately, we cannot expect to do exact inference anymore, since inference in such hybrid models is intractable (Lerner & Parr, 2001).
However, there are very good approximations for inference in Switching Kalman Filters (Lerner, 2002).
We will briefly explain our approach to inference in Section 4.5.
We also extended of our model to the semi-supervised, expert-guided classification case, where occasional expert labels for the hidden variables are available, and investigated an active learning method for selecting most informative such labels.
Due to space constraints we do not present the model derivation and experimental results for this case.
Please refer to ( Krause et al., 2006) for further details on the model for the semi-supervised case.
Our approach is general, in at least three ways.
Firstly, as argued in Section 1, the application is not limited to document streams.
Another possible application of our models is fault diagnosis in a system of machines with different failure rates, or activity recognition, where the observed person is working on several tasks in parallel with dynamic intensities.
Secondly, our models fit well in the supervised, unsupervised and semi-supervised case as demonstrated in the paper.
Lastly, instead of using a Na¨ıveNa¨ıve Bayes classifier as done here, any other generative model for classification can be "plugged" into our model, such as TAN trees (Friedman et al., 1997) or more complex graphical models.
Instead of using Latent Semantic Indexing to represent documents, it is possible to use topic mixture proportions computed using Latent Dirichlet Allocation (LDA) ( Blei et al., 2003) or some other method.
In the LDA example, one can either apply the SKF directly to the numerical topic mixture proportions, or track the mixture proportions using the Dirichlet distribution (which makes inference more difficult).
Most generally, our model can be considered as a principled way of adapting class priors according to class frequencies changing over time.
Instead of assuming that the transition probabilities θ stay constant between any two subsequent events, a possible extension is to let them depend on the actual observed message deltas, by modeling L (k) t as continuous-time Markov processes.
We experimented with this extension, but did not observe significant difference in the behavior, since in our data sets the actual observed deltas were rather uniform (Figure 1(b)).
Similarly, the Gaussian topic drift ν in the IDA-ITT model can be made dependent on the observed message delta, allowing larger drifts when the interval between messages is longer.
For a small number of topics, exact inference in the IDA-IT model is feasible.
The variables L (k) t and C t are discrete, and the continuous variables are all observed.
Hence, the standard forward-backward and Viterbi algorithm for Hidden Markov Models can be used for inference.
Unfortunately, even though the intensity processes L (k) t are all marginally independent, they become fully connected upon observing the documents and the arrival times, and the tree-width of the model increases linearly -the complexity of exact inference increases exponentially -in the number of topics.
Exact inference has complexityO(T K 2 |L| 2K ),where L is the set of intensity levels, and K, T are the number of topics and documents, respectively.
However, there are several algorithms available for approximate inference in such Factorial Hidden Markov Models (Ghahramani & Jordan, 1995).
We implemented an approach based on particle filtering, and fully-factorized mean field variational inference.
In Section 6, we present results of our comparison of these methods with the exact inference.Our implementation of the topic tracking model IDA-ITT is based on the algorithm for inference in SKFs proposed by Lerner (2002).
At each time step, the algorithm maintains a belief state over possible locations of the topic centers, represented by a mixture of Gaussians.
To avoid the multiplicative increase of mixture components during each time update step, and the resulting exponential blow-up in representation complexity, the mixture is collapsed into a mixture with fewer components.
In our implementation, we keep the four components with the largest weight from each topic and each intensity.
Synthetic datasets.
First, we evaluate our models on two synthetic datasets.
The first dataset (S1) was designed to test whether implicit data association recovers true topic intensity levels.
For each of the two topics, we generated a sequence of 300 observations, with exponentially distributed time differences.
Every hundred samples, we changed the topic intensity, in the sequence [ ] for topic 2.
The observed feature W t is a noisy copy of the topic variable C t , taking the probability 0.9 for correct topic to introduce additional classification uncertainty.The second dataset (S2) tests the resilience towards noise in the assignments of messages to topics.
Observations in the dataset are uniformly spaced four hours apart, so the observed message deltas are completely uninformative.
Every fourth email is from topic 2, the remaining emails are from topic 1.
So, the true intensity of topic 1 is 1 5 , and for topic 2 it is 1 16 .
We again observe a noisy copy of the true topic label.
However, for 30% of the observations from topic 2, the evidence points to the wrong topic -we assign the probability of the correct topic to 0.49; thus hard-assignment of topics will misclassify 30% of messages from topic 1.
Enron email corpus.
The Enron dataset contains 517,431 emails from 151 Enron employees.
We selected all 554 email messages from tech-memos and universities folders of employee Kaminski, treating each folder as a separate topic.
The email data spans from December 1999 to May 2001.
Reuters document corpus Volume 1 contains 810,000 English language news articles, spanning a year starting from August 1996.
We selected 2,303 documents from four topics (wholesale prices, environment issues, fashion, and obituaries).
The number of documents per topic varies between 259 and 938.
For each document we also know the time of publication.Document representation and training.
In both real datasets we removed stop-words and words with document frequency of less than 5.
We also applied Latent Semantic Indexing ( Deerwester et al., 1990) retaining 8 latent dimensions, with components determined on the training data.
We decreased the dimensionality of the data to increase interpretability of the results, avoid over-confidence of Na¨ıveNa¨ıve Bayes and decrease the number of estimated parameters.
In all experiments, we used the first 25% of data for training and the rest for testing.
In the Enron data set, this amounts to the first six months of data for training and in Reuters only for a month and a half.
Since the documents are not evenly distributed over time and some topics have high (low) intensity at the start of the datasets, the learned class (topic) priors may be different from the true class priors, an issue not addressed by traditional methods.
.
So the three "correct" intensity levels are available, as well as three "wrong" levels.
We set the intensity transition probability to 0.2.
Figure 3(a) presents the results.
The x-axis presents documents ordered in time of arrival and on the y-axis we plot the inverse intensity (average topic delta), i.e., time between two consecutive emails from the same topic.
The dashed lines correspond to the ground truth (topic deltas), which are not observed by the algorithms.
Notice that the exact inference successfully recovers the true intensities, in spite of the high variance of the exponential distribution.
Also observe that the Viterbi decoding successfully avoids simply matching the observed message deltas.
This indicates that IDA-IT model succeeds in the data association task.
Also, at the end of the sequence, where no messages of topic 2 are observed, the intensity of the low frequency topic is estimated as low, which means we successfully incorporate the "negative evidence".
Next, we analyze the intensity tracking in presence of classification noise using the synthetic dataset 2, where 30% of examples are misclassified.
We compare against the Weighted Automaton Model (WAM) on hard-assigned labels.
We chose the intensity levels 1 5 (correct for topic 1), 1 6 , 1 12 (both wrong, indicate misclassification) and 1 16 (correct for topic 2).
The intensity transition probability is set to 0.1.
IDA-IT recovers the true underlying rate of both topics.
Figure 3(b) shows this for the low intensity topic 2.
Estimating the rates after hard-assigning labels drastically decreases performance.
Furthermore, all 30% examples misclassified by the Na¨ıveNa¨ıve Bayes are correctly classified during our inference.
This indicates synergetic effects between intensity estimation and topic identification.
It also shows that IDA-IT does true data association of topic deltas, even with completely uninformative message deltas.Experiments on Enron and Reuters.
We compare IDA-IT and the traditional WAM model on Enron and Reuters datasets.
Figures 1(a) and 1(b) show the observed data for Enron and Reuters.
We plot the message number versus the time of the message.
Each dot represents a message.
Vertical parts correspond to bursts of activity and horizontal to low activity (long time between consecutive messages).
The algorithm observes the dashed curve in the middle and needs to associate each message with the correct topic (curves above and below the middle one).
Notice how bursts in activity of one topic dominate the observations.
Notice how in the Reuters data set (Figure 1(b)) the observed message deltas are almost uniform, but the individual topics exhibit strong bursts of activity (sharp vertical jumps on the plot).
Figure 3 shows the results on intensity tracking.
Fig- ures 3(c) and 3(d) compare our IDA-IT with the traditional approach, where each message is first assigned a topic and then WAM is run separately on each topic.
We circled the spots where the WAM model gets confused due to misclassifications and determines the wrong intensity level.
On the contrary, IDA-IT can compensate for classification noise and more accurately recover true intensity levels.Similarly, Figures 3(e) and 3(f) show the results for 2 out of 4 topics from the Reuters data set.
Notice how topic 1 interchanges the low and high activity and how using hard classification with WAM model misses several transitions between intensities.
In a data-mining application aiming at the detection of bursts, these lapses would be highly problematic.
In the previous section, we showed how coupling classification and intensity tracking better models the intensity than if classification and tracking are done separately.
Next, we evaluate how classification accuracy is influenced by combining it with intensity tracking.
Figure 4 compares the overall classification error of the baseline, the Gaussian Na¨ıveNa¨ıve Bayes classifier, with the proposed IDA-IT model.
We ran 3 experiments: Enron emails, topics 1 and 2 from Reuters and all 4 Reuters topics.
We used same preprocessing of the data as in the other experiments (see Section 5).
For Enron we determined a set of intensity levels 1, 1 4 , 1 16 , 1 64 and the transition probability of 0.1 using crossvalidation.
The error rate of Gaussian Na¨ıveNa¨ıve Bayes (GNB) is 0.053, IDA-IT scores 0.036, which is a 32% relative decrease of error.We ran two experiments with Reuters.
For both experiments we used intensity levels 1 2 , 1 8 , 1 32 and transition probability 0.2.
In first experiment, we used only topics 1 and 2.
Classification error of GNB is 0.121 and error of IDA-IT is 0.068, which means 45% relative decrease in classification error.
The second experiment uses 4 topics, so the overall performance of both classifiers is lower, but we still get 22% relative improvement in classification.Note, that our models do not have an explicit class prior but model it through topic intensity.
This has the effect that the topic which is currently at high activity also has higher prior topic probability.
Therefore the precision of the bursty topic increases at the cost of Figure 4.
Reduction in error for Enron and Reuters reduced recall for of classes with lower intensity.
This usually leads to overall improvement of classification accuracy, but there are cases where improvement is marginal or even decreases due to the lower recall on low intensity topics.
Next, we present the application of implicit data association and intensity tracking model to the unsupervised setting, where we are using the Switching Kalman Filter as introduced in IDA-ITT (section 4.2).
For this experiment we chose two Reuters topics, wholesale prices and environment issues.
Using LSI, we reduce the dimensionality of data to two dimensions.
We then represent each document as a point in this two-dimensional space and use IDA-ITT to track the evolution of content and intensity of the topics.Exploring the most important words from the cluster centroid of topic wholesale prices, measured by magnitude of LSI coefficients, we see that words economist, price, bank, index, industry, percent are important throughout the time.
However, at the beginning and the end of the dataset, important words are also bureau, indicator, national, office, period, report.
Then for few weeks in December and early January the topic drifts towards expected, higher, impact, market, strong, which are terms used when last year's trends are analyzed and estimates for next year are announced.
We presented a general approach to simultaneous classification of a stream of datapoints and identification of bursts in class intensity.
Unlike the traditional approach, we simultaneously addresses data association (classification, clustering) and intensity tracking.We showed how to combine an extension of Factorial Hidden Markov models for topic intensity tracking with exponential order statistics for implicit data association, which allows efficient inference.
Additionally, we applied a switching Kalman Filter to track the time evolution of the words associated with each topic.Our approach is general in the sense that it can be combined with a variety of learning techniques.
We demonstrated this flexibility by applying it in a supervised and unsupervised setting.
Extensive evaluation on real and synthetic datasets showed that the interplay of classification and topic intensity tracking improves the accuracy of both classification and intensity tracking.
