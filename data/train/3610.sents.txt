Our data scientists are adept in using machine learning algorithms and building model out of it, and they are at ease with their local machine to do them.
But, when it comes to building the same model from the platform, they find it slightly challenging and need assistance from the platform team.
Based on the survey results, the major challenge was platform complexity , but it is hard to deduce actionable items or accurate details to make the system simple.
The complexity feedback was very generic, so we decided to break it down into two logical challenges: Education & Training and Simplicity-of-Platform.
We have developed a system to find these two challenges in our platform, which we call an Analyzer.
In this paper, we explain how it was built and it's impact on the evolution of our machine learning platform.
Our work aims to address these challenges and provide guidelines on how to empower machine learning platform team to know the data scientist's bottleneck in building model.
We have a good strength of data scientists in Walmart.
Our system has counted more than 393 frequent users and 998 infrequent users.
The data scientists are from various teams in Walmart.
We have seen various kinds of challenges in the evolution of our platform to support end to end Machine learning (ML) model life cycle.
We have observed that our data scientists are not able to use the platform effectively.
Less adoption of our platform is the key challenge for us to solve.
It is not sufficient to be completely dependent on surveys to improve our system as people often lie in user surveys, possibly due to a phenomenon known as social desirability bias [2].
Analyzer approach: We have instrumented our machine learning platform (MLP) to collect data for analysing how users interact with the platform.
We named the data collected or sent from a product or a feature as signals.
Example of signals are clicks, interaction with MLP features, time spent [3,4].
We have defined four stages of the ML lifecycle, namely, "Data collection,cleaning,labeling", "Feature Engineering", "Model training" and "Model deployment" [1].
In each stage signals are generated for the Analyzer to capture particular challenges faced by user.
.
We present two dimensions of challenges: Education-&-Training (ET) and Simplicity-of-Platform (SoP).
In ET, the focus is on a data scientist's capability and self-sufficiency to use the MLP and SoP focuses on the technical or design challenges in ML lifecycle.
The Fig. 1 shows our system architecture.
Our instrumentation framework collect user actions from platform and sends it to our instrumentation service.
We consider all actions as events and store them in our event repository (database).
The signal generator reads the data from the repository and classifies it into multiple signals.
We name the data collected or sent form product or feature as signals.
Example of signals are clicks, interaction with the tools, time spent on the tools, time to complete certain actions.
These signals are mapped to multiple classes: MLP-actions, MLP-error and MLP-time.
The signal generator annotates these classes of signals with the user and platform related meta-data and stores it into a datastore.
Our Analyzer runs the final show.
It goes through each new signal and evaluates respective stage wise bottleneck for a user.
Fig. 2 describes this flow.
In this section, we summarize the design of Analyzer, a system that uses a user-behaviour-segregation-model and set of rules to evaluate challenges of using MLP.
The model uses K-means clustering technique, which groups the users based on action and time signals and later by looking at the centroid/clustor representative values.
Users in the group are labelled as: Beginner, Intermediate and Expert.
Our model has suggested 85 Expert, 118 Intermediate and 190 Beginner users of our platform.
We have defined set of rules for each different type of signal.
Analyzer applies these rules and smell the challenge from signal.
We have defined following set of rules:1.
If more than 50% expert and 65% intermediate and 80%beginner users see the error signal then it is mapped to SoP challenge Analyzer has been running in production for last 4 months and these above instances are the outcome of our data analysis.
This has helped our product team to build better Education and Training (effective tutorial, documentation and videos) for model-training and data-connectors features.
Above SoP instance: 'c' highlighted the complexity which user faces in creating notebook in Feature-engineering stage.
This enabled product team to integrate live resource information on the notebook list page as part of UX improvement.
The Analyser has identified 17 and 6 instances respectively of Education-&-Training (ET) and Simplicity of Platform (SoP ) challenges.
ET instances have enabled product team to create effective set of documentation and tutorials.
SoP instances helped in the evolution of ML Platform as a whole.
Our We would like to acknowledge our colleagues who built ML Platform, Senior Director: Ajay Sharma for the opportunity, Distinguised Architect: Bagavath Subramaniam, Product Manager: Rakshith Uj and Anish Mariyamparampil for helping us in instrumentation in UI and Saiyam Pathak for the design review.
