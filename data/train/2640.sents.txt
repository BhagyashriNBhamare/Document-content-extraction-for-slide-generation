Cloud computing platforms such as Amazon Web Services , Google Compute Engine, and Rackspace Public Cloud have been the subject of numerous measurement studies considering performance, reliability, and cost efficiency.
However, little attention has been paid to billing.
Cloud providers rely upon complex, large-scale billing systems that track customer resource usage at fine granularity and generate bills reflecting measured usage.
However, it is not known how visible such usage is to customers, and how closely provider charges correspond to customers' view of their resource usage.
We initiate a study of cloud billing systems, focus-ing on Amazon EC2, Google Compute Engine, and Rackspace, and uncover a variety of issues, including: inherent difficulties in predicting charges; bugs that lead to free CPU time on EC2 and overcharging for storage in Rackspace; and long and unpredictable billing-update latency.
Our measurements motivate further study on billing systems, and so we conclude with a brief discussion of open questions for future work.
Public cloud computing systems are revolutionizing how computing power is purchased.
Rather than capital expenditures on equipment up-front, cloud customers can dynamically request resources such as storage, bandwidth, and computing power, and pay only for their usage.
Powering all this is a spectrum of technologies for resource management and delivery to tenants.
Measuring and improving the reliability and performance of cloud networks [4,15,21], storage [6,[8][9][10]20], and compute resources [14,17,22,23] has seen significant attention from the academic community.While some studies have looked at the cost implications of varying workloads on pay-as-you-go cloud platforms [11,12,19], the efficacy and performance of the billing systems that enable fine-grained, dynamic resource purchasing have, by comparison, has received a paucity of research attention.
A notable exception is the work on verifiable resource accounting [5,18] which suggests the use of trustworthy computing mechanisms to prove resource usage at a very fine granularity, e.g., CPU cycles used.
The motivation stems from the perception that cloud providers may maliciously overcharge their customers (c.f., [13]).
Resource-as-a-service clouds provide resources at fine granularities [1], but like these other works are forward-looking and do not address billing concerns in existing clouds.
As it stands, the research community has not investigated the problems faced in billing in practice: whether benign (let alone malicious) errors arise, what kind of performance billing systems offer, and whether the trajectory of billingsystem design and implementation is set to meet the needs of potential future applications.We provide the first step in enacting a research agenda towards answering these questions.
In particular, we perform the first measurement study of cloud billing systems, focusing in particular on three public infrastructure-as-a-service (IaaS) clouds: Amazon's Elastic Compute Cloud (EC2) [3], Google's Compute Engine (GCE) [7], and Rackspace's Public Cloud [16].
For each cloud, we measure its billing latency, as well as the transparency and predictability of its billing for compute time, network usage, and storage.While we a priori expected this to be straightforward, in practice it is surprisingly difficult due to the coarse granularity of bills, the lack of API support for programmatic access to bills, and the fact that providers base bills on resource-usage events that are opaque to customers.
Despite these challenges, we are able to (a) reverse-engineer ambiguous aspects of billing; (b) characterize performance, namely in measuring and documenting substantial billing latency between when a resource is consumed and charges are visible to the customer; (c) uncover bugs, such as a possible race condition in EC2 CPU billing that provides two minutes of free compute time, inconsistencies across EC2 billing reports, and a bug in Rackspace storage billing that leads to overcharges; and (d) detect systematic undercharging, largely due to opaque caching/batching mechanisms.Our measurements imply the existence of significant challenges in this space, and highlight the need for more research on the question of how to build scalable, effective billing systems.
We therefore conclude with a set of open questions to be answered in order to make progress towards this goal.
We start by assessing the performance of the three clouds' billing systems.
In later sections we look at how individual resources are billed.
We refer to the delay between the use of a resource and the bill for the corresponding charge as billing latency.
Ideally, the bill would be updated with low and predictable latency.
In reality, we find that bills are often delayed by hours to days after a workload runs, and that latency to receive a bill can vary widely.
For brevity we omit our experimental details for Rackspace and GCE.EC2 console and CSV billing latency.
We measure the latency of billing information both through the EC2 webbased console and through the billing reports it publishes as comma-separated-value (CSV) files.
In total, we ran 122 m1.small instances and 150 t1.micro instances on 19 EC2 accounts, using Ubuntu 12.04 LTS unless otherwise noted.
Each account ran a single instance at a time for 3590-3600 seconds and was then idle until all billing updates had been observed.
EC2's web-based management console updated the most quickly, at an average of 6:23 hours after instance start (std.
dev.
4:13 hours), while the downloadable CSVs were available only 7:36 hours after instance start (std.
dev.
2:59 hours).
This delay is both long, and highly unpredictable.To better understand the high variability in the timing of billing-update schedules during prolonged instance runs, we launched a second experiment in which we ran 25 instances each for 10 hours.
We performed two separate runs and started instances staggered by 30 seconds in order to avoid sending too many API calls at once.
The first bills arrived an average of 4.2 hours after launching the instances, while one or two additional updates each arrived at 6-hour intervals after the first.
These results indicate that the variability could be due to a regular 6-hour billing cycle, and that instances launched near the end of the cycle experience lower latency than those launched near the beginning.
While updates across accounts occurred at roughly the same times, one account was billed for 7 hours in the first update and 3 in the second while another was billed for 2 hours in the first update, 5 in the next, and 4 in the third (similar variability occurred across all instances).
This suggests that instance-hour usage data is sampled at different times even for instances started at approximately the same time, making it impos- sible for a customer to predict when all usage is registered in a given billing update.Based on these timing results, we investigated a set of runs shown in Figure 1 where we deliberately staggered the start time of instances by 30 minutes (shown as green dots followed by lines).
One can see a clear time when bills seem to be updated, suggesting that the billing system periodically updates bills from accounting data as a batch job.
However, there was no indication that bills are collected on a fixed daily schedule, as we observe no universally fixed time of day when updates occurred.Other EC2 billing APIs.
The CloudWatch resourceusage monitoring service has even longer update latency.
We ran 10 c1.medium instances on accounts with no other activity, and waited for the corresponding bill by polling the CloudWatch API [2].
CloudWatch took an average of 11:04 hours (std.
dev.
8 minutes) to register the usage.
This is much more predictable than the management console, but almost double the latency.The cost-allocation ("tagged") billing CSV files, which allow a customer to preemptively tag resources with a key-value pair and later differentiate their billing based on these user-defined categories, experienced even more severe update delays.
We ran two batches of tests.
In the first batch we ran 3 instances each for 120, 3610, and 7210 seconds, expecting to see 1, 2 and 3 instancehours billed, respectively.
The first billing update took 13 hours, while the second took 33.
In the second batch of tests (instances run for 120, 3620, 7220 seconds), the first update arrived after 24 hours, and the second after 56 hours.
Two 120-second instances never appeared on the cost-allocation CSV bill, but were billed for 1 hour on all other billing interfaces, indicating inconsistency across interfaces.
Interestingly, some costallocation CSV billing entries reported instance-hour usage as decimals to 8 digits (though the decimals did not correctly reflect the partial hour used), while others were rounded, as usual for instance-hour billing.Discussion.
All three providers have long delays in providing bills.
We found that for GCE, over 13 tests we observed an average update latency of 2.2 days, minimum of 0.9 days, and maximum of 9.1 days, while for Rackspace, we observed an average latency of 21.5 hours with a minimum of 8 hours and maximum of 41.1 hours.
Of the providers, only Rackspace seemed to update their bills at a consistent time of day: between 9-10am UTC Amazon provides the most timely usage data (about a 6 hour delay for the web, longer for other interfaces), but its update time is still not predictable and multiple update periods are sometimes required before all usage is reported.
Rackspace only updates bills once per day, while GCE takes even longer and also exhibited extreme outliers in latency.
As a result, it is difficult to use billing information programmatically, as there is no guarantee of when it is available or whether a given update reflects all usage up to that moment.
The first question about CPU billing is when a provider is charging.
Providers typically advertise that they begin and end charging when an instance "starts" and "stops," but these terms are ambiguous and could correspond to any of the many distinct timestamps in an instance lifetime.
Figure 2 depicts some of the major events in an instance lifetime.
EC2, Rackspace, and GCE all provide T launch timestamps, while only GCE offers additional timestamps for the times that instance CREATE and DELETE API calls were issued and completed.In our experiments, all other timestamps were collected by polling the providers' APIs (10 times per second for EC2; once per second for Rackspace and GCE, which rate-limited requests) to register when an instance's status changed.
This method is inexact, however, due to the inherent jitter introduced by variable network latency, server response time, and polling granularity.
This semantic gap between a provider's and a customer's view of instance-lifetime events means that a customer cannot be certain that their calculations (and corresponding deployment decisions) are accurate.For experiments on EC2, we determined that the in- terval T kill − T running best represented billable time after testing many other timestamp intervals and determining that this had the strongest correlation to billed hours.
We used the 272 combined m1.small and t1.micro instance runs from Section 2.
In these executions, we ensured that ∆ = T kill − T running took on a value in the range of 3590-3600 seconds by controlling T kill .
Despite this narrow window, these runs demonstrated variability in the relative timing of the various instance lifetime events on EC2, as shown in Table 3.
For example, the difference T up − T running is often negative, meaning an instance is up and running before EC2 marks it as such.
Figure 4 shows a scatter plot relating T down − T up (the measured uptime) to whether the instance was billed for two hours (plotted above the line) or one hour (below the line), for the 272 instances.
Within each half, instances corresponding to each ∆ value are on the same row, starting with 3590 and going up to 3600.
We find several trends.
First, there exists a set of outliers around uptime of 3750.
These instances ran 2.5 minutes longer than requested, and in one case such an instance was billed for only an hour, meaning Amazon underbilled for the actual uptime.
Second, for the bulk of the data points, we see that while the average uptime is close to ∆, there is a horizontal spread of points indicating a variability range of approximately 10 seconds.
Finally, we see that as ∆ increases, the percentage of instances charged two hours increases.We performed additional tests to more finely de- termine whether T kill − T running provides good prediction of billing (if not uptime), and if this holds for a broader range of instance types.
We launched 20 m1.small instances simultaneously using 20 accounts and terminated ∆ seconds after T running , for each of ∆ ∈ {3585.
.3603}.
We repeat this procedure with t1.micro and with c1.medium instances using 20 and 10 instances for each instance type per value of ∆, respectively.
Fig- ure 5 shows the fraction of instances billed for one hour versus two across these tests.
Surprisingly, the c1.medium instances had the highest variance in the number of hours billed.
This data suggests that customers should terminate their instances 13 seconds before T running + 3600i to ensure being billed for only i hours, but only if they go out of their way to get an accurate timestamp T running .
The amount of uptime received, however, will vary on the order of several minutes.A bug in EC2 compute billing.
We were intrigued by the cases in Figure 5 with uptime greater than one hour that were charged for only one hour.
We also found that if we terminated an instance early in its lifetime we were occasionally not getting billed.
To explore further, we created a VM image with a minimal kernel with network support that boots quickly, connects to our local controller and frequently sends short messages with the current timestamp.
The first and last timestamps of successful connections to our controller give uptime for the instance.We performed a series of runs where we launched this image and sent a terminate API call exactly ∆ seconds after T launch for ∆ ∈ {2.
.20}, with 20 instances per value of ∆.
For ∆ ≤ 16 we got no usable uptime, but for each of ∆ ∈ {17, 18}, 1 out of 20 instances received approximately 116 seconds of free uptime, while for ∆ = 19, 4 of 20 instances received 117 seconds of unbilled, usable uptime.
For ∆ = 20, 6 of 20 instances received an average of 118 seconds of uptime, but 3 of these instances were billed for an hour of usage each, indicating a race condition in EC2's infrastructure between the billing sys-Send Receive Setup Case % Reported % Reported (1) Univ → EC2 - 95.9% (2) EC2 → Univ 94.4% - (3) Zone X → Zone X - - (4) Zone X → Zone X (public IP)97.6% 97.2%(5) Zone X → Zone Y 97.1% 97.5% (6) Reg X → Reg Y 95.9% 96.8% Table 6: Average ratios (in percent) of billed traffic volume to measured traffic volume for the sender (second column) and receiver (third column).
A "-" indicates tests for which no billing occurred, which was correct relative to the EC2 billing model.
tem and the instance management system.
This free time is difficult to use economically, though, given the chance of receiving a bill for a whole hour of usage.
A key challenge in predicting the bill for networking is knowing how much a packet will cost.
While GCE and Rackspace have relatively simple network billing models, Amazon charges different rates for inbound and outbound traffic between instances and the Internet, to a public IP in the same zone, to another zone, and to another region.
We perform six experiments on EC2 that reflect the six possibilities.
The results are shown in Table 6: for "Univ → EC2" the sender was hosted in our local university and the recipient in EC2; for "Zone X → Zone X" the sender and receiver are in the same zone using private IP addresses (unless otherwise indicated), etc.
Cases (1) and (3) should be free according to EC2's pricing model, while the remaining four cases should be billed.
We perform three runs for each of the six configurations, and report the average percentage of network traffic billed out of that measured by NetFilter in the instance's kernel.
Billing data was measured from the detailed CSV.
Compared to measuring filtered traffic in the kernel, EC2 consistently underbills.
In the case of Internetoutbound traffic, which is the most expensive category of network traffic, (case (2) in Table 6), EC2 undercharges by an average of 5.6%.
Across all experiments, EC2 undercharges traffic by 3.4%.
We found that GCE and Rackspace network-traffic billing was more predictable perhaps partly due to their simpler network billing models, with two exceptions in Rackspace.
In 2 of 11 Rackspace instances we sent 1GB from the instance to our local controller, and Rackspace charged for 35MB and 125MB less than was successfully sent and received.
This may indicate a bug in the Rackspace network billing mechanism.
Providers charge for block storage, both by the bytes stored over time and per I/O operation (only EC2).
Similarly to billing for compute time, charges for volume allocation are sensitive to opaque volume creation and deletion timestamps, while I/O billing is subject to caching and aggregation that can happen between the guest OS and the provider, making it impossible for a customer to measure.Storage charges.
We tested GB-month storage on EC2, GCE, and Rackspace by allocating volumes of a set size and for a set period, and then comparing the expected charge versus that reported on bills.
In EC2 and GCE, the bills were consistent with expected charges, and for the sake of brevity we omit details.
For Rackspace, however, creating a 100 GB drive and then deleting it resulted in charges that were 36.3% higher than expected.
The reason was a bug that we uncovered: delete operations can hang if a volume has not first successfully "detached," leaving the volume unavailable yet still unfairly accruing charges.
We have notified Rackspace of this issue.I/O charges in EC2.
We perform reads and writes totaling 1 GB to a 100 GB EBS volume using fio with direct I/O (in order to bypass caching by the guest OS) and with varying block sizes.
Figure 7 shows the ratio between the number of reads (writes) made to the volume and the number of reads (writes) charged for across a representative subset of block sizes.
Across all experiments, the number of measured operations is either the same or higher than the number of charged operations by up to a factor of 4.6, indicating we are billed for fewer operations than our instance performs.These discrepancies could be due to aggregation and caching within EC2's infrastructure, or simply undercharging.
To tease these apart, we performed additional experiments in which, fixing the block size at 4096 bytes, we read from and wrote to random locations within the 100 GB volume.
For these tests, the EC2-reported I/Os differed from instance-reported I/Os by only 0.8% and 0.2%, respectively, which suggests that EC2 is not undercharging but instead aggregating requests (for writes) or both aggregating and caching (for reads), which lowers costs but increases opacity of billing.
Our measurements surface a number of undesirable features of today's cloud billing ecosystem that arise at the intersection of the billing model and implementation.
First, billing events mostly occur within the cloud infrastructure, making them largely unobservable to customers.
Second, billing systems are asynchronous and make bills available long after the resource consumption occurs.
Finally, billing systems tend to aggregate data across many events or instances into a single line item.
This suits the provider: it alleviates the need to retain fine-grained information about usage and to invest in making bills available more quickly.
We note that although our study is a snapshot of these billing systems at one moment in time and the systems are continuously evolving, these characteristics reflect fundamental design choices that we believe should be reconsidered.We propose that providers adopt a transparent billing model in which billing information is accessible, understandable, timely, and predictable.
Concretely, we believe cloud providers should offer a billing API that exposes the provider's current view of key resources: compute time (including provider-captured start and stop timestamps), network (total usage, broken down by traffic category), and storage (both volume use start and stop timestamps, and number of I/Os billed).
Furthermore, the API should include a valid-as-of timestamp if the information is not real-time.
Such an interface would enable a new paradigm of cost-based computing, where customers could optimize their deployments for cost in real time and accurately audit their usage, gaining assurance that they are being billed accurately.
Similarly, providers can vary their prices in real time to control congestion.
It would also open access to an abundance of real-time resource-usage data, which could be used for anomaly detection, accountability, etc.This sets a goal for cloud billing systems, and leaves open several questions.
In particular, given the enormous scale and fine granularity of universal resource accounting and billing, what tradeoffs are inherent in exposing a transparent billing API?
How close to real-time can a provider perform reporting, and at what expense?
And, if such an API can be exposed, what new applications would that introduce?
We believe these are fundamentally important questions, the answers to which will be of great value to both cloud providers and customers.
