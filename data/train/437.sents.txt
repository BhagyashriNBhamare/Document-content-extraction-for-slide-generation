Lattice-based signature schemes following the Goldreich-Goldwasser-Halevi (GGH) design have the unusual property that each signature leaks information on the signer's secret key, but this does not necessarily imply that such schemes are insecure.
At Eurocrypt '03, Szydlo proposed a potential attack by showing that the leakage reduces the key-recovery problem to that of distinguishing integral quadratic forms.
He proposed a heuristic method to solve the latter problem, but it was unclear whether his method could attack real-life parameters of GGH and NTRUSign.
Here, we propose an alternative method to attack signature schemesàschemesà la GGH, by studying the following learning problem: given many random points uniformly distributed over an unknown n-dimensional parallelepiped, recover the parallelepiped or an approximation thereof.
We transform this problem into a multivariate optimization problem that can provably be solved by a gradient descent.
Our approach is very effective in practice: we present the first successful key-recovery experiments on NTRUSign-251 without perturbation, as proposed in half of the parameter choices in NTRU standards under consideration by IEEE P1363.1.
Experimentally, 400 signatures are sufficient to recover the NTRUSign-251 secret key, thanks to symmetries in NTRU lattices.
We are also able to recover the secret key in the signature analogue of all the GGH encryption challenges.
Inspired by the seminal work of Ajtai [1], Goldreich, Goldwasser and Halevi (GGH) proposed at Crypto '97 [9] a lattice analogue of the coding-theory-based public-key cryptosystem of McEliece [19].
The security of GGH is related to the hardness of approximating the closest vector problem (CVP) in a lattice.The GGH article [9] focused on encryption, and five encryption challenges were issued on the Internet [8].
Two years later, Nguyen [22] found a flaw in the original GGH encryption scheme, which allowed to solve four out of the five GGH challenges, and obtain partial information on the last one.
Although GGH might still be secure with an appropriate choice of the parameters, its efficiency compared to traditional public-key cryptosystems is perhaps debatable: it seems that a very high lattice dimension is required, while the keysize grows roughly quadratically in the dimension (even when using the improvement suggested by Micciancio [20]).
The only lattice-based scheme known that can cope with very high dimension is NTRU [15] (see the survey [24]), which can be viewed as a very special instantiation of GGH with a "compact" lattice and different encryption/decryption procedures (see [20,21]).
In [9], Goldreich et al. described how the underlying principle of their encryption scheme could also provide a signature scheme.
The resulting GGH signature scheme did not attract much interest in the research literature until the company NTRU Cryptosystems proposed a relatively efficient signature scheme called NTRUSign [11], based exactly on the GGH design but using the compact NTRU lattices.
NTRUSign had a predecessor NSS [14] less connected to the GGH design, and which was broken in [6,7].
Gentry and Szydlo [7] observed that the GGH signature scheme has an unusual property (compared to traditional signature schemes): each signature released leaks information on the secret key, and once sufficiently many signatures have been obtained, a certain Gram matrix related to the secret key can be approximated.
The fact that GGH signatures are not zero-knowledge can be explained intuitively as follows: for a given message, many valid signatures are possible, and the one selected by the secret key says something about the secret key itself.This information leakage does not necessarily prove that such schemes are insecure.
Szydlo [27] proposed a potential attack on GGH based on this leakage (provided that the exact Gram matrix could be obtained), by reducing the keyrecovery problem to that of distinguishing integral quadratic forms.
It is however unknown if the latter problem is easy or not, although Szydlo proposed a heuristic method based on existing lattice reduction algorithms applied to quadratic forms.
As a result, it was unclear if Szydlo's approach could actually work on real-life instantiations of GGH and NTRUSign.
The paper [12] claims that, for NTRUSign without perturbation, significant information about the secret key is leaked after 10,000 signatures.
However, it does not identify any attack that would require less than 100 million signatures (see [11, Sect. 4.5] and [12, Sect. 7.2 and App.
C]).
Our Results.
In this article, we present a new key-recovery attack on latticebased signature schemes following the GGH design, including NTRUSign.
The basic observation is that a list of known pairs (message, signature) gives rise to the following learning problem, which we call the hidden parallelepiped problem (HPP): given many random points uniformly distributed over an unknown n-dimensional parallelepiped, recover the parallelepiped or an approximation thereof (see Fig. 1).
We transform the HPP into a multivariate optimization problem based on the fourth moment (also known as kurtosis) of one-dimensional projections.
This problem can be solved by a gradient descent.
Our approach is very effective in practice: we present the first successful key-recovery experiments on NTRUSign-251 without perturbation, as proposed in half of the parameter choices in the NTRU standards [4] being considered by IEEE P1363.1 [18]; experimentally, 400 signatures are enough to disclose the NTRUSign-251 secret key.
We have also been able to recover the secret key in the signature analogue of all five GGH encryption challenges; the GGH case requires significantly more signatures because NTRU lattices have special properties which can be exploited by the attack.
When the number of signatures is sufficiently high, the running time of the attack is only a fraction of the time required to generate all the signatures.
From the theoretical side, we are able to show that under a natural assumption on the distribution of signatures, an attacker can recover the secret key of NTRUSign and the GGH challenges in polynomial time, given a polynomial number of signatures of random messages.
Related Work.
Interestingly, it turns out that the HPP (as well as related problems) have already been looked at by people dealing with what is known as Independent Component Analysis (ICA) (see, e.g., the book by Hyvärinen et al. [16]).
ICA is a statistical method whose goal is to find directions of independent components, which in our case translates to the n vectors that define the parallelepiped.
It has many applications in statistics, signal processing, and neural network research.
To the best of our knowledge, this is the first time ICA is used in cryptanalysis.There are several known algorithms for ICA, and most are based on a gradient method such as the one we use in our algorithm.
Our algorithm is closest in nature to the FastICA algorithm proposed in [17], who also considered the fourth moment as a goal function.
We are not aware of any rigorous analysis of these algorithms; the proofs we have seen often ignore the effect of errors in approximations.
Finally, we remark that the ICA literature offers other, more general, goal functions that are supposed to offer better robustness against noise etc.
We have not tried to experiment with these other functions, since the fourth moment seems sufficient for our purposes.Another closely related result is that by Frieze et al. [5], who proposed a polynomial-time algorithm to solve the HPP (and generalizations thereof).
Technically, their algorithm is slightly different from those present in the ICA litera-ture as it involves the Hessian, in addition to the usual gradient method.
They also claim to have a fully rigorous analysis of their algorithm, taking into account the effect of errors in approximations.
Unfortunately, most of the analysis is missing from the preliminary version, and to the best of our knowledge, a full version of the paper has never appeared.Open Problems.
It would be interesting to study natural countermeasures against our attack.
To date, the most efficient countermeasures known are perturbation techniques (as suggested by [12,4,13]), which modify the signature generation process in such a way that the hidden parallelepiped is replaced by a more complicated set.
For instance, the second half of parameter choices in NTRU standards [4] involves exactly a single perturbation.
In this case, the attacker has to solve an extension of the hidden parallelepiped problem in which the parallelepiped is replaced by the Minkowski sum of two hidden parallelepipeds: the lattice spanned by one of the parallelepipeds is public, but not the other one.
The drawbacks of perturbations is that they slow down signature generation, increase both the size of the secret key, and the distance between the signature and the message.Road map.
The paper is organized as follows.
In Section 2, we provide notation and necessary background on lattices, GGH and NTRUSign.
In Section 3, we introduce the hidden parallelepiped problem, and explain its relationship to GGH-type signature schemes.
In Section 4, we present a method to solve the hidden parallelepiped problem.
In Section 5, we present experimental results obtained with the attack on real-life instantiations of GGH and NTRUSign.
In Section 6, we provide a theoretical analysis of the main parts of our attack.
Vectors of R n will be row vectors denoted by bold lowercase letters such as b, and we will use row representation for matrices.
For any ring R, M n (R) will denote the ring of n×n matrices with entries in R.
The group of n×n invertible matrices with real coefficients will be denoted by GL n (R) and O n (R) will denote the subgroup of orthogonal matrices.
The transpose of a matrix M will be denoted by M t , so M −t will mean the inverse of the transpose.
The notation x denotes a closest integer to x. Naturally, b will denote the operation applied to all the coordinates of b.
If X is a random variable, we will denote by Exp[X] its expectation.
The gradient of a function f from R n to R will be denoted by f = ( ∂f ∂x1 , . . . , ∂f ∂xn ).
Let · and ·, ·· be the Euclidean norm and inner product of R n .
We refer to the survey [24] for a bibliography on lattices.
In this paper, by the term lattice, we mean a full-rank discrete subgroup of R n .
The simplest lattice is Z n .
It turns out that in any lattice L, not just Z n , there must exist linearly independent vectors b 1 , . . . , b n ∈ L such that:L = n i=1 n i b i | n i ∈ Z .
Any such n-tuple of vectors [b 1 , . . . , b n ] is called a basis of L: an n-dimensional lattice can be represented by a basis, that is, a matrix of GL n (R).
Reciprocally, any matrix B ∈ GL n (R) spans a lattice: the set of all integer linear combinations of its rows, that is, mB where m ∈ Z n .
The closest vector problem (CVP) is the following: given a basis of L ⊆ Z n and a target t ∈ Q n , find a lattice vector v ∈ L minimizing the distance v − t.
If we denote by d that minimal distance, then approximating CVP to a factor k means finding v ∈ L such that v −t ≤ kd.
A measurable part D of R n is said to be a fundamental domain of a lattice L ⊆ R n if the sets b+D, where b runs over L, cover R n and have pairwise disjoint interiors.If B is a basis of L, then the parallelepiped P 1/2 (B) = {xB :x ∈ [−1/2, 1/2] n } is a fundamental domain of L.All fundamental domains of L have the same measure: the volume vol(L) of the lattice L.
The GGH scheme [9] works with a lattice L in Z n .
The secret key is a non-singular matrix R ∈ M n (Z), with very short row vectors (their entries are polynomial in n).
In the GGH challenges [8], R was chosen as a perturbation of a multiple of the identity matrix, so that its vectors were almost orthogonal: more precisely, R = kI n + E where k = 4 √ n + 1 + 1 and each entry of the n × n matrix E is chosen uniformly at random in {−4, . . . , +3}.
Micciancio [20] noticed that this distribution has the weakness that it discloses the rough directions of the secret vectors.
The lattice L is the lattice in Z n spanned by the rows of R: the knowledge of R enables the signer to approximate CVP rather well in L.
The basis R is then transformed to a non-reduced basis B, which will be public.
In the original scheme [9], B is the multiplication of R by sufficiently many small unimodular matrices.
Micciancio [20] suggested to use the Hermite normal form (HNF) of L instead.
As shown in [20], the HNF gives an attacker the least advantage (in a certain precise sense) and it is therefore a good choice for the public basis.
The messages are hashed onto a "large enough" subset of Z n , for instance a large hypercube.
Let m ∈ Z n be the hash of the message to be signed.
The signer applies Babai's round-off CVP approximation algorithm [3] to get a lattice vector close to m:s = mR −1 R, so that s − m ∈ P 1/2 (R) = {xR : x ∈ [−1/2, 1/2] n }.
Of course, any other CVP approximation algorithm could alternatively be applied, for instance Babai's nearest plane algorithm [3].
To verify the signature s of m, one would first check that s ∈ L using the public basis B, and compute the distance s − m to check that it is sufficiently small.
NTRUSign [11] is a special instantiation of GGH with the compact lattices from the NTRU encryption scheme [15], which we briefly recall: we refer to [11,4] for more details.
In the NTRU standards [4] being considered by IEEE P1363.1 [18], one selects N = 251, q = 128.
Let R be the ring Z[X]/(X N − 1) whose multiplication is denoted by * .
Using resultants, one computes a quadruplet(f, g, F, G) ∈ R 4 such that f * G − g * F = q in R and f is invertible mod q,where f and g have 0-1 coefficients (with a prescribed number of 1), while F and G have slightly larger coefficients, yet much smaller than q.
This quadruplet is the NTRU secret key.
Then the secret basis is the following (2N ) × (2N ) matrix:R =          f0 f1 · · · f N −1 g0 g1 · · · g N −1 f N −1 f0 · · · f N −2 g N −1 g0 · · · g N −2 . . . . . . . . . . . . . . . . . . . . . . . .
f1 · · · f N −1 f0 g1 · · · g N −1 g0 F0 F1 · · · F N −1 G0 G1 · · · G N −1 F N −1 F0 · · · F N −2 G N −1 G0 · · · G N −2 . . . . . . . . . . . . . . . . . . . . . . . .
F1 · · · F N −1 F0 G1 · · · G N −1 G0          ,where f i denotes the coefficient of X i of the polynomial f .
Thus, the lattice dimension is n = 2N .
Due to the special structure of R, it turns out that a single row of R is sufficient to recover the whole secret key.
Because f is chosen invertible mod q, the polynomial h = g/f (mod q) is well-defined in R: this is the NTRU public key.
Its fundamental property is that f * h ≡ g (mod q) in R.The polynomial h defines a natural public basis of L, which we omit (see [11]).
The messages are assumed to be hashed in {0, . . . , q−1} 2N .
Let m ∈ {0, . . . , q− 1} 2N be such a hash.
We write m = (m 1 , m 2 ) with m i ∈ {0, . . . , q − 1} N .
It is shown in [11] that the vector (s, t) ∈ Z 2N which we would obtain by applying Babai's round-off CVP approximation algorithm to m using the secret basis R can be alternatively computed using convolution products involving m 1 , m 2 and the NTRU secret key (f, g, F, G).
In practice, the signature is simply s and not (s, t), as t can be recovered from s thanks to h. Besides, s might be further reduced mod q, but its initial value can still be recovered because it is such that s − m 1 ranges over a small interval (this is the same trick used in NTRU decryption).
This gives rise for standard parameter choices to a signature length of 251 × 7 = 1757 bits.
While this signature length is much smaller than other lattice-based signature schemes such as GGH, it is still significantly larger than more traditional signature schemes such as DSA.
This is the basic NTRUSign scheme [11].
In order to strengthen the security of NTRUSign, perturbation techniques have been proposed in [12,4,13].
Roughly speaking, such techniques perturb the hashed message m before signing with the NTRU secret basis.
However, it is worth noting that there is no perturbation in half of the parameter choices recommended in NTRU standards [4] under consideration by IEEE P1363.1.
Namely, this is the case for the parameter choices ees251sp2, ees251sp3, ees251sp4 and ees251sp5 in [4].
For the other half, only a single perturbation is recommended.
But NTRU has stated that the parameter sets presented in [13] are intended to supersede these parameter sets.
Consider the signature generation in the GGH scheme described in Section 2.
Let R ∈ M n (Z) be the secret basis used to approximate CVP in the lattice L. Let m ∈ Z n be the message digest.
Babai's round-off CVP approximation algorithm [3] computes the signature s = mR −1 R, so that s − m belongs to the parallelepiped P 1/2 (R) = {xR :x ∈ [−1/2, 1/2] n }, which is a fundamental domain of L.In other words, the signature generation is simply a reduction of the message m modulo the parallelepiped spanned by the secret basis R.
If we were using Babai's nearest plane CVP approximation algorithm [3], we would have another fundamental parallelepiped (spanned by the Gram-Schmidt vectors of the secret basis) instead: we will not further discuss this case in this paper, since it does not create any significant difference and since this is not the procedure chosen in NTRUSign.GGH [9] suggested to hash messages into a set much bigger than the fundamental domain of L.
This is for instance the case in NTRUSign where the cardinality of {0, . . . , q − 1} 2N is much bigger than the lattice volume q N .
Whatever the distribution of the message digest m might be, it would be reasonable to assume that the distribution s − m is uniform (or very close to uniform) in the secret parallelepiped P 1/2 (R).
More precisely, it seems reasonable to make the following assumption:Assumption 1 (The Uniformity Assumption) Let R be the secret basis of the lattice L ⊆ Z n .
When the GGH scheme signs polynomially many "randomly chosen" message digests m 1 , . . . , m k ∈ Z n using Babai's round-off algorithm, the signatures s 1 , . . . , s k are such that the vectors s i − m i are independent and uniformly distributed over P 1/2 (R) = {xR :x ∈ [−1/2, 1/2] n }.
Note that this is only an idealized assumption: in practice, the signatures and the message digests are integer vectors, so the distribution of s i − m i is discrete rather than continuous, but this should not be a problem if the lattice volume is sufficiently large, as is the case in NTRUSign.
Similar assumptions have been used in previous attacks [7,27] on lattice-based signature schemes.
We emphasize that all our experiments on NTRUSign do not use this assumption and work with real-life signatures.We thus arrive at the following geometric learning problem (see Fig. 1):Problem 2 (The Hidden Parallelepiped Problem or HPP) Let V = [v 1 , . . . , v n ] ∈ GL n (R) and let P(V ) = { n i=1 x i v i : x i ∈ [−1, 1]} be the parallelepiped spanned by V .
The input to the HPP is a sequence of poly(n) independent samples from U (P(V )), the uniform distribution over P(V ).
The goal is to find a good approximation of the rows of ±V .
In the definition of the HPP, we chose [−1, 1] rather than [−1/2, 1/2] like in Assumption 1 to simplify subsequent calculations.Clearly, if one could solve the HPP, then one would be able to approximate the secret basis in GGH by collecting random pairs (message, signature).
To complete the attack, we need to show how to obtain the actual secret basis given a good enough approximation of it.
One simple way to achieve this is by rounding the coordinates of the approximation to the closest integer.
This approach should work well in cases where the entries in the secret basis are small in absolute value.
Alternatively, one can use approximate-CVP algorithms to try to recover the secret basis, since one knows a lattice basis from the GGH public key.
The success of this approach depends on whether the approximation is sufficiently good for existing lattice reduction algorithms.
Previous experiments of [22] on the GGH-challenges [8] seem to suggest that in practice, even a moderately good approximation of a lattice vector is sufficient to recover the closest lattice vector, even in high dimension.The Special Case of NTRUSign.
Following the announcement of our result in [23], Whyte observed [28] that symmetries in the NTRU lattices might lead to attacks that require far less signatures.
Namely, Whyte noticed that in the particular case of NTRUSign, the hidden parallelepiped P(R) has the following property: for each x ∈ P(R) the block-rotation σ(x) also belongs to P(R), where σ is the function that maps any (x 1 , . . . , x N , y 1 , . . . , y N ) ∈ R 2N to (x N , x 1 , . . . , x N −1 , y N , y 1 , . . . , y N −1 ).
This is because σ is a linear operation that permutes the rows of R and hence leaves P(R) invariant.
As a result, by using the N possible rotations, each signature actually gives rise to N samples in the parallelepiped P(R) (as opposed to just one in the general case of GGH).
For instance, 400 NTRUSign-251 signatures give rise to 100,400 samples in the NTRU parallelepiped.
Notice that these samples are no longer independent and hence Assumption 1 does not hold.
Nevertheless, as we will describe later, this technique leads in practice to attacks using a significantly smaller number of signatures.
In this section, we describe our solution to the Hidden Parallelepiped Problem (HPP), based on the following steps.
First, we approximate the covariance matrix of the given distribution.
This covariance matrix is essentially V t V (where V defines the given parallelepiped).
We then exploit this approximation in order to transform our hidden parallelepiped P(V ) into a unit hypercube: in other words, we reduce the HPP to the case where the hidden parallelepiped is a hypercube.
Finally, we show how hypercubic instances of the HPP are related to a multivariate optimization problem based on the fourth moment, which we solve by a gradient descent.
The algorithm is summarized in Algorithms 1 and 2, and in described in more detail in the following.
The first step in our algorithm is based on the idea of approximating the covariance matrix, which was already present in the work of Gentry and Szydlo [7,27] (after this basic step, our strategy differs completely from theirs).
Namely, Gentry and Szydlo observed that from GGH signatures one can easily obtain an approximation of V t V , the Gram matrix of the transpose of the secret basis.Here, we simply translate this observation to the HPP setting.Lemma 1 (Covariance Matrix Leakage).
Let V ∈ GL n (R).
Let v be chosen from the uniform distribution over the parallelepiped P(V ).
ThenExp[v t v] = V t V /3.
In other words, the covariance matrix of the distribution U (P(V )) is V t V /3.
Proof.
We can write v = xV where x has uniform distribution over [−1, 1] n .
Hence,v t v = V t x t xV.An elementary computation shows that Exp[x t x] = I n /3 where I n is the n × n identity matrix, and the lemma follows.
Hence, by taking the average of v t v over all our samples v from U (P(V )), and multiplying the result by 3, we can obtain an approximation of V t V .
The second stage is explained by the following lemma.Lemma 2 (Hypercube Transformation).
Let V ∈ GL n (R).
Denote by G ∈ GL n (R) the symmetric positive definite matrix V t V .
Denote by L ∈ GL n (R) the Cholesky factor 1 of G −1 , that is, L is the unique lower-triangular matrix such that G −1 = LL t .
Then the matrix C = V L ∈ GL n (R) satisfies the following:1.
The rows of C are pairwise orthogonal unit vectors.
In other words, C is an orthogonal matrix in O n (R) and P(C) is a unit hypercube.
2.
If v is uniformly distributed over the parallelepiped P(V ), then c = vL is uniformly distributed over the hypercube P(C).
Proof.
The Gram matrix G = V t V is clearly symmetric positive definite.
Hence G −1 = V −1 V −t is also symmetric positive definite, and has a Cholesky factorization G −1 = LL t where L is lower-triangular matrix.
Therefore,V −1 V −t = LL t .
Let C = V L ∈ GL n (R).
Then CC t = V LL t V t = V V −1 V −t V t = I.For the second claim, let v be uniformly distributed over P(V ).
Then we can write v = xV where x is uniformly distributed over [−1, 1] n .
It follows that vL = xV L = xC has the uniform distribution over P(C).
Lemma 2 says that by applying the transformation L, we can map our samples from the parallelepiped P(V ) into samples from the hypercube P(C).
Then, if we could approximate the rows of ±C, we would also obtain an approximation of the rows of ±V by applying L −1 .
In other words, we have reduced the Hidden Parallelepiped Problem into what one might call the Hidden Hypercube Problem (see Fig. 2).
From an implementation point of view, we note that the Cholesky factorization (required for obtaining L) can be easily computed by a process close to the Gram-Schmidt orthogonalization process (see [10]).
Lemma 2 assumes that we know G = V t V exactly.
If we only have an approximation of G, then C will only be close to some orthogonal matrix in O n (R): the Gram matrix CC t of C will be close to the identity matrix, and the image under L of our parallelepiped samples will have a distribution close to the uniform distribution of some unit hypercube.
For any V = [v 1 , . . . , v n ] ∈ GL n (R) and any integer k ≥ 1, we define the k-th moment of P(V ) over a vector w ∈ R n asmom V,k (w) = Exp[u, w k ],where u is uniformly distributed over the parallelepiped P(V ).
2 Clearly, mom V,k (w) can be approximated by using the given sample from U (P(V )).
We are interested in the second and fourth moments.
A straightforward calculation shows that for any w ∈ R n , they are given bymom V,2 (w) = 1 3 n i=1 v i , w 2 = 1 3 wV t V w t , mom V,4 (w) = 1 5 n i=1 v i , w 4 + 1 3 i =j v i , w 2 v j , w 2 .
Note that the second moment is given by the covariance matrix mentioned in Section 4.1.
When V ∈ O n (R), the second moment becomes w 2 /3 while the fourth moment becomesmom V,4 (w) = 1 3 w 4 − 2 15 n i=1 v i , w 4 .
The gradient of the latter is thereforemom V,4 (w) = n i=1   4 3   n j=1 v j , w 2   v i , w − 8 15 v i , w 3   v i .
For w on the unit sphere the second moment is constantly 1/3, andmom V,4 (w) = 1 3 − 2 15 n i=1 v i , w 4 , mom V,4 (w) = 4 3 w − 8 15 n i=1 v i , w 3 v i .
(1)Lemma 3.
Let V = [v 1 , . . . , v n ] ∈ O n (R).
Then the global minimum of mom V,4 (w) over the unit sphere of R n is 1/5 and this minimum is obtained at ±v 1 , . . . , ±v n .
There are no other local minima.Proof.
The method of Lagrange multipliers shows that for w to be an extremum point of mom V,4 on the unit sphere, it must be proportional to mom V,4 (w).
By writing w = n i=1 v i , wv i and using Eq.
(1), we see that there must exist some α such that v i , w 3 = αv i , w for i = 1, . . . , n.
In other words, each v i , w is either zero or ± √ α.
It is easy to check that among all such points, only ±v 1 , . . . , ±v n form local minima.In other words, the hidden hypercube problem can be reduced to a minimization problem of the fourth moment over the unit sphere.
A classical technique to solve such minimization problems is the gradient descent described in Algorithm 2.
The gradient descent typically depends on a parameter δ, which has Parameters: A descent parameter δ.
Input: A polynomial number of samples uniformly distributed over a unit hypercube P(V ).
Output: An approximation of some row of ±V .1: Let w be chosen uniformly at random from the unit sphere of R n .2: Compute an approximation g of the gradient mom4(w) (see Section 4.3).3: Let wnew = w − δg.
return the vector w. 7: else 8:Replace w by wnew and go back to Step 2.
9: end if to be carefully chosen.
Since we want to minimize the function here, we go in the opposite direction of the gradient.
To approximate the gradient in Step 2 of Algorithm 2, we notice thatmom V,4 (w) = Exp[(u, w 4 )] = 4Exp[u, w 3 u].
This allows to approximate the gradient mom V,4 (w) using averages over samples, like for the fourth moment itself.
As usual in cryptanalysis, perhaps the most important question is whether or not the attack works in practice.
We therefore implemented the attack in C++ and ran it on a 2GHz PC/Opteron.
The critical parts of the code were written in plain C++ using double arithmetic, while the rest used Shoup's NTL library version 5.4 [26].
Based on trial-and-error, we chose δ = 0.7 in the gradient descent (Algorithm 2), for all the experiments mentioned here.
The choice of δ has a big impact on the behavior of the gradient descent: the choice δ = 0.7 works well, but we do not claim that it is optimal.
When doing several descents in a row, it is useful to relax the halting condition 5 in Algorithm 2 to abort descents which seem to make very little progress.
We performed two kinds of experiments against NTRUSign, depending on whether the symmetries of NTRU lattices explained in Section 3 were used or not.
All the experiments make it clear that perturbation techniques are really mandatory for the security of NTRUSign, though it is currently unknown if such techniques are sufficient to prevent this kind of attacks.Without exploiting the symmetries of NTRU lattices.
We applied Algorithm 1 to real-life parameters of NTRUSign.
More precisely, we ran the attack on NTRUSign-251 without perturbation, corresponding to the parameter choices ees251sp2, ees251sp3, ees251sp4 and ees251sp5 in the NTRU standards [4] under consideration by IEEE P1363.1 [18].
This corresponds to a lattice dimension of 502.
We did not rely on the uniformity assumption: we generated genuine NTRUSign signatures of messages generated uniformly at random over {0, . . . , q − 1} n .
The results of the experiments are summarized in number of signatures in the range 80,000-300,000, we generated a set of signatures, and applied Algorithm 1 to it: from the set of samples we derived an approximation of the Gram matrix, used it to transform the parallelepiped into a hypercube, and finally, we ran a series of about a thousand descents, starting with random points.
We regard a descent as successful if, when rounded to the nearest integer vector, the output of the descent gives exactly one of the vectors of the secret basis (which is sufficient to recover the whole secret basis in the case of NTRUSign).
We did not notice any improvement with Babai's nearest plane algorithm [3] (with a BKZ-20 reduced basis [25]) as a CVP approximation.
The curve shows the average number of random descents needed for a successful descent as a function of the number of signatures.
Typically, a single random descent does not take much time: for instance, a usual descent for 150,000 signatures takes roughly ten minutes.
When successful, a descent may take as little as a few seconds.
The minimal number of signatures to make the attack successful in our experiments was 90,000, in which case the required number of random descents was about 400.
With 80,000 signatures, we tried 5,000 descents without any success.
The curve given in Fig. 4 may vary a little bit, depending on the secret basis: for instance, for the basis used in the experiments of Fig. 4, the average number of random descents was 15 with 140,000 signatures, but it was 23 for another basis generated with the same NTRU parameters.
It seems that the exact geometry of the secret basis has an influence, as will be seen in the analysis of Section 6.
Exploiting the symmetries of NTRU lattices.
Based on Whyte's observation described in Section 3, one might hope that the number of signatures required by the attack can be shrunk by a factor of roughly N .
Surprisingly, this is indeed the case in practice (see Table 1): as few as 400 signatures are enough in practice to recover the secret key, though the corresponding 100,400 parallelepiped samples are not independent.
This means that the previous number of 90,000 signatures required by the attack can be roughly divided by N = 251.
Hence, NTRUSign without perturbation should be considered totally insecure.
We also did experiments on the GGH-challenges [8], which range from dimension 200 to 400.
Because there is actually no GGH signature challenge, we simply generated secret bases like in the GGH encryption challenges.
To decrease the cost of sample generation, and because there was no concrete proposal of a GGH signature scheme, we relied on the uniformity assumption: we created samples uniformly distributed over the secret parallelepiped, and tried to recover the secret basis.
When the number of samples becomes sufficiently high, the approximation obtained by a random descent is sufficiently good to disclose one of the vectors of the secret basis by simple rounding, just as in the NTRUSign case: however, the number of required samples is significantly higher than for NTRUSign; for instance, with 200,000 samples in dimension 200, three descents are enough to disclose a secret vector by rounding; whereas three descents are also enough with 250,000 samples for NTRUSign, but that corresponds to a dimension of 502 which is much bigger than 200.
This is perhaps because the secret vectors of the GGH challenges are significantly longer than those of NTRUSign.However, one can significantly improve the result by using a different rounding procedure.
Namely, instead of rounding the approximation to an integer vector, one can apply a CVP approximation algorithm such as Babai's nearest plane algorithm [3]: such algorithms will succeed if the approximation is sufficiently close to the lattice; and one can improve the chances of the algorithm by computing a lattice basis as reduced as possible.
For instance, with only 20,000 samples in dimension 200, it was impossible to recover a secret vector by rounding, but it became easy with Babai's nearest plane algorithm on a BKZ-20 reduced basis [25] (obtained by BKZ reduction of the public HNF basis): more precisely, three random descents sufficed on the average.
More generally, Figure 5 shows the average number of samples required so that ten random descents disclose a secret vector with high probability, depending on the dimension of the GGH challenge, using Babai's nearest plane algorithm on a BKZ-20 reduced basis.
Figure 5 should not be interpreted as the minimal number of signatures required for the success of the attack: it only gives an upper bound for that number.
Indeed, there are several ways to decrease the number of signatures:-One can run much more than ten random descents.
-One can take advantage of the structure of the GGH challenges.
When starting a descent, rather than starting with a random point on the unit sphere, we may exploit the fact that we know the rough directions of the secret vectors.
-One can use better CVP approximation algorithms.
Our goal in this section is to give a rigorous theoretical justification to the success of the attack.
Namely, we will show that given a large enough polynomial number of samples, Algorithm 1 succeeds in finding a good approximation to a row of V with some constant probability.
For sake of clarity and simplicity, we will not make any attempt to optimize this polynomial bound on the number of samples.
Let us remark that it is possible that a rigorous analysis already exists in the ICA literature, although we were unable to find any (an analysis under some simplifying assumptions can be found in [17]).
Also, Frieze et al. [5] sketch a rigorous analysis of a similar algorithm.In order to approximate the covariance matrix, the fourth moment, and its gradient, our attack computes averages over samples.
Because the samples are independent and identically distributed, we can use known bounds on large deviations such as the Chernoff bound (see, e.g., [2]) to obtain that with extremely high probability the approximations are very close to the true values.
In our analysis below we omit the explicit calculations, as these are relatively standard.
We start by analyzing Algorithm 2.
For simplicity, we consider only the case in which the descent parameter δ equals 3/4.
A similar analysis holds for 0 < δ < 3/4.
Another simplifying assumption we make is that instead of the stopping rule in Step 5 we simply repeat the descent step some small number r of times (which will be specified later).
For now, let us assume that the matrix V is an orthogonal matrix, so our samples are drawn from a unit hypercube P(V ).
We will later show that the actual matrix V , as obtained from Algorithm 1, is very close to orthogonal, and that this approximation does not affect the success of Algorithm 2.
Theorem 3.
For any c 0 > 0 there exists a c 1 > 0 such that given n c1 samples uniformly distributed over some unit hypercube P(V ), V = [v 1 , . . . , v n ] ∈ O n (R), Algorithm 2 with δ = 3/4 and r = O(log log n) descent steps outputs with constant probability a vector that is within 2 distance n −c0 of ±v i for some i.Proof.
We first analyze the behavior of Algorithm 2 under the assumption that all gradients are computed exactly without any error.
We write any vector w ∈ R n as w = n i=1 w i v i .
Then, using Eq.
(1), we see that for w on the unit sphere,mom V,4 (w) = 4 3 w − 8 15 n i=1 w 3 i v i .
Since we took δ = 3/4, Step 3 in Algorithm 2 performsw new = 2 5 n i=1 w 3 i v i .
The vector is then normalized in Step 4.
So we see that each step in the gradient descent takes a vector (w 1 , . . . , w n ) to the vector α · (w 3 1 , . . . , w 3 n ) for some normalization factor α (where both vectors are written in the v i basis).
Hence, after r iterations, a vector (w 1 , . . . , w n ) is transformed to the vector α · (w 3 r 1 , . . . , w 3 r n )for some normalization factor α.Recall now that the original vector (w 1 , . . . , w n ) is chosen uniformly from the unit sphere.
It can be shown that with some constant probability, one of its coordinates is greater in absolute value than all other coordinates by a factor of at least 1 + Ω(1/ log n) (first prove this for a vector distributed according to the standard multivariate Gaussian distribution, and then note that by normalizing we obtain a uniform vector from the unit sphere).
For such a vector, after only r = O(log log n) iterations, this gap is amplified to more than, say, n log n , which means that we have one coordinate very close to ±1 and all others are at most n − log n in absolute value.
This establishes that if all gradients are known precisely, Algorithm 2 succeeds with some constant probability.To complete the analysis of Algorithm 2, we now argue that it succeeds with good probability even in the presence of noise in the approximation of the gradients.
First, it can be shown that for any c > 0, given a large enough polynomial number of samples, with very high probability all our gradient approximations are accurate to within an additive error of n −c in the 2 norm (we have r such approximations during the course of the algorithm).
This follows by a standard application of the Chernoff bound followed by a union bound.
Now let w = (w 1 , . . . , w n ) be a unit vector in which one coordinate, say the jth, is greater in absolute value than all other coordinates by at least a factor of 1 + Ω(1/ log n).
Since w is a unit vector, this in particular means that w j > 1/ √ n. Let˜wLet˜ Let˜w new = w − δmom 4 (w).
Recall that for each i, ˜ w new,i = 2 5 w 3 i which in particular implies that˜wthat˜ that˜w new,j > 2 5 n −1.5 > n −2 .
By our assumption on the approximation g, we have that for each i, | ˜ w new,i − w new,i | ≤ n −c .
So for any k = j,|w new,j | |w new,k | ≥ | ˜ w new,j | − n −c | ˜ w new,k | + n −c ≥ | ˜ w new,j |(1 − n −(c−2) ) | ˜ w new,k | + n −c .
If| ˜ w new,k | > n −(c−1) , then the above is at least (1 − O(1/n))(w j /w k ) 3 .
Otherwise, the above is at least Ω(n c−3 ).
Hence, after O(log log n) steps, the gap w j /w k becomes Ω(n c−3 ).
Therefore, for any c 0 > 0 we can make the distance between the output vector and one of the ±v i s less than n −c0 by choosing a large enough c.
The following theorem completes the analysis of the attack.
In particular, it implies that if V is an integer matrix all of whose entries are bounded in absolute value by some polynomial, then running Algorithm 1 with a large enough polynomial number of samples from the uniform distribution on P(V ) gives (with constant probability) an approximation to a row of ±V whose error is less than 1/2 in each coordinate, and therefore leads to an exact row of ±V simply by rounding each coordinate to the nearest integer.
Hence we have a rigorous proof that our attack can efficiently recover the secret key in both NTRUSign and the GGH challenges.Theorem 4.
For any c 0 > 0 there exists a c 1 > 0 such that given n c1 samples uniformly distributed over some parallelepiped P(V ), V = [v 1 , . . . , v n ] ∈ GL n (R), Algorithm 1 outputs with constant probability a vector˜eVvector˜vector˜eV where˜ewhere˜where˜e is within 2 distance n −c0 of some standard basis vector e i .
Proof.
Recall that a sample v from U (P(V )) can be written as xV where x is chosen uniformly from [−1, 1] n .
So let v i = x i V for i = 1, . . . , N be the input samples.
Then our approximation G to the Gram matrix V t V is given by G = V t ˜ IV where˜Iwhere˜ where˜I = 3 N x t i x i .
We claim that with high probability, ˜ I is very close to the identity matrix.
Indeed, for x chosen randomly from [−1, 1] n , each diagonal entry of x t x has expectation 1/3 and each off-diagonal entry has expectation 0.
Moreover, these entries take values in [−1, 1].
By the Chernoff bound we obtain that for any approximation parameter c > 0, if we choose, say, N = n 2c+1 then with very high probability each entry iñ I − I is at most n −c in absolute value.
In particular, this implies that all eigenvalues of the symmetric matrix˜Imatrix˜ matrix˜I are in the range 1 ± n −c+1 .
Recall that we define L to be the Cholesky factor of G −1 = V −1 ˜ I −1 V −t and that C = V L.
Now CC t = V LL t V t = ˜ I −1 , which implies that C is close to an orthogonal matrix.
Let us make this precise.
Consider the singular value decomposition of C, given by C = U 1 DU 2 where U 1 , U 2 are orthogonal matrices and D is diagonal.
Then CC t = U 1 D 2 U t 1 and hence D 2 = U t 1 ˜ I −1 U 1 .
From this it follows that the diagonal of D consists of the square roots of the reciprocals of the eigenvalues of˜Iof˜ of˜I, which in particular means that all values on the diagonal of D are also in the range 1 ± n −c+1 .
Consider the orthogonal matrix˜Cmatrix˜ matrix˜C = U 1 U 2 .
We claim that for large enough c, samples from P(C) 'look like' samples from P( ˜ C).
More precisely, assume that c is chosen so that the number of samples required by Algorithm 2 is less than, say, n c−4 .
Then, it follows from Lemma 4 below that the statistical distance between a set of n c−4 samples from P(C) and a set of n c−4 samples from P( ˜ C) is at most O(n −1 ).
By Theorem 3, we know that when given samples from P( ˜ C), Algorithm 2 outputs an approximation of a row of ± ˜ C with some constant probability.
Hence, when given samples from P(C), it must still output an equally good approximation of a row of ± ˜ C with a probability that is smaller by at most O(n −1 ) and in particular, constant.To complete the proof, let˜clet˜ let˜c be the vector obtained in Step 4.
The output of Algorithm 1 is then As we have seen before, all eigenvalues of U 1 D −1 U t 1 are close to 1.
It therefore follows that the above is a good approximation to a row of ±V , and it is not hard to verify that the quality of this approximation satisfies the requirements stated in the theorem.Lemma 4.
The statistical distance between the uniform distribution on P(C) and that on P( ˜ C) is at most O(n −c+3 ).
Proof.
We first show that the parallelepiped P(C) is almost contained and almost contains the cube P( ˜ C):(1 − n −c+2 )P( ˜ C) ⊆ P(C) ⊆ (1 + n −c+2 )P( ˜ C).
To show this, take any vector y ∈ [−1, 1] n .
The second containment is equivalent to showing that all the coordinates of yU 1 DU t 1 are at most 1 + n −c+2 in absolute value.
Indeed, by the triangle inequality,yU 1 DU t 1 ∞ ≤ y ∞ + yU 1 (D − I)U t 1 ∞ ≤ 1 + yU 1 (D − I)U t 1 2 ≤ 1 + n −c+1 √ n < 1 + n −c+2 .
The first containment is proved similarly.
On the other hand, the ratio of volumes between the two cubes is ((1 + n −c+2 )/(1 − n −c+2 )) n = 1 + O(n −c+3 ).
From this it follows that the statistical distance between the uniform distribution on P(C) and that on P( ˜ C) is at most O(n −c+3 ).
