We describe a machine translation approach being designed at HKUST to integrate semantic processing into statistical machine translation, beginning with entity and word sense disambiguation.
We show how integrating the semantic modules consistently improves translation quality across several data sets.
We report results on five different IWSLT 2006 speech translation tasks, representing HKUST's first participation in the IWSLT spoken language translation evaluation campaign.
We translated both read and spontaneous speech transcriptions from Chinese to English, achieving reasonable performance despite the fact that our system is essentially text-based and therefore not designed and tuned to tackle the challenges of speech translation.
We also find that the system achieves reasonable results on a wide range of languages, by evaluating on read speech transcriptions from Arabic, Italian, and Japanese into English.
The role and usefulness of semantic processing for Statistical Machine Translation (SMT) has recently been much debated.
In previous work, we reported surprisingly disappointing results when using the predictions of a Senseval word sense disambiguation (WSD) system in conjunction with SMT using an IBM-style model (Carpuat and Wu, 2005b).
Nevertheless, error analysis leaves little doubt that the performance of SMT systems still suffers from inaccurate lexical choice.
Other empirical studies have shown that SMT systems perform much more poorly than dedicated WSD models, both supervised and unsupervised, on Senseval WSD tasks (Carpuat and Wu, 2005a)-also suggesting that WSD still has a role to play in improving SMT.In this paper, we describe ongoing work on an approach being designed at HKUST to investigate the effect of semantic handling on current SMT models, using dedicated word sense and entity disambiguation modules.
In particular, we propose a new architecture for integrating WSD into SMT architectures, and show that this additional semantic handling consistently improves translation quality across several data sets.We then turn to the IWSLT 2006 tasks, describing the experimental set-up and evaluation results.
This represents a first participation by HKUST in the IWSLT spoken language translation evaluation campaign.
For this first participation, we focused on building a baseline system for Chinese to English translation that could be easily ported to different language pairs.
We therefore chose to translate additional input languages from different language families.
We submitted translations of read and spontaneous speech in the Chinese to English task, as well as read speech translations from Arabic, Italian and Japanese into English.
Despite the fact that the system is essentially text-based, and therefore is not designed and tuned to tackle the challenges of speech translation, the system achieves reasonable performance, yielding a BLEU score of 15.45 and a ME-TEOR score of 44.56 for Chinese to English translation, our main language pair of interest.
Results on other language pairs suggest that the system can achieve reasonable results with little modification.
The core MT engine as used in the experiments here is an off-the-shelf phrase-based statistical machine translation model.
This is a useful engine since the approach has been shown to achieve competitive translation quality and is commonly used.
Many state-of-the-art systems employ phrase-based approaches (e.g., Zens et al. (2005), Koehn et al. (2005), Sadat et al. (2005)).
All phrase-based models make use of a phrasal bilexicon, but essentially differ in the bilexicon extraction and parameter estimation strategies, and the phrase reordering method.
For the experiments here, we used the Pharaoh decoder (Koehn, 2004), which implements a heuristic beam search for phrase based translation.
While the phrase reordering model used in Pharaoh is weaker than in other proposed models, Pharaoh was chosen for the advantages of being freely available and widely used, and therefore constitutes an appropriate point of reference.
The core phrasal bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ ( Och and Ney, 2002).
The intersection is augmented using growing heuristics proposed by Och and Ney (2002) in order to improve recall.
Following Koehn (2003), each entry in the phrasal bilexicon is scored using phrase translation conditional probabilities for both translation directions, as well as lexical weights which combine word translation probabilities according to the word alignment observed within the phrase pair during training.
The language model is a standard trigram model with Kneser-Ney smoothing trained using the SRI language modeling toolkit (Stolcke, 2002).
We now present a new architecture integrating a stateof-the-art WSD model into phrase-based SMT, and show that WSD produces small but consistent gains across several test sets.
The model consists of an ensemble of four voting models combined by majority vote.
The first voting model is a na¨ıvena¨ıve Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data.The second voting model is a maximum entropy model (Jaynes, 1979), since Klein and Manning (2002) found that this model yielded higher accuracy than na¨ıvena¨ıve Bayes in a subsequent comparison of WSD performance.The third voting model is a boosting model (Freund and Schapire, 1997), since has consistently turned in very competitive scores on related tasks such as named entity classification, as described in Section 4.1.1.
We also use the Adaboost.MH algorithm for WSD, just like for NER.The fourth voting model is a model based on Kernel PCA ( ).
Kernel Principal Component Analysis (KPCA) is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space R n to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors ( Schölkopf et al., 1998).
WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space.
We have showed that KPCAbased WSD models achieve close accuracies to the best individual WSD models, while having a significantly different bias ).
All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant.
Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent.
The WSD classifier employs much richer features than IBM-style statistical MT systems.
The feature set consists of position-sensitive, syntactic, and local collocational features, since these features yielded the best results when combined in a na¨ıvena¨ıve Bayes model on several Senseval-2 lexical sample tasks ( Yarowsky and Florian, 2002).
All these WSD models were extensively evaluated on a wide range of monolingual and multilingual lexi- cal sample disambiguation tasks both on Senseval-2 and Senseval-3 data (e.g., , , Su et al. (2004)).
Table 1 shows that our method of integrating a state-ofthe-art WSD model into phrase-based SMT produces small but consistent gains across all Chinese-English development test sets.
The main difference between this approach and our earlier experiments (Carpuat and Wu, 2005b) lies in the fact that we focus on repurposing the WSD system for SMT.
Rather than using a generic Senseval WSD model, both the WSD training and the WSD predictions are integrated into the SMT framework.
Specifically: • Instead of using a Senseval system, we redefine the WSD task to be as close as possible to the translation disambiguation task faced by the SMT system.
• Instead of using predefined senses drawn from manually constructed sense inventories such as HowNet (Dong, 1998), our WSD for SMT system directly disambiguates between all translation candidates seen during SMT training.
• Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system.Thus, in a given SMT input sentence, for every word that was seen in the training data, we have a WSD model and a context-dependent distribution over the possible translation candidates of the word.
This distribution is used to augment the baseline bilexicon.
With Pharaoh, we use the provided XML markup scheme to specifiy translation candidates and their corresponding probabilities.
At decoding time, these externally generated translation candidates are considered as if they were additional bilexicon entries, and are used to build translation hypotheses that compete with other translation hypotheses build from within the traditional SMT phrasal translation lexicon.Analysis shows that the WSD translation probabilities give better rankings and are more discriminative than the baseline translation probabilities, yielding improved translations as can be seen in Table 2.
Recognizing, disambiguating, and translating entities is a special case of word sense disambiguation for translation lexical choice, where the words or phrases in question are entities of various sorts.
Translating names correctly is particularly important to translation quality and usefulness, but does present some distinct challenges from regular phrase translation.
First, the vast majority of names are rare and often never seen in training, and, with the exception of names of well-known persons or other entities, are typically not recorded in lexicons.
Second, whether a phrase is a named-entity (NE) depends on context and is therefore ambiguous.
Third, names have specific translation patterns.
For instance, the translation of a person name usually cannot be inferred from the translation of each of its components.The first step in handling NE translation consists in identifying NE boundaries and their type.
In this system, we are focusing on identifying the PERSON, LO-CATION and ORGANIZATION entity types.
For the purpose of translation, identifying NE boundaries is not sufficient, since the type of a NE affects the translation patterns: for instance, many location and person names can typically be transliterated, while some components of organization names should be translated with a standard bilexicon instead.
After identifying NE boundaries and types, a rulebased translation approach based on name gazetteers and transliteration schemes is used to obtain one or more translations for each identified NE.The decoder integrates the NE translation candidates as additional translation candidates for the NE phrase, using the Pharaoh XML markup scheme for translation input, as for the integration of the WSD predictions.
The named-entity recognition (NER) system is based on a multilingual NER system initially developped for several European languages, and subsequently adapted to Chinese.
As NER can be framed as a classification task, we use an ensemble of three relatively high performing machine learning classifiers:Boosting: The main idea behind boosting algorithms is that a set of many weak classifiers can be effectively combined to yield a single strong classifier.
Each weak classifier is trained sequentially, increasingly focusing more heavily on the instances that the previous classifiers found difficult to classify.
Our system uses AdaBoost.MH (Freund and Schapire, 1997), an n-ary classification variant of the original binary AdaBoost algorithm.
As demonstrated by Wu et al. (2002) and Car- reras et al. (2002), boosting can be used to build language independent NER models that perform exceptionally well.Support Vector Machines: Support Vector Machines (SVMs) have gained a considerable following in recent years ( Boser et al., 1992).
Sassano and Utsuro (2000) and McNamee and Mayfield (2002) have demonstrated that SVMs show promise when applied to named entity recognition, though performance appears quite sensitive to parameter choices.Transformation-based learning: Transformationbased learning (TBL) is a rule-based machine learning algorithm that was first introduced by Brill (1995) and used for part-of-speech tagging.
The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set.
An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.
Our system uses the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training.
We use a set of primary features which can be easily obtained across languages, and require little linguistic analysis.For European languages, features are defined as follows:• Lexical (words and lemmas) and syntactic (partof-speech) information within a window of 2 words surrounding the current word• Prefixes and suffixes of up to a length of 4 characters from the current word• Capitalization: whether the word starts with a capital letter and/or the entire word is capitalized• A small set of conjunctions of POS tags and words within a window of 2 words of the current word• Previous history: the chunk tags (gold standard during training; assigned for evaluation) of the previous two words.
• Gazetteer features: whether the current word is within a NE occuring in a given gazetteer.For Chinese, the feature set must be adapted to tackle several additional challenges.
First, unlike European Yu et al., 2006) and for several European languages at CoNLL 2002 ( Wu et al., 2002) and 2003 ( Wu et al., 2003).
Training and evaluation data are drawn from the multilingual Basic Travel Expression Corpus (BTEC*), which contains relatively short sentences used in simple conversations in the travel domain, and their translations in several languages.We participated in the open track of the evaluation campaign, where we were allowed to use only the BTEC* data given for each translation task, plus any other external resources.
The training and evaluation data statistics are given in Table 3 and 4 respectively.
The ChineseEnglish and Japanese-English tasks were provided with twice as many training bisentences as the Arabic-English and Italian-English tasks.
Taking advantage of the fact that BTEC* is a multilingual parallel corpus, all training sets share the same English side.
Similarly, the evaluation test sets are composed of Arabic, Chinese, Italian and Japanese sentences that can all translate to the same English sentence.All training data was clean text, representing a mismatch to the test data used in the evaluation, which was noisy output from automatic speech recognition.
In addition to recognition errors, automatic speech transcriptions do not contain punctuation, and use digits to represent numbers.
Performance could be improved by eliminating the mismatch between training and test data.For each Chinese sentence, we are given correct speech transcriptions as well as automatic read speech transcriptions and automatic spontaneous speech transcriptions.
For the other languages, we only translated the correct and the read speech transcriptions.
For this first IWSLT participation, we did not take advantage of the availability of n-best lists, and only made use of the 1-best transcription, as if the input were text.
For all language pairs, sentence pairs containing multiple segments are split and re-aligned to provide cleaner parallel training data.
After this common processing step, each language followed a minimal language-specific tokenization scheme.English: The English was simply tokenized and case-normalized in the same manner for all languages.Chinese: The Chinese side of the parallel corpora was word segmented using the LDC segmenter.Arabic: In contrast with the 4 other languages considered, Arabic is a morphologically rich language and requires more sophisticated processing.
The Arabic text is first converted to the Buckwalter romanization scheme.
Tokenization and lemmatization are performed using the ASVMT Arabic morphological analysis toolkit (Diab, 2005).
An Arabic word is typically formed of a stem, and possibly affixes and clitics.
Affixes are inflectional markers for tense, gender and/or number, while the clitics include some prepositions, conjunctions, determiners, etc.
Tokenization, which consists of separating those Table 6: Examples of Chinese translations for different input conditions: correct speech transcription (text), read speech transcription (read), and spontaneous speech transcription (spontaneous).
Example 1 Input (text): ïå ÷ Š ` ( å, " 0@ ™ e } Output:Could you please write down the address in Japan, please.
Input (read):ïå ÷î¨(÷î¨÷î¨( å, " 0@ ™ e } Could you please write down the address in Japan, please.
Input (spontaneous):ïå ÷ ž Ù º " 0@ ™ † } Output:May handle, deal with the address of the please.
syntactic units, is the first step of processing in ASVMT.
This is followed by lemmatization which, in ASVMT, refers to a normalization step where the tokens coming from stems that were modified when agglutinated are converted back to their original form.
Italian: We preprocessed the Italian corpus just like the English corpus: it was simply tokenized, using the same rules as for English, and case-normalized.
This is obviously not optimal, as Italian presents more morphological inflexions than English, as suggested by the larger vocabulary size on the Italian side of the training data than on the English side (Table 3).
Japanese: We used the provided word segmentation and did not perform any additional processing.
Table 5 shows the evaluation of translation quality for the Chinese-English translation task, using the most common automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Baner- jee and Lavie, 2005), as well as word error rate (WER) and position-independent word error rate (PER) (Till- mann et al., 1997).
The HKUST system achieves reasonable performance, with evaluation scores situated in the middle range, compared to all systems evaluated on the open track.
As expected, translation quality degrades for all evaluation metrics when moving from correct transcriptions to read and spontaneous speech.
Table 6 shows how differences in the accuracy of speech transcription affects the final translation quality.
In the first example, the spontaneous speech transcription contains a sequence of four incorrect characters ("ž Ù º" instead of the correct "Š ` ( å,"), which makes the translation meaningless.
In contrast, the read speech translation contains only one error: the speech recognizer confuses the more formal word "¨" with the correct word "`".
However, they both translate to the same English word ("you") yielding an acceptable sentence translation despite the speech recognizer error.
The second set of sentences gives an example of a less common case, where the spontaneous speech translation is better than the read speech translation.
The read speech transcription wrongly recognizes the word "Z" ("evening") as "©?"
, which is meaningless and cannot be translated.
Translation results for the other language pairs are reported in Table 7.
Despite the smaller amount of training data available, translating from Italian yields the best performance, since Italian is closer to English than the three other input languages considered.
Table 8 shows sentence translations obtained for all the input languages for a common reference translation.
In these examples, the translation from Italian is usually the best of the four, as shown by the evaluation scores.
Japanese translations seem to be the hardest for the system, with many input words that are not or incorrectly translated, despite a phrasal bilexicon learned on twice as much data as the Italian phrasal bilexicon.
In Chinese, the phrasal lexicon coverage seems better on these sentences, but our phrase-based model fails to accurately capture differences in syntax: in the third example, the Chinese system translates most words correctly but fails to correctly disambiguate the use of the Chinese verb in assertion vs. interrogation.
We have described the design of an approach at HKUST to integrating semantic processing into statistical machine translation, with specific modules for word sense and entity disambiguation and translation, and showed how repurposing the semantic analysis modules for the translation task yields improvements in translation quality.
We discussed results obtained on four different languages in the IWSLT 2006 speech translation tasks, in HKUST's first participation in the IWSLT evaluation campaign.
On the Chinese to English translation task, the system achieved reasonable performance as measured by a set of automatic evaluation metrics.
We also reported results on the Arabic, Italian and Japanese read speech translation tasks, showing that the system is easily portable to other language pairs.
Satanjeev Banerjee and Alon Lavie.
METEOR: An automatic metric for MT evaluation with improved correlation with human judgement.
In Pro-
