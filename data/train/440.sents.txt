We present a method to speed up the dynamic program algorithms used for solving the HMM decoding and training problems for discrete time-independent HMMs.
We discuss the application of our method to Viterbi's decoding and training algorithms [21], as well as to the forward-backward and Baum-Welch [4] algorithms.
Our approach is based on identifying repeated substrings in the observed input sequence.
We describe three algorithms based alternatively on byte pair encoding (BPE) [19], run length encoding (RLE) and Lempel-Ziv (LZ78) parsing [22].
Compared to Viterbi's algorithm, we achieve a speedup of Ω(r) using BPE, a speedup of Ω(r log r) using RLE, and a speedup of Ω(log n k) using LZ78, where k is the number of hidden states, n is the length of the observed sequence and r is its compression ratio (under each compression scheme).
Our experimental results demonstrate that our new algorithms are indeed faster in practice.
Furthermore, unlike Viterbi's algorithm, our algorithms are highly parallelizable.
Over the last few decades, Hidden Markov Models (HMMs) proved to be an extremely useful framework for modeling processes in diverse areas such as errorcorrection in communication links [21], speech recognition [6], optical character recognition [2], computational linguistics [17], and bioinformatics [12].
The core HMM-based applications fall in the domain of classification methods and are technically divided into two stages: a training stage and a decoding stage.
During the training stage, the emission and transition probabilities of an HMM are estimated, based on an input set of observed sequences.
This stage is usually executed once as a preprocessing stage and the generated ("trained") models are stored in a database.
Then, a decoding stage is run, again and again, in order to classify input sequences.
The objective of this stage is to find the most probable sequence of states to have generated each input sequence given each model, as illustrated in Fig. 1.
The HMM on the observed sequence X = x 1 , x 2 , . . . , x n and states 1, 2, . . . , k.
The highlighted path is a possible path of states that generate the observed sequence.
VA finds the path with highest probability.Obviously, the training problem is more difficult to solve than the decoding problem.
However, the techniques used for decoding serve as basic ingredients in solving the training problem.
The Viterbi algorithm (VA) [21] is the best known tool for solving the decoding problem.
Following its invention in 1967, several other algorithms have been devised for the decoding and training problems, such as the forward-backward and Baum-Welch [4] algorithms.
These algorithms are all based on dynamic programs whose running times depend linearly on the length of the observed sequence.
The challenge of speeding up VA by utilizing HMM topology was posed in 1997 by Buchsbaum and Giancarlo [6] as a major open problem.
In this contribution, we address this open problem by using text compression and present the first provable speedup of these algorithms.The traditional aim of text compression is the efficient use of resources such as storage and bandwidth.
Here, however, we compress the observed sequences in order to speed up HMM algorithms.
This approach, denoted "acceleration by text-compression", was previously applied to some classical problems on strings.
Various compression schemes, such as LZ77, LZW-LZ78, Huffman coding, Byte Pair Encoding (BPE) and Run Length Encoding (RLE), were employed to accelerate exact and approximate pattern matching [14,16,19,1,13,18] and sequence alignment [3,7,11,15].
In light of the practical importance of HMM-based classification methods in state-of-the-art research, and in view of the fact that such techniques are also based on dynamic programming, we set out to answer the following question: can "acceleration by text compression" be applied to HMM decoding and training algorithms?Our results.
Let X denote the input sequence and let n denote its length.
Let k denote the number of states in the HMM.
For any given compression scheme, let n denote the number of parsed blocks in X and let r = n/n denote the compression ratio.
Our results are as follows.1.
BPE is used to accelerate decoding by a factor of Ω(r).
2.
RLE is used to accelerate decoding by a factor of Ω( r logr ).3.
Using LZ78, we accelerate decoding by a factor of Ω( log n k ).
Our algorithm guarantees no degradation in efficiency even when k > log n and is experimentally more than five times faster than VA. 4.
The same speedup factors apply to the Viterbi training algorithm.
5.
For the Baum-Welch training algorithm, we show how to preprocess a repeated substring of size once in O(k 4 ) time so that we may replace the usual O(k 2 ) processing work for each occurrence of this substring with an alternative O(k 4 ) computation.
This is beneficial for any repeat with λ nonoverlapping occurrences, such that λ > k 2 −k 2 .
6.
As opposed to VA, our algorithms are highly parallelizable.
This is discussed in the full version of this paper.Roadmap.
The rest of the paper is organized as follows.
In section 2 we give a unified presentation of the HMM dynamic programs.
We then show in section 3 how these algorithms can be improved by identifying repeated substrings.
Two compressed decoding algorithms are given in sections 4 and 5.
In section 6 we show how to adapt the algorithms to the training problem.
Finally, experimental results are presented in Section 7.
Let Σ denote a finite alphabet and let X ∈ Σ n , X = x 1 , x 2 , . . . , x n be a sequence of observed letters.
A Markov model is a set of k states, along with emission probabilities e k (σ) -the probability to observe σ ∈ Σ given that the state is k, and transition probabilities P i,j -the probability to make a transition to state i from state j.The Viterbi Algorithm.
The Viterbi algorithm (VA) finds the most probable sequence of hidden states given the model and the observed sequence.
i.e., the sequence of states s 1 , s 2 , . . . , s n which maximizen i=1 e s i (x i )P s i ,s i−1(1)The dynamic program of VA calculates a vector v t [i] which is the probability of the most probable sequence of states emitting x 1 , . . . , x t and ending with the state i at time t. v 0 is usually taken to be the vector of uniform probabilities(i.e., v 0 [i] = 1 k ).
v t+1 is calculated from v t according to v t+1 [i] = e i (x t+1 ) · max j {P i,j · v t [j]} (2)Definition 1 (Viterbi Step).
We call the computation of v t+1 from v t a Viterbi step.Clearly, each Viterbi step requires O(k 2 ) time.
Therefore, the total runtime required to compute the vector v n is O(nk 2 ).
The probability of the most likely sequence of states is the maximal element in v n .
The actual sequence of states can be then reconstructed in linear time.It is useful for our purposes to rewrite VA in a slightly different way.
Let M σ be a k × k matrix with elements M σ i,j = e i (σ) · P i,j .
We can now express v n as: [20,10] can be used to reduce the k 3 term.
However, for small values of k this is not profitable.v n = M xn M xn−1 · · · M x2 M x1 v 0 (3) where (A B) i,j = max k {A i,k · B k,j } isThe Forward-Backward Algorithms.
The forward-backward algorithms are closely related to VA and are based on very similar dynamic programs.
In contrast to VA, these algorithms apply standard matrix multiplication instead of max-times multiplication.
The forward algorithm calculates f t [i], the probability to observe the sequence x 1 , x 2 , . . . , x t requiring that s t = i as follows:f t = M xt · M xt−1 · · · · · M x2 · M x1 · f 0 (4)The backward algorithm calculates b t [i], the probability to observe the sequence x t+1 , x t+2 , . . . , x n given that s t = i as follows:b t = b n · M x n · M x n−1 · · · · · M x t+2 · M x t+1 (5)Another algorithm which is used in the training stage and employs the forwardbackward algorithm as a subroutine, is the Baum-Welch algorithm, to be further discussed in Section 6.
A motivating example.
We briefly describe one concrete example from computational biology to which our algorithms naturally apply.
CpG islands [5] are regions of DNA with a large concentration of the nucleotide pair CG.
These regions are typically a few hundred to a few thousand nucleotides long, located around the promoters of many genes.
As such, they are useful landmarks for the identification of genes.
The observed sequence (X) is a long DNA sequence composed of four possible nucleotides (Σ = {A, C, G, T }).
The length of this sequence is typically a few millions nucleotides (n 2 25 ).
A well-studied classification problem is that of parsing a given DNA sequence into CpG islands and non CpG regions.
Previous work on CpG island classification used Markov models with either 8 or 2 states (k = 8 or k = 2) [9,12].
Consider a substring W = w 1 , w 2 , . . . , w of X, and defineM (W ) = M w M w −1 · · · M w 2 M w 1 (6)Intuitively, M i,j (W ) is the probability of the most likely path starting with state j, making a transition into some other state, emitting w 1 , then making a transition into yet another state and emitting w 2 and so on until making a final transition into state i and emitting w .
In the core of our method stands the following observation, which is immediate from the associative nature of matrix multiplication.
M w M w −1 · · · M w1 in eq.
(3) with M (W ).
The application of observation 1 to the computation of equation (3) saves − 1 Viterbi steps each time W appears in X, but incurs the additional cost of computing M (W ) once.An intuitive exercise.
Let λ denote the number of times a given word W appears, in non-overlapping occurrences, in the input string X. Suppose we na¨ıvelyna¨ıvely compute M (W ) using (|W | − 1) max-times matrix multiplications, and then apply observation 1 to all occurrences of W before running VA.
We gain some speedup in doing so if(|W | − 1)k 3 + λk 2 < λ|W |k 2 λ > k(7)Hence, if there are at least k non-overlapping occurrences of W in the input sequence, then it is worthwhile to na¨ıvelyna¨ıvely precompute M (W ), regardless of it's size |W |.
Definition 2 (Good Substring).
We call a substring W good if we decide to compute M (W ).
We can now give a general four-step framework of our method:(I) Dictionary Selection: choose the set D = {W i } of good substrings.
(II) Encoding: precompute the matrices M (W i ) for every W i ∈ D. (III) Parsing: partition the input sequence X into consecutive good sub- strings X = W i 1 W i 2 · · · W i n and let X denote the compressed repre- sentation of this parsing of X, such that X = i 1 i 2 · · · i n .
(IV) Propagation: run VA on X , using the matrices M (W i ).
The above framework introduces the challenge of how to select the set of good substrings (step I) and how to efficiently compute their matrices (step II).
In the next two sections we show how the RLE and LZ78 compression schemes can be applied to address this challenge.
The utilization of the BPE compression scheme is discussed in the full version of this paper.
Another challenge is how to parse the sequence X (step III) in order to maximize acceleration.
We show that, surprisingly, this optimal parsing may differ from the initial parsing induced by the selected compression scheme.
To our knowledge, this feature was not applied by previous "acceleration by compression" algorithms.Throughout this paper we focus on computing path probabilities rather than the paths themselves.
The actual paths can be reconstructed in linear time as described in the full version of this paper.
In this section we obtain an Ω( r logr ) speedup for decoding an observed sequence with run-length compression ratio r.
A string S is run-length encoded if it is described as an ordered sequence of pairs (σ, i), often denoted "σ i ".
Each pair corresponds to a run in S, consisting of i consecutive occurrences of the character σ.
For example, the string aaabbcccccc is encoded as a 3 b 2 c 6 .
Run-length encoding serves as a popular image compression technique, since many classes of images (e.g., binary images in facsimile transmission or for use in optical character recognition) typically contain large patches of identically-valued pixels.
The fourstep framework described in section 3 is applied as follows.
(I) Dictionary Selection: for every σ ∈ Σ and every i = 1, 2, . . . , log n we choose σ 2 i as a good substring.
(II) Encoding: since M (σ 2 i ) = M (σ 2 i−1 ) M (σ 2 i−1 ), we can compute the matrices using repeated squaring.
(III) Parsing: Let W 1 W 2 · · · W n be the RLE of X, where each W i is a run of some σ ∈ Σ.
X is obtained by further parsing each W i into at most log |W i | good substrings of the form σ 2 j .
(IV) Propagation: run VA on X , as described in Section 3.
Time and Space Complexity Analysis.
The offline preprocessing stage consists of steps I and II.
The time complexity of step II is O(|Σ|k 3 log n) by applying maxtimes repeated squaring in O(k 3 ) time per multiplication.
The space complexity is O(|Σ|k 2 log n).
This work is done offline once, during the training stage, in advance for all sequences to come.
Furthermore, for typical applications, the O(|Σ|k 3 log n) term is much smaller than the O(nk 2 ) term of VA.Steps III and IV both apply one operation per occurrence of a good substring in X : step III computes, in constant time, the index of the next parsing-comma, and step IV applies a single Viterbi step in k 2 time.Since |X | = n i=1 log|W i |, the complexity is n i=1 k 2 log|W i | = k 2 log(|W 1 | · |W 2 | · · · |W n |) ≤ k 2 log((n/n ) n ) = O(n k 2 log n n ).
Thus, the speedup compared to the O(nk 2 ) time of VA is Ω( n n log n n ) = Ω( r logr ).
In this section we obtain an Ω( log n k ) speedup for decoding, and a constant speedup in the case where k > log n.
We show how to use the LZ78 [22] (henceforth LZ) parsing to find good substrings and how to use the incremental nature of the LZ parse to compute M (W ) for a good substring W in O(k 3 ) time.LZ parses the string X into substrings (LZ-words) in a single pass over X. Each LZ-word is composed of the longest LZ-word previously seen plus a single letter.
More formally, LZ begins with an empty dictionary and parses according to the following rule: when parsing location i, look for the longest LZ-word W starting at position i which already appears in the dictionary.
Read one more letter σ and insert W σ into the dictionary.
Continue parsing from position i + |W | + 1.
For example, the string "AACGACG" is parsed into four words: A, AC, G, ACG.
Asymptotically, LZ parses a string of length n into O(hn/ log n) words [22], where 0 ≤ h ≤ 1 is the entropy of the string.
The LZ parse is performed in linear time by maintaining the dictionary in a trie.
Each node in the trie corresponds to an LZ-word.
The four-step framework described in section 3 is applied as follows.
Time and Space Complexity Analysis.
Steps I and III were already conducted offline during the pre-processing compression of the input sequences (in any case LZ parsing is linear).
In step II, computing M (W σ) = M (W ) M σ , takes O(k 3 ) time since M (W ) was already computed for the good substring W .
Since there are O(n/ log n) LZ-words, calculating the matrices M (W ) for all W s takes O(k 3 n/ log n).
Running VA on X (step IV) takes just O(k 2 n/ log n) time.
Therefore, the overall runtime is dominated by O(k 3 n/ log n).
The space complexity is O(k 2 n/ log n).
The above algorithm is useful in many applications, such as CpG island classification, where k < log n. However, in those applications where k > log n such an algorithm may actually slow down VA.We next show an adaptive variant that is guaranteed to speed up VA, regardless of the values of n and k.
This graceful degradation retains the asymptotic Ω( log n k ) acceleration when k < log n. Recall that given M (W ) for a good substring W , it takes k 3 time to calculate M (W σ).
This calculation saves k 2 operations each time W σ occurs in X in comparison to the situation where only M (W ) is computed.
Therefore, in step I we should include in D, as good substrings, only words that appear as a prefix of at least k LZ-words.
Finding these words can be done in a single traversal of the trie.
The following observation is immediate from the prefix monotonicity of occurrence tries.Observation 2.
Words that appear as a prefix of at least k LZ-words are represented by trie nodes whose subtrees contain at least k nodes.In the previous case it was straightforward to transform X into X , since each phrase p in the parsed sequence corresponded to a good substring.
Now, however, X does not divide into just good substrings and it is unclear what is the optimal way to construct X (in step III).
Our approach for constructing X is to first parse X into all LZ-words and then apply the following greedy parsing to each LZ-word W : using the trie, find the longest good substring w ∈ D that is a prefix of W , place a parsing comma immediately after w and repeat the process for the remainder of W .
Time and Space Complexity Analysis.
The improved algorithm utilizes substrings that guarantee acceleration (with respect to VA) so it is therefore faster than VA even when k = Ω(log n).
In addition, in spite of the fact that this algorithm re-parses the original LZ partition, the algorithm still guarantees an Ω( log n k ) speedup over VA as shown by the following lemma.
Lemma 1.
The running time of the above algorithm is bounded by O(k 3 n/log n).
Proof.
The running time of step II is at most O(k 3 n/ log n).
This is because the size of the entire LZ-trie is O(n/ log n) and we construct the matrices, in O(k 3 ) time each, for just a subset of the trie nodes.
The running time of step IV depends on the number of new phrases (commas) that result from the re-parsing of each LZ-word W .
We next prove that this number is at most k for each word.Consider the first iteration of the greedy procedure on some LZ-word W .
Let w be the longest prefix of W that is represented by a trie node with at least k descendants.
Assume, contrary to fact, that |W | − |w | > k.
This means that w , the child of w , satisfies |W | − |w | ≥ k, in contradiction to the definition of w .
We have established that |W | − |w | ≤ k and therefore the number of re-parsed words is bounded by k + 1.
The propagation step IV thus takes O(k 3 ) time for each one of the O(n/ log n) LZ-words.
So the total time complexity remains O(k 3 n/ log n).
Based on Lemma 1, and assuming that steps I and III are pre-computed offline, the running time of the above algorithm is O(nk 2 /e) where e = Ω(max(1, log n k )).
The space complexity is O(k 2 n/logn).
In the training problem we are given as input the number of states in the HMM and an observed training sequence X.
The aim is to find a set of model parameters θ (i.e., the emission and transition probabilities) that maximize the likelihood to observe the given sequence P (X| θ).
The most commonly used training algorithms for HMMs are based on the concept of Expectation Maximization.
This is an iterative process in which each iteration is composed of two steps.
The first step solves the decoding problem given the current model parameters.
The second step uses the results of the decoding process to update the model parameters.
These iterative processes are guaranteed to converge to a local maximum.
It is important to note that since the dictionary selection step (I) and the parsing step (III) of our algorithm are independent of the model parameters, we only need run them once, and repeat just the encoding step (II) and the propagation step (IV) when the decoding process is performed in each iteration.
The first step of Viterbi training [12] uses VA to find the most likely sequence of states given the current set of parameters (i.e., decoding).
Let A ij denote the number of times the state i follows the state j in the most likely sequence of states.
Similarly, let E i (σ) denote the number of times the letter σ is emitted by the state i in the most likely sequence.
The updated parameters are given by:P ij = A ij i A i j and e i (σ) = E i (σ) σ E i (σ ) (8)Note that the Viterbi training algorithm does not converge to the set of parameters that maximizes the likelihood to observe the given sequence P (X| θ) , but rather the set of parameters that locally maximizes the contribution to the likelihood from the most probable sequence of states [12].
It is easy to see that the time complexity of each Viterbi training iteration is O(k 2 n + n) = O(k 2 n) so it is dominated by the running time of VA.
Therefore, we can immediately apply our compressed decoding algorithms from sections 4 and 5 to obtain a better running time per iteration.
The Baum-Welch training algorithm [4,12] converges to a set of parameters that locally maximize the likelihood to observe the given sequence P (X| θ), and is the most commonly used method for model training.
We give here a brief explanation of the algorithm and of our acceleration approach.
The complete details appear in the full version of this paper.Recall the forward-backward matrices: f t [i] is the probability to observe the sequence x 1 , x 2 , . . . , x t requiring that the t'th state is i and that b t [i] is the probability to observe the sequence x t+1 , x t+2 , . . . , x n given that the t'th state is i.
The first step of Baum-Welch calculates f t [i] and b t [i] for every 1 ≤ t ≤ n and every 1 ≤ i ≤ k.
This is achieved by applying the forward and backward algorithms to the input data in O(nk 2 ) time (see eqs.
(4) and (5)).
The second step recalculates A and E according toA i,j = t P (s t = j, s t+1 = i|X, θ) E i (σ) = t|x t =σ P (s t = i|X, θ)(9)where P (s t = j, s t+1 = i|X, θ) is the probability that a transition from state j to state i occurred in position t in the sequence X, and P (s t = i|X, θ) is the probability for the t'th state to be i in the sequence X.
These quantities are given by:P (s t = j, s t+1 = i|X, θ) = f t [j] · P i,j · e i (x t+1 ) · b t+1 [i] i f n [i](10)andP (s t = i|X, θ) = f t [i] · b t [i] i f n [i] .
(11)Finally, after the matrices A and E are recalculated, Baum-Welch updates the model parameters according to equation (8).
We next describe how to accelerate the Baum-Welch algorithm.
Note that in the first step of Baum-Welch, our algorithms to accelerate VA (Sections 4 and 5) can be used to accelerate the forward-backward algorithms by replacing the max-times matrix multiplication with regular matrix multiplication.
However, the accelerated algorithms only compute f t and b t on the boundaries of good substrings.
In order to solve this problem and speed up the second step of Baum-Welch as well, we observe that when accumulating the contribution of some appearance of a good substring W of length |W | = to A, Baum-Welch performs O(k 2 ) operations, but updates at most k 2 entries (the size of A).
Hence, it is possible to obtain a speedup by precalculating the contribution of each good substring to A and E. For brevity the details are omitted here and will appear in the full version of this paper.
To summarize the results, preprocessing a good substring W requires O(k 4 ) time and O(k 4 ) space.
Using the preprocessed information and the values of f t and b t on the boundaries of good substrings, we can update A and E in O(k 4 ) time per good substring (instead of k 2 ).
To get a speedup we need λ, the number of times the good substring W appears in X to satisfy:k 4 + λk 4 < λλk 2 λ > k 2 − k 2(12)This is reasonable if k is small.
If = 2k 2 , for example, then we need λ to be greater than 2k 2 .
In the CpG islands problem, if k = 2 then any substrings of length eight is good if it appears more than eight times in the text.
We implemented both a variant of our improved LZ-compressed algorithm from subsection 5.1 and classical VA in C++ and compared their execution times on a sequence of approximately 22,000,000 nucleotides from the human Y chromosome and on a sequence of approximately 1,500,000 nucleotides from chromosome 4 of S. Cerevisiae obtained from the UCSC genome database.
The benchmarks were performed on a single processor of a SunFire V880 server with 8 UltraSPARC-IV processors and 16GB main memory.
The implementation is just for calculating the probability of the most likely sequence of states, and does not traceback the optimal sequence itself.
As we have seen, this is the time consuming part of the algorithm.
We measured the running times for different values of k.
As we explained in the previous sections we are only interested in the running time of the encoding and the propagation steps (II and IV) since the combined parsing/dictionary-selections steps (I and III) may be performed in advance and are not repeated by the training and decoding algorithms.
The results are shown in Fig. 2.
Our algorithm performs faster than VA even for surprisingly large values of k. For example, for k = 60 our algorithm is roughly three times faster than VA.
