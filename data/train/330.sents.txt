In this position paper, we first review the state-of-the-art in graph-based semi-supervised learning, and point out three limitations that are particularly relevant to multimedia analysis: (1) rich data is restricted to live on a single manifold; (2) learning must happen in batch mode; and (3) the target label is assumed smooth on the manifold.
We then discuss new directions in semi-supervised learning research that can potentially overcome these limitations: (i) modeling data as a mixture of multiple manifolds that may intersect or overlap; (ii) online semi-supervised learning that learns incrementally with low computation and memory needs; and (iii) learning spectrally sparse but non-smooth labels with compressive sensing.
We give concrete examples in each new direction.
We hope this article will inspire new research that makes semi-supervised learning an even more valuable tool for multimedia analysis.
Semi-supervised learning encompasses many different model assumptions [1,2].
Graph-based semi-supervised learning is an important family of methods that make the following common assumption.
Let x i , x j ∈ X be two input items, and y i , y j ∈ Y be their labels.
Usually, X ⊆ R D and Y = {−1, 1} for binary classification, but multiclass classification and regression are common, too.
Let d(·, ·) be an appropriate distance measure on X .
The graph-based assumption states that if d(x i , x j ) is small, then y i ≈ y j .
This assumption applies regardless of whether x i , x j are labeled.
If x 1 is labeled, and there is a sequence of unlabeled items x 2 , . . . , x k such that d(x i , x i+1 ) is small for i = 1 . . . k − 1, then the label y 1 will propagate along the sequence.Formally, a graph is formed with nodes being labeled data {(x i , y i )} n i=1 and unlabeled data {x i } n+m i=n+1 .
The undirectedWe would like to thank the Wisconsin Alumni Research Foundation.
AG is supported in part by a Yahoo! Key Technical Challenges Grant.edges reflect similarity between nodes: the edge weight w ij between nodes x i , x j is large if d(x i , x j ) is small.
A common choice of edges is to connect each node to its kNNs with weight 1, and disconnect it from all other nodes (weight 0).
Let W be the (n + m) × (n + m) weight matrix, and D the diagonal degree matrix with D ii = j w ij .
Let L = D − W bef Lf = 1/2 i,j (f (x i ) − f (x j )) 2 w ij .
This assumption is behind graph-based semi-supervised methods such as Mincut [3], graph random walk [4], Gaussian Random Fields [5], local and global consistency [6], spectral graph transducer [7], manifold regularization [8,9], and many other variants.
In particular, Belkin et al. generalize graph-based learning to the manifold setting, where X is assumed to be a low dimensional manifold in R D , the labels y change smoothly on the manifold, and the graph constructed on labeled and unlabeled training data is a random realization of the manifold.
This provides an elegant conceptual model.
The graph-based assumption has been extended to directed edges like links between Web pages [10] and dissimilarity edges [11,12].
Applications of graph-based semi-supervised learning abound.Despite their success, we point out three major limitations of graph-based methods:(1) Current methods assume that X is a single manifold, or multiple well-separated manifolds.
Therefore, it makes sense to create the graph W using kNN edges, or Gaussian weighted edges w ij = exp −λd(x i , x j ) 2 , where d(·, ·) is based on Euclidean distance.
In both cases, nearby nodes are strongly connected and are assumed to have similar labels.
However, in multimedia data, the distribution of objects might form multiple manifolds that intersect or partially overlap with each other.
For example, in motion segmentation from video images, the tracked feature points on different objects form multiple intersecting and overlapping manifolds [13].
Even though each individual manifold obeys the label smoothness assumption, nearby items on different manifolds may not satisfy this assumption.
Straightforward application of existing graph-based semi-supervised learning will not achieve optimal performance.
(2) Current methods learn in batch mode.
That is, they require the training set to be available all at once.
However, consider a robot with a camera that continuously takes video of its surroundings, and learns the names of various objects.
A human annotator provides object names (labels) only occasionally on selected video frames.
This is therefore semisupervised learning.
But the robot cannot afford to store the massive amount of mostly unlabeled video before learning.
It requires an "anytime classifier" that is ready at all times, while continuously improving itself.
And training must be cheap and quick.
What we need is semi-supervised learning that operates in online mode.
(3) Current methods assume label smoothness on the graph.
As dissimilarity edges show, this may not always be the case [11,12].
In general, the relation between the label and the underlying graph can be studied from the perspective of harmonic analysis.
It is well-known that the traditional smoothness assumption is equivalent to favoring low frequency components of the graph spectrum [14].
Recent advances in compressive sensing (see, e.g., [15]) allow learning from an arbitrary combination of low and high frequency components, as long as the number of components is small.
We present, to our knowledge, the first connection between compressive sensing and graph-based transduction.
We recently introduced a novel graph as a first step in addressing data containing a mixture of manifolds [16].
The idea is to assign edge weights based on differences in local geometry around each item x.
Our intuition is that items on different manifolds, or in regions with different density, should be considered dissimilar and lead to low edge weights.
Computationally, we compare local regions using Hellinger distance, which is sensitive to local manifold structures.
We start by estimating the local sample covariance matrix Σ x around a randomly selected set of anchor items x. Then, the squared Hellinger distance between two anchor pointsx i , x j is H 2 (p, q) = 1 2 p(x) − q(x) 2dx, where p = N (x; 0, Σ xi ) and q = N (x; 0, Σ xj ) are zero mean Gaussians with those local sample covariance matrices.
The Hellinger distance H is symmetric, in [0,1], small when the local geometry is similar, and large when there is significant difference in density, manifold dimensionality or orientation (see Figure 2 in [16]).
Finally, we build a sparse kNN graph over the labeled and anchor unlabeled items as follows: Each such x is connected by a weighted, undirected edge to its k nearest Mahalanobis neighbors.
Note that, since Σ x captures the local geometry around x, we "follow the manifold" by using the Mahalanobis distance as the local distance metric at x: d 2 M (x, x ) = (x − x ) Σ −1 x (x − x ).
The neighborhood size k is set to grow with dataset size.
The graph edges are weighted using the standard RBF scheme, but with Hellinger distance: w ij = exp −λH 2 (p, q) .
See Figure 3 in [16] for an example graph using this weighting scheme.
In short, the graph combines locality and geometry: an edge has large weight when the two nodes are close in Mahalanobis distance, and have similar covariance structure.
Importantly, it effectively separates intersecting and overlapping manifolds into individual pieces.We demonstrate the effectiveness of this Hellinger graph with manifold regularization [8] on two synthetic datasets.
Dollar sign is a regression dataset containing two intersecting manifolds with target values varying greatly across intersection points (Figure 1(a), color indicates y).
Surfacehelix is a classification dataset with a 1D toroidal helix intersecting a surface-each manifold is a separate class (Fig- ure 1(b)).
Figure 1 compares three learners on these datasets: [Supervised]: supervised learner (kernel regression or SVM) trained on labeled data only, ignoring unlabeled data.
[MR]: standard manifold regularization (LapRLS or LapSVM) using a Euclidean-based 3NN graph [8].
[Hellinger-MR]: manifold regularization (LapRLS or LapSVM) using this novel Hellinger graph.
See [16] for details about the parameters governing the Hellinger graph.
All other parameters were tuned using 5-fold cross validation.
All datasets start with M = 20, 000 unlabeled items, from which we select m ∼ O(M/log(M )) anchor items.
Figure 1 shows performance on a separate test set of 20,000 items, averaged over 10 trials.
For the dollar sign data, standard MR performs only as well as supervised learning, while Hellinger-MR achieves statistically significantly better MSE in all four n conditions (based on paired t-tests).
For the surface-helix data, the three methods are all statistically significantly different for all n, with Hellinger-MR making the best use of unlabeled data.Open issues surrounding our Hellinger graph remain, including how to select anchors, and how to optimize parameters.
Furthermore, some other metrics over matrices or probability distributions may be more appropriate than Hellinger distance for this purpose.
Finally, it could be useful to exploit the labeled data for detecting and validating the presence of multiple manifolds with differing target values.
We argue that the following is an important and practical setting, especially for real-time multimedia applications:1.
At time t an adversary picks (x t , y t ) and shows x t .
2.
The learner predicts f t (x t ).3.
With (small) probability p the adversary reveals y t .
Otherwise x t remains unlabeled.
4.
The learner updates its predictor to f t+1 , even when y t is not given.
Repeat with t = t + 1.
This is clearly online learning.
It differs from the standard online setting in that learning happens even on unlabeled data.
The goal is to update f t in such a way that there is no regret, i.e., the wrong predictions the online procedure makes over time are comparable to a batch learner, which has access to the same input simultaneously but has to use the single best fixed predictor.
A good online semi-supervised learning algorithm should achieve zero regret with sublinear space and time complexity.
As a concrete example, our online semi-supervised algorithm in [17] employs online convex programming with an asymptotic zero-regret guarantee.
At the heart of the algorithm is a gradient step in kernel spacef t+1 = f t − η t ∂Jt(f ) ∂f ft, where η t is a stepsize that decays asO(1/ √ t).
The term J t (f ) is the instantaneous risk functional.
When summing over time, we recover the standard batch manifold regularization risk J(f ) = 1 n+m n+m t=1 J t (f ).
The definition of J t (f ) can be found in [17]; suffice it to say that the instantaneous graph energy ist−1 i=1 (f (x i ) − f (x t )) 2 w it .
That is, it involves the edges from x t to all previous nodes in the graph.
However, its complexity grows linearly with t.
One approximation is to use a buffer of fixed size τ :t τ t−1 i=t−τ (f (x i ) − f (x t )) 2 w it .
That is, old nodes from τ steps ago are discarded.
Figure 2 compares batch vs. online semi-supervised learning (manifold regularization)'s running time and test error on MNIST digit recognition 1 vs. 2.
The online algorithm achieves a desirable constant learning complexity at each step, and has comparable accuracy as batch mode.Keeping a fixed buffer of recent input is not optimal ultimately.
The dynamic graph constructed on items in the buffer only reflects a random and noisy snapshot of the underlying manifold structure.
Given the same space constraint, it is better to form a summary of all the input so far.
One possibility is some form of online clustering that forms a mixture model (e.g., Gaussian mixtures) on the input, using a fixed number of mixing components, as in Figure 2(right).
A "hyper-graph" can then be maintained on the mixture model, with nodes being the components.
Graph-based learning proceeds on the hyper-graph, which is a fixed-size summary of the manifold seen so far.
This is the idea behind the use of Random Projection Trees in [17]; see also [18].
Looking forward, we need more efficient online semisupervised algorithms with theoretical guarantees.
The combination of online semi-supervised learning and online active learning also deserves attention.
The spectrum of the graph is the set of eigenvalue, eigenvector pairs{(λ i , ψ i )} n+m i=1, where the Laplacian L = i λ i ψ i ψ i .
If we sort λ from small to large, then λ 1 = . . . = λ k = 0 if and only if the graph has k disconnected components.
The eigenvectors Ψ = {ψ i } form an orthonormal basis.
Any target label function on the graph can be decomposed into f = i α i ψ i , where ψ 1 corresponds to the lowest frequency component, and ψ n+m the highest frequency component.
The function's energy can be shown to be f Lf = i λ i α 2 i .
Existing semi-supervised learning algorithms assume that f is smooth with respect to the graph.
This is equivalent to assuming large (non-zero) α's for small i, and small (zero) α's for large i.
In the future, one may wish to allow non-smooth labels f to model richer data, which must still be learnable from a small number of labeled points.
Compressive sensing offers such guarantee if f is spectrally sparse, i.e., if only S n + m of the α's are non-zero.
These S non-zero components can occupy any frequency, thus allowing non-smooth f and generalizing the graph smoothness assumption.Our key insight is that transductive learning on graphs corresponds to compressive sensing using the (n + m) × (n + m) canonical basis Φ = I.
The n × (n + m) sensing matrix consists of n random rows selected from Φ.
The sensing matrix simply reads out the label values at n nodes; these correspond to the n labeled data items.
Importantly, when the graph is "nice," i.e., without small (nearly) disconnected components, the graph spectrum basis Ψ is incoherent with the canonical basis Φ.
This allows the exact recovery of the whole f from the n observations when n ≥ Cµ 2 (Φ, Ψ)S log(n + m), where µ(Φ, Ψ) = √ n + m max i,j |φ i ψ j | is the coherence (the lower the better).
We now give a concrete example.
Consider a closed chain graph (i.e., a ring) with n+m = 1024 nodes and edge weights 1.
The graph spectrum Ψ is the discrete Fourier basis, whose coherence with the canonical basis is √ 2.
Let S = 3, and f = −ψ 5 − 1.3ψ 8 + ψ 63 , which is spectrally sparse yet nonsmooth as shown in Figure 3(left).
We take n measurements Fig. 3.
Compressive sensing on a closed-chain graph from the canonical basis (i.e., select labeled items); thus we have labels y = f on those n random nodes.
We then solve the standard 1 minimization problem to recoverˆαrecoverˆ recoverˆα (thusˆfthusˆ thusˆf on the whole graph).
We vary n from 1 to 60.
For each n we run 100 trials; each takes n random rows from Φ to form the sensing matrix.
For each trial, we compute the recovery error f − ˆ f 2 /f 2 .
Each trial is a dot in Figure 3(right).
It seems exact recovery happens when n > 35 for this graph.Much work remains in improving this novel way of performing transduction on a graph.
Potential research directions include: identifying real-world problems with spectrally sparse labels on graphs, finding bases that are more localized than the Laplacian spectrum yet still incoherent with the canonical sensing basis, and studying label acquisition mechanisms when the sensing basis is not canonical (e.g., random matrices).
We have presented three new research directions for graphbased semi-supervised learning and our initial approaches at solving these novel problems.
We hope this article will inspire new research, making semi-supervised learning an even more valuable tool for multimedia analysis.
