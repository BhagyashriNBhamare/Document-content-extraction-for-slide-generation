Wh¯ anau is a novel routing protocol for distributed hash tables (DHTs) that is efficient and strongly resistant to the Sybil attack.
Wh¯ anau uses the social connections between users to build routing tables that enable Sybil-resistant lookups.
The number of Sybils in the social network does not affect the protocol's performance, but links between honest users and Sybils do.
When there are n well-connected honest nodes, Wh¯ anau can tolerate up to O(n/ log n) such "attack edges".
This means that an adversary must convince a large fraction of the honest users to make a social connection with the adversary's Sybils before any lookups will fail.
Wh¯ anau uses ideas from structured DHTs to build routing tables that contain O(√ n log n) entries per node.
It introduces the idea of layered identifiers to counter clustering attacks, a class of Sybil attacks challenging for previous DHTs to handle.
Using the constructed tables, lookups provably take constant time.
Simulation results, using social network graphs from LiveJournal, Flickr, YouTube, and DBLP, confirm the analytic results.
Experimental results on PlanetLab confirm that the protocol can handle modest churn.
Decentralized systems on the Internet are vulnerable to the "Sybil attack", in which an adversary creates numerous false identities to influence the system's behavior [9].
This problem is particularly pernicious when the system is responsible for routing messages amongst nodes, as in the Distributed Hash Tables (DHT) [24] which underlie many peer-to-peer systems, because an attacker can prevent honest nodes from communicating altogether [23].
If a central authority certifies identities as genuine, then standard replication techniques can be used to fortify these protocols [4,20].
However, the cost of universal strong identities may be prohibitive or impractical.
Instead, recent work [27,26,8,19,17,5] proposes using the weak identity information inherent in a social network to produce a completely decentralized system.
This paper resolves an open problem by demonstrating an efficient, structured DHT that enables honest nodes to reliably communicate despite a concerted Sybil attack.To solve this problem, we build on a social network composed of individual trust relations between honest people (nodes).
This network might come from personal or business connections, or it might correspond to something more abstract, such as ISP peering relationships.
We presume that each participant keeps track of its immediate neighbors, but that there is no central trusted node storing a map of the network.An adversary can infiltrate the network by creating many Sybil nodes (phoney identities) and gaining the trust of honest people.
Nodes cannot directly distinguish Sybil identities from genuine ones (if they could, it would be simple to reject Sybils).
As in previous work [27], we assume that most honest nodes have more social connections to other honest nodes than to Sybils; in other words, the network has a sparse cut between the honest nodes and the Sybil nodes.In the context of a DHT, the adversary cannot prevent immediate neighbors from communicating, but can try to disrupt the DHT by creating many Sybil identities which spread misinformation.
Suppose an honest node u wants to find a key y and will recognize the corresponding value (e.g., a signed data block).
In a typical structured DHT, u queries another node which u believes to be "closer" to y, which forwards to another even-closer node, and so on until y is found.
The Sybil nodes can disrupt this process by spreading false information (e.g., that they are close to a particular key), then intercepting honest nodes' routing queries, and responding with "no such key" or delaying queries endlessly.
Unstructured protocols that work by flooding or gossip are more robust against these attacks, but pay a performance price, requiring linear time to find a key.
This paper's main contribution is Wh¯ anau 1 , a novel protocol that is the first solution to Sybil-proof routing that has sublinear run time and space usage.
Wh¯ anau achieves this performance by combining the idea of random walks from recent work [26] with a new way of constructing IDs, which we call layered identifiers.
To store up to k keys per node, Wh¯ anau builds routing tables with O( √ kn log n) entries per node.
Using these routing tables, lookups provably take O(1) time.
Thus, Wh¯ anau's security comes at low cost: it scales similarly to one-hop DHTs that provide no security [11,10].
We have implemented Wh¯ anau in simulation and in a simple instant-messaging application running on PlanetLab [2].
Experiments with real-world social graphs and these implementations confirm Wh¯ anau's theoretical properties.Wh¯ anau provides one-hop lookups, but our implementation is not aware of network locality.
Wh¯ anau also must rebuild its routing tables periodically to handle churn in the social network and in the set of keys stored in the DHT.
However, its routing tables are sufficiently redundant that nodes simply going up and down doesn't impact lookups, as long as enough honest nodes remain online.The rest of the paper is organized as follows.
Section 2 summarizes the related work.
Section 3 informally states our goals.
Section 4 states our assumptions about the social network, and provides a precise definition of "Sybilproof".
Section 5 gives an overview of Wh¯ anau's routing table structure and introduces layered IDs.
Section 6 describes Wh¯ anau's setup and lookup procedures in detail.
Section 7 states lemmas proving Wh¯ anau's good performance.
Section 8 describes Wh¯ anau's implementation on PlanetLab [2] and in a simulator.
Using these implementations Section 9 confirms its theoretical properties by simulations on social network graphs from popular Internet services, and investigates its reaction to churn on PlanetLab.
Section 10 discusses engineering details and ideas for future work, and Section 11 summarizes.
Shortly after the introduction of scalable peer-to-peer systems based on DHTs, the Sybil attack was recognized as a challenging security problem [9,16,23,22].
A number of techniques [4,20,22] have been proposed to make DHTs resistant to a small fraction of Sybil nodes, but all such systems ultimately rely on a certifying authority to perform admission control and limit the number of Sybil identities [9,21,3].
Several researchers [17,19,8,5] proposed using social network information to fortify peer-to-peer systems against the Sybil attack.
The bootstrap graph model [8] introduced a correctness criterion for secure routing using a social network and presented preliminary progress towards that goal, but left a robust and efficient protocol as an open problem.Recently, SybilGuard and SybilLimit [27,26] have shown how to use a "fast mixing" social network and random walks on these networks (see Section 4.1) to defend against the Sybil attack in general decentralized systems.
Using SybilLimit, an honest node can certify other nodes as "probably honest", accepting at most O(log n) Sybil identities per attack edge.
(Each certification uses O( √ n) bandwidth.)
For example, SybilLimit's vetting procedure can be used to check that at least one of a set of storage replicas is likely to be honest.
A few papers have adapted the idea of random walks for purposes other than SybilLimit.
Nguyen et al.
for Sybil-resilient content rating [25], Yu et al. applied it to recommendations [28], and Danezis and Mittal used it for Bayesian inference of Sybils [7].
This paper is the first to use random walks to build a Sybil-proof DHT.
2 As illustrated in Figure 1, the Wh¯ anau protocol is a pair of procedures SETUP and LOOKUP.
SETUP(·) is used both to build routing tables and to insert keys.
It cooperatively transforms the nodes' local parameters (e.g. keyvalue records, social neighbors) into a set of routing table structures stored at each node.
After all nodes complete the SETUP phase, any node u can call LOOKUP(u, key) to use these routing tables to find the target value.
We illustrate Wh¯ anau with a simple instant messaging (IM) application which we have implemented on PlanetLab.
Wh¯ anau provides a rendezvous service for the IM clients.
Each user is identified by a public key, and publishes a single self-signed tuple (public key, current IP address) into the DHT.
3 To send an IM to a buddy identified by the public key P K, a client looks up P K in the DHT, verifies the returned tuple's signature using P K, and then sends a packet to that IP address.
In our implementation, each user runs a Wh¯ anau node which stores that user's record, maintains contact with the user's social neighbors, and contributes to the DHT.
(In this example, each node stores a single key-value record, but in general, there may be an arbitrary number k of keys stored per node.)
When the user changes location, the client updates the user's record with the new IP address.
The user's DHT node need not be continuously available when the user is offline, as long as a substantial fraction of honest nodes are available at any given time.
Wh¯ anau handles adversaries who deviate from the protocol in Byzantine ways: the adversaries may make up arbitrary responses to queries from honest nodes and may create any number of pseudonyms (Sybils) which are indistinguishable from honest nodes.
When we say that Wh¯ anau is "Sybil-proof", we mean that LOOKUP has a high probability of returning the correct value, despite arbitrary attacks during both the SETUP and LOOKUP phases.
(Section 4 makes this definition more precise.)
The adversary can always join the DHT normally and insert arbitrary key-value pairs, including a different value for a key already in the DHT.
Thus, Wh¯ anau provides availability, but not integrity: LOOKUP should find all values inserted by honest nodes for the specified key, but may also return some values inserted by the adversary.
Integrity is an orthogonal concern of the application: for example, the IM application filters out any bad values by verifying the signature on the returned key-value records, and ignoring records with invalid signatures.
(As an optimization, DHT nodes may opt to discard bad records proactively, since they are of no use to any client and consume resources to store and transmit.)
Simply flooding LOOKUP queries over all links of the social network is Sybil-resistant, but not efficient [8].
The adversary's nodes might refuse to forward queries, or they might reply with bogus values.
However, if there exists any path of honest nodes between the source node and the target key's node through the social network, then the adversary cannot prevent each of these nodes from forwarding the query to the next.
In this way, the query will always reach the target node, which will reply with the correct value.
Unfortunately, a large fraction of the participating nodes are contacted for every lookup, doing O(n) work each time.On the other hand, known one-hop DHTs are very efficient -requiring O(1) messages for lookups and O( √ n) table sizes 4 -but not secure against the Sybil attack.
Our goal is to combine this optimal efficiency with provable security.
As a matter of policy and fairness, we believe that a node's table size and bandwidth consumption should be proportional to the node's degree (i.e., highly connected nodes should do more work than casual participants).
While it is possible to adapt Wh¯ anau to different policies, this paper assumes that the goal is a proportional policy.
Like previous work [27,26], Wh¯ anau relies on certain features of social networks.
This section describes our assumptions, outlines why they are useful, and defines what it means for a DHT to be "Sybil-proof" under these assumptions.
4 If n = 5 × 10 8 , the approximate number of Internet hosts in 2010, then a table of √ n may be acceptable for bandwidth and storage constrained devices, as opposed to a table that scales linearly with the number of hosts.
The social network is an undirected graph whose nodes know their immediate neighbors.
Figure 2 conceptually divides the social network into two parts, an honest region containing all honest nodes and a Sybil region containing all Sybil identities.
An attack edge is a connection between a Sybil node and an honest node.
An honest edge is a connection between two honest nodes [27].
An "honest" node whose software's integrity has been compromised by the adversary is considered a Sybil node.The key assumption is that the number of attack edges, g, is small relative to the number of honest nodes, n.
As pointed out by earlier work, one can justify this sparse cut assumption by observing that, unlike creating a Sybil identity, creating an attack edge requires the adversary to expend social-engineering effort: the adversary must convince an honest person to create a social link to one of his Sybil identities.
Wh¯ anau's correctness will depend on the sparse cut assumption, but its performance will not depend at all on the number of Sybils.
In fact, the protocol is totally oblivious to the structure of the Sybil region.
Therefore, the classic Sybil attack, of creating many fake identities to swamp the honest identities, is ineffective.Since we rely on the existence of a sparse cut to distinguish the honest region from the Sybil region, we also assume that there is no sparse cut dividing the honest region in two.
Given this assumption, the honest region forms an expander graph.
Expander graphs are fast mixing, which means that a short random walk starting from any node will quickly approach the stationary distribution [6].
Roughly speaking, the ending node of a random walk is a random node in the network, with a probability distribution proportional to the node's degree.
The mixing time, w, is the number of steps a random walk must take to reach this smooth distribution.
For a fast mixing network, w = O(log n).
Section 9.1 shows that graphs extracted from some real social networks are fast mixing.
The random walk is Wh¯ anau's main building block, and is the only way the protocol uses the social network.
An honest node can send out a w-step walk to sample a random node from the social network.
If it sends out a large number of such walks, and the social network is fast mixing and has a sparse cut separating the honest nodes and Sybil nodes, then the resulting set will contain a large fraction of random honest nodes and a small number of Sybil nodes [26].
Because the initiating node cannot tell which individual samples are good and which are bad, Wh¯ anau treats all sampled nodes equally, relying only on the fact that a large fraction will be good nodes.
Some honest nodes may be near a concentration of attack edges.
Such loser nodes have been lax about ensuring that their social connections are real people, and their view of the social graph does not contain much information.
Random walks starting from loser nodes are more likely to escape into the Sybil region.
As a consequence, loser nodes must do more work per lookup than winner nodes, since the adversary can force them to waste resources.
Luckily, only a small fraction of honest nodes are losers, because a higher concentration of attack edges in one part of the network means a lower concentration elsewhere.
Most honest nodes will be winner nodes.In the stationary distribution, proportionally more random walks will land on high-degree nodes than lowdegree nodes.
To handle high-degree nodes well, each Wh¯ anau participant creates one virtual node [24] per social network edge.
Thus, good random samples are distributed uniformly over the virtual nodes.
All virtual nodes contribute equal resources to the DHT and obtain equal levels of service (i.e., keys stored/queried).
This use of virtual nodes fulfils the policy goal (Section 3.3) of allocating both workload and trust according to each person's level of participation in the social network.
Table 1 summarizes the social network parameters introduced thus far.
We can now succinctly define our main security property: Definition.
A DHT protocol is (g, ǫ, p)-Sybil-proof if, against an active adversary with up to g attack edges, the protocol's LOOKUP procedure succeeds with probability ≥ p on any honest key, for at least (1 − ǫ)n honest nodes.
Given a (g, ǫ, 1/2)-Sybil-proof protocol, it is always possible to amplify the probability of success p exponentially close to 1 by, for example, running multiple independent instances of the protocol in parallel.
5 For example, running 3 log 2 n instances would reduce the failure probability to less than 1/n 3 , essentially guaranteeing that all lookups will succeed with high probability (since there are only n 2 possible source-target node pairs).
The parameter ǫ represents the fraction of loser nodes, which is a function of the distribution of attack edges in the network.
If attack edges are distributed uniformly, then ǫ may be zero; if attack edges are clustered, then a small fraction of nodes may be losers.We use the parameters in Table 1 to analyze our protocol, but do not assume that all of them are known by the honest participants.
Wh¯ anau needs order-of-magnitude estimates of m, w, and k to choose appropriate table sizes and walk lengths.
It does not need to know g or ǫ.Proving that a protocol is Sybil-proof doesn't imply that it cannot be broken.
For example, Wh¯ anau is Sybilproof but can be broken by social engineering attacks that invalidate the assumption that there is a sparse cut between the honest and Sybil regions.
Similarly, a protocol may be broken by using cryptographic attacks or attacks on the underlying network infrastructure.
These are serious concerns, but these are not the Sybil attack as described by Douceur [9].
Wh¯ anau's novel contribution is that it is the first DHT protocol totally insensitive to the number of Sybil identities.
This section outlines Wh¯ anau's main characteristics.
The Sybil attack poses three main challenges for a structured DHT.
First, structured DHTs forward queries using small routing tables at each node.
Simply by creating many cheap pseudonyms, an attacker will occupy many of these table entries and can disrupt queries [23].
Second, a new DHT node builds and maintains its routing tables by querying its neighbors' tables.
An attacker can reply to these queries with only its own nodes.
Over time, this increases the fraction of table entries the attacker occupies [22].
Third, DHTs assign random IDs to nodes and apply hash functions to keys in order to spread load evenly.
By applying repeated guess-and-check, a Sybil attacker can choose its own IDs and bypass these mechanisms.
This enables clustering attacks targeted at a specific key.
For example, if the adversary inserts many keys near the targeted key, then it might overflow the tables of honest nodes responsible for storing that part of the key space.Alternatively, the adversary might choose all its IDs to fall near the targeted key.
Then, honest nodes might have to send many useless query messages to Sybil nodes before eventually querying an honest node.
To illustrate how random walks apply to the problem of Sybil-proof DHT routing, consider the following strawman protocol.
In the setup phase, each node initiates r = O( √ km) independent length-w random walks on the social network.
It collects a random key-value record from the final node of each walk, and stores these nodes and records in a local table.To perform a lookup, a node u consults its local record table.
If the key is not in this table (which is likely), u broadcasts the key to the O( √ km) nodes v 1 , . . . , v r in its table.
If those nodes' tables are sufficiently large, with high probability, at least one node v i will have the needed key-value record in its local table.The strawman protocol shows how random walks address the first and second challenges above.
If the number of attack edges is small, most random walks stay within the honest region.
Thus, the local tables contain mostly honest nodes and records.
Furthermore, nodes use only random walks to build their tables: they never look at each other's tables during the setup process.
As a result, the adversary's influence does not increase over time.The strawman sidesteps the third challenge by eschewing node IDs entirely, but this limits its efficiency.
Lookups are "one-hop" in the sense that the ideal lookup latency is a single network round-trip.
However, since each lookup sends a large number of messages, performance will become limited by network bandwidth and CPU load as the network size scales up.
By adding structure, we can improve performance.
The main challenge is to craft the structure in such a way that it cannot be exploited by a clustering attack.
Wh¯ anau's structure resembles other DHTs such as Chord [24], SkipNet [12], and Kelips [11].
Like SkipNet and Chord, Wh¯ anau assumes a given, global, circular ordering ≺ on keys (e.g., lexical ordering).
The notation x 1 ≺ x 2 ≺ · · · ≺ x z means that for any indexes i < j < k, the key x j is on the arc (x i , x k ).
No metric space.
Like SkipNet, but unlike Chord and many other DHTs, Wh¯ anau does not embed the keys into a metric space using a hash function.
If Wh¯ anau were to use a hash function to map keys into a metric space, an adversary could use guess-and-check to construct many keys that fall between any two neighboring honest keys.
This would warp the distribution of keys in the system and defeat the purpose of the hash function.
Therefore, Wh¯ anau has no a priori notion of "distance" between two keys; it can determine only if one key falls between two other keys.
This simple ordering provides some structure (e.g., a node can have a successor table), but still requires defenses to clustering attacks.Fingers and successors.
Most structured DHTs have routing tables with both "far pointers", sometimes called fingers, and "near pointers", called leaves or successors.
Wh¯ anau follows this pattern.
All nodes have layered IDs (described below) which are of the same data type as the keys.
Each node's finger table contains O( √ km) pointers to other nodes with IDs spaced evenly over the key space.
Likewise, each node's successor table contains the O( √ km) honest key-value records immediately following its ID.
Finger tables are constructed simply by sending out O( √ km) random walks, collecting a random sample of (honest and Sybil) nodes along with their layered IDs.
Successor tables are built using a more complex sampling procedure (described in Section 6.1).
Together, an honest node's finger nodes' successor tables cover the entire set of honest keys, with high probability.
This structure enables fast one-hop lookups: simply send a query message to a finger node preceding the target key.
The chosen finger is likely to have the needed record in its successor table.
(If not, a few retries with different fingers should suffice.)
In contrast with the strawman protocol above, this approach uses a constant number of messages on average, and O(log n) messages (which may be sent in parallel) in the worst case.Layered IDs.
Wh¯ anau defends against clustering attacks using layers, illustrated in Figure 3.
Each node uses a random walk to choose a random key as its layer-0 ID.
This ensures that honest nodes' layer-0 IDs are distributed evenly over the keys stored by the system.To pick a layer-1 ID, each node picks a random entry from its own layer-0 finger table and uses that node's ID.
To pick a layer-2 ID, each node takes a random layer-1 finger's ID, and so on for each of the ℓ = O(log km) lay- IDs from the set of all layer i IDs (honest and Sybil).
Thus, most layers are balanced.
Even if there is a clustering attack on a key, it will always be easy to find an honest finger near the key using a random sampling procedure.ers.
In the end, each node is present at, and must collect successor tables for, ℓ positions in the key space.Layers defend against key clustering and ID clustering attacks.
If the attacker inserts many keys near a target key, this will simply cause more honest nodes to choose layer-0 IDs in that range.
The number of keys the attacker can insert is limited by the number of attack edges.
Thus, a key clustering attack only shifts around the honest nodes' IDs without creating any hot or cold spots.Nodes choose their own IDs; thus, if the attacker chooses all its layer-0 IDs to fall immediately before a target key, it might later be difficult to find an honest finger near the key.
However, if the adversary manages to supply an honest node u with many clustered layer-0 IDs, then this increases the probability that u will pick one of these clustered IDs as its own layer-1 ID.
As a result, the distribution of honest layer-1 IDs tends to mimic any clusters in the Sybil layer-0 IDs.
This increases the honest nodes' presence in the adversary-chosen range, and increases the likelihood that layer 1 finger tables are balanced between honest and Sybil nodes.The same pattern of balance holds for layers 2 through ℓ−1.
As long as most layers have a good ratio of honest to Sybil IDs in every range, random sampling (as described in Section 6.2) can find honest fingers near any target key.
There are three sources of churn that Wh¯ anau must handle.
First, computers may become temporarily unavailable due to network failures, mobility, overload, crashes, or being turned off daily.
We call this node churn.
Wh¯ anau builds substantial redundancy into its routing tables to handle Sybil attacks, and this same redundancy is sufficient to handle temporary node failures.
Section 9.5 shows that increasing node churn results in a modest additional overhead.The second source of churn is changes to the social relationships between participants.
This social churn results in adding or deleting social connections.
A single deleted link doesn't impact Wh¯ anau's performance, as long as the graph remains fast mixing and neither endpoint became malicious.
(If one did become malicious, it would be treated as an attack edge.)
However, Wh¯ anau doesn't immediately react to social churn, and can only incorporate added links by rebuilding its routing tables.
Nodes which leave the DHT entirely are not immediately replaced.
Therefore, until SETUP is invoked, the routing tables, load distribution, and so on will slowly become less reflective of the current social network, and performance will slowly degrade.Social network churn occurs on a longer time scale than node churn.
For example, data from Mislove et al. indicates that the Flickr social network's half-life is approximately 6 weeks [18].
Running SETUP every day, or every few minutes, would keep Wh¯ anau closely in sync with the current social graph.The final and most challenging source of churn is changes to the set of keys stored in the DHT.
This key churn causes the distribution of keys to drift out of sync with the distribution of finger IDs.
Reacting immediately to key additions and deletions can create "hot spots" in successor tables; this can only be repaired by re-running SETUP.
Thus, in the worst case, newly stored keys will not become available until the tables are rebuilt.
For some applications -like the IM example, in which each node only ever stores one key -this is not a problem as long as tables are refreshed daily.
Other applications may have application-specific solutions.Unlike key churn, turnover of values does not present a challenge for Wh¯ anau: updates to the value associated with a key may always be immediately visible.
For example, in the IM application, a public key's current IP address can be changed at any time by the record's owner.
Value updates are not a problem because Wh¯ anau does not use the value fields when building its routing tables.Key churn presents a trade-off between the bandwidth consumed by rebuilding tables periodically and the delay from a key being inserted to the key becoming visible.
This bandwidth usage is similar to stabilization in other DHTs; however, insecure DHTs can make inserted keys visible immediately, since they do not worry about clustering attacks.
We hope to improve Wh¯ anau's responsiveness to key churn; we outline one idea in Section 10.
This section defines SETUP and LOOKUP in detail.
The SETUP procedure takes each node's social connections and the local key-value records to store as inputs, and constructs four routing tables:• ids(u, i): u's layer-i ID, a random key x.• fingers(u, i): u's layer-i fingers as (id , address) pairs.
• succ(u, i): u's layer-i successor (key, value) records.
• db(u): a sample of records used to construct succ.
The global parameters r f , r s , r d , and ℓ determine the sizes of these tables; SETUP also takes an estimate of the mixing time w as a parameter.
Typically, nodes will have a fixed bandwidth and storage budget to allocate amongst the tables.
Section 7 and Section 9 will show how varying these parameters impacts Wh¯ anau's performance.The SETUP pseudocode (Figure 4) constructs the routing tables in ℓ+1 phases.
The first phase sends out r d random walks to collect a sample of the records in the social network and stores them in the db table.
These samples are used to build the other tables.
The db table has the good property that each honest node's stored records are frequently represented in other honest nodes' db tables.
SETUP (stored-records(·), neighbors(·); w, r d , r f , rs, ℓ) The remaining phases are used to construct the ℓ layers.
For each layer i, SETUP chooses each node's IDs and constructs its successor and finger tables.
The layer-0 ID is chosen by picking a random key from db.
Higher-layer IDs and finger tables are defined mutually recursively.
FINGERS(u, i, r f ) sends out r f random walks and collects the resulting nodes and i th layered IDs into u's i th layer finger table.
For i > 0, CHOOSE-ID(u, i) chooses u's i th layered ID by picking a random finger ID stored in u's (i − 1) th finger table.
As explained in Section 5.3, this causes honest IDs to cluster wherever Sybil IDs have clustered, ensuring a rough balance between good fingers and bad fingers in any given range of keys.1 for each node u 2 do db(u) ← SAMPLE-RECORDS(u, r d ) 3 for i ← 0 to ℓ − 1 4 do for each node u 5 do ids(u, i) ← CHOOSE-ID(u, i) 6 fingers (u, i) ← FINGERS(u, i, r f ) 7 succ(u, i) ← SUCCESSORS(u, i, rs) 8 return fingers , succ SAMPLE-RECORDS(u, r d ) 1 for j ← 1 to r d 2 do vj ← RANDOM-WALK(u) 3 (key j , valuej ) ← SAMPLE-RECORD(vj) 4 return {(key 1 , value1), . . . , (key r d , valuer d )} SAMPLE-RECORD(u) 1 (key , value) ← RANDOM-CHOICE(stored-records(u)) 2 return (key, value) RANDOM-WALK(u0) 1 for j ← 1 to w 2 do uj ← RANDOM-CHOICE(neighbors(uj−1)) 3 return uw CHOOSE-ID(u, i) 1 if i = 0 2 then (key , value) ← RANDOM-CHOICE(db(u)) 3 return key 4 else (x, f ) ← RANDOM-CHOICE(fingers(u, i − 1)) 5 return x FINGERS(u, i, r f ) 1 for j ← 1 to r f 2 do vj ← RANDOM-WALK(u) 3 xj ← ids(vj , i) 4 return {(x1, v1), . . . , (xr f , vr f )} SUCCESSORS(u, i, rs) 1 for j ← 1 to rs 2 do vj ← RANDOM-WALK(u) 3 Rj ← SUCCESSORS-SAMPLE(vj, ids(u, i)) 4 return R1 ∪ · · · ∪ Rr s SUCCESSORS-SAMPLE(u, x0) 1 {(key 1 , value1), . . . , (key r d , valuer d )} ← db(u) (sorted so that x0 key 1 · · · key r d ≺ x0) 2 return {(key 1 , value1), . . . , (key t , valuet)} (for small t)Once a node has its ID for a layer, it must collect the successor list for that ID.
It might seem that we could solve this the same way Chord does, by bootstrapping off LOOKUP to find the ID's first successor node, then asking it for its own successor list, and so on.
However, as pointed out in Section 5.1, this recursive approach would enable the adversary to fill up the successor tables with bogus records over time.
To avoid this, Wh¯ anau fills each node's succ table without using any other node's succ table; instead, it uses only the db tables.The information about any layered ID's successors is spread around the db tables of many other nodes, so the SUCCESSORS subroutine must contact many nodes and collect little bits of the successor list together.
The straightforward way to do this is to ask each node v for the closest record in db(v) following the ID.The SUCCESSORS subroutine repeatedly calls SUCCESSORS-SAMPLE r s times, each time accumulating a few more potential-successors.
SUCCESSORS-SAMPLE works by contacting a random node and sending it a query containing the ID.
The queried node v, if it is honest, sorts all of the records in its local db(v) by key, and then returns the closest few records to the requestor's ID.
The precise number t of records sampled does not matter for correctness, so long as t is small compared to r d .
Section 7's analysis simply lets t = 1.
This successor sampling technique ensures that for appropriate values of r d and r s , the union of the repeated queries will contain all the desired successor records.
Section 7.1 will state this quantitatively, but the intuition is as follows.
Each SUCCESSORS-SAMPLE query is an independent and random sample of the set of keys in the system which are near the ID.
There may be substantial overlap in the result sets, but for sufficiently large r s , we will eventually receive all immediate successors.
Some of the records returned will be far away from the ID and thus not really successors, but they will show up only a few times.
Likewise, bogus results returned by Sybil nodes consume some storage space, but do not affect correctness, since they do not prevent the true successors from being found.
LOOKUP(u, key)1 v ← u 2 repeat value ← TRY(v, key) 3 v ← RANDOM-WALK(u) 4until TRY found valid value, or hit retry limit 5 return value TRY(u, key )1 {(x1, f1), . . . , (xr f , fr f )} ← fingers(u, 0) (sorted so key x1 · · · xr f ≺ key) 2 j ← r f 3 repeat (f, i) ← CHOOSE-FINGER(u, xj, key ) 4 value ← QUERY(f, i, key) 5 j ← j − 1 6until QUERY found valid value, or hit retry limit 7 return value CHOOSE-FINGER(u, x0, key)1 for i ← 0 to ℓ − 1 2 do Fi ← { (x, f ) ∈ fingers (u, i) | x0 x key } 3 i ← RANDOM-CHOICE({i ∈ {0, . . . , ℓ−1} | Fi non-empty}) 4 (x, f ) ← RANDOM-CHOICE(Fi) 5 return (f, i) QUERY(u, i, key )1 if (key, value) ∈ succ(u, i) for some value 2 then return value 3 else error "not found" In order to process requests quickly, each node should sort its finger tables by ID and its successor tables by key.
The basic goal of the LOOKUP procedure is to find a finger node which is honest and which has the target record in its successor table.
The SETUP procedure ensures that any honest finger f which is "close enough" to the target key y will have y ∈ succ(f ).
Since every finger table contains many random honest nodes, each node is likely to have an honest finger which is "close enough" (if r f is big enough).
However, if the adversary clusters IDs near the target key, then LOOKUP might have to waste many queries to Sybil fingers before finding this honest finger.
LOOKUP's pseudocode ( Figure 5) chooses fingers carefully to foil this category of attack.To prevent the adversary from focusing its attack on a single node's finger table, LOOKUP tries first using its own finger table, and, if that fails, repeatedly chooses a random delegate and retries the search from there.The TRY subroutine searches the finger table for the closest layer-zero ID x 0 to the target key key.
It then chooses a random layer i to try, and a random finger f whose ID in that layer lies between x 0 and the target key.
TRY then queries f for the target key.If there is no clustering attack, then the layer-zero ID x 0 is likely to be an honest ID; if there is a clustering attack, that can only make x 0 become closer to the target key.
Therefore, in either case, any honest finger found between x 0 and key will be close enough to have the target record in its successor table.Only one question remains: how likely is CHOOSE-FINGER to pick an honest finger versus a Sybil finger?
Recall from Section 5.3 that, during SETUP, if the adversary clustered his IDs in the range [x 0 , key] in layer i, then the honest nodes tended to cluster in the same range in layer i + 1.
Thus, the adversary's fingers cannot dominate the range in the majority of layers.
Now, the layer chosen by CHOOSE-FINGER is random -so, probably not dominated by the adversary -and therefore, a finger chosen from that layer is likely to be honest.In conclusion, for most honest nodes' finger tables, CHOOSE-FINGER has a good probability of returning an honest finger which is close enough to have the target key in its successor table.
Therefore, LOOKUP should almost always succeed after only a few calls to TRY.
For the same reason as a flooding protocol, Wh¯ anau's LOOKUP will always eventually succeed if it runs for long enough: some random walk (LOOKUP, line 3) will find the target node.
However, the point of Wh¯ anau's added complexity is to improve lookup performance beyond a flooding algorithm.
This section sketches the reasoning why LOOKUP uses O(1) messages to find any target key, leaving out most proofs; more detailed proofs can be found in an accompanying technical report [15].
To the definitions in Section 4, we will add a few more in order to set up our analysis.Definition (good sample probability).
Let p be the probability that a random walk starting from a winner node returns a good sample (a random honest node).
p decreases with the number of attack edges g. Specifically, we have previously shown that p ≥ 1 21 − gw ǫn for any ǫ [15].
We are interested in the case where g < ǫn 2w = O n w .
In this case, we have that p > 1/4, so a substantial fraction of random walks return good samples.Definition ("the database").
Let D be the disjoint union of all the honest nodes' db tables:D def = honest u db(u)Intuitively, we expect honest nodes' records to be heavily represented in D. D has exactly r d m elements; we expect at least (1 − ǫ)pr d m of those to be from honest nodes.
Definition (distance metric d xy ).
Recall from Section 5.3 that Wh¯ anau has no a priori notion of distance between two keys.
However, with the definition of D, we can construct an a posteriori distance metric.
Let D xy def = {z ∈ D | x z ≺ y} be all the records (honest and Sybil) in D on the arc [ x, y).
Then defined xy def = |D xy | |D| = |D xy | r d m ∈ [ 0, 1)Note that d xy is not used (or indeed, observable) by the protocol; we use it only in the analysis.
Recall that SETUP (Figure 4) We can intuitively interpret this result as follows: to get a complete successor table with high probability, we need r s r d = Ω(km log km).
This is related to the Coupon Collector's Problem: the SUCCESSORS subroutine examines r s r d random elements from D, and it must examine the entire set of km honest records.
Consider an arbitrary winner u's layer-zero finger table F 0 = fingers(u, 0): approximately pr f of the nodes in F 0 will be random honest nodes.
Picking a random honest node f ∈ F 0 and then picking a random key from db(f ) is the same as picking a random key from D. Thus, pr f of the IDs in F 0 are random keys from D. For any keys x, y ∈ D, the probability that a random honest finger's layer-zero ID falls in the range [ x, y) is simply d xy .
Lemma.
With r f fingers, we have a Prob[fail] of:Prob[ no layer-0 finger in [ x, y) ] (1 − d xy ) pr f (2)We expect to find approximately pr f d xy of these honest fingers with IDs in the range [ x, y).
We can intuitively interpret this result as follows: to see Ω(1) fingers in [ x, y) with high probability, we need r f = Ω(log m/d xy ).
In other words, large finger tables enable nodes to find a layer-0 finger in any small range of keys.
Thus layer-0 finger tables tend to cover D evenly.
The adversary may attack the finger tables by clustering its IDs.
CHOOSE-ID line 4 causes honest nodes to respond by clustering their IDs on the same keys.Pick any keys x, y ∈ D sufficiently far apart that we expect at least one layer-zero finger ID in [ x, y) with high probability (as explained above).
Let β i ("bad fingers") be the average (over winners nodes' finger tables) of the number of Sybil fingers with layer-i IDs in [ x, y).
Likewise, let γ i ("good fingers") be the average number of winner fingers in [ x, y).
Define µ def = (1 − ǫ)p. γ i+1 µ(γ i + β i )Corollary.
Let the density ρ i of winner fingers in layer ibe ρ i def = γ i /(γ i + β i ).
Then ℓ−1 i=0 ρ i µ ℓ−1 /(1 − µ)r f .
Because the density of winner fingers ρ i is bounded below, this result means that the adversary's scope to affect ρ i is limited.
The adversary may strategically choose any values of β i between zero and (1 − µ)r f .
However, the adversary's strategy is limited by the fact that if it halves the density of good nodes in one layer, the density of good nodes in another layer will necessarily double.
¯ ρ def = 1 ℓ ℓ−1 i=0 ρ i µ e [(1 − µ)µr f ] − 1 ℓ .
Observe that as ℓ → 1, the average layer's density of good fingers shrinks exponentially to O(1/r f ), and that as ℓ → ∞, the density of good fingers asymptotically approaches the limit µ/e.
We can get ¯ ρ within a factor of e of this ideal bound by setting the number of layers ℓ toℓ = log [(1 − µ)µr f ](3)For most values of µ ∈ [0, 1], ℓ ≈ log r f .
However, when µ approaches 1 (no attack) or 0 (strong attack), ℓ → 1.
The preceding sections' tools enable us to prove that Wh¯ anau uses a constant number of messages per lookup.Theorem (Main theorem).
Define κ = kme/(1 − ǫ)p 3 .
Suppose that we pick r s , r f , r d , and ℓ so that (3) and (4) are satisfied, and run SETUP to build routing tables.r s r f > r s r d p > κ(4)Now run LOOKUP on any valid key y. Then, a single iteration of TRY succeeds with probability better thanProb[ success ] > 1 20 (1 − ǫ)p = Ω(1).
The value κ is the aggregate storage capacity km of the DHT times an overhead factor e/(1−ǫ)p 3 which represents the extra work required to protect against Sybil attacks.
When g < ǫn 2w , this overhead factor is O(1).
The formula (4) may be interpreted to mean that both r s r d and r s r f must be Ω(κ): the first so that SUCCESSORS-SAMPLE is called enough times to collect every successor, and the second so that successor lists are longer than the distance between fingers.
These would both need to be true even with no adversary.Proof sketch.
Let x ∈ D be a key whose distance to the target key y is d xy = 1/pr f , the average distance between honest fingers.First, substitute the chosen d xy into (2).
By the lemma, the probability that there is an honest finger x h ∈ [ x, y) is at least 1 − 1/e.
TRY line 1 finds x r f , the closest layer-zero finger to the target key, and TRY passes it to CHOOSE-FINGER as x 0 .
x 0 may be an honest finger or a Sybil finger, but in either case, it must be at least as close to the target key as x h .
Thus, x 0 ∈ [ x, y) with probability at least 1 − 1/e.
Second, recall that CHOOSE-FINGER first chooses a random layer, and then a random finger f from that layer with ID x f ∈ [ x 0 , y ].
The probability of choosing any given layer i is ℓ −1 , and the probability of getting an honest finger from the range is ρ i from Section 7.3.
Thus, the total probability that CHOOSE-FINGER returns an honest finger is simply the average layer's density of good nodes 1 ℓ ρ i = ¯ ρ.
Since we assumed (3) was satisfied, Section 7.3 showed that the probability of success is at least ¯ ρ (1 − ǫ)p/e 2 .
Finally, if the chosen finger f is honest, the only question remaining is whether the target key is in f 's successor table.
Substituting d x f y < d xy and (4) into (1) yields Prob[ y ∈ succ(f ) ] 1 − 1/e.
Therefore, when QUERY(f, y) checks f 's successor table, it succeeds with probability at least 1 − 1/e.
A TRY iteration will succeed if three conditions hold: (1) x f ∈ [ x, y); (2) CHOOSE-FINGER returns a winning finger f ; (3) y ∈ succ(f ).
Combining the probabilities calculated above for each of these events yields the total success probability 1− 1e (1−ǫ)p e 2 1− 1 e > 1 20 (1−ǫ)p. LOOKUP is bounded by 20 (1−ǫ)p = O(1).
With high probability, the maximum number of queries is O(log n).
Each (virtual) node has S = r d + ℓ(r f + r s ) table entries in total.
To minimize S subject to (4), set r s = r f = √ κ and r d = p √ κ.
Therefore, the optimal total table size is S ≈ √ κ log κ, so S = O( √ km log km), as expected.
As the number of attack edges g increases, the required table size grows as (1−ǫ) −1/2 p −3/2 .
A good approximation for this security overhead factor is 1 + 2 gw n + 6 gw n when g < n 6w .
Thus, overhead grows linearly with g.
As one might expect for a one-hop DHT, the optimum finger tables and the successor tables are the same size.
The logarithmic factor in the total table size comes from the need to maintain O(log km) layers to protect against clustering attacks.
If the number of attack edges is small, (3) indicates that multiple layers are unnecessary.
This is consistent with the experimental data in Section 9.3.
We have implemented Wh¯ anau in a simulator and on PlanetLab.
To simulate very large networks -some of the social graphs we use have millions of nodes -we wrote our own simulator.
Existing peer-to-peer simulators don't scale to such a large number of nodes, and our simulator uses many Wh¯ anau-specific optimizations to reduce memory consumption and running time.
The simulator directly implements the protocol as described in Figures 4 and 5, takes a static social network as input, and provides knobs to experiment with Wh¯ anau's different parameters.
The simulator does not simulate realworld network latencies and bandwidths, but only counts the number of messages that Wh¯ anau sends.
The primary purpose of the simulator is to validate the correctness and scaling properties of Wh¯ anau with large social networks.We also implemented Wh¯ anau and the IM application in Python on PlanetLab.
This implementation runs a message-passing protocol to compute SETUP and uses RPC to implement LOOKUP.
When a user starts a node, the user provides the keys and current IP addresses that identify their social neighbor nodes.
The IM client stores its current IP address into the DHT.
When a user wants to send an IM to another user, the IM client looks up the target user's contact information in the DHT and authenticates the returned record using the key.
If the record is authentic, the IM application sends the IM to the IP address in the record.
Wh¯ anau periodically rebuilds its tables to incorporate nodes which join and leave.The average latency for a lookup is usually one roundtrip on PlanetLab.
Using locality-aware routing, Wh¯ anau could achieve lower than one network round-trip on average, but we haven't implemented this feature yet.Our PlanetLab experiments were limited by the number of PlanetLab nodes available and their resources: we were able to run up to 4000 Wh¯ anau nodes simultaneously.
Unfortunately, at scales smaller than this, Wh¯ anau nearly reduces to simple broadcast.
Given this practical limitation, it was difficult to produce insightful scaling results on PlanetLab.
Furthermore, although results were broadly consistent at small scales, we could not crossvalidate the simulator at larger scales.
The PlanetLab experiments primarily demonstrated that Wh¯ anau works on a real network with churn, varying delays, and so on.
This section experimentally verifies several hypotheses:(1) real-world social networks exhibit the properties that Wh¯ anau relies upon; (2) Wh¯ anau can handle clustering attacks (tested by measuring its performance versus table size and the number of attack edges); (3) layered IDs are essential for handling clustering attacks; (4) Wh¯ anau achieves the same scalability as insecure one-hop DHTs; and (5) Wh¯ anau can handle node churn in Planetlab.
Our Sybil attack model permits the adversary to create an unlimited number of pseudonyms.
Since previous DHTs cannot tolerate this attack at all, this section does not compare Wh¯ anau's Sybil-resistance against previous DHTs.
However, in the non-adversarial case, the experiments do show that Wh¯ anau scales like any other insecure one-hop DHT, so (ignoring constant factors such as cryptographic overhead) adding security is "free".
Also, similarly to other (non-locality-aware) one-hop DHTs, the lookup latency is one network round-trip.
Nodes in the Wh¯ anau protocol bootstrap from a social network to build their routing tables.
It is important for Wh¯ anau that the social network is fast mixing: that is, a short random walk starting from any node should quickly approach the stationary distribution, so that there is roughly an equal probability of ending up at any edge (virtual node).
We test if this fast-mixing property holds for social network graphs, extracted from Flickr, LiveJournal, YouTube, and DBLP, which have also been used in other studies [18,26].
These networks correspond to real-world users and their social connections.
The LiveJournal graph was estimated to cover 95.4% of the users in Dec 2006, and the Flickr graph 26.9% in Jan 2007.
We preprocessed the input graphs by discarding unconnected nodes and transforming directed edges into undirected edges.
(The majority of links were already symmetric.)
The resulting graphs' basic properties are shown in Table 2.
The node degrees follow power law distributions, with coefficients between 1.6 and 2 [18].
To test the fast-mixing property, we sample the distribution of random walks as follows.
We pick a random starting edge i, and for each ending edge j, compute the probability p ij that a walk of length w ends at j. Computing p ij for all m possible starting edges i is too timeintensive, so we sampled 100 random starting edges i and computed p ij for all m end edges j. network, we expect the probability of ending up at a particular edge to approach 1/m as w increases to O(log n).
Figure 6 plots the CDF of p ij for increasing values of w. To compare the different social graphs we normalize the CDFs so that they have the same mean.
Thus, for all graphs, p ij = 1/m corresponds to the ideal line at 1.
As expected, as the number of steps increases to 80, each CDF approaches the ideal uniform distribution.The CDFs at w = 10 are far from the ideal distribution, but there are two reasons to prefer smaller values of w. First, the amount of bandwidth consumed scales as w. Second, larger values of w increase the chance that a random walk will return a Sybil node.
Section 9.2 will show that Wh¯ anau works well even when the distribution of random walks is not perfect.Recall from Section 4.2 that when a fast-mixing social network has a sparse cut between the honest nodes and Sybil nodes, random walks are a powerful tool to protect against Sybil attacks.
To confirm that this approach works with real-world social networks, we measured the probability that a random walk escapes the honest region of the Flickr network with different numbers of attack edges.
To generate an instance with g attack edges, we marked random nodes as Sybils until there were at least g edges between marked nodes and non-marked nodes, and then removed any honest nodes which were connected only to Sybil nodes.
the number of attack edges this way actually consumes honest nodes, it is not possible to test the protocol against g/n ratios substantially greater than 1.
Figure 7 plots the probability that a random walk starting from a random honest node will cross an attack edge.
As expected, this escape probability increases with the number of steps and with the number of attack edges.
When the number of attack edges is greater than the number of honest nodes, the adversary has convinced essentially all of the system's users to form links to its Sybil identities.
In this case, long walks almost surely escape from the honest region; however, short walks still have substantial probability of reaching an honest node.
For example, if the adversary controls 2 million attack edges on the Flickr network, then each user has an average of 1.35 links to the adversary, and random walks of length 40 are 90% Sybils.
On the other hand, random walks of length 10 will return 60% honest nodes, although those honest nodes will be less uniformly distributed than a longer random walk.
To evaluate Wh¯ anau's resistance against the Sybil attack, we ran instances of the protocol using a range of table sizes, number of layers, and adversary strengths.
For each instance, we chose random honest starting nodes and measured the number of messages used by LOOKUP to find randomly chosen target keys.
Our analysis predicted that the number of messages would be O(1) as long as g ≪ n/w.
Since we used a fixed w = 10, the number of messages should be small when the number of attack edges is less than 10% of the number of honest nodes.
We also expected that increasing the table size would reduce the number of messages.Our simulated adversary employs a clustering attack on the honest nodes' finger tables, choosing all of its IDs to immediately precede the target key.
In a real-world deployment of Wh¯ anau, it is only possible for an adversary to target a small fraction of honest keys in this way: to increase the number of Sybil IDs near a particular key, the adversary must move some Sybil IDs away from other keys.
However, in our simulator, we allowed the adversary to change its IDs between every LOOKUP operation: that is, it can start over from scratch and adapt its attack to the chosen target key.
Our results therefore show Wh¯ a- nau's worst case performance, and not the average case performance for random target keys.
Figure 8 plots the number of messages required by LOOKUP versus table size.
Since our policy is that resources scale with node degree (Section 3.3), we measure table size in number of entries per social link.
Each table entry contains a key and a node's address (finger tables) or a key-value pair (successor and db tables).
As expected, the number of messages decreases with table size and increases with the adversary's power.
For example, on the Flickr network and with a table size of 10,000 entries per link, the median LOOKUP required 2 messages when the number of attack edges is 20,000, but required 20 messages when there are 2,000,000 attack edges.
The minimum resource budget for fast lookups is 1, 000 ≈ √ n: below this table size, LOOKUP messages increased rapidly even without any attack.
Under a massive attack (g > n) LOOKUP could still route quickly, but it required a larger resource budget of 10, 000.
Figure 9 shows the full data set of which Figure 8 is a slice.
Figure 9(a) shows the number of messages required for 100% of our test lookups to succeed.
Of course, most lookups succeeded with far fewer messages than this upper bound.
Figure 9(b) shows the number of messages required for 50% of lookups to succeed.
The contour lines for maximum messages are necessarily noisier than for median messages, because the lines can easily be shifted by the random outcome of a single trial.
The median is a better guideline to Wh¯ anau's expected performance: for a table size of 5,000 on the Flickr graph, most lookups will succeed within 1 or 2 messages, but a few outliers may require 50 to 100 messages.We normalized the X-axis of each plot by the number of honest nodes in each network so that the results from different datasets could be compared directly.
Our theoretical analysis predicted that Wh¯ anau's performance would drop sharply (LOOKUP messages would grow exponentially) when g > n/10.
However, we observed that, for all datasets, this transition occurs in the higher range m/10 < g < m.
In other words, the analytic prediction was a bit too pessimistic: Wh¯ anau functions well until a substantial fraction of all edges are attack edges.When the number of attack edges g was below n/10, we observed that performance was more a function of ta- ble size, which must always be at least Ω( √ m) for Wh¯ anau to function, than of g. Thus, Wh¯ anau's performance is insensitive to relatively small numbers of attack edges.
Section 9.2 showed that Wh¯ anau handles clustering attacks.
For the plots in Figure 9, we simulated several different numbers of layers and chose the bestperforming value for a given table size.
This section evaluates whether layers are important for Wh¯ anau's attack resistance, and investigates how the number of layers should be chosen.Are layers important?
We ran the same experiment as in Section 9.2, but we held the total table size at a constant 100,000 entries per link.
We varied whether the protocol spent those resources on more layers, or on bigger per-layer routing tables, and measured the median number of messages required by LOOKUP.We would expect that for small-scale attacks, one layer is best, because layers come at the cost of smaller perlayer tables.
For more large-scale attacks, more layers is better, because layers protect against clustering attacks.
Even for large-scale attacks, adding more layers yields quickly diminishing returns, and so we only simulated numbers of layers between 1 and 10.
The solid lines in Figure 10 shows the results for the clustering attack described in Section 9.2.
When the number of attack edges is small, the best performance would be achieved by spending all resources on bigger routing tables, mostly avoiding layers.
For Flickr, layers become important when the number of attack edges exceeds 5,000 (0.3% of n); for g > 20, 000, a constant number of layers (around 8) would yield the best performance.
At high attack ratios (around g/n 1), the data becomes noisy because performance degrades regardless of the choice of layers.The dashed lines in Figure 10 show the same simulation, but pitted against a na¨ıvena¨ıve attack: the adversary swallows all random walks and returns bogus replies to all requests, but does not cluster its IDs.
This control data clearly shows that multiple layers are only helpful against a clustering attack.
The trends are clearer for the larger graphs (Flickr and LiveJournal) than for the smaller graphs (YouTube and DBLP).
100,000 table entries is very large in comparison to the smaller graphs' sizes, and therefore the differences in performance between small numbers of layers are not as substantial.How many layers should nodes use?
The above data showed that layers improve Wh¯ anau's resistance against powerful attacks but are not helpful when the DHT is not under attack.
However, we cannot presume that nodes know the number of attack edges g, so the number of layers must be chosen in some other way.
Since layers cost resources, we would expect the optimal number of layers to depend on the node's resource budget.
If the number of table entries is large compared to √ m, then increasing the number of layers is the best way to protect against powerful adversaries.
On the other hand, if the number of table entries is relatively small, then no number of layers will protect against a powerful attack; thus, nodes should use a smaller number of layers to reduce overhead.
We tested this hypothesis by re-analyzing the data collected for Section 9.2.
For a given table size, we computed the number of layers that yielded optimal performance over a range of attack strengths.
The results are shown in Figure 11 The optimal number of layers is thus a function of the social network size and the resource budget, and we presume that honest nodes know both of these values at least approximately.
Since Wh¯ anau's performance is not very sensitive to small changes in the number of layers, a rough estimate is sufficient to get good performance over a wide range of situations.
Wh¯ anau is designed as a one-hop DHT.
We collected simulated data to confirm that Wh¯ anau's performance scales asymptotically the same as an insecure one-hop DHT such as Kelips [11].
Since we don't have access to a wide range of social network datasets of different sizes, we generated synthetic social networks with varying numbers of nodes using the standard technique of preferential attachment [1], yielding power-law degree distributions with exponents close to 2.
For each network, we simulated Wh¯ anau's performance for various table sizes and layers, as in the preceding sections.
Since our goal was to demonstrate that Wh¯ anau reduces to a standard one-hop DHT in the non-adversarial case, we did not simulate any adversary.
Figure 12 plots the median number of LOOKUP messages versus table size and social network size.
For a onehop DHT, we expect that, holding the number of messages to a constant O(1), the required table size scales as O( √ m): the blue line shows this predicted trend.The heat map and its contours (black lines) show simulated results for our synthetic networks.
For example, for m = 10, 000, 000, the majority of lookups succeeded using 1 or 2 messages for a Wh¯ anau's example IM application runs on PlanetLab.
We performed an experiment in which we started 4000 virtual nodes, running on 400 PlanetLab nodes.
This number of virtual nodes is large enough that, with a routing table size of 200 entries per social link, most requests cannot be served from local tables.
Each node continuously performed lookups on randomly-chosen keys.
We simulated node churn by inducing node failure and recovery events according to a Poisson process.
These events occurred at an average rate of two per second, but we varied the average node downtime.
At any given time, approximately 10% or 20% of the virtual nodes were offline.
(In addition to simulating 10% and 20% failures, we simulated an instance without churn as a control.)
We expected lookup latency to increase over time as some finger nodes became unavailable and some lookups required multiple retries.
We also expected latency to go down whenever SETUP was re-run, building new routing tables to reflect the current state of the network.
Figure 13 plots the lookup latency and retries for these experiments, and shows that Wh¯ anau is largely insensitive to modest node churn.
The median latency is approximately a single network roundtrip within PlanetLab, and increases gradually as churn increases.
As expected, the fraction of requests needing to be retried increased with time when node churn was present, but running SETUP restored it to the baseline.While this experiment's scale is too small to test Wh¯ anau's asymptotic behavior, it demonstrates two points: (1) Wh¯ anau functions on PlanetLab, and (2) Wh¯ anau's simple approach for maintaining routing tables is sufficient to handle reasonable levels of churn.
This section discusses some engineering details and suggests some improvements that we plan to explore in future work.Systolic mixing process.
Most of Wh¯ anau's bandwidth is used to explore random walks.
Therefore, it makes sense to optimize this part of the protocol.
Using a recursive or iterative RPC to compute a random walk, as suggested by Figure 4, is not very efficient: it uses w messages per random node returned.A better approach, implemented in our PlanetLab experiment, is to batch-compute r walks at once.
Suppose that every node maintains a pool of r addresses of other nodes; the pools start out containing r copies of the node's own address.
At each time step, each node randomly shuffles its pool and divides it equally amongst its social neighbors.
For the next time step, the node combines the messages it received from each of its neighbors to create a new pool, and repeats.
After w such mixing steps, each node's pool is a randomly shuffled assortment of addresses.
If r is sufficiently large, this process approximates sending out r random walks from each node.
Very many or very few keys per node.
The protocol described in this paper handles 1 k m well, where k is the number of keys per honest node.
The extreme cases outside this range will require tweaks to Wh¯ anau to handle them.
Consider the case k > m. Any DHT requires at least k = Ω(m) resources per node just to transmit and store the keys.
This makes the task easier, since we could use O(m) bandwidth to collect a nearlycomplete list of all other honest nodes on each honest node.
With such a list, the task of distributing successor records is a simple variation of consistent hashing [13].
The analysis in Section 7.1 breaks down for k > m: more than m calls to SUCCESSORS-SAMPLE will tend to have many repeats, and thus can't be treated as independent trials.
To recover this property, we can treat each node as k/m virtual nodes, as we did with node degrees.
Now consider the other extreme: k < 1, i.e. only some nodes are storing key-value records in the system.
The extreme limiting case is only a single honest node storing a key-value record into the system, i.e. k = 1/m.
Wh¯ anau can be modified to handle the case k < 1 by adopting the systolic mixing process described above and omitting empty random walks.
This reduces to flooding in the extreme case, and smoothly adapts to larger k.Handling key churn.
It is clear that more bandwidth usage can be traded off against responsiveness to churn: for example, running SETUP twice as often will result in half the latency from key insertion to key visibility.
Using the observation that the DHT capacity scales with the table size squared, we can improve this bandwidth-latency tradeoff.
Consider running SETUP every T seconds with R resources, yielding a capacity of K = O(R 2 ) keys.
Compare with this alternative: run SETUP every T /2 seconds using R/2 resources, and save the last four instances of the routing tables.
Each instance will have capacity K/4, but since we saved four instances, the total capacity remains the same.
The total resource usage per unit time also remains the same, but the responsiveness to churn doubles, since SETUP runs twice as often.This scaling trick might seem to be getting "something for nothing".
Indeed, there is a price: the number of lookup messages required will increase with the number of saved instances.
However, we believe it may be possible to extend Wh¯ anau so that multiple instances can be combined into a single larger routing table, saving both storage space and lookup time.
This paper presents the first efficient DHT routing protocol which is secure against powerful denial-of-service attacks from an adversary able to create unlimited pseudonyms.
Wh¯ anau combines previous ideas -random walks on fast-mixing social networks -with the idea of layered identifiers.
We have proved that lookups complete in constant time, and that the size of routing tables is only O( √ km log km) entries per node for an aggregate system capacity of km keys.
Simulations of an aggressive clustering attack, using social networks from Flickr, LiveJournal, YouTube, and DBLP, show that when the number of attack edges is less than 10% of the number of honest nodes and the routing table size is √ m, most lookups succeed in only a few messages.
Thus, the Wh¯ anau protocol performs similarly to insecure one-hop DHTs, but is strongly resistant to Sybil attacks.
