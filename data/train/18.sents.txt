Data integration systems offer users a uniform interface to a set of data sources.
Previous work has typically assumed that the data sources are independent of each other; however, in scenarios involving large numbers of sources, such as the Web or large enterprises, there is an ecosystem of dependent sources, where some sources copy parts of their data from others.
This paper considers the new optimization problems that arise while answering queries over large number of dependent sources.
These are the (1) cost-minimization problem: what is the minimum cost we must incur to get all answer tu-ples, (2) maximum-coverage problem: given a bound on the cost, how can we get the maximum possible coverage, and (3) the source-ordering problem: for a set of data sources, what is the best order to query them so as to retrieve answer tuples as fast as possible.
We consider these optimization problems under several cost models and we show that, in general, they are intractable.
We describe effective approximation algorithms that enable us to solve these problems in practice.
We then identify the causes of the high complexity and show that for restricted classes, the optimization problems can be solved in polynomial time.
Data integration has received significant research attention and recently enjoyed commercial success [10,11,12].
Data integration systems offer users a uniform interface to a set of data sources.
The user formulates a query over a mediated schema, and the system uses a set of semantic mappings to reformulate the query over the relevant set of data sources.
The data integration system then combines the answers from the sources appropriately.Data integration systems typically assume that the data sources are independent of each other.
However, in scenarios involving large numbers of data sources, such as the Web or large enterprises, there is an eco-system of dependent sources, where some sources copy parts of their data from others [2].
As a simple illustrative example, by searching AbeBooks.com, a listing-service website that integrates information from online bookstores, we extracted information on computer science books provided by 876 bookstores.We found 10 bookstores, including Caiman and Powell's Books, whose data are likely to be copied by at least 10 other bookstores, and we found 508 pairs of bookstores where one copied data from another.
A data integration system can benefit significantly from being aware of dependencies between its data sources.
For instance, the system can save resources by not querying data sources that are unlikely to add many new answers to the query, or order the access to the sources to maximize the distinct answers it fetches early on.
This paper considers the new optimization problems that arise when answering queries over collections of dependent data sources.
Specifically, we address the following questions:1.
Cost minimization: what is the minimal set of sources from which we can retrieve all the answers to a query.
2.
Maximum coverage: given a resource bound, what is the set of sources from which we can obtain the maximum set of independent answers.
3.
Source ordering: what is the best ordering of the data sources that provides more answers quickly.To address these problems, we model dependency between sources with a directed acyclic graph whose nodes are the data sources.
An edge from source S 1 to source S 2 specifies the percent of data from S 2 that is copied into S 1 .
Note that S 1 may have additional data that is not in S 2 .
The goal of query answering in this context is to find all independent answers.
Roughly, this means we do not want to get the same answer from S 1 and S 2 if S 1 copied the data from S 2 .
We distinguish two versions of the underlying copy model.
The first version assumes that tuples are copied at random from dependent sources, and the second assumes that we have more information, such as a selection predicate over the copied source.
The random copying model is appropriate when we have no information on how the sources are related.We begin by identifying the coverage problem as a core building block for the optimization problems.
The coverage of a subset of sources T is the expected value of the percent of the overall set of independent answers that can be computed from T .
We establish the following results about coverage.
In the case of the random-copying scenario, we show that coverage is, in general, #P-complete 1 in the total number of input data sources.
We describe a randomized algorithm that yields an arbitrarily accurate estimate of the coverage in polynomial time, and we identify a subclass of the problem that gives an exact polynomial-time solution.
In the scenario where we have more information on how tuples were copied, we show that the complexity of coverage is lower.We consider the cost-minimization problem and the maximumcoverage problem under multiple cost models, including the linear cost model, which counts the sizes of the accessed sources, and the number-of-sources cost model, which does not distinguish between the sources' sizes.
We show that in the general case both problems are intractable under these cost models, and show that we can find an approximation in polynomial time.
Moreover, we show that the number of data sources that a source can copy from and the ability to copy a fraction of the data are critical to the complexity of the above problems.
In fact, if each data source can copy either all or no tuples and from at most one other data source, which is a common case in practice, both problems are PTIME for the number-of-sources model.
For the source-ordering problem we show that there is an efficient 2-approximation of the optimal ordering.Our paper assumes that dependencies between data sources are given as input.
There are multiple ways in which dependency information can be obtained, such as from knowledge of the domain, annotation of the data, or by applying techniques that automatically detect dependency [5,6].
Also note that while our primary motivation for this work was data integration, the problems and techniques we study are also relevant for other applications, such as query answering over partially-replicated data [18].
Partial replication among data sources can be captured using dependencies, and we are interested in finding sets of sources that provide required portion of the data.The paper is organized as follows.
Section 2 defines the problem formally.
Section 3 considers the coverage problem, and Section 4 considers the cost-minimization and maximum-coverage problems.
Section 5 discusses the sourceordering problem, and Section 6 shows how our results extend to more complex queries.
We begin by formally defining the dependency model and the optimization problems we consider.
We assume the existence of a set of data sources S = {S 1 , . . . , S n }.
For the purpose of this paper, we assume that each data source contains a set of objects (e.g., books, movies, job listings).
In practice, the data sources can model these objects with relational tuples, and in fact, we refer to the contents of sources as tuples.
The names of the attributes can vary from one source to another and we assume that we have already reconciled heterogeneity with schema-matching techniques.Dependency between sources: Our goal is to capture the fact that, in addition to having original data of their own, data sources often copy data from other sources.
The copying sources may be aggregators or have some data sharing agreement with the original source.
In general, a copier may copy data by performing a query over another source and adding the result of that query to its database; in practice, however, one may not know the queries used to copy data and can only estimate the fraction of tuples that are copied.
We capture dependencies with the following dependency DAG that records the copying relationship between sources.
DEFINITION 2.1 (DEPENDENCY DAG).
The dependencies between the set of data sources S = {S 1 , . . . , S n } are given by a DAG G(S) = (V, E) where• for every source S i ∈ S, there is a node in V , associated with a number n(S i ) specifying the number of tuples independently added by S i , and • a directed edge S i → S j denotes that S i copies tuples from S j , and the edge is associated with an "annotation" describing tuples copied by S i from S j .
2There are at least three kinds of annotations for copying edges and accordingly we have three types of dependency graphs: (1) Fraction-copying DAG: the annotation on S i → S j is a fraction f i,j (called selectivity) denoting the fraction of tuples copied by S i from S j ; (2) Select-copying DAG: the annotation on edge S i → S j is a select condition composed of predicates of the form A op a, where A is an attribute in S j , a is a constant, and op is one of =, <, ≤, >, ≥, and all tuples in S j satisfying the select condition are copied into S i ; (3) Histogram-copying DAG: the annotation on edge S i → S j is a histogram specifying the copying fraction for each range of possible attribute values.
Note that we leave projection in copying for future work, as with projection the duplicate-elimination problem arises and estimating the size of projection results is known to be hard [4].
When the graph has an edge S i → S j , we refer to S i as a copier.
We say that S i is a full-copying copier, if it copies all data from the original sources whenever it copies anything.
In this case, we call the dependency graph a fullcopying DAG.
We say that S i is a single-source copier, if it is a full-copying copier and copies from a single source.
We restrict ourselves to a DAG and not an arbitrary directed graph because we do not consider cases in which sources (transitively) copy data from themselves.We assume that each tuple is annotated with the source from which it was copied, or marked as independently added.
That is, tuples are of the form (t, S) where t is the tuple value, and S is the source that independently provided t.
We assume that sources are sets of tuples.
Hence, even if a tuple (t, S) is obtained by copying from multiple sources, only one copy of the tuple is retained in the source.The total number of tuples in a source S i , denoted by |S i |, can be estimated by its dependencies and independently added tuples; however, as we show shortly, this estimation is non-trivial.
Example 2.2.
Figure 1 shows an example dependency graph for six sources.
Among the sources, S 1 and S 2 independently provide 100 tuples each.
Sources S 3 and S 4 each copies 0.5 fraction of tuples from S 1 and also independently provides 50 tuples each.
Sources S 5 and S 6 copy from multiple sources (S 5 from S 1 and S 2 , and S 6 from S 2 , S 3 , and S 4 ) without independently providing data.
2Query answering: For most of the paper we first present our solutions for one prototypical query: find all the tuples from the sources, denoted Q(S).
This prototypical query already unveils many of the challenges that arise in our context.
In practice, the majority of queries tend to ask for all the tuples that satisfy certain predicates.
Section 6 describes a transformation that shows how our solutions to the prototypical query apply to queries with selection predicates.We define the semantics of a query to be the union of all tuples from all the sources.
Formally, given a source S i , we denote by Q(S i ) the set of all answer tuples from S i (either independently added or copied).
We define Q(S) as ∪Q(S i ), where ∪ is the set union of the Q(S i ).
Recall that each tuple, (t, S) is annotated by the source that independently provide it.
Hence, if tuple t was independently provided by both S 1 and S 2 then both (t, S i ) and (t, S j ) will be in the answer.Our goal is to take advantage of the dependency between sources to compute Q(S) more efficiently.
Hence, we will typically try to answer the query (or get a near-complete answer) from a subset T ⊆ S of sources.
We denote by |Q(T )| the total number of answer tuples returned by a subset T ⊆ S of sources.
When Q is the identity query, we use |T | and |Q(T )| interchangeably.Cost models: The main cost model we consider in our optimization problems is the linear cost model.DEFINITION 2.3 (LINEAR COST MODEL (LCM)).
Given a set T of data sources, the cost of computing Q(T ) under the linear cost model is c(T ) = Si∈T |S i |.
2The intuition behind the linear cost model is that looking at the data sources and performing the union can be done in near-linear time in the size of the answers returned from each data source (and certainly in linear time in the number of I/Os) using either a hash table or an ordered index.We also consider the following variations on the cost model and show that they have subtle effects on the complexity results in several cases.
|T |.
In such a cost model, the objective is to minimize the number of sources being queried.
Such a model might be applicable if the system is being charged for every query over any of the sources.
2.
Arbitrary Source Cost Model (ASCM): Here we assume each source S i is associated with an arbitrary cost c i incurred in querying it.
Hence, c(T ) = Si∈T c i .
Such a cost model can be useful in many scenarios, such as capturing the cost of setting up network connection and the delay caused by the system being overburdened on answering other queries.Coverage: Given a subset of sources T , we would like to define the coverage of T w.r.t. S as the expected value of the fraction of answers to Q(S) that we can obtain from T .
DEFINITION 2.4 (COVERAGE PROBLEM).
Given a set S of data sources, a dependency graph G(S), and a subset T ⊆ S of sources, compute the expected value of |Q(T )| |Q(S)| .
2In certain cases, such as when we know only copying fractions (so we have a fraction-copying DAG), |Q(T )| |Q(S)| cannot be uniquely determined by G(S), and hence we are interested in obtaining the expected value of |Q(T )| |Q(S)| .
The coverage problem will play an important role in the other problems we consider in the paper.Optimization problems: We now formally define our optimization problems.The cost-minimization problem tries to find a minimal set of sources that still yields all the answers to the query: DEFINITION 2.5 (COST MINIMIZATION PROBLEM (CMP)).
Given a query Q, a set S of data sources, and a dependency graph G(S), find a subset T ⊆ S such that1.
Q(T ) = Q(S); 2.
for any subset T ′ ⊆ S, if Q(T ′ ) = Q(S), then c(T ′ ) ≥ c(T ).
2The maximum-coverage problem tries to find the best answer (measured in number of tuples) that can be obtained with a fixed cost limit.
DEFINITION 2.6 (MAXIMUM COVERAGE PROBLEM (MCP)).
Given a query Q, a set S of data sources, a dependency graph G(S), and a maximum allowed cost C max , find a subset T ⊆ S such that1.
c(T ) ≤ C max ; 2.
for any subset T ′ ⊆ S, if c(T ′ ) ≤ C max , then |Q(T ′ )| ≤ |Q(T )| 2The source-ordering problem tries to find the optimal order of sources in which to execute the query, so we return query answers as quickly as possible.
Intuitively, if we plot the curve of the number of tuples returned as we query more sources, we want to maximize the area under the curve.Formally, let Π be a permutation of the data sources, where Π(j) denotes the jth source in the permutation Π.
We define the area below a curve that represents answering the query with respect to permutation Π asA Q (Π) = l i=1 c({S Π(j) }) · |Q(∪ i j=1 {S Π(j) })|.
The source ordering problem can be defined as follows.DEFINITION 2.7 (SOURCE ORDERING PROBLEM (SOP)).
Given a query Q, a set of sources S = {S 1 , . . . , S l }, a dependency graph G(S), find a permutation Π opt of {1, . . . , l} such that for any other permutation Π, we haveA Q (Π opt ) ≥ A Q (Π).
2Example 2.8.
Consider the dependency graph in Figure 1.
We begin by considering the coverage problem, which is fundamental to all of the other three problems.
In Section 3.1, we consider the case when all we know is the fraction of tuples being copied between sources.
In Section 3.2 we study the case when we know more about the specific set of tuples being copied, specified by a selection query or a histogram.
Table 1 summarizes the results we establish in this section 2 .
In practice we often do not have a-priori knowledge of which tuples are more likely to be copied by which sources.
In such cases we cannot compute a precise coverage, since two copiers S 1 and S 2 may copy a fraction of the data of a source S 3 , and we do not know the overlap between the data they copied.
Hence, we estimate the coverage assuming each tuple is equally likely to be copied.We establish three main results.
First, we show that in general, the coverage problem is #P-complete in the number of sources (Section 3.1.1).
Second, we show that a PTIME randomized algorithm yields arbitrarily good approximations of the coverage (Section 3.1.2).
Finally, we show that the hardness of the problem comes from allowing a copier to copy only a fraction of the data from the original source (Section 3.1.3).
2 We can easily extend our results for cases where we have a combination of different types of annotations in the dependency graph.
Our results rely on identifying a limited version of the coverage problem and relating it to the computation of the probability of a boolean formula.
In the limited-coverage problem we have a single original source that independently provides data, a set of sources that copy from the original source directly or transitively, and a single sink source that is not copied by any other source.
DEFINITION 3.1 (LIMITED COVERAGE PROBLEM).
The limited coverage problem considers a set of data sources S with a dependency graph G(S) = (V, E) that satisfies the following properties:• there exists just one source S 0 ∈ S whose node v 0 ∈ V has no outgoing edges, • there exists just one source S f ∈ S whose node v f ∈ V has no incoming edges, and • only S 0 independently adds tuples.The limited coverage problem is to compute |S f | |S0| assuming equal probability of a tuple being copied, where |S| is the number of tuples in S. 2Example 3.2.
Figure 2 shows an example instance of the limited coverage problem.
Among the data sources, S 0 provides all tuples independently and all other sources copy from S 0 either directly or indirectly.
Except S f , each data source is copied by one or more data sources.
2We reduce the limited coverage problem to the problem of finding the probability of a boolean formula F in DNF form constructed as follows:• There is a boolean variable in F for every edge in G.Each variable is independent of the others.
For the variable corresponding to e ij = (v i , v j ) ∈ E, its probability of being true is f i,j , where f i,j is the copying fraction associated with the edge.
• For each distinct path from S f to S 0 in G consisting a sequence of edges {e 1 , . . . , e k }, we add a conjunct (e 1 ∧ . . . ∧ e k ) to F .
(Because of the structure of G, there must exist at least one path from S f to S 0 .)
For Example 3.2, the coverage problem can be reduced to computing probability of the following DNF Table 1: Summary of results for the coverage problem for various copy models.
Let N be the number of nodes in the input dependency graph, E be the number of edges, k be the number of attributes on which selection predicates or histograms are present, b be the maximum number of constants in predicates for each attribute on each edge when selection predicates are present and the maximum number of buckets for each attribute on each edge when histograms are present, and L = log 1 δ ǫ 2 .
For some copying models we consider two cases: attributes are independent or can be correlated.
(e 1 ∧ e 4 ∧ e 7 ) ∨ (e 2 ∧ e 5 ∧ e 7 ) ∨ (e 3 ∧ e 6 ∧ e 8 ) #P-completeO(N + E) Attr.
Dep: O((2bE) k (N + E)) #P-complete Attr.
Indep: O(bkE(N + E)) (ǫ, δ)-approx O(LN E) N/A N/A Attr.
Dep: O((bE) k LN E) Attr.
Indep: O(bkLN E 2 )0: Input: Dependency graph G for the limited coverage problem.
|S f | |S 0 | .1: Topologically sort the nodes in G: n i first and n f last; 2: Set C L = 0.
Repeat L times: 3:For each edge e i,j , include it with probability f i,j (and omit it with probability (1 − f i,j )); Decide in the topological order for each source if it is connected to S 0 ;5:if (S f is connected to S 0 ) C L ++.
6: return C L L .
Algorithm 1: LIMITEDCOVERAGE Randomized algorithm to solve the limited coverage problem.The probability of a boolean formula is defined as the sum of the probabilities of all its satisfying assignments.
The following lemma (proved in the appendix) provides the key result that we will use in the next sections by relating the coverage problem and the probability of F .
|S f | |S0| .
2The lemma below shows that even the limited-coverage problem is #P-hard.
It is proved by a reduction from the #P-complete problem of counting the number of satisfying assignments in a bipartite monotone 2-DNF formula [19].
Since the limited-coverage problem is in #P (the computation of the probability of F is in #P), and the general version of the coverage problem can be solved using a polynomial number of limited coverage problems, we obtain the following complexity result.Theorem 3.5.
Given a set S of data sources, a dependency graph G(S), and a subset T ⊆ S, the associated coverage problem is #P-complete in the number of sources in S. 2 We now show that we can approximate the coverage problem with a monte-carlo based algorithm.
We will establish this claim in two steps.
First, we show that we can give an arbitrarily accurate estimate for the limited coverage problem.
We then show that we can solve the general coverage problem by solving a linear number of limited coverage problems.To show the first part, we note that although the formula F constructed in Section 3.1.1 could be of size exponential in |S|, we can apply the following randomized algorithm to compute the probability of F in time polynomial in |S|.
The algorithm proceeds as follows (see Algorithm 1).
In every iteration, we adjust edges of the dependency graph.
For each original edge with fraction f i,j , we include the edge with probability f i,j and remove the edge otherwise.
We count 1 if there exists a path from S f to S 0 , which can be decided in polynomial time.This procedure is repeated L times and the following theorem shows that we can get an estimation that is arbitrarily close to the correct coverage in polynomial number of iterations.
Specifically, for a given allowed error ǫ > 0, we can ensure that the probability of the error exceeding ǫ is at most δ, when the number of iterations is more thanlog( 1 δ ) ǫ 2 .
In other words, we can arbitrarily reduce the probability of exceeding a given error bound in polynomial number of iterations.
Since each iteration is polynomial in the size s = |E| of the graph, the total complexity is O(s log( 1 δ ) ǫ 2 ).
Theorem 3.6.
If L > log( 1 δ ) ǫ 2 and the randomized algorithm satisfies F in C L of the L iterations, then Pr(| Pr(F ) − CL L | ≥ ǫ) ≤ δ,where Pr(F ) is the true probability of F .
2 PROOF.
Let X i ∈ {0, 1} be the random variable that is 0 if the i th iteration falsifies F and 1 if it satisfies F .
We have that expected value E(X i ) = Pr(F ).
Hence, using Hoeffding's inequality [14], which is a special case of Chernoff's bound, we havePr(| Pr(F ) − C L L | ≥ ǫ) ≤ e −2Lǫ 2Our result follows by bounding the right-side by δ.Next, we show how to solve the general coverage problem using the limited coverage problem.
Consider the set S of sources and a subset T ⊆ S, and our goal is to estimate |T | |S| .
We obtain the coverage by Algorithm COVERAGE (see Algorithm 2), which first computes the coverage of T on tuples independently added by each source.
Since the contribution of tuples to T by every source in S is independent of other sources in S, these contributions are added to obtain the total coverage.
Theorem 3.7.
Let N be the number of sources in S, E be the number of edges in the input fraction-copying DAG G(S), and L = log( 1 δ ) ǫ 2 .
Algorithm COVERAGE can give 0: Input: S, T ⊆ S, and dependency graph G(S).
Output: Estimation of |T | |S| .1: Set D = P S i ∈S n(S i ) and C = 0;2: foreach (source S i ∈ S where n(S i ) > 0 and S i has at least one descendant in T )3:Construct the subgraph I G i of G induced by vertexes S i and T as follows: with S i as the root, traverse child node to reach all possible descendants of S i in T .
Add a special node S f in I G i with edges to each descendant of S i in T .
Set the fractions associated with all these added edges to 1; Compute the coverage c i of {S f } in I G i by invoking Algorithm LIMITEDCOVERAGE;5:C = C + c i * n(S i ); 6: return C D .
Algorithm 2: COVERAGE Randomized algorithm to solve the coverage problem.0: Input: S, T ⊆ S, and dependency graph G(S).
|T | |S| .1: ns = 0; nt = 0; 2: for each (S ∈ S) ns+=n(S); //n(S) is #(independent tuples) in S.3: for each (T ∈ T ) 4:Q = {T };// the queue to traverse 5:while (Q 񮽙 = ∅)6:N = pop(Q); if (N is not visited yet)8:nt+=n(N ); Mark N as visited;10:Push N 's parents into Q;11: return nt/ns;Algorithm 3: Algorithm FULLCOPYINGCOVERAGE.an arbitrarily accurate estimate for the coverage problem in time O(LN E).
2 Finally, we show that the high complexity of the coverage problem is due to the fact that each copier can copy only a fraction of data from an original source.
Algorithm FULL-COPYINGCOVERAGE (see Algorithm 3) computes the exact coverage of a set of sources when they are all full-copiers: if they copy any data from a source, then they copy all of it.
In this section we consider cases in which we have more information.
We start with the case where we know the exact selection predicates applied in copying.
We then extend our results for a hybrid case, where we have histograms describing the fraction of tuples copied for each bucket.
When we know the predicates used for copying, we have a select-copying DAG as the input.
To solve the coverage problem, we transform the select-copying DAG to a set of full-copying DAGs, apply the PTIME algorithm FULLCOPY-INGCOVERAGE on each result DAG and then aggregate the results.
In the transformation, for each attribute we define a set of disjoint value ranges and each result DAG corresponds to a combination of value ranges for different attributes.
We specify the SELECTCOPYINGCOVERAGE algorithm rigorously as follows.
2.
Create P = k i=1 (2l i + 1) full-copying dependency graphs, G 1 , . . . , G P , each corresponding to a combination of value ranges for the k attributes, denoted by R(G i ), i ∈ [1, P ].
Do the following for each G i .
(1) For each edge e, if R(G i ) satisfies the predicate for e in G, associate a fraction of 1; otherwise, remove e. (2) For each source S, update n(S i ) as the number of independently added tuples in R(G i ) (assume such n(S i )'s are given as input).
3.
Solve the coverage problem for eachG i , i ∈ [1, P ],using Algorithm FULLCOPYINGCOVERAGE.
Sum up the results as the coverage for G.Example 3.10.
Figure 4 shows an example dependency graph with selection predicates on two attributes A and B. Attribute A has two end points: 2 and 4, so has 5 possible ranges A < 2, A = 2, 2 < A < 4, A = 4, and A > 4.
Similarly, there are 5 possible ranges for B: B < 2, B = 2, 2 < B < 5, B = 5, and B > 5.
Hence, there are a total of 25 combinations, giving 25 full-copying DAGs.
Fig- ure 4 shows two of them; in the full-copying DAGs, we have combined multiple ranges (such as B < 2 and B = 2 into B ≤ 2).
2The next theorem establishes complexity for the coverage problem over SELECTCOPY dependency graphs.
PROOF.
We create at most (2bE+1) k full-copying DAGs: each attribute has at most bE end points and so at most 2bE + 1 value ranges, and we consider all combinations of ranges for the k attributes.
For each full-copying DAG the coverage problem can be solved in time O(N + E) (Theorem 3.8).
If we know that the attributes are independent of each other, then the complexity is significantly reduced, and we can show the following result.Corollary 3.12.
Let N be the number of sources in S, E be the number of edges in the input select-copying DAG G(S), and k be the number of distinct attributes on which predicates are specified in G(S).
When for each source the attributes are independent in value distribution, the coverage problem can be solved in time O(bkE(N + E)).
2 Next, we consider histogram-copying DAG.
For each edge, a histogram specifies the copy fraction for ranges of possible attribute values, and we assume uniform copying within each bucket.
Such histograms are typically constructed by sampling data for each range of values.
Note that when the attributes are correlated, we need k-dimensional histograms.In such cases, we can proceed as in Algorithm SELECT-COPYINGCOVERAGE.
We first construct a set of dependency graphs, each corresponding to one attribute value range and is a fraction-copying DAG (each edge is associated with the copying fraction specified by the histogram for that particular range).
Then, we approximate the coverage for each constructed graph using Algorithm COVERAGE and take the sum as the result.
Theorem 3.13.
Let N be the number of sources in S, E be the number of edges in the input histogram-copying DAG G(S), k be the number of distinct attributes on which predicates are specified in G(S), and b be the maximum number of buckets for each attribute on each edge.
• The coverage problem is #P-complete.
• We can get an ǫ-approximation with confidence (1 − δ) (in the sense of Theorem 3.6) in the coverage in time O((bE) k LN E) in general, and in O(bkLN E 2 ) when the attributes are independent, where L = log( 1 δ )ǫ 2 .
2PROOF.
The #P-completeness directly follows from Theorem 3.5.
For the (ǫ, δ) approximation, the time complexity follows from Theorem 3.6, Theorem 3.11, and Corollary 3.12: The algorithm corresponding to Theorem 3.6 is either applied on O((bE) k ) graphs (similar to the proof for Theorem 3.11) in general and O(bkE) graphs (similar to Corollary 3.12) when attributes are independent of each other.
We now consider the closely-related maximum-coverage and cost-minimization problems.
We begin by showing that in general both problems are intractable w.r.t. each of the cost models we defined previously (Section 4.1).
In Section 4.2 we show that we can approximately solve both problems using a greedy algorithm.
Finally, in Section 4.3 we identify copy patterns that are common in practice, under which we can exactly solve the problems with respect to certain cost models in polynomial time.
The results of this section are summarized in Table 2.
Precisely, all the hardness results in this section refer to the decision versions of the optimization problems; i.e., deciding if there exists a solution achieving a given value of the objective function.
Note that the results apply to all copying models (fraction-copying, select-copying, histograms-copying).
As we show in Section 3, computing the coverage of a subset of sources is #P-complete.
Interestingly, although the maximum coverage problem, which requires computing coverage of a set of sources,is PP-complete (the analog of #P-completeness for decision problems), the cost minimization problem has a lower complexity bound and is NP-complete.
We first consider the restricted case where all copiers are full-copying copiers.
Section 3 shows that for this restricted case finding the coverage of a set of sources takes only polynomial time.
However, we next show that even for this case, both the cost minimization problem and the maximum coverage problem are already NP-complete.
For LCM or ASCM cost model c With PTIME coverage algorithm 0: Input: Sources S, dependency graph G(S), cost function c.Output: Set ¯ S ⊆ S as the result.1: ¯ S = ∅, ¯ A = ∅; // ¯ A is the set of answers.2: while (∃S ∈ S − ¯ S such that T (S) ⊆ ¯ A) //T (S) is the set of tuples in S. Let S 0 ∈ S − ¯ S be the source with maximum |S 0 − ¯ A| c(S 0 ) ; 4:¯ S = ¯ S ∪ {S 0 }; ¯ A = ¯ A ∪ T (S 0 ); 5: return ¯ S;Algorithm 4: GREEDYAPPROX: Greedy approximate algorithm for the cost-minimization problem.
For the maximum-coverage problem, we only need to replace the while condition with (∃S ∈ S − ¯ S such that c( ¯ S) + c(S) ≤ Cmax), where Cmax is the maximum allowed cost.For the complexity of the maximum-coverage problem, the proof, presented in the appendix, uses a reduction from the Knapsack problem for the LCM and ASCM cost models, and a reduction from the Set Cover problem for the NSCM cost model.
For the cost-minimization problem, we use a different reduction from the Set Cover Problem (the reduction for LCM is slightly different from that for the other two cost models).
For the unrestricted versions of the problems we have the following results.
• The cost-minimization problem is NP-complete w.r.t. the LCM, NSCM and ASCM cost models.
• The maximum-coverage problem is PP-complete w.r.t. the LCM, NSCM and ASCM cost models.
2Note that we have different complexity results for the two problems in the general case.
Cost minimization requires all tuples to be covered and no edge with fraction less than 1 (or select condition other than true) can guarantee all tuples in the derived relation; thus, the problem can be solved by first removing all such dependency edges and then solving the problem on the resulting full-copying dependency graph.
On the other hand, the maximum-coverage problem does not have the same property and thus requires estimating coverage of a set of nodes, resulting in PP-completeness.
We now show that we can approximate the maximumcoverage and cost-minimization problems using a greedy algorithm that runs in polynomial time.
In the following sections we shall see that under certain restricted conditions, our greedy algorithm can actually obtain optimal answers.We start with the cost-minimization problem.
Recall from Section 3.1.2 that we can approximate the coverage in polynomial time; thus, we can efficiently estimate the number of additional tuples we obtain by querying a new source.
Algorithm GREEDYAPPROX (see Algorithm 4) proceeds by including sources in a greedy fashion: it iteratively picks the source that adds the maximum number of new tuples per unit cost, until no more source can add new answer tuples.
The following result gives an approximation guarantee for GREEDYAPPROX.Theorem 4.3.
Let α be the number of tuples in the largest source in the input to the cost minimization problem.
GREEDY APPROX obtains a log α-factor approximation to the optimal solution; i.e., if the optimal cost is c, GREEDYAPPROX obtains a cost of at most c · log α.
2Note that GREEDYAPPROX cannot obtain a constant-factor approximation.
Indeed, we can prove that the problem is MaxSNP-hard, meaning that it cannot be approximated to within factor 1 + ǫ for any ǫ > 0, unless P = NP.
This is because in our NP-completeness proofs for the costminimization problem, the reduction from the Set Cover Problem preserves the approximation ratio and thus yields L-reductions, so the MaxSNP-hardness of the Set Cover Problem carries over.
Finally, we can easily revise GREEDYAPPROX for the maximum coverage problem by iterating till reaching the cost limit.
We can show the following result: Theorem 4.5.
It is possible to obtain a (1 − 1 e )-factor approximation to the optimal solution for the maximum-coverage problem.
2 We consider dependency graphs that satisfy the singlesource copying property, i.e., each source copies from at most a single source, and copies all of its data.
First, the following result establishes a PTIME complexity for cost minimization for all cost models.
The full proof by induction, based on a greedy algorithm, appears in the appendix.
In our proof, we consider T k , the optimal set of k sources, and S, the best source that can be added to T k .
We arrive at a contradiction supposing the optimal set of k + 1 sources is obtained by adding some source S ′ to a set T ′ of k sources, where T ′ 񮽙 = T k .
The main idea used in our argument is the fact that the coverage of any set of sources can be represented by the nodes covered by leaf→root paths from all the nodes in the set.
Whenever a source is added, the increase in coverage is given by the total number of uncovered nodes in its leaf→root path.Next we illustrate the greedy algorithm using an example.
The example also shows that the same algorithm is not guaranteed to obtain the optimal solution with respect to other cost models.
Figure 5(a) and assume we can query at most two sources.
Figure 5(b) shows the number of answer tuples each source can introduce initially and so we select S 4 .
Figure 5(c) shows the answer tuples each source can introduce after selecting S 4 ; accordingly, we select S 7 and obtain the answer set {S 4 , S 7 }.
Note that if we consider the linear cost model and a maximum allowed cost 35, the optimal answer is {S 5 , S 6 }, but the greedy algorithm incorrectly chooses {S 4 }.
2In fact, the maximum coverage problem remains NP-complete for the LCM and ASCM cost models.
The proof follows from the fact that the reduction from 0-1 Knapsack used for Theorem 4.1 only involves single-source copiers.
Ordering the sources optimally is the key challenge for an online query answering system over dependent sources.
Our goal is to order the sources in a way that returns answers as quickly as possible.
Recall from Section 2 that we are trying to maximize the area under the curve that plots the cumulative number of answers returned with time, and that given a permutation of the sources Π, we denote the area under the curve by A(Π).
The following theorem establishes some basic complexity results for the coverage problem.Theorem 5.1.
The decision version of the source-ordering problem is PP-complete in the number of sources.
Assuming finding the coverage takes polynomial time, then sourceordering is in NP.
The source ordering problem can be solved optimally in PTIME when all copiers are single-source copiers.
2The PP-completeness of the source ordering problem follows from the hardness of the coverage problem.
When coverage takes polynomial time, e.g., with full-copying, the source ordering problem is easily seen to be in NP.
However, the exact complexity class under full-copying remains an open problem.The main result of this section is a factor-2 approximation algorithm for the source ordering problem.
That is, if we denote the optimal permutation by Π opt and the permutation computed by our algorithm by Π, then A(Π) ≥ A(Π opt )/2.
In the rest of the section, we first show that an optimal permutation must have a monotonicity property.
We then show that although monotonicity does not guarantee an optimal solution, it ensures a 2-approximation.
Finally, we give a greedy algorithm that returns a monotonic permutation; in case we can compute coverage of a set of sources in polynomial time, our greedy algorithm can generate a 2-approximation in polynomial time.
Note that results in this section apply to all cost models.
We next start with the formal definition of the monotonicity property, which uses the following notion of Incr.
For a set S = {S 1 , . . . , S l } of data sources and a permutation Π over {1, . . . , l}, Incr(1) = |{S Π(1) }|;Incr(i) = | ∪ i j=1 {S Π(j) }| − | ∪ i−1 j=1 {S Π(j) }|.
DEFINITION 5.2 (MONOTONIC PERMUTATION).
Let S = {S 1 , .
.
.
, S l } be a set of data sources and G(S) be its dependency graph.
A permutation Π over {1, . . . , l} is said to be monotonic if for each i ∈ [1, l], we haveIncr(i) c(S Π(i) ) ≥ Incr(i+1) c(S Π(i+1) ) .
2Intuitively, the monotonicity property says that the rate of increase of answer tuples as we query more sources decreases monotonically.
Not surprisingly, we can show that the optimal permutation for source ordering must be monotonic.Lemma 5.3.
Given a set of sources S = {S 1 , . . . , S l } and a dependency graph G(S).
If Π opt is an optimal permutation to the source-ordering problem, Π opt is monotonic.
2Whereas monotonicity is a necessary condition for optimality, the following lemma shows that it is not sufficient.Lemma 5.4.
There exists a set of data sources S = {S 1 , . . . , S l }, a dependency graph G(S), and a monotonic permutation Π of {1, . . . , l}, such that Π is not an optimal permutation to the source-ordering problem.
2Next we prove the main result of this section: any monotonic permutation is at most a factor of two off from any other (and in particular, the optimal) permutation.Theorem 5.5.
Let S = {S 1 , . . . , S l } be a set of data sources and G(S) be a dependency graph of S. Let Π opt be the optimal permutation to the source ordering problem and Π be a monotonic permutation of {1, . . . , l}.
Then, A(Π) ≥A(Πopt) 2 .
2 PROOF.
Let C = l i=1 c(S i ).
Also, |S| = l i=1Incr(i).
Then, we haveA(Π opt ) ≤ C · |S|.
Since Π is monotonic, for each unit of cost, the incremental return decreases monotonically.
Thus, we haveA(Π) = l X j=1 0 @ c j · j X i=1 Incr(i) 1 A ≥ C X j=1 j · |S| C > C 2 · |S| ≥ A(Πopt) 2According to Theorem 5.5, we can design a 2-approximation algorithm to the source ordering problem by greedily picking the next source whose ratio of incremental return versus cost is maximal.
Note that this algorithm does not necessarily generate the optimal solution ( Lemma 5.4).
In cases where we can solve the coverage problem in polynomial time, we can find the 2-approximation solution in polynomial time.Theorem 5.6.
Let N be the number of sources in S and E be the number of edges in the input full-copying DAG G(S).
We can find a 2-approximation solution to the source ordering problem in time O(EN 2 ).
2 Until now we considered answering the identity query over our data sources.
We now show how our results apply to queries with selection, which are the most common in practice.
We also comment on projection and join queries.Selection queries: A typical query over a large collection of sources is specified by a selection predicate (typically by selecting values in forms).
We now show how to extend our results to queries that involve equality and comparison predicates.
We denote the set of predicates by P.We assume that for each data source S i , we can estimate the selectivity s P i of P for S i , i.e., the fraction of n(S i ) tuples independently provided by S i that satisfy P.
We can use traditional estimation techniques for this purpose.
When we assume equal probability of a source tuple being copied, the fraction of data copied from source S i should have the same selectivity as S i w.r.t. P.
When we know the exact selection condition for copying, we only consider the copied data that satisfy predicates P.Given any input I including a selection query with predicate P, we can transform the problem to an input I ′ including an identity query, such that solving any of the four problems we consider gives the same solution on I and I ′ .
In particular, given a selection query Q with predicate P, a set of sources S, and a dependency graph G(S) = (V, E), we construct G p (S) as follows: (1) (V P , E P ) = (V, E), (2) n P (S i ) = s P i * n(S i ), (3) if annotation R ij is a fraction, R P i,j = R i,j ; if R ij is a selection condition, R P i,j = R i,j ∧ P.
We then have the following result.
Theorem 6.1.
Any of the coverage problem, cost minimization problem, maximum coverage problem, and the source ordering problem gives the same solution for (a) G(S) w.r.t. Q, and (b) G P (S) w.r.t. the identity query.
2Projection queries: The main challenge introduced by projections is duplicate elimination.
When we project onto a subset of attributes, the number or fraction of tuples that merge to the same tuple value may be different for different sources.
In the general case, estimating the size of projection results requires accessing most of the data in each source [4].
If we assume that data provided independently by different sources have the same fraction for any projection, and assume random copying (so in expectation the fraction of copied tuples remaining after a projection is the same for all data sources), we can directly apply the results from this paper.Join queries: It is easy to extend our dependency model for cases in which sources contain multiple tables, each with different copying sources and patterns.
Under the randomcopying assumption, our results extend to join queries in a rather straightforward fashion.
However, when we use selection queries to model the copying pattern, we need to consider how to estimate the join selectivities.
We leave that to future work.
We considered the problem of answering queries over large collection of possibly overlapping data sources.
Although we showed that many problems are intractable in general, we proposed greedy or randomized approximation algorithms that ran in polynomial time and have provable quality guarantees.
In addition, we identified practical restricted classes of dependencies that yield polynomial-time optimal solutions.
Together, these results provide a foundation on which to build such an integration system.
One interesting direction for future work is the case in which the data itself is uncertain, and therefore seeing the data from multiple independent sources can affect our belief in the answer.Previous work [7,8,17,21] developed algorithms for detecting when a data source can be ignored in answering a query.
Yet other work [9] studied the use of probabilities to model source coverage and overlap for data integration.
These works are all based on coverage of sources and did not consider dependence between sources.Several authors have discussed mechanisms that result in dependencies between sources on the Web.
Leskovec et al. [16] study influences in web-data, such as how blog linkage structures evolve, and [1] provides a formalism for creating web documents by copying portions of data from other documents.
Our work is a first step to integrating web data with such dependencies.
Of course, a large body of recent work (see [3] for a tutorial) studies the orthogonal issue of tracking data provenance.
We prove the theorem with a reduction from the following #P-complete problem of counting the number of satisfying assignments in a bipartite monotone 2-DNF formula [19]:Given sets X = {x 1 , . . . , x n1 } and Y = {y 1 , . . . , y n2 } of boolean variables, and conjuncts C 1 , . . . , C m where each C i is of the form (x j ∧ y k ), count the number of satisfying assignments forF = (C 1 ∨ C 2 ∨ . . . ∨ C m ).
Given the input to the above problem, we construct an input to the coverage problem whose solution gives an answer to the above problem.
Let the set of sources be S = {S 0 , S 1 1 , . . . , S 1 n1 , S 2 1 , . . . , S 2 n2 , S 3 }.
Let the only source that adds new tuples be S 0 , and the dependencies between the sources be as follows.
Each S 1 j copies from S 0 , and S 3 copies from each S 2 k , and all these edges have fraction f = 0.5.
Additionally, for each clause C i = (x j ∧ y k ), add an edge from S 2 k to S 1 j , i.e., S 2 k copies from S 1 j , with corresponding fraction f = 1.0.
We are now interested in computing the coverage of the set containing the single source {S 3 }.
Intuitively, each S 1 j corresponds to variable x j , S 2 k corresponds to variable y k , and the source S 3 may obtain tuples of S 0 only through one of the "paths" corresponding to the clauses.
Therefore, using Lemma 3.3, the coverage of the set {S 3 } is given by the probability P r of F in the input problem, where all variables in X and Y are independent of one another and have a probability of 0.5 each.
Since each variable has a probability of 0.5, the number of satisfying assignments N and the probability P r of F are related by N = P r * 2 n1+n2 .
2 Clearly, | ⊎ Si∈S S i | = Si∈S n(S i ) = D. Furthermore, the number of tuples from any source S i present in ⊎ Si∈T S i is given by N i , where N i = 0 if n(S i ) = 0 or if S i does not have any descendant in T .
Therefore, we have |T | = | ⊎ Si∈T S i | = i N i , and hence coverage given byP i Ni D .
Algorithm COVERAGE invokes LIMITEDCOVERAGE at most once for each data source, and thus also takes polynomial time in the number of sources.
2Proof of Corollary 3.12: If we know that distinct attributes are independent of each other, then the complexity is significantly reduced, and we can show the following result.
In particular, if all attributes are independent of one another, the dependence on k also becomes polynomial: Instead of considering P = k i=1 (2l i + 1) dependency graphs, we now only need to consider S = k i=1 (2l i + 1) dependency graphs, corresponding to the ranges of values for each attribute independently, as described below.
3.
Define s q i to be the selectivity of the q th range for A on a particular source S 0 .
4.
Create S = k i=1 (2l i + 1) dependency graphs, G 1 , . . . , G S , with all edge fractions being 0 or 1 as follows.
Consider a graph for one range, say a iq−1 ≤ A < a iq .
For every edge disregard all predicates on attributes other than A. Associate a fraction of 0 if a predicate on A falsifies the condition above, otherwise associate a fraction of 1.
5.
Determine the coverage c q i of the graph based on the selectivities s q i 's for each source in the graph: Source is assumed to have s q i tuples, and similarly all other sources are assumed to have number of tuples given by their selectivities.
Determine c q i for the full-copying graph based on Section 3.1.3.
6.
The combined coverage is given by the following expression Proof of Theorem 4.1:C = q1=1.
.
(l1+1),...,q k =1.
.
(l k +1) i=1.
.
k We first prove hardness of the problem.
NP-hardness w.r.t. the LCM model can be proved by a reduction from the NPhard 0-1 Knapsack problem.
Given a maximum weight W , a set of n items each with value {v 1 , . . . , v n } and weight {w 1 , . . . , w n }, the 0-1 knapsack problem looks for a subset of items whose total weight does not exceed W and maximizes the total value.
Given a 0-1 Knapsack problem where w i = v i for each i ∈ [0, n], we can construct an equivalent MCP as follows.
For each item i, create a source S i with v i independent tuples (and hence cost v i ).
There is no dependency between the sources.
Set C max in the MCP to W .
We can easily prove the correspondence of the optimal solution of the 0-1 Knapsack problem and the solution of the maximum coverage problem.
NP-hardness w.r.t. the ASCM model can be proved by a similar reduction.We next prove NP-hardness w.r.t. the NSCM model by a reduction from the NP-hard Set Cover problem.
Given a universe U = {1, . . . , m}, subsets of U , {¯ s 1 , . . . , ¯ s n }, such that ∪ n i=1 ¯ s i = U , the Set Cover problem decides if there is a set cover of size k.
We construct an input to the MCP as follows.
Construct m independent sources S 1 , . . . , S m where each S i , i ∈ [1, m], has a single tuple i. Then, for each subset ¯ s j , j ∈ [1, n] in the set cover problem, construct a source S ′ j in the MCP where S ′ j copies all data from the sources corresponding to all elements of ¯ s j and does not add any new tuples.
We can prove that there is a set cover of size k if and only if with cost k the MCP has a solution that returns all independently added source tuples (this can be decided in polynomial time when all copyings are full-copying).
We next show that the decision version of the MCP is NPcomplete.
Given a target maximum coverage C, suppose a subset T of sources gives coverage ≥ C. Because all sources are full-copying, we can get the coverage of T in PTIME.
Additionally, the size of each source (i.e., coverage of that single source) can also be obtained in PTIME.
Hence, for each of the cost models, we can give polynomially give a solution exceeding some maximum coverage C. 2 NP-hardness of the problem w.r.t. the NSCM and ASCM models can be proved by a similar reduction from the Set Cover Problem as in the proof of Theorem 4.1.
The hardness w.r.t. the LCM model is also through a reduction from the Set Cover Problem, but constructed differently.
We assume an input to the Set Cover Problem, where for each element i of U , there is a singleton set containing only i. (The problem still remains NP-hard.)
We next construct an input to CMP as follows.Construct S containing (m + n + 1) sources {S M , S 1 , . . . , S m , S ′ 1 , . . . , S ′ n }, and the following dependency DAG G(S).
S M is the root of G(S) and containing M > m 2 , tuples.
For each i ∈ [1, m], there is an edge S i → S M with fraction 1 and n(S i ) = 1 (i.e., S i adds a single new tuple i that is not present in S M ).
For each j ∈ [1, n], S ′ j copies all data from the sources corresponding to all the elements of set ¯ s j (so the edge has fraction 1) and has n(S ′ j ) = 0.
The solution of the above CMP has cost k if and only if there is a set cover of size k:1.
"if": If there is a set cover of size k, pick the corresponding set T of sources from {S ′ 1 . . . . , S ′ j }.
Clearly T returns all answer tuples.
Further, the total cost of query answering c(T ) ≤ k * (M + m), since each S ′ j has the M tuples from S M and at most m other tuples.
Now consider any other solution T ′ containing at leastk + 1 sources; c(T ′ ) ≥ (k + 1) * M .
Since M > m 2 and k ≤ m, we have c(T ′ ) > c(T ).2.
"only if": Consider an optimal solution T for the CMP containing k sources.
If there is any S i ∈ T , we can replace it with S ′ j where ¯ s j is the singleton set containing i and the resulting set T ′ is still optimal.
The set of the k sets corresponding to the sources in T ′ is a set cover for U .
The proof that the decision version of CMP is NP-complete follows from the corresponding proof for MCP in Theorem 4.1.
2 0: Input: Sources S, dependency graph G(S), maximum number of allowed sources k. Output: Set T ⊆ S as the result of MCP.1: T = ∅; 2: Traverse G(S) in depth-first order; for the root node R, A[R] = n(R), and for any other node S, A[S] = n(S) + A[P (S)], where P (S) is the parent of S.3: for (i = 1 : k) 4:Find the leaf node L with the highest A[L]; Add L to T and mark A[L] = 0;6:while (A[P (L)] 񮽙 = 0) 7: for each (descendant D of P (L) but not of L)8:A[D] = A[D] − A[P (L)];9:A[P (L)] = 0; L = P (L);10: return T ;Algorithm 5: SSCMCP: Greedy algorithm for the maximum coverage problem with respect to the number-of-sources cost model when all copiers are single-source copiers.
We have the following direct L-reduction from the cost minimization problem to the weighted set cover problem: The set of all tuples corresponds to the universal set U , and each source S i corresponds to a subset s i ⊆ U , where s i contains the elements corresponding to the tuples in S i .
The weight of s i is the cost of S i .
GREEDYAPPROX mimics the greedy algorithm for weighted set cover that yields an approximation ratio of log α [20].
We can easily revise the GREEDYAPPROX algorithm for the maximum coverage problem (MCP): the only difference is that we iterate until reaching the cost limit.
We call this the GREEDYAPPROXMCP, which obtains a (1 − 1 e )-approximation for the number-of-sources cost model.
The (1 − 1 e )-approximation is based on reducing MCP to the k-Coverage problem [13].
Further, we can adapt the approximation ratio for all cost models using a an approximation algorithm for the Budgeted Maximum Coverage Problem proposed in [15]; BMCP is a generalization of the set cover problem with weights on elements and sets.
2Proof of Corollary 4.4: Note that our reductions from set cover in the proof of Theorem 4.1 give L-reductions.
A ρ-approximation to the reduced CMP problem gives a ρ-approximation to the original set cover problem.
2Proof of Theorem 4.6: A simple algorithm yields an optimal solution for all cost models.
Note that under single-source copying, the dependency graph is a tree.
(1) We first find all "special nodes" in the tree: A node is special if it adds at least one tuple independently, and no descendent node adds any tuple independently.
Note, clearly, that no two special nodes are ancestor/descendants of each other.
Further, any solution to CMP must include at least one node from the subtree rooted at each special node.
(2) For the NSCM and LCM cost models, we simply return all special nodes as the solution to CMP.
For the ASCM cost model, from each subtree rooted at each special node, we pick the source that has least cost.
The set of selected nodes gives an optimal solution to CMP.
2The following proposition establishes several properties of the restricted case on which we base Algorithm 5, used in the proof of Theorem 4.7.
Proposition A.2.
Let S be a set of sources with dependency graph G(S), where all copiers are single-source copiers.
The graph G(S) has the following properties.
• The graph G(S) is a set of trees.
• Given a number k, there exists a set ¯ L of k leaf nodes in G(S), such that there does not exist any set of k sources whose coverage is higher than ¯ L.• Let S be a leaf node in G(S).
Let ¯ S ⊆ S be a set of nodes in G(S).
Let S ′ be the node in ¯ S that has the lowest common ancestor with S and let S LCA be this ancestor.
Let ¯ A be the set of nodes on the path from S LCA (excluding S LCA ) to S.
Then | ¯ S ∪ {S}| − | ¯ S| = A∈ ¯ A n(A).
2Proof of Theorem 4.7: We prove the optimality of SSCMCP by showing that the optimal k + 1 set of sources can be obtained by adding one source to the optimal solution for k sources.
This, in conjunction with the fact that the greedy algorithm obviously returns the optimal solution for k = 1, completes the proof.We prove the result by contradiction.
Let T k be the optimal set of k sources and let S be the best source that can be added to T k .
Suppose the optimal set of k + 1 sources is T ′ ∪ {S ′ }, where T ′ is a set of sources and S ′ is a source such that S ′ ∈ T k ∪ {S}.
(We must have such a source S ′ ∈ T k ∪ {S} as otherwise T k ∪{S} = T ′ ∪{S ′ }, which is optimal for k+1 sources.)
We have |T k | ≥ |T ′ | and |T k ∪{S}| < |T ′ ∪{S ′ }|.
We show that we can either find a solution with k sources better than T k , or a solution with k + 1 sources better than T ′ ∪ {S ′ }.
The main idea used in our argument is the fact that the coverage of any set of sources can be represented by the nodes covered by leaf→root paths from all the nodes in the set.
Whenever a source is added, the increase in coverage is given by the total number of uncovered nodes from it to the covered set of nodes (Proposition A.2).
Consider sources S, S ′ , and the sets T k and T ′ as shown in Figure 6.
The figure has marked places where the leaf→root paths of S, S ′ meet the already covered nodes of T k and T ′ .
Let P k and P ′ be the points where S ′ and S meet T k and T ′ respectively.
The increase in T k due to S ′ is a and the increase in T ′ is a + δ: Since there is just one leaf→root path because each source copies from at most one other source, the a tuples added must completely overlap.
Further, S ′ must add more tuples to T ′ , otherwise T k ∪ {S ′ } would result in a higher coverage.
Similarly, the figure shows the increments on adding S to T ′ and T k .
Recall S ′ ∈ T k .
Let S ′′ be the source in T k that meets S ′ 's leaf→path at P k , with number of tuples added till then being c.
We must have c ≥ a as otherwise T k − S ′′ + S ′ is a better solution for k sources.
As S ′′ meets S ′ at P k , it cannot belong to T ′ .
Now consider adding S ′′ instead of S ′ to T ′ .
If c > a, we have an even better solution for k + 1 sources.
Otherwise, if c = a, S ′ and S ′′ are equivalent in terms of coverage addition, and T ′ ∪ {S ′′ } is also optimal for k + 1 sources.
Hence, we find some other S ′ 2 ∈ T k ∪ {S}, S ′ 2 񮽙 = S ′ , instead of S ′ and apply the same argument above.
.
Construct Π as the same as Π opt except that Π(i + 1) = Π opt (i) and Π(i) = Π opt (i + 1).
We next show that Π is strictly better than Π opt , leading to a contradiction.
Source S 101 copies all data from {S 1 , . . . , S 50 }; source S 102 copies all data from {S 51 , . . . , S 100 }; source S 103 copies all data from {S 25 , . . . , S 75 }; and none of these three sources adds new tuples.
Consider the source ordering S 103 → S 102 → S 101 and then an arbitrary ordering of the rest of the sources.
Under the number-of-sources cost model, the rate of increase of coverage is monotonically decreasing; that is, the permutation is monotonic.
However, it is not optimal: an optimal permutation is S 101 → S 102 → S 103 (and then the rest of the sources).
2 X A. ProofsProof of Lemma 3.3: The probability that a random tuple t ∈ S 0 appears in S f is given by Pr(t ∈ S f |t ∈ S 0 ) = |S f | |S0| .
Now consider tuple t in S 0 .
The tuple t can appear in S f through one of the paths from S 0 to S f .
The combined probability of these paths is given by the probability of F .2 Proof of Lemma 3.3: The probability that a random tuple t ∈ S 0 appears in S f is given by Pr(t ∈ S f |t ∈ S 0 ) = |S f | |S0| .
Now consider tuple t in S 0 .
The tuple t can appear in S f through one of the paths from S 0 to S f .
The combined probability of these paths is given by the probability of F .2
