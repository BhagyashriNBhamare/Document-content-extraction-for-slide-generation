Les sources d'incertitude et d'imprécision des données sont nombreuses.
Une ma-nière de gérer cette incertitude est d'associer aux données des annotations probabi-listes.
De nombreux modèles de bases de données probabilistes ont ainsi été proposés, dans les cadres relationnel et semi-structuré.
Ce dernier est particulièrement adapté à la gestion de données incertaines provenant de traitement automatiques.
Un important problème, dans le cadre des bases de données probabilistes XML, est celui des requêtes d'agrégation (count, sum, avg, etc.), qui n'a pas été étudié jusqu'à présent.
Dans un modèle unifiant les différents modèles probabilistes semi-structurés étudiés à ce jour, nous présentons des algorithmes pour calculer la distribution des résultats de l'agrégation (qui exploitent certaines propriétés de régularité des fonctions d'agréga-tion), ainsi que des moments (en particulier, espérance et variance) de celle-ci.
Nous prouvons également l'intractabilité de certains de ces problèmes.
The study of queries over imprecise data has generated much attention in the setting of relational databases [4,9,20,28].
The Web (with HTML or XML data) in particular is an important source of uncertain data, for instance, when dealing with imprecise automatic tasks such as information extraction.
A natural way to model this uncertainty is to annotate semi-structured data with probabilities.
Some works have recently addressed queries over such imprecise hierarchical information [2,14,15,17,19,22,26,27].
An essential aspect of query processing has been ignored in these works, namely aggregate queries.
This is the problem we study here.In this article, we consider probabilistic XML documents, described using the unifying model of p-documents [1,17].
A p-document can be thought of as a probabilistic process that generates a random XML document.
Some nodes, namely distributional nodes, specify how to perform this random selection.
We consider three kinds of distributional operators: cie, mux, det, respectively for conjunction of independent events (based on conjunctive conditions of some probabilistic events), mutually exclusive (at most one node selected from a set of a nodes), and deterministic (all nodes selected).
This model, introduced in [1,17], captures a large class of models for probabilistic trees that had been previously considered.
We consider queries that involve the standard aggregate functions count, sum, min, max, countd (count distinct) and avg.
The focus here is on aggregation itself and not arbitrary query processing.
We therefore focus our attention to aggregating all the leaves of a document.
We briefly discuss the issue of reducing the evaluation of general aggregate queries to this problem.A p-document is a (possibly compact) representation of a probabilistic space of (ordinary) documents, i.e., a finite set of possible documents, each with a particular probability.
The result of the aggregate function is a single value for each possible document.
Therefore, its result over a p-document is a random variable, i.e., a set of values, each with a certain probability.
We investigate how to compute the distribution of this random variable.
Such a distribution may be too detailed to present to a user.
This leads us to consider some summaries of the distribution, especially its expected value and other probabilistic moments.Our results highlight an (expectable) aspect of the different operators in p-documents: the use of cie (a much richer means of capturing complex situations) leads to a complexity increase.
For documents with cie nodes, we show the problems are hard (typically NP-or FP #P -complete).
This difficulty is yet another reason to consider expected values and other moments.
For count and sum, we show how to obtain them in P. Unfortunately, we show that for min, max, countd, and avg, the problem of computing moments is also FP #P -complete.
We present Monte-Carlo methods that allow tractable approximation of probabilities and moments of aggregate functions.On the other hand, with the milder forms of imprecision, namely mux and det, the complexity is lower.
Computing the distribution for count, min and max is in P.
The result distribution of sum may be exponentially large, but the computation is still in P in both input and output.
On the other hand, computing avg or countd is FP #P -complete.
The good news is that we can compute expected values (and moments) for all of them in P.Finally, we also provide results for a large class of aggregate queries where the aggregate function is based on the operator of a monoid and can be evaluated by a divide-and-conquer strategy.
Examples of these functions are count, sum, min and max.
We show how to use an algebraic structure of p-documents to evaluate monoid aggregate functions in general.
Interesting non-monoid aggregate queries are rare but they exist, e.g. avg or countd, and are typically harder to compute.
It should be noted that we don't know how to use the monoid properties in presence of the cie operator.After presenting some preliminaries and the main investigated problems in Section 2, we discuss how to aggregate p-documents with cie nodes in Section 3.
In Section 4, we present monoid aggregate functions and show how to compute distributions of these functions on p-documents with mux and det nodes by exploiting the structure of the documents.
Then we continue with this model of p-documents and study complexity of distributions and moments in Section 5.
Finally, we present related work and conclude in Section 6.
In this section we recall the model of probabilistic trees from [1], and formalize the problem we study in this paper.Documents.
We assume a countable set of identifiers V and labels L, such that V ∩ L = ∅.
A labeling function, denoted θ, maps V to L. Inspired by XML, we define a document, denoted [32] [41] [8] [25] [26] [54] [55] 1 [32] [41] [8] [23][55]2 1 [4] name [1] IT−personnel [6] name [7] bonus [51] pda [31] by d, as a finite, unordered 1 , labeled tree where each node has a unique identifier v and a label θ(v).
The set of all nodes and edges of d are denoted respectively by V(d) and E(d), whereE(d) ⊆ V(d) × V(d).
We use the common notions child and parent, descendant and ancestor, root and leaf in the usual way.
We denote the root of d by root(d) and the empty tree, that is, the tree with no nodes, by .
A forest, denoted by F , is a set of documents.
Example 1.
Consider the two example documents d 1 and d 2 in Figure 1.
Identifiers appear inside square brackets before labels.
Both documents describe the personnel of an IT department and bonuses that the personnel earned working on different projects.
Document d 1 indicates John worked under two projects (pda and laptop) and got bonuses of 37 and 50 in the former project and 50 in the latter one.Aggregate Functions.
An aggregate function is a function that maps finite bags of values into some domain such as the rationals, the reals, or tuples of reals.
For example,• count and countd return the number of the elements and the number of distinct elements in a bag, respectively.
• min, max over a bag of elements from a linearly ordered set (A, <) return, respectively, the minimal and maximal element in the bag.
(One can generalize max to topK.)
• sum and avg over bags of rational numbers compute their sum and average, respectively.Aggregate functions can be naturally extended to work on documents d:the result α(d) is α(B)where B is the bag of the labels of all leaves in d.
This makes the assumption that all leaves are of the type required by the aggregate function, e.g., rational numbers for sum.
We ignore this issue here and assume they all have the proper type (this can be ensured by a query that only selects some of the leaves, as detailed further).
We extend the notion of aggregate function over documents to forests by considering the bag of leaves of all trees in the forest.So-called monoid aggregate functions play an important role in our investigation, because they can be handled by a divide-and-conquer strategy (see [8]).
Formally, a structure (M, ⊕, ⊥) is called an abelian monoid if ⊕ is an associative and commutative binary operation with ⊥ as neutral element.
If no confusion can arise, we speak of the monoid M .
An aggregate function is a monoid one if for some monoid M and every a 1 , . . . , a n ∈ M :α({|a 1 , . . . , a n | }) = α({|a 1 | }) ⊕ · · · ⊕ α({|a n | }).
It turns out that count, sum, min, max and topK are monoid aggregate functions.
For sum, min, max: α({|a| }) = a and ⊕ is the corresponding obvious operation.
For count: α({|a| }) = 1 and ⊕ is +.
For top2 over the natural numbers (and similarly for topK): α({|l| }) = (l, 0) and(l 1 , l 2 ) ⊕ (l 3 , l 4 ) = (l i , l j ), where l i ≥ l j and l i , l j are the top-2 elements in {|l 1 , l 2 , l 3 , l 4 | }.
It is easy to check that neither avg nor countd are monoid aggregate functions.Aggregate Queries over Documents.
An aggregate query is a query that uses aggregate functions.Example 2.
Continuing with Example 1, one can compute the sum of bonuses for each project that the personnel is involved in.
In XQuery notation, this query Q can be written as follows:for $x in distinct-values(//bonus/*/name()) return <project> <name> { $x } </name> <sum-bonus> { sum(//bonus/*[name()=$x]//*) } </sum-bonus> </project>The query result Q(d 1 ) is the forest of two documents presented in the middle of Figure 1.
When the query Q above is executed over a document, the variable $x is bound to a number of distinct values, which are computed in the outer loop.
For each binding of $x, the expression q = //bonus/*[name()=$x]//* is evaluated and the result is aggregated.
In terms of XQuery, the XPath pattern q is single-path, that is, it does not require any branching.
Our further investigations focus on single-path aggregate queries (the reasons will be discussed later while presenting aggregate queries over p-documents).
Let Q have n variables and an aggregate function α applied to a single-path pattern q.
Then Q is computed in three steps: (i) find the matching n-tuples ¯ a = a 1 , ..., a n from the variables of Q to the labels of d; (ii) for each match ¯ a, instantiate the variables in q with ¯ a, resulting in a pattern q ¯ a and compute the document d(¯ a) that is the fragment of d satisfying q ¯ a ; (iii) for every ¯ a, evaluate α over d(¯ a), and structure the resulting (n + 1)-tuples (¯ a, α(d(¯ a))) according to Q.Continuing with Example 2: (i) there are two matches for $x: laptop and pda; (ii) evaluating q for them yields d(laptop) and d(pda), on the left of Figure 2; (iii) sum on these documents is sum(d(laptop)) = 87, sum(d(pda)) = 109.
The query result is in Figure 1.
px-Spaces.
A probability space over documents is an expression (D, Pr), where D is a set of documents and Pr maps each document to a probability with Σ{Pr(d) | d ∈ D} = 1.
This definition is extended in a straightforward way to probability spaces (F, Pr) over forests, where F is a set of forests.
If F is finite then (F, Pr) is called px-space and denoted by S.Using the documents from the previous example, we can construct S = ({d1, d2, . . .}, Pr) a px-space with, say, Pr(d 1 ) = 0.047 and Pr(d 2 ) = 0.008, etc. p-Documents.
Following [1], we now introduce a very general syntax for representing compactly px-spaces, called p-documents.
P-documents are similar to documents, with the difference that they have two types of nodes: ordinary and distributional ones.
Distributional nodes are only used for defining the probabilistic process that generates random forests (but they do not actually occur in those ones).
Ordinary nodes have labels and they may appear in random forests.More precisely, we assume given a set X of Boolean random variables with some specified probability distribution ∆ over them.
A p-document, denoted by P, is an unranked, unordered, [25] [26] [32] [54][55] [21] ¬x [23] [32][21]0.1 [32] ¬z, x[52][54] [55] [56] z x, z [23] 0.7 0.3[52][53][54][55][56][2] person P 1 ∈ PrXML cie , P 2 ∈ PrXML mux,det .
labeled tree.
Each node has a unique identifier v and a label µ(v) in L∪{cie(E)} E ∪{mux(Pr)} Pr ∪ {det} where L are labels of ordinary nodes, and the others labels are of distributional nodes.
We consider three kinds of the latter labels: cie(E) (for conjunction of independent events), mux(Pr) (for mutually exclusive), and det (for deterministic).
We will refer to distributional nodes labeled with these labels, respectively, as cie, mux, det nodes.
If a node v is labeled with cie(E), then E is a function that assigns to each child of v a conjunction e 1 ∧ · · · ∧ e k of literals (x or ¬x, for x ∈ X ).
If v is labeled with mux(Pr), then Pr assigns to each child of v a probability with the sum equal to 1.
We require the leaves to be ordinary nodes 2 .
There are two more kinds of distributional nodes considered in [1] that we briefly discuss further.Example 3.
Two p-documents are shown in Figure 3.
The left one, P 1 , has only cie distributional nodes.
For example, node n 21 has the label cie(E) and two children n 22 and n 24 such that E(n 22 ) = ¬x and E(n 24 ) = x.
The p-document on the right, P 2 , has only mux and det distributional nodes.
Node n 52 has the label mux(Pr) and two children n 53 and n 56 such that Pr(n 53 ) = 0.7 and Pr(n 56 ) = 0.3.
We denote classes of p-documents by PrXML with a superscript denoting the types of distributional nodes that are allowed for the documents in the class.
For instance, PrXML mux,det is the class of p-documents with only mux and det distributional nodes, like P 2 on Figure 3.
The semantics of a p-document P, denoted by P, is a px-space over random forests, where the forests are denoted by P and are obtainable from P by a randomized three-step process.1.
We choose a valuation ν of the variables in X .
The probability of the choice, according tothe distribution ∆, is p ν = x in P,ν(x)=true ∆(x) · x in P,ν(x)=false (1 − ∆(x)).2.
For each cie node labeled cie(E), we delete its children v such that ν(E(v)) is false, and their descendants.
Then, independently for each mux node v labeled mux(Pr), we select one of its children v according to the corresponding probability distribution Pr and delete the other children and their descendants, the probability of the choice is Pr(v ).
We do not delete any of the children of det nodes 3 .
[11]0.7 0.3[52][53][54][55][56][21][25] [26] [32][23]0.9 0.1 0.25 0.75 [41] [13] [8] [8] [11] ¬z, x [52] [54][55][56][21][25] [26] [32][23] 3.
We then remove in turn each distributional node, connecting each ordinary child v of a deleted distributional node with its lowest ordinary ancestor v , or, if no such v exists, we turn this child into a root.
¬x ¬x x [41] [13] z x x, z mux John mux det [4] name [1] IT− personnel [6]The result of this third step is a random forest P.
The probability Pr(P) is defined as the product of p ν , the probability of the variable assignment we chose in the first step, with all Pr(v ), the probabilities of the choices that we made in the second step for the mux nodes.
, respectively.
We notice that the semantics of both p-documents in Figure 3 is the same, that is, P 1 and P 2 represent the same px-space.
Remark.
In [1] two more types of distributional nodes are considered.
Firstly, ind nodes that select their children independently of each other according to some probability for each child.
It has been shown in [1] that ind nodes can be captured by mux and det. Moreover, PrXML cie is strictly more expressive than PrXML mux,det .
Finally, one can consider exp nodes that explicitly specify a probability for each given subset of their children to be chosen.
This kind of distributional node is a generalization of mux and det, and most of the results for these distributional nodes can be extended to exp.
For simplicity, we shall not discuss this further.
It was shown in [17,18] that query answering with projections is intractable for PrXML cie (FP #P -complete) whereas it is polynomial for PrXML mux,det .
In a similar vein, it will turn out that in a number of cases aggregate query answering is more difficult for PrXML cie than for the restricted case of PrXML mux,det .
Aggregate Queries over px-Spaces and p-Documents.
In the following, we restrict ourselves to queries of the form α(q), where q is a single-path.
Note that in Example 2 the aggregate query sum(//bonus/*[name()=$x]//*) is of this form and that such queries can occur as parts of larger queries, as shown in that example.
Moreover, we assume that our single-path queries do not contain variables.
The single-path query //bonus/*[name()=$x]//* contains the variable $x, but when the query is executed, the variable is bound to a constant.
Over the documents and p-documents in the examples the two possible constants are pda and laptop, which give rise to two constant-free queries q pda and q laptop .
The application of such a query α(q) to a document d according to the XQuery semantics can be understood as computing first the subtree d of d whose leaves are the nodes satisfying q and then α(d ) (note that according to our definition α(d ) is computed from the leaves of d ).
Moving to px-spaces, we first generalize the application of aggregate functions from documents to px-spaces by defining α(S) as the probability distribution of the values of α over S, that is,α(S) := (c, p) c is in the range of α, p = d∈S, α(d)=c Pr(d).
This is canonically extended to p-documents by defining α( P) := α( P), that is, applying α to the px-space generated by P.
In a similar vein, if q is a single-path query, then q(S) is the px-space whose elements are the images q(d) of documents d ∈ S and where the probability of an elementq(d) is Pr(q(d)) = d ∈S, q(d )=q(d) Pr(d ).
Combining aggregation and single path queries, we define the answer to an aggregate query like above as α(q)(S) := α(q(S)), that is, α is applied to the px-space q(S).
Again, this is extended canonically to p-documents.
Let P be in PrXML cie .
If q is single-path, we can apply it naively to P, ignoring the distributional nodes.
The result q( P) is the subtree of P containing the original root and as leaves the nodes satisfying q. For example, applying q pda to P 1 in Figure 3 yields "d(pda) for cie" in Figure 2.
Interestingly, one can show that for all single path queries q it holds that q( P) = q( P).
In other words, we have a strong representation system.
If P in PrXML mux,det , then q( P) can be obtained analogously (see for example "d(pda) for mux, det" in Figure 2 as the result of applying q pda to P 2 in Figure 3).
One can show that, again, we have a strong representation system.
For aggregate queries where the aggregation is performed over single-path patterns, we have isolated aggregation from query processing, which is the crux we use to solve the problem of aggregate query answering in presence of probabilities.
In this paper, the focus is on the computation of α(q ¯ a ( P)), that is, evaluation of aggregate functions and not of queries over p-documents.
The problems.
Suppose we are given a px-space under the form of a p-document and we want to compute an aggregate function over it.
(Recall that, in general, we would like to ask first a query, compute the leaves that are of interest and then apply the aggregate query on these leaves.
We deal here with the simpler problem of computing the aggregate function on all the leaves.)
The result of an aggregate function over a px-space is a random variable.
Given an aggregate function α, we are interested in the following problems, where the input parameters are a p-document P with corresponding random forest P and possibly a number c:Membership: Given c, is c in the carrier of α(P), i.e., is Pr(α(P) = c) > 0?
Probability computation: Given c, compute Pr(α(P) = c).
Distribution computation: Find all c's such that Pr(α(P) = c) > 0, and for each such c compute Pr(α(P) = c).
Moment computation: Compute E(α(P) k ),where E is the expected value.One may want to return to a user the entire distribution as the result (distribution computation problem).
The membership and probability computation problems can be used to solve it.
Computing the distribution may be too costly or the user may prefer a summary of the distribution.
For example, a user may want to know its expected value E(α(P)) and the variance Var(α(r)).
In general the summary can be an arbitrary k-th moment E(α(P) k ) and the moment computation problem addresses this issue 4 .
In the following we investigate these four problems for the aggregate functions min, count, sum, countd and avg.
We do not discuss max and topK since they behave similarly as min.
We now study the four problems highlighted in Section 2 for the more general class of pdocuments, PrXML cie .
Following the definitions, one approach is to first construct the entire px-space of a p-document P, then to apply α to each document in P separately, combine the results to obtain the distribution α( P), and finally compute the answers based on α( P).
This approach is expensive, since the number of possible documents is exponential in the number of variables occurring in P.
Our complexity results show that for practically all functions and all problems nothing can be done that would be significantly more efficient.
Most decision problems are NP-complete while computational problems are FP #P -complete.
The only exception is the computation of moments for sum and count.
The intractability is due to dependencies between nodes of p-documents expressed using variables.
All intractability results already hold for shallow p-documents with very simple dependencies that connect only events that label siblings.
As an polynomial-time alternative, we present an approach to compute approximate solutions by Monte-Carlo methods.Functions in #P and FP #P .
We recall here the definitions of some classical complexity classes (see, e.g., [23]) that characterize the complexity of aggregation functions on PrXML cie .
An Nvalued function f is in #P if there is a non-deterministic polynomial time Turing machine T such that for every input w, the number of accepting runs of T is the same as f (w).
A function is in FP #P if it is computable in polynomial time using an oracle for some function in #P. Following [7], we say that a function is FP #P -hard if there is a polynomial-time Turing reduction (that is, a reduction with access to an oracle to the problem reduced to) from every function in FP #P to it.
Hardness for #P is defined in a standard way using Karp (many-one) reductions.
For example, the function that counts for every propositional 2-DNF formula the number of satisfying assignments is in #P and #P-hard [24], hence #P-complete.
We notice that the usage of Turing reductions in the definition of FP #P -hardness implies that any #P-hard problem is also FP #P -hard.
Therefore, to prove FP #P -completeness it is enough to show FP #P membership and #P-hardness.
Note also that #P-hardness clearly implies NP-hardness.
We now consider membership in FP #P .
We say that an aggregate function α is scalable if for every p-document P ∈ PrXML cie one can compute in polynomial time a natural number M such that for every d ∈ P the product M · α(d) is a natural number.
The following result is obtained by adapting proof techniques of [11].
Theorem 1.
Let α be an aggregate function that is computable in polynomial time.
If α is scalable, then the following functions mapping p-documents to rational numbers are in FP #P :(i) for every c ∈ Q, the function P → Pr(α(P) = c); (ii) for every k ≥ 1, the function P → E(α(P) k ).
The above theorem shows membership in FP #P of both probability and moment computation for all aggregate functions mentioned in the paper, since they are scalable.
For sum and count only the computation of moments is tractable.
Theorem 2.
For PrXML cie the following complexity results hold for sum and count:1.
Membership is NP-complete.2.
Probability computation is in FP #P .3.
Moment computation is in P for moments of any degree.To prove the theorem we show three lemmas.
The first lemma highlights why membership is difficult for PrXML cie and all the aggregate functions of our interest.Lemma 1 (Reducing Falsifiability to Membership).
Let AGG = {sum, count, min, countd, avg}.
For every propositional DNF formula φ one can compute in polynomial time a p-document P φ ∈ PrXML cie such that the following are equivalent: (1) φ is falsifiable, (2) Pr(α(P) = 1) > 0 overP φ for some α ∈ AGG, (3) Pr(α(P) = 1) > 0 over P φ for all α ∈ AGG.
Proof.
(Sketch) Let φ = φ 1 ∨ · · · ∨ φ n ,where each φ i is a conjunction of literals.
Then P φ has below its root a cie-node v with children v 0 , v 1 , . . . , v n , where each of v 1 , . . . , v n is labeled with the number 1 2 and v 0 with 1.
The edge to v 0 is labeled true while the edges to the v i are labeled with φ i .
Clearly, 1 is a possible value for each of sum, count, min, countd and avg if and only if in some world, v 0 is the only child of v, that is, if and only if φ is falsifiable.The next lemma shows that computation of the expected value for sum over a px-space, regardless whether it can be represented by a p-document, can be polynomially reduced to computation of an auxiliary probability.Lemma 2 (Regrouping Sums).
Let S be a px-space and V be the set of all leaves occurring in the forests of S. Suppose that the function θ labels all leaves in V with rational numbers and let sum S be the random variable defined by sum on S. ThenE(sum S k ) = (v 1 ,...,v k )∈V k k i=1 θ(v i ) Pr ({F ∈ S | v 1 , . . . , v k occur in F }) ,where the last term denotes the probability that a random forest F ∈ S contains all the nodes v 1 , . . . , v k .
Proof.
(Sketch) Intuitively, the proof exploits the fact that E(sum S ) is a sum over forests of sums over nodes, which can be rearranged as a sum over nodes of sums over forests.The auxiliary probability introduced in the previous lemma can be in fact computed in polynomial time for px-spaces represented by P ∈ PrXML cie .
Lemma 3 (Polynomial Probability Computation).
There is a polynomial time algorithm that computes, given a p-document P ∈ PrXML cie and leaves v 1 , . . . , v k occurring in P, the probabilityPr {F ∈ P | v 1 , . . . , v k occur in F } .
Proof.
(Sketch) Let φ i be the conjunction of all formulas that label the path from the root ofP to v i for 1 ≤ i ≤ k. Obviously, Pr {F ∈ P | v 1 , . . . , v k occur in F } = Pr(φ 1 ∧ · · · ∧ φ k ).
Note that each φ i is a conjunction of literals and so is their conjunction φ.
If some variable occurs both positively an negatively in φ, then φ is unsatisfiable and Pr(φ) = 0.
Otherwise, we collect all variables that occur positively in a set X + and all variables that occur negatively in a set X − .
Then Pr(φ) =x∈X + ∆(x) × x∈X − (1 − ∆(x)), where ∆ is the probability distribution over the event variables of P. Clearly, the variable sets X + and X − can be computed by traversing the forest underlying P. By marking nodes visited, the traversal will look at every edge of the forest at most once, which yields the claim.
Now we are ready to prove the theorem.Proof of Theorem 2.
1.
Pr(sum(P) = 1) > 0 or Pr(count(P) = 1) > 0 for a given P can be checked by guessing an assignment for the event variables of P and performing the aggregation over the resulting document.
Hardness follows from Lemma 1, since falsifiability of DNF-formulas is NP-complete [10].2.
Follows from Theorem 1 and the fact that sum and count is scalable.3.
By Lemma 2, the k-th moment of sum over P is the sum of |V | k products, where V is the set of leaves of P.
The first term of each product,k i=1 θ(v i ), can be computed in time at most | P| k .
By Lemma 3, the second term can be computed in polynomial time.
This shows that for every k ≥ 1, the k-th moment of sum can be computed in polynomial time.
The claim for count follows as a special case, where all leaves carry the label 1.
The following theorem shows that nothing is tractable for min, avg and countd.
1.
Membership is NP-complete.2.
Probability computation is FP #P -complete.
3.
Moment computation is FP #P -complete for moments of any degree.To prove the theorem we use the next lemma that highlights why membership is difficult.
Recall that #DNF is the problem of counting all satisfying assignments of a propositional DNF-formula.
Lemma 4 (Reducing #DNF to Probability and Expected Value Computation).
For every propositional DNF formula φ with n variables one can compute in polynomial time p-documents P φ ∈ PrXML cie and Q φ ∈ PrXML cie such that the following are equivalent: (1) φ has m satisfying assignments.
(2) Pr(min (P φ ) = 1) is m/2 n .
(3) E(min(Q φ )) is 1 − m/2 n .
Proof.
(Sketch) Let φ = φ 1 ∨ · · · ∨ φ n be in DNF with n variables.
Consider P φ that has below its root a cie-node v with children v 0 , v 1 , . . . , v n , where each of v 1 , . . . , v n is labeled with the number 1 and v 0 with 2.
The edge to v 0 is labeled with true while the edges to the v i are labeled with φ i .
To show (1) ∼ (2) observe that (by construction) probability of every P ∈ P is 1/2 n .
Moreover, min(P) = 1 holds if an only if P has at least two leaves: v 0 and any other from v i s. Hence, P is obtained by an assignment of the event variables that satisfies φ and Pr(min(P) = 1) is the sum over all such assignments multiplied by the probability of the resulting P, that is 1/2 n .
To show (1) ∼ (3) consider Q φ , a modification of P φ where the node v 0 is labeled with 1 and all the other v i s with 0, and apply similar reasoning.
Results of Lemma 4 can be extended to reduction of #DNF to probability and expected value computation for countd and avg.
Now we are ready to prove the theorem.Proof of Theorem 3.
1.
The proof is exactly the same as of the first claim of Theorem 2.2.
Hardness follows from Lemma 4 and Remark 1, since #DNF is #P-complete [10].
Membership follows from Theorem 1 and the fact that min, avg and countd are scalable.3.
Hardness for the first moment, as in the previous part of the theorem, follows from Lemma 4 and this implies hardness for moments of any degree higher than 1.
Membership can be shown as in the previous case of theorem.
As we see earlier in this section, for several aggregate functions on PrXML cie p-documents, probability computation and moment computation are hard.
Fortunately, there are general sampling techniques which give randomized approximation algorithms for tackling intractability of computing the above quantities.
In the following we present how to estimate cumulative distributions Pr(α(P) ≤ x) and moments E(α(P) k ).
We notice that from the cumulative distribution Pr(α(P) ≤ x) one can estimate the distribution Pr(α(P) = x) by computing Pr(α(P) ≤ x − γ) and Pr(α(P) ≤ x + γ) for some small γ (that depends on α) and taking the difference of them.For instance, suppose we wish to consider the aggregate function countd on a p-document P.
In particular, say we are interested in approximating the probability Pr(countd(P) ≤ 100).
This probability can be estimated by drawing independent random samples of the document, and using the ratio of samples for which countd is at most 100 as an estimator.
Similarly, if we wish to approximate E(countd(P)), we can draw independent random samples and return the the average of countd on the drawn samples.The first important question is: is it possible at all to have a reasonably small number of samples to get a good estimation?
It would not be helpful if an enormous number of samples is necessary.
The good news is that the answer to the above question is "yes".
The second question is: how many samples do we need?
The following classical result from the probability literature helps us to answer both questions.
Pr(| ¯ U − µ| ≥ ) ≤ 2 × exp − 2 2 T R 2 .
Approximating Distribution Points.
Suppose α is an aggregation function on some PrXML cie p-document P, and we wish to approximate the probability Pr(α(P) ≤ x) for some value x.
We sample instances of the p-document, and for each sample, let X i be the corresponding value of the aggregation function.
We let U i to be the Bernoulli variable that takes value 1 when X i ≤ x, and 0 otherwise.
Then, it follows that E(U i ) = Pr(α(P) ≤ x), and ¯ U := 1 T T i=1U i is an estimate of Pr(α(P) ≤ x).
Hence, we immediately obtain the following result for approximating a point for the cumulative distribution of an aggregation function.
P ∈ PrXML cie , any value x, and for any , δ > 0, it is sufficient to have O( 1 2 log 1 δ ) samples so that with probability at least 1 − δ, the quantity Pr(α(P) ≤ x) can be estimated with an additive error of .
Observe that the number of samples in Corollary 1 is independent of the instance size.
The only catch is that if the probability that we are trying to estimate is less than , then an additive error of would render the estimate useless.
Hence, if we only care about probabilities above some threshold p 0 , then it is enough to have the number of samples proportional to 1/p 2 0 (with additive error, say p 0 /10).
Approximating Moments.
Suppose f is some function on an aggregation function α, and we are interested in computing E(f (α(P))).
For each sample, we let U i := f (X i ), and compute the estimator ¯ U := 1T T i=1 U i .
P ∈ PrXML cie be a p-document and f be a function on an aggregation function α such that f (α(P)) takes value in an interval of width R. Then, for any , δ > 0, it is sufficient to have O( R 2 2 log 1 δ ) samples so that, with probability at least 1 − δ, the quantity E(f (α(P))) can be estimated with additive error of .
In particular, if α takes value in [0, R] and f (x) := α(P) k , then the k-th moment of α(P) around zero can be estimated with O( R 2k 2 log 1 δ ) samples.
Observe that if the range R has magnitude polynomial in the problem size, then we have a polynomial-time algorithm.
In our example of approximation E(countd( P)), the range R can be at most the size of the problem instance.
Hence, to estimate the expectation, it is enough to draw a quadratic number of random samples, that gives a polynomial time approximation algorithm.
.
Hence, to obtain a certain multiplicative error, one simply needs to get an estimate with some initial additive error (say 1).
If the resulting multiplicative error is too large, then one decreases the additive error (say by half), and obtain another estimate, until the multiplicative error is small enough.
We show that any px-space S that admits a description by a p-document P in PrXML mux,det can be constructed in a bottom-up fashion from elementary spaces S e by means of three operations: rooting, convex union, and product.
This construction reflects the structure of P.
Then we show that the distribution α(S) of a monoid aggregate function α on such a space S can be computed from α(S e )'s, the distributions over elementary spaces, by means of the three operations identity, convex sum and convolution, which correspond to rooting, convex union and product, respectively.
This construction gives a bottom-up algorithm to compute distributions α( P) on p-documents P. Operations on Sets of Forests.
We first introduce three operations on sets of forests: union, product, and rooting.
Let F 1 , . . . , F n be sets of forests.
All three operations are only applied to sets of forests that are mutually node-disjoint, that is, whereV(F i ) ∩ V(F j ) = ∅ for i = j.The union of such set of forests, denoted as F 1 · · · F n , is the union of disjoint sets.
Since the arguments are node-disjoint, they are mutually disjoint and for every forest F in the union there is a unique F i such that F ∈ F i .
The product, denoted with the operator "⊗", is defined asF 1 ⊗ · · · ⊗ F n := {F 1 ∪ · · · ∪ F n | F i ∈ F i }.
Similar to a Cartesian product, this product consists of all possible combinations of elements of the arguments.
Since the arguments are node disjoint, each forest in the product can be uniquely decomposed into its components F 1 , . . . , F n .
Rooting is a unary operator.
We define it first for individual forests.
Let v be a node, l a label and F a forest without v.
The rooting of F under v with l, denoted as rt l v (F ), is the document d obtained by combining all documents d in F under the new root v. Formally,root(d) := v, θ(root(d)) := l, V(d) := V(F ) ∪ {v}, E(d) := E(F ) ∪ {(v, root(d )) | d ∈ F }.
l v (F) := {rt l v (F ) | F ∈ F} where F is such that v does not occur in any F ∈ F.Operations on px-spaces.
Next we extend the above definitions to px-spaces S 1 , . . . , S n , where S i = (F i , Pr i ).
Our operations are only defined for px-spaces over node-disjoint sets of forests.Let p 1 , . . . , p n be nonnegative rational numbers such that n i=1 p i = 1.
We call such numbers convex coefficients.
The convex union of the S i wrt the p i is the px-space (F, Pr) where F = F 1 · · · F n and for every F ∈ F we define Pr(F ) := p i Pr i (F ) where i is the unique index such that F ∈ F i .
Intuitively, this means that the probability of an elementary event in the convex union is its original probability multiplied by the corresponding coefficient.
The convex union of the S i wrt the p i is denoted asp 1 S 1 · · · p n S n .
The product of the S i is the px-space (F, Pr) where F = F 1 ⊗ · · · ⊗ F n and for every F ∈ F we define Pr(F ) := Pr 1 (F 1 ) × · · · × Pr n (F n ) where F i ∈ F i are the unique forests such that F = F 1 ∪· · ·∪F n .
Intuitively, the elementary events of the product are combinations of elementary events in the original spaces and the probability of such a combination is the product of the probabilities of its components.
The product of the S i is denoted as S 1 ⊗ · · · ⊗ S n .
Let S 0 = (F 0 , Pr 0 ) be a px-space and v be a node with label l such that v does not occur in any forest in F 0 .
The rooting of S 0 under v with l is the px-space (F, Pr) where F = rt v (F 0 ) and for every element rt v F ∈ F we define Pr(rt v (F )) := Pr 0 (F ).
That is, rooting does not change probabilities.
The rooting of a space S under a node v with label l is denoted as rt l v (S).
The Algebra of px-Expressions.
Whenever we are given a collection of px-spaces, we can construct more complex ones using the three operations above.
A document is elementary if it consists of a single node.
The elementary document whose only node is v, carrying the label l, is denoted as d l v .
A px-space (F, Pr) is elementary if F contains exactly one elementary document, say d.
Since Pr is a probability, it follows that Pr(d) = 1.
By abuse of notation, we denote also such a space as d l v if no confusion can arise.
To describe the spaces that can be constructed in this way, we introduce expressions that are composed according to the ruleE, E 1 , . . . , E n → d l v | (p 1 E 1 · · · p n E n ) | (E 1 ⊗ · · · ⊗ E n ) | rt l v (E), where v ranges over all node identifiers, l over all labels and p 1 , . . . , p n over all sequences of convex coefficients.
An expression is a px-expression if any node-identifier is unique.Every px-expression can be evaluated to yield a unique px-space.
This is clearly the case for elementary expressions d l v .
Moreover, due to the condition on node-indentifiers, convex union and product are always applied to arguments that are node disjoint.
A similar argument applies to rooting.
We refer to the px-space denoted by E as E.Example 5.
The px-expression E = rt pda n 51 (0.7 (d 15 n 54 ⊗ d 44 n 55 ) 0.3 d 15 n 56 )denotes a px-space containing two documents d 1 and d 2 , where Pr(d 1 ) = 0.7 and Pr(d 2 ) = 0.3.
The root of both is node n 51 , which has two children in d 1 , namely nodes n 54 and n 55 , and a single child in d 2 , namely n 56 .
Expressivity of the px-Algebra.
In the following we show that the px-spaces representable by px-expressions are exactly those that are definable by p-documents of the class PrXML mux,det .
Since rooting is a unary operation in our algebra, we are only able to map px-expressions to a slightly restricted subclass of PrXML mux,det , which has the same expressivity as the full class.
A p-document is normalized if each ordinary node has at most one child.
We can transform any p-document into an equivalent normalized one if we place below each ordinary node v that is not a leaf a new det-node, say v , and turn all children of v into children of v .
P be a normalized p-document.
P E such that P E = E. 2.
There exists a px-expression E P such that E P = P.Proof.
(Sketch) From E, a p-document P E can be recursively constructed by creating for every convex union expression in E a mux-node, for every product expression a det-node and for every rooting expression an ordinary node.
The resulting document is normalized because rooting is a unary operation.For the construction of E P from P one proceeds analogously.
Since P is normalized, ordinary nodes can be mapped to the rooting operator.A straightforward comparison of the semantics of p-documents in Section 2 and the semantics of px-expressions shows that P E = E and E P = P.Example 6.
A p-document P E corresponding to the expression E in Example 5 is the subtree with root n 51 of P 2 in Figure 3.
Let α be an aggregate function that maps bags of elements of X to values in Y and let S = (F, Pr) be a px-space.
The application of α to S yields as a result the probability distribution α(S) over Y that satisfies α(S)(y) = Pr({F ∈ F | α(F ) = y}).
We will study how α(S) depends on the structure of S if the latter is defined by a px-expression.
In particular, we will show that we can evaluate monoid aggregate functions α on P ∈ PrXML mux,det by, first, computing α on the elementary px-spaces corresponding to the leaves of P and then combining the resulting distributions iteratively according to the structure of P in a bottom-up fashion.
We use the letter π as the generic notation for probability distributions.
All the distributions we consider in this paper are finite, that is, if π is a distribution over Y , then there are finitely many elements y 1 , . . . , y n ∈ Y such that p i := π({y i }) = 0 and n i=1 p i = 1.
For any y ∈ Y there is a probability distribution δ y such that δ y (z) = 1 if y = z and 0 otherwise.
δ y is the distribution of a Y -valued random variable if and only if that variable returns with probability 1 the value y.If p 1 , . . . , p n are convex coefficients and π 1 , . . . , π n are finite probability distributions over Y , then the convex sum of π 1 , . . . , π n wrt p 1 , . . . , p n , written as p 1 π 1 + · · · + p n π n , maps every y ∈ Y to p 1 π 1 (y) + · · · + p n π n (y).
The convex sum is a finite probability distribution over Y .
Proposition 3.
Let α be an aggregate function, S, S 1 , . . . , S n node-disjoint px-spaces, p 1 , . . . , p n convex coefficients, v a node identifier not occurring in S and l a label.
Then1.
α(d l v ) = δ α({|l| }) 2.
α(p 1 S 1 · · · p n S n ) = p 1 α(S 1 ) + · · · + p n α(S n ) 3.
α(rt l v (S)) = α(S).
Proof.
The first claim holds because over d l v the function α returns with probability 1 the value α({|l| }).
To see the second claim, suppose that the probability for α to return y over S i is q i for i = 1, . . . , n. Then, by definition, the probability for α to return y over the convex union is p 1 q 1 + · · · + p n q n .
The third claim holds because rooting does not change probabilities.With convex union and rooting alone, one can only construct trivial px-spaces.
Unfortunately, not much can be said in general about the relationship between α applied to a product space S and α applied to the arguments S i of the product.However, the situation is much better if α is a monoid aggregate function.
An element F of S is a forest that is the union of node-disjoint forests F i in S i and the value α(F ) can be obtained from the componentsF i of F , since α(F ) = α(F 1 · · · F n ) = n i=1 α(F i ).
Distributions on a monoid can be combined by an additional operation.
For any two finite distributions π 1 , π 2 over a monoid M with binary operation "⊕", the convolution is the distribution over M defined by(π 1 * π 2 )(m) = m 1 ,m 2 ∈M : m=m 1 ⊕m 2 π 1 (m 1 )π 2 (m 2 ).
Note that the convolution not only depends on the set M , but also on the monoid operation "⊕".
It is straightforward to see that convolution is an associative and commutative operation on the probability distributions over M and that the neutral element is the distribution δ ⊥ , where ⊥ is the neutral element of M .
Therefore, the notation π 1 * · · · * π n is unambiguous for all n ≥ 0.
S 1 , . . . , S n be node-disjoint px-spaces.
Then α(S 1 ⊗ · · · ⊗ S n ) = α(S 1 ) * · · · * α(S n ).
For every monoid aggregate function α with range M , we introduce a mapping "α(·)" that maps any px-expression E to a probability distribution α(E) over M .
The mapping is defined recursively as (i)α(d l v ) = δ α({|l| }) , (ii) α(p 1 E 1 · · · p n E n ) = p 1 α(E 1 ) + · · · + p n α(E n ), (iii) α(rt l v (E)) = α(E), and (iv) α(E 1 ⊗ · · · ⊗ D n ) = α(E 1 ) * · · · * α(E n ).
The following theorem says that the computation of distributions from expressions actually reflects the semantics of expressions.
Complexity Analysis.
Let S 1 , S 2 be px-spaces and α be a monoid aggregate function.
We say that α is monotone (with respect to product) if the following holds on the sizes of the distributions:|α(S 1 )| + |α(S 2 )| ≤ |α(S 1 ⊗ S 2 )|.
It is easy to see that min, count, sum are monotone aggregate functions.
P ∈ PrXML mux,det the distribution α( P) can be computed in time polynomial in |α( P)|.
Thus, for monotone aggregate functions the time for computing the answer distribution over a PrXML mux,det document is polynomial in the size of the output.
We now study the complexity of the four problems highlighted in Section 2 for the class of pdocuments PrXML mux,det .
Recall that this class is strictly less expressive than PrXML cie and query answering is essentially easier for it: there is a complexity jump from #P-complete for PrXML cie to P for PrXML mux,det [18].
We show that for aggregate functions PrXML mux,det is still easier than PrXML cie , but the complexity gap is not as clear as in the case of queries.For the monoid aggregate functions count and min, we show that probability distributions can be computed in polynomial time.
For sum they can be computed in time polynomial in the size of the output.
It means there is the same complexity jump in aggregation as in query answering.
For the non-monoid aggregate functions countd and avg we show that distribution computation is FP #P -complete.
It means PrXML mux,det and PrXML cie are equivalently difficult for aggregation with countd and avg.
The results exhibit a complexity borderline between monoid and non monoid aggregate functions for PrXML mux,det p-documents.
For min and count all four problems can be solved in polynomial time.
Proof.
Let V be the set of all the leaves of a given P, and L be the set of V 's labels.
Since all forests F ∈ P have nodes among V( P), the values min(F ) are in L. Therefore, the carrier of the distribution min( P) is a subset of L. Analogously, the carrier of the distribution count( P) is a subset of {1, . . . , |V |}.
Therefore, |min( P)| and |count( P)| are limited by | P|.
From this fact and Theorem 5 the claim of the proposition follows.Using the previous and the following propositions, we conclude that all four problems for min and count are polynomial.
Proposition 6.
Given a distribution of an aggregate function, one can decide membership and compute probability and moments in polynomial time in the size of the distribution.
We show that computing moments for sum is polynomial, and the other problems are easy only if the size of the distribution sum( P) is small.
The next lemma highlights why membership is difficult.
The Subset-Sum problem is, given a finite set A, an N-valued function s on A, and c ∈ N, is to decide whether there is some A ⊆ A such that a∈A s(a) = c. Lemma 5 (Reducing Subset-Sum to Membership).
For every set A, N-valued (weight) function s on A, and c ∈ N there is a p-document P A,s,c ∈ PrXML mux,det such that the following are equivalent: (1) Each v i has one child a i labeled with the number s(a i ).
The edges to the a i are labeled with 1/2.
Clearly, c is a possible value for sum if and only if in some world the only children of r are a i 's with labels summing-up to c, that is, if and only if there is A ⊆ A consisting of these a i 's with the weights summing-up to c.
Now we are ready to prove the theorem.Proof of Theorem 6.
1.
Membership can be checked by guessing a child for every mux node of P and performing the aggregation over the resulting document.
Hardness follows from Lemma 5 because Subset-Sum is NP-complete [10].2.
Follows from monotonicity of sum and Theorem 5.3.
The claim holds since there are at most exponentially many forests in the semantics of pdocuments.
Consider P that has a root r with one det child that in turn has n mux-children each i-th of which has one child v i labeled with the number 2 i , and the edges to v i are labeled with 1/2.
Then |sum( P)| = 2 n .4.
Follows from Lemma 2 and the fact that one can compute the probability Pr({F ∈ S | v 1 , . . . , v k occur in F }) that P contains all the nodes v 1 , . . . , v k in polynomial time.
This probability is 0 if there is a common mux ancestor of any two nodes from v 1 , . . . , v k .
Otherwise, one takes a subtree P v 1 ,...,v k of P with the root r and the leaves v 1 , . . . , v k , and the probability is the product of all probabilities that mark the edges of P v 1 ,...,v k .
Now now see that for non-monoid aggregates all but moment computation problems are hard.
We present next a lemma that highlights why probability computation is difficult for countd.
Recall that the #K-Cover problem is, given a finite set A, a set A of subsets of A and a constant k ∈ N, to count the number of different combinations A ⊆ A of elements from A that have the size k and cover A, that is, A = S∈A S. Lemma 6 (Reducing #K-Cover to Probability Computation).
For every set A, set A of subsets of A and k ∈ N one can compute in polynomial time a p-document P A,A,k ∈ PrXML mux,det and a constant c A,A,k such that the following are equivalent: (1) the number of A ⊆ A s.t. |A | = k and A = S∈A S is m, (2) Pr(countd(P A,B,k ) = k) is m × c A,A,k .
Proof.
(Sketch) Let A = {a 1 , . . . , a n }, and A = {S 1 , . . . , S u }, let f be a N-valued injective function on A. Let D be a set of k + 1 natural numbers disjoint from {f (S 1 ), . . . , f (S u )}.
Assume for each i that S i 1 . . . , S i t i are all the sets containing a i and t is the maximum among t i 's.
We construct P A,B,k as a p-document, having below its root n children a 1 , . . . , a n .
Each a i has a det child that has t children, namely S i 1 . . . , S i t i and D i 1 , . . . , D i (t−t i ) .
The edges to S i j s and D i j s are all labeled 1/t.
Each S i j is a leaf labeled with f (S i j ) and each D i j has k + 1 children each labeled with a distinct number from D. Nodes D i j are needed to guarantee the probability of every world of P A,B,k to be the same.
Intuition behind nodes S i j is they indicate that an element a i in A is "covered" with a set S i j in A .
One can see that countd(P A,B,k ) = k if and only if P A,B,k has no nodes D i j and k different values f (S i j ) on its leaves, that is, exactly k elements of A are chosen that "cover" A. Hence, Pr(countd(P A,B,k ) = k) = m × p, where p is the probability of every world of P A,B,k .
We are ready to prove the theorem.Proof of Theorem 7.
1.
Can be shown as in Theorem 6.
aggregate functions, and neither on computing the distribution of the aggregation.
Some connections exist to these works, however.
In particular, the complexity bounds obtained for probability computation of avg in Section 5.3 are a direct translation of the corresponding results for blockindependent databases presented in [25].
In addition to providing algorithms for, and a full characterization of, the complexity of computing the aggregation for both PrXML mux,det and PrXML cie models (which corresponds to the most studied and interesting probabilistic XML models), an originality of our contribution is to consider the expected value or other moments as summaries of the probability distribution of an aggregate function.
In the case of PrXML mux,det , we have identified a specific property of aggregate functions (that of being monoid aggregate functions) that entails tractability.
The intractability of most aggregation problems for PrXML cie has also led us to propose polynomial-time randomized approximation schemes.As the first work on aggregation queries in probabilistic XML, our research can be extended in a number of ways.
First, it is important to investigate more expressive query languages than the single-path queries we considered in Section 2.
The locality of PrXML mux,det should make it possible to evaluate the result of an aggregation defined by an arbitrary tree-pattern query [18], or even other forms of navigational queries defined with a bottom-up tree automaton [7].
Adapting the query evaluation algorithms proposed in [7,18] to aggregation queries is ongoing work.
This may allow us to deal with queries involving arbitrary grouping of the aggregation results too.
The work presented here could also be straightforwardly extended to p-documents with exp distributional nodes.
Finally, aggregation queries that inherently manipulate leaf values would be especially useful in conjunction with continuous probability distribution of these leaf values: the result of an experimental measure might be a uniform distribution between two values, or a Gaussian centered around a given value, etc.
A preliminary study of this problem seems to indicate that it is not harder to deal with such continuous probability distributions on leaves of a p-document, at least when integration and differentiation of these distributions is tractable, either symbolically or numerically.
