We consider a layered approach to source coding with side information received over an uncertain channel that minimizes expected distortion.
Specifically, we assume a Gaussian source encoder whereby the decoder receives a compressed version of the symbol at a given rate, as well as an uncompressed version over a separate side-information channel with slow fading and noise.
The decoder knows the realization of the slow fading but the encoder knows only its distribution.
We consider a layered encoding strategy with a base layer describing the source assuming worst-case fading on the side-information channel, and subsequent layers describing the source under better fading conditions.
Optimization of the layering scheme utilizes the Heegard-Berger rate-distortion function that describes the rate required to meet a different distortion constraint for each fading state.
When the side-information channel has two discrete fading states, we obtain closed-form expressions for the optimal rate allocation between the fading states and the resulting minimum expected distortion.
For multiple fading states, the minimum expected distortion is formulated as the solution of a convex optimization problem.
Under discretized Rayleigh fading, we show that the optimal rate allocation puts almost all rate into the base layer associated with the worst-case fading.
This implies that uncertain side information yields little performance benefit over no side information.
Moreover, as the source coding rate increases, the benefit of uncertain side-information decreases.
In lossy data compression, side information at the decoder can help reduce the distortion in the reconstruction of the source [1].
However, in scenarios such as distributed compression in a wireless sensor network, the side information may be acquired over an unreliable wireless channel.
In this work we consider a Gaussian source where the encoder is subject to a rate constraint and the distortion metric is mean squared error.
In addition to the compressed symbol, we assume that the decoder observes the original symbol through a separate analog fading channel.
We assume, similar to the approach in [2], that the fading is quasi-static, and that the decoder knows the fading realization but the encode knows only its distribution.
The rate-distortion function that dictates the rate required to satisfy the distortion constraint associated with each fading This work was supported by the US Army under MURI award W911NF-05-1-0246, the ONR under award N00014-05-1-0168, DARPA under grant 1105741-1-TFIND, and a grant from Intel.
state is given by Heegard and Berger in [3].
In this work we consider a layered encoding strategy based on the uncertain fading in the side-information channel, and optimize the rate allocation among the possible fading states to minimize expected distortion.When the side-information channel exhibits no fading, the distortion is given by the Wyner-Ziv rate-distortion function [4].
Rate-distortion is considered in [5], [6] when the side information is also available at the encoder, and in [7] when there is a combination of decoder-only and encoder-and-decoder side information.
Successive refinement source coding in the presence of side information is considered in [8].
In [9], [10], expected distortion is minimized in the transmission of a Gaussian source over a slowly fading channel in the absence of channel state information at the transmitter (CSIT).
Another application of source coding with uncertain side information is in systematic lossy source-channel coding [11] over a fading channel without CSIT: For example, when upgrading legacy communication systems, a digital channel may be added to augment an existing analog channel.
In this case the analog reception then plays the role of side information in the decoding of the description from the digital channel.The remainder of the paper is organized as follows.
The system model is presented in Section II.
Section III derives the minimum expected distortion when the sideinformation channel has discrete fading states.
Section IV presents numerical results under discretized Rayleigh fading.
Section V considers continuous fading distributions, followed by conclusions in Section VI.
Consider the system model shown in Fig. 1.
An encoder wishes to describe a real Gaussian source sequence {X} under a rate constraint of R X bits per symbol, where the sequence of random variables are independent identically distributed (iid) with X ∼ N(0, σ 2 X ).
The decoder, in addition to receiving the encoder's description, observes side information Y 񮽙 , whereY 񮽙 = √ SX + Z, with Z ∼ iid N (0, 1).
Hence the quality of the side information depends on S, the power gain of the side-information channel.
We assume S is a quasi-static random variable that is unchanged after its realization.
The decoder knows the realization of S, but the encoder knows only its distribution given by the probability density function (pdf) f S (s).
The decoder forms an estimate of the source and reconstructs the sequence { ˆ X}.
We are interested in minimizing the expected squared error distortion E [D] of the reconstruction, where D = (X − ˆ X) 2 .
Suppose the side-information channel has M discrete fading states.
Let the probability distribution of S be given as follows:R X {X} { ˆ X} E[D] S ∼ f S (s) Y 񮽙 = √ SX + Z Z ∼ N (0, 1) X ∼ N (0, σ 2 X )Pr{S = s i } = p i , i = 1, . . . , M ; M 񮽙 i=1 p i = 1, (1)where the s i 's are enumerated in ascending orders 1 < s 2 < · · · < s M .
Let Y 񮽙i denote the side information under fading state s i :Y 񮽙 i 񮽙 √ s i X + Z, i = 1, . . . , M.(2)Note that the set of side information random variables are stochastically degraded.
LetˆXLetˆ LetˆX i be the reconstruction when side information Y 񮽙 i is available at the decoder, and D i be the corresponding squared error distortion.
The minimum expected distortion under rate constraint R X is then given byE[D] * = min D : R(D)≤RX p T D,(3)wherep 񮽙 񮽙 p 1 . . . p M 񮽙 T , D 񮽙 񮽙 D 1 . . . D M 񮽙 T , and R(D)is the rate-distortion function that simultaneously satisfies the distortion set D.
The rate-distortion function that dictates the rate required to simultaneously satisfy a set of distortion constraints associated with a set of degraded side-information random variables is given by Heegard and Berger in [3] (an alternate form for M = 2 is described in [12]).
When the side information random variables satisfy the degradedness conditionX ↔ Y M ↔ Y M −1 ↔ · · · ↔ Y 1 , the rate- distortion function is R HB (D) = min W M 1 ∈P (D) M 񮽙 i=1 I(X; W i |Y i , W i−1 1 ),(4)where W i 1 denotes the vector W 1 , . . . , W i .
The minimization takes place over P (D), the set of all W M 1 jointly distributed with X, Y M 1 such that:W M 1 ↔ X ↔ Y M ↔ Y M −1 ↔ · · · ↔ Y 1 ,(5)and there exists decoding functionsˆXfunctionsˆ functionsˆX i (Y i , W i 1 )'s under given distortion measures d i 's that satisfyE[d i (X, ˆ X i )] ≤ D i , i= 1, . . ., M.(6)As noted in [3], since R HB (D) depends on X, Y M 1 only through the marginal distribution p(x, y i ), i = 1, . . ., M , the degradedness of the side information need not be physical.
We construct Y M 1 to have the same marginals as (3) is then given by the Heegard-Berger rate-distortion function (4) with squared error distortion measuresY 񮽙 M 1 by setting p(y i |x) = p(y 񮽙 i |x), i = 1, . . ., M .
The rate-distortion function R(D) ind i (X, ˆ X i ) = (X − ˆ X i ) 2 .
First we consider the case when the side-information channel has only two discrete fading states (M = 2).
The Heegard-Berger rate-distortion function for this case isR HB (D 1 , D 2 ) = min W1,W2 ∈P (D1 ,D2) {I(X; W 1 |Y 1 ) + I(X; W 2 |Y 2 , W 1 )}.
(7)For a Gaussian source under a squared error distortion measure, a jointly Gaussian codebook is optimal [3], [13].
When W M 1 , X are jointly Gaussian, the mutual information expressions in (7) evaluate toI(X; W 1 |Y 1 ) + I(X; W 2 |Y 2 , W 1 ) = h(X|Y 1 ) − h(X|Y 1 , W 1 ) + h(X|Y 2 , W 1 ) − h(X|Y 2 , W 1 , W 2 ) (8) = 1 2 log(VAR[X|Y 1 ]) − 1 2 log VAR[X|Y 1 , W 1 ] VAR[X|Y 2 , W 1 ] − 1 2 log(VAR[X|Y 2 , W 1 , W 2 ]) (9) = − 1 2 log(s 1 + σ −2 x ) − 1 2 log 񮽙 1 + (s 2 − s 1 )VAR[X|Y 1 , W 1 ] 񮽙 − 1 2 log(VAR[X|Y 2 , W 1 , W 2 ]),(10)where log is base 2, and (10) follows from expanding the conditional variance expressions by applying Lemma 1 and Corollary 1 as given below.Lemma 1: Let X, W k 1 be jointly Gaussian random vari- ables.
If Y = √ sX +Z, where Z ∼ N (0, 1) is independent from X, W k 1 , then VAR[X|Y, W k 1 ] = 񮽙 VAR[X|W k 1 ] −1 + s 񮽙 −1 .
(11)Proof: The lemma follows from the minimum mean square error (MMSE) estimate of Gaussian random variables.
Let X, W, whereW 񮽙 񮽙 W 1 . . . W k 񮽙 T , be dis- tributed as 񮽙 W X 񮽙 ∼ N 񮽙񮽙 μ W μ X 񮽙 , 񮽙 Σ W Σ WX Σ T WX σ 2 X 񮽙񮽙 .
(12)The conditional distribution is Gaussian [14], and the corresponding variance isVAR[X|Y, W] = σ 2 X − 񮽙 Σ WX √ sσ 2 X 񮽙 T 񮽙 Σ W √ sΣ WX √ sΣ T WX sσ 2 X + 1 񮽙 −1 񮽙 Σ WX √ sσ 2 X 񮽙 (13) = σ 2 X − Σ T WX Σ W −1 Σ WX 1 + s(Σ T WX Σ W −1 Σ WX ) (14) = 񮽙 VAR[X|W] −1 + s 񮽙 −1 .
(15)񮽙 Corollary 1: Let Y j = √ s j X + Z, Y i = √ s i X + Z. VAR[X|Y i , W k 1 ] VAR[X|Y j , W k 1 ] = 1 + (s j − s i )VAR[X|Y i , W k 1 ].
(16)񮽙 We substitute (10) in (7), and minimize over W 1 , W 2 to obtain:R HB (D 1 , D 2 ) = − 1 2 log(s 1 + σ −2 x ) + min W1 񮽙 − 1 2 log 񮽙 1 + (s 2 − s 1 )VAR[X|Y 1 , W 1 ] 񮽙 + min W2 񮽙 − 1 2 log(VAR[X|Y 2 , W 1 , W 2 ]) 񮽙 񮽙 .
(17)In the inner minimization in (17),R HB (D 1 , D 2 ) is decreas- ing in VAR[X|Y 2 , W 1 , W 2 ]; hence the choice of W 2 is optimal when max W2 VAR[X|Y 2 , W 1 , W 2 ] = min(VAR[X|Y 2 , W 1 ], D 2 ),(18)where the first term in the min(·) expression follows from the non-negativity of mutual information I(X; W 2 |Y 2 , W 1 ), and the second one follows from the distortion constraint:VAR[X|Y 2 , W 1 , W 2 ] = E 񮽙񮽙 X − ˆ X 2 (Y 2 , W 1 , W 2 ) 񮽙 2 񮽙 ≤ D 2 .
(19)Similarly, in the outer minimization in (17), W 1 is optimal whenmax W1 VAR[X|Y 1 , W 1 ] = min(VAR[X|Y 1 ], D 1 ),(20)which follows from the non-negativity of I(X; W 1 |Y 1 ), and the distortion constraint:VAR[X|Y 1 , W 1 ] = E 񮽙񮽙 X − ˆ X 1 (Y 1 , W 1 ) 񮽙 2 񮽙 ≤ D 1 .
(21)Next, we consider the construction of W 1 , W 2 that achieve the rate-distortion function, namely jointly Gaussian random variables with conditional variances that satisfy (18), (20).
We construct W 1 , W 2 as given byW 1 = a 1 X + N 1 , W 2 = a 2 X + N 2 ,(22)whereN i ∼ iid N (0, 1), i = 1, 2, is independent from X, Y 1 , Y 2 .
For notational convenience, we defineR 1 񮽙 min W1 I(X; W 1 |Y 1 )(23)R 2 񮽙 min W2 I(X; W 2 |Y 2 , W 1 ).
(24)We interpret R 1 as the rate of a source coding base layer that describes X when the side-information quality is that of Y 1 or better.
On the other hand, R 2 is the rate of a top layer that describes X only when the decoder has the better side information Y 2 .
The rate of the base layer under optimal W 1 is given byR 1 = min W1 {h(X|Y 1 ) − h(X|Y 1 , W 1 )}(25)= 1 2 log (σ −2 X + s 1 ) −1 ˜ D 1 ,(26)where˜Dwhere˜ where˜D 1 񮽙 min 񮽙 D 1 , (σ −2 X + s 1 ) −1 񮽙 .
(27)The a 1 that achieves (26) is determined from the constraint˜D constraint˜ constraint˜D 1 = VAR[X|Y 1 , W 1 ], which evaluates toa 1 = ˜ D −1 1 − σ −2 X − s 1 .
(28)Similarly, under optimal W 2 , the rate of the top layer isR 2 = min W2 {h(X|Y 2 , W 1 ) − h(X|Y 2 , W 1 , W 2 )}(29)= 1 2 log( ˜ D −1 1 + s 2 − s 1 ) −1 ˜ D 2 ,(30)where˜Dwhere˜ where˜D 2 񮽙 min 񮽙 D 2 , ( ˜ D −1 1 + s 2 − s 1 ) −1 񮽙 .
(31)The a 2 that achieves (30) a 2 = ˜ D −1 2 − ˜ D −1 1 − (s 2 − s 1 ).
(32)Finally, we substitute (26), (30) in (7) to obtain the ratedistortion function:R HB (D 1 , D 2 ) = R 1 + R 2 (33) = − 1 2 log(σ −2 X + s 1 ) − 1 2 log˜Dlog˜ log˜D 2 − 1 2 log 񮽙 1 + (s 2 − s 1 ) ˜ D 1 񮽙 ,(34)where˜Dwhere˜ where˜D 1 , ˜ D 2 are as defined in (27), (31).
Note that the derivation of (34) depends on the side information only through the marginals p(y i |x)'s; therefore, the ratedistortion function applies as well to the stochastically degraded side information Y 񮽙 M , . . . , Y 񮽙 1 .
Under a source coding rate constraint of R X , the achiev-able distortion set is {(D 1 , D 2 ) | R HB (D 1 , D 2 ) ≤ R X }.
Setting R HB (D 1 , D 2 ) = R X , the boundary of {(D 1 , D 2 )} defines the Pareto optimal trade-off curve between the two distortion constraints, which is given byD 2 = 񮽙 2 2RX (σ −2 X + s 1 ) 񮽙 1 + (s 2 − s 1 )D 1 񮽙񮽙 −1 ,(35)over the interval: We find the optimal operating point on the Pareto curve to minimize the expected distortion:񮽙 2 2RX (σ −2 X + s 1 ) 񮽙 −1 ≤ D 1 ≤ (σ −2 X + s 1 ) −1 .
(36)E[D] * = min D1 ,D2 : RHB (D1,D2 )≤RX p 1 D 1 + p 2 D 2(37)After substituting (35) in (37), from the Karush-KuhnTucker (KKT) optimality conditions we obtain the optimal base layer distortion D * 1 :D * 1 = min 񮽙 max(D − 1 , D 񮽙 1 ), D + 1 񮽙 ,(38)whereD − 1 = (2 2RX (σ −2 X + s 1 ) 񮽙 −1(39)D 񮽙 1 = 1 s 2 − s 1 񮽙񮽙 2 2RX σ −2 X + s 1 s 2 − s 1 p 1 p 2 񮽙 −1/2 − 1 񮽙(40)D + 1 = (σ −2 X + s 1 ) −1 ,(41)and the optimal top layer distortion D * 2 :D * 2 = min 񮽙 max(D − 2 , D 񮽙 2 ), D + 2 񮽙 ,(42)whereD − 2 = (2 2RX (σ −2 X + s 2 ) 񮽙 −1(43)D 񮽙 2 = 񮽙 2 2RX (σ −2 X + s 1 )(s 2 − s 1 )p 2 /p 1 񮽙 −1/2(44)D + 2 = 񮽙 2 2RX (σ −2 X + s 1 ) + s 2 − s 1 񮽙 −1 .
(45)The corresponding optimal rate allocation R * 1 , R * 2 can be found as given in (26), (30).
The optimal rate allocation is plotted in Fig. 2 for R X = 1, σ 2 X = 1, and s 1 = 0 dB.
Note that R * 2 , the rate allocated to the top layer, is not monotonic with the side-information channel condition.
As fading state s 2 improves, R * 2 increases to take advantage of the better sideinformation quality.
However, when s 2 is large, R * 2 begins to decline as the expected distortion is dominated by the worse fading state.
Moreover, the optimal rate allocation is heavily skewed towards the lower layer: R * 2 > 0 only when p 2 is large.
The rate-distortion function (34) extends directly to the case when the side-information channel has multiple discrete fading states (M > 2).
Specifically, we construct the random variables W i 's to beW i = a i X + N i , i= 1, . . . , M,(46)where N i ∼ iid N (0, 1), then the rate of the i th layer isR i 񮽙 min Wi I(X; W i |Y i , W i−1 1 ) (47) = min Wi {h(X|Y i , W i−1 1 ) − h(X|Y i , W i 1 )} (48) = 1 2 log ( ˜ D −1 i−1 + s i − s i−1 ) −1 ˜ D i ,(49)where˜Dwhere˜ where˜D i 񮽙 min 񮽙 D i , ( ˜ D −1 i−1 + s i − s i−1 ) −1 񮽙 .
(50)The a i that achieves (49) a i = ˜ D −1 i − ˜ D −1 i−1 − (s i − s i−1 ).
(51)As (49) in (4) to obtain the rate-distortion function:R HB (D) = 񮽙 M i=1 R i , we substituteR HB (D) = − 1 2 log(σ −2 X + s 1 ) − 1 2 log˜Dlog˜ log˜D M − 1 2 M −1 񮽙 i=1 log 񮽙 1 + (s i+1 − s i ) ˜ D i 񮽙 ,(52)where˜Dwhere˜ where˜D i is as defined in (50).
Unlike the case when M = 2, however, a closed-form expression for the minimum expected distortion E[D] * does not appear analytically tractable.
Nevertheless, the expected distortion minimization in (3) can be formulated as the following convex optimization problem over the variablesD i , ˜ D i for i = 1, . . ., M : minimize p T D (53) subject to 0 ≤ D i , 0 ≤ ˜ D i , i= 1, . . ., M(54)− 1 2 log(σ −2 X + s 1 ) − 1 2 log˜Dlog˜ log˜D M − 1 2 M −1 񮽙 i=1 log 񮽙 1 + (s i+1 − s i ) ˜ D i 񮽙 ≤ R X(55)˜ D 1 ≤ (σ −2 X + s 1 ) −1 (56) ˜ D i ≤ ( ˜ D −1 i−1 + s i − s i−1 ) −1 , i = 2, . . . , M (57) ˜ D i ≤ D i , i= 1, . . ., M.(58)The constraints (56), (57) and (58) derive from expanding the condition in (50).
The minimization can be efficiently computed by standard convex optimization techniques [15].
Moreover, convexity implies that a local optimum is globally optimal.
Note that under the KKT optimality conditions,˜ D * i = D * i for i = 1, . . ., M .
It implies that the set of distortion inequality constraints (6) are tight when the expected distortion is minimized.
Fig. 3.
Minimum expected distortion in source coding with uncertain side information (σ 2 X = 1, ¯ S = 10 dB).
Distortion D No-SI E[D] * E[D E-SI ] D SI * , when the side-information channel is described by a discretized Rayleigh fading distribution.
We assume σ 2 X = 1, and ¯ S 񮽙 E[S] = 10 dB.
The Rayleigh distribution is truncated at 2 ¯ S, and discretized into M = 20 fading states with evenly spaced channel power gains s 1 , . . . , s M .
For comparison, along with E[D] * , in Fig. 3 we also show the distortion under different assumptions on the side information.
When no side information is available, the distortion is given by the rate-distortion function for a Gaussian source [16]:D No-SI = σ 2 X 2 −2RX .
(59)In the absence of side information, D No-SI is an upper bound to E[D] * .
Next, if the encoder knows the realization of S, then for each fading state it can achieve the Wyner-Ziv rate-distortion function [4], and the corresponding expected distortion isE[D E-SI ] = 񮽙 ∞ 0 f S (s)(σ −2 X + s) −1 ds 2 −2RX .
(60)With knowledge of S at the encoder,E[D E-SI ] is a lower bound to E[D] * .
Finally, when there is no uncertainty in the side-information channel with S = ¯ S, the distortion is given by the Wyner-Ziv rate-distortion function:D SI = (σ −2 X + ¯ S) −1 2 −2RX .
(61)By Jensen's inequality, E[D E-SI ] > D SI ; hence uncertainty in the side-information channel always hurts performance.
We observe in Fig. 3 that when R X is small, E[D] * achieves a smaller distortion than when no side information is available.
However, when R X is large, E [D] * is almost as large as D No-SI ; the uncertain side information is negligibly more useful than having no side information at all.
Remarkably, for all R X (and a wide range of parameters σ 2 X and ¯ S), the minimum expected distortion is achieved at R * 1 = R X and R * i = 0 for i = 2, . . . , M .
Hence the optimal rate allocation concentrates at the base layer of the source code.
These numerical results suggest that the optimal rate allocation to minimize the expected distortion is conservative: unless the better fading states are highly probable, no rate is allocated to the source coding layers other than the base layer, since it is not beneficial to dedicate source coding layers to take advantage of these better states.
When R X is small, the reduction in VAR [X] In this section, we investigate the optimal rate allocation and minimum expected distortion when the sideinformation channel fading distribution is continuous.
We consider the expected-distortion-rate function, i.e.,E[D] as a function of R 񮽙 񮽙 R 1 . . . R M 񮽙 T , since it is continuousand differentiable in the entire nonnegative orthant{R i ≥ 0, i = 1, . . . , M }.
The expected distortion is E[D] = p T D = p 1 D 1 + p 2 D 2 + · · · + p M D M , (62)where D i is found by recursively expanding (49):D i = 񮽙 (((σ −2 X + s 1 )2 2R1 + s 2 − s 1 )2 2R2 + · · · )2 2RM 񮽙 −1 .
(63)Suppose the channel power gains of the fading states start at s 1 = 0, and they are evenly spaced with s i+1 −s i = Δs.
In the limit of Δs → 0, M → ∞, the fading probability is given by the continuous pdf f S (s):p i ∼ = f S [i]Δs, where f S [i] 񮽙 f S ((i − 1)Δs),(64)and the rate allocation is given by the continuous rate distribution function R(s):R i ∼ = R[i]Δs, where R[i] 񮽙 R((i − 1)Δs).
(65)The expected distortion over f S (s) isE[D] = lim Δs→0 ∞ 񮽙 i=1 f S [i]D[i]Δs,(66)whereD[i] = 񮽙 σ −2 X 2 2 񮽙 i j=1 R[j]Δs + i−1 񮽙 j=1 񮽙 2 2 񮽙 i k=j R[k]Δs 񮽙 Δs 񮽙 −1 ,(67)which follows from substituting s i+1 − s i = Δs in (63 E[D] = 񮽙 ∞ 0 f S (s)u 񮽙 (s) 񮽙 σ −2 X + u(s) 񮽙 −1 ds,(68)where u(s) 񮽙񮽙 s 0 2 −2 񮽙 t 0 R(r) dr dt,with the boundary conditions:u 񮽙 (0) = 1, u 񮽙 (∞) = 2 −2RX .
Over any interval where the optimal rate distribution R * (s) is continuous, the corresponding u(s), u 񮽙 (s) are also continuous, and they have to satisfy the necessary condition for optimality as given by the Euler-Lagrange equation [17]:d ds 񮽙 ∂F ∂u 񮽙 񮽙 − ∂F ∂u = 0,(69)whereF (s, u, u 񮽙 ) 񮽙 f S (s)u 񮽙 (s) 񮽙 σ −2 X +u(s) 񮽙 −1 .
Taking the derivatives, (69) evaluates to:f 񮽙 S (s) σ −2 X + u(s) = 0.
(70)We suppose in general for the given fading distribution, f 񮽙 S (s) 񮽙 = 0; then no u(s) satisfies (70), and the EulerLagrange condition does not lead to a continuous solution for the optimal rate distribution.
We conjecture that the optimal rate allocation is discrete even when the fading distribution is continuous and smooth.
While the Euler-Lagrange equation (69) does not prescribe the necessary conditions for discrete rate allocation, the numerical results in Section IV suggest that under Rayleigh fading the optimal rate allocation may concentrate at the lowest layer, i.e., R(s) = R X δ(s).
In this section we assume such rate allocation and investigate the expected distortion under Rayleigh fading.Let the pdf of the Rayleigh fading distribution be f S (s) = ¯ S −1 e −s/ ¯ S , s ≥ 0.
When the rate distribution is R(s) = R X δ(s), the expected distortion evaluates to e −t /t dt.
Note that when R X is large such thatE[D] = 񮽙 ∞ 0 f S (s) σ −2 X 2 2RX + s ds (72) = − ¯ S −1 exp( ¯ S −1 σ −2 X 2 2RX ) Ei(− ¯ S −1 σ −2 X 2 2RX ),(σ −2 X 2 2RX 񮽙 K 񮽙 ¯ S, E[D] ≈ 񮽙 K 0 f S (s) σ −2 X 2 2RX + s ds ≈ 񮽙 K 0 f S (s) σ −2 X 2 2RX ds (74) ≈ σ 2 X 2 −2RX = D No-SI .
(75)Therefore, as is observed in Fig. 3, under Rayleigh fading with large R X , we expect uncertain side information is no more useful than no side information.
We considered the problem of Gaussian source coding under squared error distortion with uncertain side information at the decoder.
When the side-information channel has two discrete fading states, we derived closed-form expressions for the optimal rate allocation among the fading states and the corresponding minimum expected distortion.The optimal rate allocation is conservative: rate is allocated to the non-base code layer only if the better fading state is highly probable.
Otherwise the distortion reduction from utilizing non-base code rate when the better state is realized is not sufficient to compensate for the distortion increase that results from reducing the code rate of the base layer to allocate rate to the higher layer.
For multiple discrete fading states, the minimum expected distortion was shown to be the solution of a convex optimization problem.
Under discretized Rayleigh fading, it is observed that the optimal rate allocation concentrates at the base layer associated with the worst-case fading state, and the uncertain side information is negligibly more useful than no side information, especially when the source coding rate is large.
