Digital forensics can be a difficult discipline to teach effectively because of its interdisciplinary nature, closely integrating law and computer science.
Prior research in Physics and Computer Science has shown that the traditional lecture approach is inadequate for the task of provoking students' thought-processes and systematically engaging them in problem-solving during class.
Peer instruction is an established pedagogy for addressing some of the challenges of traditional lectures.
For this paper, we developed 108 peer instruction questions for a digital forensics curriculum, and evaluated a selection of the questions by holding a condensed computer forensics workshop for university students.
The evaluation results show that peer instruction helps students understand the targeted digital forensics concepts, and that 91% of students would recommend that other instructors use peer instruction.
Digital forensics is defined as the application of scientific tools and methods to identify, collect, and analyze digital artifacts in support of legal proceedings [11].
It is a challenging field to teach effectively because of its positioning at the intersection of law and computer science.
For instance, the data acquired from a suspect computer (a.k.a digital evidence) without a search warrant may prevent the evidence from being admitted in court.Students need to achieve a clear understanding of important principles of performing a forensic investigation within the constraints put forth by state and federal law.
Often, forensic instructors combine traditional lectures with hands-on lab exercises to improve the student learning outcomes.
Typical exercises include solving cybercrime cases such as theft of credit card data and malware attacks.
In our experience, forensic hands-on exercises are effective for students who have a good understanding of individual forensic concepts relating to the lab task, and are able to integrate them into a broader view of a forensic investigative scenario.Unfortunately, the traditional lecture-centric approach is often inadequate in providing the conceptual preparation for hands-on labs and exercises.
Commonly, lectures fail to motivate students to study before class, and therefore the in-class learning experience is less effective than it could be.
The traditional lecture also lacks a reliable in-class mechanism to help an instructor ensure that students understand the concepts being taught.
Although the instructor may ask ad-hoc questions to stimulate discussion, this usually engages a few students and does not provide feedback about every student in class.Peer instruction has emerged as a promising solution to enrich the in-class experience such that it yields better outcomes.
It was first introduced by Eric Mazur in physics classrooms at Harvard University [2].
With peer instruction, a lecture consists of a series of multiplechoice questions (a.k.a. ConcepTests) that are designed to provoke deep conceptual thinking in students and engage them in a meaningful discussion.
Students are required to study some reading material before coming to class, which helps them to find correct answers to the peer instruction questions.
Peer instruction has shown to be effective in physics, computer science, and biology classrooms [7].
In particular in computer science, it has halved failure rates in four courses [7] and increased student retention in the computer science major [10].
Previously, peer instruction has not been explored as a means for delivering a digital forensics curriculum.
We have developed 108 peer instruction questions for an introductory course on computer forensics.
This paper discusses some of these questions as examples and analyzes classifies with respect to two basic criteria.
First, what are the elements used in the questions to trigger conceptual thinking processes of students such as "compare and contrast," "deliberate ambiguity" and "trolling for misconception"?
We use Beatty et al.'s concept triggers [1] for the analysis.
Second, how are the questions presented, such as in a scenario, example, or diagram.The paper also presents the results of a digital forensic workshop utilizing peer instruction methodology and questions.
The evaluation results show that most of the students find the use of clickers beneficial and the group discussion for a peer question helps the students understand a target concept.
91% students would recommend other instructors use peer instruction.
The results also show a clear evidence of improvement in student learning that is measured through quizzes taken before and after a topic is covered and peer instruction questions replied to by the students before and after peer discussions.The rest of the discussion is organized as follows.
Section 2 discusses the peer instruction methodology and related work.
Section 3 presents the elements of a peer instruction question (i.e. concept triggers and question presentation) along with the steps to create a peer question.
Section 4 discusses four examples of peer instruction questions and identifies their concept triggers and question presentation types, followed by section 5 that presents an analysis of 108 peer instruction questions recently developed by the authors.
Sections 6, 7, and 8 present the peer instruction implementation in a digital forensic workshop, a list of peer instruction questions used in the workshop, and evaluation results, followed by conclusion in section 9.
Peer instruction pedagogy could be categorized under the general umbrella of a"flipped classroom" approach, but the term "peer instruction" describes a specific protocol, and does not mean generally any use of peer discussion or clickers.
The protocol has three important elements.
First, it divides a class lecture into a series of multiplechoice conceptual questions.
The questions focus on the primary concepts an instructor wishes to teach.
Second, it divides the students in class into a number of small groups for discussions.
Third, it requires students to read some material before coming to class.
The material is related to the topic covered in the class and enables the students to find answers of the questions.For each question, the following steps are involved:• A peer instruction question is posed to students.
The students use clickers to reply.
• The instructor is able to view the results.
If the answers are mostly incorrect, the instructor then encourages the students to discuss their answer choices with their group members.
• The instructor then poses the same question to the students again; the students reply to the question.
• Depending on the answers, the instructor would have the option to discuss the concept in further detail with the students to ensure a better understanding; or, if the students clearly understand the concept at this point, the instructor may choose to continue to the next question.
Peer instruction has not been widely adopted in the cybersecurity classroom.
Recently, Johnson et al. [4] developed peer instruction questions for two courses: introduction to computer security, and network penetration testing.
They developed a set of 172 peer instruction questions; however, these were not evaluated in a classroom setting.
More broadly, there have been a number of prior efforts to use peer instruction in the computer science classroom.
Porter et al. [8] perform a multi-institutional study of the usage of peer instruction in seven instructors' introductory programming courses.
Considering instructors' prior experience (or lack thereof) utilizing peer instruction, they primarily focus on student perception of peer instruction using measurements such as perceived question difficulty, question time allowed, discussion time allowed, and content difficulty.
They obtain participants' responses using surveys and note that at least 71% of students would recommend other instructors to use peer instruction.Similarly, Porter et al. conduct a measurement of peer instruction across multiple small liberal arts colleges to measure the effectiveness of peer instruction in smaller classes, using data from five instructors at three institutions [9].
They notice normalized gains in the same range that of larger universities with students generally approving of the method and their performances.Esper [3] utilizes peer instruction in a software engineering course that has 189 students.
She makes a modification in the standard peer instruction process in which a clicker question is initially shown without answers and both the students and instructor propose potential answer choices with discussions of those answers.
It is worth noting that the instructor does not mention whether an answer suggestion is correct or incorrect.
The evaluation results show that 72% of the students would recommend the course instructor, with 28% not recommending due to the reasons such as correct answers are not given and clicker questions are unclear.Liao et al. [6] create models of in-class clicker questions to predict low-performing students early in the term.
They use a linear regression model to predict final exam scores with approximately 70% accuracy in a twelve-week introductory computer science course.Lee et al. [5] examine the effectiveness of peer instruction in two upper-level computer science courses: Theory of Computation and Computer Architecture and find the average normalized learning gains of 39%.
A peer instruction question has two important elements: concept trigger(s) and question presentation.
A concepttrigger is a hidden element in multiple choices of a question that is deliberately introduced in the question.
The hidden element triggers the students thought process and helps them with useful peer discussion allowing them to understand the target concept and further eliminate either incorrect choices or identify a correct choice.
We utilize concept-triggers from Beatty et al. [1].
Table 1 presents a list of concept-triggers used in our questions such as Use "none of the above", Omit necessary information, and Trap unjustified assumptions.A question presentation refers to how a question is written or articulated.
We use five presentation types: scenario, example, definitional, diagram, and feature.
A concept may be comprised of multiple components or features.
If a question or its multiple choices target these features, we classify the question as feature.
The question asks students to identify required features of a concept in multiple choices, or a feature is provided in a question which asks students to identify its corresponding concept in the choices.To develop a question, an instructor should first thoughtfully select a target concept.
Peer instruction questions do take a significant amount of class time, and so questions should focus on the most essential elements of the course.
Second, the instructor should select a question presentation that is suitable to articulate the concept.
Finally, the instructor should choose at least one concept trigger.
A question can have multiple concept triggers (examples are given in the next section).
In the FAT32 file system, there are two primary data structures used to maintain knowledge of data: the directory entry and the file allocation table.
A directory entry is present for every file and folder and contains the file name, file size, the address of the starting cluster of a file's content.
The file allocation table is used to identify the next cluster in a file and the allocation status of clusters.
The final cluster of a file has a marking EOF in its FAT entry that indicates end of file.
Therefore, in order to access a specific file from a FAT file system, the filename and starting cluster are necessary.A peer instruction question on this concept can be created as follows: In order to access a file from a FAT file system, what information is absolutely necessary?
a) Name and ending address of file content; b) Name, file size, and ending address of file content; c) Name and starting address of file content, d) File size and starting address of file content, e) None of the above (answer: c).
Concept trigger.
We use "none of the above" to present a possibility that the provided choices may be incorrect.
We also use "trap unjustified assumptions" by introducing file-size in choices-FAT file system's data structures allow file clusters to be accessed similarly to a linked-list, whereas the last cluster is marked as EOF thereby does not require file size to access a file.Question presentation.
This question is presented as a feature question as it draws upon and asks about core features of the FAT file system and its data structures.
File carving is an important concept in computer forensics.
It is a method used to recover files when they are inaccessible through file system, such as deleted files, or a corrupted file system.
In many cases, the file content on disk remains intact and can be recovered by file carving.A basic form of file carving assumes that a file blocks are physically located in a sequence on disk and requires only the location of the first and last block of a file to recover the file content.
It searches for file headers and footers using predefined signatures to identify the first and last blocks of files.
This technique however, suffers in the presence file fragmentation that can cause a file's blocks to be store non-contiguously.
In addition, while defragmenting a drive places allocated file blocks in contiguous pieces, the defragmentation process may overwrite blocks that have been marked by the file system as deallocated.A peer instruction question on basic file carving addressing the above-stated challenges can be created as follows: In which of the following situations is file carving most effective?
a) The targeted drive is highly fragmented, b) The targeted drive has been recently defragmented, c) The system being used to examine the drive has low free space, d) The system being used to examine the drive has high free space, e) More than one of the above (answer: d).
Concept trigger.
Deconstructing the question, we note that two triggers are used.
First, as we provide the potential for multiple question choices in option (d), we use "identify a set or subset.
"To further assess a student's understanding of the downsides of both fragmentation and defragmentation processes, we use "trolling for misconceptions", as option (b).
The defragmentation presents an ideal layout of contiguous data (and thus is an attractive option), howConcept-trigger Name Compare and contrastCompare multiple situations; draw conclusions from comparison Interpret representations Provide a situation that asks students to make inferences based upon the presented features Identify a set or subset Ask them to identify a set or subset fulfilling some criterion Strategize only Provide a problem; ask students to identify the best means of reaching a solution Omit necessary information Provide less information than is essential for answer; see if students realize this Use "none of the above"An option to learn alternative understandings; use it occasionally as a correct answer Qualitative questionsQuestions are about the concepts and relationships rather than numbers or equations Analysis and reasoning questions Questions require significant decision-making, hence promote significant discussion Trap unjustified assumptions Answer choices are facilitated by potential unjustified assumptions made by the students Deliberate ambiguityUse deliberate ambiguity in questions to facilitate discussion Trolling for misconceptions Attempt to trap students with answers that require common misconceptions to choose Table 1: List of concept-triggers from Beatty et al. [1] ever, it also raises a possibility that important data has been lost.Question presentation.
This question does not specifically present a scenario, but provides examples of potential carving situations on both a target drive and an investigator's machine.
This question thus uses the "example" presentation type.
Forensics of the Microsoft Windows registry provides significant details of activities on a computer.
The registry is used by the operating system and applications as a database for storing configuration information.
It consists of hives (backed by files) and can hold information such as list of applications to run on user login, user password hashes, the last time particular USB devices were plugged into the system, and the list of installed software.
In a registry hive, keys are similar to directory paths, values are analogous to file names, and data is analogous to file content.
The SYSTEM hive has a key called USBSTOR that maintains a set of USB storage devices plugged into a system, holding their serial numbers with the last time the devices were inserted into the system.A peer instruction question on registry can be created as follows: A USB drive with an unknown owner is found in a corporate setting.
How might a forensic investigator typically determine whether that particular drive was plugged into any given Windows machine?
a) Examine all ntuser.dat files to determine if a user plugged it into the machine; b) Check the SYSTEM registry hive to see if it was plugged into that machine; c) Check the SOFT-WARE registry hive to see if it has been used by any particular piece of software; d) More than one of the above; e) None of the above (Answer: b).
Concept trigger.
The question utilizes "use none of the above" to point the students toward the possibility of each answer being potentially invalid.
However, with multiple potentially valid options, it also uses "identify a set or subset" as option d).
While each option is possible and may assist in locating references to USB drives, the use of the SYSTEM hive is the most efficient.
SYSTEM hive contains system-wide activities.
If no reference to the drive is found in the hive, there is no need to search through installed software or any user-specific locations.Question presentation.
This is a scenario question.
It provides a particular situation that a forensic investigator may face, and it asks for the best means to proceed.
Redaction of information is done generally through obscuration (with pens, opaque tape, etc.) or excision of the document through cutting the intended area out of the document.
Improper redaction, like many other processes (such as file deletion), can create unintended forensic artifacts.A peer instruction question on this concept can be created as follows: Which of the following are examples of redaction?
a) A company employee selectively covers sentences from a press release with a black marker; b) A copy editor at a nonprofit adds information to a press release draft with a red pen; c) An inside attacker at a law firm cuts documents to physically remove appearances of his name; d) Both a) and b); e) Both a) and c) (answer: e).
The question checks whether students can readily identify examples that illustrate the core features of redaction, i.e., actions that obscure, or remove, information.Concept trigger.
As the question is somewhat simple and looking for provided examples of the essential features of redaction, it uses "analysis and reasoning" as students scan the answers for information that suggests obfuscation or removal.Question presentation.
The question requires students to demonstrate the understanding of the features of the Redaction concept; it is therefore a "feature" question.
It was helpful to hear other students' explanations 11% 0% 89% The professor explained the value of using clickers in this class.
Not at all Somewhat, but I was still unclear why we were doing it Yes, they explained it well Yes, they explained it too much 0% 11% 67% 22%
