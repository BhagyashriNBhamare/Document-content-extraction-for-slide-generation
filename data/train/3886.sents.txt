Mobile apps often require access to private data, such as the device ID or location.
At the same time, popular platforms like Android and iOS have limited support for user privacy.
This frequently leads to unauthorized disclosure of private information by mobile apps, e.g. for advertising and analytics purposes.
This paper addresses the problem of privacy enforcement in mobile systems, which we formulate as a classification problem: When arriving at a privacy sink (e.g., database update or outgoing web message), the runtime system must classify the sink's behavior as either legitimate or illegitimate.
The traditional approach of information-flow (or taint) tracking applies "binary" classification, whereby information release is legitimate iff there is no data flow from a privacy source to sink arguments.
While this is a useful heuristic, it also leads to false alarms.
We propose to address privacy enforcement as a learning problem, relaxing binary judgments into a quanti-tative/probabilistic mode of reasoning.
Specifically, we propose a Bayesian notion of statistical classification, which conditions the judgment whether a release point is legitimate on the evidence arising at that point.
In our concrete approach, implemented as the BAYESDROID system that is soon to be featured in a commercial product , the evidence refers to the similarity between the data values about to be released and the private data stored on the device.
Compared to TaintDroid, a state-of-the-art taint-based tool for privacy enforcement, BAYESDROID is substantially more accurate.
Applied to 54 top-popular Google Play apps, BAYESDROID is able to detect 27 privacy violations with only 1 false alarm.
Mobile apps frequently demand access to private information.
This includes unique device and user identifiers, such as the phone number or IMEI number (identifying the physical device); social and contacts data; the user's location; audio (microphone) and video (camera) data; etc.
While private information often serves the core functionality of an app, it may also serve other purposes, such as advertising, analytics or cross-application profiling [9].
From the outside, the user is typically unable to distinguish legitimate usage of their private information from illegitimate scenarios, such as sending of the IMEI number to a remote advertising website to create a persistent profile of the user.Existing platforms provide limited protection against privacy threats.
Both the Android and the iOS platforms mediate access to private information via a permission model.
Each permission is mapped to a designated resource, and holds per all application behaviors and resource accesses.
In Android, permissions are given or denied at installation time.
In iOS, permissions are granted or revoked upon first access to the respective resource.
Hence, both platforms cannot disambiguate legitimate from illegitimate usage of a resource once an app is granted the corresponding permission [8].
Threat Model In this paper, we address privacy threats due to authentic (as opposed to malicious) mobile applications [4,18].
Contrary to malware, such applications execute their declared functionality, though they may still expose the user to unnecessary threats by incorporating extraneous behaviors -neither required by their core business logic nor approved by the user [11] -such as analytics, advertising, cross-application profiling, social computing, etc.
We consider unauthorized release of private information that (almost) unambiguously identifies the user as a privacy threat.
Henceforth, we dub such threats illegitimate.While in general there is no bullet-proof solution for privacy enforcement that can deal with any type of covert channel, implicit flow or application-specific data transformation, and even conservative enforcement approaches can easily be bypassed [19], there is strong evidence that authentic apps rarely exhibit these challenges.According to a recent study [9], and also our empirical data (presented in Section 5), private information is normally sent to independent third-party servers.
Consequently, data items are released in clear form, or at most following well-known encoding/encryption transformations (like Base64 or MD5), to meet the requirement of a standard and general client/server interface.The challenge, in this setting, is to determine whether the app has taken sufficient means to protect user privacy.
Release of private information, even without user authorization, is still legitimate if only a small amount of information has been released.
As an example, if an application obtains the full location of the user, but then releases to an analytics server only coarse information like the country or continent, then in most cases this would be perceived as legitimate.Privacy Enforcement via Taint Analysis The shortcomings of mobile platforms in ensuring user privacy have led to a surge of research on realtime privacy monitoring.
The foundational technique grounding this research is information-flow tracking, often in the form of taint analysis [23,15]: Private data, obtained via privacy sources (e.g. which reads the device's IMSI), is labeled with a taint tag denoting its source.
The tag is then propagated along data-flow paths within the code.
Any such path that ends up in a release point, or privacy sink (e.g. WebView.loadUrl(...), which sends out an HTTP request), triggers a leakage alarm.The tainting approach effectively reduces leakage judgments to boolean reachability queries.
This can potentially lead to false reports, as the real-world example shown in Figure 1 illustrates.
This code fragment, extracted from a core library in the Android platform, reads the device's IMSI number, and then either (ii) persists the full number to an error log if the number is invalid (the loge(...) call), or (ii) writes a prefix of the IMSI (of length 6) to the standard log while carefully masking away the suffix (of length 9) as 'x' characters.
Importantly, data flow into the log(...) sink is not a privacy problem, because the first 6 digits merely carry model and origin information.
Distinctions of this sort are beyond the discriminative power of taint analysis [26].
Quantitative extensions of the core tainting approach have been proposed to address this limitation.
A notable example is McCamant and Ernst's [13] information-flow tracking system, which quantities flow of secret information by dynamically tracking taint labels at the bit level.
Other approaches -based e.g. on distinguishability between secrets [1], the rate of data transmission [12] or the influence inputs have on output values [14] -have also been proposed.
While these systems are useful as offline analyses, it is highly unlikely that any of them can be en- where a prefix of the mobile device's IMSI number flows into the standard log file gineered to meet the performance requirements of a realtime monitoring solution due to the high complexity of their underlying algorithms.
As an example, McCamant and Ernst report on a workload on which their analysis spent over an hour.Our Approach We formulate data leakage as a classification problem, which generalizes the source/sink reachability judgment enforced by standard informationflow analysis, permitting richer and more relaxed judgments in the form of statistical classification.
The motivating observation is that reasoning about information release is fuzzy in nature.
While there are clear examples of legitimate versus illegitimate information release, there are also less obvious cases (e.g., a variant of the example in Figure 1 with a 10-rather than 6-character prefix).
A statistical approach, accounting for multiple factors and based on rich data sets, is better able to address these subtleties.Concretely, we propose Bayesian classification.
To label a release point as either legitimate or illegitimate, the Bayesian classifier refers to the "evidence" at that point, and computes the likelihood of each label given the evidence.
The evidence consists of feature/value pairs.
There are many ways of defining the evidence.
In this study, we concentrate on the data arguments flowing into release operations, though we intend to consider other classes of features in the future.
(See Section 7.)
Specifically, we induce features over the private values stored on the device, and evaluate these features according to the level of similarity between the private values and those arising at release points.
This distinguishes instances where data that is dependent on private values flows into a release point, but its structural and/or quantitative characteristics make it eligible for release, from illegitimate behaviors.
Failure to make such distinctions is a common source of false alarms suffered by the tainting approach [4].
To illustrate this notion of features, we return to the example in Figure 1.
Because the IMSI number is consid- ered private, we define a respective feature IMSI.
Assume that the concrete IMSI value is "404685505601234".
Then the value arising at the log(...) release point is "IMSI: 404685xxxxxxxxx".
The quantitative similarity between these two values serves as evidence for the decision whether or not log(...) is behaving legitimately.
This style of reasoning is depicted in Figure 2.
Evaluation To evaluate our approach, we have implemented the BAYESDROID system for privacy enforcement.We report on two sets of experiments over BAYESDROID.First, to measure the accuracy gain thanks to Bayesian analysis, we compared BAYESDROID with the TaintDroid system [4], a highly popular and mature implementation of the tainting approach that is considered both efficient (with average overhead of approximately 10%) and accurate.
We applied both BAYESDROID and TaintDroid to the DroidBench suite, 1 which comprises the most mature and comprehensive set of privacy benchmarks currently available.
The results suggest dramatic improvement in accuracy thanks to Bayesian elimination of false reports, yielding accuracy scores of 0.96 for BAYESDROID versus 0.66 for TaintDroid.The second experiment examines the practical value of BAYESDROID by applying it to 54 top-popular mobile apps from Google Play.
We evaluate two variants of BAYESDROID, one of which is able to detect a total of 27 distinct instances of illegitimate information release across 15 of the applications with only 1 false alarm.Contributions This paper makes the following principal contributions:1.
Novel approach to leakage detection (Section 2): We present a Bayesian classification alternative to the classic tainting approach.
Our approach is more flexible than taint tracking by permitting statistical weighting of different features as the basis for privacy judgments.
2.
Similarity-based reasoning (Section 3): We instantiate the Bayesian approach by applying quantitative 1 http://sseblog.ec-spride.de/tools/droidbench/ similarity judgments over private values and values about to be released.
This enables consideration of actual data, rather than only data flow, as evidence for privacy judgments.
3.
Implementation and evaluation (Sections 4-5): We have instantiated our approach as the BAYESDROID system, which is about to be featured in an IBM cloud-based security service.
We report on two sets of experiments, whose results (i) demonstrate substantial accuracy gain thanks to Bayesian reasoning, and (ii) substantiate the overall effectiveness of BAYESDROID when applied to real-world apps.
All the leakage reports by BAYESDROID are publicly available for scrutiny.
2 Our starting point is to treat privacy enforcement as a classification problem, being the decision whether or not a given release point is legitimate.
The events, or instances, to be classified are (runtime) release points.
The labels are legitimate and illegitimate.
Misclassification either yields a false alarm (mistaking benign information release as a privacy threat) or a missed data leak (failing to intercept illegitimate information release).
Our general approach is to base the classification on the evidence arising at the release point.
Items of evidence may refer to qualitative facts, such as source/sink dataflow reachability, as well as quantitative measures, such as the degree of similarity between private values and values about to be released.
These latter criteria are essential in going beyond the question of whether private information is released to also reason about the amount and form of private information about to be released.
A popular classification method, representing this mode of reasoning, is based on Bayes' theorem (or rule).
Given events X and Y , Bayes' theorem states the following equality:Pr(Y |X) = Pr(X|Y ) · Pr(Y ) Pr(X)(1)where Pr(Y |X) is the conditional probability of Y given X (i.e., the probability for Y to occur given that X has occurred).
X is referred to as the evidence.
Given evidence X, Bayesian classifiers compute the conditional likelihood of each label (in our case, legitimate and illegitimate).
We begin with the formal background by stating Equation 1 more rigorously.
Assume that Y is a discretevalued random variable, and let X = [X 1 ,.
.
.
,X n ] be a vector of n discrete or real-valued attributes X i .
ThenPr(Y = y k |X 1 .
.
.
X n ) = Pr(Y = y k ) · Pr(X 1 .
.
.
X n |Y = y k ) Σ j Pr(Y = y j ) · Pr(X 1 .
.
.
X n |Y = y j )(2)As Equation 2 hints, training a Bayesian classifier is, in general, impractical.
Even in the simple case where the evidence X is a vector of n boolean attributes and Y is boolean, we are still required to estimate a setθ i j = Pr(X = x i |Y = y j )of parameters, where i assumes 2 n values and j assumes 2 values for a total of 2· (2 n −1) independent parameters.Naive Bayes deals with the intractable sample complexity by introducing the assumption of conditional independence, as stated in Definition 2.1 below, which reduces the number of independent parameters sharply to 2n.
Intuitively, conditional independence prescribes that events X and Y are independent given knowledge that event Z has occurred.Definition 2.1 (Conditional Independence).
Given random variables X, Y and Z, we say that X is conditionally independent of Y given Z iff the probability distribution governing X is independent of the value of Y given Z.
That is,∀i, j, k. Pr(X = x i |Y = y j , Z = z k ) = Pr(X = x i |Z = z k )Under the assumption of conditional independence, we obtain the following equality:Pr(X 1 .
.
.
X n |Y ) = Π n i=1 Pr(X i |Y )(3)Therefore,Pr(Y = y k |X 1 .
.
.
X n ) = Pr(Y = y k ) · Π i Pr(X i |Y = y k ) Σ j Pr(Y = y j ) · Π i Pr(X i |Y = y j )(4) For leakage detection, conditional independence translates into the requirement that at a release point st, the "weight" of evidence e 1 is not affected by the "weight" of evidence e 2 knowing that st is legitimate/illegitimate.
As an example, assuming the evidence is computed as the similarity between private and released values, if st is known to be a statement sending private data to the network, then the similarity between the IMSI number and respective values about to be released is assumed to be independent of the similarity between location coordinates and respective values about to be released.The assumption of conditional independence induces a "modular" mode of reasoning, whereby the privacy features comprising the evidence are evaluated independently.
This simplifies the problem of classifying a release point according to the Bayesian method into two quantities that we need to clarify and estimate: (i) the likelihood of legitimate/illegitimate release (Pr(Y = y k )) and (ii) the conditional probabilities Pr(X i |Y = y k ).
In this section we develop, based on the mathematical background in Section 2, an algorithm to compute the conditional likelihood of legitimate versus illegitimate data release given privacy features F i .
With such an algorithm in place, given values v i for the features F i , we obtainv leg = Pr(legitimate| [F 1 = v 1 ,.
.
.
,F n = v n ]) v illeg = Pr(illegitimate| [F 1 = v 1 ,.
.
.
,F n = v n ])Bayesian classification then reduces to comparing between v leg and v illeg , where the label corresponding to the greater of these values is the classification result.
The first challenge that arises is how to define the features (denoted with italicized font: F) corresponding to the private values (denoted with regular font: F).
This requires simultaneous consideration of both the actual private value and the "relevant" values arising at the sink statement (or release point).
We apply the following computation:1 [[F]] ≡ min 1≤i≤k {d([[F]], v i )}We leave the distance metric d(.
.
.)
unspecified for now, and return to its instantiation in Section 3.2.
According to our description above, feature values are unbounded in principle, as they represent the distance between the reference value and any data-dependent sink values.
In practice, however, assuming (i) the distancemetric d(.
.
.)
satisfies d(x, y) ≤ max{|x|, |y|}, (ii) ∃c ∈ N. |[[F]]| ≤ c (as with the IMEI, IMSI, location, etc.), and (iii) [[F]]is not compared with values larger than it, we can bound [[F]] by c.
In general, any feature can be made finite, with (at most) n+1 possible values, by introducing a privileged "≥ n" value, which denotes that the distance between the reference and relevant values is at least n. To compute a quantitative measure of similarity between data values, we exploit the fact that private data often manifests as strings of ASCII characters [4,9,27].
These include e.g. device identifiers (like the IMEI and IMSI numbers), GPS coordinates, inter-application communication (IPC) parameters, etc.
This lets us quantify distance between values in terms of string metrics.Many string metrics have been proposed to date [17].
Two simple and popular metrics, which we have experimented with and satisfy the requirement that d(x, y) ≤ max{|x|, |y|}, are the following:Hamming Distance This metric assumes that the strings are of equal length.
The Hamming distance between two strings is equal to the number of positions at which the corresponding symbols are different (as indicated by the indicator function δ c 1 񮽙 =c 2 (.
.
.))
:ham(a, b) = Σ 0≤i<|a| δ c 1 񮽙 =c 2 (a(i), b(i))In another view, Hamming distance measures the number of substitutions required to change one string into the other.Levenshtein Distance The Levenshtein string metric computes the distance between strings a and b as lev a,b (|a|, |b|) (abbreviated as lev(|a|, |b|)), wherelev(i, j) =        max(i, j) if min(i, j) = 0 min   lev(i − 1, j) + 1 lev(i, j − 1) + 1 lev(i − 1, j − 1) + δ a i 񮽙 =b j   otherwiseInformally, lev(|a|, |b|) is the minimum number of single-character edits -either insertion or deletion or Data: Strings u and v Data:Distance metric d begin x ←− |u| < |v| ?
u : v // min y ←− |u| ≥ |v| ?
u : v // max r ←− y for i = 0 to |y| − |x| do y 񮽙 ←− y[i, i + |x| − 1] if d(x, y 񮽙 ) < r then r ←− d(x, y 񮽙 ) end end return r endAlgorithm 1: The BAYESDROID distance measurement algorithm substitution -needed to transform one string into the other.
An efficient algorithm for computing the Levenshtein distance is bottom-up dynamic programming [24].
The asymptotic complexity is O(|a| · |b|).
Given string metric d(x, y) and pair (u, v) of reference value u and relevant value v, BAYESDROID computes their distance according to the following steps:1.
BAYESDROID ensures that both u and v are String objects by either (i) invoking toString() on reference types or (ii) converting primitive types into Strings (via String.valueOf(. . .)), if the argument is not already of type String.
2.
To meet the conservative requirement that |x| = |y| (i.e., x and y are of equal length), BAYESDROID applies Algorithm 1.
This algorithm induces a sliding window over the longer of the two strings, whose width is equal to the length of the shorter string.
The shorter string is then compared to contiguous segments of the longer string that have the same length.
The output is the minimum across all comparisons.
To ensure that comparisons are still meaningful under length adjustment, we decompose private values into indivisible information units.
These are components of the private value that cannot be broken further, and so comparing them with a shorter value mandates that the shorter value be padded.
In our specification, the phone, IMEI and IMSI numbers consist of only one unit of information.
The Location object is an example of a data structure that consists of several distinct information units.
These include the integral and fractional parts of the longitude and latitude values, etc.
BAYESDROID handles objects that decompose into multiple information units by treating each unit as a separate object and applying the steps above to each unit in turn.
The notion of information units guards BAYESDROID against ill-founded judgments, such as treating release of a single IMEI digit as strong evidence for leakage.
The remaining challenge, having clarified what the features are and how their values are computed, is to estimate the probabilities appearing in Equation 4:• We need to estimate the probability of the legitimate event, Pr(legitimate), where illegitimate is the complementary event and thus Pr(illegitimate) = 1 − Pr(legitimate).
• We need to estimate the conditional probabilities Pr(F = u|legitimate) and Pr(F = u|illegitimate) for all features F and respective values u. Pr(legitimate) can be approximated straightforwardly based on available statistics on the frequency of data leaks in the wild.
For the conditional probabilities, assuming feature X i is discrete valued with j distinct values (per the discussion in Section 3.1 above), we would naively compute the estimated conditional probability θ i jk according to the following equation:θ i jk = 񮽙 Pr(X i = x i j |Y = y k ) = #D{X i =x i j ∧Y =y k } #D{Y =y k }(5)The danger, however, is that this equation would produce estimates of zero if the data happens not to contain any training examples satisfying the condition in the numerator.
To fix this, we modify Equation 5 as follows:θ i jk = 񮽙 Pr(X i = x i j |Y = y k ) = #D{X i =x i j ∧Y =y k }+l #D{Y =y k }+l·J (6)where l is a factor that "smoothens" the estimate by adding in a number of "hallucinated" examples that are assumed to be spread evenly across the J possible values of X i .
In Section 5.1, we provide concrete detail on the data sets and parameter values we used for our estimates.
In this section, we describe the complete BAYESDROID algorithm.
We then discuss enhancements of the core algorithm.
Algorithm 2 summarizes the main steps of BAYES-DROID.
For simplicity, the description in Algorithm 2 assumes that source statements serve private data as their return value, though the BAYESDROID implementation also supports other sources (e.g. callbacks like onLocationChanged(. . .), where the private Location object is passed as a parameter).
We also assume that each source maps to a unique privacy feature.
Hence, when a source is invoked (i.e., the OnSourceStatement event fires), we obtain the unique tag corresponding to its respective feature via the GetFeature(. . .) function.
We { f 񮽙 → p f } ←− ExtractTags p foreach f → p f ∈ { f → p f } do u ←− ref f δ ←− min{d(u, [[p]])} p∈p f f ←− δ ≥ c f ? "
≥ c f ": δ end if IsLeakageClassification { f } then Alarm snk p end end endAlgorithm 2: Outline of the core BAYESDROID algorithm then attach the tag to the return value r. Normal data flow obeys the standard rules of tag propagation, which are provided e.g. by Enck et al. [4].
(See Table 1 there.)
When an OnSinkStatement event is triggered, the arguments flowing into the sink snk are searched for privacy tags, and a mapping from features f to parameters p f carrying the respective tag is built.
The value of f is then computed as the minimal pairwise distance between the parameters p ∈ p f and ref f .
If this value is greater than some constant c f defined for f , then the privileged value "≥ c f " is assigned to f .
(See Section 3.1.)
Finally, the judgment IsLeakageClassification is applied over the features whose tags have reached the sink snk.
This judgment is executed according to Equation 4.
We illustrate the BAYESDROID algorithm with reference to Figure 3, which demonstrates a real leakage instance in com.g6677.android.princesshs, a popular gaming application.
In this example, two different private items flow into the sink statement: both the IMEI, read via getDeviceId(), and the Android ID, read via getString(...).
At sink statement URL.openConnection(...), the respective tags IMEI and AndroidID are extracted.
Values are assigned to these features according to the description in Section 3, where we utilize training data, as discussed later in Section 5.1, for Equation 6:Pr(IMEI ≥ 5|leg) = 0.071 Pr(AndID ≥ 5|leg) = 0.047 Pr(IMEI ≥ 5|ilg) = 0.809 Pr(AndID ≥ 5|ilg) = 0.833 We then compute Equation 4, where the denominator is the same for both leg and illeg, and so it suffices to evaluate the nominator (denoted with˜Prwith˜ with˜Pr(...) rather than Pr(...)):˜ Pr(leg|IMEI ≥ 5, AndID ≥ 5) = Pr(leg) × Pr(IMEI ≥ 5|leg) × Pr(AndID ≥ 5|leg) = 0.66 × 0.071 × 0.047 = 0.002˜Pr 002˜ 002˜Pr(ilg|IMEI ≥ 5, AndID ≥ 5) = Pr(ilg) × Pr(IMEI ≥ 5|ilg) × Pr(AndID ≥ 5|ilg) = 0.33 × 0.809 × 0.833 = 0.222Our estimates of 0.66 for Pr(leg) and 0.33 for Pr(ilg) are again based on training data as explained in Section 5.1.
The obtained conditional measure of 0.222 for ilg is (far) greater than 0.002 for leg, and so BAYESDROID resolves the release instance in Figure 3 as a privacy threat, which is indeed the correct judgment.
We conclude our description of BAYESDROID by highlighting two extensions of the core algorithm.Beyond Plain Text While many instances of illegitimate information release involve plain text, and can be handled by the machinery in Section 3.1, there are also more challenging scenarios.
Two notable challenges are (i) data transformations, whereby data is released following an encoding, encryption or hashing transformation; and (ii) high-volume binary data, such as camera or microphone output.
We have extended BAYESDROID to address both of these cases.We begin with data transformations.
As noted earlier, in Section 1, private information is sometimes released following standard hashing/encoding transformations, such as the Base64 scheme.
This situation, illustrated in Figure 4, Figure 4: Adaptation of the DroidBench Loop1 benchmark, which releases the device ID following Base64 encoding leading BAYESDROID to erroneous judgments.
Fortunately, the transformations that commonly manifest in leakage scenarios are all standard, and there is a small number of such transformations [9].
To account for these transformations, BAYESDROID applies each of them to the value obtained at a source statement, thereby exploding the private value into multiple representations.
This is done lazily, once a sink is reached, for performance.
This enhancement is specified in pseudocode form in Algorithm 3.
The main change is the introduction of a loop that traverses the transformations τ ∈ T , where the identity transformation, λ x. x, is included to preserve the (non-transformed) value read at the source.
The value assigned to feature f is then the minimum with respect to all transformed values.Binary data -originating from the microphone, camera or bluetooth adapter -also requires special handling because of the binary versus ASCII representation and, more significantly, its high volume.
Our solution is guided by the assumption that such data is largely treated as "uninterpreted" and immutable by application code due to its form and format.
This leads to a simple yet effective strategy for similarity measurement, whereby a fixed-length prefix is truncated out of the binary content.
Truncation is also applied to sink arguments consisting of binary data.Heuristic Detection of Relevant Values So far, our description of the BAYESDROID algorithm has relied on tag propagation to identify relevant values at the sink statement.
While this is a robust mechanism to drive feature computation, flowing tags throughout the code also has its costs, incurring runtime overheads of ≥ 10% and affecting the stability of the application due to intrusive instrumentation [4].
These weaknesses of the tainting approach have led us to investigate an alternative method of detecting relevant values.
A straightforward relaxation of data-flow tracking is bounded ("brute-force") traversal of the reachable values from the arguments to a sink statement up to some depth bound k: All values pointed-to by a sink argument or reachable from a sink argument via a sequence of ≤ k field dereferences are deemed relevant.
Though in theory For false positives, private values are often unique, and so incidental similarity to irrelevant values is improbable.
For false negatives, the arguments flowing into privacy sinks are typically either String objects or simple data structures.
Also, because the number of privacy sinks is relatively small, and the number of complex data structures accepted by such sinks is even smaller, it is possible to specify relevant values manually for such data structures.
We have encountered only a handful of data structures (e.g. the android.content.Intent class) that motivate a specification.Input: T ≡ {λ x. x, τ 1 ,...,τ n } // std.
transformations begin ... OnSinkStatement r := snk p : { f 񮽙 → p f } ←− ExtractTags p foreach f → p f ∈ { f → p f } do foreach τ ∈ T do u ←− τ (ref f ) δ ←− min{d(u, [[p]])} p∈p f f ←− min{[[ f ]], δ ≥ c f ? "
≥ c f ": δ } end end ... end In this section, we describe the BAYESDROID implementation, and present two sets of experiments that we have conducted to evaluate our approach.
Implementation Similarly to existing tools like TaintDroid, BAYESDROID is implemented as an instrumented version of the Android SDK.
Specifically, we have instrumented version 4.1.1 r6 of the SDK, which was chosen intentionally to match the most recent version of TaintDroid.
3 The experimental data we present indeed utilizes TaintDroid for tag propagation (as required for accurate resolution for relevant values).3 http://appanalysis.org/download.html Beyond the TaintDroid instrumentation scheme, the BAYESDROID scheme specifies additional behaviors for sources and sinks within the SDK.
At source points, a hook is added to record the private value read by the source statement (which acts as a reference value).
At sink points, a hook is installed to apply Bayesian reasoning regarding the legitimacy of the sink.Analogously to TaintDroid, BAYESDROID performs privacy monitoring over APIs for file-system access and manipulation, inter-application and socket communication, reading the phone's state and location, and sending of text messages.
BAYESDROID also monitors the HTTP interface, camera, microphone, bluetooth and contacts.
As explained in Section 4.1, each of the privacy sources monitored by BAYESDROID is mirrored by a tag/feature.
The full list of features is as follows: IMEI, IMSI, AndroidID, Location, Microphone, Bluetooth, Camera, Contacts and FileSystem.The BAYESDROID implementation is configurable, enabling the user to switch between distance metrics as well as enable/disable information-flow tracking for precise/heuristic determination of relevant values.
(See Section 4.2.)
In our experiments, we tried both the Levenshtein and the Hamming metrics, but found no observable differences, and so we report the results only once.
Our reasoning for why the metrics are indistinguishable is because we apply both to equal-length strings (see Section 3.2), and have made sure to apply the same metric both offline and online, and so both metrics achieve a very similar effect in the Bayesian setting.Training To instantiate BAYESDROID with the required estimates, as explained in Section 3.3, we applied the following methodology: First, to estimate Pr(legitimate), we relied on (i) an extensive study by Hornyack et al. spanning 1,100 top-popular free Android apps [9], as well as (ii) a similarly comprehensive study by Enck et al. [5], which also spans a set of 1,100 free apps.
According to the data presented in these studies, approximately one out of three release points is illegitimate, and thus 񮽙 Pr(legitimate) = 0.66 and complementarily 񮽙 Pr(illegitimate) = 1 − 0.66 ≈ 0.33.
For the conditional probabilities 񮽙 Pr(X i = x i j |Y = y k ), we queried Google Play for the 100 most popular apps (across all domains) in the geography of one of the authors.
We then selected at random 35 of these apps, and analyzed their information-release behavior using debug breakpoints (which we inserted via the adb tool that is distributed as part of the Android SDK).
Illegitimate leaks that we detected offline mainly involved (i) location information and (ii) device and user identifiers, which is consistent with the findings reported by past studies [9,5].
We confirmed that illegitimate leaks are largely correlated with high similarity between private data and sink arguments, and so we fixed six distance levels for each private item: [0,4] and "≥ 5".
(See Section 3.1.)
Finally, to avoid zero estimates for conditional probabilities while also minimizing data perturbation, we set the "smoothing" factor l in Equation 6 at 1, where the illegitimate flows we detected were in the order of several dozens per private item.
In our experimental evaluation of BAYESDROID, we tested two hypotheses:1.
H1: Accuracy.
Bayesian reasoning, as implemented in BAYESDROID, yields a significant improvement in leakage-detection accuracy compared to the baseline of information-flow tracking.
2.
H2: Applicability.
For real-life applications, BAYESDROID remains effective under relaxation of the tag-based method for detection of relevant values and its stability improves.
To assess the accuracy of BAYESDROID, we compared it to that of TaintDroid, a state-of-the-art information-flow tracking tool for privacy enforcement.
Our experimental settings and results are described below.Subjects We applied both TaintDroid and BAYES-DROID to DroidBench, an independent and publicly available collection of benchmarks serving as testing ground for both static and dynamic privacy enforcement algorithms.
DroidBench models a large set of realistic challenges in leakage detection, including precise tracking of sensitive data through containers, handling of callbacks, field and object sensitivity, lifecycle modeling, inter-app communication, reflection and implicit flows.
The DroidBench suite consists of 50 cases.
We excluded from our experiment (i) 8 benchmarks that crash at startup, as well as (ii) 5 benchmarks that leak data via callbacks that we did not manage to trigger (e.g., onLowMemory()), as both TaintDroid and BAYESDROID were naturally unable to detect leakages in these two cases.
The complete list of benchmarks that we used can be found in Table 4 of Appendix B.Methodology For each benchmark, we measured the number of true positive (TP), false positive (FP) and false negative (FN) results.
We then summarized the results and calculated the overall precision and recall of each tool using the formulas below: High precision implies that a technique returns few irrelevant results, whereas high recall implies that it misses only few relevant ones.Precision = T P T P+FP Recall = T P T P+FNSince ideal techniques have both high recall and high precision, the F-measure is commonly used to combine both precision and recall into a single measure.
The Fmeasure is defined as the harmonic mean of precision and recall, and is calculated as follows:F-Measure = 2 × Precision×Recall Precision+RecallThe value of F-measure is high only when both precision and recall are high.
We thus use the F-measure for accuracy evaluation.
The results obtained for both TaintDroid and BAYESDROID on version 1.1 of DroidBench are summarized in Table 1 and presented in detail in Table 4.
The findings reported by BAYESDROID are also publicly available.
4 Overall, TaintDroid detects 31 true leakages while also reporting 17 false positives, whereas BAYESDROID suffers from 2 false negatives, discovering 29 of the true leakages while flagging only 1 false alarm.
The recall of both TaintDroid and BAYESDROID is high (1 and 0.93, respectively) due to a low number of false-negative results.
Yet the precision of TaintDroid is much lower than that of BAYESDROID (0.64 vs. 0.96), due to a high number of false positives.
The overall F-measure is thus lower for TaintDroid than for BAYESDROID (0.78 vs. 0.94).
The results mark BAYESDROID as visibly more accurate than TaintDroid.
To further confirm this result, we performed a two-tail McNemar test, considering 48 observations for each tool.
These observations correspond to findings reported in Table 4: 31 true positives and 17 classified as false alarms.
Each observation is a boolean value that represents the accuracy of the tool and is assumed to be from a Bernoulli distribution.
We then checked whether the difference in accuracy is statistically significant by testing the null hypothesis that the set of 48 observations from TaintDroid are sampled from the same Bernoulli distribution as the set of 48 observations from BAYESDROID.
Fragment from the DroidBench ImplicitFlow1 benchmark, which applies a custom transformation to private data We found that TaintDroid was accurate in 31 out of 48 cases, and BAYESDROID was accurate in 45 out of 48 cases.
We built the 2×2 contingency table showing when each tool was correct and applied a two-tail McNemar test.
We found a p-value of 0.001, which rejects the null hypothesis that the observations come from the same underlying distribution and provides evidence that BAYESDROID is more accurate than TaintDroid, thereby confirming H1.Discussion Analysis of the per-benchmark findings reveals the following: First, the 2 false negatives of BAYESDROID on ImplicitFlow1 are both due to custom (i.e., non-standard) data transformations, which are outside the current scope of BAYESDROID.
An illustrative fragment from the ImplicitFlow1 code is shown in Figure 5.
The obfuscateIMEI(. . .) transformation maps IMEI digits to English letters, which is a non-standard behavior that is unlikely to arise in an authentic app.The false positive reported by BAYESDROID, in common with TaintDroid, is on release of sensitive data to the file system, albeit using the MODE PRIVATE flag, which does not constitute a leakage problem in itself.
This can be resolved by performing Bayesian reasoning not only over argument values, but also over properties of the sink API (in this case, the storage location mapped to a file handle).
We intend to implement this enhancement.Beyond the false alarm in common with BAYES-DROID, TaintDroid has multiple other sources of imprecision.
The main reasons for its false positives are• coarse modeling of containers, mapping their entire contents to a single taint bit, which accounts e.g. for the false alarms on ArrayAccess{1,2} andHashMapAccess1;• field and object insensitivity, resulting in false alarms on Tag Propagation Overhead Bayesian Analysis Overhead Figure 6: Overhead breakdown into tag propagation and Bayesian analysis at sinkObjectSensitivity{1,2}; and more fundamentally, • ignoring of data values, which causes TaintDroid to issue false warnings on LocationLeak{1,2} even when location reads fail, yielding a Location object without any meaningful information.
The fundamental reason for these imprecisions is to constrain the overhead of TaintDroid, such that it can meet the performance demands of online privacy enforcement.
BAYESDROID is able to accommodate such optimizations while still ensuring high accuracy.
The second aspect of the evaluation compared between two versions of BAYESDROID, whose sole difference lies in the method used for detecting relevant values: In one configuration (T-BD), relevant values are detected via tag propagation.
The other configuration (H-BD) uses the heuristic detailed in Section 4.2 of treating all values reachable from sink arguments (either directly or via the heap graph) up to a depth bound of k as relevant, which places more responsibility on Bayesian reasoning.
We set k at 3 based on manual review of the data structures flowing into privacy sinks.We designed a parametric benchmark application to quantify the overhead reduction imposed by the H-BD variant of BAYESDROID.
The application consists of a simple loop that flows the device IMEI into a log file.
Loop iterations perform intermediate data propagation steps.
We then performed a series of experimentsover the range of 1 to 19 propagation steps -to quantify the relative overhead of tag propagation versus Bayesian analysis.The results, presented in Figure 6, suggest that the overhead of tag propagation is more dominant than that of Bayesian analysis (with a ratio of roughly 2:1), even when the set of relevant values is naively over approximated.
Discussion of the methodology underlying this experiment is provided in Appendix A.In general, H-BD trades overhead reduction for accuracy.
H2 then asserts that, in practice, the tradeoff posed by H-BD is effective.
Below, we discuss our empirical evaluation of this hypothesis over real-life subjects.Subjects To avoid evaluators' bias, we applied the following selection process: We started from the 65 Google Play apps not chosen for the training phase.
We then excluded 8 apps that do not have permission to access sensitive data and/or perform release operations (i.e., their manifest does not declare sufficient permissions out of INTERNET, READ PHONE STATE, SEND SMS, etc), as well as 3 apps that we did not manage to install properly, resulting in 54 apps that installed successfully and exercise privacy sources and sinks.The complete list of the application we used is given in Table 5 of Appendix B.
A subset of the applications, for which at least one leakage was detected, is also listed in Table 3.
Methodology We deployed the apps under the two BAYESDROID configurations.
Each execution was done from a clean starting state.
The third column of both Tables 3 and 5 denotes whether our exploration of the app was exhaustive.
By that we mean exercising all the UI points exposed by the app in a sensible order.
Ideally we would do so for all apps.
However, (i) some of the apps, and in particular gaming apps, had stability issues, and (ii) certain apps require SMS-validated sign in, which we did not perform.
We did, however, create Facebook, Gmail and Dropbox accounts to log into apps that demand such information yet do not ask for SMS validation.
We were also careful to execute the exact same crawling scenario under both the T-BD and H-BD configurations.
We comment, from our experience, that most data leaks happen when an app launches, and initializes advertising/analytics functionality, and so for apps for which deep crawling was not possible the results are still largely meaningful.For comparability between the H-BD and T-BD configurations, we counted different dynamic reports involving the same pair of source/sink APIs as a single leakage instance.
We manually classified the findings into true positives and false positives.
For this classification, we scrutinized the reports by the two configurations, and also -in cases of uncertainty -decompiled and/or reran the app to examine its behavior more closely.
As in the experiment described in Section 5.3, we then calculated the precision, recall and F-measure for each of the tools.
Results The results obtained for H-BD and T-BD are summarized in Table 2.
Table 3 summarizes the findings reported by both H-BD and T-BD at the granularity of privacy items: the device number, identifier and location, while Table 5 provides a detailed description of the results across all benchmarks including those on which no leakages were detected.
The warnings reported by the H-BD configuration are also publicly available for review.
5 As Table 2 indicates, the H-BD variant is more accurate than the T-BD variant overall (F-measure of 0.98 vs. 0.73).
As in the experiment described in Section 5.3, we further performed a two-tail McNemar test, considering 67 observations for each tool: 27 that correspond to true positives, 1 to the false positive due to H-BD and 39 to cases where no leakages were found.We found that H-BD was accurate in 66 out of 67 cases, and T-DB was accurate in 54 out of 67 cases.
Building the 2×2 contingency table and applying the two-tail McNemar test showed that the difference between the tools in accuracy is significant (with a pvalue of 0.001 to reject the null hypothesis that the accuracy observations for both tools come from the same Bernoulli distribution).
Moreover, H-BD has a lower number of crashes and lower runtime overhead, which confirms H2.Discussion To give the reader a taste of the findings, we present in Figures 7-8 two examples of potential leakages that BAYESDROID (both the H-BD and the T-BD configurations) deemed legitimate.
The instance in Figure 7 reflects the common scenario of obtaining the current (or last known) location, converting it into one or more addresses, and then releasing only the country or zip code.
In the second instance, in Fig- ure 8, the 64-bit Android ID -generated when the user first sets up the device -is read via a call to Settings$Secure.getString(ANDROID ID).
At the release point, into the file system, only a prefix of the Android ID consisting of the first 12 digits is published.As Table 3 mentation required for tag propagation, which can cause instabilities, and (ii) inability to track tags through native code, as discussed below.
The T-BD variant introduces significantly more instability than the H-BD variant, causing illegal application behaviors in 21 cases compared to only 12 under H-BD.
We have investigated this large gap between the H-BD and T-BD configurations, including by decompiling the subject apps.
Our analysis links the vast majority of illegal behaviors to limitations that TaintDroid casts on loading of third-party libraries.
For this reason, certain functionality is not executed, also leading to exceptional app states, which both inhibit certain data leaks.
6 A secondary reason why H-BD is able to detect more leakages, e.g. in the lockscreen app, is that this bench- Figure 8: Suppressed warning on fruitninjafree mark makes use of the mobileCore module, 7 which is a highly optimized and obfuscated library.
We suspect that data-flow tracking breaks within this library, though we could not fully confirm this.
At the same time, the loss in accuracy due to heuristic identification of relevant values is negligible, as suggested by the discussion in Section 4.2.
H-BD triggers only one false alarm, on ios7lockscreen, which is due to overlap between irrelevant values: extra information on the Location object returned by a call to LocationManager.getLastKnownLocation(...) and unrelated metadata passed into a ContextWrapper.startService(...) request.
Finally, as expected, H-BD does not incur false negatives.
As most of the research on privacy monitoring builds on the tainting approach, we survey related research mainly in this space.
We also mention several specific studies in other areas.
The state-of-the-art system for realtime privacy monitoring is TaintDroid [4].
TaintDroid features tolerable runtime overhead of about 10%, and can track taint flow not only through variables and methods but also through files and messages passed between apps.
TaintDroid has been used, extended and customized by several follow-up research projects.
Jung et al. [10] enhance TaintDroid to track additional sources (including contacts, camera, microphone, etc).
They used the enhanced version in a field study, which revealed 129 of the 223 apps they studied as vulnerable.
30 out of 257 alarms were judged as false positives.
The Kynoid system [20] extends TaintDroid with userdefined security policies, which include e.g. temporal constraints on data processing as well as restrictions on destinations to which data is released.The main difference between BAYESDROID and the approaches above, which all apply information-flow tracking, is that BAYESDROID exercises "fuzzy" reasoning, in the form of statistical classification, rather than enforcing a clear-cut criterion.
As part of this, BAYES-DROID factors into the privacy judgment the data values flowing into the sink statement, which provides additional evidence beyond data flow.Quantitative Approaches Different approaches have been proposed for quantitative information-flow analysis, all unified by the observation that data leakage is a quantitative rather than boolean judgment.
McCamant and Ernst [13] present an offline dynamic analysis that measures the amount of secret information that can be inferred from a program's outputs, where the text of the program is considered public.
Their approach relies on taint analysis at the bit level.
Newsome et al. [14] develop complementary techniques to bound a program's channel capacity using decision procedures (SAT and #SAT solvers).
They apply these techniques to the problem of false positives in dynamic taint analysis.
Backes et al. [1] measure leakage in terms of indistinguishability, or equivalence, between outputs due to different secret artifacts.
Their characterization of equivalence relations builds on the information-theoretic notion of entropy.
Budi et al. [2] propose kb-anonymity, a model inspired by k-anonymity that replaces certain information in the original data for privacy preservation, but beyond that also ensures that the replaced data does not lead to divergent program behaviors.While these proposals have all been shown useful, none of these approaches has been shown to be efficient enough to meet realtime constraints.
The algorithmic complexity of computing the information-theoretic measures introduced by these works seriously limits their applicability in a realtime setting.
Our approach, instead, enables a quantitative/probabilistic mode of reasoning that is simultaneously lightweight, and therefore acceptable for online monitoring, by focusing on relevant features that are efficiently computable.Techniques for Protecting Web Applications There exist numerous static and dynamic approaches for preventing attacks on web applications, e.g., [23,22,7].
Most relevant to our work are Sekar's taint-inference technique for deducing taint propagation by comparing inputs and outputs of a protected server-side application [21] and a similar browser-resident technique developed in a subsequent study [16].
While BAYESDROID shares ideas with these approaches, it is explicitly designed for mobile devices and applications.
Curtsinger et al. [3] apply a Bayesian classifier to identify JavaScript syntax elements that are highly predictive of malware.
The proposed system, ZOZZLE, analyzes the application's code statically, while BAYESDROID operates dynamically and focuses on data values.
In this paper, we articulated the problem of privacy enforcement in mobile systems as a classification problem.
We explored an alternative to the traditional approach of information-flow tracking, based on statistical reasoning, which addresses more effectively the inherent fuzziness in leakage judgements.
We have instantiated our approach as the BAYESDROID system.
Our experimental data establishes the high accuracy of BAYESDROID as well as its applicability to real-world mobile apps.Moving forward, we have two main objectives.
The first is to extend BAYESDROID with additional feature types.
Specifically, we would like to account for (i) sink properties, such as file access modes (private vs public), the target URL of HTTP communication (same domain or third party), etc; as well as (ii) the history of privacy-relevant API invocations up to the release point (checking e.g. if/which declassification operations were invoked).
Our second objective is to optimize our flowbased method for detecting relevant values (see Section 3.1) by applying (offline) static taint analysis to the subject program, e.g. using the FlowDroid tool [6].
Algorithm TPs FPs FNs ActivityCommunication1 BAYESDROID 1 0 0 TaintDroid 1 0 0 ActivityLifecycle1 BAYESDROID 1 0 0 TaintDroid 1 0 0 ActivityLifecycle2 BAYESDROID 1 0 0 TaintDroid 1 0 0 ActivityLifecycle4 BAYESDROID 1 0 0 TaintDroid 1 0 0 Library2 BAYESDROID 1 0 0 TaintDroid 1 0 0 Obfuscation1 BAYESDROID 1 0 0 TaintDroid 1 0 0 PrivateDataLeak3 BAYESDROID 1 1 0 TaintDroid 1 1 0 AnonymousClass1 BAYESDROID 0 0 0 TaintDroid 0 1 0 ArrayAccess1 BAYESDROID 0 0 0 TaintDroid 0 1 0 ArrayAccess2 BAYESDROID 0 0 0 TaintDroid 0 1 0 HashMapAccess1 BAYESDROID 0 0 0 TaintDroid 0 1 0 Button1 BAYESDROID 1 0 0 TaintDroid 1 0 0 Button3 BAYESDROID 2 0 0 TaintDroid 2 0 0 Ordering1 BAYESDROID 0 0 0 TaintDroid 0 2 0 RegisterGlobal1 BAYESDROID 1 0 0 TaintDroid 1 0 0 DirectLeak1 BAYESDROID 1 0 0 TaintDroid 1 0 0 FieldSensitivity2 BAYESDROID 0 0 0 TaintDroid 0 1 0 FieldSensitivity3 BAYESDROID 1 0 0 TaintDroid 1 0 0 FieldSensitivity4 BAYESDROID 0 0 0 TaintDroid 0 1 0 ImplicitFlow1 BAYESDROID 0 0 2 TaintDroid 2 0 0 InheritedObjects1 BAYESDROID 1 0 0 TaintDroid 1 0 0 ListAccess1 BAYESDROID 0 0 0 TaintDroid 0 1 0 LocationLeak1 BAYESDROID 0 0 0 TaintDroid 0 2 0 LocationLeak2 BAYESDROID 0 0 0 TaintDroid 0 2 0 Loop1 BAYESDROID 1 0 0 TaintDroid 1 0 0 Loop2 BAYESDROID 1 0 0 TaintDroid 1 0 0 ApplicationLifecycle1 BAYESDROID 1 0 0 TaintDroid 1 0 0 ApplicationLifecycle3 BAYESDROID 1 0 0 TaintDroid 1 0 0 MethodOverride1 BAYESDROID 1 0 0 TaintDroid 1 0 0 ObjectSensitivity1 BAYESDROID 0 0 0 TaintDroid 0 1 0 ObjectSensitivity2 BAYESDROID 0 0 0 TaintDroid 0 2 0 Reflection1 BAYESDROID 1 0 0 TaintDroid 1 0 0 Reflection2 BAYESDROID 1 0 To complete the description in Section 5.4, we now detail the methodology governing our overhead measurements.
The behavior of the benchmark app is governed by two user-controlled values: (i) the length 񮽙 of the source/sink data-flow path (which is proportional to the number of loop iterations) and (ii) the number m of values reachable from sink arguments.
Based on our actual benchmarks, as well as data reported in past studies [23], we defined the ranges 1 ≤ 񮽙 ≤ 19 and 1 ≤ m ≤ 13 = Σ 2 n=0 3 n .
We then ran the parametric app atop a "hybrid" configuration of BAYESDROID that simultaneously propagates tags and treats all the values flowing into a sink as relevant.
For each value of 񮽙, we executed the app 51 times, picking a value from the range [0,2] for n uniformly at random in each of the 51 runs.
We then computed the average overhead over the runs, excluding the first (cold) run to remove unrelated initialization costs.
The stacked columns in Figure 6 each correspond to a unique value of 񮽙.
Table 4 summarizes the results of the H1 experiment described in Section 5.3.
For each of the benchmarks, it specifies the number of true-positive, false-positive and false-negative findings for the compared tools, BAYES-DROID and TaintDroid.
The benchmarks on which the tools differ are highlighted for convenience.
Similarly, Table 5 summarizes the results of the H2 experiment described in Section 5.4.
The first two columns of Table 5 list the applications and their respective domain, and the third column denotes whether crawling was exhaustive.
Then, the number of crashes, truepositive, false-positive and false-negative findings are reported for both the H-BD and the T-BD variants of BAYESDROID.In Section 5.4, we describe an experiment designed to evaluate our Bayesian analysis in "pure" form, i.e. without the support of information-flow tracking to detect relevant values.
To make our description of this experiment complete, we include Table 5, which provides a detailed summary of the results of this experiment across all benchmarks (including ones on which no leakages were detected).
For comparability between the H-BD and T-BD configurations, we count different dynamic reports involving the same pair of source/sink APIs as a single leakage instance.
Table 5: Detailed summary of the results of the H2 experiment described in Section 5.4
