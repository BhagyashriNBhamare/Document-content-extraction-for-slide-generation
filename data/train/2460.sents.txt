There is a growing need to perform real-time analytics on evolving graphs in order to deliver the values of big data to users.
The key requirement from such applications is to have a data store to support their diverse data access efficiently, while concurrently ingesting fine-grained updates at a high velocity.
Unfortunately, current graph systems, either graph databases or analytics engines, are not designed to achieve high performance for both operations.
To address this challenge , we have designed and developed GRAPHONE, a graph data store that combines two complementary graph storage formats (edge list and adjacency list), and uses dual ver-sioning to decouple graph computations from updates.
Importantly , it presents a new data abstraction, GraphView, to enable data access at two different granularities with only a small data duplication.
Experimental results show that GRA-PHONE achieves an ingestion rate of two to three orders of magnitude higher than graph databases, while delivering al-gorithmic performance comparable to a static graph system.
GRAPHONE is able to deliver 5.36× higher update rate and over 3× better analytics performance compared to a state-of-the-art dynamic graph system.
We live in a world where information networks have become an indivisible part of our daily lives.
A large body of research has studied the relationships in such networks, e.g., biological networks [33], social networks [20,41,46], and web [9,31].
In these applications, graph queries and analytics are being used to gain valuable insights from the data, which can be classified into two broad categories: batch analytics (e.g. PageRank [61], graph traversal [11,49,51]) that analyzes a static snapshot of the data, and stream analytics (e. g. anomaly detection [8], topic detection [64]) that studies the incoming data over a time window of interest.
Generally speaking, batch analytics prefers a base (data) store that can provide indexed access on the non-temporal property of the graph such as the source vertex of an edge, and on the other hand, stream analytics needs a stream (data) store where data can be stored quickly and can be indexed by their arrival order for temporal analysis.Increasingly, one needs to perform batch and stream processing together on evolving graphs [78,68,10,69].
The key requirement here is to sustain a large volume of fine-grained updates at a high velocity, and simultaneously provide highperformance real-time analytics and query support.This trend poses a number of challenges to the underlying storage and data management system.
First, batch and stream analytics perform different kinds of data access, that is, the former visits the whole graph while the latter focuses on the data within a time window.
Second, each analytic has a different notion of real time, that is, data is visible to the analytics at different granularity of data ingestion (updates).
For example, an iterative algorithm such as PageRank can run on a graph that is updated at a coarse granularity, but a graph query to output the latest shortest path requires data visibility at a much finer granularity.
Third, such a system should also be able to handle a high arrival rate of updates, and maintain data consistency while running concurrent batch and stream processing tasks.Unfortunately, current graph systems can neither provide diverse data access nor at the right granularity in the presence of a high data arrival rate.
Many dynamic graph systems [47,54] only support batched updates, and a few others [21,70] offer data visibility at fine granularity of updates but with a weak consistency guarantee, which as a result may cause an analytic iteration to run on different data versions and produce undesired results.
Relational and graph databases such as Neo4j [59] can handle fine-grained updates, but suffer from poor ingestion rate for the sake of strong consistency guarantee [56].
Also, such systems are not designed to support high-performance streaming data access over a time window.
On the other hand, graph stream engines [58,17,32,72,75,67] interleave incremental computation with data ingestion, i.e., graph updates are batched and not applied until the end of an iteration.
In short, the existing systems manage a private data store in a way to favor their specialized analytics.In principle, one can utilize these specialized graph systems side-by-side to provide data management functions for dynamic graphs and support a wide spectrum of analytics and queries.
However, such an approach would be suboptimal [78], as it is only as good as the weakest component, in many cases the graph database with poor performance for streaming data.
Worse, this approach could also lead to excessive data duplication, as each subsystem would store a replica of the same underlying data in their own format.In this work, we have designed GRAPHONE, a unified graph data store offering diverse data access at various granularity levels while supporting data ingestion at a high ar- rival rate.
Fig. 1 provides a high-level overview.
It leverages a hybrid graph store to combine a small circular edge log (henceforth edge log) and an adjacency store for their complementary advantages.
Specifically, the edge log keeps the latest updates in the edge list format, and is designed to accelerate data ingestion.
At the same time, the adjacency store holds the snapshots of the older data in the adjacency list format that is moved periodically from the edge log, and is optimized for batch and streaming analytics.
It is important to note that the graph data is not duplicated in two formats, although a small amount of overlapping is allowed to keep the original composition of the versions intact.GRAPHONE enforces data ordering using the temporal nature of the edge log, and keeps the per-vertex edge arrival order intact in the adjacency store.
A dual versioning technique then exploits the fine-grained versioning of the edge list format and the coarse-grained versioning of the adjacency list format to create real-time versions.
Further, GRAPHONE allows independent execution of analytics that run parallel to data management, and can fetch a new version at the end of its own incremental computation step.
Additionally, we provide two optimization techniques, cacheline sized memory allocation and special handling of high degree vertices of power-law graphs, to reduce the memory requirement of versioned adjacency store.GRAPHONE simplifies the diverse data access by presenting a new data abstraction, GraphView, on top of the hybrid store.
Two types of GraphView are supported as shown in Fig. 1 : (1) the static view offers real-time versioning of the latest data for batch analytics; and (2) the stream view supports stream analytics with the most recent updates.
These views offers visibility of data updates to analytics at two levels of granularity where the edge log is used to offer it at the edge level, while the adjacency store provides the same at coarse granularity of updates.
As a result, GRAPHONE provides high-level applications with the flexibility to trade-off the granularity of data visibility for a desired performance.
In other words, the edge log can be accessed if fine-grained data visibility is required, which can be tuned ( §7.3).
We have implemented GRAPHONE as an in-memory graph datastore with a durability guarantee on external nonvolatile memory express solid-state drives (NVMe SSD).
For comparison, we have evaluated it against three types of inmemory graph systems: Neo4j and SQLite, two graph data management systems; Stinger [21], a dynamic graph system; and Galois [60], a static graph system, as well as GRA-PHONE itself working with static graphs.
The experimental results show that GRAPHONE can support a high data ingestion rate, specifically it achieves two to three orders of magnitude higher ingestion rate than graph databases, and 5.36× higher ingestion rate than Stinger.
In addition, GRAPHONE outperforms Stinger by more than 3× on different analytics, and delivers equivalent algorithmic performance compared to Galois.
The stream processing in GRAPHONE runs parallel to data ingestion which offers 26.22% higher ingestion rate compared to the current practice of interleaving the two.To summarize, GRAPHONE makes three contributions:• Unifies stream and base stores to manage the graph data in a dynamic environment; • Provides batch and stream analytics through dual versioning, smart data management, and memory optimization techniques; • Supports diverse data access of various usecases with GraphView and data visibility abstractions.The rest of the paper is organized as follows.
We present a usecase in §2, opportunities and GRAPHONE overview in §3, the hybrid store in §4, data management internals and optimizations in §5, GraphView data abstraction in §6, evaluations in §7, related work in §8, and conclusion in §9.
Graph analytics is a natural choice for data analysis on an enterprise network.
Fig. 2(a) shows a graph representation of a simple computer network.
Such a network can be analyzed in its entirety by calculating the diameter [48], and betweenness centrality [13] to identify the articulation points.
This kind of batch analysis is very useful for network infrastructure management.
In the meantime, as the dynamic data flow within the network captures the real-time behaviors of the users and machines, the stream analytics is used to identify security risks, e.g., denial of service, and lateral movement, which can be expressed in the form of path queries, parallel paths and tree queries on a streaming graph [38,18].
Los Alamos Nation Laboratory (LANL) recently released a comprehensive data set [37] that captures a wide range of network information, including authentication events, process events, DNS lookups, and network flows.
The LANL data covers over 1.5 billion events, 12,000 users, and 17,000 computers, and spans 58 consecutive days.
For example, the network authentication data captures the login information that a user logs in to a network machine, and also from that machine to other machines.
When the network defense system identifies a malicious user and node, it needs to find all the nodes that may have been infected.
Instead of analyzing every node of the network, one can quickly run a path traver- sal query on the real-time authentication graph to identify the possible infected nodes, that is, find all the nodes whose login has originated from the chain of nodes that are logged in from the first infected machine [38] as shown in Fig. 2(b).
In summary, a high-performance graph store that captures dynamic data in the network, combined with user, machine information and network topology, is advantageous in understanding the health of the network, accelerating network service, and protecting it against various attacks.
This work presents a graph storage and APIs for such usecases.
A graph can be defined as G = (V, E, W), where V is the vertex set, and E is the edge set, and W is the set of edge weights.
Each vertex may also have a label.
In this section, graph formats and their traits are described as relevant for GRAPHONE, and then we present its high-level overview.
Fig. 3 shows three most popular data formats for a sample graph.
First, the edge list is a collection of edges, a pair of vertices, and captures the incoming data in their arrival order.
Second, the compressed sparse row (CSR) groups the edges of a vertex in an edge array.
There is a metadata structure, vertex array, that contains the index of the first edge of each vertex.
Third, the adjacency list manages the neighbors of each vertex in separate per-vertex edge arrays, and the vertex array stores a count (called degree) and pointer to indicate the length and the location of the corresponding edge arrays respectively.
This format is better than the CSR for ingesting graph updates as it affects only one edge array at a time.
In the edge list, the neighbors of each vertex are scattered across, thus is not the optimal choice for many graph queries and batch analytics who prefer to get the neighboring edges of a vertex quickly [34,29,30,12] etc .
On the other hand, the adjacency list format loses the temporal ordering as the incoming updates get scattered over the edge arrays, thus not suited for stream analytics.
Given their advantages and disadvantages, neither format is ideally suited for supporting both batch and stream analytics on its own.
We now identify two opportunities for this work: Opportunity #1: Utilize both the edge list and the adjacency list within a hybrid store.
The edge list format preserves the data arrival order and offers a good support for fast updates as each update is simply appended to the end of the list.
On the other hand, the adjacency list keeps all the neigh- bors of a vertex indexed by the source vertex, which provides efficient data access for graph analytics.
Thus it allows GRAPHONE to achieve high-performance graph computation while simultaneously supporting fine-grained updates.
Opportunity #2: Fine-grained snapshot creation with the edge list format.
Graph analytics and queries require an immutable snapshot of the latest data for the duration of their execution.
The edge list format provides a natural support for fine-grained snapshot creation without creating a physical snapshot due to its temporal nature, as tracking a snapshot is just remembering an offset in the edge list.
Meanwhile, the adjacency list format through its coarse-grained snapshot capability [54,26] is used to complement the edge list.
GRAPHONE utilizes a hybrid graph data store (discussed in §4) that consists of a small circular edge log and the adjacency store.
Fig. 4 shows an high-level overview of GRA-PHONE architecture.
The hybrid store is managed in several phases (presented in §5).
Specifically, during the logging phase, the edge log records the incoming updates in the edge list format in their arrival order, and supports a high ingestion rate.
We define non-archived edges as the edges in the edge log that are yet to be moved to the adjacency store.
When their number crosses the archiving threshold, a parallel archiving phase begins, which merges the latest edges to the adjacency store to create a new adjacency list snapshot.
This duration is referred to as an epoch.
In the durable phase, the edge log is written to a disk.To efficiently create and manage immutable versions for data analytics in presence of the incoming updates, we provide a set of GraphView APIs (discussed in §6).
Specifically, static view API is for batch processing, while stream view API is for stream processing.
Internally, the views utilize dual versioning technique where the versioning capability of both formats are exploited.
For example, a real-time static view can be composed by using the latest coarse-grained version of the adjacency store, and the latest fine-grained version of non-archived edges.It is important to note that the GraphView also provides analytics with the flexibility to trade-off the granularity of data visibility for better performance, e.g., the analytics that prefer running only on the latest adjacency list store will avoid the cost associated with the access of the latest edges from the non-archived edges.
The hybrid store design presented in Fig. 5 consists of a small circular edge log that is used to record the latest updates in the edge list format.
For deletion cases, we use tombstones, specifically the edge log also adds a new entry but the most significant bit (MSB) of the source vertex ID of the edge is set to denote its deletion as shown in Fig 5 for deleted edge (2, 4) at time t 7 .
The adjacency store keeps the older data in the adjacency list format.
The adjacency store is composed of vertex array, per-vertex edge arrays, and multi-versioned degree array.
The vertex array contains a per-vertex flag and pointers to the first and last block of the edge arrays.
Addition of a new vertex is done by setting a special bit in the per-vertex flag.
Vertex deletion sets another bit in the same flag, and adds all of its edges as deleted edges to the edge log.
These bits help GRAPHONE in garbage collecting the deleted vertex ID.The edge array contains per-vertex edges of the adjacency list.
It may contain many small edge blocks, each of which contains a count of the edges in the block and a memory pointer to the next block.
The connection of edge blocks are referred to as chaining.
An edge addition always happens at the end of the edge array of each vertex, which may require the allocation of a new edge block and linked to the last block.
Fig. 5 shows chained edge arrays for the vertices with ID 1 to 4 for data updates that arrive in between t 4 to t 7 .
The adjacency list treats an edge deletion as an addition but the deleted edge entry in the edge array keeps the negative position of the original edge, while the actual data is not modified at all, as shown for edge (2,4).
As a result, deletion never breaks the convergence of a previous computation as it does not modify the dataset of the computation.The degree array contains the count of neighboring edges of each vertex.
Thus, a degree array from an older adjacency store snapshot can identify the edges to be accessed even from the latest edge arrays due to the latter's append-only property.
Hence, the degree array in GRAPHONE is multiversioned to support adjacency store snapshots.
It keeps the total added and deleted edge counts of each vertex.
Both counts help in efficiently getting the valid neighboring edges, as a client can do the exact memory allocation (refer to the S1, 8 Circular Edge Log Global Snapshot List get-nebrs-*() API in Table 2).
When an edge is added or deleted for a vertex, a new entry is added for this vertex in the degree array in each epoch.
Two different versions S0 and S1 of the degree array are shown in Fig. 5 for two epochs t 0 − t 3 and t 4 − t 7 .
One can note that degree nodes are shared across epochs if there is no later activity in a vertex.
For example, the same degree nodes for vertices with ID 5 and 6 are valid for both epochs in Fig. 5.
The degree array nodes of an older versions may be garbage collected when the corresponding adjacency store snapshot retires, i.e., not being used actively by any analytics, and is tracked using reference counting mechanism through the global snapshot list, which will be discussed shortly.
For example, if snapshot S0 is retired, then the degree nodes of snapshot S0 for vertices with ID 1 − 4 can be reused by later snapshots (e.g. S2).
The global snapshot list is a linked list of snapshot objects to manage the relationship between the edge log and adjacency store at each epoch.
Each node contains an absolute offset to the edge log where the adjacency list snapshot is created, and a reference count to capture the number of views using this adjacency list snapshot.
A new entry in the global snapshot list is created after each epoch, and it implies that the edge log data of the last epoch has been moved to the adjacency store atomically, and is now visible to the world.
Weighted Graphs.
Edge weights are generally embedded in the edge arrays along with the destination vertex ID.
Some graphs have static weights, e.g., an edge weight in an enterprise network can represent the network speed between the two nodes.
A weight change is then treated internally as an edge deletion followed by an edge addition.
On the other hand, if edge weights are dynamic, such as network data flow, then such weights are suited for various analytics if kept for a configurable time window, e.g., anomaly detection in the network flow.
In this case GRAPHONE is configured to treat weight changes as a new edge to aid such analytics.
Dual Versioning and Data Overlap GRAPHONE uses dual versioning to create the instantaneous read-only graph views (snapshot isolation) for data analytics.
It exploits both the fine-grained versioning property of the edge log, and the coarse-grained versioning capability of the adjacency list format.
It should be noted that the adjacency list provides one version per epoch, while the edge log supports multiple versions per epoch, as many as the number of edges arrived during the epoch.
So the dual versioning provides many versions within an epoch which is the basis for static views, and should not be confused with the adjacency list snapshots.
In Fig. 5, static view at the time t 6 would be adjacency list snapshot S0 plus the edges from t 4 − t 6 .
A small amount of data overlap between the two stores keeps the composition of the view intact.
This makes the view accessible even when the edge log data is moved to the adjacency store to create a new adjacency list version.
Thus both stores have the copy of a few epochs of the same data.
For one or more long running iterative analytics, we may use the durable edge log or a private copy of non-archived edges to provide data overlap, so that analytics can avoid interfering with data management operations of the edge log.
Data management faces the key issues of minimizing the size of non-archived edges, providing atomic updates, data ordering, and cleaning of older snapshots.
Addition and deletion of vertices and edges, and edge weight modification are all considered as an atomic update.
Fig. 4 depicts the internals of the data management operations.
It consists of four phases: logging, archiving, durable and compaction.
Client threads send updates, and the logging to the edge log happens in the same thread context synchronously.
The archiving phase moves the non-archived edges to the adjacency store using many worker threads, and one of them assumes the role of the master, called the archive thread.
The durable phase happens in a separate thread, while compaction is multi-threaded but happens much later.
A client thread wakes up the archive thread and durable thread to start the archiving and durable phases when the number of non-archived edges crosses a threshold, called archiving threshold.
The logging phase continues as usual in parallel to them.
Also, the archive thread and durable thread check if any non-archived edges are there at the end of each phase to repeat their process, or wait for work with a timeout.The edge log has a distinct offset or marker, head, for logging, which is incremented every time an edge is ingested as shown in Fig. 6 the tail archive marker to the head archive marker, because the head will keep moving due to new updates.
The durable phase also has a pair of markers to work with.
Markers are always incremented and used with the modulo operator.
The incoming update is converted to numerical identifiers, and acquires an edge list format.
The mapping between vertex label to vertex ID and vice-versa manages this translation.
Then a unique spot is claimed within the edge log by the atomic increment of the head, and the edge is written to a spot calculated using the modulo operation on the head, that also stores the operator ( §4), addition or deletion, along with the edges.
The atomicity of updates is ensured by the atomic increment of the head.
The edge log is automatically reused in the logging phase due to its circular nature, and thus is overwritten by newer updates.
Hence the logging may get blocked occasionally if the whole buffer is filled as the archiving or durable phases may not be able to catch up.
We keep sufficiently large edge log to avoid frequent blocking.
In case of blocked client threads, they are woken up when the archiving or durable phases complete.
This phase moves the non-archived edges from the edge log to the adjacency store.
A naive multi-threaded archiving, where each worker can directly work on a portion of nonarchived edges, may not keep the data ordering intact.
If a deletion comes after the addition of an edge within the same epoch, the edge may become alive or dead in the edge arrays depending on the archiving order of the two data points.
An edge sharding stage in the archiving phase ( Fig. 7) maintains per-vertex edges as per the edge log arrival to address the ordering problem.
It shards the non-archived edges to multiple local buffers based on the range of their source vertex ID.
For undirected graphs, the total edge count in the local buffer is twice of the non-archived edge count, as the ordering of reverse edges is also managed.
For directed edges, both directions have their own local buffers.The edges in each local buffer are then archived in parallel without using any atomic instructions.
A heuristic is required for workload distribution, as the equal division is not possible among threads, thereby the last thread may get more work assigned.
To handle the workload imbalance among worker threads, we create a larger number of local buffers with smaller vertex range than the available threads, and assign different numbers of local buffers to each thread so that each gets an approximately equal number of edges to archive.
The idea here is to assign slightly more than equal work to each thread, so that all the threads are balanced while the last thread is either balanced or lightly loaded.This stage allocates new degree nodes or can reuse the same from the older degree array versions if they are not being used by any analytics.
We follow these rules for reusing the degree array from older versions.
We track the degree array usage by analytics using reference counting per epoch [40], and can be reused if all static views created within that epoch have expired, i.e., the references are dropped to zero (not being used by any running analytics).
It also ensures that a newly created view uses the latest adjacency list snapshot that should never be freed.The stage then populates the degree array, and allocates memory for edge blocks that are chained before filling those blocks.
We then create a new snapshot object, fill it up with relevant details, and add it atomically to the global snapshot list.
At the end of the archiving phase, the archive thread sets the tail archive marker atomically to the value of the head archive marker, and wakes up any the blocked client threads.
The edge log data is periodically appended to a durable file in a separate thread context instead of logging immediately to the disk to avoid the overhead of IO system calls during each edge arrival.
Also this will not guarantee durability unless fsync() is called.
The logging uses buffered sequential write, and allows the buffer cache to work as spillover buffer for the access of non-archived edges if the edge log is over-written.
The durable edge log is a prefix of the whole ingested data, so GRAPHONE may lose some recent data in the case of an unplanned shutdown.
The recovery depends on upstream backup that keep the latest data for some time, such as kafka [42], and replays it for the lost data, and creates the adjacency list on the whole data.
Recovery is faster than building the data structures at an edge level, as only the archiving phase is involved working on bulk of data.
Alternatively, persistent memory may be used for the edge log to provide durability at each update [45].
The durable phase also performs an incremental checkpointing of the adjacency store data from an old timewindow, and frees the memory associated with it.
This is useful for streaming data such as LANL network flow, where the old adjacency data can be checkpointed in disk, as the in-memory adjacency store within the latest time window is sufficient for stream analytics.
By default, it is not enabled.
During checkpointing the adjacency store, the vertex ID and length of the edge array are persisted along with edge arrays so that data can be read easily later, if required.
The compaction of the edge arrays removes deleted data from per-vertex edge array blocks up to the latest retired snapshot identified via the reference counting scheme discussed in §5.1.2.
The compaction needs a similar reference counting for the private static views ( §6.1).
For each vertex, it allocates new edge array block and copies valid data up to the latest retired snapshot from the edge arrays, and creates a link to the rest of the original edge array blocks.
The newly created edge array block is then atomically replaced in the vertex array, while freeing happens later to ensure that cached references of the older data are dropped.
This phase is generally clubbed with archiving phase where the degree array is updated to reflect the new combination.
The edge log and degree array are responsible for versioning.
The edge log size is relatively small as it contains only the latest updates which moves quickly to the base store, e.g, the archiving threshold of 2 16 edges translates to only 1MB for a plain graph assuming 8 byte vertex ID.
Thus the edge log is only several MBs.
The memory in degree arrays are also reused ( §5.1.2).
This leaves us to memory analysis of edge arrays which may consume a lot of memory due to excessive chaining in their edge blocks.
For example, GRA-PHONE runs archiving phase for 2 16 times for Kron-28 graph if the archiving threshold is 2 16 .
In this case, the edge arrays would consume 148.73GB memory and have average 29.18 chain per-vertex.
We will discuss the graph datasets used in this paper shortly.
If all the edges were to be ingested in one archiving phase, this static system needs only an average 0.45 chain and 33.80GB memory.
The chain count is less than one as 55% vertices do not have any neighbor.
GRAPHONE uses two memory allocation techniques, as we discuss next, to reduce the level of chaining to make the memory overhead of edge arrays modest compared to a static engine.
The techniques work proactively, and do not affect the adjacency list versioning.
Compaction further reduces the memory overhead to bring GRAPHONE at par with static analytics engine, but is performed less frequently.
Optimization #1: Cacheline Sized Memory Allocation.
Multiples of cacheline sized memory is allocated for the edge blocks.
One cacheline (64 bytes) can store up to 12 neighbors for the plain graph of 32bit type, leaving the rest of the space for storing a count to track space usage in the block and a link to the next block.
In this allocation method, the ma- (Fig. 15).
GraphView data abstraction hides the complexity of the hybrid store by providing simple data access APIs as shown in Fig. 8: GRAPHONE hybrid store illustrating various views with two adjacency store versions, S0 and S1, with a small edge log is composed of the same adjacency store and non-archived edges as shown in Fig. 8.
The access of non-archived edges provides data visibility at the edge level granularity.
Due to the cost of indexing the non-archived edges, GraphView provides an option to trade-off the granularity of data visibility to gain performance.
Further, one can use vertex-centric compute model [73] on the adjacency list plus edge-centric compute model [81,43,66] on non-archived edges, so there is no need to index the latter as plotted later to find its optimal minimum size (Fig. 13).
Batch analytics and queries prefer snapshots for computation, which can be created in real-time using create-staticview() API.
It is represented by an opaque handle that identifies the view composition, i.e., the non-archived edges and the latest adjacency list snapshot, and serves as input to other static view APIs.
A created handle should be destroyed using delete-static-view().
Based on the input supplied to createstatic-view() API, many types of static view are defined.
Basic Static View.
This view is very useful for advanced users and higher level library development which prefer more control and performance.
The main low-level API are: get-nebrs-archived-*() that returns the reference to the pervertex edge array; and get-non-archived-edges() that returns the non-archived edges.
On the other hand, it also provides a high-level API, get-nebrs-*(), that returns the neighbor list of a vertex by combining the adjacency store and the nonarchived edges in a user supplied memory buffer.
It may be preferable by queries with high selectivity that only need to scan the non-archived edges for one or a few vertex, e.g. 1-hop query, and is not apt for long running analytics.The implementation of get-nebrs() for the non-deletion case is a simple two step process: copy the per-vertex edge array to the user supplied buffer, followed by a scan of the non-archived edges to find and add the rest of the edges of the vertex to the buffer.
For the deletion case, both the steps track the deleted positions in the edge arrays, and the last few edges from edge arrays and/or non-archived edge log are copied into those indexes of the buffer.
Private Static View.
For long running analytics, keeping basic static views accessible have some undesirable impacts: (1) all the static views may have to use the durable edge log if the corresponding non-archived edges in the edge log has been overwritten; (2) the degree array cannot be reused in the archiving phase as it is still in use.
To solve this, one can create a private static view by passing private=true in the create-static-view() API.
In this case, a private copy of the non-archived edges and the degree array are kept inside the view handle with their global references dropped to make it independent from archiving.
One can pass simple=true in the create-static-view() to create a temporary in-memory adjacency list from the non-archived edges for optimizing getnebr-*() API, as shown in Algorithm 1 for a simplified BFS (push model) implementation.
This approach is more flexible than static analytics engine which converts the whole data, or dynamic graph system that disallows the user to choose fine-grained control on snapshot creation.Creation to many private static views may introduce memory overhead.
To avoid this, a reference of the private degree array is kept in the snapshot object and is shared by other static views created within that epoch, and are locally reference counted for freeing.
Thus, creating many private views within an epoch has overhead of just one degree array.
However, creating many private static views across epochs may still cause the memory overhead, if older views are still being accessed by long running analytics.
This also means that the machine is overloaded with computations, and they are not real-time in nature.
In such a case, a user may prefer to copy the data to another machine to execute them.
Stale Static View.
Many analytics are fine with data visibility at coarse-grained ingestions, thus some stale but consistent view of the data may be better for their performance.
In this case, passing stale=true returns the snapshot of the latest adjacency list only.
This view can be combined with private static view where degree array will be copied.
Stream computations follow a pull method in GRAPHONE, i.e., the analytics pulls new data at the end of incremental compute to perform the next phase of incremental compute.
The stream view APIs around the handle simplify the data access and its granularity in presence of the data ingestion.
Also, checkpointing the computation results and the associated data offset is the responsibility of the stream engine, so that the long running computation can be resumed from that point onwards in case of a fault.
Stateless Stream Processing.
A stateless computation, e.g. counting incoming edges (aggregation), only needs a batch of new edges.
It can be registered using the reg-streamview() API, and the returned handle contains the batch of new edges.
Algorithm 2 shows how one can use the API to do stateless stream computation.
The handle also allows a pointer to point to analytics results to be maintained by the stream compute implementation.
The implementation also needs to checkpoint only the edge log offset and the computation results as GRAPHONE keeps the edge log durable.An extension of the model is to process on a data window instead on the whole arrived data.
For sliding window implementation, GRAPHONE manages a cached batch of edge data around the start marker of the data window in addition to the batch of new edges.
The old cached data can be accessed by the analytics for updating the compute results, e.g., subtracting the value in aggregation over the data window.
The cached data is fetched from the durable edge log, and shows sequential read due to the sliding nature of the window.
A tumbling window implementation is also possible where the batch size of new edges is equal to the window size, and hence does not require older data to be cached.
Additional checkpointing of the starting edge offset is required along with the edge log offset and computation results.Stateful Stream Processing.
A complex computation, such as graph coloring [67], is stateful that needs the streaming data and complete base store to access the computational state of the neighbors of each vertex.
A variant of static view is better suited for it because its per-vertex neighbor information eases the access of the computational state of neighbors.
It is registered using reg-sstream-view(), and returns sstreamhandle.
For edge-centric computation, the handle also contains a batch of edges to identify the changed edges.
For vertex-centric computation, the handle contains per-vertex one-bit status to denote the vertex with edge updates that can be identifies using the has-vertex-changed() API.
This is updated during update-sstream-view() call that also updates the degree array.
Algorithm 3 shows an example code snippet.As the degree array plays an important role for a stateful computation due to its association with the static view, using an additional degree array at the start marker of the data window eases the access of the data within the window from the adjacency store.
The sstream-handle manages the degree array on behalf of the stream engine, and internally keeps a batch of cached edges around the start marker of the window Algorithm 3 A stateful stream compute (vertex-centric) skeleton 1: handle ← reg-sstream-view(global-data, v-centric, stale=true) 2: init-sstream-compute(handle) Application specific 3: while true do Or application specific criteria 4:if update-sstream-view(handle) then 5: for v=0; v < vertex-count; v++ do 6:if has-vertex-changed(handle, v) then 7:do-sstream-compute(handle, v) Application specific 8: unreg-sstream-view (handle) to update the old degree array.
The get-nebrs-*() function returns the required neighbors only.
Checkpointing the computational results, the edge log offset at the point of computation, and window information is sufficient for recovery.
GRAPHONE provides many views from recent past, but it is not designed for getting arbitrary historic views from the adjacency store.
However, durable edge log can provide the same using get-prior-edges() API in edge list format as it keeps deleted data, behaving similar to existing data stores [14,23].
Moreover, in case of no deletion, one can create a degree array at a durable edge log offset by scanning the durable edge log, and the degree array will serve older static or stream view from the adjacency store to gain insights from the historical data.
For data access from a historical time-window in this case, one need to build two degrees arrays at both the offsets of the durable edge log.
GRAPHONE is implemented in around 16,000 lines of C++ code including various analytics.
It supports plain graphs and weighted graphs with either 4 byte or 8 byte vertex sizes.
We store the fixed weights along with the edges, variable length weights in a separate weight store using indirection.
Any type of value can be stored in place of weight such as integers, float/double, timestamps, edge-id or any custom weight as the code is written using C++ templates.
So one can write a small plug-in describing the weight structures and other functions, and GRAPHONE would be ready to serve a custom weight.
All experiments are run on a machine with 2 Intel Xeon CPU E5-2683 sockets, each having 14 cores with hyper-threading enabled.
It has 512GB memory, Samsung NVMe 950 Pro 512GB, and CentOS 7.2.
Prior results have also been performed on the same machine.We choose data ingestion, BFS, PageRank and 1-Hop query to simulate the various real-time usecases to demonstrate the impact of GRAPHONE on analytics.
BFS and PageRank are selected because many real-time analytics are iterative in nature, e.g. shortest path, and many prior graph systems readily implement them for comparison.
1-Hop query accesses the edges of random 512 non-zero degree vertices and sums them up to make sure we access them all.
1-Hop query simulates many small query usecases, such as listing one's friends, or triangle completion to get friend suggestions in a social graph, etc.
During the ingestion, vertex name to vertex ID conversion was not needed as we directly used the vertex ID supplied with these datasets as followed by other graph systems.
All the edges will be stored twice in the adjacency list: in-edges and out-edges for directed graphs, and symmetric edges for undirected graphs.
No compaction was running in any experiments unless mentioned.
Datasets.
Table 3 lists the graph datasets.
Twitter [3], Friendster [1] and Subdomain [4] are real-world graphs, while Kron-28 and Kron-21 are synthetic kronecker graphs generated using graph500 generator [25], all with 4 byte vertex size and without any weights.
LANL network flow dataset [74] is a weighted graph where vertex and weight sizes are 4 bytes and 32 bytes respectively, and weight changes are treated as new streaming data.
We run experiment on first 10 days of data.
We test deletions on a weighted RMAT graph [15] generated with [56] where vertex and weight sizes are 8 bytes.
It contains 4 million vertices, and 64 million edges, and a update file containing 40 million edges out of which 2,501,937 edges are for deletions.
Logging and Archiving Rate.
Logging to edge log is naturally faster, while archiving rate depends upon the archiving threshold.
Table 3 lists the logging rate of a thread, and archiving rate at the archiving threshold of 2 16 edges for our graph dataset.
A thread can log close to 80 million edges per second, while archiving rate is only around 45 million edges second at the archiving threshold for most of the graphs.
Both the rates are lower for LANL graph, as the weight size is 32 bytes, while others have no weights.
Ingestion Rate.
It is defined as single threaded ingestion to the edge log at one edge at a time, and leaving the archive thread and durable phase to automatically change with the arrival rate.
The number is reported when all the data are in the adjacency store, and persisted in the NVMe ext4 file.
GRAPHONE achieves an ingestion rate of more than 45 million edges per second, except LANL graph.
The ingestion rate is higher than archiving rate (at the archiving threshold) except in Kron-21, as edges more than the archiving threshold are archived in each epoch due to higher logging rate.
This indicates that GRAPHONE can support a higher arrival rate as archiving rate can dynamically boost with increased arrival velocity.
The Kron-21 graph is very small graph, and the thread communication cost affects the ingestion rate.
Compaction Rate.
We run compaction as a separate benchmark after all the data has been ingested.
The graph compaction rate is 345.53 million edges per second for the RMAT graph which has more than 2.5 million deleted edges out of total 104 million edges.
Results for other graphs are shown in Table 3.
The poor rate for LANL graph is due to long tail for compacting edge arrays of few vertices.
As shown later in Fig. 12, the compaction improves the analytics performance where the static GRAPHONE serves compacted adjacency list as it had no link in its edge arrays.
Durability.
The durable phase has less than 10% impact on the ingestion rate.
Table 3 shows the in-memory ingestion rate and can be compared against that of GRAPHONE, which uses NVMe SSD for durability.
This is because durable phase runs in a separate thread context, and exhibits only sequential write.
The NVMe SSD can support up to 1500MB/s sequential write and that is sufficient for GRAPHONE as it only needs smaller write IO throughput, as shown in Fig. 9 for Friendster graph.
This indicates that a higher logging rate can easily be supported by using a NVMe SSD.
Recovery.
Recovery only needs to perform archiving phase at bulk of data.
As we will show later in Fig. 13, the archiving is fastest when around 2 27 -2 31 edges are cleaned together.Hence we take the minimum of this size as recovery threshold to minimize the memory requirement of IO buffer and the recovery time, and also gets an opportunity to pipeline the IO read time of the data with recovery.
Table 3 shows the total recovery time, including data read from NVMe SSD after dropping the buffer cache.
Clearly, GRAPHONE hides the IO time when compared against in-memory recovery.
The recovery rate varies a lot for different graph due to different distribution of the batch of graph data that has profound impact on parallelism and hence locality access of edge arrays.
We choose different classes of graph systems to compare against GRAPHONE.
Stinger is a dynamic graph system, Neo4j and SQLite are graph databases, and Galois and static version of GRAPHONE are static graph systems.
Except stream computations, all the analytics in this section are performed on private static view containing no non-archived edges as it is created at the end of the ingestion.
Dynamic Graph System.
Stinger is an in-memory graph system that uses atomic instructions to support fine-grained updates.
So it cannot provide semantically correct analytics if updates and computations are scheduled at the same time, as different iteration of the analytics will run on the different versions of the data.
We used the benchmark developed in [56] to compare the results on the RMAT graph.Stinger is able to support 3.49 million updates/sec on the same weighted RMAT graph, whereas GRAPHONE ingests 18.67 million edges/sec, achieving 5.36× higher ingestion rate.
Part of the reason for poor update rate of Stinger is that unlike GRAPHONE, it directly updates the adjacency store using atomic constructs.
We have implemented PageRank and BFS in a similar approach as Stinger.
The comparison is plotted in Fig. 10.
Clearly, GRAPHONE is able to provide a better support for BFS and PageRank achieving 12.76× and 3.18× speedup respectively.
The reason behind the speedup is explicit optimization to reduce the chaining which removes a lot of pointer chasing, and better cache access locality due to cacheline sized edge blocks.
Databases.
We compare against SQLite 3.7.15.2, a relational database, Neo4j 3.2.3, a graph database for ingestion test.
SQLite and Neo4j support ACID transaction, and do not provide native support for graph analytics.
It is known that higher update rate is possible by trading off the strict serializability of databases, however to measure the magnitude of improvement, it is necessary to conduct experiment.
The in-memory configuration of SQLite can ingest 12.46K edges per second, while GRAPHONE is able to support 18.67 million edges per second in the same configuration for above dataset.
Neo4j could not finish the benchmark after more than 12 hours, which is along the same line as observed in [56].
Hence we have tested on a smaller graph with 32K vertices, 256K edges, and 100K updates.
Neo4j is configured to use disk to make it durable.
Neo4j and GRAPHONE both use the buffer cache while persisting the graph data.
Neo4j can ingest only 14.81K edges per second, whereas GRAPHONE ingests at 3.63M edges per second.
Static Graph System.
We compare against Galois, a representative in-memory static graph engine based on CSR format.
It does not provide the data management capabil- ity, so the whole graph is constructed in one time, called pre-processing time, which takes a significant amount of time [55].
In contrast, GRAPHONE can start the analytics without any pre-processing.
Fig. 11 shows the speed up of GRAPHONE for PageRank and BFS over Galois (without pre-processing cost) for all the graphs except Kron-28 as Galois had a memory error.
The PageRank results are almost same as it is compute intensive, thus effect of chaining is not observed.
For Kron-21-16 which is very small, the performance of Galois is bad.
We suspect that the cost of manual workload division in Galois for small graphs affects its performance, while we use dynamic scheduling of OpenMP.
For BFS, GRAPHONE performs better than Galois with an exception in the Subdomain graph.
Both systems have same BFS implementation (direction-optimized BFS [11]) with a minor implementation difference.
Our BFS is implemented using the status array metadata where the level of each vertex is maintained as one byte word, and tracking the active vertices requires revisiting whole status array.
Galois uses the frontier queue where active vertices are kept in a separate work queue.
Based on our experience with graph systems, status array implementation is faster for small diameter graphs, otherwise frontier queue approach is better.
The Subdomain graph has 140 BFS levels (the highest of all graphs) hence we perform poorly, but Kron-21 has only 7 levels (the least of all the graphs) so the speedup is the highest.
Static GRAPHONE.
GRAPHONE is expected to perform slightly worse than the static graph engine without including the pre-processing cost, but much better if including.
Therefore to demonstrate the performance overhead of data management and chaining without any specific algorithm differences, we compare GRAPHONE against the static configuration of itself where maximum chain count is just one.
Fig. 12 shows this performance drop (without including pre-processing cost), specifically trading off just 17% average performance for real-world graphs (26% for all the graphs plotted) from the static system, one can support high arrival velocity of fine-grained updates.
However, the performance drop is only temporary as the compaction process will remove the chaining in the background.
Moreover, when adding the pre-processing cost to the static system, GRA-PHONE is able to perform better.
For example, the preprocessing cost for Kron-28 graph is 32.73s, one or multiple orders of magnitude longer than the runtime of these algorithms, e.g. 34.12× more than the run-time of BFS.
Stream Graph Engines.
The logging and archiving operations are examples of different categories of stream analyt- ics: logging is a proxy to continuous stateless stream analytics, while archiving is same to the discrete stateful stream analytics.
Thus, Table 3 is also an indication of their performance.
We have also implemented a streaming weakly connected components using ideas from COST [57] using stateless stream view APIs and it can process 33.60 million stream edges/s on Kron-28 graph.
GRAPHONE runs stream computation and data ingestion concurrently, while prior stream processing systems interleave them that results into lower ingestion rate.
To demonstrate the advantage of this design decision, we have implemented a streaming PageRank using stateful stream view APIs that runs in parallel to data ingestion in GRAPHONE.
To simulate the prior stream processing we interleave the two.
The execution shows that GRAPHONE improves the data ingestion by 26.22% for Kron-28 graph.
We leave the comparison against other stream processing engine as future work as the focus of this work is on graph data-store.
Performance Trade-off in Hybrid Store.
We first characterize the behavior of the hybrid store for different number of non-archived edges.
Fig. 13 shows the performance variation of archiving rate, BFS, PageRank, and 1-hop query for Kron-28 graph when the non-archived edge counts are increased, while the rest of the edges are kept in the adjacency store for Kron-28.
The figure shows that up to 2 17 nonarchived edges in the edge log brings negligible drop in the analytics performance.
Hence, we recommend the value of archiving threshold as 2 16 edges as the logging overlaps with the archiving.
GRAPHONE is able to sustain an archiving rate 43.68 million edges per second at this threshold.
The 1-Hop query latency of all 512 queries together is only 53.766 ms, i.e. 0.105 ms for each query.The archiving threshold of 2 16 edges is not unexpected as it is small compared to total edge count (2 33 ) in Kron-28, and the analytics on non-archived edges are parallelized.
Further, the parallelization cost dominates when the number of nonarchived edges are small (2 10 ).
Thus the analytics cost drops only when the number of non-archived edges becomes large.
Fig. 13 also shows that higher archiving threshold leads to better archiving rate, e.g., a archiving threshold of 1,048,576 (2 20 ) edges can sustain a archiving rate of 56.99 million edges/second.
The drawback is that the analytics performance will be reduced as it will find more number of nonarchived edges.
On the contrary, archiving works continuously and tries to minimize the number of non-archived edges, so a smaller arrival rate will lead to frequent archiving, and thus fewer non-archived edges will be observed at any time.
The drop in archiving rate at the tail is due to the impact of large working set size that leads to more last-levelcache transactions and misses while filling the edge arrays.
Scalability.
The edge sharding stage removes the need of atomic instruction or locks completely in the archiving phase, which results into better scaling of archiving rate with increasing number of threads as plotted in Fig. 14.
There is some super-linear behavior when thread count is increased from 16 to 32.
This is due to the second socket coming into picture with its own hardware caches, and non-atomic behavior makes it to scale super-linear.
This observation is confirmed by running the archiving using 16 threads spread equally across two sockets, and achieves higher archiving rate compared to the case when the majority of threads belong to one socket.
Memory Allocation.
Fig. 15 shows that the cacheline sized memory allocation and special handling of hub-vertices improve the performance of the archiving and analytics.
The cacheline sized memory optimization improves the archiving rate at the archiving threshold by 2.20× for Kron-28 graph, while speeding up BFS, PageRank and 1-Hop query performance by 1.37×, 3.11× and 8.82×.
Hub vertices handling additionally improves the query performance (by 7.5%).
Edge Log Size.
Fig. 16 shows the effect of edge log size on overall ingestion rate on Kron-28 graph.
Clearly, an edge log Edge Log Size (in Million Edges) Fig. 16: Showing Ingestion rate when edge log size increases.size greater than 4 million edges (32 MB) does not have any impact on overall ingestion rate.
Static graph analytics systems [66,50,43,60,52,79,16,36,53,65,27,44,81,28,76,7,80] support only batch analytics where pre-processing consumes much more time than the computation itself [55].
Grapchi [47] and other snapshot based systems [35,54,62,39,26] support bulk updates only.
Naiad [58], a timely dataflow framework, supports iterative and incremental compute but does not offer the data window on the graph data.
Other stream analytics systems [17,32,58,72] support stream processing and snapshot creation, some offering data window but all at bulk updates only.
Stream databases [5,6] provide only stream processing.
TIDE [77] introduces probabilistic edge decay that samples data from base store.
Prior works [24,69] follow integrated graph system model that manage online updates and queries in the database, and replicate data in an offline analytics engines for long running graph analytics tasks.
As we have identified in §1, they suffers from excessive data duplication and weakest component problem.
Zhang et al [78] also argue that such composite design is not optimal.
GraPU [71] proposes to preprocesses the buffered updates instead of making them available to compute as in GRAPHONE.
Trading-off granularity of data visibility is similar to Lazybase [19], but we additionally tune the access of non-archived edges to reduce performance drop in our setup and offer diverse data views.The in-memory adjacency list in Neo4j [59] is optimized for read-only workloads, and new updates generally require invalidating and rebuilding those structures [63].
Titan [2], an open source graph analytics framework, is built on top of other storage engines such as HBase and BerkeleyDB, and thus does not offer adjacency list at the storage layer.
We have presented GRAPHONE, a unified graph data store abstraction that offers diverse data access at different granularity for various real-time analytics and queries at highperformance, while simultaneously supporting high arrival velocity of fine-grained updates.
The authors thank the USENIX FAST'19 reviewers and our shepherd Ashvin Goel for their suggestions.
This work was supported in part by National Science Foundation CAREER award 1350766 and grants 1618706 and 1717774.
