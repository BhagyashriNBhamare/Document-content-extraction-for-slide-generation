In Infrastructure as a Service (IaaS), virtual machines (VMs) use virtual hard disks (VHDs) provided by a remote storage service via the network.
Due to separation of VMs and their VHDs, a new type of failure, called VHD failure, which may be caused by various components in the IaaS stack, becomes the dominating factor that reduces VM availability.
The current state-of-the-art approaches fall short in localizing VHD failures because they only look at individual components.
In this paper, we designed and implemented a system called Deepview for VHD failure localization.
Deepview composes a global picture of the system by connecting all the components together, using individual VHD failure events.
It then uses a novel algorithm which integrates Lasso regression and hypothesis testing for accurate and timely failure localization.
We have deployed Deepview at Microsoft Azure, one of the largest IaaS providers.
Deepview reduced the number of unclassified VHD failure events from tens of thousands to several hundreds.
It unveiled new patterns including unplanned top-of-rack switch (ToR) reboots and storage gray failures.
Deepview reduced the time-to-detection for incidents to under 10 minutes.
Deepview further helped us quantify the implications of some key architectural decisions for the first time, including ToR switches as a single-point-of-failure and the compute-storage separation.
Infrastructure-as-a-Service (IaaS) is one of the largest cloud services today.
Customers rent virtual machines (VMs) hosted in large-scale datacenters, instead of managing their own physical servers.
VMs are hosted in compute clusters, and mount OS and data VHDs (virtual hard disks) from remote storage clusters via datacenter networks.
Resources can be scaled up and down elastically since compute and storage are separated by design.Achieving high availability is arguably the most important goal for IaaS.
Recently, large-scale system design [18,33,27], failure detection and mitigation techniques [23,43,25,7,6,29,36], and better engineering practices [10] have been applied to improve cloud system availability.
Yet, attaining the gold standard of five-nines (99.999%) VM availability remains a challenge [32,12].
At Microsoft Azure, there are on the order of thousands of VM down events daily.
The biggest category of down events (52%) is what we call VHD failures.
Due to compute-storage separation, when a VM cannot access its remote VHDs, the hypervisor has to crash the VM, resulting in a VHD failure.
Those VHD failures are caused by various failures in the IaaS stack and constitute the biggest obstacle towards attaining five-nines availability for our IaaS 1 .
Compute-storage separation brings unique challenges to locating VHD failures.
First, it is hard to find the failing component in a timely fashion, among a large number of interconnected components across compute, storage, and network.
The current practice of monitoring individual components is not sufficient.
The complex dependencies and interactions among components in our IaaS mean that a single root cause can have multiple symptoms at different places.
A network or storage failure may ripple through many other components and affect many VMs and applications.
It becomes hard to distinguish causes from effects, resulting in a lengthy troubleshooting process as the incidents get ping-ponged among different teams.Second, many component failures in the IaaS stack are gray in nature and hard to detect [27].
For failures such as intermittent packet drops and storage performance degradation, some VHD requests that pass through the component can fail but not others.
The failure signals in these cases are weak and sporadic in time and space, making fast and accurate detection difficult.To address these challenges, we have designed and deployed a system called Deepview.
Deepview takes a global view: it gathers VHD failure events as well as the VHD paths between the VMs and their storage as inputs, and constructs a model that connects the compute, storage, and network components together.
We further introduce an algorithm which integrates Lasso regression [39] and hypothesis testing [14] for principled and accurate failure localization.We implement the Deepview system for near-real-time VHD failure localization on top of a high-performance log analytics engine.
To meet the near-real-time requirement, we add streaming support to the engine.
Our implementation can run the Deepview failure localization algorithm in seconds, at the scale of thousands of compute and storage clusters, tens of thousands of network switches, and millions of servers and VMs.Now in deployment at Azure, Deepview helped us identify many new VHD failure root causes which were previously unknown such as gray storage cluster failure and unplanned Top of Rack switch (ToR) reboot.
With Deepview, unclassified VHD failure events dropped from several thousands per day to less than 500, and the Time to Detection (TTD) for incidents was reduced from tens of minutes and sometimes hours to under 10 minutes.Contributions.
We identified VHD failures as the biggest obstacle to five-nines VM availability for our IaaS cloud, and proposed a system to quickly detect and localize them.
In particular, we • Introduce a global-view-based algorithm that accurately localizes VHD failures, even for gray failures.
• Build and deploy a near-real-time system that localizes VHD failures in a timely manner.
• Quantify the implications of key IaaS architectural design decisions, including ToR as a single-point-offailure and compute-storage separation (Section 7).
In this section, we first provide background on Azure's IaaS architecture.
We explain how compute-storage separation can result in a new type of failure-VHD failures.
Then, we introduce the state-of-the-art industry practice in localizing VHD failures and explain how it is slow and inaccurate.
Finally, we motivate the approach Deepview takes and explain the challenges for putting the system into production uses.
A region has tens to hundreds of compute/storage clusters.
Each Tier2 (T2) switch connects some subset of clusters, while Tier3 (T3) switches connect the T2 switches.
T3 switches are connected by inter-region network (not drawn).
backed by the elastic block store [1].
Every VM has one OS VHD and one or more data VHDs attached.One key design decision is to separate compute and storage physically-VMs and their VHDs are provisioned from different physical clusters.The main benefit of this separation is to keep customer data available when their VMs become unavailable, e.g., due to a localized power failure.
As a result, VM migration becomes easy as we only need to create a new VM (possibly on a different host or cluster) and attach the same VHDs.In our datacenters, VHDs are provisioned and served from a highly available, distributed storage service [13,21].
Azure's storage service is deployed in selfcontained units of clusters with their own Clos-like network [5,24,13], software load balancers, frontend machines and disk/SSD-equipped servers.
Similarly, VMs are hosted on physical servers grouped in what we call compute clusters.
Each metro region typically has tens to hundreds of compute clusters and storage clusters, interconnected by a datacenter network.Another benefit is load-balancing.
A VM in a compute cluster can remotely mount VHDs from many different storage clusters.
A compute cluster therefore uses VHDs from multiple storage clusters, and a storage cluster can serve many VMs from different compute clusters.
As we will see later in section 3, this many-to-many relationship is leveraged by Deepview.
VHD Access is Remote.
Compute-storage separation requires all VMs to access their VHDs over the network.
When a VM accesses its disks, it is unaware that they are remotely mounted.
The VHD driver in the host hypervisor provides the needed disk virtualization layer.
The driver intercepts VM disk accesses, and turns them into VHD remote procedure call (RPC) requests to the remote storage service.
The VHD requests and responses traverse over multiple system components (e.g., the VHD driver and the remote storage stack) and through multiple network hops (e.g., ToR/T1/T2/T3 switches).
Figure 2: Daily VHD failures normalized the by 3-month average.
Every day had at least one failure.
On the worst day there were 3.5x more failures than average.
Compute-storage separation causes a new type of failure.
In our datacenter, whenever VHD accesses are too slow to respond (the default timeout is 2 minutes), the hypervisor crashes the guest OS.
In order to protect data integrity, when VHDs do not respond, the guest OS must be paused.
But the pause cannot be indefinite-an unresponsive VM can cause customers to fail their own application level SLAs.
After some wait, one reasonable option is to surface the underlying VHD access failure to the customer by crashing the guest OS.
We call this VHD failure caused by Compute-Storage Separation or VHD failure for short.
VHD failure is the biggest cause of unplanned VM downtime.
We analyzed an entire year's of IaaS VM down events, including their durations and causes (an internal team finds root causes for VM down events).
Table 1 shows that 52% of VM downtime is due to VHD Failures, 41% due to Software Failures (data-plane software and host OS), and 6% due to Hardware Failures, and 1% due to unknown causes.
Figure 2 further shows the daily number of VHD failures normalized by the 3-month average across tens of regions.
VHD failures happen daily.
Occassionally, they are particularly numerous.
The worst day over the 3-month period saw a 3.5x spike in volume.To minimize the impact of VHD failures and improve VM availability, the most direct approach is to quickly localize and mitigate these failures.
Next, we explain the prior VHD failure handling approach and its drawbacks.
Our datacenter operators prioritize by the impact of each incident.
A large rise in VHD failure events would automatically trigger incident tickets and set off an investigation.The site reliability engineers (SREs) look at system components individually and locally, to see if any local component anomaly coincides in time with the VHD failure incident.
The Compute team might look for missed heartbeats to see if the impacted physical machines have failed.
The Storage team might look at performance counters to see if the storage system is experiencing an overload.
The Network team might look at network latency and link packet discard rates to determine if some network devices/links could be at fault.
Once the failure location is confirmed, the responsible team often has standard procedures for quick mitigation.Prior to Deepview, failure localization was slow.
It was common that we needed tens of minutes, sometimes more than one hour, to localize and mitigate big incidents, and hours to tens of hours to detect and localize gray failures.
When a big incident happened, often more than one component had an anomaly because a single root cause could cascade to other services.
For example, one big network incident caused as many as 363 related incidents from different services!
As a result, the incident ticket could get ping-ponged among the teams.Further, localization for gray failures [27] was often inaccurate and slow.
For example, while we know ToR uplink packet discards can cause VHD requests to fail, it was unclear how severe the discard rate has to be.
Setting a threshold to catch those failures became an art: too low generated too many false positives, while too high delayed diagnosis or missed the issue.
Our key insight is that rather than looking at the components individually and locally, we should take a global view.
The intuition can be illustrated by the bipartite model in Figure 3a.
In this model, we put compute clusters on the left side and storage clusters on the right.
We draw an edge from a compute cluster to a storage cluster if it has VMs that mount VHDs from the storage cluster.
We also assign an edge weight equal to the fraction of VMs that have experienced VHD failures.For a compute cluster issue such as an unplanned ToR reboot that causes all VMs under the ToR to crash regardless of what storage clusters they use, we see the edges (highlighted in red) from the impacted compute cluster with high VHD failure rates, as in Figure 3a.
When a storage cluster fails, causing all VMs using that storage cluster to experience VHD failures, we see edges with high VHD failure rates coming to the impacted storage cluster.If we put the compute clusters along the y-axis and the storage clusters along the x-axis, we get a matrix view as shown in Figure 3b.
In this matrix view, a horizontal pattern points the incident to the computer cluster, while a vertical pattern points to the storage cluster.Challenges.
Though the bipartite model looks intuitive and promising, there are several challenges to use that insight in a production setting.
First, since the bipartite model cannot be easily extended to model the multitier network layers, we cannot use it to diagnose failures in the network.
Second, while we can use some voting/scoring heuristics to automate the visual pattern recognition, they work well only when the failures are fail-stop.
For gray failures [27], fewer VMs would crash so the VHD failure signals are often weaker, and the VHD failure patterns are less clear cut.
Third, when big incidents happen, many customers may feel the impact, making timely failure localization imperative.
Our system must therefore operate in near-real-time.
Problem Statement.
Our goal is to localize VHD failures for both fail-stop and gray failures to component failures in compute, storage or network, at the finest granularity possible (clusters, ToRs and network tiers), all within a TTD target of 15 minutes, in line with our availability objectives.
In this section, we explain how the Deepview algorithm solves the first two challenges-handling network and gray failures.
We first describe our new model, a generalization of the bipartite model to include network devices.
Then, we introduce our inference algorithm with two main techniques: 1) Lasso regression [39] to select a small subset of components as candidates to blame; 2) hypothesis testing [14] as a principled and interpretable way to handle strong and weak signals and decide on the components to flag to operators.
There are other failure localization algorithms that can be adapted for our problem.
We compare Deepview with them in Section 6.2, and show that our approach has better recall and precision.
In Section 3, we introduced a bipartite model that takes a global view of compute and storage clusters.
Here we generalize the model to include network devices.
In this new model, we have three types of components: compute clusters, network devices and storage clusters.
Figure 1 shows that compute clusters and storage clusters are interconnected by a number of Tier-2 (T2 for short) and Tier-3 (T3) switches in a Clos topology.
ToR and Tier-1 (T1) switches are within the clusters, and are part of the clusters.
To model the network, we replace each edge in the bipartite model with a path through the network that connects a compute cluster to a storage cluster.Here we describe the model at the level of clusters (which we call Cluster View in Section 6).
We have also extended the model to the granularity of ToRs inside compute clusters (ToR View).
For this work, we keep the storage cluster as a blackbox due to its complexity.
As future work, we plan to apply our approach to the host level and the storage clusters internals.
One complication in modeling the network is that each compute/storage cluster pair is connected by many paths.
Due to Equal-Cost Multi-path (ECMP) routing [24,38], we do not know precisely which path a VHD request takes, and therefore, we do not know which path to blame when the request fails.Our solution is to transform the Clos topology (Fig- ure 1) to a tree topology (Figure 4) so that there is a unique shortest path between each cluster pair.
We start from the bottom and go up for each cluster and aggregate the network devices by tiers, and then use shortest path routing to find the lowest overlap between each cluster pair.The detailed procedure is as follows.
First, we start with ToR switches in a cluster and find the T1 switches they connect to.
Then, we group those T1 switches as an aggregated T1 group for that cluster.
Similarly, we can find the connected T2 switches for those T1 switches and group them as an aggregated T2 group for that cluster.
We repeat this procedure to find the aggregated T3 groups.
At the end of the aggregation, we have determined the aggregated T1, T2, T3 groups for each cluster in a region.
The next step is to find the shortest path for each compute-storage pair .
If the aggregated T2 groups of a cluster pair overlap, the midpoint is that overlapped aggregated T2 group; if their aggregated T2 groups do not overlap but their aggregated T3 groups do, the midpoint is the T3 group.Due to the simplification, we cannot pinpoint to a specific network device, but only to within a network tier.
In practice, Deepview is mainly used to decide which SRE teams to notify when VMs crash.
Upon notification, network teams have other tools (e.g. Traceroute) to further narrow down to a device for mitigation.
Next, we use our observations of VHD failure occurrences to pinpoint which component has failed.
We assume that components fail independently, which is a practical and reasonable approximation of the real world.
For example, a compute cluster failure is unlikely to be correlated with a storage cluster failure.
We can write down a simple probabilistic equation for a path consisting of compute, storage and network components:P(path i is fine) = ∏ j∈path(i) P (component j is fine) (1)We approximate 1 − P(path i is fine) using the rate of VHD failures observed for that path:n i − e i n i ≈ ∏ j∈path(i) p j(2)where n i is the total number of VMs, e i is the number of VMs that have VHD failures for a given time window, and p j is the probability that component j is fine.
We get a system of equations by writing down (2) for every path.
Next, we infer the values of p j for all components.
We know there is noise in our measurement, so we cannot directly solve the system of equations and would need to explicitly model the noise.
Specifically, after taking log on both sides of equation (2) and adding a noise term ε i , we get a set of linear models:y i = N ∑ j=1 β j x i j + ε i , ε i i.i.d. ∼ N(0, σ 2 )(3)where y i = log n i −e i n i , β j = log p j , and the binary variable x i j = 1 iff i-th path goes through the j-th component.
Interpretation of β j : Once we get estimates for β j , the probability that component j is fine can be computed from β j because p j := exp(β j ).
If β j is close to 0, we can clear component j from blame.
Otherwise, if β j is unusually negative, we have strong evidence to blame component j (see Section 4.3).
We would ensure β ≤ 0.
Next, we answer the following two questions: (1) how to get fast, accurate, and interpretable estimates for β j ; (2) given the estimates, how to decide which component to blame in a principled and interpretable manner?
In practice, the number of unknown variables (β 's) can be larger than the number of equations.
We illustrate this in a simple example shown in Figure 5.
We can list 4 equations with 5 free variables (the β s):y 1 = β c1 + β net + β s1 + ε 1 y 2 = β c1 + β net + β s2 + ε 2 y 3 = β c2 + β net + β s1 + ε 3 y 4 = β c2 + β net + β s2 + ε 4 .
(4)Suppose all four paths saw equal probability of VHD failures.
The blame can be pushed to the compute clusters C1 and C2, or the storage clusters S1 and S2, or the network, or a mix of those.
Traditional least-square regression cannot give a solution in this case.
But our experience tells us that multiple simultaneous failures are rare for a short window of time (e.g., 1 hour) because individual incidents are rare and failures are (mostly) independent.
How do we encode this domain knowledge into our model to help us identify the most likely solution?To prefer a small number of failures is mathematically equivalent to prefer the estimates β = (β 1 , · · · , β N ) to be sparse (mostly zeros).
We express this preference by imposing a constraint on model parameters β .
By asking the sum of absolute values of β , i.e., β 1 to be small, we can force most of the components of β to zero, leaving only a small number of components of β remaining.
This technique of adding a L1-norm constraint is known as Lasso [39], a computationally efficient technique widely used when sparse solutions are needed.
We also ensure β ≤ 0 to get valid probabilities.
The estimate procedure that encodes all our beliefs in our model is thus the following convex program,ˆ β = arg min β ∈R N ,β ≤0 y − Xβ 2 2 + λ β 1 .
(5)Simplicity vs. Goodness-of-fit via λ : This loss function tries to strike a balance between goodness-of-fit in the first term (i.e., how well the model explains the observation) and sparsity in the second term (i.e., fewer failing components are more likely).
The regularization parameter λ is the knob.
Larger λ prefers fewer components to be blamed at potentially worse goodness-of-fit.
The optimal value of λ is set by an automatic (data-adaptive) cross-validation procedure [26].
While big incidents are relatively easy to localize with a fixed threshold, it is much harder to find a threshold that can discriminate gray failures from normal components when there are random measurement errors.
The estimated failure probabilities for gray failures can be very close to zero (see Section 6.1).
The challenge then becomes how big an estimated failure probability is for a gray failure versus just measurement error.
Setting such a threshold manually requires laborious data-fitting and is often based on some vague notions of anomaly.
In practice, it can be difficult and fragile.
Can we use data to find the decision threshold in a principled and automatic way?
Intuitively, the larger the magnitude a (negative) Lasso estimate has, the higher its estimated failure probability, and correspondingly more likely the component has failed.
We had a painful experience manually tuning the threshold, but the process gave us some experience in distinguishing true failures (big incidents and gray failures) from measurement noise.
We found that if a component's Lasso estimate is much worse than the average, then it is likely a real failure and should be flagged.
The further from average, the more confident we are that the component has failed.This decision can be automated in a hypothesis testing framework.
Consider the following one-sided test:H 0 ( j) : β j = ¯ β v.s. H A ( j) : β j < ¯ β(6)The null hypothesis H 0 ( j) says the true probability that component j is fine is no different from the grand average of all components.
We then use the data to tell us if we can reject H 0 ( j) or not.
If the data allow us to reject H 0 ( j) in favor of the alternative hypothesis H A ( j), then we can blame component j. Otherwise, we do not blame component j.
The hypothesis test has three steps.Step 1: Compute Test Statistic.
Given Lasso estimates for components in a region, we find the mean ¯ ˆ β and standard deviation σ ˆ β .
Then we compute a modified Z-score for each component j,z j = ˆ β j − ¯ ˆ β σ ˆ β / √ N .
(7)Under the assumptions that the measurement error is Gaussian, and other caveats, 2 we approximate the distribution of z j as a Gaussian distribution with mean zero (under H 0 ( j)) and certain variance.Step 2: Compute p-value.
We then compute the pvalue [14] for each component j.
The p-value is the probability of seeing a failure probability for component j as extreme as currently observed simply by chance assuming that it is no different from the average.
If the p-value is really small, then we do not believe the failure probability for component j is just about average.
See the Appendix for more discussion on p-value.
Step We have two main system requirements:• Near-real-time (NRT) processing: VHD failures result in customer VM downtime, so failure localization must be speedy and accurate.
We have the requirement that the time-to-detection (TTD) be within 15 minutes.
• Speedy iteration: VHD failures are the biggest obstacle to higher VM availability, so there is an immediate need by the operations team for better diagnosis.
Our system is designed for quick iteration.Our system requires two types of input data: non-realtime structural data and real-time event data.
The former include the compute and storage clusters information, all the VMs and their VHD storage account information and related context, the paths for all the compute-storage pairs, and the network topology.
Taking periodic snapshots of those every few hours suffices for our purposes.
Figure 6: Deepview system architecture.
Data schema is given in Table 2.
The latter are the VHD failure signals from servers.
To meet near-real-time requirements, our algorithm needs to see VHD failure signals within minutes, ideally through a streaming system.
We need to scale to thousands of compute and storage clusters, tens of thousands of VHD failures per day, tens of thousands of network switches, hundreds of thousands paths, and millions of VMs.The non-real-time information is either already in our in-house log analytics engine called Azure Kusto [3,2], or can be generated and ingested into Kusto.
Kusto stores data as tables but the tables are append-only, and it supports a SQL-like declarative query language.
Kusto is backed by reliable persistent storage from a distributed storage service, using memory and SSD for a read-only cache.
By default, it builds indices for all columns to improve query speed.
VHD failure events are generated by hypervisors.
They are collected by a real-time pipeline.
Since most of our data is already in Kusto, and Kusto provides highly expressive declarative language and fast data analysis, we ingest the VHD failure events into Kusto and build Deepview system on top of it.System Architecture: The resulting system architecture is shown in Figure 6.
It has four components.
The realtime path and non-real-time path are for the input data ingestion for the Deepview algorithm.
The Kusto platform provides both data analysis and storage for input, intermediate, and output data.
The visualization and alert are tools for the consumption of Deepview results.
The NRT scheduler is what we build on top of Kusto to support stream processing for the Deepview algorithm.
We build our own stream processing system on top of Kusto because most of our data are already there; a few • A computation directed-acyclic-graph (DAG) declared as a set of SQL-like queries with their output tables.
• A scheduler that runs each query at a given frequency.We store the DAG and its scheduling policy as tables, since tables are Kusto's only supported data structure.Computation DAG.
The computation DAG consists of a set of queries that read from input tables and produce one or multiple output tables.
The queries are the "edges" and the input/output tables are the "nodes".
To maintain the DAG in Kusto, we give each query a name and store the query definition and the query output table name in yet another table.
NRT Scheduler.
To provide a streaming window abstraction, we use a schedule to describe when each query in the DAG should be executed.
The schedule describes how often it should run and how many times to retry.
To meet availability requirements, we use a one-hour sliding window that moves forward every 5 minutes.
The algorithm implementation has three parts: first, construct the model-instantiate the design matrix x i j and observation y i based on the Deepview raw data tables, then run Lasso regression to infer β , and finally carry out hypothesis testing to pinpoint the failures.
and memory footprint.
Constructing a full design matrix requires filling in entries for every path and every component with either zero or one.
This can be slow and has high memory usage.
However, x i j are mostly zeros since each path has at most tens of components, so we only need to store the non-zero entries.
Another simple technique is to only get data from Kusto for regions with non-zero VHD failure occurrences.
Since simultaneous failures are rare, region filtering can avoid running the algorithm for some regions without hurting accuracy.Coordinate Descent.
Lasso regression has no closed form solution.
Coordinate descent [22] is one of the fastest algorithms to solve the Lasso regression.
We minimize the loss function as in Equation 5 with respect to each coordinate β j while holding all others constant.
We cycle among the coordinates until all coefficients stabilize.
In practice, with warm start, we found that coordinate descent almost always converges in only a few rounds.Cross-Validation with Warm Start to Set λ .
We set the regularization parameter λ for Lasso using a dataadaptive method, i.e., cross-validation [26].
We use 5-fold cross validation where we split the data by paths into 5 partitions, and use any four of them to fit β for a given choice of λ and then compute the mean squared error (MSE) on the holdout partition using the fitted β .
The optimal λ is the one that minimizes the average MSE.
We speed up cross validation using a warm start technique [22].
Recall that a larger λ meant fewer non-zero β j .
We start with the smallest λ that turns off all β j , and then we gradually decrease λ .
Since β tends to change only slightly for a small change in λ , we reduce the number of rounds for coordinate descent by reusing β (λ k−1 ) as the initial values for β (λ k ).
We have deployed Deepview in production at Azure.
Here, we first evaluate how well Deepview localizes VHD failures using production case studies.
In this subsection, we ask how effective Deepview is at detecting and diagnosing incidents in production use.
We examined the Deepview results for one month.
The number of VHD failures generated per day can be up to tens of thousands.
For this month, Deepview detected 100 patterns, and reduced the number of unclassified VHD failure events to less than 500 per day.
We also tried to associate the detected patterns with incident tickets: 70 of the patterns were directly associated with incident tickets.
The other 30 patterns were not associated with tickets.
These 30 patterns turned out to be generated by weak VHD failure signals.
They were all real underlying component failures that escaped the previous alerting system, either because of their smaller impact (e.g., unplanned ToR reboot) or their gray failure nature (e.g., gray storage failure).
Next we examine some of the representative patterns we found and discuss the insight we learn from them.
From time to time, ToRs undergo scheduled downtime for firmware upgrade or other maintenance operations.
Impacted customers are notified in advance, with their VMs safely migrated to other places.
However, occasionally, a ToR may experience an unplanned reboot due to a hardware or software bug.
Since each server connects to only one ToR, the VMs under the ToR will not be able to access their VHDs.
We get VHD failures as a result.
To detect unplanned ToR reboots, Deepview first estimates the failure probability and p-value for the ToR, and then checks the following conditions for confirmation: all the VMs under the ToR get VHD failures, the ToR OS boot time matches the failure time detected by Deepview, and the neighboring ToRs are working fine.
Figure 7 shows one such unplanned ToR reboot detected by Deepview in a small region.
4 It shows a portion of the Deepview UI, which we call ToR view.
compute/storage clusters).
Deepview estimated the failure probability for the failed ToR to be 100% with a pvalue of 1.84E−64, which is much less than 0.01.
Deepview therefore makes it possible to study how often ToRs cause downtime.
We discuss this in detail in Section 7.1.
Our storage cluster runs a full storage stack including load balancer/frontend, meta-data management, storage layer, etc.
VHD failures can happen due to a variety of failure modes in the storage stack.
When storage cluster failures are non-fail-stop, the VHD signals can be weak and noisy.
For example, the load balancer could discard VHD requests to shed load, and in other cases, software bugs could cause some VHDs to become unavailable, impacting only a subset of VMs.We next discuss such a storage gray failure case.
A new storage cluster was brought online, but with a misconfiguration that allowed a test feature in the caching subsystem to be enabled.
This bug mistakenly put some VHDs in negative cache (denoting deletion), rendering them "invisible" and unavailable for VM access.Based on the VHD failure events at hour 0 in Figure 8, Deepview found three non-zero failure probability entities in the region, 0.34 for storage cluster S0, 0.002 and 0.047 for compute clusters C0 and C1.
Notice that because this storage cluster failure only affected a small number of VMs, we did not get a failure probability of 1 for S0.
Further, the two compute clusters saw nonzero failure probabilities because they also saw VHD failure events.
However, despite the weak signal, our algorithm was able to correctly pinpoint the failure to S0.
Our hypothesis testing procedure computed a p-value of 3.9E−34 for S0, identifying it as a failed cluster with very high confidence.
On the other hand, C0 and C1 had p-values 0.51 and 0.54, respectively, and signifying a lack of evidence.
Using our prior threshold method for detection, we would have delayed the detection by 22 hours.
As shown in Figure 8, the signal is weak: the number of VMs affected per hour in the beginning was only around 10, and the peak number was only 28.
In our datacenter, switches other than ToRs have replicas.
Single switch failures thus seldom lead to wide impact outages.
However, in rare cases, a combination of capacity loss and traffic surge can cause network failures.In one region, we have over 100 compute clusters and 50 storage clusters.
They are connected by four T2 aggregated switches (numbered T2 0 to T2 3) with a T3 aggregated switch (T3 0) on top, as annotated along the axes in Figure 9.
Each aggregated switch contains multiple switches.
One day, a T3 0 switch underwent a major maintenance event, which triggered some T2 switches in T2 0 to mistakenly detect Frame Check Sequence (FCS) errors on the links to T3 0.
Our automatic network service then kicked in and shut down most of links between the T3 0 switch and T2 0 except for three links saved by a built-in safety mechanism.This loss in capacity together with a surge in storage replication traffic caused significant congestion between T2 0 and T3 0.
As a consequence, we saw a significant increase in VHD failures experienced by customer VMs.
Figure 9 shows the pattern in the Deepview UI (partial Cluster View) with compute clusters on the y-axis and storage clusters on the x-axis.
The switch aggregated per cluster is annotated on each axis.
Yellow cells have a VHD failure rate at most 5%.
The VHD failure rates are moderate because the VHD failures in this case were caused by network congestion; most of the time network connectivity was still working.Deepview identified three aggregated switches with non-zero failure probabilities: 0.21%, 0.11%, 0.03% for T3 0, T2 0, and T2 2, respectively.
Their corresponding p-values are 9.91E−12, 4.25E−04, 0.221.
We point to T3 0 and T2 0 as the faulty network layers.
The failure location is correct, as the root cause is the link conges-tion between these two network layers.
We note Deepview gave small failure probabilities because the VHD failure signals are weak: only a very small percentage of affected VMs crashed.
But since T3 0 and T2 0 are high in the network hierarchy, they impact a large number of VMs.We also experienced network incidents where network connectivity for many VMs were lost.
They were easy for Deepview to detect and localize, as the signals were strong: many VMs died at the same time.
We present this gray failure case to show the strength of Deepview.To summarize, we have shown that Deepview can localize various incidents in which the signals can be weak or strong.
Deepview has also deepened our understanding of VHD failures by identifying various patterns including horizontal patterns caused by incidents including unplanned ToR reboot, vertical patterns caused by storage outages, and network failure patterns.
Several algorithms that have been previously used to localize failures in the network can be extended to localize VHD failures.
We compare with two tomography algorithms and a Bayesian network algorithm:• Boolean-Tomo [20,19]: Classify paths into good and bad paths based on a threshold (bad if at least γ VHD failures).
Iteratively find the component on the largest number of unexplained bad paths, as the top suspect until all bad paths are explained.
For the threshold γ, we tried γ = 1, 2, 3, 4, 5, and picked γ = 1 to maximize its recall and then precision.
• SCORE [31]: Classify paths into good and bad paths based on a threshold (γ).
Iteratively compute for each component its hit ratio numBadPaths(c) numPaths(c) and coverage ratio numUnexplainedBadPaths(c) totalNumUnexplainedBadPaths .
Only consider components above a hit ratio threshold (η).
Take the component with the highest coverage ratio as the top suspect.
For the threshold γ and η, we tried γ = 1, 2, 3, 4, 5 and η = 0.001, 0.01, 0.1, and picked γ = 1 and η = 0.01 to maximize its recall and then precision.
• Approximate Bayesian Network [35]: The runtime to compute exact Bayesian network is exponential in the number of components, and thus is infeasible for us.
We tried an approximation [35].
It uses meanfield variational inference to approximate the Bayesian network with a Noisy-OR model, and estimates the component j's failure rate as the posterior mean of a Beta distribution B(α j , β j ).
A component is blamed ifˆαifˆ ifˆα j ˆ α j + ˆ β jis above certain threshold.
We do not include its accuracy numbers, because we are unable to make it give meaningful results on our data.
The estimated posterior means of component failure rate allows us to apply a threshold.
The computation takes 10 minutes for a single region, so this approach is not fast enough for our problem.Dataset.
As we cannot run the other algorithms in production, we use trace data to compare algorithms.
We had already hand-curated 42 incidents from a detailed study of tickets, so we use trace data from those incidents.
They consist of 16 compute cluster issues (not ToR-related), 14 storage cluster issues, 10 unplanned ToR reboots, and 2 network issues.
Only time periods when there is an incident are considered because a random sample is too sparse.
Thus, we may overestimate the precision.
But our comparison is fair since all algorithms use the same baseline ground truth.Metrics.
We compare each algorithm on recall and precision.
Recall is the percentage of true failures that have been localized and precision is the percentage of localizations that are correct.
In other words, high recall means we can localize most real failures, while high precision means we have few false positives.
Figure 10 summarizes the precision and recall for the 42 incidents.
SCORE achieves a recall of 0.88, beating Boolean-Tomo, but it gives many false positives.
Deepview, achieves both a high precision of 0.90 and a high recall of 1.0, beating both alternatives.
Table 3 shows a breakdown of the precision and recall by failure types for Deepview.
Overall, Deepview handles cases with a strong failure signal (Compute/ToR) and those with a weak failure signal (Storage/Network) well.
Deepview also does well for unplanned ToR reboots and Network incidents.
However, there were fewer of these incidents, so the estimates are to be taken with a grain of salt.The other advantage of Deepview is that its parameters needs no manual tuning.
Parameters are set by cross-validation (for λ ) or using a standard interpretable criterion (false positive tolerance of 1% for p-value).
Boolean-Tomo and SCORE, instead, need careful tuning of their thresholds.
In fact, we find that their precision and recall are sensitive to the thresholds.
We picked those that maximize recall (as recall is typically more important than precision in production), while keeping precision as high as possible.
We note that Deepview beats the performance of Boolean-Tomo and SCORE for all combinations of thresholds (omitted for lack of space).
We have introduced a set of techniques for our algorithm.
Here, we analyze how useful each technique is.Cross-validation and λ in Lasso Regression.
The regularization parameter λ is set by cross-validation for each region.
The optimal values found for incidents in Section 6.2 span three orders of magnitude with a minimum of 0.00012 and a maximum of 0.48.
In fact, it is well known in statistical literature that choosing a universally optimal λ for all problems is impossible.
The theoretical optimal [11] depends on the number of paths, the number of components, the structure of the network, and the error variance (i.e., how stable are VHD failures among different paths).
When cross-validation is fast, it is preferred to a manual threshold.Hypothesis Testing and Gray Failures.
We use hypothesis testing to find a decision threshold to localize both big incidents and gray failures in the presence of random noise.
The gray failure case studies in Section 6.1 show that hypothesis testing is essential.
For the storage case, the failure probabilities are 0.34 for the truly failed storage cluster S0, and 0.002 and 0.047 for two normal compute clusters.
Their p-values 3.9E−34 and 0.51 and 0.54 are needed to accentuate the difference and allow us to pick only S0.
Similarly, for the network case, looking at p-values allow us to filter out T2 2.
Algorithm Running Time.
We measure the running time for Deepview algorithm in production.
The worstcase running time is 18.3 seconds on a single server.
It includes the time to read input data from Kusto, execute the algorithm and write the output data to Kusto.Time to Detection (TTD) TTD is defined as the time between when an incident happens and when the failure is localized.
The average time from a VHD failure event to its appearance in Kusto is 3.5 minutes.
Adding the 5 minutes windowing time and the processing time, Kusto achieves a TTD under 10 minutes.
This is a significant improvement over the previous TTD which typically lasted from tens of minutes to hours.
Several architectural decisions were made when our IaaS was built.
One is that a server connects to only a single ToR via a single NIC.
While this makes ToR a singlepoint-of-failure (SPOF), the decision dramatically reduces networking cost.
Another decision is that a VM can host its VHDs in any storage cluster in the same region.
This makes load-balancing for storage clusters easy, but with potentially higher network latency and lower throughput.
Further, both decisions may adversely impact VM availability.
Using the data collected from Deepview, we can now study the impact of these decisions quantitatively.
As we have described in Section 6.1, Deepview can detect unplanned ToR reboots.
From the failure patterns, we find that there are two types of ToR failures: soft failures and hard failures.
Soft failures can be recovered by rebooting the ToR, while hard failures cannot.Our data shows that: (1) less than 0.1% switches experience unplanned reboots in a month; (2) 90% of the failures are soft failures, with the rest hard failures.
The hard failure rate agrees with our ToR Return Merchandise Authorization (RMA) rate, which indicates that 0.1% switches need to be RMAed in one year.
These numbers are obtained from a fleet of tens of thousands of ToRs.The impact of a soft failure typically lasts for less than 20 minutes: 10 minutes for the ToRs to come up and 10 minutes for the VMs to recover.
The impact of a hard failure lasts longer as the failed switch needs to be physically replaced.
The impact to VMs can be shorter though as the VMs can be migrated to other hosts due to the separation of compute and storage.
We conservatively use 2 hours as the impact period for hard failures.If the ToR is the only failure source for VMs on that rack, the availability of our IaaS is no better than1 − 0.9 × 20 + 0.1 × 120 1000 × 30 × 24 × 60 = 99.99993%Even with ToR as the single point of failure, the service can achieve six-nines.
This meets the rule of thumb that critical dependencies need to offer one additional 9 relative to the target service [40].
Thanks to Deepview data, for the first time, we are able to show that ToR as a single point of failure is an acceptable design choice for IaaS as it is not on the critical path for five-nines availability.Note that simply examining ToR logs would not have given us these numbers, as many ToR reboots are planned, with no impact on VM availability.
A VM can use VHDs from any storage cluster in the same region, due to the separation of compute and storage.
We look at the network distance between VMs and the storage clusters for their VHDs.
We find that some 51.8% of VHD paths go through T2, 41.0% need to go through T3 and the rest go above T3 in Azure.A longer network path may result in higher network latency and packet drop rate.
However, it is not clear whether it will also negatively affect VM availability.Here we use the Deepview data to answer this quantitatively.
We look at our data for three months.
For each day, we first compute the VHD failure rates r 0 and r 1 for VMs crossing T2 only and VMs crossing T3 and above, respectively.
Then, we find the percentage increase (r 1 − r 0 )/r 0 .
Figure 11 shows the daily percentage increase over a 3-month period.
VMs whose network paths cross T3 network layer or above see a higher VHD failure rate than those that only need to cross T2 on most days.
There is a 11.4% increase ((r 1 − r 0 )/r 0 ) in the VHD failure rate if the VHD access needs to cross T3 or above.One possible explanation is that as VHD requests go up the network tiers, they traverse more switches which may become oversubscribed.
Thus VHD requests may become more likely to fail when network path lengths get longer.
An implication of this study is that there is some benefit to colocating VMs and their VHDs in nearby clusters for availability.
Machine Learning.
Machine learning techniques have been used for failure localization, such as decision trees [6,17], Naive Bayes [45], SVM [42], correlations [43], clustering [16], and outlier detection [36].
They allow domain knowledge to be encoded as features, but in general require a rich set of signals to discriminate different failure cases and may rely on assumptions about traffic that are not generally applicable.
The most relevant work is NetPoirot [6], which targets a similar scenario as ours, but with a very different approach.
NetPoirot is a single node solution where end-hosts independently run pre-trained classification models on local TCP statistics to infer failure locations.
We believe NetPoirot and Deepview are complementary-TCP metrics from IaaS VMs may provide a useful signal to Deepview.Tomography.
There has been a large body work in network tomography (see [15] for a survey), and specifically binary tomography and its variants [20,19,31] for network failure localization.
Typically, greedy heuristics are used to select among multiple solutions that all explain the observations.
Various thresholds are often needed to tradeoff between precision and recall ratios.
Compared with those approaches, Deepview avoids manual threshold tuning and achieves both higher recall and precision as shown in section 6.2.
Bayesian Network.
Bayesian network [34] is a principled probabilistic approach to failure localization.
It can model complex system behaviors [7] and handle measurement errors [28].
While exact inference is intractable [30], there are various approximation techniques such as using noisy-or to simplify conditional probability calculation [35,7,37], considering k-subset root-causes to shortcut marginalization [28,7], using a simple factored form for joint posterior [35], or using message passing for faster inference [37].
For our problem, we find that using a combination of approximation techniques (we tried two [35]) was essential.
It is future work to compare Deepview with some practical Bayesian network approach.
We identified VHD failures caused by compute-storageseparation as the main factor that reduces VM availability at our IaaS cloud.
We introduced Deepview, a system that quickly localizes failures from a global view of different system components and a novel algorithm integrating Lasso regression and hypothesis testing.
Data from production allowed us to quantitatively evaluate precision and recall across many failure events.
We also used Deepview data to evaluate the impact of system architecture on VM availability.
We thank our Azure colleagues Brent Jensen, Girish Bablani, Dongming Bi, Rituparna Paul, Abhishek Mishra, Dong Xiang for their valuable discussions and support.
We thank our MSR colleagues Pu Zhang, Myeongjae Jeon and Lidong Zhou, and intern Jin Ze for their contributions to an early prototype of Deepview.
We thank our shepherd Mike Freedman and the anonymous reviewers for their feedback.
This work was partially supported by the NSF (CNS-1616774).
gTo decide if a component has failed, we could make a decision based on a threshold for the estimated failure probability for that component.
But we can make a more principled decision by conducting a hypothesis test for each component as specified in (6).
In this appendix, we explain the details in doing this testing.
We explain our p-value, and motivate and explain how we do multiple testing.
A.1 Interpretation of p-valuesTo conduct the test for each component, we construct the test statistics as in (7) for each of the N components.
We then compute the p-value for each test to decide whether to reject the null hypothesis.
The p-value is defined as, the probability, assuming the null hypothesis is true, of the sampling test statistic having a value at least as extreme as observed.
If the null hypothesis is true, we should expect a moderate p-value.
However, if the computed p-value is small, we have evidence to believe that the null hypothesis is false.
In fact, when the p-value is too small (e.g., below the conventional 1%, 5%, and 10% significance level), we should reject the null hypothesis H 0 ( j), since it is highly unlikely that it can explain the values we have observed.If the p-value is greater than the significance level, then the test is inconclusive.
However, we give extra attention to borderline cases to decrease the false negative rate.
For example, we produce warnings with lower priority for those components whose p-values are only slightly greater than the significance level.
A.2 Choice of Significance LevelHow to choose an appropriate significance level?
For testing a single hypothesis, conventional choices of significance level include 1%, 5%, and 10%.
However, when testing multiple hypotheses, we need to be more careful about false positives.
Suppose we are testing 100 null hypotheses, all of which are true.
If we use 5% as the significance level, then there is roughly 5% probability that we incorrectly reject the null hypothesis-committing a false positive.
Further, if these 100 tests are independent, then we are almost certain to make at least one false positive: P (at least one false positive) = 1 − P (no false positive) = 1 − 0.95 100 = 0.994.
Intuitively, the more hypotheses we test simultaneously, the more likely we are to make a mistake.To reduce the tendency of making mistakes when testing multiple hypotheses, we need to provide a stricter significance level than a single test.
This is called the multiple testing correction.
A.3 Multiple Testing CorrectionThere are two approaches to multiple testing correction: family-wise error rate (FWER) control correction or false discovery rate (FDR) control correction.
We use FDR control in Deepview algorithm since it is the more powerful alternative.Let V be the number of false positives (the healthy components that we falsely blame), and R be the number of rejected hypotheses (the total number of components we blame).
Then the false discovery rate (FDR) is defined asThe Benjamini-Hochberg procedure [9] is the most popular FDR control procedure due to its simplicity and effectiveness.
The procedure is as follows:1.
Do N individual tests and get their p-values P 1 , P 2 , . . . , P N corresponding to null hypothesis H 0 (1), H 0 (2), . . . , H 0 (N).2.
Sort these p-values in ascending order and denote them by P (1) , P (2) , · · · , P (N) .3.
For a given threshold on FDR α, find the largest K such that P (K) ≤ K N α.
4.
Reject all null hypotheses for which their p-values are smaller than or equal to P (K) .
To decide if a component has failed, we could make a decision based on a threshold for the estimated failure probability for that component.
But we can make a more principled decision by conducting a hypothesis test for each component as specified in (6).
In this appendix, we explain the details in doing this testing.
We explain our p-value, and motivate and explain how we do multiple testing.
To conduct the test for each component, we construct the test statistics as in (7) for each of the N components.
We then compute the p-value for each test to decide whether to reject the null hypothesis.
The p-value is defined as, the probability, assuming the null hypothesis is true, of the sampling test statistic having a value at least as extreme as observed.
If the null hypothesis is true, we should expect a moderate p-value.
However, if the computed p-value is small, we have evidence to believe that the null hypothesis is false.
In fact, when the p-value is too small (e.g., below the conventional 1%, 5%, and 10% significance level), we should reject the null hypothesis H 0 ( j), since it is highly unlikely that it can explain the values we have observed.If the p-value is greater than the significance level, then the test is inconclusive.
However, we give extra attention to borderline cases to decrease the false negative rate.
For example, we produce warnings with lower priority for those components whose p-values are only slightly greater than the significance level.
How to choose an appropriate significance level?
For testing a single hypothesis, conventional choices of significance level include 1%, 5%, and 10%.
However, when testing multiple hypotheses, we need to be more careful about false positives.
Suppose we are testing 100 null hypotheses, all of which are true.
If we use 5% as the significance level, then there is roughly 5% probability that we incorrectly reject the null hypothesis-committing a false positive.
Further, if these 100 tests are independent, then we are almost certain to make at least one false positive: P (at least one false positive) = 1 − P (no false positive) = 1 − 0.95 100 = 0.994.
Intuitively, the more hypotheses we test simultaneously, the more likely we are to make a mistake.To reduce the tendency of making mistakes when testing multiple hypotheses, we need to provide a stricter significance level than a single test.
This is called the multiple testing correction.
There are two approaches to multiple testing correction: family-wise error rate (FWER) control correction or false discovery rate (FDR) control correction.
We use FDR control in Deepview algorithm since it is the more powerful alternative.Let V be the number of false positives (the healthy components that we falsely blame), and R be the number of rejected hypotheses (the total number of components we blame).
Then the false discovery rate (FDR) is defined asThe Benjamini-Hochberg procedure [9] is the most popular FDR control procedure due to its simplicity and effectiveness.
The procedure is as follows:1.
Do N individual tests and get their p-values P 1 , P 2 , . . . , P N corresponding to null hypothesis H 0 (1), H 0 (2), . . . , H 0 (N).2.
Sort these p-values in ascending order and denote them by P (1) , P (2) , · · · , P (N) .3.
For a given threshold on FDR α, find the largest K such that P (K) ≤ K N α.
4.
Reject all null hypotheses for which their p-values are smaller than or equal to P (K) .
