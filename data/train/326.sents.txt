We consider the task of human collaborative category learning , where two people work together to classify test items into appropriate categories based on what they learn from a training set.
We propose a novel collaboration policy based on the Co-Training algorithm in machine learning, in which the two people play the role of the base learners.
The policy restricts each learner's view of the data and limits their communication to only the exchange of their labelings on test items.
In a series of empirical studies, we show that the Co-Training policy leads collaborators to jointly produce unique and potentially valuable classification outcomes that are not generated under other collaboration policies.
We further demonstrate that these observations can be explained with appropriate machine learning models.
We first review the classic Co-Training algorithm of (Blum and Mitchell 1998) as it is closely related to our policy.
Assume that each item is parametrized by a feature vector x and has a corresponding class label y.
The input consists of l labeled items (x 1 , y 1 ) . . . (x l , y l ) and u unlabeled items x l+1 . . . x l+u .
The goal is to learn a classifier f : x → y using both the labeled and unlabeled data.Further assume that the feature vector can be split into two parts (called "views"):x = x (1)x (2) .
The Co-Training algorithm trains two base learners f (1) : x (1) → y and f (2) : x (2) → y, each working exclusively on one view.
In the beginning, these two base learners are trained on the labeled data.
More specifically, f (1) is trained with the first view of the labeled data (x(1) 1 , y 1 ) . . . (x (1)l , y l ).
Subsequently, whenever f (1) encounters an item x during training or prediction, it always works with the first view x (1) of the item only and disregards the second view x (2) .
f (2) operates similarly, working only with the second view.
The ingenuity is in how the unlabeled data is utilized in an iterative fashion: At each iteration, f (1) classifies a few unlabeled items that it is most confident about and passes these and their predicted labels as additional training data to f (2) .
Simultaneously, f (2) reciprocates.
Co-Training then updates both base learners with this additional "pseudo-labeled" data.
This repeats until the unlabeled data is exhausted.
A slightly simplified version of Blum and Mitchell's Co-Training algorithm is given in Algorithm 1.
To classify a new test item x, one can compare the predictions f (1) ( x (1) ) and f (2) ( x (2) ) and pick the one with higher confidence.Co-Training is a "wrapper" method in that the two base learners f (1) and f (2) can be any learning systems.
The only requirement is that each base learner has a notion of confidence, which is used to select which unlabeled items to turn into pseudo labeled data for the other view.
Importantly for this work, being a wrapper method enables Co-Training to treat two human collaborators as the base learners.It is important to understand the conditions under which Co-Training will succeed.
We present the sufficient conditions in the original analysis (Blum and Mitchell 1998), but with new interpretations geared toward our collaboration policy for human learning.Input: labeled and unlabeled data where each item has two views; learning speed s.Initialize L 1 = L 2 =labeled data repeat Train f (1) from L 1 , f (2) from L 2 .
Classify unlabeled items with f (1) , f (2) separately.
Add f (1) 's top s most confident predictions (x, f (1) (x)) to L 2 , and vice versa.
Remove these items from the unlabeled data.
until unlabeled data is exhausted ;Algorithm 1: The Co-Training algorithmThe conditions are:1.
The unlabeled data distribution and the target concept f are compatible under the two views.
In particular, let p(x) be the marginal distribution of items.
We require that with probability one,x ∼ p(x) satisfies f (1) (x (1) ) = f (2) (x (2) ).
That is, no item shall have conflicting labels between the two views.2.
Each base learner is able to learn the target concept under its view, given enough labeled data.
This refers to standard supervised learning, where the amount of labeled data required may be much larger than in Co-Training.3.
The two views are conditionally independent given the class label:p(x (2) | x (1) , y) = p(x (2) | y).
If one knows the class y, then knowing the features in one view x (1) does not help one guess the other view x (2) .
This condition ensures that the most confident items from f (1) 's perspective do not "pile up on top of each other" from f (2) 's perspective.
Rather, they spread out and provide representative (pseudo) training data for the second view.In subsequent sections, we will see how consideration of these conditions shape our collaboration policy.
The reader might wonder why Co-Training keeps the two views separate.
Why not stack the two views back intox = x (1)x (2) , and train a supervised learner on x?
One reason is their inductive biases leading to different classifiers for the same data, as shown in Figure 1.
To see why this happens, consider how the base learners respond to the bottom and right clusters.
For the bottom cluster, the x-axis view will be highly confident that the items belong to the red class because from this view they are nearly identical to the labeled red item.
In contrast, the class of the right cluster will be uncertain from this view, since these items are not particularly similar to either labeled item.
So, the x-view learner may choose to label some bottom cluster items and pass these to the y-view learner.
For the y-view learner, the reverse pattern occurs: the right cluster items very likely belong to the blue class, whereas the class of the bottom cluster items is uncertain.
Each view is confident about the items for which the opposing view is uncertain.
Thus the two views, working together, converge on the solution shown in Figure 1(c).
Such difference between supervised learning and Co-Training is general and can be observed with other datasets and choices of base learners.
Another example is given in the last section.
We now consider how these ideas from Co-Training can be used to shape a policy for human collaboration.
The task we consider is category learning: Two human collaborators are given a number of labeled training items (x 1 , y 1 ) . . . (x l , y l ) and together must label the unlabeled items x l+1 . . . x l+u .
One may view the labeled training items as teaching experiences given to the collaborators, e.g., by a teacher or a senior worker.
It is reasonable to assume that in many cases the availability of teaching is limited.
Therefore, the goal is for the dyad to grasp the target concept using as little teaching experience as possible.
We assume that the collaborators can see all of the unlabeled items upfront, which is known as transduction in machine learning 2 .
Our main interest is in exploring different collaboration policies between the two learners and how these policies affect the learning outcomes.
One obvious policy is to allow the two collaborators full access to the data (x 1 , y 1 ) . . . (x l , y l ), x l+1 . . . x l+u , and to allow them to fully interact with each other (in terms of discussions, gesturing, etc.).
We call this the "full-collaboration" policy.
Another policy might be to isolate the learners so that they each have full independent access to the data but cannot communicate or interact.
We call this the "no-collaboration" policy.We introduce a third policy, described in Algorithm 2 and explained below, that is inspired by and closely follows the Co-Training machine learning algorithm.
This policy splits each item's feature vector into two views:x = x (1) x (2).
The intention is to allow each collaborator only one of the views.
In contrast to machine learning, however, it is sometimes impossible to create artificial stimuli with a single view.
For instance, the often used Gabor patches (Vandist, Schryver, and Rosseel 2009) vary in frequency and orientation, and it is impossible to depict an orientation without any information about frequency or vice versa.
In this case, our policy constructs artificial stimuli that vary along the "viewed" dimension while holding a constant value on the "hidden" dimension (specifically the mean µ of the values on the missing view).
So if Alice and Bob are the two collaborators, Alice might see the stimuli as x (1) orx (1) µ (2) , while Bob sees them as x (2) or µ (1) x (2) .
Both Alice and Bob also see the labels for the labeled data.Alice and Bob cannot directly communicate.
Instead, at each iteration the policy requires both Alice and Bob to label the s unlabeled items that each is most confident about.
After they have both finished, the policy shows Bob's chosen items and labelings (x B1 , y B1 ) . . . (x Bs , y Bs ) to Alice.
Note that, although Bob labeled these item from his view, Alice sees them from her own view.
Alice understands that the labels come from Bob, but -in contrast to machine learning -it is up to her whether to believe Bob's labelings (i.e., whether to use them as pseudo labeled data).
At the same time, Alice's labelings are shown to Bob.
The policy then removes any unlabeled item that has been labeled by either 2 However, the dyad is also capable of making inductive inferences when faced with new test items later on.
Alice or Bob, and proceeds to the next iteration.
This repeats until the unlabeled data is exhausted.
In the end, each unlabeled item has received a label from Alice or Bob.
In the rare cases when both Alice and Bob label the same item differently, the policy breaks the tie arbitrarily.The only communication that is allowed in the CoTraining policy is label exchange 3 .
In this sense, CoTraining falls between the no-collaboration and fullcollaboration policies.
Our main question is whether the CoTraining policy leads learners toward different classification outcomes than the no-collaboration and full-collaboration policies.
We hypothesize that human behavior in the CoTraining policy will be well-predicted by the behavior of the Co-Training algorithm in machine learning, whereas participants will primarily learn linear category boundaries in the other two collaboration conditions.
This is not a trivial hypothesis given the differences between human and machine learning discussed above, and the general difficulty human beings have in learning nonlinear decision boundaries without extensive supervision.
We designed and conducted a series of experiments to compare human category learning behaviors under the three collaboration policies introduced in the previous section.
Across three separate experiments a total of 324 undergraduate students participated for course credit under IRB approval.
We programmed networked software to run on a pair of computers so that two participants in separate rooms could collaborate according to the Co-Training policy, preventing any communication between them except that explicitly allowed by the software.
The software also runs on a single computer for the full-collaboration and nocollaboration policies.
The software was implemented in the ActionScript programming language and runs in Flash Player.The category learning task was implemented as a card sorting game, see Figure 2.
Each item x is represented as a card.
The user interface contains a central bin holding the unlabeled cards as well as a bin to the left and to the right The participants' task is to sort all cards in the central bin into the left or right bins.
Before starting the experiments, participants were told whether or not they would be working with a partner, and were instructed to begin with the card they were most confident about.
We assessed learning behavior in all collaboration conditions with two stimulus sets.
Both included items defined over two continuous perceptual features, but differed in the psychological separability of the dimensions.
The "separable" set contained Gabor patches varying in spatial frequency and orientation of the grating (Vandist, Schryver, and Rosseel 2009;Ashby and Maddox 1990).
These dimensions are considered separable because it is possible for people to attend to one dimension to the exclusion of the other (Shepard 1964).
The "integral" set contained colored squares of a fixed hue but varying in saturation and brightness.
These dimensions are considered to be integral because it is difficult for people to attend to one dimension without also processing the other (Lockhead 1966).
Extensive research has shown that people respond differently to stimuli defined on separable versus integral dimensions in supervised and unsupervised learning tasks ( Ashby and Maddox 1990;Love 2002;Nosofsky and Palmeri 1996).
In both cases a stimulus is parametrized by x = x1 x2 ∈ [0, 1] 2 .
The range of values on each was determined in extensive pilot testing to ensure that participants could discriminate important distances along all dimensions.
For Gabor patches, the frequencies were calculated using λ = (x 1 * 5/34) + 2/17, and the orientations were calculated using θ = x 2 * 100, varying from 0 to 100 degrees clockwise from horizontal.
For colored squares, the brightness was calculated using b = x 1 * 0.5 + 0.25 and the saturation was calculated using c = x 2 * 0.9 + 0.5.
Figure 3 shows four stimuli corresponding to the cluster centers in Figure 1(a), in both the separable and integral stimulus spaces.
Most of our experiments employ the diamond dataset shown in Figure 1(a).
It consists of n = 80 items evenly divided into 4 clusters.
All clusters have radius 0.1.
Items within a cluster lie on a regular grid.
The two views are the x-axis and y-axis coordinates paired with the mean value of 0.5 on the hidden dimension as previously discussed.We constructed this dataset with the aim of satisfying the three technical conditions for the Co-Training algorithm.
Condition 1 is easy to verify: there exists at least one target concept f , shown in Figure 1(c), that is consistent with the marginal p(x).
In other words, no item receives contradictory labels across the two views (note this is not true for the concept in Figure 1(b)).
From the Figure we can also verify that Condition 3 is approximately true: 4 For both classes, knowing an item's x-axis position tells us little about its yaxis position and vice versa.
Condition 2 cannot be verified by consideration of the stimulus set alone.
It stipulates that each base learner in CoTraining must, with full supervision and sufficient labeled data, be capable of learning the target concept from only one view.
Because the base learners in our study are human beings, we need to determine empirically whether this condition holds.
Our first experiment addresses this question.
[Experiment 1] 13 participants were divided into two groups: 7 in the first-view group and 6 in the second-view group.
Each worked alone as a base learner, and viewed stimuli from the "integral" stimulus set.
Participants in the first-view condition saw items varying in the x dimension but fixed at 0.5 on the y dimension, whereas those in the second-view condition saw items varying along the y dimension and fixed at 0.5 in the x dimension, effectively collapsing the dataset into one dimension as shown in Figure 4.
Participants viewed four labeled items corresponding to the four cluster centers in Figure 1(a), and were asked to classify the remaining 76 items.
Note that this labeled data is twice what is provided in Co-Training.
The purpose of the study is to verify that, when provided with this supervised experience, human learners are capable of learning the target concept as it is projected in one view.Result: The average classification accuracy on the unlabeled items was quite high: 98.9% in the first-view group and 94.7% in the second-view group.
These results suggest that people were able to learn the target concept f using only one view in a supervised learning setting given four labeled training items, thus verifying the final technical condition of Co-Training.
Another pilot study also showed that in Experiment 1, humans cannot learn the concept in Figure 1(c) if they saw only the two labeled items in Figure 1(a) instead of the four.
However, as we show next, they will be able to learn it from two labeled items if they perform Co-Training label exchange.
[Experiment 2] Our second experiment compares human learning on the diamond dataset under the Co-Training, fullcollaboration, and no-collaboration policies, now using just two labeled items as in Figure 1(a).
These three policies were implemented as follows:Co-Training (C): Two partners sit in separate rooms working on a shared categorization task.
Each partner sees one of the views and no communication is permitted except through the labeling of cards.
Each partner labels one card (s = 1) and is then asked to wait for the other partner.
The card labeled by the other partner is highlighted and automatically moves from the unlabeled bin to the appropriate labeled bin.
If the partners have by chance labeled the same card, that card is automatically moved from the labeled bin, across the unlabeled bin, into the other labeled bin.
This process of labeling followed by viewing is repeated until all cards are labeled.Full-collaboration (F): Two partners sit side-by-side before a single computer working on the same categorization task.
They are able to view both features on each card simultaneously.
No restriction is made on their communication.No-collaboration (N): A single participant categorizes all cards while viewing both features simultaneously.Each collaboration policy was paired with the separable (S) or integral (I) stimuli, resulting in 6 conditions.
Participants were assigned randomly to conditions as follows: 21 dyads for CS, 25 dyads for CI; 20 dyads for FS, 26 dyads for FI; 45 singles for NS, and 34 singles for NI.To summarize the results of a given dyad or individual, we classified each cluster in the diamond dataset as either "red" or "blue" based on a simple majority vote (i.e. the cluster was designated red if more than 50% of the items in it were classified as red, and blue otherwise).
Thus there were 2 4 = 16 different possible patterns for the four clusters.
Table 1 shows the proportion of participants whose behavior matched each of these patterns across the different conditions.
For example, in the CS condition, 17/21 ≈ 0.8 fraction of dyads produced the "cross" pattern.Several observations can be made from Table 1.
First, the Co-Training policy robustly produces the nonlinear "cross" pattern in about three quarters of the dyads.
This pattern was rarely observed in the full-collaboration and the nocollaboration policies (χ 2 test, p 0.01), which both mainly produce linear decision boundaries.
This is the main finding of our work: the Co-Training human collaboration policy leads to outcomes dramatically different from no- collaboration and full-collaboration policies, and consistent with that predicted by the machine learning algorithm.
Second, in the full-collaboration and no-collaboration policies, participants showed quite different behaviors for stimuli defined over separable versus integral dimensions, producing axis-parallel boundaries with separable dimensions and "integrated" oblique boundaries with integral dimensions.
This pattern has been previously documented in a variety of work in cognition.
In Co-Training, however, the separability of the stimulus dimensions does not affect behavior (CS vs. CI, χ 2 test, p = 0.5).
This is not surprising given that each person sees only one view, but it suggests an interesting application of the policy: Co-Training can enforce consistent classification regardless of the separability of the stimulus dimensions.Additionally, there was no significant difference between full-and no-collaboration (χ 2 test, p = 0.7).
Thus the differences observed under the Co-Training policy were not simply the result of having two individuals working together.Although the Co-Training human collaboration outcome fits machine learning model predictions at the cluster level, we observed some subtle differences suggesting that machine learning algorithms like 1NN may not be the ideal models for human base learners.
One difference concerns the unlabeled items that humans label first.
Machine base learners would label the items they are most confident about, which will likely be an item that overlaps with a labeled item under that view.
Participants in our experiments did not always pick such overlapping items, but seemed to settle for items loosely similar to labeled ones, see Figure 5(a) and (b).
Another difference is in how sure the humans are.
For each unlabeled item, we may average its classification across all dyads in the Co-Training conditions where, if the average is close to -1 (blue) or 1 (red), all dyads label it consistently; 0 if they are quite unsure.
Figure 5(c) shows this per-item average using a color coding.
Items in the top and left clusters (with labeled items) are very certain, while those in the bottom and right clusters are relatively uncertain (though they do have the correct per-item majority vote label).
Typical machine Co-Training learners will have higher certainty on these clusters.
Finally, we investigated human behavior under the CoTraining policy in a learning problem that violates CoTraining's technical conditions.
The new dataset was iden- tical to the diamond dataset except that the unlabeled items were distributed on a grid, see Figure 6(a).
The dataset therefore violates Condition 1: items near the four corners receive conflicting labels between the two views.
[Experiment 3] 24 dyads worked on this counterexample under the Co-Training policy with the separable stimuli.
Apart from the distribution of the unlabeled items, all aspects of the study were identical to Experiment 2.
Fig- ure 6(b) shows the per-item average labels in Experiment 3.
Classification decisions in this study were clearly less certain than those observed in Experiment 2 (see corresponding items in Figure 5(c)).
To compare with the CS row in Table 1, we also computed the majority vote pattern for every dyad on each of the four rectangular "clusters" in We showed that, when collaborating according to a novel policy inspired by Co-Training, two human learners behave differently than individual learners or learning pairs collaborating in an unconstrained manner.
Specifically, they jointly acquire a nonlinear labeling on the diamond dataset that is highly consistent with the behavior of the machine learning algorithm, yet unusual for human category learning generally.
We have also shown that the behaviors elicited by the policy depend upon the distribution of the unlabeled data.The current work employs very simple stimuli constructed to highlight the differences between Co-Training and other learning models.
The question thus arises, what relevance do these results have for real-world learning tasks?
We believe there are several potentially important implications.
First, under the Co-Training policy each participant need view only a subset of an item's features.
For problems where the number of relevant features are overwhelming, the policy may provide an efficient way of dividing the problem up so as to make best use of costly human effort.
Second, in Co-Training each learner is satisfied with the final result (meaning there is little conflict between the labels given by one partner and the other), even though jointly the team arrives at a solution that would seem unlikely had they both viewed the full features.
Co-Training thus provides a means of promoting agreement among team members for classification solutions that otherwise might cause disagreement.
Third, the only communication required is label-exchange, which might be useful in situations where communication is costly.
Fourth, each learner is "blind" to some of the feature dimensions.
The policy might therefore prove useful in sensitive classification tasks where data security is an issue.Of course, all of these applications depend upon there being real-world tasks of interest that meet the technical conditions that allow Co-Training to work.
In this vein, it is worth noting that Co-Training does apply to other datasets beyond the "diamond" set used here.
For example, here is a 2D dataset with 8 clusters, two of them initially labeled:• + • • • • o • .
The outcome + + o o + + o ois predicted by the CoTraining machine algorithm, and we have observed this behavior in preliminary human studies.
To determine whether Co-Training has application for a real dataset, the task organizer must be able to assess whether the problem meets Co-Training's technical conditions, and must also be able to find views of the data that exploit Co-Training's properties.These constitute interesting problems for machine learning in their own right, and are a focus for future research.
