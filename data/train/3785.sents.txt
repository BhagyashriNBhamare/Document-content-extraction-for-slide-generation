Execution of a program almost always involves multiple address spaces, possibly across separate machines.
Here, an approach to reducing such costs using compiler optimization techniques is presented.
This paper elaborates on the overall vision, and as a concrete example, describes how this compiler assisted approach can be applied to the optimization of system call performance on a single host.
Preliminary results suggest that this approach has the potential to improve performance significantly depending on the program's system call behavior.
Execution of a program almost always involves multiple address spaces, whether it executes on a standalone system such as a desktop or a PDA or across multiple machines such as a client-server program or a program based on the Web or the Grid.
On a standalone system, user-level address spaces-i.e., processes-request services from the kernel address space using system calls and potentially interact with other user-space processes or system daemons.
Programs that span machines are by definition composed of multiple processes, which also interact with kernels, e.g., to exchange messages.
Address spaces play a valuable role as protection boundaries, and typically represent units of independent development, compilation, and linking.
Largely for these reasons, crossing address spaces-even on the same machine-has considerable execution cost, typically orders of magnitude higher than the cost of a procedure call [11,12].
Here, we describe an approach to reducing this costsometimes dramatically-using compiler optimization techniques.Unlike traditional uses of such techniques that are confined to optimizing within procedures (intra-procedural optimization), across procedures (interprocedural optimization), or across compilation units (whole-program optimization), our approach focuses on applying these techniques across address spaces on the same or different machines while preserving the desirable features of separate address spaces.
The specific focus is on profiling-based optimization where the address space crossing behavior of a component (e.g., the system calls made by a process) is profiled and then optimized by reducing the number of crossings or the cost of each crossing.
This compiler assisted approach to system optimization is being realized in a system called Cassyopia.
This paper elaborates on this overall vision.
We first highlight its application in one context, that of optimizing traditional system call performance on a single host.
This work complements existing techniques for system call optimization [5,6,11,14,15,16], which focus on optimizing calls in isolation rather than as collections of multiple calls as done here.
We then briefly discuss other areas in which these ideas could be applied.
Overview.
As an application of the general approach described above, we describe system call clustering, a profile-directed approach to optimizing a program's system call behavior.
In this approach, execution profiles are used to identify groups of systems calls that can be replaced by a single call implementing their combined functionality, thereby reducing the number of kernel boundary crossings.
A key aspect of the approach is that the optimized system calls need not be consecutive statements in the program or even within the same procedure.
Rather, we exploit correctness preserving compiler transformations such as code motion, function inlining, and loop unrolling to maximize the number and size of the clusters that can be optimized.
The single combined system call is then constructed using a new multi-call mechanism that is implemented using kernel extension facilities like those described in [3,4,7,8,15,20].
We also introduce an extension to the basic technique called looped multi-calls, and illustrate the approach using a simple copy program.
Our approach goes beyond earlier work on batching system calls to improve performance [2,6] by its use of compiler-based techniques to create optimization opportunities and by its orientation towards optimizing a program's entire system call behavior in a holistic manner.Clustering mechanisms.
We first describe the mechanisms used to realize system call clustering in a traditionally structured operating system.
The goal, in addition to reducing boundary crossing costs, is to reduce the number of boundary crossings required.
Specifically, we want to extend the kernel to allow the execution of a sequence of system calls in a single boundary crossing.
The new mechanism must not compromise protection, transparency or portability, significant advantages provided by the existing system call mechanism.
We now look at one such mechanism, the multi-call [17].
A multi-call is a mechanism that allows multiple system calls to be performed on a single kernel crossing, thereby reducing the overall execution overhead.
Multi-calls can be implemented as a kernel level stub that executes a sequence of system calls.
At the application level, the multi-call interface resembles a standard system call and uses the same mechanism to perform the kernel boundary crossing, thereby retaining the desirable features of the system call abstraction.
An ordered list of system calls to be executed is passed as a parameter to the multicall.
Each system call in the list is described by its system call number and parameters.
Error behavior is preserved by generating the stub so that it returns control to the application level if an error is detected during execution of any of the constituent calls.
Also, since the multi-call stub uses the original system call handlers, permissions and parameters are checked as in the original system call.Modifications to a program to replace a sequence of system calls by a multi-call are conceptually simple and can be done using a compiler without requiring changes to any other system component (e.g. the linker).
Profiling.
Given this mechanism, the issue becomes one of identifying optimization opportunities in the program, both in the sense of identifying sequences of calls that can be replaced by a multi-call and identifying correctness-preserving program transformations that can be used to create such sequences.
Profiling does this by characterizing the dynamic system call behavior of a program on a given set of inputs.
Operating system kernels often have utilities for generating such traces (e.g., strace in Linux), or they can be obtained by instrumenting kernel entry points to write to a log file.A system call graph is then constructed to provide a graphical representation of a collection of such traces for a given program.
An example of such a graph for a simple copy program is shown in figure 1.
c; the code for this program is in figure 1.
a, while 1.
b shows the control flow.
Each node in this graph represents a system call with a given set of arguments.
Consecutive system calls in the trace appear as nodes connected by directed edges indicating the order.
The weight of each edge indicates the number of times the sequence appears.
This graph forms the basis for compile-time transformations for grouping system calls.
The general idea is to find frequently executed sequences of calls in the system call graph; if the corresponding system calls are not syntactically adjacent in the program source, we attempt to restructure the program code so as to make them adjacent, as described below.Applying compiler optimizations.
The fact that two system calls appear as a sequence in the graph does not, by itself, imply that the system calls can be grouped together.
This is because even if two system calls follow each other in the trace, the system calls in the program code may be separated by arbitrary user code that does not include system calls.
Replacing these calls by a multi-call would require moving the intervening code into the multi-call, which may compromise safety.
Instead, we use compiler techniques like function inlining, code motion, and loop unrolling to transform the program and create sequences of system calls that can be optimized.
The use of these standard and well-understood optimization techniques ensures that the transformations are correctness preserving and that the optimized program behaves the same as the original [1].
This also allows tools such as those for checking program safety to be used on the optimized program in the same way as they would for the original.
Although code rearrangement is a common compiler transformation, to our knowledge it has not been used to optimize system calls as done here.A number of program transformations can be used to rearrange the statements in a program to allow system call grouping without affecting the observable behavior of the program.
A simple example involves interchanging independent statements.
Two statements are said to be independent if neither one reads from or writes to any variable that may be written to by the other.
Two adjacent statements that are independent and have no externally visible side-effects may be interchanged without affecting a program's observable behavior.
This transformation can be used to move two system calls in a program closer to each other, so as to allow them to be grouped into a multi-call.
Note that such system calls may actually start out in different procedures, but can be brought together (and hence, optimized) using techniques such as function inlining.Another useful transformation is loop unrolling.
In the control flow graph ( figure 1.
b), the if statement in basic block B2 prevents the read and the write system calls from being grouped together.
Programs like FTP, Table 1: CPU cycles for entry and exit these experiments; these results were obtained using the rdtscl call, which reads the lower half of the 64 bit hardware counter Read Time Stamp Counter, RDTSC, provided on Intel Pentium processors.
These results indicate that clustering even two system calls and replacing them with a multi-call can result in savings of over 300 cycles every time the pair of system calls is executed.
Table 2 gives the results of applying system call clustering using both the multi-call and the looped multi-call to the copy program shown in figure 1.
To do this, the multi-call or looped multi-call was assigned system call number 240 and added as a loadable kernel module.
The numbers reported in table 2 were calculated by taking the average of 10 runs on files of 3 sizes ranging from a small 80K file to large files with size around 2MB.
The block was chosen as 4096 bytes since it was the page size and hence, the optimal block size for both the optimized and unoptimized versions of the copy program.
The maximum benefit in this example is for small and medium file sizes, since the cost of disk and memory operations dominates for larger files.The second example program is the popular mpeg play video software decoder [18].
The effects of optimizing this program using our approach are shown in Table 2: Optimization of a copy program with block size of 4096 others existed partially or completely in the X-windows libraries used by the player.
The program was executed using different input files taken from [13] with sizes varying from 4.7MB to 15MB.
Overall, our approach shows a more dramatic effect than for the copy program, largely because the system calls here are not I/O bound as was the case for copy.
In addition to the savings in CPU cycles, this optimization also improved the framerate and performance of mpeg play.
Specifically, there was an average 25% improvement in the frame rate and 20% reduction in execution time across all file sizes.More details on these and other examples can be found in [17].
The multi-call mechanism can be extended further to include code other than the system calls, error checking, and loops in the multi-call.
Specifically, we can extend the basic code-motion transformations to identify a clusterable region, possibly containing arbitrary code, that can then be added to the body of a multi-call.
Optimization techniques like dead-code elimination, loop invariant elimination, redundancy elimination, and constant propagation can then be applied to optimize the program.
For example, the data transformation code in programs such as compression or multimedia encoding/decoding can be included in the multi-call.
Another avenue of optimization is to replace general purpose code in the kernel by compiler-generated casespecific code in user-space.
Examples of such general code are the register saves and restores executed by the kernel before and after each system call.
Since the kernel does not know which registers are actually used by the application process, it must save and restore all of them.
This can be quite expensive on processors with a large number of registers.
However, the compiler has this information, and it can therefore generate specialized userspace code for saving and restoring registers.
Simulations using this strategy for a 3 parameter read system call on the Intel StrongARM processor show up to 20% reduction in the number of cycles required to enter the kernel.
Other such examples include the general permission checking performed by each system call.
Note that both of the above extensions require use of a trusted compiler for safety reasons.The profiling and compiler-based optimization can also be used to enable controlled information sharing between address spaces.
Traditionally, components in different address spaces optimize their internal behavior not knowing what type of interactions will be received from other address spaces.
For example, the operating system conserves battery power by switching hardware devices such as the CPU, display, hard disk, and wireless cards into power-saving modes based on a period of inactivity.
These policies are generally based on statistical models of application behavior that attempt to predict future (in)activity based on patterns of past activity.
Because of their stochastic nature, they can be quite inaccurate for individual applications, and result in significant performance overheads [21,22,23].
However, by carefully exposing some of the components internal state to other address spaces using a translucent boundary API, each address space can optimize its behavior to better match the requirements of other system components, and hence aim for a global optimum.
The profiling and compiler techniques can be used to collect and generate the information at the application's translucent boundary API to the kernel.
The same approach can be used for compiler assisted scheduling, where an adaptive scheduler can fine-tune the scheduling policy based on the processes running in the system and their requirements.
The compiler could place "yield" points within the body of the program to indicate schedulable regions and changes in requirements.
Conversely, if the kernel exposes changes in the state of an existing resource, e.g., a reduction in CPU speed to conserve power, the application process may be able to adapt its internal algorithms [9] to degrade the service gracefully while still satisfying user requirements.Finally, note that the same principles can be applied to any address space crossing, including distributed programs where the "boundary crossing" cost involving network communication may have a delay of tens or hundreds of milliseconds.
In particular, clustering multiple remote procedure calls (or remote method invocations Table 3: Optimization of mpeg play using multi-calls in distributed object systems such as CORBA and Java RMI) can lead to significant savings [25].
Furthermore, more general code movement techniques such as moving client code to the server or server code to the client when appropriate can also be used [24].
Note that the object migration techniques used in systems such as Emerald [10] have the same goal, but without the systematic support provided by our profiling and compiler techniques.
Our current work is aimed at integrating these optimizations into the PLTO binary rewriting tool [19], and then using the tool to test further the effectiveness of the approach.
A number of potential targets have been identified ranging from utility programs like gzip and pzip, to web servers and database applications.
We also intend to explore the applicability of these techniques for small mobile devices.
Independent of the savings in CPU cycles, we believe our approach will yield significant energy savings in this context, greater in fact than what the reduction in CPU cycles would imply.
