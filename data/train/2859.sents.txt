Maze solving has been used as an example parallel-programming problem for some years.
Suggested solutions are often based on a sequential program, using work queues to allow multiple threads to explore different portions of the maze concurrently.
This paper analyzes such an implementation, but also explores an alternative implementation based on strategies long used by human maze solvers.
This alternative implementation outperforms the conventional approach on average, and furthermore exhibits large superlinear speedups.
The paper uses insights into the cause of these superlinear speedups to derive a faster sequential algorithm, and finally considers further implications and future work.
Labyrinths and mazes have been objects of fascination for millenia [15], so it should come as no surprise that they are generated and solved using computers, including biological computers [1], GPGPUs [6], and even discrete hardware [10].
Parallel solution of mazes is sometimes used as a class project in universities [7,14] and as a vehicle to demonstrate the benefits of parallelprogramming frameworks [8].
Common advice is to use a parallel work-queue algorithm (PWQ) [7,8].
This paper evaluates this advice by comparing PWQ against a sequential algorithm (SEQ) and also against an alternative parallel algorithm, in all cases solving randomly generated square mazes.
Section 2 discusses PWQ, Section 3 discusses an alternative parallel algorithm, Section 4 analyzes its anomalous performance, Section 5 derives an improved sequential algorithm from the alternative parallel algorithm, Section 6 makes further performance comparisons, and finally Section 7 presents future directions and concluding remarks.
PWQ is based on SEQ, which is shown in Figure 1.
The maze is represented by a 2D array of cells and a lineararray-based work queue named ->visited.Line 7 visits the initial cell, and each iteration of the loop spanning lines 8-21 traverses passages headed by one cell.
The loop spanning lines 9-13 scans the ->visited[] array for a visited cell with an unvisited neighbor, and the loop spanning lines 14-19 traverses one fork of the submaze headed by that neighbor.
Line 20 initializes for the next pass through the outer loop.The pseudocode for maze try visit cell() is shown on lines 1-12 of Figure 2.
Line 4 checks to see if cells c and n are adjacent and connected, while line 5 checks to see if cell n has not yet been visited.
The celladdr() function returns the address of the specified cell.
If either check fails, line 6 returns failure.
This approach does provide significant speedups on a dual-CPU Lenovo TM W500 running at 2.53GHz, as shown in Figure 4, which shows the cumulative distribution functions (CDFs) for the solution times of the two algorithms, based on the solution of 500 different square 500-by-500 randomly generated mazes.
The substantial overlap of the projection of the CDFs onto the x-axis will be addressed in Section 4.
Interestingly enough, the sequential solution-path tracking works unchanged for the parallel algorithm.
However, this uncovers a significant weakness in the parallel algorithm: At most one thread may be making progress along the solution path at any given time.
This weakness is addressed in the next section.
Youthful maze solvers are often urged to start at both ends, and this advice has been repeated more recently in the context of automated maze solving [14].
This advice amounts to partitioning, which has been a powerful parallelization strategy in the context of parallel programming for both operating-system kernels [3,9] and applications [13].
This section applies this strategy, using two child threads that start at opposite ends of the solution path, and takes a brief look at the performance and scalability consequences.The partitioned parallel algorithm (PART), shown in Figure 5, is similar to SEQ, but has a few important differences.
First, each child thread has its own visited .
Line 7 stores a pointer to this array into the per-thread variable myvisited to allow access by helper functions, and similarly stores a pointer to the local visit index.
Second, the parent visits the first cell on each child's behalf, which the child retrieves on line 8.
Third, the maze is solved as soon as one child locates a cell that has been visited by the other child.
When maze try visit cell() detects this, it sets a ->done field in the maze structure.
Fourth, each child must therefore periodically check the ->done field, as shown on lines 13, 18, and 23.
The ACCESS ONCE() primitive must disable any compiler optimizations that might combine consecutive loads or that might reload the value.
A C++1x volatile relaxed load suffices [4].
Finally, the maze find any next cell() function must use compare-and-swap to mark a cell as visited, however no constraints on ordering are required beyond those provided by thread creation and join.The pseudocode for maze find any next cell() is identical to that shown in Figure 2, but the pseudocode for maze try visit cell() differs, and is shown in The first reaction to a performance anomaly is to check for bugs.
Although the algorithms were in fact finding valid solutions, the plot of CDFs in Figure 7 assumes independent data points.
This is not the case: The performance tests randomly generate a maze, and then run all solvers on that maze.
It therefore makes sense to plot the CDF of the ratios of solution times for each generated maze, as shown in Figure 8, greatly reducing the CDFs' overlap.
This plot reveals that for some mazes, PART is more than forty times faster than SEQ.
In contrast, PWQ is never more than about two times faster than SEQ.
A forty-times speedup on two threads demands explanation.
After all, this is not merely embarrassingly parallel, where partitionability means that adding threads does not increase the overall computational cost.
It is instead humiliatingly parallel: Adding threads significantly reduces the overall computational cost, resulting in large algorithmic superlinear speedups.
Further investigation showed that PART sometimes visited fewer than 2% of the maze's cells, while SEQ and PWQ never visited fewer than about 9%.
The reason for this difference is shown by Figure 9.
If the thread traversing the solution from the upper left reaches the circle, the other thread cannot reach the upper-right portion of the maze.
Similarly, if the other thread reaches the square, the first thread cannot reach the lower-left portion of the maze.
Therefore, PART will likely visit a small fraction of the non-solution-path cells.
In short, the superlinear speedups are due to threads getting in each others' way.
This is a sharp contrast with decades of experience with parallel programming, where workers have struggled to keep threads out of each others' way.
The fraction of cells visited by PWQ is similar to that of SEQ.
In addition, PWQ's solution time is greater than that of PART, even for equal visit fractions.
The reason for this is shown in Figure 11, which has a red circle on each cell with more than two neighbors.
Each such cell can result in contention in PWQ, because one thread can enter but two threads can exit, which hurts performance [11].
In contrast, PART can incur such contention but once, namely when the solution is located.
Of course, SEQ never contends.Although PART's speedup is impressive, we should not neglect sequential optimizations.
Figure 12 shows that SEQ, when compiled with -O3, is about twice as fast as unoptimized PWQ, approaching the performance of unoptimized PART.
Compiling all three algorithms with -O3 gives results similar to (albeit faster than) those shown in Figure 8, except that PWQ provides almost no speedup compared to SEQ, in keeping with Amdahl's Law [2].
However, if the goal is to double performance Cache alignment and padding often improves performance by reducing false sharing.
However, for these maze-solution algorithms, aligning and padding the maze-cell array degrades performance by up to 42% for 1000x1000 mazes.
Cache locality is more important than avoiding false sharing, especially for large mazes.
For smaller 20-by-20 or 50-by-50 mazes, aligning and padding can produce up to a 40% performance improvement for PART, but for these small sizes, SEQ performs better anyway because there is insufficient time for PART to make up for the overhead of thread creation and destruction.In short, the partitioned parallel maze solver is an interesting example of an algorithmic superlinear speedup.
If "algorithmic superlinear speedup" causes cognitive dissonance, please proceed to the next section.
The presence of algorithmic superlinear speedups suggests simulating parallelism via co-routines, for example, manually switching context between threads on each pass through the main do-while loop in Figure 5.
This context switching is straightforward because the context consists only of the variables c and vi: Of the numerous ways to achieve the effect, this is a good tradeoff between context-switch overhead and visit percentage.
As can be seen in Figure 13, this coroutine algorithm (COPART) is quite effective, with the performance on one thread being within about 30% of PART on two threads.
oretical energy-efficiency breakeven against COPART at roughly the 200-by-200 maze size, given that power consumption rises as roughly the square of the frequency for high frequencies [12], so that 1.4x scaling on two threads consumes the same energy as a single thread at equal solution speeds.
In contrast, PWQ shows poor scalability against both SEQ and COPART unless unoptimized: Figures 14 and 15 were generated using -O3.
Figure 16 shows the performance of PWQ and PART relative to COPART.
For PART runs with more than two threads, the additional threads were started evenly spaced along the diagonal connecting the starting and ending cells.
Simplified link-state routing [5] was used to detect early termination on PART runs with more than two threads (the solution is flagged when a thread is connected to both beginning and end).
PWQ performs quite poorly, but PART hits breakeven at two threads and again at five threads, achieving modest speedups beyond five threads.
Theoretical energy efficiency breakeven is within the 90% confidence interval for seven and eight threads.
The reasons for the peak at two threads are (1) the lower complexity of termination detection in the twothread case and (2) the fact that there is a lower probability of the third and subsequent threads making useful forward progress: Only the first two threads are guaranteed to start on the solution line.
This disappointing performance compared to results in Figure 15 is due to the less-tightly integrated hardware available in the larger and older Xeon R system running at 2.66GHz.
Much future work remains.
First, this paper applied only one technique used by human maze solvers.
Others include following walls to exclude portions of the maze and choosing internal starting points based on the locations of previously traversed paths.
Second, different choices of starting and ending points might favor different algorithms.
Third, although placement of the PART algorithm's first two threads is straightforward, there are any number of placement schemes for the remaining threads.
Optimal placement might well depend on the starting and ending points.
Fourth, study of unsolvable mazes and cyclic mazes is likely to produce interesting results.
Fifth, the lightweight C++11 atomic operations might improve performance.
Finally, for mazes, humiliating parallelism indicated a more-efficient sequential implementation using coroutines.
Do humiliatingly parallel algorithms always lead to more-efficient sequential implementations, or are there inherently humiliatingly parallel algorithms for which coroutine context-switch overhead overwhelms the speedups?
This paper demonstrated and analyzed parallelization of maze-solution algorithms.
A conventional workqueue-based algorithm did well only when compiler optimizations were disabled, suggesting that some prior results obtained using high-level/overhead languages will be invalidated by advances in optimization.This paper gave a clear example where approaching parallelism as a first-class optimization technique rather than as a derivative of a sequential algorithm paves the way for an improved sequential algorithm.
High-level design-time application of parallelism is likely to be a fruitful field of study.
This paper took the problem of solving mazes from mildly scalable to humiliatingly parallel and back again.
It is hoped that this experience will motivate work on parallelism as a first-class design-time whole-application optimization technique, rather than as a grossly suboptimal after-the-fact micro-optimization to be retrofitted into existing programs.
I owe thanks to Josh Triplett, Jonathan Walpole, and Cheedoong Drung for many interesting discussions, and to the anonymous referees for many helpful suggestions.
I am indebted to Jim Wasko for his support of this effort.
This work represents the views of the author and does not necessarily represent the view of IBM.
Linux is a registered trademark of Linus Torvalds.
Xeon is registered trademark of Intel Corporation or its subsidiaries in the United States, other countries, or both.
Other company, product, and service names may be trademarks or service marks of such companies.
Open-source code available at git://git.kernel.org/ pub/scm/linux/kernel/git/paulmck/perfbook.git.
