We consider the problem of ranking refinement, i.e., to improve the accuracy of an existing ranking function with a small set of labeled instances.
We are, particularly, interested in learning a better ranking function using two complementary sources of information, ranking information given by the existing ranking function (i.e., the base ranker) and that obtained from users' feedbacks.
This problem is very important in information retrieval where feedbacks are gradually collected.
The key challenge in combining the two sources of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from users' feedbacks tends to be noisy.
We present a novel boosting algorithm for ranking refinement that can effectively leverage the uses of the two sources of information.
Our empirical study shows that the proposed algorithm is effective for ranking refinement, and furthermore it significantly outper-forms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature.
Learning to rank is a relatively new area of study in machine learning.
It aims to learn an assignment of scores to objects and rank the objects on the basis of the scores.
It has received much attention in recent years because of its important role in information retrieval.
Most research in learning to rank is conducted in the supervised fashion, in which a ranking function is learned from a given set of training instances.
The drawback with the supervised approach is that they tend to fail when the number of training instances is small.In several real-world applications, in addition to the labeled training instances, a base ranker is available that can be used to rank the objects.
Then, the research question is how to exploit the outputs from the base ranker when learning a ranking function from a small number of labeled instances.
We refer to this problem as Ranking Refinement to distinguish it from supervised learning for ranking.
Below we show two examples for the application of ranking refinement:Relevance feedback In information retrieval, documents are often ordered by a predefined relevance ranking function, such as BM25 [1] and Language Model for IR [2], that assesses the relevancy of documents to a given query.
Relevance feedback techniques are proposed to improve the retrieval accuracy by allowing users to provide relevance judgments for the first a few retrieved documents.
The research question here is how to enhance the accuracy of relevance feedback by combining the uses of the two types of information.
In this case, the base ranker is the relevance ranking function, and the training instances are the documents that are judged by the users.Recommender system The goal of a recommender system is to rank the items according to the interest of an active user (i.e., the test user).
Usually, a few rated items are provided to indicate the preference of the active user.
Using the collaborative filtering techniques [3], we can come up with a preliminary list of items ranked by using the preference information of other users.
The research question here is how to enhance the final ranking accuracy by leveraging the two types of information.
In this case, the base ranker is the collaborative filtering algorithm, and the labeled instances are the items labeled by the active user.Furthermore, any online learning of ranking functions can be viewed as a ranking refinement problem in that the ranking function is updated iteratively with new training instances collected on the fly.A straightforward approach toward ranking refinement is to view the scores of the base ranker as an additional feature, and learn a ranking function over the augmented features.
As will be shown in the experiments, this is not the best approach for exploiting the information hidden in the base ranking function.
We believe that the most valuable information behind the base ranker is not its scores but the ranked list of objects it produces.
We therefore view the base ranker and the labeled instances as two complementary sources of information.
The key challenge in combining these two sources of information is that the ranked list generated by the base ranker tends to be imperfect while the labeled instances tend to be noisy.
In this paper, we present a boosting algorithm for ranking refinement that can effectively utilize the two types of information.
Our empirical study with relevance feedback and recommender system show that the proposed algorithm is effective for ranking refinement, and it significantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature for the objects.
Most learning to rank algorithms are designed for the setting of supervised learning, in which a ranking function is learned from labeled instances.
The problem of learning to rank is often cast as a classification problem where the goal is to correctly classify the ordering relationship between any two instances.
Three well-known approaches in this category are Ranking-SVM [4,5], RankBoost [6], and RankNet [7].
Ranking-SVM minimizes the number of incorrectly ordered pairs within the maximum margin framework.
Several variants [8,9] are developed to further enhance the performance of Ranking-SVM.
RankBoost learns a ranking model based on the same consideration, but by means of Boosting.
RankNet [7] is a neural network based approach that uses cross entropy as its loss function.
Recently, Xu et al. [10] proposed another approach that is aimed at directly optimizing the performance measures in information retrieval, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG).
A special group of ranking problem is called ordinal regression [4], in which the output of the ranking function is restricted to a few ordinal categories.
Example algorithms for ordinal regression include the maximum margin based approach [11] and the Gaussian process based approach [12].
The ranking refinement problem differs from the supervised ranking problem in that an imperfect base ranker is provided in addition to the labeled training instances.The ranking problem is essential to information retrieval, whose goal is to rank a collection of documents by their relevance to a given query.
In particular, relevance feedback techniques [13] are developed to improve the accuracy of the existing retrieval algorithms.
There are two types of relevance feedback.
The first type, termed user relevance feedback, enhances the retrieval accuracy by collecting the user relevance judgments for the documents that are ranked on the top of the list.
As pointed out in the introduction section, the user relevance feedback problem can be treated as a problem of ranking refinement.
In the empirical study, we will show that the proposed algorithm for ranking refinement significantly outperforms the standard relevance feedback algorithm (i.e., the Rocchio algorithm) over several datasets.
The second type of relevance feedback, often termed pseudo relevance feedback, does not explicitly collect the user relevance judgments.
Instead, it treats the top ranked documents as relevant to the given query, and the documents ranked at the bottom as irrelevant.
These pseudo relevance judgments are used to improve the existing ranking function.
It is well known in information retrieval that pseudo relevance feedback may result in degradation of retrieval performance given the high probability of errors in pseudo relevance judgments [13].
This is similar to the noise of training instances in ranking refinement.
Let D = (x1, x2, . . . , xn) denote the set of instances to be ordered, where each instance x i ∈ R d is a vector of d dimensions.
Let G : R d → R denote the base ranking function (base ranker), and g i = G(x i ) denote the ranking score assigned to xi by the base ranking function G. Instance xi is ranked before xj if gi > gj.
To make our problem general, we assume the label information collected from user feedback is presented as a set of ordered pairs, denoted by O = {(xi k xj k )|k = 1, . . . , m} where each pair xi xj indicates that instance xi is ranked before xj 1 .
The goal of ranking refinement is to learn a ranking function F : R d → R by exploiting both the labeled pairs in O and the ranking information given by G.
The first important question for ranking refinement is how to encode the ranking information provided by the base ranking function G.
A straightforward approach is to use the ranking scores computed by G as an additional feature, and apply the existing algorithms, such as RankBoost and Ranking-SVM, to learn a ranking function from the labeled instances.
The drawback of this approach is twofold:• First, this approach only utilizes the ranking scores of the labeled instances.
The ranking information generated by the base ranking function for the unlabeled instances is completely ignored by this approach.
Since the number of labeled instances collected from users' feedbacks is considerably smaller than the number of unlabeled instances, this approach is not optimal in exploiting the information provided by the base ranking function.
• Second, we believe that the ranking orders generated by the base ranking function is substantially more reliable than the numerical values of the ranking scores.
Similar observation is found in the study of meta search whose goal is to combine the retrieval results of multiple search engines to create a better ranking list [14].
Empirical studies [14] showed that the meta search algorithms based on the document ranks often outperform the algorithms that directly use the relevance scores.To address the above problems, we encode the order information generated by the base ranking function G with matrix W ∈ [0, 1] n×n .
Each Wi,j in the matrix represents the probability of ranking xi before xj and is defined as followsWi,j = exp(λgi) exp(λg i ) + exp(λg j )(1)In the above, Wi,j is defined by a softmax function and the parameter λ ≥ 0 represents the confidence of the base ranking function.
To see the effect of λ, we consider two extreme cases:• λ = 0.
In this case, we have Wi,j = 0.5, which indicates that the ordering information generated by the base ranker is completely ignored.
• λ = ∞.
In this case, we haveWi,j =    1 gi > gj 0.5 gi = gj 0 g i < g j(2)Thus, W is almost a binary matrix, which implies that we completely trust ranked list generated by the base ranking function.By varying the parameter λ, we are able to alleviate the negative effect from the base ranking function.
In our experiment, we set λ to be inverse to the standard deviation of ranking scores of the first 10 retrieved documents.
Similarly, we encode the ordering information inside the set O with matrix T as follows:T i,j = 1 − η/2 (x i x j ) ∈ O η/2 otherwise (3)where parameter η ∈ [0,1].
Ti,j represents the probability of ranking ranking xi before xj in the training data.
The parameter η reflects the error rate of training data, and is particularly useful when the labeled instances are derived from implicit user feedback that is usually noisier.
In our experiment, we set η = 1/2.
The goal of ranking refinement is to learn a ranking function F : R d → R from matrix W and T that produces a more accurate ranked list than the base ranking function G.
In particular, the optimal ranking function F should be consistent with the ranking information in W and T .
To this end, we measure the ranking errors of F with respect to both W and F , i.e.,errw = n i,j=1 Wi,jI(Fj ≥ Fi) (4) errt = n i,j=1Ti,jI(Fj ≥ Fi)In the above, we introduce F i = F (x i ) and the indicator function I(x) that outputs 1 when the input boolean variable x is true and zero otherwise.
There are two problems with directly using the ranking errors err w and err t as the objective function:• First, both error functions are non-smooth functions since the indicator function I(x) is non-smooth.
It is well know that optimizing a non-smooth function is computationally more challenging than optimizing a smooth function because the derivative of a nonsmooth function is not well defined [15].
• Second, with two objectives at hand, the problem is essentially a multi-objective optimization problem [16].
Thus, another important question is how to combine multiple objectives into one single objective.In the following subsections, we will address these two questions separately.
To address the problem with non-smooth objective functions, we follow the idea of boosting by replacing the indicator function I(x ≥ y) with an exponential function exp(x − y).
The resulting new objective functions are:errw = n i,j=1 Wi,j exp(Fj − Fi) (6) errt = n i,j=1Ti,j exp(Fj − Fi)Note that since exp(x − y) ≥ I(x ≥ y), by minimizing the errors err w and err t , we are effective in reducing the original ranking errors errw and errt.
Another advantage of using errw and errt comes from the theoretic result of AdaBoost [17], i.e., by minimizing the exponential loss function, the resulting classifier will not only reduce the training errors but also maximize the classification margin.
The enlarged classification margin is the key to guarantee a low generalization error for testing instances [17].
Remark: It is interesting to examine the effect of the smoothing parameter η on the ranking error err t .
By substituting the expression (3) for Ti,j in (7), we have errt expressed as follows:errt = (1 − η) (x i x j )∈O exp(Fj − Fi) + η 2 n i,j=1 [exp(Fi − Fj) + exp(Fj − Fi)] ≈ (1 − η) (x i x j )∈O exp(Fj − Fi) + η 2 n i,j=1 (Fi − Fj) 2 = (1 − η)   (x i x j )∈O exp(Fj − Fi) + η 2(1 − η) F 2 S   (8)where F 2 S is a norm of vector F = (F1, . . . , Fn) defined as follows:F 2 S = F (nI − ee)Fwhere I is the identity matrix and e is a vector of all ones.
The approximation of the second step in the above derivation follows the Taylor expansion of the exponential function.
Clearly, the second term in (8), i.e., ηF 2 S /2(1 − η), is similar to the regularization term used by Support Vector Machines (SVM) [18].
Thus, the parameter η plays the role of coefficient in the regularized ranking error errt.
The problem of optimizing multiple objectives is usually called multi-objective optimization problem [16].
The most common approach is to linearly combine objectives, which in our case is to linearly combine the two error functions, i.e.,L a = γ err w + err t = n i,j=1 (γW i,j + T i,j ) exp(F j − F i ) (9)where parameter γ is used to combine two classification errors.
We refer to the approach based on the above objective function as "Linear Ranking Refinement", or LRR, for short.
The main problem with using the linearly combined objectives is how to decide an appropriate value for γ.
In our experiments, we will show that different γ could result in very different performance in information retrieval.To resolve the difficulty, we consider another approach which makes combination of the two errors by their products, i.e.,Lp = errw × errt = n i,j=1 Ti,j exp(Fj − Fi) n i,j=1Wi,j exp(Fj − Fi)We refer to the approach as "Multiplicative Ranking Refinement", or MRR for short.
The first concern on using the product is whether the resulting solution is Pareto efficient [16].
A solution F = (F1, . . . Fn) is Pareto efficient for the objectives errw and err t if there does not exist any other solutionF = (F 1 , . . . , F n ) that is either 1.
err w (F ) < err w (F ) and err t (F ) ≤ err t (F ), or 2.
errw(F ) ≤ errw(F ) and errt(F ) < errt(F ).
In other words, if F is Pareto efficient, it guarantees that no solution is able to further reduce the two objectives simultaneously than F.
It is well known that, according to multi-objective optimization theory [16], the solution found by minimizing L a is guaranteed to be Pareto efficient.
Regarding the Pareto efficiency when minimizing L p in (10), we have the following theorem:Theorem 1.
The optimal solution F = (F 1 , . . . , F n ) found by minimizing the objective function L p is Pareto efficient.The proof of this theorem can be found in Appendix A.
The main advantage of using L p rather than L a is that it does not need a weight parameter.
This will be revealed in our empirical studies in that minimization of Lp usually significantly outperforms minimization of L a even when the optimal combination weight γ is used for L a .
In order to compare the properties of the two different approaches for combination, we examine their first order derivatives.
Let ξ denote the parameters used by the ranking function F (x).
Then, the first order derivatives of L a and L p with respect to ξ are given as follows:ξ La = n i,j=1 (T i,j + γW i,j ) exp(F j − F i )( ξ F (x j ) − ξ F (x i )) ξ L p = Lp i,j=1 (ai,j + bi,j) exp(Fj − Fi)( ξ F (xj) − ξ F (xi))wherea i,j = W i,j exp(F j − F i ) n i,j=1 W i,j exp(F j − F i ) (11) b i,j = Ti,j exp(Fj − Fi) n i,j=1 T i,j exp(F j − F i )(12)Note that both derivative shares similar structures.
The key difference between ξ L a and ξ L p is that in ξ L p , a i,j and bi,j are used to weight the contribution from W and T for instance pair (xi, xj) when computing the derivative.
This is in contrast to ξ L a where the weights for instance pair(x i , x j ) are γW i,j exp(F j − F i ) and T i,j exp(F j − F i ).
The 1: Compute W i,j and T i,j based on the ranked list and the set of labeled pairs 2: Initialize F (x) = 0 for all instances 3: repeat 4:Compute γi,j for each instance pair asγ i,j = a i,j + b i,j(13)where a i,j and b i,j are defined in (11) and (12).
4:Compute the weight for each instance aswi = n j=1 γi,j − γj,i(14)4: Assign each instance the class label y i = sign(w i ).
Train a classifier f (x) : R d → {0, 1} that maximizes the following quantityθ = n i=1 |w i |f (x i )y i(15)6:Predict fi for all instances in D 7:Compute combination weight α as follows:α = 1 2 log n i,j=1 γ i,j δ(f i , 1)δ(f j , 0) n i,j=1 γi,jδ(fj, 1)δ(fi, 0)(16)where Update the ranking function asf i = f (x i ).
δ(x,F (x) ← F (x) + αf (x)(17)9: until reach the maximum number of iterations main advantage of using a i,j and b i,j comes from the fact that they are normalized, i.e., n i,j=1 a i,j = n i,j=1 b i,j = 1, and therefore the contributions from W and T are naturally balanced when calculating the derivative.
In this section, we will consider algorithms for learning the ranking function F (x) by respectively minimizing the objective function L a and L p .
The objective function L a is similar to the objective function used by Rank-Boost except that a weight (Ti,j +γWi,j) is used for each instance pair.
We thus can simply modify the Rank-Boost algorithm to learn the optimal ranking function F (x).
Hence, in the sequel, we will focus on the boosting algorithm for minimizing L p .
To learn the optimal ranking function F (x), we follow the greedy approach of boosting algorithms.
Since the information for training is a set of labeled instance pairs, a straightforward boosting approach is to iteratively update the weights of instance pairs and train a new ranking function for the given weighted pairs.
This is the strategy employed in the Rank-Boost algorithm [6].
However, since the number of instance pairs is O(n 2 ), this approach could be computationally expensive when the number of instance n is large.To address the above problem, we present a new boosting algorithm that converts the weights of instance pairs into weights for individual instances.
for the target objective that decouples functions for pairs of instances into functions for individual instances.
It is this decoupling that makes it possible to infer weights for individual instances from weights for instance pairs.
In addition, the new boosting algorithm is able to derive an appropriate binary class label for each instance using the computed weights.
Using both the weights and the class labels of instances, we can train a binary classifier f : R d → {0, +1} and update the overall ranking function byF (x) = F (x)+αf (x)where α is the combination weight.
Note that by converting a ranking problem into a series of binary classification problems, the new boosting algorithm avoids the high computational cost arising from the large number of instance pairs.Algorithm 1 summarizes the overall procedures for the proposed boosting algorithm minimizing Lp.
As the first step in the iteration, we compute γi,j for every pair of instances that measures the uncertainty of ranking instance x i ahead of x j .
Next, we calculate the weight for instance x i as w i = n j=1 γ i,j − γ j,i .
It is important to note that w i can be both positive and negative.
In particular, wi > 0 indicates that it is more likely to have xi ranked on the top of the ranked list than on the bottom of the list; w i ≤ 0 indicates the opposite.
Hence, we can derive the class label y i for x i based on the sign of wi: a positive class for placing instances on the top of the ranked list, and a negative class for placing instances on the bottom of the list.
Since |w i | indicates the overall confidence in deciding the ranking position of x i in the list, it is used to weight the importance of individual instances.
With this information, we will train a classifier that maximizes θ in (15), which can be interpreted as a sort of classification accuracy.
Since most binary classifiers are unable to take weights into consideration, we will divide the training procedure into two steps: in the first step, we sample s instances according to the distribution that is proportional to the weights |wi|; we then train a binary classifier f : R d → {0, +1} using the sampled instances.
We manually set s = max(20, n/5) in our empirical study.
A similar strategy is employed in the AdaBoost algorithm [6] and its effectiveness has been verified in empirical studies.In the remaining of this section, we will give justification to the proposed algorithm described in Table 1.
The main result is summarized in Theorem 2.
Theorem 2.
Let f k (x) denote the binary classification function obtained in the kth iteration, and γ k i,j denote γ i,j learned in that iteration.
The objective function after T iterations, denoted by L T p , is bounded as follows:L T p ≤ n i,j=1Ti,j n i,j=1Wi,j exp − T k=1 ( √ µ k − √ ν k ) 2(18)whereµ k = n i,j=1 γ k i,j δ(f k (x i ), 1)δ(f k (x j ), 0) ν k = n i,j=1 γ k i,j δ(f k (x i ), 0)δ(f k (x j ), 1)The above theorem essentially shows that by using the proposed algorithm, the objective function L p will be reduced exponentially.The key to proving Theorem 2 is to establish the relationship between the objective function Lp of two consecutive iterations.
This is because by upper bounding the log-ratio between L p of two consecutive iterations, i.e.,rt ≥ log L t p − log L t−1 p ,(19)we will haveL T p = L 0 p T t=1 L t p L t−1 p ≤ L 0 p exp T t=1 r t(20)For the convenience of presentation, in the following, we only consider two consecutive iterations without specifying the index of iteration.
Instead, we denote the quantities of the current iteration by symbol˜to differentiate the quantities of the previous iteration.
In order to establish an upper bound for the log ratio, we first introduce the following lemma Lemma 1.
Assume˜FAssume˜ Assume˜F (x) = F (x)+αf (x) where˜Fwhere˜ where˜F (x) and F (x) are the ranking functions of two consecutive iterations, respectively.
f : R d → {0, 1} is a binary classifier and α is the combination weight.
We have the following inequality hold for any F , f , and α:log˜L log˜ log˜L p Lp ≤ −2 + n i,j=1 (a i,j + b i,j ) exp(α(f j − f i )) (21)where a i,j and b i,j are defined (11) and (12), respectively.The proof of Lemma 1 can be found in Appendix B. Using the Lemma 1, we present the proof of Theorem 2 in Appendix C. Finally, we can show the relationship between the objective function Lp and the quantity θ (in (15)) that is used to guide the training of binary classifiers in iterations.
This result is summarized in the following theorem:Theorem 3.
Let θ k denote the value of the quantity θ (in (15)) that is maximized by the binary classifier f k (x) learned in the kth iteration.
Assume that θ k ≥ 0 for each iteration.
Then, the objective function after T iterations, denoted by L T p , is bounded as follows:L T p ≤ n i,j=1 T i,j n i,j=1 W i,j exp − T k=1 θ k(22)The proof of the above theorem can be found in Appendix D. Theorem 3 provides a theoretical justification for Algorithm 1.
In particular, by maximizing θ, Algorithm 1 effectively reduces the objective function L p .
This is further confirmed by our empirical study.
Figure 1 shows an example of reduction in the objective function Lp.
We clearly see that the objective function is reduced exponentially and receives the largest reduction during the first few iterations.
In this section, we evaluate the proposed algorithm for ranking refinement by two tasks, i.e., user relevance feedback and recommender system.
The objectives of our experiments are: (1) to compare the proposed algorithm for ranking refinement to the existing ranking algorithms, (2) to examine the performance of the proposed algorithm for ranking refinement with different numbers of training instances, (3) to examine the effect of different base rankers on the performance of the proposed algorithm, and (4) to examine the time efficiency of the proposed algorithm for ranking refinement.
For the Relevance Feedback experiment, we used the LETOR testbed [20] that includes the OHSUMED dataset and the datasets from TREC 2003 and 2004.
The OHSUMED dataset consists of 106 queries.
For each query, a number of documents are retrieved and their relevance to the query is given at three levels: definitely (2), possibly (1), or irrelevant (0).
There are a total of 16, 140 query-document relevance judgments.
For each query-document pair, a total of 25 ranking features are extracted.
There are 50 queries in the dataset of TREC 2003, and 75 queries in TREC 2004.
For each query, about 1000 documents are retrieved, which amounts to a total of 49, 171 query-document pairs for TREC 2003 and 74, 170 query-document pairs for TREC 2004.
A binary relevance judgment is provided for each query-document pair.
There are 44 features extracted for each query-document pair.
The detailed information about OHSUMED and TREC data sets are available in [20].
For the Recommender System experiment, we used the MovieLens dataset, available at [19], contains 100, 000 ratings (from 1 to 5) for 1682 movies given by 943 users.
Each movie is represented by 51 binary features: 19 features are derived from the genres of movies and the rest 32 features are derived from the keywords that are used to describe the content of movies 2 .
To examine the effectiveness of the proposed algorithm for ranking refinement, we compared the following ranking algorithms:Base Ranker: It is the base ranker used in the ranking refinement.Rocchio: This algorithm extends the standard Rocchio algorithm for user relevance feedback and it creates a new query vector by linearly combining the query vector and vectors of feedback documents.
Given the initial query Q 0 , the relevant documents (R 1 , R 2 , ..., R n 1 ) 2 We downloaded the keywords of each movie from the online movie database IMBD.
The 32 most popular keywords used by the 1682 movies were selected.
and non-relevant documents (S 1 , S 2 , ..., S n 2 ), the new query according to Rocchio is:Q = Q0 + α n 1 i=1 Ri n1 − β n 2 i=1Si n2Note, in our case, that each document is not represented by a vector of word frequency, but a vector of features that are computed based on its match to the query.
Hence, we don't have Q0, i.e., the representation vector for query itself.
We therefore set Q0 to be a vector of all zeros.
We used the inner product between the new query and documents as the scores to rank the documents.
To obtain the best performance, we vary α and β from 1 to 10 and choose the best setting.SVM: This implements the Ranking-SVM algorithm using the SVM light package.
Note that it is commonly believed that Rank-Boost performs equally well as Ranking SVM.
The experimental results provided in the LETOR collection also confirm this.
Hence, we only compare the proposal algorithm with Ranking-SVM, but not Rank-Boost.
MRR: This is the Multiplicative Ranking Refinement algorithm that minimizes Lp in (10).
LRR: This is the Linear Ranking Refinement algorithm that minimizes La in (9).
Since the performance of LRR depends on the parameter γ, we run LRR with 100 different values from 0.1 to +10 and choose the best and worst performance.
We referred them to as LRR-Worst and LRR-Best, respectively.For a fair comparison, the output from the base ranker is used as an extra feature when using SVM (i.e., Ranking-SVM) and Rocchio.
To evaluate the performance of different algorithms, we used precision and normalized discounted cumulative gain (NDCG) at rank position k. Let (d R 1 , d R 2 , ..., d R n ) denote the top ranked documents according to the ranker R, and (r R 1 , r R 2 , .
.
, r Rn ) denote their binary judgments.
The precision at rank position k measures the relevancy of the first k documents and is defined as followsP R @k = k i=1 r R i /kFor the OHSUMED dataset, a document is deemed relevant when its score is two.
In the case of MovieLense data, a movie is deemed to be interesting to a test user when its rating is no less than 4 3 .
Since the first top documents are more important than the other documents, we also employ the NDCG metric [21] that is defined as follows:N DCG R @k = DCGR@k DCG T @kwhere T stands for the oracle ranker and DCG X @k is defined as follows [21]:DCG X @k = rX 1 if k = 1 r X 1 + k i=2 r X i log 2 i if k > 1 Unless specified, for all the experiments with relevance feedback, we used the standard BM25 retrieval algorithm (i.e., the 21st feature in OHSUMED data set and 16th in TREC data sets) as the base ranker.
We followed the common practice of user relevance feedback by collecting the relevance judgments for the first 10 retrieved documents.
These user relevance judgments served as labeled instances in ranking refinement.For the experiment with recommender system, the base ranker was created by applying a collaborative filtering algorithm, more specifically, the Personality Diagnosis algorithm [3], to the user rating data.
In particular, 20 users were randomly selected as the training users, and the remaining 923 users were used for testing.
For each test user, 10 rated movies were randomly selected and were used by the collaborative filtering algorithm to identify the 20 training users who share the common interests with the test user.
Note that we did not compare the proposed algorithm to other information filtering algorithms because the focus of this study is to examine the effectiveness and the generality of the proposed approach for ranking refinement.
Figure 2 and 3 show the performance results of different algorithms in terms of precision and NDCG for the first 25 ranked documents.
First, by comparing the performance of the two variants of ranking refinement, we observed that the Multiplicative Ranking Refinement (MRR) algorithm is significantly more effective than the Linear Ranking Refinement (LLR) algorithm.
Indeed, MRR performs significantly better than the best case of LRR (i.e., LRR-best).
The key difference between MRR and LRR is that MRR minimizes the product of the two error functions while LRR minimizes the weighted sum.
We believe it is the normalization scheme brought by MRR (see equations in (11) and (12)) that makes it performing better than LRR.
Second, comparing to the other three baseline algorithms, i.e., the base ranker, Rocchio, Ranking-SVM, we observed that MRR always significantly outperforms the baseline algorithms in all the cases.
More noticeable is the improvement made by the ranking refinement over the first a few ranking positions.
We thus conclude that Multiplicative Ranking Refinement is more effective than the baseline algorithms for user relevance feedback in information retrieval.
To examine the robustness of the proposed algorithm with respect to the imperfectness of the base ranker, we tested the MRR algorithms with three different base rankers.
We plotted the results of all different features and selected three features which cover a good range of ranking quality.
The chosen base rankers for OHSUMED data set are the features 7, 11, and 21 and those for the TREC data sets are features 16, 21, and 36.
Figure 4 shows how the MRR algorithm performs using different base rankers (NDCG shows similar results).
The result indicates that the quality of base rankers has a direct impact on the performance of the MRR algorithm.
We also observed that the proposed algorithm is able to significantly improve the performance even with a poor base ranker.
More impressively, by comparing Figure 4 to Figure 2, we observed that even using the worst base ranker (i.e., feature 7 for OHSUMED, 21 for TREC 2003 ,and 36 for TREC 2004), the retrieval accuracy of MRR is comparable to the other methods using the best base ranker (i.e., the BM25 retrieval algorithm).
We thus conclude that the MRR algorithm is resilient to the imperfectness of base rankers.
To investigate the effect of the number of feedback documents on the performance, we ran the MRR algorithm by varying the number of feedback documents from 5 to 20.
Figure 5 shows the result using varied number of feedback documents.
We clearly observed that the number of feedback documents have a direct effect on the performance of ranking refinement.
However, even with a small amount of feedback, MRR is able to improve the retrieval performance considerably, particularly for the accuracy of the first few ranked documents.
We thus conclude that the proposed algorithm for ranking refinement is robust to the size of feedback data.
We evaluated the generality of the proposed algorithm by applying it to recommender system (movie recommendation).
Figure 6(a) and Figure 6(b) show the results of different algorithms when applied on the MovieLens dataset.
It is surprising to observe that the results of LRR, the linear ranking refinement algorithm, even with the tuned parameter γ, is not even comparable to the the performance of the base ranker.
In contrast, the MRR algorithm is able to significantly improve the accuracy of the base ranker and outperform the other baseline algorithms considerably.
This result further indicates the importance of appropriately combining the two information sources, i.e., the ranking information behind the base ranker and the feedback information provided by users.
Figure 6(c) shows the sensitivity of MRR to the size of feedback data by varying the number of rated movies by the test user from 5 to 20.
Similar to the result for relevance feedback, we observed that the size of feedback data affects the performance of MRR considerably.
However, even with 5 rated movies, the MRR algorithm is able to make a noticeable improvement in the prediction accuracy compared to the base ranker.
This result further confirms the robustness of the proposed algorithm for ranking refinement with respect to the size of feedback data.
Figure 7 shows the efficiency of the MRR algorithm in terms of the running time for different numbers of rated movies for each test user.
We chose movies data set for the experiment because it provides a good range for the number of objects.
We partitioned the test users into groups where each group of users has a different number of rated movies.
The running time of MRR for each group is calculated by averaging it across all the users in the group.
As pointed in Section 3.4 and seen in Figure 7, the running time is linear in the number of instances.
Note that the relatively long running time is due to the MATLAB implementation.
In this paper, we propose the problem of ranking refinement, whose goal is to improve a given ranking function by a small number of labeled instances.
The key challenge in combining the ranking information from the base ranker and the labeled instances arises from the fact that the information in the base ranker tends to be inaccurate and the information from the training data tends to be noisy.
We present a boosting algorithm for ranking refinement that is resilient to the errors.
Empirical studies with relevance feedback and recommender system show promising performance of the proposed algorithm.
W i,j exp(F j − F i + α(f j − f i )) × n i,j=1Ti,j exp(Fj − Fi + α(fj − fi))= n i,j=1 a i,j exp(α(f j − f i ) n i,j=1 b i,j exp(α(f j − f i )where a i,j and b i,j are defined in (11) Ti,j + Wi,j, we obtain the result in Theorem 2.
Proof.
We rewrite the quantity θ as follows:θ = n i=1 fiyi|wi| = n i=1 fi n j=1 γi,j − γj,i = n i,j=1 γi,j(fi − fj) = µ − ν Since µ − ν = √ µ − √ ν √ µ + √ ν ≥ √ µ − √ ν 2 ,we have θ ≥ √ µ − √ ν 2 .
Substituting this result into the expression of Theorem 2, we have Theorem 3.
1Proof.
First, note that the objective function L p is convex in terms of F.
This is because L p can be expanded as follows:Since Lp is a convex function, the solution found by minimizing L p will always be global optimal, instead of local optimal.
Second, to show that the optimal solution found by minimizing Lp is Pareto efficient, we prove by contradiction.
Let F * denote the global minimizer of function L p .
By assuming that Theorem 1 is not correct, there will exist a solution F = F * that either (1).
We can easily infer Lp(F) < Lp(F * ) since (1) both err w and err t are non-negative for any solution F, and (2) L p = err w × err t .
Clearly, this conclusion contracts the fact that F * is a global minimizer of L p .
Proof.
First, note that the objective function L p is convex in terms of F.
This is because L p can be expanded as follows:Since Lp is a convex function, the solution found by minimizing L p will always be global optimal, instead of local optimal.
Second, to show that the optimal solution found by minimizing Lp is Pareto efficient, we prove by contradiction.
Let F * denote the global minimizer of function L p .
By assuming that Theorem 1 is not correct, there will exist a solution F = F * that either (1).
We can easily infer Lp(F) < Lp(F * ) since (1) both err w and err t are non-negative for any solution F, and (2) L p = err w × err t .
Clearly, this conclusion contracts the fact that F * is a global minimizer of L p .
