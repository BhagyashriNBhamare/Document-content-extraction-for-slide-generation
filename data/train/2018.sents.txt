Performance and eeciency requirements are driving a trend towards specialized accelerators in both datacenters and embedded devices.
In order to cut down communication overheads, system components are pinned to cores and fast-path communication between them is established.
These fast paths reduce latency by avoiding indirections through the operating system.
However, we see three roadblocks that can impede further gains: First, accelerators today need to be assisted by a general-purpose core, because they cannot autonomously access operating system services like le systems or network stacks.
Second, fast-path communication is at odds with preemptive context switching, which is still necessary today to improve eeciency when applications underutilize devices.
Third, these concepts should be kept orthogonal, such that direct and unassisted communication is possible between any combination of accelerators and general-purpose cores.
At the same time, all of them should support switching between multiple application contexts, which is most diicult with accelerators that lack the hardware features to run an operating system.
We present M 3 X, a system architecture that removes these roadblocks.
M 3 X retains the low overhead of fast-path communication while enabling context switching for general-purpose cores and specialized accelerators.
M 3 X runs accelerators autonomously and achieves a speedup of 4.7 for PCIe-attached image-processing accelerators compared to traditional assisted operation.
At the same time, utilization of the host CPU is reduced by a factor of 30.
The end of Dennard scaling [18] prevents further frequency gains and the prospect of dark silicon [21] hampers generalpurpose parallelism.
Hardware and system designers thus turn to new architectures to increase performance or reduce power consumption.
These new ideas often revolve around specialization through custom accelerators [9,26,35,37,48,67,68] and streamlined communication that bypasses the operating system to avoid overheads [10,39,46].
TPUs [27] are a key example of the rst approach.
By creating a xed-function accelerator for neural network training and inference, Google managed to increase performance per socket 30-fold and performance per watt 80-fold over a contemporary CPU.
The second approach of preferring data fast paths to avoid indirections through the operating system can be observed today with technologies like single root I/O virtualization (SR-IOV) or Innniband.
System designs like M 3 [10] and DLibOS [39] have shown that fast-path communication achieves latency reductions of 5× for a le system workload on M 3 , and 20× for memcached on DLibOS.
Furthermore, our previous work M 3 demonstrates that this idea can be generalized to provide fast-path communication between all compute units in the system.Encouraged by these beneets, we expect ongoing development and increased deployment of these solutions.
Use-case-driven accelerators will nd their place in datacenters, also because of their deterministic execution model which helps to meet tail-latency requirements.
We therefore assume that more applications will entail complex interactions between a mix of accelerators and general-purpose cores.
Additionally, in multi-tenant cloud environments context switching is essential, because a single user will typically underutilize accelerators.
We also envision advantages for small embedded and edge devices.
Due to their limited hardware resources, these devices beneet from the power eeciency of accelerators and require context switching to exibly time-share these resources.
We extract three fundamental architectural challenges from our assumptions: First, the system architecture should enable accelerators to run autonomously.
Currently, accelerators are often treated as peripheral devices whose execution needs to be assisted by a general-purpose CPU [57].
The TPUs described in Google's paper burden their controlling CPU with 11 -76% load just to operate the TPU [27].
Our comparison to the traditional usage of accelerators in § 7.6 connrms this experience by showing that even a 3 GHz out-of-order We believe that accelerators should not be forced to adapt to operating system requirements, but focus on their main task: a fast and energy-eecient solution to a speciic problem.
In this paper, we take the extreme position and rethink the system architecture to enable a rst-class integration of accelerators without imposing changes on them.In contrast to conventional architectures, we do not build upon coherent shared memory for two reasons: First, providing global cache coherency is challenging for systems that consists of a wide variety of compute units such as general-purpose cores, DSPs, and xed-function accelerators.
Second, the costs of cache coherency in terms of chip area, power, complexity, and performance are expected to increase with an increasing number of compute units [28,41].
For these reasons, it is still unclear whether and how future systems will support cache coherency.
Therefore, we keep cache coherency optional.Additionally, our long-term goal is to support arbitrary accelerators as rst-class citizens.
In this paper, we begin to address this challenge by demonstrating our approach for accelerators that are arguably the most diicult to support as rst-class citizens: xed-function accelerators [22,26,35,37,48,65] that do not execute software and therefore provide none of the features that are required to run an operating-system kernel.
We believe our eeorts towards a uniied interface for all compute units will generalize to more feature-rich accelerators in the future.
We propose M 3 X, a solution for the identiied issues using a hardware-software co-design approach.
• We explore the design space for fast-path communication and context switching.
We explain the fundamental problems, when combining both techniques ( § 2) and discuss solutions in terms of interaction modes (autonomous vs. assisted) and mechanisms (hardware vs. software).
• We converge on a design for M 3 X that allows fast-path communication without involving the OS kernel and enables accelerators to access data sources and sinks without assistance by a general-purpose core.
At the same time, M 3 X supports context switching on both general-purpose cores and accelerators ( § 4).
• We implement these mechanisms within the M 3 OS and hardware architecture ( § 5).
M 3 already supports fast-path communication within a tile-based architecture and uniies communication among heterogeneous instruction sets [10].
Thus, it constitutes a suitable starting point, which we extend with support for context switching and autonomous accelerators.
• In the evaluation ( § 7), we demonstrate how M 3 X retains the low overhead of fast-path communication while enabling context switching.
We show the performance and utilization beneets of autonomous accelerators using an accelerator benchmark suite and an application scenario that might occur in datacenters.We rely on gem5 [16] as our simulation platform.
Its high accuracy and modularity enable us to experiment with new hardware components.
The implementation of M 3 X 1 and our extensions to gem5 2 are available as open source.
Traditional communication via UNIX pipes, sockets, or microkernel IPC involves the kernel in every communication.For that reason, the kernel can buuer messages until the recipient is ready to receive them, can schedule recipients based on pending messages, and can easily switch to a diierent thread if the current thread needs to wait for I/O.
Communication that bypasses the kernel ooers signiicant gains in terms of latency and throughput, as has been shown by M 3 [10] and DLibOS [39].
We call such communication fast-path communication in this paper.
Using fast-path communication with dedicated cores for the applications is easy, because none of the aforementioned actions are required, which is why M 3 and DLibOS chose to omit context switching support altogether.
We also believe the still increasing core counts and the dark silicon eeect [21,25] will reduce the context switching frequency and lead to dedicated cores for applications by default.
However, in the foreseeable future, provisioning enough hardware resources to handle all load spikes is not feasible.
These load spikes therefore require oversubscription of cores and accelerators.
Thus, fast-path communication needs to be combined with context switching to use the system as eeciently as possible.Combining fast-path communication with context switching is a hard problem, though.
If the kernel is not involved in the communication, how can we determine whether the recipient is running and how can we deliver the message if it is not running?
Even without relying on coherency (see § 1.2), we could buuer all messages in DRAM to cope with non-running recipients.
However, this would eeectively route all communication over DRAM, which increases latency and DRAM load and is therefore detrimental to the goals of fast-path communication.
On the other hand, communicating directly between compute units, without involving the OS, has the consequence that a message cannot be delivered if the designated recipient is not running.
The naive solution of waking up the recipient and retrying the fast-path communication is not suucient.
Since these two steps are not atomic, the recipient can be suspended in between, leading to no progress at the sender side.
Furthermore, the kernel can no longer make scheduling or placement decisions if it cannot tell whether applications are currently waiting for a message or are doing useful work.Accelerators typically lack the architectural features to run an OS kernel locally.
To avoid the indirection of all communication through a remote kernel, accelerators require fast-path communication to interact with other accelerators [57].
However, as described before, fast-path communication is possible only if the recipient is running.
Thus, the combination of fast-path communication and context switching is necessary to run accelerators autonomously, without indirection through the kernel.
There are industry solutions for accelerator integration such as the coherent accelerator processor interface (CAPI) [6,62] and the heterogeneous system architecture (HSA) [4,51].
Both allow the integration of accelerators into a cache coherent virtual memory system, but in contrast to M 3 X, these hardware solutions do not consider direct access to operating system services by accelerators.
Such access is investigated by other works for speciic OS services and speciic accelerators like GPUs [15,29,34,52,58,64,69] or FPGAs [31,47,59].
In contrast, M 3 X does not target a speciic kind of accelerator, but provides a general construction principle for fast-path communication of any accelerator with any OS service or application.
K2 [36] and Popcorn Linux [12] demonstrate how the Linux kernel can be extended to support multiple coherence domains and potentially heterogeneous cores.
However, heterogeneity in these cases is represented by general-purpose cores with diierent instruction sets and does not include xed-function accelerators.
Barrellsh [14] introduced the multikernel concept, where message passing is used to communicate between the operating system instances on each core.
This concept is close to the design of M 3 X, but it assumes that all cores ooer the architectural features to run an OS kernel.
M 3 X sets out to remove this requirement to integrate xed-function accelerators as well.Arrakis [46] and OmniX [57] integrate peripherals and accelerators using SR-IOV.
Instead of requiring the architectural features to run a kernel, these works assume the hardware to manage multiple contexts.
M 3 X explores a more lightweight design, yet with enough hardware support to enable context switching and fast-path communication between components.
Like NIX [11] or FlexSC [60], M 3 X adopts the idea of redirecting system calls to kernel cores to reduce the duties of non-kernel cores such as accelerators.Horizontal system layouts [66] with diierent services on separate cores were explored in DLibOS [39] and M 3 [10].
Both works have shown latency reductions due to the use of fast-path communication between cores, but they do not support context switching due to the problems explained in the previous section.
Context switching of accelerators has been explored speciically for GPUs [13,45], but without considering fast-path communication.
M 3 X combines contextswitching with the beneets of fast-path communication.
We start this section with an introduction to the basic system architecture and discuss the design space for accelerator integration.
Afterwards, we describe how fast-path communication is combined with context switching.
Finally, we explain how this combination is used to run accelerators autonomously.
In this work, we assume a tiled system architecture as depicted in Fig. 1.
The system uses an interconnect to communicate between tiles, similar to M 3 [10] and DLibOS [39] and also similar to upcoming system architectures based on GenZ [3] or CCIX [2].
The tiles can contain heterogeneous compute units (CUs), ranging from general-purpose cores to DSPs to xed-function accelerators.
These CUs can be part of the host system (left) and can be attached as an expansion card (right).
The host system has a shared DRAM.
We use the term activity to unify the active entities on these CUs.
On general-purpose cores, an activity is typically a thread, whereas on accelerators it is the logic operating on a context.
Adding accelerators into a system design poses the question of how to balance responsibilities between hardware and software.
First, there are diierent ways to support arbitrary data sources and sinks for accelerators.
Access to OS services like le systems or network stacks can be performed by software, which is the typical approach today.
This approach requires a general-purpose core to assist the accelerator by continuously moving data back and forth.
However, if the protocol to access OS services and data is suuciently simple, it can be implemented in hardware.
Such a hardware-friendly protocol allows accelerators to autonomously access arbitrary sources and sinks and removes software from the critical path.
We present our protocol in § 4.8.
Second, if a system wants to support multiple activities with diierent priorities on a single accelerator, a low-latency context switch to the prioritized activity is needed.
However, accelerators are typically invoked by software and are not interruptible until the computation is complete.
One way to lower the latency is to reduce the amount of data per invocation.
Consequently,the compute time per invocation is reduced,but software needs to continuously invoke the accelerator, which causes more CPU utilization and power consumption.
Alternatively, the ne-grained invocation can be done in hardware by adding a simple state machine with preemption points next to the accelerator logic, as described in § 4.9.
Finally, to improve the utilization of accelerators, support for multiple contexts is necessary.
One solution is to require the accelerator to provide a suucient number of contexts and multiplex the hardware accordingly (e.g., based on SR-IOV).
Alternatively, a combination of hardware and software can be used, which requires only a single context in hardware.
To keep the hardware simple, we chose the latter approach: We perform the potentially complex scheduling decisions in software and add a simple state machine to the hardware, which saves and restores contexts.
To increase the exibility and applicability of our system design, we chose not to rely on shared memory.
Hence, messages cannot be delivered if the receiving activity is suspended (e.g., by preemption).
There are two basic solutions to this problem:1.
Eagerly invalidate all incoming communication channels to an activity before suspending it or 2.
keep the communication channels alive, but lazily detect communication attempts with suspended activities.The eager approach does not require hardware support, but leads to more context switching overhead that grows linearly with the number of communication channels.
In contrast, the lazy approach requires hardware support, but communication channels do not need to be invalidated on context switches.
We chose the lazy approach, because our system supports many incoming communication channels (at most (−1) * per compute unit with CUs and communication endpoints per CU).
Furthermore, many communication channels are typically not used while an activity is suspended.
To this end, we inform the hardware of the running activity and of the intended recipient activity when communicating (see § 5.5 for details).
The hardware compares both and reports an error upon communication attempts with suspended activities.
Independent of eager invalidation or lazy detection, the hardware reports an error to the sender if the intended recipient is not running.
Unfortunately, the naive solution of scheduling the recipient and retrying the fast-path communication introduces the following race condition: Since the kernel is not involved in this communication, it does not know when the communication has been completed successfully.
If the kernel suspends the recipient before the communication has been nished, the sender does not make progress.
The problem is that context switching and communication are decoupled, because the kernel performs the context switching, but activities bypass the kernel when performing fast-path communication.For example, if multiple senders try to communicate with multiple recipients scheduled on the same CU, the kernel could decide to schedule the next recipient before the communication with the current recipient has been nished.
We resolve this race condition by falling back to the traditional kernel-based communication model, if a communication failed due to a suspended activity.
The kernel performs both the context switching and the communication: If activity A receives an error after trying to send a message to activity B, it asks the kernel to forward this message to B.
When receiving the forward request, the kernel will rst schedule B and afterwards send the message to B. To guarantee progress, the kernel does not suspend B until the message has been successfully delivered.
Another consequence of fast-path communication is that the kernel does not know whether an activity is currently computing or idling, because it waits for a message.
We solve this problem by sending an idle notiication to the kernel, similarly to scheduler activations [8].
Alternatively, the kernel could poll all CUs periodically to check whether the current activity is performing useful work, but we opted against this solution in favor of a less loaded and more scalable kernel.We employ two optimizations.
First, to prevent overeager context switches, we delay the sending of idle notiications by a kernel-deened value called idle delay.
The idle delay is stored in the address space of the current activity and updated by the kernel.
Second, an activity does not need to send idle notiications at all if there is no ready activity that can run on its CU.
In this case the idle delay is set to zero.Note that we cannot force activities to report idling.
However, threads on traditional systems can also decide to poll instead of using blocking system calls.
On both systems, CPU-hogging activities can be penalized (e.g., priority degradation) and forcefully preempted.
The concepts described so far allow to suspend activities, resume activities based on communication attempts, and to use the system's resources eeciently by switching to a diierent activity in case the current activity idles.
However, if a set of heavily communicating activities contend with other activities for the same CUs, a systematic scheduling approach is required to maintain good performance.
For example, consider a chain of accelerator activities that perform stream processing and therefore exchange messages and data at a high rate.
If multiple such chains are contending for the same accelerators, the kernel needs to context switch these activities.
However, uncoordinated context switching among the activities of all chains leads to many failed communication attempts when activities of diierent chains run simultaneously.We solve this problem by introducing a simple form of gang scheduling [44].
Applications deene the gang of a new activity at its creation time and the kernel pins all activities in a gang on diierent CUs and schedules them at the same time.
We use this to run all activities of a single chain simultaneously.
As the evaluation shows,multiple sets of heavily-communicating activities can therefore eeciently share the same CUs.
In this work, we consider two types of accelerators, depicted in Fig. 2: stream-processing accelerators that process a stream of data in blocks and compute on each block exactly once (e.g., AES encryption) and request-processing accelerators that receive the entire data for the computation with a single request and can access all data during the entire computation (e.g., graph processing or garbage collection).
The stream-processing accelerators use DMA-based memory access to load a block of data from a source (e.g., a le or network socket), perform the computation on the block, and store the result to a sink.
The request-processing accelerators use cache-based memory access to the request data to support large requests while maintaining ne-grained data access.
For both accelerators, we add an accelerator support module (ASM), implemented as a nite-state machine, to the accelerator logic.
The accelerator logic performs the computation, whereas the ASM interacts with other CUs and invokes the accelerator logic.
We implemented the ASM as a piece of hardware in the current gem5-based prototype to demonstrate its simplicity.Running these two types of accelerators autonomously requires access to OS services such as le systems, network stacks, and pipes to load and store data.
Furthermore, accelerators need to be interruptible without requiring assistance by a general-purpose core.
We describe our solution for both problems in the following sections.
Enabling accelerators to access les or network sockets requires a simple and uniied protocol to obtain access to these resources.
To this end, we designed a simple protocol for all le-like objects, in the same spirit as UNIX's everything-is-a--le principle.
In contrast to UNIX, we implement OS services as microkernel-style servers and support both applications and accelerators as clients.The le protocol uses a fast-path messaging channel between client and server.
The server is expected to make the data available in memory and to provide the client with access to the data via a fast-path data channel.
This channel enables accelerators to access large amounts of data autonomously, preventing frequent client-server interactions.The protocol comprises two main requests: next_in and next_out.
The former requests access to the next piece of data to read, whereas the latter requests access to the memory to which the next piece of data should be written.
For example, a le-system server will provide the client with access to a fragmented le piece by piece, as described in more detail in § 5.7.
After providing the client access to the data, the server returns the ooset and size of the piece.
Upon receiving this reply, the client can access the data via the fast-path data channel without involving the server again.
After nishing the current piece, the client issues another next_in or next_out request to the server.
A piece of length zero from the server denotes end-of--le.As the client accesses the data on its own, the server does not know how many bytes the client has actually read or written.
Therefore, input and output requests need to be committed.
Each next_in and next_out request implicitly commits the complete previous piece of input or output data, respectively.
Additionally, the commit(nbytes) request can be used to explicitly commit the rst nbytes of the previous input or output request.
The commit request is used, for example, if a client wants to stop writing to a le, in which case it might have written less than it got access to.Finally, some servers support the seek request to change the le position.
As described in more detail in § 5.8, the le protocol is implemented within the ASM of the stream-processing accelerators to load input data from arbitrary le-like sources and store the result to arbitrary le-like sinks.
Note that request-processing accelerators can access OS services via the le protocol as well, but this has not been implemented.
To test the generality of the protocol, we added a POSIX-like API on top and implemented a le system server, pipe server, and virtual terminal.
As discussed in § 4.2, accelerators should be interruptible with low latency, requiring ne-grained invocations.
At the same time, accelerators should run autonomously, asking for coarse-grained interactions with software.
We achieve both by using the ASM as an indirection.
Software performs the coarse-grained invocation of the hardware-implemented ASM, which in turn invokes the accelerator logic in a ne-grained fashion and is interruptible between these invocations.
We implemented this scheme for request-processing accelerators, because the considered stream-processing accelerators already perform their computation block-wise with relatively small block sizes.
Our prototype implementation is based on the hardware and software part of M 3 [10].
The hardware platform of M 3 exists by now as custom silicon in the Tomahawk 4 chip [24].
To extend the hardware part, we build on top of the already existing gem5 prototype.
Both the gem5 prototype platform and the OS are open source and have been extended in this work to support context switching and autonomous accelerators.
The key idea of M 3 is to introduce a new hardware component next to each CU, which serves as an abstraction for the heterogeneity of the CUs and supports fast-path communication between CUs.
This hardware component is called data transfer unit (DTU) and is accessible over memory-mapped I/O (MMIO).
Each CU is integrated with its DTU as a tile into the network-on-chip.
The DTU provides a set of communication endpoints that can be conngured as send, receive, or memory endpoints.
Send and receive endpoints allow to establish a fastpath messaging channel, whereas memory endpoints are used for fast-path data channels.
Data channels provide DMA-like access to a contiguous and byte-granular memory region.The M 3 kernel runs on a dedicated kernel tile, because not all tiles can be expected to run an OS kernel.
The M 3 kernel is diierent from traditional kernels, because it does not run user applications on the kernel tile.
Instead, the kernel runs applications on other tiles, called user tiles, and waits for system calls in the form of messages, sent by applications via the DTU.
Since only the kernel tile can conngure DTU endpoints, applications are isolated from each other by default.
The main responsibility of the kernel is to establish communication channels between applications by connguring DTU endpoints remotely.
After a communication channel has been established, applications communicate directly with each other, bypassing the kernel.On M 3 , the same activity abstraction 3 is used for all types of tiles, because the kernel is only concerned with their DTU state.
The M 3 kernel uses capabilities to manage the permissions in the system.
Each activity has its own address space and capability space and system calls allow to exchange capabilities between activities.
Since M 3 does not support context switching, an activity is assigned to a free tile on creation and occupies this tile until its termination.Outside of the kernel, M 3 provides servers to host the actual functionality of the OS.
M 3 ooers an in-memory lesystem, called m3fs, that organizes the data similarly to classical UNIX lesystems.
The important diierence is that m3fs grants applications direct access to le data via the DTU.
Additionally, M 3 ooers a pipe server to connect activities via a unidirectional, rst-in--rst-out communication channel.
So far, M 3 supports only simple general-purpose cores without virtual memory.
Instead, cores have untranslated access to their dedicated scratchpad memory.
To support more complex applications and be able to switch between them without rst saving their entire memory state to DRAM, we added virtual memory support to M 3 .
However, to prepare for future systems, M 3 X does not take advantage of cache coherency, but keeps it optional.As virtual memory is also desirable for the cache-based memory access of request-processing accelerators, we added virtual memory support in two variants.
For general-purpose cores, we use their memory management unit (MMU) and run a small helper on the core that receives page faults.
For accelerators, we add an MMU to the DTU, consisting of a page table walker and translation lookaside buuer (TLB).
In both cases, page faults are resolved by a pager in collaboration with the M 3 X kernel,which updates the page table entries,similar to other microkernel-based systems [33,61].
The pager is a server in M 3 X that supports copy-on-write and demand loading.
On general-purpose cores,the helper sends a message to the pager to resolve page faults.
On accelerators, the DTU sends the message to the pager, which is transparent to the accelerator.
Context switches are performed remotely on the user tiles, initiated by the M 3 X kernel.
This approach is required for accelerators that do not have the architectural features to run an OS kernel,but is optional for general-purpose cores with these features.
In other words, the implementation could be extended to perform context switches on general-purpose cores locally.
These four components have two important interfaces.
The rst interface between the context switcher and RCTMux (green in Fig. 3) is used by the kernel to request saves and restores from RCTMux and by RCTMux to acknowledge their completion.
Second, the DTU-CU interface (red) is used by the kernel to signal the CU about an imminent context switch.
Depending on the type of CU, the signal injects an interrupt request into a core or notiies the ASM of an accelerator.
We incorporated the context switcher module into the M 3 kernel to perform context switches on user tiles.
First, the context switcher asks RCTMux to save the CU state.
The context switcher then saves the DTU state of the current activity, restores the DTU state of the new activity, and asks RCTMux to restore said activity's CU state.
Each of these steps is executed individually to be able to handle other requests (e.g., system calls) in the meantime.Furthermore, we introduced a system call to forward messages upon communication attempts with suspended activities.
The kernel buuers the message to forward, schedules the recipient, and delivers the message to the recipient as soon as it is running.
Finally, we added a system call for idle notiications, upon which the kernel switches to the next ready activity or work-steals an activity from another tile in case no activity was ready.
For application activities, the kernel sets the idle delay to 20,000 cycles 4 .
For server activities, the kernel uses an idle delay of one cycle, because servers are typically only activated on demand.
Hence, switching to an application is more beneecial for the system's performance.To facilitate fast-path communication, the kernel migrates activities in two situations.
First,if two activities are scheduled on the same CU and attempt to communicate, the kernel tries to migrate the currently suspended activity to another CU.
If migration is not possible (e.g., no other compatible CU is available), the kernel instead performs a context switch from the activity that attempted the communication to the suspended activity.
Second, if an activity is idling (see § 4.5), the kernel tries to work-steal a ready activity from a compatible CU.
To detect communication attempts with suspended activities, we equipped the DTU with the ID of the current activity and added the destination activity ID to the message header.
If the destination activity ID at the recipient's DTU does not match the current activity ID, the DTU reports an error to the sender.
In this case, the sender asks the kernel to forward the message to the recipient via the forward system call.If the kernel decides to perform a context switch on a user tile, the DTU might currently be busy with a communication.
As explained in § 5.1, the DTU supports messaging channels and data channels.
Messages need to be delivered exactly once, whereas data accesses can be repeated.
To keep the DTU simple, we decided against a complicated protocol to abort potentially ongoing communication.
Instead, the DTU has an abort command, which consists of two parts.
First, further communication attempts are rejected with an error until reenabled by the kernel.
Second,the DTU waits until all messagebased communication is completed, whereas data accesses are aborted with an error and need to be repeated later.
We implemented RCTMux both for accelerators and for general-purpose cores.
As mentioned before, on accelerators, RCTMux receives a signal from the kernel if a context switch is desired.
The ASM checks for the signal only at convenient points in time, because the accelerator logic is not assumed to be interruptible.
Upon the signal, it saves the ASM's state and the accelerator's local memory via DTU to a previously allocated space in DRAM, uses the DTU's abort command, and notiies the kernel that the state has been saved.
Analogously, the state is restored upon a restore request from the kernel.We also implemented RCTMux for x86-64 as a small piece of software running in ring 0.
In this case, RCTMux is activated by an interrupt injection, saves the CPU registers, uses the DTU's abort command, and notiies the kernel.
Upon a restore request from the kernel, it restores the CPU registers and resumes a previously aborted data access, if necessary.
M 3 already features an in-memory le system, called m3fs, and a pipe server.
However, since M 3 was only evaluated on general-purpose cores (in some cases with instruction extensions), the protocols to access these OS services are not suited for accelerators.
First, M 3 uses a diierent protocol for m3fs than for the pipe server, requiring accelerators to implement multiple protocols.
Second, m3fs's protocol is based on the exchange of capabilities to obtain access to the data and requires clients to manage the le position.
In summary, the existing protocols are too complex to be implemented in hardware.
For that reason, we replaced them with the le protocol, as introduced in § 4.8.
On M 3 X, the le protocol is based on a messaging channel between client and server and a data channel to access the le data.
Since m3fs manages the le data in extents, similar to other modern le systems [40,50], the next_in request provides the client with access to the next extent of the le by asking the kernel to establish a corresponding data channel.
The le position is managed and advanced by m3fs and can also be changed by the seek request.
For appends, the next_out request allocates new space and provides the client with access to this space.
Upon commit, m3fs truncates this space, if necessary, and makes it visible to other clients.The pipe server uses a single and contiguous shared memory area in DRAM per pipe to exchange data between clients.
For that reason, the pipe server asks the kernel to conngure the client's data channel only once for the complete area and tells the clients where to read or write next.
If no data can be read or written, the pipe server delays its response to the next_in or next_out request correspondingly.
On the client side, we implemented the le protocol in software (for general-purpose cores) and in hardware (for accelerators).
The software version is part of M 3 X's standard library and allows applications to use a POSIX-like le API.
The library maps this API to the corresponding next_in, next_out, commit, and seek requests.To enable access to le-like resources for stream-processing accelerators, we implemented the le protocol as part of the accelerator support module (ASM).
The stream-processing accelerator has an input stream and an output stream, each using one messaging channel (M) to the server and one data channel (D) to access to data, as shown in Fig. 4.
Like many other accelerators [17,38,55,56,63], the computation is performed on scratchpad memory (SPM), because it allows many parallel memory accesses (indicated by the thick arrows) and has predictable access latency.The ASM loads data via the DTU from the input stream into the accelerator's SPM, activates the accelerator logic, and writes the result to the output stream.
The ASM starts in state , which checks whether the input data channel has data left to read.
If so, it directly transitions to state to read the next block of data into the SPM.
Otherwise, it sends an input request (next_in) to the input server to request access to new input data and transitions to state .
State waits until a message arrives and transitions to as soon as the reply to the input request has been received.
After the next data block has been read into the SPM, the accelerator logic is activated and the ASM transitions to state for the computation.
As soon as the computation has been completed, the ASM transitions to state .
Analogously to the input phase, rst checks whether the output data channel has space left for the result of the computation.
If so, it directly transitions to and writes the data.
Otherwise it rst requests new space from the output server (next_out).
Afterwards, the ASM transitions back to state , which repeats the procedure until the reply to an input request indicates end-of--le.
In this case, the ASM commits the written data by sending commit to the output server, if required, and transitions to state .
We believe that our architecture provides a good foundation for very heterogeneous systems, but we are aware that CUs will be diverse and have diierent requirements.
This section discusses a few examples of how our current prototype can be extended to support other use cases.Our context switching mechanism handles the simple save and restore actions on user tiles and the decision making in the M 3 X kernel.
While we show in the evaluation that context switches on xed-function accelerators have acceptable overhead, the mechanism is probably not a good t for accelerators that have a large state such as GPUs.
General-purpose cores provide native mechanisms to save/restore their state, which are used by the software version of RCTMux.
Therefore we believe that large-state accelerators need tailored context-switching mechanisms as already partially supported by modern GPUs.As described in § 5.4, the M 3 X kernel currently migrates an activity to a diierent tile if two activities on the same tile try to communicate.
This policy assumes that the communication attempt starts a series of interactions between these activities, which mostly holds true for our current workloads.
Clearly this is not the best solution in all cases.
For example, if the activities communicate just once, a local communication channel with context switching can be preferable, if supported by the compute unit.
And nally, our current prototype does not queue messages at the recipient's compute unit if the recipient is suspended, but forwards the message to the recipient via the M 3 X kernel.
We chose this solution to keep the hardware extensions small and, most importantly, minimize the burden on accelerators.
If accelerators support message queues or such queues are added externally, the number of kernel involvements can be reduced.
Thus, arguably our solution uses a queue size of zero, which can be extended to queue a few messages locally and only resort to the M 3 X kernel if the queue is full.
Our evaluation answers the following questions:• How does fast-path and forwarded communication perform?
• Do the changes to the le protocol reduce its performance?
• What is the performance impact if activities share tiles?
• What are the beneets of autonomous accelerators?
We used the gem5-based prototype platform for our evaluation.
General-purpose tiles contain a single out-of-order x86-64 core with 32 KiB L1 instruction cache, 32 KiB L1 data cache, and 256 KiB L2 cache.
The request-processing accelerators use 32 KiB L1 cache, whereas the stream-processing accelerators use 2 KiB scratchpad memory.
General-purpose cores are simulated with a 3 GHz clock frequency, whereas accelerators are clocked with 1 GHz.
All DTUs are conngured to have 16 endpoints available.
We use the DDR3_1600_8x8 model of gem5 as the physical memory, clocked at 1 GHz.
To keep the simulation times manageable, we connect the tiles via a crossbar instead of a full network-on-chip, which was suucient, because our evaluation does not require large numbers of tiles.
Due to the still long simulation times we used representative, but short-running benchmarks.
M 3 X combines fast-path communication with context switching.
In a rst step, we use micro-benchmarks to determine the costs of forwarded communication, requiring a context switch, compared to fast-path communication.
We measure the round-trip-time between activities on diierent CUs.
Fig. 5 shows the average time over 16 runs with warm caches.
The uppermost two rows show the time for stream-processing accelerators (SP), rst if the recipient is running, resulting in fast-path communication, and second if the recipient is suspended, resulting in forwarded communication.
The next two rows show the results for request-processing accelerators (RP), followed by two rows for an x86-64 core.
The nal row shows the time for a core-local round trip, using forwarded communication for both the request and the response.As the results in Fig. 5 show, fast-path communication is more than one order of magnitude faster than forwarded communication on our system.
All forwarded communication requires a forward system call, upon which the kernel performs a context switch to the receiving activity and forwards the message to the recipient.
Most of the time is spent with the actual context switch, because it requires multiple steps and is carried out partially by RCTMux and partially by the kernel.
Since stream-processing accelerators use a local scratchpad memory, the content needs to be saved and restored, leading to additional overhead.
On x86-64, the overhead is larger, because the RCTMux is implemented in software.
Finally, the core-local round trip requires two context switches.
However, it is not twice as expensive as a single context switch, because the kernel optimizes this case by omitting the idle notiication.
As described in § 5.7, we simpliied and uniied the le protocol to be hardware-friendly.
To evaluate whether these changes impact performance, we used the system call tracing infrastructure from M 3 .
It allows to run an application on Linux, trace the system calls including timing information and replay the trace on M 3 .
We used the following applications:1.
tar: creates a tar archive from les with sizes between 128 and 8192 KiB and 16 MiB in total, 2.
untar: unpacks the same archive, 3.
shasum: computes the SHA256 hash of a 512 KiB le, 4.
sort: sorts a 256 KiB le with 408 lines, 5.
nd: searches 24 directories with 40 les each, 6.
SQLite: creates a table and inserts/selects 32 entries, and 7.
LevelDB: creates a table and inserts/selects 512 entries.The applications tar, untar, shasum, sort, and nd have been taken from BusyBox 1.26.2 [1].
SQLite is an embedded and highly reliable database engine [7].
LevelDB is a light-weight and high-performance key-value store, created by Google [5].
We chose these applications to stress the system in diierent ways: tar and untar are data intensive, shasum and sort are compute intensive, nd performs many small le-system requests, and SQLite and LevelDB are mixtures of these.We used these applications to compare the performance between Linux 4.10, M 3 with the original le protocol, and M 3 X using the uniied and hardware-friendly le protocol.
In this section, M 3 and M 3 X use three dedicated x86-64 tiles (without accelerator tiles) for the application, m3fs, and the pager, whereas Linux uses a single x86-64 core.
However,M 3 do not take advantage of multiple tiles, because all cross-tile interactions are synchronous and therefore,at no point in time is useful work done in parallel.
On M 3 and M 3 X, we use extents of at most 512 KiB, requiring multiple requests to m3fs to read and write les.
On Linux, we use tmpfs as the in-memory le system.
All le systems use a block size of 4 KiB.
Fig. 6 shows the average runtime of 7 runs after one warmup run, broken down into the application time, time for data transfers, and OS overhead.
We account Linux's time for the system calls, which are unsupported 5 , as application time as well.
Since the standard deviation is below 1%, we omit error bars.In our previous work, we have already shown that dataintensive workloads like tar and untar have signiicantly less OS overhead on M 3 than on Linux when running on simple Xtensa cores.
As Fig. 6 shows, these improvements can be seen on x86-64 cores as well.
Note however, that the diierence is about a factor of two on x86-64 instead of ve as on Xtensa, because the Xtensa cores did not have a cacheline prefetcher, resulting in poor performance on Linux [10].
On both architectures, the DTU's data channel can be conngured in constant time for any byte-granular and contiguous region of memory, independent of its size.
After the channel has been established, applications access the data via DMA with almost no overhead.
Therefore, M 3 and M 3 X outperform Linux signiicantly.For the remaining applications, computation dominates the runtime, leading to smaller overall performance improvements.
Note that SQLite is slightly faster on M 3 , because the new le protocol in M 3 X currently does not provide clients with read-write access to data and SQLite often switches between reading and writing of the same le.
These switches require a commit request and a new next_in or next_out request, causing additional overhead.
After the performance comparison using three tiles, we show the performance impact when activities share tiles by means of context switching.
In the rst step, we ran both OS servers (m3fs and pager) on the same tile and in the second step, we ran the OS servers and the application on a single tile.
Fig. 7 shows the average runtime of three runs, preceded by one warmup run, normalized to the average runtime on 5 In these benchmarks, the system calls access, brk, chdir, chmod, chown, dup2, fchown, fcntl, fdatasync, futex, geteuid, getpid, getrlimit, gettimeofday, getuid, ioctl, and utimes were unsupported on M 3 /M 3 X.
The sum of the times for the ignored system calls was at most 0.4 ms. M 3 X with three tiles.
The standard deviation is less than 2%.
As the results show, using the same tile for both servers and a dedicated tile for the application (two tiles in total) has almost no performance impact.
Running servers and application on a single tile leads to a performance degradation in some cases.
For tar and untar, the runtime is increased by 17% and 12%, respectively, but M 3 X is still about twice as fast as Linux.shasum and sort show almost no performance degradation, whereas nd and SQLite experience a signiicant slowdown.
The reason is, that both nd and SQLite communicate heavily with m3fs, leading to many context switches.
The performance of LevelDB degrades slightly, but is still better than on Linux.
We conclude that some workloads require faster core-local context switches.
We could improve M 3 X by running a kernel with context-switching support directly on the core, in case the necessary hardware features are available.
After the evaluation on general-purpose cores, we want to demonstrate the beneets of autonomous accelerators.
In this section, we start with request-processing accelerators.
As described in § 4.9, software invokes the ASM, which in turn invokes the accelerator logic.
In this section, we evaluate the impact of the invocation granularity on performance and CPU wake-up frequency.
We simulate the accelerators using gem5-Aladdin [56], which is a power-performance accelerator modeling framework that can be used to explore the design space for xed-function accelerators.
gem5-Aladdin simulates the accelerator logic and uses the memory subsystem of gem5 to perform memory accesses.
gem5-Aladdin achieves an error of less than 6% for the accelerator's performance compared to real hardware.
We adapted gem5-Aladdin to be invoked by the ASM and to notify the ASM of completions.To use a request-processing accelerator, an application creates an activity for the desired accelerator, creates the memory mappings for the input and output data in the activity's virtual address space, and establishes the communication channel to invoke the ASM.
In this case, the input data is stored in les and the output data should be written to les as well.
Therefore, these les are mapped into the virtual address space of the accelerator activity.We use diierent accelerator workloads from MachSuite [49].
MachSuite has been analyzed by gem5-Aladdin with the result that some accelerators beneet from DMAbased memory access and others beneet from cache-based memory access.
For this evaluation,we picked the accelerators that beneet from cache-based memory access:1.
Stencil-3D: a three-dimensional stencil computation, 2.
MD-KNN: a -nearest-neighbor computation from molecular dynamics, 3.
FFT-1D: a one-dimensional fast Fourier transform, and 4.
SPMV: a sparse matrix-vector multiplication.We adjusted each accelerator to perform a single indivisible step per invocation by the ASM.
Multiple such invocations are batched in a single invocation by the CPU.
We analyze the spectrum between assisted and autonomous operation by varying the batch size.
Fig. 8 shows the results from three runs after one warmup run with batch sizes of 1 to 256.
Performing all invocations in a single batch is shown as 'N', because the total number of invocations depends on the workload.
The standard deviation is less than 1%.
As can be seen in the left part of the gure, larger batch sizes lead to better performance.
More importantly, the right part of the gure shows the average (bars) and maximum (lines) accelerator execution times for each ASM invocation.
For example, using the MD workload and a batch size of 16 shows acceptable performance, but leads to a context switching latency of 48 µs and the CPU is woken up every 8 µs on average.
High wake-up frequencies are a problem on modern cores, which can only achieve signiicant power savings in deep sleep states.
However, the deeper the sleep state, the longer the time to bring the core back into a functional state (e.g., on Intel's Haswell generation, dozens of microseconds to leave C6 and up to several milliseconds to leave C10 [32,54]).
Hence, deeper sleep states are only beneecial during longer idle periods.M 3 X performs all accelerator invocations in a single batch and uses an ASM that is interruptible between invocations to get the best of both worlds: On the one hand, a single batch leads to the best performance.
On the other hand, the ne-grained interruptibility allows to context-switch to a more important activity with a low latency.
Note that an immediate interruption can be achieved by resetting the accelerator logic, but requires to repeat the last step of the computation.
If all invocations are done by the ASM in hardware (autonomous), the accelerator needs to repeat only a single indivisible step.
If all invocations are done in software (assisted), the accelerator needs to repeat as many steps as the performance and energy constraints allow.
Finally, we want to show the beneets of autonomous stream-processing accelerators.
Stream processing is used in various domains such as mobile communication, image processing, and audio processing.
In this work, we consider an image processing scenario as is imaginable in data centers, similar to Google's TPU [27] workloads.
The cloud provider ooers a set of image-processing accelerators as a service and allows customers to perform large-scale image processing on these accelerators.
An eecient method for large images is FFT convolution [42], which rst performs a 2D fast Fourier transform (FFT) on the input image, then multiplies the result pointwise with an image lter, and nally performs the inverse FFT.
Depending on the lter, FFT convolution can be used, for example, for edge detection or low-pass ltering.To evaluate this scenario, we use three types of accelerators called FFT, MUL, and IFFT.
Each accelerator has 2 KiB (the block size for the 32×32 point FFT) of local scratchpad memory (SPM) and uses the le protocol (see § 5.8) to stream the data block-wise from the input stream via the SPM to the output stream.
Due to the deterministic execution model of these accelerators, we did not use gem5-Aladdin to simulate the accelerator logic, but used Aladdin [55] to determine the computation times ooine.
To get reasonable results, we generated all sensible conngurations and picked the sweet spot between performance and the product of chip area and powerconsumption.
We obtained 5,856 cycles for FFT and IFFT and 1,189 cycles for MUL.
To put these numbers into perspective, the FFT/IFFT accelerator is about three times as fast as a simple software implementation and Aladdin reports a three orders of magnitude lower power consumption than a typical modern x86 core.These three types of accelerators run activities that form an FFT-MUL-IFFT chain to process a 4 MiB image le and store the resulting image as a le.
In our rst experiment, we run 1 to 4 such chains simultaneously without context switching, thus using 1 to 4 instances of each accelerator type.
To show the benets of autonomous accelerators, we compare M 3 X's autonomous approach with the assisted approach.
The assisted approach drives the accelerators from software using a single general-purpose core.
Hence, software loads the input data into the SPM, starts the accelerator via a message, and asks the accelerator's DTU to move the result from the SPM to the next accelerator or to the output le.
The autonomous variant connects the DTU endpoints of the accelerators as follows: The input of the rst and the output of the last accelerator are connected to a le.
The output of the rst and second accelerator are directly pushed to the successor.We simulate two ways to attach accelerators to the system: network-on-chip and PCI Express (PCIe).
The former leads to superior performance due to lower latency, whereas the latter allows a exible combination of independently developed components.
The result for the NoC version is depicted in Fig. 9, whereas the PCIe version is depicted in Fig. 10.
We show the overall runtime (a) and the CPU time spent to drive the accelerators (b), depending on the number of accelerator chains using three runs, preceded by one warmup run.
The standard deviation is below 3%.
We simulate the PCIe-attached add-on card by connecting the accelerators via a bridge with a delay of 500 ns to the host system, which is the typical one-way latency for PCIe gen 3 [20,23,30,53].
In other words, we do not simulate a complete PCIe interconnect, but only the latency PCIe introduces.
The one-way latency within the NoC is about 10 ns.
In both cases, the DRAM is part of the host system and stores the in-memory le system.
Using the assisted approach leads to slightly worse overall runtime with an increasing number of accelerator chains when integrating the accelerators into the NoC.
With PCIe, the overall runtime increases signiicantly, leading to a slowdown of factor 4.7.
In contrast, the autonomous approach always achieves the same runtime, independent of the number of chains.
More importantly, the assisted approach keeps the CPU busy most of the time.
Within the NoC,the CPU is utilized 100% of the time starting at four accelerator chains, whereas with PCIe, the CPU is already fully utilized starting with two chains.
The autonomous approach does not cause signiicant CPU load in either case.
Additionally, Fig. 10 shows that the autonomous approach outperforms the assisted approach for PCIe-based accelerators even if the assisted approach does not fully utilize the CPU.
The reason is the 500 ns delay when communicating with the accelerators, which prevents the assisted approach from fully utilizing the accelerators.Finally, we evaluate the context switching overhead when two chains of activities compete for the same accelerators.
In this case, we use only the autonomous approach and put each chain of activities into the same gang to beneet from gang scheduling.
Plot (c) in Fig. 9 and Fig. 10 shows the context switching overhead by comparing the runtime of two activity chains running consecutively with the runtime of two chains running interleaved.
We vary the time slice length for context switching between 1 ms and 4 ms. As the results show, using a still rather short time slice of 4 ms leads to less than 0.9% overhead with accelerators integrated into the NoC and less than 2.9% overhead when attaching them via PCIe.Note that the performance can still be improved for both the assisted and the autonomous approach.
For the assisted approach, batching could be used to reduce the interaction frequency with the accelerators.
However, batching is only possible by increasing the SPM sizes of the accelerators, which is expensive in terms of area and energy and increases the time the accelerator is not interruptible.
Additionally, the assisted approach can trade more CPU time for more accelerator performance by using multiple cores to drive the accelerators, until the PCIe bus becomes the bottleneck.
The autonomous approach does not suuer from the trade-oo between SPM size and CPU utilization and can further improve performance by overlapping data transfers to the DRAM instead of issuing one transfer at a time.
In this work, we presented M 3 X, which combines fast-path communication, bypassing the kernel, with context switching and thereby enables autonomous accelerators.
To this end, we re-evaluated the boundary between hardware and software.
We found that (1) introducing a hardware-friendly le protocol enables accelerators to autonomously access le systems ornetwork stacks,(2) performing potentially complex scheduling decisions in software and simple save and restore actions in hardware allows to context switch accelerators, and (3) attaching a simple hardware component to the accelerator logic allows to combine autonomous operation and interruptibility.We demonstrated in our evaluation that M 3 X retains the performance advantages of M 3 's fast-path communication, while using the system's resources more eeciently by performing context switches, if required.
Additionally, we have shown that running PCIe-attached image processing accelerators autonomously achieves a speedup of 4.7 and reduces the CPU utilization by a factor of 30.
In future work, we plan to study other types of accelerators such as FPGAs and GPUs and apply our insights from simulation to real hardware.
We would like to thank our shepherd, Christopher Rossbach, and the anonymous reviewers for their helpful suggestions.
This work was funded by the German Research Council DFG through the Cluster of Excellence Center for Advancing Electronics Dresden (cfaed) and by public funding of the state of Saxony/Germany.
