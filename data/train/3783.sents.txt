Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms.
We propose a novel discriminative training method that projects the raw term vectors into a common , low-dimensional vector space.
Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high-dimensional space.
Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches , but also achieves high accuracy at low dimensions and is thus more efficient.
Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities.
For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998;Agirre et al., 2009).
Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009).
Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008).
In all these applications, the vector-based similarity method is the most widely used.
Term vectors are first constructed to represent the original text objects, where each term is associated with a weight indicating its importance.
A pre-selected function operating on these vectors, such as cosine, is used to output the final similarity score.
This approach has not only proved to be effective, but is also efficient.
For instance, only the term vectors rather than the raw data need to be stored.
A pruned inverse index can be built to support fast similarity search.However, the main weakness of this term-vector representation is that different but semantically related terms are not matched and cannot influence the final similarity score.
As an illustrative example, suppose the two compared term-vectors are: {purchase:0.4, used:0.3, automobile:0.2} and {buy:0.3, pre-owned: 0.5, car: 0.4}.
Even though the two vectors represent very similar concepts, their similarity score will be 0, for functions like cosine, overlap or Jaccard.
Such an issue is more severe in cross-lingual settings.
Because language vocabularies typically have little overlap, term-vector representations are completely inapplicable to measuring similarity between documents in different languages.
The general strategy to handle this problem is to map the raw representation to a common concept space, where extensive approaches have been proposed.
Existing methods roughly fall into three categories.
Generative topic models like Latent Dirichlet Allocation (LDA) ( Blei et al., 2003) assume that the terms are sampled by probability distributions governed by hidden topics.
Linear projection methods like Latent Semantic Analysis (LSA) (Deerwester et al., 1990) learn a projection matrix and map the original term-vectors to the dense low-dimensional space.
Finally, metric learning approaches for high-dimensional spaces have also been proposed ( Davis and Dhillon, 2008).
In this paper, we propose a new projection learning framework, Similarity Learning via Siamese Neural Network (S2Net), to discriminatively learn the concept vector representations of input text objects.
Following the general Siamese neural network architecture ( Bromley et al., 1993), our approach trains two identical networks concurrently.
The input layer corresponds to the original term vector and the output layer is the projected concept vector.
Model parameters (i.e., the weights on the edges) are equivalently the projection matrix.
Given pairs of raw term vectors and their labels (e.g., similar or not), the model is trained by minimizing the loss of the similarity scores of the output vectors.
S2Net is closely related to the linear projection and metric learning approaches, but enjoys additional advantages over existing methods.
While its model form is identical to that of LSA, CCA and OPCA, its objective function can be easily designed to match the true evaluation metric of interest for the target task, which leads to better performance.
Compared to existing high-dimensional metric learning methods, S2Net can learn from a much larger number of labeled examples.
These two properties are crucial in helping S2Net outperform existing methods.
For retrieving comparable cross-lingual documents, S2Net achieves higher accuracy than the best approach (OPCA) at a much lower dimension of the concept space (500 vs. 2,000).
In a monolingual setting, where the task is to judge the relevance of an ad landing page to a query, S2Net also has the best performance when compared to a number of approaches, including the raw TFIDF cosine baseline.In the rest of the paper, we first survey some existing work in Sec. 2, with an emphasis on approaches included in our experimental comparison.
We present our method in Sec. 3 and report on an extensive experimental study in Sec. 4.
Other related work is discussed in Sec. 5 and finally Sec. 6 concludes the paper.
In this section, we briefly review existing approaches for mapping high-dimensional termvectors to a low-dimensional concept space.
Latent Semantic Analysis (PLSA) (Hofmann, 1999) assumes that each document has a document-specific distribution θ over some finite number K of topics, where each token in a document is independently generated by first selecting a topic z from a multinomial distribution MULTI(θ), and then sampling a word token from the topic-specific word distribution for the chosen topic MULTI(φ z ).
Latent Dirichlet Allocation (LDA) ( Blei et al., 2003) generalizes PLSA to a proper generative model for documents and places Dirichlet priors over the parameters θ and φ.
In the experiments in this paper, our implementation of PLSA is LDA with maximum a posteriori (MAP) inference, which was shown to be comparable to the current best Bayesian inference methods for LDA (Asuncion et al., 2009).
Recently, these topic models have been generalized to handle pairs or tuples of corresponding documents, which could be translations in multiple languages, or documents in the same language that are considered similar.
For instance, the Poly-lingual Topic Model (PLTM) ( Mimno et al., 2009) is an extension to LDA that views documents in a tuple as having a shared topic vector θ.
Each of the documents in the tuple uses θ to select the topics z of tokens, but could use a different (languagespecific) word-topic-distribution MULTI(φ L z ).
Two additional models, Joint PLSA (JPLSA) and Coupled PLSA (CPLSA) were introduced in ( Platt et al., 2010).
JPLSA is a close variant of PLTM when documents of all languages share the same word-topic distribution parameters, and MAP inference is performed instead of Bayesian.
CPLSA extends JPLSA by constraining paired documents to not only share the same prior topic distribution θ, but to also have similar fractions of tokens assigned to each topic.
This constraint is enforced on expectation using posterior regularization ( Ganchev et al., 2009).
The earliest method for projecting term vectors into a low-dimensional concept space is Latent Semantic Analysis (LSA) (Deerwester et al., 1990).
LSA models all documents in a corpus using a n × d document-term matrix D and performs singular value decomposition (SVD) on D.
The k biggest singular values are then used to find the d × k projection matrix.
Instead of SVD, LSA can be done by applying eigen-decomposition on the correlation matrix between terms C = D T D.
This is very similar to principal component analysis (PCA), where a covariance matrix between terms is used.
In practice, term vectors are very sparse and their means are close to 0.
Therefore, the correlation matrix is in fact close to the covariance matrix.To model pairs of comparable documents, LSA/PCA has been extended in different ways.
For instance, Cross-language Latent Semantic Indexing (CL-LSI) ( Dumais et al., 1997) applies LSA to concatenated comparable documents from different languages.
Oriented Principal Component Analysis (OPCA) ( Diamantaras and Kung, 1996;Platt et al., 2010) solves a generalized eigen problem by introducing a noise covariance matrix to ensure that comparable documents can be projected closely.
Canonical Correlation Analysis (CCA) ( Vinokourov et al., 2003) finds projections that maximize the crosscovariance between the projected vectors.
Measuring the similarity between two vectors can be viewed as equivalent to measuring their distance, as the cosine score has a bijection mapping to the Euclidean distance of unit vectors.
Most work on metric learning learns a Mahalanobis distance, which generalizes the standard squared Euclidean distance by modeling the similarity of elements in different dimensions using a positive semi-definite matrix A. Given two vectors x and y, their squared Mahalanobis distance is:d A = (x − y) T A(x − y).
However, the computational complexity of learning a general Mahalanobis matrix is at least O(n 2 ), where n is the dimensionality of the input vectors.
Therefore, such methods are not practical for high dimensional problems in the text domain.In order to tackle this issue, special metric learning approaches for high-dimensional spaces have been proposed.
For example, high dimension low-rank (HDLR) metric learning ( Davis and Dhillon, 2008) constrains the form of A = UU T , where U is similar to the regular projection matrix, and adapts information-theoretic metric learning (ITML) ( Davis et al., 2007) to learn U. sim(v p ,v q ) 1 t d t v p v q i t 1 c k c j c ' tw tw Network (S2Net)Given pairs of documents with their labels, such as binary or real-valued similarity scores, our goal is to construct a projection matrix that maps the corresponding term-vectors into a low-dimensional concept space such that similar documents are close when projected into this space.
We propose a similarity learning framework via Siamese neural network (S2Net) to learn the projection matrix directly from labeled data.
In this section, we introduce its model design and describe the training process.
The network structure of S2Net consists of two layers.
The input layer corresponds to the raw term vector, where each node represents a term in the original vocabulary and its associated value is determined by a term-weighting function such as TFIDF.
The output layer is the learned low-dimensional vector representation that captures relationships among terms.
Similarly, each node of the output layer is an element in the new concept vector.
In this work, the final similarity score is calculated using the cosine function, which is the standard choice for document similarity ( Manning et al., 2008).
Our framework can be easily extended to other similarity functions as long as they are differentiable.
The output of each concept node is a linear com-bination of the weights of all the terms in the original term vector.
In other words, these two layers of nodes form a complete bipartite graph as shown in Fig. 1.
The output of a concept node c j is thus defined as:tw (c j ) = t i ∈V α ij · tw(t i )(1)Notice that it is straightforward to add a non-linear activation function (e.g., sigmoid) in Eq.
(1), which can potentially lead to better results.
However, in the current design, the model form is exactly the same as the low-rank projection matrix derived by PCA, OPCA or CCA, which facilitates comparison to alternative projection methods.
Using concise matrix notation, let f be a raw d-by-1 term vector, A = [α ij ] d×k the projection matrix.
g = A T f is thus the k-by-1 projected concept vector.
For a pair of term vectors f p and f q , their similarity score is defined by the cosine value of the corresponding concept vectors g p and g q according to the projection matrix A.sim A (f p , f q ) = g T p g q ||g p ||||g q || ,where g p = A T f p and g q = A T f q .
Let y pq be the true label of this pair.
The loss function can be as simple as the mean-squared error 1 2 (y pq − sim A (f p , f q )) 2 .
However, in many applications, the similarity scores are used to select the closest text objects given the query.
For example, given a query document, we only need to have the comparable document in the target language ranked higher than any other documents.
In this scenario, it is more important for the similarity measure to yield a good ordering than to match the target similarity scores.
Therefore, we use a pairwise learning setting by considering a pair of similarity scores (i.e., from two vector pairs) in our learning objective.Consider two pairs of term vectors (f p 1 , f q 1 ) and (f p 2 , f q 2 ), where the first pair has higher similarity.
Let ∆ be the difference of their similarity scores.
Namely, ∆ = sim A (f p 1 , f q 1 ) − sim A (f p 2 , f q 2 ).
We use the following logistic loss over ∆, which upperbounds the pairwise accuracy (i.e., 0-1 loss):L(∆; A) = log(1 + exp(−γ∆))(2)Because of the cosine function, we add a scaling factor γ that magnifies ∆ from [−2, 2] to a larger range, which helps penalize more on the prediction errors.
Empirically, the value of γ makes no difference as long as it is large enough 1 .
In the experiments, we set the value of γ to 10.
Optimizing the model parameters A can be done using gradient based methods.
We derive the gradient of the whole batch and apply the quasi-Newton optimization method L-BFGS ( Nocedal and Wright, 2006) directly.
For a cleaner presentation, we detail the gradient derivation in Appendix A. Given that the optimization problem is not convex, initializing the model from a good projection matrix often helps reduce training time and may lead to convergence to a better local minimum.
Regularization can be done by adding a term β 2 ||A − A 0 || 2 in Eq.
(2), which forces the learned model not to deviate too much from the starting point (A 0 ), or simply by early stopping.
Empirically we found that the latter is more effective and it is used in the experiments.
We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures.
With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents.
For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005).
Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998;Rapp, 1999).
In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language.
We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper.
This data set consists of Wikipedia documents in two languages, English and Spanish.
An article in English is paired with a Spanish article if they are identified as comparable across languages by the Wikipedia community.
To conduct a fair comparison, we use the same term vectors and data split as in the previous study.
The numbers of document pairs in the training/development/testing sets are 43,380, 8,675 and 8,675, respectively.
The dimensionality of the raw term vectors is 20,000.
The models are evaluated by using each English document as query against all documents in Spanish and vice versa; the results from the two directions are averaged.
Performance is evaluated by two metrics: the Top-1 accuracy, which tests whether the document with the highest similarity score is the true comparable document, and the Mean Reciprocal Rank (MRR) of the true comparable.When training the S2Net model, all the comparable document pairs are treated as positive examples and all other pairs are used as negative examples.
Naively treating these 1.8 billion pairs (i.e., 43380 2 ) as independent examples would make the training very inefficient.
Fortunately, most computation in deriving the batch gradient can be reused via compact matrix operations and training can still be done efficiently.
We initialized the S2Net model using the matrix learned by OPCA, which gave us the best performance on the development set 2 .
Our approach is compared with most methods studied in (Platt et al., 2010), including the best performing one.
For CL-LSI, OPCA, and CCA, we include results from that work directly.
In addition, we re-implemented and improved JPLSA and CPLSA by changing three settings: we used separate vocabularies for the two languages as in the Poly-lingual topic model ( Mimno et al., 2009), we performed 10 EM iterations for folding-in instead of only one, and we used the Jensen-Shannon distance instead of the L1 distance.
We also attempted to apply the HDLR algorithm.
Because this algorithm does not scale well as the number of training examples increases, we used 2,500 positive and 2,500 negative document pairs for training.
Unfortunately, among all the 2 S2Net outperforms OPCA when initialized from a random or CL-LSI matrix, but with a smaller gain.
For example, when the number of dimensions is 1000, the MRR score of OPCA is 0.7660.
Starting from the CL-LSI and OPCA matrices, the MRR scores of S2Net are 0.7745 and 0.7855, respectively.
hyper-parameter settings we tested, HDLR could not outperform its initial model, which was the OPCA matrix.
Therefore we omit these results.
Fig. 2 shows the MRR performance of all methods on the development set, across different dimensionality settings of the concept space.
As can be observed from the figure, higher dimensions usually lead to better results.
In addition, S2Net consistently performs better than all other methods across different dimensions.
The gap is especially large when projecting input vectors to a low-dimensional space, which is preferable for efficiency.
For instance, using 500 dimensions, S2Net already performs as well as OPCA with 2000 dimensions.
Table 1 shows the averaged Top-1 accuracy and MRR scores of all methods on the test set, where the dimensionality for each method is optimized on the development set (Fig. 2).
S2Net clearly outperforms all other methods and the difference in terms of accuracy is statistically significant 3 .
Paid search advertising is the main revenue source that supports modern commercial search engines.
To ensure satisfactory user experience, it is important to provide both relevant ads and regular search results.
Previous work on ad relevance focuses on constructing appropriate term-vectors to represent queries and ad-text ( Broder et al., 2008;Choi et al., 2010).
In this section, we extend the work in (Yih and Jiang, 2010) and show how S2Net can exploit annotated query-ad pairs to improve the vector representation in this monolingual setting.
The ad relevance dataset we used consists of 12,481 unique queries randomly sampled from the logs of the Bing search engine.
For each query, a number of top ranked ads are selected, which results in a total number of 567,744 query-ad pairs in the dataset.
Each query-ad pair is manually labeled as same, subset, superset or disjoint.
In our experiment, when the task is a binary classification problem, pairs labeled as same, subset, or superset are considered relevant, and pairs labeled as disjoint are considered irrelevant.
When pairwise comparisons are needed in either training or evaluation, the relevance order is same > subset = superset > disjoint.
The dataset is split into training (40%), validation (30%) and test (30%) sets by queries.Because a query string usually contains only a few words and thus provides very little content, we applied the same web relevance feedback technique used in (Broder et al., 2008) to create "pseudodocuments" to represent queries.
Each query in our data set was first issued to the search engine.
The result page with up to 100 snippets was used as the pseudo-document to create the raw term vectors.
On the ad side, we used the ad landing pages instead of the short ad-text.
Our vocabulary set contains 29,854 words and is determined using a document frequency table derived from a large collection of Web documents.
Only words with counts larger than a pre-selected threshold are retained.How the data is used in training depends on the model.
For S2Net, we constructed preference pairs in the following way.
For the same query, each relevant ad is paired with a less relevant ad.
The loss function from Eq.
(2) encourages achieving a higher similarity score for the more relevant ad.
For HDLR, we used a sample of 5,000 training pairs of queries and ads, as it was not able to scale to more training examples.
For OPCA, CCA, PLSA and JPLSA, we constructed a parallel corpus using only relevant pairs of queries and ads, as the negative examples (irrelevant pairs of queries and ads) cannot be used by these models.
Finally, PCA and PLSA learn the models from all training queries and documents without using any relevance information.We tested S2Net and other methods in two different application scenarios.
The first is to use the ad relevance measure as an ad filter.
When the similarity score between a query and an ad is below a preselected decision threshold, this ad is considered irrelevant to the query and will be filtered.
Evaluation metrics used for this scenario are the ROC analysis and the area under the curve (AUC).
The second one is the ranking scenario, where the ads are selected and ranked by their relevance scores.
In this scenario, the performance is evaluated by the standard ranking metric, Normalized Discounted Cumulative Gain (NDCG) (Jarvelin and Kekalainen, 2000).
We first compare different methods in their AUC and NDCG scores.
TFIDF is the basic term vector representation with the TFIDF weighting (tf · log(N/df )).
It is used as our baseline and also as the raw input for S2Net, HDLR and other linear projection methods.
Based on the results on the development set, we found that PCA performs better than OPCA and CCA.
Therefore, we initialized the models of S2Net and HDLR using the PCA matrix.
Table 2 summarizes results on the test set.
All models, except TFIDF, use 1000 dimensions and their best configuration settings selected on the validation set.TFIDF is a very strong baseline on this monolingual ad relevance dataset.
Among all the methods we tested, at dimension 1000, only S2Net outperforms the raw TFIDF cosine measure in every evaluation metric, and the difference is statistically sig- When the cosine scores of these vector representations are used as ad filters, their ROC curves (focusing on the low false-positive region) are shown in Fig. 3.
It can be clearly observed that the similarity score computed based on vectors derived from S2Net indeed has better quality, compared to the raw TFIDF representation.
Unfortunately, other approaches perform worse than TFIDF and their performance in the low false-positive region is consistent with the AUC scores.Although ideally we would like the dimensionality of the projected concept vectors to be as small 4 For AUC, we randomly split the data into 50 subsets and ran a paired-t test between the corresponding AUC scores.
For NDCG, we compared the DCG scores per query of the compared models using the paired-t test.
The difference is considered statistically significant when the p-value is less than 0.01.
as possible for efficient processing, the quality of the concept vector representation usually degrades as well.
It is thus interesting to know the best tradeoff point between these two variables.
Table 3 shows the AUC and NDCG scores of S2Net at different dimensions, as well as the results achieved by TFIDF and PCA, HDLR and CPLSA at 1000 dimensions.
As can be seen, S2Net surpasses TFIDF in AUC at dimension 300 and keeps improving as the dimensionality increases.
Its NDCG scores are also consistently higher across all dimensions.
It is encouraging to find that S2Net achieves strong performance in two very different tasks, given that it is a conceptually simple model.
Its empirical success can be attributed to two factors.
First, it is flexible in choosing the loss function and constructing training examples and is thus able to optimize the model directly for the target task.
Second, it can be trained on a large number of examples.
For example, HDLR can only use a few thousand examples and is not able to learn a matrix better than its initial model for the task of cross-lingual document retrieval.
The fact that linear projection methods like OPCA/CCA and generative topic models like JPLSA/CPLSA cannot use negative examples more effectively also limits their potential.In terms of scalability, we found that methods based on eigen decomposition, such as PCA, OPCA and CCA, take the least training time.
The complexity is decided by the size of the covariance matrix, which is quadratic in the number of dimensions.
On a regular eight-core server, it takes roughly 2 to 3 hours to train the projection matrix in both experiments.
The training time of S2Net scales roughly linearly to the number of dimensions and training examples.
In each iteration, performing the projection takes the most time in gradient derivation, and the complexity is O(mnk), where m is the number of distinct term-vectors, n is the largest number of non-zero elements in the sparse term-vectors and k is the dimensionality of the concept space.
For cross-lingual document retrieval, when k = 1000, each iteration takes roughly 48 minutes and about 80 iterations are required to convergence.
Fortunately, the gradient computation is easily parallelizable and further speed-up can be achieved using a cluster.
Although the high-level design of S2Net follows the Siamese architecture ( Bromley et al., 1993;Chopra et al., 2005), the network construction, loss function and training process of S2Net are all different compared to previous work.
For example, targeting the application of face verification, Chopra et al. (2005) used a convolutional network and designed a contrastive loss function for optimizing a Eucliden distance metric.
In contrast, the network of S2Net is equivalent to a linear projection matrix and has a pairwise loss function.
In terms of the learning framework, S2Net is closely related to several neural network based approaches, including autoencoders ( Hinton and Salakhutdinov, 2006) and finding low-dimensional word representations (Col- lobert and Weston, 2008;Turian et al., 2010).
Architecturally, S2Net is also similar to RankNet (Burges et al., 2005), which can be viewed as a Siamese neural network that learns a ranking function.
The strategy that S2Net takes to learn from labeled pairs of documents can be analogous to the work of distance metric learning.
Although high dimensionality is not a problem to algorithms like HDLR, it suffers from a different scalability issue.
As we have observed in our experiments, the algorithm can only handle a small number of similarity/dissimilarity constraints (i.e., the labeled examples), and is not able to use a large number of examples to learn a better model.
Empirically, we also found that HDLR is very sensitive to the hyperparameter settings and its performance can vary substantially from iteration to iteration.Other than the applications presented in this paper, concept vectors have shown useful in traditional IR tasks.
For instance, Egozi et al. (2008) use explicit semantic analysis to improve the retrieval recall by leveraging Wikipedia.
In a companion paper, we also demonstrated that various topic models including S2Net can enhance the ranking function ( Gao et al., 2011).
For text categorization, similarity between terms is often encoded as kernel functions embedded in the learning algorithms, and thus increase the classification accuracy.
Representative approaches include latent semantic kernels (Cris- tianini et al., 2002), which learns an LSA-based kernel function from a document collection, and work that computes term-similarity based on the linguistic knowledge provided by WordNet ( Basili et al., 2005;Bloehdorn and Moschitti, 2007).
In this paper, we presented S2Net, a discriminative approach for learning a projection matrix that maps raw term-vectors to a low-dimensional space.
Our learning method directly optimizes the model so that the cosine score of the projected vectors can become a reliable similarity measure.
The strength of this model design has been shown empirically in two very different tasks.
For cross-lingual document retrieval, S2Net significantly outperforms OPCA, which is the best prior approach.
For ad selection and filtering, S2Net also outperforms all methods we compared it with and is the only technique that beats the raw TFIDF vectors in both AUC and NDCG.The success of S2Net is truly encouraging, and we would like to explore different directions to further enhance the model in the future.
For instance, it will be interesting to extend the model to learn nonlinear transformations.
In addition, since the pairs of text objects being compared often come from different distributions (e.g., English documents vs. Spanish documents or queries vs. pages), learning two different matrices instead of one could increase the model expressivity.
Finally, we would like to apply S2Net to more text similarity tasks, such as word similarity and entity recognition and discovery.
nThe gradient of the loss function in Eq.
(2) can be derived as follows.where g p = A T f p and g q = A T f q are the projected concept vectors of f q and f q .
The gradient of the cosine score can be further derived in the following steps.Let a, b, c be g T p g q , 1/g p and 1/g q , respectively.
The gradient of the loss function in Eq.
(2) can be derived as follows.where g p = A T f p and g q = A T f q are the projected concept vectors of f q and f q .
The gradient of the cosine score can be further derived in the following steps.Let a, b, c be g T p g q , 1/g p and 1/g q , respectively.
