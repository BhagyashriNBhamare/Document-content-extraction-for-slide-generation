A number of researchers have studied delta compression techniques for improving the efficiency of web page accesses over slow communication links.
Most of these schemes exploit the fact that updated web pages often change only very slightly, thus resulting in very small sizes for the transmitted deltas.
However, these schemes are only applicable to a minority of page accesses, and require web or proxy servers to retain potentially many different outdated versions of pages for use as reference files in the encoding.
Another approach, studied by Chan and Woo [4], encodes a page with respect to similar files located on the same web server that are already in the client's browser cache.
Based on the latter approach, we study different delta compression policies for web access.
Our emphasis is on web and proxy server-friendly policies that do not require the maintenance of multiple older versions of a page, but only use reference files accessed by the client within the last few minutes.
We compare several policies for identifying appropriate reference files and evaluate their performance on a set of traces.
We show that there are very simple policies that achieve significant benefits over gzip compression on most web accesses, and that can be efficiently implemented at web or proxy servers.
We also investigate the potential of file synchronization techniques such as rsync [28] for efficient web access.
Delta compression (delta encoding) is the process of encoding a target file with respect to one or several, usually similar, reference files.
This encoding, called a delta, describes the target file in terms of the reference files, and a recipient that receives the encoding and already knows the reference files can thus efficiently reconstruct the target.
Delta compression has numerous applications in scenarios where there are several versions of a file or many similar files, such as software revision control systems, distribution of software updates, content distribution networks, or efficient storage of re-¡ Contact author.
Email: suel@poly.edu lated files.
Several tools for delta compression, such as bdiff, vcdiff [10,13], Xdelta [14], and zdelta [25], are freely available.
We refer to [23] for an overview of delta compression techniques and applications.
A number of authors have proposed the use of delta compression techniques to improve the efficiency of web access [1,4,7,9,16,17,21,27,29].
In particular, when web pages are updated, they typically do not change by much, and thus delta compression can be used to very succinctly encode the difference between a new version of a web page and an outdated version already in the client's browser cache.
Most proposals focus on encodings between different versions located at the same URL, which results in small sizes for the deltas but is restricted to pages that have been previously visited by the client.
One exception is the work by Chan and Woo [4], which proposes to use as reference files other pages on the same site recently visited by the client, which tend to have a significant degree of similarity due to common layout features and HTML structure.
In general, delta compression schemes for web access can be distinguished along the following axes: ¢ End-to-end vs. proxy-based: A recent proposal [16] discusses how to integrate delta compression into the HTTP/1.1 standard.
This enables end-toend use of delta compression techniques between web servers and end clients, potentially leading to significant savings in bandwidth over the internet backbone.
On the other hand, proxy-based schemes allow savings over a bottleneck link, say a dialup connection of an end user, without requiring changes in the HTTP/1.1 protocol to be adopted by millions of servers.
A common architecture is the dual proxy architecture, where a pair of proxies, one located on each side of the bottleneck link, use a usually proprietary protocol incorporating compression, image transcoding, and various other optimizations to increase performance over this link.
A number of such systems have for example been deployed by the major cellular network providers, supplied by companies such as Bytemobile, Venturi Wireless, Slipstream Data, and others.
In this paper, we focus on such dual-proxy architectures, and we do not discuss in detail how to integrate our approach with existing HTTP standards.
¢ Standard vs. optimistic delta: In the standard approach, both client and proxy have the same old version of a page.
A proxy first waits for the updated version of the page to arrive from the server, and then transmits the delta between this and the old version to the client.
In the optimistic delta approach in [1] only the proxy needs to hold the old version.
Upon receiving a request, the proxy immediately starts sending the old version to the client, while waiting for the new version to arrive from the server; later, a delta between the versions is sent to the client.
In a proxy-based environment, neither approach decreases the amount of traffic between server and proxy.
The standard approach reduces the amount of data sent from proxy to client, thus reducing delay for low-bandwidth links.
The optimistic delta approach does not reduce the amount of data sent to the client, but may decrease total delay in cases where server response delays are also significant compared with the transmission time over the bottleneck link.
We focus on the standard approach.
¢ Same URL vs. different URLs: As described, we can limit delta compression to different versions of the same URL, or allow compression between pages corresponding to different URLs.
In the latter case, clients and proxies need to choose appropriate reference pages, and this is our main focus.
¢ Delta compression vs. file synchronization: In the standard delta compression scenario, the proxy needs a copy of the old and the new version to compute a delta.
File synchronization techniques such as rsync [28], on the other hand, allow the proxy to send a delta without knowing the old version in the client cache, based on only a set of hash values sent by the client as part of the request.
File synchronization can be seen as a special, more restricted, case of delta compression, and usually achieves compression ratios that are significantly worse than those of the best delta compression tools [23], particularly on files that share only short common substrings.
(Formally, any file synchronization protocol can be used to produce a delta by storing all messages sent from target file to reference file.)
We consider both techniques.
Delta compression between different versions of the same page typically achieves a high compression ratio, but suffers from two major shortcomings.
First, it only gives benefits for pages that have been previously visited and that have since been updated.
A 1996 study [17] found that this only applies to about ¢ ¡ ¤ £ of page accesses.
Second, it imposes significant costs on the proxy, or the server in an end-to-end approach, which needs to retain older versions of each web page for potential use as reference files in future accesses, possibly for a significant amount of time.
Also, additional disk accesses may be required to fetch the reference files.Delta compression between different pages typically achieves a more moderate compression ratio.
One problem is how to identify appropriate reference files for a requested page.
A simple scheme for doing this is proposed in [4], based on the directory paths of the URLs.
However, the scheme requires several reference files for best compression, which could seriously slow down the throughput of the proxy due to the costs of fetching and processing the reference files for each requested file.A preliminary evaluation of rsync for web access was performed in [27] as part of the rproxy project.
However, results are limited to different versions of the same URL, and only a few numbers are provided.
As mentioned, file synchronization achieves a more limited compression ratio than delta compression.
For the case of related files, the result might not be better at all than using gzip, due to the more limited similarity between the files.
File synchronization techniques typically require the client to send a set of hash values or other information about the reference file to the server, and there is a trade-off between the amount of this data and the achieved compression in the other direction.
The primary advantage of file synchronization is that no old versions of pages have to be stored and then fetched from disk by the proxy.An interesting alternative called value-based web caching was very recently proposed in [21].
As in file synchronization techniques, bandwidth savings are obtained by using hash values to refer to blocks of data already known to the client.
However, the technique does not require the client or proxy to choose a particular set of reference files, but exploits similarity between the requested page and previously transmitted content independent of file boundaries.
This is achieved by using Karp-Rabin fingerprints [11] to identify block boundaries in a consistent manner independent of position as proposed in [15], and keeping a limited amount of state for each client.
Similar techniques have also recently been used in the Low Bandwidth File System [18] and the Pastiche distributed backup system [6].
Comparing value-based web caching to rsync, we would expect similar performance in cases where we can reliably select the best reference file, but better performance in cases where it is not clear which previously accessed file contains the similar content.
In particular, value-based web caching resolves problems due to aliasing, i.e., different URLs returning the same content.
Another advantage comes from caching hashes at the proxy, although one could also use an rsync-based approach to do the same (see Subsection 3.3).
Our main focus is on studying web and proxy serverfriendly schemes for delta compression between different pages that achieve good compression without adversely affecting throughput.
In fact, we believe that delta compression between different pages has more practical potential than the more commonly studied case of delta compression between different versions of the same page.
On the other hand, we conjecture that file synchronization techniques may be the most appropriate approach for delta compression between different versions of the same page, at least in a proxy environment.
In this paper, we discuss techniques and provide experimental results for web access based on delta compression and file synchronization.
In particular: ¢ We compare several policies for selecting appropriate reference files for delta compression, and evaluate the achieved compression ratio.
Using a large set of plausible site visits by clients distilled from NLANR proxy traces, we show that significant average improvements over gzip are achieved by the best policy.
Moreover, we show that very simple policies achieve close to optimal compression using only one or two reference files visited within the last few minutes, thus allowing for an extremely efficient main-memory based implementation in a web or proxy server.
We discuss and evaluate the use of file synchronization techniques for efficient web access.
We show that they are of limited use for delta compression between different pages, and currently studied improvements in file synchronization techniques are unlikely to change this.
On the other hand, we show that file synchronization techniques have potential for delta compression between different versions of the same page, and we discuss how file synchronization tools could be reengineered for additional improvements.While the chance for a broad adoption of the proposed delta compression schemes at clients and servers is very small, we believe that there is a significant potential for using such techniques in the context of proprietary proxy systems such as those deployed by wireless or dialup service providers.
We note that the benefits of the techniques are limited to text/html files and do not apply to images and other multimedia objects, though there are other specialized techniques for such objects.
According to the traces, text/html files made up ¡ to ¡ ¡ ¤ £ of the data going through the proxy, and about the same amount was due to image files.
(Common statistics indicate that embedded images make up almost ¢ ¡ ¤ £ of all data in a displayed page; however, images have better caching behavior and thus make up a relatively smaller part of the traffic over the internet.)
The benefits are in addition to any benefits due to clientside caching, which do not appear in our traces.
The schemes we propose are applicable to about £ ¥ ¤ £ of all accesses to text/html files going through the proxy, and for those eligible pages we get up to a factor of ¦ ¨ § © average improvement over gzip.
For all pages, the average improvement over gzip is up to § ¢ .
Benefits are lower if duplicate URLs are removed from site visits; these duplicates in the proxy traces may be due to a changed page or a server that does not support the IF-MODIFIED-SINCE header.In the next section, we study reference file selection policies for delta compression of different pages.
In Section 3 we investigate the utility of file synchronization techniques for web access.
Finally, Section 4 provides some concluding remarks.
This section contains the main results of this paper.
We are interested in delta compression schemes that encode a requested page in terms of other pages that the client already has in its cache.
We restrict ourselves to reference files from the same site that have been very recently accessed by the client as part of the current site visit.
We define a site visit of a client as a set of consecutive accesses to pages on the same site.
We show that even with this limitation on the choice of reference files, we can obtain significant average compression benefits over gzip, by a factor of § ¢ for all pages and ¦ ¨ § © for "eligible" pages.
One challenge in the experimental evaluation is that it is not easy to obtain client traces that can be used to evaluate our schemes, due to privacy concerns.In our case, we need a large number of site visits that are very recent, so that we are still able to obtain the pages from the origin servers.
Since we were unable to get a representative set of end client traces, we decided to try to "distill" a plausible set of client visits from the publicly available NLANR proxy traces, as described later.
We then fetched and stored all pages in those visits, and ran simulations based on the stored pages.We used version 2.0 of the zdelta tool [25] to perform delta compression between pages.
1 The zdelta tool is carefully optimized for both compression and speed, and supports the use of up to four reference files.
We note that in principle a similar compression ratio could probably also be obtained with the vcdiff compressor [13] provided that separate Huffman coders are applied to the different fields of the generated instruction stream; however, vcdiff currently only supports a single reference file.
Both tools achieve throughputs of several MB per second for encoding and up to tens of MB for decoding, similar to the speed of gzip.
We now define a few simple policies for choosing reference files that we investigate.
The policies are very simple and we make no claims of algorithmic originality.
Our primary goal is to evaluate their behavior on a large set of traces.
Suppose that the client is performing the th page access in the current site visit.
The policies for choosing reference files from among the pages previously visited during the same site visit are as follows: We also consider two simple improvements over last-k and longest match-k, called last-k¢ and longest matchk¢ , that add the following two rules: (1) if the requested page was previously accessed, then that page is always selected as a reference file, (2) we make sure that all selected reference files are distinct, since there is no benefit in using two identical reference files.
Note that if 1 Available at http://cis.poly.edu/zdelta/.we eliminate duplicates in the traces, then these policies will be identical to the basic ones.Of course, we can choose from at most ¤ £ reference files for the th page; thus for ¦ ¥ ¡ all policies are the same.
The last-k and last-k¢ policies were chosen as the simplest heuristics we could come up with, and they also tend to minimizes the time a server or proxy has to retain pages for later use as reference files.
The longest match-k and longest match-k¢ policies are almost identical to the policy proposed by Chan and Woo [4], and should achieve very similar performance.
The best setof-k policy is clearly optimal, but inefficient since there are § © ¨ possible choices of sets.
(Finding the best set is NP Complete due to a reduction from Set Cover, under reasonable assumptions about the delta compressor.)
The third policy, best-k, is not guaranteed to be optimal, but more efficient to implement.
To evaluate the policies, we downloaded traces from the NLANR proxy servers and partitioned the traces into "plausible" site visits as follows.
In the traces, clients are identified by IDs that are unique during a given day and for a particular proxy.
However, these clients are typically not end clients (browsers) but populations of users in a particular organization, and for our purposes we need access patterns for end clients.
We defined a site visit with timeout as a sequence of accesses to the same site by users with a common ID, with at most minutes between any two consecutive accesses.
If is large, then a site visit to a popular web site is likely to consist of accesses by different end clients with a common ID.
However, if we assume some degree of independence between the access times of the different users throughout the day, then by decreasing we expect to eventually split up most of these visits into shorter visits by different users.
We found that after decreasing to less than an hour, subsequent decreases of to a few minutes do not result in many additional splits, indicating that most of the site visits at this point are probably due to a single end user.
Of course, we cannot claim that the site visits thus obtained are all due to single individuals, but we believe that our heuristic is close enough to allow an evaluation.
We note that such sessionizing problems have been extensively studied, see, e.g., [22].
We downloaded all NLANR proxy traces for April 22,2003, and extracted site visits with timeout ¡ minutes.
We then took a sample of site visits with a total of ¢ ¥ pages, which we downloaded and stored on April 24.
We excluded any dynamic pages (with parameters in the URL), as those parameters are removed by NLANR for privacy reasons.
We see no reason to believe that the approach would not work on dynamic pages; in fact, such pages may be particularly suitable for delta compression as they frequently change in minor ways and share a lot of content with other pages on the site.
However, we have not yet obtained good traces to evaluate this.
We made sure that our sample has the same distribution of site visit lengths (number of page accesses) as the entire set.
Note that the proxy traces did not contain entries for accesses that were already satisfied by client-side caches; this is fine for our purpose since we are interested in benefits beyond those already obtained by client side caching.The generated site visits did contain a significant number of duplicates, i.e., repeat accesses to the same URL in a single visit with return code 200.
These accesses may be due to changed pages or due to servers that are not set up to support the IF-MODIFIED-SINCE feature in HTTP.
While some of the duplicates may belong to different end clients, we believe most are due to the same client.
In the following, we report results with duplicates included, and later discuss how the results are impacted if they are excluded.We compare our results to gzip as a baseline compressor.
Note that gzip is not optimized for HTML, and recent grammar-based approaches based on [12] can achieve significantly better results.
(In fact, proprietary versions of these algorithms are the basis of the web acceleration system designed by Slipstream Data and deployed by NetZero.)
These approaches can also be adapted to exploit similarity between different pages.
In Figures 1 and 2 we see the size reductions obtained by the last-k and longest match-k policies, respectively, for different values of ¡ .
For the first page access in a site visit, all policies and also gzip achieve the same reduction, to about 22% of the original size on average.
2 For the second access, pages are on average reduced to 10% of their uncompressed size.
As there is only one available reference file, all policies except gzip achieve the same performance.
On the third access, policies with .
Compression improves to about 5% of the uncompressed size after a few pages.
(However, since most site visits are fairly short, the impact on the average benefit over all pages decreases as we move to the right, as shown later.)
We also see that the poli-2 If no reference file is given, zdelta becomes identical to zlib, which itself has a similar performance as the closely related gzip.cies last-1¢ and longest match-1¢ perform significantly better than the base policies.
(Note that the base policy longest match-k only tries to match the directory part of the URL and does not distinguish between different pages in the same directory; otherwise, longest match-1 and longest match-1¢ would be identical.)
In summary, there is significant benefit in using delta compression versus gzip, and the benefit increases with the number of reference files and the length of the site visit.
One curious detail is that gzip compression appears to get slightly worse as more pages on a site are visited; this is primarily due to a decrease in average page size as discussed further below.In Figure 3 we show the performance of the best-k and best set-of-k policies, and in Figure 4 we show the performance of a selection of all policies.
All of the best-k and best set-of-k policies have almost the same performance, independent of ¡ .
and best set-of-2 policies.
The three policies result in almost identical graphs.due to the way current delta compressors works.
The interpretation of these results is important.
In general, using more than one reference file can improve delta compression for two reasons: it could be that each reference file contributes to the compression of the target file (e.g., a target file could be similar to one reference file in the first half, and to another reference file in the second half), or it could be that by using several files we simply have a better chance of including the one reference file that contributes most of the benefit.
and longest match-4, and best-1 and best set-of-2.
The above results implies that for our application it is primarily the second reason, since by choosing the best reference file we are getting essentially all of the possible benefit.
A similar observation was also recently made in [8] in the context of file system compression.
The last-k policy, and also the longest match-k policy proposed in [4], are not really good at identifying the best reference file, but by increasing ¡ we improve our chances of including the best reference file in the list of selected pages.
For efficiency it would be preferable if we could directly identify the best reference file, rather than use up to reference files or try all different files as in the best-1 policy; we address this issue later.Comparing last-k and longest match-k, we see that longest match-1 is better than last-1 at identifying good candidates; however, this advantage largely disappears if we use the improved policies last-k¢ and longest match-k¢ instead.
Thus, it seems that in practice the directory-based heuristic proposed in [4] really does not do much better than a trivial heuristic such as last-k¢ .
In Figures 8 and 9, we show the average compression factor, in terms of total transmitted bytes, over all eligible page accesses and all page accesses achieved by the various policies.
Compression ranges from a factor of § ¡ for gzip to more than for the best policy for eligible pages.
We note that the various policies are actually much closer in terms of average benefit than suggested by Figures 1 to 4, since most transmitted bytes fall into the left half of those figures.
In particular, even last-1¢ and longest match-1¢ perform quite well.
We now address the problem of how to efficiently identify the reference file that provides the best compression ratio.
Our proposed solution is very simple, and uses random hash functions to create fingerprints for files according to a technique proposed by Broder in [2].
This technique has been previously used to cluster documents in [3], and was shown in [8,20] to provide reasonable estimates for the size of a delta between two files.
In particular, we hash all substrings of length in a file to integer values, and then retain the smallest hash values.
(The method does not seem to be very sensitive to the length of this substring.)
We then estimate the similarity of two files by intersecting their samples.
Given a page request, we select the reference file that is most similar to the requested page under this measure.In the first experiment, we used this sampling technique to approximate the best-1 policy, which as we saw earlier is essentially as good as the best policy, best setof-k, in practice.
In Figure 10 we see the results for the standard best-1 policy and for the sampling based approach with main memory indexed by MD5 hash, and may be deleted by the proxy at any point in time.
¢ A client sends each page request to the proxy accompanied by several MD5 values of previously visited pages, called candidate pages.
These candidate pages can be selected by the client based on last-k¢ , longest match-k¢ , or any other policy.
The proxy checks which of the candidate pages it still holds in main memory, and uses the most similar of those as reference file.Thus, if the client uses the last-4 policy to identify candidates, then the resulting compression performance will closely track that of the standard last-4 policy, using only a single reference file.
Note from Figures 8 and 9 that both last-4 and longest match-4 achieve average compression quite close to the optimal, making them viable approaches.
For the proxy (or server in an end-to-end approach), this scheme is highly efficient as only a single reference file is used, and as pages only need to be retained for a few minutes.
In Figure 11 we show the performance for the case where only pages accessed in the preceding few minutes are used as candidate files; the achieved compression ratio is very close to optimal even for windows of ¡ and ¡ minutes.
(While site visits were defined by us as having at most ¡ minutes between consecutive accesses, most are actually much closer in time.
This also supports our contention that our distilled site visits are mostly due to a single end client.)
We note two drawbacks of this scheme.
First, the computation of the samples could become a bottleneck, though we believe this can be handled through careful optimization.
Second, the most similar reference file can only be selected once the entire requested page has arrived at the proxy.
In contrast, in policies such as lastk and longest match-k the proxy can select the reference files as soon as it receives the request.
These can then be immediately inserted into the hash table of the delta compressor, and the requested page can be streamed through the compressor and forwarded as it arrives from the server.
3 This second drawback might make schemes such as last-k¢ or longest match-k¢ for ¡ ¡ or ¡ ¡ ¦ more attractive in many scenarios.
We now look at how results change if we remove any page accesses with return code 200 that go to a page previously accessed in the same site visit.
Recall that such accesses may be due to changed pages or due to servers not supporting IMS.
Table 1 shows the compression ratios achieved in this case.
As we see, benefits of delta compression are reduced, but still significant.
In particular, the average benefit over gzip is now a factor of about § instead of § ¢ for all pages, and § © instead of ¦ ¨ § © for eligible pages under the best policy.
The main observations from the experiments in this section are as follows: (1) even very simple schemes for selecting reference files achieve significant compression over gzip and come close to the optimum, (2) there seems to be only very limited benefit for the directory matching technique in [4] over other simple heuristics, (3) essentially all of the potential benefit can be achieved with a single reference file and there are sim- , and best-1.
With duplicates Duplicates removed Policy Eligible All pages Eligible All pages GZIP¢ ¡ ¦ ¢ ¡£ £ ¤ ¡£ ¥ ¤ ¡ § ¦ § L1¨¡¨¦ L1¨ L1¨¡ L1¨¡¨ L1¨¡¨¦ ¦ © ¡¡ ¥ ¡ ¦ ¡ ¨ £ ¢ ¡ £ L1+ ¥ ¡ ¦ ¦ © ¡ ¦ ¦ ¡ ¨ £ ¢ ¡ £ L4+ £ ¡¡ £ ¦ © ¡ ¦ ¥ ¦ ¢ ¡ § ¦ £ ¢ ¡£ § M1 ¥ ¡ £ ¦ © ¡ £ ¥ £ ¢ ¡ § £ ¢ ¡ £ ¨ M1+ § ¡ ¦ ¦ © ¡ ¦ ¢ ¡ § £ ¢ ¡ £ ¨ M4+ £ ¡ ¢ ¡ ¦ © ¡ ¦ ¨ ¢ ¡ ¥ £ ¢ ¡£ ¥ MS 1,10 £ ¡¡ § ¦ ¡ § ¢ ¡ § ¦ £ £ ¡£ B1 £ ¡ £ ¦ © ¡ ¥ £ ¨ ¡¡ § £ ¢ ¡ § ¦ple sampling based methods for identifying this file, (4) however policies such as last-1¢ and longest match-1¢ perform quite well and might be preferable in practice for other technical reasons, and (5) benefits are somewhat lower if page size distribution is taken into account and duplicates are removed.
In this section, we study the potential for using rsync and other file synchronization techniques for web access.
By file synchronization, we refer to techniques where the server has the requested pages, but not the reference file held by the client.
The most widely used tool for file synchronization is rsync [28], which uses a single round-trip between client and server as follows: the client partitions the reference file into blocks of a few hundred bytes, and sends a hash value for each block to the server.
(The hash value has a strength of £ bytes for common file sizes.)
The server then sends the requested file to the client, but replaces any block that hashes to one of the received hash values by a reference to the hash.
This method is then combined with standard gzip compression.
The cost is given by the size of the hashes and the size of the encoded page, with a trade-off between the two.
The size of the encoded page sent to the client is clearly lower-bounded by the size of a delta.In fact, measurements in [23] show that the size of the data sent from server to client in rsync is usually significantly larger than a delta.
This raises the question of whether file synchronization techniques are efficient enough for web access.
In the following, we present results for related pages and versions of the same page.
Figure 12 shows results for related pages, using the last-1 and longest match-1 policies and several block sizes in rsync.
We also show results for the most similar-1 policy, with and without duplicates, to get an upper bound on the possible benefit, even though this policy is not realistic as the selection of reference files for rsync is performed at the client.
(We used most similar-1 instead of best-1 for efficiency but the result should be very similar.
One caveat is that we did not adapt the substring size in the sampling step to the block size of rsync; we plan to correct this in the final version.)
We see from Figure 12 that rsync performs only slightly better than gzip if we consider the total data sent in both directions.
Even for the most similar-1 policy with block size In Figure 13 we investigate the use of more than one reference file in connection with rsync.
To use rsync on two reference files, we concatenated the two files into one longer file.
We do not show results for more than two reference files as there clearly would be no benefit in this.
We note that the amount of data sent from client to proxy is proportional to the number of blocks, and thus increases with the inverse of the block size.
We see that for block sizes of ¢ ¡ ¡ and ¦ ¡ ¡ ¡ bytes, adding a second reference file gives some negligible benefits under the last-k and longest match-k policies.
Benefits are small because the increase in read size cancels out most of the decrease in write size.
In general, the reason for the disappointing performance of rsync on related pages on the same site is that these pages, while overall similar, differ in a number of places.
While there is some benefit for aliased and very similar pages, in general the fine granularity of differences does not allow rsync to find large matches at the level of blocks of several hundred bytes, while block sizes of ¡ ¡ bytes or less would greatly increase the number of hashes sent by the client.
We note that Tridgell, in Section 4.4.3 of [26], proposes to use the Burrows-Wheeler transform to increase the locality of changes in updated files.
This approach works well when files are updated by replacing all occurrences of a string by another string (e.g., renaming of a variable or a consistent change in format such as line breaks), but it does not appear to work well at all in our scenario.
In fact, we observed a significant decrease in performance under this approach, as it also has the reverse effect of spreading out blocks of changed bytes all over the transformed files.
We conclude that simple rsync is not a good approach for compression between related files.
We now consider the case of several versions of the same page.
Our NLANR traces are not very useful for this case since the client IP hashes change from day to day.
Since we did not have alternative traces, we instead used data obtained from repeated web crawls of a certain subset of pages.
In particular, we chose pages at random from two large page collections of more than ¡ ¡ million pages each, one from the Internet Archive and one from our own crawls.
The selected pages were than crawled every night for several weeks in Fall 2001 In the experiments, we measure the benefit of using rsync to fetch an updated page given that the client already has an outdated version from the base set.
We give average results for all pages and for only those that have changed.
.
We note that user surfing behavior is likely to be biased towards sites that are frequently updated, and thus our results for a random sample of pages may be overly optimistic.
Nonetheless, there seems to be significant potential for using rsync between different versions of a page, as previously suggested by the rproxy project [27].
Note that examination of the pages shows that page updates tend to be highly clustered within a page; this explains the good performance of rsync in such cases.
The results indicate that file synchronization using rsync is efficient for different versions of the same page, but performs only slightly better than gzip on related pages from the same site.
Of course, this could be due to shortcomings in the rsync algorithm, and there are several proposals for improved file synchronization techniques [5,19,24].
However, these proposals rely on additional roundtrips and would not be appropriate for a web access scenario since modem latencies are high and objects are not very large.
It appears difficult to achieve significant bandwidth savings over rsync without incurring more than the single roundtrip used by rsync.
This suggests that an approach that uses delta compression between related pages in a site visit, and file synchronization between different versions of the same page, might be a good combination.
In such a scheme, the server or proxy only needs to keep old files for a few minutes, and still achieves good performance for different versions of the same page using rsync.
This leads us to the following ideas and observations: ¢ Instead of sending block hashes to the proxy with each request under the rsync protocol, we could allow the proxy to compute and store block hashes for the references files as they pass through the proxy.
In this case, the approach becomes very similar to value-based web caching [21], except for the use of fixed rather than value-based block boundaries.
We can estimate the benefit by deducting the dark portion of the bars from the charts in this section.
Some additional benefits might arise since there is no need anymore to choose a small set of reference files for each access.
¢ In the case of the Low Bandwidth File System [18], the use of value-based block boundaries is crucial due to the fact that files have to be transmitted in both directions.
In our scenario, where files are only sent from proxy to client, we can also use fixed boundaries based on position and then try to match these blocks with all positions in the file to be encoded, as done in rsync.
¢ When caching hash values at the proxy, it might be interesting to consider a "multi-resolution" approach for the block size: when sending a file to the client, we initially compute and store hash values for blocks of fairly small size, say ¦ bytes.
This takes a lot of space at the proxy, but allows compression of similar files that is much better than that achieved with block sizes of several hundred bytes as in rsync.
To limit space consumption, instead of evicting hashes we can combine two adjacent hashes into one hash for a larger block, provided that the hash function is composable (i.e., the hash of a block can be computed from the hashes of the left and right half).
Alternatively, we could for each file generate hashes at different levels of granularity, and first evict fairly old hashes of small size.
(This takes at most a factor of ¦ more space than a combining approach and removes some other complications.)
Based on these ideas, we plan to investigate an approach that combines delta compression, value-based caching, and synchronization, as follows: Transmitted files are cached for a short time to allow delta compression, while block hashes for the files are cached for longer periods based on the above multi-resolution approach.
Thus, hashes for small blocks are kept for a shorter period of time, and hashes for large blocks for longer periods.
In addition, we could also allow the client to send hashes in some cases, such as when a page is revisited after several days and the old hashes are likely to have been evicted at the proxy.
There are a number of questions to explore in this context, such as the advantages and disadvantages of fixed and value-based block boundaries and the details of the multi-resolution approach, and we plan to address these in our future work.
In this paper, we have studied the performance of simple delta compression and file synchronization schemes for efficient web access.
Our focus was on web and proxy server-friendly schemes that do not require the storage and retrieval of old versions of web pages at the server.
Our main conclusion is that there is significant benefit to using delta compression between pages on the same site, and we gave several low-overhead schemes that improve significantly over gzip.
On the other hand, we found that single-roundtrip file synchronization techniques such as rsync obtain good compression between different versions of a page at low overhead, but do not significantly improve over gzip for related pages.We are working on several extensions of this work.
We plan to study the approach combining delta compression, value-based web caching, and file synchronization described in Subsection 3.3, and to integrate it into a dual proxy architecture called SPAWN 4 that we have implemented at Polytechnic University.
We are also working on improved software tools for file synchronization and plan to evaluate these in the current context and for content distribution networks and large replicated document collections.
We thank Dimitre Trendafilov for help with experiments, and the anonymous referees for helpful comments.
This research was supported by NSF CAREER Award NSF CCR-0093400, Intel Corporation, and New York State through the Wireless Internet Center for Advanced Technology (WICAT) at Polytechnic University.
Proxy traces were provided by the IRCache Project, which is supported by the National Science Foundation (grants NCR-9616602 and NCR-9521745) and the National Laboratory for Applied Network Research.
