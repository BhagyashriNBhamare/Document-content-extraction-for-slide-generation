Speech-based training agents can be described as virtual humans posing as interactive training characters with the capability to communicate in a spoken conversational manner.
While creating this technology, developers face two stumbling blocks: 1) modeling the agent and its training knowledge is a time-consuming and tedious task, and 2) modern speech recognition software suffers from high Word-Error Rates caused by noisy environmental conditions.
This paper presents a dialog management architecture that addresses these problems using the Context-Based Reasoning paradigm.
The system minimizes the time necessary to build the training knowledge in the instructional agent, as well as tolerates the relatively high Word-Error Rates related to automatic speech recognition.
Ultimately, these advantages lead to quick development of speech-based training agents.
The dialog manager was directly implemented into the LifeLike Avatar, an embodied conversational agent funded by the National Science Foundation.
A set of quantitative results is presented to reflect the effectiveness of the system.
With the appearance of ELIZA (Weizenbaum, 1966), the technology world was introduced to conversationbased interaction with a computer.
Additionally, advances in automatic speech recognition (ASR) technology have resulted in hopes that voice interaction will someday replace the keyboard and mouse with a microphone.
The melding of conversation agents with ASR has made speech-based agents almost a reality.
In turn, these entities are very likely to find their way into the training realm.Nevertheless, conversational, speech-based interaction in any type of embodied agent remains a difficult problem to solve.
Dialog systems that catch the spoken words, make sense of them, and compose an intelligent and natural response must be built robustly and reliably.
In fact, Kang et al (2009) assert that a challenging aspect of dialog systems is processing speech-based input.
Environmental factors, such as interfering background noise, can muddle a machine's ability to pick up spoken utterances.
Additionally, a non-native speaker could give a grammatically inaccurate response, causing even more confusion when an agent must determine the user's needs.
Speech recognizer systems have seen Word-Error Rates (WER) peak out at 30%, under controlled conditions.
( Kang et al, 2009) This is an unacceptable level of integrity if a developer wishes to use full syntactic processing of user inputs.This paper presents a method to elevate the level of speech-based discourse to a new echelon of naturalness in embodied conversation agents (ECA) by exhibiting a tolerance to high WERs in speech understanding systems, yet providing answers to asked questions, without pre-linking questions to answers.
An additional aspect of producing embodied training agents is the amount time required to develop the knowledge base used to generate the appropriate response.
There is a need for a generalized ECA knowledge infrastructure, yielding a quicker domain-independent agent development lifecycle.Hence, this general problem yields two specific problems to be addressed: 1) tolerating the limitations of current untrained ASR technology, and 2) developing a domain-independent knowledge management system.
In this paper, we show that a domain-independent, context-driven conversational discourse infrastructure applied to speech-based conversational dialog systems provides an effective level of natural language understanding for an assistive interaction between an ECA and a human in spite of low performing ASR systems.
The intent of this work is to pass on the benefits of such a dialog manager to the realm of embodied training agent design.
Two major themes permeate the work presented in this paper: conversation agent design and context-based methods.
This section will present a brief discussion of each of these topics.
Conversation agents, or chatbots, represent a specialized field of natural language processing (NLP) whose purpose is to provide a method of interaction that resembles a conversational exchange between two humans.
Additionally, assistive dialog systems exist to serve a particular purpose, such as providing information to its users or aiding them with completing a certain task.
The channel of communication between the user and the dialog system may be based on text, on speech and gestures, or on a combination of all three.Conversation agents began life with ELIZA (Weizenbaum, 1966), followed by the development of PARRY (Colby, 1973) and SHRDLU (Winograd, 1980).
Chatbot development experienced a period of inactivity until the 1990's with the introduction of ECAs, where they were paired with a physical embodiment.
(Gorin et al, 1997) (Casell et al, 2000) (McBreen andJack, 2000) (Massaro et al, 2001) (Catrambone, 2002) ( Béchet et al, 2004) Innovations in ECA technology would follow, such as virtual storytellers ( Tarau and Figa, 2004), museum guides ( Kopp et al, 2005), emotional entities ( Alm et al, 2005), caring agents ( Bickmore and Picard, 2004 Context-based methods refer to the techniques used by a machine to drive behavior based on the immediate environmental state, or current context.
Context-based reasoning (CxBR) formalizes this concept as a paradigm of agent behavior in which only a subset of an entity's total knowledge is active at any one time ( Ahlers, 1998) (Gonzalez et al, 2008), reflective of how humans themselves only require a fraction of their knowledge for any given situation.In general, NLP problems can easily be enhanced through contextually-driven methods (Porzel and Strube, 2002), such as those found in spoken language translation ( Levin et al, 1995), semantic clarification (Kladke, 1989) (Towhidnejad, 1990) and knowledge modeling ( Porzel et al, 2006).
Perhaps the group of natural language protocol researchers that has benefited the most from contextualization is the ASR community (Young, 1989) (Serridge, 1997) ( King et al, 1998) (Goulian et al, 2003) ( Fügen et al, 2004) ( Yan and Zheng, 2004) ( Sarma and Palmer, 2004) (Lieberman et al, 2005) Sammut's ProBot (2001) touches upon using context in conversation agent discourse, which uses contexts when unexpected utterances are received.This section brings to attention the general body of chatbot work that has surfaced since its infancy.
It also mentions the extent to which context-based methods have been used for the sake of natural language systems.
However, there lacks a presence of combining context-based methods with dialog systems.
The premise of this work is to provide a linkage between these technologies, producing a conversation agent whose dialog management is driven by context-based methods.
Devising a new dialog system involves three major design decisions: user input processing method, knowledge management, and agent response discourse mechanism.
The aggregation of all three sub-system selections results in the final approach of the prototype, the CONtext-driven Corpus-based Utterance Robustness dialog manager (CONCUR) prototype.
CONCUR was incorporated into the LifeLike Avatar ( DeMara et al, 2008) ECA.
The next sections describe the inner-workings of the CONCUR prototype.
The first component of CONCUR is the Input Processor.
This module parses the raw user utterance for contextual keyphrases.
The result from the speech recognizer is chunked into phrases, which begins with a word-for-word Parts-of-Speech tagging.
This procedure is performed using an NLP toolkit.
The utterance phrase chunks are then filtered for noun and verb phrases, discarding the remainder of the sentence.The Input Processor also contributes to annotating the Knowledge Manager's corpora with keyphrase tags.
As part of the corpus pre-processing routine, an NLP treatment of corpus data is performed, providing an automatically generated keyphrase list for each contextual layer.
The next section delves further into the Knowledge Manager.
A major feature of CONCUR remains its dependency on contextual relevance.
This concept speaks to the idea that two data points may be within contextual proximity of each other if they share some form of conceptual commonality.
Hence, contextualization requires a predefined set of related data.
For dialog-based systems, contextualization exists when groups of words maintain conceptual relationships with each other.
These lingual relationships are contained in the Knowledge Manager portion of the CONCUR architecture.
Knowledge bases used by CONCUR all reflect a pre-established contextual relationship mapping.
This is done by using a contextual layering system of organizing information, a format similar to that of an outline or an encyclopedia entry.
Hence, all responses that will be used as spoken replies by the conversation agent are pre-annotated in the knowledge base with a contextually-driven tagging system derived from its outline depth.The Knowledge Manager consists of three sources of data: user data, conversational knowledge, and domainspecific knowledge.
The domain-specific knowledge features the expertise of the National Science Foundation (NSF) Industry/University Cooperative Research Center program (I/UCRC).
The scope and depth of the domain-specific knowledge is modeled after that of a traditional expert system.
( Gonzalez and Dankel, 1993) The entirety of the conversational knowledge base encompasses all of the agent quips that do not reflect any expertise, but rather serve as transitioning actions.
The user profile database serves as a repository of individualized account data, and it reflects the importance of memory during a conversation.
(MacWhinney et al, 1982) The strength of the Knowledge Manager is its ability to fetch contextualized knowledge.
Contextualized knowledge refers to a cross-section of any of the three knowledge sources that is relevant for the active context of the conversation.
Once the dialog system determines the context of the conversation, knowledge that is labeled with the current context is elicited as valid information for the conversation and funneled into the contextualized knowledge base.
Once this subset of information is established, the dialog manager can then work with a manageable portion of the entire knowledge base.
CONCUR uses a conversation discourse model driven by the CxBR paradigm.
With knowledge of the current state of the conversation, the discourse model pieces together the information of the Input Processor and the Knowledge Manager, as well as adds its own CxBR devices to provide an appropriate reply to the user.The underlying theme of the discourse model is the supervision of conversation goals.
In essence, a spoken dialog between parties is a sequence of passing goaloriented statements to one another.
(Grice, 1975) The intent is to achieve some form of resolve for each of these exchanges, otherwise viewed as completing goals.
For this paper, the dialog manager is charged with managing these goal completion tasks.
The detection of goals is performed in the inference engine.
Contexts directly correlate to reaching specific goals.
Servicing of goals is the work of traversing the context topology until the mission is completed.
This goal management comprises the processes that recognize and satisfy the interlocutor's needs as conveyed by her/his utterances.
CONCUR's goal management involves two parts: 1) goal bookkeeping and 2) context topology.The Goal Bookkeeper maintains the goal-based activities of conversation agent, and it consists of two parts: the Inference Engine and the Goal Stack.
The primary purpose of the Inference Engine is to recognize the user's immediate goals.
Goal recognition refers to the process of analyzing user input utterances to determine the proper conversational goal that is to be addressed.
This is analogous to the context activation process in CxBR methods.
The CONCUR Inference Engine takes the user utterance and performs a keyphrase matching among the current set of relevant contexts.
This means that a strong knowledge base must be in place for proper goal recognition.
It is up to the Inference Engine to determine if the user has remained within the current conversational context, or if there has been a topic switch.The Goal Stack directly manages the conversation flow.
This mechanism serves as an agent's short-term memory during a conversation.
Its job is to ensure that all contexts that are introduced into a dialog exchange are attended to and eventually brought to closure by the end of the conversation.
The Goal Stack performs its context management immediately upon the Inference Engine's selection of the current context.
CONCUR's Context Topology gives structure to its entire set of speech.
This includes the transitional actions when moving between contexts when a goal shift is detected.
The Context Topology organizes the contexts that represent the set of behaviors through which the system will respond.
Two major types of contexts make up these mixed-initiative dialog-based actions: Agent Goal-Driven Contexts, and User GoalDriven Contexts.
These two sides of conversational contexts reflect Grice's (1975) treatment on goals in dialog.
Agent Goal-Driven Contexts pertain to those actions needed for the avatar itself to perform its duties as an interfacing agent.
User Goal-Driven Contexts refer to those behaviors needed to help support the user fulfill her/his needs.This section gave a description of the CONCUR prototype's basic architecture.
The next section describes how it will be test for effectiveness as a framework for building speech-based training agents.
Evaluation of chatbots has always remained a controversial topic, as it is unclear on how to quantitatively describe how well a conversation agent performs or how naturally it responds.
Current research has found success in using both quantitative and quality metrics to make relative comparisons between conversation agents.
( Semeraro et al, 2003) ( Rzepka et al, 2005) (Shawar and Atwell, 2007) An assistive dialog system proves its effectiveness under the light of two primary objectives: 1) dialog performance, and 2) task success.
( Walker et al, 1997) ( Dybkjaer and Bernsen, 2001) Walker et al (1997) further break down the dialog performance to efficiency measures and quality measures.
Efficiency costs refer to the quantitativelymeasurable resource consumption needed to accomplish a single task or sub-task.
Quality costs measure the actual conversational content.
These metrics may be recorded quantitatively or qualitatively.
Table 1 delineates the relevant cost metrics for CONCUR, as inspired by Walker et al (1997), Stibler and Denny (2001), Charfuelán et al (2002), and Hassel and Hagen (2005).
Efficiency Metrics pertain to those interaction traits that can be empirically observed, with no need for quality interjection.
A second set of results, called the Quality Metrics, was observed using both Quantitative Analysis and Questionnaire-based data.The Quality metrics were collected after a user interaction was completed, where the transcript of the experiment was analyzed.
At the conclusion of each experimental interaction, the users were given an exit survey.
On this instrument, each question is a statement in which the user provides her/his level of agreement, where a '1' rating is 'I disagree' and a '7' corresponds to 'I agree.'
The "Naturalness" statements aim to garner whether the user was able to experience a natural conversational exchange, while the "Usefulness" statements check if the agent was able to perform as a capable information deployment tool.The Naturalness statements are as follows:• If I told someone the character in this tool was real they would believe me.
• The character on the screen seemed smart.
• I felt like I was having a conversation with a real person.
• This did not feel like a real interaction with another person.The Usefulness statements consist of:• I would be more productive if I had this system in my place of work.
• The tool provided me with the information I was looking for.
• I found this to be a useful way to get information.
• This tool made it harder to get information than talking to a person or using a website.
• This does not seem like a reliable way to retrieve information from a database.
During the experimentation process, a single conversational scenario was employed on four different assistive dialog systems.
The idea was to provide a comparison study between different conversation agents, with special attention to two of CONCUR's traits: 1) ability to provide usefulness as a speech-based ECA, and 2) capability to maintain effectiveness over different expert domains.The first experiment involved the speech-based CONCUR dialog manager.
A photorealistic animated embodiment, the LifeLike Avatar (DeMara et al, 2008), was used as the agent interface and it was fully operational with CONCUR as its dialog manager.
This ECA represented the performance of speech-based systems specializing in context-sensitive dialog management, which is the primary intent of this paper.
A rich corpus pertaining to an NSF research funding effort known as the Industry/University Collaborative Research Centers (I/UCRC) program, corpus was loaded into this agent.
The speech-based component of the CONCUR reflects a real-world spoken conversation situation with a less-than-optimal WER.
Thirty trials were conducted in this experiment using 21 male 9 female participants.
Six of these trials involved a non-native English speaker, and approximately half of the test subjects were already familiar with the NSF I/UCRC program.The second experiment engaged the user with another speech-based agent, but one whose speech action engine does use an open dialog discourse, but rather, more of a menu-driven model.
The AlexDSS Expert System ( Sherwell et al, 2005) knowledge engine was directly implemented as the dialog engine for the LifeLike Avatar.
This ECA's method of dialog management resembles that of an automated phone, using a highly constrained style of user input expectation.
Thirty trials were performed for this experiment, resulting in 30 collected exit surveys and 20 speech transcripts for analysis.
Of these transcripts, 14 belonged to male participants, and 6 came from female users.
Five of the recorded experiments were completed by non-native English speakers and 40% of the analyzed data were from personnel familiar with the NSF I/UCRC program.In the third experiment, current events knowledge was fitted into the CONCUR framework.
This experiment evaluates the domain-independence aspect of CONCUR.
The corpus was built from a selection of current news articles from the World Wide Web pertaining to U.S. and international news stories, sporting events, and personal health issues.
To eliminate ASR errors, a purely text-based user input system was implemented.
This also meant that no physical embodiment of the agent was used, such as that used in the LifeLike Avatar.
Twenty surveys and chat transcripts were collected from this experiment, consisting of 14 male and 6 female participants.The final experiment used a text-based CONCUR dialog manager to simulate ideal ASR conditions.
User inputs taken from Experiment 1's transcripts were entered into a text-based version of CONCUR.
The responses from the agent were recorded to reflect a version of CONCUR that does not have input errors associated with ASR facilities.
This experiment gave a baseline for measuring CONCUR's effectiveness.
Since no extra participants were needed for this experiment, no surveys data was necessary.
Table 2 gives a summary of all of the experiments featured in this paper, establishing the differences in dialog manager selection, agent style, domain expertise, and number of trials conducted.
The four experiments were conducted and their results are presented in this section.
Table 4 shows the efficiency metrics collected from the four experiments.
These metrics deal with the measurable, non-quality judgments recorded by each agent.
The WER metric reports how well the ASR performed.
In this table, it is exhibited that both speechbased ECAs yielded less than 45% accuracy in detecting user speech, a number much less than those found in recent ASR efforts.
(Kang et al, 2009) Table 5 gives the results of the quality metrics.
In these metrics, each chat transcript was analyzed for misunderstanding, erroneous agent responses, and context goal satisfaction.
The final two columns, Goal Completion Accuracy and Conversational Accuracy, give a quantitative account of each agent's usefulness and naturalness, respectively.
Goal Completion Accuracy gives the percentage of user information requests that were adequately fulfilled by the agent.
Conversational Accuracy accounts for the percentage of system responses that are not classified as a general misunderstanding or an erroneous reply.From this table, the CONCUR chatbot with the current events corpus achieved the highest Conversational Accuracy, despite exhibiting the lowest Goal Completion Accuracy.
The LifeLike Avatar CONCUR with the NSF I/UCRC corpus produced the worst Conversational Accuracy, and its Goal Completion ability were nearly as accurate as that of the AlexDSS NSF I/UCRC ECA.
The impetus of the experiments in this paper was to weave a story about building a dialog manager that could overcome ASR limitations and provide domainindependent knowledge management.
This section discusses the experimental results with these two themes in mind.
To assess CONCUR's ability to handle ASR limitations, the results from Experiment 1 were compared against those of the other experimental agents.
Table 6 gives a comparative look at a subset of metrics from the experimentation.
In this table, it is observed that the avatar-based NSF I/UCRC CONCUR from Experiment 1 yielded a similar WER and a much lower Conversational Accuracy when compared to the AlexDSS avatar from Experiment 2, yet it still scored higher in the user survey ratings for Naturalness and Usefulness.
Experiment 4's text-based NSF I/UCRC CONCUR served as the baseline for a perfect speech recognition system.
Even with these ideal ASR conditions, the speech-based CONCUR was still able to achieve Goal Completion and Conversational Accuracy numbers within those of its text-based counterpart.
CONCUR is based on an open dialog method, while AlexDSS aligns to a more direct, automated phone operator style.
In relation to the results, this difference in user response expectation would account for the worsened ASR results for CONCUR, as well as its deteriorated Conversational Accuracy.To put CONCUR's performance in perspective, four recent projects were selected for comparison, as depicted in From the above comparisons of CONCUR with the AlexDSS ECA, as well as with other conversation agent research efforts, it was determined that a comparable level of usefulness could be achieved using a context-driven discourse method.
Furthermore, the subjective assessments regarding the naturalness dictated that the ECA-based CONCUR's ability to conduct a conversation was slightly more natural than some contemporary speech-based agents.
Hence, from these experiments, it was observed that CONCUR could perform adequately in the presence of ASR limitations.
The results of the CONCUR agents from Experiment 1 and Experiment 3, as seen in Table 8, pertain to CONCUR's domain-independence.
It is quite noticeable that a decrease in performance was experienced when moving from the speech-based interface to a text-based one, as well as the shift from the NSF I/UCRC expertise to the Current Events knowledge.
Perhaps most concerning are the drops in Naturalness User Rating and Goal Completion Accuracy.
Nevertheless, the current events CONCUR chatbot in Experiment 3 agent was able to improve upon the Conversational Accuracy metric.There is, however, a problem with comparing the performance metrics of these two agents against each other, as they both could garner better gains due to the nature of their input methods or their corpus differences.
For example, general misunderstandings in text-based agents are eliminated because any errors associated with misconstrued ASR results do not come into play.
On other hand, the speech-based agent may get better Naturalness ratings only because it resides within a full-blown ECA setup, such as the LifeLike Avatar, instead of a chat window.Additionally, the wide range of topics in the Experiment 3 corpus seemed to cause a false assumption of agent omniscience in its users, a phenomenon not prevalent in the NSF I/UCRC-based Experiment 1.
In both experiments, users were encouraged to engage in topic-based information requests.
Experiment 1's tightly-knit expertise on a niche subject caused users to limit their information requests to I/UCRC-centric inquiries.
The wider-scoped current events corpus of Experiment 3 made its users assume that the agent had more information than it was actually equipped to handle.
This misunderstanding of the agent's expertise is presumed to be the primary cause of the decrease in Goal Completion Accuracy in the Experiment 3 chatbot.The idea to take away from Table 8 is the fact that the same CONCUR agent infrastructure can still provide a usable and functionally acceptable dialog management experience regardless of any changes to its input method or corpus data, serving as the experimental basis for CONCUR's ability to support domainindependence.
In this section, the CONCUR infrastructure demonstrated its ability to effectively maintain its primary functionality as an assistive conversation agent regardless of its domain expertise.
Additionally, a comparison of the knowledge base development times for different dialog systems saw that CONCUR's corpus-based knowledge management yielded the quickest turnover time.
The research described here dealt with spoken interaction with a computer with emphasis on natural conversation flow for use in embodied training agents.
Specifically, it presented a context-driven method of dialog management to fortify the robustness of assistive speech-based ECAs.The particular areas of improvement were concentrated in two themes: overcoming ASR limitations and providing a domain-independent knowledge management system.
An approach to building the context-driven dialog manager was presented with special emphasis on three primary design decisions: the input processing method, the knowledge management process, and discourse model.
A prototype of this approach was reflected in the CONCUR dialog system, whose architecture focused primarily on the use of contextual information to drive a conversation.
Experiments for CONCUR were conducted to validate the two themes of context-based dialog management: ASR limitations and domainindependent knowledge management.
The results consisted of quantitative metrics, survey responses, and quantitative analyses of quality data.
Analyzing these data led to the experimental verification of the aforementioned themes.The results from this work show that speech-based training agents can be effectively developed by driving the focus of conversation toward context-level processing instead of relying heavily on syntactic interpretations.
This research is supported by NSF Collaborative Research award 0703927.
