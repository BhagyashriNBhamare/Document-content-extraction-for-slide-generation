Internet censorship measurements rely on lists of web-sites to be tested, or "block lists" that are curated by third parties.
Unfortunately, many of these lists are not public, and those that are tend to focus on a small group of topics, leaving other types of sites and services untested.
To increase and diversify the set of sites on existing block lists, we use natural language processing and search engines to automatically discover a much wider range of websites that are censored in China.
Using these techniques, we create a list of 1125 websites outside the Alexa Top 1,000 that cover Chinese politics, minority human rights organizations , oppressed religions, and more.
Importantly, none of the sites we discover are present on the current largest block list.
The list that we develop not only vastly expands the set of sites that current Internet measurement tools can test, but it also deepens our understanding of the nature of content that is censored in China.
We have released both this new block list and the code for generating it.
Internet censorship is pervasive in China.
Topics ranging from political dissent and religious assembly to privacyenhancing technologies are known to be censored [6].
However, the Chinese government has not released a complete list of websites that they have filtered.
[11].
When performing measurements of Internet filtering, then, the inability to know what sites are blocked creates a circular problem of discovering the sites to measure in the first place.
To do so, various third parties currently curate lists of websites that are known to be censored, or "block lists".
These lists are used to both understand what content is censored in China and how that censorship is implemented.
Indeed, the Open Observatory of Network Interference notes that "censorship findings are only as interesting as the sites and services that you test."
[24].
Instead of curating a block list by hand, Darer et al. proposed a system called FilteredWeb that automatically discovers web pages that are censored in China [8].
Their approach is summarized in the following steps.
First, keywords are extracted from web pages on the Citizen Lab block list, a small, hand-curated list.
These are English words that are ranked through TF-IDF, a technique which we describe in Section 3.2.
Then, each keyword is used as a query for a search engine, such as Bing.
The intuition is that censored web pages contain similar keywords.
Finally, each web page that appears in the search results is tested for DNS manipulation in China.
These tests are performed by sending DNS queries to IP addresses in China that don't belong to DNS servers.
If a DNS response is received, then, it is inferred that the request was intercepted in China and that the website is censored [8,18,20].
Each web page that is censored is fed back to the beginning of the system.
FilteredWeb discovered 1,355 censored domains, 759 of which are outside the Alexa Top 1,000.
In this paper, we build upon the approach of FilteredWeb in the following ways.
First, we extract content-rich phrases for search queries.
In contrast, FilteredWeb only uses single words for search queries.
These phrases provide greater context regarding the subject of censored web pages, which enables us to find websites that are very closely related to each other.
For example, consider the phrase 中国侵犯人权 (Chinese human rights violation).
When we perform the searches Chinese, human, rights , and violation independently, we mainly get websites for Western media outlets, many of which are known to be censored in China.
By contrast, if we search for Chinese human rights violation as a single phrase, then we discover a significant number of websites related to Chinese culture, such as homepages for activist groups in China and Taiwan.
Identifying and extracting such key phrases is a non-trivial task, as we discuss later.Second, we use natural language processing to parse Chinese text when adding to the blocklist.
In contrast, FilteredWeb only extracts English words that appear on a web page.
As such, any website that is written in simplified Chinese is ignored, neglecting a significant portion of censored sites.
For example, there are many censored websites and blogs that cover Chinese news and culture, and many of them only contain Chinese text.
As such, to discover region-specific, censored websites, such a system should be able to parse Chinese text.
Because Chinese is typically written without spaces separating words, this requires the use of natural-language processing tools.Third, we make our block list public, in contrast to previous work.
The authors of FilteredWeb made their block list available to us for validation; we have published our block list so others can build on it [15].
In summary, we built and now maintain a large, public, culture-specific list of websites that are censored in China.
These websites cover topics such as political dissent, historical events pertaining to the Chinese Communist Party, Tibetan rights, religious freedom, and more.
Furthermore, because many of these website are written from the perspective of Chinese nationals and expatriates, we are able to get first-hand accounts of Chinese culture that are not present in other block lists.
This new resource can help researchers who are interested in studying Chinese censorship from the perspective of marginalized groups that most affected by it.In this paper, we make the following contributions:• We build upon the approach of FilteredWeb to discover censored websites in China that specifically pertain to its culture.
We do so by extracting potentially sensitive Chinese phrases from censored web pages and using them as search terms to find related websites.
• We build a list of 1125 censored domains in China, which is 12.5× larger than the standard list for censorship measurements [6].
Furthermore, none of these websites are on the largest block list available [8].
• We perform a qualitative analysis of our block list to showcase its advantages over previous work.The rest of the paper proceeds as follows.
First, we describe our approach to building a large, culture-specific block list for China.
This includes an in-depth analysis of the advantages of our approach over previous work.
Then, we describe three large-scale evaluations that we performed.
Each of these evaluations produced qualitatively different results due to different configurations of our system.
Finally, we conclude with a discussion of how the block list we built could be used by researchers.
We also briefly explore directions for future work.
Existing block lists have several limitations.
First, some of these block lists have been curated by organizations that study Internet censorship, but these lists are now outdated [5,23].
Other lists have been automatically generated by systems similar to ours, but they are not publicly available at the time of publication [8,9,32].
Furthermore, these systems do not focus on blocked websites that are particular to Chinese culture.
There are also systems that create block lists through crowd-sourcing, but are unable to automatically detect newly censored websites [13,14].
Finally, the Citizen Lab block list focuses on popular websites-such as social media, Western media outlets, and VPN providers-but not websites pertaining to Chinese culture [6].
Other systems use block lists to determine how censorship works, but they do not create more block lists.
For example, Pearce et al. proposed a system that uses block lists to measure how DNS manipulation works on a global scale by combining DNS queries with AS data, HTTPS certificates, and more [26].
Pearce et al. also built Augur, a system that utilizes TCP/IP side channels between a host and a potentially censored website to determine whether or not they can communicate with each other [25].
Furthermore, Burnett et al. proposed Encore, a system that utilizes cross-origin requests to potentially censored websites to passively measure how censorship works from many vantage points [3].
Lastly, several platforms have been proposed that crowd-source censorship measurements from users by having them install custom software on their devices [16,22,30].
Figure 1 summarizes our approach to finding censored websites.
For the most part, our approach is similar to that of FilteredWeb [8].
We start by bootstrapping a list of web pages that are known to be censored, such as the Citizen Lab block list [6].
Then, we extract Chinese and English phrases that characterize these web pages.
Then, we use these phrases as search queries to find related web pages that might also be censored.
Finally, we test each search result for DNS manipulation in China and feed the censored web pages back to the beginning of the system.
The rest of this section details the new capabilities of our approach beyond the state of the art.
An n-gram is a building block for natural-language processing that represents a sequence of "n" units of text, e.g. words.
For example, if we were to compute all of the bigrams of words in the English phrase Chinese human rights violation, we'd get Chinese human, human rights, and rights violation.
Similarly, if we were to compute all the trigrams of words, we'd get Chinese human rights and human rights violation.
On the other hand, if we simply extract unigrams from the sentence, we'd be left with Chinese, human, rights, and violation.
As such, we believe that by extracting content-rich unigrams, bigrams, trigrams, we're able to learn more about the subject of a web page.Unfortunately, computing the n-grams of words in Chinese text is difficult because it is typically written without spaces, and there is no clear indication of where characters should be separated.
Computing such boundaries in natural-language processing tasks is often probabilistic, and depending on where characters are separated, one can arrive at very different meanings for a given phrase [33].
For example, consider the text 特首.
This is considered a Chinese unigram that translates to "chief executive" in English, but the character 特 on its own translates to "special", and the character 首 on its own translates to "first" [12].
Given that we do not have domain-specific knowledge of the web pages that we're analyzing, we consider a unigram to be whatever the segmenter software in the Stanford CoreNLP library considers to be a unigram, even if the output consists of multiple English words.
[4,36].
We believe that using Chinese unigrams, bigrams, and trigrams for search queries instead of individual English words is more effective for discovering censored websites in China.
For example, we know that websites that express collective dissent are considered sensitive by the Chinese government [17].
If we used the words destroy, the, Communist, or Party individually as search terms, we might not get websites that express collective dissent because the words have been taken out of context.
On the other hand, the phrase "destroy the Communist Party" as a whole expresses collective dissent, which might enable us to find many censored domains when used as a search term.
We evaluate the effectiveness of such phrases in Section 5.3.
To "rank" the phrases on a censored web page, we use term-frequency/inverse document-frequency (TF-IDF).
TF-IDF is a natural language technique that allows us to determine which phrases best characterize a given web page [29].
It can be thought to work in three steps.
First, we compute the term-frequency for each phrase on a web page, which means that we count the frequency of each unigram, bigram and trigram.
Then, we compute the document-frequency for each phrase on a web page, which entails searching a Chinese corpus for the frequency of a given phrase across all documents in the corpus [27].
Finally, we multiply the term frequency by the inverse of the document frequency.
The resulting score gives us an idea of how important a given Chinese phrase is to a web page.Using this method, phrases like 1989年 民 主 运 动 [1989 democracy movement] and 天 安 门 广 场示威 [Tienanmen Square Demonstrations] might rank highly on a website about Chinese political protests.
We would then use these phrases as search terms in order to find related websites that might also be censored.
If we find a lot of censored URLs as a result, then we can infer that the topic covered by that phrase is considered sensitive in China.
Before we can compute TF-IDF for a given web page, we need to tokenize the text.
Doing so is simple enough for English because each word is separated by a space.
For languages such as Chinese, however, all of the words in a sentence are concatenated, without any spaces between them.
As such, we need to apply natural-language processing techniques to perform fine-grained analysis of the text on a web page.To do so, we make use of Stanford CoreNLP, a set of natural language processing tools that operate on text for English, Arabic, Chinese, French, German, and Spanish [7].
For Chinese web pages, it allows us split a sentence into a sequence of unigrams, each of which may represent one word or even multiple words.
By combining neighboring unigrams, then, we can extract key phrases from a web page that describe its content.As previously mentioned, although FilteredWeb is concerned with finding web pages that are censored in China, it is only able to parse text for the ISO basic Latin alphabet [8].
By being able to parse Chinese text as well as ISO Latin text, then, we are able to cover many more web pages and extract regional information that may explain why a given web page is censored.
In future work, we could make use of more intricate tools from Stanford CoreNLP likes its part-of-speech tagger to identify phrases that convey relevant information.
We performed a large-scale evaluation of our approach to discover region-specific websites that are censored in China.
We began by seeding with the Citizen Lab block list, which is the most widely used list by censorship researchers.
The list contains 220 web pages that either are blocked or have been alleged to be blocked in the past.
Since we only extract phrases from web pages that are currently censored, we began by testing each web page on the block list for censorship.
This left us with 108 unique web pages and 85 domains.From November 11th, 2017 to July 9th, 2018, we used Bing's Search API to search for websites related to known censored websites [21].
According to the API, each call can return at most 50 search results.
Since it would be expensive to perform multiple API calls for each search term, we limited ourselves to one API call, i.e. 50 search results, per search term.
For each website in the search results, we tested for censorship by sending a DNS request to a set of controlled IP addresses in China that don't belong to DNS servers.
As with FilteredWeb, if we received a DNS response, we inferred that the DNS request was intercepted, and thus the tested website is censored.
Finally, we performed three separate evaluations to measure the effectiveness of different phrase sizes for finding censored websites.
With each evaluation, we only extracted unigrams, bigrams, and trigrams, respectively.
We also limited each evaluation to 1,000,000 URLs to be consistent with the methodology of FilteredWeb [8].
We also configured the Bing API calls so that any URLs from Blogspot, Facebook, Twitter, YouTube, and Tumblr would be ignored.
We did this for a couple of reasons.
For one, these websites are widely known to be censored in China, so we would not be providing new information by having these websites or their subdomains in our result set.
Furthermore, Tumblr and Blogspot assign a unique subdomain to each user's blog, and in some cases we'd get a dozen blogs from a single search query.
In order to find culture-specific websites that are normally buried by the top 50 search results, then, we need to omit these blogs.
We reached several key insights from performing our evaluations.
First, we were able to discover hundreds of censored websites that are not present on existing block lists.
Furthermore, we noticed that many websites on our block list receive very little traffic.
Lastly, we found that by using politically-charged phrases as search terms, we were able to find a disproportionately large amount of censored websites.
The rest of this section discusses the main findings in depth.
By using natural-language processing on Chinese web pages, we were able to discover hundreds of censored websites that are not present on the Citizen Lab block list-the standard for censorship measurements-and FilteredWeb's block list [6,8].
Furthermore, only 3 of the top 50 censored domains that appeared in our search results are in the top 50 censored domains found by FilteredWeb.
Figure 2 breaks down the URL count for the top 25 censored domains that we discovered.
These websites seem to mainly cover Chinese human rights issues, news, censorship circumvention, and more.
For instance, wiki.zhtube.com appears to be a Chinese Wikipedia mirror.The web page for zhtube.com seems to be the default page for a Chinese LNMP (Linux, NginX MySQL, PhP) installation on a virtual private server [19].
We found over 3000 URLs in our dataset that point to this website, which may suggest that Chinese websites are trying to circumvent the on-and-off censorship of Wikipedia in China [35].
Furthermore, blog.boxun.com is a United States-based outlet for Chinese news that relies heavily on anonymous submissions.
The owners of the website note that "Boxun often reports news that authorities do not tell the public, such as outbreaks of diseases, human rights violations, corruption scandals and disasters" [2].
Thus, by building on the approach of FilteredWeb, we were able to produce a qualitatively different block list.
We recommend putting these two block lists together to create a single block list that is both wide in scope and large in size.Intuitively, we were able to find hundreds of qualitatively different censored domains than those found by FilteredWeb because we could extract Chinese-specific topics from web pages.
We were able to do so by using TF-IDF with a Chinese corpus to rank the "uniqueness" of Chinese phrases that appeared on a given web page.
For example, the trigram 仅限于书面 ("only written") was poorly ranked on tiananmenmother.org-a Chinese democratic activist group-because although it appears frequently, it's a common phrase in Chinese, according to the Chinese corpus on PhraseFinder [27].
On the other hand, the trigram 自由亚洲电台 ("Radio Free Asia") was highly ranked because it didn't appear frequently, and it's an uncommon phrase in Chinese.
It should also be noted that Radio Free Asia is a broadcasting corporation whose stated mission is to "provide accurate and timely news and information to Asian countries whose governments prohibit access to a free press" [28].
Thus, by scoring Chinese phrases with TF-IDF against a Chinese corpus, we were able to determine which phrases are "content-rich" on a given web page.
Figure 3 shows the ranking of the websites we discovered on the Alexa Top 1,000,000.
Notably, many of the websites we discovered are spread throughout the tail of the list, and some of the websites are not even on the list at all.
Given that the top 100,000 websites likely receive the vast majority of traffic on the Internet, we can infer that censors in China are not just interested in blocking "big-name", popular websites.
They are actively seeking out websites of any size that contain "sensitive" content.
We also discovered a number of websites that fall outside the Alexa Top 1,000,000 altogether.
Without the use of an automated system that can discover censored websites, it's unlikely that the public would even be aware that these websites are blocked.
able to discover 719 domains that are not on any publicly available block list.
By using bigrams, we were able to discover 598 censored domains.
Lastly, by using trigrams, we were able to discover 579 censored domains.
In total, we discovered 1125 censored domains, none of which are on the Alexa Top 1000 or Darer et al.'s blocklist [1,8].
Each evaluation found 273 censored domains in common.
Furthermore, each of these evaluations were performed with 1,000,000 unique URLs, consistent with the methodology of FilteredWeb [8].
Figure 4 shows how many censored domains we discovered as a function of unique URLs crawled for each evaluation.
We also wanted to see if there is a correlation between the presence of certain phrases and whether or not a given website is censored, even if we have no ground truth.
For example, if we make a search for 中国侵犯人权 (Chinese human rights violation) and find that a particular search result is censored, then we cannot be certain whether the presence of that phrase caused the website to be blocked.
through search engines to find sensitive websites, the website could have been blocked because it contained totally different content.
Nevertheless, there seems to be some correlation for certain phrases.
Table 2 shows a sample of unigrams that returned the most number of unique filtered domains from Bing.
For example, the top four unigrams-Wang Qishan, Li Hongzhi, Guo Boxiong, and Hu Jintao-are the names of controversial figures in Chinese history.
Li Hongzhi is the leader of the Falun Gong spiritual movement, whose practitioners have been subject to persecution and censorship by the Chinese government since 1999 [10].
Similarly, Guo Boxiong is a former top official of the Chinese military that was sentenced to life in prison in 2016 for accepting bribes, according to the Chinese government [34].
Thus, if a Chinese website discusses these people, the website may become censored for containing "sensitive" content.There are also bigrams that correlate with a large percentage of censored domains, but they are more explicitly political than the unigrams.
Table 3 shows this result.
First, most of the bigrams refer to the Chinese Communist Party in some way.
Phrases such as 中共的威胁 (Chinese Communists threaten), 江泽民胡锦涛 (Jiang Zemin Hu Jintao), and 中国共产党的治安 (Public security of the CPC) do not necessarily convey sensitive information, but they nonetheless refer to the government of China.
On the other hand, some phrases clearly refer to political dissent, such as 官员呼吁 (Officials called on), 迫害活动 (Persecution activities), 非法拘留 (Illegal detention), and 宣称反共 (Declared anti-communist).
We see a similar result with trigrams, as shown by Table 4.
Phrases that stand out include 中国共产党的宗教 政策 (The Chinese Communist Party's religious policy), 天安门广场示威 (Tienanmen Square demonstrations), 1989年民主运动 (1989 democracy movement), and 采 取暴力镇压 (To take a violent crackdown).
Interestingly, we also see that discussion of China's religious policy, the "New Tang Dynasty"-a religious radio broadcast located in the United States-, and European Union legislation may also be considered sensitive content.
[31].
Together, these results suggest that references to collective political dissent are highly likely to be censored.
This is consistent with the findings of King et al. [17].
We built a block list of 1125 censored websites in China that are not present on the largest block list [8].
Furthermore, our list is 12.5× larger than the most widely used block list [6] and more.
We've made our source code and block list available on GitHub [15].
Automatically detecting which websites are censored in a given country is an open problem.
One way of improving our work would be to experiment with advanced natural-language processing techniques to identify better search terms.
For example, we could try using Stanford CoreNLP's part-of-speech tagger to identify phrases that describe some action against the Chinese government.
This approach might identify the following phrase: "Chinese citizens protest against the Communist Party on June 4th".
We believe that using such culture-specific phrases as search terms would enable us to discover even more websites that are censored in China.
We thank the anonymous FOCI '18 reviewers for their helpful feedback.
This research was supported by NSF awards CNS-1409635, CNS-1602399, and CNS- 1704105.
