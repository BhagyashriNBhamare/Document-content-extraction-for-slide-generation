Question informers play an important role in enhancing question classification for factual question answering.
Previous works have used conditional random fields (CRFs) to identify question informer spans.
However, in CRF-based models, the selection of a feature subset is a key issue in improving the accuracy of question informer prediction.
In this paper, we propose a hybrid approach that integrates Genetic Algorithms (GAs) with CRF to optimize feature subset selection in CRF-based question informer prediction models.
The experimental results show that the proposed hybrid GA-CRF model improves the accuracy of question informer prediction of traditional CRF models.
Question informers play an important role in enhancing question classification for factual question answering [3].
Krishnan et al. [3] introduced the notion of the answer type informer span of a question for question classification and showed that human-annotated informer spans lead to large improvements in the accuracy of question classification.
They defined that choosing a minimal, appropriate contiguous span of a question token, or tokens, as the informer span of a question, which is adequate for question classification.
For example, in the question: "What is the biggest city in the United State?"
the question informer is "city".
Thus "city" is the most important clue in the question for question classification.
Krishnan et al. reported that perfect knowledge of informer spans can enhance the predictive accuracy from 79.4% to 88% using linear Support Vector Machines (SVMs) on standard benchmarks of question classification.Question informers are only useful if question informer spans can be identified automatically.
Previous works have used Conditional Random Fields (CRFs) to identify question informer spans.
By using a parse of the question, Krishnan et al. [3] derived a set of multiresolution features to train a CRF model and achieved 85%-87% accuracy for question informer prediction.Krishnan et al. [3] also showed that the effect of the features chosen by a CRF model varies significantly depending on the accuracy of the CRF model.
In a machine learning approach, feature selection is an optimization problem that involves choosing an appropriate feature subset.
In CRF-based models, selection of the feature subset is a key issue in improving the accuracy of question informer prediction.
Genetic Algorithms (GAs) [2] have been widely used in feature selection in machine learning [10].
In this paper, we propose a hybrid approach that integrates GA with CRF to optimize feature subset selection in CRF-based question informer prediction models.The remainder of this paper is organized as follows.
Section 2 describes the background to question informers and previous works.
In section 3, we propose the hybrid GA-CRF approach for question informer prediction.
Section 4 discusses the experiment and the test bed, and Section 5 contains the experimental results.
Finally, in Section 6 we present our conclusions and indicate some future research directions.
Lafferty et al. [4] proposed using Conditional Random Fields (CRFs), a framework for building probabilistic models, to segment and label sequence data.
A CRF models Pr(y|x) using a Markov random field, with nodes corresponding to elements of the structured object y, and potential functions that are conditional on features of x. Learning is performed by setting parameters to maximize the likelihood of a set of (x,y) pairs given as training data [8].
CRFs are widely used for sequential learning problems like NP chunking, POS tagging, and name entity recognition (NER).
Recent works [1,4,8] have shown that CRFs have a consistent advantage over traditional Hidden Markov Models (HMMs) and Maximum Entropy Markov Models (MEMMs) [6] in the face of many redundant features.
Krishnan et al. [3] reported that they achieved 85%-87% accuracy of question informer prediction by using CRF model with a set of features.CRF++, which is developed by Taku Kudo, is a simple, customizable, and open source implementation of CRFs for segmenting and labeling sequenced data (CRF++ is available at: http://chasen.org/~taku/software/CRF++/).
It was designed for generic purposes and can be applied to a variety of NLP tasks, such as Named Entity Recognition, Information Extraction, and Text Chunking.
The benefit of using CRF++ is that it enables us to redefine feature sets and specify the feature templates in a flexible way.
Genetic Algorithms (GAs) are a class of heuristic search methods and computational models of adaptation and evolution based on the mechanics of natural selection and genetics [2].
GAs have been widely used for feature selection in machine learning [10] methods, such as SVM.
Feature selection is an optimization problem that involves the process of picking a subset of features that are relevant to the target concept and removing irrelevant or redundant features.
This is an important factor that determines the performance of the machine learning models.
In this paper, we propose integrating the GA architecture with CRF to optimize feature selection for CRF-based question informer prediction.
Fig 1 shows the architecture of the proposed hybrid GA-CRF approach for question informer prediction.
There are two phases in the architecture.
The first is the GA-CRF learning phase with a training dataset, while the second is the CRF test phase with a test dataset.The application of GA to CRF-based question informer prediction comprises the following steps.1) Encoding a feature subset of CRF with the structure of chromosomes: To apply GA to the search for the optimal feature subsets of CRF, the subsets must be encoded on a chromosome in the form of binary strings.The gene structure of the chromosomes for feature subset selection is presented in Figure 2.
The value of the codes for feature subset selection is set to a one-bit digit '0' or '1', where '0' means the corresponding feature is not selected, and '1' means that it is selected.
The length of each chromosome is n bits, where n is the number of features.
We use f i-2 , f i-1 , f i+0 , f i+1 , f i+2 to represent the sliding windows of each feature.
Figure 3 shows an example of feature subset encoding for GA.
3) Population: The population is a set of seed chromosomes used to find the optimal feature subsets.
calculate the fitness score of each chromosome.
In addition, the population is searched to find the encoded chromosome that maximizes the specific fitness function.
The values of the fitness functions for the items in the evaluation set are calculated and used to determine the suitability of each chromosome.
5) CRF model 10-fold Cross validation: In this procedure, the feature subsets derived by the previous procedure are applied to the CRF module.
The fitness function is determined by the F-score of 10-fold cross validation of the CRF model.
We use 10-fold cross validation on the training dataset of the CRF model as the fitness function of each chromosome to avoid over-fitting on the test dataset.6) Stopping criteria satisfied?
If the stopping criteria are satisfied, the best chromosome and a near optimal feature subset of CRF model is obtained; otherwise, apply GA operators and produce a new generation.
In this procedure, we use three GA operators, namely, reproduction, crossover, and mutation to produce a new generation.8) Apply the selected feature subsets to the CRF test dataset: After the GA-CRF learning process, we can obtain a near optimal feature subset of CRF.
We then train the whole training set on that feature subset to obtain a near optimal CRF prediction model, which we use to test the test dataset for CRF-based question informer prediction.
We use the UIUC QC dataset from Li and Roth [5] and the corresponding question informer dataset from Krishnan et al. [3].
There are 5,500 training questions, 500 test questions, and corresponding question informers 1 [5,9,11].
It has 6 coarse and 50 fine answer types in a two level taxonomy, together with 5,500 training and 500 test questions.
Krishnan et al. [3] reported that they had two volunteers to tag 6,000 UIUC questions with informer spans, which they call human-annotated "perfect" informer spans.
We adopt the 3-state transition model suggested by Krishnan et al [3] and follow the "begin/in/out" (BIO) model proposed by Ramshaw and Marcus [7] to tag question informers.
In our dataset for the CRF model, "O-QIF0" indicates outside and before a question informer, "B-QIF1" indicates the start of a question informer, while "O-QIF2" indicates outside and after a question informer.
[3] indicate that using features with 2 levels is adequate.
Features 12 to 18 are derived from heuristics that are suggested by Krishnan et al.
We add Features 19 to 21, namely, question wh-word (6W1H1O: who, what, when, where, which, why, how, and others), question length, and token position.In this study, we regard each feature candidate as a gene, and treat the corresponding F-score as the performance value of the feature (gene) for the CRF model.
Table 1 shows that word, POS, parser level 1, and parser level 2 have better performance for the CRF model.
For example, the F-score of a single feature used with the "word" feature is 58.35%, while using the "parser level 2" feature solely achieves a score of 48.13%.
The experiment result shows that each feature candidate has a different effect on the performance of CRF-based question informer prediction.
Figure 4 shows an example of a feature with sliding windows for a CRF model.
For example, "city" is the feature f ij for x i , where i = 4 and j=0.
Given that x 4 = "city", the label of prediction y 4 = "B-QIF1".
Figure 5 shows an example of feature generation and a feature template for CRF++.
For example, we can specify the feature "city" in feature f 0,0 as the feature template "U02:%[0,0]", and the previous feature "oldest" in feature f -1,0 as the feature template "U01:%[-1,0]".
Features fij for xiSliding Windows i -2 i -1 i +0 i +1 i +2 O-QIF2 DT [+2, 1] the [+2, 0] +2 O-QIF2 IN [+1, 1] in [+1, 0] +1 B-QIF1 NN [ 0, 1] city [ 0, 0] 0 O-QIF0 JJS [-1, 1] oldest[-1, 0] -1 O-QIF0 DT [-2, 1] the [-2, 0] -2 yi POS xi i 1 0 jFeatures fij for xi Figure 4.
An example of feature with sliding windows for CRF Null_1 SBARQ_1 IsTag0 IsNum0 IsPrevTag1 IsNextTag0 IsEdge0 IsBegin0 IsEnd0 Wh_what 10 1 O-QIF0 1 We encode all 21 feature candidates and sliding windows with the structure of chromosomes to form the feature subset for GA.
The candidates corresponding sliding window size is 5 (w= -2, -1, 0, +1, +2), We use f i-2 , f i-1 , f i+0 , f i+1 , f i+2 to represent the windows of each feature candidate.
Since there are 21 basic features and 5 sliding windows, we can generate 105 (21 basic features * 5 sliding windows) features (genes) for each chromosome.
Figure 7 shows an example of encoding a feature subset with the structure of chromosomes for GA.
For example, the chromosome "1 0 1 1 0 0 1 1 0 1" represents the selected feature subset is {F1, F3, F4, F7, F8 , F10}, and the corresponding feature template for CRF++ is { U00: F10 U09:%x[+2,1] f+2,1 DT F9 U08:%x[+1,1] f+1,1 IN F8 U07:%x[ 0,1] f0,1 NN F7 U06:%x[-1,1] f-1,1 JJS F6 U05%x[-2,1] f-2,1 DT F5 U04:%x[+2,0] f+2,0 the F4 U03:%x[+1,0] f+1,0 in F3 U02:%x[ 0,0] f0,0 city F2 U01:%x[-1,0] f-1,0 oldest F1 U00:%x[-2,0] f-2,0 the%x[-2,0] U02:%x[ 0,0] U03:%x[+1,0] U06:%x[-1,1] U07:%x[ 0,1] U09:%x[+2,1]}.
F10 U09:%x[+2,1] f+2,1 DT F9 U08:%x[+1,1] f+1,1 IN F8 U07:%x[ 0,1] f0,1 NN F7 U06:%x[-1,1] f-1,1 JJS F6 U05%x[-2,1] f-2,1 DT F5 U04:%x[+2,0] f+2,0 the F4 U03:%x[+1,0] f+1,0 in F3 U02:%x[ 0,0] f0,0 city F2 U01:%x[-1,0] f-1,0 oldest F1 U00:%x[-2,0] f-2,0 the In the learning and validation phases, we use 10-fold cross validation with 5,500 UIUC training data to reduce the over-fitting problem, and use the selected near optimal feature subset for the CRF model.
Figure 8 shows the experimental results of 10-fold cross validation on the training dataset and the corresponding performance on the test dataset using GA for feature subset selection of CRF-based question informer prediction.
The F-score is approximately 95% for 10-fold cross validation on the training dataset (UIUC Q5500), and approximately 88% on the test dataset (UIUC Q500).
Figure 9 shows the experimental results of CRF-based question informer prediction using GA for feature subset selection for a population whose characteristics are: size = 40, crossover rate = 80%, and mutation rate = 10%.
The F-score for question informer prediction is 93.87%.
It should be noted that the fitness function is used to evaluate on the test dataset (UIUC Q500) with the training dataset (UIUC Q5500).
F Score: 93.87, Population: 40, Crossover: 80%, Mutation: 10%, Generation: 100 After 100 generations of GA, we obtain the near optimal chromosomes and their corresponding feature subset for CRF++.
Figure 10 shows the near optimal chromosomes and the corresponding feature subset for the CRF model selected by GA.
The experimental results show that we can improve the F-score of CRF-based question informer prediction from 88.9% to 93.87% using GA to reduce the number of features from 105 to a 40-feature subset.
The accuracy of our proposed GA-CRF model for UIUC dataset is 95.58% compared with 87% for the traditional CRF model reported by Krishnan et al. [3].
The experimental results show that our proposed hybrid GA-CRF model for question informer prediction outperforms the traditional CRF model.
We have proposed a hybrid approach that integrates Genetic Algorithm (GA) with Conditional Random Field (CRF) to optimize feature subset selection in a CRFbased model for question informer prediction.
The experimental results show that the proposed hybrid GA-CRF model of question informer prediction improves the accuracy of the traditional CRF model.
By using GA to optimize the selection of the feature subset in CRF-based question informer prediction, we can improve the F-score from 88.9% to 93.87%, and reduce the number of features from 105 to 40.
Figure 10.
The near optimal chromosome and the corresponding feature subset for the CRF model selected by GA We would like to thank Vijay Krishnan for providing the question informer dataset and informative discussions.
This research was supported in part by the thematic program of Academia Sinica under Grant AS 95ASIA02, the National Science Council under Grant NSC 95-2752-E-001-001-PAE, NSC 95-2416-H-002-047, and NSC94-2218-E-324-003.
