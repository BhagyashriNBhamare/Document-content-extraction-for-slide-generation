The eigenspectrum of a graph Laplacian encodes smoothness information over the graph.
A natural approach to learning involves transforming the spectrum of a graph Laplacian to obtain a kernel.
While manual exploration of the spectrum is conceivable, non-parametric learning methods that adjust the Laplacian's spectrum promise better performance.
For instance, adjusting the graph Laplacian using kernel target alignment (KTA) yields better performance when an SVM is trained on the resulting kernel.
KTA relies on a simple surrogate criterion to choose the kernel; the obtained kernel is then fed to a large margin classification algorithm.
In this paper, we propose novel formulations that jointly optimize relative margin and the spectrum of a kernel defined via Laplacian eigenmaps.
The large relative margin case is in fact a strict generalization of the large margin case.
The proposed methods show significant empirical advantage over numerous other competing methods.
This paper considers the transductive learning problem where a set of labeled examples is accompanied with unlabeled examples whose labels are to be predicted by an algorithm.
Due to the availability of additional information in the unlabeled data, both the labeled and unlabeled examples will be utilized to estimate a kernel matrix which can then be fed into a learning algorithm such as the support vector machine (SVM).
One particularly successful approach for estimating such a kernel matrix is by transforming the spectrum of the graph Laplacian [8].
A kernel can be constructed from the eigenvectors corresponding to the smallest eigenvalues of a Laplacian to maintain smoothness on the graph.
In fact, the diffusion kernel [5] and the Gaussian field kernel [12] are based on such an approach and explore smooth variations of the Laplacian via specific parametric forms.
In addition, a number of other transformations are described in [8] for exploring smooth functions on the graph.
Through the controlled variation of the spectrum of the Laplacian, a family of allowable kernels can be explored in an attempt to improve classification accuracy.
Further, Zhang & Ando [10] provide generalization analysis for spectral kernel design.Kernel target alignment (KTA for short) [3] is a criterion for evaluating a kernel based on the labels.
It was initially proposed as a method to choose a kernel from a family of candidates such that the Frobenius norm of the difference between a label matrix and the kernel matrix is minimized.
The technique estimates a kernel independently of the final learning algorithm that will be utilized for classification.
Recently, such a method was proposed to transform the spectrum of a graph Laplacian [11] to select from a general family of candidate kernels.
Instead of relying on parametric methods for exploring a family of kernels (such as the scalar parameter in a diffusion or Gaussian field kernel), Zhu et al. [11] suggest a more general approach which yields a kernel matrix nonparametrically that aligns well with an ideal kernel (obtained from the labeled examples).
In this paper, we propose novel quadratically constrained quadratic programs to jointly learn the spectrum of a Laplacian with a large margin classifier.
The motivation for large margin spectrum transformation is straightforward.
In kernel target alignment, a simpler surrogate criterion is first optimized to obtain a kernel by transforming the graph Laplacian.
Then, the kernel obtained is fed to a classifier such as an SVM.
This is a two-step process with a different objective function in each step.
It is more natural to transform the Laplacian spectrum jointly with the classification criterion in the first place rather than using a surrogate criterion to learn the kernel.Recently, another discriminative criterion that generalizes large absolute margin has been proposed.
The large relative margin [7] criterion measures the margin relative to the spread of the data rather than treating it as an absolute quantity.
The key distinction is that large relative margin jointly maximizes the margin while controlling or minimizing the spread of the data.
Relative margin machines (RMM) implement such a discriminative criterion through additional linear constraints that control the spread of the projections.
In this paper, we consider this aggressive classification criterion which can potentially improve over the KTA approach.
Since large absolute margin and large relative margin criteria are more directly tied to classification accuracy and have generalization guarantees, they potentially could identify better choices of kernels from the family of admissible kernels.
In particular, the family of kernels spanned by spectral manipulations of the Laplacian will be considered.
Since the RMM is more general compared to SVM, by proposing a large relative margin spectrum learning, we encompass large margin spectrum learning as a special case.
In this paper we assume that a set of labeled examples (x i , y i ) l i=1 and an unlabeled set (x i ) n i=l+1 are given such that x i ∈ R m and y i ∈ {±1}.
We denote by y ∈ R l the vector whose i th entry is y i and by Y ∈ R l×l a diagonal matrix such that Y ii = y i .
The primary aim is to obtain predictions on the unlabeled examples; we are thus in a so-called transductive setup.
However, the unlabeled examples can be also be utilized in the learning process.Assume we are given a graph with adjacency matrix W ∈ R n×n where the weight W ij denotes the edge weight between nodes i and j (corresponding to the examples x i and x j ).
Define the graph Laplacian as L = D−W where D denotes a diagonal matrix whose i th entry is given by the sum of the i th row of W.
We assume that L = n i=1 θ i φ i φ ⊤ i is the eigendecomposition of L.
It is assumed that the eigenvalues are already arranged such that θ i ≤ θ i+1 for all i. Further, we let V ∈ R n×q be the matrix whose i th column is the (i + 1) th eigenvector (corresponding to the (i + 1) th smallest eigenvalue) of L. Note that the first eigenvector (corresponding to the smallest eigenvalue) has been deliberately left out from this definition.
Further, U ∈ R n×q is defined to be the matrix whose i th column is the i th eigenvector.
v i (u i ) denotes the i th column of V ⊤ (U ⊤ ).
For any eigenvector (such as φ, u or v), we use the horizontal overbar (such as ¯ φ, ¯ u or ¯ v) to denote the subvector containing only the first l elements of the eigenvector, in other words, only the entries that correspond to the labeled examples.
We overload this notation for matrices as well; thus ¯ V ∈ R l×q ( ¯ U ∈ R l×q ) denotes 1 the first l rows of V (U).
∆ is assumed to be a q × q diagonal matrix whose diagonal elements denote scalar values δ i (i.e., ∆ ii = δ i ).
Finally 0 and 1 denote vectors of all zeros and all ones; their dimensionality can be inferred from the context.
The graph Laplacian has been particularly popular in transductive learning.
While we can hardly do justice to all the literature, this section summarizes some of the most relevant previous approaches.Spectral Graph Transducer The spectral graph transducer [4] is a transductive learning method based on a relaxation of the combinatorial graph-cut problem.
It obtains predictions on labeled and unlabeled examples by solving for h ∈ R n via the following problem:min h∈R n 1 2 h ⊤ VQV ⊤ h + C(h − τ ) ⊤ P(h − τ ) s.t. h ⊤ 1 = 0, h ⊤ h = n (1)where P is a diagonal matrix 2 with P ii = 1 l+ ( 1 l− ) if the i th example is positive (negative); P ii = 0 for unlabeled examples (i.e., for l + 1 ≤ i ≤ n).
Further, Q is also a diagonal matrix.
Typically, the diagonal element Q ii is set to i 2 [4].
τ is a vector in which the values corresponding to the positive (negative) examples are set tol− l+ ( l+ l− ).
Non-parametric transformations via kernel target alignment (KTA) In [11], a successful approach to learning a kernel was proposed which involved transforming the spectrum of a Laplacian in a non-parametric way.
The empirical 1 We clarify that ¯ V ⊤ ( ¯ U ⊤ ) denotes the transpose of ¯ V ( ¯ U).
alignment between two kernel matrices K 1 and K 2 is defined as [3]:ˆ A(K 1 , K 2 ) := K 1 , K 2 F K 1 , K 1 F K 2 , K 2 F .
When the target y (the vector formed by concatenating y i 's) is known, the ideal kernel matrix is yy ⊤ and a kernel matrix K can be learned by maximizing the alignmentˆAalignmentˆ alignmentˆA( ¯ K, yy ⊤ ).
The kernel target alignment approach [11] learns a kernel via the following formulation:3 max ∆ ˆ A( ¯ U∆ ¯ U ⊤ , yy ⊤ ) (2) s.t. trace(U∆U ⊤ ) = 1 δ i ≥ δ i+1 ∀2 ≤ i ≤ q − 1, δ q ≥ 0, δ 1 ≥ 0.
The above optimization problem transforms the spectrum of the given graph Laplacian L while maximizing the alignment score of the labeled part of the kernel matrix ( ¯ U∆ ¯ U ⊤ ) with the observed labels.
The trace constraint on the overall kernel matrix (U∆U ⊤ ) is used merely to control the arbitrary scaling.
The above formulation can be posed as a quadratically constrained quadratic program (QCQP) that can be solved efficiently [11].
The ordering on the δ's is in reverse order as that of the eigenvalues of L which amounts to monotonically inverting the spectrum of the graph Laplacian L. Only the first q eigenvectors are considered in the formulation above due to computational considerations.The eigenvector φ 1 is made up of a constant element.
Thus, it merely amounts to adding a constant to all the elements of the kernel matrix.
Therefore, the weight on this vector (i.e. δ 1 ) is allowed to vary freely.
Finally, note that the φ's are the eigenvectors of L so the trace constraint on U∆U ⊤ merely corresponds to the constraintq i=1 δ i = 1 since U ⊤ U = I.Parametric transformations A number of methods have been proposed to obtain a kernel from the graph Laplacian.
These methods essentially compute the Laplacian over labeled and unlabeled data and transform its spectrum with a particular mapping.
More precisely, a kernel is built asK = n i=1 r(θ i )φ i φ ⊤ iwhere r(·) is a monotonically decreasing function.
Thus, an eigenvector with a small eigenvalue will have a large weight in the kernel matrix.
Several methods fall into this category.
For example, the diffusion kernel [5] is obtained by the transformation r(θ) = exp(−θ/σ 2 ) and the Gaussian field kernel [12] uses the transformation r(θ) = 1 σ 2 +θ .
In fact, kernel PCA [6] also performs a similar operation.
In kPCA, we retain the top k eigenvectors of a kernel matrix.
From an equivalence that exists between the kernel matrix and the graph Laplacian (shown in the next section), we can in fact conclude that kernel PCA features also fall under the same family of monotonic transformations.
While these are very interesting transformations, [11] showed that KTA and learning based approaches are empirically superior to parametric transformations so we will not elaborate further on these approaches but rather focus on learning the spectrum of a graph Laplacian.
We start with an optimization problem which is closely related to the spectral graph transducer (1).
The main difference is in the choice of the loss function.
Consider the following optimization problem:min h∈R n 1 2 h ⊤ VQV ⊤ h + C l i=1 max(0, 1 − y i h i ),(3)where Q is assumed to be an invertible diagonal matrix to avoid degeneracies.
4 The values on the diagonal of Q depend on the particular choice of the kernel.The above optimization problem is essentially learning the predictions on all the examples by minimizing the so-called hinge loss and the regularization defined by the eigenspace of the graph Laplacian.
The choice of the above formulation is due to its relation to the large margin learning framework given by the following theorem.Theorem 1.
The optimization problem (3) is equivalent to min w,b 1 2 w ⊤ w + C l i=1 max(0, 1 − y i (w ⊤ Q − 1 2 v i + b)).
(4)Proof.
The predictions on all the examples (without the bias term) for the optimization problem (4) are given byf = VQ − 1 2 w. Therefore Q 1 2 V ⊤ f = Q 1 2 V ⊤ VQ − 1 2 w = w since V ⊤ V = I.Substituting this expression for w in (4), the optimization problem becomes,min f ,b 1 2 f ⊤ VQV ⊤ f + C l i=1 max(0, 1 − y i (f i + b)).
Let h = f + b1 and consider the first term in the objective above,(h − b1) ⊤ VQV ⊤ (h − b1) =h ⊤ VQV ⊤ h + 2h ⊤ VQV ⊤ 1 + 1 ⊤ VQV ⊤ 1 = h ⊤ VQV ⊤ h,where we have used the fact that V ⊤ 1 = 0 since the eigenvectors in V are orthogonal to 1.
This is because 1 is always an eigenvector of L and other eigenvectors are orthogonal to it.
Thus, the optimization problem (3) follows.
⊓ ⊔The above theorem 5 thus implies that learning predictions with Laplacian regularization in (3) is equivalent to learning in a large margin setting (4).
It is easy to see that the implicit kernel for the learning algorithm (4) (over both labeled and unlabeled examples) is given by VQ −1 V ⊤ .
Thus, computing predictions on all examples with VQV ⊤ as the regularizer in (3) is equivalent to large margin learning with the kernel obtained by inverting the spectrum Q. However, it is not clear why inverting the spectrum of a Laplacian is the right choice for a kernel.
The parametric methods presented in the previous section construct this kernel by exploring specific parametric forms.
On the other hand, the kernel target alignment approach constructs this kernel by maximizing alignment with labels while maintaining an ordering on the spectrum.
The spectral graph transducer in Section 2 uses 6 the transformation i 2 on the Laplacian for regularization.
In this paper, we explore a family of transformations and allow the algorithm to choose the one that best conforms to a large (relative) margin criterion.
Instead of relying on parametric forms or using a surrogate criteria, this paper presents approaches that jointly obtain a transformation and a large margin classifier.
Relative margin machines (RMM) [7] measure the margin relative to the data spread; this approach has yielded significant improvement over SVMs and has enjoyed theoretical guarantees as well.
In its primal form, the RMM solves the following optimization problem:7 min w,b,ξ 1 2 w ⊤ w + C l i=1 ξ i (5) s.t. y i (w ⊤ x i + b) ≥ 1 − ξ i , ξ i ≥ 0, |w ⊤ x i + b| ≤ B ∀1 ≤ i ≤ l.Note that when B = ∞, the above formulation gives back the support vector machine formulation.
For values of B below a threshold, the RMM gives solutions that differ from SVM solutions.
The dual of the above optimization problem can be shown to be:max α,β,η − 1 2 γ ⊤ X ⊤ Xγ + α ⊤ 1 − B β ⊤ 1 + η ⊤ 1 (6) s.t. α ⊤ y − β ⊤ 1 + η ⊤ 1 = 0, 0 ≤ α ≤ C1, β ≥ 0, η ≥ 0.
In the dual, we have defined γ := Yα − β + η for brevity.
Note that α ∈ R l , β ∈ R l and η ∈ R l are the Lagrange multipliers corresponding to the constraints in (5).
Based on the motivation from earlier sections, we consider the problem of jointly learning a classifier and weights on various eigenvectors in the RMM setup.
We restrict the family of weights to be the same as that in (2) in the following problem:min w,b,ξ,∆ 1 2 w ⊤ w + C l i=1 ξ i (7) s.t. y i (w ⊤ ∆ 1 2 u i + b) ≥ 1 − ξ i , ξ i ≥ 0 ∀1 ≤ i ≤ l |w ⊤ ∆ 1 2 u i + b| ≤ B ∀1 ≤ i ≤ l δ i ≥ δ i+1 ∀2 ≤ i ≤ q − 1, δ 1 ≥ 0, δ q ≥ 0, trace(U∆U ⊤ ) = 1.
By writing the dual of the above problem over w, b and ξ, we get:min ∆ max α,β,η − 1 2 γ ⊤ ¯ U∆ ¯ U ⊤ γ + α ⊤ 1 − B β ⊤ 1 + η ⊤ 1(8)s.t. α ⊤ y − β ⊤ 1 + η ⊤ 1 = 0, 0 ≤ α ≤ C1, β ≥ 0, η ≥ 0, δ i ≥ δ i+1 ∀2 ≤ i ≤ q − 1, δ 1 ≥ 0, δ q ≥ 0, q i=1 δ i = 1.
where we exploited the fact that trace(U∆U ⊤ ) = q i=1 δ i .
Clearly, the above optimization problem, without the ordering constraints (i.e., δ i ≥ δ i+1 ) is simply the multiple kernel learning 8 problem (using the RMM criterion instead of the standard SVM).
A straightforward derivation-following the approach of [1]results in the corresponding multiple kernel learning optimization.
Even though the optimization problem (8) without the ordering on δ's is a more general problem, it may not produce smooth predictions over the entire graph.
This is because, with a small number of labeled examples (i.e., small l), it is unlikely that multiple kernel learning will maintain the spectrum ordering unless it is explicitly enforced.
In fact, this phenomenon can frequently be observed in our experiments where multiple kernel learning fails to maintain a meaningful ordering on the spectrum.
This section poses the optimization problem (8) in a more canonical form to obtain practical large-margin (denoted by STOAM) and large-relative-margin (denoted by STORM) implementations.
These implementations achieve globally optimal joint estimates of the kernel and the classifier of interest.
First, the min 8 In this paper, we restrict our attention to convex combination multiple kernel learning algorithms.and the max in (8) can be interchanged since the objective is concave in ∆ and convex in α, β and η and both are strictly feasible [2] 9 .
Thus, we can write:max α,β,η min ∆ − 1 2 γ ⊤ q i=1 δ i ¯ u i ¯ u ⊤ i γ + α ⊤ 1 − B β ⊤ 1 + η ⊤ 1 (9) s.t. α ⊤ y − β ⊤ 1 + η ⊤ 1 = 0, 0 ≤ α ≤ C1, β ≥ 0, η ≥ 0, δ i ≥ δ i+1 ∀2 ≤ i ≤ q − 1, δ 1 ≥ 0, δ q ≥ 0, q i=1 δ i = 1.
We first discuss a naive attempt to simplify the optimization that is not fruitful.
Consider the inner optimization over ∆ in the above optimization problem (9):min ∆ − 1 2 q i=1 δ i γ ⊤ ¯ u i ¯ u ⊤ i γ(10)s.t. δ i ≥ δ i+1 ∀2 ≤ i ≤ q − 1, δ 1 ≥ 0, δ q ≥ 0, q i=1 δ i = 1.
Lemma 1.
The dual of the above formulation is:max τ,λ − τ s.t. 1 2 γ ⊤ ¯ u i ¯ u ⊤ i γ = λ i−1 − λ i + τ, λ i ≥ 0 ∀1 ≤ i ≤ q.where λ 0 = 0 is a dummy variable.Proof.
Start by writing the Lagrangian of the optimization problem:L = − 1 2 q i=1 δ i γ ⊤ ¯ u i ¯ u ⊤ i γ − q−1 i=2 λ i (δ i − δ i+1 ) − λ q δ q − λ 1 δ 1 + τ ( q i=1 δ i − 1),where λ i ≥ 0 and τ are Lagrange multipliers.
The dual follows after differentiating L with respect to δ i and equating the resulting expression to zero.
⊓ ⊔ Caveat While the above dual is independent of δ's, the constraints 1 2 γ ⊤ ¯ u i ¯ u ⊤ i γ = λ i−1 − λ i + τ involve a quadratic term in an equality.
It is not possible to simply leave out λ i to make this constraint an inequality since the same λ i occurs in two equations.
This is non-convex in γ and is problematic since, after all, we eventually want an optimization problem that is jointly convex in γ and the other variables.
Thus, a reformulation is necessary to pose relative margin kernel learning as a jointly convex optimization problem.
We proceed by instead considering the following optimization problem:min ∆ − 1 2 q i=1 δ i γ ⊤ ¯ u i ¯ u ⊤ i γ (11) s.t. δ i − δ i+1 ≥ ǫ ∀2 ≤ i ≤ q − 1, δ 1 ≥ ǫ, δ q ≥ ǫ, q i=1 δ i = 1where we still maintain the ordering of the eigenvalues but require that they are separated by at least ǫ.
Note that ǫ > 0 is not like other typical machine learning algorithm parameters (such as the parameter C in SVMs), since it can be arbitrarily small.
The only requirement here is that ǫ remains positive.
Thus, we are not really adding an extra parameter to the algorithm in posing it as a QCQP.
The following theorem shows that a change of variables can be done in the above optimization problem so that its dual is in a particularly convenient form; note, however, that directly deriving the dual of (11) fails to give the desired property and form.Theorem 2.
The dual of the optimization problem (11) is:max λ≥0,τ − τ + ǫ q i=1 λ i(12)s.t. 1 2 γ ⊤ i j=2 ¯ u j ¯ u ⊤ j γ = τ (i − 1) − λ i ∀2 ≤ i ≤ q 1 2 γ ⊤ ¯ u 1 ¯ u ⊤ 1 γ = τ − λ 1 .
Proof.
Start with the following change of variables:κ i :=    δ 1 for i = 1, δ i − δ i+1 for 2 ≤ i ≤ q − 1, δ q for i = q.This gives:δ i = κ 1 for i = 1, q j=i κ j for 2 ≤ i ≤ q.(13)Thus, (11) can be stated as,min κ − 1 2 q i=2 q j=i κ j γ ⊤ ¯ u i ¯ u ⊤ i γ + κ 1 γ ⊤ ¯ u 1 ¯ u ⊤ 1 γ(14)s.t. κ i ≥ ǫ ∀1 ≤ i ≤ q, and Consider simplifying the following term within the above formulation:q i=2 q j=i κ j γ ⊤ ¯ u i ¯ u ⊤ i γ = q i=2 κ i i j=2 γ ⊤ ¯ u j ¯ u ⊤ j γ and q i=2 q j=i κ j = q i=2 (i − 1)κ i .
It is now straightforward to write the Lagrangian to obtain the dual.
⊓ ⊔ Even though the above optimization appears to have non-convexity problems mentioned after Lemma 1, these can be avoided.
This is facilitated by the following helpful property.Lemma 2.
For ǫ > 0, all the inequality constraints are active at the optimum of the following optimization problem:max λ≥0,τ − τ + ǫ q i=1 λ i (15) s.t. 1 2 γ ⊤ i j=2 ¯ u j ¯ u ⊤ j γ ≤ τ (i − 1) − λ i ∀2 ≤ i ≤ q 1 2 γ ⊤ ¯ u 1 ¯ u ⊤ 1 γ ≤ τ − λ 1 .
Proof.
Assume that λ * is the optimum for the above problem and constraint i (corresponding to λ i ) is not active.
Then, clearly, the objective can be further maximized by increasing λ * i .
This contradicts the fact that λ * is the optimum.
In fact, it is not hard to show that the Lagrange multipliers of the constraints in problem (15) are equal to the κ i 's.
Thus, replacing the inner optimization over δ's in (9), by (15), we get the following optimization problem, which we call STORM (Spectrum Transformations that Optimize the Relative Margin): max α,β,η,λ,τα ⊤ 1 − τ + ǫ q i=1 λ i − B β ⊤ 1 + η ⊤ 1 (16) s.t. 1 2 (Yα − β + η) ⊤ i j=2 ¯ u j ¯ u ⊤ j (Yα − β + η) ≤ (i − 1)τ − λ i ∀2 ≤ i ≤ q 1 2 (Yα − β + η) ⊤ ¯ u 1 ¯ u ⊤ 1 (Yα − β + η) ≤ τ − λ 1 α ⊤ y − β ⊤ 1 + η ⊤ 1 = 0, 0 ≤ α ≤ C1, β ≥ 0, η ≥ 0, λ ≥ 0.
The above optimization problem has a linear objective with quadratic constraints.
This equation now falls into the well-known family of quadratically constrained quadratic optimization (QCQP) problems whose solution is straightforward in practice.
Thus, we have proposed a novel QCQP for large relative margin spectrum learning.
Since the relative margin machine is strictly more general than the support vector machine, we obtain STOAM (Spectrum Transformations that Optimize the Absolute Margin) by simply setting B = ∞.
Obtaining δ values Interior point methods obtain both primal and dual solutions of an optimization problem simultaneously.
We can use equation (13) to obtain the weight on each eigenvector to construct the kernel.Computational complexity STORM is a standard QCQP with q quadratic constraints of dimensionality l.
This can be solved in time O(ql 3 ) with an interior point solver.
We point out that, typically, the number of labeled examples l is much smaller than the total number of examples (which is n).
Moreover, q is typically a fixed constant.
Thus the runtime of the proposed QCQP compares favorably with the O(n 3 ) time for the initial eigendecomposition of L which is required for all the spectral methods described in this paper.
To study the empirical performance of STORM and STOAM with respect to previous work, we performed experiments on both text and digit classification problems.
Five binary classification problems were chosen from the 20-newsgroups text dataset (separating categories like baseball-hockey (b-h), pc-mac (p-m), religion-atheism (r-a), windows-xwindows (w-x), and politics.mideast-politics.
misc (m-m)).
Similarly, five different problems were considered from the MNIST dataset (separating digits 0-9, 1-2, 3-8, 4-7, and 5-6).
One thousand randomly sampled examples were used for each task.A mutual nearest neighbor graph was first constructed using five nearest neighbors and then the graph Laplacian was computed.
The elements of the weight matrix W were all binary.
In the case of MNIST digits, raw pixel values (note that each feature was normalized to zero-mean and unit variance) were used as features.
For digits, nearest neighbors were determined by Euclidean distance, whereas, for text, the cosine similarity and tf-idf was used.
In the experiments, the number of eigenvalues q was set to 200.
This was a uniform choice for all methods which would not yield any unfair advantages for one approach over any other.
In the case of STORM and STOAM, ǫ was set to a negligible value of 10 −6 .
The entire dataset was randomly divided into labeled and unlabeled examples.
The number of labeled examples was varied in steps of 20; the rest of the examples served as the test examples (as well as the unlabeled examples in graph construction).
We then ran KTA to obtain a kernel; the estimated kernel was then fed into an SVM (this was referred to as KTA-S in the Tables) as well as to an RMM (referred to as KTA-R).
To get an idea of the extent to which the ordering constraints matter, we also ran multiple kernel learning optimization which are similar to STOAM and STORM but without any ordering constraints.
We refer to the multiple kernel learning with the SVM objective as MKL-S and with the RMM objective as MKL-R.
We also included the spectral graph transducer (SGT) and the approach of [9] (described in the Appendix) in the experiments.
Predictions on all the unlabeled examples were obtained for all the methods.
Error rates were evaluated on the unlabeled examples.
Twenty such runs were done for various values of hyper-parameters (such as C,B) for all the methods.
The values of the hyper-parameters that resulted in minimum average error rate over unlabeled examples were selected for all the approaches.
Once the hyperparameter values were fixed, the entire dataset was again divided into labeled and unlabeled examples.
Training was then done but with fixed values of various hyper-parameters.
Error rates on unlabeled examples were then obtained for all the methods over hundred runs of random splits of the dataset.
Table 1.
Mean and std.
deviation of percentage error rates on text datasets.
In each row, the method with minimum error rate is shown in dark gray.
All the other algorithms whose performance is not significantly different from the best (at 5% significance level by a paired t-test) are shown in light gray.DATA l [9] MKL-S MKL-R SGT KTA-S KTA-R STOAM STORM r-The results are presented in Table 1 and Table 2.
It can be seen that STORM and STOAM perform much better than all the methods.
Results in the two tables are further summarized in Table 3.
It can be seen that both STORM and STOAM have significant advantages over all the other methods.
Moreover, the formulation of [9] gives very poor results since the learned spectrum is independent of α.
Table 2.
Mean and std.
deviation of percentage error rates on digits datasets.
In each row, the method with minimum error rate is shown in dark gray.
All the other algorithms whose performance is not significantly different from the best (at 5% significance level by a paired t-test) are shown in light gray.To gain further intuition, we visualized the learned spectrum in each problem to see if the algorithms yield significant differences in spectra.
We present four typical plots in Figure 1.
We show the spectra obtained by KTA, STORM and MKL-R (the difference between the spectra obtained by STOAM (MKL-S) was much closer to that obtained by STORM (MKL-R) compared to other methods).
Typically KTA puts significantly more weight on the top few eigenvectors.
By not maintaining the order among the eigenvectors, MKL seems to put haphazard weights on the eigenvectors.
However, STORM is less aggressive and its eigenspectrum decays at a slower rate.
This shows that STORM obtains a markedly different spectrum compared to KTA and MKL and is recovering a qualitatively different kernel.
It is important to point out that MKL-R (MKL-S) solves a more general problem than STORM (STOAM).
Thus, it can always achieve a better objective value compared to STORM (STOAM).
However, this causes over-fitting and the experiments show that the error rate on the unlabeled examples actually increases when the order of the spectrum is not preserved.
In fact, MKL obtained competitive results in only one case (digits:1-2) which could be attributed to chance.
[9] MKL-S MKL-R SGT KTA-S KTA-R STOAM STORM #dark gray 0 Tables 1 & 2.
For each method, we enumerate the number of times it performed best (dark gray), the number of times it was not significantly worse than the best performing method (light gray) and the total number of times it was either best or not significantly worse from best.
We proposed a large relative margin formulation for transforming the eigenspectrum of a graph Laplacian.
A family of kernels was explored which maintains smoothness properties on the graph by enforcing an ordering on the eigenvalues of the kernel matrix.
Unlike the previous methods which used two distinct criteria at each phase of the learning process, we demonstrated how jointly optimizing the spectrum of a Laplacian while learning a classifier can result in improved performance.
The resulting kernels, learned as part of the optimization, showed improvements on a variety of experiments.
The formulation (3) shows that we can learn predictions as well as the spectrum of a Laplacian jointly by convex programming.
This opens up an interesting direction for further investigation.
By learning weights on an appropriate number of matrices, it is possible to explore all graph Laplacians.
Thus, it seems possible to learn both a graph structure and a large (relative) margin solution jointly.
