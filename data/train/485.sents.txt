Balancing peer-to-peer graphs, including zone-size distributions , has recently become an important topic of peer-to-peer (P2P) research [1], [2], [6], [19], [31], [36].
To bring analytical understanding into the various peer-join mechanisms , we study how zone-balancing decisions made during the initial sampling of the peer space affect the resulting zone sizes and derive several asymptotic results for the maximum and minimum zone sizes that hold with high probability.
The latest peer-to-peer networks organize users into massive (millions of nodes) graphs called Distributed Hash Tables (DHTs), which provide a scalable, efficient, and faulttolerant environment for exchanging information between end-users.
Even though static DHTs received significant attention in traditional approaches [35], [37], [38], [43], [45] and more-recent developments [8], [15], [17], [23], [25], [27], [33], [42], one of the most important areas of peer-to-peer research remains the study of evolving DHT graphs as users randomly join and leave the system [2], [3], [6], [18], [19], [24], [31], [36].
Many performance metrics in a dynamic graph are determined by the distribution of DHT zones held by each peer.The imbalance in zone sizes may lead to increased diameter, smaller node degree, lower bisection width, and higher local congestion during routing through the graph.
In addition, uneven zone distribution results in an unfair allocation of user objects to peers and creates "hotspots" in certain parts of the graph.
Even though hotspots can be relieved with more sophisticated object-hashing techniques [4], [6], [7], they have no effect on the weakened structure of the underlying graph.In this paper, we study several recently proposed nodejoin mechanisms in peer-to-peer networks and derive the corresponding probabilistic bounds on the maximum and minimum zone size after all users have joined the system.
In a random graph of size n, define f max to be the ratio of the largest zone size to the average zone size and fmin to be the ratio of the average zone size to the smallest zone size.
Among methods that sample a single point in the DHT space [35], [37], [38], it is well known that f max is Θ(log n) with high probability [18], [33], [38].
We improve this result by deriving the exact upper and lower bounds on fmax that hold with probability 1 − n −ε , for arbitrary constants ε, under both random and center splits of existing nodes.Although the largest zone is usually studied for the purposes of load-balancing user objects/keys and the bounds are clear [18], [33], [38], the minimum zone has not received as much attention.
Naor et al. [33] state without proof that the minimum zone is smaller than average by the same factor Θ(log n).
Both Loguinov et al. [25] and Fraigniaud et al. [15] implicitly assume in their derivations that fmin is o(n), while [15] additionally concludes that fmin ≤ 2 O(log n) , which essentially means that the upper bound is any powerfunction of n. To reconcile these partial results, we show that f min is upper bounded by n 1+ε with probability 1−n −ε under random splits and by 3.246 √ log n with probability 1 − o(1) under center splits, where log(.)
represents the natural logarithm throughout the paper.
We further show that splitting existing neighbors in the center is in fact optimal among all possible splitting methods and the use of uniform (as opposed to non-uniform) hash indexes provides the best possible performance in terms of both f max and f min .
Among multi-point sampling methods, Naor et al. [33] select d random points in the DHT space and choose the largest node to split (i.e., the approach used in the classical balls-into-bins "power of two choices" [4], [20], [30], [40]).
They show using Chernoff-type bounds that for d = 8 log n, f max is upper-bounded by 2 with probability 1 − n −2 .
We analyze the same problem using an approach from "ballsinto-bins" [29] and derive asymptotic upper/lower bounds on f max for arbitrary d.
Our results show that in large graphs:f max ≤ 2 + (1 + ε) log n d (1 − o(1))(1)simultaneously holds for all values of n with probability at least 1 − n −ε .
Specifically, for d = c log n, fmax ≤ 2 + 1+ε c with high probability.
Since the bound in (1) is tight, this result extends the one shown in [33] and demonstrates that c must tend to infinity for f max to converge to 2.
Also notice that multi-point sampling does not lead to the classical Θ( log log n log d) bound on fmax as might have been expected from the analysis of various balls-into-bins problems [4], [30].
Another zone-balancing approach is first suggested in CAN [35] and later analyzed by Adler et al. in [2].
In this method, each new node samples a random peer x in the graph and then queries d direct neighbors of x (the graph is assumed to be d-regular).
The paper demonstrates that as long as the degree of each node is Ω(log n), both fmax and fmin are some constants (the exact value of the constants is not shown).
We study a similar problem, in which nodes are allowed to sample other parts of the graph based on some deterministic function (which, for example, may represent the graph's linking rules), and derive upper bounds on fmax under this model.
Our analysis shows that when d = c log n, the following bound holds with probability at least 1 − n −ε :f max ≤ 2 + 1 + ε c + η,(2)where η is log(1+ 1+εc +log(1+ 1+ε c +.
.
.
This is in contrast to the purely random model where fmax converges to 2 + 1+ε c .
Using this insight, we find that for d ≈ log n and ε ≈ 1, the deterministic model requires 2.2 times more samples than the purely random model to achieve the same bounds on f max .
Finally, Loguinov et al. [25] use a variation of Adler's approach [2], in which the joining peer walks along the edges of the graph starting in a random location and splitting the largest node found within a certain number of hops from the initial node.
At each step, the walk is biased towards the largest neighbor; however, since the location of this neighbor varies during the evolution of the graph, closed-form analysis of this approach is rather complicated.
We do not offer a model for this method at this time, but compare its performance with that of the remaining methods in simulations.Other P2P balancing methods include the virtual-server approach originally used in Chord [17], [34], [38], the Messor system [32], proximity-aware balancing [44], cluster-based balancing [31], and several other dynamic algorithms [1], [19], [36], which provide alternative mechanisms for balancing P2P graphs and are orthogonal to our analysis.
This paper is organized as follows.
Section 2 provides the background and motivation.
Section 3 studies the randomsplit model and derives bounds for both f max and f min .
In section 4, we re-derive the same bounds for the singlesample, center-split model.
Section 5 studies the maximum zone of multi-point methods and Section 6 shows P2P simulations of de Bruijn DHTs.
Section 7 concludes the paper.
Generic load balancing is an relatively old and very wellresearched area [20], [30].
This problem typically assumes the existence of n fixed bins and m ≥ n objects, which are placed into the bins using uniform, or possibly nonuniform, random selection.
Assuming m = n, the largest bin contains Θ(log n) balls, which can be further reduced to Θ(log log n/ log d) by sampling d random bins before placing each object [4].
The main application of these results in P2P systems has been the equal distribution of object keys (which we simply call "objects") between the peers [6].
While balancing the number of keys per P2P node is an important objective, we are also concerned with the structure of the graph since failure of high-degree nodes (i.e., peers with large zones) compromises the strength of the underlying graph, congestion in large zones leads to increased response delay, and the presence of low-degree nodes (i.e., peers with small zones) increases the diameter of the system.
The first two problems are common to all graphs, while the third one is most noticeable in de Bruijn DHTs [15], [17], [25], [33].
For example, in a system with n = 10 6 peers, the maximum zone is between 12 and 28 times larger than average with probability 1 − 1/n (we show this result later in the paper).
Given a Chord-like system with the average node degree log 2 n = 20, the in-degree of the largest peer is between 240 and 560 with high probability.
Once this peer fails, over 200 links are broken simultaneously, leading to rather adverse effects on the graph.
It is also true that the largest peer receives routing traffic in proportion to its degree, which may increase the response delay of all queries passing through this node.
Finally, if the system utilizes a variation of de Bruijn graphs [15], [33], peers with the smallest zone will have their out-degree equal to 1, will be susceptible to disconnection from the graph, and will experience a larger routing diameter 1 .
As we show later in the paper, almost 6% of all nodes in de Bruijn graphs end up with degree 1 under random node join.We next briefly describe the model of the DHT space utilized in this paper and then proceed to zone-balancing analysis.
We use the unit-ring model 2 shown in Figure 1 as the DHT space and dissect how "well" the various distributed join algorithms partition its circumference between the nodes of a P2P system.
Figure 1 shows four peers holding non-overlapping parts of the ring and three edges originating from peer y.
While the linking rules vary between the different types of graphs, they all have the same characteristic -the location of each neighbor is computed based on the location of the zone being held by each peer.
Assuming a generic k-regular graph used in most DHTs, Chord links to k = log 2 n neighbors at exponentially increasing distances (the left side of the figure), while de Bruijn graphs link to k sequential nodes at a certain offset from the original node (the right side of the figure).
The construction of the ring is accomplished through a distributed join process.
A new node x selects a random location X in the DHT space based on some hashing function and then attempts to join the peer-to-peer system in or around that random location.
In the first approach (e.g., Chord), node x splits the existing peer at exactly X.
This is illustrated in Figure 2 (left) where node x splits peer y in the point of x's random hash index X. Notice that this construction leads to the possibility of having very small zones when X lands near one of the boundaries of an existing zone.
In the second approach (e.g., CAN), x splits the peer in half as demonstrated in Figure 2 (right).
To keep the notation consistent, we call the former method a "random split" and the latter method a "center split.
"To further improve fairness in zone sizes, several recent DHTs [15], [33] sample d random locations in the graph and then use a center split of the largest zone they find.
Even though these methods perform much better than any of the single-point approaches, sampling d random points in the graph may become costly, especially if d is on the order of 8 log n [33].
This generally leads to Θ(d log n) = Θ(log 2 n) messages per join, where the constants inside Θ(·) depend on the diameter of the graph.
In Chord with 1 million nodes (both the diameter and degree are 20), sampling 8 log n peers requires on average 1,105 messages and appears excessive.To decrease the message join overhead, an alternative approach [1], [25] is to deterministically sample the neighbors of the first peer and subsequently walk along the edges of the graph to discover more nodes.
This reduces the join overhead by a factor of:dD av Dav + d/k − 1 = Θ(kDav),(3)where k is the degree of the graph and Dav is the average distance between nodes.
For example, in Chord with 1 million nodes, the deterministic method can sample the same d = 8 log n peers using 76 times fewer messages than the previous approach (i.e., using only 14.5 messages per join).
Note, however, that the deterministic method generally must sample more than d points in the graph to provide the same bounds on f max as in the purely random approach.In the rest of the paper, we address such issues as whether 8 log n is the "correct" value of d for the graph to achieve a desired level of balancing and how many samples in the deterministic method make it equivalent to the purely random approach.
Our treatment of the DHT space assumes a toroidal unit circle, a purely random and perfectly uniform number generator, and infinite precision of each random hash index (i.e., the probability of collision is zero).
We use n to represent the number of peers in the system and focus on deriving the bounds on max/min zone sizes that hold with high probability.
Due to limited space, certain proofs have been omitted from this paper and can be found in [41].
Definition 1.
An event E n occurs with high probability (w.h.p.) with respect to n if there exists a fixed constant ε > 0 such that:P (E n ) ≥ 1 − n −ε , ∀n.(4)Typically, (4) ensures stronger bounds on the likelihood of event En compared to simply saying that En happens "almost surely," or with probability 1 − o(1).
Although it is customary [1], [18], [33] in this class of problems to derive bounds that hold w.h.p. and study only the asymptotic behavior of the system as n → ∞, we pay special attention to o(1) terms whenever possible and keep our results applicable even to graphs of small size n.
We next formally define the performance metrics mentioned in the introduction.Definition 2.
Random variable f max is the ratio of the maximum zone size to the average zone size after n points (peers) have joined a random instance of the system.
Definition 3.
Random variable fmin is the ratio of the average zone size to the minimum zone size after n points have joined the system.Both f max and f min are always no less than 1 and provide the main performance metric used throughout the paper.
Now suppose that n random points X 1 , X 2 , . . . , X n are independently and uniformly chosen on the unit circle.
Define Y i to be the i-th spacing between the points along the circle, M n to be the largest spacing: M n = max(Y 1 , . . . , Y n ), and S n to be the smallest spacing:S n = min(Y 1 , . . . , Y n ).
Theorem 1.
Under random splits, each of the following inequalities holds with probability 1 − n −ε : log n − log(ε log n) ≤ f max ≤ (1 + ε) log n.(5)Proof.
First, recall the following result due to Darling [10]:lim n→∞ P M n < log n + c n = e −e −c .
(6)Next, notice that there exists a critical point c at which (6) makes a sharp transition from "almost never" to "almost surely."
This percolation effect is common to our problem regardless of how the user joins the graph and is often found in other areas of networking [16].
Recalling that e x for small x is approximately 1 + x and substituting c = − log(ε log n) and c = ε log n into (6), we get both bounds in (5).
Hence, one can conclude that there almost always exists a zone larger than average by a factor of log n − log log n, but almost never larger by a factor of (1 + ε) log n. For example, in a graph with n = 10 6 peers, fmax is between 12 and 28 with probability 1 − 1/n.
To understand how well these bounds hold for small n ∞, we generated 1,000 random graphs of three different sizes -3,000, 30,000, and 300,000 nodes.
Table 1 shows in columns p l and p u the fraction of graphs in which the actual fmax complies with (respectively) the lower and upper bounds of (5) for ε = 0.3 (ideally, both p l and p u should equal 1 − n −ε ).
As the table shows, f max found in these graphs violates the bounds in (5) with probability very close to the predicted n −ε .
We next examine the behavior of f min in the following theorem and show that these bounds are exponentially worse than those in (5).
Theorem 2.
Under random splits, each of the following inequalities holds with probability 1 − n −ε : n ε log n ≤ f min ≤ n 1+ε .
Proof.
Recall that all Yi's are uniformly distributed on the simplex {(x1, . . . , xn): n i=1 xi = 1} and that [12], [13]:P n i=1 [Y i > a] = (1 − na) n−1 na < 1 0 na ≥ 1 .
(8)Note that the left side of (8) is the probability that the minimum zone size S n is at least a. Re-write (8) in terms of S n and assume sufficiently large n:P (Sn > n −δ ) = 1 − n 2−δ n n−1 ≈ e −n 2−δ .
(9)Substituting δ = 2 + ε into (9), we get the upper bound of (7).
Similarly, using δ = 2 − log(ε log n)/ log n, we get the lower bound of (7).
To illustrate the extent of fluctuation in f min , we again generated 1,000 random graphs and examined the number of graphs violating (7) for ε = 0.4.
Table 2 shows that (7) holds with high accuracy for a variety of graph sizes and that the range to which f min can be confined w.h.p. is substantially larger than traditionally expected [33].
Thus, a 10,000-node graph almost always has a peer whose zone size is smaller than average by a factor of 2,700.
Furthermore, unfairness by a factor of over 400,000 occurs in n −ε = 2.5% of all random graphs.
We next show how these bounds can be improved simply by using a different peer-splitting algorithm and derive more pleasant results for fmin.
Notice that when existing users are split in half by incoming nodes, the DHT space is organized into a dynamic binary trie.
The join process of each peer x can be modeled as a ball that drops into the root of the virtual trie and then descends down the tree randomly choosing whether it goes left or right.
The leaf at which the ball ends up is the node that x will split.
The movement of the ball represents the digits in the binary expansion of x's hash index X (recall that these digits are independent and uniform across all peers according to our assumptions).
This model is shown in Figure 3 where a new incoming node with X = 0101... splits node y, which is the leaf that shares the longest common prefix with x among the existing nodes.
Note that a similar tree-based model was independently proposed by Adler et al. in [2]; however, their analysis is completely different from ours.Further notice that the zone size of each peer x is a simple exponential function of its depth h x in the binary trie, i.e., 2 −h x .
Thus, the problem of finding fmax and fmin in "center-split" peer-to-peer DHTs boils down to estimating the probabilistic bounds on the smallest and largest depth of any leaf in the trie.
Let h i be the depth of peer i in a particular (random) instance of the graph, D = min n i=1 {h i } be the smallest depth, and H = max n i=1 {hi} be the largest depth of any leaf.
Assuming that we can bound both random variables D and H with high probability, what can be said about the resulting bounds on f max and f min ?
We state the obvious answer to this question in the following lemma without proof.
n2 −D u ≤ fmax ≤ n2 −D l (10) n −1 2 H l ≤ f min ≤ n −1 2 H u ,(11)In what follows, we examine the distribution of D and derive its probabilistic bounds D l and D u .
The discussion of H is given in the next section.To begin, we define a sequence of indicator random variables {A i }, i ≥ 0, where A i = 1 signifies that level i of the split-trie is full.
We say that a level is full if all nodes of that level are present and non-leaf.
Recall that level i can be full only if i < D and that Ai = 1 implies that A k = 1, ∀k < i.
It immediately follows that the smallest leaf depth D is at least k + 1 if and only if all levels from 0 to k are full:P (D ≥ k + 1) = P k i=0 [A i = 1] .
(12)Using this insight, our next result formulates the distribution of D as a simple recurrence equation.Lemma 2.
In a center-split trie with n leaves, the tail distribution of D for n ≥ 1 and k ≥ 0 is given by:P (D ≥ k + 1) = P (D ≥ k)P n (A k |A k−1 ),(13)where P (D ≥ 0) = 1 and Pn(A k |A k−1 ) is the conditional probability of level k being full given that all previous levels 0, . . . , k − 1 are full:Pn(A k |A k−1 ) = P (A k = 1|D ≥ k) .
(14)Notice that recurrence (13) does not limit the number of samples d used in the join process and applies to both single-point and multi-point methods.
The only difference between these two approaches is the shape of P n (A k |A k−1 ).
We show the analysis of single-point split in this section and leave the discussion of multi-point methods for Section 5.
Lemma 3.
For single-point center-split of the unit-ring, the probability that level k is full given that all previous levels are full is:Pn(A k |A k−1 ) ≈ exp −2 k e −n2 −k +1 .
(15)Proof.
First notice that any split-trie built using n peers contains n leaves and n − 1 non-leaf nodes.
Next, examine level k of the trie and observe that all 2 k possible nodes at this level must be non-leaf for level k to be fully split.
Assuming that all previous levels are full (i.e., A k−1 = 1), exactly 2 k − 1 non-leaf nodes contributed to filling up levels 0, . . . , k − 1 and the remaining n − 1 − (2 k − 1) = n − 2 k non-leaf nodes had a chance to split level k.
After the first k − 1 levels have filled up, each node at level k is "hit" by an incoming ball (which splits the node in half) with an equal probability 2 −k .
Thus, our problem reduces to finding the probability that u = n − 2 k uniformly and randomly placed balls into m = 2 k bins manage to occupy each and every bin with at least one ball.
There are many ways to solve this problem, one of which involves the application of wellknown results from the coupon collector's problem [11].
We use this approach below.Define Z(u) to be the random number of non-empty bins after u balls are thrown into m bins.
Thus, we can write Pn(A k |A k−1 ) = P (Z(u) = m).
Recall that in the coupon collector's problem, u coupons are drawn uniformly randomly (i.e., each with an equal probability 1/m) from a total of m different coupons.
Then, the probability Z(u) = m to obtain m distinct coupons at the end of the experiment is given by [11]:P (Z(u) = m) = m j=0 (−1) j m j 1 − j m u .
(16)For large u, the term (1 − j/m) u can be approximated by e −uj/m , yielding:P (Z(u) = m) ≈ m j=0 (−1) j m j e −uj/m = 1 − e −u/m m .
(17)Since we are only interested in asymptotically large m = Θ(log n), (17) allows a further approximation:P (Z(u) = m) ≈ e −me −u/m ,(18)which immediately leads to the result in (15).
The accuracy of (15) is demonstrated in Table 3, which shows the distribution of D in simulations for different n.
As the table shows, the combined result of (13)-(15) matches simulations very well, especially as n increases.With the result in Lemma 3, we are now in the place to derive the probabilistic bounds on D. D l = log 2 n − log 2 ((1 + ε) log n − ρ) + 1,(19)and ρ is Θ(log log n) as shown in the following proof.Proof.
First, we examine the lower bound on D and deriveD l such that P (D ≥ D l ) = 1 − n −ε .
We do not restrict the value of ε throughout the proof and later show that ε ≤ 1 conveniently allows D to be constrained to only two values D l and D l + 1.
For ease of presentation, set k = D l − 1.
Then, P (D ≥ D l ) = P (D ≥ k + 1) and from (13)-(15) we have:P (D ≥ k + 1) ≈ P (D ≥ k) exp −2 k e −n2 −k +1 ≈ exp −2 k e −n2 −k +1 ,(20)where the last approximation holds since P (D ≥ k) is no smaller than P (D ≥ k + 1) ≥ 1 − n −ε .
Now, select a particular value of k equal to: where ρ is some function of n that we will determine below.k = log 2 n − log 2 ((1 + ε) log n − ρ),(21)Expanding e z ≈ 1 − z for small z, we get from (20)- (21):P (D ≥ k + 1) ≈ 1 − n −ε e ρ (1 + ε) log n − ρ = 1 − n −ε ,(22)which holds as long as ρ satisfies e ρ = (1 + ε) log n − ρ.
Solving for ρ, we get:ρ = (1 + ε) log n − W en 1+ε = Θ(log log n),(23)where W (z) is Lambert's function (i.e., a multi-valued solution to e W (z) W (z) = z) [9].
Adding 1 to k in (21) and taking the floor function, we get D l in (19).
Next consider the upper bound on D, for which we must find Du such that P (D ≤ Du) = 1 − n −ε .
Notice that this is equivalent to finding Du such that P (D ≥ Du + 1) = n −ε .
To accomplish this task, set: k = log 2 n − log 2 (log n − ρ).
Then, we can write:P (D ≥ k + 1) ≈ exp −e ρ+1 log n − ρ = n −ε ,(25)which holds if ρ satisfies e ρ+1 /(log n − ρ) = ε log n. Solving this equation for ρ, we have ρ = log n − W en ε log n = Θ(log log n).
Since D u is an integer, we must apply the ceiling function to k in (24):Du = k = log 2 n − log 2 (log n − ρ),(27)which leads to P (D ≤ Du) = 1 − P (D ≥ Du + 1) ≥ 1 − P (D ≥ k + 1) = 1 − n −ε .
Our final step is to show that D u − D l = 1 for ε ≤ 1.
Neglecting asymptotically small ρ, we have:Du − D l = log 2 (1 + ε) log n log n = log 2 (1 + ε),(28)which equals 1 as long as 1 < 1 + ε ≤ 2.
To verify the correctness of the random-tree model, we generated 1,000 random graphs using center splits of the unit circle and examined the smallest depth D in each execution.
We used ε = log 1, 000/ log n in (19) to guarantee 99.9% confidence in the bounds (this also ensured that ε was no more than 1).
Figure 4 ( log n − Θ(log log n) and (1 + ε) log n − Θ(log log n).
Next, we focus on estimating the largest depth H (i.e., the height) of the tree in Figure 3 (left).
Even though this problem appears similar to the one just studied, the results are substantially different as can be seen in the next theorem.
H l = log 2 n + 2 log 2 n − 1.5 .
(29)Proof.
The random binary tree constructed by dropping balls and splitting leaves in Figure 3 (left) is a well-known structure called the Patricia trie [21], [39].
This tree is a collapsed version of the regular binary trie in which all intermediate nodes with a single child are removed.
Recalling that with probability 1 − o(1), the height of a random Patricia trie is concentrated on three values H l , H l + 1, and H l + 2 [21], where H l is given by (29), we immediately get the statement of the theorem.Note that the "o(1)" term in the statement of Theorem 4 depends on the decimal expansion of n and simply equals zero in many practical graphs of non-trivial size [21].
Figure 4 (right) shows simulation results (99.9% confidence) from the unit-ring topology for the largest leaf depth H and the corresponding bounds from (29).
Together with Table 4, these simulations demonstrate that the mass of H in fact centers on three values as n → ∞.
The result of Theorem 4 is quite interesting since it shows that by constructing a simple split-tree, the bound on fmin can be significantly improved from Θ(n 1+ε ) shown in the previous section to Θ(2 √ 2 log 2 n ) = Θ(3.246 √ log n ).
Nevertheless, this bound is still noticeably worse than fmax's Θ(log n).
Neglecting the ceiling and floor functions in (19) and (29), consider n = 10 6 and ε = 1.
In this case, f min is upper limited by 67, while f max is just below 28.
For n = 10 9 , f min is limited by 274 and f max by 41.
Another example of this difference can be observed from the simulations in Tables 3 and 4.
Using the last row in both tables (n = 300, 000), notice that f max ≤ 18.3, while f min ≤ 55.9 with probability 1 − o(1).
We conclude this section by observing that splitting an existing neighbor in half is in fact optimal among all methods that sample a single peer in the circle.Theorem 5.
The best (i.e., lowest) bounds on fmax and fmin are achieved by using a uniform hashing function and splitting existing neighbors in the center.The result of this theorem is illustrated in Figure 5, which shows the average values of fmax and fmin in 1,000 random graphs for n = 30, 000 and off-center splitting of existing peers (p and 1 − p are the two fractions into which each peer is split).
The same splitting method can be interpreted as peers applying a non-uniform hashing function in which 1's appear with probability p and 0's with probability 1 − p.
The figure clearly shows that p = 0.5 is optimal in both cases and confirms our prior observation that f min is substantially harder to bound than f max .
We start this section by examining the behavior of the maximum zone when each incoming peer is allowed to sample d random locations in the ring (as before, the implicit assumption here is that the peer will split the largest discovered node).
We again model this problem with split-trees, examine the evolution of the system as we add a new peer into the network during each time step, and derive the conditional probability Pn(A k |A k−1 ) that level k is fully split given that all previous levels are.We only model the center-split approach since all proposed multi-point methods split found nodes in the middle.
Further note that in multi-point sampling, Du is equal to either D l (large d) or D l + 1 (small d).
Therefore, in the rest of the paper, we limit our analysis to D l since its value can be trivially used to obtain D u and also deduce tight upper bounds on fmax.In what follows, we convert the non-linear stochastic process of the model into differential equations borrowing our inspiration from continuum (mean-field) theory [14] that is often used in modeling scale-free dynamics of the Internet topology (e.g., [5]).
Note that an identical approach has been independently proposed by Mitzenmacher in [28], [29] based on Kurtz's theorem and general theory of densitydependent Markov processes [22].
We use the same technique as in the previous section and model u = n − 2 k non-leaf nodes as balls dropping into m = 2 k bins.
Before each ball is placed into a bin, we sample d random bins and place the ball into the least occupied bin.
The goal of our analysis is to determine the probability that all m bins are occupied at the end of this process.
We assume that the system starts at time t 0 = 2 k , stops at time n, and adds one new node to the DHT at each integer time unit t ≥ t 0 .
Next, suppose that Z(t) is the number of non-empty bins at time t in a given (random) instance of the graph process and E[Z(t)] is the expectation of Z(t) over all random graphs.
Under this notation, P n (A k |A k−1 ) = P (Z(n) = m) and Z(t) = 0, ∀t ≤ t 0 .
The following lemma shows how E[Z(t)] can be approximated with a solution µ(t) to a simple differential equation and used to derive P n (A k |A k−1 ).
Lemma 4.
For d-point sampling and center-splits of the unit-ring, Pn(A k |A k−1 ) is given by:P n (A k |A k−1 ) = µ(n) 2 k 2 k ,(30)where µ(t) is the solution to the following differential equation:dµ(t) dt = 1 − µ(t) 2 k d , t ≥ 2 k (31)with initial condition µ(t) = 0, ∀t ≤ 2 k .
We conducted numerous balls-into-bins simulations to verify the accuracy of (31).
For all values of d and even values of n as small as 500, µ(n) matched E[Z(n)] remarkably well.
Figure 6 (left) shows the quality of the fit between µ(n) and E[Z(n)] for m = 10, 000 and two cases of n: 10,000 and 20,000 balls (simulation results are plotted as isolated points and the models are drawn as continuous lines).
that we call these bounds "continuous" since they generally produce non-integer k.
Since (30) does not generally allow a closed-form solution for large d, one must resort to binary search or similar methods to obtain the probability that f max exceeds a certain threshold.
This approach is time consuming and says nothing about how fmax behaves as a function of d. Thus, to overcome these limitations, we next derive an asymptotic expansion of (30) for arbitrary d and demonstrate its accuracy in simulations.
In this section, we study the behavior of the solution to (31) and obtain a closed-form expression for the bounds on D that are satisfied with high probability.Theorem 6.
Under d-point sampling and center-splits, the minimum tree depth D is bounded from below by D l with probability at least 1 − n −ε , where:D l = log 2 n + log 2 d − log 2 ((1 + ε) log n − β) + 1, (32) and β = log (2d + (1 + ε) log n) − log ξ − 2d,(33)for some small constant 0.2 < ξ < 0.5.
We verify the result of this theorem by again solving (30) for D l using a binary search to achieve 99.9% confidence.
We test these numerical bounds against the model (32) using two examples with 3,000 and 1 million nodes n.
In the former case, ε = 0.86 and in the latter case, ε = 0.5.
We use these values of ε in (32) and directly obtain D l , which leads to the corresponding upper bound on f max .
Figure 8 shows the result of this process and confirms that (32) is very accurate.
As both figures show, the value of fmax first drops almost linearly, but then the slope flattens out and f max converges to 2.1 and 2.2, respectively, at d = 100.
We can now re-write the main result (32) in terms of f max .
Corollary 1.
For all sufficiently large n, f max is bounded by the following with probability at least 1 − n −ε :f max ≤ 2 + (1 + ε) log n d − Θ(log(d + log n)) d .
(34)This bound cannot be lowered and is best possible.Note that (34) is an upper bound that holds for all large n.
It is possible to carefully select n such that the term inside the floor function in (32) is an integer, in which case f max can be bounded by half of what is shown in (34).
For other choices of n, fmax will fluctuate between 1 + (1+ε) log n 2d and 2 + (1+ε) log n d .
Since we are concerned with an upper bound that simultaneously holds for all n, (34) represents the best possible result on f max .
Further analysis of (34) for d = c log n yields:f max ≤ 2 + 1 + ε c − o(1).
(35)Assuming sufficiently large graphs and neglecting the o(1) term, d = log n samples can bound fmax by 3 + ε and d = 8 log n samples by 2.125 + ε/8 with high probability.
In this section, we use a different model of sampling d points along the circle, which relies on one random and d − 1 deterministic choices.
This method arises when the new node samples direct neighbors of a randomly chosen peer, where the neighbors are pre-determined by some fixed rules (a similar model is studied in [2] as discussed in the introduction).
To model this situation, we organize the nodes at level k of the split-trie into non-overlapping groups of size d.
If the first random point (ball) lands into group j, the peer is allowed to sample the remaining d − 1 points of the group.
Hence, grouping is symmetric and deterministically leads to the same result regardless of where within the group any given point (ball) lands.One example of this framework is shown in Figure 9 for d = 4.
In the figure, zone A always samples three other (known) locations of the circle.
This can be implemented by adding 1 /4, 1 /2, and 3 /4 of the circle's circumference to the location of the first point and then sampling the peers holding these additional points.
If these locations happen to be A's neighbors, then sampling comes with no additional message overhead.
This model is simple to generalize to any value of d as long as the individual zones do not overlap.Finally, note that this deterministic model does not directly correspond to the linking rules of any particular P2P network since it isolates the nodes in each group from the rest of the graph.
Nevertheless, the above model leads to very interesting results and provides a baseline comparison with the purely random approach.Lemma 5.
Assuming deterministic sampling of d bins in each group and non-overlapping groups, P n (A k |A k−1 ) is given by:Pn(A k |A k−1 ) = B d/2 k (d, n − 2 k − d + 1) B(d, n − 2 k − d + 1) 2 k /d ,(36)where B(a, b) is the beta function and B x (a, b) is the incomplete beta function [26]:Bx(a, b) = x 0 t a−1 (1 − t) b−1 dt.
(37)Proof.
We apply the same approach as in previous sections and study the probability that u = n − 2 k balls placed into m = 2 k bins are able to "split" each of the m bins.
First notice that every bin within a given group j is split as long as at least d balls land into group j. Therefore, we need to compute the probability that each group receives at least d random balls out of u.
The number of balls N j that are thrown into group j is given by a binomial distribution B(u, d/m), where u is the number of balls and d/m is the probability that a new ball is randomly placed into group j. Ignoring the mild dependency between {N j } (which asymptotically makes no difference), the probability that all groups receive at least d points is:P   m/d j=1 [N j ≥ d]   ≈ P (B(u, d/m) > d) m/d .
(38)Next, recall that the upper tails of a binomial random variable can be estimated using the regularized beta function [26]:P (B(u, d/m) ≥ d) = B d/m (d, u − d + 1) B(d, u − d + 1) ,(39)where B(a, b) is the beta function and B x (a, b) is the incomplete beta function in (37).
From (38) and (39), the result (36) follows immediately.As expected, for d = 1, (36) simplifies to become (15); however, for larger values of d, we need to use numerical methods to compute (36).
An alternative method is to derive an estimate for the upper tails of the binomial distribution and simplify (36) to a more workable form.
We carry In Figure 10 (right), we show the discrete version of f max (i.e., after applying the corresponding floor function) from model (36) and compare it to that obtained in simulations.
As the figure shows, the fixed-bin structure of the model is too "conservative" for the unit-ring (mostly in cases when the number of groups 2 k /d is not an integer) and overestimates the real f max in points when D makes a jump.
This issue notwithstanding, we find that (36) provides a good approximation to our class of deterministic sampling methods.
We next study how (36) behaves for different values of d.Lemma 6.
The result in (36) can be converted to a more "digestible" form as following:P n (A k |A k−1 ) ≈ exp − n −ε e −β+d (d − 1)!
(40) × ((1 + ε) log n + β − d) d ((1 + ε) log n + β − d) 2 − d 2 ,whereβ = dn2 −k − (1 + ε) log n. (41)The approximation in (40) was almost identical to the original beta function in (36) in all comparisons that we performed.
A typical fit between the two models for one case of n = 30, 000 is shown in Figure 11 (left).
However, since (40) by itself is not very useful and requires a binary search just like (36), our next step is to derive the exact bound on the smallest depth D that holds with probability 1 − n −ε .
Theorem 7.
In deterministic sampling, the minimum splittree depth D is bounded from below by D l = − log 2 M n + 1 with probability at least 1−n −ε , where M n is the largest zone size:M n =                W (en 1+ε ) n d = 1 (1 + ε) log n + 2 2n d = 2 (d − 2)Q(n, d, ε) + d dn d ≥ 3 ,(42)where W (z) is Lambert's function as before, Q(n, d, ε) is given by:Q(n, d, ε) = −W −1   − (d − 1)!
n −(1+ε) 1/(d−2) d − 2    ,(43)and W −1 (z), for negative z, is the secondary branch of multivalued Lambert's function W [9].
The result in (42) is a major improvement over (36) since it requires no binary search to compute the upper bounds on f max .
For any given d, n, and ε, (42) directly produces the result, where W−1(z) can be easily computed in many software packages (such as Matlab or Mathematica).
We verified the bounds on f max derived from (42) in numerous tests.
One example for 1 − n −ε = 0.999 and 1 million nodes is shown in Figure 11 (right).
While the result of the last theorem allows an easy computation of the bounds on f max , it is still not clear how this metric in the deterministic method compares to that in the random approach.
To address this question, we present a much simpler shape of (43) assuming d = c log n.Theorem 8.
For d = c log n, (42) − (43) simplify to the following:f max ≤ 2 + 1 + ε c + η − Θ(log log n) c log n ,(44)where η is:η = log 1 + 1+ε c + log 1 + 1+ε c + . . .(45)The result in (44) is very interesting as it shows that for example, for d = log n and ε = 0, η is log(2 + log(2 + . . . ≈ 1.1462 and f max converges to 3 + η ≈ 4.1462 for sufficiently large n.
This is in contrast to random sampling, n 10 6 10 9 10 12 10 32 10 72 10 305Model (36) where f max converges to 3.
We next verify the asymptotics of (44) for growing n and ε = 0.22.
For this ε, the value of η is 1.2418 and the asymptotic bound in (44) is 4.4618.
Table 5 shows the convergence process for f max computed using both the beta function in (36) and Lambert's function in (43).
Matlab's ability to compute the incomplete beta function (36) stops at approximately n = 10 12 , while (43) provides results up to n = 10 305 .
The table shows that the o(1) term in (44) slowly decays to zero and that f max converges to a value very close to the one predicted by the model.
The deterministic model clearly provides worse performance than the random model studied earlier; however, the difference in terms of fmax between the models is not as significant as one might have expected.
This is shown in Figure 12 for two values of n, where the deterministic model obtains f max larger than that in the random model by a small additive constant.Several remaining issues are whether random or deterministic sampling can achieve optimal (i.e., best possible) load balancing of P2P zones using logarithmic d and how many samples make the deterministic model equal to the random one.
We first define "optimality" and then discuss which models can actually achieve it.Theorem 9.
For any N > 0, there always exists such n > N that under arbitrary splitting mechanisms and for any number of samples d, the actual f max in every random graph of size n is at least 2.
With the aid of this theorem, it becomes apparent that the random sampling mechanism in (35) achieves optimal load balancing with d = c log n only when c → ∞.
All such functions (e.g., log log n · log n and log 2 n) are super-logarithmic and thus provide a negative answer to our question above.
In deterministic sampling, fmax in (44) also converges to 2 if and only if c → ∞ (which makes η → 0 as can be observed in (45)).
We next study the issue of making the random and deterministic models exhibit similar performance.Corollary 2.
Assuming that the random method samples c1 log n nodes and the deterministic method samples c 2 log n nodes, the corresponding upper bounds on f max are equal if:c2 = (1 + ε)c 1 1 + ε − c 1 log(1 + 1+ε c 1 ).
Assuming that d ≈ log n and ε ≈ 1, the two methods are equivalent in terms of fmax if the deterministic model uses approximately 2.2 times more samples than the random model.
Notice, however, that as c 1 → ∞, the deterministic factor c 2 in (46) asymptotically grows as 2c 2 1 /(1 + ε), which increases quite aggressively and quickly voids any benefits (such as the reduced message overhead) obtained by the deterministic method.
Therefore, one must conclude that if an application desires bounds on f max very close to 2 (i.e., large c1), it will typically find the random model more appealing.
In other cases when the application needs a "quick and dirty" bound (i.e., small c 1 ), the deterministic method provides a viable alternative.
For example, to achieve f max ≤ 3 with probability 1 − 1/n (i.e., c 1 = 2), the deterministic model requires only 3.3 times more samples than the random model.We apply this analysis in the next section to study the performance of multi-point methods in actual peer-to-peer systems.
In this section, we briefly analyze the performance of multipoint sampling methods in actual DHTs.
We selected de Bruijn graphs for the underlying model since multi-point methods have been proposed mostly in this context and also because the linking rules of this graph provide an interesting platform for observing the effect of large/small zone sizes on node degree.We implemented a variation of k-regular de Bruijn DHTs borrowing design ideas from [15], [17], [25], [33].
In all simulations, we use n = 30, 000, k = 8 (diameter of the graph is 5), and examine the following sampling methods: 1) random sampling of d = log n points in the DHT [15], [33]; 2) deterministic sampling of approximately 2.2d points using a random walk along the out-going edges of the graph [1];Model 1 ≤ k ≤ 3 k = 8 k > 16Random Sampling 0.23% 40.9% 0.13% Random Walk 0.41% 40.0% 0.22% Biased Walk 0.001% 40.6% 0.001% Table 6: Fraction of nodes in the final graph with a certain degree k. and 3) deterministic sampling of the same 2.2d points using a biased walk [25].
Our main performance metric is the degree distribution of the nodes in the graph after all peers have joined the system.
We average our results over 100 simulations and show the resulting distribution of degree below.
A baseline example is shown in Figure 13 (left) for the single-point, center-split method.
Although the maximum degree 81 is quite rare, there are 5.7% of nodes with degree 1, 13% with degree 2 or less, and 22% with degree 3 or less.In Figure 13 (right), we show the CDF of the degree distribution for the three multi-point methods.
We sample d = 11 random points in the first method and 24 points in the two deterministic methods.
The random walk method examines 8 neighbors of the original peer and then randomly walks for two hops recording zone sizes of the neighbors of each visited node (i.e., an extension of [2]).
The biased method does the same, except it always chooses the largest neighbor to walk towards to [25].
After the walk is finished, the largest discovered node is split by the joining peer.
As shown in Figure 13 (right), sampling 2.2d nodes in the deterministic method approximates the purely random model rather well.
Additional results in Table 6 confirm this observation and also show that the biased walk performs better than the other two methods at removing the extreme values (i.e., below 4 and above 16) of degree k from the graph.We finally analyze the message overhead involved in the three methods.
The diameter of the graph is 5 hops (which also happens to be the average distance in de Bruijn graphs [25]) and the join overhead of the purely random method is approximately 55 messages.
The same metric in the other two approaches is only 7 as long as each peer maintains a list of zone sizes held by its current neighbors.
We examined the distribution of the maximum and minimum zone sizes in peer-to-peer networks and derived tight bounds for these metrics.
We found that deterministic methods were in fact suboptimal, but could match purely random methods using a larger sampling size.
Future work involves analysis of the height of split-trees under multi-point sampling and design of greedy algorithms for the random walk that can improve the balancing performance of existing deterministic methods.
