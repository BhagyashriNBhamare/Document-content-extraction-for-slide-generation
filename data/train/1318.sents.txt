Data swapping, a term introduced in 1978 by Dalenius and Reiss for a new method of statistical disclosure protection in confidential data bases, has taken on new meanings and been linked to new statistical methodologies over the intervening twenty-five years.
This paper revis-its the original (1982) published version of the the Dalenius-Reiss data swapping paper and then traces the developments of statistical disclosure limitation methods that can be thought of as rooted in the original concept.
The emphasis here, as in the original contribution, is on both disclosure protection and the release of statistically usable data bases.
Data swapping was first proposed by Tore Dalenius and Steven Reiss (1978) as a method for preserving confidentiality in data sets that contain categorical variables.
The basic idea behind the method is to transform a database by exchanging values of sensitive variables among individual records.
Records are exchanged in such a way to maintain lower-order frequency counts or marginals.
Such a transformation both protects confidentiality by introducing uncertainty about sensitive data values and maintains statistical inferences by preserving certain summary statistics of the data.
In this paper, we examine the influence of data swapping on the growing field of statistical disclosure limitation.Concerns over maintaining confidentiality in public-use data sets have increased since the introduction of data swapping, as has access to large, computerized databases.
When Dalenius and Reiss first proposed data swapping, it was in many ways a unique approach the problem of providing quality data to users while protecting the identities of subjects.
At the time most of the approaches to disclosure protection had essentially no formal statistical content, e.g., see the 1978 report of the Federal Committee on Statistical Methodology, FCSM (1978), for which Dalenius served as as a consultant.Although the original procedure was little-used in practice, the basic idea and the formulation of the problem have had an undeniable influence on subsequent methods.
Dalenius and Reiss were the first to cast disclosure limitation firmly as a statistical problem.
Following Dalenius (1977), Dalenius and Reiss define disclosure limitation probabilistically.
They argue that the release of data is justified if one can show that the probability of any individual's data being compromised is appropriately small.
They also express a concern regarding the usefulness of data altered by disclosure limitation methods by focusing on the type and amount of distortion introduced in the data.
By construction, data swapping preserves lower order marginal totals and thus has no impact on inferences that derive from these statistics.The current literature on disclosure limitation is highly varied and combines the efforts of computer scientists, official statisticians, social scientists, and statisticians.
The methodologies employed in practice are often ad hoc, and there are only a limited number of efforts to develop systematic and defensible approaches for disclosure limitation (e.g., see FCSM, 1994;and Doyle et al., 2001).
Among our objectives here are the identification of connections and common elements among some of the prevailing methods and the provision of a critical discussion of their comparative effectiveness.
3 What we discovered in the process of preparing this review was that many of those who describe data swapping as a disclosure limitation method either misunderstood the Dalenius-Reiss arguments or attempt to generalize them in directions inconsistent with their original presentation.The paper is organized as follows.
First, we examine the original proposal by Dalenius and Reiss for data swapping as a method for disclosure limitation, focusing on the formulation of the problem as a statistical one.
Second, we examine the numerous variations and refinements of data swapping that have been suggested since its initial appearance.
Third, we discuss a variety of model-based methods for statistical disclosure limitation and illustrate that these have basic connections to data swapping.
Dalenius and Reiss originally presented data swapping as a method for disclosure limitation for databases containing categorical variables, i.e., for contingency tables.
The method calls for swapping the values of sensitive variables among records in such a way that the t-order frequency counts, i.e., entries in the the t-way marginal table, are preserved.
Such a transformed database is said to be t-order equivalent to the original database.The justification for data swapping rests on the existence of sufficient numbers of t-order equivalent databases to introduce uncertainty about the true values of sensitive variables.
Dalenius and Reiss assert that any value of a sensitive variable is protected from compromise if there is at least one other database or table, t-order equivalent to the original one, that assigns it a different value.
It follows that an entire database or contingency table is protected if the values of sensitive variables are protected for each individual.
The following simple example demonstrates how data swaps can preserve second-order frequency counts.Example: Table 1 contains data for three variables for seven individuals.
Suppose variable X is sensitive and we cannot release the original data.
In particular, notice that record number 5 is unique and is certainly at risk for disclosure from release of the three-way tabulated data.
However, is it safe to release the two-way marginal tables?
Table 1b shows the table after a data-swapping transformation.
Values of X were swapped between records 1 and 5 and between records 4 and 7.
When we display the data in tabular form as in Table 2, we see that the two-way marginal tables have not changed from the original data.
Summing over any dimension results in the same 2-way totals for the swapped data as for the original data.
Thus, there are at least two data bases that could have generated the same set of two-way tables.
The data for any single individual cannot be determined with certainty from the release of this information alone.
An important distinction arises concerning the form in which data are released.
Releasing the transformed data set as microdata clearly requires that enough data are swapped to introduce sufficient uncertainty about the true values of individuals' data.
In simple cases such as the example in Table 1 above, appropriate data swaps, if they exist, can be identified by trial and error.
However identifying such swaps in larger data sets is difficult.
An alternative is to release the data in tabulated form.
All marginal tables up to order t are unchanged by the transformation.
Thus, tabulated data can be released by showing the existence of appropriate swaps without actually identifying them.
Schlörer (1981) discusses some the trade-offs between the two approaches and we return to this issue later in the context of extensions to data swappping.
(a) Original Data Z 0 Y X 0 1 0 0 2 1 2 0 1 Y X 0 1 0 2 0 1 0 1 (a) Swapped Data Z 0 Y X 0 1 0 1 1 1 1 1 1 Y X 0 1 0 1 1 1 1 0Dalenius and Reiss developed a formal theoretical framework for data swapping upon which to evaluate its use as a method for protecting confidentiality.
They focus primarily on the release of data in the form of 2-way marginal totals.
They present theorems and proofs that seek to determine conditions on the number of individuals, variables, and the minimum cell counts under which data swapping can be used to justify the release of data in this form.
They argue that release is justified by the existence of enough 2-order equivalent databases or tables to ensure that every value of every sensitive variable is protected with high probability.In the next section we discuss some of the main theoretical results presented in the paper.
Many of the details and proofs in the original text are unclear, and we do not attempt to verify or replace them.
Most important for our discussion is the statistical formulation of the problem.
It is the probabilistic concept of disclosure and the maintenence of certain statistical summaries that has proved influential in the field.
Consider a database in the form of an N × V matrix, where N is the number of individuals and V is the number of variables.
Suppose that each of the V variables is categorical with r ≥ 2 categories.
Further define parameters a i , i ≥ 1, that describe lower bounds on the marginal counts.
Specifically, a i = N/m i where m i is the minimum count in the i-way marginal table.Dalenius and Reiss consider the release of tabulated data in the form of 2-way marginal tables.
In their first result, they consider swapping values of a single variable among a random selection of k individuals.
They then claim that the probability that the swap will result in a 2-equivalent database isp ≈ r (V −1)r (πk) (V −1)(r−1) .
Observations:1.
The proof of this result assumes that only 1 variable is sensitive.2.
The proof also assumes that variables are independent.
Their justification is: "each pair of categories will have a large overlap with respect to k." But the specific form of independence is left vague.
The 2-way margins for X are in fact the minimal sufficient statistics for the model of conditional independence of the other variables given X (for further details, see Bishop, Fienberg, and Holland, 1975).
Dalenius and Reiss go on to present results that quantify the number of potential swaps that involve k individuals.
Conditions on V , N , and a 2 follow that ensure the safety of data released as 2-order statistics.
However the role of k in the discussion of safety for tabulated data is unclear.
First they let k = V to get a bound on the expected number of data swaps.
The first main result is:Theorem 1.
If V < N/a 2 , V ≥ 4, and N ≥ 1 4 a 1 F 1/(V −1) V (V r−r+1)/(V −1)for some function F then the expected number of possible data-swaps of k = V individuals involving a fixed variable is ≥ F .
Unfortunately, no detail or explaination is given about the function F .
Conditions on V , N , and a 2 that ensure the safety of data in 2-way marginal tables are stated in the following theorem:Theorem 2.
If V < N/a 2 , andN {log(5N V p * )} 2/(V −1) ≥ a 1 V (V r−r+1)/(V −1)where p * = log(1−p)/ log(p), then, with probability p, every value in the database is 2-safe.
1.
The proof depends on the previous result that puts a lower bound on the expected number of data swaps involving k = V individuals.
Thus the result is not about releasing all 2-way marginal tables but only those involving a specic variable, e.g., X. 2.
The lower bound is a function F , but no discussion of F is provided.In reading this part of the paper and examining the key results, we noted that Dalenius and Reiss do not actually swap data.
They only ask about possible data swaps.
Their sole purpose appears to have been to provide a framework for evaluating the likelihood of disclosure.In part, the reason for focusing on the release of tabulated data is that identifying suitable data swaps in large databases is difficult.
Dalenius and Reiss do address the use of data swapping for release of microdata involving non-categorical data.
Here, it is clear that a database must be transformed by swapping before it can safely be released; however, the problem of identifying enough swaps to protect every value in the data base turns out to be computationally impractical.
A compromise, wherein data swapping is performed so that t-order frequency counts are approximately preserved, is suggested as a more feasible approach.
Reiss (1984) gives this problem extensive treatment and we discuss it in more detail in the next section.We need to emphasize that we have been unable to verify the theoretical results presented in the paper, although they appear to be more specialized that the exposition suggests, e.g., being based on a subset of 2-way marginals and not on all 2-way marginals.
This should not be surprising to those faminiliar with the theory of log-linear models for contingency tables, since the cell probabilities for the no 2nd-order interaction model involving the 2-way margins does not have an explicit functional representation (e.g., see Bishop, Fienberg, and Holland, 1975).
For similar reasons the extension of these results to orders greater than 2 is far from straightforward, and may involve only marginals that specify decomposable log-linear models (c.f., Dobra and Fienberg, 2000).
Nevertheless, we find much in the authors' formulation of the disclosure limitation problem that is important and interesting, and that has proved influential in later theoretical developments.
We summarize these below.1.
The concept of disclosure is probabilistic and not absolute:(a) Data release should be based on an assessment of the probability of the occurrence of a disclosure, c.f., Dalenius (1977).
(b) Implicit in this conception is the trade-off between protection and utility.
Dalenius also discusses this in his 1988 Statistics Sweden monograph.
He notes that essentially there can be no release of information without some possibility of disclosure.
It is in fact the responsibility of data managers to weigh the risks.
Subjects/respondents providing data must also understand this concept of confidentiality.
(c) Recent approaches rely on this trade-off notion, e.g., see Duncan, et al. (2001) and the Risk-Utility frontiers in NISS web-data-swapping work (Gomatam, Karr, and Sanil, 2004).
2.
Data utility is defined statistically:(a) The requirement to maintain a set of marginal totals places the emphasis on statistical utility by preserving certain types of inferences.
Although Dalenius and Reiss do not mention log-linear models, they are clearly focused on inferences that rely on t-way and lower order marginal totals.
They appear to have been the first to make this a clear priority.
(b) The preservation of certain summary statistics (at least approximately) is a common feature among disclosure limitation techniques, although until recently there was little reference to the role these statistics have for inferences with regard to classes of statistical models.We next discuss some of the immediate extensions by Delanius and Reiss to their original data swapping formulation and its principal initial application.
Then we turn to what others have done with their ideas.
Two papers followed the original data swapping proposal and extended those methods.
Reiss (1984) presented an approximate data swapping approach for the release of microdata from categorical databases that approximately preserves t-order marginal totals.
He computed relevant frequency tables from the original database, and then constructed a new database elementwise to be consistent with these tables.
To do this he randomly selected the value of each element according to probability distribution derived from the original frequency tables and and then updated the table each time he generated a new element.Reiss, Post, and extended the original data swapping idea to the release of microdata files containing continuous variables.
For continuous data, they chose data swaps to maintain generalized moments of the data, e.g., means, variances and covariances of the set of variables.
As in the case of categorical data, finding data swaps that provide adequate protection while preserving the exact statistics of the original database is impractical.
They present an algorithm for approximately preserving generalized kth order moments for the case of k = 2.
The U.S. Census Bureau began using a variant of data swapping for data releases from the 1990 decennial census.
Before implementation, the method was tested with extensive simulations, and the release of both tabulations and microdata was considered (for details, see Navarro, et al. (1988) and Griffin et al. (1989)).
The results were considered to be a success and essentially the same methodology was used for actual data releases.
Fienberg, et al. (1996) describe the specifics of this data swapping methodology and compare it against Dalenius and Reiss' proposal.
In the Census Bureau's version, records are swapped between census blocks for individuals or households that have been matched on a predetermined set of k variables.
The (k + 1)-way marginals involving the matching variables and census block totals are guaranteed to remain the same; however, marginals for tables involving other variables are subject to change at any level of tabulation.
But, as Willenborg and de Waal (2001) note, swapping affects the joint distribution of swapped variables, i.e, geography, and the variables not used for matching, possibly attenuating the association.
One might aim to choose the matching variables to approximate conditional independence between the swapping variables and the others.Because the swapping is done between blocks, this appears to be consistent with the goals of Dalenius and Reiss, at least as long as the released marginals are those tied to the swapping.
Further, the method actually swaps a specified (but unstated) number of records between census blocks, and this becomes a data base from which marginals are released.
However the release of margins that have been altered by swapping suggests that the approach goes beyond the justification in Dalenius and Reiss.Interestingly, the Census Bureau description of their data swapping methods makes little or no reference to Dalenius and Reiss's results, especially with regard to protection.
As for ultility, the Bureau focuses on achieving the calculation of summary statistics in released margins other than those left unchanged by swapping (e.g., correlation coefficients) rather than on inferences with regard to the full cross-classification.
Procedures for the U.S. 2000 decennial census were similar, although with modifications ( Zayatz 2002).
In particular, unique records that were at more risk of disclosure were targeted to be involved in swaps.
While the details of the approach remain unclear, the Office of National Statistics in the United Kingdom has also applied data swapping as part of its disclosure control procedures for the U.K. 2001 census releases (see ONS, 2001).
Moore (1996) described and extended the rank-based proximity swapping algorithm suggested for ordinal data by Brian Greenberg in an 1987 unpublished manuscript.
The algorithm finds swaps for a continuous variable in such a way that swapped records are guaranteed to be within a specified rank-distance of one another.
It is reasonable to expect that multivariate statistics computed from data swapped with this algorithm will be less distorted than those computed after an unconstrained swap.
Moore attempts to provide rigorous justification for this, as well as conditions on the rank-proximity between swapped records that will ensure that certain summary statistics are preserved within a specified interval.
The summary statistics considered are the means of subsets of a swapped variable and the correlation between two swapped variables.
Moore makes a crucial assumption that values of a swapped variable are uniformly distributed on the interval between its bottom-coded and top-coded values, although few of those who have explored rank swapping have done so on data satisfying such an assumption.
He also includes both simulations (e.g., for skewed variables) and some theoretical results on the bias introduced by two independent swaps on the correlation coefficient.
Torra (2001a, 2001b) use a simplified version of rank swapping and in a series of simulations of microdata releases and claim that it provides superior performance among methods for masking continuous data.
Trotinni (2003) critiques their performance measures and suggests great caution in interpreting their results.
Carlson and Salabasis (2002) also present a data-swapping technique based on ranks that is appropriate for continuous or ordinally scaled variables.
Let X be such a variable and consider two databases containing independent samples of X and a second variable, Y Suppose that these databases,S 1 = [X 1 , Y 1 ] and S 2 = [X 2 , Y 2 ]are ranked with respect to X.
Then for large sample sizes, the corresponding ordered values of X 1 and X 2 should be approximately equal.
The authors suggest swapping X 1 and X 2 to form the new databases,S * 1 = [X 1 , Y 2 ] and S * 2 = [X 2 Y 1 ].
The same method can be used given only a single sample by randomly dividing the database into two equal parts, ranking and performing the swap, and then recombining.Clearly this method, in either variation, maintains univariate moments of the data.
Carlson and Salabasis' primary concern, however, is the effect of the data swap on the correlation between X and Y .
They examine analytically the case where X and Y are bivariate normal with correlation coefficient ρ, using theory of order statistics and find bounds on ρ.
The expected deterioration in the association between the swapped variables increases with the absolute magnitude of ρ and decreases with sample size.
They support these conclusions by simulations.While this paper provides the first clear statistical description of data swapping in the general non-categorical situation, it has a number of shortcomings.
In particular, Fienberg (2002) notes that: (1) the method is extremely wasteful of the data, using 1/2 or 1/3 according to the variation chosen and thus is highly ineffecient.
Standard errors for swapped data are approximately 40% to 90% higher than for the original unswapped data; (2) the simulations and theory apply only to bivariate correlation coefficients and the impact of the swapping on regression coefficients or partial correlation coefficients is unclear.
Researchers at the National Institute of Statistical Science (NISS), working with a number of U.S. federal agencies, have developed a web-based tool to perform data swapping in databases of categorical variables.
Given user-specified parameters such as the swap variables and the swap rate, i.e., the proportion of records to be involved in swaps, this software produces a data set for release as microdata.
For each swapping variable, pairs of records are randomly selected and values for that variable exchanged if the records differ on at least one of the unswapped attributes.
This is performed iteratively until the designated number of records have been swapped.
The system is described in Gomatam, Karr, Chunhua, and Sanil (2003).
Documentation and free downloadable versions of the software are available from the NISS web-page, www.niss.org.Rather than aiming to preserve any specific set of statistics, the NISS procedure focuses on the trade-off between disclosure risk and data utility.
Both risk and utility diminish as the number of swap variables and the swap rate increase.
For example, a high swapping rate implies that data are well-protected from compromise, but also that their inferential properties are more likely to be distorted.
Gomatam, Karr and Sanil (2004) formulate the problem of choosing optimal values for these parameters as a decision problem that can be viewed in terms of a risk-utility frontier.
The risk-utility frontier identifies the greatest amount of protection achievable for any set of swap variables and swap rate.One can measure risk and utility in a variety of ways, e.g., the proportion of unswapped records that fall into small-count cells (e.g., with counts less than 3) in the tabulated, post-swapped data base.
Karr (2003, 2004) examine and compare several "distance measures" of the distortion in the joint distributions of categorical variables that occurs as a result of data swapping, including Hellinger distance, total variation distance, Cramer's V, the contingency coefficient C, and entropy.
Gomatam, Karr, and Sanil (2004) consider a less general measures of utility -the distortion in inferences from a specific statistical analysis, such as a log-linear model analysis.Given methods for measuring risk and utility, one can identify optimal releases are empirically by first generating a set of candidate releases by performing data swapping with a variety of swapping variables and rates and then measuring risk and utility on each of the candidate releases and provide a means of making comparisons.
Those pairs that dominate in terms of having low risk and high utility comprise a risk-utility frontier that leads optimal swaps for allowable levels of risk.
Sanil (2003, 2004) provide a detailed discussion of choosing swap variables and swap rates for microdata releases of categorical variables.
Takemura (2002) suggests a disclosure limitation procedure for microdata that combines data swapping and local recoding (similar to micro-aggregation).
First, he identifies groups of individuals in the database with similar records.
Next, he proposes "obscuring" the values of sensitive variables either by swapping records among individuals within groups, or recoding the sensitive variables for the entire group.
The method works for both continuous and categorical variables.Takemura suggests using matching algorithms to identify and pair similar individuals for swapping, although other methods (clustering) could be used.
The bulk of the paper discusses optimal methods for matching records, and in particular he focuses on the use of Edmond's algorithm which represents individuals as nodes in a graph, linking the nodes with edges to which we attach weights, and then matches individuals by a weighting maximization algorithm.
The swapping version of the method bears considerable resemblance to rank swapping, but the criterion for swapping varies across individuals.
Mulalidhar and Sarathy (2003a, 2003b) report on their variation of data swapping which they label as data shuffling, in which they propose to replace sensitive data by simulated data with similar distributional properties.
In particular, suppose that X represents sensitive variables and S non-sensitive variables.
Then they propose a two step approach:-Generate new data Y to replace X by using the conditional distribution of X given S, f (X|S), so that f (X|S, Y) = f (X|S).
Thus they claim that the released versions of the sensitive data, i.e., Y, provide an intruder with no additional information about f (X|S).
One of the problems is, of course, that f is unknown and thus there is information in Y .
-Replace the rank order values of Y with those of X, as in rank swapping.They provide some simulation results that they argue show the superiority of their method over rank swapping in terms of data protection with little or no loss in the ability to do proper inferences in some simple bivariate and trivariate settings.
We can define model-based methods in two ways: (a) methods that use a specific model to perturb or transform data to protect confidentiality; or (b) methods that involve some perturbation or transformation to protect confidentiality, but preserve minimal sufficient statistics for a specific model, thereby maintaining the data users' inferences under that model.
The former is exemplified by postrandomization methodologies and the latter by work on the release of margins from contingency tables or perturbed tables from conditional distributions.
We describe these briefly in turn.
The Post Randomization Method (PRAM) is a perturbation method for categorical databases (Gouweleeuw, et al., 1998).
Suppose that a sensitive variable has categories 1, . . . , m.
In PRAM, each value of the variable in the database is altered according to a predefined transition probability (Markov) matrix.
That is, conditional on its observed value, each value of the variable is assigned one of 1, . . . , m. Thus, observations either remain the same or are changed to another possible value, all with known probability.
This is essentially Warner's (1965) method of randomized response but applied after the data are collected rather than before.
Willenborg and de Waal (2001) note some earlier proposals of a similar nature and describe PRAM in a way that subsumes data swapping.The degree of protection provided by PRAM depends on the probabilities in the transition matrix, as well as the frequencies of observations in the original database.
PRAM has little effect on frequency tables.
Given the transition matrix, it is straightforward to estimate the univarite frequencies of the original data, as well as the additional variance introduced by the method.
The precise effect on more complicated analyses, such as regression models, can be difficult to assess.
See the related work in the computer science literature by Agrawal andSrikant (2000) andEvfimievski, Gehrke, andSrikant (2003).
Other statistics Makov (1996, 1998) suggest "bootstrap-like" sampling from the empirical distribution of the data, and then releasing the sampled data for analysis.
Multiple replicates are required to assess the the added variability of estimates when compared with the those that could be generated from the original data.
In the case of categorical data, this procedure is closely related to the problem of generating entries in a contingency table given a fixed set of marginals.
Preserving marginal totals is equivalent to preserving sufficient statistics of certain log-linear models.
Diaconis and Sturmfels (1978) developed an algorithm for generating such tables using Gröbner bases.
Dobra (2003) shows that such bases correspond to simple data swaps of the sort used by Delanius and Reiss when the corresponding log-linear model is decomposable, e.g., conditional independence In this paper we have revisited the original work of Dalenius and Reiss on data swapping and surveyed the some of the literature and applications it has spawned.
In particular, we have noted the importance of linking the idea of data swapping to the release of marginals in a contingency table that are useful for statistical analysis.
This leads rather naturally to a consideration of log-linear models for which marginal totals are minimal sufficient statistics.
Although Dalenius and Reiss made no references to log-linear models, they appear in retrospect to provide the justification for much of the original paper.
A key role in the relevant theory is played by the conditional distribution of a log-linear model given its marginal minimal sufficient statistics.
There is an intimate relationship between the calculation of bounds for cell entries in contingency tables given a set of released marginals (Dobra andFien- berg, 2000,2001) and the generation of tables from the exact distribution of a log-linear model given its minimal sufficient statistics marginals.
Work by Aoki and Takemura (2003) and unpublished results of de Loera and Ohn effectively demonstrate the possibility that the existence of non-simple basis elements can yield multi-modal exact distributions or bounds for cells where there are gaps in realizable values.
These results suggest that data swapping as originally proposed by Dalenius and Reiss does not generalize in ways that they thought.
But the new mathematical and statistical tools should allow us to reconsider their work and evolve a statistically-based methodology consistent with their goals.
The preparation of this paper was supported in part by National Science Foundation Grant No.
EIA-0131884 to the National Institute of Statistical Sciences and by the Centre de Recherche en Economie et Statistique of the Institut National de la Statistique et desÉtudesÉconomiquesdes´desÉtudesdesÉtudes´desÉtudesÉconomiques, Paris, France.
