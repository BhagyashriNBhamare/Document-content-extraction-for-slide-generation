Systems of autonomous and self-interested agents interacting to achieve individual and collective goals may exhibit undesirable or unexpected properties if left unconstrained.
Using deontic concepts of obligations, permissions and prohibitions to describe, what must, may and should not be done, norms have been widely proposed as a means of defining and enforcing societal constraints.
Recent efforts to provide norm-enabled agent architectures that limit plan choices suffer from interfering with an agent's reasoning process, and thus limit autonomy more than is required by the norms alone.
In response, in this paper we describe nu-BDI, an extension of the BDI architecture, which enables normative reasoning, providing agents with a means to choose and customise plans (and their constituent actions), so as to ensure compliance with norms.
We make three significant contributions, in providing: fine-grained tailoring of plan restrictions ; a plan annotation mechanism to identify violating plans, and limit possible plan instantiations; and a technique allowing the selective and incremental violation of norms in cases where goal achievement would not otherwise be possible.
Systems of autonomous and self-interested agents interacting to achieve individual and collective goals may exhibit undesirable or unexpected properties if left unconstrained.
One way to address this issue, in both human and artificial societies, has been through the use of norms, which have been proposed as a means of defining and enforcing constraints to ensure that such undesired behaviour is avoided if agents are norm-compliant (cf. [11] and [6]).
Norms are generally specified using deontic concepts of obligations, permissions and prohibitions to identify, respectively, what must, may and should not be done so as to ensure certain system properties.
Early work on normative systems focused on model-theoretic or philosophical aspects of deontic logics [18], but more recent work has addressed how norms may be more suitably represented in computational systems (e.g., [13] and [14]), their enforcement [8], and their impact on the society as a whole, abstracting away the details of mechanisms through which individual agents reason with and about norms and how individual behaviours are affected by norms.However, practical normative systems require analysis and specification of the processes through which norms are recognised, decisions about whether to comply with them are taken, and behaviour is adjusted appropriately.
Some recent efforts in this direction have sought to provide norm-enabled architectures (e.g., [12] and [15]) to specify how an agent's behaviour may be constrained to comply with norms in terms of permitted or forbidden mental states.
For example, compliance with an obligation to move to a certain location limits an agent's choice of plans containing moving actions to only those in which the target of the actions is the obliged location.
While such architectures capture this notion at a basic level, for example in preventing parts of a plan library from being adopted [15], or replacing the goals of an agent with the fulfilment of specific norms [12], they suffer from interfering with an agent's reasoning process, and thus limit autonomy more than is required by the norms alone.In response, in this paper we introduce ν-BDI, an extension of the BDI architecture [16] that enables normative reasoning, and provides a means for agents to choose and customise plans (and their constituent actions), so as to ensure compliance with norms.
The paper makes three significant contributions.
First, it avoids the rather coarse blanket retraction of specific plans (as adopted in previous work) by introducing constraints, enabling fine-grained tailoring of plan restrictions.
Second, it provides a plan annotation mechanism used as an efficient means of identifying (and potentially avoiding) plans that violate norms by examining norm scope (in relation to actions), and limiting possible plan instantiations.
This effectively transforms normative restrictions into extended context conditions that incur a similar computational overhead as selecting a plan.
Finally, it provides a technique for the selective and incremental violation of norms in cases where goal achievement would not be possible otherwise.
Importantly, unlike some earlier efforts, such agents are able to comply with specific normative stipulations with minimum disruption to traditional non-norm influenced reasoning.To illustrate our approach, we adopt a scenario in which software agents support humans responding to an emergency situation.
Humans communicate with each other and synchronise their activities through personal assistants responsible for intermediating communication among members of a team, prompting their human counterparts for actions to be carried out, as well as providing information to help humans decide which course of action to take.
We address the situation in which heavy and continuous rain in an area prone to flooding has led to emergency services being put on alert: a team of humans supported by personal assistants is to carry out alternative plans, depending on the current conditions (e.g., severity of the flooding, size of the affected area, which buildings are more at risk, the people affected, etc.).
The personal assistants monitor the latest information on weather and rising levels of water, and also have access to data on high-security installations (e.g., power plants, fuel and chemical depots, etc.), high-risk buildings (e.g., hospitals with intensive care patients, primary schools, prisons, etc.), routes for evacuation, and so on.
They have the following plan available, which we use to illustrate our approach:If one detects that the level of flooding in an area X is medium, and if the area is of high-risk (that is, it contains high-risk buildings), then the plan is to: i) isolate the area (to prevent people entering it); ii) evacuate everyone from the affected area to another area; and iii) reroute the traffic to another area.
In our scenario we also assume two norms:1) It is forbidden to evacuate an area X to an area Y , if area Y is unsafe.
This prohibition should be revoked if area Y becomes safe.
2) It is obligatory to reroute traffic through Z to avoid area X, if area X is deemed not safe.
This obligation is revoked when area X becomes safe again.
We start the paper by reviewing the BDI agent model and introducing a basic interpreter in Section II.
In order to define our proposed extension, we introduce in Section III a notation for precisely specifying normative restrictions, including restrictions over acceptable domains.
Using this notation, we develop in Section IV an agent architecture capable of reasoning with these norms, thus affecting specific plan instances that are adopted, deciding on norm compliance as plan instances are selected.
In doing so, we fulfil the need of pragmatic normative agent architectures capable of filtering norm compliant plans and decide upon them.
Finally, we draw conclusions and point to future work in Section V.
In this section we review the well-known BDI architecture, based on Bratman's philosophical model of reasoning centred around the three mental components of beliefs, desires and intentions (BDI) [2].
We use this as the foundation of our norm-aware architecture.
In order to explain the operation of ν-BDI, we need to introduce some notation and definitions.
We use first-order constructs for various elements of the agents and norms.Definition 1: A term, denoted generically as τ , is a variable w, x, y, z (with or without subscripts), a constant a, b, c (with or without subscripts) or f n (τ 0 , . . . , τ n ), that is, an n-ary function f n applied to (possibly nested) terms τ 0 , . . . , τ n .
Definition 2: A first-order atomic formula (or a predicate), denoted as ϕ, is any construct of the form p n (τ 0 , . . . , τ n ), where p n is an n-ary predicate symbol applied to terms τ 0 , . . . , τ n .
A first-order formula, denoted as Φ, is defined asΦ ::= Φ ∧ Φ|¬Φ|∀x.Φ|ϕ.We assume the usual abbreviations: Φ ∨ Φ stands for ¬(¬Φ ∧ ¬Φ ), ∃x.Φ stands for ¬∀x.¬Φ, Φ → Φ stands for¬Φ ∨ Φ and Φ ↔ Φ stands for (Φ → Φ ) ∧ (Φ → Φ).
Additionally, we also adopt the equivalence {Φ 1 , . . . , Φ n } ≡ (Φ 1 ∧ · · · ∧ Φ n ) and use these interchangeably.
In our mechanisms we use first-order unification [5] which is based on the concept of substitutions.Definition 3: A substitution σ is a finite and possibly empty set of pairs x/τ , where x is a variable and τ is a term.
We define the application of a substitution as follows:1) c · σ = c for a constant c.2) x · σ = τ · σ if x/τ ∈ σ; otherwise x · σ = x. 3) p n (τ 0 , . . . , τ n ) · σ = p n (τ 0 · σ, .
.
.
, τ n · σ).
Unifications can be composed; that is, for anyσ 1 = {x 1 /τ 1 , . . . , x n /τ n } and σ 2 = {y 1 /τ 1 , . . . , y k /τ k }, their composition, denoted as σ 1 σ 2 , is defined as {x 1 /(τ 1 · σ 2 ), . . . , x n /(τ n · σ 2 ), z 1 /(z 1 · σ 2 ), . . . , z m /(z m · σ 2 )}, where {z 1 , . . . , z m } are those variables in {y 1 , . . . , y k } that are not in {x 1 , . . . , x n }.
A substitution σ is a unifier of two terms τ 1 , τ 2 , if τ 1 · σ = τ 2 · σ.
Definition 4: unify(τ 1 , τ 2 , σ) holds iff τ 1 · σ = τ 2 · σ, for some σ.
unify(p n (τ 0 , . . . , τ n ), p n (τ 0 , . . . , τ n ), σ) holds iff unify(τ i , τ i , σ), 0 ≤ i ≤ n.Two terms τ 1 , τ 2 are related through the unify relation if there exists a substitution σ that makes the terms syntactically equivalent.
We assume a suitable implementation of a unification algorithm for unify to determine the existence of such a substitution.We denote as ¯ ϕ a first-order predicate whose terms are either constants or variables associated (via a substitution) with constants.
Here, we adopt Prolog's convention [1] and use strings starting with a capital letter to represent variables and strings starting with a small letter to represent constants.
We assume the availability of a sound and complete first-order inference mechanism 1 which decides if Φ can be inferred from Φ, denoted as Φ Φ .
In this paper we use a mechanism to determine if a formula Φ can be inferred from a set of ground predicates, and, if so, under which substitution; that is, { ¯ ϕ 0 , . . . , ¯ ϕ n } Φ · σ.
In this work we consider an abstract BDI interpreter, inspired by the dMARS architecture [4].
We define an agent in terms of its information model as follows.
2 Definition 5: An agent is a tuple Ag, Rl , Ev , Bel , Plib, Int, where Ag is the agent identifier, Rl is a set of roles, Ev is a queue of events, Bel is a belief base, Plib is a plan library, and Int is an intention structure.The Rl component is a finite and non-empty set of roles {r 1 , . . . , r n }, used to identify stereotypical agents classes to which one belongs (e.g. {fire marshall , evacuation team}).
Recently perceived events are stored in a queue and ordered by arrival time.
An event may be a belief addition or deletion, or a goal addition or deletion.
Belief additions are positive ground predicates perceived as true, and belief deletions are negative ground predicates perceived as false.
Goal additions indicate new goals posted, and goal deletions represent goals dropped for some reason.Definition 6: An event queue Ev is composed of ground first-order predicates representing events [e 1 , . . . , e n ] ordered by occurrence time.
Events e i can be one of four possible cases: i) a belief addition + ¯ ϕ; ii) a belief deletion − ¯ ϕ; iii) a goal addition +!
¯ ϕ; or iv) a goal deletion −!
¯ ϕ.
The belief base comprises a set of logic predicates, which can be queried through an entailment relation, as follows.Definition 7: A belief base Bel is a finite and possibly empty set of ground first-order logic predicates { ¯ ϕ 1 , . . . , ¯ ϕ n }, with an associated logical entailment relation for first-order formulae.The plan library, defined below, stores the plans of action available.
Each step in a plan body may be either an action (causing effects in the environment) or a subgoal (causing the addition of a new plan to the intention structure).
Definition 8: A plan library Plib is a finite and possibly empty set of uninstantiated plans {P 1 , . . . , P n }.
Each plan P i is a tuple t, c, bd where t is an invocation condition (cf. Definition 6), indicating the event that causes the plan is to be adopted, c is a context condition in the form of a firstorder formula over the agent's belief base, and bd is a body consisting of a finite and possibly empty sequence of steps[s 0 , . . . , s n ].
Actions are first-order atomic formulae; our focus is not on what an action entails, just that action execution might be the target of a normative stipulation as we will see later.
Finally, the intention structure comprises the agent's intentions, each of which contains partially instantiated plans to be executed by the agent.Definition 9: An intention structure Int is a finite and possibly empty set of intentions {int 1 , . . . , int n }.
Each int i is a tuple σ, ¯ st, where σ is a substitution and ¯ st is an intention stack (containing the steps remaining to be executed to achieve the intention).
EXAMPLE.
The plan of our scenario is represented as follows:+level(X, medium), (high risk (X )),   isolate(X), evacuate(X, Y ), reroute(X, Z)  This represents that if a belief level (X; medium) has been added to the belief base, stating that the level of emergency of area X is medium, and the context condition "X is a high risk area" holds, then the plan should be adopted.
The specification above provides a minimal information model required for BDI agent execution.
In this section we describe the mechanisms needed (using this information) for BDI-style computational behaviour.
Before considering norms, we specify a basic abstract BDI agent interpreter and subsequently extend it with mechanisms for normative reasoning and compliance.
A high-level description of the basic BDI interpreter is illustrated as the white boxes in Figure 1; grey boxes are our proposed extensions.below.
Initially, new events are perceived from the environment and added to an event queue Ev , then, the new events are used to update a belief base Bel and to select new plans from a plan library P lib to be adopted as intentions in the intention structure Int.
Finally, an intention is selected and one of its steps executed.
Updating events consists of gathering all new events and pushing them onto the event queue, while updating beliefs consists of querying new events in the event queue and adding beliefs when the events are positive predicates, and removing them when they are negative predicates.
We are not concerned here with more complex belief revision mechanisms, but such an interpreter could use them [7].
New events trigger the adoption plans from the plan library.
If the event is a belief update, a new intention may be created for it, otherwise the event is a subgoal for some existing intention and the plan is added to it.
The steps of a plan are adopted as intentions for execution, each of which can be either an action in the environment or the adoption of a subgoal that will trigger the adoption of further plans.These procedures have a very low computational cost, as demonstrated by various practical implementations such as dMARS [4], PRS [9], and others.
We now proceed to considering the normative aspects and how we incorporate them to the basic BDI interpreter.
Using deontic concepts of obligations, permissions and prohibitions to describe, what must, may and should not be done, norms have been widely proposed as a means of defining and enforcing societal constraints.
In this paper, since we are concerned with the impact of norms on reasoning and behaviour, we pay particular attention to the scope of influence of norms.
In this respect, we consider two distinct means of addressing this: first, we draw on aspects similar to those presented in [6] and [12] in that our norms are conditional, both for activation (when they come into force) and expiration (when they cease effect), limiting application to periods of time; and second, we add constraints [10], limiting application to particular plans and actions, and ensuring that norms are not over-restrictive.
In this section, we adapt and extend the notation for specifying norms of [17], beginning with constraints.Definition 10: Constraints, represented as γ, are any construct of the form τ τ , where τ, τ are first-order terms (that is, a variable, a constant or a function applied to terms) and is one of the infix binary operators =, =, >, ≥, <, or ≤.
A conjunction of constraints is denoted as Γ = (γ 1 ∧ · · · ∧ γ n ).
We use numbers and arithmetic functions to build terms τ ; arithmetic functions may appear infix, following their usual conventions.
For example, 10 > Temp and Price < (Cost + Z).
To improve readability, constraints of the form 3 ≤ X ∧ X ≤ 10 are written as 3 ≤ X ≤ 10.
Since constraints limit the acceptable range of parameters within instantiated plan steps (as we will see later), when determining if a plan complies with current norms, their satisfiability must be checked.
We use existing constraint satisfaction techniques [10] to implement a satisfy predicate that holds if a given conjunction of constraints admits a solution (if each variable of the constraints admits at least one value that simultaneously fulfils all constraints).
Definition 11:satisfy(γ 0 ∧ · · · ∧ γ n , σ) holds iff (γ 0 · σ ∧ · · · ∧ γ n · σ) is true for some σ.Constraints are associated with first-order predicates, imposing restrictions on their variables.
We represent this association as ϕ • Γ, as in, for instance, move(b 1 , X, Y ) • (100 ≤ X ≤ 500 ∧ 5 ≤ Y ≤ 45).
Now, to define for the core aspect of norms, we use constraint-annotated atomic deontic formulae.Definition 12: An annotated deontic formula ν is any construct of the form O α:ρ ϕ • Γ (an obligation) or F α:ρ ϕ • Γ (a prohibition), where α, ρ are terms, and ϕ is a first-order atomic formula with associated constraints Γ.
Term α identifies the agent(s) to which the norm is applicable and ρ is the role of such agent(s).
O α:ρ ϕ • (γ 1 ∧ . . . ∧ γ n ) thus represents an obligation on agent α taking up role ρ to bring about ϕ, subject to all constraints γ i , 0 ≤ i ≤ n.
The γ i terms express constraints on variables of ϕ.
The relation between constraints and the deontic formula is akin to the quantifier restrictions introduced in [3].
If we assume a universal quantification in our annotated deontic formulae, that is, ∀α.
∀ρ.
∀ x(X α:ρ ϕ • Γ) (where x are all variables occurring in ϕ and Γ, and X is either O or F) then our formula stands for ∀α.
∀ρ.
∀ x(Γ → X α:ρ ϕ).
Alternatively, if we assume an existential quantification, that is, ∃α.
∃ρ.
∃ x(X α:ρ ϕ • Γ), then the formula stands for ∃α.
∃ρ.
∃ x(Γ ∧ X α:ρ ϕ).
Our representation here is precise (as constraints provide a fine-grained way to specify values of variables) and compact (as constrained predicates amount to possibly infinite sets of ground formulae).
Let us assume the deontic formulae {Fp(X) • {X = a}, Oq(Y ) • {Y = b}} are currently in effect.
We also assume that at a particular point there is the following choice of plans to achieve a particular goal (for brevity, we assume the agent and role are known, dropping the subscripts from the formulae, and simplifying the formulae as {Fp(a), Oq(b)}, respectively):1) [s(a, b), p(a) , q(a), r(a)] 2) [q(a), p(b), s(a, b), r(a)] 3) [ q(b) , p(b), s(a, b), r(a)]A rational agent should give priority to Plan 3, which fulfils the obligation (shown boxed) and does not violate the prohibition.
Plan 2 neither fulfils the obligation nor violates the prohibition.
Plan 1 is the worst choice as it violates the prohibition (boxed).
More interesting situations arise when plans both fulfil obligations and violate prohibitions.
We develop in Section IV-C a means to manipulate plans, annotating them with constraints on the values of variables of its actions, thus ensuring that all norms in effect are factored in.
In Section IV-D we propose a means for agents to rank plans according to their normcompliance.
Thus, norms are defined as follows.Definition 13: An abstract norm ω A is a tuple ν, Act, Exp, id where:• ν is an annotated deontic formula (cf. Def.
12), • Act, the activation condition, is a conjunction of possibly negated first-order atomic formulae ϕ 1 ∧ · · · ∧ ϕ n specifying the condition that must hold in the agent's belief base for the norm to take effect; • Exp, the expiration condition, is a conjunction of possibly negated first-order atomic formulae ϕ 1 ∧ · · · ∧ ϕ n specifying the condition that must hold in the agent's belief base for the norm to stop being in effect; • id is a unique norm identifier We denote a set of abstract norms as Ω A .
If the activation condition of an abstract norm holds, then a specific norm is obtained, whereby variables may be instantiated to specific values.
Abstract norms generically define circumstances when norms should be adopted and dropped; when norms are adopted, the abstract formulation is instantiated to specific circumstances.Definition 14: A specific norm ω S is a tuple ν, Act, Exp, σ, id where ν, Act, Exp, id are as above and are bound by a substitution σ.
We denote a set of specific norms as Ω S .
As agents interact with their environment and with other agents, their perception of reality, as recorded in their sets of beliefs, change.
Agents use their beliefs to update their normative positions, adding norms whose activation conditions hold, and removing norms whose expiration conditions holds.
Given a set of beliefs Bel and a specific norm ω S of the form ν, Act, Exp, σ, id , then ω S holds (or is in effect) if, and only if, the following two conditions hold: 1) Bel Act · σ; that is, we can deduce Act · σ from the set of beliefs, and 2) Bel Exp · σ; that is, we cannot deduce Exp · σ from the set of beliefs.
Since beliefs change, norms also change as their activation and expiration conditions may no longer hold; this is how dynamic aspects are captured in our representation of norms.EXAMPLE.
The norms of our scenario are represented as the following abstract norms:1.
F A:R evacuate(X, Y ) • {Y = W }, ¬safe(W ), safe(W ), 1 2.
O A:R reroute(X, Z) • {X + 1 ≤ Z ≤ X + 3}, ¬safe(X), safe(X), 2The first norm states that all agents (in all roles) are forbidden to evacuate an area X to an area Y ; the prohibition becomes active if area Y (constrained to be W ) is unsafe and expires when area Y (constrained to be W ) becomes safe; unifications are dealt with like constraints, hence the need to use a third variable W .
The second norm states that all agents (in all roles) are obliged to reroute traffic through Z to avoid area X, but the rerouting must be within nearby zones.
The norm becomes active when area X is deemed not safe, and the norm is deactivated when area X becomes safe again.
Now, suppose these norms give rise to the following specific norms:3.
F A:R evacuate(X, Y ) • {Y = W }, ¬safe(W ), safe(W ), {W/3}, 1 4.
F A:R evacuate(X, Y ) • {Y = W }, ¬safe(W ), safe(W ), {W/6}, 1 5.
O A:R reroute(X, Z) • Γ, ¬safe(X), safe(X), {X/2}, 2That is, abstract Norm 1 gives rise to two specific norms, one instantiating W to 3 and another W to 6.
Abstract Norm 2 (shown with constraints abbreviated as Γ to save space) gives rise to one specific norm, instantiating X to 2.
For simplicity, in our discussion we assume an implicit universal quantification over variables in ν, Act and Exp.
However, our approach can naturally be extended to cope with any quantification.
Given the representation of norms as detailed above, we can now address the issues surrounding their integration into an effective BDI architecture.
First, we describe the key processes required in the agent interpreter to manage the activation and expiration of norms.
Although beliefs are generally [4] assumed to contain exclusively ground first-order predicates, in this paper we store both abstract and specific norms in the belief base.
In doing so we avoid adding extra components to the architecture.
We extend and adapt the mechanisms to update beliefs and to reason with beliefs, enabling them to deal with norms.The process of updating norms consists of going through each abstract norm ω A ∈ Ω A , of the form ω A = ν, Act, Exp, id , checking if their activation condition is supported by the agent's belief base, that is, Bel Act · σ.
Then, for each norm and each possible substitution σ in which the activation condition holds in Bel , a new specific norm ω S = ω A · σ is created and added to the set of specific norms.
Afterwards, for each specific norm ω S ∈ Ω S , if the expiration condition is supported by the agent's belief base, that is, Bel Exp · σσ , the specific norm is removed from Ω S .
EXAMPLE.
Let us suppose we have an abstract norm ω A :O A:R use(hlc, X) • Γ, high risk (X), weather (X, poor ), 3This represents an obligation on all agents/roles to fly a helicopter (represented as hlc) over X; the norm becomes active if X is a high-risk area, and the norm expires if the weather conditions in X are poor.
The Γ stipulates which areas can be flown over, and its details are not relevant to our example.
If we have a belief base Bel = {high risk (10), ω A }, where ω A is the abstract norm above, then we would add to Bel the specific normO A:R use(hlc, X) • Γ, high risk (X), weather (X, poor ), {X/10}, 3If, however, the belief base also had a predicate weather (10, poor ), then no specific norms would be added, as the expiration condition of the newly added norm would holdthe mechanism would add and subsequently remove a specific norm, leaving the set of specific norms unchanged.
As indicated previously, our key concern in this paper is with the impact of norms on plans.
Critical to this is determining when an action (represented as an atomic formula ϕ) is within the scope of influence of a specific norm ω S .
Definition 15 introduces predicate inScope which, given an agent specified by its unique identifier Ag and one of the roles R ∈ Rl of the agent, holds if a first-order predicate ϕ is within the influence of a specific norm ω S (in the format of Definition 14).
Definition 15: An action literal ϕ of an agent Ag with role R is in the scope of a specific norm ω S = X α ϕ • Γ, Act, Exp, σ, id , represented as inScope(Ag, R, ϕ, ω S ), if, and only if, ω S ∈ Ω S , unif y(Ag, R, ϕ, α, ρ, ϕ · σ, σ ), and satisf y(Γ · σ, σ ) As indicated in Section III, one of our primary concerns is with the impact of norms on agent plans in terms of constraints on the values of variables of an action.
Since actions and achievable world-states are components of plans, instances of restricted actions and world states must be found and marked with these constraints.
To achieve this, we propose a mechanism that scans a plan, annotating each step within the scope of a norm with constraints stemming from that norm.Each plan step is checked against the predicates specified in the specific norms (Ω S ), taking into account the role the agent adopts.
If a step is within the scope of a norm, then the mechanism gradually assembles the constraints of the norms Γ i , and annotates the plan step with them.
If the norm is an obligation, the constraints are added as they appear in the norm, instantiated (or customised) to the substitutions σ, σ .
If the norm is a prohibition, the constraints are then negated; formally, neg((γ 1 , . . . , γ n )) = (neg(γ 1 ), . . . , neg(γ n )), and each constraint can be negated as neg(τ > τ ) = (τ ≤ τ ), neg(τ < τ ) = (τ ≥ τ ), neg(τ ≥ τ ) = (τ < τ ), and so on.
If the step is not in the scope of any norm, no constraints are added.Once plan steps have been annotated, it is possible for an agent to check before executing each step if its execution violates a norm.
However, it is inefficient to adopt a plan and execute it partially before discovering that the plan was not, in fact, desirable from the perspective of norm compliance.
Fortunately, since the specific values of the variables within a plan are bound when a plan is instantiated, it is possible to determine at plan instantiation if any normative restriction applied to individual plan steps would be violated if the plan is adopted.
In order to do this, we must make all annotations available for checking when the plan is instantiated so, at the end of each iteration over the steps of a plan, we collect the annotations into a global plan annotation Γ , which is later used when selecting norm compliant plans.EXAMPLE.
The plan annotation mechanism, when applied to the plan introduced above, and using the specific norms shown previously, yields the following annotated plan:+level(X, medium), (high risk (X )),   isolate(X) • , evacuate(X, Y ) • {Y = 3, Y = 6}, reroute(X, Z) • {3 ≤ Z ≤ 5}   , {Y = 3, Y = 6, 3 ≤ Z ≤ 5}We notice on the evacuate step of the plan, the negated constraints of the specific norms arising from norm 1, shown with the substitutions applied.
We also notice the reroute step annotated with the constraints of the specific version of the obligation (also with the substitutions applied, and the mathematical expressions of the constraints simplified to improve visualisation).
The annotated plan factors in the constraints of the active norms, making ν-BDI agents norm-aware.
We now describe the process of selecting plans that comply with the constraints imposed by currently active norms.
As we have seen, plans are annotated with constraints on the values that action variables may have when a plan is instantiated; in order to filter out non-compliant plan instances we simply verify their satisfiability with variable bindings for candidate plan instances.
For example, if there is a plan containing an action move(X, Y ), and norm O A:R move(X, Y )•{X ≤ 10∧ Y ≤ 5} is active, then instances of the plan with X bound to values greater than 10 should not be adopted.In practice, violating situations are identified if the plan constraints become unsatisfiable after substitutions stemming from the plan instantiation are applied to the plan's annotations.
In our example, if X is bound to 11, the annotation becomes {11 ≤ 10 ∧ Y ≤ 5}, which is not satisfiable.
Now, in order to select compliant plans, the plan annotations created earlier, customised (via substitution σ) to the instantiation of the plan caused by event e, need to be satisfiable.
In this paper we have described a new norm representation formalism, using constraints as means to precisely specify the target of normative stipulations.
These constraints are used to determine specific plan instantiations that comply with active norms, thus narrowing the acceptable domains for operation.
Based on this, we have described mechanisms that enable these plan instantiations to restrict behaviour in support of compliance, avoiding violating plans.
Importantly, our work enables selective and incremental norm violation in a controlled manner in cases where goal achievement would not otherwise be possible, or where norms are deliberately ignored.We have implemented these mechanisms within ν-BDI, extending a traditional BDI interpreter, such as dMARS [4].
However, our mechanisms are sufficiently generic to enable inclusion in any BDI interpreter and sufficiently detailed that implementation is straightforward.
In addressing normative reasoning to this level of analysis, we have tackled various technical challenges posed by norm processing, such as the detection of activation and expiration conditions and the management of the norm life cycle between these two conditions, through the management of abstract and specific norms.
Finally, we have shown the applicability of the mechanisms developed in an emergency evacuation scenario.
In future work, we intend to refine the evacuation scenario as a testbed for our interpreter, and handle norm deadlines as well as normative conflicts.
