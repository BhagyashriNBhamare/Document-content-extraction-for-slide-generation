With rising network rates, cloud vendors increasingly deploy FPGA-based SmartNICs (F-NICs), leveraging their inline processing capabilities to offload hypervisor networking infrastructure.
However, the use of F-NICs for accelerating general-purpose server applications in clouds has been limited.
NICA is a hardware-software co-designed framework for inline acceleration of the application data plane on F-NICs in multi-tenant systems.
A new ikernel programming abstraction, tightly integrated with the network stack, enables application control of F-NIC computations that process application network traffic, with minimal code changes.
In addition, NICA's virtualization architecture supports fine-grain time-sharing of F-NIC logic and provides I/O path virtualization.
Together these features enable cost-effective sharing of F-NICs across virtual machines with strict performance guarantees.
We prototype NICA on Mellanox F-NICs and integrate ikernels with the high-performance VMA network stack and the KVM hypervisor.
We demonstrate significant acceleration of real-world applications in both bare-metal and virtualized environments, while requiring only minor code modifications to accelerate them on F-NICs.
For example, a transparent key-value store cache ikernel added to the stock memcached server reaches 40 Gbps server throughput (99% line-rate) at 6 µs 99 th-percentile latency for 16-byte key-value pairs, which is 21× the throughput of a 6-core CPU with a kernel-bypass network stack.
The throughput scales linearly for up to 6 VMs running independent instances of memcached.
SmartNICs with integrated FPGAs (F-NICs) [19,68,89,119] are an appealing platform for accelerating I/O intensive network applications.
They have been increasingly deployed in data centers and public clouds [33,66], e.g., in each MS Azure server, enabling line-rate throughput and low, predictable latency at high power efficiency [33].
Many hardware vendors, including Intel, have already announced F-NICs in their future offerings [98].
Data-center F-NICs are used mainly to accelerate infrastructure tasks, such as network functions [61,72,80,119] and software-defined networking [33,66].
These tasks leverage the F-NIC's inline processing capabilities, where data is processed while being transferred between the host and the network, without CPU involvement.
In addition, F-NICs are often repurposed as standalone FPGAs running complete applications, e.g., search or deep learning [20,24,60,86].
This paper explores new acceleration opportunities emerging from the growing deployment of F-NICs in clouds, beyond infrastructure tasks and monolithic applications.
We seek to leverage F-NICs for inline acceleration of data plane processing in network-intensive applications.
For example, F-NICs may run datacenter tax tasks, such as deserialization, hashing, and authentication, which reportedly consume over a quarter of the CPU cycles in data centers [48].
An F-NIC may serve as an extra caching layer for key-value stores, responding directly in case of a hit and eliminating the CPU involvement.
We show, for example, that this architecture achieves near line-rate throughput (40 Gbps) for stock memcached ( §6.2.1).
Promising results for application-specific traffic steering, packet transformation, and network stack offloading have been reported in prior work [50,83].
We discuss these and other applications in §3.
Unfortunately, building such F-NIC-accelerated applications today is hard.
First, there are no adequate operating system abstractions for inline acceleration of generalpurpose applications on F-NICs.
Such abstractions should associate F-NIC tasks with the application process, and they should provide well-defined execution boundaries and isolated per-task state while supporting easy integration of F-NIC functionality with the application logic.
OpenCL and CUDA provide general lookaside acceleration support, but they are a poor match for F-NICs because they require explicit kernel invocation and data transfers that are irrelevant for the inline processing scenario.
Floem [83] provides language-level constructs to accelerate applications on SmartNICs, but it targets CPU-based rather than FPGA-based SmartNIC architectures, and requires application refactoring to use its data hardware task scheduler that switches contexts at a fine granularity, thus allowing better hardware utilization for latencysensitive applications.
We describe AFU virtualization in §4.2 and show how it enables performance isolation in §6.
NICA provides necessary on-FPGA services for accelerating applications on F-NICs in a multi-tenant setting, including an FPGA-resident network transport layer, compute and I/O scheduling blocks, and AFU state isolation.
However, the development of high-throughput network-focused AFUs on FPGAs is beyond the scope of this paper.
Fortunately, some promising solutions are emerging, such as template libraries with optimized building blocks for network processing [32].
In addition, we believe that cloud providers will increasingly offer AFUs using an "app marketplace" deployment model [4,17,40], with a variety of AFUs ready to be used on their infrastructure (see §3).
We prototype NICA 1 on Mellanox Innova F-NICs [68] with a Xilinx FPGA and 2GB of onboard memory.
We implement the ikernel API, integrate it with the VMA kernel-bypass network stack [69], and implement the AFU virtualization support in the KVM hypervisor.
We also co-design the FPGA hardware support for the software abstractions and AFU virtualization, and we integrate full UDP and partial TCP layer implementation in FPGA.We evaluate the system with microbenchmarks and accelerate two real-world applications: a memcached server and a Node.js-based IoT monitoring server, by implementing the respective AFUs on the F-NIC.
Enabling F-NIC acceleration required minimal software changes: 107 additional lines of C and 20 additional lines of JavaScript respectively.A transparent hot-item cache AFU integrated with memcached serves GET hits at 6 µs 99 th -percentile latency and 40.3 Mtps throughput for 16B keys/values, 99% of the 40 Gbps line rate and 21.6× faster than the 6-core CPU baseline.
For a Zipf(0.99)-distributed workload with 0.2% SETs, NICA acceleration results in a 4.6× speedup.NICA allows sharing of an F-NIC among multiple VMs while providing significant performance gains.
It introduces negligible throughput and latency overheads while maintaining a fair bandwidth allocation, controllable by the hypervisor.In summary, we make the following contributions: • We introduce an ikernel OS abstraction for inline acceleration of applications on F-NICs.
• We design an F-NIC virtualization framework that supports I/O QoS and low-latency time sharing of compute resources.
• We implement NICA for Mellanox F-NICs, analyze its performance, and demonstrate the development simplicity and performance benefits for accelerating memcached and a Node.js-based IoT server.
We describe the F-NIC architecture and survey FPGA programming principles and sharing mechanisms.
We describe bump-in-the-wire F-NICs, focusing on Mellanox Innova, but others [19,80,89] are similar.
Bump-in-the-wire.
A typical F-NIC ( Figure 1) combines a commodity network ASIC (e.g., ConnectX-4 Lx NIC) with an FPGA and local DRAM.
The FPGA is located between the ASIC and the network port, interposing on all Ethernet traffic in and out of the NIC.
The FPGA and the ASIC communicate directly via an internal bus (e.g., 40 Gbps Ethernet), and a PCIe bus connects the ASIC to the host.
The bump-in-the-wire design reuses the existing data and control planes between the CPU and the NIC ASIC, with its QoS management, and virtualization support (SR-IOV), mature DMA engines, and software stack.
F-NIC programming.
The development of an F-NICaccelerated application involves both hardware logic on FPGA and associated software on the CPU.
F-NIC vendors provide a lightweight shell IP: a set of low-level hardware interfaces for basic operations, including link-layer packet exchange with the network and the host, onboard DRAM access, and control register access.
However, the vendor SDK leaves it to customers to implement higher level features such as FPGA network stack processing or virtualization support.
Field Programmable Gate Arrays (FPGAs) are "a sea" of logic, arithmetic, and memory elements, which users can configure to implement custom compute circuits.
FPGA compute capacity is determined by the area available for the circuits.
FPGA development.
FPGAs can be seen as "softwaredefined" hardware.
The software definition, a design, is implemented using register transfer languages (RTL) such as Verilog.
Additionally, designers can use high-level synthesis (HLS) tools to generate RTL, e.g., from a restricted version of C++ [67].
However, HLS C++ programs are different from CPU programs, and must follow certain rules, including explicit exposure of fine-grain pipeline-and task-parallelism to achieve high performance.
Implementation tools then compile the design into an FPGA image targeting specific hardware.Finally, users can load the image onto an FPGA (slow, up to a few seconds), entirely replacing the previous design.
Some FPGAs support partial reconfiguration to replace only a subset of the entire FPGA, a much faster process (milliseconds), which unfortunately incurs significant area overheads [51].
FPGA sharing.
There are three ways to share an FPGA: space partitioning, coarse-grain, and fine-grain time sharing.Space partitioning divides FPGA resources into disjoint sets used by different AFUs [18,20,51].
If shared I/O interfaces (memory, PCIe bus) are securely isolated and multiplexed, this method enables low-overhead FPGA sharing among mutually distrustful AFUs but requires larger FPGAs to fit them all.
Coarse-grain time sharing dynamically switches AFUs via full or partial reconfiguration [20,51].
It incurs high switching latency and thus is not suitable for F-NICs' latency-sensitive applications.
Fine-grain time sharing allows multiple CPU applications to use the same AFU [44].
The AFU implements the context switch internally, in hardware.
Packet processing applications such as AccelNet [33] use this approach to process each packet in the context of its associated flow.
Such AFUs oversee switching between the contexts; therefore this type of sharing requires AFUs to be trusted to ensure fair use and state isolation between their users.NICA combines both space sharing for untrusted AFUs, and fine-grain time sharing for trusted AFUs, to achieve maximum utilization under area constraints of F-NICs.
We consider emerging opportunities for application acceleration by using F-NICs in clouds.
Microsoft has been among the first to deploy F-NICs at large scale, having installed the Catapult F-NICs in over a million Azure servers.
Their recent work [33] analyzes the cost, power, and performance trade-offs of F-NICs in data centers and decisively shows their benefits.
Following Azure, other data centers, such as China Mobile [115], Tencent [66], Huawei [88], and Selectel [94], are deploying F-NICs, and leading hardware vendors are adding F-NICs to their offerings [98].
These technology trends suggest that F-NICs will become a commodity, motivating our goal to broaden the scope of their applications.
What sets F-NICs apart from stand-alone FPGAs is their ability to interpose and process the network traffic to and from the host with low overhead.
For application acceleration, the application data plane can be partitioned between the F-NIC and the CPU, even for latency-sensitive fine-grain tasks.We identify several common task categories in the server data plane that benefit from F-NIC acceleration.Filtering.
F-NICs may execute compute-intensive processing, such as per-message stateless authentication (e.g., JSON web token validation [47]), and filter invalid requests before reaching the CPU.
We evaluate this example in §6.2.2.
Such filtering patterns arise in many server applications.
For example, F-NICs may implement high-performance, simplified versions of popular services to accelerate common behavior (fast path), falling back to the CPU for corner cases (slow path).
We show in §6.2.1 how an F-NIC-hosted keyvalue store cache reduces server load.Transformation.
F-NICs may convert data formats, perform (de)serialization, compression, encryption, or similar datacenter tax tasks [48].
They can change data layout, e.g., transpose matrices [35], sample, or realign data [2,38] for efficient CPU/GPU processing or storage.
As F-NICs may run a (potentially limited) network transport layer, they may speed up CPU transport layer processing [50], as we show in §6.
Transformation is often combined with filtering.
For example, to accelerate the log-structured merge (LSM)-trees [22,79], the F-NIC may store the tree's first level in its local memory, executing updates without interrupting the CPU, batching and sorting them before sending them to the host.
Steering.
F-NICs may improve server performance using application-specific packet steering and inter-core load balancing [50,89], processing complex steering policies at line-rate, e.g., using heavy-hitter approximation sketches [62].
Generation.
Applications may offload the transmission of outgoing messages to multiple destinations.
Examples include data replication and erasure coding in storage systems [38,53,77], and the shuffle stage in distributed analytics engines.
AFUs are custom accelerators that can be instantiated on any compatible FPGA and used via a companion software library.
There are two deployment models for cloud AFUs: in the FPGA-as-a-Service (FaaS) model, tenants use their own AFUs on cloud infrastructure [3,5,41], whereas with the app marketplace model, cloud providers offer common AFUs for on-demand deployment [40].
For example, while Amazon provides FaaS, its Marketplace offers third-party AFUs [4].
Similarly, Microsoft deploys its own cloud hardware microservices [17,24,33].
The marketplace model opens more opportunities for better F-NIC utilization.
As cloud providers develop or audit these AFUs, they can trust them to allow fine-grain sharing.
By co-locating tenants that request the same AFU, cloud providers may increase their infrastructure utilization, thereby increasing power efficiency [108] and reducing costs.
Pre-designed AFUs are less flexible than customer-provided AFUs, but vendors can offer them at a lower cost due to the more aggressive sharing.NICA's design supports both deployment models.
NICA overview.
Figure 2 shows the main NICA components with a single physical AFU.
NICA comprises three layers: application-visible OS abstractions and services inside a VM integrated with the network stack ( §4.1); the hypervisor layer for managing F-NIC resources and QoS ( §4.2); the hardware layer which includes the support for OS abstractions, physical AFU logic (pAFU), a virtualization framework exposing virtual AFUs (vAFUs), and a hardware runtime with network I/O services for application-level message processing on AFUs.
Among our primary goals is to simplify the use of inline accelerators in existing applications with minimal changes.
Our abstractions thus provide a general interface for AFU management, which is compatible with standard network I/O interfaces.
They allow application control of AFU execution and efficient communication between the host and the AFU.
An ikernel is an OS object that represents an AFU in a user program.
An owner process creates an ikernel and controls it exclusively.
Essentially, the ikernel extends the process abstraction into the AFU, and NICA protects the ikernel state from other CPU processes and ikernels.To invoke an AFU, it must be associated with an active network flow.
Thus, applications activate the AFU by attaching one or more sockets to its ikernel, thereby rerouting the associated traffic through the AFU.
The ikernel stops processing the socket's traffic when the application detaches or closes the socket, keeping the AFU state intact for later invocations.
Dynamic attachment adds flexibility by enabling software involvement in connection establishment and session preparation, thereby allowing applications to activate an ikernel only for specific clients or request types, for example.The attachment semantics depends on the protocol.
For a UDP socket, the ikernel receives all incoming packets destined to the socket's listening port.
For TCP sockets, the effect of attachment depends on the socket state.
Attaching a connected TCP socket migrates its state to the AFU hardware network layer.
After a process attaches a listening TCP socket to an ikernel, the AFU handles new connection requests, as applications with a high connection rate may benefit from accelerating the connection establishment process.
Nevertheless, NICA notifies the host network stack about new connections, off the critical path, to provide application control over these connections from the host.A process may create several shared-nothing ikernels of the same AFU, e.g., to keep different cryptographic contexts for a crypto-AFU, but our intended usage is one ikernel per AFU per process.
Multiple threads of a process may attach Poll ring for completion Table 1: Control (top) and data plane (bottom) ikernel API.
their sockets to the same ikernel, thereby sharing the AFU state among them.For now, a socket can be attached only to a single ikernel, but we plan to enable ikernel chaining in the future ( §4.4).
Alternatives.
We choose the ikernel abstraction because it captures the intuitive application-level semantics of inline network processing.
We also considered using match-action rules, as in FlexNIC [50] and DPDK [84].
These are not associated with sockets, but rather with packet header rules, e.g., selecting packets of a specific five-tuple.
Such an interface suits packet processing but is too low-level for application logic offloading.
The ikernel socket-level abstraction hides the details of the hardware-resident network stack and allows simpler integration with existing applications.
The control APIs (Table 1) allow initialization, teardown, and access to the AFU-resident application state.
Under the hood, they interact with the network stack on the host and on the F-NIC to coordinate resource allocation and AFU processing.
Initialization and attachment.
An ik_create call initializes an ikernel given an AFU's UUID.
When the ikernel attaches to a socket it updates the F-NIC network stack.
Once the ikernel is attached, the NICA driver tracks the socket state, detaching the flow when the socket is closed.
The application may also detach an ikernel before the connection terminates.The ik_create call may initialize a pre-loaded AFU or load it at runtime using partial reconfiguration.
The ikernel abstraction hides the AFU hardware initialization details from the user, leaving the OS in charge of manipulating the FPGA-AFU allocation, similarly to AmorphOS [51].
Application state.
Applications may access the ikernel state in the AFU.
The hardware could expose the state in two ways: (1) as shared memory between the host and the F-NIC; or (2) using remote procedure calls (RPC) from the CPU to the AFU that retrieve/set the state.
Shared memory might not be efficient, however.
First, FPGA logic can keep frequentlyaccessed data in private memory, such as registers or block RAM, for efficiency.
This memory is not exposed to the CPU.
Further, access to the shared state requires explicit synchronization, which is costly over PCIe.
Therefore, we chose the RPC model, which allows the AFU to implement arbitrary atomic transactions, including e.g., getting a snapshot of its state.
Internally, NICA also uses the same mechanism to control transport layer and QoS parameters.
Error handling.
An AFU that encounters an error exposes it to an ikernel runtime which periodically checks for errors via the RPC mechanism.
In addition, the ikernel may abort the connection or detach itself from the respective sockets and forward packets without offloading.
NICA provides two ways to perform network I/O with inline acceleration: POSIX API and custom rings.
POSIX networking API.
After attaching an ikernel to a socket, the application may use standard I/O calls, e.g., send, recv, and epoll_wait, while the AFU transparently processes the data in-flight.
We currently support the POSIX APIs only for UDP sockets.
Custom rings.
POSIX I/O interfaces incur the overhead of extra data copies into user buffers [81] and host-side network stack processing.
On the other hand, an AFU may need to exchange application-level messages with the host application.
For example, a deserialization AFU may send ready-to-use data objects to the application.
Furthermore, an AFU may need to steer the processed messages to different CPU cores, i.e., for application-aware load balancing.NICA introduces a custom ring 2 (CR) abstraction that provides a zero-copy API for sending/receiving application messages, bypassing the host network stack.
Each ikernel may create multiple associated CRs to enable message steering for multi-core systems.The CR interfaces are similar to VIA/RDMA verbs [30] (Table 1).
Specifically, each CR comprises a queue pair (QP) and a completion queue (CQ).
The application allocates its communication memory buffers and registers them with the CR.
It then posts the send/receive requests to the respective queue in the QP.
The request completions show up in the CQ.
Custom rings vs. random access.
FPGA acceleration frameworks [37,43,101] and some I/O intensive AFUs [29,38,60] allow random access to CPU memory from the AFU, which is useful for fine-grain sharing of data-structures between the CPU and the AFU.
NICA currently focuses on the AFU tasks that communicate with the CPU via a streaming I/O pattern, which is much easier to implement using a producerconsumer CR interface.
We leave support for random host memory access for future work.
Synchronization.
In the most common application scenarios, networking or custom ring operations implicitly synchronize the CPU application and the AFU processing.
In more complex cases, when the AFU accumulates the application state (e.g., for network I/O monitoring or consensus), the ikernel RPC interface allows AFU developers to provide applicationspecific mechanisms to safely access ikernel state.
We expect adding ikernels to existing applications to require relatively small design or code changes.
In case of filtering (see §3.2), an application may still use POSIX sockets as before, while receiving only the filtered data.
For example, memcached requires no changes to its data processing to use the KVS cache AFU ( §6).
Data transformation tasks, such as deserialization, may use custom rings to obtain or send back the data in an application-friendly form.
Steering applications may use per-core custom rings to get the contents directly to the correct application thread or a GPU.
A generation application, e.g., replication, may send only one data copy via the custom ring, while the AFU will distribute it to preconfigured destinations.
To support fine-grain sharing of AFUs, as required for low latency applications, we introduce the notion of a virtual AFU, vAFU, which represents a single isolated hardware entity on the F-NIC.
Each vAFU provides state protection and performance isolation across all the shared resources on the F-NIC.
To clarify, a vAFU is a hardware entity, whereas an ikernel is an OS object that belongs to a process.
Connecting multiple ikernels to the same vAFU might be possible, i.e., allowing in-VM resource allocation policy enforcement, yet we do not support it in our prototype.One F-NIC may host multiple physical AFUs via space sharing, whereas each such AFU may support multiple vAFUs via fine-grain time sharing, as explained below.
For example, our key-value-store cache AFU supports 64 vAFUs, allowing concurrent acceleration of up to 64 different memcached servers on the same F-NIC ( §6).
Fine-grain AFU sharing.
Supporting multiple vAFUs on a single physical AFU requires low-overhead hardware context switching mechanism.
The vAFU context includes the ikernel state in DRAM and registers and the contexts of the sockets connected via that vAFU.
Each received packet may belong to a different vAFU so slow context switch would not only increase application latency but also increase the required NICA internal buffer space.To support fine-grain sharing, we store the vAFU context by reserving fast memory for each vAFU rather than evict/reload it to/from slow DRAM memory.
Specifically, the AFU registers are replicated to store data for all concurrently active vAFU contexts.
Each vAFU is associated with a hypervisorchosen tag.
The AFU switches to the context requested by the scheduler by updating the active tag register.
Such a context switch can be extremely fast, e.g., up to 3 clock cycles in our prototype.However, the number of vAFUs that can be supported is constrained due to the limited size of fast memory on the FPGA.
For more vAFUs, AFUs may use DRAM to store the contexts and use latency hiding techniques, i.e., increased concurrency.
Our current prototype uses fast memory, yet it is enough to host up to 64 vAFUs for the evaluated applications.
NICA protects the vAFU state in DRAM, fast memory, and hardware registers.
For the DRAM, we use a segment-based MMU for simplicity.
Similarly, we protect the control registers of the RPC interface by including a vAFU tag.Additionally, NICA ensures correct steering of network traffic to and from the vAFU via its on-NIC network stack ( §5.3).
In particular, it guarantees that a vAFU will not perform network spoofing attacks toward the host and will receive only the packets destined to that vAFU.
These two aspects are essential for supporting untrusted AFUs in NICA.
NICA supports isolation of I/O channels and compute resources.
The compute scheduling is necessary only among the vAFUs of the same physical AFU.
The FPGA loads different physical AFUs into different partitions, and thus they do not share FPGA compute resources.
DRAM bandwidth partitioning is left for future work.
I/O bandwidth sharing.
The bandwidth allocation between tenants is often implemented inside a virtual switch or in the NIC internal switch.
However, in a bump-in-the-wire architecture the F-NIC sends vAFU-generated messages directly to the network, bypassing these policies.
Therefore, NICA provides its own bandwidth allocation mechanisms, similar to the traffic class (TC) mechanisms used in NICs [10].
To control the vAFU egress bandwidth, both towards the CPU and towards the network, we add a set of TC queues (see Figure 3).
Packets are classified to these queues and scheduled.
We use a work-conserving deficit round robin (DRR) scheduler [95] to allocate bandwidth, but more complex policies can be used.
NICA's bandwidth scheduler is trusted and used by all the vAFUs on the F-NIC.
The vAFU recognizes when the TC queues are full and may drop the packets or propagate the contention if possible.
For example, it may slow down the host by using custom ring flow control or slow down the sender through explicit congestion notification (ECN).
NICA does not manage the ingress bandwidth into the vAFU from the network or the host, as the sender (TOR or host virtual switch) already shapes ingress traffic.
AFU compute sharing.
An AFU must determine which vAFU to activate at any given time, and which packets to serve first.
We considered two design options: a general compute scheduler for all AFUs (similar to the I/O scheduler) or an internal AFU-specific scheduler for each AFU.
These two options represent an inherent trade-off between FPGA resource consumption and design generality.A generic scheduler in front of the vAFUs could reorder packets according to a global policy, simplifying the AFU design.
However, such a scheduler requires deep input queues, therefore increasing consumption of F-NIC fast memory.
Further, the need for queuing is protocol-dependent.
For example, TCP has its own input queues to receive out-of-order packets, so extra scheduling queues would be wasteful.
Moreover, AFUs may customize queue contents to save resources, e.g. by keeping parsed requests instead of full packets.We thus decided to implement a custom, applicationspecific scheduler in each AFU.
AFUs implement hardware interfaces to receive/transmit transport layer and custom ring data, configuration and control interfaces for RPC, and, optionally, provide vAFU scheduling and virtualization.All the packets passing through an AFU are tagged with metadata that identifies the associated ikernel and flow, which can be used by the AFU for ikernel state isolation.
The AFU receives per-TC usage levels and CR flow control (see §5.2).
While designing such FPGA hardware can be difficult, we try to simplify the development by using high-level synthesis to design our AFUs in C++, and use the ntl class library [32] to implement common modules such as AFU schedulers and control-plane interfaces.
In addition, the NICA hardware runtime handles some common tasks such as transport processing and egress scheduling, thus simplifying AFU development.
F-NIC transport layer.
An inline AFU requires transport layer services to process data at the application layer; it may terminate flows or generate and send new messages.
Our current design uses a full implementation of UDP and TCP logic in hardware.
With this solution, the F-NIC effectively runs its own complete network stack.A complete TCP/IP stack in hardware simplifies AFU development but increases F-NIC resource consumption and maintenance difficulty [75].
To eliminate NIC transmission buffers, an AFU could generate retransmissions on-demand or use host memory [85,97].
If packet reordering is rare, an AFU may process received data only in-order, deferring out-of-order packets to the CPU [85].
A resource-efficient TCP design for inline AFUs warrants further research, so we choose a simple solution to evaluate the ikernel abstraction compatibility with TCP.
Virtual switch offloading.
F-NICs intercept the inbound network traffic before it reaches the CPU.
As a result, it becomes difficult to handle hypervisor policy and virtual networking rules, e.g., as in Open vSwitch, because they are typically handled by the hypervisor's virtual switch software running on CPU.
This issue is not unique to NICA and exists with standard SR-IOV NICs [33].
Typical solutions pass the first packet to software and offload per-flow policy to hardware match-action rules [33,59,78].
While this may take significant area of the F-NIC's FPGA [33], future F-NIC designs may be able to harden this functionality [20,31].
Multi-AFU support and services.
Our design provides all the necessary mechanisms to run multiple AFUs on the F-NIC: packet schedulers, steering, RPC and MMU isolation modules.
Currently, a single socket may only be attached to a single AFU.
However, there are use cases for chaining several AFUs in a single application to accelerate various aspects of the server's traffic [16,56,117].
Multi-AFU chaining requires extensions to resource isolation mechanisms and software interfaces, which we plan to explore in the future.
We implement NICA for the Mellanox Innova F-NIC and integrate it with the KVM/QEMU hypervisor and VMA userspace networking library [69].
NICA implements hardware virtualization of the physical AFUs, exposing virtual AFUs (vAFUs in Figure 2) to VMs.
Currently, the hypervisor allocates one vAFU for each requested ikernel.
NICA isolates the vAFU I/O channels in hardware and requires no software mediation.We utilize the NIC's SR-IOV functionality to virtualize the data path (both POSIX and custom rings).
SR-IOV enables unmediated overhead-free access from the guest to the NIC hardware.
In general, implementing SR-IOV in custom accelerators is quite challenging, but the bump-in-the-wire architecture of our F-NIC allows reusing the existing NIC hardware SR-IOV mechanism.
For the control plane, which is less sensitive to performance, NICA uses para-virtualization.
We implement the NICA API in the libnica library.
It integrates with the VMA user-space networking library, providing the POSIX socket API with kernel bypass and direct hardware access.
We modify VMA to support the ikernel abstraction.The NICA VM driver mediates between libnica and the hypervisor's NICA manager daemon, using a para-virtual device (virtio-serial).
The NICA manager runs in the hypervisor and controls AFU hardware through the F-NIC kernel driver.
NICA software stack is about 2,200 LOC.
Custom ring using RoCE.
We use the F-NIC's RoCE support [105] hardware and software layers using the bump-in-the-wire architecture.
The implementation associates CRs with RoCE unreliable connected (UC) queue pairs (QPs).
To send to a specific CR, NICA's transport layer generates RoCE packets to the host, targeting the appropriate QP.
The ASIC RoCE engine writes the data directly to the application buffers, providing address translation, DMA, and completion notifications.In our bump-in-the-wire F-NIC, the FPGA logic does not have a direct end-to-end flow control mechanism with the host, and UC does not provide such a mechanism either.
Therefore, NICA adds a credit-based flow control mechanism between the AFU and the CPU application.
The custom ring APIs transparently invoke this mechanism.
Figure 3 shows our FPGA processing pipeline.
For clarity, we describe ingress (from network to host) only.
The FPGA runtime provides the hardware support for inline programming abstractions and the essential services for inline acceleration.
These include: (1) the custom rings and RPC mechanism to support efficient data and control plane primitives for ikernels; (2) a memory management unit (MMU) for memory isolation; (3) a network processing stack to support application-level processing in the AFU, which includes the parser, flow steering, and the transport layer; and (4) a virtualization layer, implementing AFU and packet schedulers.
We develop NICA and the evaluated AFUs in HLS [114] and Verilog.
TCP/IP implementation.
Our prototype includes full support for UDP and partial support for TCP.
The UDP/IP service splits/combines the header and the payload.
As the CR utilizes RoCE over UDP, it also uses the UDP/IP service.The TCP implementation builds on an existing 10 Gbps FPGA TCP/IP stack [96].
Its integration with NICA is incomplete, as it lacks virtualization and socket migration support (though existing techniques apply [8,27]).
It is included primarily to validate how NICA abstractions hold with TCP.
Our prototype may run only two physical AFUs, where one is a minimal AFU that passes through unmodified traffic.
This is not a design limitation but stems from the FPGA area constraints (see Table 2).
Further, NICA does not yet support virtual switch offloading, and our current CR implementation does not transmit, only receives.
In addition, our F-NIC does not support partial reconfiguration.
We hope the next generation of the F-NIC [31] will resolve these limitations, as it is expected to have a larger FPGA with more space and hardened network virtualization support.
NICA performance drops dramatically when crossing NUMA links.
We are investigating a potential hardware bug.
CPU baseline.
We use VMA [69] user-level network stack with kernel bypass, optimized by Mellanox and broadly used for high-performance networking [33].
We use commodity NICs with the same ASIC as our F-NIC but without the FPGA.
Due to the NUMA performance issue of the current prototype ( §5.4), to allow a fair comparison, we constrain our experiments to the NUMA node closer to the NIC.
We use several microbenchmarks to evaluate the benefits of NICA acceleration through filtering and transport layer acceleration and to estimate virtualization overheads.
Experiment 1: Virtualization performance.
Figure 4a shows the throughput-latency comparison of bare-metal and virtualized echo server AFU vs. the CPU baseline for 64-byte packets.
We measure no overheads of the AFU virtualization.
At 5 Gbps, the latency of the virtualized AFU is 2×/2.8× lower than bare-metal/in-VM CPU server respectively.
At 6.7 Gbps, the baseline latency spikes to 38 µs, while the AFU achieves up to 27.6 Gbps at 4 µs latency, above which we see packet drops.
The stable low latency at high throughput is a valuable property of F-NIC accelerators.
Experiment 2: UDP performance.
We run a pass-through AFU that receives UDP packets and transfers them to the host via CRs, saving the host UDP processing.
The CPU baseline uses VMA for POSIX API kernel bypass, with 6 CPU cores.
Figure 4b shows the throughput for different packet sizes.
Offloading UDP processing to the AFU boosts the throughput from 2.9× and 1.7× for small and large packets respectively.
For larger packets, a single-core CR outperforms 6-core UDP.
Experiment 3: TCP performance.
We evaluate NICA's preliminary TCP support by accelerating a monitoring server microbenchmark.
The server receives integers as 18-byte messages (4-byte integers with a 14-byte sockperf header) and computes their average, alerting the user when the received values are above a given threshold.
With NICA, the AFU maintains the average and sends only the messages above the threshold via the custom ring (bypassing the host TCP stack).
For 6 flows from 6 clients, the AFU consumes 34.8M messages/sec, 3× faster than the baseline's 11.5M messages/sec (single core).
The AFU benefits diminish as the portion of the messages sent to the host increases, down to a modest 11% throughput improvement.
This indicates that the F-NIC transport layer processing contributes much less than filtering to the overall performance benefits.
Experiment 4: I/O isolation overheads.
We evaluate the egress scheduler when using two AFUs: a traffic generator AFU and a pass-through AFU.
The former generates messages to the network at maximum throughput.
The latter transfers messages between the host and the network.
These AFUs share the network egress I/O channel and are assigned to separate traffic classes.
We set the scheduler quantum to 1 KB.We measure the latency of a few 64-byte packets sent via the pass-through AFU while the generator AFU sends 1514-byte packets.
At 38.4 Gbps load, the low-latency pass-through packets suffer a 1 µs overhead to 99 th -percentile latency compared to an empty system.
This result demonstrates that the I/O isolation mechanism achieves low overhead even under heavy contention.
We accelerate two large applications: memcached and a Node.js-based IoT server.
We build a transparent cache AFU for the former and an authentication AFU for the latter, integrating both into the CPU software.
We prototype a transparent look-through cache for memcached, called NICA-KVcache.
The AFU parses memcached's ASCII UDP protocol and serves GETs directly from its F-NIC DRAM-resident cache.
The AFU passes GET misses and other update requests to the host.
Upon update, the AFU invalidates the respective cache entry.
The AFU populates the cache by intercepting GET responses from the host, ensuring coherence even if the host drops the updates due to overload.
The AFU caches keys/values of up to 16-byte and uses a direct-mapped cache for simplicity.We implement two designs: one with POSIX API and another with CRs.
The former requires changing memcached to instantiate the ikernel and attach sockets.
The latter introduces CR polling to the memcached worker thread event loop.
Adding the F-NIC acceleration support required 107 and 135 LOC for the POSIX API and CR versions respectively.
Workload.
We initialize the CPU server with 32 M 16-byte keys and values (4 GB with overheads) and set the AFU cache to store 2 M keys per-ikernel (128 MB RAM).
The CPU baseline uses an unmodified memcached with the VMA network stack.
Clients generate a YCSB-like [25] workload with varying skew using sockperf.
Bare-metal performance.
Figure 5a shows that for lower skews (high miss rate), the CPU (6 cores) is the bottleneck.
With Zipf(0.99) distribution (YCSB's default), NICA+CR achieves 9× speedup.
For 100% hit-rate, the AFU becomes network-bound (99% of 40 Gbps line-rate), resulting in 21× higher throughput than the baseline.The cache hit-rate also dictates the latency distribution (not shown).
We observe a mixture of two distributions: cache hits and cache misses.
With Zipf(0.99) distribution and 1 Mtps load, the F-NIC serves cache hits at a stable 2.1 µs.
Misses, served by the host, are 6 µs at the 99 th -percentile, versus 10.5 µs in the baseline.
The latency improvement is due to the reduced CPU load as a result of filtering.
Table 3 shows the throughput with 0.2% SETs (common in Facebook [15]).
At Zipf(0.99), NICA is 4.6× faster than the baseline.
With 10% SETs (not shown), CPU throughput dominates, thus NICA shows no performance improvement.Other KVS implementations.
NICA-KVcache offers significant advantages even when used with highly optimized CPU-only KVS implementations, such as MICA [63], which achieve line-rate throughput using CPU cores alone.
In this case, NICA-KVcache reduces the required number of CPU cores by filtering all the cache hits and leaving only the misses to the CPU, thereby improving the overall system efficiency.
More specifically, for a given hit rate in the NICAKVcache, achieving line-rate requires the CPU throughput to be line_rate · (1 − hit_rate) transactions per second.For example, MICA [63] reaches 5 Mtps on a single CPU core with 100% GETs for 32 M 16-byte keys and values (1GB of data) with a Zipf(0.99) distribution.
Optimistically assuming perfect scaling, MICA would reach line-rate (59.5 Mtps) with 12 cores, without NICA-KVcache acceleration.
In contrast, with NICA-KVcache of size 128MB, running the same Zipf(0.99) key distribution results in 75% hit-rate, thus the CPU only handles 14.9 Mtps, utilizing just 3 CPU cores.This result demonstrates that the use of NICA for accelerating key-value stores is cost-effective, considering that a single CPU core is reportedly more expensive than a SmartNIC [33].
Accelerated KVS.
Floem [83] implemented a similar keyvalue store cache on a Cavium SmartNIC and reported a 3.6× performance improvement with 100% hit-rate with writeback, and no benefits for 10% SETs write-through, as in NICA.
Rather than memcached, Floem required a custom KVS server, however.
KV-Direct [60] with small requests achieves comparable performance to NICA (with 100% hits) but reaches 180 Mtps using client-side batching.
Unlike NICA-KVcache, its data-plane is fully implemented in hardware, and it only uses the host for slab allocation.
Contribution of network-stack processing.
Figure 5a shows that using CR for low cache hit rates results in 2.2× speedup over the CPU baseline.
In this case, the use of CR eliminates the network stack processing on the host but keeps the application processing on the CPU.
Naturally, higher hit rates result in a higher portion of the requests handled by the AFU, and much higher speedups.
This experiment suggests that the network stack offloading alone is not enough to reach the full performance potential of the F-NIC acceleration.
Virtualization performance.
We evaluate the performance with a varying number of VMs.
Each VM uses 5 GB of server RAM, 1 dedicated CPU core, a vAFU, and 2 M keys worth of vAFU cache.
For Zipf(0.9), Figure 5b shows nearlinear scaling, consistently achieving a 5.6× speedup over the CPU.
Further, we observe no measurable negative impact of virtualization on vAFU latency.
The system achieves similar results with 64 M keys per VM, utilizing most of the 64 GB RAM of our machine.
Figure 5c shows the latency distribution of a single VM and 6 VMs executions under 1.3 Mtps load, for a Zipf(0.90) workload.
The latency increases for the top 40% of the requests, which matches the expected hit-rate.
We observe that the VM CPU latency is much higher than the bare-metal latency reported above, but cache hits are served at the same latency with and without virtualization.This experiment confirms that AFU fine-grain sharing is feasible and effective.
Network bandwidth isolation.
We use 3 VMs, associated with 3 TCs, and initially configure them to share the egress bandwidth equally.
We use a Zipf(1.4) distribution (99.9% hitrate), and a 20 Mtps load on each VM, to stress the scheduler.
Figure 6a shows the throughput of each VM over time.
At first, only VM 1 is active, using the whole AFU.
When VM 2's clients start, the combined egress throughput is barely above the AFU's maximum (39 Mtps), and the clients process 19 Mtps each.
When VM 3's clients start, the combined throughput surpasses the maximum, and the scheduler divides the bandwidth equally (13 Mtps per VM).
At t 3 , we change the bandwidth allocation to 40%/40%/20% and observe an asym- metric allocation.
This confirms the NICA egress isolation is successful in allocating bandwidth among the tenants.
We prototype an IoT monitoring server using Node.js with JSON web token (JWT) stateless authentication.
The JavaScript-based server exposes an endpoint to which IoT devices publish their measurement using the CoAP protocol [12], similarly to the SAMSUNG Artik IoT cloud API [92].
The payload of each request contains an authentication token, which includes the device ID and a timestamp, and signed using HMAC-SHA256.
Invalid requests are discarded.
Our prototype authentication AFU parses received packets, extracts the token, verifies the signatures (using a SHA-256 accelerator [99]) and drops requests with invalid tokens.
Valid requests are passed to the CPU and only undergo token expiration check there.We evaluate our IoT authentication accelerator against a software-only Node.js server.
Adding NICA support using POSIX APIs required 20 JavaScript LOC and 34 lines for the libnica generic Node.js module, demonstrating the simplicity of integrating the ikernel abstraction with complex software.In this experiment, we simulate a Denial of Service (DoS) attack by sending a varying number of invalid tokens with incorrect signatures in the input stream.
Table 4 shows the goodput, in requests/sec, as a function of the valid packet ratio.
While the baseline degrades linearly, NICA maintains a constant goodput by filtering the invalid packets.One may wonder whether optimizing the Node.js server (e.g., rewriting it in C) would diminish the AFU acceleration benefits.
We argue that this is not the case.
The AFU hardware achieves the throughput of 3.5 Mtps, about 3 orders of magnitude higher than the software throughput.
As long as the rest of the CPU processing pipeline remains the bottleneck, the AFU remains effective.
Additionally, compute acceleration alone results in only 30% speedup.
The remaining speedup is due to filtering invalid packets, which would be helpful in the CPU-optimized version too.
AFU compute sharing.
The AFU's throughput can be bounded by its SHA-256 hashing units and depends on the input JWT token sizes.
To fairly share the hashing units among vAFUs, we introduce a custom DRR scheduler ( §4.2.2)) that controls the per-VM utilization of the AFU hashing units.We use 2 VMs to demonstrate the performance isolation.
Clients send 10 Mtps of invalid requests to each VM, but VM 2 receives requests with 40% larger tokens.
We start the experiment with the scheduler disabled and enable it mid-run.
Figure 6b shows the throughput of each VM over time.
At first, only VM 1 clients are active, allowing the AFU to process at max speed (3.5 Mtps).
When VM 2 begins receiving at t 1 , VM 1 processes only 28% of the requests, which is below its fair share.
With the scheduler enabled, at t 2 , both VMs receive half of their respective maximum throughput.
We observe that NICA's compute performance isolation is essential to allow sharing of compute-bound AFUs.
NIC-based acceleration.
Commodity NICs have been offering network stack offloads ranging from checksum calculations, segmentation, and receive-side-scaling (RSS) to RDMA [9,82,87,105,110] and TCP offload engines [75].
Such offloads are limited to network and transport layer processing, while NICA focuses on the application layer.Our work builds upon previous attempts to accelerate general purpose applications through inline processing in SmartNICs.
Early work on Network Processing Units (NPUs) [1,113] programming abstractions [16,56] has shown the potential of customizing the I/O path for applications.
More recently, FlexNIC [50] has proposed an RMT-based [14] NIC for inline acceleration of application packet processing, showing how to leverage RMT hardware for application acceleration.
Floem [83] aids design of NPU accelerated applications.
sPIN [38] offers inline acceleration of high-performance computing (HPC) tasks such as tag-matching, data transformation, or replication, but the Portals 4 host abstraction is unsuitable for socket applications.While we also consider inline acceleration, our goals, design, platform, and evaluation methodology are different.
FlexNIC focuses on applications of SmartNIC RMT acceleration, whereas NICA offers convenient OS abstractions for integrating inline accelerators into user applications.
FlexNIC targets RMT SmartNICs with constrained functionality, whereas NICA targets more flexible bump-in-the-wire FPGAs.
These may run large parts of application logic, necessitating more expressive interfaces for state and execution management, such as host-NIC network stack interaction.
As RMT devices are designed to work at line-rate, performance isolation of concurrent application pipelines is unnecessary; conversely, we show that QoS support is essential to expose F-NICs in cloud systems.Packet processing frameworks such as DPDK and eBPF-XDP [39] include inline acceleration mechanisms, e.g., for cryptographic protocols such as IPSec [84] or offloading eBPF programs to SmartNICs [52].
However, these target systemwide packet processing tasks, so they lack a transport layer, network stack integration, and multiple application support.Linux also supports attaching eBPF programs to sockets [26], similarly to ikernels, to perform inline packet processing.
However, such programs cannot process transmitted packets or generate new ones, and use a POSIX API data-path, whereas ikernels enable zero-copy application messaging.C-CORE [56] proposes the stream handlers abstraction for inline processing, but unlike ikernels, they provide no virtualization mechanisms.
Streamline [16] is an OS subsystem for tailoring application I/O path that uses UNIX pipes as an abstraction, but it does not allow dynamic attachment and configuration of filters.Some F-NIC vendors have proprietary APIs for inline application development.
Solarflare AOE allows low latency TCP transmission [97] from an F-NIC.
Unlike NICA, it only offloads transmissions.
Maxeler MPC-N supports inline UDP/ TCP application acceleration [7].
All the above lack virtualization support, and their proprietary host application abstractions are too hardware specific.
SmartNIC applications.
Eden [6] and AccelNet [33] accelerate network functions on data-center end-nodes with SmartNICs.
However, these are loosely coupled with host applications, whereas NICA's model couples the AFU logic with the host server logic.Hardware accelerators for Network Function Virtualization (NFV) [18,34,117] target the NFV domain and hence do not provide abstractions for general purpose applications, lack host-accelerator network stack integration provided by ikernels, and provide no I/O path virtualization to/from the accelerator.Several works have accelerated specific applications on F-NICs [24,57,60,64,106,107].
NICA provides an infrastructure for building such AFUs in the clouds.
Languages for SmartNIC AFU development.
P4 [13] is a DSL for implementing network functions with implementations for FPGAs [100,111].
The Click [55] router has been ported to F-NICs [61,90].
Emu [102] enables the development of network functions on NetFPGA using HLS.
These can be used to simplify AFU development for NICA, but do not provide application-level abstractions.Floem [83] is a DSL for NPU-accelerated applications.
However, it requires refactoring applications to its DSL, while ikernel abstraction is less intrusive.
FPGA virtualization and sharing.
AmorphOS [51] improves FPGA utilization by sharing an FPGA among multiple AFUs, and dynamically switching AFUs.
Its hull isolates different AFUs used by different applications.
We apply similar mechanisms to F-NIC.
However, AmorphOS does not isolate FPGA network interfaces, and its context switching mechanism is not suitable for latency-sensitive networking applications.Multes [44] shares an FPGA among tenants using a single pipeline.
AccelNet [33] allows flow-context switching on a packet-by-packet basis.
NICA's fine-grained time-sharing design is similar, but its goal is to virtualize inline accelerators for application layer, rather than a standalone FPGA application or cloud network/transport layers.Remote/distributed FPGA frameworks [7,19,104] share FPGAs over the network with a remote CPU.
Other have virtualized local look-aside accelerators [23,36,101,118].
In contrast, NICA virtualizes local inline networking AFUs.
Standalone FPGAs, GPUs, or switches.
Our choice of FPGA-based SmartNICs has been motivated by prior works on accelerating networking applications [11,21,45,76,103,109].
Unlike NICA, they focus on standalone FPGAs.Other inline acceleration techniques let GPU kernels control communication using GPU-centric networking abstractions [28,54,58,74], or process data in transit on programmable switches or network accelerators [46,62,65,93].
Conversely, NICA provides tighter integration of server software and AFUs.
This simplifies integration with legacy programs and makes acceleration transparent for clients.
As F-NICs are becoming common in data centers, new use cases for application layer inline acceleration are starting to emerge.
NICA provides the ikernel OS abstraction to easily integrate F-NIC-based accelerators into applications and introduces virtualization mechanisms to share them securely and fairly in cloud systems.
NICA's real-world prototype demonstrates the significant performance potential for inline acceleration of virtualized server systems, with minimal software development effort.We believe NICA's inline abstractions are suitable beyond F-NICs and plan to investigate their use in CPU-FPGA systems and non-FPGA SmartNICs.
NICA raises a range of research topics, such as distributed heterogeneous architectures, accelerator chaining, and reliable transport offloading, which we will explore in the future.
We thank Chris Rossbach, Michael Swift, Ada Gavrilovska, Aleksandar Dragojevic, and our shepherd Scott Rixner for their valuable feedback.
We also gratefully acknowledge the support of the Israel Science Foundation (grant No. 1027/18), the Israeli Innovation Authority Hiper Consortium, the Technion Hiroshi Fujiwara Cybersecurity center, as well as Mellanox hardware donations and technical support.
