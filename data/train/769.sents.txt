Most e-mail readers spend a non-trivial amount of time regularly deleting junk e-mail (spam) messages , even as an expanding volume of such e-mail occupies server storage space and consumes network bandwidth.
An ongoing challenge, therefore, rests within the development and refinement of automatic clas-sifiers that can distinguish legitimate e-mail from spam.
A few published studies have examined spam detectors using Naïve Bayesian approaches and large feature sets of binary attributes that determine the existence of common keywords in spam, and many commercial applications also use Naïve Bayesian techniques.
Spammers recognize these attempts to thwart their messages and have developed tactics to circumvent these filters, but these evasive tactics are themselves patterns that human readers can often identify quickly.
This preliminary study tests an alternative approach using a neural network (NN) classifier on a corpus of e-mail messages from one user.
The feature set uses descriptive characteristics of words and messages similar to those that a human reader would use to identify spam.
The results of this study are compared to previous spam detectors that have used Naïve Bayesian classifiers.
The volume of junk e-mail (spam) transmitted by the Internet has arguably reached epidemic proportions.
While the inconvenience of spam is not new -public comments about unwanted e-mail messages identified the problem as early as 1975 -the volume of unsolicited commercial e-mail was relatively limited until the mid-1990s [3].
Spam volume was estimated to be merely 8% of network e-mail traffic in 2001 but has ballooned to about 40% of e-mail today.
One research firm has predicted that the cost of fighting spam across the U.S. will approach $10 billion in 2003 [12].
Most e-mail readers must spend a non-trivial amount of time regularly deleting spam messages, even as an expanding volume of junk e-mail occupies server storage space and consumes network bandwidth.
An ongoing challenge, therefore, rests within the development and refinement of automatic classifiers that can distinguish legitimate e-mail from spam.Many commercial and open-source products exist to accommodate the growing need for spam classifiers, and a variety of techniques have been developed and applied toward the problem, both at the network and user levels.
The simplest and most common approaches are to use filters that screen messages based upon the presence of common words or phrases common to junk e-mail.
Other simplistic approaches include blacklisting (automatic rejection of messages received from the addresses of known spammers) and whitelisting (automatic acceptance of message received from known and trusted correspondents).
In practice, effective spam filtering uses a combination of these three techniques.
The primary flaw in the first two approaches is that it relies upon complacence by the spammers by assuming that they are not likely to change (or forge) their identities or to alter the style and vocabulary of their sales pitches.
Whitelisting risks the possibility that the recipient will miss legitimate e-mail from a known or expected correspondent with a heretofore unknown address, such as correspondence from a long-lost friend, or a purchase confirmation pertaining to a transaction with an online retailer.A variety of text classifiers have been investigated that categorize documents topically or thematically, including probabilistic, decision tree, rule-based, example-based ("lazy learner"), linear discriminant analysis, regression, support vector machine, and neural network approaches [10].
A prototype system has also been designed to recognize hostile messages ("flames") within online communications [11].
However, the body of published academic work specific to spam filtering and classification is limited.
This may seem surprising given the obvious need for effective, automated classifiers, but it suggests two likely reasons for the low volume of published material.
First, the effectiveness of any given anti-spam technique can be seriously compromised by the public revelation of the technique since spammers are aggressive and adaptable.
Second, recent variations of Naïve Bayesian classifiers have demonstrated high degrees of success.
In general, these classifiers identify attributes (usually keywords or phrases common to spam) that are assigned probabilities by the classifier.
The product of the probabilities of each attribute within a message is compared to a predefined threshold, and the messages with products exceeding the threshold are classified as spam.Sahami, et al. [9] proposed a Naïve Bayesian approach that examined manually-categorized messages for a set of common words, phrases ("be over 21", "only $", etc.), and non-textual characteristics (such as the time of initial transmission or the existence of attachments) deemed common to junk e-mail.
Androutsopoulos, et al. [1] used an edited, 1 encrypted, and manually-categorized corpus of messages with a lemmatizer and a stop-list, using words-attributes.
Both approaches used binary attributes, where X n = 1 if a property is represented and X n = 0 if it is not.
In each case, the selected words were the result of hand-crafted, manually-derived selections.
In addition to these approaches, several applied solutions exist that claim high success rates (as high as 99.5%) with Naïve Bayesian classifiers [2], [5], [6], [7], [8] that use comprehensive hash tables comprised of hundreds of thousands of tokens and their corresponding probability values, essentially creating attribute sets of indefinite size.
2 While these Naïve Bayesian approaches generally perform effectively, they suffer from two intrinsic problems.
The first is that they rely upon a consistent vocabulary by the spammers.
New words that become more frequently used must be identified as they appear in waves of new spam, and, in the case of hash tables, any new word must be assigned an initial arbitrary probability value when it is created.
Spammers use this flaw to their advantage, peppering spam with strings of random characters to slip the junk messages under the classification thresholds.
The second problem is one of context.
Binary word-attributes, and even phrase-attributes, do not identify the common patterns in spam that humans can easily and readily identify, such as unusual spellings, images and hyperlinks, and patterns typically hidden from the recipient, such as HTML comments.In summary, Naïve Bayesian classifiers are indeed naïve, and require substantial calculations for each e-mail classification.
A human reader, by contrast, requires relatively little calculation to deduce if a given e-mail is a legitimate message or spam.
While spammers send messages that vary widely in composition, subject, and style, they typically include identifiable tactics that are designed to garner attention or to circumvent filters and classifiers and that are rarely used in traditional private correspondence.
These evasive tactics are themselves patterns that human readers can often identify quickly.In this paper we apply a neural network (NN) approach to the classification of spam using attributes comprised from descriptive characteristics of the evasive patterns that spammers employ, rather than the context or frequency of keywords in the messages.
This approach produces similar results but with fewer attributes than the Naïve Bayesian strategies.
This project used a corpus of 1654 e-mails received by one of the authors over a period of several months.
None of the e-mails contained embedded attachments.Each e-mail message was saved as a text file, and then parsed to identify each header element (such as Received: or Subject:) to distinguish them from the body of the message.
Every substring within the subject header and the message body that was delimited by white space was considered to be a token, and an alphabetic word was defined as a token delimited by white space that contains only English alphabetic characters (A-Z, az) or apostrophes.
The tokens were evaluated to create a set of 17 hand-crafted features from each e-mail message (Table 1).
Binary feature indicating whether a color of any text within the body message was set to white: 1 = yes, 0 = no Number of URLs within hyperlinks that contain any numeric digits or any of three special characters ("&", "%" or "@") in the domain or subdomain(s) of the link The e-mails were manually categorized into 800 legitimate e-mails and 854 junk e-mails.
Half of each category was randomly selected to comprise a training set (n = 827) and the remaining e-mails were used as a testing set.
All feature values were scaled (normalized) to range from 0 to 1.
The training data were used to train a three-layer, backpropagation neural network with the number of hidden nodes ranging from 4 to 14 and the number of epochs from 100 to 500.
After training, the e-mail messages of the testing set were classified to obtain generalization accuracy results.
The relative success of spam filtering techniques is determined by classic measures of precision and recall on the testing subsets of legitimate e-mail and junk e-mail.
Spam precision (SP) is defined as the percentage of messages classified as spam that actually are spam.
Likewise, legitimate precision (LP) is the percentage of messages classified as legitimate that are indeed legitimate.
Spam recall (SR) is defined as the proportion of the number of correctly-classified spam messages to the number of messages originally categorized as spam.
Similarly, legitimate recall (LR) is the proportion of correctly-classified legitimate messages to the number of messages originally categorized as legitimate.
Thus, we define the counts: n SS = the number of spam messages correctly classified as spam.
n SL = the number of spam messages incorrectly classified as legitimate n LL = the number of legitimate messages correctly classified as legitimate n LS = the number of legitimate messages incorrectly classified as spam and the precision and recall formulas: Table 2 gives the results on the testing set by hidden node count and training epochs.
The trial with 12 hidden nodes and 500 epochs (highlighted in the table) produced the lowest number of misclassifications, with 35 of the 427 spam messages (8.20%) classified as legitimate (nSL), and 32 of the 400 legitimate messages (8.00%) classified as spam (nLS), for a total of 67 misclassifications.
Of the 35 misclassified spam messages, 30 were short in length -only a few lines, including HTML tagssome as brief as "save up to 27% on gas" followed by a hyperlink.
Among the remaining five messages: one had many "comments" without comment delimiters, thus creating nonsense HTML tags that some browsers ignore (but some do not -a risk this spammer was willing to take); two were written almost entirely in ASCII 5.5 escape codes; one followed four image files with English words in jumbled, meaningless sentences; and one creatively used an off-white color for fonts to disguise the random characters appended to the end of the e-mail.
LS SS SS n n n SP precision Spam + = ) ( (1) SL LL LL n n n LP precision Legitimate + = ) ( (2) SL SS SS n n n SR recall Spam + = ) ( (3) LS LL LL n n n LR recall Legitimate + = ) ( (4)The 32 legitimate messages were misclassified due mostly to characteristics that are unusual for personal email.
Twenty-two affected the features normally triggered by spam: six were from a known correspondent that prefers to write in white typeface on a colored background, ten were responses or forwards that quoted HTML that triggered several features, five were commercial e-mail from known vendors (with many hyperlinks and linkable images), and one was ranked as "low" priority from a known correspondent.
The remaining ten messages were less obvious: four included special characters or vowel-less words in the subject header, three had several words with multiple occurrences of rare English characters (feature 2), and three had an unusual number of hyperlinks (due, in part, to links in signature lines).
The NN accuracy of this study is similar to that of the Naïve Bayesian classifiers described in [1] and [9], and Table 3 presents a comparison.
For comparison purposes we also ran a small experiment with spam blacklist databases.
While some databases are part of commercial programs, most require manual entry of IP addresses one at a time, apparently designed primarily for mail server administrators who are trying to determine whether their legitimate e-mails are being incorrectly tagged as spam.
To test how accurately legitimate and spam e-mails are tagged by the blacklist databases, we manually entered the IP addresses of the e-mail messages that were incorrectly tagged by the NN classifier (32 legitimate and 35 spam e-mails) into a site that sends IP addresses to 173 working spam blacklists and returns the number of hits [4].
We entered both the first (original) IP address of each message and also, when present, a second IP address (a possible mail server or ISP).
While it is likely that the second IP column includes bulk e-mail servers of spammers, it is also likely that it includes non-spamming ISPs or Web portals that route junk e-mail messages but presumably do not participate intentionally in spamming.
Because we considered single-list hits to be anomalies since they aren't confirmed by any other blacklists on the site, we counted only hit counts greater than one as spam that would have been blacklisted.
The blacklisting results are presented in Table 4.
While the percentages of legitimate e-mails considered spam by the blacklists are lower than the percentages of spam correctly identified as spam, it is surprising to see that over half were incorrectly screened using our "at least two blacklists" criterion.
Even though we tested the blacklist databases with potentially difficult e-mails, the ones incorrectly classified by the NN classifier, the poor blacklisting results indicate that the blacklisting strategy, at least for these databases, is inadequate.
Although the NN technique is accurate and useful, its spam precision performance is not high enough for it to be used without supervision.
For this technique to be more useful, the feature set would require additional members or modifications.
It should be noted, however, that the NN required fewer features to achieve results similar to the Naïve Bayesian approaches, indicating that descriptive qualities of words and messages, similar to those used by human readers, can be used effectively to distinguish spam by a classifier.
As suggested in previous work [9], a combination of keywords and descriptive characteristics may provide more accurate classification.
A neural network classifier using these descriptive features, however, may not degrade over time as rapidly as classifiers that rely upon a relatively static vocabulary from spammers.
Strategies that apply a combination of techniques, such as a NN with a whitelist, would likely yield better results.
