Ordinal regression has become an effective way of learning user preferences, but most research focuses on single regression problems.
In this paper we introduce collabora-tive ordinal regression, where multiple ordinal regression tasks are handled simultaneously.
Rather than modeling each task individually , we explore the dependency between ranking functions through a hierarchical Bayesian model and assign a common Gaussian Process (GP) prior to all individual functions.
Empirical studies show that our collaborative model outperforms the individual counterpart in preference learning applications .
In recent years there has been quite some work on learning from ranking relations, which, in the literature, is often referred to as ordinal regression (McCul- lagh, 1980;Cohen et al., 1999;Herbrich et al., 2000;Crammer & Singer, 2002).
In this paper we are interested in probabilistic conditional models with a generative process for ranking data.
Some recent work in this direction include (Chu & Ghahramani, 2005a) in which Gaussian Processes (GP) are applied to ordinal regression, and ( Burges et al., 2005;Chu & Ghahra- mani, 2005b) where a probabilistic model for pairwise preferences is employed.Although ordinal regression research has often been motivated from user preference learning using the features of items or products (e.g., movies or books), a unified framework is still missing which can explore the correlation among different ranking functions and reflect the social effects of user preferences.
The problem differs from traditional collaborative filtering (see, e.g., Rennie & Srebro, 2005), because the item features should be elegantly explored in the new framework.
Web page ranking can also be viewed as an ordinal regression problem, since a ranked list of web pages should be returned for a given query ( Burges et al., 2005).
In this context it is interesting to explore the correlations across different queries, which can also be viewed as the problem of learning a set of related ranking functions.
Instead of treating the ranking functions individually, we would like to model them jointly to uncover the dependency between them.
We call this problem collaborative ordinal regression.In this paper we propose a Bayesian approach to collaborative ordinal regression.
The preference labels for one regression task are assumed to be generated from a latent function, and all the latent functions share a common GP prior to account for the interdependencies.
Learning in this model is based on expectation propagation (EP) (Minka, 2001) along with variational method to update the model parameters.
Our experimental results demonstrate that collaborative ordinal regression shows better performance than individual regressions in the presence of dependencies between ranking functions.The rest of paper is organized as follows.
In Section 2 we formally introduce collaborative ordinal regression with two example models.
Then in Section 3 and 4 we consider learning and inference, respectively.
Some empirical results are shown in Section 5, followed by Section 6 with conclusion and some future directions.
In this paper we use preference learning as a working example to describe our model.
We consider a set of n items, and each item has a d-dimensional feature representation x ∈ R d .
In ordinal regression we observe a preference label y for each item, which is an integer from 1 (lowest preference) to r (highest preference).
When a single ordinal regression function is considered, the data consist of pairs {(x i , y i )} n i=1 , where y i denotes the preference label for item x i .
One natural assumption in ordinal regression is that there is an unobserved latent function f (·) : R d → R which maps items into a real line, and the preference labels are then generated from the latent values f (x 1 ), . . . , f (x n ).
Put in a probabilistic way, let X = [x 1 , . . . , x n ] be the training items, y = [y 1 , . . . , y n ] be the labels, and f = [f (x 1 ), . . . , f (x n )] be the vector of latent values, the likelihood of preference labels is written asP (y|X, f, θ) = P (y|f (x 1 ), . . . , f (x n ), θ) = P (y|f , θ),where θ denote some likelihood parameters.
We further assume that the labels are mutually independent given the latent values, which leads to a factorized form P (y|f , θ) = n i=1 P (y i |f (x i ), θ).
Since f are not observable, we need to assign a prior P (f ) such that we can integrate them out in a Bayesian framework.
In this paper we take a nonparametric approach and assume that f are a realization of random variables in a Gaussian Process (GP) (Ras- mussen & Williams, 2006).
The GP can be fully specified by a mean function h(·) and a covariance matrix K.
The (s, t)-th entry of K, i.e., the covariance between the function values f (x s ) and f (x t ), is defined by any Mercer kernel function κ(x s , x t ).
One simple example is the Gaussian kernel κ(x s ,x t ) = exp −α d l=1 (x l s − x l t ) 2, where α > 0 and x l s denotes the l-th element of x s .
Then the prior for latent values f is a multivariate Gaussian,P (f |h, K) = 1 (2π) n 2 |K| 1 2 exp − 1 2 (f − h) K −1 (f − h) with h = [h(x 1 ), . . . , h(x n )].
In this paper we use the notation N (f ; h, K) to denote a (multivariate) Gaussian distribution for f with mean h and covariance K.The final likelihood of the preference labels is obtained by integrating over the latent function values f ,P (y|X, θ, h, K) = P (y|f , θ)P (f |h, K) df .
(1)In general the model has three parameters θ, h and K. θ are the parameters for the likelihood model and can assume various forms.
We will illustrate two example models in Section 2.3.
h and K are the parameters for the GP prior and are respectively defined by the mean function h and kernel function κ.
In the context of ordinal regression, h defines a prior preference for each item, and κ specifies the smoothness of the latent function f , i.e., how likely two similar items get the same preference label.
In typical GP models, h is assumed to be the zero function, and κ takes a parametric form with some kernel parameters, e.g., α in the Gaussian kernel.
Therefore, in model fitting one only needs to optimize likelihood parameters θ and the kernel parameters.
As introduced in the first section, in many real-world problems we need to learn multiple correlated ordinal regression functions for the same set of items.
One can of course treat each function separately and apply the model just introduced, but then we lose the collaborative effects of these correlated functions, which include (i) common preferences: they may share similar preference labels on some items; and (ii) similar variabilities: they tend to have the same predictability on very similar items.
For example, if items are movies and functions are users who give ratings, the first effect means the users may have common interests on some movies because of, e.g., popularities, famous directors or topics.
The second effect is related to the correlation of items in terms of associated user interests, i.e., a user who likes item A tends to like (or dislike) item B.
This correlation reflects some intrinsic properties of items, expressed by their feature vectors and user's opinions.In this subsection we introduce collaborative ordinal regression in which the multiple functions are modeled jointly in a hierarchical Bayesian framework.
For a bit of notations, assume there are m functions, and function j has preference labels y j on an item set X j of size n j .
1 Let Y = {y 1 , . . . , y m } be the labels of all functions, and X = {x 1 , . . . , x n } ⊃ m j=1 X j be the total item set.
The item indices of X that X j contains are denoted as I j .
Note that we allow "untouched" items which are not labeled by any function.In collaborative ordinal regression, we model the preference labels of each function j as an ordinal regression model P (y j |f j , θ j ) as in Section 2.1, where f j (·) is the latent function for j, and θ j are the corresponding likelihood parameters.
We then connect these models by assigning a common GP prior (h, K) to all the latent functions, following recent works on multi-task learning ( Schwaighofer et al., 2005;Yu et al., 2005).
Here h = [h(x 1 ), . . . , h(x n )] are specified by a mean function h(·), and K denote the n × n covariance matrix for X. Let h j = h(I j ) and K j,j = K(I j , I j ) be the sub-matrices of h and K with respect to X j , the label likelihood for function j is written asP (y j |X, θ j , h, K) = P (y j |f j , θ j )P (f j |h j , K j,j ) df j .
By assuming that labels for different functions are independent given the GP prior, we obtain the following likelihood for the whole label set Y:P (Y|X, Θ, h, K) = m j=1 P (y j |X, θ j , h, K),(2)where Θ = {θ 1 , . . . , θ m } denote the set of all likelihood parameters.In this framework, the collaborative effects among different functions are modeled via a GP prior (h, K).
In particular, mean function h defines the common preferences of all functions, and covariance matrix K specifies the smoothness of these functions over all items.
Unlike the single function case where we fix h to zero and take a parametric form for K, we can effectively learn the common preferences h and the (non-stationary) covariance matrix K from the data.
This is described in the next section.One can assign hyperpriors to Θ and to (h, K).
For Θ the hyperprior depends on the likelihood model and should be i.i.d. for each θ j .
For the GP prior (h, K), we can assign a conjugate prior which takes a Normal-Inverse-Wishart distribution:P (h, K) = N (h; h 0 , 1 π K) IW(K; τ, K 0 ), where the parameters h 0 and K 0 are respectively the prior mean and base kernel, and π, τ correspond to the equivalent sample sizes before we observe any data ( Schwaighofer et al., 2005;Yu et al., 2005).
For the maximum a posteriori (MAP) estimate of h and K, they correspond to a smooth term in the learning process (cf. Section 3).
The proposed framework for ordinal regression is general and connected to many existing models.
One can define different likelihood P (y i |f (x i ), θ) to obtain different models, and all such models can be extended in a collaborative manner.
In this paper we illustrate two example models in detail, and briefly discuss other likelihood forms in Section 6.
Gaussian Process Regression (GPR): This model grants the ordinal regression problem simply as a GP regression problem.
The preference label y i is assumed to be sampled from a Gaussian with mean f (x i ) and variance σ 2 , i.e., (θ ≡ σ 2 )P (y i |f (x i ), θ) = N (y i ; f (x i ), σ 2 ).
The treatment is close to Rankprop (Caruana et al., 1996), which learns ranks by least squares.
This model is discussed in (Chu & Ghahramani, 2005a) and specifies boundary parameters b 0 < b 1 < . . . < b r for the r labels.
Given the latent function value f (x i ), the likelihood of a particular label y i is defined by how likely a corrupted valuez i ∼ N (z i ; f (x i ), σ 2 ) is in the corresponding interval (b yi−1 , b yi ): P (y i |f (x i ), θ) = by i by i −1 N (z i ; f (x i ), σ 2 ) dz i = Φ z + i − Φ z − i ,wherez + i = by i −f (xi) σ , z − i = by i −1−f (xi) σand Φ(t) = t −∞ N (z; 0, 1) dz.
We can define b 0 = −∞ and b r = +∞ without loss of generality, and the model takes parameters θ ≡ {b 1 , . . . , b r−1 , σ}.
Both of these two models can be extended to collaborative models.
In the following we call their collaborative versions as CGPR and CGPOR, respectively.
We derive a general learning scheme based on expectation propagation (EP) (Minka, 2001) for collaborative ordinal regression model, where in principal we can use any likelihood model for P (y i |f (x i ), θ).
We apply EP along with a variational method for parameter optimization, which has been applied for, e.g., GP classification (Seeger, 2002;Kim & Ghahramani, 2003).
In our setting, EP is applied for each function j and attempts to approximate the a posteriori distribution of f j , i.e. P (f j |y j , θ j , h, K), as a multivariate GaussianQ(f j ) = N (f j ; ˆ f j , ˆ K j ).
Note that here we use f j to denote the j-th function values on all the n items in X, because the EP algorithm is able to predict means and covariances for missing data, which in our case are the unlabeled items for function j.
This also means thatˆf thatˆthatˆf j is a length-n vector, andˆKandˆ andˆK j is a n × n matrix.The EP approximation can be done by taking a product form Q(f j ) = i∈Ij t i (f j (x i ))P (f j |h, K) with Require: A size-n item set, with features X ∈ R n×d and preference labels Y = {y 1 , . . . , y m } of m functions.
1: Initialize mean vector h, covariance matrix K and possible hyperparameters π, τ, h0, K0.
2: Initialize likelihood parameters θj for function j. for j = 1, . . . , m do 5:Obtain Q(f j ) = N (f j ; ˆ f j , ˆ Kj) by running EP until convergence.
6:Optimize parameter θj using (3).
end for 8:Update GP prior h and K using (4).
9: until the improvement is smaller than a threshold.t i (f j (x i )) = s i exp(− 1 2 p i (f j (x i ) − m i ) 2 ).
We then optimize parameters {s i , m i , p i } in {t i } successively by minimizing the Kullback-Leibler divergence,t new i = arg min ti KL Q(f j ) t old i P (y j (i)|f j (x i )) Q(f j ) t old i t i ,where y j (i) denote the observed label for item i in function j. (Minka, 2001) points out that we can do this approximation by moment matching, and efficient algorithms exist for our setting which do not require any matrix inversion (Rasmussen & Williams, 2006;Herbrich, 2005).
The model parameters Θ and (h, K) can be optimized by variational methods.
We apply Jensen's inequality to the likelihood (2) and obtainlog P (Y|X, Θ, h, K) = m j=1 log P (y j |X, θ j , h, K) ≥ m j=1 Q(f j ) log P (y j |f j , θ j )P (f j |h, K) Q(f j ) df j .
We can then maximize this lower bound with Q(f j ) fixed as N (f j ; ˆ f j , ˆ K j ) to get new model parameters.
It is seen that θ 1 , . . . , θ m and (h, K) are not coupled in this optimization problem.
For θ j we need to solveˆ θ j = arg min θj Q(f j ) log P (y j |f j , θ j ) df j(3)analytically or numerically.
The updates for (h, K) can be easily obtained asˆhasˆ asˆh = 1 π+m m j=1ˆf j=1ˆj=1ˆf j + πh 0 andˆ K = 1 τ + m π( ˆ h − h 0 )( ˆ h − h 0 ) + τ K 0 + m j=1 ( ˆ f j − ˆ h)( ˆ f j − ˆ h) + ˆ K j ,(4)which can be seen as averaged over the sufficient statistics of all functions and the hyperpriors.
Then we perform EP approximations again with the updated parameters, and the whole process is repeated until convergence.
Algorithm 1 illustrates the learning algorithm.The GP prior updates are the keys to connect all the individual ordinal regression functions and make them collaborative.
On one hand, one can model each task separately and optimize for each task the parameters of a pre-chosen kernel function.
But then we lose the collaborative effect among the tasks.
On the other hand, one can take a parametric kernel function and optimize the kernel parameters with regards to all the tasks, but then the model can be restrictive, because it is highly non-trivial to design a parametric kernel family that is sufficiently expressive.
In our solution, the (non-stationary) covariance matrix K is directly estimated from the repeated random samples (namely, functions in our case), which has sufficient flexibilities.
By assigning a prior to K we can also compromise the degrees of freedom and avoid overfitting.
In CGPR model the likelihood P (y j |f j , θ j ) already takes a Gaussian form, so the EP approximation is exact and turns out to beˆ f j = K n,j (K j,j + σ 2 I) −1 (y j − h j ) + h, ˆ K j = K − K n,j (K j,j + σ 2 I) −1 K n,j, where K n,j = K(:, I j ) denote the n × n j rectangle sub-matrix of K, and I is the identity matrix.
The parameter update can also be done analytically asˆ σ 2 j = 1 n j y j − ˆ f j 2 + tr[ ˆ K j ] ,where tr[·] denote matrix trace.
Given model parameters, the EP approximation for one function in CGPOR model is very similar to the EP solution in GPOR ( Chu & Ghahramani, 2005a), except that we allow unlabeled items to be considered and return an approximated Gaussian Q(f j ) which is defined on the function values of all items.
Then we update the variance and boundary parameters for each task using the same gradient descent methods as in GPOR, and instead of optimizing a kernel parameter we update the GP priors directly using (4).
Due to lack of space we do not include the gradient formula in this paper and refer interested readers to (Chu & Ghahramani, 2005a) for details.
The proposed model allows us to do two types of inference: label prediction of unlabeled items for each function, and label predictions for new functions.
Let us fix a function j and omit the subindex j for simplicity.
Given an unlabeled item x * , we want to infer the preference label y * .
We distinguish two scenarios based on the availability of x * before learning.
When the item is known before learning, i.e., x * ≡ x i ∈ X, we can include it in the learning procedure and obtain the a posteriori Gaussian Q(f * ) = N (f * ; µ * , σ 2 * ) from EP, where f * = f (x * ) = f (x i ), and µ * = ˆ f (i), σ 2 * = ˆ K(i, i) take the entries corresponding to item i from the mean and covariance of Q(f ).
Then the likelihood of a given label y isP (y|x * , D, θ, h, K) = P (y|f * , θ)Q(f * ) df * ,(5)where D = {X, Y} and P (y|f * , θ) depends on the likelihood model.
This is a one-dimensional integral and can be analytically calculated for CGPR and CGPOR models.
Finally the predicted label y * = arg max y P (y|x * , D, θ, h, K).
When x * is not in X, this is a new item and we cannot directly obtain its a posteriori distribution from EP.
The only information available is the vectork * = [κ(x * , x 1 ), . . . , κ(x * , x n )], which is defined via base kernel function κ.
We can solve this problem by optimizing in the dual space: Instead of taking a Gaussian form for Q(f j ) in EP, we take a Gaussian form forQ(α j ) where α j = K −1 f j .
Then after EP approxima- tion we obtain Q(α j ) = N (α j ; K −1 ˆ f j , K −1 ˆ K j K −1 )which is equivalent to Q(f j ).
Given the vector k * for new item x * , for function j we have the latent value f * = k * α j , which also takes a Gaussian formQ(f * ) = N (f * ; µ * , σ 2 * ) with µ * = k * K −1 ˆ f j and σ 2 * = k * K −1 ˆ K j K −1 k * .
Then (5) follows and we are able to predict the label for x * .
For more details on inductive inference for multi-task learning please refer to ( Yu et al., 2005).
In our model all the latent functions are sampled from the GP prior (h, K).
Therefore a new function will by default take h as the predicted mean preferences, and take K as the covariances of latent functions corresponding to every two items.
By using (5) we can already make default predictions, and after observing labels for some items we need to iteratively run EP and variational updates with fixed GP prior until convergence.In Figure 1 we show a toy problem for inference with GPR and CGPR models.
It is seen from (a) that the covariance is non-stationary such that all the functions sampled from this GP prior have a highly nonsmooth part in the middle (see (b)).
This corresponds to normal preference learning case where people tend to agree on very good (around 5) and very bad (around 1) items, but have large diversity for items in between.
Then when a new function comes with 4 labeled items (see (c)), model fitting simply using GPR will totally ignore this collaborative effect and end up with similar variances for all the test items.
CGPR, on the other hand, considers this prior in inference and thus obtains higher variances for test items in the middle (see (d)).
The predicted labels (red dashed line) are also more reasonable.
We evaluate the effectiveness of collaborative ordinal regression by comparing the two example models GPR and GPOR with their collaborative counterparts CGPR and CGPOR.
In GPR and CGPR, we initialize the model with σ 2 = 0.01 and estimate the mean preference of each function empirically from the sample mean.
For GPOR and CGPOR models, we take the same initializations as suggested in (Chu & Ghahra- mani, 2005a), i.e., σ = 1 and b k = −1 + (k−1)r 2 for k = 1, . . . , r − 1.
These model parameters are optimized using scaled conjugate gradient methods.We apply these models on two data sets MovieLens and EachMovie, both of which have a large number of users who gave discrete ratings on a set of movies.
It is known that there are strong collaborative effects among users, and each movie has its own features such as genre, directors and keywords.
Therefore, they are the ideal test beds for collaborative ordinal regression since we can take one movie as one item, and one user as one ordinal regression function.The MovieLens data we use consists of 100,000 ratings for 1682 movies from 943 users.
2 Ratings are made on a five-star scale, and we remove the movies with less than 50 ratings and have 591 movies left.
To build a feature vector for each movie, we use the "genres" part of the movie information which is a 19-dimensional binary-valued vector.
A 1 in the vector means the movie takes the corresponding genre.EachMovie contains 2,811,983 ratings entered by 72,916 users for 1628 movies.
The ratings are zeroto-five stars, and we change them to 1-to-6 scores to comply with our notations.
To show the model performance on different features, we write a script to download movie information from IMDB 3 including directors, genres, keywords, casts and languages.
We then extract all the words and build a 23,753-dimensional tf-idf vector for each movie.
After ignoring the notfound movies and the movies which are rated less than 50 times, we end up with 1075 movies.
In ordinal regression we usually consider the following two evaluation metrics given the target labels R and the predicted labelsˆRlabelsˆ labelsˆR: mean absolute error (MAE) which is the average deviation of the prediction from the target, i.e., MAE( ˆ R) = 1 to every incorrect prediction and then averages, asMZOE( ˆ R) = 1 t t i=1 1 ˆ R(i) =R(i) .
In the collaborative case, we need to average these errors over all the functions to give a single metric, but sometimes this is not straightforward since the number of test items for each function may be very different.
Therefore we propose the macro-averaged and micro-averaged versions of these metrics for collaborative ordinal regression.
Macro average is simply the algebraic average of the metrics over all the functions, and micro average is a weighted average with the weights given by the number of test items for each function.
We then have four metrics which we denote as MAE-Macro, MAE-Micro, MZOE-Macro and MZOE-Micro, respectively.Besides evaluating every individual label, we are also interested in the ranking quality of test items.
This is sometimes more informative since for a recommendation system the top-ranked items are considered more important than the bottom ones.
Here we use the normalized discounted cumulative gain (NDCG) (Jarvelin & Kekalainen, 2000) to evaluate a predicted ranking, which is calculated by summing over all the "gains" along the rank list with a log discount factor as NDCG( ˆ R) = Z k (2 r(k) − 1)/ log(1 + k), where r(k) denote the target label for the k-th ranked item inˆRinˆ inˆR, and Z is chosen such that a perfect ranking obtains value 1.
To focus more on the top-ranked items, we also consider the NDCG@10 which only counts the top 10 items in the rank list.
These two scores are averaged over all functions for comparison.
For label prediction of new items we fix 100 users with the most number of ratings as the ordinal regression functions.
Then for each user, we randomly pick up 10, 20 and 50 labeled items for training and test on the rest of labeled items.
The whole process is repeated 10 times independently.
We do not explicitly distinguish between transductive inference and inductive inference because we can handle both of them very well.
For GPR and GPOR, we train for each user separately and there is no information sharing among users.
For CGPR and CGPOR we apply the collaborative learning algorithm and let all the latent functions share the same GP prior.
The base kernel for both data sets are linear kernel κ(x s , x t ) = x s , x t , and the mean function is initialized as zero function.
Other hyperparameters are initialized as π = 1 and τ = 1, and they are not sensitive to the performance metrics.
For comparison we also show the results of maximum margin matrix factorization (MMMF) which is recently proposed for collaborative filtering (Rennie & Srebro, 2005).
MMMF directly optimizes a rank related cost function and is thus very related to collaborative ordinal regression.
However, it cannot use the low level features of the data and is purely based on the collaborative effects among users.
In MMMF we set the dimensionality to 100 and regularization factor to 10.
In Table 1 we show the results for the two data sets MovieLens and EachMovie.
It can be seen that both the collaborative models outperform the corresponding individual models, and the better performance is achieved for all the metrics.
This means that the quality of each ordinal regression task can be improved if we consider the collaborative effects and model all the tasks jointly.
The difference is especially big for MovieLens data, which probably indicates that MovieLens has stronger collaborative effects than EachMovie.GPR and GPOR show different behaviors on the two data sets.
In MovieLens GPOR is consistently better than GPR, but in EachMovie is worse except for mean zero-one errors.
While a theoretical analysis is still missing, we suspect this is related to the feature representation and kernel function.
For MovieLens we only use the "genres" for low level features, and thus for GP regression with linear kernel the performance is very poor.
GPOR, on the other hand, does a good job because it is modeling ranking more elegantly.
When we use the more informative term features for EachMovie, GPR is able to make reasonable predictions, but GPOR does not work very well because of the high dimensionality of features which makes it more likely to overfit.
GPOR is learning a pair of boundary values for each label, so when the training data is small the local minima problem occurs.MMMF does show a good performance on MovieLens, especially for the NDCG scores which are higher than all other methods.
This means by directly minimizing a rank related cost, MMMF can obtain very good ranking results.
On EachMovie the performance is similar to GPOR, which may also suffer from overfitting.
Figure 2.
Learning curves of GPR for label predictions of new users on MovieLens.
We show the means and standard deviations of MAE-Macro, MZOE-Macro, NDCG, and NDCG@10 over 20 trials from left to right.
We vary the number of training items for each test user as 10, 20, 50 and 100.
The pink dotted lines are training with base linear kernel (without collaborative effects), and other lines use the learned GP priors trained in CGPR models with the corresponding N users.
As discussed in Section 4.2, inference for new functions in collaborative ordinal regression simply means we fix the learned GP prior for the new latent functions.
So our goal is to investigate whether or not we can improve label predictions for new users using the new prior.
In this experiment we use GPR as the baseline, and train different CGPR models to get different GP priors for testing.
To show the collaborative effectiveness on new users, we vary the number of training users for CGPR models from 50 to 200, and test on the rest users.
For training in CGPR models we fix the number of labeled items to be 100.
In Figure 2 we show the learning curves of 4 metrics on MovieLens data set and omit others since the results are similar.
It can be seen that by using the learned priors, prediction performance of new users can be greatly improved.
With more training users in CGPR model training, better performance can be achieved.
This indicates a stronger collaborative effects for more users.
We have proposed a Bayesian framework for collaborative ordinal regression and empirically evaluated two models.
(C)GPOR is slow due to hard parameter optimization, so it is interesting to look for better likelihood models in future.
To improve scalability, one can assume a linear form for each f j , i.e., f j (x i ) = w j x i , and learn the Gaussian prior jointly for all w j 's.
Extending this framework to collaborative pairwise preference learning is also worth investigating.
