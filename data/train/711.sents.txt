The goal is to monitor multiple numerical streams, and determine which pairs are correlated with lags, as well as the value of each such lag.
Lag correlations (and anti-correlations) are frequent, and very interesting in practice: For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in the drinking water may lead to fewer dental cavities, some years later.
Additional settings include network analysis, sensor monitoring, financial data analysis, and moving object tracking.
Such data streams are often correlated (or anti-correlated), but with an unknown lag.
We propose BRAID, a method to detect lag correlations between data streams.
BRAID can handle data streams of semi-infinite length, incrementally, quickly, and with small resource consumption.
We also provide a theoretical analysis , which, based on Nyquist's sampling theorem, shows that BRAID can estimate lag correlations with little, and often with no error at all.
Our experiments on real and realistic data show that BRAID detects the correct lag perfectly most of the time (the largest relative error was about 1%); while it is up to 40,000 times faster than the naive implementation .
The processing and mining of data streams have attracted on increasing amount of interest recently.
Data streams ap- * This work was done while this author was visiting Carnegie Mellon University.
† This material is based upon work supported by the National Science Foundation under Grants No.IIS-0083148, IIS-0113089, IIS-0209107 IIS-0205224 INT-0318547 SENSOR-0329549 EF-0331657IIS-0326322 CNS-0433540 by the Pennsylvania Infrastructure Technology Alliance (PITA) Grant No. 22-901-0001.
Additional funding was provided by Intel and Northrop-Grumman Corporation.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, or other funding parties.pear in a variety of settings, such as environmental, medical and socioeconomic systems.
Typical data-stream applications include network analysis, sensor monitoring, financial data analysis, and moving object tracking.
In all these situations, the data sources generate data with no end in sight, making it impossible to store all the historical data.
Moreover, we want fast response times, in 'any time' fashion.There are many, fascinating research problems in such settings, as we survey later, like clustering [20], summarization [34], and forecasting [33,30].
Here we focus on a less-studied problem, namely on lag correlations.
Our goal is to monitor k numerical sequences, X1, . . ., X k , and to determine automatically all the pairs of sequences that have a lag correlation.
That is, we want to report all the pairs of sequences Xi and Xj, for which sequence Xi follows sequence Xj, with an unknown, arbitrary, lag l.
We also want BRAID to report the lag l for each such pair of sequences.Let's focus on two sequences first (k = 2), and then we generalize for more.
Without loss of generality, we can assume that the two sequences have the same length n, by zero-padding the shortest.
Then, the sub-problem we want to solve is as follows:Problem 1 (Pair-wise Lag Correlation).
Given two co-evolving sequences of equal length n= 1, 2, . . ., determine, at any point of time, two things: (a) whether there is a lag correlation between them, and (b) if yes, what is the lag length l.The full problem we want to solve is as follows:Problem 2 (Group Lag Correlation).
Given k coevolving sequences of equal length n, determine, at any point of time, which pairs have a lag correlation, and report all such pairs, as well as the corresponding lags.Intuitively, two sequences have a lag correlation of l if they look very similar when one is delayed by l time-ticks.
The formal definition of the lag correlation is given in Definition 1 (Section 3.1).
Figure 1 (a) illustrates two lag-correlated sequences X and Y , with lag l=1300 time-ticks.
is called the cross-correlation function (CCF) in time-series literatures.
We will give the exact definitions for all these symbols and terms later.We continue the discussion focusing on two streams, for simplicity.
If the sequences X and Y were static, the problem would be trivial: simply compute the CCF (for lag l=0, 1, 2, . . .) and report the lag l, for which the CCF is maximized.
However, when X and Y continuously increase in length, the problem is challenging.
We need a method which will monitor X and Y , and whenever the user wants, the method should determine whether there is a lag correlation, and if yes, the value l of the lag.
Specifically, we need a method that has the following characteristics:• 'Any-time' processing, and fast.
The computation time per time tick should be sub-linear (and, ideally, constant) on the length n of the sequences.
• Nimble: The memory space requirement should also be sub-linear on the length n.• Accurate: Given that the exact results require too much space and time, we need approximations.
Such approximation should introduce small error.We propose BRAID, a lag capture method for two or more data streams.
To our knowledge, BRAID is the first to possess all the above characteristics.
The idea is to use careful approximations, exploiting the Nyquist sampling theorem.
The net effect is that BRAID has dramatically better performance in terms of speed and memory, while it maintains excellent accuracy.
Our experiments on real and realistic streams show that BRAID is up to 40,000 times faster than the straightforward lag computation, while maintaining relative error of 1% or less.
The rest of the paper is organized as follows.
Section 2 gives related work on data streams and stream mining.
Section 3 describes our method, BRAID.
We show how lags of data streams can be captured.
Section 4 presents an enhanced algorithm for better accuracy.
Section 5 gives our theoretical analysis for BRAID.
Section 6 reviews the results of the experiments, which clearly show the effectiveness of BRAID.
Section 7 is a brief conclusion.
Time-series and sensor networks have been attracting much interest recently.
Although there are numerous publications in these research topics, we have not seen any method for that can automatically determine arbitrary lag correlations.
We provide a survey of the related literatures.
For indexing of time-series data, Agrawal et al. [2] proposed to use the DFT (Discrete Fourier Transform) to extract features for indexing, with R*-trees as the underlying spatial access method.
Follow-up work examined several related problems, including subsequence matching [13], the Adaptive Piecewise Constant Approximation (APCA) [24], indexing for Dynamic Time Warping (DTW) [23], similarity query processing for multi-dimensional data streams [26].
Feature extraction, summarization and lossy compression are closely related, with powerful tools like wavelets [18,19] and random projections [11,15].
Remotely related is also the work on data stream management systems (DSMSs).
Sample systems include Aurora [1], Stream [29], Telegraph [7] and Gigascope [9] OSCAR [8].
Algorithmic work includes query processing [28], scheduling [3,6], load shedding [10,31].
Guha et al. [20] have proposed an algorithm that solves the k-median problem for data streams in a single pass.
Domingos et al. [12] have presented an algorithm for constructing decision trees.
The work in [22,31] focuses on streams with concept drifting.
Zhu et al. [35] study burst detection in streams.
AWSOM [30] is one of the first streaming methods for forecasting, in a single time sequence.
Multiple streams have also attracted significant interest.
Ganti et al. [16] propose a generic framework for streaming mining.
Zhu et al. [34] focus on monitoring multiple streams in real time.
They use the "short window" Fourier Transform to summarize streams, and then compute all the pairwise correlations.
However, the method will clearly miss any lag correlation that is longer than the window w of the short-window Fourier Transform.
MUSCLES [33] applies multi-variate linear regression on multiple co-evolving time sequences; moreover, it expects the user to specify a constant, upper limit m on the longest possible lag that it will consider (the default is m=5).
In conclusion, none of the above methods satisfies the specifications that we listed in the introduction.
A data stream X is a discrete sequence of numbers { x1, . . ., xt, . . ., xn }, where xn is the most recent value.
Notice that n increases with every new time-tick.
The definition of the correlation coefficient R(0) between two time sequences X and Y of equal length n and zero lag, is a traditional one, known as Pearson's ρ coefficient:ρ = R(0) = P t ((xt − ¯ x) * (yt − ¯ y)) σ(x) * σ(y)(1)where ¯ x, ¯ y denote the mean of X and Y , respectively.
For lag l (l ≥ 0), we consider only the common part of X and the shifted Y ; that is, only n − l time ticks, and the equation becomesR(l) = P n t=l+1 (xt − ¯ x)(y t−l − ¯ y) q P n t=l+1 (xt − ¯ x) 2 q P n−l t=1 (yt − ¯ y) 2(2)¯ x = 1 n − l n X t=l+1 xt, ¯ y = 1 n − l n−l X t=1 ytwhere R(l) denotes the correlation coefficient, when X is delayed by l. Notice that we can easily handle the symmetric case; that is, when Y is the one delayed.We are interested in high absolute values of R(l).
Thus, we call score at lag l the absolute value of R(l), that isscore(l) = |R(l)|(3)Definition 1 (Lag Correlation).
Two sequences X and Y have a lag correlation of l, and specifically, that X lags Y by lag l, if 1.
the score (=absolute value of the correlation coefficient ) between xt and y t−l is above a threshold γ, say γ = 0.4, and is actually a local maximum.2.
and this is the earliest such maximum, if more maxima exist.The first requirement is straightforward.
For the second requirement, the reasoning is more subtle: if the two sequences are periodic with the same period T , (which is often the case with real sequences -say, daily, or yearly periodicities), there will be many local maxima (l, l+T , l+2 * T , . . .).
Clearly, the earliest of these lags is the most important.
Another subtle point from time series analysis is that the estimate R(l) for large values of the lag l ≈ n are undesirable, because the original and shifted time sequences will have too few overlapping time-ticks.
Thus, following recommendations from the time series analysis [4], we restrict the maximum lag m to be n/2.
Notice that even with this restriction, the maximum considered lag m increases with time, since we made it a function of the sequences' length.
This changing nature of m creates subtle, but hard problems for all the earlier methods that we mentioned in the survey.
As we mentioned earlier, if we had infinite space and time, the problem would be trivial:Naive Solution.
At time n, we would access all the values of X and Y , we would compute the CCF, that is, the R(l) for all values of the lag l (=0,1, . . .), and we would choose the earliest maximum score above γ, or report that there is no lag correlation.Our solution is based on the three major steps, each described next.Observation 1.
The correlation coefficient R() is an algebraic measure [21].
That is, it can be computed incrementally.More specifically, all we need is some sufficient statistics (namely, sums, sum of squares, sum products); then, R can be easily computed.
Let Sx(1, n) be the sum of X of length n (i.e., Sx(1, n) = P n t=1 xt), and Sxx(1, n) be the sum of the squares of X (i.e., Sxx(1, n) = P n t=1 x 2 t ).
Sxy(l) means the inner-product for X and the shifted Y :Sxy(l) = n X t=l+1 xty t−l (4)We shall refer to all these values collectively as sufficient statistics .
Given our sufficient statistics, the correlation coefficient R(l) is obtained by:R(l) = C(l) p V x(l + 1, n) · V y(1, n − l)(5)where C(l) is the covariance of X and Y :C(l) = Sxy(l) − Sx(l + 1, n) · Sy(1, n − l) n − land V x(l + 1, n) means the variance of the subsequence of X, starting from t = l + 1:V x(l + 1, n) = Sxx(l + 1, n) − (Sx(l + 1, n)) 2 n − l (6)The variance V y(1, n − l) of Y is computed similarly.
In conclusion, for the given value of lag l, we only need to keep track of five numbers, the sufficient statistics, because they are enough to help us estimate the correlation R(l), at any point of time.
Although important, this observation is not enough to have a streaming method: It still needs linear time to compute the cross-correlation function (CCF) between the two given sequences X and Y .
To reduce the lag-estimation time, we introduce an approximation:Observation 2 (Geometric Probing).
We compute (probe) the CCF at values of the lag l that form a geometric progression.
Thus, we need only O(log n) numbers to estimate the CCF.Specifically, instead of computing R(l) for every possible value of the lag l, we propose to keep track of only a geometric progression of the lag values: l= 0,1, 2, 4, . . ., 2 i , . . ..
The justification is that it achieves a dramatic reduction in computation time, since we need only O(log n) numbers to keep track of, instead of O(n) that the "Naive Solution" requires.
As we show later (see Theorem 4), this approximation introduces little error, and occasionally zero error.
The intuition is that our method will give good accuracy for small l, exactly because for small l's we have many points to interpolate; it may give a larger error for large lag l, but the relative error will probably be small.There is only one remaining problem: The space required grows linearly with the length n.
The reason is subtle: in order to have the ability to compute the correlation coefficient R(l) at any time t (t = n, n + 1, . . .) we need to keep a sliding window of size l.
Since l grows geometrically up to m = n/2, eventually we need O(n) space.
(a-1) (b-1) (c-1) (a-2) (b-2) (c-2) We propose to solve the above problem with our third and final idea, an approximation: Instead of operating on the original time sequences, we also compute their smoothed version, by computing the means of non-overlapping windows.
The window widths will be powers of g=2, although any other number would also be acceptable.
Let X be the original time sequence, and Ax h be its smoothed version with windows (window average) of length 2 h .
That is, Ax0 is the original sequence; Ax1 consists of n/2 ticks, with the pair means; Ax2 has n/4 ticks, with the quadruplet means, and so on.
At time n, we need O(log n) levels; for each level, we compute the sufficient statistics.
Ax h and its sufficient statistics need to be computed every 2 h time ticks.
On average, we require O(1) time to update the sufficient statistics since P log m h=0 1/2 h ≈ 2.
Not only 'smoothing' saves space, but we can further prove that it results into a small or even zero error, under certain assumptions.
Formally we approximate: R(l) ≈ ˆ R1(l/2), and in generalR(l) ≈ ˆ R h (l/2 h )( 7 )wherê R h () is the correlation coefficient of the h-level smoothed sequences, andˆR0andˆ andˆR0() ≡ R().
Lemma 1 in Section 5.1.1 gives a theoretical justification.
As we mentioned, the correlation R(l) of Equation (5) can be calculated incrementally.
Moreover, we operate on increasingly coarser (smoother) representations of the sequences, by using window averages, with increasing window size.
Figure 2 (b-1) shows an example of the window average.
We compute the average of data points falling within a window, and organize all the windows hierarchically.
The window size increases as the level of the hierarchy becomes higher.BRAID is a lag capture method for data streams.
To obtain a smoother curve to find the local maximum of the CCF, we can use an off-the-shelf interpolation method.
We chose cubic splines [14], but the choice of interpolation method is orthogonal to BRAID.We give some examples to explain how BRAID approximates the correlation coefficients and captures the lag for which the sequences are correlated.
compute R 0 (0, t); delete R 0 (0, t − 1); // Compute the correlation coefficient for the level h for h = 0 to 񮽙log m񮽙 do if t mod 2 h = 0 then compute R h (1, t h ); delete R h (1, t h − 1); else break; endif endfor // Fit splines on the graph R h (l/2 h ) vs l // The series of l is a geometric progression // l = {0,1,2,4,. . . ,2 h ,. . . ,2 񮽙log m񮽙 } ˆ R h := Spline(R 0 (0), R 0 (1/2 0 ),R 1 (2/2 1 ),. . . ,R h (l/2 h ),. . . );extract the optimal lag fromˆRfromˆ fromˆR h (if any); are computed from the window averages for higher levels.
Thus, we use a cubic spline to interpolate the missing correlation coefficients between the approximated coefficients.
It effectively estimates that the correlation coefficients vary between these lags.
Finally, we can see the lag correlation (if any) from the local maximum of the cubic spline curve (solid line in Figure 2 (c-2)).
Before introducing our algorithm, we give some definitions.
Let Ax h (t) be the window average at time tick t for level h. Ax h (t) is computed as:Ax h (t) = Ax h−1 (2t − 1) + Ax h−1 (2t) 2(8)t = 1, . . . , n/2 hwith Ax0(t) ≡ xt.
We are now able to define the "sufficient statistics" for the window averages.
The sum of Ax h and the sum of the squares on Ax h at t are obtained as:Sx h (1, t) = Sx h (1, t − 1) + Ax h (t) Sxx h (1, t) = Sxx h (1, t − 1) + (Ax h (t)) 2(9)The inner-product of Ax h and Ay h for lag l at t is incrementally updated every 2 h time ticks.Sxy h (l, t) = Sxy h (l, t − 1) + Ax h (t) · Ay h (t − l)(10)By using the obtained sufficient statistics, we can derive the correlation coefficient R h (l, t) as:R h (l, t) = C h (l, t) p V x h (l + 1, t) · V y h (1, t − l)(11)Algorithm SumKeeping(X) // Compute sum and sum square computeSx 0 (1, t), Sx 0 (2, t), Sxx 0 (1, t), Sxx 0 (2, t); delete Sx 0 (1, t − 2), Sx 0 (2, t − 1), Sxx 0 (1, t − 2), Sxx 0 (2, t − 1); for h = 1 to 񮽙log n񮽙 do if t mod 2 h = 0 then // Compute window average for h compute Ax h (t h ); delete Ax h−1 (t h−1 − 3), Ax h−1 (t h−1 − 2);// Compute sums and sum squares for h computeSx h (1, t h ), Sx h (2, t h ), Sxx h (1, t h ), Sxx h (2, t h ); delete Sx h (1, t h − 2), Sx h (2, t h − 1), Sxx h (1, t h − 2), Sxx h (2, t h − 1); else break; endif endfor Algorithm ProductKeeping(X, Y )// Compute inner-products compute Sxy 0 (0, t), Sxy 0 (1, t); delete Sxy 0 (0, t − 1), Sxy 0 (1, t − 1); for h = 1 to 񮽙log n񮽙 do if t mod 2 h = 0 then // Compute inner products for h compute Sxy h (1, t h ); delete Sxy h (1, t h − 1); else break; endif endfor whereC h (l, t) = Sxy h (l, t) − Sx h (l + 1, t) · Sy h (1, t − l) t − l V x h (l + 1, t) = Sxx h (l + 1, t) − (Sx h (l + 1, t)) 2 t − lThe basic algorithm for incremental computations is shown in Figures 3 and 4.
In these figures, t h denotes the time tick for level h (i.e., t h = t/2 h ).
For each incoming data point, we first incrementally update the window averages and their sufficient statistics for every level (See Figure 4).
We then approximate the correlation coefficient of the two sequences at l, R(l, t), from their window averages (See Figure 3).
By using multiple window sizes, we can compute the correlation coefficients for only a geometric progression of lags l:l = {0, 1, 2, 4, 8, . . . , 2 h , . . . , 2 񮽙log m񮽙 }The missing correlation coefficients caused by the approximation are estimated by interpolation with a cubic spline.
After interpolation, we can use any known method to find the local maxima -we chose Brent's method [5].
If a high enough local maximum exists, we report the corresponding lag.
In Figures 3 and 4, the number of window averages that BRAID computes for each level, is 1.
In fact, as described, BRAID is extremely nimble: for example, for two sequences of size ≈ 2 20 (1 million long each), it requires about 5 * log 2 20 =5*20 = 100 float numbers, which is about 800 bytes -way less than 1KB!
The question is what can we do in the highly likely case that larger memory is available.
The proposed solution is to enhance our probing scheme, so that we can probe in many more places (= lags), while still using O(log n) space.To make our probing denser, one idea would be to have windows of powers of g, where g 񮽙 = 2.
A simpler idea, that we propose and implemented, is the use a mix of arithmetic plus geometric probing.
So far, BRAID uses only one window at each smoothing level (0, 1, . . . , h).
We propose to use b > 1 such windows, say, b = 4 instead (See Figure 5).
The algorithm shown in Section 3.3.1 keeps one number for each level, that is b = 1, with one exception, namely that the bottom level has 2b coefficients.
While computing the correlation coefficients at l, we end up sampling the CCF in a mixture of geometric and arithmetic progressions: In this section we give a theoretical analysis to show the accuracy and complexity of BRAID.
Again, we focus on two sequences X and Y .
To simplify the discussion, and without loss of generality, we assume that the given sequences X and Y are normalized to zero mean and unit standard deviation (i.e., ¯ x = ¯ y = 0 and σ(x) = σ(y) = 1).
Our upcoming experiments show that we can closely estimate lag correlations despite our two approximations, the geometric probing and the smoothing.
It turns out that this can be explained.
In fact, if X and Y are "smooth" enough, then the errors introduced by smoothing and by probing are small.
In fact, the probing error can even be zero, if the input sequences each has a Nyquist frequency.
Next we elaborate on each type of error.
Informally, the intuition is the following: Observation 4.
For sequences with low frequencies, smoothing introduces only small error.Let xt be the values of the original sequence X at time t (t = 1, . . . , n), andˆxtandˆ andˆxt h be the smooth version of xt (t h = 񮽙t/2 h 񮽙).
Given the Haar wavelet coefficients of X, wi (i = 1, . . . , n), then the error in the approximation of Equation (11) depends on the energy in the high frequencies.n X t=1 (xt − ˆ xt h ) 2 = X i>n/2 h w 2 i(12)Since very few of the wavelet coefficients of real data sets are often significant and a majority are small [18], Equation (12) shows that the error is limited to a very small value for most of practical data.Theorem 1.
Let R be the correlation coefficient between sequences X and Y , thenR = 2 h X t h ˆ xt h ˆ yt h + ΔC (13)Algorithm SumKeeping(X) // Compute sum and sum square compute Sx 0 (1, t), Sxx 0 (1, t); wheredelete Sx 0 (1, t − 2b), Sxx 0 (1, t − 2b); for i = 2 to 2b do compute Sx 0 (i, t), Sxx 0 (i, t); delete Sx 0 (i, t − 1), Sxx 0 (i, t(1, t h ), Sxx h (1, t h ); delete Sx h (1, t h − 2b), Sxx h (1, t h − 2b); for i = b + 1 to 2b do compute Sx h (i, t h ), Sxx h (i, t h ); delete Sx h (i, t h − 1), Sxx h (i, tΔC = X t (ΔxtˆytΔxtˆ Δxtˆyt h + ΔytˆxtΔytˆ Δytˆxt h + ΔxtΔyt) Δxt = xt − ˆ xt hProof.
Since xt = ˆ xt h + Δxt, the covariance isC = X t (ˆ xt h ˆ yt h + ΔxtˆytΔxtˆ Δxtˆyt h + ΔytˆxtΔytˆ Δytˆxt h + ΔxtΔyt)If X and Y are normalized, σ(x) = σ(y) = 1.
Thus, we have R = P t xtyt σ(x) * σ(y) = 2 h X t h ˆ xt h ˆ yt h + ΔCR ≈ R h = P t h ˆ xt h ˆ yt h ˆ σ(x) * ˆ σ(y)(14)Proof.
By Equation (12), we can see xt >> Δxt, then R >> ΔC, and σ 2 (x) ≈ 2 h ˆ σ 2 (x).
Therefore, we have R ≈ R h .
The second source of error is the fact that BRAID probes a small subset of the actual values of the lag l.
It turns out that, for smooth enough sequences, this introduces little, or even no error.Observation 5.
Let fX and fY be the Nyquist frequencies of X and Y , and let l be the lag length for X and Y .
Then BRAID will find the lag correlations perfectly, if 0 ≤ l < lR where lR = 2b fR fR = min(fX , fY ) BRAID uses coarse sequences to find lag correlations efficiently.
To avoid missing lag correlations, we also need a reliable criterion upon which to check the accuracy.
Before showing the criterion, we need to describe some theorems.Theorem 2 (Sampling Theorem).
If a continuous function contains no frequencies higher than f high , it is completely determined by its value at a series of points less than 1/2f high apart.Proof.
See [27].
In the theorem, the minimum sampling frequency, fNq = 2f high , is called the Nyquist frequency.We next show the relationship between CCF and the Fourier transform, known as the cross-correlation theorem.Theorem 3 (Cross-Correlation Theorem).
Let F be the Fourier transform, and R(l) be the CCF of X and Y , then we haveF R = FX F * Y (15)where '*' denotes the complex conjugate.
Proof.
See [27].
Based on Theorems 2 and 3, we can derive the following theorem.Theorem 4.
Let fX and fY be the Nyquist frequencies of X and Y , and let fR = min(fX , fY ).
Then R(l) is perfectly reconstructed from its samples taken uniformly if the sampling (i.e., 'probing') frequency is at least fR.Proof.
For X and Y , we haveFX (f ) = 0 (f > fX /2) FY (f ) = 0 (f > fY /2) then FR(f ) = 0 (f > fR/2, fR = min(fX , fY ))Therefore, fR is the Nyquist frequency of R(l), i.e., R(l) can be reconstructed perfectly if the sampling frequency is fR.Since probing by BRAID is not uniform, we need a new lemma.
As always, we assume that the stream rate is one measurement per time unit.Lemma 2.
Given b coefficients for each level, BRAID will have no error in the estimation of a lag correlation of l if2 h ≤ 1 fR (16)where h is the level that covers the lag correlation of l:h = j 񮽙log 2 l b 񮽙 (l ≥ b) 0 ( l < b)(17)Proof.
Theorem 4 shows that no information is lost if a signal is sampled at fR.
Therefore, R(l) can be determined if the interval, 2 h , is less than or equal to 1/fR.
Lemma 3.
BRAID spots lag correlations of 0 ≤ l < lR:lR = 2b fR (18)Proof.
Lag correlations can be detected if the following condition is satisfied:l < 2b & 1 ≤ 1/fR, or l < 4b & 2 ≤ 1/fR, · · · or l < 2 h+1 b & 2 h ≤ 1/fR.
R(l) can be determined if 0 ≤ l < 2 h+1 b ≤ 2b/fR.
In this section, we discuss the complexity of BRAID and show that BRAID can efficiently estimate lag correlations.Let m be the maximum lag we want to capture.
Suppose that m is proportional to the sequence length n.Lemma 4.
For every pair of sequences, the "Naive Solution" requires space O(n) and time O(n).
Proof.
When xn and yn arrive, it computes the correlation coefficient R(l, n) for each lag l (l = 0, 1, . . . , m).
To compute R(l, n), it keeps yt (t=n − m,. . . ,n) and xn.
Thus, it needs to store 2m + 3 values and update the m + 1 values.
Since m is proportional to n, it requires space O(n) and time O(n).
Lemma 5.
For each pair of sequences, the proposed BRAID requires O(log n) space.Proof.
BRAID needs to maintain the (log m+2) correlation coefficients since the bottom level has double the number of coefficients.
To compute the correlation coefficients, BRAID keeps the log m + 1 values for X and (log m + 2) values for Y .
This requires space O(log n).
While BRAID needs to maintain sufficient statistics, they are the same complexity.
Therefore, the space complexity of BRAID is O(log n).
Lemma 6.
The proposed BRAID requires O(1) amortized time per time-tick for updating sufficient statistics.Proof.
The window average needs to be computed every 2 h time ticks (h=1,. . . ,log m).
On average, BRAID computes one value for each incoming data point, because P log m h=1 1/2 h ≈ 1.
Similarly, the sufficient statistics can be updated in O(1) time.
Therefore, the amortized time complexity of BRAID is O(1).
When output is required, BRAID employs interpolation to closely estimate lag correlations.Lemma 7.
The proposed BRAID requires O(log n) time for interpolating.Proof.
The cubic spline for the geometric probing requires O(log n) time.
See [14].
BRAID can retain more than one correlation coefficient for each level, that is b ≥ 1.
Since b is a small constant (i.e., b 񮽙 n), the space required is still O(log n), and the time complexity is O(1).
To evaluate the effectiveness of BRAID, we performed experiments on real and synthetic datasets.
We compared BRAID with the naive implementation.
As mentioned in Section 4, we can have "enhanced BRAID", where b > 1.
That is, we keep more than one window-average for each level.
We performed our experiments with b = 16, on an Intel Xeon 2.8GHz with 1GB of memory, running Linux.The experiments were designed to answer the following questions:1.
How well does BRAID estimate the correlation coefficients for periodic and/or bursty datasets?
2.
How successful is it in spotting lag correlations?
3.
How does it scale with the sequence lengths n in terms of the computation time?
We performed experiments on the following real and synthetic data sets.
For synthetic data sets, we used the following:• Sines: the data set consists of two sequences of length n = 32, 768.
Each sequence is a mixture of sine waves of different frequencies.
We chose this setting because it resembles real data, such as network and automobile traffic (daily, weekly, and yearly periodicities), product sales (umbrellas, flu medicine -yearly periodicities), etc.• SpikeTrains: a pair of periodic pulse trains with white noise.
The period of these sequences is 6500, and the length is 100,000.
This data set is also realistic: for example, disk access traffic is bursty [32], with daily periodicity; the famous sunspot dataset has spikes, with a 9-11 year periodicity [30].
For Real data sets, we used the following:• Humidity, Light, Temperature: humidity, illuminance, and temperature readings, from 55 sensors within several buildings.
Each sensor gives a reading every 30 seconds.
We chose two sequences for each of the data sets, Humidity and Light, and used all sequences for Temperature.
• Kursk: Seismic recordings from multiple sensors, showing the explosion of the Russian submarine "Kursk" [25].
Each sequence has a single burst.
We extracted two subsequences of length n=70,000.
• Sunspots: Number of sunspots per day.
The dataset has a period of approximately 11 years 1 .
We chose two intervals from the dataset, each length n=25,900, and treated them as if they were two different time sequences.
Figures 8 and 9 show the estimation of BRAID for all data sets.
In these figures, "Naive" denotes the exact correlation coefficients computed by the naive implementation.
"Approximation" means the correlation coefficients computed from the window averages.
BRAID interpolates the missing values between these correlation coefficients.
1 http://csep10.phys.utk.edu/astr162/lect/sun/sscycle.html Table 2 shows the estimation error of the captured lag correlations.
As shown in Definition 1, the most important lag correlation is the earliest local maximum.
The experiments clearly demonstrate that BRAID detects the correct lag perfectly, most of the time.
The largest relative error was about 1%.
We theoretically discussed the complexity of BRAID in Section 5.2.
However, BRAID needs not only the computation of correlation coefficients, but also overheads including the ones for the interpolation and extraction of the lag correlation from the spline curve.
Therefore, we did an empirical study of the computation time.
Figure 10 compares BRAID and the naive implementation in terms of wall clock time under varying sequence lengths n.
We used Sines for this experiment.
The wall clock time is the average of the processing time to update sufficient statistics for each time tick and detect the lag correlations.For stream data, since the sequence length continues to grow, the computation time also increases.
Instead of the O(n) that the naive implementation requires, BRAID can achieve a dramatic reduction in computation time.
This trend shown in the figure corresponds to our theoretical discussion in Section 5.2.
Theoretically, BRAID requires time O(1) for updating the sufficient statistics; the computation time does not depend on n.
In this experiment, the wall clock time increases slightly as n grows.
This increase is caused by interpolation, because we need to interpolate through a larger, O(log n) number of points.
Specifically, BRAID is up to about 40,000 times faster than the naive implementation.
In Section 5.1.2 we provided that says when exactly BRAID can estimate the lag without error.
(see Lemma 3) In this section, we will show examples of the criterion and discuss the effect of probing by exploiting Lemma 3 to reinforce our theoretical analysis.
Figure 11 (a) shows the power spectrum of Sines in the frequency domain.
Note that Sines #1 and #2 have the same power spectrum.
The power spectrum is computed from the normalized sequences.
That is, P F 2 = 1 and F 2 (f0) = 0.
Intuitively, f on the y-axis means the first f Fourier coefficients.
We can observe that in this figure, the energy is distributed in the range of 0 ≤ f ≤ 32.
Thus, the Nyquist frequency is fR = 64/n.
According to Lemma 3, we have lR = 1024b.
Figure 11 (b) presents an estimation of BRAID for Sines.
We used one coefficient (i.e., b = 1) for each level in Figure 11 (b).
We saw how BRAID can closely estimate the lag correlation with b = 1, because the lag is less than lR = 1024.
We next discuss the criterion for real data.
Real data often include high frequencies of very small energy.
The Nyquist frequency could be extremely high for such data.
Since the frequency limit is widely understood in various areas (e.g., audio processing, network analysis, and electrical engineering), we will focus on the frequency components that are larger than a very small value, say 񮽙.
We set 񮽙 = 0.01 in this experiment.Figures 12 (a) and (b) present the power spectra of Light, and Figure 12 (c) shows the estimation for b = 1.
The power of the frequency components larger than 񮽙 is distributed in the frequency range of 0 ≤ f ≤ 52.
Therefore, we have lR ≈ 630.
As shown in Figure 12 (c), we can spot the lag correlation, because the local maximum is near lR.In Lemma 3, we expect the frequency of each sequence to obtain lR.
Incremental algorithms have been proposed to compute the frequency in the streaming sense (e.g., [17]).
BRAID can utilize any and all of these solutions to compute the frequency efficiently.
However, this research topic is beyond the scope of this paper.
We applied BRAID to detecting group lag correlations We have introduced the problem of automatic lag correlation detection on streaming data and proposed BRAID to address this problem, using careful approximations and smoothing.
The resulting method has all the desired characteristics:1.
It is 'any-time': it can give a response at any time, pinpointing the lag if it exists, or declaring that there is no lag correlation.
2.
Low resource consumption: it needs O(log n) space, and O(1) time to update the statistics.
3.
High accuracy: it detects the correct lag within 1% relative error or lessOur experiments on real and realistic data show that BRAID works as expected, estimating the unknown lags with excellent accuracy and high speed.
Specifically, BRAID can be up to 40,000 times faster than the naive implementation while the largest relative error was 1%.
