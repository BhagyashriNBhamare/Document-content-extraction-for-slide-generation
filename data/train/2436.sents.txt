Solid-state drives (SSDs) are used in a wide array of computer systems today, including in datacenters and enterprise servers.
As the I/O demands of these systems continue to increase, manufacturers are evolving SSD ar-chitectures to keep up with this demand.
For example, manufacturers have introduced new high-bandwidth interfaces to replace the conventional SATA host-interface protocol.
These new interfaces, such as the NVMe protocol , are designed specifically to enable the high amounts of concurrent I/O bandwidth that SSDs are capable of delivering.
While modern SSDs with sophisticated features such as the NVMe protocol are already on the market, existing SSD simulation tools have fallen behind, as they do not capture these new features.
We find that state-of-the-art SSD simulators have three shortcomings that prevent them from accurately modeling the performance of real off-the-shelf SSDs.
First, these simulators do not model critical features of new protocols (e.g., NVMe), such as their use of multiple application-level queues for requests and the elimination of OS intervention for I/O request processing.
Second, these simulators often do not accurately capture the impact of advanced SSD maintenance algorithms (e.g., garbage collection), as they do not properly or quickly emulate steady-state conditions that can significantly change the behavior of these algorithms in real SSDs.
Third, these simulators do not capture the full end-to-end latency of I/O requests, which can incorrectly skew the results reported for SSDs that make use of emerging non-volatile memory technologies.
By not accurately modeling these three features, existing sim-ulators report results that deviate significantly from real SSD performance.
In this work, we introduce a new simulator, called MQSim, that accurately models the performance of both modern SSDs and conventional SATA-based SSDs.
MQSim faithfully models new high-bandwidth protocol implementations, steady-state SSD conditions, and the full end-to-end latency of requests in modern SSDs.
We validate MQSim, showing that it reports performance results that are only 6%-18% apart from the measured actual performance of four real state-of-the-art SSDs.
We show that by modeling critical features of modern SSDs, MQSim uncovers several real and important issues that were not captured by existing simulators, such as the performance impact of inter-flow interference.
We have released MQSim as an open-source tool, and we hope that it can enable researchers to explore directions in new and different areas.
Solid-state drives (SSDs) are widely used in today's computer systems.
Due to their high throughput, low response time, and decreasing cost, SSDs have replaced traditional magnetic hard disk drives (HDDs) in many datacenters and enterprise servers, as well as in consumer devices.
As the I/O demand of both enterprise and consumer applications continues to grow, SSD architectures are rapidly evolving to deliver improved performance.For example, a major innovation has been the introduction of new host interfaces to the SSD.
In the past, many SSDs made use of the Serial Advanced Technology Attachment (SATA) protocol [67], which was originally designed for HDDs.
Over time, SATA has proven to be inefficient for SSDs, as it cannot enable the fast I/O accesses and millions of I/O operations per second (IOPS) that contemporary SSDs are capable of delivering.
New protocols such as NVMe [63] overcome these barriers as they are designed specifically for the high throughput available in SSDs.
NVMe enables high throughput and low latency for I/O requests through its use of the multi-queue SSD (MQ-SSD) concept.
While SATA exposes only a single request port to the OS, MQ-SSD protocols provide multiple request queues to directly expose applications to the SSD device controller.
This allows (1) an application to bypass OS intervention for I/O request processing, and (2) the SSD controller to schedule I/O requests based on how busy the SSD's resources are.
As a result, the SSD can make higher-performance I/O request scheduling decisions.As SSDs and their associated protocols evolve to keep pace with changing system demands, the research community needs simulation tools that reliably model these new features.
Unfortunately, state-of-the-art SSD simulators do not model a number of key properties of modern SSDs that are already on the market.
We evaluate several real modern SSDs, and find that state-of-the-art simulators do not capture three features that are critical to accurately model modern SSD behavior.First, these simulators do not correctly model the multi-queue approach used in modern SSD protocols.
Instead, they implement only the single-queue approach used in HDD-based protocols such as SATA.
As a result, existing simulators do not capture (1) the high amount of request-level parallelism and (2) the lack of OS intervention in modern SSDs.Second, many simulators do not adequately model steady-state behavior within a reasonable amount of simulation time.
A number of fundamental SSD maintenance algorithms, such as garbage collection [11- 13, 23], are not executed when an SSD is new (i.e., no data has been written to the drive).
As a result, manufacturers design these maintenance algorithms to work best when an SSD reaches the steady-state operating point (i.e., after all of the pages within the SSD have been written to at least once) [71].
However, simulators that cannot capture steady-state behavior (within a reasonable simulation time) perform these maintenance algorithms on a new SSD.
As such, many existing simulators do not adequately capture algorithm behavior under realistic conditions, and often report unrealistic SSD performance results (as we discuss in Section 3.2).
Third, these simulators do not capture the full end-toend latency of performing I/O requests.
Existing simulators capture only the part of the request latency that takes place during intra-SSD operations.
However, many emerging high-speed non-volatile memories greatly reduce the latency of intra-SSD operations, and, thus, the uncaptured parts of the latency now make up a significant portion of the overall request latency.
For example, in Intel Optane SSDs, which make use of 3D XPoint memory [9,25], the overhead of processing a request and transferring data over the system I/O bus (e.g., PCIe) is much higher than the memory access latency [16].
By not capturing the full end-to-end latency, existing simulators do not report the true performance of SSDs with new and emerging memory technologies.Based on our evaluation of real modern SSDs, we find that these three features are essential for a simulator to capture.
Because existing simulators do not model these features adequately, their results deviate significantly from the performance of real SSDs.
Our goal in this work is to develop a new SSD simulator that can faithfully model the features and performance of both modern multi-queue SSDs and conventional SATA-based SSDs.To this end, we introduce MQSim, a new simulator that provides an accurate and flexible framework for evaluating SSDs.
MQSim addresses the three shortcomings we found in existing simulators, by (1) providing detailed models of both conventional (e.g., SATA) and modern (e.g., NVMe) host interfaces; (2) accurately and quickly modeling steady-state SSD behavior; and (3) measuring the full end-to-end latency of a request, from the time an application enqueues a request to the time the request response arrives at the host.
To allow MQSim to adapt easily to future SSD developments, we employ a modular design for the simulator.
Our modular approach allows users to easily modify the implementation of a single component (e.g., I/O scheduler, address mapping) without the need to change other parts of the simulator.
We provide two execution modes for MQSim: (1) standalone execution, and (2) integrated execution with the gem5 full-system simulator [8].
We validate the performance reported by MQSim using several real SSDs.
We find that the response time results reported by MQSim are very close to the response times of the real SSDs, with an average (maximum) error of only 11% (18%) for real storage workload traces.By faithfully modeling the major features found in modern SSDs, MQSim can uncover several issues that existing simulators are unable to demonstrate.
One such issue is the performance impact of inter-flow interference in modern MQ-SSDs.
For two or more concurrent flows (i.e., streams of I/O requests from multiple applications), there are three major sources of interference: (1) the write cache, (2) the mapping table, and (3) the I/O scheduler.
Using MQSim, we find that inter-flow interference leads to significant unfairness (i.e., the interference slows down each flow unequally) in modern SSDs.
This is a major concern, as fairness is a first-class design goal in modern computing platforms [4, 17, 19, 31, 37, 56- 60, 66, 73-76, 80, 84, 88].
Unfairness reduces the predictability of the I/O latency and throughput for each flow, and can allow a malicious flow to deny or delay I/O service to other, benign flows.We have made MQSim available as an open source tool to the research community [1].
We hope that MQSim enables researchers to explore directions in several new and different areas.We make the following key contributions in this work: • We use real off-the-shelf SSDs to show that stateof-the-art SSD simulators do not adequately capture three important properties of modern SSDs: (1) the multi-queue model used by modern host-interface protocols such as NVMe, (2) steady-state SSD behavior, and (3) the end-to-end I/O request latency.
• We introduce MQSim, a simulator that accurately models both modern NVMe-based and conventional SATA-based SSDs.
To our knowledge, MQSim is the first publicly-available SSD simulator to faithfully model the NVMe protocol.
We validate the results reported by MQSim against several real state-of-the-art multi-queue SSDs.
• We demonstrate how MQSim can uncover important issues in modern SSDs that existing simulators cannot capture, such as the impact of inter-flow interference on fairness and system performance.
In this section, we provide a brief background on multiqueue SSD (MQ-SSD) devices.
First, we discuss the internal organization of an MQ-SSD (Section 2.1).
Next, we discuss host-interface protocols commonly used by SSDs (Section 2.2).
Finally, we discuss how the SSD flash translation layer (FTL) handles requests and performs maintenance tasks (Section 2.3).
Modern MQ-SSDs are typically built using NAND flash memory chips.
NAND flash memory [11,12] supports read and write operations at the granularity of a flash page (typically 4 kB).
Inside the NAND flash chips, multiple pages are grouped together into a flash block, which is the granularity at which erase operations take place.
Flash writes can take place only to pages that are erased (i.e., free).
To minimize the write latency, MQ-SSDs perform out-of-place updates (i.e., when a logical page is updated, its data is written to a different, free physical page, and the logical-to-physical mapping is updated).
This avoids the need to erase the old physical page during a write operation.
Instead, the old page is marked as invalid, and a garbage collection procedure [11][12][13]23] reclaims invalid physical pages in the background.
Figure 1 shows the internal organization of an MQ-SSD.
The components inside the MQ-SSD are divided into two groups: (1) the back end, which includes the memory devices; and (2) the front end, which includes the control and management units.
The memory devices (e.g., NAND flash memory [11,12], phase-change Plane1 Multiplexed Interface Multiplexed Interface Bus Interface Bus Interface Multiplexed Interface Bus Interface 1 2 3 3 Figure 1: Organization of an MQ-SSD.
As highlighted in the figure ( 1 , 2 , 3 ), our MQSim simulator captures several aspects of MQ-SSDs not modeled by existing simulators.memory [42], STT-MRAM [40], 3D XPoint [9]) in the back end are organized in a highly-hierarchical manner to maximize I/O concurrency.
The back end contains multiple independent bus channels, which connect the memory devices to the front end.
Each channel connects to one or more memory chips.
For a NAND flash memory based SSD, each NAND flash chip is typically divided into multiple dies, where each die can independently execute memory commands.
All of the dies within a chip share a common communication interface.
Each die is made up of one or more planes, which are arrays of flash cells.
Each plane contains multiple blocks.
Multiple planes within a single die can execute memory operations in parallel only if each plane is executing the same command on the same address offset within the plane.In an MQ-SSD, the front end includes three major components [47].
(1) The host-interface logic (HIL) implements the protocol used to communicate with the host (Section 2.2).
(2) The flash translation layer (FTL) manages flash resources and processes I/O requests (Section 2.3).
(3) The flash chip controllers (FCCs) send commands to and transfer data to/from the memory chips in the back end.
The front end contains on-board DRAM, which is used by the three components to cache application data and store data structures for flash management.
The HIL plays a critical role in leveraging the internal parallelism of the NAND flash memory to provide higher I/O performance to the host.
The SATA protocol [67] is commonly used for conventional SSDs, due to widespread support for SATA on enterprise and client systems.
SATA employs Native Command Queuing (NCQ), which allows the SSD to concurrently execute I/O requests.
NCQ allows the SSD to schedule multiple I/O requests based on which back end resources are currently idle [29,50].
The NVM Express (NVMe) protocol [63] was designed to alleviate the bottlenecks of SATA [90], and to enable scalable, high-bandwidth, and low-latency communication over the PCIe bus.
When an application issues an I/O request in NVMe, it bypasses the I/O stack in the OS and the block layer queue, and instead directly inserts the request into a submission queue (SQ in Fig- ure 1) dedicated to the application.
The SSD then selects a request from the SQ, performs the request, and inserts the request's job completion information (e.g., ack, read data) into the request completion queue (CQ) for the corresponding application.
NVMe has already been widely adopted in modern SSD products [30,64,79,85,86].
The FTL executes on a microprocessor within the SSD, performing I/O requests and flash management procedures [11,12].
Handling an I/O request in the FTL requires four steps for an SSD using NVMe.
First, when the HIL selects a request from the SQ, it inserts the request into a device-level queue.
Second, the HIL breaks the request down into multiple flash transactions, where each transaction is at the granularity of a single page.
Next, the FTL checks if the request is a write.
If it is, and the MQ-SSD supports write caching, the write cache management unit stores the data for each transaction in the write cache space within DRAM, and asks the HIL to prepare a response.
Otherwise, the FTL translates the logical page address (LPA) of the transaction into a physical page address (PPA), and enqueues the transaction into the corresponding chip-level queue.
There are separate queues for reads (RDQ) and for writes (WRQ).
The transaction scheduling unit (TSU) resolves resource contention among the pending transactions in the chip-level queue, and sends transactions that can be performed to its corresponding FCC [20,78].
Finally, when all transactions for a request finish, the FTL asks the HIL to prepare a response, which is then delivered to the host.The address translation module of the FTL plays a key role in implementing out-of-place updates.
When a transaction writes to an LPA, a page allocation scheme assigns the LPA to a free PPA.
The LPA-to-PPA mapping is recorded in a mapping table, which is stored within the non-volatile memory and cached in DRAM (to reduce the latency of mapping lookups) [24].
When a transaction reads from an LPA, the module searches for the LPA's mapping and retrieves the PPA.The FTL is also responsible for memory wearout management (i.e., wear-leveling) and garbage collection (GC) [11][12][13]23].
GC is triggered when the number of free pages drops below a threshold.
The GC procedure reclaims invalidated pages, by selecting a candidate block with a high number of invalid pages, moving any valid pages in the block into a free block, and then erasing the candidate block.
Any read and write transactions generated during GC are inserted into dedicated read (GC-RDQ) and write (GC-WRQ) queues.
This allows the transaction scheduling unit to schedule GC-related requests during idle periods.
Modern MQ-SSDsIn this section, we compare the capabilities of state-ofthe-art SSD simulators to the common features of the modern SSD devices.
As shown in Figure 1, we identify three significant features of modern SSDs that are not supported by current simulation tools: 1 multi-queue support, 2 fast modeling of steady-state behavior, and 3 proper modeling of the end-to-end request latency.
While some of these features are also present in some conventional SSDs, their lack of support in existing simulators is more critical when we evaluate modern and emerging MQ-SSDs, resulting in large deviations between simulation results and measured performance.
A fundamental difference of a modern MQ-SSD from a conventional SSD is its use of multiple queues that directly expose the device controller to applications [90].
For conventional SSDs, the OS I/O scheduler coordinates concurrent accesses to the storage devices and ensures fairness for co-running applications [66,68].
MQSSDs eliminate the OS I/O scheduler, and are themselves responsible for fairly servicing I/O requests from concurrently-running applications and guaranteeing high responsiveness.
Exposing application-level queues to the storage device enables the use of many optimized management techniques in the MQ-SSD controller, which can provide high performance and a high level of both fairness and responsiveness.
This is mainly due to the fact that the device controller can make better scheduling decisions than the OS, as the device controller knows the current status of the SSD's internal resources.
We investigate how the performance of a flow 1 changes when the flow is concurrently executed with other flows on real MQ-SSDs.
We conduct a set of experiments where we control the intensity of synthetic workloads that run on four new off-the-shelf MQ-SSDs released between 2016 and 2017 (see Table 4 and Appendix A).
In each experiment, there are two flows, is the response time of f i when it runs alone.
Fairness (F) is calculated as [22,56,58]:F = MIN i {S f i } MAX i {S f i } (1)According to the above definition: 0 < F ≤ 1.
Lower F values indicate higher differences between the minimum and maximum slowdowns of all concurrently-running flows, which we say is more unfair to the flow that is slowed down the most.
Higher F values are desirable.
Figure 2 depicts the slowdown, normalized throughput (IOPS), and fairness results when we execute Flow-1 and Flow-2 concurrently on our four target MQ-SSDs (which we call SSD-A, SSD-B, SSD-C, and SSD-D).
The x-axes in all of the plots in Figure 2 represent the queue depth (i.e., the flow intensity) of Flow-2 in the experiments.
For each SSD, we show three plots from left to right: (1) the slowdown and normalized throughput of Flow-1, (2) the slowdown and normalized throughput of Flow-2, and (3) fairness.
We make four major observations from Figure 2.
First, in SSD-A, SSD-B, and SSD-C, the throughput of Flow-2 substantially increases proportionately with the queue depth.
Aside from the maximum bandwidth available from the SSD, there is no limit on the throughput of each I/O flow.
Second, Flow-1 is slowed down significantly due to interference from Flow-2 when the I/O queue depth of Flow-2 is much greater than that of Flow-1.
Third, for SSD-A, SSD-B, and SSD-C, the slowdown of Flow-2 becomes almost negligible (i.e., its value approaches 1) as the intensity of Flow-2 increases.
Fourth, SSD-D limits the maximum throughput of each flow, and thus the negative impact of Flow-2 on the performance of Flow-1 is well controlled.
Further experiments with a higher number of flows reveal that one flow cannot exploit more than a quarter of the full I/O bandwidth of SSD-D, indicating that SSD-D has some level of internal fairness control.
In contrast, one flow can unfairly exploit the full I/O capabilities of the other three SSDs.We conclude that (1) the relative intensity of each flow significantly impacts the throughput delivered to each flow; and (2) MQ-SSDs with fairness controls, such as SSD-D, perform differently from MQ-SSDs without fairness controls when the relative intensities of concurrently-running flows differ.
Thus, to accurately model the performance of MQ-SSDs, an SSD simulator needs to model multiple queues and enable multiple concurrently-running flows.
SSD performance evaluation standards explicitly clarify that the SSD performance should be reported in the steady state [71].
2 As a consequence, pre-conditioning (i.e., quickly reaching steady state) is an essential requirement for SSD device performance evaluation, in order to ensure that the results are collected in the steady state.
This policy is important for three reasons.
First, the garbage collection (GC) activities are invoked only when the device has performed a certain number of writes, which causes the number of free pages in the SSD to drop below the GC threshold.
GC activities interfere with user I/O activity and can significantly affect the sustained device performance.
However, a fresh out-ofthe-box (FOB) device is unlikely to execute GC.
Hence, performance results on an FOB device are unrealistic as they would not account for GC [71].
Second, the steadystate benefits of the write cache may be lower than the short-term benefits, particularly for write-heavy workloads.
More precisely, in the steady state, the write cache is filled with application data and warmed up, and it is highly likely that no free slot can be allocated to new write requests.
This leads to cache evictions and increased flash write traffic in the back end [33].
Third, the physical data placement of currently-running applications is highly dependent on the device usage history and the data placement of previous processes.
For example, which physical pages are currently free in the SSD depends on how previous I/O requests wrote to and in-validated physical pages.
As a result, channel-and chiplevel parallelism in SSDs is limited in the steady state.Although a number of works do successfully precondition and simulate steady-state behavior, many previous studies have not explored the effect of steady-state behavior on their proposals.
Instead, their simulations start with an FOB SSD, and never reach steady state (e.g., when each physical page of the SSD has been written to at least once).
Most well-known storage traces are not large enough to fill the entire storage space of a modern SSD.
Figure 3 shows the total write volume of popular storage workloads [6,[53][54][55]61].
We observe that most of the workloads have a total write volume that is much smaller than the storage capacity of most SSDs, with an average write volume of 60 GB.
Even for the few workloads that are large enough to fill the SSD, it is time consuming for many existing simulators to simulate each I/O request and reach steady state (see Section 5).
Therefore, it is crucial to have a simulator that enables efficient and high-performance steady-state simulation of SSDs.
Request latency is a critical factor of MQ-SSD performance, since it affects how long an application stalls on an I/O request.
The end-to-end latency of an I/O request, from the time it is inserted into the host submission queue to the time the response is sent back from the MQ-SSD device to the completion queue, includes seven different parts, as we show in Figure 4.
Existing simulation tools model only some parts of the end-to-end latency, which are usually considered to be the dominant parts of the end-to-end latency [3,26,27,35,38].
Figure 4a illustrates the end-to-end latency diagram for a small 4 kB read request in a typical NAND flashbased MQ-SSD.
It includes I/O job enqueuing in the submission queue (SQ) 1 , host-to-device I/O job transfer over the PCIe bus 2 , address translation and transaction scheduling in the FTL 3 , read command and address transfer to the flash chip 4 , flash chip read 5 , read data transfer over the Open NAND Flash Interface (ONFI) [65] bus 6 , and device-to-host read data transfer over the PCIe bus 7 .
Steps 5 and 6 are assumed to be the most time-consuming parts in the end-to-end request processing.
Considering typical latency values for an 8 kB page read operation, the I/O job insertion (< 1 µs, as measured on our real SSDs), the FTL request processing on a multicore processor (1 µs) [47] quest processing may not always be negligible, and can even become comparable to the flash read access time.
For example, prior work [26] shows that if the FTL uses page-level address mapping, then a workload without locality incurs a large number of misses in the cached mapping table (CMT).
In case of a miss in the CMT, the user read operation stalls until the mapping data is read from the SSD back end and transferred to the front end [24].
This can lead to a substantial increase in the latency of Step 3 in Figure 4a, which can become even longer than the combined latency of Steps 5 and 6 .
In an MQ-SSD, as a greater number of I/O flows execute concurrently, there is more contention for the CMT, leading to a larger number of CMT misses.Second, as shown in Figure 4b, cutting-edge nonvolatile memory technologies, such as 3D XPoint [7,9,16,48], dramatically reduce the access and data transfer times of the MQ-SSD back end, by as much as three orders of magnitude compared to that of NAND flash [25,40,42,43].
The total latency of the 3D XPoint read and transfer (< 1 µs) contributes less than 10% to the end-to-end I/O request processing latency (<10 µs) [7,16].
In this case, a conventional simulation tool would be inaccurate, as it does not model the major steps contributing to the end-to-end latency.In summary, a detailed, realistic model of end-to-end latency is key for accurate simulation of modern SSD devices with (1) multiple I/O flows that can potentially lead to a significant increase in CMT (cached mapping table) misses, and (2) very-fast NVM technologies such as 3D XPoint that greatly reduce raw memory read/write latencies.
Existing simulation tools do not provide accurate performance results for such devices.
To our knowledge, there is no SSD modeling tool that supports multi-queue I/O execution, fast and efficient modeling of the SSD's steady-state behavior, and a full end-to-end request latency estimation.
In this work, we present MQSim, a new simulation framework that is developed from scratch to support all of these three important features that are required for accurate performance modeling and design space exploration of modern MQ-SSDs.
Although mainly designed for MQ-SSD simulation, MQSim also supports simulation of the conventional SATA-based SSDs that implement native command queuing (NCQ).
Our new simulator models all of the components shown in Figure 1, which exist in modern SSDs.
Table 1 provides a quick comparison between MQSim and previous SSD simulators.
MQSim is a discrete-event simulator written in C++ and is released under the permissive MIT License [1].
MQSim provides a simple yet detailed model of the flash memory chips.
It considers three major latency components of the SSD back end: (1) address and command transfer to the memory chip; (2) flash memory read/ write execution for different technologies that store 1, 2, or 3 bits per cell [32]; and (3) data transfer to/from memory chips.
MQSim's flash model considers the constraints of die-and plane-level parallelism, and advanced command execution [65].
One important new feature of MQSim is that it can be configured or easily modified to simulate new NVM chips (e.g., those that do not need erase-before-write).
Due to decoupling of the NVM chip communication interface from the chip's internal implementation of the memory operations, one can modify the NVM chip of MQSim without the need to change the implementation of the other MQSim components.Another new feature of MQSim is that it decouples the sizes of read and write operations.
This feature helps to exploit large page sizes of modern flash memory chips in that can enable better write performance, while preventing the negative effects of large page sizes on read performance.
For flash chip writes, the operation is always page-sized [11,12].
MQSim's data cache controller can delay writes to eliminate write-back of partially-updated logical pages (where the update size is smaller than the physical page size).
When a partially-updated logical page should be written back to the flash storage, the unchanged sub-pages (sectors) of the logical page are first read from the physical page that stores page data.
Then, unchanged and updated pieces of the page are merged.
In the last step, the entire page data is written to a new free physical page.
For flash chip reads, the operation could be smaller than the physical page size.
When a read operation finishes, only the data pieces that are requested in the I/O request are transferred from flash chips to the SSD controller, avoiding the data transfer overhead of large physical pages.
The front end model of MQSim includes all of the basic components of a modern SSD controller and provides many new features that do not exist in previous SSD modeling tools.
The host interface component of MQSim provides both NVMe multi-queue (MQ) and SATA native command queue models for a modern SSD.
To our knowledge, MQSim is the first modeling tool that supports MQ I/O request processing.
There is a request fetch unit within the host interface of MQSim that fetches and schedules application I/O requests from different input queues.
The NVMe host interface provides users with a parameter, called QueueFetchSize, that can be used to tune the behavior of the request fetch unit, in order to accurately model the behavior of real MQ-SSDs.
This parameter defines the maximum number of I/O requests from each SQ that can be concurrently serviced in the MQ-SSD.
More precisely, at any given time, the number of I/O requests that are fetched from a host SQ to the device-level queue is always less than or equal to QueueFetchSize.
This parameter has a large impact on the MQ-SSD multiflow request processing characteristics discussed in Section 3.1 (i.e., on maximum achievable throughput per I/O flow and probability of inter-flow interference).
Appendix A.3 analyzes the effect of this parameter on performance.MQSim also models different priority classes for hostside request queues, which are part of the NVMe standard specification [63].
MQSim has a data cache manager component that implements a DRAM-based cache with the least-recentlyused (LRU) replacement policy.
The DRAM cache can be configured to cache (1) recently-written data (default mode), (2) recently-read data, or (3) both recentlywritten and recently-read data.
A new feature of MQSim's cache manager, compared to previous SSD modeling tools, is that it implements a DRAM access model in which the contention among the concurrent accesses to DRAM chips and the latency of DRAM commands are considered.
The DRAM cache models in MQSim can be extended to make use of detailed and fast DRAM simulators, such as Ramulator [2,39], to perform detailed studies of the effect of DRAM cache performance on the overall MQ-SSD performance.
We leave this to future work.
MQSim implements all the main FTL components, including (1) the address translation unit, (2) the garbage collection (GC) and wear-leveling (WL) unit, and (3) the transaction scheduling unit.
MQSim provides different options for each of these components, including state-ofthe-art address translation strategies [24,78], GC candidate block selection algorithms [10,18,23,45,81,91], and transaction scheduling schemes [34,87].
MQSim also implements several state-of-the-art GC and flash management mechanisms, including preemptible GC I/O scheduling [44], intra-plane data movement from one physical page to another physical page using copyback read and write command pairs [27], and program/erase suspension [87] to reduce the interference of GC operations with application I/O requests.
One novel feature of MQSim is that all of its FTL components support multi-flow (i.e., multi-input queue) request processing.
For example, the address mapping unit can partition the cached mapping table space among the concurrently running flows.
This inherent support of multi-queueaware request processing facilitates the design space exploration of performance isolation and QoS schemes for MQ-SSDs.
In addition to the flash operation and internal data transfer latency (steps 3 , 4 , 5 , and 6 in Figure 4), there is a mix of variable and constant latencies that MQSim models to determine the end-to-end request latency.
Variable Latencies.
These are the variable request processing times in FTL that result from contention in the cached mapping table and the DRAM write cache.
Depending on the request type (either read or write) and the request's logical address, the request processing time in FTL includes some of the following items: (1) the time required to read/write from/to the data cache, and (2) the time to fetch mapping data from flash storage in case of a miss in the cached address mapping table.
Constant Latencies.
These include the times required to transmit the I/O job information, the entire user data, and the I/O completion information over the PCIe bus, and the firmware (FTL) execution time on the controller's microprocessor.
The PCIe transmission latencies are calculated based on a simple packet latency model provided by Xilinx [41] that considers: (1) the PCIe communication bandwidth, (2) the payload and header sizes of the PCIe Transaction Layer Packets (TLP), (3) the size of the NVMe management data structures, and d) the size of the application data.
The firmware execution time is estimated using both a CPU and cache latency model [1].
The basic assumption of MQSim is that all simulations should be executed when the modeled device is in steady state.
To model the steady-state behavior, MQSim, by default, automatically executes a preconditioning function before starting the actual simulation process.
This function performs preconditioning in a short time (e.g., less than 8 min when running tpcc [53] on an 800 GB MQ-SSD) without the need to execute additional I/O requests.
During preconditioning, all available physical pages of the modeled SSD are transitioned to either a valid or invalid state, based on the steady-state valid/invalid page distribution model provided in [82] (only very few flash blocks are assumed to remain free and are added to the free block pool).
MQSim pre-processes the input trace to extract the LPA (logical page address) access characteristics of the application I/O requests in the trace, and then uses the extracted information as inputs to the valid/invalid page distribution model.
In addition, input trace characteristics, such as the average write arrival rate and the distribution of write addresses, are used to warm up the write cache.
MQSim provides two modes of operation: (i) standalone mode, where it is fed a real disk trace or a synthetic workload, and (ii) integrated mode, where it is fed disk requests from an execution-driven engine (e.g., gem5 [8]).
The increasing usage of SSDs in modern computing systems has boosted interest in SSD design space exploration.
To this end, several simulators have been developed in recent years.
Table 2 summarizes the features of MQSim and popular existing SSD modeling tools.
The table also shows the average error rates for the performance of real storage workloads reported by each simulator, compared to the performance measured on four real MQ-SSDs (see Appendix A.1 for our methodology).
Existing tools either do not model some major components of modern SSDs or provide very simplistic component models that lead to unrealistic I/O request latency estimation.
In contrast, MQSim provides detailed implementations for all of the major components of modern SSDs.
MQSim is written in C++ and has 13K lines of code (LOC).
Next, we discuss the main advantages of MQSim compared to the previous tools.
Host-Interface Logic.
As Table 2 shows, most of the existing simulators assume a very simplistic HIL model with no explicit management mechanism for the I/O request queue.
This leads to an unrealistic SSD model regarding the requirements of both NVMe and SATA protocols.
As we mention in Section 3, the concurrent execution of I/O flows presents many challenges for performance predictability and fairness in MQ-SSDs.
No ex- 13 Built-in support for multi-queue-aware request processing in FTL 14 Lines of source code isting simulator implements NVMe and multi-queue I/O request management, and, hence, accurately models the behavior of MQ-SSDs.
Also, except for WiscSim, we find that no existing simulator implements an accurate model of the SATA protocol and NCQ request processing.
This leads to unrealistic SATA device simulation, as NCQ-based I/O scheduling plays a key role in the performance of real SSD devices [15,26].
Steady-State Simulation.
To our knowledge, accurate and fast steady-state behavior modeling is not provided by many existing SSD modeling tools.
Among the tools listed in Table 2, only SSDSim provides a function, called make aged, to change the status of a set of physical pages to valid before starting the actual execution of an input trace.
This simple method cannot accurately replicate the steady-state behavior of an SSD for two reasons.
First, after the execution of make aged, the physical blocks would include only valid pages or only free pages.
This is far from the steady-state status of blocks in real devices, where each non-free block has a mix of valid and invalid pages [28,81,82].
Second, the steadystate status of the data cache is not modeled, i.e., the simulation starts with a completely empty write cache.In general, it is possible to bring these simulators to steady state.
However, there is no fast pre-conditioning support for them, and pre-conditioning must be performed by executing traces.
Preconditioning an existing simulator requires users to generate traces with a large enough number of I/O requests, and can significantly slow down the simulator, especially when a highcapacity SSD is modeled.
For example, our studies with SSDSim show that pre-conditioning may increase the simulation time up to 80x if an 800 GB SSD is modeled.
3 Detailed End-to-End Latency Model.
As described in Section 3.3, the end-to-end latency of an application I/O request includes different components.
Table 2 shows that latency modeling in existing simulators is mainly focused on the latency of the flash chip operation and the SSD internal data transfer.
As we explain in Section 3.3, this is an unrealistic model of the end-to-end I/O request processing latency, even for a conventional SSD.To study the accuracy of the existing tools in modeling real devices, we create four models for the four real SSDs shown in Table 4 in each simulator, and execute three real traces, i.e., tpcc, tpce, and exchange.
We exclude the simulators that do not support trace-based execution.
The four rightmost columns of Table 2 show the average error rate of each simulator in modeling the performance (i.e., read and write latency) of these four real devices.
The error rates of the four evaluated simulators are almost one order of magnitude higher than that of MQSim.
We believe that these high error rates are due to four major reasons: (1) the lack of write cache or inaccurate modeling of the write cache access latency, (2) the lack of built-in support for steady-state modeling, (3) incomplete modeling of the request processing latency in FTL, and (4) the lack of modeling of the host-to-device communication latency.
MQSim is a flexible simulation tool that enables different studies on both modern and conventional SSD devices.
In this section, we discuss two new research directions enabled by MQSim, which could not be explored easily using existing simulation tools.
First, we use MQSim to perform a detailed analysis of inter-flow interference in a modern MQ-SSD (Section 6.1).
We explain how sharing different internal resources in an MQ-SSD, such as the write cache, cached mapping table, and back end resources, can introduce fairness issues.
Second, we explain how the full-system simulation mode of MQSim can enable detailed application-level studies (Section 6.2).
As we describe in Section 1, fairness and QoS should be considered as first-class design criteria for modern datacenter SSDs.
MQSim provides an accurate framework to study inter-flow interference, thus enables the ability to devise interference-aware MQ-SSD management algorithms for sharing of the internal MQ-SSD resources.As we show in Section 3.1, concurrently running two I/O flows might lead to disproportionate slowdowns for each flow, greatly degrading fairness and proportional progress.
This is particularly important in high-end SSD devices, which provide higher throughput per I/O flow, as we show in Appendix A.3.
We find that this inter-flow interference is mainly the result of contention that takes place at three locations in an MQ-SSD: 1) the write cache in the front end, 2) the cached mapping table (CMT) in the front end, and 3) the storage resources in the back end.
In this section, we use MQSim to explore the impact of these three points of contention on performance and fairness, which cannot be explored accurately using existing simulators.6.1.1 Methodology MQ-SSD Configuration.
Table 3 lists the specification of the MQ-SSD that we model in MQSim for our contention studies.
Metrics.
To measure performance, we use weighted speedup (WS) [70] of the average response time (RT), which represents the overall efficiency and system-level throughput [21] provided by an MQ-SSD during the concurrent execution of multiple flows:WS = ∑ i RT alone i RT shared i(2)where RT alone i and RT shared i are defined in Section 3.1.
To demonstrate the effect of inter-flow interference on fairness, we report slowdown and fairness (F) metrics, as defined in Section 3.1.
One point of contention among concurrently-running flows in an MQ-SSD is the write cache.
For flows with low to moderate write intensity (where the average depth of the I/O queue less than 16), or with high spatial locality, the write cache decreases the response time of write requests, by avoiding the need for the requests to wait for the write to complete to the underlying memory.
For flows with high write intensity or with highlyrandom accesses, the write requests fill up the limited capacity of the write cache quickly, causing significant cache thrashing and limiting the decrease in write request response time.
Such flows not only do not benefit from the write cache themselves, but also prevent other lower-write-intensity flows from benefiting from the write cache, leading to a large performance loss for the lower-write-intensity flows.To understand how the contention at the write cache affects system performance and fairness, we perform a set of experiments where we run two flows, Flow-1 and Flow-2, both of which perform only random-access write requests.
In both flows, the average request size is set to 8 kB.
We set Flow-1 to have a moderate write intensity, by limiting the queue depth to 8 requests.
We vary the queue depth of Flow-2 from 8 requests to 256 requests, to control the write intensity of the flow.
In order to isolate the effect of write cache interference in our experiments, we (1) assign each flow to a dedicated subset of back end resources (i.e., Flow-1 uses Channels 1-4, and Flow-2 uses Channels 5-8), to avoid introducing any interference in the back end; and (2) use a perfect CMT, where all address translation requests are hits, to avoid interference due to limited CMT capacity.
Figure 6a shows the slowdown of each flow when the two flows run concurrently, compared to when each flow runs alone.
Figure 6b shows the fairness and performance of the system when the two flows run concurrently.
We make four key observations from the figures.
First, Flow-1 is slowed down significantly when Flow-2 has a high write intensity (i.e., its queue depth is greater than 16), indicating that at high write intensities, Flow-2 induces write cache trashing.
Second, the slowdown of Flow-2 is negligible, because of the low write intensity of Flow-1.
Third, fairness degrades greatly, as a result of the write cache contention, when Flow-2 has a high write intensity.
Fourth, write cache contention causes an MQ-SSD to be inefficient at concurrently running multiple I/O flows, as the weighted speedup is reduced by over 50% when Flow-2 has a high write intensity compared to when it has a low write intensity.
(b) Fairness (left) and system performance (right) Figure 6: Impact of write cache contention.We conclude that write cache contention leads to unfairness and overall performance degradation for concurrently-running flows when one flow has a high write intensity.
In these cases, the high-write-intensity flow (1) does not benefit from the write cache; and (2) prevents other, lower-write-intensity flows from taking advantage of the write cache, even though the other flows would otherwise benefit from the cache.
This motivates the need for fair write cache management algorithms for MQ-SSDs that take inter-flow interference and flow write intensity into account.
As we discuss in Section 3.3, address translation can noticeably increase the end-to-end latency of an I/O request, especially for read requests.
We find that for I/O flows with random access patterns, the cached mapping table (CMT) miss rate is high due to poor reuse of address translation mappings, which causes the I/O requests generated by the flow to stall for long periods of time during address translation.
This is not true for I/O flows with sequential accesses, for which the CMT miss rate remains low due to spatial locality.
However, when two I/O flows run concurrently, where one flow has a random access pattern and another flow has a sequential access pattern, the poor locality of the flow with the random access pattern may cause both flows to have high CMT miss rates.To understand how contention at the CMT affects system performance and fairness, we perform a set of experiments where we concurrently run two flows that issue read requests with an average request size of 8 kB.
In these experiments, Flow-1 has a fully-sequential access pattern, and Flow-2 has a random access pattern for a fraction of the total execution time, and has a sequential access pattern for the remaining time.
We vary the randomness (i.e., the fraction of the execution time with a random access pattern) of Flow-2.
To isolate the effects of CMT contention, we assign Flow-1 to Channels 1-4 in the back end, and assign Flow-2 to Channels 5-8.
Figure 7a shows the slowdown and change in CMT hit rate of each flow when Flow-1 and Flow-2 run concur-rently, compared to when each flow runs alone.
Figure 7b shows the fairness and overall performance of the system when the two flows run concurrently.
We make two observations from the figures.
First, as the randomness of Flow-2 increases, the CMT hit rate of Flow-1 decreases, while the CMT hit rate of Flow-2 remains constant.
This indicates that the randomness of Flow-2 introduces contention at the CMT, which hurts the CMT hit ratio of Flow-1.
Second, as the CMT hit rate of Flow-1 decreases, the flow experiences a greater slowdown, with a 2.1x slowdown when Flow-2's access pattern is completely random.
Third, as the randomness of Flow-2 increases, both fairness and overall system performance decrease, as the interference introduced by Flow-2 hurts the performance of Flow-1 without providing any noticeable benefit to Flow-2.
(a) Slowdown and CMT hit rate (normalized to the hit rate when Flow-2 randomness is 0%) for Flow-1 (left) and Flow-2 (right) We conclude that the CMT contention induced by an I/O flow with a random access pattern disproportionately slows down concurrently-running flows with sequential access patterns, which would otherwise benefit from the CMT, leading to high unfairness and system performance degradation.
To avoid such unfairness and performance loss, an MQ-SSD should use CMT management algorithms that are aware of inter-flow interference.
A third point of contention is at the back end resources within an MQ-SSD (see Section 2.1).
A high-intensity flow can use up most of the back end resources if the flow issues a large number of requests in a short period of time.
This stalls the requests issued by a low-intensity concurrently-running flow, as the requests cannot be serviced before the back end resources finish servicing requests from the high-intensity flow.To understand how contention at the back end resources affects system performance and fairness, we perform a set of experiments where we concurrently run two I/O flows that issue random reads with a request size of 8 kB.
Flow-1 is a low-intensity I/O flow, as we limit its submission queue size (see Section 2.2) to 2 requests.We vary the submission queue size of Flow-2 from 2 requests to 256 requests, to control the flow intensity.
In order to isolate the effect of back end resource contention, we disable the write cache, and simulate a CMT where address translation requests always hit.
Figure 8a shows the slowdown when Flow-1 and Flow-2 run concurrently, and the change in the average chip-level queue depth (i.e., the number of requests waiting to be serviced by the back end; see Section 2.3) for each flow during concurrent execution, compared to the depth when each flow runs alone.
Figure 8b shows the fairness and overall performance of the system when the two flows run concurrently.
We make four observations from the figures.
First, the average chip-level queue depth of Flow-1 increases significantly when the intensity of Flow-2 increases.
Second, Flow-1 is slows down significantly when we increase the host-side queue depth of Flow-2 beyond 16.
For example, when Flow-2 is at the highest intensity that we test (with a host-side queue depth of 256 requests), Flow-1 slows down by 14.4x. Third, the effect of inter-flow interference on Flow-2 is negligible, as its slowdown is almost equal to 1 for hostside queue depths larger than 4.
Fourth, the asymmetric slowdowns (i.e., the large slowdown for Flow-1 and the lack of slowdown for Flow-2) cause both fairness and the overall system performance to decrease.
We conclude that a high-intensity flow can significantly increase the depth of the chip-level queues and thus lead to a large slow-down for concurrently-running low-intensity flows.
The FTL transaction scheduling unit must be aware of the inter-flow interference at the MQ-SSD back end to make the per-flow performance more fair and thus keep the overall performance high.
To study the effect of SSD device-level design choices on application-level performance metrics, such as instructions per cycle (IPC), an SSD simulator must be integrated and run together with a full-system simulator.
We integrate MQSim with gem5 [8] to provide a complete model of multi-queue I/O execution and a complete computer system.
As Table 2 shows, among existing SSD simulators, only SimpleSSD [35] is integrated with a full-system simulator, and SimpleSSD does not simulate multi-queue I/O execution.
In this section, we show the effectiveness of our integrated simulator, by studying how changes to QueueFetchSize (see Section 4.2.1) affect the IPC of concurrently-executing applications due to storage-level interference.We conduct a set of experiments, running instances of file server (fs) [77], mail server (ms) [77], web server (ws) [77], and IOzone large file access (io) [62] applications using the integrated execution mode of MQSim.
We first execute each application alone (i.e., without interference from other applications), and then concurrently execute the application with a second application to study the effect of inter-application interference.
To isolate the effect of inter-flow interference, where each flow belongs to one application, we assign each application to a single processor core and a single memory channel.
We test two different values of QueueFetchSize (16 entries and 1024 entries) to examine how QueueFetchSize affects inter-application interference.
For these experiments, we measure application slowdown (S app ), which is calculated as S app i = IPC alone app i /IPC shared app i , and use application slowdown to determine fairness using Equation 1.
Figure 9 shows the slowdown of each application and the system fairness for six pairs of concurrentlyexecuting applications.
On the x-axis, we list the applications used in each pair, along with the value of QueueFetchSize that we use.
We make two observations from the figure.
First, for application pairs where one of the applications is ms or ws, the impact of QueueFetchSize on fairness is negligible.
Both ms and ws benefit mainly from caching a large part of their data set in main memory, and hence issue very few requests to the SSD.
This keeps storage-level interference low, as ms and ws do not contend often for access to the SSD with the other applications that they are paired with.
Second, fs and io have high storage access intensities, and hence interfere significantly when they are paired together.
In this case, we observe that a large QueueFetchSize value leads to 60% fairness reduction.
We conclude that full-system behavior can greatly impact the fairness and performance of I/O flows on an MQ-SSD, as it affects the storage-level intensity of each flow.
To our knowledge, MQSim is the first simulator that (1) accurately simulates both modern and conventional SSDs, (2) faithfully models modern host-interface protocols such as NVMe, and (3) supports the accurate simulation of SSDs that use emerging ultra-fast memory technologies.
We compare MQSim to existing stateof-the-art SSD simulation tools in Section 5, and show that MQSim provides greater capabilities and accurate results.
In this section, we provide a brief summary of other related works.A number of prior works consider the performance and implementation challenges of MQ-SSDs [5,31,89,90].
Xu et al. [89] analyze the effect of MQ-SSDs on the performance of modern hyper-scale and database applications.
Awad et al. [5] evaluate the impact of different NVMe host-interface implementations on the system performance.
Vučini´Vučini´c et al. [83] show that the current NVMe protocol will be a performance bottleneck in future PCM-based storage devices.
The authors modify the NVMe standard in order to improve its performance for future PCM-based SSDs.Other works [31,72] focus on managing multiple flows in modern SSDs.
Song and Yang [72] partition the SSD back end resources among concurrently-running I/O flows to provide performance isolation and alleviate inter-flow interference.
Jun and Shin [31] propose a device-level scheduling technique for MQ-SSDs with built-in virtualization support.None of these previous studies provide a simulation framework for MQ-SSDs or study the sources of interflow interference inside MQ-SSDs.
We introduce MQSim, a new simulator that accurately captures the behavior of both modern multi-queue SSDs and conventional SATA-based SSDs.
MQSim faithfully models a number of critical features absent in existing state-of-the-art simulators, including (1) modern multi-queue-based host-interface protocols (e.g., NVMe), (2) the steady-state behavior of SSDs, and (3) the end-to-end latency of I/O requests.
MQSim can be run as a standalone tool, or integrated with a fullsystem simulator.
We validate MQSim against real offthe-shelf SSDs, and demonstrate that it provides highlyaccurate results.
By accurately modeling modern SSDs, MQSim can uncover important issues that cannot be modeled accurately using existing simulators, such as the impact of inter-flow interference.
We have released MQSim as an open-source tool [1], and we hope that MQSim enables researchers to explore new ideas and directions.
To validate the accuracy of MQSim, we compare its performance results to four state-of-the-art MQ-SSDs (SSD-A, SSD-B, SSD-C, and SSD-D) manufactured between 2016 and 2017.
Table 4 lists key properties of the four MQ-SSDs.
We precondition each device with full-load write traffic to write to 70% of the available logical space [71].
The device preconditioning process includes two 4-hour phases.
In the first phase, we perform sequential writes, while in the second phase, we perform random writes.
We perform real-system experiments on a server that contains an Intel Xeon E3-1240 v6 3.70GHz processor and 32 GB of DDR4 main memory.
The system uses Ubuntu 16.04.2 with version 2.6.27 of the Linux kernel, and the OS is stored in a 500 GB Western Digital HDD.
We run the fio benchmark tool for performance evaluations, and all storage devices are connected to the PCIe bus as add-in cards.
We validate our simulator with four different configurations that correspond to our four real MQ-SSDs.
To this end, we extract the main structural parameters of each real SSD using a microbenchmarking program.
This program analyzes and estimates the SSD's internal configuration (e.g., NAND flash page size, NAND flash read/write latency, number of channels in the SSD, address mapping strategy, write cache size) based on the methods described in prior SSD modeling studies [14,15,36].
We have open-sourced our microbenchmark [1].
For garbage collection (GC) management, we enable all of the advanced GC mechanisms described in Section 4.2.3, except write suspension, in MQSim.
According to the specifications of the flash chips used in two of the SSD devices, write suspension is not supported.
We validate MQSim against real devices using both synthetic and real workloads.
Our synthetic workloads issue random accesses, and consist of only read requests or only write requests, where we set the queue depth to 1 request.
Figure 10 compares the read and write request response time 4 measured on our four real MQ-SSDs with the latencies reported by MQSim for our synthetic workloads.
The plots in Figure 10a and 10b show the read and the write latencies, respectively.
The x-axes reflect different I/O request sizes, ranging from 4 kB to 1 MB.
The blue curves show the error percentage of the simulation model.
We observe that across all request sizes, the response times reported by MQSim match very closely 4 Response time is defined as the time from when a host request is enqueued in the submission queue to when the SSD response is enqueued in the completion queue.
with the measured response times of the real devices, especially for SSD-B and SSD-D.
Averaged across all four MQ-SSDs and all I/O request sizes, the error rates for read and write requests are 2.9% and 4.9%, respectively.
Figure 11 shows the accuracy of the request response time reported by MQSim as a cumulative distribution function (CDF), for three real workloads [53]: tpcc, tpce, and exchange.
We observe that MQSim's reported response times are very accurate when compared to the response times measured on the real MQ-SSDs.
The average error rates for SSD-A, SSD-B, SSD-C, and SSD-D are 8%, 6%, 18%, and 14%, respectively.We conclude that MQSim accurately models the performance of real MQ-SSDs.
To validate the accuracy of the multi-queue I/O execution model in MQSim, we conduct a set of simulation experiments using two I/O flows, Flow-1 and Flow-2, where each flow generates only sequential read requests.
We maintain a constant request intensity for Flow-1, by setting its I/O queue depth to 8 requests.
We vary the intensity of Flow-2 across our experiments, by varying the I/O queue depth between 8 entries and 256 entries.
Figure 12 shows the slowdown and normalized throughput (IOPS) of Flow-1 (left) and Flow-2 (center), and the fairness (see Section 3.1) of the system (right).
We make two key observations from the figure.
First, we find that MQSim successfully models the behavior of real MQ-SSDs that are optimized for higher per-flow throughput (e.g., SSD-A, SSD-B, SSD-C) when the value of QueueFetchSize is equal to 1024.
Fig- ure 12a shows similar trends for slowdown, throughput, and fairness to the measurements we perform on real MQ-SSDs, which we show in Figure 2.
When QueueFetchSize is set to 1024, a higher number of I/O requests from each flow are fetched into the devicelevel queue of the MQ-SSD.
In both our MQSim results and the measured results on real MQ-SSDs, we observe that as the intensity of Flow-2 increases, its throughput increases significantly with little slowdown, while the throughput of Flow-1 decreases significantly, causing Flow-1 to slow down greatly.
This occurs because when Flow-2 has a high intensity, it unfairly uses most of the back end resources in the MQ-SSD, causing requests from Flow-1 to wait for longer latencies before they can be serviced.Second, MQSim accurately models the behavior of real MQ-SSD products that implement mechanisms to control inter-flow interference, such as SSD-D, when QueueFetchSize is set to 16.
We see that the trends in Figure 12b are similar to those observed in our measured results from SSD-D in Figure 2.
When QueueFetchSize is set to 16, only a limited number of I/O requests for each concurrently-running flow are serviced by the back end, preventing any one flow from unfairly using most of the resources within the MQ-SSD.
As a result, even when Flow-2 has a high intensity, Flow-1 does not experience significant slowdown.We conclude that by adjusting QueueFetchSize, MQSim successfully models different multi-queue I/O processing mechanisms in modern MQ-SSD devices.
As we discuss in Section 4.4, MQSim pre-conditions the flash storage space and warms up the SSD data cache based on the characteristics of the co-running workloads.
To validate the steady-state model in MQSim, we conduct a set of experiments using MQSim under high write intensity, and compare the results to those from real MQ-SSD devices.
Figure 13 plots the read and write response times for (1) actual I/O execution on SSD-B (which is representative of the general behavior of the state-ofthe-art SSDs we examine); (2) MQSim-NoPrec, where MQSim is run without pre-conditioning, and (3) MQSimPrec, where MQSim is run with pre-conditioning.
Actual SSD-B Figure 13: MQSim accurately models the steady-state read and write response times (RT) of SSD-B, using fast preconditioning.We make two observations from the figure.
First, MQSim with pre-conditioning successfully follows the response time results extracted from SSD-B.
Second, MQSim without pre-conditioning reports lower response time results at the beginning of the experiment, since the simulated SSD is not yet in steady state.
Once the whole storage space is written, the response time results become similar to the real device, as garbage collection and write cache evictions now take place in simulation at a rate similar to the rate measured on SSD-B.
We conclude that MQSim's pre-conditioning quickly and accurately models the steady-state behavior of real MQ-SSDs.
We thank our shepherd Haryadi Gunawi and the anonymous referees for their feedback on this work.
We thank our industrial partners, especially Google, Huawei, Intel, and VMware, for their generous support.
