A variety of tools are used to support software vulnerability analysis processes.
However, when analysts want to select the optimal tool for a particular use case, or attempt to compare a new tool against others, no standard method is available to do so.
In addition, we have determined that although vulnerabil-ities can be categorized into vulnerability types, these types are often disproportionately represented in current datasets.
Hence, when comparative analyses of tools based upon these data sets are conducted, the results are distorted and unreal-istic.
To address this problem, we are building a Benchmark for Vulnerability Analysis Tools (B-VAT).
Representativeness is a key element of a good benchmark.
In this paper, we use stratified sampling to identify a representative set of vulnerabilities from over 800 CWE's and 75,000 CVE's.
This set becomes the foundation upon which we will build a purpose-based benchmark for vulnerability analysis tools.
By using the correlation between current CWE and CVE data, the proposed B-VAT will assess tools using vulnerabilities in the proportions their types occur in the wild.
The security community relies on tools to support software vulnerability analysis processes.
However, there is no benchmark to support the comparison of vulnerability analysis tools.
One current comparison approach is to see how well tools perform using test cases.Hundreds of thousands of publicly available test cases containing known software flaws are aggregated into datasets of vulnerabilities, each with its own structure, test cases, supported languages, and reporting method, e.g., Juliet Test Suite [1].
Databases such as the Software Assurance Reference Dataset (SARD) attempt to inject order by providing a consolidated repository of vulnerability datasets and test cases [2].
Unfortunately, even the SARD, which contains 40 datasets and over 170,000 test cases, is not exhaustive-it excludes datasets such as the Cyber Grand Challenge (CGC)Corpus [3], and OWASP Benchmark Project [4].
We will show that many of these datasets contain an unrealistic representation of weakness types, i.e., Common Weakness Enumeration (CWE) entries, when compared to known vulnerability instances in the wild, i.e., accepted Common Vulnerabilities and Exposures (CVE) entries.
Consequently, even if vulnerability analysis tools were assessed using all of the SARD test cases, the results would would still not reflect reality.
In our effort to create a Benchmark for Vulnerability Analysis Tools (B-VAT), test cases that are statistically representative of the vulnerabilities found in the wild are needed.The CWE provides a repository of weakness types, and the CVE, a dictionary of vulnerability instances.
A weakness is a mistake in software or hardware that, under proper conditions, could lead to the introduction of a vulnerability [5,6].
A vulnerability is an occurrence of one or more weaknesses that can be used to, "modify or access unintended data, interrupt proper execution, or perform incorrect actions that were not specifically granted to the party who uses the weakness" [6].
We analyzed four popular datasets of software vulnerabilities: Juliet C/C++, Juliet Java [1], the CGC Corpus [3], and the OWASP Benchmark [4].
Each contains test cases that can be used to assess vulnerability analysis tools.
We show that none represent vulnerability types in the proportions those types occur in the wild.
By correlating the data from 839 CWE's, and over 75,000 CVE's we identify a representative set of known vulnerabilities that can be used to design a purpose-based benchmark for vulnerability analysis tools.
This paper makes the following contributions:• We synthesize 839 CWE's with over 75,000 CVE's to determine the relative proportions of vulnerability instances and weakness types in the wild.
• We analyze four popular software vulnerability datasets, and show that none accurately represents vulnerability instances and weakness types as they occur in the wild.
• We perform stratified sampling to determine the appropriate number of test cases for each weakness type.
This provides the foundational data upon which B-VAT is being constructed.We next provide a brief background on benchmarks, and identify the key benchmark property motivating this paper: representativeness.
Section 3 explores how representativeness is achieved, and Section 4 offers a conclusion and future work.
There are many types of computer benchmarks.
In the 1960's traditional benchmarks were used to compare the speed with which computers accomplished basic data processing functions [7].
In 1965, Joslin proposed an application benchmark that used specific applications to emphasize the relative throughput performance of different system configurations [8].
Specification-based benchmarks define functions, and include required inputs and expected output [9].
Gustafson defined a purpose-based benchmark that, "explicitly and comprehensively measures the ability of a computing system to reach a goal of human interest" [10].
Our benchmark has a clear purpose: to facilitate comparison of vulnerability analysis tools.
For this reason, we are designing a purpose-based benchmark.
Researchers have proposed a number of desirable benchmark characteristics [9,[11][12][13].
We consider the following key characteristics for B-VAT:Relevant The benchmark problems should be closely connected to reality.
Repeatable The same results should be consistently reproduced when the benchmark is run with the same tool.
Usable The benchmark should be able to be used in multiple operating environments, and run with a variety of tools.
Fair The benchmark should impartially measure each tool.
Verifiable There should be confidence that benchmark results are accurate.
Determining the relevance of benchmark problems involves a number of elements.
From a design perspective, relevance involves two dimensions: the breadth of the benchmark's applicability, and the degree to which benchmark problems are relevant in each area [9,12].
In this paper, we focus on relevance, and specifically on the representativeness of the problems in B-VAT.
See Section 4 for a description of future work related to B-VAT.
The most important property of a benchmark relates to its problems [8,10].
Joslin called this the "representativeness" of a benchmark, and Gustafson the benchmark's "problem size."
Gustafson proposed a balance between a benchmark's problem size and its usefulness -too many problems in a benchmark raises its cost, while too few reduces its utility [10].
We will refer to benchmark problems as test cases.We define a representative set of vulnerabilities as a subset of vulnerabilities instances that adequately represents the larger set of known vulnerability instances and types [14].
We identify a representative set from a repository of over 75,000 accepted CVE's published between 2014 and 2019.
Much like the SARD, the CVE provides organization and standardization.
The CVE is a dictionary of publicly known vulnerability and exposure instances [15].
Each entry describes an instance of a vulnerability, and includes metadata such as a unique identifier (CVE ID), standardized description, and where applicable, a corresponding CWE entry 1 .
We cannot predict future vulnerabilities; our work is constrained to the set of "known known" and "known unknown" vulnerabilities [16].
Additionally, we recognize that the CVE is not exhaustive, however, it provides an extensive repository of known vulnerability instances that is suitable for our purposes.To date, the greatest number, 21, 598, of publicly disclosed vulnerabilities and exposures was reported by the CVE in 2018.
Over half, 93, 056 out of 160, 544, of all of vulnerabilities ever reported (excluding 2020) by the CVE were published between 2014 to 2019 [15].
We use these six years of 75, 535 community-accepted CVE's as the set from which we identify a representative subset of vulnerability instances.The CVE provides instances of known vulnerabilities, and the simplest method to identify a representative subset from these data would be to take a random sample of the larger set.
However, a simple random sample may result in the misrepresentation of vulnerability types [17].
By using the existing correlation between CVE ID's and CWE ID's we can link each CVE ID to one of ten CWE pillars and take a stratified sample of the set.
What the CVE provides for vulnerability instances, the CWE provides for weakness types.
The CWE is a repository of over 1200 hardware and software weaknesses, and provides a common language, identifier, and definition for each weakness type referenced.
CWE entries are organized into a number of views to support different objectives.
We use view CWE-1000, Research Concepts, that includes a hierarchy of 839 CWE entries.
The hierarchy contains one of the following abstraction types for each CWE ID [18]: Pillar Weaknesses that are described in the most abstract fashion (10 CWE's).
Pillars include: Class Abstract weakness, typically independent of any specific language or technology (96 CWE's).
Base A more specific type of weakness (441 CWE's).
Variant A weakness that is described at a very low level of detail, typically limited to a specific language or technology (285 CWE's).
Composite A set of weaknesses that must all be present simultaneously in order to produce an exploitable vulnerability (7 CWE's).
The weakness hierarchy presented by view CWE-1000 can be organized into ten rooted trees.
A rooted tree is a tree with a single root vertex that is distinguished from all others.
Each pillar in the hierarchy is a root node of a rooted tree.
[19], pandas [20], and D3 [21] we crawled over 1000 individual pages on the CWE website to create and visualize the tree data structures for each of the ten CWE Pillars.
This approach allows us to view the most up-todate information on CWE relationships and hierarchies.
Then, by using CWE-1000 as the root node of a tree we can create a single rooted tree that includes every CWE in the CWE-1000 view.
Figure 1 depicts the ten CWE pillars and their 839 children as a hierarchical radial dendrogram with root node CWE-1000.
We use the CWE relationship and hierarchy data to organize the CVE's into ten strata (see Section 3.4).
Of the 75, 535 community-accepted CVE's, 55, 128 have an associated CWE ID.
By using the correlations between CVE ID's and CWE ID's, we classify each of these vulnerability instances by their weakness type.
Figure 2 shows the sum of known vulnerability instances (CVE IDs), by type (CWE IDs) from 2014-2019.
This visualization shows the relative proportions of CWE ID's in the wild.
The enclosing circles show the cumulative size of each of the ten CWE pillars (i.e. subtrees), while maintaining relationship and hierarchical data.
The exterior circle represents root node, CWE-1000.
for comparison.
These diagrams illustrate the stark contrast between the types of weaknesses in the wild, and those in current vulnerability datasets 2 .
Like the CVE, each test case in the reviewed datasets has a corresponding CWE ID that can be traced to a pillar node.
Table 1 shows the relative percentages of test cases in each pillar by vulnerability dataset.
It shows that none of the vulnerability datasets accurately reflects vulnerabilities as they have occurred in the wild, i.e., CVE's from 2014-2019.
By taking a stratified sample of the CVE-CWE data we propose a subset of test cases for B-VAT that proportionately represents vulnerability instances, and weakness types.
Stratified sampling is a statistical method that allows subgroups, or strata to be proportionately represented [22], thus providing a representative sample of a larger population.
Unlike random sampling, which may result in the misrepresentation of vulnerability instances and weakness types, stratified sampling allows us to preserve the relative proportions of each pillar, or strata.
between CWE and CVE entries our B-VAT will represent vulnerability instances and weakness types in the proportions they occur in the wild.
In this paper, we used CWE pillars as the strata in our sample, however, it may be prudent to add additional variables before taking a stratified sample.
For example, we could also include a severity ranking for each CVE, and force our sample to include vulnerability instances with a high severity [23].
Despite the large number of tools used to support software vulnerability assessments, there is no benchmark that permits evaluation and comparison of those tools.
To address this problem, we are developing a Benchmark for Vulnerability Analysis Tools.
In this paper, we have discussed a fundamental property of B-VAT: representativeness.
We examined four popular datasets of software vulnerability test cases: Juliet C/C++, Juliet Java, the OWASP Benchmark, and the CGC Corpus, and determined that none represent vulnerabilities as they have occurred in the wild from 2014-2019.
First, we synthesized the data from 839 CWE's and over 75,000 CVE's, then we used stratified sampling to identify a distribution of weakness types that is representative of known vulnerability instances.
This analysis provides a foundation for B-VAT; however, much work remains.
Currently, we are exploring the impact of including additional variables before taking a stratified sample (e.g., relevance of specific CVE's and CWE's, severity rankings, and a weighted component for rare vulnerabilities).
After determining the desired number of test cases for B-VAT, we must determine a method to score vulnerability analysis tools.
Existing datasets contain over 150,000 test cases; Table 3 shows the collective number of available test cases corresponding to each pillar.
We are exploring methods to reuse these test cases in a more representative way, as described in this paper.
This work is ongoing, and we welcome collaboration.
All data and code is available upon request.
We wish thank Lyn Whitaker for valuable discussions.
