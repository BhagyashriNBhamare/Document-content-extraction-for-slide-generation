While there has been a tremendous interest in processing graph-structured data, existing distributed graph processing systems take several minutes or even hours to mine simple patterns on graphs.
In this paper, we try to answer the question of whether it is possible to build a graph pattern mining engine that is both fast and scalable.
Leveraging the observation that in several pattern mining tasks, providing an approximate answer is good enough, we propose the use of approximation for graph pattern mining.
However, we find that existing approximation techniques do not work for this purpose.
Based on this, we present a new approach for approximate graph pattern mining that leverages recent advancements in graph approximation theory.
Our preliminary evaluations show encouraging results: compared to state-of-the-art, finding 3-motifs in Twitter graph is 165⇥ faster while incurring only 5% error.
We conclude by discussing several systems challenges to make our proposal practical.
The past few years has seen a resurgence in enterprises storing and processing massive amounts of graph-structured data [1,2].
Algorithms for graph processing can broadly be classified into two categories.
The first, graph analysis algorithms, consists of those which compute properties of a graph, typically using neighborhood information.
Examples of such algorithms include page rank [41], community detection [26] and label propagation [57].
The second, graph pattern mining algorithms, focuses on discovering structural patterns in a graph.
Examples of this include motif finding [38], frequent sub-graph mining (FSM) [55] and clique mining [16].
Both categories have been thoroughly explored in academic literature, with researchers proposing several algorithms.Today, a deluge of graph processing frameworks exist, developed both in academia and open-source [17, 20, 21, 29-31, 35-37, 39, 45-47, 53].
These frameworks typically provide high-level abstractions that make it easy for developers to implement many graph algorithms.
While both categories of graph algorithms are equally important, a vast majority of the existing graph processing frame-񮽙 Equal contribution.works have focused on graph analysis algorithms.
These frameworks are fast and can scale out to accommodate really large graphs: for instance, GraM [54] can run one iteration of page rank on a trillion-edge graph in 140 seconds in a cluster.
In contrast, graph pattern mining systems fail to scale to even moderately sized graphs, and are slow, taking several hours to mine simple patterns [25,50].
The main culprit that hinders the scalability of pattern mining is the complexity of these algorithms-mining patterns requires complex computations and storing exponentially large intermediate candidate sets.
For example, a graph with a million vertices may possibly contain 10 17 triangles.
While distributed graph-processing solutions are good candidates for processing such massive intermediate data, the need to do expensive joins to create candidates severely degrades performance.
As a result, state-of-the-art systems like Arabesque [50] propose new abstractions for storing candidates in a distributed setting.
However, even with optimized methods to store candidates, Arabesque takes over 10 hours to count motifs in a graph with less than 1 billion edges.In this paper, we ask the question "Is it possible to build a graph mining system that is both fast and scalable?"
A key observation that we leverage in answering this question is: in many pattern mining tasks, it is often not necessary to output the exact answer.
For instance, in FSM the task is to find the frequency of subgraphs with an end-goal of ordering them by occurrences.
Similarly, motif counting determines the number of occurrences of a given motif.
In these scenarios, it is su񮽙cient to provide an approximately correct answer.
Indeed, our conversations with a social networking firm revealed that one of their most time-consuming production jobs is counting graphlets [44] to determine social graph similarity.
Another company's core business depends on classifying fraudulent patterns in graphs and this is done by counting the frequency of pattern occurrences.
In both cases, an approximate count is good enough.
Further more, it is not necessary to materialize all possible occurrences of a pattern1.
Thus, we propose using approximate methods to build a fast and scalable graph mining system.1In large graphs, it may even be infeasible to output all embeddings.Approximate analytics is an area that has garnered attention for big data analytics [5,12,27], where the goal is to let the user trade-o accuracy for much faster results.
The basic idea in approximation systems is to execute the exact algorithm on small portions of the data, referred to as samples, and then rely on the statistical properties of these samples to compose partial results and/or error characteristics.
The fundamental assumption underlying these systems is that there exists a relationship between the input size and the accuracy of the results.
However, this assumption falls apart when applied to graph pattern mining.
In particular, running the exact algorithm on a sampled graph may not result in reducing the runtime or provide a good estimation of the result ( §2.2).
In this paper, we propose leveraging graph approximation theory to build approximate pattern mining systems.The key idea we exploit is that approximate pattern mining can be viewed as equivalent to probabilistically sampling random instances of the pattern.
This observation lets us run sampling methods with very high parallelism and provides drastic reduction in run-time while sacrificing a small amount of accuracy.
For example, our preliminary evaluation shows that our approach is 165⇥ faster compared to the state-of-the-art for mining 3-motifs while incurring only 5% error.There are a number of systems challenges in realizing a practical approximate pattern mining system.
While we use the theory as a foundation, we need to extend the state-of-the-art approximation techniques not only to general patterns, but also to distributed settings.
Further, an important problem in any approximation system is in allowing users to navigate the tradeo between the result accuracy and latency.
While existing approximate processing systems have proposed a number of approaches for this task, we find that they do not fit our needs.
In the rest of this paper, we discuss our approach, and our initial directions in tackling each of these challenges.
We begin by motivating the need for a new approach to approximate pattern mining.
Mining patterns in a graph represents an important class of graph processing problems.
The objective here is to find instances of a given pattern in a graph where a pattern is any arbitrary subgraph.
Thus, pattern mining algorithms aim to output all subgraphs, commonly referred to as embeddings, that match the input pattern.
Matching is done via sub-graph isomorphism, which is well known to be NP-complete.
Several varieties of graph pattern mining problems exist, ranging from finding cliques to mining frequent subgraphs.
We refer the reader to [6,50] for an excellent, in-depth overview of graph mining algorithms.A common approach to implement pattern mining algorithms is to iterate over all possible embeddings in the graph starting with the simplest pattern (e.g., a vertex or an edge).
The system checks all such candidate embeddings, and prunes those which cannot be part of the final answer.
The resulting candidates are then expanded by adding one vertex or edge, and the process repeated until it is not possible to expand the exploration further.
The obvious challenge in graph pattern mining, as opposed to graph analysis, is the exponentially large candidate set that need to be checked.Distributed graph processing frameworks are built to support massive amounts of data, and thus may seem like an ideal candidate for this situation.
Unfortunately when applied to graph mining problems, they face several challenges.
Arabesque [50], a recently proposed distributed graph mining system, discusses these challenges in detail, and proposes solutions to tackle several of them.
However, even Arabesque is unable to scale to large graphs due to the need to materialize candidates and exchange them between machines.
As an example, Arabesque takes over 10 hours to count motifs of size 3 in a graph with less than a billion edges in a cluster of 20 machines, each having 32 cores and 256GB of memory.
Approximate processing is an approach that has been used with tremendous success in solving similar problems in both big data analytics [5,27] and databases [19,22,23].
Thus it is natural to explore similar techniques for pattern mining in graphs given our earlier description of enterprise use cases.
However, simply extending existing approaches to graphs is insu񮽙cient.The common underlying idea in approximate processing systems is to sample the input that a query or an algorithm works on.
Several techniques for sampling the input exists, for instance, BlinkDB [5] leverages stratified sampling.
To estimate the error, approximation systems rely on the assumption that the sample size relates to the error in the output (e.g., if we sample K items from the original input, then the error in aggregate queries, such as SUM, is inversely proportional to p K).
It is straightforward to envision extending this approach to graph pattern mining-given a graph and a pattern to mine in the graph, we first sample the graph, and run the pattern mining algorithm on the sampled graph.
Figure 1a depicts the idea as applied to triangle counting.
In this example, the input graph contains 10 triangles.
Using uniform sampling on the edges we obtain a graph with 50% of the edges.
Applying triangle counting on this sample yieldings an answer of 1.
There are a number of approaches to scale this number to the actual graph.
One naive approach is to double it, since we reduced the input by half.
To verify the feasibility of the approach, we eval- uated it on the Twitter graph [34] for finding 3-chains and the UK webgraph [14] graph for triangle counting.
The relation between the sample size, error and the speedup compared to running on the original graph ( figs. 1b and 1c respectively.T or ig T s ample ) is shown inThese results show fundamental limitations of the approach.
We see that there is no relation between the size of the graph (sample) and the error or the speedup achieved.
Even very small samples do not provide noticeable speedups, and conversely, even very large samples end up with significant errors.
Thus, we conclude that the existing approximation approach of running the exact algorithm on one or more samples of the input is incompatible with graph pattern mining.
The key idea we leverage is to sample instances of a given pattern from the graph, and based on the sampling probability and how many instances we find, we estimate the total number of instances of that pattern in the graph.
Since only sampling once would yield large variance on the results, we independently sample multiple times and take the average to reduce the variance.
We call each sampling process an estimator.
By using r estimators and making r su񮽙cient large, we are able to get accurate results with bounded errors.
Since an estimator takes computation and memory resource to sample a pattern, picking the number of estimators r provides a trade-o񮽙 between result accuracy and resource consumption.While the intuition of using such sampling to approximate pattern counts is straightforward, the approximation bound analysis is quite subtle.
In fact there is a large body of theoretical work on various algorithms to sample patterns and analysis to prove their bounds [7,10,18,32,42,43,51].
Consider triangle counting as an example.
Naively, one would design an estimator that uniformly samples three edges from the graph without replacement.
Since the probability of sampling one edge is 1/m in a graph of m edges, the probability of sampling three edges is 1/m 3 .
If the three sampled edges form a triangle, the estimator outputs triangle count to be m 3 ; otherwise, it outputs 0.
While such an estimator is unbiased, since m is large, the probability that the estimator would find a triangle is very low and the variance of the result is very large.
To reduce the variance, we would require a large number of estimators, and this increases time and memory consumption.
We use neighborhood sampling [43] in our framework, which is an e񮽙cient technique to sample graph patterns.
Intuitively, compared to the naive sampling approach, neighborhood sampling increases the probability that an estimator would actually find an instance of the given pattern, and thus requires fewer estimators to achieve the same accuracy.
Neighborhood sampling models the graph as a stream of edges and starting from a random edge, we gradually add more edges until the edges form the pattern or it is impossible to form the pattern.
Let E denote the event that a pattern is formed, E 1 , E 2 , . . ., E k are the events that edges m 1 , m 2 , . . ., m k are sampled and stored.
Thus the probability that a pattern is actually sampled can be calculated as Pr(E) = Pr(E 1 \ E 2 · · · \ E k ) = Pr(E 1 ) ⇥ Pr(E 2 |E 1 ) · · · ⇥ Pr(E k |E 1 , . . ., E k񮽙1 ).
We leverage neighborhood sampling to build several estimators to sample patterns.
For triangle counting, if an estimator successfully samples a triangle, converting probability to expectation, e i = mc will be the estimate of triangles in the graph, where m is the number of edges in the graph and c is the number of neighbors of the first sampled edge that appearing after it.
For a total of r estimators, we will output 1 r Õ r e i as the approximate value.
Figure 2 presents an example graph with five nodes.
Estimator E 0 finds the triangle formed by edges (0, 3), (0, 4) and (3,4).
The probability of finding (0, 3) is 1/m = 1/10.
Since (0, 3) has four adjacent edges that appear after it in the order (i.e., (0, 4), (1, 3), (2, 3), and (3, 4)), the probability that finds (0, 4) is 1/c = 1/4.
Therefore, the probability of finding this specific triangle (0, 3, 4) is 1/(mc) = 1/40, and thus E 0 estimates the number of triangles to be 40, which is a biased result.
With more independent estimators E 1 , E 2 , and E 3 , the estimated count becomes more accurate as the final result takes the average of the four estimators.
One of the first questions that we need to answer before exploring the practicality and challenges in our proposal is to understand how much benefit we can obtain by leveraging approximation.
To do so, we implemented a simple pattern mining task, counting 3-motifs, using the approximation technique described earlier in Apache Spark [56].
We chose two datasets: LiveJournal (68.9M edges) [3] and Twitter (1.47B edges) [34], and use 16 machines on Amazon EC2 (8 cores each) to run an experiment which tries to find the count of 3-motifs, and compare against Arabesque [50].
We set the number of estimators to achieve an error rate of 5%.
Table 1 shows the results.
We were unable to get Arabesque to handle the Twitter graph in our cluster, so we use the numbers in [50] for the larger graph.
We see that our approach significantly outperforms Arabesque, and that the performance gap increases in the larger graph.
Our approach is able to achieve more than 2 orders of magnitude (165⇥) reduction in computation time while using less resources and incurring only a small (5%) loss in accuracy.
Several challenges lie ahead of us before we can achieve our goal of a general purpose approximate graph mining system.
We describe some of them in this section.
Neighborhood sampling was proposed in the context of triangle counting, so we need to extend it to handle general patterns.
We plan to explore this using one simple observation: the sampling process in each estimator can be seen as comprising of two phases.
In the first, sampling phase, edges are sampled either randomly, or using adjacency information of already sampled edges.
The phase ends when the sampled edges have fixed the vertices for a given pattern.
Then we wait for the edges that complete the pattern, hence the process enters closing phase.The amount of time an estimator process spends in each of these phases, and the number of edges sampled in them depend on the pattern.
In triangle counting, there is only one way to form the triangle: the sampling phase finds two adjacent edges, and the closing phase awaits the third edge to form a triangle with the two sampled edges.
For a general graph pattern with multiple nodes, there can be multiple ways to form the pattern.
One approach to generalize the sampling is to restrict the implementation of mining tasks using the two phases (e.g., using a simple API).
Then, the challenge is to compute the probability of finding the pattern automatically given a mining task written using this restricted model.
Neighborhood sampling viewed as comprising of two phases is massively parallel, since the sampling and closing phases remains the same for each estimator and can be captured using a simple data-parallel map phase, and the results can be aggregated using a reduce phase.
Unfortunately, we cannot simply scale up this process horizontally by locally running the process in each machine and aggregating results, since the probability analysis assumes that each estimator sees the entire graph.
Partitioning the graph into multiple machines results in missing patterns that span partitions, and significantly underestimates results, the magnitude of which depends on the partitioning strategy.
One possible solution for this problem is to account for the error due to this underestimation by scaling the result by a factor f (w), which is related to the number of partitions w. For this to work, we must not only precisely compute f (w), but also do it for any pattern.
A key feature in any approximate processing system is the ability for users to trade-o accuracy for result latency.
To allow users to navigate this trade-o񮽙, our solution needs to understand the relation between latency and error.
In our approach, the only configurable parameter is the number of estimators used for mining.
Setting a specific number of estimators, N e , results in a fixed runtime and an error within a bound.
Thus, by varying the number of estimators, we can vary the accuracy achieved and the computation time accordingly.
To enable picking the right number of estimators, we would need two profiles, estimators vs. latency and estimators vs. error.
The time complexity of our approximation algorithm is linear in terms of the number of edges in the graph and the number of estimators.
Given a graph and a particular pattern, the computation time is dominated by the number of estimators when the number of estimators is large enough.
As an example, Figure 3 shows the relationship between the computation time and the number of estimators for triangle counting in the Twitter graph [34].
We can see that the curve is close to linear when the number of estimators is larger than 0.5M.
When the number of estimators is small, the computation time is also a񮽙ected by other items and thus the curve is not strictly linear.
However, for these regions, it is not computationally expensive to profile more data points, and as these regions have high error, they are less likely to be of interest to users.
Thus, we plan to study classical regression-based techniques to build this profile and using the profile can aid in picking the number of estimators to use within a profiling cost.
As seen from Figure 3, the estimator vs. error profile is non-linear.
Building this profile is challenging due to several reasons.
Exhaustive profiling is out of the question due to its prohibitive time requirements.
Further, the actual errors vary within some range for the same number of estimators due to the randomness in our approach.
This makes theoretical closed-bound solutions di񮽙cult.
Finally, to estimate the error, we need to know the groundtruth.
However, computing the ground-truth undermines the usefulness of approximate processing.We plan to investigate a number of ways to build this curve in an e񮽙cient manner.
This includes using a piecewise modeling of the curve and leveraging experiment design [52] or bayesian optimization [11] to fit the model.
Further, we plan to explore well known statistical techniques like bootstrap [33].
Finally, we are also planning to look at probabilistic techniques that can use a small portion of the graph to build this profile without the need to know the ground-truth.
While existing graph processing systems assume graphs to be static, real-world graphs are dynamic.
Some previous works have looked at supporting incremental computations on evolving graphs [31,39,40], but they do not extend to pattern mining.
The challenge here is to incorporate incremental profile building techniques to support graph evolution, i.e., can we avoid rebuilding the profiles?For instance, it may be possible to use stale profile without much degradation if we can predict when the profile is not good enough to constitute a rebuilding.
Further, it may be possible to cache estimator states and reuse them later during rebuilding.
Finally, an interesting direction to look at is the possibility of precomputing some base patterns that could be building blocks for other patterns.
A number of systems have been proposed in the literature for graph processing [17,29,30,35,36,[45][46][47]53] and graph mining [4,48,50].
Processing systems typically only focus on graph analysis, and do not support e񮽙cient pattern mining.
Mining systems on the other hand require significant time to process even moderately sized graphs.
[49] discusses an approximate motif counting algorithms in HPC clusters.
However, its focus is on optimizing MPI communication techniques for one specific algorithm, and hence does not extend to general graph patterns.Approximate analytics systems [5,12,27] have recently gained popularity due to the time required to process large datasets.
These systems reduce the amount of data that is used in the analysis in the hope that this reduces processing time.
However, as we show in this work, this technique does not extend to graph pattern mining.Theory community has invested significant e񮽙ort in approximate graph algorithms for several graph analysis tasks [8,9,13,15,24,28].
None of these are aimed at distributed processing, nor do they propose ways to understand the performance profile of the algorithms when deployed in the real-world.
We leverage this rich theoretical foundation by proposing the use of these techniques to mine general patterns in distributed settings.
In this paper, we proposed our approach towards building a distributed graph pattern mining system that is both fast and scalable to large graphs.
Our proposal leverages approximation to achieve this goal, by building on advancements in graph approximation theory.
We discussed several challenges in realizing our proposal, which we are actively pursuing.
Our preliminary evaluations show promise in our proposed techniques.
