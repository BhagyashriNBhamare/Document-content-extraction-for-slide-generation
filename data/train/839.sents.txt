Runtime monitoring is an established technique for enforcing a wide range of program safety and security properties.
We present a formalization of monitoring and monitor inlining, for the Java Virtual Machine.
Monitors are security automata given in a special-purpose monitor specification language, ConSpec.
The automata operate on finite or infinite strings of calls to a fixed API, allowing local dependencies on parameter values and heap content.
We use a two-level class file annotation scheme to characterize two key properties: (i) that the program is correct with respect to the monitor as a constraint on allowed program behavior, and (ii) that the program has an instance of the given monitor embedded into it, which yields state changes at prescribed points according to the monitor's transition function.
As our main application of these results we describe a concrete inliner, and use the annotation scheme to characterize its correctness.
For this inliner, correctness of the level II annotations can be decided efficiently by a weakest precondition annotation checker, thus allowing on-device checking of inlining correctness in a proof-carrying code setting.
Program monitoring is a firmly established and efficient approach for enforcing a wide range of program security and safety properties [6,10,5,9].
Several approaches to program monitoring have been proposed in the literature.
In "explicit" monitoring, target program actions are intercepted and tested by some external monitoring agent [10].
A variant, examined by Schneider and Erlingsson [6], is monitor inlining, under which target programs are rewritten to include the desired monitor functionality, thus making programs essentially self-monitoring.
This eliminates the need for a runtime enforcement infrastructure which may be costly on small devices.
Also, it opens the possibility for third party developers to use inlining as a way of providing runtime guarantees to device users or their proxies.
This, however, requires that users are able to trust that inlining has been performed correctly.
In this work we propose a formalization of monitoring and monitor inlining as a first step towards addressing this concern. 񮽙
This work was partially funded by the S3MS project, IST-STREP-27004.
The second author was partially supported by the Swedish Research Council grant 2003-6108.
We focus on monitors as security automata that operate on calls to some fixed API from a target program given as an abstract Java Virtual Machine (JVM) class file.
Automaton transitions are allowed to depend locally on argument values, heap at time of call and (normal or exceptional) return, and return value.
Our main contributions are characterizations, in terms of JVM class files annotated by formulas in a suitable Floyd-like program logic, of the following two conditions on a program:1.
That the program is policy-adherent.
2.
The existence of a concrete representation of the monitor state inside the target program itself, as an inlined monitor which is compositional, in the sense that manipulations of the monitor state do not cross method call boundaries.The annotations serve as an important intermediate step towards a decidable annotation validity problem, once the inliner is suitably instantiated.
Compositionality allows validity to be checked per method.
This is uncontroversial, and satisfied by all inliners we know of.
By these results, the verification of a concrete inliner reduces to proof of validity of the corresponding annotations.
We use this to prove correctness for an inlining scheme which is introduced in the paper.
We also sketch how, for a program inlined by such an inliner, the annotations can be completed to produce a fully annotated program for which validity can be efficiently decided.
Such a fully annotated program can then be used by a bytecode weakest precondition checker in a proof-carrying code setting to certify monitor compliance to a third party such as a mobile device.Related Work A closely related result is the recent work on type-based monitor certification by Hamlen et al [8].
That work focuses on per-object monitoring rather than the "per-session" model considered here.
Also, their results are restricted to one particular inliner, whereas we give a characterization of a whole class of compositional inliners.Our results can be seen as providing theoretical underpinnings for the earlier work by Schneider and Erlingsson [6].
The PoET/PSLang framework developed by Erlingsson represents monitors as Java snippets connected by an automaton superstructure.
The code snippets are inserted into target programs at suitable points to implement the inlined monitor functionality.
This approach, however, makes many monitor-related problems such as policy adherence and correctness undecidable.
To overcome this, we base our results on a restricted monitor specification language, ConSpec [3], developed in the context of the EU project S3MS.Organization In section 2 we present the JVM model used in this paper.
Sections 3 and 4 introduce the automaton model in concrete and symbolic forms, the ConSpec language, and relations between the three.
In section 5 we give an account of monitoring by interleaved co-execution of a target program with a monitor, and establish the equivalence of policy adherence and co-execution.
In section 6, the two annotation levels are presented, and the main characterization theorems are proved.
In section 7 an inliner and its correctness are presented.
We also sketch how to produce, for this inliner, fully annotated programs with a decidable validity problem.
Finally, in section 8 we conclude and discuss future work.
Due to space limitations, many technical details, all proofs and further examples are delegated to a technical report [1].
We briefly present the components of JVM used in this paper.Types Fix sets of class names c ∈ C, method names m ∈ M, and field names f ∈ F.
A type τ ∈ T ype is either a primitive type, not further specified, or an object type, determined by a class name c.
An object type determines a set of fields and methods defined through its class declaration.
Class declarations induce a class hierarchy, and c 1 <: c 2 if c 1 is a subclass of c 2 .
If c is the smallest superclass (under <:) of c 񮽙 that contains an explicit definition of c.m then c defines c 񮽙 .
m. Single inheritance ensures that definitions are unique, if they exist.Values and Methods Values of object type are (typed) locations 񮽙 ∈ Loc, mapped to objects by a heap h ∈ H, a partial assignment of objects to locations.
Objects determine typed fields and methods, using standard dot notation, and type(, h) is the type of 񮽙 in h, if defined.
A method definition is an environment Γ (usually elided) taking a method reference M = c.m to a definition (P, H) consisting of a method body (instruction sequence) P , and an exception handler array H. Method overloading is not considered.
The notation M [L] = I indicates that Γ (M ) = (P, H) and P (L) is defined and equal to the instruction I.
The exception handler array H is a partial map from integer indices to exception handlers.
An exception handler (b, e, t, c) catches exceptions of type c and its subtypes raised by instructions in the range [b, e) and transfers control to address t, if it is the topmost handler that covers the instruction for this exception type.Machine Configurations, Transitions and Type Safety A configuration of the JVM is a pair C = (R, h), where R is a stack of activation records of the form either (M, pc, s, lv) for some method reference M , program counter pc, operand stack s, and local variables lv , or, for exceptional states, of the form (񮽙) e , where 񮽙 is the location of an exceptional object.
Unhandled (C) holds if C has an exceptional frame on top of the frame stack, and the current method does not have a handler for the exception.
We assume a standard transition relation −→ JVM on JVM configurations (cf. [7]).
An execution E of a program (class file) T is then a (possibly infinite) sequence of JVM configurations C 1 C 2 . . . where C 1 is an initial configuration consisting of a single, normal activation record with an empty stack, no local variables, M as a reference to the main method of P , pc = 1, Γ set up according to T, and for each i ≥ 1,C i −→ JVM C i+1 .
We restrict attention to configurations that are type safe, in the sense that heap contents match the types of corresponding locations, and that arguments and return/exceptional values for primitive operations as well as method invocations match their prescribed types.
The Java bytecode verifier serves, among other things, to ensure that type safety is preserved under machine transitions.API Method Calls The only non-standard aspect of −→ JVM is the treatment of API methods.
We assume a fixed API for which we have access only to the signature, but not the implementation, of its methods.
We therefore treat API method calls as atomic instructions with a non-deterministic semantics.
Our approach hinges on our ability to recognize such method calls.
This property is destroyed by the reflect API, which is left out of consideration.
Among the method invocation instructions, we discuss here only invokevirtual; the remaining invoke instructions are treated similarly.
Let T be a program for which we identify a set of security relevant actions A. Each execution of T determines a corresponding set Π(T) ⊆ A * ∪ A ω of finite or infinite traces of actions in A.
A security policy is a predicate on such traces, and T satisfies a policy P if P(Π(T)).
The notion of security automata was introduced by Schneider [11].
We view a security automaton over alphabet A as an automaton A = (Q, δ, q 0 ) where Q is a countable set of states, q 0 ∈ Q is the initial state, and δ : Q × A A Q is a (partial) transition function.
All q ∈ Q are viewed as accepting.
A security automaton A induces a security policyP A ⊆ 2 A * ∪A ω through its language L A by P A (X) ⇔ X ⊆ L A .
In this study, we focus on security automata which are induced by policies in the ConSpec language (see section 4) and therefore are named ConSpec automata.
The security relevant actions are method calls, represented by the class name and the method name of the method, along with a sequence of values that represent the actual arguments.
We partition the set of security relevant actions into pre-actions A 񮽙 ⊆ C × M × Val * × H and post-actions A 񮽙 ⊆ RVal × C × M × Val * × H × H, corresponding to method invocations and returns.
Both types of actions may refer to the heap prior to method invocation, while the latter may also refer to the heap upon termination and to a return value from RVal = V al ∪ {exc} where exc is used to mark exceptional return from a method call 1 .
The partitioning on security relevant actions induces a corresponding partitioning on the transition function δ of ConSpec automata into a function δ 񮽙 on pre-actions, and a function δ 񮽙 on post-actions.
A monitor specification in ConSpec determines a collection of security relevant actions (sra's), a security state, and for each security relevant action, a transition rule, using a guarded command-like syntax.
In addition, in [3] a scope declaration is introduced which is ignored in this paper.
As an example, consider the following specification: The formal semantics of ConSpec policies is defined in terms of symbolic security automata, which in turn induce ConSpec automata.
(ii) Init s : q s → Val is an initialization function; (iii) A s = A 񮽙 s ∪ A 񮽙 sis a countable set of symbolic actions, where: of the guards that lie above it in the clause, and the effect of the guarded command.
The updates to security state variables, which are presented as a sequence of assignments in ConSpec, are captured in the automaton as functions that return one ConSpec expression per symbolic state variable, determining the value of that variable after the update.
In fig. 1 we illustrate the construction on the earlier example, using "a" for accessed and "p" for permission.
Symbolic automata are converted to ConSpec automata without too much effort.
The details are given in [1].
Here it suffices to note that states in the induced ConSpec automaton are members of the lifted function space (q s → Val ) ⊥ .
The bottom element, in particular, is used only as the target of posttransitions that are disallowed (has an unsatisfied boolean guard) in the symbolic automaton; it has no outgoing transitions.A 񮽙 s ⊆ C × M × (Type × V ar) * and A 񮽙 s ⊆ {(Type × V ar) ∪ {exc}} × C × M × (Type × V ar) * are the symbolic pre-and post-actions, respectively; (iv) δ s = δ 񮽙 s ∪ δ 񮽙 s is a symbolic transition relation, where: δ 񮽙 s ⊆ A 񮽙 s × BoolExp × (q s → Exp) and δ 񮽙 s ⊆ A 񮽙 s × BoolExp × (q s → Exp) In this section we formalize the enforcement language of a ConSpec automaton as a set of finite strings of security relevant actions.
Each target transition can give rise to zero, one, or two security relevant actions, namely, in the latter case, a pre-action followed by a post-action.
Accordingly, we define the security relevant pre-action, act 񮽙 (C), of the configuration C, and the corresponding post-action, act 񮽙 (C 1 , C 2 ), as in the table below.
If none of the conditions of the table hold, the corresponding action is 񮽙.
act 񮽙 (C) Condition (c, m, s, h b ) C = ((M, pc, s · [d] · s 񮽙 , lv ) · R, h 񮽙 ) M [pc] = invokevirtual c 񮽙 .
m c defines type(d, h 񮽙 ).
m type(h 񮽙 , d) <: c 񮽙 (c, m, s, h 񮽙 ) ∈ A 񮽙 act 񮽙 (C1, C2) Condition (v, c, m, s, h 񮽙 , h 񮽙 ) C1 = ((M, pc, s · d · s 񮽙 , lv ) · R, h 񮽙 ) M [pc] = invokevirtual c 񮽙 .
m C2 = ((M, pc + 1, v · s 񮽙 , lv) · R, h 񮽙 ) c defines type(h 񮽙 , d).
m type(h 񮽙 , d) <: c 񮽙 (v, c, m, s, h 񮽙 , h 񮽙 ) ∈ A 񮽙 (exc, c, m, s, h 񮽙 , h 񮽙 ) C1 = ((M, pc, s · d · s 񮽙 , lv ) · R, h 񮽙 ) M [pc] = invokevirtual c 񮽙 .
m C2 = ((b)e · (M, pc, s · d · s 񮽙 , lv ) · R, h 񮽙 ) c defines type(h 񮽙 , d).
m type(h 񮽙 , d) <: c 񮽙 (exc, c, m, s, h 񮽙 , h 񮽙 ) ∈ A 񮽙We obtain the security relevant trace, srt A (w), of an execution w by lifting the operations act 񮽙 and act 񮽙 co-inductively to executions in the following way:srt A (񮽙) = 񮽙 srt A (C) = act 񮽙 (C) srt A (C 1 C 2 · w) = act 񮽙 (C 1 ) · act 񮽙 (C 1 , C 2 ) · srt A (C 2 · w)Then a target program T adheres to a policy P, if the security trace of each execution of T is in the enforcement language of the corresponding automatonA P , i.e. ∀E ∈ Π(T).
srt A (E) ∈ L AP .
Program-Monitor co-execution.
A basic application of a ConSpec automaton is to execute it alongside a target program to monitor for policy compliance.
We can view such an execution as an interleaving w = (C 0 , q 0 )(C 1 , q 1 ) · · · such that C 0 and q 0 is the initial configuration and state of T and A, respectively, and such that for each consecutive pair (C i , q i )(C i+1 , q i+1 ), either the target (only) progresses: C i −→ JVM C i+1 and q i+1 = q i or the automata (only) progresses: C i+1 = C i and ∃a ∈ A. δ(q i , a) = q i+1 .
In the former case we write (C i , q i ) −→ JVM (C i+1 , q i+1 ), and in the latter case we write (C i , q i ) −→ AUT (C i+1 , q i+1 ).
We can w.l.o.g. assume that at most one of these cases apply, for instance by tagging each interleaving step.The first projection function w ↓ 1 on interleavings w = (C 1 , q 1 )(C 2 , q 2 ) · · · extracts the underlying execution sequence C 񮽙 1 C 񮽙 2 · · · such that C 񮽙 1 = C 1 , and C 񮽙 2 = C 1 if (C 1 , q 1 ) −→ AUT (C 2 , q 2 )and C 񮽙 2 = C 2 otherwise, and so on.
To extract the automaton states and the security relevant actions, we use the (coinductive) function extract:extract((C 1 , q 1 )(C 2 , q 2 )w) = q 1 q 2 extract((C 2 , q 2 )w) if (C 1 , q 1 ) −→ AUT (C 2 , q 2 ), extract((C 1 , q 1 )(C 2 , q 2 )w) = act 񮽙 (C 1 )act 񮽙 (C 1 , C 2 )extract((C 2 , q 2 )w), if (C 1 , q 1 ) −→ JVM (C 2 , q 2 ), extract(C, q) = act 񮽙 (C), and extract(񮽙) = 񮽙.
Note that extract (w) may well be finite even if w is infinite.Definition 2 (Co-Execution).
Let E 񮽙 = {qq 񮽙 a 񮽙 | q, q 񮽙 ∈ Q, a 񮽙 ∈ A 񮽙 , δ 񮽙 (q, a 񮽙 ) = q 񮽙 }, E 񮽙 = {a 񮽙 qq 񮽙 | q, q 񮽙 ∈ Q, a 񮽙 ∈ A 񮽙 , δ 񮽙 (q, a 񮽙 ) = q 񮽙 }.
An interleaving w is a co- execution if extract(w) ∈ (E 񮽙 ∪ E 񮽙 ) * ∪ (E 񮽙 ∪ E 񮽙 ) ω .
In other words, an interleaving is a co-execution, if the sequence of extracted automaton states corresponds to an automaton run for the security relevant trace of the underlying execution.
-execution).
The program T adheres to policy P if, and only if, for each execution C 1 C 2 · · · of T there is a co-execution w for the automaton A P such that w ↓ 1 = C 1 C 2 · · · .
We specify monitor inlining correctness using annotations in a Floyd-style logic for bytecode.
The idea behind our annotation scheme is the following.
In a first annotation, referred to as the policy (or, level I) annotation, we define a monitor for the given policy by means of "ghost" variables, updated before or after every security relevant action according to the symbolic automaton induced by the given security policy.
In a second annotation, referred to as synchronisation check annotation (or level II), we add assertions that check at all relevant program points that the actual inlined monitor (represented by global program variables) agrees with the specified one (represented by ghost variables).
Assertions Methods are augmented with annotations that determine assertions on the extended state (current configuration and current ghost variable assignment), and actions on ghost variables.
Let g range over ghost variables, i ∈ ω, and let Op (Bop) range over a standard, not further specified, collection of unary and binary operations (comparison operations) on strings and integers.
Assertions a, and expressions e used in assertions, have the following shape:e ::= ⊥ | v | g | e.f | s[i] | Op e | e Op e a ::= e Bop e | e : c | ¬a | a ∧ a | a ∨ aHere, s [i] is the value at the i'th position of the current operation stack, if defined, and ⊥ otherwise, and e : c is a class membership test.Ghost Variable Assignments Ghost variables are assigned using a single, guarded multi-assignment of the form− → gs := a 1 → − → e 1 | · · · |a m → − → e m(1)such that the arities (and types) of − → gs and the − → e i match.
The idea is that the first assignment − → gs := − → e i is assigned such that the guard a i is true in the current extended state.
If no guard is true, the ghost state is assigned the constant ⊥-vector.
This happens, in particular, when m ≤ 0 in (1) above, which we write as − → gs := ().
Method Annotations.
A target program is annotated by an extended environment, Γ * , which maps method references M to tuples (P, H, A, Requires, Ensures, Exsures) such that Requires, Ensures and Exsures are assertions, and such that A is an assignment to each program point n ∈ Dom(P ) of a sequence, ψ, of atomic annotations, either an assertion or a ghost variable assignment.
Annotation Semantics In the absence of ghost variable assignments the notion of annotation validity is the expected one, i.e. that the assertions annotating any given program point (or the point of exceptional return) are all guaranteed to be valid.
To extend this account to ghost variables, we use a rewrite semantics, shown on table 1.
In the table, extended configurations are triples of the form (ψ, C, σ) such that ψ is the sequence of annotations remaining to be evaluated Table 1.
Operational Semantics of Annotations(1) Assert (a, C, σ) Γ * 񮽙 (aψ, C, σ) → (ψ, C, σ) (2) 񮽙 a1 񮽙 (C, σ) = TRUE, m > 0 Γ * 񮽙 (( − → gs := a1 → − → e1 | · · · |am → − → em)ψ, C, σ) → (ψ, C, σ[񮽙 − → e1 񮽙 (C, σ)/ − → gs]) (3) 񮽙 a1 񮽙 (C, σ) 񮽙 = TRUE, m > 0 Γ * 񮽙 (( − → gs := a1 → − → e1| · · · |am → − → em)ψ, C, σ) → (( − → gs := a2 → − → e2 | · · · |am → − → em)ψ, C, σ) (4) · Γ * 񮽙 (( − → gs := ())ψ, C, σ) → (ψ, C, σ[ − → ⊥ / − → gs]) (5) C −→JVM C 񮽙 Unexc(C 񮽙 ) Γ * 񮽙 (, C, σ) → (A(Γ * (M (C 񮽙 )))(pc(C 񮽙 )), C 񮽙 , σ) (6) C −→JVM C 񮽙 , Unhandled (C 񮽙 ) Γ * 񮽙 (, C, σ) → (Exsures (Γ * (M (C))), C 񮽙 , σ) (7) C −→JVM C 񮽙 Handled (C 񮽙 ) Γ * 񮽙 (, C, σ) → (, C 񮽙 , σ)for the current program point in C.
We use abbreviations M , pc, A, Requires, Ensures, and Exsures for the first to sixth projections, respectively.
Unexc holds of a configuration that does not have an exceptional frame on the top of the stack, and Unexc(C) ⇔ ¬(Handled (C) ∨ Unhandled (C)).
The side condition Assert(a, C, σ) always returns true, but as a sideeffect causes the arguments to be "asserted", e.g. to appear on some output channel.
For rule (6), note that unhandled exceptions causes the assertions in the Exsures clause to be asserted.
above is valid for the annotated environment Γ * , if all predicates asserted as a result of a Γ * -derivation(ψ 0 , C 0 , σ 0 ) −→ JVM · · · −→ JVM (ψ n , C n , σ n ) −→ JVM · · · are valid, where ψ 0 is Requires(Γ * (񮽙main񮽙)) · A 񮽙main񮽙 [1], C 0 is an initial configuration, and σ 0 = ⊥.
The policy annotations define a monitor for the given policy by means of a ghost state.
The ghost state is initialized in the precondition of the 񮽙main񮽙 method and updated at relevant points by annotating all the methods defined by the classes of the target program.
We call each such method an application method.
We assume that 񮽙main񮽙 is not called by any application method (including itself) and that all exceptions that may be raised by a security relevant instruction (i.e. an instruction that may lead to a security relevant action) are covered by a handler.
We also assume that the exception handling is structured such that the only way an instruction in an exception handler gets executed is if an exception has been raised and caught by the handler that the instruction belongs to.
Finally, we assume w.l.o.g. that there are no jumps to instructions below method invocations.Updating the Specified Security State.
The updates to the specified security state are done according to the transitions of the symbolic automaton.
If the automaton does not have a transition for a security relevant method call, the call is violating and the corresponding annotation sets the value of the specified state to undefined.
Such a program should terminate without executing the next security relevant action in order to adhere to the policy.
This is specified by asserting, as a precondition to each security relevant method invocation and at updates to the ghost state, that the ghost state is not undefined.
If a security relevant instruction may cause a pre-action (an unexceptional post-action) of the automaton, then a ghost assignment annotation is inserted as a precondition (as a postcondition) to this instruction.
Finally, if the instruction can cause an exceptional post-action, the update is inserted as a precondition to the first instruction of each exception handler that covers the instruction.
L 1 , L 2 , L, c) ∈ H M such that L 1 ≤ L 񮽙 < L 2, and as 񮽙 if such an L 񮽙 does not exist.
Given these annotations, the level I annotation of program T is given for each application method M as a precondition Requires I M and an array A I M of annotation sequences defined as follows (where L > 0):Requires I M = 񮽙 ( − → gs := − −− → e Init s ) · (g pc := 0) if M = 񮽙main񮽙 (g pc := 0)otherwise.
A I M [1] = A 񮽙 M [1] · A 񮽙 M [1][0] · A e M [1][0] A I M [L] = 񮽙 Exc(L, M ) · A 񮽙 M [L] · A 񮽙 M [L][0] · A e M [L][0] ifHandler (L, M ) A e M [L− 1][1] · A 񮽙 M [L− 1][1] · A 񮽙 M [L] · A 񮽙 M [L][0] · A e M [L][0] otherwiseA 񮽙 M [L][0] = ((g 0 , . . . , g n−1 , g this ) := (s[0], . . . , s[n])) · Defined 񮽙The assertion Defined 񮽙 checks if the ghost variables are defined:Defined 񮽙 = ((g this : c 񮽙 1 ∨ . . . ∨ g this : c 񮽙 p ) ⇒ ( − → gs 񮽙 = − → ⊥ ))while the postcondition of the instruction uses these saved values to compute the new security state:A 񮽙 M [L][1] = ( − → gs := α 1 | · · · | α m | α)where the α k are the guarded expressions (− → gs 񮽙 = − → ⊥ )∧g this : c 񮽙 i ∧a b ρ i → − → e E ρ i where class c 񮽙񮽙 defines (c 񮽙 i , m) and there exists a 񮽙 s = (τ x, c 񮽙񮽙 , m, (τ 0 x 0 , . . . , τ n−1 x n−1 )), a 񮽙 s ∈ A 񮽙 s \ A e s such that (a 񮽙 s , b, E) ∈ δ 񮽙 s .
The substitution ρ i is defined as [s[0]/x, g 0 /x 0 , . . . , g n−1 /x n−1 , g this /this].
Finally, α = ¬(g this : c 񮽙 1 ∨ . . . ∨ g this : c 񮽙 p ) → − → gs.The annotation arrays A 񮽙 M and A e M are defined similarly (see [1] for details).
Each execution of a program that is valid w.r.t. level I annotations for policy P is a co-execution of the program and the automaton for P, where the automaton states are given by the ghost state; hence the program adheres to P. with level I annotations for policy P is valid, if and only if T adheres to P.
An inlined program can be expected to contain an explicit representation of the security state, an embedded state, which is updated in synchrony with the execution of security relevant actions.
The level II annotations aim to capture this idea in a generic form that is independent of the design choices a specific inliner may make.
To this end, we make two assumptions on the inliner.
We require that the embedded state is in agreement with the ghost state immediately prior to execution of a security relevant action.
This condition would be violated by, for example, an optimized inliner which determines in advance that a fixed sequence of security relevant actions is permissible and reflects this to the embedded state through only a single update.
The second assumption we make in this section is that updates to the embedded state are made locally, that is by the method that executes the security relevant method call.
The specified and the embedded states are synchronized then at all call points.For simplicity we assume that the embedded state is determined as a fixed vector − → ms of global static variables of the target program, of types corresponding pointwise to the type of ghost state vector − → gs.
The synchronisation assertion is the equality − → gs = − → ms, and the level II annotations are formed by appending the synchronization assertion to the level I annotations of each application method M at the following points: (i) each annotation A(Γ * (M ))(i) such that P (Γ * (M ))(i) is an invoke or a return instruction, and (ii) the annotation Exsures(Γ * (M )).
We can then construct a sequence w(E, − → ms) = (C 0 , q 0 )(C 1 , q 1 ) · · · such that: q 0 is the initial automaton state, for all sampling points i > 0, q i = C i ( − → ms), where C i ( − → ms) denotes the value of − → ms in configuration C i , and for any two consecutive sampling points i and i 񮽙 , for all j : i ≤ j < i 񮽙 , q j = q i .
A II [L] L M[L] . . . L3 dup L4 astore r1 ⎧ ⎨ ⎩ g this := s[0] · g this : GUI ⇒ (ga, gp) 񮽙 = (⊥, ⊥) · (ga, gp) = (SecState.accessed, SecState.permission) ⎫ ⎬ ⎭ L5 invokevirtual GUI/AskConnect()Z ⎧ ⎪ ⎨ ⎪ ⎩ (ga, gp) := ((ga, gp) 񮽙 = (⊥, ⊥) ∧ g this : GUI ∧ s[0]) → (ga, true) | ((ga, gp) 񮽙 = (⊥, ⊥) ∧ g this : GUI ∧ ¬s[0]) → (ga, false) | (¬(g this : GUI)) → (ga, gp) ⎫ ⎪ ⎬ ⎪ ⎭ L6 istore r2 L7 aload r1 L8 instanceof GUI L9 ifeq L12 L10 iload r2 L11 putstatic SecState/permission L12 iload r2 񮽙 (ga, gp) = (SecState.accessed, SecState.permission) 񮽙 L13 ireturnThe role of the sequence w(E, − → ms) is similar to that of interleavings in section 5.
However, the sequence q 0 q 1 · · · may not necessarily correspond to an automaton run: the intermediate automaton state is not sampled when a postaction is followed by a pre-action without an intermediate method boundary crossing, as there is no well-defined point where this might be done.
The construction also needs to account for the method-local nature of embedded state updates.
For this reason, we define the operation extract II , taking sequences w to strings over the alphabet Q ∪ A ∪ {I} where I is a distinguished symbol, by the following conditions:-extract II ((C 1 , q 1 )(C 2 , q 2 )w)= q 1 act 񮽙 (C 1 )act 񮽙 (C 1 , C 2 )q 2 extract II ((C 2 , q 2 )w), if C 1 is an API method call.
-extract II ((C 1 , q 1 )(C 2 , q 2 )w) = q 1 Iq 2 extract II ((C 2 , q 2 )w), if C 1 is an appli- cation method call and Unexc(C 2 ), i.e. C 2 is a method entry point.
-extract II ((C, q)w) = qIq extract II (w), if C is a return point from an appli- cation method, either normal or exceptional.
-extract II ((C 1 , q 1 )(C 2 , q 2 )w) = extract II ((C 2 , q 2 )w), otherwise.
-extract II ((C, q)) = q act 񮽙 (C)if C is a method call and 񮽙 otherwise.
Σ 0 = {I, q, a 񮽙 , a 񮽙 | q ∈ Q, a 񮽙 ∈ A 񮽙 , a 񮽙 ∈ A 񮽙 }, Σ 1 = {I} ∪ Q ∪ E 񮽙 ∪ E 񮽙 ∪ {a 񮽙 qq 񮽙 a 񮽙 | ∃q 񮽙񮽙 .
δ 񮽙 (q, a 񮽙 ) = q 񮽙񮽙 , δ 񮽙 (q 񮽙񮽙 , a 񮽙 ) = q 񮽙 }, Σ 2 = {qq 񮽙 q 񮽙񮽙 , qq 񮽙 q, Iqq 񮽙 a 񮽙 , Iqq 񮽙 I, Iqq 񮽙 q 񮽙 , qa 񮽙 q 񮽙 , qa 񮽙 a 񮽙 q 񮽙 , a 񮽙 qq 񮽙 q 񮽙 , a 񮽙 qq 񮽙 I, a 񮽙 qq 񮽙 a 񮽙 , qIq 񮽙 , qa 񮽙 q 񮽙 | q 񮽙 = q 񮽙 񮽙 = q 񮽙񮽙 } A sequence w is a method-local co-execution, if extract II (w) ∈ (Σ * 1 ∪ Σ ω 1 ) \ (Σ * 0 · Σ 2 · (Σ * 0 ∪ Σ ω 0 ))We can then extend theorem 2 to the situation where a target program T has a monitor for the given policy inlined into it.
As an application of the annotation scheme described in the previous section, we characterize the correctness of a class of inliners in the flavor of PoET/PSLang [6].
We first describe the operation of a simple inliner that embeds, in target programs, a method-local monitor for a ConSpec policy.
The inliner adds a class definition to the program.
The static variables of this class serve as the embedded state.
Since this class is not in the original namespace, the embedded state is safe from interference by the target.
For each clause in the policy, a piece of bytecode is created, which evaluates, in turn, the guards of guarded commands and either updates the security state according to the update block associated with the first condition that holds or quits the program if none of them hold.The rewriting process consists of identifying method invocation instructions that lead to security relevant actions (security relevant instructions), and for each such instruction, inserting code produced by policy compilation in an appropriate manner.
The inliner inserts, immediately before the security relevant instruction, code that records the object the method is called for, and the arguments (and possibly parts of the heap) in local variables.
Then, code for the relevant BEFORE clauses of the policy (if any) is inserted.
Next, the object and the method arguments are restored on the stack.
If there are AFTER clauses in the policy for the instruction, first the return value (if any) is recorded in a local variable, the code compiled from the AFTER clauses is inlined, followed by code to restore the return value on the stack.
Finally, if there are EXCEP-TIONAL clauses for the instruction, an exception handler is created that covers only the method invocation instruction and catches all types of exceptions.
It is placed highest amongst the handlers for this label in the handler list, so that whenever the instruction throws an exception, this handler will be executed.
The code of this exception handler consists of code created for the related EXCEP-TIONAL clauses and ends by rethrowing the caught exception.
All (original) exception handlers of the program that cover the security relevant instruction are redirected to cover this last throw instruction instead.Due to virtual method call resolution, execution of an invocation instruction can give rise to different security relevant actions.
The inliner inserts code to resolve, at runtime, the signature of the method that is called, using the type of the object that the method is invoked on, and information on which methods have been overridden.
A check to compare this signature against the signature of the event mentioned in the clause is prepended to code compiled for the clause.
The blocks inlined above and at the exception handlers of security relevant instructions can be specified similarly.We claim that it is possible to devise an inliner in accordance with the description above.
Let I be such an inliner, and let I(T,P) denote the program T inlined by I for the policy P.
Our implementation of such an inliner is found at [2].
The following result shows that programs inlined for a policy contain a monitor as characterized by theorem 3, and that level II annotations can be efficiently completed to a "fully" annotated program for which annotation validity, and hence policy adherence, is decidable.
In the result, local validity refers to logical validity of the verification conditions resulting from a fully annotated program (see [4] for details).
An extended (or level III) annotation as referred to above can be obtained by: (a) annotating all non-inlined instructions with the synchronisation assertion − → gs = − → ms, (b) extending the annotation to inlined instructions by means of a syntactic weakest precondition function wp(M [L]) (as defined in [4]), and (c) collapsing every annotation to an equivalent single assertion (see [1] for details).
As a corollary of theorem 2 and the above result, every program inlined with the described inliner adheres to the policy it was inlined for.
Another corollary of theorem 4 is that the inlined program I(T,P) yields only method-local co-executions.
This is so since programs that validate level III annotations validate also level II annotations and thus theorem 3 applies to inlined programs.As a consequence, a level III annotation as described above can be used for on-device checking of inlining correctness in a proof-carrying code setting.
This extended abstract presents a specification language for security policies in terms of security automata, and a two-level class file annotation scheme in a Floyd-style program logic for Java bytecode, characterizing two key properties: (i) that a program adheres to a given policy, and (ii) that the program has an embedded method-compositional monitor for this policy.
The annotation scheme thus characterizes a whole class of monitor inliners.
As an application, we describe a concrete inliner and prove its correctness.
For this inliner, validity of the annotations can be decided efficiently using a weakest precondition annotation checker, thus allowing the annotation scheme to be used in a proof-carrying code setting for certifying monitor compliance.
This idea is currently being developed within the European S3MS project.Future effort will focus on generalizing the level II annotations by formulating suitable state abstraction functions to extend the present approach to programs that are not inlined but still self-monitoring.
Another interesting challenge is to extend the annotation framework to programs with threading.
