At IEEE Security & Privacy 2015, Bos, Costello, Naehrig, and Stebila proposed an instantiation of Peik-ert's ring-learning-with-errors-based (Ring-LWE) key-exchange protocol (PQCrypto 2014), together with an implementation integrated into OpenSSL, with the affirmed goal of providing post-quantum security for TLS.
In this work we revisit their instantiation and stand-alone implementation.
Specifically, we propose new parameters and a better suited error distribution, analyze the scheme's hardness against attacks by quantum computers in a conservative way, introduce a new and more efficient error-reconciliation mechanism, and propose a defense against backdoors and all-for-the-price-of-one attacks.
By these measures and for the same lattice dimension , we more than double the security parameter, halve the communication overhead, and speed up computation by more than a factor of 8 in a portable C implementation and by more than a factor of 27 in an optimized implementation targeting current Intel CPUs.
These speedups are achieved with comprehensive protection against timing attacks.
The last decade in cryptography has seen the birth of numerous constructions of cryptosystems based on lattice problems, achieving functionalities that were previously unreachable (e.g., fully homomorphic cryptogra- * This work was initiated while Thomas Pöppelmann was a Ph.D. student at Ruhr-University Bochum with support from the European Union H2020 SAFEcrypto project (grant no. 644729).
This work has furthermore been supported by TÜBITAK under 2214 phy [38]).
But even for the simplest tasks in asymmetric cryptography, namely public-key encryption, signatures, and key exchange, lattice-based cryptography offers an important feature: resistance to all known quantum algorithms.
In those times of quantum nervousness [73,74], the time has come for the community to deliver and optimize concrete schemes, and to get involved in the standardization of a lattice-based cipher-suite via an open process.For encryption and signatures, several competitive schemes have been proposed; examples are NTRU encryption [50,83], Ring-LWE encryption [67] as well as the signature schemes BLISS [31], PASS [48] or the proposal by Bai and Galbraith presented in [8].
To complete the lattice-based cipher-suite, Bos et al. [20] recently proposed a concrete instantiation of the key-exchange scheme of Peikert's improved version of the original protocol of Ding, Xie and Lin [52,77].
Bos et al. proved its practicality by integrating their implementation as additional cipher-suite into the transport layer security (TLS) protocol in OpenSSL.
In the following we will refer to this proposal as BCNS.Unfortunately, the performance of BCNS seemed rather disappointing.
We identify two main sources for this inefficiency.
First the analysis of the failure probability was far from tight, resulting in a very large modulus q ≈ 2 32 .
As a side effect, the security is also significantly lower than what one could achieve with Ring-LWE for a ring of rank n = 1024.
Second the Gaussian sampler, used to generate the secret parameters, is fairly inefficient and hard to protect against timing attacks.
This second source of inefficiency stems from the fundamental misconception that high-quality Gaussian noise is crucial for encryption based on LWE 1 , which has also made various other implementations [29,79] slower and more complex than they would have to be.
In this work, we propose solutions to the performance and security issues of the aforementioned BCNS proposal [20].
Our improvements are possible through a combination of multiple contributions:• Our first contribution is an improved analysis of the failure probability of the protocol.
To push the scheme even further, inspired by analog errorcorrecting codes, we make use of the lattice D 4 to allow error reconciliation beyond the original bounds of [77].
This drastically decreases the modulus to q = 12289 < 2 14 , which improves both efficiency and security.
• Our second contribution is a more detailed security analysis against quantum attacks.
We provide a lower bound on all known (or even presupposed) quantum algorithms solving the shortestvector problem (SVP), and deduce the potential performance of a quantum BKZ algorithm.
According to this analysis, our improved proposal provides 128 bits of post-quantum security with a comfortable margin.
• We furthermore propose to replace the almostperfect discrete Gaussian distribution by something relatively close, but much easier to sample, and prove that this can only affect the security marginally.
• We replace the fixed parameter a of the original scheme by a freshly chosen random one in each key exchange.
This incurs an acceptable overhead but prevents backdoors embedded in the choice of this parameter and all-for-the-price-of-one attacks.
• We specify an encoding of polynomials in the number-theoretic transform (NTT) domain which allows us to eliminate some of the NTT transformations inside the protocol computation.
• To demonstrate the applicability and performance of our design we provide a portable reference implementation written in C and a highly optimized vectorized implementation that targets recent Intel CPUs and is compatible with recent AMD CPUs.We describe an efficient approach to lazy reduction inside the NTT, which is based on a combination of Montgomery reductions and short Barrett reductions.Availability of software.
We place all software described in this paper into the public domain and make it available online at https://cryptojedi.
org/crypto/#newhope and https://github.com/ tpoeppelmann/newhope.
Full version of the paper.
The full version of this paper contains various appendices in addition to the material presented in this proceedings version.
The full version is available online at https://eprint.iacr.
org/2015/1092/ and at https://cryptojedi.org/ papers/#newhope.
Let Z be the ring of rational integers.
We define for an x ∈ R the rounding function 񮽙x񮽙 = 񮽙x + 1 2 񮽙 ∈ Z. Let Z q , for an integer q ≥ 1, denote the quotient ring Z/qZ.
We define R = Z[X]/(X n + 1) as the ring of integer polynomials modulo X n + 1.
By R q = Z q [X]/(X n + 1) we mean the ring of integer polynomials modulo X n + 1 where each coefficient is reduced modulo q.
In case χ is a probability distribution over R, then x $ ← χ means the sampling of x ∈ R according to χ.
When we write a $ ← R q this means that all coefficients of a are chosen uniformly at random from Z q .
For a probabilistic algorithm A we denote by y $ ← A that the output of A is assigned to y and that A is running with randomly chosen coins.
We recall the discrete Gaussian distribution D Z,σ which is parametrized by the Gaussian parameter σ ∈ R and defined by assigning a weight proportional to exp( −x 2 2σ 2 ) to all integers x.
In this section we briefly revisit the passively secure keyencapsulation mechanism (KEM) that was proposed by Peikert [77] and instantiated in [20] (BCNS).
Peikert's KEM scheme is defined by the algorithms (Setup, Gen, Encaps, Decaps) and after a successful protocol run both parties share an ephemeral secret key that can be used to protect further communication (see Protocol 1).
The KEM scheme by Peikert closely resembles a previously introduced Ring-LWE encryption scheme [66] but due to a new error-reconciliation mechanism, one R q component of the ciphertext can be replaced by a more compact element in R 2 .
This efficiency gain is possible due to the observation that it is not necessary to transmit an explicitly chosen key to establish a secure ephemeral session key.
In Peikert's scheme, the reconciliation just allows both parties to derive the session key from an approximately agreed pseudorandom ring element.
For Alice, this ring element is us = ass 񮽙 + e 񮽙 s and for Bob it is v = bs 񮽙 + e 񮽙񮽙 = ass 񮽙 + es 񮽙 + e 񮽙񮽙 .
For a full explanation of the reconciliation we refer to the original paper [77] but briefly recall the cross-rounding function 񮽙·· 2 defined as 񮽙v񮽙 2 := 񮽙 4 q · v񮽙 mod 2 and the randomized function dbl(v) := 2v − ¯ e for some random ¯ e where ¯ e = 0 with probability 1 2 , ¯ e = 1 with probability 1 4 , and ¯ e = −1 with probability 1 4 .
Let I 0 = {0, 1,.
.
.
,񮽙 q 2 񮽙 − 1},I 1 = {−− q 2 񮽙,.
.
.
,−1}, and E = [− q 4 , q 4 ) then the reconcil- iation function rec(w, b) is defined as rec(w, b) = 񮽙 0, if w ∈ I b + E ( mod q) 1, otherwise.If these functions are applied to polynomials this means they are applied to each of the coefficients separately.Parameters: q, n, χ KEM.Setup() : a $ ← R q Alice (server) Bob (client) KEM.Gen(a) : KEM.Encaps(a, b) : s, e $ ← χ s 񮽙 , e 񮽙 , e 񮽙񮽙 $ ← χ b←as + e b − → u←as 񮽙 + e 񮽙 v←bs 񮽙 + e 񮽙񮽙 ¯ v $ ← dbl(v) KEM.Decaps(s, (u, v 񮽙 )) : u,v 񮽙 ← − − v 񮽙 = 񮽙¯ v񮽙 2 µ←rec(2us, v 񮽙 ) µ←←¯ v񮽙 2Protocol 1: Peikert's KEM mechanism.
In a work by Bos, Costello, Naehrig, and Stebila [20] (BCNS), Peikert's KEM [77] was phrased as a keyexchange protocol (see again Protocol 1), instantiated for a concrete parameter set, and integrated into OpenSSL (see Section 8 for a performance comparison).
Selection of parameters was necessary as Peikert's original work does not contain concrete parameters and the security as well as error estimation are based on asymptotics.
The authors of [20] chose a dimension n = 1024, a modulus q = 2 32 − 1, χ = D Z,σ and the Gaussian parameter σ = 8/ √ 2π ≈ 3.192.
It is claimed that these parameters provide a classical security level of at least 128 bits considering the distinguishing attack [62] with distinguishing advantage less than 2 −128 and 2 81.9 bits of security against an optimistic instantiation of a quantum adversary.
The probability of a wrong key being established is less than 2 −2 17 = 2 −131072 .
The message b sent by Alice is a ring element and thus requires at least log 2 (q)n = 32 kbits while Bob's response (u, r) is a ring element R q and an element from R 2 and thus requires at least 33 kbits.
As the polynomial a ∈ R q is shared between all parties this ring element has to be stored or generated on-the-fly.
For timings of their implementation we refer to Table 2.
We would also like to note that besides its aim for securing classical TLS, the BCNS protocol has already been proposed as a building block for Tor [84] on top of existing elliptic-curve infrastructure [41].
In this section we detail our proposal and modifications of Peikert's protocol 2 .
For the same reasons as described in [20] we opt for an unauthenticated key-exchange protocol; the protection of stored transcripts against future decryption using quantum computers is much more urgent than post-quantum authentication.
Authenticity will most likely be achievable in the foreseeable future using proven pre-quantum signatures and attacks on the signature will not compromise previous communication.
Additionally, by not designing or instantiating a latticebased authenticated key-exchange protocol (see [33,85]) we reduce the complexity of the key-exchange protocol and simplify the choice of parameters.
We actually see it as an advantage to decouple key exchange and authentication as it allows a protocol designer to choose the optimal algorithm for both tasks (e.g., an ideal-latticebased key exchange and a hash-based signature like [16] for authentication).
Moreover, this way the design, security level, and parameters of the key-exchange scheme are not constrained by requirements introduced by the authentication part.Parameter choices.
A high-level description of our proposal is given in Protocol 2 and as in [20,77] all polynomials except for r ∈ R 4 are defined in the ring R q = Z q [X]/(X n + 1) with n = 1024 and q = 12289.
We decided to keep the dimension n = 1024 as in [20] to be able to achieve appropriate long-term security.
As polynomial arithmetic is fast and also scales better (doubling n roughly doubles the time required for a polynomial multiplication), our choice of n appears to be acceptable from a performance point of view.
We chose the modulus q = 12289 as it is the smallest prime for which it holds that q ≡ 1 mod 2n so that the number-theoretic transform (NTT) can be realized efficiently and that we can transfer polynomials in NTT encoding (see Section 7).
As the security level grows with the noise-to-modulus ratio, it makes sense to choose the modulus as small as possible, improving compactness and efficiency together with security.
The choice is also appealing as the prime is already used by some implementations of Ring-LWE encryption [29,63,81] and BLISS signatures [31,78]; thus sharing of some code (or hardware modules) between our proposal and an implementation of BLISS would be possible.Noise distribution and reconciliation.
Notably, we also change the distribution of the LWE secret and error and replace discrete Gaussians by the centered binomial distribution ψ k of parameter k = 16 (see Section 4).
The reason is that it turned out to be challenging to implement a discrete Gaussian sampler efficiently and protected against timing attacks (see [20] and Section 5).
On the other hand, sampling from the centered binomial distribution is easy and does not require high-precision computations or large tables as one may sample from ψ k by computing∑ k i=0 b i − b 񮽙 i ,where the b i , b 񮽙 i ∈ {0, 1} are uniform independent bits.
The distribution ψ k is centered (its mean is 0), has variance k/2 and for k = 16 this gives a standard deviation of ς = 񮽙 16/2.
Contrary to [20,77] we hash the output of the reconciliation mechanism, which makes a distinguishing attack irrelevant and allows us to argue security for the modified error distribution.
Moreover, we generalize Peikert's reconciliation mechanism using an analog error-correction approach (see Section 5).
The design rationale is that we only want to transmit a 256-bit key but have n = 1024 coefficients to encode data into.
Thus we encode one key bit into four coefficients; by doing so we achieve increased error resilience which in turn allows us to use larger noise for better security.Short-term public parameters.
NEWHOPE does not rely on a globally chosen public parameter a as the efficiency increase in doing so is not worth the measures that have to be taken to allow trusted generation of this value and the defense against backdoors [13].
Moreover, this approach avoids the rather uncomfortable situation that all connections rely on a single instance of a lattice problem (see Section 3) in the flavor of the "Logjam" DLP attack [1].
No key caching.
For ephemeral Diffie-Hellman keyexchange in TLS it is common for servers to cache a key pair for a short time to increase performance.
For example, according to [24], Microsoft's SChannel library caches ephemeral keys for 2 hours.
We remark that for the lattice-based key exchange described in [77], for the key exchange described in [20], and also for the key exchange described in this paper, such short-term caching would be disastrous for security.
Indeed, it is crucial that both parties use fresh secrets for each instantiation (thus the performance of the noise sampling is crucial).
As short-term key caching typically happens on higher layers of TLS libraries than the key-exchange implementation itself, we stress that particular care needs to be taken to eliminate such caching when switching from ephemeral (elliptic-curve) Diffie-Hellman key exchange to post-quantum lattice-based key exchange.
This issue is discussed in more detail in [32].
One could enable key caching with a transformation from the CPA-secure key exchange to a CCA-secure key exchange as outlined by Peikert in [77, Section 5].
Note that such a transform would furthermore require changes to the noise distribution to obtain a failure probability that is negligible in the cryptographic sense.3 Preventing backdoors and all-for-theprice-of-one attacksOne serious concern about the original design [20] is the presence of the polynomial a as a fixed system parameter.
As described in Protocol 2, our proposal includes pseudorandom generation of this parameter for every key exchange.
In the following we discuss the reasons for this decision.Backdoor.
In the worst scenario, the fixed parameter a could be backdoored.
For example, inspired by NTRU trapdoors [50,83], a dishonest authority may choose mildly small f, g such that f = g = 1 mod p for some prime p ≥ 4 · 16 + 1 and set a = gf −1 mod q. Then, given (a, b = as + e), the attacker can compute bf = afs + fe = gs + fe mod q, and, because g, s, f, e are small enough, compute gs + fe in Z.
One countermeasure against such backdoors is the "nothing-up-my-sleeve" process, which would, for example, choose a as the output of a hash function on a common universal string like the digits of π.
Yet, even this process may be partially abused [13], and when not strictly required it seems preferable to avoid it.All-for-the-price-of-one attacks.
Even if this common parameter has been honestly generated, it is still rather uncomfortable to have the security of all connections rely on a single instance of a lattice problem.
The scenario is an entity that discovers an unforeseen cryptanalytic algorithm, making the required lattice reduction still very costly, but say, not impossible in a year of computation, given its outstanding computational power.
By finding once a good enough basis of the lattice Λ = Parameters: q = 12289 < 2 14 , n = 1024 Error distribution: ψ 16 Alice (server) Bob (client)seed $ ← {0, 1} 256 a←Parse(SHAKE-128(seed)) s, e $ ← ψ n 16 s 񮽙 , e 񮽙 , e 񮽙񮽙 $ ← ψ n 16 b←as + e (b,seed) −−−−→ a←Parse(SHAKE-128(seed)) u←as 񮽙 + e 񮽙 v←bs 񮽙 + e 񮽙񮽙 v 񮽙 ←us (u,r) ←−− r $ ← HelpRec(v) ν←Rec(v 񮽙 , r) ν←Rec(v, r) µ←SHA3-256(ν) µ←SHA3-256(ν)Protocol 2: Our Scheme.
For the definitions of HelpRec and Rec see Section 5.
For the definition of encodings and the definition of Parse see Section 7.
{(a, 1)x + (q, 0)y|x, y ∈ R}, this entity could then compromise all communications, using for example Babai's decoding algorithm [7].
This idea of massive precomputation that is only dependent on a fixed parameter a and then afterwards can be used to break all key exchanges is similar in flavor to the 512-bit "Logjam" DLP attack [1].
This attack was only possible in the required time limit because most TLS implementations use fixed primes for DiffieHellman.
One of the recommended mitigations by the authors of [1] is to avoid fixed primes.Against all authority.
Fortunately, all those pitfalls can be avoided by having the communicating parties generate a fresh a at each instance of the protocol (as we propose).
If in practice it turns out to be too expensive to generate a for every connection, it is also possible to cache a on the server side 3 for, say a few hours without significantly weakening the protection against all-for-the-price-of-one attacks.
Additionally, the performance impact of generating a is reduced by sampling a uniformly directly in NTT format (recalling that the NTT is a one-to-one map), and by transferring only a short 256-bit seed for a (see Section 7).
A subtle question is to choose an appropriate primitive to generate a "random-looking" polynomial a out of a short seed.
For a security reduction, it seems to the authors that there is no way around the (nonprogrammable) random oracle model (ROM).
It is argued in [34] that such a requirement is in practice an overkill, and that any pseudorandom generator (PRG) should also work.
And while it is an interesting question how such a reasonable pseudo-random generator would interact with our lattice assumption, the cryptographic notion of a PRG is not helpful to argue security.
Indeed, it is an easy exercise 4 to build (under the NTRU assumption) a "backdoored" PRG that is, formally, a legitimate PRG, but that makes our scheme insecure.Instead, we prefer to base ourselves on a standard cryptographic hash-function, which is the typical choice of an "instantiation" of the ROM.
As a suitable option we see Keccak [19], which has recently been standardized as SHA3 in FIPS-202 [72], and which offers extendable-output functions (XOF) named SHAKE.
This avoids costly external iteration of a regular hash function and directly fits our needs.We use SHAKE-128 for the generation of a, which offers 128-bits of (post-quantum) security against collisions and preimage attacks.
With only a small performance penalty we could have also chosen SHAKE-256, but we do not see any reason for such a choice, in particular because neither collisions nor preimages lead to an attack against the proposed scheme.
On non-Gaussian errors.
In works like [20,29,81], a significant algorithmic effort is devoted to sample from a discrete Gaussian distribution to a rather high precision.
In the following we argue that such effort is not necessary and motivate our choice of a centered binomial ψ k as error distribution.Indeed, we recall that the original worst-case to average-case reductions for LWE [80] and Ring-LWE [67] state hardness for continuous Gaussian distributions (and therefore also trivially apply to rounded Gaussian, which differ from discrete Gaussians).
This also extends to discrete Gaussians [21] but such proofs are not necessarily intended for direct implementations.
We recall that the use of discrete Gaussians (or other distributions with very high-precision sampling) is only crucial for signatures [65] and lattice trapdoors [39], to provide zero-knowledgeness.
The following Theorem states that choosing ψ k as error distribution in Protocol 2 does not significantly decrease security compared to a rounded Gaussian distribution with the same standard deviation σ = 񮽙 16/2.
As explained in Section 6, our choice of parameters leaves a comfortable margin to the targeted 128 bits of post-quantum security, which accommodates for the slight loss in security indicated by Theorem 4.1.
Even more important from a practical point of view is that no known attack makes use of the difference in error distribution; what matters for attacks are entropy and standard deviation.Simple implementation.
We remark that sampling from the centered binomial distribution ψ 16 is rather trivial in hardware and software, given the availability of a uniform binary source.
Additionally, the implementation of this sampling algorithm is much easier to protect against timing attacks as no large tables or datadependent branches are required (cf. to the issues caused by the table-based approach used in [20]).
In most of the literature, Ring-LWE encryption allows to encrypt one bit per coordinate of the ciphertext.
It is also well known how to encrypt multiple bits per coordinate by using a larger modulus-to-error ratio (and therefore decreasing the security for a fixed dimension n).
However, in the context of exchanging a symmetric key (of, say, 256 bits), we end up having a message space larger(0, 0) (1, 0) (0, 1) (1, 1) ( 1 2 , 1 2 )Figure 1: The lattice˜Dlattice˜ lattice˜D 2 with Voronoi cells than necessary and thus want to encrypt one bit in multiple coordinates.In [79] Pöppelmann and Güneysu introduced a technique to encode one bit into two coordinates, and verified experimentally that it led to a better error tolerance.
This allows to either increase the error and therefore improve the security of the resulting scheme or to decrease the probability of decryption failures.
In this section we propose a generalization of this technique in dimension 4.
We start with an intuitive description of the approach in 2 dimensions and then explain what changes in 4 dimensions.
Appendices C and D in the full version of this paper give a thorough mathematical description together with a rigorous analysis.Let us first assume that both client and server have the same vector x ∈ [0, 1) 2 ⊂ R 2 and want to map this vector to a single bit.
Mapping polynomial coefficients from {0,.
.
.
,q − 1} to [0, 1) is easily accomplished through a division by q.
Now consider the lattice˜Dlattice˜ lattice˜D 2 with basis {(0, 1), ( 1 2 , 1 2 )}.
∈ ˜ D 2 .
If v = ( 1 2 , 1 2 ) (i.e., x is in the grey Voronoi cell), then the output bit is 1; if v ∈ {(0, 0), (0, 1), (1, 0), (1, 1)} (i.e., x is in a white Voronoi cell) then the output bit is 0.
This map may seem like a fairly complex way to map from a vector to a bit.
However, recall that client and server only have a noisy version of x, i.e., the client has a vector x c and the server has a vector x s .
Those two vectors are close, but they are not the same and can be on different sides of a Voronoi cell border.Error reconciliation.
The approach described above now allows for an efficient solution to solve this agreement-from-noisy-data problem.
The idea is that one of the two participants (in our case the client) sends as a reconciliation vector the difference of his vector x c( 1 2 , 1 2 )Figure 2: Splitting of the Voronoi cell of ( 1 2 , 1 2 ) into 2 rd = 16 sub-cells, some with their corresponding difference vector to the center and the center of its Voronoi cell (i.e., the point in the lattice).
The server adds this difference vector to x s and thus moves away from the border towards the center of the correct Voronoi cell.
Note that an eavesdropper does not learn anything from the reconciliation information: the client tells the difference to a lattice point, but not whether this is a lattice point producing a zero bit or a one bit.This approach would require sending a full additional vector; we can reduce the amount of reconciliation information through r-bit discretization.
The idea is to split each Voronoi cell into 2 dr sub-cells and only send in which of those sub-cells the vector x c is.
Both participants then add the difference of the center of the sub-cell and the lattice point.
This is illustrated for r = 2 and d = 2 in Figure 2.
Blurring the edges.
Figure 1 may suggest that the probability of x being in a white Voronoi cell is the same as for x being in the grey Voronoi cell.
This would be the case if x actually followed a continuous uniform distribution.
However, the coefficients of x are discrete values in {0, 1 q ,.
.
.
, q−1 q } and with the protocol described so far, the bits of ν would have a small bias.
The solution is to add, with probability 1 2 , the vector ( 1 2q , 1 2q ) to x before running the error reconciliation.
This has close to no effect for most values of x, but, with probability 1 2 moves x to another Voronoi cell if it is very close to one side of a border.
Appendix E in the full version of this paper gives a graphical intuition for this trick in two dimensions and with q = 9.
The proof that it indeed removes all biases in the key is given in Lemma C.2.
in the full version of this paper.From 2 to 4 dimensions.
When moving from the 2-dimensional case considered above to the 4-dimensional case used in our protocol, not very much needs to change.
The lattice˜Dlattice˜ lattice˜D 2 becomes the lattice˜Dlattice˜ lattice˜D 4 with basis B = (u 0 , u 1 , u 2 , g), where u i are the canonical basis vectors ofZ 4 and g t = 񮽙 1 2 , 1 2 , 1 2 , 1 2񮽙 .
The lattice˜Dlattice˜ lattice˜D 4 is a rotated and scaled version of the root lattice D 4 .
The Voronoi cells of this lattice are no longer 2-dimensional "diamonds", but 4-dimensional objects called icositetrachoron or 24-cells [61].
Determining in which cell a target point lies in is done using the closest vector algorithm CVP˜DCVP˜ CVP˜D 4, and a simplified version of it, which we call Decode, gives the result modulo Z 4 .
As in the 2-dimensional illustration in Figure 2, we are using 2-bit discretization; we are thus sending r · d = 8 bits of reconciliation information per key bit.Putting all of this together, we obtain the HelpRec function to compute the r-bit reconciliation information asHelpRec(x; b) = CVP˜DCVP˜ CVP˜D 4 񮽙 2 r q (x + bg) 񮽙 mod 2 r ,where b ∈ {0, 1} is a uniformly chosen random bit.
The corresponding function Rec(x, r) = Decode( 1 qx − 1 2 r Br) computes one key bit from a vector x with 4 coefficients in Z q and a reconciliation vector r ∈ {0, 1, 2, 3} 4 .
The algorithms CVP˜DCVP˜ CVP˜D 4 and Decode are listed as Algorithm 1 and Algorithm 2, respectively.Algorithm 1 CVP˜DCVP˜ CVP˜D 4 (x ∈ R 4 )Ensure: An integer vector z such that Bz is a closest vector tox: x − Bz ∈ V 1: v 0 ←←x񮽙 2: v 1 ←←x − g񮽙 3: k←(񮽙x − v 0 񮽙 1 < 1) ?
0 : 1 4: (v 0 , v 1 , v 2 , v 3 ) t ←v k 5: return (v 0 , v 1 , v 2 , k) t + v 3 · (−1, −1, −1, 2) t Algorithm 2 Decode(x ∈ R 4 /Z 4 )Ensure: A bit k such that kg is a closest vector to x+Z 4 :x − kg ∈ V + Z 4 1: v = x − −x񮽙 2: return 0 if 񮽙v񮽙 1 ≤ 1 and 1 otherwise Finally it remains to remark that even with this reconciliation mechanism client and server do not always agree on the same key.
Lemma D in the full version of this paper.
provides a detailed analysis of the failure probability of the key agreement and shows that it is smaller than 2 −60 .
In [20] the authors chose Ring-LWE for a ring of rank n = 1024, while most previous instantiations of the Ring-LWE encryption scheme, like the ones in [29,42,63,79], chose substantially smaller rank n = 256 or n = 512.
It is argued that it is unclear if dimension 512 can offer post-quantum security.
Yet, the concrete post-quantum security of LWE-based schemes has not been thoroughly studied, as far as we know.
In this section we propose such a (very pessimistic) concrete analysis.
In particular, our analysis reminds us that the security depends as much on q and its ratio with the error standard deviation ς as it does on the dimension n.
That means that our effort of optimizing the error recovery and its analysis not only improves efficiency but also offers superior security.Security level over-shoot?
With all our improvements, it would be possible to build a scheme with n = 512 (and k = 24, q = 12289) and to obtain security somewhat similar to the one of [20,42], and therefore further improve efficiency.
We call this variant JARJAR and details are provided in Appendix A of the full version of this paper.
Nevertheless, as history showed us with RSA-512 [28], the standardization and deployment of a scheme awakens further cryptanalytic effort.
In particular, NEWHOPE could withstand a dimension-halving attack in the line of [36, Sec 8.8.1] based on the GentrySzydlo algorithm [40,60] or the subfield approach of [2].
Note that so far, such attacks are only known for principal ideal lattices or NTRU lattices, and there are serious obstructions to extend them to Ring-LWE, but such precaution seems reasonable until lattice cryptanalysis stabilizes.We provide the security and performance analysis of JARJAR in Appendix A of the full version of this paper mostly for comparison with other lower-security proposals.
We strongly recommend NEWHOPE for any immediate applications, and advise against using JARJAR until concrete cryptanalysis of lattice-based cryptography is better understood.
We analyze the hardness of Ring-LWE as an LWE problem, since, so far, the best known attacks do not make use of the ring structure.
There are many algorithms to consider in general (see the survey [3]), yet many of those are irrelevant for our parameter set.
In particular, because there are only m = n samples available one may rule out BKW types of attacks [53] and linearization attacks [6].
This essentially leaves us with two BKZ [26,82] attacks, usually referred to as primal and dual attacks that we will briefly recall below.The algorithm BKZ proceeds by reducing a lattice basis using an SVP oracle in a smaller dimension b.
It is known [47] that the number of calls to that oracle remains polynomial, yet concretely evaluating the number of calls is rather painful, and this is subject to new heuristic ideas [25,26].
We choose to ignore this polynomial factor, and rather evaluate only the core SVP hardness, that is the cost of one call to an SVP oracle in dimension b, which is clearly a pessimistic estimation (from the defender's point of view).
Typical implementations [23,26,35] use an enumeration algorithm as this SVP oracle, yet this algorithm runs in super-exponential time.
On the other hand, the sieve algorithms are known to run in exponential time, but are so far slower in practice for accessible dimensions b ≈ 130.
We choose the latter to predict the core hardness and will argue that for the targeted dimension, enumerations are expected to be greatly slower than sieving.Quantum sieve.
A lot of recent work has pushed the efficiency of the original lattice sieve algorithms [69,75], improving the heuristic complexity from (4/3) b+o(b) ≈ 2 0.415b down to 񮽙 3/2 b+o(b) ≈ 2 0.292b (see [10,55]).
The hidden sub-exponential factor is known to be much greater than one in practice, so again, estimating the cost ignoring this factor leaves us with a significant pessimistic margin.
Most of those algorithms have been shown [54,56] to benefit from Grover's quantum search algorithm, bringing the complexity down to 2 0.265b .
It is unclear if further improvements are to be expected, yet, because all those algorithms require classically building lists of size 񮽙 4/3 b+o(b) ≈ 2 0.2075b , it is very plausible that the best quantum SVP algorithm would run in time greater than 2 0.2075b .
Irrelevance of enumeration for our analysis.
In [26], predictions of the cost of solving SVP classically using the most sophisticated heuristic enumeration algorithms are given.
For example, solving SVP in dimension 100 requires visiting about 2 39 nodes, and 2 134 nodes in dimension 250.
Because this enumeration is a backtracking algorithm, it does benefit from the recent quasiquadratic speedup [70], decreasing the quantum cost to about at least 2 20 to 2 67 operations as the dimension increases from 100 to 250.
On the other hand, our best-known attack bound 2 0.265b gives a cost of 2 66 in dimension 250, and the best plausible attack bound 2 0.2075b ≈ 2 39 .
Because enumeration is super-exponential (both in theory and practice), its cost will be worse than our bounds in dimension larger than 250 and we may safely ignore this kind of algorithm.
5 The primal attack consists of constructing a unique-SVP instance from the LWE problem and solving it using BKZ.
We examine how large the block dimension b is required to be for BKZ to find the unique solution.
Given the matrix LWE instance (A, b = As + e) one builds the lattice Λ = {x ∈ Z m+n+1 : (A| − I m | − b)x = 0 mod q} of dimension d = m + n + 1, volume q m , and with a unique-SVP solution v = (s, e, 1) of norm λ ≈ ς √ n + m. Note that the number of used samples m may be chosen between 0 and 2n in our case and we numerically optimize this choice.Success condition.
We model the behavior of BKZ using the geometric series assumption (which is known to be optimistic from the attacker's point of view), that finds a basis whose Gram-Schmidt norms are given by [3,25].
The unique short vector v will be detected if the projection of v onto the vector space spanned by the last b Gram-Schmidt vectors is shorter than b 񮽙 d−b .
Its projected norm is expected to be ς √ b, that is the attack is successful if and only if񮽙b 񮽙 i 񮽙 = δ d−2i−1 · Vol(Λ) 1/d where δ = ((πb) 1/b · b/2πe) 1/2(b−1)ς √ b ≤ δ 2b−d−1 · q m/d .
(1) The dual attack consists of finding a short vector in the dual lattice w ∈ Λ 񮽙 = {(x, y) ∈ Z m ×Z n : A t x = y mod q}.
Assume we have found a vector (x, y) of length 񮽙 and compute z = v t · b = v t As + v t e = w t s + v t e mod q which is distributed as a Gaussian of standard deviation 񮽙ς if (A, b) is indeed an LWE sample (otherwise it is uniform mod q).
Those two distributions have maximal variation distance bounded by 6 ε = 4 exp(−2π 2 τ 2 ) where τ = 񮽙ς /q, that is, given such a vector of length 񮽙 one has an advantage ε against decision-LWE.
The length 񮽙 of a vector given by the BKZ algorithm is given by 񮽙 = 񮽙b 0 񮽙.
Knowing that Λ 񮽙 has dimension d = m + n and volume q n we get 񮽙 = δ d−1 q n/d .
Therefore, obtaining an ε-distinguisher requires running BKZ with block dimension b where− 2π 2 τ 2 ≥ ln(ε/4).
(2)Note that small advantages ε are not relevant since the agreed key is hashed: an attacker needs an advantage of at least 1/2 to significantly decrease the search space of the agreed key.
He must therefore amplify his success 6 A preliminary version of this paper contained a bogus formula for ε leading to under-estimating the cost of the dual attack.
Correcting this formula leads to better security claim, and almost similar cost for the primal and dual attacks.
Table 1: Core hardness of NEWHOPE and JARJAR and selected other proposals from the literature.
The value b denotes the block dimension of BKZ, and m the number of used samples.
Cost is given in log 2 and is the smallest cost for all possible choices of m and b. Note that our estimation is very optimistic about the abilities of the attacker so that our result for the parameter set from [20] does not indicate that it can be broken with ≈ 2 80 bit operations, given today's state-of-the-art in cryptanalysis.
probability by building about 1/ε 2 many such short vectors.
Because the sieve algorithms provide 2 0.2075b vectors, the attack must be repeated at least R times whereR = max(1, 1/(2 0.2075b ε 2 )).
This makes the conservative assumption that all the vectors provided by the Sieve algorithm are as short as the shortest one.
According to our analysis, we claim that our proposed parameters offer at least (and quite likely with a large margin) a post-quantum security of 128 bits.
The cost of the primal attack and dual attacks (estimated by our script scripts/PQsecurity.py) are given in Table 1.
For comparison we also give a lower bound on the security of [20] and do notice a significantly improved security in our proposal.
Yet, because of the numerous pessimistic assumption made in our analysis, we do not claim any quantum attacks reaching those bounds.
Most other RLWE proposals achieve considerably lower security than NEWHOPE; for example, the highestsecurity parameter set used for RLWE encryption in [42] is very similar to the parameters of JARJAR.
The situation is different for NTRUENCRYPT, which has been instantiated with parameters that achieve about 128 bits of security according to our analysis 7 .
Specifically, we refer to NTRUENCRYPT with n = 743 as suggested in [49].
A possible advantage of NTRUENCRYPT compared to NEWHOPE is somewhat smaller message sizes, however, this advantage becomes very small when scaling parameters to achieve a similar security margin as NEWHOPE.
The large downside of using NTRUENCRYPT for ephemeral key exchange is the cost for key generation.
The implementation of NTRUENCRYPT with n = 743 in eBACS [17] takes about an order of magnitude longer for key generation alone than NEWHOPE takes in total.
Also, unlike our NEWHOPE software, this NTRUENCRYPT software is not protected against timing attacks; adding such protection would presumably incur a significant overhead.
In this section we provide details on the encodings of messages and describe our portable reference implementation written in C, as well as an optimized implementation targeting architectures with AVX vector instructions.
The key-exchange protocol described in Protocol 1 and also our protocol as described in Protocol 2 exchange messages that contain mathematical objects (in particular, polynomials in R q ).
Implementations of these protocols need to exchange messages in terms of byte arrays.
As we will describe in the following, the choice of encodings of polynomials to byte arrays has a serious impact on performance.
We use an encoding of messages that is particularly well-suited for implementations that make use of quasi-linear NTT-based polynomial multiplication.Definition of NTT and NTT −1 .
The NTT is a tool commonly used in implementations of ideal lattice-based cryptography [29,42,63,79].
For some background on the NTT and the description of fast software implementations we refer to [46,68].
In general, fast quasi-logarithmic algorithms exist for the computation of the NTT and a polynomial multiplication can be performed by computing c = NTT −1 (NTT(a) • NTT(b)) for a, b, c ∈ R.
An NTT targeting ideal lattices defined in R q = Z q [X]/(X n + 1) can be implemented very efficiently if n is a power of two and q is a prime for which it holds that q ≡ 1 mod 2n.
This way a primitive n-th root of unity ω and its square root γ exist.
By multiplying coefficient-wise by powers of γ = √ ω mod q before neous Ring-LWE instance.
We do not take into account the combinatorial vulnerabilities [51] induced by the fact that secrets are ternary.
We note that NTRU is a potentially a weaker problem than Ring-LWE: it is in principle subject to a subfield-lattice attack [2], but the parameters proposed for NTRUENCRYPT are immune.the NTT computation and after the reverse transformation by powers of γ −1 , no zero padding is required and an n-point NTT can be used to transform a polynomial with n coefficients.
For a polynomial g = ∑ 1023 i=0 g i X i ∈ R q we defineNTT(g) = ˆ g = 1023 ∑ i=0ˆg i=0ˆ i=0ˆg i X i , with ˆ g i = 1023 ∑ j=0 γ j g j ω i j ,where we fix the n-th primitive root of unity to ω = 49 and thus γ = √ ω = 7.
Note that in our implementation we use an in-place NTT algorithm which requires bitreversal operations.
As an optimization, our implementations skips these bit-reversals for the forward transformation as all inputs are only random noise.
This optimization is transparent to the protocol and for simplicity omitted in the description here.The function NTT −1 is the inverse of the function NTT.
The computation of NTT −1 is essentially the same as the computation of NTT, except that it uses ω −1 mod q = 1254, multiplies by powers of γ −1 mod q = 8778 after the summation, and also multiplies each coefficient by the scalar n −1 mod q = 12277 so thatNTT −1 (ˆ g) = g = 1023 ∑ i=0 g i X i , with g i = n −1 γ −i 1023 ∑ j=0ˆg j=0ˆ j=0ˆg j ω −i j .
The inputs to NTT −1 are not just random noise, so inside NTT −1 our software has to perform the initial bit reversal, making NTT −1 slightly more costly than NTT.Definition of Parse.
The public parameter a is generated from a 256-bit seed through the extendable-output function SHAKE-128 [72, Sec. 6.2].
The output of SHAKE-128 is considered as an array of 16-bit, unsigned, littleendian integers.
Each of those integers is used as a coefficient of a if it is smaller than 5q and rejected otherwise.
The first such 16-bit integer is used as the coefficient of X 0 , the next one as coefficient of X 1 and so on.
Earlier versions of this paper described a slightly different way of rejection sampling for coefficients of a.
The more efficient approach adopted in this final version was suggested independently by Gueron and Schlieker in [45] and by Yawning Angel in [5].
However, note that a reduction modulo q of the coefficients of a as described in [45] and [5] is not necessary; both our implementations can handle coefficients of a in {0,.
.
.
,5q − 1}.
Due to a small probability of rejections, the amount of output required from SHAKE-128 depends on the seed -what is required is n = 1024 coefficients that are smaller than 5q.
The minimal amount of output is thus 2 KB; the average amount is ≈ 2184.5 bytes.
The resulting polynomial a (denoted asâasˆasâ) is considered to be in NTT domain.
This is possible because the NTT transforms uniform noise to uniform noise.Using a variable amount of output from SHAKE-128 leaks information about a through timing information.
This is not a problem for most applications, since a is public.
As pointed out by Burdges in [22], such a timing leak of public information can be a problem when deploying NEWHOPE in anonymity networks like Tor.
Appendix F in the full version of this paper describes an alternative approach for Parse, which is slightly more complex and slightly slower, but does not leak any timing information about a.The message format of (b, seed) and (u, r).
With the definition of the NTT, we can now define the format of the exchanged messages.
In both (b, seed) and (u, r) the polynomial is transmitted in the NTT domain (as in works like [79,81]).
Polynomials are encoded as an array of 1792 bytes, in a compressed little-endian format.
The encoding of seed is straight-forward as an array of 32 bytes, which is simply concatenated with the encoding of b. Also the encoding of r is fairly straight-forward: it packs four 2-bit coefficients into one byte for a total of 256 bytes, which are again simply concatenated with the encoding of u.
We denote these encodings to byte arrays as encodeA and encodeB and their inverses as decodeA and decodeB.
For a description of our keyexchange protocol including encodings and with explicit NTT and NTT −1 transformations, see Protocol 3.
This paper is accompanied by a C reference implementation described in this section and an optimized implementation for Intel and AMD CPUs described in the next section.
The main emphasis in the C reference implementation is on simplicity and portability.
It does not use any floating-point arithmetic and outside the Keccak (SHA3-256 and SHAKE-128) implementation only needs 16-bit and 32-bit integer arithmetic.
In particular, the error-recovery mechanism described in Section 5 is implemented with fixed-point (i.e., integer-) arithmetic.
Furthermore, the C reference implementation does not make use of the division operator (/) and the modulo operator (%).
The focus on simplicity and portability does not mean that the implementation is not optimized at all.
On the contrary, we use it to illustrate various optimization techniques that are helpful to speed up the key exchange and are also of independent interest for implementers of other ideal-lattice-based schemes.NTT optimizations.
All polynomial coefficients are represented as unsigned 16-bit integers.
Our in-place NTT implementation transforms from bit-reversed to natural order using Gentleman-Sande butterfly operations [27,37].
One would usually expect that each NTT is preceded by a bit-reversal, but all inputs to NTT are noise polynomials that we can simply consider as being already bit-reversed; as explained earlier, the NTT −1 operation still involves a bit-reversal.
The core of the NTT and NTT −1 operation consists of 10 layers of transformations, each consisting of 512 butterfly operations of the form described in Listing 2.
Montgomery arithmetic and lazy reductions.
The performance of operations on polynomials is largely determined by the performance of NTT and NTT −1 .
The main computational bottleneck of those operations are 5120 butterfly operations, each consisting of one addition, one subtraction and one multiplication by a precomputed constant.
Those operations are in Z q ; recall that q is a 14-bit prime.
To speed up the modular-arithmetic operations, we store all precomputed constants in Montgomery representation [71] with R = 2 18 , i.e., instead of storing ω i , we store 2 18 ω i (mod q).
After a multiplication of a coefficient g by some constant 2 18 ω i , we can then reduce the result r to gω i (mod q) with the fast Montgomery reduction approach.
In fact, we do not always fully reduce modulo q, it is sufficient if the result of the reduction has at most 14 bits.
The fast Montgomery reduction routine given in Listing 1a computes such a reduction to a 14-bit integer for any unsigned 32-bit integer in {0,.
.
.
,2 32 − q(R − 1) − 1}.
Note that the specific implementation does not work for any 32-bit integer; for example, for the input 2 32 − q(R − 1) = 1073491969 the addition a=a+u causes an overflow and the function returns 0 instead of the correct result 4095.
In the following we establish that this is not a problem for our software.Aside from reductions after multiplication, we also need modular reductions after addition.
For this task we use the "short Barrett reduction" [9] detailed in Listing 1b.
Again, this routine does not fully reduce modulo q, but reduces any 16-bit unsigned integer to an integer of at most 14 bits which is congruent modulo q.In the context of the NTT and NTT −1 , we make sure that inputs have coefficients of at most 14 bits.
This allows us to avoid Barrett reductions after addition on every second level, because coefficients grow by at most one bit per level and the short Barrett reduction can handle 16-bit inputs.
Let us turn our focus to the input of the Montgomery reduction (see Listing 2).
Before subtracting a[j+d] from t we need to add a multiple of q to avoid unsigned underflow.
Coefficients never grow larger than 15 bits and 3 · q = 36867 > 2 15 , so adding 3 · q is sufficient.
An upper bound on the Parameters: q = 12289 < 2 14 , n = 1024 Error distribution: ψ n 16 Alice (server) Bob (client)seed $ ← {0,.
.
.
,255} 32â←Parse 32ˆ32â←Parse(SHAKE-128(seed)) s, e $ ← ψ n 16 s 񮽙 , e 񮽙 , e 񮽙񮽙 $ ← ψ n 16ˆs←NTT 16ˆ 16ˆs←NTT(s) ˆ b←â • ˆ s + NTT(e) m a =encodeA(seed, ˆ b) − −−−−−−−−−−− → 1824 Bytes ( ˆ b, seed)←decodeA(m a ) ˆ a←Parse(SHAKE-128(seed)) ˆ t←NTT(s 񮽙 ) ˆ u←â • ˆ t + NTT(e 񮽙 ) v←NTT −1 ( ˆ b • ˆ t) + e 񮽙񮽙 ( ˆ u, r)←decodeB(m b ) m b =encodeB( ˆ u,r) ← −−−−−−−−− − 2048 Bytes r $ ← HelpRec(v) v 񮽙 ←NTT −1 ( ˆ u • ˆ s) ν←Rec(v, r) ν←Rec(v 񮽙 , r) µ←SHA3-256(ν) µ←SHA3-256(ν)Protocol 3: Our proposed protocol including NTT and NTT −1 computations and sizes of exchanged messages; • denotes pointwise multiplication; elements in NTT domain are denoted with a hat (ˆ) expression ((uint32_t)t + 3*12289 -a [j+d]) is obtained if t is 2 15 − 1 and a[j+d] is zero; we thus obtain 2 15 + 3 · q = 69634.
All precomputed constants are in {0,.
.
.
,q − 1}, so the expression (W * ((uint32_t)t + 3*12289 -a [j+d]), the input to the Montgomery reduction, is at most 69634 · (q − 1) = 855662592 and thus safely below the maximum input that the Montgomery reduction can handle.Listing 1 Reduction routines used in the reference implementation.
(a) Montgomery reduction (R = 2 18 ).
cak permutation and slightly modified code taken from the "TweetFIPS202" implementation [18] for everything else.The sampling of centered binomial noise polynomials is based on a fast PRG with a random seed from /dev/urandom followed by a quick summation of 16-bit chunks of the PRG output.
Note that the choice of the PRG is a purely local choice that every user can pick independently based on the target hardware architecture and based on routines that are available anyway (for example, for symmetric encryption following the key exchange).
Our C reference implementation uses ChaCha20 [12], which is fast, trivially protected against timing attacks, and is already in use by many TLS clients and servers [57,58].
Intel processors since the "Sandy Bridge" generation support Advanced Vector Extensions (AVX) that operate on vectors of 8 single-precision or 4 double-precision floating-point values in parallel.
With the introduction of the "Haswell" generation of CPUs, this support was extended also to 256-bit vectors of integers of various sizes (AVX2).
It is not surprising that the enormous computational power of these vector instructions has been used before to implement very high-speed crypto (see, for example, [14,16,43]) and also our optimized reference implementation targeting Intel Haswell processors uses those instructions to speed up multiple components of the key exchange.NTT optimizations.
The AVX instruction set has been used before to speed up the computation of lattice-based cryptography, and in particular the number-theoretic transform.
Most notably, Güneysu, Oder, Pöppelmann and Schwabe achieve a performance of only 4 480 cycles for a dimension-512 NTT on Intel Sandy Bridge [46].
For arithmetic modulo a 23-bit prime, they represent coefficients as double-precision integers.We experimented with multiple different approaches to speed up the NTT in AVX.
For example, we vectorized the Montgomery arithmetic approach of our C reference implementation and also adapted it to a 32-bitsigned-integer approach.
In the end it turned out that floating-point arithmetic beats all of those more sophisticated approaches, so we are now using an approach that is very similar to the approach in [46].
One computation of a dimension-1024 NTT takes 8 448 cycles, unlike the numbers in [46] this does include multiplication by the powers of γ and unlike the numbers in [46], this excludes a bit-reversal.
Fast sampling.
Intel Haswell processors support the AES-NI instruction set and for the local choice of noise sampling it is obvious to use those.
More specifically, we use the public-domain implementation of AES-256 in counter mode written by Dolbeau, which is included in the SUPERCOP benchmarking framework [17].
Transformation from uniform noise to the centered binomial is optimized in AVX2 vector instructions operating on vectors of bytes and 16-bit integers.For the computation of SHAKE-128 we use the same code as in the C reference implementation.
One might expect that architecture-specific optimizations (for example, using AVX instructions) are able to offer significant speedups, but the benchmarks of the eBACS project [17] indicate that on Intel Haswell, the fastest implementation is the "simple" implementation by Van Keer that our C reference implementation is based on.
The reasons that vector instructions are not very helpful for speeding up SHAKE (or, more generally, Keccak) are the inherently sequential nature and the 5 × 5 dimension of the state matrix that makes internal vectorization hard.Error recovery.
The 32-bit integer arithmetic used by the C reference implementation for HelpRec and Rec is trivially 8-way parallelized with AVX2 instructions.
With this vectorization, the cost for HelpRec is only 3 404 cycles, the cost for Rec is only 2 804 cycles.
In the following we present benchmark results of our software.
All benchmark results reported in Table 2 were obtained on an Intel Core i7-4770K (Haswell) running at 3491.953 MHz with Turbo Boost and Hyperthreading disabled.
We compiled our C reference implementation with gcc-4.9.2 and flags -O3 -fomit-frame-pointer -march=corei7-avx -msse2avx.
We compiled our optimized AVX implementation with clang-3.5 and flags -O3 -fomit-frame-pointer -march=native.As described in Section 7, the sampling of a is not running in constant time; we report the median running time and (in parentheses) the average running time for this generation, the server-side key-pair generation and client-side shared-key computation; both over 1000 runs.
For all other routines we report the median of 1000 runs.
We built the software from [20] on the same machine as ours and-like the authors of [20]used openssl speed for benchmarking their software and converted the reported results to approximate cycle counts as given in Table 2.
Comparison with BCNS and RSA/ECDH.
As previously mentioned, the BCNS implementation [20] also uses the dimension n = 1024 but the larger modulus q = 2 32 − 1 and the Gaussian error distribution with Gaussian parameter σ = 8/ √ 2π = 3.192.
When the authors of BCNS integrated their implementation into SSL it only incurred a slowdown by a factor of 1.27 compared to ECDH when using ECDSA signatures and a factor of 1.08 when using RSA signatures with respect to the number of connections that could be handled by the server.
As a reference, the reported cycle counts in [20] for a nistp256 ECDH on the client side are 2 160 000 cycles (0.8 ms @2.77 GHz) and on the server side 3 221 288 cycles (1.4 ms @2.33 GHz).
These numbers are obviously not state of the art for ECDH software.
Even on the nistp256 curve, which is known to be a far-fromoptimal choice, it is possible to achieve cycle counts of less than 300 000 cycles for a variable-basepoint scalar multiplication on an Intel Haswell [44].
Also OpenSSL optionally includes fast software for nistp256 ECDH by Käsper and Langley and we assume that the authors of [20] omitted enabling it.
Compared to BCNS, our C implementation is more than 8 times faster and our AVX implementation even achieves a speedup factor of more than 27.
At this performance it is in the same ballpark as state-of-the-art ECDH software, even when TLS switches to faster 128-bit secure ECDH key exchange a Includes reading a seed from /dev/urandom b Includes one bit reversal c Excludes reading a seed from /dev/urandom, which is shared across multiple calls to the noise generation based on Curve25519 [11], as recently specified in RFC 7748 [59].
In comparison to the BCNS proposal we see a large performance advantage from switching to the binomial error distribution.
The BCNS software uses a large precomputed table to sample from a discrete Gaussian distribution with a high precision.
This approach takes 1 042 700 cycles to samples one polynomial in constant time.
Our C implementation requires only 32 684 cycles to sample from the binomial distribution.
Another factor is that we use the NTT in combination with a smaller modulus.
Polynomial multiplication in [20] is using Nussbaumer's symbolic approach based on recursive negacyclic convolutions [76].
The implementation in [20] only achieves a performance of 342 800 cycles for a constant-time multiplication.
Additionally, the authors of [20] did not perform pre-transformation of constants (e.g., a) or transmission of coefficients in FFT/Nussbaumer representation.Follow-Up Work.
We would like to refer the reader to follow-up work in which improvements to NEWHOPE and its implementation were proposed based on a preprint version of this work [4].
In [45] Gueron and Schlieker introduce faster pseudorandom bytes generation by changing the underlying functions, a method to decrease the rejection rate during sampling (see Section 7.1), and a vectorization of the sampling step.
Longa and Naehrig [64] optimize the NTT and present new modular reduction techniques and are able to achieve a speedup of factor-1.90 for the C implementation and a factor-1.25 for the AVX implementation compared to the preprint [4] (note that this version has updated numbers).
