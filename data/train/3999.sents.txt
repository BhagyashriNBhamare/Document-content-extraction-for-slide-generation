Can bits of an RSA public key leak information about design and implementation choices such as the prime generation algorithm?
We analysed over 60 million freshly generated key pairs from 22 open-and closed-source libraries and from 16 different smartcards, revealing significant leakage.
The bias introduced by different choices is sufficiently large to classify a probable library or smartcard with high accuracy based only on the values of public keys.
Such a classification can be used to decrease the anonymity set of users of anonymous mail-ers or operators of linked Tor hidden services, to quickly detect keys from the same vulnerable library or to verify a claim of use of secure hardware by a remote party.
The classification of the key origins of more than 10 million RSA-based IPv4 TLS keys and 1.4 million PGP keys also provides an independent estimation of the libraries that are most commonly used to generate the keys found on the Internet.
Our broad inspection provides a sanity check and deep insight regarding which of the recommendations for RSA key pair generation are followed in practice, including closed-source libraries and smartcards 1 .
The RSA key pair generation process is a crucial part of RSA algorithm usage, and there are many existing (and sometimes conflicting) recommendations regarding how to select suitable primes p and q [11,13,14,17,18] to be later used to compute the private key and public modulus.
Once these primes have been selected, modulus computation is very simple: n = p · q, with the public exponent usually fixed to the value 65 537.
But can the modulus n itself leak information about the design and implementation choices previously used to generate the primes p and q?
Trivially, the length of the used primes is directly observable.
Interestingly, more subtle leakage was also discovered by Mironov [20] for primes generated by the OpenSSL library, which unwantedly avoids small factors of up to 17 863 from p−1 because of a coding omission.
Such a property itself is not a security vulnerability (the key space is decreased only negligibly), but it results in sufficiently significant fingerprinting of all generated primes that OpenSSL can be identified as their origin with high confidence.
Mironov used this observation to identify the sources of the primes of factorizable keys found by [12].
But can the origins of keys be identified only from the modulus n, even when n cannot be factorized and the values of the corresponding primes are not known?To answer this question, we generated a large number of RSA key pairs from 22 software libraries (both opensource and closed-source) and 16 different cryptographic smartcards from 6 different manufacturers, exported both the private and public components, and analysed the obtained values in detail.
As a result, we identified seven design and implementation decisions that directly fingerprint not only the primes but also the resulting public modulus: 1) Direct manipulation of the primes' highest bits.
2) Use of a specific method to construct strong or provable primes instead of randomly selected or uniformly generated primes.
3) Avoidance of small factors in p−1 and q−1.
4) Requirement for moduli to be Blum integers.
5) Restriction of the primes' bit length.
6) Type of action after candidate prime rejection.
7) Use of another non-traditional algorithm -functionally unknown, but statistically observable.As different design and implementation choices are made for different libraries and smartcards (cards) with regard to these criteria, a cumulative fingerprint is sufficient to identify a probable key origin even when only the public key modulus is available.
The average classification accuracy on the test set was greater than 73% even for a single classified key modulus when a hit within the top 3 matches was accepted 2 .
When more keys from the same (unknown) source were classified together, the analysis of as few as ten keys allowed the correct origin to be identified as the top single match in more than 85% of cases.
When five keys from the same source were available and a hit within the top 3 matches was accepted, the classification accuracy was over 97%.
We used the proposed probabilistic classifier to classify RSA keys collected from the IPv4 HTTPS/TLS [9], Certificate Transparency [10] and PGP [30] datasets and achieved remarkably close match to the current market share of web servers for TLS dataset.The optimal and most secure way of generating RSA key pairs is still under discussion.
Our wide-scale analysis also provides a sanity check concerning how closely the various recommendations are followed in practice for software libraries and smartcards and what the impact on the resulting prime values is, even when this impact is not observably manifested in the public key value.
We identified multiple cases of unnecessarily decreased entropy in the generated keys (although this was not exploitable for practical factorization) and a generic implementation error pattern leading to predictable keys in a small percentage (0.05%) of cases for one type of card.Surprisingly little has been published regarding how key pairs are generated on cryptographic cards.
In the case of open-source libraries such as OpenSSL, one can inspect the source code.
However, this option is not available for cards, for which the documentation of the generation algorithm is confidential and neither the source code nor the binary is available for review.
To inspect these black-box implementations, we utilized the side channels of time and power consumption (in addition to the exported raw key values).
When this sidechannel information was combined with the available knowledge and observed characteristics of open-source libraries, the approximate key pair generation process could also be established for these black-box implementations.This paper is organized as follows: After a brief summary of the RSA cryptosystem, Section 2 describes the methodology used in this study and the dataset of RSA keys collected from software libraries and cryptographic cards.
Section 3 provides a discussion of the observed properties of the generated keys.
Section 4 describes the modulus classification method and its results on large real-world key sets, the practical impact and mitigation of which are discussed in Section 5.
Additional analysis performed for black-box implementations on cards and a discussion of the practical impact of a faulty/biased random number generator are presented in Section 6.
Finally, conclusions are offered in Section 7.
2 The correct library is listed within the first three most probable groups of distinct sources identified by the classification algorithm.
To use the RSA algorithm, one must generate a key:1.
Select two distinct large primes 3 p and q.2.
Compute n = p · q and ϕ(n) = (p − 1)(q − 1).3.
Choose a public exponent 4 e < ϕ(n) that is coprime to ϕ(n).4.
Compute the private exponent d as e −1 mod ϕ(n).
The pair (e, n) is the public key; either (d, n) serves as the secret private key, or (p, q) can be used ((d, n) can be calculated from (p, q, e) and vice versa).
The basic form of attack on the RSA cryptosystem is modulus factorization, which is currently computationally unfeasible or at least extremely difficult if p and q are sufficiently large (512 bits or more) and a general algorithm such as the number field sieve (NFS) or the older quadratic sieve (MPQS) is used.
However, special properties of the primes enable more efficient factorization, and measures may be taken in the key pair generation process to attempt to prevent the use of such primes.
The primes used to generate the modulus should be of approximately the same size because the factorization time typically depends on the smallest factor.
However, if the primes are too close in value, then they will also be close to the square root of n and Fermat factorization can be used to factor n efficiently [16].
Pollard's p − 1 method outperforms general algorithms if for one of the primes p, p − 1 is B-smooth (all factors are ≤ B) for some small B (which must usually be guessed in advance).
The modulus can be factored using Williams' p + 1 method if p + 1 has no large factors [27].
Despite the existence of many special-purpose algorithms, the easiest way to factor a modulus created as the product of two randomly generated primes is usually to use the NFS algorithm.
Nevertheless, using special primes may potentially thwart such factorization attacks, and some standards, such as ANSI X9.31 [28] and FIPS 186-4 [14], require the use of primes with certain properties (e.g., p − 1 and p + 1 must have at least one large factor).
Other special algorithms, such as Pollard's rho method and the Lenstra elliptic curve method, are impractical for factoring a product of two large primes.Although RSA factorization is considered to be an NPhard problem if keys that fulfil the above conditions are used, practical attacks, often relying on a faulty random generator, nevertheless exist.
Insufficient entropy, primarily in routers and embedded devices, leads to weak and factorizable keys [12].
A faulty card random number generator has produced weak keys for Taiwanese citizens [3], and supposedly secure cryptographic tokens have been known to produce corrupted or significantly biased keys and random streams [6].
Implementation attacks can also compromise private keys based on leakage in side channels of timing [8] or power [15].
Active attacks based on fault induction [26] or exploits aimed at message formatting [2,5] enable the recovery of private key values.
We largely excluded these classes of attacks from the scope of our analysis, focusing only on key generation.
Our purpose was to verify whether the RSA key pairs generated from software libraries and on cards provide the desired quality and security with respect to the expectations of randomness and resilience to common attacks.
We attempted to identify the characteristics of the generated keys and deduce the process responsible for introducing them.
The impact of the techniques used on the properties of the produced public keys was also investigated.
We used the following methodology: open-source cryptographic libraries with known implementations.2.
Gather a large number of RSA key pairs from cryptographic software libraries and cards (one million from each).3.
Compare the keys originating from open-source libraries and black-box implementations and discuss the causes of any observed similarities and differences (e.g., the distribution of the prime factors of p − 1).4.
Analyse the generated keys using multiple statistical techniques (e.g., calculate the distribution of the most significant bytes of the primes).
Throughout this paper, we will use the term source (of keys) when referring to both software libraries and cards.
We examined the source codes of 19 open-source cryptographic libraries variants 5 and match it to the relevant algorithms for primality testing, prime generation and RSA key generation from standards and literature.
We then examined how the different methods affected the distributions of the primes and moduli.
Summary results together for all sources are available in Table 1 6 .
No other library generates such primes exclusively (however, some cards do).
In the random sampling method, large integers (candidates) are generated until a prime is found.
If the candidates are chosen uniformly, the distribution is not biased (case of GNU Crypto 2.0.1, LibTomCrypt 1.17 and WolfSSL 3.9.0).
An incremental search algorithm selects a random candidate and then increments it until a prime is found (Botan 1.11.29, Bouncy Castle 1.54, Cryptix 20050328, cryptlib 3.4.3, Crypto++ 5.6.3, FlexiProvider 1.7p7, mbedTLS 2.2.1, SunRsaSign -Open-JDK 1.8.0, OpenSSL 1.0.2g, and PGPSDK4).
Primes preceded by larger "gaps" will be selected with slightly higher probability; however, this bias is not observable from the distribution of the primes.Large random integers are likely to have some small prime divisors.
Before time-consuming primality tests are performed, compositeness can be revealed through trial division with small primes or the computation of the greatest common divisor (GCD) with a product of a few hundred primes.
In the case of incremental search, the sieve of Eratosthenes or a table of remainders that is updated when the candidate is incremented can be used.
If implemented correctly, these efficiency improvements do not affect the distribution of the prime generator.OpenSSL creates a table of remainders by dividing a candidate by small primes.
When a composite candidate is incremented, this table is efficiently updated using only operations with small integers.
Interestingly, candidates for p for which p − 1 is divisible by a small prime up to 17 863 (except 2) are also rejected.
Such a computational step is useful to speed up the search for a safe prime; however, (p − 1)/2 is not required (as would be for safe prime) to be prime by the library.
This strange behaviour was first reported by Mironov [20] and can be used to classify the source if the primes are known.
Figure 1: RSA key generation.
The maximal region for the generated primes is defined by the precise length of the modulus and equal lengths of the primes.
Such keys can be generated through rejection sampling.
To avoid generating short moduli (which must be discarded), alternative square regions may be used.
Several implementations, such as that of the NXP J2A080 card, generate primes from arbitrary combinations of square regions.Provable primes.
Primes are constructed recursively from smaller primes, such that their primality can be deduced mathematically (using Pocklington's theorem or related facts).
This process is randomized; hence, a different prime is obtained each time.
An algorithm for constructing provable primes was first proposed by Maurer [18] (used by Nettle 3.2).
For each prime p, p − 1 must have a large factor (≥ √ p for Maurer's algorithm or ≥ 3 √ p for an improved version thereof).
Factors of p + 1 are not affected.
Strong primes.
A prime p is strong if both p − 1 and p + 1 have a large prime factor (used by libgcrypt 1.65 in FIPS mode and by the OpenSSL 2.0.12 FIPS module).
We also refer to these primes as FIPS-compliant, as FIPS 186-4 requires such primes for 1024-bit keys (larger keys may use probable primes).
Differing definitions of strong primes are given in the literature; often, the large factor of p − 1 itself (minus one) should also have a large prime factor (PGPSDK4 in FIPS mode).
Large random primes are not "weak" by comparison, as their prime factors are sufficiently large, with sufficient probability, to be safe from relevant attacks.Strong primes are constructed from large prime factors.
They can be generated uniformly (as in ANSI X9.31, FIPS 186-4, and IEEE 1363-2000) or with a visibly biased distribution (as in a version of Gordon's algorithm [11] used in PGPSDK4).
The key size is the bit length of the modulus.
Typically, an algorithm generates keys of an exact bit length (the only exception being PGPSDK4 in FIPS mode).
The primes are thus generated with a size equal to half of the modulus length.
These two measures define the maximal region for RSA primes.
The product of two k-bit primes is either 2k or 2k − 1 bits long.
There are two principal methods of solving the problem of short (2k − 1)-bit moduli, as illustrated in Figure 1.
Rejection sampling.
In this method, pairs of k-bit primes are generated until their product has the correct length.
To produce an unbiased distribution, two new primes should be generated each time (Cryptix 20050328, FlexiProvider 1.7p7, and mbedTLS 2.2.1).
If the greater prime is kept and only one new prime is generated, some bias can be observed in the resulting distribution of RSA moduli (Bouncy Castle up to version 1.53 and SunRsaSign in OpenJDK 1.8.0).
If the first prime is kept (without regard to its size) and the second prime is re-generated, small moduli will be much more probable than large values (GNU Crypto 2.0.1).
"Square" regions.
This technique avoids the generation of moduli of incorrect length that must be discarded by generating only larger primes such that their product has the correct length.
Typically, both primes are selected from identical intervals.
When the prime pairs are plotted in two dimensions, this produces a square region.The smallest k-bit numbers that produce a 2k-bit modulus are close to √ 2 · 2 k−1 .
Because random numbers can easily be uniformly generated from intervals bounded by powers of two, the distribution must be additionally transformed to fit such an interval.
We refer to prime pairs generated from the interval񮽙 √ 2 · 2 k−1 , 2 k − 1 񮽙as being generated from the maximal square region (Bouncy Castle since version 1.54, Crypto++ 5.6.3, and the Microsoft cryptography providers used in CryptoAPI, CNG and .
NET).
Crypto++ approximates this interval by generating the most significant byte of primes from 182 to 255.
A more practical square region, which works well for candidates generated uniformly from intervals bounded by powers of two, is achieved by fixing the two most significant bits of a candidate to 11 2 (Botan 1.11.29, cryptlib 3.4.3, libgcrypt 1.6.5, LibTomCrypt 1.17, OpenSSL 1.0.2g, PGPSDK4, and WolfSSL 3.9.0).
Additionally, the provable primes generated in Nettle 3.2 and the strong primes generated in libgcrypt 1.6.5 (in FIPS mode) and in the OpenSSL 2.0.12 FIPS module are produced from this region.
To obtain representative results of the key generation procedures used in cards (for which we could not inspect the source codes), we investigated 16 different types of cards from 6 different established card manufacturers (2×Gemalto, 6×NXP, 1×Infineon, 3×Giesecke & Devrient (G&D), 2×Feitian and 2×Oberthur) developed using the widely used JavaCard platform.
The key pair generation process itself is implemented at a lower level, with JavaCard merely providing an interface for calling relevant methods.
For each type of card (e.g., NXP J2D081), three physical cards were tested to detect any potential differences among physical cards of the same type (throughout the entire analysis, no such difference was ever detected).
Each card was programmed with an application enabling the generation and export of an RSA key pair (using the KeyPair.generateKey() method) and truly random data (using the RandomData.generate() method).
We focused primarily on the analysis of RSA keys of three different lengths -512, 1024 and 2048 bits.
Each card was repeatedly asked to generate new RSA 512-bit key pairs until one million key pairs had been generated or the card stopped responding.
The time required to create these key pairs was measured, and both the public (the modulus n and the exponent e) and private (the primes p and q and the private exponent d) components were exported from the card for subsequent analyses.
No card reset was performed between key pair generations.
In the ideal case, three times one million key pairs were extracted for every card type.
The same process was repeated for RSA key pairs with 1024-bit moduli but for only 50 000 key pairs, as the key generation process takes progressively longer for longer keys.
The patterns observed from the analyses performed on the 512-bit keys was used to verify the key set with longer keys 7 .
Surprisingly, we found substantial differences in the intervals from which primes were chosen.
In some cases, non-uniform distributions of the primes hinted that the prime generation algorithms are also different to those used in the software libraries.
Several methods adopted in software libraries, such as incremental search, seem to be suitable even for limited-resource systems.
This argument is supported by a patent application [21] by G&D, one of the manufacturers of the examined cards.
All tested cards from this manufacturer produced Blum integers, as described in the patent, and these integers were distributed uniformly, as expected from the incremental search method.A duration of approximately 2-3 weeks was typically required to generate one million key pairs from a single card, and we used up to 20 card readers gathering keys in parallel.
Not all cards were able to generate all required keys or random data, stopping with a non-specific error (0x6F00) or becoming permanently non-responsive after a certain period.
In total, we gathered more than 30 million card-generated RSA key pairs 8 .
Power consumption traces were captured for a small number of instances of the key pair generation process.In addition, 100 MB streams of truly random data were extracted from each card for tests of statistical randomness.
When a problem was detected (i.e., the data failed one or more statistical tests), a 1 GB stream was generated for fine-grained verification tests.
The key pairs extracted from both the software libraries and the cards were examined using a similar set of analytical techniques.
The goal was to identify sources with the same behaviour, investigate the impact on the public key values and infer the probable key generation algorithm used based on similarities and differences in the observed properties.
To visualize the regions from which pairs of primes were chosen, we plotted the most significant byte (MSB) of each prime on a heat map.
It is possible to observe the intervals for prime generation, as discussed in Section 2.3.
Figure 2 shows a small subset of the observed nonuniform distributions.
Surprisingly, the MSB patterns were significantly different for the cards and the software implementations.
The patterns were identical among different physical cards of the same type and were also shared between some (but not all) types of cards from the same manufacturer (probably because of a shared code base).
We did not encounter any library that produced outputs comparable to those of the first two cards from the examples shown in Figure 2.
The third example could be reproduced by generating primes alternately and uniformly from 14 different regions, each characterized by a pattern in the top four bits of the primes.
By comparison, it was rarer for a bias to be introduced by a library.
Q P Pmin = 01100000 Pmax = 11101111 Qmin = 01100100 Qmax = 11111111 Nmin = 00100110 Nmax = 11101000 P = Q Figure 2: Example distributions of the most significant byte of the prime p and the corresponding prime q from 8 million RSA key pairs generated by three software libraries and three types of cards.
The histograms on the top and the side of the graph represent the marginal distributions of p and q, respectively.
The colour scheme expresses the likelihood that primes of a randomly generated key will have specific high order bytes, ranging from white (not likely) over orange to red (more likely).
For distributions of all sources from the dataset, see our technical report [25].
The relation between the values of p and q reveals additional conditions placed on the primes, such as a minimal size of the difference p − q (PGPSDK4, NXP J2D081, and NXP J2E145G).
It is possible to verify whether small factors of p − 1 are being avoided (e.g., OpenSSL or NXP J2D081) or whether the primes generally do not exhibit same distribution as randomly generated numbers (Infineon JTOP 80K) by computing the distributions of the primes, modulo small primes.
It follows from Dirichlet's theorem that the remainders should be distributed uniformly among the φ (n) congruence classes in Z * n [19, Fact 4.2].
The patterns observed for the 512-bit keys were found to be identical to those for the stronger keys of 1024 and 2048 bits.
For the software implementations, we checked the source codes to confirm that there were no differences in the algorithms used to generate keys of different lengths.
For the cards, we assume the same and generalize the results obtained for 512-bit RSA keys to the longer (and much more common) keys.
The MSB of a modulus is directly dependent on the MSBs of the corresponding primes p and q.
As seen in Figure 3, if an observable pattern exists in the distributions of the MSBs of primes p and q, a noticeable pattern also appears in the MSB of the modulus.
The preservation of shared patterns was observed for all tested types of software libraries and cards.
The algorithm used for prime pair selection can often be observed from the distribution of the moduli.
If a source uses an atypical algorithm, it is possible to detect it with greater precision, even if we do not know the actual method used.Non-randomness with respect to small factors of p − 1 can also be observed from the modulus, especially for small divisors.
Whereas random primes are equiprobably congruent to 1 and 2 modulo 3, OpenSSL primes are always congruent to 2.
As a result, an OpenSSL modulus is always congruent to 1 modulo 3.
This property is progressively more difficult to detect for larger prime divisors.
The moduli are more probably congruent to 1 modulo all small primes, which are avoided from p − 1 by OpenSSL.
However, the bias is barely noticeable for prime factors of 19 and more, even in an analysis of a million keys.
OpenSSL primes are congruent to 1 modulo 5 with probability 1/3 (as opposed to 1/4 for random primes), to 1 modulo 7 with probability 1/5 (as opposed to 1/6), and to 1 modulo 11 with probability 1/9 (as opposed to 1/10).
For the practical classification of only a few keys (see Section 4), we use only the remainder of division by 3.
The use of Blum integers can also be detected from the moduli with a high precision, as random moduli are equiprobably congruent to 1 and 3 modulo 4, whereas Blum integers are always congruent to 1 modulo 4.
The probability that k random moduli will be Blum integers is 2 −k .
Neither libraries nor cards attempt to achieve a uniform distribution of moduli.
Existing algorithms [13,17] have the disadvantage that sometimes a prime will be one bit larger than half of the modulus length.
All sources sacrifice uniformity in the most significant bits of the modulus to benefit from more efficient methods of prime and key generation.We verified that the distribution of the other bytes of the moduli is otherwise uniform.
The second least significant bit is biased in the case of Blum integers.
Sources that use the same algorithm are not mutually distinguishable from the distributions of their moduli.
It is possible to verify whether strong primes are being used.
Most algorithms generate strong primes from uniform distributions (ANSI X9.31, FIPS 186-4, IEEE 1363, OpenSSL FIPS, libgcrypt FIPS, Microsoft and Gemalto GCX4 72K), matching the distribution of random primes, although PGPSDK4 FIPS produces a highly atypical distribution of primes and moduli, such that this source can be detected even from public keys.
Hence, we were obliged to search for the sizes of the prime factors of p − 1 and p + 1 directly 9 by factoring them using the YAFU software package [29].
We then extended the results obtained for 512-bit keys to the primes of 1024-bit key pairs (though based on fewer factorized values because of the longer factorization time).
Finally, we extrapolated the results to keys of 2048 bits and longer based on the known patterns for shorter keys.As confirmed by the source code, large factors of p±1 generated in OpenSSL FIPS and by libgcrypt in FIPS mode always have 101 bits; this value is hardcoded.
PGPSDK4 in FIPS mode also generates prime factors of fixed length; however, their size depends on the size of the prime.Additionally, we detected strong primes in some of our black-box sources.
Gemalto GCX4 72K generates strong primes uniformly, but the large prime factors always have 101 bits.
The strong primes of Gemalto GXP E64, which have 112 bits, are not drawn from a uniform distribution.
The libraries that use Microsoft cryptography providers (CryptoAPI, CNG, and .
NET) produce prime factors of randomized length, ranging from 101 bits to 120 bits, as required by ANSI X9.31.
For large primes, p ± 1 has a large prime factor with high probability.
A random integer p will not have a factor larger than p 1/u with a probability of approximately u −u [19].
Approximately 10% of 256-bit primes do not have factors larger than 100 bits, but 512-bit keys are not widely used.
For 512-bit primes, the probability is less than 0.05%.
Therefore, the requirement of a large factor does not seem to considerably decrease the number of possible primes.
However, many sources construct strong primes with factors of exact length (e.g., 101 bits).
Using the approximation of the prime-counting function π(n) ≈ n ln(n) [19], we estimate that the interval required by ANSI X9.31 (prime factors from 101 to 120 bits) contains approximately 2 20 times more primes than the number of 101-bit primes.
Hence, there is a loss of entropy when strong primes are generated in this way, although we are not aware of an attack that would exploit this fact.
For every choice of an auxiliary prime, 2 93 possible values are considered instead of 2 113 , which implies the loss of almost 20 bits of entropy.
If the primes are to be (p − , p + )-safe, then 2 auxiliary primes must be generated.
Because we require two primes p and q for every RSA key, we double the estimated loss of entropy compared with ANSI-compliant keys to 80 bits for 1024-bit keys.When p − 1 is guaranteed to have a large prime factor but p + 1 is not, the source is most likely using provable Bit lengths of the largest prime factors of p−1The second largest prime factor (bit length)The largest prime factor (bit length) Concerning the distribution of factors, most of the software libraries (14) and card types (8) yield distributions comparable to that of randomly generated numbers of a given length (see Figure 4).
The Infineon JTOP 80K card produces significantly more small factors than usual (compared with both random numbers and other sources).
This decreases the probability of having a large factor.We estimated the percentage of 512-bit RSA keys that are susceptible to Pollard p − 1 factorization within 2 80 operations.
This percentage ranges from 0% (FIPScompliant sources) to 4.35% (Infineon JCOP 80K), with an average of 3.38%.
Although the NFS algorithm would still be faster in most cases of keys of 512 bits and larger, we found a card-generated key (with a small maximal factor of p − 1) that was factorized via Pollard p − 1 method in 19 minutes, whereas the NFS algorithm would require more than 2 000 CPU hours.
Note that for 1024-bit keys, the probability of such a key being produced is negligible.
Based on the exported private and public components of the generated RSA keys obtained from all sources, we can summarize their basic properties as follows (see also • All values p and q are primes and are not close enough for Fermat factorization to be practical.
• All card-generated keys use a public exponent equal to 0x10001 (65 537), and all software libraries either use this value as the default or support a usersupplied exponent.
• Most modulus values are of an exactly required length (e.g., 1024 bits).
The only exception is PG-PSDK4 in FIPS mode, which also generates moduli that are shorter than the specified length by one or two bits.
• Neither libraries nor cards ensure that p is a safe prime (p = 2 · q + 1, where q is also prime).
• Some sources construct strong primes according to the stricter definition or at least comply with the requirements defined in the FIPS 186-4 and ANSI X9.31 standards, such that p−1 and p+1 both have a large prime factor.
Other libraries are not FIPScompliant; however, keys of 1024 bits and larger resist p − 1 and p + 1 attacks for practical values of the smoothness bound.
• Some libraries (5) and most card types (12) order the primes such that p > q, which seems to be a convention for CRT RSA keys.
PGPSDK4 (in both regular and FIPS modes) and libgcrypt (used by GnuPG) in both modes order the primes in the opposite manner, q > p.
In some sources, the ordering is a side effect of the primes having fixed (and different) most significant bits (e.g., 4 bits of p and q are fixed to 1111 and 1001, respectively, by all G&D cards).
• All generated primes were unique for all libraries and all types of cards except one (Oberthur Cosmo Dual 72K).
• All G&D and NXP cards, the Oberthur Cosmo Dual 72K card and the GNU Crypto library generate Blum integers.
As seen from a bug in the implementation of the Miller-Rabin test in GNU Crypto, a simpler version of the test suffices for testing Blum primes.
However, we hypothesize that the card manufacturers have a different motivation for using such primes.
The distinct distributions of specific bits of primes and moduli enable probabilistic estimation of the source library or card from which a given public RSA key was generated.
Intuitively, classification works as follows: 1) Bits of moduli known to carry bias are identified with additional bits derived from the modulus value (a mask, 6 + 3 bits in our method).2) The frequencies of all possible mask combinations (2 9 ) for a given source in the learning set are computed.
3) For classification of an unknown public key, the bits selected by the mask are extracted as a particular value v.
The source with the highest computed frequency of value v (step 2) is identified as the most probable source.
When more keys from the same source are available (multiple values v i ), a higher classification accuracy can be achieved through elementwise multiplication of the probabilities of the individual keys.We first describe the creation of a classification matrix and report the classification success rate as evaluated on our test set [31].
Later, classification is applied to three real-world datasets: the IPv4 HTTPS handshakes set [9], Certificate Transparency set [10] and the PGP key set [30].
The classification process is reasonably straightforward.
For the full details of the algorithm, please refer to our technical report [25].1.
All modulus bits identified through previous analysis as non-uniform for at least one source are included in a mask.
We included the 2 nd − 7 th most significant bits influenced by the prime manipulations described in Section 3.1, the second least significant bit (which is zero for sources that use Blum integers), the result of the modulus modulo 3 (which is influenced by the avoidance of factor 3) and the overall modulus length (which indicates whether an exact length is enforced).2.
A large number of keys (learning set) from known generating sources are used to create a classification matrix.
For every possible mask value (of which there are 2 9 in our case) and every source, the relative frequency of the given mask value in the learning set for the given source is computed.3.
During the classification phase for key K with modulus m, the value v obtained after the application of mask to modulus m is extracted.
The row (probability vector) of the classification matrix that corresponds to the value v contains, as its i th element, the probability of K being produced by source i.4.
When a batch of multiple keys that are known to have been produced by the same (unknown) source is classified, the probability vectors for every key obtained in step 3 are multiplied element-wise and normalized to obtain the source probabilities p b for the entire batch, and the source with the highest probability is selected.Note that the described algorithm cannot distinguish between sources with very similar characteristics, e.g., between the NXP J2D081 and NXP J2E145G cards, which likely share the same implementation.
For this reason, if two sources have the same or very similar profiles, they are placed in the same group.
Figure 5 shows the clustering and (dis-)similarity of all sources considered in this study.
If the particular source of one or more key(s) is missing from our analysis (relevant for the classification ; the primes p and q have a fixed pattern in their top four bits, 1111 2 and 1001 2 , respectively (FX×9X); Gemalto non-uniform strong primes (Gem.)
; Infineon algorithm (Inf.)
; NXP regions -14 distinct square regions characterized by patterns in the top four bits of p and q (NXP).
Blum integers: the modulus n is always a Blum integer n ≡ 1 (mod 4) (񮽙); the modulus is n ≡ 1 (mod 4) and n ≡ 3 (mod 4) with equal probability (×).
Small factors of p − 1: p − 1 contains small prime factors (񮽙); some prime factors are avoided in p − 1 (×).
Large factors of p − 1: p − 1 is guaranteed to have a large prime factor -provable and strong primes (񮽙); size of the prime factors of p − 1 is random (×).
Large factors of p + 1: similar as for p − 1, typically strong primes are (񮽙); random and provable primes are (×).
|p − q| check: p and q differ somewhere in their top bits (񮽙); the property is not guaranteed (×); the check may be performed, but the negative case occurs with a negligible probability (?
Figure 5: Clustering of all inspected sources based on the 9 bits of the mask.
The separation line shows which sources were put by us into the same classification category.
Finer separation is still possible (e.g., SunRsaSign vs mbedTLS), but the number of the keys from same source needs to be high enough to distinguish these very similar sources.
Table 2: The classification success rate of 13 groups created from all 38 analyzed sources using test set with same prior probability of sources (see Figure 5 for libraries and cards in particular group).
Columns corresponds to different number of keys (1, 2, 5, 10 and 100) classified together from same (unknown) source.
of real-world datasets), any such key will be misclassified as belonging to a group with a similar mask probability vector.
Both the construction of the classification matrix and the actual classification are then performed using these groups instead of the original single sources.
The observed similarities split the examined sources into 13 different groups (labelled I to XIII and listed in Figure 5).
The resulting classification matrix 10 has dimensions of 13 × 512.
10 Because of its large size, the resulting matrix is available in our technical report [25] and at http://crcs.cz/papers/usenix2016.
To evaluate the classification success of our method, we randomly selected 10 000 keys from the collected dataset (that were not used to construct the classification matrix) for every source, thereby endowing the test set with equal prior probability for every source.A single organization may use the same source library to generate multiple keys for its web servers.
The classification accuracy was therefore evaluated not only for one key (step 3 of the algorithm) but also for five, ten and one hundred keys (step 4) originating from the same (unknown) source.
We evaluated not only the ability to achieve the "best match" with the correct source group but also the ability to identify the correct source group within the top two and top three most probable matches (top-n match).
As shown in Table 2, the average accuracy on the test set of the most probable source group was over 40% for single keys and improved to greater than 93% when we used batches of 100 keys from the same source for classification.
When 10 keys from the same source were classified in a batch, the most probable classified group was correct in more than 85% of cases and was almost always (99%) included in the top three most probable sources.A significant variability in classification success was observed among the different groups.
Groups I (G&D cards) and IV (PGPSDK4 FIPS) could be correctly identified from even a single key because of their distinct distributions of possible mask values.
By contrast, group X (Microsoft providers) was frequently misclassified when only a single key was used because of the wider range of possible mask values, resulting in a lower probability of each individual mask value.We conclude that our classification method is moderately successful even for a single key and very accurate when a batch of at least 10 keys from the same source is classified simultaneously.Further leakage in other bits of public moduli might be found by applying machine learning methods to the learning set, potentially leading to an improvement of the classification accuracy.
Moreover, although we have already tested a wide range of software libraries and cards, more sources could also be incorporated, such as additional commercial libraries, various hardware security modules and additional types of cards and security tokens.
One can attempt to classify keys from suitable public datasets using the described method.
However, the classification of keys observed in the real world may differ from the classification scenario evaluated above in two respects:1.
The prior probabilities of real-world sources can differ significantly (e.g., OpenSSL is a more probable source for TLS keys than is any card), and the resulting posterior probabilities from the classification matrix will then also be different.2.
Our classification matrix does not include all existing sources (e.g., we have not tested high-speed hardware security modules), and such sources will therefore always be misclassified.The classification success rate can be significantly improved if the prior distribution of possible sources can be estimated.
Such an estimate can be performed based on meta information such as statistics concerning the popularity of various software libraries or sales figures for a particular card model.
Note that the prior distributions may also significantly differ for different application areas, e.g., PGP keys are generated by a narrower set of libraries and devices than are TLS keys.
In this work, we did not perform any prior probability estimations.
We used IPv4 HTTPS handshakes collected from the Internet-Wide Scan Data Repository [9] as our source of real-world TLS keys.
The complete set contains approximately 50 million handshakes; the relevant subset, which consists of handshakes using RSA keys with a public exponent of 65 537, contains 33.5M handshakes.
This set reduces to 10.7M unique keys based on the modulus values.
The keys in this set can be further divided into batches with the same subject and issue date (as extracted from their certificates), where the same underlying library is assumed to be responsible for the generation of all keys in a given batch.
As the classification accuracy improves with the inclusion of more keys in a batch, we obtained classification results separately for batches consisting of a single key only (users with a single HTTPS server), 2-9 keys, 10-99 keys (users with a moderate number of servers) and 100 and more keys (users with a large number of servers).
Intuitively, batches with 100+ keys will yield very accurate classification results but will capture only the behaviour of users with a large number of HTTPS servers.
Conversely, batches consisting of only a single key will result in low accuracy but can capture the behaviours of different types of users.The frequency of a given source in a dataset (for a particular range of batch sizes) is computed as follows: 1) The classification probability vector p b for a given batch is computed according to the algorithm from Section 4.1.
2) The element-wise sum of p b · n b over all batches b (weighted by the actual number of keys n b in the given batch) is computed and normalized to obtain the relative proportion vector, which can be found as a row in Table 3.
As shown in Section 4.1.1, a batch of 10 keys originating from the same source should provide an average classification accuracy of greater than 85% -sufficiently high to enable reasonable conclusions to be drawn regarding the observed distribution.
Using batches of 10-99 keys, the highest proportion of keys generated for TLS IPv4 (82%) were classified as belonging to group V, which contains a single library -OpenSSL.
This proportion increased to almost 90% for batches with 100+ keys.
The second largest proportion of these keys (ap- Table 3: The ratio of resulting source groups identified by the classification method described in Section 4.
Datasets are split into subsets based on the number of keys that can be attributed to a single source (batch). '
-' means no key was classified for the target group. '
0.00%' means that some keys were classified, but less than 0.005%.
proximately 10.2%) was assigned to group X, which contains the Microsoft providers (CAPI, CNG, and .
NET).
These estimates can be compared against the estimated distribution of commonly used web servers.
Apache, Nginx, LiteSpeed, and Google servers with the OpenSSL library as the default option have a cumulative market share of 86% [32].
This value exhibits a remarkably close match to the classification rate obtained for OpenSSL (group V).
MS Internet Information Services (IIS) is included with Microsoft's cryptographic providers (group X) and has a market share of approximately 12%.
Again, a close match is observed with the classification value of 10.2% obtained for users with 10-99 certificates certified within the same day (batch).
Users with 100 and more keys certified within the same day show an even stronger preference for OpenSSL library (89.9%; group V) and also for group VIII (4.6%; this group contains popular libraries such as OpenJDK's SunRsaSign, Bouncy Castle and mbedTLS) at the expense of groups X and XI.The classification accuracy for users with only singlekey batches or a small number of keys per batch is significantly less certain, but the general trends observed for larger batches persist.
Group V (OpenSSL) is most popular, with group X (Microsoft providers) being the second most common.
Although we cannot obtain the exact proportions of keys generated using particular sources/groups, we can easily determine the proportion of keys that certainly could not have been generated by a given source by means of the occurrence of impossible values produced by the bit mask, i.e., values that are never produced by the given source.
Using this method, we can conclude for certain that 19%, 25%, 17% and 10% of keys for users with 1, 2-9, 10-99 and 100+ keys per batch, respectively, could not have been generated by the OpenSSL library (see [25] for details).
Another dataset of TLS keys was collected from Google's Pilot Certificate Transparency server [10].
The dataset processing was the same as that for the previous TLS dataset [9].
For users with small numbers of keys (1 and 2-9), the general trends observed from the TLS IPv4 dataset were preserved.
Interestingly, however, Certificate Transparency dataset indicates that group X (Microsoft) is significantly more popular (47%) than group V (OpenSSL) for users with 10-99 keys.
A different set of real-world keys can be obtained from PGP key servers [30].
We used a dump containing nearly 4.2 million keys, of which approximately 1.4 million were RSA keys suitable for classification using the same processing as for the TLS datasets.
In contrast to the TLS handshakes, significantly fewer PGP keys could be attributed to the same batch (i.e., could be identified as originating from the same unknown source) based on the subject name and certification date.
Still, 84 thousand unique keys were extracted in batches of 2-9 keys and 1 732 for batches of 10-99 keys.The most prolific source group is group XI (which contains both libgcrypt from the GnuPG software distribution and the PGPSDK4 library), as seen in Ta- ble 3.
This is intuitively expected because of the widespread use of these two software libraries.
Group VIII, consisting of the Bouncy Castle library (containing the org.bouncycastle.openpgp package), is also very common (36%) for batches of 10-99 keys.Because of the lower accuracy of classification for users with smaller numbers of keys (1 and 2-9), it is feasible only to consider the general properties of these key batches and their comparison with the TLS case rather than being concerned with the exact percentage values in these settings.
The results for the PGP dataset indicate a significant drop in the proportion of keys generated using the OpenSSL library.
According to an analysis of the keys that certainly could not have been obtained from a given source, at least 47% of the single-key batches were certainly not generated by OpenSSL, and this percentage increases to 72% for batches of 2-9 keys.
PGPSDK4 in FIPS mode (group IV) was found to be significantly more common than in the TLS datasets.Note that an exported public PGP key usually contains a Version string that identifies the software used.
Unfortunately, however, this might be not the software used to generate the original key pair but merely the software that was used to export the public key.
If the public key was obtained via a PGP keyserver (as was the case for our dataset), then the Version string indicates the version of the keyserver software itself (e.g., Version: SKS 1.1.5) and cannot be used to identify the ratios of the different libraries used to generate the keys 11 .
The possibility of accurately identifying the originating library or card for an RSA key is not solely of theoretical or statistical interest.
If some library or card is found to produce weak keys, then an attacker can quickly scan for other keys from the same vulnerable source.
The possibility of detection is especially helpful when a successful attack against a weak key requires a large but practically achievable amount of computational resources.
Preselecting potentially vulnerable keys saves an attacker from spending resources on all public keys.The identification of the implementations responsible for the weak keys found in [3,12] was a difficult problem.
In such cases, origin classification can quickly provide one or a few of the most probable sources for further manual inspection.
Additionally, a set of already identified weak keys can be used to construct a new classification group, which either will match an already known one (for which the underlying sources are known) or can be used to search for other keys that belong to this new group in the remainder of a larger dataset (even when the source is unknown).
11 A dataset with the original Version strings could be used to test these predictions.Another practical impact is the decreased anonymity set of the users of a service that utilizes the RSA algorithm whose users are not intended to be distinguishable (such as the Tor network).
Using different sources of generated keys will separate users into smaller anonymity groups, effectively decreasing their anonymity sets.
The resulting anonymity sets will be especially small when individual users decides to use cryptographic hardware to generate and protect their private keys (if selected device does not fall into into group with widely used libraries).
Note that most users of the Tor project use the default client, and hence the same implementation, for the generation of the keys they use.
However, the preservation of indistinguishability should be considered in the development of future alternative clients.Tor hidden services sometimes utilize ordinary HTTPS certificates for TLS [1], which can be then linked (via classification of their public keys) with other services of the same (unknown) operator.Mixnets such as mixmaster and mixminion use RSA public keys to encrypt messages for target recipient and/or intermediate mix.If key ID is preserved, one may try to obtain corresponding public key from PGP keyserver and search for keys with the same source to narrow that user's anonymity set in addition to analysis like one already performed on alt.anonymous.messages [22].
Same as for Tor network, multiple seemingly independent mixes can be linked together if uncommon source is used to generate their's RSA keys.A related use is in a forensic investigation in which a public key needs to be matched to a suspect keygenerating application.
Again, secure hardware will more strongly fingerprint its user because of its relative rarity.An interesting use is to verify the claims of remote providers of Cryptography as a Service [4] regarding whether a particular secure hardware is used as claimed.
As the secure hardware (cards) used in our analysis mostly exhibit distinct properties of their generated keys, the use of such hardware can be distinguished from the use of a common software library such as OpenSSL.
The impact of successful classification can be mitigated on two fronts: by library maintainers and by library users.
The root cause lies with the different design and implementation choices for key generation that influence the statistical distributions of the resulting public keys.
A maintainer can modify the code of a library to eliminate differences with respect to the approach used by all other sources (or at least the most common one, which is OpenSSL in most cases).
However, although this might work for one specific library (mimicking OpenSSL), it is not likely to be effective on a wider scale.
Changes to all major libraries by its maintainers are unlikely to occur, and many users will continue to use older versions of libraries for legacy reasons.More pragmatic and immediate mitigation can be achieved by the users of these libraries.
A user may repeatedly generate candidate key pairs from his or her library or device of choice and reject it if its classification is too successful.
Expected number of trials differs based on the library used and the prior probability of sources within the targeted domain.
For example, if TLS is the targeted domain, five or less key generation trials are expected for most libraries to produce "indecisive" key.The weakness of the second approach lies in the unknown extent of public modulus leakage.
Although we have described seven different causes of leakage, others might yet remain unknown -allowing for potential future classification of keys even after they have been optimized for maximal indecisiveness against these seven known causes.This strategy can be extended when more keys are to be generated.
All previously generated keys should be included in a trial classification together with the new candidate key.
The selection process should also be randomized to some extent; otherwise, a new classification group of "suspiciously indecisive" keys might be formed.
The algorithms used in open-source libraries can be inspected and directly correlated to the biases detected in their outputs.
To similarly attribute the biased keys produced by cards to their unknown underlying algorithms, we first verified whether the random number generator might instead be responsible for the observed bias.
We also examined the time-and power-consumption side channels of the cards to gain insight into the processes responsible for key generation.Truly random data generated on-card are a crucial input for the primes used in RSA key pair generation.
A bias in these data would influence the predictability of the primes.
If a highly biased or malfunctioning generator is used, factorization is not necessary (only a small number of fixed values can be taken as primes) or is feasible even for RSA keys with lengths otherwise deemed to be secure [3,6,12].
The output of an on-card truly random number generator (TRNG) can be tested using statistical batteries, and deviances are occasionally detected in commercial security tokens [6].
We generated a 100 MB stream of ran- dom data from one card of each type and tested these data streams using the common default settings of the NIST STS and the Dieharder battery of statistical tests [7,23] as well as our alternative EACirc distinguisher [24].
All types of cards except two (Infineon JTOP 80K and Oberthur Cosmo Dual 72K) passed the tests with the expected number of failures at a confidence level of 1%.
The Infineon JTOP 80K failed the NIST STS Approximate Entropy test (85/100, expected entropy contained in the data) at a significant level and also failed the group of Serial tests from the Dieharder suite (39/100, frequency of overlapping n-bit patterns).
Interestingly, the serial tests began to fail only for patterns with lengths of 9 bits and longer (lengths of up to 16 bits were tested), suggesting a correlation between two consecutive random bytes generated by the TRNG.
As shown in Figure 6, for 16-bit patterns, all bytes in the form of xyxy (where x and y denote 4-bit values) were 37% less likely to occur than other combinations.
At least three more distinct groups of inputs with smaller-than-average probabilities were also identified.
Note that deviating distributions were observed in all three physical Infineon JTOP 80K cards that were tested and thus were probably caused by a systematic defect in the entire family of cards rather than a single malfunctioning device.
The detected bias is probably not sufficient to enable faster factorization by guessing potential primes according to the slightly biased distribution.
However, it may be used to identify this type of card as the source of a sufficiently large (e.g., 1KB) random data stream (i.e., to fingerprint such a random stream).
The Oberthur Cosmo Dual 72K failed more visibly, as two cards were blocked after the generation of only several MB of random data.
The statistical tests then frequently failed because of the significant bias in the data.
Several specific byte values were never produced in the "random" stream.We also generated data streams directly from the concatenated exported primes with the two most significant bytes and the least two bits dropped, as the previous analysis had revealed a non-uniform distribution in these bits.
Interestingly, both the Infineon JTOP 80K and the Oberthur Cosmo Dual 72K failed only for their random data streams (as described above) but successfully passed 12 for the streams generated from the concatenated primes, hinting at the possibility that either random data are generated differently during prime generation or (unlikely) the prime selection process is able to mask the bias observed in the raw random data.
All primes for the card-generated 512-and 1024-bit keys were tested for uniqueness.
All tested card types except one generated unique primes.
In the exceptional case of the Oberthur Cosmo Dual 72K cards, approximately 0.05% of the generated keys shared a specific value of prime q.
The flaw was discovered in all three tested physical cards for both 512-bit and 1024-bit keys.
The repeated prime value was equal to 0xC000...0077 for 512-bit RSA keys and 0xC000...00E9B for 1024-bit RSA keys.
These prime values correspond to the first Blum prime generated when starting from the value 0xC000...000 in each case.The probable cause of such an error is the following sequence of events during prime generation: 1) The random number generator of the card was called but failed to produce a random number, either by returning a value with all bits set to zero or by returning nothing into the output memory, which had previously been zeroed.
2) The candidate prime value q (equal to 0 at the time) had its highest four bits fixed to 1100 2 (to obtain a modulus of the required length 13 when multiplied by the prime p), resulting in a value of 0xC0 in the most significant byte.3) The candidate prime value was tested for primality and increased until the first prime with the required properties (a Blum prime in the case of the Oberthur Cosmo Dual 72K) was found (0xC000...0077 in the case of 512-bit RSA).
The faulty process described above that leads to the observed predictable primes may also occur for other cards or software libraries as a result of multiple causes (e.g., an ignored exception in random number generation or a programming error).
We therefore inspected our key pair dataset, the TLS IPv4 dataset [9] and the PGP dataset [30] for the appearance of such primes relevant to key lengths of 512, 1024 and 2048 bits.
Interestingly, no such corrupt keys were detected except for those already described.Note that a random search for a prime is much less likely to fail in this mode.
Even if some of the top bits and the lowest bit are set to one, the resulting value is not a prime for common MSB masks.
New values will be generated if the starting value contains only zeroes.
Analysis of power consumption traces is a frequently used technique for card inspection.
The baseline power trace expected should cover at least the generation of random numbers of potential primes, primality testing, computation of the private exponent and storage of generated values into a persistent key pair object.
We utilized the simple power analysis to reveal significant features like random number generation, RSA encryption, and RSA decryption operation, separately.
By programming a card to call only the desired operation (generate random data, encrypt, decrypt), the feature pattern for the given operation is obtained.
These basic operations were identified in all tested types of cards.
Once identified, the operations can be searched for inside a more complex operations like the RSA key pair generation.A typical trace of the RSA key pair generation process (although feature patterns may differ with card hardware) contains: 1) Power consumption increases after the generating key pair method is called (cryptographic RSA coprocessor turned on).
2) Candidate values for primes p and q are generated (usage of a TRNG can be observed from the power trace) and tested.
3) The modulus and the private exponent are generated (assumed, not distinguishable from the power trace).
4) Operation with a private key is executed (decryption, in 7 out of 16 types of cards) to verify key usability.
5) Operation with a public key is executed (encryption, 3 types of cards only).
Note that even when the key generation process is correctly guessed, it is not possible to simply implement it again and compare the resulting power traces -as only the card's main CPU is available for user-defined operations, instead of a coprocessor used by the original process.
Additional side-channel and fault induction protection techniques may be also applied.
Therefore, one cannot obtain an exactly matching power trace from a given card due to unavailability of low-level programming interfaces and additionally executed operations for verification of key generation hypothesis.Whereas some steps of the key generation, such as the randomness generation, take an equal time across multiple runs of the process, the time required to generate a prime differs greatly as can be also seen from the example given in Figure 7, where timing is extracted from the power trace.
The variability can be attributed to the randomized process of the prime generation.
Incremental search will find the first prime greater than a random number selected as the base of the search.
Since both primes p and q are distributed as distances from a random point to a prime number, the resulting time distribution will be affected by a mixture of these two distributions.In samples collected from 12 out of 16 types of cards, the distribution of time is concentrated at evenly spaced points 14 as seen in Figure 7.
The distance between a pair of points is interpreted as the duration of a single primality test, whereas their amount corresponds to the number of candidates that were ruled out by the test as a composite.
Then it is possible to obtain a histogram of number of tested candidates, e.g., by binning the distribution with breaks placed in the midpoints of the empty intervals.
We experimentally obtained distributions for a number of needed primality tests for different parameters of trial division.
Then we were able to match them with distributions from several cards, obtaining a likely estimate for the number of primes used by the card in the trial division (sieving) phase.
For some types of cards, a single parameter did not match distributions of neither 512-bit nor 1024-bit keys.
There may exist a different optimal value of trial division tests and primality tests for different key lengths.
Notably, in some cases of cardgenerated 512-bit keys, the number of primality tests would have to be halved to exactly match a referential distribution.
However, we are not aware of a mechanism that would perform two primality tests in parallel or at least in the same time, as is required for testing a candidate of double bit length.The exact time distribution for software implementations is of less concern since the key generation process tends to be much faster on an ordinary CPU.
The source code can be modified to accommodate for counting the number of tests directly (as shown in the inlay in Figure 7) without relying on time measurement that may be influenced by other factors specific to the implementation.
This paper presents a thorough analysis of key pairs generated and extracted from 38 different sources encompassing open-source and proprietary software libraries and cryptographic cards.
This broad analysis allowed us to assess current trends in RSA key pair generation even when the source codes for key generation were not available, as in the case of proprietary libraries and cards.
The range of approaches identified indicates that the question of how to generate an optimal RSA key has not yet been settled.
The tested keys were generally found to contain a high level of entropy, sufficient to protect against known factorization attacks.
However, the source-specific prime selection algorithms, postprocessing techniques and enforcement of specific properties (e.g., Blum primes) make the resulting primes slightly biased, and these biases serve as fingerprints of the sources.
Our paper therefore shows that public moduli leak significantly more information than previously assumed.
We identified seven properties of the generated primes that are propagated into the public moduli of the generated keys.
As a result, accurate identification of originating library or smartcard is possible based only on knowledge of the public keys.
Such an unexpected property can be used to decrease the anonymity set of RSA keys users, to search for keys generated by vulnerable libraries, to assess claims regarding the utilization of secure hardware by remote parties, and for other practical uses.
We classified the probable origins of keys in two large datasets consisting of 10 and 15 million (mostly) TLS RSA keys and 1.4 million PGP RSA keys to obtain an estimate of the sources used in real-world applications.The random number generator is a crucial component for the generation of strong keys.
We identified a generic failure scenario that produces weak keys and occasionally detected such keys in our dataset obtained from the tested cards.
Luckily, no such weak key was identified in the datasets of publicly used RSA keys.
