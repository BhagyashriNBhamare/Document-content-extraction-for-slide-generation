We study the problem of maintaining large replicated collections of files or documents in a distributed environment with limited bandwidth.
This problem arises in a number of important applications, such as synchronization of data between accounts or devices, content distibution and web caching networks , web site mirroring, storage networks, and large scale web search and mining.
At the core of the problem lies the following challenge, called the file synchronization problem: given two versions of a file on different machines, say an outdated and a current one, how can we update the outdated version with minimum communication cost, by exploiting the significant similarity between the versions?
While a popular open source tool for this problem called rsync is used in hundreds of thousands of installations, there have been only very few attempts to improve upon this tool in practice.
In this paper, we propose a framework for remote file synchronization and describe several new techniques that result in significant bandwidth savings.
Our focus is on applications where very large collections have to be maintained over slow connections.
We show that a prototype implementation of our framework and techniques achieves significant improvements over rsync.
As an example application, we focus on the efficient synchronization of very large web page collections for the purpose of search, mining, and content distribution.
Consider the problem of maintaining large replicated collections of files, such as user files, web pages, or other documents, over a slow network.
In particular, assume that we have two machines, ¡ and ¢ , that each hold a copy of the collection, and that files are frequently modified at one of the machines, say ¡ .
Periodically, machine ¢ initiates a synchronization operation that updates all its replicas to the latest version.
This operation involves identifying all files that have changed, deciding which version of the file is the latest one (if files can be changed at either location), and finally updating £ Work supported by NSF CAREER Award NSF CCR-0093400 and by New York State through the Wireless Internet Center for Advanced Technology (WICAT) at Polytechnic University.
the files that have changed.
Given the size of the collections, we are interested in performing the synchronization with a minimum amount of communication over the network.
The above scenario arises in a number of applications, such as synchronization of user files between different machines, remote backups, mirroring of large web and ftp sites, content distribution, and web search engines, to name just a few.
In many cases, updated files differ only slightly from their previous version; for example, updated web pages usually change only in a few places.
In this case, instead of sending the entire updated version over the network, it would be desirable to be able to perform the update by sending only an amount of data proportional to the change between the two versions.In this paper, we focus on this problem of updating files in a bandwidth efficient manner; we refer to this as the file synchronization problem.
Our work is primarily motivated by several applications in large scale web search and content distribution discussed later, but our techniques are applicable to the more general case and we believe that file synchronization is a fundamental operation in distributed systems.
We note that there is a very widely used open source software tool called rsync that addresses exactly this problem and that is described in [47,48].
Our goal is to derive techniques that achieve significant savings over rsync particularly in the case of large collections and slow networks.Before continuing, we point out a few assumptions.
We assume that collections consist of unstructured files that may be modified in arbitrary ways, including insertion and deletion operations that change byte and page alignments between different versions.
Thus, approaches that identify changed disk pages or bit positions or that assume fixed record boundaries do not work -though some of them are potentially useful for identifying those files that have been changed and need to be synchronized.
We also note that the problem is much easier if all update operations to the files are saved in an update log that can be transmitted to the other machine.
However, in many cases such logs are not available.
We are also not concerned with issues of consistency in between synchronization steps, and with the question of how to resolve conflicts if changes are simultaneously performed at several locations [3,39].
It is left up to an application to decide when and how often files should be synchronized.
The rsync file synchronization tool is currently widely used to exchange user files between different machines (e.g., between a machine at work and a machine at home), to mirror web and ftp sites, and to perform backup over a network.
In addition to these general scenarios, we are particularly interested in the following potential applications:1.
Sharing crawled web pages for mining and search: One of the main bottlenecks in large-scale web search and mining is the cost of crawling large sets of web pages and then keeping these pages up to date.
A lot of recent work has focused on efficient strategies for recrawling changing web pages based on their past history of updates [6,7,8,9,14].
An alternative or complementary approach would be to share the results of recrawls between many parties.
For example, several organizations or even nodes in a P2P system [19] could independently perform crawls and later exchange their results, or one centralized high-performance crawler could allow clients to obtain the latest versions of web pages of interest.
As an example of the latter, the Stanford WebBase project [20] enables researchers at other institutions to receive a feed of web pages from the WebBase collection.
However, due to bandwidth limitations, large data sets today are still frequently shared via "sneakernet", i.e., by sending disks or tapes by mail [18].
Use of file synchronization would allow other organizations to receive updated content over the network at a fraction of the current bandwidth cost.
In fact, our main motivation for this work is to build a system for efficiently sharing large recrawls over a wide area network.2.
Maintaining massive collections at clients: Given the speed at which hard disk capacity is currently expanding, several researchers have considered the possibility of storing and maintaining the entire web, or large parts of the world's content, at desktop machines.
For example, Garcia-Molina [17] outlined a scenario where the world's changing content is distributed to end users in monthly or weekly updates shipped out on future versions of CDs or DVDs.
One could also imagine instead using file synchronization to maintain large up-to-date collections at desktop clients for personalized browsing, search, or mining.
Browsers such as Internet Explorer already allow users to subscribe to web pages that are then periodically downloaded and stored; the proposed techniques could be used to significantly improve the efficiency of this process.3.
Server-Friendly Web Crawling: Closely related to the first scenario is the idea of integrating file synchronization into web servers to support more efficient recrawling.
We caution that there have been previous proposals to modify servers for this purpose [5,16], but that widespread adoption of such schemes appears unlikely for various reasons.
Several companies in the CDN space have studied and deployed file synchronization techniques.
We are not aware of any published work in this direction, but file synchronization techniques are a natural approach for updating content that is widely replicated at the network edge.5.
Replication in P2P File Sharing: While much of the content in current file sharing networks is static, file synchronization could be used to maintain dynamic content that is replicated for fault tolerance or performance.All of these scenarios have in common that they involve massive amounts of data, and thus bandwidth efficiency is of primary importance.
Our goal is to design improved protocols for file synchronization that use multiple roundtrips to significantly decrease the amount of data sent over the network.
We now define the file synchronization problem and describe the algorithm of Tridgell and MacKerras [47,48], which forms the basis of the widely used rsync tool.
1 We note that a similar approach was proposed by Pyne in a US Patent [38].
The setup for the file synchronization problem is as follows.
The basic approach in rsync, as well as in our algorithms, is to split a file into blocks and use hash functions to compute hashes or "fingerprints" of the blocks.
These hashes are then sent to the other machine, where the recipient attempts to find matching blocks in its own file.
One issue is the lack of alignment between matching blocks in the two files; this is addressed by comparing received hashes not just with the corresponding block in the other file, but with all substrings of the same size.
For efficiency, hashes are composed from two different hash functions, a fast but unreliable one, and a very reliable one that is more expensive to compute.
Then the steps in rsync are as follows:1.
At the client: (a) Partition % into blocks ¢ B A 8 C % ') $ D E § G F H ) P I R Q T S $ D ¨ U V Q W 0 of some block size D .
(a) Use the incoming stream of symbols and indices of hashes in¤ # to reconstruct ¢ ¡ ¤ £ ¦ ¥ .
The process is illustrated in Figure 2.1.
All symbols and indices sent from server to client in steps (iii) and (iv) are also compressed using an algorithm similar to gzip.
A checksum on the entire file is used to detect the (fairly unlikely) failure of both checksums, in which case the algorithm could be repeated with different hashes, or we can simply transfer the entire file.
The reliable checksum is implemented using MD4 ( Q ¨ § © bits), but only two bytes of the MD4 hash are used since this provides sufficient power.
The unreliable checksum is implemented as a § -bit "rolling checksum" that allows efficient sliding of the block boundaries by one character, i.e., the checksum for( '@ I Q ¤ § 1 @ I D © 0 can be computed in con- stant time from ( '@ ¤ § @ I D U R Q W 0 .
Thus,bytes per block are transmitted from client to server.
Clearly, the choice of block size is critical to the performance of the algorithm, but this choice depends on the degree of similarity between the two files -the more similar, the larger the optimal block size.
Moreover, the location of changes in the file is also important.
If a single character is changed in each block of ¤ # , then no match will be found by the server and rsync will be completely ineffective; on the other hand, if all changes are clustered in a few areas of the file, rsync will do well even with a large block size.
Given these observations, some basic performance bounds based on block size and number and size of file modifications can be shown.
However, rsync does not have any good performance bounds with respect to common metrics such as edit distance [34].
In practice, rsync uses a default block size of [10,25,34] are based on recursive splitting of unmatched blocks.
We will also adopt this approach, but combine it with several other ideas that save on communication costs and allow us to utilize much smaller block sizes efficiently.
We note that recursive splitting, as some of our other techniques, increases the number of roundtrips between the two parties.
However, as in rsync itself, the roundtrip latencies are not incurred for each file since many files can be processed simultaneously.
Thus, for large collections additional roundtrips are not a problem.
For small files, e.g., to fetch individual web pages, a less bandwidth efficient algorithm based on a single roundtrip would be preferable.
We study the file synchronization problem for large collections of files and documents, and propose and evaluate new techniques that significantly improve on previous approaches in terms of bandwidth usage.
Our main contributions are:We describe a framework for file synchronization algorithms that partitions the problem into two phases, map construction, where the two parties use a multiround protocol to determine the common parts of the corresponding files, and delta compression, where the remaining parts are encoded in relation to the common parts and then transmitted to the other side.
The framework allows for a variety of algorithms and techniques.Within the framework, we describe and implement a number of known and new techniques.
In particular, we use recursive partitioning as proposed in [10,25,34].
We introduce new techniques for extending matches via "continuation hashes" and for the optimized verification of suspected matches in the two files, plus several other optimizations.
The techniques are related to classical problems in group testing and "searching with liars" (also known as Ulam's Problem), and insights from these problems may lead to additional moderate improvements in the future.We evaluate the framework and techniques on several data sets, including a large set of changing web pages that we recrawled daily over several weeks.
The results show that our algorithm allows the maintenance of large file collections with significantly lower communication costs than the widely used rsync tool, and in many cases comes within 2 of the best delta compressor (which provides a reasonable lower bound in practice).
The remainder of this paper is organized as follows.
In the next section, we discuss some related work.
Section 5 describes the new framework and algorithmic techniques.
Section 6 presents the experimental evaluation of our implementation.
Finally, Section 7 discusses some limitations of our current results and open problems for future research.
The most important previous work is the rsync file synchronization algorithm proposed by Tridgell and MacKerras [47,48], which is the basis of the very widely used rsync open source tool.
There are a number of theoretical studies of the file synchronization problem [10,15,32,33], also called the document exchange problem in [10].
In particular, Orlitsky [32,33] presents almost tight bounds for the problem with varying numbers of communication phases, under some assumptions about the assumed file distance metric.
However, many of the theoretical algorithms assume that a hash function is reversed as part of the decoding operation; while this is allowable under the standard model for communication complexity [24], it makes the algorithms impossible to implement in practice.
An exception are algorithms proposed in [10,15,25,34] that are based on recursive partitioning of blocks similar to our implementation.
These algorithms can also be shown to achieve provable bounds with respect to some common file distance measures.
Some limited experimental results are given in [25,34].
Recent work in [40] presents a version of the rsync algorithm that updates files in-place without using additional temporary space.Delta compression is the problem of encoding one file relative to another similar file, where both files are available during the encoding.
Thus, file synchronization can be seen as a distributed version of delta compression where the two files are located at different machines.
Our framework reduces the file synchronization problem to delta compression; on the other hand any algorithm for file synchronization also solves the delta compression problem, though typically at significantly higher cost.
Some available open source tools for delta compression are described in [23,26,46], and an overview of delta compression and file synchronization techniques and their applications is given in [45].
A number of authors have studied problems related to identifying disk pages, files, or data records that have been changed or added or deleted, or that differ between two or more replicas; see, e.g., [1,4,27,28,29,30,36,42].
These problems differs from ours in that data is assumed to be partitioned into fixed units such as pages, records, or files that are treated as atomic.
The work is nonetheless related to ours in two ways.
First, it addresses the problem of efficienctly identifying files that have changed in scenarios where almost all objects are unchanged; afterwards, our file synchronization techniques can be applied to update those files.
We do not focus on this aspect and instead use a fingerprint for each file as this is efficient enough for our data sets.
Second, some of the results [27] are also based on techniques from Group Testing, while others are based on Error Correcting Codes and probably not as useful in our context.In addition to rsync, there are many other tools for synchronizing data between different machines.
Some of these tools, such as Microsoft's ActiveSync, Palm's HotSync, or Puma Technologies' IntelliSync, are used to synchronize data between a desktop or online account and a mobile device.
They typically transfer the entire file if a change has occurred, though for record-based data such as appointments and contacts only updated records are transmitted in most cases.
Recent work in [2,44] surveys and studies synchronization techniques for handheld devices, while [3,39] discuss correctness issues when files are modified at several locations.Hash-based techniques similar to rsync have been explored by the OS and Systems community for purposes such as compression of network traffic [43], distributed file systems [31], distributed backup [11], and web caching [41].
These techniques use string fingerprinting techniques proposed by Karp and Rabin [22] to partition a data stream into blocks in a consistent manner on both sides of a communication link, and then send hash values to encode repeated substrings.
Group Testing is a set of combinatorial problems, introduced by Dorfman [12], that deal with identifying "defective" elements in a set through a sequence of simple tests on subsets.
We are not aware of previous results on the exact version of the group testing problem that arises in our work here.
Group testing was used by Madej [27] to identify files that have changed.
Search problems with liars were introduced by Ulam [49] and have been studied extensively over the last 50 years; see the recent survey of Pelc [37] for an overview.
In particular, Pelc discusses a relationship of the problem to communication over noisy channels.
On the other hand, Orlitsky and Viswanathan [35] recently established a relationship between Error Correcting Codes for noisy channels and one-way communication problems where a receiver has related information.Finally, several recent studies look at the type and frequency of web page updates and propose efficient strategies for refreshing pages or other objects based on observations about their past behavior [6,7,8,9,13,14].
These techniques are complementary to ours in the context of our web page update application.
Of course, if file synchronization were to become widely deployed at web servers, then this would change the cost model assumed in current recrawling strategies.
We now describe our technical contributions.
We first introduce our basic framework in the next subsection, and then give a detailed description of techniques for the map construction phase of our framework in Subsections 5.2 to 5.5.
Finally, Subsection 5.6 summarizes the resulting protocol.
Recall that the client !
has a copy of an outdated file # , and the server " has a copy of the current file¤ ¡ ¤ £ ¦ ¥ .
The goal is to design a protocol between the two parties that results in !
obtaining a copy of ¤ ¡ ¤ £ ¦ ¥ , while minimizing the communication cost.
As we saw in the description of the rsync algorithm, hash values can be used to identify common substrings in both files, allowing the recipient to learn about the structure of the file at the other machine.All the algorithms in our framework consist of the following two phases:(1) Map construction:: in this phase, the two parties use multiple roundtrips to create an approximate representation of the parts of the two files that are identical.
In particular, the client will generate a map of the current file with ') 0 C ( ') 0 or ') 1 0 C "?"
for ) C 2 § § ¦ 4 U Q .
In other words, is identical to in some areas (called the known areas), and unknown in the other areas, labeled by "?"
.
Throughout the remainder of this section, we will focus on optimized techniques for the map construction phase, since good delta compression tools for the second phase are already available from several sources.
Note that in the above example, hashes are transmitted from server to client, while in rsync the client sends hashes to the server.
We could in principle also send hashes from client to server, and then have the server build a map of the client file We now focus on techniques for efficient map contruction.
While the example in the previous section might indicate that there is not much to do apart from repeatedly sending hashes, the problem turns out to be surprisingly rich in terms of possibilities.
All the following techniques are based on exchanging hash values, but use various ideas to minimize the number of bits needed for the hashes.
In particular we employ the following ideas, described in more detail further below:(a) Recursive splitting of the block size by powers of § .
This technique was already used in [10,25,34].
We start out with a block size of & 2or some other power of § , and split any blocks that remain unmatched by a factor of § in each round.
This technique is straightforward and does not require additional explanation.
(b) Optimized match verification to minimize the number of bits that are needed to verify, beyond a reasonable doubt, that substrings in the two files match.
The idea is to first send a fairly weak hash that can be used to identify a possible match, and then use an optimized protocol based on ideas from group testing to filter out any false matches.
A more limited version of this approach was also proposed in [25].
(c) Local and continuation hashes to decrease the number of bits that have to be initially sent in certain cases.
In particular, continuation hashes are very weak hashes that are used to entend known matches towards the left and right in both files, allowing us to recurse down to block sizes for which "global" hashes (i.e, hashes that are matched against all positions in & % ) are too expensive.
Local hashes trade off these two cases.
The case of continuation hashes can be modeled as a version of the problem of "searching with liars", also called Ulam's problem [37,49].
(d) Decomposable hash functions to decrease the number of bits used for the initial hashes for finding possible matches.
The simple idea is that if we have already transmitted a hash value for the parent block and the left sibling, then for certain types of hash functions we could compute the hash value of the right sibling from the other two hashes.
In practice though, designing appropriate hash functions to implement this is nontrivial.In the remainder of this section, we describe these ideas in more detail.
Subsection 5.6 summarizes the protocol.
Suppose bits to allow the client to identify candidates for matches; these candidates are then verified in an optimized manner, by having the client send verification hashes of its matches back to the server.
We use three different ideas for this:(i) The client can send one verification hash for each candidate back to the server.
This hash is then compared only against the block that generated the original hash.
The advantage is that verification hashes are only issued by the client for those hashes that found a possible match in % , which is typically a minority of hashes.
(ii) The client can send one verification hash for several candidate matches, in essence asking: Are all these matches correct?
This should only be done once some degree of confidence is achieved, since one bad apple may cause the entire group to fail the test.
(iii) After a group match has failed, we can try to salvage some of the elements in the group by reissuing hashes for individual candidates or smaller groups.Thus, we propose using a sequence of tests on various subsets of the candidates to efficiently identify those that are correct matches with fairly high certainty.
This can be modeled by the following group testing problem [12], where false matches correspond to defective items and our goal is to identify all nondefective items, or a large subset of them, through a sequence of question of the following type: Are all elements in a specified group nondefective?The answer to the question is unreliable in the following sense: If all elements are nondefective, the answer is always correct, but if there exists a defective item, then the correct answer is only returned with probability Q U .
The cost of each question is ¡ I Q (a ¡ -bit hash for the subset and one bit for the reply), and the goal is to reliably identify nondefective items with minimum cost.
We are not aware of previous results on this precise version of the problem.
In practice, it appears that using only two or three batches of tests already gives close to optimal results, as we see later.
In the previous subsection, we saw that at least ) in the following sense: if the correct answer is "© " ("go to the right") then this answer is always returned; otherwise with probability 61 a wrong answer is returned.
The cost of each query is again ¡ I Q .
Known results show that it is not optimal to verify each answer with a high probability before going down one level in the search tree.
However, in our case we are performing many such searches concurrently, and we could perform group testing across these searches to more efficiently verify answers on each level; it is not clear which strategy is best in this case.
The same reasoning that motivated continuation matches also leads to another possible optimization.
Suppose that a block at a particular level results in a confirmed match in the client file.
In that case, it is unlikely that the sibling of this block would also find a match because (1) any match that is a continuation of the first match would have already been discovered at the parent level, and (2) any other match is unlikely since the match found by the first sibling will likely extend at least slightly into the other block.
Thus, if we split the processing for each block size into two phases, first a search for matches using continuation hashes on blocks adjacent to confirmed matches, and then a search using global or local hashes, then in the second phase we can omit sending hashes for any blocks whose sibling found a confirmed match in the first phase (and also for any blocks for which continuation hashes were sent but no matches found).
We could extend this idea by also first sending hashes, say, for all left siblings, and then only for those right siblings whose left sibling did not find a match.
We did not implement this last idea since a technique described in the next subsection already avoids sending hashes for both siblings in this case.
We did implement the idea of first sending continuation hashes, and then global hashes, and observed some moderate benefits.
, and it should be strong in the sense that different strings should have a probability close to 61 of mapping to the same hash value, for a ¡ -bit hash.
Finally, strings that can be obtained from each other through permutation should not be mapped to the same hash too often, as such cases are quite common in practice.
Some of the techniques used in practice are also limited to certain ranges of block sizes.
There are trade-offs between some of these properties, and in the end we designed our own modification of the Adler checksum in rsync, which appears to perform well on all our data sets and block sizes.
We integrate all of the above techniques into a protocol consisting of a sequence of rounds, one for each block size.
Each round consists of one or more roundtrips of communication.Consider the left part of Figure 5.2 for an example.
The round starts with the server sending a set of hashes to the client; these hashes may be global, local, or continuation hashes and are usually strong enough to identify candidates for matches, but not strong enough to reliably verify the matches.
The decomposability of the hash function is implemented at a lower level by surpressing the transmission of hash bits that can be computed from sibling and ancestor hashes.
The client then replies with a bitmap specifying which hashes found a match candidate, immediately followed by a set of verification hashes for the candidates.
Each verification hash, based on MD5, can be for a single candidate or a group of candidates.
The server than receives and checks the verification hashes, and replies with a bitmap specifying which verification hash was confirmed.
If the round consists of a single batch of verification hashes, then this bitmap is included into the first roundtrip of the next round, and is immediately followed by hashes for the next smaller block size.
Otherwise, the client may send additional batches of verification hashes.
On the right side of Figure 5.2 we see a simple example consisting of two rounds.
The client first sends a request to the server, who then sends hashes for the first block size.
In the example, the first round has a single batch of verification hashes, while the second round involves two batches, maybe first a set of weak hashes for each candidate, and then stronger hashes for small subsets of or We now report on a preliminary experimental evaluation of our prototype software.
We first describe the experimental setup in terms of implementation, data sets, and the other tools that we compare against.
Subsection 6.2 provides results on two data sets previously used to evaluate delta compression tools, and Subsection 6.3 looks at the case of our web page update application.
We note that our work is still ongoing and thus some numbers may improve slightly in subsequent versions.
We implemented a prototype with the techniques from the previous section in around Q & 2 & 2 lines of C code.
The program was compiled with gcc and was run on several Linux servers.
The prototype was not optimized in terms of CPU performance yet, and thus we do not focus too much on this aspect.
We used the zdelta delta compressor 2 for the delta compression step, and used a very simple but efficient hash table based on double hashing to search for matching blocks.
The prototype can be configured easily to vary the number of rounds and the set of techniques that are applied in each round, allowing us to look at the impact of each technique.For the hashes sent from server to client, we used two implementations: a high quality one based on MD5 that is not rolling or decomposable (thus requiring significantly more running time and moderately more bandwidth) and a more heuristic construction of a hash function that is both rolling and decomposable.
For the verification hashes, we used another MD5-based hash in both implementations, since in this case we do not need the other properties.
As in rsync, there is a small probability of failure on each file.
Such failures are detected through the exchange of a very strong Q -byte hash value for each file in the beginning; this also allows our code to detect unchanged files at that point.
We compare our prototype implementation to rsync with default block size, rsync with an optimally chosen block size for each individual file, and the zdelta [46] and vcdiff [23] delta compressors.
We note that current delta compressors are already fairly optimized in terms of compression, and thus provide a reasonable bound on what we could hope to expect from a file synchronization tool that does not have access to the outdated file (guarding a major breakthrough in delta compression techniques).
We used the following data sets for our evaluation:1.
The gcc and emacs data sets used for evaluating delta compression performance in [21,45], consisting of versions 2.7.0 and 2.
, and § days later, respectively.
In the experiments, we measure the cost of updating the base set to one of the updated sets.
We first look at the performance of a very basic version of our protocol that uses a decomposable hash function, recursive halving of blocks, and a separate verification hash for each candidate match, but none of the other techniques.
In Figures 6.1 and 6.2, we give results for the gcc and emacs data sets, respectively.
For each block, our protocol sends a hash of 2 4 3FS8 I bits to identify candidates, and the client replies with a bytes, and we plot the total cost in KB for the entire data set under different minimum block sizes after which we terminate the recursion.
We also show results for rsync with default block size, an idealized rsync that knows the best block sizes for each file, and the zdelta delta compressor.
For rsync, we show the costs of client-to-server and server-to-client communication, and for our protocol we show the cost of server-toclient and client-to-server communication during the map construction phase and the cost of the final delta (in order from bottom to top).
As we see, recursive halving and the use of decomposable hashes and verification hashes already gives significant benefits.
The recursion should be stopped around a block size of Q § © or bytes for best results.
(At that point, the increase in the cost of the map construction phase becomes higher than the decrease in the delta compression phase.)
However, the best result is still about a factor of § away from the performance of the highly optimized delta compressor.
We note that without decomposable hash functions, the amount of data sent from server to client in the map building phase would be about twice as high, and as a result the optimal minimum block size is also slightly larger in that case.Next, we consider the benefit of using an improved match verification approach and continuation hashes in the protocol.
In particular, we send -bit continuation hashes for all We see from the leftmost bars in the two groups that the simple single-roundtrip group verification already gives some improvements over the corresponding numbers in Figure 6.1.
For the gcc data sets, the best choice appears to be to use continuation hashes down to Q -byte blocks; for emacs it would be © -byte blocks (not shown).
Also, we see that when using continuation hashes down to Q -byte blocks, having a minimum block size for global hashes of Q § © bytes becomes better than bytes.
We also experimented with the use of local hashes as described in Subsection 5.4; however, we were unable to get any significant improvements from this technique.
One issue here is the "harvest rate", i.e., the percentage of hashes that result in confirmed matches.
Not surprisingly, blocks that qualify for continuation hashes have a fairly high harvest rate, which is another reason why they can be profitably used for much smaller block sizes.
Local hashes do not fare well in this context, though we plan to revisit this issue in future experiments.
We now look at the impact of the optimized match verification techniques from Subsection 5.3.
In particular, we compare the following five strategies (note that we always send 2 5 3F ¤ S bits less from server to client for continuation hashes):(1) the trivial verification with § -bit hashes for each candidate, used in the first experiment, (2) the slightly smarter approach above where we used The results in Figure 6.4 show slight improvements for each method for gcc (the same holds for emacs).
However, almost all benefits are obtained with only one or two roundtrips.
We experimented with a number of other settings, but none did significantly better than the best one shown.
In particular, we did not find any benefit in being very aggressive about grouping large numbers of fairly uncertain candidates and then trying to salvage candidates from failed groups.
Instead, it appears to be preferable to slowly grow the size of the groups as our confidence in the candidates grows.
There are a number of subtle tradeoffs at work here: for example, decreasing the number of bits sent to the server from but results in some real matches being lost due to false positives taking their place, and ultimately a larger delta.
There are several other minor optimizations that we implemented, such as first sending continuation hashes and then global hashes in the next roundtrip, or the selective use of local hashes.
None of these showed significant improvements.
In Table 6.1 we show the best results that we were able to obtain by using all techniques; we note that this results in a total of more than 2 roundtrips and is thus probably not good in practice since each roundtrip also requires some computation (and sometimes a scan of the files).
Looking at as for gcc this results in a data rate of only a few hundred kbits/s over the network.
For faster networks and highly redundant data sets, CPU performance would currently be a bottleneck.
Finally, our current implementation is also not optimized in terms of memory consumption.
We now look at the web page update application that motivated our work.
Recall that we are given ten thousand pages, selected at random from the web, that were recrawled every night for several weeks.
Each set has a total size of around Q 2 MB, with about Q KB per page on average.
Some of the files are not updated at all between crawls, while others change only slightly.
In Table 6.2, we show the bandwidth cost of maintaining such collections at a client using our techniques, under different update frequencies.
We observe that our techniques support the maintenance of very large replicated sets of web pages even over fairly slow links, and improve over rsync by nearly a factor of § .
For example, if we synchronize the pages every two days then slightly more than § MB of data transfer suffices to maintain Q 2 & 2 ¤ 2 & 2 pages at a client PC, which is easily done over cable or DSL links.
The result shown are the best we could get for our protocol with all optimizations, but as before there are simpler settings with fewer roundtrips that perform within to of optimal.
There are a number of unresolved issues and open problems left by our work.
First, the current implementation is a very early prototype, and we have not optimized it in terms of CPU performance.
This may result in the CPU becoming a bottleneck for faster networks.
While some overhead is due to the repeated passes over the data in the different communication phases, we believe that significant improvements are possible through careful optimization.
Some minor additional improvements in bandwidth efficiency should also be possible.
We plan to integrate our implementation into a system for maintaining large sets of changing web pages over wide area networks.
We also intend to use the presented techniques as the basis for a new general purpose tool for file synchronization over slow links that we plan to release.
Ideally, such a tool would be adaptive and thus choose the best set of parameters and number of roundtrips based on the characteristics of the data set and communication link.
On the theoretical side, we are working on improved asymptotic bounds for file synchronization under some common file similarity metrics.
Interestingly, the idea of continuation hashes used in this paper appears to be very promising in this context as well.
A detailed study of the group testing and "searching with liars" problems discussed in this paper might also lead to slight improvements in practice.
We are also studying how to improve file synchronization if we are restricted to just one or two round-trips.
In this case, it seems difficult to improve significantly over rsync in practice, but we believe that at least some moderate gains are possible.
Finally, we plan to look at synchronization in asymmetric cases, e.g., in cases with server broadcast capability, lower upload speed, or a bottleneck at a busy server.
