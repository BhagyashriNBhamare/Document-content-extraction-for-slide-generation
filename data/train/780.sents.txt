This paper proposes and evaluates Prefetching B +-Trees pB +-Trees, which use prefetching to accelerate two important operations on B +-Tree indices: searches and range scans.
To accelerate searches, pB +-Trees use prefetching to eeectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages.
These wider nodes reduce the height of the B +-Tree, thereby decreasing the number of expensive misses when going from parent t o c hild without signiicantly increasing the cost of fetching a given node.
Our results show that this technique speeds up search and update times by a factor of 1.221.5 for main-memory B +-Trees.
In addition, it outperforms and is complementary to Cache-Sensitive B +-Trees."
To accelerate range scans, pB +-Trees provide arrays of pointers to their leaf nodes.
These allow the pB +-Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.
Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys.
Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency.
As the gap between processor speed and both DRAM and disk speeds continues to grow exponentially, it is becoming increasingly important to make eeective use of caches to achieve high performance on database management systems.
Caching exists at multiple levels within modern memory hierarchies: typically two or more levels of SRAM serves as caches for the contents of main memory in DRAM, which i n turn is a cache for the contents on disk.
While database researchers have historically focused on the importance of this latter form of caching also known as the buuer pool", recent studies have demonstrated that even on traditional disk-oriented databases, roughly 50 or more of execution time is often wasted due to SRAM cache misses 1, 2, 10, 188.
For main-memory databases, it is even clearer that SRAM cache performance is crucial 199.
Hence several recent studies have revisited core database algorithms in an eeort to make them more cache friendly 5, 17, 19, 20, 211.
Figure 1: Execution time breakdown for index operations B+ = B + -Trees, CSB+ = CSB + -Trees.
Index structures are used extensively throughout database systems, and they are often implemented as B + -Trees.
While database management systems perform several diierent operations that involve B + -Tree indices e.g., selections, joins, etc., these higher-level operations can be decomposed into two k ey lower-level access patterns: i searching for a particular key, which i n v olves descending from the root to a leaf node using binary search within a given node to determine which c hild pointer to follow; and ii scanning some portion of the index, which i n v olves traversing the leaves through a linked-list structure for a non-clustered index.
For clustered indices, one can directly scan the database table after searching for the starting key.
While search time is the key factor in single value selections and nested loop index joins, scan time is the dominant eeect in range selections.To illustrate the need for improving the cache performance of both search and scan on B + -Tree indices, Figure 1 shows a breakdown of their simulated performance on a state-ofthe-art machine.
For the sake of concreteness, we pattern the memory subsystem after the Compaq ES40 88|details are provided later in Section 4.
The search" experiment i n Figure 1 looks up 100,000 random keys in a main-memory B + -Tree index after it has been bulkloaded with 10 million keys.
The scan" experiment performs 100 range scan operations starting at random keys, each of which scans through 1 million hkey, tupleIDi pairs retrieving the tupleID values.
The results for shorter range scans|e.g., 1000 tuple scans|are similar.
The B + -Tree node size is equal to the cache line size, which i s 6 4 b ytes.
Each bar in Figure 1 is broken down into three categories: busy time, data cache stalls, and other stalls.
Both search and scan accesses on B + -Tree indices the bars labeled B+"|we will explain the CSB+" bar later spend a signiicant fraction of their time|65 and 84, respectively|stalled on data cache misses.
Hence there is considerable room for improvement.
In an eeort to improve the cache performance of index searches for main-memory databases, Rao and Ross proposed two new types of index structures: Cache-Sensitive Search T rees" CSS-Trees 199 and Cache-Sensitive B + -T rees" CSB + -Trees 200.
The premise of their studies is the conventional wisdom that the optimal tree node size is equal to the natural data transfer size, which corresponds to the disk page size for disk-resident databases and the cache line size for main-memory databases.
Because cache lines are roughly two orders of magnitude smaller than disk pages e.g., 64 bytes vs. 4 Kbytes, the resulting index trees for main-memory databases are considerably deeper.
Since the number of expensive cache misses is roughly proportional to the height of the tree, it would be desirable to somehow increase the eeective fanout also called the branching factor of the tree, without paying the cost of additional cache misses that this would normally imply.To accomplish this, Rao and Ross 19, 200 exploit the following insight: by restricting the data layout such that the location of each c hild node can be directly computed from the parent node's address or a single pointer, we can eliminate all or nearly all of the child pointers.
Assuming that keys and pointers are the same size, this eeectively doubles the fanout of cache-line-sized tree nodes, thus reducing the height of the tree and the number of cache misses.
CSSTrees 199 eliminate all child pointers, but do not support incremental updates and therefore are only suitable for readonly environments.
CSB + -Trees 200 do support updates by retaining a single pointer per non-leaf node that points to a contiguous block of its children.
Although CSB + -Trees outperform B + -Trees on searches, they still perform signiicantly worse on updates 200 due to the overheads of keeping all children for a given node in sequential order within contiguous memory, especially during node splits.Returning to Figure 1, the bar labeled CSB+" shows the execution time of CSB + -Trees normalized to that of B + -Trees for the same index search experiment.
As we see in Figure 1, CSB + -Trees eliminate 20 of the data cache stall time, thus resulting in an overall speedup 1 of 1.15 for searches.
While this is a signiicant improvement, over half of the remaining execution time is still being lost to data cache misses; hence there is signiicant room for further improvement.
In addition, these search-oriented optimizations provide no beneet to scan accesses, which are suuering even more from data cache misses.
Modern microprocessors provide the following mechanisms for coping with large cache miss latencies.
First, they allow multiple outstanding cache misses to be in ight simultaneously for the sake of exploiting parallelism within the memory hierarchy.
F or example, the Compaq ES40 88 supports 32 in--ight loads, 32 in--ight stores, and eight outstanding oo-chip cache misses per processor, and its crossbar memory system supports 24 outstanding cache misses.
Second, to help applications take full advantage of this parallelism, they also provide prefetch instructions which enable software to move data into the cache before it is needed.
Previous studies which did not target databases speciically have demonstrated that for both array-based and pointer-based program codes, prefetching can successfully hide much o f the performance impact of cache misses by o v erlapping them with computation and other misses 13, 166.
Hence for modern machines, it is not the number of cache misses that dictates performance, but rather the amount o f exposed miss latency that cannot be successfully hidden through techniques such as prefetching.In this paper, we propose and study Prefetching B + -Trees pB + -Trees, which use prefetching to limit the exposed miss latency.
T ree-based indices such a s B + -Trees pose a major challenge for prefetching search and scan accesses since both access patterns suuer from the pointer-chasing problem ñ®½™133: The data dependencies through pointers make it diicult to prefetch suuciently far ahead to limit the exposed miss latency.
F or index searches, p B + -Trees reduce this problem by h a ving wider nodes than the natural data transfer size, e.g., eight vs. one cache lines or disk pages.
These wider nodes reduce the height of the tree, thereby decreasing the number of expensive misses when going from parent t o c hild.The key observation is that by using prefetching, the wider nodes come almost for free: all of the cache lines in a wider node can be fetched almost as quickly as the single cache line of a traditional node.
To accelerate index scans, w e introduce arrays of pointers to the B + -Tree leaf nodes which allow us to prefetch arbitrarily far ahead, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range.
Of course, indices may be frequently updated.
Perhaps surprisingly, w e demonstrate that insertion and deletion times actually decrease with our techniques, despite any o v erheads associated with maintaining the wider nodes and the arrays of pointers.
This paper makes the following contributions.
First, to our knowledge, this is the rst study to explore how prefetching can be used to accelerate search and scan operations on B + -Tree indices.
We propose and study the Prefetching B + -Tree pB + -Tree.
Second, we demonstrate that contrary to conventional wisdom, the optimal B + -Tree node size on a modern machine is often wider than the natural data transfer size, since we can use prefetching to fetch each piece of the node simultaneously.
Our approach ooers the following advantages relative to CSB + -Trees: i we a c hieve better search performance because we can increase the fanout by more than the factor of two that CSB + -Trees provide e.g., by a factor of eight; ii we a c hieve better rather than worse performance on updates relative t o B + -Trees, because our improved search speed more than oosets any increase in node split cost due to wider nodes; and iii we do not require fundamental changes to the original B + -Tree data structures or algorithms.
In addition, we nd that our approach i s c omplementary to CSB + -Trees.
Third, we demonstrate how the pB + -Tree can eeectively hide over 90 of the cache miss latency suuered by non-clustered index scans, thus resulting in a factor of 6.558.7 speedup over a range of scan lengths.
While our experimental evaluation is performed within the context of main memory databases, we believe that our techniques are also applicable to hiding disk latency, in which case the prefetches will move data from disk into main memory.The remainder of this paper is organized as follows.
Sections 2 and 3 discuss how p B + -Trees use prefetching to accelerate index searches and scans, respectively.
T o quantify the beneets of these techniques, we present experimental results in Section 4.
Finally, w e discuss further issues and conclude in Sections 5 and 6, respectively.
a Four levels of one-cache-line-wide nodes.
0000 1111 00000 11111 0000 1111 0000 1111 0000 1111 00000 11111 node 1 node 2 cache miss node 0 0 150 300 450 600 750 900 time (cycles) b Three levels of two-cache-lines-wide nodes.
Recall that during a B + -Tree search, we start from the root, performing a binary search in each non-leaf node to determine which c hild to visit next.
Upon reaching a leaf node, a nal binary search returns the key position.
Regarding the cache behavior, we expect at least one expensive cache miss to occur each time we m o v e d o wn a level in the tree.
Hence the numb e r o f c a c he misses is roughly proportional to the height of the tree minus any nodes that might remain in the cache if the index is reused.
Thus, having wider tree nodes for the sake of reducing the height of the tree might seem like a good idea.
Unfortunately, i n the absence of prefetching i.e. when all cache misses are equally expensive and cannot be overlapped, making the tree nodes wider than the natural data transfer size|i.e. a cache line for main-memory databases and a disk page for disk-resident databases|actually hurts performance rather than helps it, as has been shown in previous studies 19, 200.
The reason for this is that the number of additional cache misses at each node more than oosets the beneets of reducing the number of levels in the tree.As a small example, consider a main-memory B + -Tree holding 1000 keys where the cache line size is 64 bytes and the keys, child pointers, and tupleIDs are all four bytes.
If we limit the node size to one cache line, then the B + -Tree will contain at least four levels.
Figure 2a illustrates the resulting cache behavior, where the four cache misses cost a total of 600 cycles on our Compaq ES40-based machine model 88.
If we double the node width to two cache lines, the height of the B + -Tree can be reduced to three levels.
However, as we see in Figure 2b, this would result in six cache misses, thus increasing execution time by 50.
With prefetching, however, it becomes possible to hide the latency of any miss whose address can be predicted sufñ®½™ciently early.
Returning to our example, if we prefetch the second half of each t w o-cache-line-wide tree node so that it is fetched in parallel with the rst half|as illustrated in Figure 2c|we can achieve signiicantly better rather than worse performance compared with the one-cache-linewide nodes in Figure 2a.
The extent to which the misses can be overlapped depends upon the implementation details of the memory hierarchy, but the trend is toward supporting greater parallelism.
In fact, with multiple cache and memory banks and crossbar interconnects, it is possible to completely overlap multiple cache misses.
lustrates the timing on our Compaq ES40-based machine model, where back-to-back misses to memory can be serviced once every 10 cycles, which is a small fraction of the overall 150 cycle miss latency.
Therefore even without perfect overlap of the misses, we can still potentially achieve large performance gains a speedup of 1.25 in this example by creating wider than normal B + -Tree nodes.Hence the rst aspect of our pB + -Tree design is to use prefetching to create" nodes that are wider than the natural data transfer size, but where the entire miss penalty for each extra-wide node is comparable to that of an original B + -Tree node.
We consider a standard B + -Tree node structure: Each non-leaf node is comprised of some number, d ñ®½™ 1, of childptr ñ®½™elds, d , 1 key ñ®½™elds, 2 and one keynum eld that records the number ofkeys stored in the node at most d,1.
All notation is summarized in Table 1.
Each leaf node is comprised of d,1 key ñ®½™elds, d,1 associated tupleID ñ®½™elds, one keynum eld, and one next-leaf eld that points to the next leaf node in key order.
Our rst modiication is to store the keynum and all of the keys prior to any of the pointers or tupleIDs in a node.
This simple layout optimization allows the binary search to proceed without waiting to fetch all the pointers.
Our search algorithm is a straightforward extension of the standard B + -Tree algorithm, and we n o w describe only the parts that change.Search: Before starting a binary search, we prefetch all of the cache lines that comprise the node.Insertion: Since an index search is rst performed to locate the position for insertion, all of the nodes on the path from the root to the leaf are already in the cache before the real insertion phase.
The only additional cache misses are caused by newly allocated nodes, which w e prefetch in their entirety before redistributing the keys.
Prefetching can also be used to accelerate the bulkload of a B + -Tree.
However, because this is expected to occur infrequently, w e focus instead on the more frequent operations of search, insertion and deletion.
As discussed earlier in this section, we expect search times to improve through our scheme because it reduces the number of levels in the B + -Tree without signiicantly increasing the cost of accessing each level.
What about the performance impact on updates?
Updates always begin with a search phase, which will be sped up.
The expensive operations only occur either when the node becomes too full upon an insertion and must be split, or when a node becomes empty upon a deletion and keys must be redistributed.
Although node splits and key redistributions are more costly with larger nodes, the relative frequency of these expensive events should decrease.
Therefore we expect update performance to be comparable to, or perhaps even better than, B + -Trees with single-line nodes.The space overhead of the index is strictly reduced with wider nodes.
This is primarily due to the reduction in the number of non-leaf nodes.
For a full tree, each leaf node contains d,1 hkey, tupleIDi pairs.
The number of non-leaf nodes is dominated by the number of nodes in the level immediately above the leaf nodes, and hence is approximately N dd,1 .
As the fanout d increases with wider nodes, the node size grows linearly but the number of non-leaf nodes decreases quadratically, resulting in a near linear decrease in the non-leaf space overhead.Finally, a n i n teresting consideration is to determine the optimal node size, given prefetching.
Should nodes simply be as wide as possible?
There are two system parameters that aaect this answer.
The rst is the extent to which the memory subsystem can overlap multiple cache misses.
We quantify this as the latency of a full cache miss T1 divided by the additional time until another pipelined cache miss would also complete T next .
We call this ratio i.e. T 1 Tnext the normalized b andwidth B. For example, in our Compaq ES40-based machine model, T1 = 150 cycles, T next = 1 0 cycles, and hence B = 15.
The larger the value of B, the greater the system's ability t o o v erlap parallel accesses, and hence the greater likelihood of beneeting from wider index nodes.
In general, we do not expect the optimal number of cache lines per node w optimal to exceed B, since beyond that point w e could have completed a binary search and moved down to the next level in the tree.
The second system parameter that potentially limits the optimal node size is the size of the cache, although in practice this does not appear to be a limitation given realistic values of B.
In summary, comparing our pB + -Trees with conventional B + -Trees, we expect better search performance, comparable or somewhat better update performance, and lower space overhead.
Having addressed index search performance, we now turn our attention to index range scans.
Given starting and ending keys as arguments, an index range scan returns a list of either the tupleIDs or the tuples themselves with keys that fall within this range.
First the starting key is searched in the B + -Tree to locate the starting leaf node.
Then the scan follows the next-leaf pointers, visiting the leaf nodes in order.
As the scan proceeds, the tupleIDs or tuples are copied into a return buuer.
This process stops when either the ending key is found or the return buuer lls up.
In the latter case, the scan procedure pauses and returns the buuer to the caller often a join node in a query execution plan, which then consumes the data and resumes the scan where it left oo.
Hence a range selection involves one key search and often multiple leaf node scan calls.
Throughout this section, we will focus on range selections that return tupleIDs, although returning the tuples themselves or other variations is a straightforward extension of our algorithm, as discussed in the full paper 77.
As we s a w already in Figure 1, the cache performance of range scans is abysmal: 84 of execution time is being lost to data cache misses in that experiment.
Figure 3a illustrates the problem: a full cache miss latency is suuered for each leaf node.
A partial solution is to use the technique described in Section 2: If we make the leaf nodes multiple cache lines wide and prefetch each component of a leaf node in parallel, we can reduce the frequency of expensive cache misses, as illustrated in Figure 3b.
While this is helpful, our goal is to fully hide the miss latencies to the extent permitted by the memory system, as illustrated in Figure 3c.
In order to do that, we m ust rst overcome the pointer-chasing problem.
Figure 4a illustrates the pointer-chasing p r oblem, which was observed by Luk and Mowry 13, 144 in the context of prefetching pointer-linked data structures i.e. linked-lists, trees, etc. in general-purpose applications.
Assuming that three nodes worth of computation are needed to hide the miss latency, then when node ni in Figure 4a is visited, we w ould like to be launching a prefetch o f n o d e n i +3 .
T o compute the address of node ni+3, w e w ould normally follow the pointer chain through nodes ni+1 and ni+2.
H o w ever, this would incur the full miss latency to fetch ni+1 and then to fetch ni+2, before the prefetch o f n i +3 could be launched, n i i+1 n i+2 n i+3 n . . .. . . thereby defeating our goal of hiding the miss latency of ni+3.
Luk and Mowry proposed two solutions to the pointerchasing problem that are applicable to linked lists 13, 144.
The rst scheme data-linearization p r efetching i n v olves arranging the nodes in memory such that their addresses can be trivially calculated without dereferencing any pointers.
For example, if the leaf nodes of the B + -Tree are arranged sequentially in contiguous memory, they would be trivial to prefetch.
However, this will only work in read-only situations, and we w ould like to support frequent updates.
The second scheme history-pointer prefetching i n v olves creating new pointers|called jump pointers|which point from a node to the node that it should prefetch.
For example, Figure 4b shows how n o d e n i could directly prefetch n o d e n i +3 using three-ahead jump pointers.In our study, w e will build upon the concept of jump pointers, but customize them to the speciic needs of B + -Tree indices.
Rather than storing jump pointers directly in the leaf nodes, we instead pull them out into a separate array, which w e call the jump-pointer array, as illustrated in Figure 4c.
To initiate prefetching, a back-pointer in the starting leaf node is used to locate the leaf's position within the jump pointer array; then based on the desired prefetching distance, an array ooset is adjusted to nd the address of the appropriate leaf node to prefetch.
As the scan proceeds, the prefetching task simply continues to walk ahead in the jump-pointer array which itself is also prefetched without having to dereference the actual leaf nodes again.Jump-pointer arrays are more exible than jump pointers stored directly in the leaf nodes.
We can adjust the prefetching distance by simply changing the ooset used within the array.
This allows dynamic adaptation to changing performance conditions on a given machine, or if the code migrates to diierent machines.
In addition, the same jump-pointer array can be reused to target diierent latencies in the memory hierarchy e.g., disk latency vs. memory latency.From an abstract perspective, one might think of the jump-pointer array as a single large, contiguous array, a s i llustrated in Figure 5a.
This would be eecient in read-only situations, but in such cases we could simply arrange the leaf nodes themselves contiguously and use data-linearization prefetching 13, 144.
Therefore a key issue in implementing jump-pointer arrays is to handle updates gracefully.
Let us brieey consider the problems created by updates if we attempted to maintain the jump-pointer array a s a single contiguous array a s s h o wn in Figure 5a.
When a leaf is deleted, we can simply leave an empty slot in the array.
H o w ever, insertion can be very expensive.
When a new leaf is inserted, an empty slot needs to be created in the appropriate position for the new jump pointer.
If no nearby empty slots could be located, this could potentially involve copying a very large amount of data within the array i n order to create the empty slot.
In addition, for each jumppointer that is moved within the array, the corresponding back-pointer from the leaf node into the array also needs to be updated, which could be very costly too.
Clearly we would not want t o p a y such a high cost upon insertions.We improve upon the naive contiguous array implementation in the following three ways.
First, we break the contiguous array i n to a chunked linked list|as illustrated in Figure 5b|which allows us to limit the impact of an insertion to its corresponding chunk.
We will discuss the chunk size selection later in Section 3.3.
Second, we actively attempt to interleave empty slots within the jump-pointer array so that insertions can proceed quickly.
During bulkload or when a chunk splits, the jump pointers are stored such that empty slots are evenly distributed to maximize the chance of nding a nearby empty slot for insertion.
When a jump-pointer is deleted, we simply leave a n empty slot in the chunk.Finally, w e alter the meaning of the back-pointer in a leaf node to its position in the jump-pointer array such that it is merely a hint.
The pointer should point to the correct chunk, but the position within that chunk may be imprecise.
Therefore when moving jump pointers in a chunk for inserting a new leaf address, we do not need to update the hints for the moved jump pointers.
We only update a hint eld when: i the precise position in the jump-pointer array is looked up during range scan or insertion, in which case the leaf node should be already in cache and updating the hint is almost free; and ii when a chunk splits and addresses are redistributed, in which case we are forced to update the hints to point to the new chunk.
The cost of using hints, of course, is that we need to search for the correct location within the chunk in some cases.
In practice, however, the hints appear to be good approximations of the true positions, and searching for the precise location is not a costly operation e.g., it should not incur any cache misses.In summary, the net eeect of these three enhancements is that nothing moves during deletions, typically only a small number of jump pointers between the insertion position and the nearest empty slot move during insertions, and in neither case do we normally update the hints within the leaf nodes.
Thus we expect jump-pointer arrays to perform well during updates.
Having described the data structure to facilitate prefetching, we n o w describe our prefetching algorithm.
Recall that the basic range scan algorithm consists of a loop that visits a leaf on each iteration by following a next-leaf pointer.To support prefetching, we add prefetches both prior to this loop for the startup phase, and inside the loop for the steady-state phase.
Let k be the desired prefetching distance, in units of leaf nodes we discuss below h o w to select k.
During the startup phase, we issue prefetches for the ñ®½™rst k leaf nodes.
3 These prefetches proceed in parallel, exploiting the available memory hierarchy bandwidth.
During each loop iteration i.e. in the steady-state phase, prior to visiting the current leaf node in the range scan, we prefetch the leaf node that is k nodes after the current leaf node.
The goal is to ensure that by the time the basic range scan loop is ready to visit a leaf node, that node is already prefetched into the cache.
With this framework in mind, we n o w describe further details of our algorithm.First, in the startup phase, we m ust locate the jump pointer of the starting leaf within the jump-pointer array.
To do this, we follow the hint pointer from the starting leaf to see whether it is precise|i.e. whether the hint points to a pointer back to the starting leaf.
If not, then we start searching within the chunk in both directions relative to the hint position until the matching position is found.
As discussed earlier, the distance between the hint and the actual position appears to be small in practice.Second, we need to prefetch the jump-pointer chunks as well as the leaf nodes, and handle empty slots in the chunks.
During the startup phase, both the current c h unk and the next chunk are prefetched.
When looking for a jump pointer, we test for and skip all empty slots.
If the end of the current c h unk is reached, we will go to the next chunk to get the rst non-empty jump-pointer there is at least one nonempty jump pointer or the chunk should have been deleted.
We then prefetch the next chunk ahead in the jump-pointer array.
Because we always prefetch the next chunk before prefetching any leaf nodes pointed to by the current c h unk, we expect the next chunk to be in the cache by the time we access it.Third, although the actual number of tupleIDs in the leaf node is unknown when we do range prefetching, we will assume that the leaf is full and prefetch the return buuer area accordingly.
T h us the return buuer will always be prefetched suuciently early.We n o w discuss how to select the prefetching distance and the chunk size.Selecting the prefetching distance k.
The prefetching distance k, in units of nodes to prefetch ahead is se-lected as follows.
Normally this quantity is derived by dividing the expected worst-case miss latency by the computation time spent on one leaf node similar to what has been done in other contexts 166.
However, because the computation time associated with visiting a leaf node during a range scan is quite small relative to the miss latency, w e will assume that the limiting factor is the memory bandwidth.
To account for empty c h unk slots, we can multiply the denominator in equation 3 by the occupancy of chunk slots a value similar to the bulkload factor, which w ould increase c somewhat.
Another factor that could in theory dictate the minimum chunk size is that each c h unk should contain at least k leaf pointers so that our prefetching algorithm can get suuciently far ahead.
However, since k B from equation 2, the chunk size in equation 3 should be suucient.Increasing c beyond this minimum value to create some extra slack for empty leaf nodes and empty c h unk slots does not hurt performance in practice.
4Remarks.
Given suucient memory system bandwidth, our prefetching scheme hides the full memory latency experienced at every leaf node visited during range scan operations.
With the data structure improvements in Section 3.2, we also expect good performance on updates.
However, there is a space overhead associated with the jump-pointer array.
Since the jump pointer array only contains one pointer per leaf node, the space overhead is relatively small.
Since a next-leaf pointer and a back-pointer are stored in every leaf, there are at most d , 2 ever, if this space overhead is still a concern, we n o w describe how it can be reduced further.
So far we h a v e described how a jump-pointer array can be implemented by creating a new external structure to store the jump pointers as illustrated earlier in Figure 5.
However, there is an existing structure within a B + -Tree that already contains pointers to the leaf nodes, namely, the parents of the leaf nodes.
We will refer to these parent nodes as the bottom non-leaf nodes.
The child pointers within a bottom non-leaf node correspond to the jump-pointers within a chunk of the external jump-pointer array described in Section 3.2.
A key diierence, however, is that there is no easy way to traverse these bottom non-leaf nodes quickly enough to perform prefetching.
A potential solution is to connect these bottom non-leaf nodes together in leaf key order using linked-list pointers.
Note that this is sometimes done already for concurrency control purposes 222.
Figure 6 illustrates the internal jump-pointer array.
Note that leaf nodes do not contain back-pointers to their positions within their parents.
It turns out that such pointers are not necessary for this internal implementation, because the position will be determined during the search for the starting key.
I f w e simply retain the result of the bottom non-leaf node's binary search, we will have the position to initiate the prefetching appropriately.This approach is attractive with respect to space overhead, since the only overhead is one additional pointer per bottom non-leaf node.
The overhead of updating this pointer should be insigniicant, because it only needs to be changed in the rare event that a bottom non-leaf node splits or is deleted.
One potential limitation of this approach, however, is that the length of a chunk" in this jump-pointer array is dictated by the B + -Tree structure, and may not be easily adjusted to satisfy large prefetch distance requirements e.g., for hiding disk latencies.In the remainder of this paper, we will use the notations peB + -Tree" and piB + -Tree" to refer to pB + -Trees with external and internal jump-pointer arrays, respectively.
Further details on the algorithms using external and internal jump pointer arrays can be found in the full paper 77.
To facilitate comparisons with CSB + -Trees, we present our experimental results in a main-memory database environment.
We begin by describing the framework for the experiments, including our performance simulator and the implementation details of the index structures that we compare.
The three subsections that follow present our experimental results for index search, index scan, and updates.
Finally, w e present a detailed cache performance study for a few of our earlier experiments.
Machine Model.
We e v aluate the performance impact of Prefetching B + -Trees through detailed simulations of fully-functional executables running on a state-of-the-art machine.
Since the gap between processor and memory speeds is continuing to increase dramatically with each new generation of machines, it is important to focus on the performance characteristics of machines in the near future rather than in the past.
Hence we base our memory hierarchy o n the Compaq ES40 88 one of the most advanced computer systems announced to date, but we update it slightly to include a dynamically-scheduled, superscalar processor similar to the MIPS R10000 233 running at a clock rate of 1 GHz.
The simulator performs a cycle-by-cycle simulation, modeling the rich details of the processor including the pipeline, register renaming, the reorder buuer, branch prediction, branching penalties, the memory hierarchy including all forms of contention, etc.
Table 2 shows the key parameters of the simulator.
Given the parameters in and iv p 8 i B + -Trees.
For these latter two cases, the node width w = 8 w as selected because our experiments showed that this choice resulted in the best search performance consistent with the analytical computation in Section 2.
We also implemented bulkload and search for CSB + -Trees and pCSB + -Trees.
Although we did not implement insertion or deletion for CSB + -Trees, we conduct the same experiments as in Rao and Ross 200 albeit in a diierent memory hierarchy to facilitate a comparison of the results.
Although Rao and Ross present techniques to improve CSB + -Tree search performance within a node 200, we only implemented standard binary search for all the trees studied because our focus is on memory performance which is the primary bottleneck, as shown earlier in Figure 1.
Our pB + -Tree techniques improve performance over a range of key, pointer, and tupleID sizes.
For concreteness, we report experimental results where the keys, pointers, and tupleIDs are 4 bytes each, as was done in previous studies 19, 200.
As discussed in Section 2, we use a standard B + -Tree node structure, consistent with previous studies.
For the B + -Tree, each node is one cache line wide i.e. 64 bytes.
Each non-leaf node contains a keynum eld, 7 key elds and 8 childptr elds, while each leaf node contains a keynum eld, 7 key elds, 7 associated tupleID elds, and a next-leaf pointer.
The nodes of the pB + -Trees are the same as the B + -Trees, except that they are wider.
So for eight-cache-line-wide nodes, each non-leaf node is 512 bytes and contains a keynum eld, 63 key elds, and 64 childptr elds, while each leaf node contains a keynum eld, 63 key elds, 63 associated tupleID elds, and a next-leaf pointer.
For the p 8 e B + -Tree, non-leaf nodes have the same structure as for the pB + -Tree, while each leaf node has a hint eld and one fewer key and tupleID elds.
The only difference with a p 8 i B + -Tree compared to a pB + -Tree is that each bottom non-leaf node has a next-sibling pointer, and one fewer key and childptr elds.
For the CSB + -Tree and the pCSB + -Tree, each non-leaf node has only one childptr eld.
For example, a CSB + -Tree non-leaf node has a keynum eld, 14 key elds, and a childptr eld.
All tree nodes are aligned on a 64 byte boundary when allocated.
Our sensitivity analysis 77 showed that selecting c = 1 through 32 results in similar scan performance.
We rst evaluate index search performance for B + -Trees, CSB + -Trees, p w B + -Trees where w = 2, 4, 8, and 16, and p 8 CSB + -Trees which combine our prefetching approach with CSB + -Trees.Varying the number of leaf nodes.
Figure 7 shows the execution time of 100K random searches after bulkloading 10K, 30K, 100K, 300K, 1M, 3M, and 10M keys into the trees nodes are 100 full except the root.
5 In the experi-5 Note that throughout this paper, K" and M" correspond to 1000 ments shown in Figure 7a, search operations are performed one immediately after another the warm cache" case; whereas in the experiments shown in Figure 7b, the cache is cleared between each search the cold cache" case.
Depending on the operations performed between the searches, the real-world performance of an index search w ould lie in between the two extremes: closer to the warm cache case for index joins, while often closer to the cold cache case for single value selections.
From these experiments, we see that: i the B + -Tree has the worst performance; ii the trees with wider nodes and prefetching support pB + -Trees, pCSB + -Tree all perform better than their non-prefetching counterparts B + -Tree, CSB + -Tree; and iii the p 8 B + -Tree is comparable to or better than all other pB + -Trees over the entire range of tree sizes.
For warm caches, the speedup of the p 8 B + -Tree over the B + -Tree is between a factor of 1.27 to 1.47.
The warm cache speedup of the p 8 B + -Tree over the CSB + -Tree is between a factor of 1.14 to 1.28 once the tree no longer ts in the L2 cache.
Likewise, the cold cache speedups are 1.32 to 1.55 and 1.14 to 1.34, respectively.
The cold cache curves provide insight i n to the index search performance.
The trend of every single curve is clearly shown in the cold cache experiment: the curves all increase in discrete large steps, and within the same step they increase only slightly.
The large steps for a curve occur when the number of levels in the tree increases.
This can be veriied by examining Table 3, which depicts the number of levels in the tree for each data point plotted in Figure 7.
Within a step, additional leaf nodes result in more keys in the root node the other nodes in the tree remain full, which in turn increases the cost to search the root.
The step-up trend is blurred in the warm cache curves because the top levels of the tree may remain in the cache.
For diierent curves, we can see that generally the higher the tree structure, the larger the search cost; when trees are of the same height, the smaller node size yields better performance.
We and 1,000,000, respectively, except for when we refer to the size of a memory structure e.g., a cache, in which case they correspond to 1024 and 1,048,576, respectively.
conclude that the performance gains for wider nodes stem mainly from the resulting decrease in tree height.Another observation worth mentioning is that when the number of levels are the same, the p 2 B + -Tree and the CSB + -Tree have v ery similar performance.
This is because the second cache line in a p 2 B + -Tree node stores pointers, and the cost of retrieving these second lines is partly hidden by the key comparisons.
By eliminating all but one pointer, the CSB + -Tree has almost the same number of keys as the p 2 B + -Tree, resulting in similar key comparison costs.Varying the bulkload factor.
Figure 8 shows the effect on search performance of varying the bulkload factor.All the trees are bulkloaded with 3M hkey, tupleIDi pairs, with bulkload factors of 60, 70, 80, 90, and 100.
Because the actual number of used entries in leaf nodes in an experiment is the product of the bulkload factor and the maximum number of slots rounded to the nearest integer, we computed and used the true percentage of used entries when plotting the data|hence they may not be aligned with the target bulkload factors.
As in the previous experiments, Figure 8 shows that: i the B + -Tree has the worst performance; ii the trees with wider nodes and prefetching support pB + -Trees, pCSB + -Tree all perform better than their non-prefetching counterparts B + -Tree, CSB + -Tree; and iii the p 8 B + -Tree is the best of all the pB + -Trees.In the cold cache experiment, we see a step-down pattern in the curves: the steps correspond to the numb e r o f l e v els in the trees, since the tree height decreases in a step-wise fashion as the bulkload factor increases.
Within a step, however, the curves increase slightly.
This is because in our bulkload algorithms, the bulkload factor also determines the numb e r o f k eys in non-leaf nodes.
So the larger the bulkload factor, the larger the number of keys in each non-root node, and hence the larger the key comparison cost.In the full paper 77, we also present experimental results for mature trees 200, created by bulkloading 10 of the hkey, tupleIDi pairs and then inserting the remaining 90.
We nd similar performance for mature trees as for trees immediately after bulkloads.Searches on trees with jump-pointer arrays.
Our next experiment determines whether the diierent structures for speeding up range scans have an impact on search performance.
We use node width w = 8 for these experiments, because the p 8 B + -Tree resulted in the best search performance among the pB + -Trees.
Figure 9 compares the search performance of the p 8 B + -Tree, the p 8 e B + -Tree, and the p 8 i B + -Tree.The same experiments as in Figure 7 were performed.
Recall that both the p 8 e B + -Tree and the p 8 i B + -Tree consume space in the tree structures relative to the p 8 B + -Tree: the maximum number of keys in leaf nodes is one fewer for the show that these diierences have a negligible impact on search performance.
In one cold cache case, when 10M keys are in the tree, the p 8 e B + -Tree suuers from having one more level than the other two trees, but otherwise both the warm and cold cache performances are basically the same for all three trees, over the entire range of 10K to 10M keys.
In our next set of experiments, we e v aluate the eeectiveness of our techniques for improving range scan performance.
We compare B + -Trees, p 8 B + -Trees, p 8 e B + -Trees, and p 8 i B + -Trees.
As indicated above, we restrict our attention to node width w = 8 because this is the best width for searches, which are presumed to occur more frequently than range scans.
As discussed in Section 4.1, we set the prefetching distance to 3 nodes and the chunk size to 8 cache lines.Varying the range size and the bulkload factor.
Figure 10 shows the execution time of range scans while varying a the number of tupleIDs to scan per request i.e. the size of the range, or b the bulkload factor.
Because of the large performance gains for pB + -Trees, the execution time is shown on a logarithmic scale.
In Figure 10a, the trees are bulkloaded with 3M hkey, tupleIDi pairs, using a 100 bulkload factor.
Then 100 random starting keys are selected, and a range scan is requested for m tupleIDs starting at that starting key value, for m = 10, 100, 1K, 10K, 100K, and 1M.
The execution time plotted for each m is the total for the 100 starting keys.
In Figure 10b random starting keys are then selected, and a range scan is requested for 1000 tuple IDs starting at that value.
Between the range scan requests, the caches are cleared to more accurately reeect scenarios in which range scan requests are interleaved with other database operations or application programs which w ould tend to evict any cache-resident nodes.As we see in Figure 10, p 8 e B + -Trees and p 8 i B + -Trees achieve a factor of 6.5 to 8.7 speedup over standard B + -Trees for range scans of 1K to 1M tupleIDs.
As the bulkload factor decreases, the number of leaf nodes to be scanned increases since we m ust skip an increasing number of empty slots, and hence our prefetching schemes achieve e v en larger speedups.
Figure 10 also shows the contribution of both aspects of our pB + -Tree design to overall performance.
First, extending the node size and simultaneously prefetching all cache lines within a node while scanning and performing the initial search|similar to what was illustrated earlier in nearly identical performance, there does not appear to be a compelling need to build an external rather than an internal jump-pointer array, at least for these system parameters.
Note that this conclusion depends upon the ratio of w and k; in other scenarios with diierent ratios|e.g., when prefetching to hide disk as well as memory latencies|the exibility of an external jump-pointer array m a y be needed.When scanning far fewer than 1K tupleIDs, however, the startup cost of our prefetching schemes becomes noticeable.
For example, when scanning only 100 tupleIDs, pB + -Trees are only twice as fast as standard B + -Trees.
When scanning only 10 tupleIDs, p 8 B + -Trees are only slightly faster than B + -Trees, and p 8 e B + -Trees and p 8 i B + -Trees are actually slower.
This suggests a scheme where jump-pointer arrays are only exploited for prefetching if the expected number of tupleIDs within the range is signiicant e.g., 100 or more.
This estimate of the range size could be computed either by using standard query optimization techniques such as histograms, or else by simultaneously searching for both the starting and ending keys to see how far apart they are.Large segmented range scans.
We n o w consider the behavior of large range scans.
In practice, these large scans are often broken up into smaller segments either to permit other operations and queries to proceed, or else to avoid overrowing the return buuer.
For example, an indexed scan providing sorted input to a sort-merge join operator will have its return buuer consumed at a rate dependent o n the data proole of the other input to the join.
shows the execution time for performing segmented range scans: each scan consists of a search for the starting key followed by 1000 range scan requests, each of which scans and places into the return buuer the next segment of 1000 hkey, tupleIDi pairs, resulting in a total of 1M pairs.
The trees are bulkloaded with 3M hkey, tupleIDi pairs, with bulkload factors ranging from 60 to 100.
The reported execution times are the total for 100 segmented range scans, starting from 100 randomly selected starting keys.
As we see in Figure 11, the performance gains for segmented range scans are similar to what we s a w earlier in Figure 10 for non-segmented range scans.
In addition to improving search and scan performance, another one of our goals is to achieve good performance on updates, especially since this had been a problem with earlier cache-sensitive index structures 19, 200.
To quantify the impact of pB + -Trees on update performance, Figure 12 shows the execution time for 100K random insertions or deletions on a tree bulkloaded with 3M hkey, tupleIDi pairs, with bulkload factors ranging from 60 to 100, and with warm caches.
The cold cache results exhibit the same trends 77.
As we see in Figure 12, all three pB + -Tree schemes i.e. p 8 B + -Trees, p 8 e B + -Trees, and p 8 i B + -Trees perform roughly the same, and all are signiicantly faster than the B + -Tree.
For example, when the bulkload factor is 100, the pB + -Trees achieve at least a 1.24 speedup over the B + -Tree for both insertions and deletions.
This result may appear somewhat surprising, given the additional overheads of maintaining the external jump-pointer arrays for p 8 e B + -Trees.There are two primary factors contributing to the faster update times for pB + -Trees compared with the B + -Tree.
First, search i s a n i n tegral part of both insertion and deletion, and our pB + -Trees enjoy faster search times due to their wider nodes as we s a w earlier in Section 4.2.
Second, node splits occur less frequently for wider nodes.
Let us start by considering trees that are not full|i.e. with bulk- load factors ranging from 60 to 90.
Figure 13a shows that when the bulkload factor is within this range, the number of the 100K insertions that cause node splits is extremely small for each of the pB + -Trees, and is even less than 10 for B + -Trees.
Given that Figures 12a and b show similar trends to Figure 8 within this range of bulkload factors, we conclude that the dominant eeect is improved search time for these less-than-full trees.
In contrast, when the trees are full, many insertions will cause node splits, as shown in Figure 13b.
Due to their smaller nodes, B + -Trees suuer far more node splits than pB + -Trees, and over 40 of the insertions result in multiple splits due to splitting non-leaf nodes.
Hence although the cost of each node split in a pB + -Trees is greater, this cost is more than ooset by the reduced frequency of node splits and the improved search times.Turning our attention to deletion performance in Fig- ure 12b, since both pB + -Trees and B + -Trees use lazy deletion, very few deletions result in a deleted node or a key redistribution.
Hence the performance gains for pB + -Tree deletions are due solely to faster search times.
Finally, our last set of experiments present a more detailed cache performance study, using two representative experiments: one for index search and one for index range scan.
A central claim of this paper is that the demonstrated speedups for pB + -Trees are obtained by eeectively limiting the exposed miss latency of previous approaches.
In these experiments, we connrm that claim.Our starting point is the experiments presented earlier in Figure 1 which illustrated the poor cache performance of existing B + -Trees on index search and scan.
We reproduce those results now in Figure 14, along with several variations of our pB + -Trees.
The bars on the left labeled search" correspond to the experiment shown earlier in Figure 7a with 10M hkey, tupleIDi pairs bulk-loaded, and the bars on the right labeled scan" correspond to the experiment shown earlier in Figure 10a with 1M tupleIDs scanned.Each bar in Figure 14 represents execution time normalized to a B + -Tree, and is broken down into the following three categories that explain what happened during all potential graduation slots.
6 The bottom section busy is the number of slots where instructions actually graduate.
The other two sections are the number of slots where there is no graduating instruction, broken down into data cache stalls and other stalls.
Speciically, the top section dcache stalls is the number of such slots that are immediately caused by the oldest instruction suuering a data cache miss, and the middle section other stalls is all other slots where instructions do not graduate.
Note that the dcache stalls section is only a rst-order approximation of the performance loss due to data cache misses: these delays also exacerbate subsequent data dependence stalls, thereby increasing the number of other stalls.As we see in Figure 14, pB + -Trees signiicantly reduce the amount of exposed miss latency i.e. the dcache stalls component o f e a c h bar.
For the index search experiments, we see that while CSB + -Trees eliminated 20 of the data cache stall time that existed with B + -Trees, p 8 B + -Trees eliminate 45 of this stall time, thus resulting in an overall speedup of 1.47 compared with 1.15 for CSB + -Trees.
A signiicant amount of data cache stall time still remains for index searches, since we still experience the full miss latency each time we m o v e d o wn a level in the tree unless the node is already in the cache due to previous operations.
Eliminating this remaining latency appears to be diicult, as we will discuss in the next section.
In contrast, we a c hieve nearly ideal performance for the index range scan experiments shown in Figure 14, where both p 8 e B + -Trees and p 8 i B + -Trees eliminate 97 of the original data cache stall time, resulting in an impressive eightfold overall speedup.
These results demonstrate that the pB + -Tree speedups are indeed primarily due to a signiicant reduction in the exposed miss latency.
We n o w discuss several possible improvements to pB + -Trees and related issues.
While our approach of using prefetching to create wider nodes improves search performance by a factor of 1.221.5, we still suuer a full cache miss latency at each level of the tree.
Unfortunately, this is a very diicult problem to solve given: i the data dependence through the child pointer; ii the relatively large fanout of the tree nodes; and iii the fact that it is equally likely that any c hild will be visited assuming uniformly distributed random search k eys.
While one might consider prefetching the children or even the grandchildren of a node in parallel with accessing the node, there is a duality b e t w een this and simply creating wider nodes.
Compared with our approach, prefetching children or grandchildren suuers from: i additional storage overhead for the children and grandchildren pointers, and ii the restriction that the size" of a node i.e. the number of cache lines prefetched can only grow b y m ultiples of the tree fanout.Extending the idea of adding pointers to the bottom nonleaf nodes, it is possible to use no additional pointers at all.
Potentially, w e could retain all the pointers from the root to the leaf during the search, and then keep moving this set of pointers, sweeping through the entire range prefetching the leaf nodes.
Note that with wider nodes, trees are shallower and this scheme may be feasible.Lehman and Carey, in an early paper on index structures for main memory databases, proposed and studied the TTree 11, 122.
At the time of their study the mid-80's, the T-Tree outperformed the B + -Tree, and was considered the index structure of choice for main memory databases for over a decade.
However, more recent studies have shown that the B + -Tree outperforms the T-Tree on modern processors 199, due in large part to the exponential growth these past 15 years in cache miss latency relative to processor speed.Previous work has also considered key compression schemes e.g., 4, 6, 99, in order to pack more keys into an index node.
As with CSB + -Trees, these techniques can be used in conjunction with our approach, as desired.Although our discussions and experiments have focused on main memory databases, pB + -Trees can also be used to improve both the IIO performance and the memory performance of disk-resident databases.
Because the index node size for a disk-resident database is typically a disk page of 4KB, the fanout is much larger than with main memory indices.
This may eeect the beneets of using even wider nodes for searches.
However, our range scan prefetching techniques applied to pages would likely continue to have a signiicant beneet.
Furthermore, main memory performance is important e v en for disk-resident databases, so it would be interesting to apply our methods for both cache lines and pages, and quantify the overall performance gains.
While eliminating child pointers through data layout techniques has been shown to signiicantly improve main memory B + -Tree search performance, a large fraction of the execution time for a search is still spent in data cache stalls, and index insertion performance is hurt by these techniques.
Moreover, the cache performance of index scan another important B + -Tree operation has not been studied.
In this paper, we explored how prefetching could be used to improve the cache performance of index search, update, and scan operations.
We proposed the Prefetching B + -Tree pB + -Tree and evaluated its eeectiveness in modern memory systems.We showed that the optimal B + -Tree node size is often wider than a cache line on a modern machine, when prefetching is used to retrieve the pieces of a node, eeectively overlapping multiple cache misses.
Our results can be summarized as follows: ñ®½™ For index search, this prefetching technique achieves a speedup of 1.27 to 1.55 over the B + -Tree, by decreasing the height of the tree. ñ®½™
For index updates insertions and deletions, the technique achieves a speedup of 1.24 to 1.52 over the B + -Tree, due to the faster search and the less frequent node splits with wider nodes. ñ®½™
For index scan, the technique achieves a speedup of 3.5 to 3.7 over the B + -Tree, again due to the faster search and wider nodes.
Moreover, we proposed jump-pointer arrays, which enable eeective range scan prefetching across node boundaries.
Overall, the pB + -Tree achieves a speedup of 6.5 to 8.7 over the B + -Tree for range scans.
We proposed two alternative implementations of jump-pointer arrays, with comparable performance.
From our results, we conclude that the cache performance of B + -Tree indices can be greatly improved by exploiting the prefetching capabilities of state-of-the-art computer systems.
We believe that this work makes an important contribution towards applying prefetching techniques to advantage throughout a DBMS.
We thank Berni Schiefer at IBM for his many helpful comments regarding this work.
To d d C .
M o wry is partially supported by an Alfred P. Sloan Research F ellowship and by a F aculty Development A w ard from IBM.
