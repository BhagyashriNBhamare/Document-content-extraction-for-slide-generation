In this paper, we examine the problem of estimating the capacity of bottleneck links and available bandwidth of end-to-end paths under non-negligible cross-traffic conditions.
We present a simple stochastic analysis of the problem in the context of a single congested node and derive several results that allow the construction of asymptotically-accurate bandwidth estimators.
We first develop a generic queuing model of an Internet router and solve the estimation problem assuming renewal cross-traffic at the bottleneck link.
Noticing that the renewal assumption on Internet flows is too strong, we investigate an alternative filtering solution that asymptotically converges to the desired values of the bottleneck capacity and available bandwidth under arbitrary (including non-stationary) cross-traffic.
This is one of the first methods that simultaneously estimates both types of bandwidth and is provably accurate.
We finish the paper by discussing the impossibility of a similar estimator for paths with two or more congested routers.
Bandwidth estimation has recently become an important and mature area of Internet research [1], [2], [4], [5], [6], [7], [10], [11], [12], [14], [15], [16], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27].
A typical goal of these studies is to understand the characteristics of Internet paths and those of cross-traffic through a variety of end-to-end and/or router-assisted measurements.
The proposed techniques usually fall into two categories -those that estimate the bottleneck bandwidth [4], [5], [6], [12], [13] and those that deal with the available bandwidth [11], [19], [24], [26].
Recall that the former bandwidth metric refers to the capacity of the slowest link of the path, while the latter is generally defined as the smallest average unused bandwidth among the routers of an end-to-end path.The majority of existing bottleneck-bandwidth estimation methods are justified assuming no cross-traffic along the path and are usually examined in simulations/experiments to show that they can work under realistic network conditions [4], [5], [6], [7], [12].
With available bandwidth estimation, crosstraffic is essential and is usually taken into account in the analysis; however, such analysis predominantly assumes a fluid model for all flows and implicitly requires that such models be accurate in non-fluid cases.
Simulations/experiments are again used to verify that the proposed methods are capable of dealing with bursty conditions of real Internet cross-traffic [11], [19], [24], [26].
To understand some of the reasons for the lack of stochastic modeling in this field, this paper studies a single-node bandwidth measurement problem and derives a closed-form estimator for both capacity C and available bandwidth A = C − ¯ r, where ¯ r is the average rate of cross-traffic at the link.
For an arbitrary cross-traffic arrival process r(t), we define ¯ r as the asymptotic time-average of r(t) and assume that it exists and is finite:¯ r = lim t→∞ 1 t t 0 r(u)du < ∞.
(1)Notice that the existence of (1) does not require stationarity of cross-traffic, nor does it impose any restrictions on the arrival of individual packets to the router.
While other definitions of available bandwidth A and the average cross-traffic rate ¯ r are possible, we find that (1) serves our purpose well as it provides a clean and mathematically tractable metric.The first half of the paper deals with bandwidth estimation under i.i.d. renewal cross-traffic and the analysis of packetpair/train methods.
We first show that under certain conditions and even the simplest i.i.d. cross-traffic, histogram-based methods commonly used in prior work (e.g., [5]) can be misled into producing inaccurate estimates of C.
We overcome this limitation by developing an asymptotically accurate model for C; however, since this approach eventually requires ergodicity of cross-traffic, we later build another model that imposes more restriction on the sampling process (using PASTA principles suggested in [26]), but allows cross-traffic to exhibit arbitrary characteristics.Unlike previous studies [26], we prove that the corresponding PASTA-based estimators converge to the correct values and show that they can be used to simultaneously measure capacity C and available bandwidth A. To our knowledge, this is the first estimator that measures both C and A without congesting the link, assumes non-negligible, non-fluid cross-traffic in the derivations, and applies to non-stationary r(t).
Note that while this estimator can measure A over multiple links, its inherent purpose is not to become another measurement tool or to work over multi-node paths, but rather to understand the associated stochastic models and disseminate the knowledge obtained in the process of writing this paper.We conclude the paper by discussing the impossibility of building an asymptotically optimal estimator of both C and A for a two-node case.
While estimation of A may be possible for multi-node paths, our results suggest that provably-convergent estimators of C do not exist in the context of end-to-end measurement.
Thus, the problem of deriving a provably accurate estimator for a two-node case with arbitrary cross-traffic remains open; however, we hope that our initial work in this direction will stimulate additional research and prompt others to prove/disprove this conjecture.
In this section, we build a simple model of a router that introduces random delay noise into the measurements of the receiver and use it to study the performance of packet-pair bandwidth-sampling techniques.
Note that we depart from the common assumption of negligible and/or fluid cross-traffic and specifically aim to understand the effects of random queuing delays on the bandwidth sampling process.
First consider an unloaded router with no cross-traffic.
The packet-pair mechanism is based on an observation that if two packets arrive at the bottleneck link with spacing x smaller than the transmission delay ∆ of the second packet over the link, their spacing after the link will be exactly ∆.
In practice, however, packets from other flows often queue between the two probe packets and increase their spacing on the exit from the bottleneck link to be larger than ∆.
Assume that packets of the probe traffic arrive to the bottleneck router at times a 1 , a 2 , . . . and that inter-arrival times a n − a n−1 are given by a random process x n determined by the server's initial spacing.
Further assume that the bottleneck node delays arriving packets by adding a random processing time ω n to each received packet n. For the remainder of the paper, we use constant 1 packet size q for the probing flow and arbitrarily-varying packet sizes for cross-traffic.
Furthermore, there is no strict requirement on the initial spacing x n as long as the modeling assumptions below are satisfied.
This means that both isolated packet pairs or bursty packet trains can be used to probe the path.
Let the transmission delay of each application packet through the bottleneck link be q/C = ∆, where C is the transmission capacity of the link.
Under these assumptions, packet departure times d n are expressed by the following recurrence 2 :d n = a 1 + ω 1 + ∆ n = 1 max(a n , d n−1 ) + ω n + ∆ n ≥ 2 .
(2)In this formula, the dependence of d n on departure time d n−1 is a consequence of FIFO queuing (i.e., packet n cannot depart before packet n − 1 is fully transmitted).
Furthermore, packet n cannot start transmission until it has fully arrived (i.e., time a n ).
The value of the noise term ω n is proportional to the number of packets generated by cross-traffic and queued 1 Methods that vary the probing packet size also exist (e.g., [6], [10]).
2 Times d n specify when the last bit of the packet leaves the router.
Similarly, times a n specify when the last bit of the packet is fully received and the packet is ready for queuing.
in front of packet n.
The final metric of interest is interdeparture delay y n = d n − d n−1 as the packets leave the router.
The various variables and packet arrival/departure are schematically shown in Figure 1.
Even though the model in (2) appears to be simple, it leads to fairly complicated distributions for y n if we make no prior assumptions about cross-traffic.
We next examine several special cases and derive important asymptotic results about process y n .
We start our analysis with a rather common assumption in queuing theory that cross-traffic arrives into the bottleneck link according to some renewal process (i.e., delays between crosstraffic packets are i.i.d. random variables).
In what follows in the next few subsections, we show that modeling of this direction requires stationarity (more specifically, ergodicity) of cross-traffic.
However, since neither the i.i.d. assumption nor stationarity holds for regular Internet traffic, we then apply a different sampling methodology and a different analytical direction to derive a provably robust estimator of capacity C and average cross-traffic rate ¯ r.
The goal of bottleneck bandwidth sampling techniques is to queue probe packets directly behind each other at the bottleneck link and ensure that spacing y n on the exit from the router is ∆.
In practice, however, this is rarely possible when the rate of cross-traffic is non-negligible.
This does present certain difficulties to the estimation process; however, assuming a single congested node, the problem is asymptotically tractable given certain mild conditions on cross-traffic.
We present these results below.To generate measurements of the bottleneck capacity, it is commonly derived that the server must send its packets with initial spacing no more than ∆ (i.e., no slower than C).
This is true for unloaded links; however, when cross-traffic is present, the probes may be sent arbitrarily slower as long as each packet i arrives to the router before the departure time of packet i − 1.
This condition translates into a n ≤ d n−1 and (2) expands to:d n = a 1 + ω 1 + ∆ n = 1 d n−1 + ω n + ∆ n ≥ 2 .
(3)From (3), packet inter-departure times y n after the bottleneck router are given by:y n = d n − d n−1 = ∆ + ω n , n ≥ 2.
(4)Notice that random process ω n is defined by the arrival pattern of cross-traffic and also by its packet-size distribution.
Since this process is a key factor that determines the distribution of sampled delays y n , we next focus on analyzing its properties.
Assume that inter-packet delays of cross-traffic are given by independent random variables {X i } and the actual arrivals occur at times X 1 , X 1 + X 2 , . . . Thus, the arrival pattern of cross-traffic defines a renewal process M (t), which is the number of packet arrivals in the interval [0, t].
Using common convention, further assume that the mean inter-arrival delay E[X i ] is given by 1/λ, where λ = ¯ r is the mean arrival rate of cross-traffic in packets per second.To allow random packet sizes, assume that {S j }, j = 1, 2, . . . are independent random variables modeling the size of packets in cross-traffic.
We further assume that the bottleneck link is probed by a sequence of packet-pairs, in which the delay between the packets within each pair is small (so as to keep their rate higher than C) and the delay between the pairs is high (so as not to congest the link).
Under these assumptions, the amount of cross-traffic data received by the bottleneck link between probe packets 2m − 1 and 2m (i.e., in the interval (a 2m−1 , a 2m ]) is given by a cumulative reward process:v n = M (an)−M (an−1) j=1 S j , n = 2m,(5)where n represents the sequence number of the second packet in each pair.
For now, we assume a general (not necessarily equilibrium) process M (t) and re-write (4) as:y n = ∆ + v n C = ∆ + M (a n )−M (a n−1 ) j=1 S j C , n = 2m.
(6)Since classical renewal theory is mostly concerned with limiting distributions, y n by itself does not lead to any tractable results because the observation period of the process captured by each of y n is very small.Define a time-average process W n to be the average of {y i } up to time n:W n = 1 n n i=1 y i .
(7)Then, we have the following result.
Claim 1: Assuming ergodic cross-traffic, time-average process W n converges to: where E[x n ] is the mean inter-probe delay of packet-pairs.
lim n→∞ W n = ∆ + λE[x n ]E[S j ] C = ∆ + E[ω n ],(8)Proof: Process W n samples a much larger span of M (t) and has a limiting distribution as we demonstrate below.
Applying Wald's equation to (6) [28]:E [y n ] = ∆ + E[M (a n ) − M (a n−1 )]E[S j ] C .
(9)The last result holds since M (t) and S j are independent and M (a n ) − M (a n−1 ) is a stopping time for sequence {S j }.
Equation (9) can be further simplified by noticing thatE[M (t)] is the renewal function m(t): E [y n ] = ∆ + (m(a n ) − m(a n−1 ))E[S j ] C .
(10)Assuming stationary cross-traffic, (10) expands to [28]:E [y n ] = ∆ + λE[x n ]E[S j ] C .
(11)Finally, assuming ergodicity of cross-traffic (which implies that of process y n ), we can obtain (11) using a large number of packet pairs as the limit of W n in (7) as n → ∞.
Notice that the second term in (8) is strictly positive under the assumptions of this paper.
This leads to an interesting observation that the filtering problem we are facing is quite challenging since the sampled process y n represents signal ∆ corrupted by a non-zero-mean noise ω n .
This is a drastic departure from the classical filter theory, which mostly deals with zero-mean additive noise.
It is also interesting that the only way to make the noise zero-mean is to either send probe traffic with E[x n ] = 0 (i.e., infinitely fast) or to have no crosstraffic at the bottleneck link (i.e., λ = ¯ r = 0).
The former case is impossible since x n is always positive and the latter case is a simplification that we explicitly want to avoid in this work.We will present our analysis of packet-train probing shortly, but in the mean time, discuss several simulations to provide an intuitive explanation of the results obtained so far.
Before we proceed to estimation of C, let us explain several observations made in previous work and put them in the context of our model in (6) and (8).
For the simulations in this section, we used the ns2 network simulator with the topology shown in Fig. 2.
In the figure, the source of probe packets Snd1 sends its data to receiver Rcv1 across two routers R 1 and R 2 .
The speed of all access links is 100 mb/s (delay 5 ms), while the bottleneck link R 1 → R 2 has capacity C = 1.5 mb/s and 20 ms delay.
Note that there are five cross-traffic sources attached to Snd2.
We next discuss simulation results obtained under UDP cross-traffic.
In the first case, we initialize all five sources in Snd2 to be CBR streams, each transmitting at 200 kb/s (¯ r = 1 mb/s total cross-traffic).
Each CBR flow starts with a random initial delay to prevent synchronization with other flows and uses 500-byte packets.
The probe flow at Snd1 sends its data at an average rate of 500 kb/s for the probing duration, which results in 100% utilization of the bottleneck link.
In the second case, we lower packet size of cross-traffic to 300 bytes and increase its total rate to 1.3 mb/s to demonstrate more challenging scenarios when there is packet loss at the bottleneck.These simulation results are summarized in Fig. 3, which illustrates the distribution of the measured samples y n based on each pair of packets sent with spacing x n ≤ ∆ (the results exclude packet pairs that experienced loss).
Given capacity C = 1.5 mb/s and packet size q = 1, 500 bytes, the value of ∆ is 8 ms. Fig. 3 shows that none of the samples are located at the correct value of 8 ms and that the mean of the sampled signal (i.e., W n ) has shifted to 11.7 ms for the first case and 14.5 ms for the second one.Next, we employ TCP cross-traffic, which is generated by five FTP sources attached to Snd2.
The TCP flows use different packet sizes of 540, 640, 840, 1,040, and 1,240 bytes, respectively.
The histogram of y n for this case is shown in Fig.
4 for two different average cross-traffic rates ¯ r = 750 kb/s and ¯ r = 1 mb/s.
As seen in the figure, even though some of the samples are located at 8 ms, the majority of the mass in the histogram (including the peak modes) are located at the values much higher than 8 ms.Recall from (8) that W n of the measured signal tends to ∆+E[ω n ].
Under CBR cross-traffic, we can estimate the mean of the noise E[ω n ] to be approximately 11.7 − 8 = 3.7 ms in the first case and 14.5−8 = 6.5 ms in the second one.
The two naive estimates of C based on W n are˜Care˜ are˜C = q/W n = 1, 025 kb/s and˜Cand˜ and˜C = 827 kb/s, respectively.
Likewise, for the TCP case, the measured averages (i.e., 12.2 and 13.3 ms each) of samples y n lead to incorrect naive estimates˜Cestimates˜ estimates˜C = 983 kb/s and˜Cand˜ and˜C = 902 kb/s, respectively.In order to understand how˜Chow˜ how˜C and the value of W n evolve, we run another simulation under 1 mb/s total TCP cross-traffic and plot the evolutions of the absolute error | ˜ C −C| and that of W n in Fig. 5.
As Fig. 5(a) shows, the absolute error between C and˜Cand˜ and˜C converges to a certain value after 5,000 samples y n , providing a rather poor estimate˜Cestimate˜ estimate˜C ≈ 1, 010 kb/s.
Fig. 5(b) illustrates that W n in fact converges to ∆ + E[ω n ], where the mean of the noise is W n − ∆ ≈ 11.9 − 8 = 3.9 ms.Previous work [1], [4], [5], [15], [22] focused on identifying the peaks (modes) in the histogram of the collected bandwidth samples and used these peaks to estimate the bandwidth; however, as Figs. 3 and 4 show, this can be misleading when the distribution of the noise is not known a-priori.
For example, the tallest peak on the right side of Fig. 3 is located at 13 ms ( ˜ C = 923 kb/s), which is only a slightly better estimate than 827 kb/s derived from the mean of y n .
Moreover, the tallest peak in Fig. 4(a) is located at 14.5 ms, which leads to a worse estimate˜Cestimate˜ estimate˜C = 827 kb/s compared to 983 kb/s computed from the mean of y n .
To combat these problems, existing studies [1], [4], [5], [15], [22] apply numerous empirical methods to find out which mode is more likely to be correct.
This may be the only feasible solution in multi-hop networks; however, one must keep in mind that it is possible that none of the modes in the measured histogram corresponds to ∆ as evidenced by both graphs in Fig. 3.
Another topic of debate in prior work was whether packettrain methods offer any benefits over packet-pair methods.
Some studies suggested that packet-train measurements converge to the available bandwidth 3 for sufficiently long bursts of packets [1], [4]; however, no analytical evidence to this effect has been presented so far.
Other studies [5] employed packet-train estimates to increase the measurement accuracy of bottleneck bandwidth estimation, but it is not clear how these samples benefit asymptotic convergence of the estimation process.We consider a hypothetic packet-train method that transmits probe traffic in bursts of k packets and averages the interpacket arrival delays within each burst to obtain individual samples {Z k n }, where n is the burst number.
For example, if k = 10, samples y 2 , . . . , y 10 define Z 10 1 , samples y 12 , . . . , y 20 define Z 10 2 , and so on.
The reason for excluding samples y 1 , y n+1 , . . . , y nk+1 is because they are based on the leading packets of each burst, which encounter large inter-burst gaps in front of them and do not follow the model developed so far.In what follows in this section, we derive the distribution of {Z k n } as k → ∞.
Claim 2: For sufficiently large k, constant x n = x, and a regenerative processes M (t), packet-train samples converge to the following Gaussian distribution for large n:Z k n D −→ N ∆ + λxE[S j ] C , λxV ar[S j − λE[S j ]X i ] (k − 1)C 2 ,(12)where D −→ denotes convergence in distribution, N (µ, σ) is a Gaussian distribution with mean µ and standard deviation σ, and X i are inter-packet arrival delays of cross-traffic.
Proof: First, define a k-sample version of the cumulative reward process in (5):V k n = M (a kn )−M (a k(n−1)+1 ) j=1 S j , n = 1, 2, . . . .(13)Process V k n is also a counting process, however, its timescale is measured in bursts instead of packets.
Thus, V k n determines the amount of cross-traffic data received by the bottleneck link during an entire burst n. Equation (13) shows that Z k n can be asymptotically interpreted as the reward rate of the reward-renewal process V k n :Z k n = ∆ + V k n (k − 1)C ,(14)where k − 1 is the number of inter-packet gaps in a k-packet train of probe packets.
Assuming M (t) is regenerative and for sufficiently large k, we have [28]:Z k n = ∆ + V k 1 (k − 1)C + o(1).
(15)Applying the regenerative central limit theorem, constraining the rest of the derivations in this section to constant x n = x, and assumingE[X i ] < ∞ [28]: V k 1 k − 1 D −→ N λxE[S j ], λxV ar[S j − λE[S j ]X i ] k − 1 .
(16)Combining (15) and (16), we get (12).
First, notice that the mean of this distribution is the same as that of samples {y n } in (11), which, as was intuitively expected, means that both measurement methods have the same expectation.
Second, it is also easy to notice that the variance of Z k n tends to zero as long as V ar [X i ] is finite.
Claim 3: If V ar [X i ] is finite, the variance of packet-train samples Z k n tends to zero for large k. Proof: Since λ, x, and E[S j ] are all finite and do not depend on k, using independence of S j and X i in (12), we have:V ar[Z k n ] = λxV ar[S j ] + λ 3 xE 2 [S j ]V ar[X i ] (k − 1)C 2 2 ,(17)which tends to 0 for k → ∞.
As a result of this phenomenon, longer packet trains will produce narrower distributions centered at ∆ + E[ω n ].
The CBR case already studied in Fig. 3(b) clearly has finite V ar [X i ] and therefore samples {Z k n } must exhibit decaying variance as k increases.
One example of this convergence for packet trains with k = 5 and k = 10 is shown in Fig. 6.
Now we address several observations of previous work.
It is noted in [5] that while packet-pair histograms usually have many different modes, the histogram of packet-train samples becomes unimodal with increased k.
This readily follows from (12) and the Gaussian shape of {Z k n }.
Previous papers also noted (e.g., [5]) that as packet-train size k is increased, the distribution of samples {Z k n } exhibits lower variance.
This result follows from the above discussion and (17).
Furthermore, [5] found that packet-train histograms for large k tend to a single mode whose location is "independent of burst size k." Our derivations provide an insight into how this process happens and shows the location of this "single mode" to be ∆ + E[ω n ] in (12).
In summary, packet-train samples {Z k n } represent a limited reward rate that asymptotically converges to a Gaussian distribution with mean E[y n ].
Perhaps it is possible to infer some characteristics of {X i } by observing the variance of {Z k n } and applying the result in (12); however, since there are several unknown parameters in the formula (such as V ar [X i ] and E[S j ]), this direction does not lead to any tractable results unless we assume a particular process M (t).
Since {Z k n } asymptotically tend to a very narrow Gaussian distribution centered at ∆ + E[ω n ], we find that there is no evidence that {Z k n } measure the available bandwidth or offer any additional information about the value of ∆ as compared to traditional packet-pair samples {y n }.
In this section, we relax the stationarity and renewal assumptions about cross-traffic and derive a robust estimator of C and ¯ r. Assume an arbitrary arrival process r(t) for crosstraffic, where r(t) is its instantaneous rate at time t.
We impose only one constraint on this process -it must have a finite time average ¯ r shown in (1).
The goal of the sampling process is to determine both C and ¯ r.
Since ¯ r > C imply constant packet loss and zero available bandwidth, we are generally interested in non-trivial cases of ¯ r ≤ C. Stochastic process r(t) may be renewal, regenerative, a superposition of ON/OFF sources, self-similar, or otherwise.
Furthermore, since packet arrival patterns in the current Internet commonly exhibit nonstationarity (due to day-night cycles, routing changes, link failure, etc.), our assumptions on r(t) allow us to model a wide variety of such non-stationary processes and are much broader than commonly assumed in traffic modeling literature.Next, notice that if the probing traffic can sample r(t) using a Poisson sequence of probes at times t 1 , t 2 , . . ., the average of r(t i ) converges to ¯ r (applying the PASTA principle [28]):lim n→∞ r(t 1 ) + r(t 2 ) + ... + r(t n ) n = lim t→∞ 1 t t 0 r(u)du = ¯ r,(18)as long as delays τ i = t i − t i−1 are i.i.d. exponential random variables.
In order to accomplish this type of sampling, the sender must emit packet-pairs at exponentially distributed intervals.
Assuming that the i-th packet-pair arrives to the router at time t i , it will sample a small segment of r(t) by allowing g i amount of data to be queued between the probes:g i = t i +x i ti r(u)du ≈ r(t i )x i ,(19)where x i is the spacing between the packets in the i-th packetpair.
Again, assuming that y i is the i-th inter-arrival sample generated by the receiver, we have:y i = ∆ + g i C = ∆ + r(t i )x i C .
(20)Finally, fixing the value of x i = x, notice that W n has a well-defined limit:lim n→∞ W n = lim n→∞ 1 n n i=1 ∆ + r(t i )x i C = ∆ + x C lim n→∞ n i=1 r(t i ) n = ∆ + x¯ r C .
(21)In essence, this result 4 is similar to our earlier derivations, except that (21) requires much weaker restrictions on crosstraffic and also shows that a single-node model is completely tractable in the setting of almost arbitrary cross-traffic.
We next show how to extract both C and ¯ r from (21).
Observe that (21) is a linear function of x, where ¯ r is the slope and ∆ is the intercept 5 .
Therefore, by injecting packetpairs with two different spacings x a and x b , one can compute the unknown terms in (21) using two sets of measurements {y a i } and {y b i }.
To accomplish this, define the corresponding average processes to be W a n and W b n :W a n = 1 n n i=1 y a i , W b n = 1 n n i=1 y b i .
(22)The simplest way to obtain both W a n and W b n using a single measurement is to alternate spacing x a and x b while preserving the PASTA sampling property.
Using a one-bit header field, the receiver can unambiguously sort the interarrival delays into two sets {y a i } and {y b i }, and thus compute their averages in (22).
While samples are being collected, the receiver has two running averages produced by (22).
Subtracting W b n from W a n , we are able to separate ¯ r/C from ∆:lim n→∞ (W a n − W b n ) = (x a − x b )¯ r C .
(23)Next, denote by˜∆by˜ by˜∆ n the following estimate of ∆ at time n:˜ ∆ n = W a n − x a W a n − W b n x a − x b .
(24)Taking the limit of (24), we have the following result.
Claim 4: Assuming a single congested bottleneck for which time-average rate ¯ r exists, ˜ ∆ n converges to ∆:lim n→∞˜∆ n→∞˜ n→∞˜∆ n = ∆.
(25)Proof: Re-writing (25):lim n→∞˜∆ n→∞˜ n→∞˜∆ n = ∆ + x a ¯ r C − x a (x a − x b )¯ r C(x a − x b ) = ∆,(26)which is obtained with the help of (21), (23), and (24).
Our next result shows a more friendly restatement of the previous claim.
Corollary 1: Assuming a single congested bottleneck for which time-average rate ¯ r exists, estimate˜Cestimate˜ estimate˜C n = q/ ˜ ∆ n converges to capacity C:lim n→∞˜C n→∞˜ n→∞˜C n = lim n→∞ q W a n − x a W a n −W b n xa−x b = lim n→∞ q(x a − x b ) x a W b n − x b W a n = C.(27) Notice that knowing an estimate of C in (27) and using ¯ r in (23), it is easy to estimate the mean rate of cross-traffic:lim n→∞ (W a n − W b n ) ˜ C n x a − x b = ¯ r,(28)which leads to the following result.
Corollary 2: Assuming a single congested bottleneck for which time-average rate ¯ r exists, the following converges to the available bandwidth A = C − ¯ r:lim n→∞ q x a − x b − W a n + W b n x a W b n − x b W a n = C − ¯ r = A.(29) We confirm these results and compare our models with several recent methods spruce [26], IGI [8], and pathload [12] through ns2 simulations.
Since the main theme of this paper is bandwidth estimation in heavily-congested routers, we conduct all simulations over a loaded bottleneck link in Fig. 2 with utilization varying between 82% and 92% (the exact value changes depending on C and the interaction of TCP cross-traffic with probe packets).
Delays x a and x b are set to maintain the desired range of link utilization.Define E A = | ˜ A − A|/A and E C = | ˜ C − C|/C to be the relative estimation errors of A and C, respectively, where A is the true available bandwidth of a path, ˜ A is its estimate using one of the measurement techniques, C is the true bottleneck capacity, and˜Cand˜ and˜C is its estimate.
Table I shows relative estimation errors E A for spruce, IGI, and pathload.
For pathload, we averaged the low and high values of the produced estimates˜Aestimates˜ estimates˜A.
In the IGI case, we used the estimates available at the end of IGI's internal convergence algorithm.
Also note that we fed both spruce and IGI the exact bottleneck capacity C, while model (29) and pathload operated without this information.As the table shows, spruce performs better than pathload in heavily-congested cases, which is expected since it can utilize the known capacity information.
Interestingly, however, IGI's estimates are worse than those of pathload even though IGI utilizes the true capacity C in its estimation algorithm.
A similar result is observed in [26] under a relatively small amount of cross-traffic (20% to 40% link utilization).
Next, we examine models (27), (29) with a large number of samples to show their asymptotic convergence and estimation accuracy.
We plot the evolution of relative estimation errors E C and E A in Fig. 7.
As Fig. 7(a) shows, ˜ C converges to a value that is very close (within 3%) to the true value of C.
In Fig. 7(b), the available bandwidth estimates quickly converge within 10% of A. For the purpose of comparison, we next plot estimation errors E A produced by spruce and IGI in Fig. 8.
As Fig. 8(a) shows, even with the exact value of C and after 1,000 samples, spruce exhibits an error of 27%.
Furthermore, IGI's estimates are much worse than spruce's as illustrated in Fig. 8(b), which plots the evolution of errors until IGI's internal algorithm terminates.To better understand these results, we next study how the accuracy of capacity information provided to spruce and IGI affects their measurement accuracy.
Since bottleneck capacities of Internet paths are not generally known, the use of spruce and IGI may be limited to a small number of known paths, unless these methods can obtain capacity measurements from other tools such as nettimer [15], [16], pathrate [5], or CapProbe [13].
These estimators of C are usually very accurate when the routers along the path are not highly utilized; however, as link utilization increases, their results often become inaccurate.
Hence, a natural question is how robust are spruce and IGI to inaccurate values of C provided to their estimation process?
We examine this issue below, but first discuss simulation results of CapProbe to illustrate the pitfalls that many estimators of C experience over heavy-loaded bottleneck links.For the CapProbe simulation, we use the topology shown in Fig. 2 with a single bottleneck link whose utilization was kept at 85% using five TCP cross-traffic sources.
Fig. 9(a) plots the evolution of E C produced by CapProbe in this experiment.
As the figure shows, CapProbe's minimum filtering is sensitive to random queuing delays in front of the first packet of the pair and can eventually converge to a completely wrong value (60% error).
We next examine how the value of˜Cof˜ of˜C = C supplied to IGI and spruce affects their accuracy.
We plot the relative estimation errors in Fig. 9(b), in which the accuracy of both methods deteriorates wheñ C/C becomes large.
To understand the exact effect of˜Cof˜ of˜C on these methods, we have the following simple analysis.According to [8], IGI first sends packet trains Z i with interpacket spacing x i (x i < x j for i < j) to determine the turning pointˆxpointˆ pointˆx.
Each packet-train consists of k back-to-back packets and the turning point is the n-th inter-packet spacing x n at which the receiving rate of probe traffic starts to match the sending rate:ˆ x = x n = 1 k − 1 k i=2 y i .
(30)Subsequently, IGI computes the average cross-traffic rate ¯ r as following [8]:¯ r = y i >max(∆,ˆ x) ˜ C (y i − ∆) (k − 1)ˆ x (31)and estimates available bandwidth by subtracting (31) from its a-priori known value of˜Cof˜ of˜C: ˜ A = ˜ C − ¯ r. Notice from (31) that the average cross-traffic ¯ r depends on the capacity value provided to the estimation algorithm.
This dependency explains the increased estimation inaccuracy wheñ C/C = 1 as illustrated in Fig. 9(b).
Similarly, estimation accuracy of spruce also changes as a function of˜Cof˜ of˜C.
For example, even though spruce provides better estimates than IGI with a correct capacity value˜Cvalue˜ value˜C = C = 1.5 mb/s, it exhibits large estimation errors (over 200%) wheñ C exceeds C by 33%.
This means that the accuracy of spruce is also heavily dependent on the performance of the underlying bottleneck bandwidth estimation method.To better understand this observation, we next analyze spruce's estimation process.
Spruce collects individual samples A i [26]:A i = C 1 − y i − ∆ ∆ , (32)where y i is the i-th measured packet spacing at the receiver.
The algorithm averages samples A i to obtain a running estimate of the available bandwidth:A n = 1 n n i=1 A i = 2C − C 2 qn n i=1 y i .
(33)Taking the limits of (33) and substituting W n from (21) into (33), we get˜Aget˜ get˜A = lim n→∞ A n = C − x ∆ ¯ r.(34)This brief analysis shows that the bandwidth estimation mechanism in spruce requires that the sender set its interpacket spacing x to be ∆ = q/C.
This is possible when C is known exactly; however, in cases when C is not correctly estimated, the initial spacing x cannot be set to ∆ and spruce cannot converge to A. Also notice that if ¯ r C, estimation errors are generally small; however, as link utilization increases, any deviation of x/∆ from 1 will have a significant impact oñ A.
In this section, we extend our single-node model to the case of multiple congested routers and conjecture that it is impossible to derive a closed-form solution that filters out the noise introduced by cross-traffic at several links of an end-toend path.
Consider the original model of a router in (2).
This time, assume that none of the probe packets queue behind each other at the bottleneck router.
This means that packet n − 1 leaves the router before packet n arrives, which is expected if interpacket spacing x n at the source is very large compared to the transmission delay ∆.
Under these assumptions, d n−1 < a n and (2) becomes:d n = a n + ω n + ∆, n ≥ 1.
(35)Hence, inter-departure delays y n are:y n = a n − a n−1 + ω n − ω n−1 , n ≥ 2.
(36)Notice that the first term a n −a n−1 in (36) is the inter-arrival delay x n of the probe traffic and the second term ω n − ω n−1 can be modeled as some zero-mean random noise.
This can be explained intuitively by noticing that under the assumption of large x n , each router delays probe packets (on average) by the same amount.
Then the distance between each pair of subsequent packets fluctuates around the mean of x n .
Using an inductive argument, it is also easy to show the following.Claim 5: If the initial spacing x n is larger (in a statistical sense) than queuing delays experienced by packets at each router of an N -hop end-to-end path, the mean of the sampled signal y n is equal to E[x n ] and the following holds for each router j:E[ω (j) n − ω (j) n−1 ] = 0.
(37)To confirm that the zero-mean model in (37) holds in practice, we run ns2 simulations with 85% utilization at the bottleneck link and varying packet sizes of CBR and TCP cross-traffic.
The plots of E[y n ] as a function of x n for different values of x n = x are shown in Fig. 10.
As the two figures show, E[y n ] converges to x n at 40 and 100 ms, respectively, at which time the noise at the bottleneck router becomes zero-mean-additive.
Note that similar results hold for multiple congested routers, different traffic patterns, and different packet sizes.
Also note that the point at which E[y n ] converges to x is not necessarily the value of the available bandwidth as was suggested in prior work [8].
Assuming multiple congested routers along a path, the result in (25) no longer holds.
To better understand multi-link effects, we use the topology in Fig. 11, where the bottleneck link C has capacity 1.5 mb/s and pre-bottleneck link C p = 1.8 mb/s.
Five FTP sources are attached to each of snd2 and snd3 and both links are utilized by cross-traffic at 85%.
Fig. 12 illustrates the convergence process of˜∆of˜ of˜∆ n for the single-node and multi-node cases.
As Fig. 12(b) shows, estimates˜∆estimates˜ estimates˜∆ n do not converge to the true value of ∆ = 8 ms, regardless of the number of samples y i .
Notice that it is possible to recursively extend the original model in (26) to multiple congested links where the input x n of each link is the output y n of the previous link.
However, this model becomes intractable as we show next.
Add index j to each process and each random variable to indicate that it is local to router j along the path from the sender to the receiver.
Further assume that Q (j) i is the queuing delay (due to cross-traffic and other probe packets) in front of packet i inside router j and φ (j) i is some zero-mean noise process at router j at time i. Then, we define the following recursive model:y (j) i = ∆ j + ω (j) i , x (j) i < Q (j) i−1 ∆ j + φ (j) i , x (j) i ≥ Q (j) i−1 ,(38)where y (j) i is the departure spacing of the i-th packet-pair from router j, ∆ j = q/C j is the transmission delay of the probe packets over link j, and x One of the main difficulties of this situation is the stochastic mixture of the two types of noise in (38).
While at this time we do not offer a complete treatment of this problem, we show that even when all links follow the first model (i.e., all packets queue behind each other), the problem appears intractable.
Under these assumptions, (38) leads to: where r j (t) is the cross-traffic process at router j. For a two congested-router case, (39) becomes:W (j) n = ∆ j + 1 nC j n i=1 r j (t i )x (j) i ,(39)W (2) n = ∆ 2 + 1 nC 2 n i=1 r 2 (t i )y (1) i = ∆ 2 + 1 nC 2 n i=1 r 2 (t i ) ∆ 1 + r 1 (t i )x C 1 → ∆ 2 + ∆ 1 ¯ r 2 C 2 + x C 1 C 2 ∞ 0 r 1 (u)r 2 (u)du.
(40)Since Poisson samples taken at the receiver lead to obtaining a time average of a mixed signal introduced by the two routers, it appears impossible to filter out the unknown cross-traffic statistics or the main integral in (40).
We next examine how the amount of cross-traffic in two congested links affects the estimation accuracy of the earlier models developed in the paper.
We set the average crosstraffic rates r 1 (t) and r 2 (t) to be the same in both routers (i.e., ¯ r 1 = ¯ r 2 = µ) and vary this value between 500 kb/s and 1.2 mb/s.
We again use the topology in Fig. 11 and plot in Fig. 13 the value of˜Cof˜ of˜C n computed using (27) when crosstraffic is simultaneously injected into both links (C p = 2 mb/s, C = 1.5 mb/s).
As the figure illustrates, the estimates of the bottleneck bandwidth converge to a value that is significantly less than the true capacity C and become progressively worse as µ increases.
This paper examined the problem of estimating the capacity/available bandwidth of a single congested link and showed a simple stochastic analysis of the problem.
Unlike previous approaches, our estimation did not rely on empirically-driven methods, but instead used a queuing model of the bottleneck router that specifically assumed non-negligible, non-fluid cross-traffic.
It is also the first model to provide simultaneous asymptotically-accurate estimation of both C and A in the presence of arbitrary cross-traffic.
Our analysis in the multi-node cases suggests that the problem of obtaining provably-convergent estimates of C and A in such environments does not have a solution.
This insight possibly explains the lack of analytical/stochastic justification for many of the proposed methods.
Knowing that accurate estimation of C for the multi-node case is impossible will potentially provide an important foundation for the methods that rely on empirically-optimized heuristics.
