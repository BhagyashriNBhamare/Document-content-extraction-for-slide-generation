Real-time micro-blogging services such as Twitter are widely recognized for their social dynamics-how they both encapsulate a social graph and propagate information across it.
However, the content of this information is equally interesting since it frequently reflects individual experiences with a broad variety of real-time events.
Indeed, events of broad interest are commonly revealed in correlated spikes of semantically-related posting activity.
In this paper, we explore one such application this of phenomenon: using Twitter data to infer on-line Internet service availability.
We show that simple techniques are sufficient to extract key semantic content from "tweets" (i.e., service X is down) and also filter out extraneous noise.
We demonstrate the efficacy of this approach at identifying a range of large-scale service outages in 2009 for popular services such as Gmail, Bing and PayPal.
Real-time micro-blogging, exemplified by Twitter, has quickly achieved widespread popularity among Internet users.
Twitter allows users to publish 140-character long messages (known as tweets).
Individuals that subscribe to a particular user's updates, called the followers of that user, see these updates in real-time.
Moreover, these messages are instantly viewable and searchable to anyone on the internet.
One recent report [19] estimates that Twitter's user population climbed to over 75 million in 2009, and the number of legitimate (i.e., non-spam) tweets posted to the site per day has reached over 50 million [52].
However, this distribution is by no means uniform, and a significant fraction of Twitter users send a myriad of updates throughout the day on a vast array of topics.
This combination of a broad user base, large message volume and the dynamic range of the information has increasingly made Twitter a "go to" venue for acquiring current information on a topic.
Indeed, a number of reporting services have employed Twitter in their research for mainstream news stories [13].
Conversely, when an event of sufficient magnitude occurs, large numbers of contemporaneous tweets about the event inevitably follow.
Thus, aggregating across the messages of Twitter users is a kind of passive crowdsourcing, one in which individual users act as noisy sensors whose updates can be correlated and filtered to extract the presence of key events.In this paper we focus on one such application of this phenomenon: inferring Internet service availability.
Thus, we seek to determine when popular services experience downtime.
Understanding the prevalence of such events is increasingly important as consumers and businesses alike move to "cloud-hosted" applications.
For example, according to [7], more than 1.75 million businesses utilize Google Apps (and businesses pay $50 per user annually for access to the Google Apps collaboration suite), while the social networking site Facebook commands over 400 million users.
Thus, an outage at one of these sites can translate into both lost revenue and a significant disruption in the lives of users.
Moreover, while availability is a key priority for virtually all service providers, it is also clear that 100% availability has not been achievable in practice.In 2009, a large number of widely used Web services (e.g., Amazon, Gmail, Bing, etc.) suffered from widespread outages.
In this paper, we will demonstrate that these outages are in turn revealed across user tweets via a small number of lexical features such as the phrase "is down" and "fail" hash tags (e.g., "#gfail"), to signal an outage.
However, Twitter is by no means the only way to measure such outages.
There are a range of direct methods for measuring service availability, such as Keynote's well-known commercial network monitoring service, that are strictly superior in many ways.
In particular, direct and active measurements can be used to monitor a particular service over well-known regular intervals (e.g., every second) and with well-defined failure semantics such as a failed TCP connection to the site.
Thus, they offer results that are both timely and precise.Why then consider the indirect human-mediated approach expressed via Twitter messages?
Because inferring service downtime indirectly via human complaints offers a number of distinct advantages as well.
First, this approach does not require pre-specifying the service of interest; so long as the service is popular, its failure is likely to appear in the Twitter record.
Thus, service dependencies and relationships that may not be known a priori are implicitly revealed.
For example, a site that uses Google's AppEngine or Amazon's AWS may experience downtime when their underlying service infrastructure has an outage.
Second, Twitter offers a unique set of vantage points for observing failures -literally millions of locations across the Internet -that would be hard for any explicitly provisioned sensor infrastructure to match.
Finally, the human-mediated nature of a Twitter-based failure detector offers a degree of breadth and flexibility in the definition of failure that is difficult to encapsulate in a direct sensor.
Humans identify a far broader array of experiences as a failure (e.g., site is too slow, messages aren't being received, messages are corrupted, buttons don't work, data is out of date, site time's out, etc.) than are typically or easily encoded as predicate conditions in a measurement device.
In a sense the end-user's perception of downtime is the closest to a true end-to-end measurement and certainly the one that most constitutes customer satisfaction.
As Protagoras is known for saying, "Man is the measure of all things.
"Thus, while we would not argue for using Twitter to replace existing measurement services, we do argue that it is a useful complement that offers its own advantages.The remainder of the paper is organized as follows.
Section 2 provides background on the Twitter service and related work focused on mining this data, followed by a description of our technique in Section 3.
Sections 4 describe our results and an analysis over a large Twitter data set, and Section 5 summarizes our findings.
Social networking Web sites are increasingly important for a range of personal and professional interactions.
While many social networks focus on sharing information within a limited circle of well-known connections, the Twitter social network has a different design point focused on broad real-time information dissemination.
The units of information produced and consumed in the Twitter network are short textual messages known as tweets.These are limited to 140 characters and are published under the user's identifier.
The sole explicit relationship in the network is that of following.
A user can choose to follow anyone in the network (with the exception of protected users who must approve the relationship).
When a user U follows a user V , messages created by V are assembled in a chronologically ordered list of messages called the timeline for user U.Various textual conventions exist that assign specific semantics to different tokens in Tweets.
Words or phrases preceeded by a # symbol, which are known as hash tags, provide the ability to designate that a message discusses or references a particular topic.
One popular trend is to communicate failures or malfunctions using the #fail hash tag.
Our work will use mentions of #fail and its variants to detect anomalous conditions.The ease of composing a short message in any of the myriad of Twitter clients encourages users to share information spontaneously and in real-time.
Furthmore, Twitter differs from many other services in that messages are immediately made globally available and searchable.
These characteristics together make Twitter an attractive tool for studying a range of social phenomena and especially so for acute and broadly felt events such as the failures of popular Internet services we study here.
Moreover, Twitter offers significant advantages over other social networking services from a concrete data collection standpoint as well.
While almost all social networks offer an application programming interface (API) to extract and manipulate network data in a structured form, these APIs typically come with a substantial number of restrictions including limitations on the number of requests or duration over which one is permitted to store data.
Moreover, rapid page scraping or crawling is typically either prohibited by the terms of service or results in the crawler's IP address being blocked.
Thus, in practice these immense and valuable data sources are largely sealed in virtual walled gardens, available only to the service provider themselves.
By contrast, Twitter has embraced an open ecosystem that allows for large-scale data collection and the creation of real-time services.For many of these same reasons, Twitter has been an attractive data source for a range of academic research which can be roughly broken into two categories.
The first concerns studies of Twitter itself, its user base, their relationships and the resulting social network.
This work is plentiful, and among the better known examples, Java et al. examine the topological, geographical and communal properties of the social network [23], while Krishnamurthy et al. present a comprehensive characterization of Twitter users (e.g., followers vs. followings) [27].
A second, but smaller, set of research has looked at applications of the Twitter data itself (typically social applications).
For example, Jansen et al. analyze more than 150,000 postings to determine branding sentiments [22].
In particular, the authors identify a small number of brands a priori and search for tweets containing those words; their search terms are generally unambiguous (e.g. "Banana Republic").
Similarly, Jagen et al. investigate building a news processing service based on Twitter [37].
In particular they hand-pick a set of 2,000 reputable Twitter users (e.g., CNN, MSNBC) whose tweets are used to determine that a topic has become "news".
To the best of our knowledge, the only other paper that considers the use of Twitter for inferring non-social phenomena is Sakaki et al.'s study of Earthquake sensing [36].
Their recent paper describes techniques for inferring the epicenter of earthquakes and the trajectories of typhoons based on tweet geo-location.
However, most of the complexity in their application is centered around the development of the associated spatial models and not in detecting the presence of the underlying phenomenon.
This is because the authors are able to use very uncommon words, "earthquakes" and "typhoons", to locate pertinent tweets and thus have few problems with filtering noise.
By contrast, our application is less complex since we only care about the presence of the event (Internet service outage) but must contend with more noise in the data stream, since all of our terms are commonly found in a wide variety of tweets.
At a high level, our approach is to treat certain tweets as signals that a particular service is experiencing an outage, whether planned or unplanned.
Thus, the Twitter network becomes a large heterogeneous sensor network, where the Twitter users serve as sensor nodes.
Our service outage signals consist of two simple lexical features: the phrase "X is down" and the hash tag idiom "#Xfail" where X is typically the name of an online service.
For example, T3.
Gmail is down!
EVERYBODY PANIC!
T4.
...oh not again #gmailfail To drive our study, we have collected content from the Twitter social network using their publicly available API.
However, due to the scale of Twitter, this itself is a significant effort.
A complete description of the data collection methodology and infrastructure is beyond the scope of this paper, but we outline the basic approach here.Twitter implements their API using simple HTTP methods that accept or return data in a structured format, such as XML or strings in JavaScript Object Notation (JSON) form.
To avoid excessive use and abuse of this service, the number of requests per client is limited; the baseline number of requests is 150 per hour per client.
However, users can request their screen name or IP address to be white-listed, at which point they can make 20,000 requests per hour from that resource.
We have created a distributed crawler that can capture user profiles, network topology and message content at scale.
Our distributed infrastructure consists of crawler machines and a centralized controller.
The controller prioritizes workloads, which are assigned to the crawler machines.
These workloads consist of a collection of user identifiers and specific content to retrieve, such as the user's profile, social links, or messages.
We implemented this system in Python.
To enable better network concurrency, each crawler launches many worker processes which are each given a small fraction of the crawl list.
With this architecture we regularly maintain more than one thousand concurrent connections to the API.
Once a crawler receives a response indicating it has no remaining API calls, data retrieved from the API is aggregated and sent to the Amazon Web Services S3 platform for persistent storage.
This data is then parsed, filtered, and organized in a distributed filesystem for later consumption by analysis processes.
To detect outage tweets, we focus on those that express a predicate over some set of entities.
We define two particular predicates, IsDown(X) and Fail(X), corresponding to the associated lexical features we described earlier.IsDown Predicate.
Specifically, we say a tweet expresses the predicate IsDown(X) if it simply contains the phrase "X is down," where X is a token consisting of alphanumeric characters.
Tweet T3 earlier expresses the predicate IsDown(Gmail).
In general, X is the word immediately preceding the phrase; however, we make exceptions to include two preceding words when X is in the set {mail, email, store}.
While predominantly used to describe services, the expression "is down" can be used for other purposes.
For example, the subject of the abridged tweet "kicking a man while he is down ;)" is Tiger Woods.
Thus, our signal detection must be resilient to noise resulting from semantic ambiguity.Fail Predicate.
Users sometimes express that a service is unavailable using hash tags of the form "#Xfail", where X is a (sometimes abbreviated) service name.
A tweet might also convey the same meaning by containing two separate hash tags, "#X" and "#fail".
We say that a tweet expresses the Fail(X) predicate if it contains either form.
These hash tags are more generally used as an expression of the "fail" meme [55], e.g., "#obamafail," "#gopfail," and "#epicfail."
Since this trend is rather widespread in the meme-aware Twitter community, we do not use this signal in isolation.
Rather, we use the Fail(X) predicate to reinforce the signal generated by IsDown(X).
We chose these predicates because they appeared in a large number of tweets concerning service outages.
We hand labeled 878 tweets from the Gmail(2), Hotmail, PayPal, and Bing outages.
We discovered that the top bigram among this set was "is down" (2.4% of all bigrams), and the top hashtag (not consisting of a company name) was fail (8.2% of all hashtags).
To validate both the subject and topic detection of the predicates, we extracted the two words preceding and following the name of a company during a known outage.
Table 1 shows the top five, 2-word phrases surrounding the service name.
Note that, if the tweet is comprised of "is gmail down?"
, then the preceding phrase is simply "is", and the following phrase is "down".
The table shows that users who tweeted about the service in question expressed that the service was down, and they did so by tweeting "service is down".
We can incorporate additional predicates into our system (capturing both synonyms such as "crashed" and syntactic variants such as "its down") to further improve our detection mechanism.
However, we find that IsDown is sufficient to capture many service outages, and we restrict our analysis to this single predicate.We emphasize that expressing an IsDown or Fail predicate is defined lexically; we do not attach any formal semantics to each individual tweet.
Rather, our premise is that in the aggregate an increase in the frequency of expression of a given predicate correlates to a real event.
In the next section, we evaluate the validity of this premise.
Before doing so, we need to specify how many expressions of the IsDown or Fail predicates are combined to infer that a service outage has taken place.
In the normal course of events, we expect that there will be some number of tweets that have the lexical signature of the IsDown or Fail predicates.
In some cases, the user truly intended to express that something was "down", while in others, the user was expressing something unrelated.
Because Fail is more ambiguous than IsDown we focus our analysis on detecting outages using IsDown.
Later, we describe how to incorporate the Fail predicates to filter out false positives from our detection technique.
We partition the tweets matching the IsDown predicate into five-minute bins.
Subsequently, we use an Exponentially Weighted Moving Average (EWMA), commonly used to predict the next value in a time series, to detect significant deviations away from normal behavior [8,53].
Let y n denote the number of tweets expressing the IsDown predicate at time interval n.
We then compute the EWMA value, M n at time n as follows:M n = α * y n + (1 − α) * M n−1where α dictates how much weight the current value is given.
We also compute a smoothed deviation σ n to determine whether an anomaly is occurring at n.D n = y n − M n−1 σ 2 n = β * D 2 n + (1 − β ) * σ 2 n−1Finally, we compute threshold T n as:T n = M n−1 + ε * σ n−1If y n is greater than equal to T n for two consecutive intervals, we signal a service outage.
We wait for two consecutive threshold violations because flagging the first violation yields a large number of false positives, and waiting for three produces too many false negatives.
Once we signal an outage for a particular entity, we do not signal additional outages for that entity for 12 hours.
In our experience, most outages are repaired within this period.
There are three parameters to configure in the above equations: α, β , and ε.
We obtain a validation set of labeled events by searching for outages affecting a small set of varied services: Flickr, Hotmail, LiveJournal, Ning, PayPal, and TMobile.
To construct the validation set, we located days where the IsDown predicate was expressed more than 2 times, then we searched for news articles pertaining to the outages.
Additionally, Flickr [15], LiveJournal [28], and Ning [32] maintain blogs that mention downtimes; we use these to check for false positives and to find false negatives.
In total, we discovered eight outage events for Flickr, four for Hotmail, 10 for LiveJournal, 14 for Ning, three for PayPal, and six for TMobile.
Note that we do not differentiate between planned and unplanned downtime.To find the best settings for the three parameters, we attempted various combinations and picked the set that yielded the highest average F-Score across the six validation sets.
Note that we express α and β in terms of their half lives; a half life of x for either parameter means that an observation from x minutes ago receives half the weight of the current one.
We varied the half lives for α and β in five-minute increments, iterating over the range [5,30] for α and [5,90] for β .
Additionally, we varied ε from one to three in 0.5 increments.
We found that α with a half life of 15 minutes, β with a half life of 60 minutes and ε equal to two produced the best results, with an average F-Score of 0.510.
In this section, we evaluate how well our technique identifies service outages.
Using the data collected in Section 3 we ran our analysis on tweets authored during 2009 calendar year.
To assess how well our system detects real outages, we identified a priori eight events when a known service disruption took place, and examined the strength of the corresponding signal generated using our technique.
Subsequently, we applied our analysis on the entire corpus of tweets.
We manually inspected the top 50 events (as measured by the volume of tweets matching the IsDown predicate) identified by our system, as well as 50 random events detected by our methodology.
We begin by evaluating the performance of our system on known outages.
For this part of the evaluation, we selected several of the top services listed by Alexa.
Using Google News, we attempted to find articles pertaining to outages for the services in question.
We chose the events shown in column lists the hours (US Pacific time) that the article claims the outage took place.Recall that we signal an outage when our observations exceed a threshold defined by both a smoothed average and variance.
To decrease the number of false positives, we require that a threshold violation span two consecutive bin intervals (5 minutes) before we raise an alarm.
To further minimize false positives, we can check whether the Fail predicate is expressed within 60 minutes of the triggered IsDown alarm.
Figures 1 and 2 show our results on the known events.
For each top graph, three lines are shown: the line labeled isdown displays the volume of tweets matching the IsDown predicate, while the lines labeled avg and threshold show the computed EWMA and threshold values, respectively.
Below each graph we show 3 impulses.
The one labeled reported indicates when the downtime transpired according to the news articles.
The one labeled isdown indicates when our system triggers an outage alarm using only the EWMA violation.
The final impulse labeled isdown+fail shows when we signal an error using the added Fail predicate.We can see that the technique triggers an outage alarm for each known event.
The outages are detected anywhere from 10 minutes (Bing, PayPal) to 50 minutes (Gmail(1), Hotmail) after the reported downtime.
There is an opportunity improve upon our delay times by utilizing additional predicate expressions; we leave this objective to future work.
Adding in the Fail predicate to filter false positives adds little to no delay, as demonstrated by the overlayed signals on Gmail(1), RackSpace, and Bing.
In the worst case, we delayed triggering our alarm for an additional 25 minutes (Hotmail).
We now utilize our threshold-based signal detection on the entire corpus of tweets by extracting all tweets matching any of the predicates.
First, we ran our analysis without checking for the Fail predicate, discovering 5,358 "outages" over 1,556 entities.
However, we found a fair number of invalid entities in this set, such as "attendance", "visibility" and "tourism".
To filter out these false positives, we employed the Fail predicate, thus ensuring that at least 1 Fail occurs within 60 minutes of the IsDown trigger.
By including this predicate, our methodology produced 894 "outages" spread across 245 entities.
We inspected the 245 entities; 59 did not appear to be related to technology or Internet services.
For example, a myriad of sports teams appeared in the outage set (such as USC, Michigan, Duke, Liverpool), as did subjects related to fiscal matters (such as the DOW, economy).
We then analyzed the top 50 service outages, as determined by the volume of IsDown tweets that occurred within 12 hours of the trigger time.
Table 3 details 25 of the events captured by our methodology.
Of the top 50 events, we were able validate 48 of them with online news references.
YouTube downtimes comprised 11 of our top 50 events (or 22%), while Gmail outages contributed nine (18%).
One of the more unexpected downtimes in the top 50 was the Wikipedia outage on July 31st, 2009 [38]; we did not anticipate such a vociferous reaction to the event.Surprisingly, we are able to detect Twitter service outages.
Our experiences crawling and utilizing Twitter, as well as the commentary provided on the Twitter blog, indicate that Twitter often succumbs to sporadic API or website failures (affectionately known as 'fail-whales').
These errors tend to be transient; a user who encounters such an error might be able to successfully post a message on his next attempt.
As users can vocalize problems with Twitter during periods of degraded performance, we are able to, and frequently do, detect such events.Next, we randomly selected 50 outages that affected valid entities.
We successfully located news articles or forum postings that confirmed 35 of these downtimes.
We show these outages in Figure 3, and color code them according to whether they were confirmed.
In the random set of outages, we find many references to social media sites (LiveJournal [24], YouTube [51]), online games (Azphel [1], World of Warcraft [17]), and hosting services (1and1 [5] and BlueHost [21]).
Of the remaining outages that we could not confirm, one in particular seems rather large: YouTube on June 19th, 2009.
Over 165 tweets expressing the IsDown predicate were posted within the 12 hour window after the trigger time.
Also, we could not verify either of the outages detected for the sizable service LinkedIn on Oct. 7th or Dec. 22nd, 2009, although the magnitudes of the events were rather low.
Real-time microblogging represents a new "kind" of communication on the Internet, one that encapsulates information that was not previously shared broadly.
We believe that the "user as implicit sensor" model has tremendous potential for capturing a broad array of current events and sentiments as they happen.
This paper has explored one such application focused on Internet service outages, and we have demonstrated that even simple techniques can identify important events.Looking forward we identify three key areas of further work.
The first is in better extracting the semantics of tweets.
For our application, there was sufficiently large message volume and sufficiently narrow range of common expressions that a single phrase ("is down") was commonly found during true failures.
Trying to identify events with less broad impact or that engender a broader array of expression will require more language pre-processing.
Such processing can be particularly challenging because tweets are short, and make extensive use of a quickly evolving cultural short-hand that influences lexicon and grammar both.
One possible technique we can employ is latent semantic analysis.
Using LSA on a bag-of-words model of Tweets, we can possibly extract predicates that are word-position independent and capture additional ways of expressing "is down" in an unsupervised manner.Second, we would like to apply our techniques to a real-time stream of messages, to provide users with upto-date status information about their frequented sites.The last issue where we see a need is in maintaining state, both about the lexical conventions of individual users as well as capturing which users are typically "primary reporters" vs. users who simply provide "hearsay" tweets.
The first represents a real sample of a user's experience while the latter is simple a repetition of someone else's information (indeed, we have seen tweets reporting that they have heard a service "is down" even after it is back up).
We believe the most significant research opportunities revolve around identifying new applications that exploit the real-time nature of Twitter.
What kinds of questions about the world at large can be answered via mining this modality?
It is here that we expect to focus most of our future effort.
