Graph processing systems are used in a wide variety of fields, ranging from biology to social networks, and a large number of such systems have been described in the recent literature.
We perform a systematic comparison of various techniques proposed to speed up in-memory multicore graph processing.
In addition, we take an end-to-end view of execution time, including not only algorithm execution time, but also pre-processing time and the time to load the graph input data from storage.
More specifically, we study various data structures to represent the graph in memory, various approaches to pre-processing and various ways to structure the graph computation.
We also investigate approaches to improve cache locality, synchronization, and NUMA-awareness.
In doing so, we take our inspiration from a number of graph processing systems, and implement the techniques they propose in a single system.
We then selectively enable different techniques, allowing us to assess their benefits in isolation and independent of unrelated implementation considerations.
Our main observation is that the cost of pre-processing in many circumstances dominates the cost of algorithm execution, calling into question the benefits of proposed algorithmic optimizations that rely on extensive pre-processing.
Equally surprising, using radix sort turns out to be the most efficient way of pre-processing the graph input data into adjacency lists, when the graph input data is already in memory or is loaded from fast storage.
Furthermore, we adapt a technique developed for out-of-core graph processing, and show that it significantly improves cache locality.
Finally, we demonstrate that NUMA-awareness and its attendant pre-processing costs are beneficial only on large machines and for certain algorithms.
Interest in processing graph-structured data has grown over the last few years, especially for mining relationships in social network graphs.
Many graph processing systems have been built, including single-machine, cluster-based, in-memory and out-of-core systems [7, 8, 12-14, 16, 17, 19, 20, 22, 23, 26, 27, 29, 33, 36, 37].
In this paper we focus on single-machine in-memory graph processing systems.
With the recent increase in main memory size and number of cores, such machines can now process very large graphs in a reasonable amount of time.With few exceptions [4,28], most papers on graph processing systems present a new system and compare its performance (and occasionally its programmability) to previous systems.
While interesting, these comparisons are often difficult to interpret, because systems are multidimensional, and therefore a variety of features may contribute to observed performance differences.
Variations in hardware and software infrastructure, input formats, algorithms, graphs and measurement methods further obscure the comparison.In this paper we take a different approach.
Rather than comparing different systems, we compare different techniques used in graph processing systems, and we try to answer the question: what techniques provide what benefits for what types of algorithms and graphs?
We implement various techniques proposed in different papers in a single system.
We then selectively enable the different techniques, and compare the performance of the resulting approach on the same hardware platform for the same algorithms and graphs.In particular we take an end-to-end view of graph processing, often absent in other papers.
Graph processing involves loading the graph as an edge array from storage, pre-processing the input to construct the necessary data structures, executing the actual graph algorithm, and storing the results.
Most papers focus solely on the algo-rithm phase, but we demonstrate that there is an important trade-off between pre-processing time and algorithm execution time.
While we recognize that pre-processing can potentially be amortized over repeated executions, we show that gains in algorithm execution time can be completely undone by increases in pre-processing time.We structure our investigation of algorithm execution time along two dimensions.
In a first dimension, we distinguish between a vertex-centric approach, in which the algorithm iterates over vertices, and an edge-centric approach, in which the algorithm iterates over edges.
In addition, we propose a new iteration approach, adapted from out-of-core systems [37], in which the algorithm iterates over grids, with improved cache locality as a result.
In a second dimension, we distinguish between algorithms that push information to their neighbors, or pull information from them.
We also consider algorithms that dynamically choose between push and pull.To illustrate through a simple example the importance of an end-to-end view, we analyze the push-pull approach to Breadth First Search (BFS) 1 .
Earlier papers [2,3,29] have demonstrated that, for BFS, a push-pull approach results in better algorithm execution time than the conventional push approach.
Figure 1 shows the endto-end execution time of BFS on the well-known Twitter follower graph [18] using both approaches.
While the algorithm execution time is indeed 3× smaller for push-pull, the overall execution is completely dominated by pre-processing.
The pre-processing time is 2× larger for push-pull, resulting in 1.5× worse overall end-to-end time.In addition to different methods of iteration and information flow, various optimizations have been proposed to take advantage of memory locality on NUMA machines.
These optimizations often take the form of partitioning data structures during pre-processing, such that most accesses during algorithm execution are local to a NUMA node.
Continuing the theme of the trade-off of pre-processing versus algorithm execution times, we investigate whether such pre-processing pays off for graph processing.The main results of this paper are:• An illustration of the fundamental trade-off between pre-processing and algorithm execution time in graph processing.
• An evaluation of different techniques for building adjacency lists, showing that radix sort provides the best performance when the graph is in memory or when it is loaded from a fast storage medium.
• An evaluation of the pre-processing vs. algorithm execution time trade-off for vertex-centric vs. edge-1 see Section 6 for a precise definition of push-pull Pre-processing Algorithm Figure 1: Example of the trade-off between pre-processing and algorithm execution time for BFS on the Twitter graph: push-pull improves algorithm execution time, but the required pre-processing time leads to overall worse end-to-end execution time (measured on Ligra [29]).
centric computation, showing that the construction of adjacency lists for vertex-centric processing may or may not pay off, depending on the algorithm execution time.
• An evaluation of a push vs. pull information flow, illustrating the benefits of reduced computation for push vs. reduced synchronization for pull.
• An evaluation of the pre-processing vs. computation trade-off for combined push-pull information flow, showing that the extra pre-processing costs associated with this combination outweigh gains in algorithm execution time.
• The adaptation of an out-of-core technique for improving the cache locality and the synchronization overhead of an in-memory graph processing system.
• An evaluation of the pre-processing vs. computation tradeoff for NUMA-aware optimizations, demonstrating that their large pre-processing times can be compensated by gains in algorithm execution time only on large NUMA machines and only for certain algorithms.The outline of this paper is somewhat unusual.
We start in Section 2 with an overview of the hardware and software used in this paper.
We discuss data structures and pre-processing costs in Section 3.
In Section 4 we look at the relationship between the data layout and vertex-centric or edge-centric computation.
Section 5 discusses methods for improving cache locality.
In Section 6 we evaluate the choice between push and pull approaches and its implications for algorithm execution time, pre-processing time and synchronization overhead.
Section 7 evaluates graph partitioning approaches to take advantage of NUMA characteristics.
Section 8 summarizes results on graphs and algorithms not discussed in previous sections.
Section 9 provides an overview of all the results in one place.
Section 10 discusses the graph processing systems from which we draw inspiration for this work.
Section 11 concludes the paper.The code used for the experiments in this paper and instructions on how to run them is available at: https: //github.com/epfl-labos/EverythingGraph.
Experimental environment.
We evaluate the preprocessing and algorithm execution times on two machines, each representative of a large class of machines.
Machine A has 2 NUMA nodes, and is less sensitive to NUMA effects than machine B, which has 4 NUMA nodes.
More precisely, machine A has 2 Intel Xeon E5-2630 processors, each with 8 cores (16 cores in total) and a 20MB LLC cache, and 128GB of RAM.
Machine B has 4 AMD Opteron 6272 processors, each with 8 cores (32 cores in total) and a 16MB LLC cache, and 256GB of RAM.
Unless otherwise stated, all experiments are run on Machine B.The pre-processing times, unless otherwise stated, assume the graph is already loaded in memory.
The costs of loading the graph into memory and its implications on pre-processing are discussed separately.The subset of vertices or edges to be processed during a computation step is kept in a work queue.
Threads take work items from the queue in large enough chunks to reduce the work distribution overheads.
We parallelize both pre-processing and computation using the Cilk 4.8 parallel runtime system.
When needed, Cilk balances the work among threads by allowing threads to steal work items from one another.
Our experiments using OpenMP and PThreads show comparable execution times and are therefore not reported.Algorithms.
We select six algorithms with different characteristics in terms of functionality (traversal, machine learning, ranking), vertex metadata, as well as the number of vertices active during computation steps (iterations).
We evaluate the following three traversal algorithms.
Breadth-first search (BFS) traverses a graph from a given source vertex and builds a tree in breadth-first order.
Weakly connected components (WCC) discovers connected vertices within a graph and classifies them into components using label propagation.
Single source shortest path (SSSP) finds the (length of the) shortest path between a given source vertex and every other reachable vertex in the graph.
We also evaluate two algorithms that compute over the entire graph: Pagerank (PR) [24] is a ranking algorithm used to rank web pages based on their popularity.
Sparse matrix vector multiplication (SpMV) multiplies the adjacency matrix of a graph with a vector of values.
The matrix entries are stored as edge weights.
Finally, Alternating Least Squares (ALS) is an optimization method used in recommender systems.Datasets.
Table 1 gives an overview of the graphs used along with their number of vertices and edges.
We use both synthetic and real-world datasets.
The synthetic datasets are power-law graphs generated by the RMAT graph generator [5].
We generate graphs of different sizes to evaluate the scalability of optimizations in terms of graph size.
RMAT26 is the biggest RMAT graph that we can fit on all machines for all approaches.
As a realworld power-law dataset, we use the Twitter follower graph [18], which is the largest real-world dataset that fits on all machines for all approaches.In addition to these two graphs, we also use the USRoad graph from the DIMACS challenge [1].
This graph has a different shape than power-law graphs: it has a high diameter, and all vertices have a small in/out degree.
We use it to study the impact of the shape of the graph on different computation approaches.
Finally, for ALS we use the bipartite Netflix graph [35].
Vertices Due to space constraints, in Sections 3 to 7, we primarily present results for BFS and Pagerank (with 10 iterations).
These algorithms represent opposite ends of the spectrum, both in terms of the percentage of the graph that is active during each step of the computation and in terms of computation complexity.
Furthermore, we report results primarily for the RMAT26 graph.
We include results for other algorithms and graphs only when they provide additional insights that depend on the algorithm or the shape of the graph.
Section 8 completes the picture by presenting data on the combinations of algorithms and input graphs not discussed in earlier sections.Edges RMAT-N 2 N 2 N+4 Twitter 62M 1468M US-Road 23.9M 58M Netflix 0.5M 100M In this section we first present different data layouts and their associated pre-processing costs.
Edge arrays are the simplest and the default way to distribute graphs [27] and are used by many systems [6,12,27].
Graphs are stored as an array containing pairs of integers corresponding to the source and the destination vertex of each edge.
In the remainder of the paper, we assume the graph input takes the form of an edge array and needs to be further converted into other formats.Adjacency lists store edges in per-vertex edge arrays.
Each vertex points to an array containing the destination vertices of its outgoing edges, and possibly also to an array containing the source vertices of its incoming edges.
Edge array.
The layout of edge arrays matches the format of the input file, and it suffices to map the input file in memory to be able to start computation.
As such, edge arrays incur no pre-processing cost.Adjacency lists.
We explore two techniques to build adjacency lists.The simplest technique consists of reading the input file and dynamically allocating and resizing the edge arrays of vertices as new edges are discovered.The second technique avoids reallocations by loading the graph as an edge array and then sorting it by source vertex.
Vertices use an index in the sorted edge array to point to their outgoing edge array.
The incoming edge array is created by sorting the edge array by destination vertex.
This way the edges are stored contiguously in memory, corresponding to compressed sparse row format (CSR).
The performance of this approach depends on the sorting algorithm.The most common approach to sort edges is to use a count sort.
In a first pass over the edge array, we count the number of outgoing (incoming) edges for each vertex.
In a second pass over the edge array, we place edges at the correct location in the sorted edge array.
Most existing graph analytics frameworks use this approach, as it is optimal in terms of complexity (the input array is only scanned twice).
An alternative approach is based on radix sort.
Radix sort treats keys as multi-digit numbers, and sorts the keys into buckets one digit at a time.
In the parallel version, each thread recursively sorts a subset of edges into a small number of buckets [32].
The advantage of radix sort is that buckets are written sequentially, and therefore have good locality.
The complexity of the sort is relatively low.
We use a radix size of 8 bits (256 buckets) which only requires log 2 (#vertices)/8 recursions to sort the edge array (e.g., 4 recursions for a graph with 4 billion vertices, 8 recursions with 2 64 vertices).
than dynamically building the per-vertex edge arrays.
Radix sort is faster, because it has better cache locality than the other solutions.
Both the dynamic approach and count sort sequentially read the input edge array, but the subsequent steps have poor cache locality.
The dynamic approach requires jumping between per-vertex arrays to insert a newly read edge.
Count sort requires jumping between vertices as well in order to count their degree.
It then does another scan of the input to place edges at their corresponding offsets in the sorted edge array.
This step jumps between distant positions in the array.
Figure 2 presents the evolution of the pre-processing time for RMAT graphs depending on the graph size.
All approaches scale as the graph size increases.
The radix sort approach is always faster than the count sort and the dynamic sort approach (3.3× and 3.8×, respectively, on RMAT26).
For smaller graphs, count sort is slower than both the dynamic and radix approaches.
The approach requires reading the edge array twice (once for counting, and then once to place edges in the sorted array).
As the graph grows, however, the fact that the second pass in count sort does no reallocations makes it slightly better than the dynamic approach (e.g. there are 32 million reallocations for an RMAT26 graph).
RMAT-(N+1) is double the size of RMAT-N, and so is the preprocessing time.
The previous discussion assumes that the graph is already loaded into memory.
Conclusions are different when the graph is to be read from storage or over the network.
Indeed, doing a radix sort can only be partially overlapped with loading the graph in memory.
In contrast, the dynamic approach of allocating and resizing per-vertex edge arrays can be fully overlapped with loading.
For count sort, only the first pass can be overlapped with loading.
Table 3 presents the combined loading and preprocessing time when the graph is loaded from an SSD (380MB/s maximum bandwidth) and from a regular hard drive disk (100MB/s).
If we take loading speed into account, dynamically allocating per-vertex edge arrays becomes faster than radix sort when the storage medium is slow.
On the SSD the total time for the radix sort approach is shorter than or more or less the same as the dynamic approach.
The results for count sort are, as before, inferior, and are not included for that reason.
Summary.
Costs associated with loading and building data structures in memory are non-negligible, and different approaches shine in different situations.
Surprisingly, using radix sort to build adjacency lists is the fastest approach when the input file is in memory or loaded from a fast medium.
When the graph is loaded from a slow medium, building adjacency lists dynamically is a better option, because it can be overlapped with loading.4 Data layout and graph traversal 4.1 Vertex-centric vs. edge-centricThe choice of data layout impacts the decision of how to traverse the graph.
In this section, we show that the best performing data layout and corresponding traversal model depend on the algorithm.
Computation on edge arrays happens in an edgecentric manner, and is quite simple: at every iteration of the computation the whole edge array is scanned, and the graph algorithm is called on every edge.
This computation model is efficient, because scanning an edge array is cache-friendly: most of the accessed data is prefetched before being used.
The drawback of this layout is that it offers no easy way to work on a subset of the vertices: a full scan of the edge array is required to find the edges of a vertex.Adjacency lists are a natural solution to this problem.
They enable vertex-centric computation, in which work is only performed on the subset of active vertices.
To illustrate the impact of data layout and traversal model on the end-to-end execution time, we show in Fig- ure 3 the pre-processing and algorithm execution times of BFS, Pagerank, and SpMV on RMAT26.
For BFS, vertex-centric computation performs the best, because during an iteration BFS only works on a limited subset of the graph.
Edge arrays are not well suited for this type of computation, as all edges of the graph are read at every iteration.In contrast, Pagerank accesses the entire graph in every iteration.
Looking only at algorithm execution time, vertex-centric computation still performs a bit better, because it has better cache locality (all edges from a vertex are processed on the same core).
When taking into account the pre-processing time, however, the end-to-end execution time is the same as for edge-centric computation.Finally, SpMV is an algorithm that makes only a single pass over the graph.
Here, edge-centric computation produces the best end-to-end result, since the cost of building adjacency lists for vertex-centric execution is not amortized by any gains in algorithm execution time.
Due to their irregular access patterns, graph algorithms usually exhibit poor cache locality.
Last-level cache (LLC) misses may happen during three key steps of the computation: fetching an edge, fetching the metadata associated with the source vertex of the edge, and fetching the metadata associated with the destination vertex of the edge.
In this section, we study how to lay out the data in memory to reduce the number of LLC misses, and we explain the pre-processing costs associated with creating those layouts.
Edge array.
In edge-centric computation, since edges are streamed, they are prefetched efficiently and do not incur cache misses.
Fetching the metadata of the vertices, however, leads to random accesses with poor spatial and temporal locality.
Adjacency lists.
In adjacency lists, computation is performed from the point of view of a vertex: a core iterates over all edges of a given vertex before processing another vertex.
As a consequence, the metadata of the source vertex is read only once, after which it is cached.
This is beneficial for vertices that have a large number of edges.
Fetching edges may introduce a cache miss for the first edge, but subsequent edges are prefetched, as with the edge array.
Also similar to the case of the edge array, the metadata of the destination vertices exhibits poor cache behavior.Grids: optimizing edge arrays.
To improve the cache locality of edge arrays, data is laid-out as a grid of cells.
Each cell contains the edges from a range of vertices to another range of vertices.
Figure 4 shows an example of a graph transformed into a grid.
This data structure is inspired by the grid data structure first introduced in GridGraph [37], which aimed at maximizing reuse of data read from disks.
Computation then iterates over cells.
The goal is that the metadata associated with the vertices in the cell stays in cache and can therefore be reused.
We construct the grid using the same radix sort approach as for building adjacency lists.
Instead of bucketing edges by source vertex, we bucket them by the cell to which they belong.
The optimal number of cells in the grid depends on the graph shape and size.
We experimentally find that a grid of 256x256 cells performs best on the Twitter and RMAT26 graphs.
Building a grid is slightly more expensive than building an adjacency list (the number of cells in the grid is equal to (#vertices/256) 2 , which is higher than the number of vertices for large graphs).
We compare using radix sort with a dynamic approach for buiding the grid, and the conclusions regarding different pre-processing approaches made in Section 3.
applicable to grids as well: radix sort is faster when the graph is in memory or loaded from a fast medium, while dynamically building the grid is faster otherwise.Optimizing adjacency lists.
An intuitive idea to improve cache locality in adjacency lists is to sort the pervertex edge arrays by destination.
Indeed, the metadata of vertices with contiguous IDs is also contiguous in memory, thus when accessing vertex 0 and then vertex 1, the metadata of vertex 1 is likely to be present in cache.
Of course, sorting the per-vertex edge arrays increases the pre-processing cost.
Figure 5 compares the pre-processing and algorithm execution times of BFS and Pagerank on RMAT26, on the unsorted adjacency list, the sorted adjacency list, the edge array and the grid.
Table 4 presents the cache miss rate for these four data layouts.
BFS.
For BFS, the unsorted adjacency list remains the solution with the best end-to-end execution time.
Looking at algorithm execution time alone, BFS is 2.4× faster with a grid than with unsorted per-vertex edge arrays.
However, creating the grid adds significant preprocessing time (9s), making the grid the slowest solution overall for BFS.
Sorting the per-vertex edge arrays also leads to end-to-end performance inferior to unsorted adjacency lists.
The pre-processing time increases, and the algorithm execution time does not decrease.
Table 4 shows that sorting the per-vertex arrays does not significantly impact the cache miss rate.
The destination vertices are accessed in order, but in practice a cache line only contains the metadata associated with very few vertices (64 in the case of BFS).
Even when sorted, the destination vertex identifiers in the per-vertex edge arrays are sufficiently far apart for their metadata to fall in different cache lines, which explains the limited impact of this optimization on the number of cache misses and therefore on algorithm execution time.
The increased pre-processing time for sorting the per-vertex arrays increases end-to-end execution time.
Pagerank.
Even with the added pre-processing cost, the grid outperforms all other data layouts for Pagerank: it is 1.4× faster than an edge array and 1.3× faster than an unsorted adjacency list.
This improvement is a direct result of the reduced cache miss rate when using a grid.
As shown in Table 4, the cache miss ratio for the grid is less than half of that for the other data layouts.
As for BFS, sorting the per-vertex edge arrays provides no benefit for Pagerank, for the same reasons.
A cache line can fit at most 6 vertices for Pagerank, leading to an even smaller improvement in spatial locality than for BFS.Summary.
Creating a grid improves cache reuse and has a significant impact on algorithm execution time.
Yet, this comes at the cost of an extra pre-processing, which is not always amortized.
Different layouts also shine in very different situations.
For instance, the grid is the best solution for Pagerank, but the slowest on BFS.
One of the core design decisions for a graph processing system is the information flow model it adopts.
Information propagates through the graph in one of two ways: a vertex either pushes data along its out edges, writing to the state of its neighbors, or it pulls data along its incoming edges and updates its own state.
These two approaches have important implications on computation, synchronization and pre-processing that we detail in this section.6.1 Impact on end-to-end execution time 6.1 The push and pull approaches have different impact on the number of vertices and edges that need to be accessed during an iteration.First, the push approach allows working on a subset of the vertices, while the pull approach does not.
When pushing, vertices that do not need to propagate their value can be safely ignored.
In contrast, the pull approach requires a vertex to scan all its incoming edges for neighbors that could potentially propagate a value.
It also requires a pass over all vertices to check whether they need to look at their incoming edges (e.g., whether they have already been discovered in BFS).
Second, for some algorithms, the pull approach allows stopping the computation for a vertex in the middle of an iteration, while the push approach does not.
Indeed, while pulling data a vertex may stop pulling before exploring all its incoming edges.
For instance in BFS, if a vertex marks itself as discovered in the middle of an iteration, it stops exploring its remaining incoming edges.
This guarantees that the vertex is discovered only once.
In the push approach, vertices need to check that all their neighbors have been discovered, which leads to redundant work if multiple vertices have the same neighbors.
Figure 6 shows the per-iteration execution time of pushing vs. pulling for BFS on an RMAT26 graph.
During the first iteration and after the third iteration, pushing is faster than pulling.
During iterations 2 and 3, pulling is faster than pushing.
This difference is explained by the percentage of the graph that is accessed during the iterations: most vertices in the graph are discovered during iterations 2 and 3.
When pushing data, lots of redundant work is done in these iterations.Because pushing data and pulling data perform best at different phases of the computation, some frameworks dynamically switch between pushing and pulling, depending on the number of active vertices in an iteration [2,3,29].
A significant part of the algorithm execution time may involve synchronization.
For example, in Pagerank on an RMAT26 graph with 16 cores, 40% of the algorithm execution time is spent in code protected by locks.
The goal of this section is to evaluate the possibilities for lock removal, how they depend on the data layout and the information flow, and what if any pre-processing costs they induce.In push mode, a vertex pushes updates to all its neighbors, and thus needs to lock them to update their metadata.
In pull mode, a vertex only updates its own state.
Thus, lock removal with adjacency lists requires execution in pull mode.The grid offers a natural partition of the graph: edges in different rows have different source vertices, and edges in different columns have different destination vertices.
To perform computation without locks in push mode, it suffices to assign different columns to different cores.
To perform computation without locks in pull mode, it suffices to assign different rows to different cores.
Adjacency lists.
To use push-pull, a system needs to iterate over both outgoing and incoming edges.
As a result, when the graph is directed, we need to build both the outgoing and incoming per-vertex edge arrays.
In contrast, for push we only need to build the outgoing, and for pull only the incoming per-vertex edge arrays.
As a result, for directed graphs push-pull comes with an increased pre-processing cost, compared to push or pull, as seen in Section 3.2.
When the graph is undirected, it suffices to build the outgoing per-vertex edge arrays (outgoing and incoming edges are the same), and push-pull induces no extra pre-processing cost.Edge array.
Computation over an edge array always requires scanning all the edges in the graph, so there is no advantage to using either push or pull.
Furthermore, since the computation is edge-centric and not vertexcentric, locks need to be acquired for all updates.
For these reasons, edge arrays are not considered any further in this section.Lock removal.
Lock removal does not require any additional pre-processing, beyond what is otherwise necessary for adjacency lists and grids, but it cannot be used with edge arrays, which have zero pre-processing cost.
Figure 7 presents the end-to-end execution times for BFS running on a directed RMAT26 graph, with adjacency lists, using push-pull, push (with locks) and pull (without locks).
We do not show any results for edge array or grid for BFS, as we have shown in Section 5 that these approaches lead to inferior results compared to adjacency lists.
Push-pull is much faster in terms of algorithm execution time, but it is 1.5× slower than the push approach in terms of end-to-end execution time because of the extra pre-processing time.
When taking pre-processing time into account, we find no combination of graphs, algorithms and machines in which push-pull is beneficial on directed graphs.
On undirected graphs, push-pull does not add any pre-processing time, and is thus much faster than just pulling or pushing data.
Furthermore, due to the fact that, on average, only a small percentage of vertices is processed per iteration, BFS in push mode performs 20% better than BFS in pull mode, even though push uses locks and pull does not.
BFS on RMAT26 using push-pull, push (with locks) and pull (without locks).
Figure 8 shows the end-to-end execution times for Pagerank in push mode on an adjacency list (with locks), in pull mode on an adjacency list (without locks), in push mode on a grid (with locks), and in pull mode on a grid (without locks).
Here, the advantages of removing locks can be clearly seen.
On adjacency lists, the version without locks is 40% faster than the push version when looking at end-to-end time.
On a grid, the version without locks shows a gain of 1.5× in end-to-end time when comparing to the version with locks.
Summary.
Push and pull on adjacency lists have conflicting benefits.
Push works better for algorithms that only access a subset of the vertices in a given iteration, while pull allows vertices to be updated without locks.
With grids, locking can be avoided regardless of whether push or pull is used, but the advantage of push remains for algorithms that only access a subset of the vertices.
Whether push or pull comes out ahead depends heavily on the nature of the algorithm.
A combined push-pull approach requires extra pre-processing, which outweighs the benefits in terms of algorithm execution time.
We evaluate the trade-offs between the potential benefits of being NUMA-aware and the overheads it introduces in both the pre-processing and algorithm execution phase.
In NUMA-aware solutions, the graph is partitioned across the NUMA nodes, and threads prioritize work from partitions that are local to their NUMA node.
The partitioning scheme divides graph data evenly across NUMA nodes and places related data on the same NUMA node.
Partitioning is performed so as to minimize the number of edges whose source and destination vertices are on different NUMA nodes, while still balancing the number of vertices and edges per NUMA node.
We evaluate in particular the partitioning schemes of Polymer [33] and Gemini [36].
The vertices are split into as many subsets as there are NUMA nodes.
The outgoing edges of vertices are colocated with their target vertices.
This approach avoids random remote writes and balances the number of edges across NUMA-nodes.
Threads first process their local partitions.
After that, they start working on remote partitions by updating the target vertices that are local to their NUMA node.
We evaluate the potential performance improvement of NUMA-aware data placement on the two machines presented in Section 2.
Figure 9 shows the impact of NUMA-aware graph partitioning of an RMAT26 graph when running BFS and Pagerank.
We compare NUMA partitioning to a solution that randomly interleaves the graph data on all NUMA nodes.
We use, for each application, the best algorithm in terms of algorithm execution, as presented in the previous sections (push/pull for BFS and pull without locks for Pagerank).
The end-toend execution time is broken down into pre-processing, partitioning and algorithm execution.Looking at Figure 9b, the NUMA-aware data layout improves the algorithm execution time for Pagerank 1.3× on Machine A and 2× on Machine B. However, only on the machine B, with 4 NUMA nodes, does the end-to-end execution time benefit from being NUMAaware.In contrast, looking at Figure 9a, for BFS the NUMAaware version is 3.5× slower on Machine A and 1.8× slower on Machine B. For BFS the time spent in partitioning dwarfs the algorithm execution time on both machines.
More surprisingly, even when looking only at algorithm execution time, the NUMA-aware version performs worse than the interleaved version.
In BFS, in a given iteration, only a small number of vertices is processed, and these vertices often share a common ancestor (e.g., during the first iteration, all processed vertices are the children of the root vertex).
As a consequence, vertices processed during a given iteration often reside in the same partition.
This leads to all cores accessing the same NUMA node, which creates memory contention [9].
This undesirable effect is even more visible on high-diameter graphs with low-degree vertices, as shown in Figure 10 when running BFS on the US-Road graph.
The NUMA-aware version is 12× slower than the interleaved version.
Summary.
NUMA-aware data partitioning has a high pre-processing cost.
This cost is amortized for algorithms that run for a long time and that work on most of the data during every iteration.
For algorithms that run only for a short time, this may not be the case.
For algorithms that only work on a subset of the data, NUMAaware partitioning may exacerbate memory contention.
Table 5 shows the best solutions for BFS and Pagerank for graphs not evaluated in previous sections.
The Twitter graph has a degree distribution similar to that of RMAT, and benefits from the same approaches: using an adjacency list while pushing data for BFS, and using a grid for Pagerank.
The US-Road graph leads to slightly different conclusions.
The best approach on Pagerank is to use an edge array and not a grid.
Since the graph has a lower per-vertex degree than the RMAT and Twitter graphs, the grid data structure reduces only slightly the cache miss ratio, and therefore its pre-processing cost is not amortized.
In Table 6 we report the best approaches for WCC, SpMV, SSSP and ALS, their end-to-end execution time and its breakdown over pre-processing and algorithm execution time.SPMV is a very short algorithm, and edge arrays are always the fastest approach, as they induce no preprocessing cost.Intuitively, WCC should perform best on adjacency lists, because it is a traversal algorithm (only a subset of the graph is processed during every iteration of the computation), but WCC runs on an undirected graph.
We therefore first have to build an undirected version of the graph from the input file.
In the case of adjacency lists, an edge has to be inserted in both the outgoing edge array of its source and its destination.
Thus, the pre-processing cost for creating adjacency lists is increased.
In contrast, no additional pre-processing is required for edge arrays and grids to perform computation on an undirected graph.
As a consequence, on graphs with a low diameter, WCC works best with an edge array, because the preprocessing time of building adjacency lists is too high.
On graphs that have a higher diameter, like the US-Road graph, WCC needs more iterations to converge, and an adjacency list works best.SSSP is very similar to BFS, and previous conclusions regarding the trade-offs between algorithm execution time and pre-processing for BFS are applicable to this algorithm as well.
The only difference is that BFS discovers a vertex only once, whereas in SSSP a vertex may update its path many times during the computation, leading to an increase both in the number of iterations and the number of vertices active in each iteration.ALS computes recommendations from a bipartite graph.
The left side of the graph represents users and the other side items being rated.
During every iteration, a subset of the graph (the left or right side) is active, and hence adjacency lists are the best data layout.
Improvements in algorithm execution time often come at the cost of increased pre-processing time.
As seen in the previous sections, no approach fits every graph, algorithm or machine.
In this section we try to provide a roadmap for choosing between different data layouts and computation approaches.The first step consists of choosing an appropriate data layout.
The layout is chosen based on the algorithm and graph characteristics.
Short algorithms, such as SPMV, that complete in one iteration, should use an edge array, as it incurs no pre-processing cost.
When the computation works only on a small subset of the graph at every computation step, adjacency lists in push mode improve algorithm execution time.
The cost of building them is usually amortized compared to computation over edge arrays, especially on graphs with a high diameter.
Other algorithms that run on graphs that have a large average per-vertex degree and iterate over most of the graph at every iteration, may benefit from using a grid, because the grid improves cache locality.Second, if the machine is a large NUMA machine and the algorithm execution time is predicted to be large, then partitioning the graph to be NUMA-aware is beneficial (Figure 9b).
Third, if the data layout and computation approach chosen during the first step allow for execution without locking (e.g., pull mode in grids), then it is always beneficial to remove locks.
We do not find any algorithm or directed graph for which switching between a pull mode without locks and push mode is beneficial when looking at end-to-end execution time.Finally, when pre-processing cannot be avoided, it induces a non-negligible cost, and it should be optimized by using appropriate sorting techniques.
Very few papers compare the benefits of different graph processing systems.
Satish et al. [28] evaluate various single-machine and distributed systems and compare them to a hand-optimized baseline.
The paper looks at complete systems rather than individual techniques.
Capota et al. [4] introduce a benchmark for graph processing platforms.
A large number of graph processing systems have been proposed [7, 8, 12-17, 19, 20, 22, 23, 25-27, 29, 31, 33, 36, 37].
We cover here only those works that have directly inspired the techniques evaluated in this paper.
For a brief summary of the main features of these systems, see Table 7.
Beamer et al. [2,3] are the first to propose push-pull for BFS.
Ligra [29] extends this idea to other graph algorithms.
It also uses radix sort for creating adjacency lists.
X-Stream [27] introduces edge-centric graph processing in the context of out-of-core systems.
GridGraph [37] improves on this idea by organizing the edges into a grid.
Polymer [33] and Gemini [36] optimize graph processing for NUMA machines.
We use their data placement technique in Section 7.
In addition to the techniques used in Polymer, Gemini adds work stealing to balance work across NUMA nodes.Not explored in this paper, the use of GPUs for graph processing has been the subject of some recent works [10,11,21,30,34].
This approach could affect the relative magnitude of pre-processing vs. algorithm execution time, and thereby impact the conclusions for certain algorithms.
We have presented an analysis of various techniques aimed at improving the algorithm execution time in graph processing systems, and we have explained their impact on pre-processing time.
Our main observation is that pre-processing often dominates the end-to-end execution time of graph analytics.
Therefore, it is often better to work with simple graph data layouts that induce less pre-processing than to invest time in elaborate pre-processing to speed up the algorithm execution phase.
We argue that future works on graph analytics frameworks must more carefully consider this trade-off between pre-processing and algorithm execution time.
