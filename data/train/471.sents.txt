Existing methods of measuring lifetimes in P2P systems usually rely on the so-called Create-Based Method (CBM) [16], which divides a given observation window into two halves and samples users "created" in the first half every ∆ time units until they die or the observation period ends.
Despite its frequent use [2], [17], [19], this approach has no rigorous accuracy or overhead analysis in the literature.
To shed more light on its performance, we first derive a model for CBM and show that small window size or large ∆ may lead to highly inaccurate lifetime distributions.
We then show that create-based sampling exhibits an inherent tradeoff between overhead and accuracy, which does not allow any fundamental improvement to the method.
Instead, we propose a completely different approach for sampling user dynamics that keeps track of only residual lifetimes of peers and uses a simple renewal-process model to recover the actual lifetimes from the observed residuals.
Our analysis indicates that for reasonably large systems, the proposed method can reduce bandwidth consumption by several orders of magnitude compared to prior approaches while simultaneously achieving higher accuracy.
We finish the paper by implementing a two-tier Gnutella network crawler equipped with the proposed sampling method and obtain the distribution of ultrapeer lifetimes in a network of 6.4 million users and 60 million links.
Our experimental results show that ultrapeer lifetimes are Pareto with shape α ≈ 1.1; however, link lifetimes exhibit much lighter tails with α ≈ 1.9.
Peer-to-peer networks are popular platforms for many applications such as file-sharing, content distribution, and multimedia streaming.
Besides modeling and simulating system dynamics of P2P networks under churn (e.g., [3], [6], [8], [11]), validation of proposed techniques in real networks has recently become an important area for understanding P2P performance and design limitations in practice.
In this regard, several efforts have been undertaken to characterize peer-topeer systems by measuring churn-related user behavior (e.g., distribution of lifetime, inter-arrival delays, and availability) [1], [2], [4], [5], [17], [19], topological information (e.g., degree distribution and clustering coefficients) [13], [21], and traffic flow rate [8], [18].
Sampling of large-scale networks usually faces two fundamental problems -1) obtaining an unbiased distribution of the target quantity and 2) keeping bandwidth overhead reasonable as system size increases.
While sampling bias in topology measurement is understood fairly well [20], the same issue in lifetime sampling has not been addressed before.
What makes the latter problem different is that sampled users * Supported by NSF grants CCR-0306246, ANI-0312461, CNS-0434940, and CNS-0519442.
cannot be queried for their lifetimes or even arrival instances.
Measurement in such cases generally requires taking repeated snapshots of the system every ∆ time units, detecting new arrivals by user appearance in a given snapshot, and inferring departures based on user absence in another snapshot.
Since ∆ cannot be lowered below the delay it takes to crawl the network, the issue of precisely reconstructing the lifetime distribution from measured samples remains open.In this paper, we aim to formalize the notion of lifetime sampling bias, understand its source in existing methods, and design a robust and bandwidth-efficient sampling mechanism for estimating peer and link lifetime distributions in unstructured P2P networks (e.g., Gnutella [7], KaZaA [9]).
Note that peer lifetimes are important for understanding general user behavior, their habits, and application performance offered by the peers to the system.
Link lifetimes, on the other hand, have a significant impact on resilience [11], [22] and routing ability [10] of the network since broken links, rather than dead peers, contribute to formation of stale neighbor pointers, network disconnection, and routing failure.
1 We start by creating a novel analytical framework for understanding and characterizing bias in network sampling.
We first explain what constitutes inaccuracy in measuring the target distribution of lifetimes F (x) and define sampling methods to be biased if, given an infinite population of sampled users, they cannot reproduce F (x) in all discrete points j∆ in the interval [∆, T ].
Armed with this definition, we then offer a closed-form model for the measurements obtained by CreateBased Method (CBM) [16], which is a widely used heuristic for sampling lifetimes in computer systems.
We show that both CBM and its modification in [2], [17], [19] are biased as long as ∆ > 0, where the bias is caused by two factorsinconsistent round-offs (i.e., some user lifetimes are rounded up and others down) and missed users (i.e., users arrive and depart within a ∆ interval).
In fact, we generalize this result to show that any sampling technique that attempts to directly measure user lifetimes every ∆ time units is biased as long as ∆ > 0 and that the bias is not removable regardless of the mathematical manipulation applied to the measured samples.To overcome the discovered limitations of direct sampling, we next propose a technique called ResIDual-based Estimator (RIDE), in which a crawler takes a snapshot of the entire network and then tracks the residual (i.e., remaining) lifetimes of the users seen in the first crawl.
We show that this approach produces an unbiased version of the residual distribution H(x), which allows us to develop a simple mechanism based on renewal churn models of [11], [22] that accurately reconstructs the lifetime distribution F (x) from the sampled residuals with a negligible amount of error.The next issue we address is bandwidth consumption.
With small ∆ and large T , CBM requires significant overhead since it must track all users that appear in the system in the observation interval, i.e., old peers discovered early in the crawl and new ones constantly arriving into the system.
In RIDE, however, initial users die quickly and the amount of bandwidth needed to sustain the crawl decays to zero proportionally to the tail of the residual lifetime distribution H(x).
Additional bandwidth savings are possible if the initial set S 0 of users found in the system is uniformly subsampled and only -fraction of the users is monitored during the interval [0, T ].
For example, given Pareto lifetimes with α = 1.1 observed in our experiments, window T = 24 hours, and sampling interval ∆ = 3 minutes, the proposed technique reduces the download overhead compared to that in CBM by a factor of 16 for = 0.1 and a factor of 125 for = 0.01.
We finish the paper by implementing a Gnutella crawler that is about 18 times faster than the fastest prior crawler [19], which allows it to cover the entire network of 6.4 million users (1.2 million contacted ultrapeers) in under 3 minutes.
Our results using RIDE indicate that ultrapeer lifetimes are Pareto distributed with shape α ≈ 1.1, which is very close to the results of [2].
At the same time, Gnutella links are much more volatile and can be described by a Pareto distribution with shape α ≈ 1.9.
These results, fed into the latest resilience models for unstructured systems [11], [12], [22], suggest that node isolation among joining ultrapeers in Gnutella and thus partitioning of the network must indeed be extremely rare events.The remainder of the paper is organized as follows.
In Section II, we formalize sampling and bias.
In Section III, we derive the sampling bias of CBM and examine it under different simulation settings.
We propose the residual-based method and discuss its simulation results in Section IV.
We examine the bandwidth overhead of the various methods in Section V and present our measurement study of Gnutella in Section VI.
Section VII reviews prior work and Section VIII concludes the paper.
We start by defining the objective of our measurement process.
Assume that each user spends a random amount of time in the system, where the lifetime L of the next joining user is drawn from some distribution F (x).
This is similar to the heterogeneous churn model proposed in [22].
Then, the goal of the sampling process is to estimate with as much accuracy as possible function F (x), which we assume is continuous almost everywhere 2 in the interval (0, ∞).
As shown in [22], distribution F (x) represents the lifetimes of arriving rather than existing peers in the system.
The latter metric is known in renewal process theory as the spread of user lifetimes and can be obtained from F (x) using simple integration.The measurement process is assumed to have periodic access to the information about which users are currently present in the system.
This process allows the sampler to test whether a given user i is still alive as well as discover the entire population of the system at any time t. However, due to bandwidth and connection-delay constraints on obtaining this information, the sampling process cannot query the system for longer than T or more frequently than once per ∆ time units, where ∆ usually varies from several minutes to several hours depending on the speed of the crawler and network size.Given the above requirements, notice that reconstructing the entire F (x) from discrete samples is simply impossible.
There are three biases arising from discrete sampling: 1) the measuring process cannot observe any lifetimes larger than T ; 2) all samples are rounded to a multiple of ∆; 3) an empirical distribution based on a finite sample size will not necessarily match the theoretical one.
We are not concerned with the last issue since all methods require an infinitely large sample size to converge to the desired distribution F (x).
Instead, we are interested in the bias arising from finite T and non-zero ∆.
We start with the following definition that formalizes samples obtained during periodic measurements.Definition 1: A non-negative random variable X ∆ for some ∆ > 0 is called lattice if:∞ j=1 P (X ∆ = j∆) = 1,(1)where ∆ is called the periodicity of X ∆ and points x j = j∆ are called the support of X ∆ .
For all lattice distributions, we assume that P (X ∆ ≤ 0) = F (0) = 0 and that the probability mass of X ∆ starts from the point x 1 = ∆.
We are now ready to define a sampling process.
Definition 2: A (∆, T )-sampling process is a lattice random variable M ∆ with periodicity ∆ and P (∆ ≤ M ∆ ≤ T ) = 1.
Note that the above defines a sampling process using the limiting distribution of the values it measures (i.e., assuming an infinite population size).
The reason for doing so is to understand whether a method can provide accurate results given a sufficiently large sampling size.
As we show below, some methods always exhibit bias, no matter how long they measure the system.Definition 3: For a random variable X, function E(x) is called an estimator of X in some interval [a, b] if it is the CDF of some random variable Y that approximates X in [a, b].
Note that Y can be arbitrarily dissimilar to X, in which case the estimator will be biased.
We next explain what makes an estimator unbiased.Definition 4: A (∆, T )-sampling process with estimator E(x) is unbiased with respect to a target continuous random variable X if it can correctly reproduce the distribution of X in all discrete points x j in the interval [∆, T ] for any ∆ > 0:E(x j ) = P (X ≤ x j )(2)for x j = j∆ and j = 1, 2, . . . , T /∆.
Since one may measure different aspects of the system, we finally classify sampling methods based on whether they measures the target random variable or some other related distribution.Definition 5: A (∆, T )-sampling process of a random variable X is called direct, if it measures quantities whose distribution is the same as that of X.
It is called indirect otherwise.For example, direct lifetime sampling must measure session lengths of all arriving users, while indirect sampling may record the lifetimes of peers alive in the system at some time t. Given an established relationship between the two metrics, an estimator can then be used to reconstruct lifetimes L from indirect samples.
In another example, direct sampling of network size must count the number of users present in the system at different times t, while indirect sampling may measure the arrival process of peers into the system.
With proper selection of the indirect sampling method, estimation may be more accurate and/or may require lower overhead than direct sampling.
We demonstrate one such example later in the paper.
In this section, we first examine the source of bias in direct sampling and study the problem of constructing an unbiased estimator for measuring lifetimes.
We then derive a model for the distribution obtained by Create-Based Method (CBM) and demonstrate examples of its bias.
In direct sampling, the measured random variable M ∆ is the lifetime of individual users conditioned on them being smaller than T and being present in the crawl:P (M ∆ ≤ x) = P (L ≤ x|L ≤ T, not missed),(3)where missed samples arise when a user joins and departs between consequent crawls.
Note, however, that not all users with lifetimes smaller than ∆ are missed and that some of them are actually taken into account in the distribution of M ∆ .
Another issue that we discover in this work is that some lifetime samples are rounded up and others rounded down during the measurement, which together with missed users gives rise to the bias we derive below.
We next formalize round-off errors and explain how they affect direct sampling.
Definition 6: For a continuous random variable X, a (∆, T ) sampling process is consistent if measured samples are all rounded up to the nearest multiple of ∆.
Since a crawler in direct sampling never knows the exact arrival time of users it observes, there is an ambiguity in how to round-off the lifetimes of measured peers.
Consider the example in Fig. 1, where sample L 1 = 0.5∆ is indistinguishable from sample L 2 = 1.8∆ from the perspective of the crawler.
This causes both of these lifetimes to be rounded off to ∆, which using our terminology makes L 1 consistent and L 2 inconsistent.
Also observe in the figure that samples L 3 = 0.4∆ and L 4 = 0.6∆ are completely missed by the crawler, even though sample L 1 is captured.
This case can also be treated as inconsistent round-off as we define below.LetQ j = 1 inconsistently rounded down to x j 0 otherwiseto be an indicator variable of the event that a user's lifetime x j ≤ L < x j+1 is inconsistently rounded down to x j by the sampling process, where rounding down to x 0 = 0 represents missing the entire sample.
For simplicity of notation, we define ρ j = P (Q j = 1) and obtain the probability of inconsistent round-off in the interval [x j , x j+1 ) in the next theorem.
Theorem 1: In direct sampling, the probability that lifetime samples are inconsistently rounded down tox j = j∆ (j = 0, 1, ..., T /∆) is: ρ j = 1 ∆ xj+1 x j F (x)dx − F (x j ),(4)where F (x) is the CDF of the lifetime distribution of samples.
Equipped with result in (4), we next derive an unbiased estimator for the continuous random variable L.Theorem 2: For direct lifetime sampling, the following is an unbiased estimator of L:E(x j ) = P (M ∆ ≤ x j )P (L ≤ T |Q 0 = 0)(1 − ρ 0 ) + ρ 0 − ρ j ,(5)where ρ j is given in (4).
Note that both (4)-(5) are exact, but due to limited space we do not show simulations verifying their accuracy.
From the result of Theorem 2, it becomes clear that unbiased measurement requires access to the distribution of samples (i.e., variable M ∆ ), the fraction of observed lifetimes that are no larger than T (i.e., P (L ≤ T |Q 0 = 0)), and all individual ρ j .
While the first two metrics are easily measurable in practice, recovery from inconsistent round-offs requires the exact join time of each sampled user and the number of missed users.
Unfortunately, within the constraints of our problem (i.e., crawling of alive users with a period no less than ∆), the effect of round-off errors is impossible to overcome no matter what manipulation is applied to M ∆ .
We next study how inconsistent round-offs exhibit themselves in a widely used [2], [17], [19] [16] in the context of operating systems.
Recall from [16] that CBM uses an observation window of size 2T , which is split into small intervals of size ∆.
Within the observation window [0, 2T ], the algorithm takes a snapshot of the system at the beginning of each interval.
To avoid sampling bias, [16] suggests dividing the window into two halves and only including samples that appear during the first half of the window, disappear somewhere within the window, and stay in the system no longer than T time units.
Fig. 2 shows an example of create-based sampling with three valid, four invalid, and two missed lifetime samples.
The invalid cases include users who join the system before the observation window begins or in its second half, a peer that survives beyond time 2T , and a user whose lifetime is larger than T .
Based on the collected snapshots, the algorithm obtains individual lifetimes M ∆ and calculates the corresponding distribution P (M ∆ ≤ x j ) for j = 1, 2, ..., T /∆.
Assume that N is the number of users that arrive into the system in the first half of the window [0, T ] and N (x) is the number of such users with lifetimes less than or equal to x. Observe that N (T ) is the number of valid samples collected by CBM and lim N →∞ N (T )/N is the simply metric P (L ≤ T |Q 0 = 0) defined earlier.
One possible way to estimate F (x) is to take the ratio of N (x j ) to N (T ) as the estimator of the probability P (L ≤ x j ), which leads to our first CBM estimator [16]:E A (x j ) = lim N →∞ N (x j ) N (T ) = P (M ∆ ≤ x j ).
(6)Recent work in [2], [17], [19] normalizes E A by the percentage of samples no larger than T (i.e., N (T )/N ) and defines the following modified estimator:E B (x j ) = lim N →∞ N (x j ) N .
(7)With the result in (5), we can express both CBM estimators as functions of the actual distribution F (x j ) = P (L ≤ x j ).
Theorem 3: Both CBM estimators (6)- (7) are biased when any ρ j > 0 and produce the following distributions:E A (x j ) = F (x j ) − ρ 0 + ρ j F (T ) − ρ 0 , E B (x j ) = F (x j ) − ρ 0 + ρ j 1 − ρ 0 .
(8)The result in (8) shows that E B is closer to F (x) than E A since its accuracy is not affected by the value of T .
Next, we explore in more detail the effect of (∆, T ) on the fidelity of these estimators using model (8) and simulations.
We first explain how T and ∆ skew the shape of estimator E A .
To simplify the discussion below, define ¯ E(x) = 1−E(x) to be the tail distribution of any CDF function E(x).
It then follows from (8) that:¯ E A (x) = ¯ F (x) − ¯ F (T ) − ρ j F (T ) − ρ 0 ,(9)which shows that the measured tail distribution is a shifted and scaled version of the true tail.
The influence of the shift/scale factors on the right side of (9) could be illustrated using the example of a Pareto distribution commonly used to model user lifetimes [11], [22]:F (x) = 1 − (1 + x/β) −α , α > 1, x ≥ 0,(10)with E[L] = β/(α − 1).
We use CBM with T = 24 hours in a hypothetical network with n = 1 million users that join and depart using the churn model of [22].
Even though F (T ) = 99.8% of users have lifetimes smaller than T , Fig. 3 shows that E A suffers from significant bias that increases as ∆ becomes larger.
Not only does the measured distribution E A produce incorrect estimates α ≈ 2.4, β ≈ 0.5 of Pareto parameters when fitted with the corresponding curve, but the shape of the tail in Fig. 3 does not even resemble that of ¯ F (x), which may lead to erroneous conclusions about the family of distributions F (x) belongs to.We now study how ρ j affects the shape of E B .
It follows from (8) that for j = 0, 1, 2, ..., T /∆:¯ E B (x j ) = ¯ F (x j ) − ρ j 1 − ρ 0 ,(11)which is the true tail shifted by ρ j and then scaled by 1 − ρ 0 .
For small ρ j ≈ 0, this transformation on log scale preserves the Pareto shape parameter α as seen in Fig. 4, but makes scale parameter β inaccurate (i.e., α ≈ 1.14, β ≈ 0.15 for ∆ = 15 minutes).
For cases of non-negligible ρ j that arise when ∆ is very large or when distribution F (x) does not admit shape invariance during scaling (e.g., Gaussian, uniform), estimator E B may produce significantly misleading results.
In this section, we seek a solution to the problem of achieving both high accuracy and low overhead using indirect sampling.
It has been suggested [1], [11], [22] that users in peer-to-peer systems can be modeled as alternating between available (ON) and unavailable (OFF) states.
Inspired by these efforts, we now propose our measurement algorithm, called ResIDual-based Estimator (RIDE), that exploits renewal theory [14] to reconstruct F (x) from sampled residual lifetimes.
Consider a P2P system with n participating users, where each user i is either alive (i.e., present in the system) at time t or dead (i.e., logged off).
This behavior can be modeled by an ON/OFF process {Z i (t)} for each user i = 1, 2, ..., n:Z i (t) = 1 user i is alive at time t 0 otherwise .
(12)This framework is illustrated in Fig. 5, where X i and Y i are the durations of user i's ON (life) and OFF (death) periods, respectively, and R(t) is the remaining lifetime of user i at time t. Assume that {X i } are drawn from distribution F i (x) and {Y i } from G i (x).
It has been proven in [22] that the lifetime of the next joining user into the system is drawn from a weighted distribution:F (x) = ω n i=1 F i (x) E[X i ] + E[Y i ] ,(13)whereω = 1/ n l=1 (E[X l ] + E[Y l ]) −1 .
As before, define L to be the lifetime of the next joining peer whose distribution is specified by F (x).
Then, the goal of our and other measurement studies is not to sample each of F i (x), but rather to measure the users' aggregate behavior F (x) = P (L < x).
We first define the sampling algorithm in RIDE and then discuss its estimator E R (x).
At time 0, RIDE takes a snapshot of the whole system and records in set S 0 all users found to be alive.
For all subsequent intervals j (j = 1, 2, ..., T /∆) of ∆ time units, the algorithm keeps probing peers in set S 0 either until they die or T expires.
After the observation window is over, the algorithm obtains the distribution of residual lifetime M ∆ of the users in set S 0 .
Two important properties about residual sampling can be drawn from its definition: 1) no valid samples can be missed since only users who are alive at time t = 0 are valid measurements; 2) no samples can be inconsistently rounded off since all valid residual lifetimes start from the time of the first crawl.
Fig. 6 illustrates an example of five valid samples captured in the first crawl and five irrelevant lifetimes that are safely ignored by the algorithm.
Given the above observations, it immediately follows that the measured distribution can be used to obtain an unbiased estimator of the actual residual distribution:P (R(t) ≤ x j ) = lim |S0|→∞ N (x j ) |S 0 | ,(14)where as before N (x) denotes the number of samples in S 0 smaller than or equal to x.
At any time t 0, the distribution of the remaining lifetime R(t) of users present in the system is given by [22]:H(x) = P (R(t) ≤ x) = 1 µ x 0 (1 − F (u))du,(15)where µ = E[L] is the expected lifetime of a joining peer and system size n is sufficiently large for all Z i (t) to be in equilibrium.
This leads to the following result.
Theorem 4: For residual lifetime sampling, the following is an unbiased estimator of L:E R (x j ) = 1 − h(x j ) h(0) ,(16)where x j = j∆ and h(x j ) is the PDF of R(t).
Furthermore, the expected user lifetime is given by E[L] = 1/h(0).
Since H(x) is obtained without bias, it is now possible to numerically compute its derivative h(x) using Taylor expansion with error bounded by O(∆ k /k!)
, where k = T /∆ is the number of samples in the curve.
For ∆ = 3 minutes and T = 24 hours commonly used in our experiments, the resulting error is ∆ 480 /480!
≈ 10 −1960 , which for all practical purposes can be considered zero.
In simulations, however, we find that using only 3 points is often sufficient for achieving good estimation accuracy (see below).
It is worthwhile mentioning that residual sampling acquires all valid samples during the very first crawl.
Therefore, given that |S 0 | is sufficiently large, it is possible to randomly subsample the initial set of users and track the residuals of only percent of the entire user population.
This significantly reduces traffic requirements and allows RIDE to achieve orders of magnitude lower bandwidth overhead in practice compared to CBM.
One example of applying subsampling is shown in Fig. 7, where a system of 1 million users in the same setup as in Fig. 4(b) is subsampled by a factor of 100.
First notice in Fig. 7(a) that RIDE recovers F (x) with much higher accuracy than E B and obtains α = 1.099 and β = 0.049.
Second, observe in Fig. 7(b) that RIDE achieves reasonable estimation accuracy (α = 1.13, β = 0.087) even with just 10, 000 users; however, the tail of the subsampled distribution is highly variable, which potentially makes it difficult to understand the distribution's qualitative behavior.To overcome the tail noise arising when |S 0 | is heavily subsampled, we next present an algorithm for reducing the variance in the measured distribution E R (x).
Notice that E R (x) is a mapping between two discrete sets, i.e., from set X = {j∆} to set Y = {j/|S 0 |} for j = 1, 2, . . . , T /∆.
For each y ∈ Y, we find all x i ∈ X such that E R (x i ) = y and calculate the corresponding averagê x(y):ˆ x(y) = i x i 1 E R (xi)=y k 1 E R (xi)=y ,(17)where 1 A is the indicator function of event A. Denote byˆXbyˆ byˆX the set of all possiblê x(y) from (17), i.e., ˆ X = {ˆx{ˆx(y)|y ∈ Y} and define inverse averaging to be a relation (ˆ x(y), y) for all y ∈ Y. By smoothing out the tail, inverse averaging improves the shape of the distribution and allows better accuracy in estimation.Next, we examine two cases of inverse averaging using the example in Fig. 7(a) subsampled with = 0.1 and = 0.01.
The resulting distributions are shown in Fig. 8, which demonstrates much better preservation of the Pareto shape in the tail and less oscillations than without the use of inverse averaging.
For |S 0 | = 10 5 in Fig. 8(a), curve fitting produces α = 1.12, β = 0.067, and for |S 0 | = 10 4 in Fig. 8(b), we obtain α = 1.09, β = 0.079.
This shows that even when ∆ is comparable to the average lifetime E [L] and with very few samples, RIDE is capable of reasonably accurate estimation.
Due to limited space, we omit additional simulations that show insensitivity of RIDE to selection of ∆ and T , significant improvement in accuracy for less heavytailed cases, and convergence of E(x) to F (x) as |S 0 | → ∞.
This section formalizes the overhead of the various studied sampling methods and compares bandwidth requirement of RIDE to that of CBM.In CBM, we are interested in the expected overhead of crawling all users in the graph every ∆ time units in the interval [0, T ] and then probing peers that were present in the system at time t = T under they die or their lifetime exceeds T time units.
After some technical manipulation, we obtain the following result.Theorem 5: Total bandwidth overhead of (∆, T )-sampling using CBM is given by:b CBM = Cn ∆ T + T 0 [H(T ) − H(x)]dx ,(18)where n is the number of users in the system, C is the cost of probing or crawling a user, and H(x) is the CDF of residual lifetimes.Note that RIDE only probes users that are captured in the first crawl until they die or T expires.
Taking into account -subsampling, we have the following theorem.Theorem 6: Total bandwidth overhead of (∆, T )-sampling using RIDE is given by:b RIDE = C|S 0 | ∆ ∆ + T 0 [1 − H(x)]dx ,(19)where is the fraction of peers retained in the initial set S 0 .
As long as |S 0 | is sufficiently large, RIDE has the same accuracy as E R (x), but at significantly smaller overhead.
Define q() to be the ratio b CBM /b RIDE for |S 0 | = n and notice that for very small /∆, metric q() reduces to a simple formula T /∆, which for example is 480 for T = 24 hours and ∆ = 3 minutes.
Assuming Pareto lifetimes with shape α, Table I shows the exact savings gained by using residual subsampling.
The table shows that RIDE can reduce traffic overhead by a factor of 16 − 800 compared to CBM depending on the tail weight of F (x), sampling duration T , and subsampling factor .
In practice, one can choose based on the size of the initial set S 0 such that |S 0 | is fixed at some pre-determined value.
Simulations show that |S 0 | between 10 4 and 10 5 users allow accurate reconstruction of F (x) even when lifetimes are very heavy-tailed and ∆ is large.
Given this dynamic selection of , it becomes clear that RIDE can scale to arbitrarily large systems since it requires monitoring only a fixed number of users that does not depend on system size |S 0 |.
Note that similar subsampling is not possible in CBM since the latter requires full system crawls to discover new users.
In what follows in this section, we apply the residual-based algorithm to crawl the Gnutella network and estimate the distributions of peer/link lifetimes.
Recent Gnutella networks are implemented in a two-tier structure that contains ultrapeers and leaves.
Ultrapeers are responsible for forwarding search requests between each other, while leaves stay at the "edge" of the network and connect to several ultrapeers that provide them with search capabilities.
A recent extension to the Gnutella protocol provides a crawlerfriendly mechanism: upon receiving a crawl request (i.e., a handshake message with the "Crawler" field), a Gnutella client replies with a complete list of the identities of its neighboring peers.
According to our experiments, an ultrapeer on average connects to 28.5 ultrapeers and 25.7 leaves, while a leaf is usually attached to only one or two ultrapeers.To sample lifetimes of real Internet users using RIDE, we designed and built a scalable Gnutella crawler called GnuSpider that can operate in networks with millions of hosts and maintain reasonably small values of sampling period ∆.
As most other crawlers, GnuSpider starts the crawl using a default seed file of ultra-peers and then contact them to obtain their neighbor lists, which are then used in a BFS search to discover all currently alive ultra-peers in the system.
Neighbor lists in Gnutella include other ultra-peers with whom a connection is currently active, suggested ultra-peers who may or may not be online, and leaf peers currently attached as children.
The crawler records leaf peers for statistical purposes, but only contacts nodes found in the other two lists.Our GnuSpider implementation is a single-threaded Windows process that uses asynchronous completion ports (IOCP) to manage up to 60, 000 simultaneous connections to other Experiments with GnuSpider show that we can cover the entire Gnutella network in 3 minutes and typically discover close to 6.4 million users in the process (1.2 million of which are the ultra-peers that we attempt to contact and 5.2 million are leaf nodes).
During the first 120 seconds of the crawl, the discovery rate of new leaves shown in Figure 9(c) varies between 40, 000/second and 10, 000/second and that of new ultra-peers stays on average at 3, 000/second.
It can also be seen from the figures that the last 60 seconds of the crawl usually produce a very small number of new peers since most of these connections experience a timeout.
As illustrated in Figure 9(d), 90% of ultra-peers (i.e., 1.1 million) and leaf nodes (i.e., 4.5 million) can be discovered in just 100 seconds.Comparison of GnuSpider to crawlers in prior experimental P2P work is shown in Table II, which provides the sampling period ∆, window duration T , the number of peers periodically probed with SYN packets or discovered during an actual crawl, and the crawling speed in terms of contacted hosts per minute.
Observe in the table that GnuSpider is not only 18 times faster than the fastest crawler in prior literature [19], but it also discovers almost 5 times more concurrent users than any other study.
Users arriving into Gnutella immediately attempt to establish several neighboring connections to other peers currently in the system to increase their own resilience and enable themselves to route requests into the network.
However, since leaves and users behind firewalls do not generally accept connection requests, selection of neighbors is often limited to non-firewalled, or as we call them responsive, ultrapeers.
3 Therefore, measurement of responsive ultrapeers provides the most useful information about the lifetime of future neighbors acquired by arriving users and allows parameter selection for existing P2P models based on lifetime distributions [10], [11], [12], [22].
Thus, our experiments below focus only on lifetimes of ultrapeers that respond to our connection requests and the links associated with them.To measure peer lifetimes for the plots shown below, we first obtained using GnuSpider the initial set S 0 of about 468 thousand responsive ultrapeers and subsampled it using = 0.213.
Then, GnuSpider probed |S 0 | = 100, 000 users for T = 72 hours checking if each peer was alive every 3 minutes.
It should be noted that we found that in our experiments that a certain amount of peers exhibited erratic behavior, i.e., they would respond to one request, then become silent for several subsequent requests, and eventually become responsive again.
This phenomenon appeared when a peer either was too busy to reply or implemented a certain rate-limiting strategy.
To filter out the effect of this behavior, we set a threshold u for how many times a peer must appear unresponsive before we declare that user dead.
In the crawls below, we use u = 3.
After the observation window in GnuSpider had expired, an off-line program read the GnuSpider logs and applied RIDE to reconstruct F (x).
Fig. 10(a) shows the resulting inverseaveraged tail distribution 1 − E R (x) for the set of responsive ultrapeers.
The figure matches well with a Pareto distribution with α = 1.09 and β = 0.85, where the shape parameter is very close to that observed in [2].
Denote by r the expectation of residual lifetimes conditioned on the fact that R(t) is within the observation window T , i.e., r = E[R(t)|R(t) ≤ 72].
Crawl 3 The Gnutella protocol suggest that peers behind firewalls should not become an ultrapeer.
But in our measurement, about 5% of users behind firewalls act as an ultrapeer.
results show that r = 10.5 hours, but 5% of the peers in S 0 leave the system in just 8 minutes.We next proceed to compare the associated link lifetime distribution with that of peers in terms of α and r.
It is straightforward to apply the residual-based algorithm to measure the link lifetime distribution in Gnutella networks.
In the experiment of section VI-B, GnuCrawler kept track of the links of responsive ultrapeers found in S 0 and updated their status (i.e., connected or broken) in subsequent crawls.
Using this information, we applied the same processing program to extract link residuals from GnuSpider logs and perform the proposed recovery technique to obtain E R (x).
Fig. 10(b) shows that the resulting distribution of all link lifetimes is also power-law, but this time with α = 1.92, which is much larger than that in the peer lifetime distribution.
This observation establishes that the lifetime of a link is probabilistically smaller than that of a peer and one may expect more frequent changes in neighboring relationships.
We also find that r is 3.8 hours and 16.4% of links disappear within 8 minutes.Next, we treat the links between ultrapeers and leaves separately from those among ultrapeers and plot the corresponding distributions in Fig. 11.
Interestingly, the figure shows that the ultra-leaf links are slightly more stable (i.e., exhibit a heavier tail) than ultra-ultra links: the former has α = 1.88 and the latter has α = 1.99; the conditional expected lifetimes r of the two types of links are 3.9 and 3.5 hours, respectively.
This can be plausibly explained by the fact that a leaf is usually inactive in collecting information about alternate ultrapeers and is thus less likely to switch its attachment point.
With the experimental results of this section, we are now able to study resilience properties of Gnutella networks by applying models from [11], which use the average residual link lifetime and average node degree d as input parameters.
Given d = 28.5 neighbors observed in our experiments and a 1-minute failed-neighbor replacement delay, we obtain that the probability for the network to disconnect at the ultrapeer level is below 10 −64 .
However, leaves may be isolated with a non-negligible probability, because they only have one or two attachment points, i.e., d ≤ 2, which we plan to explicitly verify in future work.
Some of the first P2P sampling studies date to 2001 [15], [17] and the first use of CBM can be traced to Saroiu et al. [17] who sampled 17, 000 Gnutella peers every 7 minutes using TCP SYN packets over a period of 60 hours.
In a follow-up effort in [4], Chu et al. used a similar method, but probed 5, 000 peers every 7 minutes for 10 weeks.
Bhagwan et al. [1] improved over [4], [17] by implementing the Overnet protocol and probing a randomly chosen subset of peers in the system to measure their availability (i.e., the portion of time they were present online).
Their experiment selected 2, 400 out of around 90, 000 peers and kept probing them every 20 minutes for 7 days.
Liang et al. [13] measured lifetime distributions of links in the KaZaa network, but these experiments were limited to the connections passing through the authors' monitoring hosts.More related work can be found in [2] and [19].
Bustamante et al. [2] implemented a Gnutella sampler using 17 monitoring clients that periodically probed 500, 000 peers in the network every 21 minutes for 7 days.
In more recent work, Stutzbach et al. [19] developed a much faster crawler that in 2004 was able to cover the entire Gnutella network of 158, 000 ultrapeers within 7 minutes.
The closest approach to understanding sampling bias is another recent paper by Stutzbach et al. [20], which focused on capturing unbiased snapshots of joint properties of users currently alive in P2P systems using random walks.
In this paper, we showed that direct lifetime sampling suffered from estimation bias and did not admit any fundamental improvement besides reducing probing interval ∆.
To overcome this limitation, we proposed and analyzed a novel residual-based lifetime sampling algorithm, which measured lifetime distributions with high accuracy and required several orders of magnitude less bandwidth than the prior approaches.
Using this method, we sampled Gnutella users and discovered that lifetimes of peers and links exhibited power-law distributions, but with different shape parameters, where links are indeed much more volatile than actual peers.
