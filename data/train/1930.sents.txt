Troubleshooting network performance issues is a challenging task especially in large-scale data center networks.
This paper presents deTector, a network monitoring system that is able to detect and localize network failures (manifested mainly by packet losses) accurately in near real time while minimizing the monitoring overhead.
deTector achieves this goal by tightly coupling detection and localization and carefully selecting probe paths so that packet losses can be localized only according to end-to-end observations without the help of additional tools (e.g., tracert).
In particular, we quantify the desirable properties of the matrix of probe paths, i.e., coverage and identifiability, and leverage an efficient greedy algorithm with a good approximation ratio and fast speed to select probe paths.
We also propose a loss localization method according to loss patterns in a data center network.
Our algorithm analysis, experimental evaluation on a Fattree testbed and supplementary large-scale simulation validate the scalability, feasibility and effectiveness of deTector.
A variety of services are hosted in large-scale data centers today, e.g., search engines, social networks and file sharing.
To support these services with high quality, data center networks (DCNs) are carefully designed to efficiently connect thousands of network devices together, e.g., a 64-ary Fattree [9] DCN has more than 60,000 servers and 5,000 switches.
However, due to the large network scale, frequent upgrades and management complexity, failures in DCNs are the norm rather than the exception [21], such as routing misconfigurations, link flaps, etc.
Among these failures, those leading to userperceived performance issues (e.g., packet losses, latency spikes) are among the first priority to be detected and eliminated promptly [27,26,21], in order to maintain high quality of service (QoS) for users (e.g., no more than a few minutes of downtime per month [21]) and to increase revenue for operators.
Rapid failure recovery is not possible without a good network monitoring system.
There have been a number of systems proposed in the past few years [36,26,37,48].
Several limitations still exist in these systems that prohibit fast failure detection and localization.First, existing monitoring systems may fail to detect one type of failures or another.
Traditional passive monitoring approaches, such as querying the device counter via SNMP or retrieving information via device CLI when users have perceived some issues, can detect clean failures such as link down, line card malfunctions.
However, gray failures may occur, i.e., faults not detected or ignored by the device, or malfunctioning not properly reported by the device due to some bugs [37].
Active monitoring systems (e.g., Pingmesh [26], NetNORAD [37]) can detect such failures by sending end-to-end probes, but they may fail to capture failures that cause low rate losses, due to ECMP in data center ( §2).
Second, probe systems such as Pingmesh and NetNO-RAD inject probes between each pair of servers without selection, which may introduce too much bandwidth overhead.
In addition, they typically treat the whole DCN as a black box, and hence require many probes to cover all parallel paths between any server pair with high probability.Third, failures in the network can be reported in these active monitoring systems, but the exact failure locations cannot be pinpointed automatically.
The network operator typically learns a suspected source-destination server pair once packet loss happens.
Then she/he needs to resort to additional tools such as tracert to verify the issue and locate the faulty spot.
However, it may be difficult to play back the issues due to transient failures.
Hence this diagnosis approach (i.e., separation of detection and localization) may take several hours or even days to pinpoint the fault spot [21], yet ideally the failures should be repaired as fast as possible before users complain.A desirable monitoring system in a DCN should meet three objectives: exhaustive failure detection (i.e., detecting as many types of losses as possible), low overhead and real-time failure localization.
In this paper, we seek to investigate the following question: if we are aware of the network topology of a DCN, can we design a much better network monitoring system that achieves all these goals?
Our answer is deTector, a topology-aware network monitoring system that we design, implement and evaluate following the three design objectives.
The secret weapon of deTector is a carefully designed probe matrix ( §4), which achieves good link coverage, identifiability and evenness.
deTector is designed to detect and localize network failures manifested by user-perceptible performance problems such as packet losses and latency spikes in large-scale data centers.
We mainly focus on packet loss in this paper, but deTector can also handle latency issues by treating a round trip time (RTT) larger than a threshold as a packet loss.
Throughout the paper, we use "failure localization", "fault localization" and "loss localization" interchangeably.
Specifically, we make the following contributions in developing deTector.񮽙
As compared to the existing active monitoring systems adopting end-to-end probes (e.g., Pingmesh [26], NetNORAD [37]), we treat each switch instead of the whole network as a blackbox, i.e., our system requires the knowledge of the network topology and routing protocols in a DCN (i.e., topology-aware) and we use source routing to control the probing path.
In order to achieve real-time failure localization, we couple detection and localization closely and only rely on end-to-end measurements to localize failures without the help of other tools (e.g., fbtracert [3]).
To make it possible, we quantify several desirable properties of probe matrix (e.g., identifiability) and propose a greedy algorithm to minimize probe cost.
To address the scalability issue in DCNs, we apply several optimization heuristics and exploit characteristics of the DCN topology to accelerate path computation ( §4).񮽙
We modify a failure localization algorithm based on packet loss characteristics in large-scale data centers.
Compared to the existing algorithms, our algorithm runs within seconds and achieves higher accuracy and lower false positive rate ( §5).񮽙
We implement and evaluate our system on a 4-ary Fattree testbed built with 20 switches.
The experiments show that deTector is practically deployable and can accurately localize failures in near real time with less probe overhead, e.g., for 98% accuracy, deTector requires 3.9x and 1.9x times fewer probes than Pingmesh and NetNORAD while localizing failures 30 seconds in advance without the use of other loss localization tools.
Our supplementary simulation further shows that deTector achieves greater than 98% accuracy in failure localization with a less than 1% false positive ratio for most failures in large-scale DCNs ( §6).
We have open sourced deTector [6].
DCNs are usually multi-stage Clos networks with multiple paths between commodity servers for load balancing and fault tolerance [9,22,26,45].
Each DCN has its favorable routing protocols for path selection.
For example, in a Fattree topology [9] and a VL2 topology [22], the shortest paths between any two ToRs are typically used in practice [30].
We describe how existing monitoring systems fall short in achieving the three design objectives.
Table 1 shows detailed comparison among deTector and the existing systems.The passive approach stores packet statistics on switch counters, which are polled from SNMP or CLI periodically.
In Fig. 1, if link AB is down, the switch counters will show a lot of packet losses.
However, if the failure is a gray failure rather than link down, it may go undetected.
For example, when silent packet drops occur, the switch do not show any packet drop hints (e.g., syslog errors) due to various reasons (e.g., ASIC deficit), and hence SNMP data may not be fully trustworthy [26].
Furthermore, switches counters can be noisy, such that problems identified by this approach may or may not lead to end-to-end delay or loss perceived by users.Pingmesh and NetNORAD adopt an end-to-end probing approach to measure network latency and packet loss.
Pingmesh selects probe paths by constructing two complete graphs within a DCN: one includes all servers under the same ToR switch (i.e., the switch in the edge layer in Fig. 1) and the other spans all ToR switches.
NetNORAD is similar to Pingmesh but places pingers in a few pods instead of all servers.
Their approaches simplify the design but bring quite significant overhead ( §6).
Although gray failures can be captured, it is difficult to detect failures causing low rate losses (e.g., 1%) of a link, when ECMP is adopted in the DCN: there are many paths be- Figure 1: A 4-radix Fattree topology: failure on link AB can be detected by sending probes from s1 to s3.
tween a pair of servers, low-rate losses on a particular link may not affect much the overall end-to-end loss rate between the two servers.
The exact location of losses cannot be pinpointed using Pingmesh or NetNORAD, since they do not know which paths the probes take (e.g., due to ECMP).
Therefore, other tools such as Netbouncer [4] and fbtracert [3] are needed, which send additional probes to play back the losses.
These post-alarm tools may fail to pinpoint transient failures, those caused by transient bit errors, nonatomic rule updates or network upgrade (e.g., a transient inconsistency between the link configuration and routing information [21]).
To pinpoint such failures, close coupling of detection and localization is required, so that losses are localized only according to detection data, instead of additional probes after detection alarms.
Such coupling further enables near real-time fault localization.
Controller.
The logical controller periodically constructs the probe matrix indicating the paths for sending probes (see §4 for details).
We mainly focus on failure localization on links inter-connecting switches, as the fault on a link connecting a server with a ToR switch can be easily identified as discussed in the next paragraph.
The probe matrix indicates paths between ToRs.
Since we do not rely on ToRs with ping capability, probes are sent by 2-4 selected servers (pingers) under each ToR.Pinger.
Each pinger receives the pinglist from the controller, which contains server targets, probe format and ping configuration ( §6.1).
The probe paths from a ToR switch to different destinations are distributed among pinglists of pingers under the ToR switch, with each path distributed to at least 2 pingers for fault tolerance.
In this way, in case that one pinger is down, other pingers in the same rack can still probe the paths, avoiding any large drop in link coverage.
To detect failure on links connecting servers and the respective ToRs, pingers are also responsible for probing other servers under the same ToR.
The number of probe paths for each pinger is no more than a hundred even for a large DCN ( §4.4).
The probe packets are sent over UDP.
Though TCP is used to carry most traffic in a DCN, the DCN does not differentiate TCP and UDP traffic (e.g., the forwarding behavior) in the vast majority of cases [37,26], and hence UDP probes can also manifest network performance.
When a pinger detects a probe loss, it confirms the loss pattern by sending two probe packets of the same content additionally.Responder.
The responder is a lightweight module running on all servers.
Upon receiving a probe packet, the responder echoes it back.
A responder does not retain any states and all probing results are logged by pingers.Diagnoser.
Each pinger records packet loss information and sends it to the diagnoser for loss localization.
These logs are saved into a database for real-time analysis and later queries.
The diagnoser runs the PLL algorithm ( §5) to pinpoint packet losses and estimates the loss rates of suspected links.For the controller and the diagnoser to be fault-tolerant and scalable, we can use existing solutions (e.g., Software Load-Balancer [41,26]).
deTector works in three steps in cycles: path computation, network probing and loss localization.Path computation.
At the beginning of each cycle, the controller reads the data center topology and server health from data center management service (e.g., [31]), and selects the minimal number of probe paths ( §4).
The controller then selects pingers in each ToR, constructs and dispatches the pinglists to them.Network probing.
Next, probe packets are sent along the specified paths across the DCN.
Since data center usually adopts ECMP for load balancing, we have to use source routing to control the path traveled by each probe packet, which can be implemented using various methods.
1 A general and feasible solution is to employ packet encapsulation and decapsulation to create end-toend tunnels, though it may cause encapsulating packets twice in virtualized networks created by VXLAN [1] or NVGRE [2].
Take the Fattree network in Fig. 1 as an example: fixing a core switch, there is only one path between two inter-pod servers; we can use IP-in-IP encapsulation to wrap the probe on a server; after the packet arrives at the core switch, the outer header is removed and the packet is routed to the real destination.
Such a source routing mechanism incurs little overhead on servers and core switches.Loss localization.
The probe loss measurements are aggregated and analyzed by our loss localization algorithm ( §5) on the diagnoser.
We pinpoint the faulty links, estimate the loss rates, and send alerts to the network operator for further action (e.g., examining switch logs).
The main limitation of existing monitoring systems is that the probe path selection is far from optimum, such that not enough useful information can be collected and additional probes are needed to reproduce losses for localization.
In this section, we elaborate how we carefully select probe paths to overcome such a limitation.
Consider a data center network graph G = (V, E), where V is the set of switches and E is the set of links.
R is the m × n routing matrix defined byR i, j = 񮽙 1 if link j is on path i 0 otherwisewhere m is the number of paths and n = |E| is the number of links.
The possible paths and the routing matrix are decided by the routing protocols employed in the data center, e.g., ECMP is typically used to exploit k 2 /4 parallel paths between any two ToRs in a k-ary Fattree.
Fig. 3 gives a routing matrix R with 3 paths and 3 links.
Note that each link in a DCN is typically bi-directional.
Once we select a path from server s1 to server s2 and send a probe, the reverse path from s2 to s1 is automatically selected, since the response packet can probe faults along the reverse direction.
When we identify that link AB has failed, it implies that the failure may lie in either direction of the link, switch A, or switch B.R = ⎛ ⎝ l 1 l 2 l 3 p 1 1 1 0 p 2 1 0 1 p 3 0 0 1 ⎞ ⎠ → R' = ⎛ ⎝ l 1 l 2 l 3 l 12 l 13 l 23 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 ⎞ ⎠ Problem 1 Given a DCN routing matrix R, select a set of paths to construct a probe matrix P, such that P simultaneously (1) minimizes the number of paths, and achieves (2) α-coverage and (3) β -identifiability.Minimizing the number of probe paths is desirable for minimizing network bandwidth consumption and analysis overhead, such that we may finish probing and diagnosing the entire DCN in merely a few minutes.
Under the same probing bandwidth budget, it allows each pinger to probe the same set of paths more frequently.α-coverage requires that each link is covered by at least α paths in the probe matrix.
Covering a link multiple times brings higher statistical accuracy for loss detection, as well as better resilience to pinger failures (since a link is more likely to be covered by probes from multiple pingers).
β -identifiability states that the simultaneous failures of any (no more than) β links in the DCN can be localized correctly.
For the routing matrix in Fig. 3, suppose we select p 1 and p 2 to constitute the probe matrix, i.e., the probe matrix contains the first two rows of R.
If 2 or more links fail simultaneously, the faulty links cannot be correctly identified, as the observation from the end is the same, i.e., packet losses are observed on both paths.
On the other hand, if only one link is faulty, the bad link can be identified effectively: losses are observed on both paths, p 1 , or p 2 if link 1, 2, or 3 is faulty, respectively.
Therefore, the probe matrix achieves 1-identifiability, but not 2 or higher identifiability.
Better identifiability contributes to higher accuracy of loss localization.We find that Problem 1 is NP-hard for general DCNs as the Minimum Set Cover Problem is a special case of the problem.
We hence resort to an approximation algorithm to compute the probing path, which is at the heart of deTector.
We extend a well-known greedy algorithm [13] for constructing a probe matrix achieving 1-identifiability to one achieving β -identifiability, as well as α-coverage using a minimal number of probe paths.In a probe matrix, a link belongs to a set of paths.
To achieve 1-identifiability, the path sets of different links should all be different, so that losses can be observed on a particular set of paths to identify the faulty link.
Recall that the set of links in our DCN is E.
Once we select a path from the set of all feasible paths decided by the routing matrix based on some criterion, it splits E into two subsets E 1 and E 2 , containing links on the selected path and the other links, respectively.
If we do not observe any packet loss on this path, it implies that all links in E 1 are good; otherwise, there must be at least one bad link in E 1 .
Similarly, we select another path to further split E 1 and E 2 into smaller subsets, and repeat this procedure.
Eventually if we can obtain subsets each containing only one link, then the probe matrix constructed using the selected paths achieves 1-identifiability (since the set of paths traversing each link is unique); otherwise, there does not exist a 1-identifiable probe matrix in the DCN.
Throughout the process, if we always select a path whose links are present in the largest number of link sets to further split the link sets as much as possible, we will end up with the minimal number of paths needed.To achieve β -identifiability, we expand the DCN graph G with "virtual links".
A virtual link is a combination of multiple physical links, and the set of paths a virtual link belongs to can be computed by "OR"-ing together the paths including the individual links [13].
For the example in Fig. 3, the original routing matrix R is extended to R' with three additional virtual links l 12 , l 13 and l 23 added; the column corresponding to the virtual link l 12 can be computed by "OR"-ing the two columns corresponding to links l 1 and l 2 .
For β -identifiability, The probe matrix does not achieve even path coverage among the links yet.
For example, for a 1-identifiable probe matrix constructed on a 64-ary Fattree, the gap between the maximal and minimal numbers of probing paths passing through any two links can be as large as 188.
To achieve better evenness (i.e., spreading paths and thus probe overhead evenly among the physical links), we introduce a link weight w [link], denoting the number of paths that the link resides on, and ensure that it is no smaller than α for any physical link.
We also define a score for each (extended) path, i.e., the path includes virtual links from the extended routing matrix R':score(path) = ∑ link∈path w[link] − # of link sets on path (1)Here the link sets are the split link sets produced by the procedure above.
We say that a link set is on a path if the link set contains at least one (physical or virtual) link of the path.
Thus, a lower score indicates that the links on the path are not covered much by paths already selected and/or more link sets can be split if the path is selected update setnum as the total number of link sets after split by path 15: return probe matrix constructed by paths in sel paths (retaining only physical links on the paths) in the above procedure.
We strive to achieve better evenness among the links while guaranteeing α-coverage, by always selecting a path with the lowest score.
Our Probe Matrix Construction algorithm, PMC, is summarized in Alg.
1.
We first reduce the problem of constructing a β -identifiable matrix to one constructing 1-identifiable matrix, by adding virtual links to the original routing matrix of the DCN graph (line 2, where LINKOR denotes the method for extending routing matrix discussed above).
Then in each iteration we update the score of each (extended) path (lines 5-6) and select a path which has the minimal score among all candidate paths (lines 7-8).
We remove the selected path from the candidate path set (line 9), and update the weight of physical links (w [physlink]) on the selected path (lines 10-11) and the total number of link sets that the already selected paths can split into (line 14, which corresponds to the procedure discussed in the second paragraph of this subsection).
If the number of paths that cover one (physical) link exceeds α, we remove the link from the set of all links (line 12-13).
The loop stops when the probe matrix achieves α-coverage (i.e., the set physlinks is empty) and β -identifiability (i.e., the number of link sets split equals the number of links), or there are no more candidate paths (i.e., the set paths is empty).
Theorem 1 The PMC algorithm achieves (1 − 1 e ) approximation of the optimum in terms of the total number of probe paths selected, where e is natural logarithm.We can prove Theorem 1 by showing that the score of a path set is monotone, submodular and non-negative.
The detailed proof is in the technical report [7].
In practice, the PMC algorithm performs much better than the (1 − 1 e ) ≈ 0.63 approximation ratio ( §4.4).
The issue of this algorithm, however, is the computation time.
The time complexity of the algorithm is O(m 2 ), where m is the number of paths, since in the worst case we may update the scores of all paths in each iteration and end up with selecting all paths.
In a 64-radix Fattree, there are about 4.3 × 10 9 desirable paths among ToRs.
As we will see in §4.4, the algorithm is still too slow for any data center at a reasonable scale, and we adopt a number of optimizations to further speed it up.
To speed up the PMC algorithm, we apply several optimizations based on the following three observations.
We can construct a bipartite graph according to the routing matrix: one partition corresponds to paths and the other consists of links; an edge exists between a path node and a link node if the link is on the path.
We observe that if the routing matrix can be partitioned into sets of paths with no links in common, then the problem can be divided into independent subproblems.
For example, in Fig. 1, paths traversing the red link have no link overlapping with paths traversing the blue link.
Therefore, the bipartite graph can typically be divided into connected subgraphs and each subgraph represents a smaller routing matrix and hence a subproblem.
Finding connected subgraphs can be done in linear time by traversing the bipartite graph once.
Then the PMC algorithm can be applied to the subproblems in parallel.
The score of each path is non-decreasing over all iterations.It can be proved that the score of a path is non-decreasing (Appendix A in [7]).
Inspired by the CELF algorithm for outbreak detection in networks [38], we adopt a strategy called lazy update which defers the update of a path score as much as possible even though we know the score is outdated.
Specifically, we maintain a min-heap for all paths with scores as the keys and only update the score of a path when the path is at the top of the heap.
After score update, if the path still stays at the top of the heap, i.e., the path has the minimal score among all available paths, we will select the path as a probe path, even though some path scores have yet to be updated.
The correctness of this heuristic is guaranteed by submodularity of the score of a path set: the marginal gain provided by a path selected in the current iteration can not be larger than that provided by the path in the previous iteration.
Due to symmetry, when a path is selected, all its topologically isomorphic paths can be selected.
For example in Fig. 1, if the dashed green path spanning Pod 1 and Pod 2 is selected, then the dashed purple path spanning Pod 3 and Pod 4 may be a good choice too.
This helps us reduce the scale of the problem since the routing matrix R can be reduced to a smaller matrix by excluding paths that are topologically isomorphic to other paths.
For example, if the green path is in the matrix, we do not need to include the purple path.
For this purpose, we first need to compute the symmetric components in a DCN graph.
There are many fast algorithms available for symmetry discovery [17,15], e.g., O 2 [15] can finish computation within 5 seconds for a Fattree(100) DCN, and we only need to precompute it once for a DCN.
We run our PMC algorithm on a Dell PowerEdge R430 rack server with 10 Intel Xeon E5-2650 CPUs and 48GB memory, to test its running time and number of paths selected.
We compare results on three well-known DCNs, Fattree [9], VL2 [22] and BCube [24].
2 Running time.
Table 2 shows the algorithm running time for constructing a probe matrix achieving 2-coverage and 1-identifiability.
The strawman approach is our PMC algorithm without any optimizations.
The last three columns contain results when the respective optimization is in place (in addition to the previous one(s)).
The results show that PMC can efficiently select probe paths for very large DCNs.
Specifically, without algorithm speedup, the computation time of PMC can be larger than 24 hours; after each optimization, the time decreases significantly and we can compute the probe matrix for Fattree(72), VL2(140,120,100) and BCube(8,4) within 18 seconds, 86 seconds and 70 seconds, respectively.
We note that the running time in case of problem decomposition for VL2 and BCube is a bit longer than that of strawman.
This is because decomposition does not apply to the two DCN topologies, but we need extra time to decide whether the matrix is decomposable.Path number.
Table 3 shows the number of selected paths with different α and β in different DCNs.
Compared with the number of original paths in DCNs, PMC only selects a small percentage of paths.
We can prove that the least number of paths for achieving 1-coverage and 1-identifiability is k 3 /5 for any k-ary Fattree (Appendix B in [7]).
Thus, a Fattree(64) DCN needs at least 52428 paths and our algorithm selects slightly more, i.e., 61440 paths.
This implies that pingers under each selected ToR in the Fattree are only responsible for probing about 60 paths, much fewer than that of Pingmesh (about 2000-5000 paths).
We also find that VL2 requires much fewer paths than Fattree and BCube.
This is because VL2 has a much smaller number of links between switches (12288 links in VL2(128, 96,80)), as compared to Fattree (131072 links in Fattree(64)) and BCube (163840 links in BCube (8,4)).
Note that the number of selected path may change when the third optimization, based on topology symmetry, is in place.
Our evaluation shows that the number of selected paths with symmetry reduction is very similar to that without symmetry reduction.
This is consistent with the result in [30], and we hence omit the analysis.Results for β ≥ 3.
The probe matrices we constructed above achieve at most 2-identifiability.
For β ≥ 3, the computation of PMC is not efficient in large DCNs.
For the example of a 48-ary Fattree, computing a probe matrix achieving 3-identifiability requires at least 24 hours, even when we apply all speedup optimizations in §4.3.
The fundamental reason is that the routing matrix R becomes much larger when the number of column increases from n to ∑ 1≤i≤β C(n, i), by adding virtual links.
However, surprisingly, we find that 2-identifiability is enough for loss localization in DCNs, as we will see in §6.4.
After collecting the probe data, the first step is to preprocess the data, removing outliers and normal cases.
Severe packet losses could be caused by bad pingers and responders (e.g., the server is down or was rebooting during probing, thus causing many false alarms [37]).
Such outliers can be identified by keeping track of the status of servers using a watchdog service.
In addition, a link normally has a regular low loss rate, e.g., 10 −4 -10 −5 , due to transient congestion, bit errors, which should not be considered as failures [26].
To exclude such normal cases, we filter out paths with extremely low packet loss rates by setting a threshold on the number of packet losses in a period of time or on packet loss ratio (e.g., 10 −3 [26,21]).
3 After pre-processing, the loss data that remain (in the form of (path, number o f losses)) are likely manifest of network failures rather than noises.
Our fault localization problem is: given end-to-end packet loss observations, find the smallest set of faulty links that best explains the observations.
This problem is NP-hard as it can be reduced to the NP-complete Minimum Hitting Set Problem [18].
Besides, we face two challenges not existed in previous work:Much larger problem scale.
Our study focuses on large-scale DCN networks, different from smaller networks investigated in the existing loss localization work [10,18,42].
At our problem scale, the existing algorithms are not fast enough (taking tens of seconds or even minutes) for real-time loss localization.Different loss patterns.
Network failures are mainly exhibited as two kinds of packet losses: full packet loss and partial packet loss, meaning that all or part of the packets traversing a link are dropped.
Existing tomography techniques assume that if all links on a path are good, then the path is good [19].
This is not true in case of partial packet loss in data centers, e.g., packet blackhole may lead to losses on a link only for a subset of paths using that link.
Based on the Tomo algorithm in [18], we design an efficient Packet Loss Localization algorithm, PLL, to local-ize packet losses in DCNs (see [7] for more details).
The basic idea of PLL is as follows.Step 1: Divide the problem into a series of subproblems, by decomposing the probe matrix following the same steps discussed for decomposing the routing matrix in §4.3.
For each subproblem, run the following steps.Step 2: If all probe paths traversing a link experience no packet loss, we exclude the link.
For the remaining links, we calculate a hit ratio for each link, i.e., the ratio of the number of observed lossy paths through the link over the number of all probe paths using the link [34].
Step 3: We compute a score for each link as the number of lost packets that the link can explain, i.e., if a link lies in the packet path, we say the link can explain the packet loss.Step 4: Among those links whose hit ratios are larger than a preset threshold, we greedily select the link with the maximal score and remove those losses this link can explain.Step 5: Repeat Step 3 and Step 4 until no loss remains unexplained.PLL differs from Tomo mainly in handling partial packet losses, i.e., we use a hit ratio threshold to filter suspected links.
Setting the threshold requires network operator's experience and, if possible, by learning from real loss data.
The analysis on setting this threshold is presented in [7] due to space constraint and we set it to 0.6 by default in our experiments.We have compared performance of PLL and other existing loss localization methods (e.g., Tomo, SCORE [34] and OMP [42]), and present the results in [7].
The results show that given the same probe matrix, PLL achieves 2% higher accuracy (defined as true positive ratio, i.e., the percentage of bad links correctly identified as bad over all truly bad links), 2% lower false positive ratio (i.e., the percentage of good links incorrectly identified as bad over all correctly and incorrectly identified links), and is an order of magnitude faster (e.g., localizing failures within 1 second in a large DCN with 82944 links) than the other algorithms.
We run the controller on one Dell server (or it can run in a distributed fashion over multiple servers for large-scale networks).
A watchdog service also runs on the server for monitoring the health of other servers and removing bad ones.
The controller runs the PMC algorithm to recompute the probe matrix every 10 minutes, based on the current network topology from the watchdog service.
4 The computed probe matrix is divided into XML pinglist files for dispatching to pingers.
A pinglist file contains file version, the pinger's IP address, IP addresses of responders, transport port numbers, the packet-sending interval and IP addresses of core switches.
Our measurement shows that the controller can handle 4473 pinglist requests per second on average with maximal bandwidth consumption 688.56Mb/s using one core.
Since pingers are deployed on a small number of servers (about 10% among all servers), the controller can support more than 100,000 pingers by slightly randomizing the time when pingers request for pinglists in each cycle.Each pinger implements a communication module and a probing module.
The communication module is responsible for connections with the controller and the diagnoser.
It fetches the pinglist file from the controller by an HTTP GET request in every cycle (i.e., 10 minutes).
The probing module generates probe packets according to the pinglist and encapsulates them by IP-in-IP ( §3.1).
In our experiments, a pinger loops over a range of ports for each path, and emits several packets for every port.
Each probe packet has an average size of 850 Bytes, carrying a specified DSCP value in the IP header to test different QoS classes [12].
If there is no response for a probe within 100ms, we mark it as a loss.
A pinger repeatedly sends packets by looping through the paths in the pinglist for multiple times (for statistical accuracy), at the rate of 10 packets per second.
Every 30 seconds, the pinger aggregates the probing results (i.e., the number of packet losses and the number of packets sent on each probe path) into an XML file and sends it to the diagnoser by an HTTP POST request.
The responder module runs in the userspace of all servers, which listens to a particular port, and upon packet arrival, it adds a timestamp and sends the packet back.
The pinger and responder incur little overhead on servers, as we will see in §6.3.
The diagnoser is a Web server module running on the same server where the controller is in our experiments.
It runs the PLL algorithm for fault localization once every half a minute, using collected probe results in the past 30 seconds.
Given the limited number of servers in our testbed, we run a virtual machine to emulate a server.
We build a 4-ary Fattree testbed with 20 ONetSwitch [5,29,28], each equipped with FPGA-based hardware reconfigurable dataplane, four 1GbE ports and one dedicated management port.
Though we do not require programmable switches in deTector, employing SDN switches facilitates our emulation of various failure cases that may happen in a real-world DCN.
Specifically, we categorize all losses into three types: it does not affect symmetry computation which only pre-runs once on the original DCN topology.Full packet loss.
We install OpenFlow rules with high priority to drop all packets coming from a particular port, to emulate a faulty link with full packet loss.
To emulate a switch down case, we install rules to drop all packets at the switch.Deterministic partial loss.
Packets with certain features (e.g., specific IPs, port numbers) may be dropped on a link deterministically, e.g., in case of packet blackhole or misconfigured routing rules.
To emulate such failures, we install rules on the switches to match and drop packets with certain headers.Random partial loss.
Sometimes packets on a link are dropped randomly, as caused by bit flips, CRC errors, buffer overflow, etc.
SDN switches do not support random packet dropping.
To emulate such losses, we install rules on the switches to redirect all packets on an emulated bad link to the SDN controller, and the SDN controller drops the received packets with certain probability, following the pattern extracted from [12].
Due to no access to loss data in real-world data centers, we produce the above loss types according to the failure measurements in [20] and traffic measurements in [12].
Specifically, we set parameters such as link vs. switch failure percentage, link loss rates (ranging from 10 −4 to 1), failure probabilities for switches in different tiers, all based on the above measurements.
The loss distribution for links in different tiers is extracted from Fig. 3 in [12].
Aside from deTector, we also implement the probing modules of Pingmesh and NetNORAD on our testbed for performance comparison, as well as their failure localization tools, Netbouncer and fbtracert.
Since we do not know some of their implementation details (e.g., how data pre-processing is done), we implement those details in the same way across all three systems.
We first investigate how probing itself affects the whole DCN.
We use realistic packet traces (including information such as packet header, timestamp) from a university data center [11] (mostly HTTP flows) to generate workload traffic in our testbed, where each server continuously replays flows based on the packet traces and sends them to a random receiver.
We evaluate how our probing frequency (i.e., the number of probes a pinger sends per second) affects the performance of the PLL algorithm, the overhead on the pinger, and RTT and jitter experienced by the workload traffic.
In each minute of our experiment, we emulate a failure randomly picked among the three types of failures, with the failed switches or links randomly picked in the DCN.
We run our experiment for 2000 minutes and obtain the average results.
Fig. 4 shows that a higher probe sending frequency leads to a higher accuracy and a lower false positive ra- Fig. 4(a)), but causes higher CPU utilization and bandwidth consumption on pingers (Fig. 4(b)) as well as slightly larger fluctuation of the RTT (Fig. 4(c)) and jitter ( Fig. 4(d)) experienced by the workload.
We find that 10-15 probes per second is good enough since we can still achieve higher than 95% accuracy and a lower than 3% false positive ratio, while only consuming about 100Kbps bandwidth, 0.4% CPU and 13MB memory on each pinger.
Besides, it does not introduce apparent delay and jitter variations for workload traffic.
Note that the overhead of a responder is much smaller than a pinger because it resumes fewer tasks (e.g., no communication with the controller and the diagnoser), and hence the results are omitted.
5 In all our experiments, the pinger sends 10 packets per second by default (i.e., the red square in Fig. 4).
We then compare the accuracy, false positive ratio and overhead among deTector, Pingmesh and NetNORAD.
Since Pingmesh can not localize failures by itself, once it detects a suspected source-destination server pair, we use Netbouncer [4] to go through all possible paths between this server pair for loss localization.
As for NetNORAD, similarly, we use fbtracert [3] to probe all possible paths between the suspected server pair.
The interval of loss data collection is 30 seconds for three systems.
Fig. 5 shows the comparison when one failure is emulated in the testbed (the failure is randomly picked as in the previous experiment).
The number of (ping and reply) probes in the figure includes probes sent for detection and probes for localization (if any) in each minute of the experiment.
More probes indicate not only more bandwidth consumption, but also higher CPU and memory usage.
For deTector, we use a probe matrix with 1-identifiability and 3-coverage (since it is impossible to achieve 2-identifiability in a 4-ary Fattree).
As we can see, deTector achieves high accuracy and a low false positive ratio with a much smaller number of probes, because deTector covers more types of losses (e.g., low rate loss) and takes carefully planned paths.
For instance, to achieve 98% accuracy and 1% false positives, deTector, NetNORAD and Pingmesh need to send 7200, 20700 and 35100 probes per minute, respectively.
When the probe overhead is same (same number of probes sent per minute), the accuracy and false positive ratio achieved by deTector is better than those of NetNROAD; as compared to Pingmesh, the accuracy of deTector is much better, while the false positive ratio of Pingmesh is slightly smaller sometimes, since it possibly probes all paths.
Fig. 6 further shows the accuracy and false positive ratio with multiple failures, when the probe overhead is fixed to be the same, i.e., 5850 probes per minute.
deTector always achieves much better performance than Pingmesh and NetNORAD.
Note that deTector also detects and localizes failures much faster than NetNORAD and Pingmesh (30 seconds in advance in our experiments), because deTector does not need any other diagnosis tools to send an additional round of probes for loss localization, while others do.
We supplement our experimental evaluation with simulations, to investigate how identifiability of the probe ma- trix influences the accuracy of our failure localization, when running deTector in larger Fattree networks.
We first vary α and β for probe matrix construction in an 18-radix Fattree.
Table 4 shows that higher coverage and higher identifiability lead to higher accuracy, while the overhead (i.e., the number of selected paths) does not increase much.
Also, we find that identifiability is more effective and desirable than coverage for failure localization, since a 1-identifiability matrix increases the accuracy a lot (from one with 0-identifiability guarantee), with much less overhead than a 3-coverage probe matrix.Note that further increasing the level of identifiability for β > 1 does not increase the accuracy much, and probe matrices achieving 1-identifiability can already lead to higher than 90% accuracy.
According to the measurements in [12], less than 10% failure events (failures occurring concurrently) contain more than four failures and less than 1% failure events contain more than 20 failures.
This implies that a probe matrix with 1-identifiability can guarantee higher than 93% accuracy for 90% failure events and 2-identifiability provides a 98% accuracy for 99% failure events.The result is surprising but reasonable: Since we use a number of optimizations ( §4.3) to reduce the size of the routing matrix, the PMC algorithm in fact achieves β 񮽙 -identifiability (where β 񮽙 is larger than β used in the algorithm) for the whole probe matrix, rather than β -identifiability computed for each small probe matrix (corresponding to a small network topology).
Therefore, deTector may fail to localize all failures only if more than β failures appear in a small topology, which occurs with relatively low probability.
This shows that using a probe matrix with a low level of identifiability guarantee is good enough to identify a much larger number of concurrent failures.In addition, by examining the failure events that deTector fails to localize with a low identifiability probe matrix but can identify using a high identifiability matrix, we find that higher identifiability achieves better results only when the number of simultaneously failed links is very large.
Such a failure event with many concurrent link failures is usually triggered by a common bug in practice (e.g., 180 links fail simultaneously due to scheduled maintenance to multiple aggregation switches [20]), and thus those faulty links are spatially clustered.
In such cases, operators can locate the failure spot effectively according to the positions of most failed links.
We further examine the fault localization accuracy, false positive and false negative (bad links incorrectly identified as good) ratios achieved using a probe matrix of 2-identifiability in a 48-ary Fattree.
Table 5 shows that the false positive and false negative ratios remain in a very low level.
In particular, the false positive rate is extremely low (< 1%), which is desirable in practice [18].
The false negatives are mainly caused by losses of extremely low loss rate and intermittent losses which may happen at longer intervals (than 1 minute) [23].
Since it takes longer time to expose these losses, we can further reduce false negatives by examining loss measurements in larger time windows, e.g., 10 minutes.
Packet entropy.
deTector tries to increase packet entropy (i.e., different packet patterns) by varying IP addresses, port numbers and DSCP values, to cover as many failures as possible.
However, our implementation uses IP-in-IP encapsulation for source routing, and hence the range of destination IP addresses is somewhat limited.
In addition, since we use UDP for network probing, deTector may not be able to detect failures related to other protocols, e.g., misconfigured TCP parameters [26].
Adopting other source routing solutions and adding more protocols to increase packet entropy are part of our future work.
Loss diagnosis.
While deTector can localize where packet drops occur, it does not know what causes the drops, e.g., software bugs, misconfigured rules or bursty traffic.
This is a common deficiency of existing monitoring systems, since network diagnosis is rather complex.
However, it is possible to distinguish full losses, deterministic partial losses, random partial losses and losses due to congestion, to narrow down the diagnosis scope (e.g., using machine learning approaches), since they exhibit different loss characteristics.
We consider this as a promising future direction to explore.
Beyond deTector.
As opposed to probe-based solutions like deTector, there are some recent efforts on embedding metadata in the packet header to trace packet path for network debugging (e.g., CherryPick [46], PathDump [47]).
Our technique can be applied to reduce the overhead involved in these approaches, i.e., only packets traversing those paths computed by the PMC algorithm need to carry routing information in the packet headers.
Probe design.
Many existing work (e.g., [14,18,43,33,27]) exploit logs on switches, or utilize multicast or network coding for network probing.
Instead, we treat each switch as a blackbox, and adopt a topology-aware endto-end probing approach.
Some studies [16,40,23] estimate loss rates of all links, while we aim at identifying bad links (i.e., failure spots).
Zeng et al. [48] and Nicolas et al. [23] propose monitoring solutions for backbone networks that do not apply in DCNs due to scalability, and the main difference lies in probe matrix design.
Fault localization.
Our goal of accurately identifying faulty links falls squarely in the area of binary network tomography.
Tomography algorithms such as Sherlock [10], Tomo [18], GREEDY [35], SCORE [34] and OMP [42] do not work well for DCNs due to their problem scales and loss characteristics.
Our PLL algorithm is built on these work and conquers their limitations.
DCN monitoring.
Our work mainly differs from existing monitoring systems such as Pingmesh [26] and NetNORAD [37] in the design of probe matrix.
We argue that loss detection and localization must be coupled together to localize more failures (e.g., transient failures) in real time with low overhead.
Carefully designed probe matrix is the key to achieve them.
LossRadar [39] is a switch-based solution but it requires programmable switches.
Dapper [44] and Zipkin [8] are distributed tracing systems to gather timing data for root-cause analysis.
deTector is a real-time, low-overhead and high-accuracy monitoring system for large-scale data center networks.
At its core is a carefully designed probe matrix, constructed by a scalable greedy path selection algorithm with minimized probe overhead.
We also design an efficient failure localization algorithm according to different patterns of packet losses.
Our analysis, testbed experiments and large-scale simulations show that deTector is highly scalable, practically deployable with low overhead, and can localize failures with high accuracy in near real time.
