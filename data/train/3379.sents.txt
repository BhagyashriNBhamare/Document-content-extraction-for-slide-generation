In this paper, we study how to optimally provide isolation guarantees in multi-resource environments, such as public clouds, where a tenant's demands on different resources (links) are correlated.
Unlike prior work such as Dominant Resource Fairness (DRF) that assumes static and fixed demands, we consider elastic demands.
Our approach generalizes canonical max-min fairness to the multi-resource setting with correlated demands, and extends DRF to elastic demands.
We consider two natural optimization objectives: isolation guarantee from a tenant's viewpoint and system utilization (work conservation) from an operator's perspective.
We prove that in non-cooperative environments like public cloud networks , there is a strong tradeoff between optimal isolation guarantee and work conservation when demands are elastic.
Even worse, work conservation can even decrease network utilization instead of improving it when demands are inelastic.
We identify the root cause behind the tradeoff and present a provably optimal allocation algorithm, High Utilization with Guarantees (HUG), to achieve maximum attainable network utilization without sacrificing the optimal isolation guarantee, strategy-proofness, and other useful properties of DRF.
In cooperative environments like private datacenter networks, HUG achieves both the optimal isolation guarantee and work conservation.
Analyses, simulations, and experiments show that HUG provides better isolation guarantees , higher system utilization, and better tenant-level performance than its counterparts.
In shared, multi-tenant environments such as public clouds [2,5,6,8,40], the need for predictability and the means to achieve it remain a constant source of discussion [15,44,45,50,51,58,59,61,62].
The general consensus -recently summarized by Mogul and Popa [53] -is that tenants expect guaranteed minimum bandwidth (i.e., isolation guarantee) for performance predictability, while network operators strive for work conservation to achieve high utilization and strategy-proofness to ensure isolation.Max-min fairness [43] -a widely-used [16,25,34,35,55,63,64] allocation policy -achieves all three in the context of a single link.
It provides the optimal isolation guarantee by maximizing the minimum amount of bandwidth allocated to each flow.
The bandwidth allocation of a user (tenant) determines her progress -i.e., how fast she can complete her data transfer.
It is work-conserving, because, given enough demand, it allocates the entire bandwidth of the link.
Finally, it is strategyproof, because tenants cannot get more bandwidth by lying about their demands (e.g., by sending more traffic).
However, a datacenter network involves many links, and tenants' demands on different links are often correlated.
Informally, we say that the demands of a tenant on two links i and j are correlated, if for every bit the tenant sends on link-i, she sends at least α bits on link-j.
More formally, with every tenant-k, we associate a correlation vector− → d k = �d 1 k , d 2 k , .
.
.
, d n k �, where d i k ≤ 1, which captures the fact that for every d i k bits tenant-k sends on link-i, it should send at least d j k bits on link-j.
Examples of applications with correlated demands include optimal shuffle schedules [22,23], long-running services [19,52], multi-tiered enterprise applications [39], and realtime streaming applications [10,69].
Consider the example in Figure 1a with two independent links and two tenants.
The correlation vector − → d A = � 1 2 , 1� means that (i) link-2 is tenant-A's bottleneck, (ii) for every M A rate tenant-A is allocated on the bottleneck link, she requires at least M A /2 rate on link-1, resulting in a progress of M A , and (iii) except for the bottleneck link, tenants' demands are elastic, meaning tenant-A can use more than M A /2 rate on link-1.
1 Similarly, tenant-B requires at least M B /6 on link-2 for M B on link-1.
If we denote the rate allocated to tenant-k on link-i by a i k , thenM k = min i 񮽙 a i k d i k , the minimum demand-normalized rate allocation over all links, captures her progress.In this paper, we want to generalize max-min fairness to tenants with correlated and elastic demands while maintaining its desirable properties: optimal isolation guarantee, high utilization, and strategy-proofness.
Intuitively, we want to maximize the minimum progress over all tenants, i.e., maximize min k M k , where min k M k corresponds to the isolation guarantee of an allocation algorithm.
We make three observations.
First, when there is a single link in the system, this model trivially reduces to max-min fairness.
Second, getting more aggregate bandwidth is not always better.
For tenant-A in the example, �50Mbps, 100Mbps� is better than �90Mbps, 90Mbps� or �25Mbps, 200Mbps�, even though the latter ones have more bandwidth in total.
Third, simply applying max-min fairness to individual links is not enough.
In our example, max-min fairness allocates equal resources to both tenants on both links, resulting in allocations � 1 2 , 1 2 � on both links (Figure 1b).
Corresponding progress (M A = M B = 1 2 ) result in a suboptimal isolation guarantee (min{M A , M B } = 1 2 ).
Dominant Resource Fairness (DRF) [33] extends maxmin fairness to multiple resources and prevents such suboptimality.
It equalizes the shares of dominant resources -link-2 (link-1) for tenant-A (tenant-B) -across all tenants with correlated demands and maximizes the isolation guarantee in a strategyproof manner.
As shown in Figure 1c, using DRF, both tenants have the same progress -M A = M B = 2 3 , 50% higher than using max-min fairness on individual links.
Moreover, DRF's isolation guarantee (min{M A , M B } = 23 ) is optimal across all possible allocations and is strategyproof.However, DRF assumes inelastic demands [40], and it is not work-conserving.
For example, bandwidth on link-2 in shades is not allocated to either tenant.
In fact, we show that DRF can result in arbitrarily low utilization (Lemma 6).
This is wasteful, because unused bandwidth cannot be recovered.We start by showing that strategy-proofness is a necessary condition for providing the optimal isolation guarantee -i.e., to maximize min k M k -in non-cooperative environments ( §2).
Next, we prove that work conservation -i.e., when tenants are allowed to use unallocated resources, such as the shaded area in Figure 1c, without constraints -spurs a race to the bottom.
It incentivizes each tenant to continuously lie about her demand correlations, and in the process, it decreases the amount of useful work done by all tenants!
Meaning, simply making DRF work-conserving can do more harm than good.We propose a two-stage algorithm, High Utilization with Guarantees (HUG), to achieve our goals ( §3).
Fig- ure 2 surveys the design space for cloud network sharing and places HUG in context by following the thick lines.
At the highest level, unlike many alternatives [13,14,37,44], HUG is a dynamic allocation algorithm.
Next, HUG enforces its allocations at the tenant-/network-level, because flow-or (virtual) machine-level allocations [61,62] do not provide isolation guarantee.Due to the hard tradeoff between optimal isolation guarantee and work conservation in non-cooperative environments, HUG ensures the highest utilization possible while maintaining the optimal isolation guarantee.
It incentivizes tenants to expose their true demands, ensuring that they actually consume their allocations instead of causing collateral damages.
In cooperative environments, where strategy-proofness might be a nonrequirement, HUG simultaneously ensures both work conservation and the optimal isolation guarantee.
In contrast, existing solutions [33,45,51,58,59] are suboptimal in both environments.
Overall, HUG generalizes single- [25,43,55] and multi-resource max-min fairness [27,33,38,56] and multi-tenant network sharing solutions [45,51,58,59,61,62] under a unifying framework.
HUG is easy to implement and scales well.
Even with 100, 000 machines, new allocations can be centrally calculated and distributed throughout the network in less than a second -faster than that suggested in the literature [13].
Moreover, each machine can locally enforce HUG-calculated allocations using existing traffic control tools without any changes to the network ( §4).
We demonstrate the effectiveness of our proposal using EC2 experiments and trace-driven simulations ( §5).
In non-cooperative environments, HUG provides the optimal isolation guarantee, which is 7.4× higher than existing network sharing solutions like PS-P [45,58,59] and 7000× higher than traditional per-flow fairness, and 1.4× better utilization than DRF for production traces.
In cooperative environments, HUG outperforms PS-P and per-flow fairness by 1.48× and 17.35× in terms of the 95th percentile slowdown of job communication stages, and 70% jobs experience lower slowdown w.r.t. DRF.We discuss current limitations and future research in Section 6 and compare HUG to related work in Section 7.
M 1 A 1 B 1 M 2 A 2 B 2 M 3 A 3 B 3 L 1 L 4 L 2 L 5 L 3 L 6Figure 3: Two VMs from tenant-A (orange) and three from tenant-B (dark blue) and their communication patterns over a 3 × 3 datacenter fabric.
The network fabric has three uplinks (L 1-L3) and three downlinks (L4-L6) corresponding to the three physical machines.
In this section, we elaborate on the assumptions and notations used in this paper and summarize the three desirable requirements -optimal isolation guarantee, high utilization, proportionality -for bandwidth allocation across multiple tenants.
Later, we show the tradeoff between optimal isolation guarantee and high utilization, identifying work conservation as the root cause.
We consider Infrastructure-as-a-Service (e.g., EC2 [2], Azure [8], and Google Compute [5]) and Container-asa-Service (e.g., Mesos [40] and Kubernetes [6]) models where tenants pay per-hour flat rates for virtual machines (VMs) and containers.
2 We abstract out the datacenter network as a nonblocking switch (i.e., the fabric/hose model [11,12,14,23,28,45,49]) with P physical machines connected to it.
Each machine has full-duplex links (i.e., 2P independent links) and can host one or more VMs from different tenants.
Figure 3 shows an example.
We assume that VM placement and routing are implemented independently.
Not only does this model provide analytical simplicity, it is mostly a reality today: recent EC2 and Google datacenters have full bisection bandwidth networks [4,7].
We denote the correlation vector of the k-th tenant(k ∈ {1, ..., M }) as − → d k = �d 1 k , d 2 k , . . . d 2P k �, where d i k and d P +i k(1 ≤ i ≤ P ) respectively denote the uplink and downlink demands normalized 3 by link capacities− → d k Correlation vector of tenant-k's demand − → a k Guaranteed allocation to tenant-k M k Progress of tenant-k; M k := min 1≤i≤2P 񮽙 a i k d i k 񮽙 ,where subscript i stands for link-i − → c k Actual resource consumption of tenant-k Table 1: Important notations and definitions.Isolation Guarantee min k M k Optimal Isolation Guarantee max {min k M k } (Network) Utilization 񮽙 i 񮽙 k c i k(C i ) and񮽙 P i=1 d i k = 񮽙 P i=1 d P +i k .
For the example in Figure 3, consider tenant correlation vectors:− → d A = � 1 2 , 1, 0, 1, 1 2 , 0� − → d B = �1, 1 6 , 0, 0, 1, 1 6 �where d i k = 0 indicates the absence of a VM and d i k = 1 indicates the bottleneck link(s) of a tenant.Correlation vectors depend on tenant applications, that can range from elastic-demand batch jobs [3,24,42,68] to long-running services [19,52], multi-tiered enterprise applications [39], and realtime streaming applications [9,69] with inelastic demands.
We focus on scenarios where a tenant's demand changes at the timescale of seconds or longer [13,18,58], and she can use provider-allocated resources in any way for her own workloads.
Given correlation vectors of M tenants, a cloud provider must use an allocation algorithm A to determine the allocations of each tenant:A({ − → d 1 , − → d 2 , . . . , − → d M }) = { − → a 1 , − → a 2 , . . . , − → a M } where − → a k = �a 1 k , a 2 k , .
.
.
a 2P k � and a i k is the fraction of link-i guaranteed to the k-th tenant.As identified in previous work [15,58], any allocation policy A must meet three requirements -(optimal) isolation guarantee, high utilization, and proportionality -to fairly share the cloud network: 1.
Isolation Guarantee: VMs should receive minimum bandwidth guarantees proportional to their correlation vectors so that tenants can estimate worst-case performance.
Formally, progress of tenant-k (M k ) is defined as her minimum demand satisfaction ratio across the entire fabric:M k = min 1≤i≤2P 񮽙 a i k d i k 񮽙 100% 50% 0% L 1 L 2 1/2 1/2 1/2 1/2Figure 4: Bandwidth consumptions of tenant-A (orange) and tenant-B (dark blue) with correlation vectors− → dA = � 1 2 , 1� and − → dB = �1, 1 6� using PS-P [45,58,59].
Both tenants run elasticdemand applications.For example, progress of tenants A and B in Figure 4 areM A = M B = 1 2 .
4 Note that M k = 1 M if − → d k = �1, 1, . . . , 1�for all tenants (generalizing PS-P [58]), and M k = 1 M for flows on a single link (generalizing per-flow max-min fairness [43]).
Isolation guarantee is defined as the lowest progress across all tenants, i.e., mink M k .2.
High Utilization: Spare network capacities should be utilized by tenants with elastic demands to ensure high utilization as long as it does not decrease anyone's progress.
A related concept is work conservation, which ensures that either a link is fully utilized or demands from all flows traversing the link have been satisfied [43,58].
Although existing research conflates the two [14,15,45,51,58,59,61,62,67], we show in the next section why that is not the case.
3.
Proportionality: A tenant's bandwidth allocation should be proportional to its payment similar to resources like CPU and memory.
We discuss this requirement in more details in Section 3.3.1.
Prior work also identified two tradeoffs: isolation guarantee vs. proportionality and high utilization vs. proportionality.
However, it has been implicitly assumed that tenant-level optimal isolation guarantee 5 and networklevel work conservation can coexist.
Although optimal isolation guarantee and network-level work conservation can coexist for a single link -max-min fairness is an example -optimal isolation guarantee and work conservation can be at odds when we consider the network as a whole.
This has several implications on both isolation guarantee and network utilization.
In particular, we can (1) either optimize utilization, then maximize the isola- 4 We are continuing the example in Figure 3 but omitted the rest of − → a k , because there is either no contention or they are symmetric.
5 Optimality means that the allocation maximizes the isolation guarantee across all tenants, i.e., maximize 񮽙 min tion guarantee with best effort; or (2) optimize the isolation guarantee, then maximize utilization with best effort.
6 Please refer to Appendix C for more details.k M k 񮽙 .
100% 50% 0% L 1 L 2 2/3 1/3 2/3 1/9 (a) Optimal isolation guarantee 100% 50% 0% 1/2 1/2 11/12 1/12 L 1 L 2 (b) Tenant-A lies 100% 50% 0% 3/4 1/4 1/2 1/2 L 1 L 2 (c) Tenant-B lies 100% 50% 0% L 1 L 2 1/2 1/2 1/2 1/2 (d) Both lie As shown in prior work [58, Section 2.5], flow-level and VM-level mechanisms -e.g., per-flow, per sourcedestination pair [58], and per-endpoint fairness [61,62] -can easily be manipulated by creating more flows or by using denser communication patterns.
To avoid such manipulations, many allocation mechanisms [45,58,59] equally divide link capacities at the tenant level and allow work conservation for tenants with unmet demands.
Fig- ure 4 shows an allocation using PS-P [58] with isolation guarantee 1 2 .
If both tenants have elastic-demand applications, they will consume entire allocations; i.e.,− → c A = − → c B = − → a A = − → a B = � 1 2 , 1 2 �, where − → c k = �c 1 k , c 2 k , . . . c 2P k � and c i k is the fraction of link-i consumed by tenant-k.
Recall that a i k is the guaranteed allocation of link-i to tenant-k.
However, PS-P and similar mechanisms are also suboptimal.
For the ongoing example, Figure 5a shows the optimal isolation guarantee of 2 3 , which is higher than that provided by PS-P.
In short, full utilization does not necessarily imply optimal isolation guarantee!
In contrast, optimal isolation guarantee does not necessarily mean full utilization.
In general, optimal isolation guarantees can be calculated using DRF [33], which generalizes max-min fairness to multiple resources.
In the example of Figure 5a, each uplink and downlink of the fabric is an independent resource -2P in total.
Given this premise, it seems promising and straightforward to keep the DRF-component for optimal isolation guarantee and strategy-proofness and try to ensure full utilization by allocating all remaining resources.In the following two subsections, we show that work conservation may render isolation guarantee no longer optimal, and even worse, may reduce useful network utilization.
We first illustrate that even the optimal isolation guarantee allocation degenerates into the classic prisoner's dilemma problem [30] in the presence of work conservation.
In particular, we show that reporting a false correlation vector �1, 1� is the dominant strategy for each tenant, i.e., her best option, no matter whether the other tenants tell the truth or not.
As a consequence, optimal isolation guarantees decrease ( Figure 6).
If tenant-A can use the spare bandwidth in link-2, she can increase her progress at the expense of tenant-B by changing her correlation vector to Figure 5b).
Overall, progress of tenant-A would increase to 11 12 , while decreasing it to 1 2 for tenant-B.
As a result, the isolation guarantee decreases from 2 3 to 1 2 .
The same is true for tenant-B as well.
Consider again that only tenant-B reports a falsified correlation vector Figure 5c).
Overall, progress of tenant-B would increase to 3 4 , while decreasing it to 1 2 for tenant-A, resulting in the same suboptimal isolation guarantee 1 2 .
Since both tenants gain by lying, they would both si- multaneously lie:− → d 񮽙 A = �1, 1�.
With an unmodified − → d B = �1, 1 6 �, the new allocation would be − → a A = � 1 2 , 1 2 � and − → a B = � 1 2 , 1 12 �.
However, work con- servation would increase it to − → a A = � 1 2 , 11 12 � (− → d 񮽙 B = �1, 1� to receive a favorable allocation: − → a A = � 1 4 , 1 2 � and − → a B = � 1 2 , 1 2 �.
Work conservation would in- crease it to − → a B = � 3 4 , 1 2 � (100% 50% 0% L 1 L 2 2/3 1/3 2/3 1/9 (a) Optimal isolation guarantee 100% 50% 0% L 1 L 2 1/2 1/12 11/12 11/24 (b) Tenant-A lies 100% 50% 0% 3/4 1/4 1/2 1/8 L 1 L 2 (c) Tenant-B lies 100% 50% 0% 1/2 1/4 1/2 1/12 L 1 L 2 (d) Both lie− → d 񮽙 A = − → d 񮽙 B = �1, 1�, resulting in a lower isolation guarantee 1 2 ( Figure 5d).
Both are worse off!
In this example, the inefficiency arises due to allocating all spare resources to the tenant who demands more.
We show in Appendix B that intuitive allocation policies of all spare resources -e.g., allocating all to who demands the least, allocating equally to all tenants with non-zero demands, and allocating proportionally to tenants' demands -do not work as well.
Now consider that neither tenant has elastic-demand applications; i.e., they can only consume bandwidth proportional to their correlation vectors.
A similar prisoner's dilemma unfolds ( Figure 6), but this time, network utilization decreases as well.Given the optimal isolation guarantee allocation, − → a A = − → c A = � 1 3 , 2 3 � and − → a B = − → c B = � 2 3 , 1 9 �, The primary takeaways of this section are the following:• Existing mechanisms provide either suboptimal isolation guarantees or low network utilization.
• There exists a strong tradeoff between optimal isolation guarantee and high utilization in a multi-tenant network.The key lies in strategy-proofness: optimal isolation guarantee requires it, while work conservation nullifies it.
We provide a formal result about this (Corollary 2) in the next section.
• Unlike single links, work conservation can decrease network utilization instead of increasing it.
In this section, we show that despite the tradeoff between optimal isolation guarantee and work conservation, it is possible to increase utilization to some extent.
Moreover, we present HUG, the optimal algorithm to ensure maximum achievable utilization without sacrificing optimal isolation guarantees and strategy-proofness of DRF.
We defer the proofs from this section to Appendix A. Going back to Figure 5, both tenants were incentivized to lie because they were receiving spare resources without any restriction due to the pursuit of work conservation.
After tenant-A lied in Figure 5b, both M A and M B decreased to 1 2 .
However, by cheating, tenant-A managed to increase her allocation in link-1 to 1 2 from 1 3 .
Next, indiscriminate work conservation increased her allocation in link-2 to 11 12 from the initial 1 2 , effectively increasing M A to 11 12 .
Similarly in Figure 5c, tenant-B first increased her allocation in link-2 to 1 2 from 1 9 and then work conservation increased her allocation in link-1 to Algorithm 1 High Utilization with Guarantees (HUG) Input: { − → d k }: reported correlation vector of tenant-k, ∀k Output: { − → a k }: Given the tradeoff, our goal is to design an allocation algorithm that can achieve the highest utilization while keeping the optimal isolation guarantee and strategyproofness.
Formally, we want to design an algorithm toMaximize 񮽙 i∈ [1,2P ] 񮽙 k∈ [1,M ] c i k subject to mink∈[1,M ] M k = M * ,(1)where c i k is the actual consumption 7 of tenant-k on link-i for allocation a i k , and M * is the optimal isolation guarantee.We observe that an optimal algorithm would have restricted tenant-A's progress in Figure 5b and tenant-B's progress in Figure 5c to 2 3 .
Consequently, they would not have been incentivized to lie and the prisoner's dilemma could have been avoided.
Algorithm 1 -referred to as High Utilization with Guarantees (HUG) -is such a two-stage allocation mechanism that guarantees maximum utilization while maximizing the isolation guarantees across tenants and is strategyproof.In the first stage, HUG allocates resources to maximize isolation guarantees across tenants.
To achieve this, we pose our problem as a 2P -resource fair sharing problem and use DRF [33,56] to calculate M * .
By reserving these allocations, HUG ensures isolation.
Moreover, because DRF is strategy-proof, tenants are guaranteed to use these allocations (i.e., c i k ≥ a i k ).
While DRF maximizes the isolation guarantees (a.k.a. dominant shares), it results in low network utilization.
In some cases, DRF may even have utilization arbitrarily close to zero, and HUG can increase that to 100% (Lemma 6).
To achieve this, the second stage of HUG maximizes utilization while still keeping the allocation strategyproof.
In this stage, we calculate upper bounds to restrict how much of the spare capacity a tenant can use in each link, with the constraint that the largest share across all links cannot increase (Lemma 1).
As a result, Algorithm 1 remains strategy-proofness across both stages.
Because spare usage restrictions can be applied locally, HUG can be enforced in individual machines.Illustrated in Figure 8, the bound is set at 2 3 for both tenants, and tenant-B can use its elastic demand on link-2's spare resource, while tenant-A cannot as she has reached its bound on link-2.
We list the main properties of HUG in the following.
1.
In non-cooperative cloud environments, HUG is strategyproof (Theorem 3), maximizes isolation guarantees (Corollary 4), and ensures the highest utilization possible for an optimal isolation guarantee allocation (Theorem 5).
In particular, Lemma 6 shows that under some cases, DRF may have utilization arbitrarily close to 0, and HUG improves it to 100%.
We defer the proofs of properties in the Section to Appendix A. 2.
In cooperative environments like private datacenters, HUG maximizes isolation guarantees and is workconserving.
Work conservation is achievable because strategy-proofness is a non-requirement in this case.
3.
Because HUG provides optimal isolation guarantee, it provides min-cut proportionality ( § 3.3.1) in both non-cooperative and cooperative environments.
Regardless of resource types, the identified tradeoff exists in general multi-resource allocation problems and HUG can directly be applied.
Prior work promoted the notion of proportionality [58], where tenants would expect to receive total allocations proportional to their number of VMs regardless of communication patterns.
Meaning, two tenants, each with N VMs, should receive equal bandwidth even if tenant-X has an all-to-all communication pattern (i.e., shows an example.
Clearly, tenant-Y will be bottlenecked at her only receiver; trying to equalize them will only result in low utilization.
As expected, FairCloud proved that such proportionality is not achievable as it decreases both isolation guarantee and utilization [58].
None of the existing algorithms provide proportionality.
Instead, we consider a relaxed notion of proportionality, called min-cut proportionality, that depends on communication patterns and ties proportionality with a tenant's progress.
Specifically, each tenant receives minimum bandwidth proportional to the size of the minimum cut [31] of their communication patterns.
Meaning, in the earlier example, tenant-X would receive P times more total bandwidth than tenant-Y, but they would have the optimal isolation guarantee (M X = M Y = 1 2 ).
Min-cut proportionality and optimal isolation guarantee can coexist, but they both have tradeoffs with work conservation.
− → d X = �1, 1 This section discusses how a cloud operator can implement, enforce, and expose HUG to the tenants ( §4.1), how to exploit placement to further improve HUG's performance ( §4.2), and how HUG can handle weighted, heterogeneous scenarios ( §4.3).
HUG can easily be implemented atop existing monitoring infrastructure of cloud operators (e.g., Amazon CloudWatch [1]).
Tenants would periodically update their correlation vectors through a public API, and the operator would compute new allocations and update enforcing agents within milliseconds.HUG API The tenant-facing API simply transfers a tenant's correlation vector (− → d k ) to the operator.
− → d k = �1, 1, . . . , 1�is used as the default correlation vector.
By design, HUG incentivizes tenants to report and maintain accurate correlation vectors.
This is because the more accurate it is -instead of the default− → d k = �1, 1, . . . , 1� -the higher are her progress and performance.Many applications already know their long-term profiles (e.g., multi-tier online services [19,52]) and others can calculate on the fly (e.g., bulk communication dataintensive applications [22,23]).
Moreover, existing techniques in traffic engineering can provide good accuracy in estimating and predicting demand matrices for coarse time granularities [17,18,20,47,48].
Centralized Computation For any update, the operator must run Algorithm 1.
Although Stage-1 requires solving a linear program to determine the optimal isolation guarantee (i.e., the DRF allocation) [33], it can also be rewritten as a closed-form equation [56] when tenants can scale up and down following their normalized correlation vectors.
The progress of all tenants after Stage-1 of Algorithm 1 -the optimal isolation guarantee -is:M * = 1 max 1≤i≤2P M 񮽙 k=1 d i k(2)Equation (2) is computationally inexpensive.
For our 100-machine cluster, calculating M * takes about 5 microseconds.
Communicating the decision to all 100 machines takes just 8 milliseconds and to 100, 000 (emulated) machines takes less than 1 second ( §5.1.2).
The guaranteed minimum allocations of tenant-k can then be calculated asa i k = M * d i k for all 1 ≤ i ≤ 2P.
Local Enforcement Enforcement in Stage-2 of Algorithm 1 is simple as well.
After reserving the minimum uplink and downlink allocations for each tenant, each machine needs to ensure that no tenant can consume more than M * fraction of the machine's up or down link capacities (C i ) to the network; i.e., a i k ≤ c i k ≤ M * .
The spare is allocated among tenants using local maxmin fairness subject to tenant-specific upper-bounds.
Because we only care about inter-tenant behavior -not how a tenant performs internal sharing -stock Linux tc is sufficient ( §5).
A tenant has the flexibility to choose from traditional per-flow fairness, shortest-first flow scheduling [12,41], or explicit rate-based flow control [29].
While M * is optimal for a given placement, it can be improved by changing the placement of tenant VMs based on their correlation vectors.
One must perform load balancing across all machines to minimize the denominator of Equation (2).
Cloud operators can employ optimization frameworks like [19] to perform initial VM placement and periodic migrations with an additional load balancing constraint.
However, VM placement is a notoriously difficult problem because of often-incompatible constraints like fault-tolerance and collocation [19], and we consider its detailed study an important future work.
It is worth noting that with any VM placement, HUG provides the highest attainable utilization without sacrificing optimal isolation guarantee and strategy-proofness.
Weighted Tenants Giving preferential treatment to tenants is simple.
Just using (2) a k in one round using Equation (2) will be inefficient.
This is because tenant-k might require less than the calculated allocation, and being bounded, she cannot elastically scale up to use it.
Instead, we must use the multi-round DRF algorithm [56, Algorithm 1] in Stage-1 of HUG; Stage-2 will remain the same.
Note that this is similar to max-min fairness in a single link when a flow has a smaller demand than its 1 n -th share.w k − → d k instead of − → d k in Equa- tion We evaluated HUG using trace-driven simulations and EC2 deployments.
Our results show the following:• HUG isolates multiple tenants across the entire network, and it can scale up to 100, 000 machines with less than one second overhead ( §5.1).
• HUG ensures the optimal isolation guarantee -almost 7000× more than per-flow fairness and about 7.4× more than PS-P in production traces -while providing 1.4× higher utilization than DRF ( §5.2).
• HUG outperforms per-flow fairness (PS-P) by 17.35× (1.48×) in terms of the 95th percentile slowdown and by 1.49× (1.14×) in minimizing the average shuffle completion time ( §5.3).
• HUG outperforms Varys [23] Methodology We performed our experiments on 100 m2.4xlarge Amazon EC2 [2] instances running on Linux kernel 3.4.37 and used the default htb and tc implementations.
While there exist proposals for more accurate qdisc implementations [45,57], the default htb worked sufficiently well for our purposes.
Each of the machines had 1 Gbps NICs, and we could use close to full 100 Gbps bandwidth simultaneously.
We consider a cluster with 100 EC2 machines, divided between three tenants A, B, and C that arrive over time.
Each tenant has 100 VMs; i.e., VMs A i , B i , and C i are collocated on the i-th physical machine.
However, they have different communication patterns: tenants A and C have pairwise one-to-one communication patterns (100 VM-VM flows each), whereas tenant-B follows an all-to-all pattern using 10, 000 flows.
Specifically, A i communicates with A (i+50)%100 , C j communicates with C (j+25)%100 , and any B k communicates with all B l , where i, j, k, l ∈ {1, ..., 100}.
Each tenant demands the entire capacity at each machine; hence, the entire capacity of the cluster should be equally divided among the active tenants to maximize isolation guarantees.
Figure 10a shows that as soon as tenant-B arrives, she takes up the entire capacity in the absence of isolation guarantee.
Tenant-C receives only marginal share as she arrives after tenant-B and leaves before her.
Note that tenant-A (when alone) uses only about 80% of the available capacity; this is simply because just one TCP flow per VM-VM pair often cannot saturate the link.
Figure 10b presents the allocation using HUG.
As tenants arrive and depart, allocations are dynamically calculated, propagated, and enforced in each machine of the cluster.
As before, tenants A and C use marginally less than their allocations because of creating only one flow between each VM-VM pair.
The key challenge in scaling HUG is its centralized resource allocator, which must recalculate tenant shares and redistribute them across the entire cluster whenever any tenant changes her correlation vector.We found that the time to calculate new allocations using HUG is less than 5 microseconds in our 100 machine cluster.
Furthermore, a recomputation due to a tenant's arrival, departure, or change of correlation vector would take about 8.6 milliseconds on average for a 100, 000-machine datacenter.Communicating a new allocation takes less than 10 milliseconds to 100 machines and around 1 second for 100, 000 emulated machines (i.e., sending the same message 1000 times to each of the 100 machines).
While Section 5.1 evaluated HUG in controlled, synthetic scenarios, this section focuses on HUG's instantaneous allocation characteristics in the context of a largescale cluster.Methodology We use a one-hour snapshot with 100 concurrent jobs from a production MapReduce trace, which was extracted from a 3200-machine Facebook cluster by Popa et al. [58, Section 5.3].
Machines are connected to the network using 1 Gbps NICs.
In the trace, a job with M mappers and R reducers -hence, the corresponding M × R shuffle -is described as a matrix with the amount of data to transfer between each M -R pair.
We calculated the correlation vectors of individual shuffles from their communication matrices ourselves using the optimal rate allocation algorithm for a single shuffle [22,23], ensuring all the flows of each shuffle to finish simultaneously.Given the workload, we calculate progress of each job/shuffle using different allocation mechanisms and cross-examine characteristics like isolation guarantee, utilization, and proportionality.
Figure 11a presents the distribution of progress of each shuffle.
Recall that the progress of a shuffle -we consider each shuffle an individual tenant in this section -is the amount of bandwidth it is receiving in its bottleneck up or downlink (i.e., progress can be at most 1 Gbps).
Both HUG and DRF (overlapping vertical lines in Fig- ure 11a) ensure the same progress (0.74 Gbps) for all shuffles.
Note that despite same progress, shuffles will finish at different times based on how much data each one has to send ( §5.3).
Per-flow fairness and PS-P provide very wide ranges: 112 Kbps to 1 Gbps for the former and 0.1 Gbps to 1 Gbps for the latter.
Shuffles with many flows crowd out the ones with fewer flows under per-flow fairness, and PS-P suffers by ignoring correlation vectors and through indiscriminate work conservation.
By favoring heavy tenants, per-flow fairness and PS-P do succeed in their goals of increasing network utilization (Figure 11b).
Given the communication patterns of the workload, the former utilizes 69% of 3.2 Tbps total capacity across all machines and the latter utilizes 68.6%.
In contrast, DRF utilizes only 45%.
HUG provides a common ground by extending utilization to 62.4% without breaking strategy-proofness and providing optimal isolation guarantee.
Figure 11c breaks down total allocations of each shuffle and demonstrates two high-level points: 1.
HUG ensures overall higher utilization (1.4× on average) than DRF by ensuring equal progress for smaller shuffles and by using up additional bandwidth for larger shuffles.
It does so while ensuring the same optimal isolation guarantee as DRF.
2.
Per-flow fairness crosses HUG at the 90-th percentile; i.e., the top 10% shuffles receive more band-Bin 1 (SN) 2 (LN) 3 (SW) 4 (LW)% of Shuffles 52% 16% 15% 17% Table 2: Shuffles binned by their lengths (Short and Long) and widths (Narrow and Wide).
width than they do under HUG, while the other 90% receive less than they do using HUG.
PS-P crosses over at the 76-th percentile.
A collateral benefit of HUG is that tenants receive allocations proportional to their bottleneck demands.
Consequently, despite the same progress across all shuffles (Figure 11a), their total allocations vary (Figure 11c) based on the size of minimum cuts in their communication patterns.
We have shown in Section 5.2 that HUG provides optimal isolation guarantee in the instantaneous case.
However, similar to all instantaneous solutions [33,43,58], HUG does not provide any long-term isolation or fairness guarantees.
Consequently, in this section, we evaluate HUG's long-term impact on performance using a production trace through simulations.Methodology For these simulations, we use a MapReduce/Hive trace from a 3000-machine production Facebook cluster.
The trace includes the arrival times, communication matrices, and placements of tasks of over 10, 000 shuffle during one day.
Shuffles in this trace have diverse length (i.e., size of the longest flow) and width (i.e., the number of flows) characteristics and roughly follow the same distribution of the original trace ( Ta- ble 2).
We consider a shuffle to be short if its longest flow is less than 5 MB and narrow if it has at most 50 flows; we use the same categorization.
We calculated the correlation vector of each shuffle as we did before ( § 5.2).
Metrics We consider two metrics: 95th percentile slowdown and average shuffle completion time to respectively measure long-term progress and performance characteristics.We define the slowdown of a shuffle as its completion time due to a scheme normalized by its minimum completion time if it were running alone; i.e.,Slowdown = Compared Duration Minimum DurationThe minimum value of slowdown is one.
We measure performance as the shuffle completion time of a scheme normalized by that using HUG; i.e., If the normalized completion time of a scheme is greater (smaller) than one, HUG is faster (slower).
HUG improves over per-flow fairness both in terms of slowdown and performance.
The 95th percentile slowdown using HUG is 17.35× better than that of per-flow fairness (Table 12b).
Overall, HUG provides better slowdown across the board (Figure 12a) -61% shuffles are better off using HUG and the rest remain the same.
HUG improves the average completion time of shuffles by 1.49× ( Figure 13).
The biggest wins comes from bin-1 (4.8×) and bin-2 (6.15×) that include the so-called narrow shuffles with less than 50 flows.
This reinforces the fact that HUG isolates tenants with fewer flows from those with many flows.
Overall, HUG performs well across all bins.
HUG improves over PS-P in terms of the 95th percentile slowdown by 1.48×, and 45% shuffles are better off using HUG.
HUG also providers better average shuffle completion times than PS-P for an overall improvement of 1.14×.
Large improvements again come in bin-1 (1.19×) and bin-2 (1.27×) because PS-P also favors tenants with more flows.Note that instantaneous high utilization of per-flow fairness and PS-P ( §5.2) does not help in the long run due to lower isolation guarantee.
While HUG and DRF has the same worst-case slowdown, 70% shuffles are better off using HUG.
HUG also provides better average shuffle completion times than DRF for an overall improvement of 1.14×.
Varys outperforms HUG by 1.33× in terms of the 95th percentile slowdown and by 1.45× in terms of average shuffle completion time.
However, because Varys attempts to improve the average completion time by prioritization, it risks in terms of the maximum completion time.
More precisely, HUG outperforms Varys by 1.77× in terms of the maximum shuffle completion time (not shown).
Payment Model Similar to many existing proposals [32,33,45,46,58,59,61,62], we assume that tenants pay per-hour flat rates for individual VMs, but there is no pricing model associated with their network usage.
This is also the prevalent model of resource pricing in cloud computing [2,5,8].
Exploring whether and how a network pricing model would change our solution and what that model would look like requires further attention.Determining Correlation Vectors Unlike long-term correlation vectors, e.g., over the course of an hour or for an entire shuffle, accurately capturing short-term changes can be difficult.
How fast tenants should update their vectors and whether that is faster than centralized HUG can react to requires additional analysis.Decentralized HUG HUG's centralized design makes it easier to analyze its properties and simplifies its implementation.
We believe that designing a decentralized version of HUG is an important future work, which will be especially relevant for sharing wide-area networks in the context of geo-distributed analytics [60,66].
Single-Resource Fairness Max-min fairness was first proposed by Jaffe [43] to ensure at least 1 n -th of a link's capacity to each flow.
Thereafter, many mechanisms have been proposed to achieve it, including weighted fair queueing (WFQ) [25,55] and those similar to or extending WFQ [16,34,35,63,64].
We generalize max-min fairness to parallel communication observed in scale-out applications, showing that unlike in the single-link scenario, optimal isolation guarantee, strategy-proofness, and work conservation cannot coexist.Multi-Resource Fairness Dominant Resource Fairness (DRF) [33] maximizes the dominant share of each user in a strategyproof manner.
Solutions that have attempted to improve the system-level efficiency of multiresource allocation -both before [54,65] and after [27,38,56] DRF -sacrifice strategy-proofness.
We have proven that work-conserving allocation without strategyproofness can hurt utilization instead of improving it.Dominant Resource Fair Queueing (DRFQ) [32] approximates DRF over time in individual middleboxes.
In contrast, HUG generalizes DRF to environments with elastic demands to increase utilization across the entire network and focuses only on instantaneous fairness.Joe-Wong et al. [46] have presented a unifying framework to capture fairness-efficiency tradeoffs in multiresource environments.
They assume a cooperative environment, where tenants never lie.
HUG falls under their FDS family of mechanisms.
In non-cooperative environments, however, we have shown that the interplay between work conservation and strategy-proofness is critical, and our work complements the framework of [46].
Network-Wide / Tenant-Level Fairness Proposals for sharing cloud networks range from static allocation [13,14,44] and VM-level guarantees [61,62] to variations of network-wide sharing mechanisms [45,51,58,59,67].
We refer the reader to the survey by Mogul and Popa [53] for an overview.
FairCloud [58] stands out by systematically discussing the tradeoffs and addresses several limitations of other approaches.
Our work generalizes FairCloud [58] and many proposals similar to FairCloud's PS-P policy [45,59,61].
When all tenants have elastic demands, i.e., all correlation vectors have all elements as 1, we give the same allocation; for all other cases, we provide higher isolation guarantee and utilization.Efficient Schedulers Researchers have also focused on efficient scheduling and/or packing of datacenter resources to minimize job and communication completion times [12, 21-23, 26, 36, 41].
Our work is orthogonal and complementary to these work focusing on applicationlevel efficiency within each tenant.
We guarantee isolation across tenants, so that each tenant can internally perform whatever efficiency or fairness optimizations among her own applications.
In this paper, we have proved that there is a strong tradeoff between optimal isolation guarantees and high utilization in non-cooperative public clouds.
We have also proved that work conservation can decrease utilization instead of improving it, because no network sharing algorithm remains strategyproof in its presence.To this end, we have proposed HUG to restrict bandwidth utilization of each tenant to ensure highest utilization with optimal isolation guarantee across multiple tenants in non-cooperative environments.
In cooperative environments, where strategy-proofness might be a non-requirement, HUG simultaneously ensures both work conservation and the optimal isolation guarantee.HUG generalizes single-resource max-min fairness to multi-resource environments where a tenant's demand on different resources are correlated and elastic.
In particular, it provides optimal isolation guarantee, which is significantly higher than that provided by existing multitenant network sharing algorithms.
HUG also complements DRF with provably highest utilization without sacrificing other useful properties of DRF.
Regardless of resource types, the identified tradeoff exists in general multi-resource allocation problems, and all those scenarios can take advantage of HUG.
Proof Sketch (of Lemma 1) Consider tenant-A from the example in Figure 5.
Assume that instead of reporting her true correlation vector− → d A = � 1 2 , 1�, she reports − → d � A = � 1 2 + �, 1�, where � > 0.
As a result, her alloca- tion will change to − → a � A = � 1/2+� 3/2+� , 1 3/2+� �.
Her allocation in link-1 񮽙 1/2+� 3/2+�񮽙 is already larger than before 񮽙 1 3 񮽙 .
If the work conservation policy allocates the spare resource in link-2 by δ (δ may be small but a positive value), her progress will change to M � A = min 񮽙a 񮽙 1 A d 1 A , a 񮽙 2 A d 2 A 񮽙 = min 񮽙 1+2� 3/2+� , 1 3/2+� +δ 񮽙 .
As long as � < 3/2δ 2/3−δ (if δ ≥ 2 3, we have no constraint on �), her progress will be better than when she was telling the truth, which makes the policy not strategyproof.
The operator cannot prevent this because she knows neither a tenant's true correlation vector nor �, the extent of the tenant's lie. 񮽙
Proof Sketch (of Theorem 3) Because DRF is strategyproof, the first stage of Algorithm 1 is strategyproof as well.
We show that adding the second stage does not violate strategy-proofness of the combination.
Assume that link-b is a system bottleneck -the link DRF saturated to maximize isolation guarantee in the first stage.
Meaning, b = arg maxi M 񮽙 k=1 d i k .
We use D b = M 񮽙 k=1 d b k to denote the total demand in link-b (D b ≥ 1),andM b k = 1/D b for corresponding progress for all tenant-k (k ∈ {1, ..., M }) when link-b is the system bottleneck.
In Figure 5, b = 1.
The following arguments hold even for multiple bottlenecks.Any tenant-k can attempt to increase her progress (M k ) only by lying about her correlation vector ( − → d k ).
Formally, her action space consists of all possible correlation vectors.
It includes increasing and/or decreasing demands of individual resources to report a different vector,− → d � k and obtain a new progress, M � k (> M k ).
Tenantk can attempt one of the two alternatives when reporting − → d � k : either keep link-b still the system bottleneck or change it.
We show that Algorithm 1 is strategyproof in both cases; i.e., M � k ≤ M k .
Case 1: link-b is still the system bottleneck.
Her progress cannot improve because• if d �b k ≤ d b k, her share on the system bottleneck will decrease in the first stage; so will her progress.
There is no spare resource to allocate in link-b.
For example, if tenant-A changes Figure 5, her allocation will decrease to 1 5 -th of link-1; hence, d �1 A = 1 4 instead of d 1 A = 1 2 inM � A = 2 5 instead of M A = 2 3 .
• if d �b k > d b k , her .
Therefore her progress will be smaller than that when she tells the truth (M �b Figure 5, her allocation will increase to 1 2 of link-1.
However, progress of both tenants will decrease:k < M b k ).
For example, if tenant-A changes d �1 A = 1 instead of d 1 A = 1 2 inM A = M B = 12 .
The second stage will restrict her usage in link-2 to 1 2 as well; hence, M � A = 1 2 instead of M A = 2 3 .
Case 2: link-b is no longer a system bottleneck; instead, link-b � (� = b) is now one of the system bottlenecks.We need to consider the following two sub-cases.
• If D �b 񮽙 ≤ D b , the progress in the first stage will increase; i.e., M �b 񮽙 k ≥ M b k .
However, tenant-k's allocation in link-b will be no larger than if she had told the truth, making her progress no better.
To see this, consider the allocations of all other tenants in link-b before and after she lies.
Denote by c b −k and c �b −k the resource consumption of all other tenants in link-b when tenantk was telling the truth and lying, respectively.
We alsohave c b −k = a b −k and a b −k + a b k = 1because link-b was the bottleneck, and there was no spare resource to allocate for this link.
When tenant-k lies,a �b −k ≥ a b −k because M �b 񮽙 k ≥ M b k .
We also have c �b −k ≥ a �b −k and c �b −k + c �b k ≤ 1.
This implies c �b k ≤ 1 − c �b −k ≤ 1 − a �b −k ≤ 1 − a b −k = a b k = c b k .
Meaning, tenant-k's progress is no larger than that when she was telling the truth.
• If D �b 񮽙 > D b , everyone's progress including her own decreases in the first stage (M �b 񮽙 k < M b k ).
Similar to the second scenario in Case 1, the second stage will restrict tenant-k to the lowered progress.Regardless of tenant-k's approaches -keeping the same system bottleneck or not -her progress using Algorithm 1 will not increase. 񮽙
Figure 14: Hard tradeoff between work conservation and strategy-proofness.
Adding one more tenant (tenant-C in black) to Figure 5 with correlation vector 񮽙1, 0񮽙 makes simultaneously achieving work conservation and optimal isolation guarantee impossible, even when all three have elastic demands.1+(K−1)� K → 0 when K → ∞, � → 0 for any N .
100%HUG will allocate to each tenant 1 N on every link and achieve 100% utilization. 񮽙
We demonstrate the tradeoff between work conservation and strategy-proofness (thus isolation guarantee) by extending our running example from Section 2.
Consider another tenant (tenant-C) with correlation vector − → d C = �1, 0� in addition to the two tenants present earlier.
The key distinction between tenant-C and either of the earlier two is that she does not demand any bandwidth on link-2.
Given the three correlation vectors, we can use DRF to calculate the optimal isolation guarantee (Figure 14a), where tenant-k has M k = 2 5 , link-1 is completely utilized, and 7 15 -th of link-2 is proportionally divided between tenant-A and tenant-B.
This leaves us with two questions: 1.
How do we completely allocate the remaining 8 15 -th bandwidth of link-2?
2.
Is it even possible without sacrificing optimal isolation guarantee and strategy-proofness?
We show in the following that it is indeed not possible to allocate more than 4 5 -th of link-2 (Figure 14b) without sacrificing the optimal isolation guarantee.Let us consider three primary categories of workconserving spare allocation policies: demand-agnostic, unfair, and locally fair.
All three will result in lower isolation guarantee, lower utilization, or both.
Demand-agnostic policies equally divide the resource between the number of tenants independently in each link, irrespective of tenant demands, and provide isolation.
Although strategyproof, this allocation (Figure 15a timal isolation guarantee allocation shown in Figure 14a (M A = M B = M C = 2 5 , therefore isolation guarantee is 2 5 ).
PS-P [45,58,59] fall in this category.
Worse, when tenants do not have elastic-demand applications, demand-agnostic policies are not even workconserving (similar to the example in §2.3.4).
Lemma 7 When tenants do not have elastic demands, per-resource equal sharing is not work-conserving.
12 -th of link-1 and 5 9 -th of link-2 will be consumed; i.e., none of the links will be saturated! 񮽙
To make it work-conserving, PS-P suggests dividing spare resources based on whoever wants it.
Proof Sketch If tenant-B gives up her spare allocation in link-2, tenant-A can increase her progress to M A = 2 3 and saturate link-1; however, tenant-B and tenant-C will remain at M B = M C = 1 3 .
If tenant-A gives up her spare allocation in link-1, tenant-B and tenant-C can increase their progress to M B = M C = 3 8 and saturate link-1, but tenant-A will remain at M A = 1 2 .
Because both tenant-A and tenant-B have chances of increasing their progress, both will hold off to their allocations even with useless traffic -another instance of Prisoner's dilemma. 񮽙
Instead of demand-agnostic policies, one can also consider simpler, unfair policies; e.g., allocating all the resources to the tenant with the least or the most demand.Lemma 9 Allocating spare resource to the tenant with the least demand can result in zero spare allocation.Proof Sketch Although this strategy provides the optimal allocation for Figure 5, when at least one tenant in a link has zero demand, it can trivially result in no additional utilization; e.g., tenant-C in Figure 14. 񮽙
Lemma 10 Allocating spare resource to the tenant with the least demand is not strategyproof.
3 ).
Because tenant-B is still receiving more than 1 6 -th of her allocation in link-1 in link-2, she does not need to lie. 񮽙
Corollary 13 (of Lemmas 10, 12) Allocating spare resource randomly to tenants is not strategyproof. 񮽙
Finally, one can also consider equally or proportionally dividing the spare resource on link-2 between tenant-A and tenant-B.
Unfortunately, these strategies are not strategyproof either.Lemma 14 Allocating spare resource equally to tenants is not strategyproof.Proof Sketch If the remaining 8 15 -th of link-2 is equally divided, the share of tenant-A will increase to 2 3 -rd and incentivize her to lie.
Again, the isolation guarantee will be smaller (Figure 15c). 񮽙
Lemma 15 Allocating spare resource proportionally to tenants' demands is not strategyproof.Proof Sketch If one divides the spare in proportion to tenant demands, the allocation is different (Figure 15d) than equal division.
However, tenant-A can again increase her progress at the expense of others. 񮽙
We thank our shepherd Mohammad Alizadeh and the anonymous reviewers of SIGCOMM'15 and NSDI'16 for useful feedback.
This research is supported in part by NSF CISE Expeditions Award CCF-1139158, NSF Award CNS-1464388, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, Cisco, Cray, Cloudera, EMC2, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal, Samsung, Schlumberger, Splunk, Virdata, and VMware.
gThe two conflicting requirements of the network sharing problem can be defined as follows.
Utilization: 񮽙i∈ [1,2P ] 񮽙 k∈ [1,M ] c i k 2.
Isolation guarantee: min k∈ [1,M ] M k Given the tradeoff between the two, one can consider one of the two possible optimizations: 8 O1 Ensure highest utilization, then maximize the isolation guarantee with best effort; O2 Ensure optimal isolation guarantee, then maximize utilization with best effort.O1: Utilization-First In this case, the optimization attempts to maximize the isolation guarantee across all tenants while keeping the highest network utilization.Maximize minwhere ] c i k is the highest utilization possible.
Although this ensures maximum network utilization, isolation guarantee to individual tenants can be arbitrarily low.
This formulation can still be useful in private datacenters [36].
To ensure some isolation guarantee, existing cloud network sharing approaches [14,45,51,58,59,61,62,67] use a similar formulation:The objective here is to maximize utilization while ensuring at least 1 M -th of each link to tenant-k.
However, this approach has two primary drawbacks ( § 2.3): 1.
suboptimal isolation guarantee, and 2.
lower utilization.O2: Isolation-Guarantee-First Instead, in this paper, we have formulated the network sharing problem as follows:Maximize 񮽙 i∈ [1,2P ] 񮽙 k∈ [1,M ] c i k subject to min8 Maximizing a combination of these two is also an interesting future direction.Here, we maximize resource consumption while keeping the optimal isolation guarantee across all tenants, denoted by M * k .
Meanwhile, the constraint on consumption being at least guaranteed minimum allocation ensures strategy-proofness; thus, guaranteeing that guaranteed allocated resources will be utilized.Because c i k values have no upper bounds except for physical capacity constraints, optimization O2 may result in suboptimal isolation guarantee in non-cooperative environments ( §2.3.3).
HUG introduces the following additional constraint to avoid this issue only in noncooperative environments:This constraint is not necessary when strategy-proofness is a non-requirement -e.g., in private datacenters.
The two conflicting requirements of the network sharing problem can be defined as follows.
i∈ [1,2P ] 񮽙 k∈ [1,M ] c i k 2.
Isolation guarantee: min k∈ [1,M ] M k Given the tradeoff between the two, one can consider one of the two possible optimizations: 8 O1 Ensure highest utilization, then maximize the isolation guarantee with best effort; O2 Ensure optimal isolation guarantee, then maximize utilization with best effort.O1: Utilization-First In this case, the optimization attempts to maximize the isolation guarantee across all tenants while keeping the highest network utilization.Maximize minwhere ] c i k is the highest utilization possible.
Although this ensures maximum network utilization, isolation guarantee to individual tenants can be arbitrarily low.
This formulation can still be useful in private datacenters [36].
To ensure some isolation guarantee, existing cloud network sharing approaches [14,45,51,58,59,61,62,67] use a similar formulation:The objective here is to maximize utilization while ensuring at least 1 M -th of each link to tenant-k.
However, this approach has two primary drawbacks ( § 2.3): 1.
suboptimal isolation guarantee, and 2.
lower utilization.O2: Isolation-Guarantee-First Instead, in this paper, we have formulated the network sharing problem as follows:Maximize 񮽙 i∈ [1,2P ] 񮽙 k∈ [1,M ] c i k subject to min8 Maximizing a combination of these two is also an interesting future direction.Here, we maximize resource consumption while keeping the optimal isolation guarantee across all tenants, denoted by M * k .
Meanwhile, the constraint on consumption being at least guaranteed minimum allocation ensures strategy-proofness; thus, guaranteeing that guaranteed allocated resources will be utilized.Because c i k values have no upper bounds except for physical capacity constraints, optimization O2 may result in suboptimal isolation guarantee in non-cooperative environments ( §2.3.3).
HUG introduces the following additional constraint to avoid this issue only in noncooperative environments:This constraint is not necessary when strategy-proofness is a non-requirement -e.g., in private datacenters.
