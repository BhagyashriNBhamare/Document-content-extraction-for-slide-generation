We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum.
Standard BP on our graphi-cal model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates.
Empirically, the resulting algorithm is on average faster than popular combina-torial implementations, while still scaling at the same asymptotic rate of O(bn 3).
Furthermore , the algorithm shows promising performance in machine learning applications.
Belief propagation (BP) is a method for efficiently performing probabilistic inference on most graphical models.
However, its convergence is often unreliable if these graphical models contain loops.
In a few interesting cases, convergence proofs have been made for certain graphical models.
This paper identifies another situation where BP on a loopy graphical model can provably and efficiently solve a combinatorial optimization known as b-matching.
Classical applications of b-matching include resource allocation problems.
Variations on b-matching are also useful for VLSI applications [6].
In particular, bipartite b-matching applies when one has n suppliers and n customers, and needs to ship b supplies between customers and suppliers.
However, there are varying costs for assigning different suppliers to customers.
Other b-matching applications include providing referrals on social networks or matching bidders to sellers in auction situations.In addition to its classical applications, recent work has demonstrated weighted b-matching's utility in machine learning [4].
The b-matching algorithm can be used in place of k-nearest neighbors where it permits a given datum to select k neighboring points but also ensures that exactly k points will select the given datum as their neighbor.
Furthermore, [4] shows how b-matching can be used as a preprocessing step for improving spectral clustering by pruning the weighted affinity graph and removing noisy edges.We cast maximum weighted b-matching as a graphical model because we are interested in the usefulness of BP for solving combinatorial problems in general.
From a practical sense, BP inherently allows for parallel computation and we experienced a significant constant speedup when comparing the 1-matching algorithm in [1] to classical 1-matching algorithms.
Belief propagation also can be stopped early for an approximate solution in a real-time environment.This paper is organized as follows.
Section 2 discusses BP and prior efforts to characterize convergence on some graphical models.
Section 3 sets up the b-matching optimization as BP on a loopy graphical model.
Section 4 proves that loopy BP will converge and gives a bound on the number of iterations.
Section 5 presents experiments with the loopy BP method that show improved computational efficiency over a popular combinatorial algorithm and improved accuracy over k-nearest neighbors in a classification setting.
We conclude and outline future directions in Section 6.
This work generalizes and extends previous work on BP to solve maximum weight matching (which is equivalent to b-matching when b = 1) [1].
It uses a similar approach to that of [3], where we describe a probability distribution for a combinatorial optimization on which standard BP is impractical, and we compute the belief propagation in closed form, producing an efficient algorithm.Graphical models are powerful probabilistic constructions that describe the dependencies between different dimensions in a probability distribution.
In an acyclic graph, a graphical model can be correctly maximized or marginalized by collecting messages from all leaf nodes at some root, then distributing messages back toward the leaf nodes [7].
However, when loops are present in the graph, belief propagation methods often reach oscillation states or converge to either local maxima or incorrect marginals.
Cases with a single loop have been analyzed in [10], which gives an analytical expression relating the steady-state beliefs to the true marginals.Previous work related convergence of BP to types of free energy in [8], and [5] describes general sufficient conditions for convergence towards marginals.Belief propagation has been generalized into a larger set of algorithms called tree-based reparameterization (TRP) in [9].
These algorithms iteratively reparameterize the distribution without changing it based on various trees in the original graph.
Pairwise BP can be interpreted as doing this on the two-node trees of each edge.
The set of graphs on which TRP converges subsumes that of BP.
However, we use standard belief propagation here because it converges on our graph, it is simpler to implement and has additional benefits such as parallel computation.In this work, we provide a proof of convergence based on our specific graphical model and use the topology of our graph to find our convergence time.
u 1 u 2 u 3 u 4 v 1 v 2 v 3 v 4Figure 1: Example b-matching MG on a bipartite graph G. Consider a bipartite graph G = (U, V, E) such that U = {u 1 , . . . , u n }, V = {v 1 , . . . , v n }, and E = (u i , v j ), ∀i ∈ {1, . . . , n}, ∀j ∈ {1, . . . , n}.
Let A be the weight matrix of G such that the weight of edge (u i , v j ) is A ij .
Let a b-matching be characterized by a function M (u i ) or M (v j ) that returns the set of neighbor vertices of the input vertex in the b-matching.
The b-matching objective function can then be written asmax M W(M ) = max M n i=1 v k ∈M(ui) A ik + n j=1 u ℓ ∈M(vj ) A ℓj s.t. |M (u i )| = b, ∀i ∈ {1, . . . , n} |M (v j )| = b, ∀j ∈ {1, . . . , n} .
(1)If we define variables x i ∈ X and y j ∈ Y for each vertex such that x i = M (u i ), and y j = M (v j ), we can define the following functions:φ(x i ) = exp( vj ∈xi A ij ), φ(y j ) = exp( ui∈yj A ij ) ψ(x i , y j ) = ¬(v j ∈ x i ⊕ u i ∈ y j ).
(2)Note that both X and Y have values over n b configurations.
For example, for n = 4 and b = 2, x i could be any entry from {{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}}.
Similarly, if we were to crudely view the potential functions φ(x i ), φ(y j ) and ψ(x i , y j ) as tables, these tables would be of size n b , n b and n b 2 entries respectively.
The potential function ψ(x i , y j ) is a binary function which goes to zero if y j contains a configuration that chooses node u i when x i does not contain a configuration that includes node v j and viceversa.
These are zero configurations since they create an invalid b-matching.
Otherwise ψ(x i , y j ) = 1 if x i and y j are in configurations that could agree with a feasible b-matching.
In Section 3.2 we will show how to avoid ever directly manipulating these cumbersome tables explicitly.Using the potentials and pairwise clique functions, we can write out the weighted b-matching objective as a probability distribution p(X, Y ) ∝ exp(W(M )) [1].
p(X, Y ) = 1 Z n i=1 n j=1 ψ(x i , y j ) n k=1 φ(x k )φ(y k )(3) We maximize this probability function using the maxproduct algorithm.
The max-product algorithm iteratively passes messages, which are vectors over settings of the variables, between dependent variables and stores beliefs, which are estimates of max-marginals.
The following are the update equations for messages from x i to y j .
To avoid clutter, we omit the formulas for messages from y j to x i because these update equations for the reverse messages are the same except we swap the x and the y terms.
In general, the default range of subscript indices is 1 through n; we only indicate the exceptions.m xi (y j ) = 1 Z max xi   φ(x i )ψ(x i , y j ) k񮽙 =j m y k (x i )   b(x i ) = 1 Z φ(x i ) k m y k (x i )Loopy belief propagation on this graph converges to the optimum.
However, since there are n b possible settings for each variable, direct belief propagation is not feasible with larger graphs.
OF n b We exploit three peculiarities of the above formulation to fully represent the n b length messages as scalars.First, the ψ functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.m xi (y j ) ∝ max vj ∈xi φ(x i ) k񮽙 =j m y k (x i ), if u i ∈ y j m xi (y j ) ∝ max vj / ∈xi φ(x i ) k񮽙 =j m y k (x i ), if u i / ∈ y j (4)This is because the ψ function changes based only on whether the setting of y j indicates that v j shares an edge with u i .
Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically asµ xiyj ∝ max vj ∈xi φ(x i ) u k ∈xi\vj µ ki u k / ∈xi\vj ν ki ν xiyj ∝ max vj / ∈xi φ(x i ) u k ∈xi\vj µ ki u k / ∈xi\vj ν ki .
(5)Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result.
We divide all entries in the message vector by ν xiyj to getˆ µ xiyj = µ xiyj ν xiyj andˆνandˆ andˆν xiyj = 1 .
This lossless compression scheme simplifies the storage of message vectors from length n b to 1.
We rewrite the φ functions as a product of the exponentiated A ij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition ofˆµofˆ ofˆµ xiyj givesˆ µ xiyj = max j∈xi φ(x i ) k∈xi\jˆµk∈xi\jˆ k∈xi\jˆµ ki max j / ∈xi φ(x i ) k∈xi\jˆµk∈xi\jˆ k∈xi\jˆµ ki = max j∈xi k∈xi exp(A ik ) k∈xi\jˆµk∈xi\jˆ k∈xi\jˆµ ki max j / ∈xi k∈xi exp(A ik ) k∈xi\jˆµk∈xi\jˆ k∈xi\jˆµ ki = exp(A ij ) max j∈xi k∈xi\j exp(A ik )ˆ µ ki max j / ∈xi k∈xi exp(A ij )ˆ µ ki (6)We cancel out common terms and are left with the simple message update rule,ˆ µ xiyj = exp(A ij ) exp(A iℓ )ˆ µ y ℓ xi .
Here, the index ℓ refers to the the bth greatest setting of k for the term exp(A ik )m yk (x i ), where k 񮽙 = j.
This compressed version of a message update can be computed in O(bn) time.We cannot efficiently reconstruct the entire belief vector but we can efficiently find its maximum.max xi b(x i ) ∝ max xi φ(x i ) k∈xî k∈xî µ y k xi ∝ max xi k∈xi exp(A ik )ˆ µ y k xi (7)Finally, to maximize over x i we enumerate k and greedily select the b largest values of exp(A ik )ˆ µ y k xi .
The above procedure avoids enumerating all n b entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube.
The maximum of the hypercube is found efficiently by searching each dimension independently.
Note that each dimension represents one of the b edges for node u i .
We begin with the assumption that M G , the maximum weight b-matching on G, is unique.
Moreover, W(M G ), the weight of M G , is greater than that of any other matching by a constant ǫ.ǫ = W(M G ) − max M񮽙 =MG W(M )Let T be the unwrapped graph of G from node u i .
An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking.
Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T .
Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to u i in the original graphu 1 v 1 v 2 v 3 v 4 u 2 u 2 u 2 u 2 u 3 u 3 u 3 u 3 u 4 u 4 u 4 u 4 v 2 v 3 v 4 v 2 v 3 v 4 v 2 v 3 v 4 v 1 v 3 v 4 v 1 v 3 v 4 v 1 v 3 v 4 v 1 v 2 v 4 v 1 v 2 v 4 v 1 v 2 v 4 v 1 v 2 v 3 v 1 v 2 v 3 v 1 v 2 v 3Figure 2: Example unwrapped graph T of G at 3 iterations.
The matching˜MTmatching˜ matching˜MT is highlighted based on MG from Figure 1.
Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do.
One possible path PT is highlighted, which is discussed in Lemma 1.3.
G.
The belief at iteration d of node u i during BP on G is equivalent to the true max-marginal of r on T of depth d [10,8].
In this case, the true max-marginal corresponds to the true maximum weight b-matching.
All maximum weight b-matchings on T are imperfect since leaf nodes are unable to complete matchings.
We relax the definition of b-matching on trees to be that all non-leaf nodes are b-matched.
Let M T be the maximum weight b-matching on T .
See Figure 2 for an example of a b-matching on an unwrapped graph.
Theorem 1.
Belief propagation on G converges for any node in Ω(n) iterations.
In other words, maximizing the belief at the root node r of an unwrapped graph T of depth Ω(n) is equivalent to maximizing the belief of r's corresponding node u i in the bipartite graph G.Formally, when d = Ω(n), M G (u i ) = M T (r).
Our proof will be structured as follows: Start with the contradiction to Theorem 1: the maximum setting of r is different from that of u i .
By modifying M T , the optimal b-matching on T to include edges from M G , we get a b-matching on T with higher weight.
We modify M T by finding a path in T that alternates between edges exclusively in M T and a mapped representation of M G .
Taking the complement of this path provides the new b-matching.
We bound the weight of the path by decomposing it into cycles and show the contradiction is impossible once d is large enough, which gives a bound on iterations to convergence.
We next give the proof in full detail.Proof.
First we introduce a lemma on the structure of b-matchings on trees.
Lemma 1.1.
For any non-leaf vertex v in T , we refer to the subtree rooted by v as T v .
The intersection of T v and M T is either case (1) or (2):(1) the maximum weight b-matching of the subtree rooted at v (2) the maximum weight b-matching such that v only has degree b − 1.
Proof.
For case (1), if the edge connecting v to its parent is not in M T , the subtree rooted at v is disjoint from the rest of the tree.
Therefore, any suboptimal b-matching on v's subtree could be improved by replacing it with its optimal b-matching.
For case (2), if the edge connecting v to its parent is in M T , v already has one incident edge outside of its subtree.
Thus, when considering only its subtree, v only has b − 1 matches available for its children.Since the edge connecting v to its parent is either in M T or not, these two cases cover all possibilities.Consider the following contradiction to Theorem 1.
Contradiction 1.
For all d, M G (u i ) 񮽙 = M T (r)Let˜MLet˜ Let˜M T be the mapped representation of M G on T .
Contradiction 1 can be rewritten as˜Mas˜ as˜M T (r) 񮽙 = M T (r).
Lemma 1.2.
For any two distinct b-matchings on T , M 1 and M 2 such that M 1 (r) 񮽙 = M 2 (r), there is an alternating path P T on T from a leaf to the root r then to another leaf.
Here, alternating path means that for any two adjacent edges e i , e j ∈ P T , if e i ∈ M 1 then e j ∈ M 2 and vice-versa.
Proof.
We construct P T in two halves.
For both constructions, we start at the midpoint, r, and go to two different leaf nodes.
Each path proceeds down the tree by choosing alternating edges from M 1 and M 2 .
The first half of P T starts with an edge in M 1 .
Since M 1 (r) 񮽙 = M 2 (r), we know there is a least one edge in M 1 (r) that is not in M 2 (r).
Select one of these edges and add it to P T .
We continue from the endpoint of the added edge to the next vertex v. Add an edge to one of v's children that is in M 2 (v) \ M 1 (v).
Such an edge must exist because the parent edge of v was in M 1 , but not in M 2 , and according to Lemma 1.1, v in M 1 can only have b − 1 matched children, and v in M 2 must have b matched children.Therefore, v must have at least one child edge in M 2 that is not in M 1 .
We add one of these edges then move on to its endpoint to a new vertex.
Now we add an edge that is in M 1 but not in M 2 .
Such an edge must exist due to the converse argument of the previous paragraph.The second half of P T starts with an edge in M 2 .
As above, we defined M 1 and M 2 so they are different at the root, so such an edge must exist.
We can then apply the same arguments as above to show how to select edges down to the final leaf node.We concatenate the two halves of P T obtaining a path from one leaf to another.
P T alternates between edges in b-matchings M 1 and M 2 .
We apply Lemma 1.2 to b-matchings M T and˜Mand˜ and˜M T to form a path P T that alternates between edges iñ M T and M T .
Next we will analyze the weights of edges in P T by decomposing P T into cycles.
Lemma 1.3.
Let P G be the path on G that visits the corresponding nodes that P T traverses on tree T (See Figure 4).
P G can be decomposed into a set of cycles, C = {c 1 , . . . , c τ }, such that τ ≥ d−1 n , and a remainder path ρ of length at most 2n.
Proof.
Start with C = {}.
As we traverse P G on G from one end to the other, once the length of P G is greater than 2n, the Pigeonhole Principle tells us at least one node will be visited more than once.
The first time this occurs, we have a cycle.Each time we discover a cycle c k , we remove the cycle from P G , add it to C, and continue on P G .
Note that removing cycles does not break the continuity of P G because cycles begin and end on the same node.
See Figure 4 for an example of this decomposition.Since the longest possible cycle is of length 2n (one that visits every node in G) and P G is of length 2d − 1, we know that τ , the number of cycles in P G is:τ ≥ 2d − 1 2n > d − 1 n .
After fully pruning P G , the remainder path, ρ must be continuous on G.
Since there is no cycle in ρ, it must be of length less than 2n.
Consider one of these cycles, c k ∈ C. Edges in c k are either members of M G or mapped edges from M T .
We are interested in the difference of the weights between these two subsets of c k .
For each c k , consider a modification of M G along cycle c k , which will produce b-matching M c k .
Starting with M G , toggle the matching of edges in c k .
Consequently, edges that were matched in M G are free in M c k and edges that were free are matched.
More importantly, the edges that were free were members of M T .
Forming M c k in this manner preserves a valid b-matching; each node in c k loses a matched edge and also gains another thereby preserving its total degree.Because M c k is identical to M G except on c k , we can bound the weight difference of the two b-matchings in c k by comparing the weight of M G and M c k .
The difference between the weight of M G and any other bmatching on G is at least ǫ.
Multiplying this minimum difference of weight for all τ possible cycles, we have:W(C ∩ M G ) − W(C \ M G ) ≥ τ ǫ ≥ (d − 1)ǫ n(8)To account for ρ, the remainder of P G , we create another cycle c ρ that is a modified version of ρ.
Path ρ is of even length path.
Without loss of generality, ρ begins with an edge from M G and ends with an edge from M T .
To create cycle c ρ , remove the last edge, which is from M T , and replace it with an edge back to the first node.This creates cycle c ρ , and we use the same method as with the cycles in C to bound the weights.W(c ρ ∩ M G ) − W(c ρ \ M G ) ≥ ǫSince c ρ replaced an edge from ρ, we add terms to the bound by pessimistically assuming the edge with least weight completed c ρ , and the uncounted edge from ρ had the most weight of M T .
W(ρ ∩ M G ) − W(ρ ∩ M T ) ≥ ǫ + min e∈E W(e) − max e∈MT W(e).
(9)We reconstruct P G = C ∪ρ and by summing Equations (8) and (9) we have the following bound:W(P G ∩ M G ) − W(P T ∩ M T ) ≥ dǫ n + min e∈E W(e) − max e∈MT W(e).
If the difference between these two weights is positive, there exists some b-matching on T , specifically M T . . . with edges in P T toggled, that has a higher weight than M T .
This should be impossible because M T is defined as the optimal b-matching.
This occurs whenu 2 v 2 u 4 v 1 u 1 v 3 u 4 v 1 u 3 First cycle (a) PG u 4 v 3 u 1 v 1 (b) c1 . . . u 2 v 2 u 4 v 1 u 3 (c) PG \ c1d ≥ n ǫ max e1,e2∈E W(e 1 ) − W(e 2 ) .
We can consider ǫ and the maximum difference between weights as constants.
If d = Ω(n), Contradiction 1 always leads to this impossible circumstance, and therefore Theorem 1 is true.The bound in this proof is for the worst-case.
In our experiments, the maximum and minimum weights had little effect and the ǫ term only changed convergence time noticeably if its value was near zero.
Due to finite numerical precision, this may appear as a tie between different settings, which often causes BP to fail [11].
We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.
We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN.
Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn 3 ) time [2].
The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations.
So, its overall running time is also O(bn 3 ).
We ran both algorithms on randomly generated bipartite graphs of 10 ≤ n ≤ 100 and 1 ≤ b ≤ n/2.
We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.1 Available at: http://www.math.uni-augsburg.de/opt/goblin.htmlThe GOBLIN library is C++ code and our implementation 2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz.
Pentium 4 processor.In general, the belief propagation runs hundreds of times faster than GOBLIN.
Figure 5 shows various comparisons of their running times.
The surface plots show how the algorithms scale with respect to n and b.
The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following the GOBLIN line).
Since both algorithms have running time O(bn 3 ), when we fix b = 5, we get a cubic curve.
When we fix b = n/2, we get a quartic curve because b becomes a function of n.
One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification.
Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy.
The algorithm starts by computing an affinity matrix between all data points.
For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate.
This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.Instead, using b-matching enforces uniform connectivity across the training and testing points.
For example, assume the number of testing points and training points is the same and we perform b-matching.
Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points.
This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data.
This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN.
This can occur in situations where training data comes from a different source than the testing data.In both algorithms, we compute a bipartite affinity graph between a set of training points and a set of testing points.
We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to obtain classifications).
We use either KNN or weighted b-matching to prune the graph, then we classify each test point based on the majority of the classes of the neighbors.
In our experiments, we use negative Euclidean distance as our affinity metric.
We created synthetic data by sampling 50 training data points from two spherical Gaussians with means at (3, 3) and (−3, −3).
We then sampled 50 testing data points from similar Gaussians, but translated along the x-axis by +8.
This is a severe case of translation where it is obvious that KNN could be fooled by the proximity of the testing Gaussian to the training Gaussian of the wrong class.
Figure 6 shows the points we sampled from this setup.
As expected, KNN could only achieve 65% accuracy on this data for the optimal setting of k, while b-matching classified the points perfectly for all settings of b.
We sampled the MNIST digits dataset of 28 × 28 grayscale digits.
For each of the digits 3, 5, and 8, we sampled 100 training points from the training set and 100 from the testing set.
We average accuracy over 20 random samplings to avoid anomalous results.
We cross-validate over settings of b and k in the range [1,300] and save the best accuracy achieved for each algorithm on each sampling.We examine the case where training and testing data are collected under drastically different conditions (e.g. non-IID sampling) by simulating that the testing digits are printed against various backgrounds.
We replace all white (background) pixels in the testing images with these backgrounds and attempt classification using both algorithms on these new datasets.
Figure 7 contains examples of digits placed in front of backgrounds.The results show that b-matching outperforms knearest-neighbors, sometimes by significant margins.
On the unaltered white background and the diagonal lines, both algorithms classify at slightly higher than 90% accuracy.
On the grid, white noise, brushed metal, wood and marble backgrounds, b-matching is invariant to the transformation while k-nearestneighbors suffers a drop in accuracy.
Figure 7 contains a plot of both algorithm's average accuracies for each background type.Since the background replacement is like a translation of the image vectors that preserves the general shape of the distribution, b-matching is a more useful tool for connecting training examples to testing points than knearest-neighbors.
We proved the convergence of BP for maximization on a graphical model for the weighted b-matching problem.
We provided an efficient method to update the n b -length messages.
The practical running time of this algorithm is hundreds of times faster than the classical implementations we have found.
This makes b-matching a more practical tool in machine learning settings and lets us tackle significantly larger datasets.Furthermore, by formulating weighted b-matching in this manner, we can take advantage of the inherent parallel computation of belief propagation.
In addition to the empirical evidence of a faster constant on the asymptotic running time, the procedure could be distributed over a number of machines.
For example, n servers and n clients could run this algorithm in parallel to optimize throughput.Theoretically, the convergence of loopy BP on this graph and the ability to update the n b length messages are pleasant surprises.
We have tried to generalize this method to all graph structures, not just bipartite graphs, but found that there were cases that led to oscillation in the belief updates.
This is due to the fact that cycles can be both even and odd in general graphs.
Generalizing to such cases is one future direction for this work.In addition to being limited to bipartite graphs, belief propagation methods have difficulty with functions that have multiple solutions.
This is why we assume there is a unique solution in the proof.
In a way, our convergence bound still holds when there are multiple solutions, in the sense that the ǫ value is in the denominator, so the limit as ǫ goes to zero implies infinite iterations to convergence.
Belief propagation cannot handle these ties in the function due to the fact that the information being passed between variables is only the maximum value, and no information about the actual variable values is sent.
Therefore, the algorithm has no mechanism to distinguish between one maximum and the other.
We are currently investigating if tree-based reparameterization techniques or other graphical model inference methods can solve the multiple-optimum or non-bipartite b-matching cases.
We thank Stuart Andrews, Andrew Howard and Blake Shaw for insightful discussions.
Supported in part by NSF grant IIS-0347499.
