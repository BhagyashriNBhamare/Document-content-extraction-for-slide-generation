We compute a common feature selection or kernel selection configuration for multiple support vector machines (SVMs) trained on different yet interrelated datasets.
The method is advantageous when multiple classification tasks and differently labeled datasets exist over a common input space.
Different datasets can mutually reinforce a common choice of representation or relevant features for their various classifiers.
We derive a multi-task representation learning approach using the maximum entropy discrimination formalism.
The resulting convex algorithms maintain the global solution properties of support vector machines.
However , in addition to multiple SVM classifica-tion/regression parameters they also jointly estimate an optimal subset of features or optimal combination of kernels.
Experiments are shown on standardized datasets.
In applied settings, many supervised datasets are available over a common input data-type (be it text, images, or sequence data) and represent different problem tasks (i.e. various classification or regression scenarios).
Multi-task learning or meta-learning leverages these many datasets synergistically, aggregating them and augmenting the effective size of the total training data (Baxter, 1995;Thrun & Pratt, 1997;Caru- ana, 1997).
This can lead to improvement in overall classification and regression performance compared to learning the tasks in isolation.
We elaborate metalearning in a support vector machine setting and focus on the case where many small datasets over different tasks select one common underlying representation.
More specifically, we discuss optimizing the representation either as a feature selection configuration (Jebara & Jaakkola, 2000;Weston et al., 2000) or as a convex kernel combination ( Lanckriet et al., 2002;Cristianini et al., 2001).
This is done jointly while estimating a support vector classification or regression machine by using the maximum entropy discrimination (MED) framework.
While previous efforts suggest finding these representations for a single task, recent theoretical results (Baxter, 2000;Ben-David & Schuller, 2003) suggest that improvements are possible with multi-task learning.
This article combines the above motivations into a joint multi-task feature and kernel selection SVM framework.This paper is organized as follows.
Section 2 summarizes maximum entropy discrimination and how it generates the support vector machine.
We then discuss augmenting the SVM with feature selection in Section 3 and kernel selection in Section 4 yet only in isolated learning settings.
In Section 5 we upgrade feature selection to a meta-learning scenario where many classifiers share one common feature selection configuration.
Section 6 similarly upgrades kernel selection.
Experiments are interleaved within each section when appropriate.
Section 7 ends with a discussion.
The maximum entropy discrimination MED formalism introduced in ( Jaakkola et al., 1999) is a flexible generalization of support vector machines.
MED produces a solution that is a distribution of parameter models P (Θ) rather than finding a single parameter setting Θ * .
It is this characteristic that makes it straightforward to augment MED solution distributions to be joint densities over not only parameters of a single classifier, but several classifiers, representations, feature selection and kernel selection configurations.
In other words, one may consider M support vector machines sharing a common feature selection configuration s and readily solve for P (θ 1 , . . . , θ M , s) in the maximum entropy formalism.
The implementation of these MED extensions still produces a large margin support vector while also estimating additional parameters all within a globally optimizable convex program.We now summarize the maximum entropy discrimination formalism in its simplest form.
Assume we are given training examples X t ∈ R d with their corresponding labels y t ∈ {±1} for t = 1 . . . T .
We want to find a linear discriminant function L(X; Θ) = θ T X + b whose sign agrees with this labeled training set.
The discriminant is specified by Θ = {θ, b} containing both a linear parameter vector θ as well as a scalar bias value b.
An SVM finds a specific Θ * setting of these two deterministic quantities that agrees with the labeling, in other words y t L(X t ; Θ * ) ≥ γ t ∀t where γ t are scalars typically set to unity, i.e. γ t = 1.
This is done while ensuring that the magnitude 1 2 θ 2 is kept small for regularization.
In MED, however, we solve for a distribution over solutions P (Θ) such that the expected value of the discriminant under this distribution agrees with the labeling.
In addition to finding a P (Θ) that satisfies classification constraints in the expectation, MED regularizes the solution distribution P (Θ) by either maximizing its entropy or minimizing its relative entropy towards some prior target distribution P 0 (Θ).
We use the relative Shannon entropy given by D(P P 0 ) = Θ P (Θ) ln P (Θ)/P 0 (Θ)dΘ.
Note that minimizing relative entropy is more general since choosing P 0 (Θ) uniform gives maximum entropy.
Thus, MED solves the constrained optimization problemmin P (Θ) D(P P 0 ) s.t. Θ P (Θ)[y t L(X t ; Θ) − γ t ] ≥ 0 ∀tThe solution for the posterior P (Θ) is straightforward ( Jaakkola et al., 1999) and is given by the usual maximum entropy methodP (Θ) = P 0 (Θ) Z(λ) e P t λt[ytL(Xt;Θ)−γt] .
Here Z(λ) is obtained by normalizing P (Θ) and the optimal setting of the λ Lagrange multipliers (λ t for t = 1 . . . T ) is found by maximizing the concave objective function J(λ) = − ln Z(λ).
Given λ, the solution distribution P (Θ) is fully specified.
It is then straightforward to use this distribution for predicting the label of a new exemplar X viâ y = sign( Θ P (Θ)L(X; Θ)dΘ).
Furthermore, often the effect of the distribution P (Θ) can be mimicked by a deterministic parameter setting on our discriminant function since we can solve the integrals analytically without computational effort.
Interestingly, the above framework exactly reproduces support vector machines.
We simply assume the prior distribution factorizes into a prior over the vector parameters and a prior over scalar bias, P 0 (Θ) = P 0 (θ)P 0 (b).
The first is a white zero-mean Gaussian of the form P 0 (θ) = N (θ|0, I) which encourages linear models of low magnitude (a traditional regularization principle in the literature).
The second is a non-informative (i.e. flat) prior P 0 (b) = N (b|0, ∞) indicating that any scalar bias is equally probable a priori.
The resulting objective function J(λ) is identical to the SVM dual optimization problemJ(λ) = t λ t − 1 2 t,t ′ λ t λ t ′ y t y t ′ X T t X t ′subject to non-negativity constraints on the λ t values (since the expectation constraints in the maximum entropy problem used greater-than inequalities).
Furthermore, the non-informative prior yields the equality constraint t λ t y t = 0.
The above can be readily maximized using quadratic programming or axis-parallel methods that update only a limited number of Lagrange multipliers at a time while others are fixed.
The final decision rule is given by Θ P (Θ)L(X; Θ)dΘ = t y t λ t X T t X + ˆ b where the expected biasˆbbiasˆbiasˆb is found via the Karush-Kuhn-Tucker conditions as usual.
The above is readily kernelizable as well simply by replacing all inner products X T t X t ′ with kernel function evaluations k(X t , X t ′ ).
It is now straightforward to obtain variations on support vector machines by considering other prior distributions instead of the Gaussians above.
We can also augment the solution distribution P (Θ) such that it is a density over other variables in addition to the linear classifier's θ, b parameters on their own.
For instance, we can consider a non-separable support vector machine if we make the γ t scalars variable and consider solving for an MED distribution that is a joint function of both γ and the parameters Θ as P (Θ, γ).
Assume the prior distribution factorizes over margins as follows P 0 (Θ, γ) = P 0 (Θ) t P 0 (γ t ).
Here, we choose P 0 (γ t ) ∝ exp(−c + cγ t ), γ < 1 as an exponential distribution which favors large margins yet also allows negative margin values (i.e. miss-classifications).
The solution distribution P (Θ, γ) is found again as in the above general form (a product of the prior P 0 (Θ, γ) and exponentiated constraints):P (Θ, γ) = P 0 (Θ, γ) Z(λ) e P t λt[ytL(Xt;Θ)−γt] .
(1)The resulting SVM-like concave maximization problem emerges from the partition function Z(λ).
This new objective function J(λ) = − ln Z(λ) equalst [λ t + ln(1 − λ t /c)] − 1 2 t,t ′ λ t λ t ′ y t y t ′ X T t X t ′ .
The above effectively adds barrier functions preventing λ t 's from growing beyond the regularization parameter c.
The decision rulê y = Θ,γ P (Θ, γ)L(X; Θ)dΘdγ remains identical.
We now continue this line of reasoning and the notion of augmented solution distributions to endow support vector machines with other interesting properties beyond non-separability.
For instance, we can now readily deal with feature and kernel selection.
Feature selection for support vector machines was discussed in earlier work in maximum entropy discrimination (Jebara & Jaakkola, 2000) as well as in deterministic SVM settings ( Weston et al., 2000).
P 0 (Θ, γ) = P 0 (θ)P 0 (b) d P 0 (s d ) t P 0 (γ t ).
We again assume a Gaussian prior over θ, a non-informative prior over b, and exponential priors over γ t (although other choices are possible).
The natural choice of a prior over switches s d is a Bernoulli distribution given byP 0 (s d ) = ρ s d (1 − ρ) 1−s d .
Here the prior is controlled by ρ which affects the likelihood of including any given feature; ρ = 1 preserves all features and would simply give us an SVM.
Meanwhile smaller ρ values in the prior encourage the SVM linear model solution to have many zeros and use fewer features.
The solution distribution is found using Equation 1 yet computing the partition function now involves normalizing over the switch settings as well as over the parameters θ and b.
This yields an alternative J(λ) objective functiont ln(e λ − e λ λ t /c) − d ln 1 − ρ + ρe 1 2 ( P t λtytX t,d ) 2which is maximized subject to t λ t y t = 0.
The J(λ) above can be maximized iteratively via NewtonRaphson or axis-parallel methods (i.e. iteratively locking all Lagrange multipliers except for 1 or 2 and searching for their optimal setting).
Clearly, if ρ is held at unity in the above, we once again obtain exactly the support vector machine optimization problem.
However, smaller settings of ρ affect the optimization of Lagrange multipliers values and ultimately provides a sparsified linear model to use in the dot product with each input datum vector.
Note that this J(λ) remains a concave objective with convex constraints and hence can be solved without local minima problems.
It yields the largest margin linear support vector machine while also sparsifying features.
Given the final setting of the (non-negative) Lagrange multipliers andˆbandˆandˆb (the expected value of b via the Karush-Kuhn Tucker conditions), we compute our discriminant function outputˆyoutputˆ outputˆy as the sign of θ,b,s P (θ, b, s)L(X; θ, b, s) which simplifies to:d ρ t λ t y t X t,d ρ + (1 − ρ) exp(− 1 2 [ t λ t y t X t,d ] 2 ) X d + ˆ b.In the above, X t,d refers to the d'th dimension of the vector representing the t'th datum in our training dataset.
Note how we have terms that are attenuated nonlinearly with smaller ρ settings in their interaction with the query datum X.
We use W d to denote the terms multiplying each X d in the parentheses.
These behave as attenuated weights which are driven close to zero when ρ is small.
While the model is still a linear classifierˆyclassifierˆ classifierˆy = sign(d X d W d + ˆ b), most of the dimensions of X d get multiplied with vanishingly small scalar W d weights and will be ignored (selected out).
To evaluate feature selection with SVMs in an isolated learning setting, the UCI splice-site dataset was used to classify gene sequences as intron-exon or exonintron sites.
Each datum in the dataset is a sequence of 60 nucleotides (A, G, C, T ) which are represented as 4-tuple codes according to A = (0001), C = (0100), G = (0010) and T = (0001).
Codes were mixed with weights to represent uncertain symbols such as "G or T ".
Therefore, our final input space's dimensionality is D = 240.
Of a total of 1535 sequences, 200 were used for training an SVM and the remaining were used for testing.
Figure 1 depicts the classification accuracy on test data of the feature selection SVM at various settings of the regularization parameter c as well as the sparsification parameter ρ.
Note that at ρ = 1 we have a traditional support vector machine with no feature selection.
Clearly, some feature selection is advantageous and improves generalization increasing overall accuracy from 92% up to approximately 96%.
In Figure 2 we plot the 240 weights W d of the resulting linear model for both a traditional support vector machine when ρ = 1 and a feature selection SVM with ρ = 1e − 4.
Feature selection can be clearly seen in the latter where many weights W d are effectively 0.
The above feature selection technique is straightforward to derive for support vector machine regression and is not limited to classification (Jebara & Jaakkola, 2000).
This is actually the case for all subsequent derivations and SVM extensions in this article.
We next go beyond linear feature selection and consider the kernel selection problem.
Another crucial issue for support vector machines is choosing the kernel function.
Kernels introduce different nonlinearities into the SVM problem by mapping input data X implicitly into Hilbert space via a function φ(X) where it may then be linearly separable.
While the explicit computation of the features or the mapping to Hilbert space may be unwieldly, a support vector machine only requires inner products of these φ(X) features.
A kernel is an efficient way to compute such an inner product and provides the same scalar output k(X t , X t ′ ) = φ(X t ) T φ(X t ′ ).
It is straightforward to kernelize the SVM and related learning machines to accommodate nonlinear classification and to potentially handle non-vectorial inputs by swapping all inner products X T t X t ′ in the formulation with kernel function evaluations k(X t , X t ′ ) (subject to some caveats, i.e. Mercer's condition).
In fact, in implementing a support vector machine and optimizing for the Lagrange multipliers (for instance, via quadratic programming), it is natural to build a T × T Gram matrix K whose entries are given by the kernel function evaluated over all pairs of data-points K t,t ′ = k(X t , X t ′ ).
Different kernels will accommodate different nonlinear mappings and the performance of the resulting SVM will often hinge on the appropriate choice of the kernel.However, searching for different kernels either via trialand-error or other exhaustive means can be a computationally daunting problem (Chapelle & Vapnik, 1999).
This search is particularly difficult if we also want to consider combining kernels in convex combinations (a continuous optimization problem) to mix various nonlinear mappings in search of the optimal Hilbert space.
More efficient kernel selection approaches involve either directly manipulating entries of the Gram matrix or, alternatively, searching over a convex combination of user-specified kernel functions ( Lanckriet et al., 2002;Cristianini et al., 2001).
In this article, we will approach the problem of kernel selection by estimating the optimal weighted combination of several prototype or base kernels provided by the user.
It is well known that a legitimate kernel can be created by convex combination of kernels as long as they individually satisfy Mercer's condition.
We will derive this additional estimation problem in a straightforward manner using the maximum entropy discrimination framework.We cast the kernel selection problem by estimating a positive linear combination of a set of D kernel functions k d (.
, .)
where d = 1 . . . D. Excuse the abuse of notation as we recycle variables and use d to index into the set of kernel or Hilbert space mappings under consideration (as opposed to indexing into the dimensionality of the input space).
We wish to estimate the best combination of these base kernels while jointly finding the optimal support vector machine classifier.
In other words, we will find a resulting final kernel of the form: k(.
, .)
= d W d k d (.
, .)
where W d are nonnegative weights.
In fact, we would also like to have many of these W d weights drop close to zero to fully reject the contribution of some kernels that are not useful for the given classification task.We approach the problem by noting that different combinations of kernels correspond to different weightings of their corresponding mappings φ d (.)
for d = 1 . . . D.
We are uncertain which mapping to use and consider using them all by concatenating all mappings into an augmented Hilbert space.
Consider the resulting discriminant function: L(X; Θ) = d s d θ T d φ d (X) + b.(Θ, γ) = P 0 (b) d P (θ d )P (s d ) t P (γ t ).
Here, each P (θ d ) is individually a zero-mean white Gaussian distribution over its vector θ d (each θ d vector can potentially be of a different dimensionality).
For simplicity, we will also use an informative prior for the bias given by P 0 (b) = N (b|0, σ 2 ).
This removes the equality constraint t λ t y t = 0 and replaces it with a quadratic penalty (to be derived shortly).
The priors on margins γ t are as before.
Given the prior, we recover the posterior distribution P (Θ, γ) via Equation 1.
We next turn to the computation of the partition function which normalizes this P (Θ, γ).
We will simplify it by considering the contribution of each of the variables θ, s, b, γ to the partition individually as followsZ(λ) = Θ P 0 (Θ)e P t λtytL(Xt;Θ) γ P 0 (γ)e − P t λtγt = Z Θ (λ)Z γ (λ)We can then further simplify the Z Θ (λ) term as θ,s P 0 (θ, s)eP t λtyt P d s d θ T d φ d (Xt) b P 0 (b)e P t λtytb .
The left integral evaluates Z θ,s (λ) while the right integral evaluates Z b (λ).
The computation of the bias contribution is straightforward producing Z b (λ) = e 1 2 σ 2 ( P t ytλt) 2 .
Note that if we let σ → ∞, the partition function diverges unless we have the constraint t y t λ t = 0 as in the non-informative prior case.
The remaining term involves the contribution of all the d = 1 . . . D models and the binary switches, and is given byZ θ,s (λ) = θ,s d P 0 (s d )P 0 (θ d )e P t λtyt P d s d θ T d φ d (Xt) = d 1 s d =0 P 0 (s d )e s d 1 2 P t,t ′ λtλ t ′ yty t ′ k d (Xt,X t ′ ) .
The resulting overall MED objective function is then the negated logarithm of the product of all the partition function terms:J(λ) = t λ t + ln(1 − λ t /c) − σ 2 2 t y t λ t 2 − d ln 1 − ρ + ρe 1 2 P t,t ′ λtλ t ′ yty t ′ k d (Xt,X t ′ )which is maximized only over non-negativity constraints on the Lagrange multipliers.
This is straightforward to do via Newton-Raphson or axis-parallel updates on individual values of λ t .
Once again, if we select ρ = 1, we note that the above kernel selection problem degenerates since all logarithms cancel with the exponential functions and the optimization is equivalent to the case where all kernels are summed equally.
This means our final kernel is just the total of the base kernels k(.
, .)
= d k d (.
, .)
.
However, in the cases where ρ is less than unity, we note that the objective function involves a nonlinear combination of the quadratic terms from the different kernels, i.e. a nonlinear mixing of multiple support vector machine quadratic cost problems.To compute the actual discriminant function, the MED recipe suggests taking the expectation under P (Θ, γ) of the discriminant function L(X; Θ).
This gives the following decision rule:ˆ y = sign t y t λ t d ˆ s d k d (X t , X) + ˆ bThe formula involves the expected value of the biasˆb biasˆbiasˆb and the expected value of the binary switchesˆsswitchesˆ switchesˆs d .
It also implicitly involves the expected value of the modelsˆθmodelsˆ modelsˆθ d but these are only written in terms of the Lagrange multipliers.
In the non-informative case, the expected value of the bias is given by the mean of the Gaussian posterior P (b) and is merelyˆbmerelyˆmerelyˆb = σ 2 t y t λ t .
The expected value of each switch (or the weight of each kernel in our SVM problem) isˆ s d = ρ ρ + (1 − ρ) exp(− 1 2 t,t ′ λ t λ t ′ y t y t ′ k d (X t , X t ′ )).
Since these are actually positive scalars, the above final formula for the decision rule involves a positive combination of the base kernels.
We can immediately rewrite the decision rule as a standard SVM classifier yet using the aggregated kernel k(.
, .)
= d ˆ s d k d (.
, .)
.
Another benefit is that, as in the feature selection case, manyˆsmanyˆ manyˆs d values will be vanishingly small (particularly as we use smaller values of ρ).
For computationally efficiency in practice, we round off small values ofˆsofˆ ofˆs d to zero and avoid computing their corresponding kernel evaluations altogether in the final classifier's formula.
This indicates that some base kernels will be pruned away completely from our final combination while the remaining ones will be mixed with various weights.
In practice, we assume all base kernels are normalized and k d (X t , X t ) = 1 since the switches combining them are on the same scalê s d ∈ [0,1].
One standard way to normalize the base kernels is to replace them as follows:k d (X t , X ′ t ) ← k d (X t , X ′ t ) k d (X t , X t )k d (X t ′ , X t ′ )Finally, the aggregated kernel is legitimate since positive combinations of Mercer base-kernels are Mercer.We evaluated the kernel selection technique on the UCI Isolet dataset.
We used 200 training examples and 600 testing examples.
The task is to compute a class label representing which letter of the alphabet was spoken from a vector of auditory features.
This is a 26-class multi-class classification problem.
We convert this problem into 26 binary classification problems by considering the one versus rest scenario.
At this point, the standard approach is to train individual support vector machines on these 26 binary classification problems.
To help improve accuracy, we investigate finding the best combination of different kernels on the auditory features.
Each binary classifier optimizes the kernel selection individually by choosing a combination of polynomial kernels and radial basis function kernels.
The polynomials used were of order 1,2,3,4 and the RBF kernels used were of standard deviation 10,1,0.1, and 0.01.
Thus, a total of 8 base kernels were combined to form a final aggregate kernel for each classifier in isolation.
We show the test error of the resulting classifiers in terms of the total binary error rate, in other words, the sum of all the errors the individual binary classifiers made.
This is a pessimistic error estimate since other methods may be used to reduce errors by fusing the output of the 26 binary classifiers (for example via error-correcting codes).
Figure 3 summarizes the total error rate for the kernel selection method as we explore various settings of the ρ and c parameters.
At ρ = 1, we have the SVM case which assumes that our aggregated kernel is the sum of all 8 base kernels with equal (unity) weight.
Lower values of ρ prune away poor kernels and reduce error by selecting the subset of kernels which are more appropriate for the task.
The fact that lowering ρ keeps improves accuracy suggests that pruning kernels improves performance and that one kernel is consistently outperforming the others.
So far, we have seen a representational aspect of the learning process, be it a feature selection on the input space or a kernel selection, yet it was only driven by a single task and model.
However, such types of representations may be learned for multiple tasks more powerfully than in isolation.
We will now explicate the case where many tasks are present requiring different classification models yet dealing with the same input space which is consistently corrupted by nuisance or irrelevant features.
For instance, we may have a dataset of face images that classifies each face as male or female and in another dataset, face images are classified as child or adult.
These datasets can synergistically discover that only the pixels which are occupied by facial imagery are useful for either task and that pixels corresponding to background imagery should be ignored.
This intuition can be formalized via the theoretical work of (Baxter, 2000) and (Ben-David & Schuller, 2003).
The latter discusses how generalization can benefit from a multi-task or meta-learning situation when several classifiers are F -related and involve controlled changes from a base classifier.
Θ = {θ 1 , . . . , θ M , b 1 , . . . , b M , s}.
Each of the M discriminant functions can then be written as:L(X; Θ m ) = L(X; θ m , b m , s) = d s d θ m,d X d + b m .
Unlike before, the MED classification constraints are now over many datasets as follows:Θ,γ P (Θ, γ)[y m,t L(X m,t ; θ m , b m , s) − γ m,t ] ≥ 0 ∀m, tThere are m T m total such constraints from our M classification problems and we therefore anticipate having m T m non-negative Lagrange multipliers λ m,t .
Minimizing the relative entropy of P (Θ, γ) to a prior P 0 (Θ, γ) subject to the above constraints again produces the classical maximum entropy solution, a product of the prior and the exponentiated constraints:P (Θ, γ) = P 0 (Θ, γ) Z(λ) e P m,t λm,t[ym,tL(Xm,t;Θm)−γm,t] .
We select a prior that factorizes as followsP 0 (Θ, γ) = d P 0 (s d ) m P 0 (θ m )P 0 (b m ) t P 0 (γ m,t ).
Due to the lack of a priori knowledge about how tasks interact, we start with a factorized prior over tasks yet will eventually find a posterior that need not remain factorized afterwards (starting with non-factorized priors may be possible as in related work by (Evegniou & Pontil, 2004)).
We again utilize zero-mean white Gaussian priors for models, Bernoulli priors for switches, zeromean Gaussian priors for biases and exponential priors for margins.
We can now readily evaluate the partition function.
Focusing on the contribution from the biases, we obtain Z b1,...,bM = m e 1 2 σ 2 ( P t ym,tλm,t) 2 .
We also have the contribution from the vector models and the single switch; this term Z θ1,...,θM ,s (λ) is θ1,...,θM ,sP 0 (s) m P 0 (θ m )e P m,t λm,tym,t P d s d θ m,d X m,t,d = d 1 s d =0 P 0 (s d )e s d 1 2 P M m=1 [ P Tm t=1 λm,tym,tX m,t,d ] 2 .
Multiplying all contributions of the partition function and taking the negative logarithm yields J(λ) asm,t λ m,t + ln(1 − λ m,t /c) − m σ 2 2 t y m,t λ m,t 2 − d ln 1 − ρ + ρe 1 2 P m [ P t λm,tym,tX m,t,d ] 2 .
The objective is maximized subject to non-negativity constraints and produces the optimal Lagrange multipliers.
To recover the final decision rule, each of the m = 1 . . . M classifiers is then computed from the sign of the expected m'th discriminant:Θ P (Θ)L(X; θ m , b m , s) = d ˆ s d ˆ θ m,d X d + ˆ b mwhere we have the following expected quantities from the solution distribution P (Θ):ˆ θ m = t y m,t λ m,t X m,t ˆ s d = ρ ρ + (1 − ρ) exp(− 1 2 m [ t λ m,t y m,t X m,t,d ] 2 ) ˆ b m = σ 2 t y m,t λ m,t .
Note that the meta-learning case performs no feature selection when ρ = 1 and there is no notion of multitask learning.
All models become optimized individually as separate SVMs and the switches are all unity.
Conversely, for smaller settings of ρ we will find a single feature selection configuration that achieves good classification performance for all models.The UCI Dermatology dataset was used to evaluate meta-learning with linear feature selection.
This is a 6-class dataset which was converted to 6 binary oneversus-rest classification problems.
Typically, individual SVMs would be trained on each binary classification.
If, however, we are to estimate a feature selection configuration for this dataset, we would like all 6 SVMs to share the same one.
This makes the problem a meta-learning one 1 .
The inputs here were 33-dimensional and we used 200 training and 166 testing data points.
Figure 4 reports the total binary classification error for the 6 tasks as we vary ρ and crossvalidate over c.
The dashed line depicts performance when each SVM for each binary classification task has its own feature selection configuration (i.e., the independent learning case).
Meanwhile, the solid line depicts the performance when each SVM has to share a common feature selection configuration (i.e., the metalearning case).
The upper left corner of the plot is the performance of an SVM when no feature selection whatsoever is used (there, meta-learning equals isolated learning).
Feature selection improves the SVM error while meta-learned feature selection improves results even further.
It is now immediate to derive the multi-task kernel selection case since it is merely a combination of the extensions from Section 3 to Section 4 and from Section 3 to Section 5.
We only write out the resulting objective function and the decision rule.
For brevity, we rewrite indexes without any commas, i.e. λ mt = λ m,t .
The concave objective J(λ) is then ρ exp(− 1 2 mtt ′ λ mt λ mt ′ y mt y mt ′ k d (X mt , X mt ′ ))which give us the weights on the combination of base kernels to use for multi-tasks.
A framework for feature and kernel selection in an SVM classification (and regression) setting was put forward.
However, here, features or kernel combinations are found from multiple synergistic problems.
If such problems have commonalities in terms of the features they should be sensitive to or in terms of the nonlinearities they require for good classification, they can mutually benefit from multi-task or metalearning.
Maximum entropy discrimination generated these novel extensions of support vector machines which were implemented as convex programs.
These avoid local minima as they simultaneously estimate classifier models and perform feature/kernel selection.
Preliminary empirical results were promising and encourage further empirical and theoretical work.
