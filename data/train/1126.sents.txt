It is widely accepted that the disproportionate scaling of transistor and conventional on-chip interconnect performance presents a major barrier to future high performance systems.
Previous research has focused on wire-centric designs that use parallelism, locality, and on-chip wiring bandwidth to compensate for long wire latency.
An alternative approach to this problem is to exploit newly-emerging on-chip transmission line technology to reduce communication latency.
Compared to conventional RC wires, transmission lines can reduce delay by up to a factor of 30 for global wires, while eliminating the need for repeaters.
However, this latency reduction comes at the cost of a comparable reduction in bandwidth.
In this paper, we investigate using transmission lines to access large level-2 on-chip caches.
We propose a family of Transmission Line Cache (TLC) designs that represent different points in the latency/bandwidth spectrum.
Compared to the recently-proposed Dynamic Non-Uniform Cache Architecture (DNUCA) design, the base TLC design reduces the required cache area by 18% and reduces the interconnection network's dynamic power consumption by an average of 61%.
The optimized TLC designs attain similar performance using fewer transmission lines but with some additional complexity.
Simulation results using full-system simulation show that TLC provides more consistent performance than the DNUCA design across a wide variety of workloads.
TLC caches are logically simpler than DNUCA designs, but require greater circuit and manufacturing complexity.
The disproportionate scaling of VLSI interconnect and transistor performance has been recognized as a key challenge for future high performance systems [17,35].
This problem manifests itself most strongly in global wires that communicate across a large fraction of a chip.
For example, sending a signal across a 2 cm die required only one to two clock cycles at the beginning of this decade [13], but will take over 25 cycles by its end for aggressively clocked processors [14,18].
The problem of slow global wires has prompted substantial microarchitectural research to reduce their impact on system performance [30,31].
For example, Kim et al. recently proposed a novel design for large on-chip level-2 caches, which are increasingly performance critical due to longer memory latencies, more pressing power constraints, and limited off-chip bandwidth [24].
Their Dynamic Non-Uniform Cache Architecture (DNUCA) is a physical organization that exploits the fact that closer cache banks can be accessed more rapidly than more distant banks.
DNUCA achieves impressive performance improvements over other alternatives, but introduces significant logical complexity, along with power and area inefficiencies.An emerging alternative approach to the slow global wire problem is to use on-chip transmission lines [8].
Transmission lines exhibit much lower latencies than conventional wires since their signalling speed is dominated by a relatively short inductive-capacitance (LC) delay rather than a series of a relatively large resistive-capacitance (RC) delays 1 .
The speed of the incident wave across a transmission line is analogous to the speed of a ripple moving across water in a bathtub, while the latency across conventional RC wires is analogous to changing the water level of the bathtub.Despite their substantial speed advantage-up to a factor of 30 by the end of the decade-transmission lines will not replace most conventional on-chip wires because they sacrifice significant bandwidth.
Transmission lines require very wide, thick wires and dielectric spacing to operate in the LC range, which are only available in the uppermost layers of a chip's interconnection metal stack.
These extremely sparse metal layers are best utilized for This work was supported by the National Science Foundation (CDA-9623632, EIA-9971256, EIA-0205286, and CCR-0324878), a Wisconsin Romnes Fellowship (Wood), and donations from Intel Corporation and Sun Microsystems, Inc.
Dr. Wood has a significant financial interest in Sun Microsystems, Inc.1.
In other words, the latency of a transmission line to the first order is determined by the speed of light in the dielectric surrounding the interconnect instead of the time to change the charge across the wire's capacitance.
the few long distance communication links whose latency can have a significant impact on overall system performance.In this paper, we explore using transmission lines for communication between the storage banks of large onchip caches and their central controllers.
We refer to these caches as Transmission Line Cache (TLC) designs.
By using long on-chip transmission lines, TLC achieves the following advantages over DNUCA:• TLC provides consistent high performance for a wide variety of workloads with different sized memory footprints because its entire storage is accessible within 16 cycles using low contention point-to-point links.
• TLC's simple logical design eases logical verification and integration with dynamic instruction schedulers.
• By eliminating repeaters and communicating through on-chip transmission lines that can be routed over the cache banks, TLC consumes 18% less substrate area than DNUCA and allows for more efficient layout.
• TLC reduces the power consumed within the communication network of a large on-chip cache.
However TLC does have the following disadvantages compared to DNUCA:• TLC's transmission line drivers and receivers require a greater circuit verification effort to ensure proper signalling in the noisy environments of future integrated circuits.
• TLC demands significantly more metal layers resulting in a higher per wafer manufacturing cost than the DNUCA design, which uses conventional interconnect.The rest of the paper is organized as follows.
Section 2 reviews the global wire problem and how the DNUCA design addresses it.
Section 3 discusses on-chip transmission lines and the technology assumptions we made for this study.
Section 4 describes the family of TLC designs.
Section 5 and Section 6 describe the methodology and results of our simulation experiments that compare the performance of TLC and DNUCA.
As previously mentioned, the delay of conventional interconnect relative to transistors is increasing as integrated circuits move to smaller geometries.
Global wires (> 1 mm) are particularly vulnerable because the RC delay of conventional interconnect grows quadratically with distance [42].
To keep wire delay linear with distance, designers insert repeaters to break long wires into multiple shorter segments.
However, increasing wire density and operational frequencies dictate an increasing number of repeaters.Overall, the use of repeaters for global communication leads to three key problems [17]:• Repeaters require a substantial amount of area for their large transistors.
• Repeaters necessitate disciplined floorplanning to allocate the necessary substrate area at the proper locations.
• Repeaters need many via cuts from the upper metal layers down to the substrate, which congest the interconnection layers below and reduce the overall wire bandwidth.Furthermore, more localized (< 1 mm) wire delay is also a significant factor in the design of on-chip caches.
For instance, current level-2 caches are divided into multiple smaller banks to optimize the individual bank's area/delay tradeoff [25].
While partitioning a cache into banks mitigates the impact of localized wire delay, the global wire delay to access the appropriate banks becomes the dominant factor as chips move to smaller geometries.
Kim et al. [24] showed that-for a 16 MB L2 cache in the 45 nm generation-the delay to reach individual banks ranged from 3 to 47 cycles.
Clearly, a conventional cache with uniformly slow access time would have unacceptable latency and bandwidth.
Kim et al. address this problem by defining a family of Non-Uniform Cache Architecture (NUCA) designs.
Similar to Figure 1, all practical NUCA designs assume a 2D array of cache banks accessed via a 2D switch interconnect implemented using conventional RC-delay wires.
The dedicated communication channels between the cache banks reserve the necessary substrate area for the data link's repeaters.
The static, or SNUCA, designs use loworder address bits to determine which cache blocks map to each bank.
The dynamic, or DNUCA, designs exploit locality by migrating frequently accessed blocks to the cache banks closest to the controller.
Dynamic placement reduces the average access time, but introduces significant additional design complexity, power consumption, and bandwidth demand.The DNUCA design is a very large (+30-way) setassociative cache, with banks grouped into different bank sets where a given block address may reside.
A reference that hits in the closest two banks of a bank set, a close hit, takes the minimum time, but a miss may require a search of all the remaining banks in the bank set.
DNUCA uses a partial tag structure to avoid this worst case for most accesses.
The partial tag structure stores the six least significant bits of all tags and is accessed in parallel with the closest two banks of the bank set.
If a request misses in the closest banks, the partial tag comparison indicates which other banks need to be searched.
In some cases, the partial tag check indicates that no other banks need be search, a so-called fast miss.
Partial tags improve performance directly, by reducing searches, and indirectly, by reducing interconnect link contention.While partial tags provide many benefits, keeping them consistent with the cache contents introduces significant complexity.
In particular, the partial tags must be updated when blocks migrate to closer banks.
Due to contention in the mesh network, blocks are not guaranteed to move from one bank to the other in a fixed time.
Thus a complex synchronization mechanism is required to ensure that blocks are not missed during a search.
While these complications are certainly manageable, they represent a significant additional design and verification effort.
Printed-circuit board and other off-chip wire technologies are commonly designed to behave as transmission lines [10].
Conversely, although on-chip transmission lines using non-conventional technology have been explored for over 20 years [38], on-chip wires using CMOS technology are normally designed to operate as lossy RC lines [41].
But with improving fabrication technology, on-chip transmission lines are starting to emerge in CMOS circuits.
For example, several current high performance chips use transmission lines for the long global wires (~ 0.75 cm) used for clock distribution [29,40,43].
Longer (> 1 cm) transmission lines operating in the 7.5-10 GHz frequency range have been shown to work on CMOS test chips using very wide wires [8] or low operating temperatures [11].
With the introduction of lower-k dielectrics [7] and increasing on-chip frequencies [18], more practical on-chip transmission lines will be available before the end of the decade.In this paper, we explore on-chip transmission line communication.
Specifically, we investigate using singleended voltage-mode signalling, where standard voltage signals propagate across a single point-to-point link.
To reduce reflection noise across these relatively low loss transmission lines, we assumed source-terminated drivers with digitally-tuned resistance [10].
Receivers use a large input impedance termination for full wave reflection of the received signal.
Single-ended voltage-mode signalling best fits the low utilization of on-chip interconnection networks.The physical transmission line is a single long wire that is routed directly from the driver to the receiver without repeaters.
Because of the length of transmission lines, thicker and wider metal tracks are required to maintain low wire resistance.
Additionally, thicker intermetal dielectrics are necessary to control wire capacitance on these long fat wires so that they can operate as transmission lines.
These transmission lines must be laid out in stripline fashion with a reference plane both above and below the transmission line metal layer to provide low resistance return paths for inductive induced currents [32].
While transmission line dimensions are much larger than the dimensions proposed for future conventional interconnect, they are actually very similar to the upper metal layers of previous high performance processors [6] and current silicon microwave chips [33] At these large wire dimensions, the "skin effect" significantly increases the signals' susceptibility to noise.
The skin effect phenomenon arises because at high frequencies, magnetic repulsion forces current towards the perimeter of the conductor, thereby reducing the wire's effective cross section.
Thus higher frequency signals encounter effective resistances greater than the wire's DC resistance.
This effect is compounded by the fact that a digital pulse is composed of many sinusoidal signals of different frequencies.
Because the different components of a digital pulse encounter different effective resistances, the receiver sees a signal that is rounded and stretched out.
Noise is a significant issue when receiving these attenuated signals.To reduce the noise susceptibility, we propose using alternating power and ground shielding [22] lines between each transmission line, in addition to the reference planes above and below the signal layer.
Laying out the lines in this manner not only provides several individual low-resistive return paths, but also isolates each line from most capacitive and inductive cross-coupling noise.Adding metal layers for reference planes will add significant manufacturing cost to the chip compared to con- 11 0 0 1 1 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 11 11 0 0 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 11 11 11 0 0 0 1 1 1 00 00 00 ventional CMOS technology.
However, the International Technology Roadmap for Semiconductors already projects, for the year 2010, integrating four reference planes into high performance chips to provide inductive shielding and decoupling capacitance [14].
Only time will tell if the benefits of transmission lines will justify their cost, but the history of silicon processing shows us that many complex and expensive enhancements have been adopted, including copper wires [13] and SOI devices [9].
We believe on-chip transmission lines could be the next manufacturing enhancement that drives system performance into the next decade.
One interesting opportunity for on-chip transmission lines is as a low latency interface between the cache's storage and its controller.
We targeted our Transmission Line Cache designs for the 45 nm technology generation [14], with an aggressive CPU core operating frequency of 10 GHz [18].
At this design point, the tremendous speed advantage of transmission lines will not only provide improved cache performance, but will also permit trading off some performance for a simpler design consuming less area and power.
We analyze a 16MB TLC to allow direct comparison with the 16 MB DNUCA design using the same technology assumptions [24].
TLC exploits the tremendous speed and layout benefits of transmission lines to decouple the cache storage from the cache controller.
Because transmission lines can quickly communicate across long distances without using repeaters, the large storage area of the cache can consume the less valuable real estate on the edges of the chip, while the cache controller can be moved to the center of the chip where it can be quickly accessed by the processor core.
This design is less feasible using conventional global wires because of their intermediate repeater requirement discussed previously.
Conversely, the on-chip transmission lines used by TLC don't require repeaters and can be routed over other logic without congesting intermediate wiring tracks and the substrate area below.
Figure 2 shows the high-level floorplan for the base TLC design.
This cache is composed of 32-512 KB banks where half the banks line one edge of the die and the other half line the opposite edge.
The space between the banks would be consumed by the processor core and L1 caches.
On each edge, the banks are stacked in two columns of eight.
Each pair of adjacent banks share two eight-byte wide unidirectional transmission line links to the L2 cache controller, creating a high bandwidth, low latency interface between the controller and the storage banks.Because the individual transmission lines vary in length, we adjust their width to maintain appropriate resistance and capacitance, as shown in Figure 3 and Table 1.
The width of the transmission lines used in TLC determine the size of a cache controller.
The cache controller must be tall enough so that all the transmission line links can connect with it.
The cache controller must be wide enough so that the each link has a direct connection to the center of the controller where the cache request originates.
The TLC cache controller uses conventional wires to communicate between the transmission lines located on its edges and the controller logic located at its center.
These conventional wires add up to three additional delay cycles to the TLC access times.As mentioned in the previous section, transmission lines are increasingly sensitive to noise corruption.
To further compensate for skew due to discontinuities across the transmission lines, we enforce extremely conservative setup and hold times of at least 40% of the entire clock cycle for the TLC signals.
Remaining faults on the transmission lines could be repaired using end-to-end ECC checks.
For instance, the IBM Power 4 already performs ECC checks when accessing the on-chip L2 cache [37].
End-to-end ECC simply means generating and checking the codes in the central controller.
We believe that these measures are enough to ensure that single-ended voltage-mode transmission lines will perform correctly in the noisy environments of future chips.
If one desires extra reliability, there are other techniques to increase noise immunity such as Optimized Transmission Line Caches.
The high bandwidth interface of the base TLC design comes at considerable cost in wire area.
We anticipate that extra metal layers will be required to implement the transmission lines needed by the base TLC design.
As a cheaper alternative, we consider optimized TLC designs that require fewer transmission lines, perhaps permitting their integration into the existing uppermost metal layers.
Figure 4 introduces the top level floorplan of these designs and Table 2 summarizes the parameters of our entire family of TLC designs.The Optimized TLC designs (TLCopt) are able to reduce the number of required wires through three methods:• Storing the 64-byte cache block across multiple banks to reduce the amount of data needed to be transferred between the cache controller and an individual bank per cache request.
• Doubling the cache bank size from 512KB to 1MB thus reducing the number of banks the cache controller interfaces with by half.
• Supplying each bank with only enough address information to access the correct set and perform a 6-bit partial tag [21] comparison.
The full tag comparison is performed later at the cache controller.In these designs, each bank is responsible for storing only a portion of the most significant bits of the cache tag along with the lower 6-bits of the tag.
The bank uses this 6-bit partial tag to do a quick comparison, determining if a request hits.
Because all banks holding a block store the same lower 6-bit partial tag, all tag comparisons among them will have the same result.
When the banks respond to a load request, they send its higher order tag bits along with the data to the cache controller which performs the full tag comparison.
In the infrequent case of multiple partial tag matches, the banks respond with the high order tag bits of all matching entries.
The controller determines which set entry, if any, actually matches, and then request the specific block.
Because all our TLC designs are exclusive write-back caches, store requests are simply written to the cache without requiring any tag comparisons.As an additional benefit of using fewer transmission lines, the TLCopt designs require smaller cache controllers.
This is because the TLC cache controller's height is Our evaluation methodology can be broken down into two separate parts.
First, we designed and simulated the physical on-chip transmission lines used by TLC.
Second, we evaluated the performance and estimated the dynamic power consumption of TLC as it compares to DNUCA using a full-system simulator.Physical Evaluation.
The goal of our physical evaluation was to first investigate the usage of on-chip transmission lines in future technology and then to evaluate their performance.
We started by using Linpar [12], a 2-dimensional field-solver program, to extract the inductance, resistance and capacitance characteristics of on-chip transmission lines.
Once we had RLC matrices describing the transmission lines, we simulated 10 GHz pulses travelling across the lines using HSPICE.
Specifically, we modeled the transmission line's frequency dependent attenuation with HSPICE's W element transmission line model.
We simulated four signal wires with shielding wires separating each of them under worst case signalling conditions.
We took the output waveforms to determine the latency of the transmission lines, as well as ensured the received signals had an amplitude of at least 75% of V dd and a pulse width of at least 40% of the processor cycle time.We used the tool ECACTI [1] to determine the access latency and layout of the cache banks.
Our models for delay [3], gate capacitance [17], and transistor sizes [34] allowed us to estimate the size and power of future interconnect as well as the switches used in NUCA [23,39].
Performance Evaluation.
We evaluated the system performance of each cache design using a dynamically scheduled SPARC V9 uniprocessor.
To simulate our target system, we used the full system simulator Simics [26] extended with a detailed processor [27] and memory system timing model.
Our detailed memory timing simulator for DNUCA and TLC included modelling contention within the links, switches and banks in each design.
Table 3 summarizes our simulation parameters.We evaluated all cache designs using 12 different benchmarks: four SPECint 2000 benchmarks (bzip, gcc, mcf, and perl), four SPECfp 2000 benchmarks (equake, lucas, swim, and applu) [36], and four commercial benchmarks described in Table 5 [2].
We warmed up the caches, as shown in Column 3 of Table 4, then evaluated each design over the amount of work indicated in Column 4.
This section evaluates the impact of TLC on a future high performance microprocessor.
Section 6.1 shows that the base TLC design provides comparable overall performance to DNUCA while providing more predictable behavior and consuming less area and power within the interconnect.
Section 6.2 evaluates the link utilization of all the TLC designs and shows that the TLCopt designs can attain similar performance to the base TLC design, while using significantly fewer wires.
Overall Performance.
Figure 5 compares the normalized execution time of the DNUCA and base TLC designs using the statically partitioned SNUCA2 cache design as a baseline [24].
SNUCA2 is the static NUCA design using a two-dimensional grid interconnect.
Except for some of the SPECFP benchmarks, both TLC and DNUCA significantly improve overall system performance compared to SNUCA2.
The lack of performance impact TLC and DNUCA have on the SPECfp benchmarks, lucas, swim, and applu, is due to the extremely high L2 miss rates of these benchmarks, as shown in Columns 3 and 4 of Table 6 [15].
DNUCA is particularly hurt by the low temporal locality and high miss rates of the swim and applu benchmarks.
DNUCA inserts all data blocks brought in from memory into the furthest banks from the cache controller and then promotes the blocks to its closer, quickly accessible banks every time the block is accessed.
This promotion policy relies on the expectation that most cache requests will be for a small set of frequently accessed blocks.
However, in benchmarks where most requests miss in the cache, this policy fails to improve cache access times.
This behavior is shown by the low ratio of DNUCA block promotions to block insertions for these two SPECfp benchmarks, Column 6 of Table 6.
The fourth SPECfp benchmark, equake, is particularly interesting.
Equake uses a finite element method on sparse matrices to simulate seismic waves propagating in a * 4-way set associative, with LRU replacement [5] to generate web requests.
We use a repository of 80,000 files (totaling ~2 GB).
These files are fetched by 400 clients.
Java Server Workload: SPECjbb (Sjbb).
SPECjbb2000 is a server-side java benchmark that models a 3-tier system, focusing on the middleware server business logic.
We use Sun's HotSpot 1.4.0 Server JVM.
Our experiments use 24 threads and 24 warehouses (a data size of ~500MB per warehouse).
Online Transaction Processing (OLTP): DB2 with a TPC-C-like workload.
The TPC-C benchmark models the database activity of a wholesale supplier, with many concurrent users performing transactions.
Our OLTP workload is based on the TPC-C v3.0 benchmark using IBM's DB2 v7.2 EEE database management system.
We use an 5GB database with 25000 warehouses stored on eight raw disks and an additional dedicated database log disk.
We reduced the number of districts per warehouse, items per warehouse, and customers per district to allow for concurrency provided by a larger number of warehouses.
There are 16 simulated users.large basin [4].
Like the other three SPECfp benchmarks, equake streams through a lot of data, but Equake also has a large data set that it frequently accesses.
DNUCA's frequency replacement policy separates the two groups of data within its highly associative sets, so that the streaming data does not evict the frequently accessed data.
On the other hand, TLC's LRU replacement policy is unable to disambiguate between the two data sets leading to a higher miss rate and lower performance for this benchmark.Figure 5 also shows DNUCA and TLC perform very well for the SPECint and commercial workloads that have much lower miss rates.
While DNUCA significantly improves performance for these workloads which have a high percentage of hits to the closest banks of cache, TLC significantly improves the performance of workloads like mcf which has a large memory footprint [15].
Overall, TLC moves the cache storage away from the processor core, while providing comparable performance improvement to DNUCA over the set of benchmarks.Performance Predictability.
TLC exhibits more predictable performance than DNUCA because it provides a more consistent response latency for L2 cache accesses.
Therefore an instruction scheduler can rely on TLC's predictable latency for scheduling dynamic operations, thus simplifying its circuits.
Additionally, schedulers performing speculative memory scheduling on L2 accesses will encounter significantly fewer replays using TLC.TLC's statically partitioned banks and high bandwidth interface enable TLC to provide more consistent lookup latency than DNUCA.
Figure 6 plots the mean cache lookup latency for the two cache designs over all twelve benchmarks.
As expected, TLC encounters more bank contention due to its fewer banks and longer bank access latencies, while DNUCA encounters more contention in the routing network to and from the banks.
The key observation is that TLC offers a more consistent mean lookup latency of around 13 cycles for all the benchmarks, while the mean lookup latency of DNUCA varies tremendously among benchmarks.Columns 7 and 8 of Table 6 compare the predictability of lookup latency for the TLC and DNUCA designs.
Column 7 shows that 10% or less of TLC lookup latencies are mispredicted for all but mcf.
Column 8 shows that at least 40% of DNUCA lookups are mispredicted, for twothirds of the benchmarks.
Because TLC has a predictable lookup latency and a high fraction of non-delayed requests, TLC can be easily integrated into a dynamic instruction scheduler, while the wide variation of access times for DNUCA significantly complicates dynamic instruction scheduling.Furthermore, as pipelines become deeper with a greater distance between when the instruction issue and execution stages, we believe aggressive dynamic schedulers will perform speculative memory scheduling on L2 accesses.
Speculative memory scheduling is a technique performed by current high performance microprocessors to improve the load-to-use latency between instructions.
Rather than waiting for a cache "hit" signal, some processors with predictable cache lookup latencies [16,20] will either predict a load hits in the cache and speculatively issue the load's dependent instructions or predict that the load misses and issue other independent instructions instead.
Speculative memory scheduling reduces load-touse latency by allowing dependent instructions to meet their source data at the execution units as soon as possible, while not wasting valuable issue bandwidth in the scheduler.
However, when the scheduler mispredicts that a load will hit in the cache, the speculatively issued dependent instructions must be replayed.Speculative memory scheduling on L2 accesses is significantly more difficult due to their difficult to predict access latencies.
One solution is to access the L2 cache tags early to provide a hint of when the data will arrive.
For example, Itanium 2 [28] uses a centralized tag structure to provide early hit or miss indication for its 256KB L2 cache.However, due to increasing wire delays, a centralized cache tag structure for future large on-chip caches may be impractical.
For instance the tag array for a 16 MB cache is nearly 1 MB, accessing such a large tag array will add several cycles of unnecessary latency to many cache lookups.
Instead, we believe future large caches will use a more distributed design and be partitioned into banks of tag and data arrays.
These more distributed caches, like the NUCA caches, will have much lower mean access times than a centralized cache of similar size.
However, the wide variance in their access times will only add to the nondeterminism of their accesses.
On the other hand, TLC has very predictable lookup latency and therefore could be easily integrated into future aggressive schedulers.Area.
Although TLC requires additional metal layers, it significantly reduces substrate area and hence overall die size.
Table 7 breaks down the substrate area requirements of DNUCA and TLC.
Table 7 shows that the latency benefit of DNUCA's smaller banks comes at an increased cost in bank area (Column 2) and an even greater increased cost in routing channel area (Column 3).
Column 4 shows that the DNUCA partial tag structure adds a relatively small amount to the total cache area.
On the other hand, TLC's large dense banks and lack of repeaters in the communication network saves storage and channel area, though its cache controller area is much larger due to its interface with the wide transmission line wires.
Overall, TLC reduces substrate area by 18% compared to DNUCA.Power.
Current low-power, low-voltage drivers [19] for off-chip transmission lines consume too much static λ λ power to be implemented for low utilized on-chip signals.Instead, TLC uses a more traditional single-ended voltagemode driver with active high signalling.
These drivers not only save power as compared to their contemporary lowvoltage counterparts, but actually allow TLC to consume less power than a cache using conventional RC interconnect.
The rest of this section breaks down both the TLC and DNUCA interconnection networks' static and dynamic power consumption to show how TLC can save power compared to DNUCA.While determining the exact static power consumption is difficult early in the design process, it is well understood that static power is dominated by transistor leakage current which is directly dependent on transistor width [34].
By removing intermediate switches, latches, and repeaters, as well as not requiring a partial tag array, TLC significantly reduces the transistor demand of the cache communication network.
As shown in Table 8, we estimate an over 50 fold reduction of transistors for TLC in comparison to DNUCA.
Table 8 also indicates the total transistor gate width would be reduced by over an order of magnitude.
Therefore the TLC communication network will save leakage power versus the DNUCA network.Dynamic power dissipation is dependent on the signalling strategy of the interconnect.
Signalling across conventional RC interconnect using repeaters relies on charging and discharging the capacitance of each wire segment from one voltage value to another.
Therefore for conventional signalling, dynamic power equals the power required to change the voltage, V, across the wire's total capacitance, C, for a given frequency, f, and data activity factor, [34]:In voltage-mode transmission line signalling, the dynamic power consumed is the power required to create the incident wave.
At the driver, the transmission line looks like a resistor equal to the characteristic impedance of the line.
Therefore the power supplied by the driver is determined by voltage across its internal resistance, R D , in series with the transmission line's characteristic impedance, Z 0 , for the duration of the signal pulse, t b , [10]:Comparing the dynamic power dissipation of matched voltage-mode transmission lines (R D = Z 0 ) to that of conventional wires, one sees that when , transmission lines will consume less dynamic power than conventional interconnect.
As cycle times continue to decrease, this relationship will hold for long global links beyond ~1 cm in length.
Table 9 compares the dynamic power components of the two cache designs.
While the total amount of dynamic power is relatively small for both designs, TLC does reduce dynamic power dissipation within the communication network by utilizing on-chip transmission lines.
TLC also significantly reduces the number of banks accessed per cache request leading to a greater reduction in dynamic power consumption as compared to DNUCA.
α Conventional Signalling Dynamic Power α C V 2 f × × × = Transmission Line Dynamic Power α t × b V 2 R D Z 0 + ( ) -------------------------- × f × = t b 2 Z 0 × ( ) ⁄ C < This section evaluates link utilization for the family of TLC designs and shows that similar performance to the base TLC design can be achieved using significantly fewer wires.
Link utilization is the percentage of cycles where the transmission lines actually communicate data.
Figure 7 plots the average link utilization for each TLC design across the spectrum of benchmarks.
One should first notice that the base TLC link utilization never exceeds 2% for any benchmark and for most benchmarks it hovers below 1%.
This extremely low utilization shows that the base TLC design has more bandwidth than necessary.
As expected, the TLCopt designs have an increasing degree of link utilization consistent with their reduction in transmission line wires.
However, even the utilization of the TLCopt 350 design remains relatively low, never surpassing 13%.
Figure 8 shows that this increase in link and bank contention for the TLCopt designs does not translate into a significant performance degradation compared to the base TLC design.
For some benchmarks, the TLCopt designs achieve slight improvements in execution time due to their slightly lower cache access latencies (Table 2).
Overall multiple partial tag matches in the TLCopt designs occurred in approximately 1% of the cache lookups, thus the increased messages sent between the cache controller and the banks has little effect on performance.
We have proposed an alternative family of cache designs using emerging on-chip transmission line technology.
On-chip transmission lines offer a significant latency advantage to conventional global interconnect for communicating distances greater than a few millimeters.
However, due to their power and bandwidth characteristics, onchip data transmission lines will be practically limited to long (> 1cm) performance critical signals.TLC is one such application of on-chip transmission lines.
Our TLC designs perform comparably to the previous DNUCA strategy, while saving area and power.
Furthermore, they provide a spectrum of reduced logical complexity solutions, but require significant circuit and manufacturing cost.
To combat the increased wire demand of the base TLC design, we introduced three optimized TLC designs that consume less wires and perform comparably for most benchmarks.
We thank Peter Hsu for inspiring this work and giving us helpful feedback.
We thank Doug Burger, Steve Keckler, Changkyu Kim and the Texas CART group for help with the DNUCA comparison.
We thank Virtutech AB, the Wisconsin Condor group, and the Wisconsin Computer Systems Lab for their help and support.
We thank Alaa Alameldeen, Brian Fields, Mark Hill, Mike Marty, Carl Mauer, Kevin Moore, Min Xu, the Wisconsin Computer Architecture Affiliates, and the anonymous reviewers for their comments on this work.
