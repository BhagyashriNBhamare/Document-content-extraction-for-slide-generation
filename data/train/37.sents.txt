Action-graph games (AGGs) are a fully expressive game representation which can compactly express both strict and context-specific independence between players' utility functions.
Actions are represented as nodes in a graph G, and the payoff to an agent who chose the action s depends only on the numbers of other agents who chose actions connected to s.
We present algorithms for computing both symmetric and arbitrary equilibria of AGGs using a continuation method.
We analyze the worst-case cost of computing the Jacobian of the payoff function, the exponential-time bottleneck step, and in all cases achieve exponential speedup.
When the in-degree of G is bounded by a constant and the game is symmetric, the Jacobian can be computed in polynomial time.
When modeling interactions between self-interested agents in a multiagent system, it is natural to use game theory as a theoretical foundation.
(For an introduction to games and equilibrium concepts, see e.g., Fudenberg and Tirole [1991].)
The central game-theoretic solution concept is the Nash equilibrium, a fixed-point in mixed strategy space which Nash [1950] proved exists in every finite game.
It remains an important open question to determine whether the problem of finding a Nash equilibrium belongs to P [Papadimitriou, 2001]; the best known algorithms for computing equilibria are exponential.
One state of the art general-purpose algorithm is the continuation method of Govindan and Wilson [2003], a gradientfollowing algorithm which is based on topological insight into the graph of the Nash equilibrium correspondence by Kohlberg and Mertens [1986].
(For a good survey describing earlier algorithms for games with more than two players, see [McKelvey & McLennan, 1996].)
The worst-case complexity of Govindan and Wilson's algorithm is open because the worst-case number of gradient-following steps is not known; however, in practice the algorithm's runtime is dominated by the computation of Jacobian of the payoff function, required for computing the gradient, which is both best-and worst-case exponential in the number of agents.
For many games this algorithm is an impressive step beyond the previous state of the art; however, it is still only practical when the numbers of players and of actions per player are small (roughly on the order of 5-10).
One response to the computational difficulty of computing the equilibria of general games has been the investigation of compact game representations that can be leveraged to yield more efficient computation.
One influential class of representations exploits strict independencies between players' utility functions; this class includes graphical games [Kearns et al., 2001], multi-agent influence diagrams [Koller & Milch, 2001] and game nets [La Mura, 2000].
Various algorithms were proposed to take advantage of these representations, including exact algorithms for games with special structure [Kearns et al., 2001;Koller & Milch, 2001] and approximate algorithms for arbitrary games [Vickrey & Koller, 2002].
Recently, Blum et al. [2003] adapted Govindan and Wilson's continuation algorithm to leverage these representations in an exact algorithm for arbitrary games.
Their algorithm computes the Jacobian of the payoff function for unrestricted graphical games and MAIDs.
It requires time exponential in the tree width of the underlying graph, an exponential improvement over the Govindan and Wilson algorithm.A second approach to compactly representing games focuses on context-specific independencies in agents' utility functions-that is, games in which agents' abilities to affect each other depend on the actions they choose-and often also on symmetries in agents' utility functions [Rosen- thal, 1973;Monderer & Shapley, 1996;Kearns & Mansour, 2002;Roughgarden & Tardos, 2001].
Our past work on local-effect games (LEGs) also falls into this class [Leyton- Brown & Tennenholtz, 2003].
LEGs are similar in spirit to action-graph games (AGGs), the game representation we introduce here.
They have the same graphical representation, with actions corresponding to nodes, and edges indicating context-specific utility dependencies between actions.
However, LEGs involve a series of assumptions: that utility independence between actions is always bidirectional; that all agents share the same action sets; and that utility decomposes into a sum of local effects from individual actions.
Thus, LEGs cannot represent arbitrary games.
Intuitively, AGGs can be understood as unrestricted LEGs.
An action-graph game (AGG) is a tuple N, S, ν, u. Let N ≡ {1, . . . , n} denote the set of agents in the game.
Each agent i has the set of action choices S i , so the set of pure action profiles isS ≡ i∈N S i (1)where is the Cartesian product.
Although the actions available to different agents may be distinct, agents may also have action choices in common.
LetS ≡ i∈N S i(2)denote the set of distinct action choices in the game.
Let ∆ denote the set of possible distributions of agents over actions, where a distribution is a number of agents who chose each action.
For a given distribution D ∈ ∆, denote by D(s) the number of agents who chose action s. D : S → N |S| is a function mapping from a pure strategy profile s to an agent distribution D.Let G be the action graph: a graph having one node for every action s ∈ S.
The neighbor relation is given by ν : S → 2 S .
Let there be a directed edge from s to s in G iff s ∈ ν(s).
Note that s ∈ ν(s) is possible.
The utility functionu : S × ∆ → R(3)maps from an action choice s and a distribution of agents D to a payoff.
Observe that all agents have the same utility function.
The utility function has the property that given any action s and any pair of distributions D and D ,[∀s ∈ ν(s), D(s ) = D (s )] ⇒ u(s, D) = u(s, D ).
(4)In other words, for every i and j agent i's utility is independent of agent j's action choice conditional on agent j choosing an action which is not in the neighborhood of agent i's action choice.
This is the sense in which AGGs express context-specific independencies in utility functions.
Beyond this condition, there are no restrictions on the function u.
In some cases it will be notationally simpler for us to write u(s) as a shorthand for u(s i , D(s)).
Any arbitrary game can be encoded as an AGG as follows.
Create a unique node s i for each action available to each agent i. Thus ∀s ∈ S, D(s) ∈ {0, 1}, and ∀ı, s∈Si D(s) must equal 1.
The distribution simply indicates each agent's action choice, and the representation is no more or less compact than the normal form.Example 1 Figure 1 shows an arbitrary 3-player, 3-action game encoded as an AGG.
As always, nodes represent actions and directed edges represent membership in a node's neighborhood.
The dotted boxes represent the players' action sets: player 1 has actions 1, 2 and 3; etc.
Observe that there is always an edge between pairs of nodes belonging to different action sets, and that there is never an edge between nodes in the same action set.In a graphical game [Kearns et al., 2001] nodes denote agents and there is an edge connecting each agent i to each other agent whose actions can affect i's utility.
Each agent then has a payoff matrix representing his local game with neighboring agents; this representation is more compact than normal form whenever the graph is not a clique.
Graphical games can be represented as AGGs by replacing each node i in the graphical game by a distinct cluster of nodes S i representing the action set of agent i.
If the graphical game has an edge from i to j, create edges so that ∀s i ∈ S i , ∀s j ∈ S j , s i ∈ ν(s j ).
The AGG and graphical game representations are equally compact.
In Corollary 1 below we show that our general method for computing the payoff Jacobian for AGGs is as efficient as the method specialized to graphical games due to Blum et al. [2003].
Example 2 Figure 2 shows the AGG representation of a graphical game having three nodes and two edges between them (i.e., player 1 and player 3 do not directly affect each others' payoffs).
The AGG may appear more complex than the graphical game; in fact, this is only because players' actions are made explicit.The AGG representation becomes even more compact when agents have actions in common, with utility functions depending only on the number of agents taking these actions rather than on the identities of the agents.
Figure 3 represents a setting in which n ice cream vendors must choose one of four Note that this game exhibits context-specific independence without any strict independence, and that the graph structure is independent of n.
Other examples of compact AGGs that cannot be compactly represented as graphical games include: location games, role formation games, traffic routing games, product placement games and party affiliation games.
Let ϕ(X) denote the set of all probability distributions over a set X. Define the set of mixed strategies for i asΣ i ≡ ϕ(S i ),(5)and the set of all mixed strategy profiles asΣ ≡ i∈N Σ i .
(6)We denote an element of Σ i by σ i , an element of Σ by σ, and the probability that player i plays action s by σ i (s).
Next, we give notation for applying some of the concepts defined in Section 2.1 to situations where one or more agents are omitted.
By ∆ −{i,i } we denote the set of possible distributions of agents other than i and i , and by Define the expected utility to agent i for playing pure strategy s, given that all other agents play the mixed strategy profile σ −i , asD −{i,i } we denote an element of ∆ −{i,i } .
Analogously, we define N −{i,i } , S −{i,i } , Σ −V i s (σ −i ) ≡ s −i ∈S −i u(s, s −i ) Pr(s −i |σ −i ).
(7)The set of i's pure strategy best responses to a mixed strat-egy profile σ −i is arg max s V i s (σ −i ), and hence the full set of i's pure and mixed strategy best responses to σ −i isBR i (σ −i ) ≡ ϕ(arg max s V i s (σ −i )).
(8)A strategy profile σ is a Nash equilibrium iff ∀i ∈ N, σ i ∈ BR i (σ −i ).
(9)D (s) (s ) ≡ D(s ) s ∈ ν(s) s ∈ν(s) D(s ) s = ∅ .
(10)In the analogous way, we define S (s) , s (s) , Σ (s) and σ (s) .
Continuation methods are a technique for solving certain systems of equations when the solution to a perturbed system is known.
Given a game of interest with payoff function u(s i , s −i ), one constructs a continuum of games Γ λ with payoff functions u λ parameterized by a single parameter λ, such that λ = 0 yields the game of interest Γ 0 , and λ = 1 yields a game Γ 1 having a known solution.
Then beginning with the known solution, the continuation method traces the solution as λ is decreased to zero, at which point the desired solution is obtained.
A good introduction to continuation methods is given in Blum et al.[2003]; we follow their treatment here.
A more detailed explanation of the method can be found in Govindan and Wilson [2003].
Nash equilibria are fixed points of a mapping that improves a player's utility by changing his strategy.
This mapping yields a system of equations to which continuation methods can be applied.
The expected utility of agent i isE[u(s i , s −i )] = si∈Si σ i (s i )V i si (σ −i ),(11)where we recall that V i s i (σ −i ) from Equation (7) is the expected payoff to agent i when he plays pure strategy s i and other agents play mixed strategy profile σ −i .
Consider the following strategy-improvement mapping for all agents:σ = R(σ + V (σ)).
(12)Here we introduce the retraction operator R : R m → Σ.The retraction operator takes a vector of dimension m ≡ i∈N |S i |, and normalizes it by mapping it to the nearest point in Σ in Euclidean distance.
Mapping from σ to σ in Equation (12) corresponds to increasing the probabilities of playing strategies that have better payoffs, while lowering the probabilities of playing strategies that have worse payoffs.
Its fixed points σ = σ, where no further (local) improvement can be achieved for any agent, are the Nash equilibria.
Rather than searching in Σ, Govindan and Wilson found it computationally expedient to search in the unnormalized space R m for a w such that σ = R(w); then the equilibrium is given by σ = R(w) such that w satisfiesR(w) = w + R (V (R(w))) .
(13)The perturbed system can now be introduced: replace V with V + λb, where b is a bonus to each agent depending on his identity and action choice, but independent of the actions of the other agents.
If for each given agent i, b i s i is sufficiently large for one particular action s i and zero for all others, then there will be a unique Nash equilibrium where each agent i plays the pure strategy s i .
We then have the system of equations F (w, λ) = 0, whereF (w, λ) = w − R(w) − (V (R(w)) + λb).
(14)For λ = 1, w is known; we then wish to decrease λ to zero while maintaining F (w, λ) = 0.
This requires that dF = 0 along the path.
Pairs (w, λ) satisfying this condition then map out a graph of the correspondence w(λ), which is with probability one over all choices of the bonus a one-manifold without boundary.
ThusdF (w, λ) = w F ∂F ∂λ dw dλ = 0(15)A nontrivial solution requires that w F ∂F ∂λ be singular, and its null space defines the direction that is followed by the graph of the correspondence w(λ).
Using Equation (14) we obtainw F = I − (I + V )R(16)where I is the m × m identity matrix.
Computing the Jacobian of F is dominated by the Jacobian of V .
The derivatives can be taken analytically; elements of the Jacobian for which i = i vanish, and we obtain for the i = i elements of the Jacobian of V ,∂V i si (σ −i ) ∂σ i (s i ) ≡ V i,i si,s i (σ)(17)= s∈S u (s i , D(s i , s i , s)) P r(s|σ)(18)andP r(s|σ) = j∈N σ j (s j ).
(19)(Recall that whenever we use an overbar in our notation, it is equivalent to the subscript −{i, i }.
For example, s ≡ s −{i,i } .)
Equation (18) shows that the V i,i si,s i (σ) element of the Jacobian can be interpreted as the expected utility of agent i when she takes action s i , agent i takes action s i , and all other agents use mixed strategies according to σ.The size of the set S is exponential in n, but since the sum must visit each member of the set, both best-and worstcase scenario computation of the right-hand side of Equation (18) is exponential in the number of agents.
In the sections that follow, we describe algorithms for the efficient computation of this expression.
Efficient computation of the payoff Jacobian is important for more than this continuation method.
For example, the iterated polymatrix approximation (IPA) method of Govindan and Wilson [2004] has the same problem at its core.
At each step the IPA method constructs a polymatrix game that is a linearization of the current game with respect to the mixed strategy profile, the Lemke-Howson algorithm is used to solve this game, and the result updates the mixed strategy profile used in the next iteration.
Though theoretically it offers no convergence guarantee, IPA is typically much faster than the continuation method; often it is used to give the continuation method a quick start.
Another application of the payoff Jacobian is in multiagent reinforcement learning algorithms that perform policy search.
Whenever ∃s i , ∃s such that s ∈ ν(s i ), there exist distributions of agents that are equivalent from the point of view of agent i. Furthermore, whenever ∃i, ∃j = i, S i S j = ∅, there exist pure action profiles that are equivalent from the point of view of agents who choose certain actions.
That is, when some action s is available to multiple agents, agents care only about the number of agents who choose s and not their identities.
We express the first kind of indifference by projecting the action graph; the second is expressed through partitioning the pure action profiles into distributions of agent numbers.
Each provides computational benefits.
It will be seen below that partitioning amounts to dynamic programming, i.e. pushing in of sums that analytically accounts for symmetry in the problem.
For arbitrary equilibria, the speedup due to projection is exponential as long as the maximum indegree of the graph G is less than the number of nodes in G.
This speedup typically overwhelms the gain due to partitioning in non-symmetric equilibria; however, for the important case of symmetric action space (i.e. ∀i, S i = S), partitioning guarantees computation of the Jacobian in polynomial time.
In Section 3.1 we consider equilibria of arbitrary action-graph games; in Section 3.2 we analyze symmetric equilibria.
Given arbitrary action set S, the Jacobian can be expressed compactly through projection at the level of the pure action profiles (recall the definition of projection in Equation (10)).
Projecting onto action s i , we rewrite the Jacobian given in Equations (18) and (19) asV i,i s i ,s i (σ) = s (s i ) ∈S (s i ) u s i , D(s i , s (si) i , s (s i ) ) P r s (s i ) |σ (si)(20)whereP r s (s i ) |σ (s i ) = j∈N σ (si) j (s (si) j ).
(21)To reflect the indifference between pure action profiles giving rise to the same distribution, we define an equivalence relation that partitions S:s ∼ s iff D (s) = D s .
(22)Then denote by S(D) the equivalence class containing all s such that D (s) = D.
The analogous partitioning can be done in the projected space, yielding e.g. S(D (si) ).
To exploit this indifference, we rewrite the elements of the Jacobian in Equations (20) and (21) V i,i s i ,s i (σ) = D (s i ) ∈∆ (s i ) u s i , D s i , s i , D (si) P r D (si) |σ (s i )(23)whereP r D (si) |σ (s i ) = s (s i ) ∈ S D (s i ) P r s (s i ) |σ (s i ) .
(24)Given the value P r(s (s i ) |σ (s i ) ) for some s (s i ) , dynamic programming can be used to compute the result for another "adjacent" value s (s i ) in constant time.
We introduce the permutation operation (j ↔ j ), such that s (si) is the pure strategy profile obtained from s (s i ) by switching the actions of agents j and j .
Thus s (si) = s (s i ) (j↔j ) .
Then we have that P r s(s i ) (j↔j ) |σ (si) = σ (si) j s (si) j σ (si) j s (si) j σ (s i ) j s (s i ) j σ (s i ) j s (s i ) j P r s (si) |σ (si) .
(25)We note that some elements of the Jacobian are identical:∀s / ∈ ν(s i ), s / ∈ ν(s i ), V i,i s i ,s (σ) = V i,i s i ,s (σ).
(26)Intuitively, Equation (26) expresses context-specific independence: the property that if agent i takes an action outside the neighborhood of the action taken by i, then i is indifferent to which particular action was taken by i .
It amounts to projecting the Jacobian.
Given that the Jacobian has O(m 2 ) entries, and given Equation (26), we have that the Jacobian has O(n 2 |S|(I + 1)) independent entries.Theorem 1 Computation of the Jacobian for arbitrary action-graph games using Equations (23) and (24) takes time that is O (I + 1) n poly(n)poly (|S|) .
Proof.
Computing elements of the Jacobian using Equations (23) and (24) involves computing the right hand side of Equation (21) , for eachD (s i ) ∈ ∆ (s i ) .
Since this is just a partitioning of the space S (si) , it amounts to a sum over all elements of the set S (si) .
This is the same number of elements as in the sum for the Jacobian in Equations (20) and (21).
Depending on their specific action sets, each agent has the choice of some subset of the at most I + 1 actions in the projected graph G (si) , and these choices are independent.
Thus S (si) has O (I + 1) n elements.
Using Equation (26), we have that the Jacobian has O(n 2 |S|(I + 1)) independent elements.
Then computation of the full Jacobian takes time that is O (I + 1) n poly(n)poly (|S|) .
f is the maximum family size and α is the maximum number of actions available to each agent, the Jacobian can be computed in time that is O poly(α f )poly(n)poly (|S|) .
Proof.
Graphical games can be written as AGGs following the procedure given in Section 2.2.
Define the family of agent i, f am(i), to be the set of all agents j such that ∃s i ∈ S i , ∃s j ∈ S j , s j ∈ ν(s i ).
Compute the Jacobian using the method of Equations (20) and (21).
The key to the complexity of computing the Jacobian is the size of the set S (s i ) .
For all elements of S (s i ) , all agents j / ∈ f am(i) take action ∅.
Each agent in f am(i) has at most α available actions in the projected graph G (si) .
Since there are at most f such agents, and since all agents choose independently, therefore S (s i ) = O(α f ).
Then the full Jacobian can be computed in time that is O poly(α f )poly(n)poly (|S|) .
This result is consistent with that of Blum et al. [2003], in which the graphical game representation is leveraged to speed up computation of the payoff Jacobian.
We note that for strict independence there is a result for AGGs similar to Equation (26) indicating that further elements of the Jacobian are equal; also, the domain of the product in equation (21) can be reduced.
These provide further (polynomial) speedup.
We omit these results here for reasons of space but note that the full exponential speedup described in [Blum et al., 2003] is already obtained by projecting the action graph.We note that for an arbitrary action-graph game, it may not be convenient to list the elements of each class S D (si) ; in such a case the Jacobian should be computed using Equations (20) and (21).
However, many games have structure that allows convenient iteration over members of these classes, and for such games the method of Equations (23) and (24) provide a constant speedup in computation of the Jacobian.
In the method of Equations (20) and (21), the utility function is evaluated O((I + 1) n ) times, whereas using Equations (23) and (24) nonnegative integers (see e.g. [Nijenhuis & Wilf, 1975]), ∆ (si) ∀i,S i →S = n + S (s i ) − 1 S (si) − 1 .
(27)This expression is a polynomial in n with degree S(s i ) −1,and it follows that ∆(s i ) ∀i,S i →S = Θ n S (s i ) −1 .
(28)Thus max si ∆ (si) = O(n I ).
So whereas computing the Jacobian using Equations (20) and (21) requires evaluating the utility function O((I + 1) n ) times, using Equations (23) and (24) require evaluating the utility function O(n I ) times.
Since P r(s (s i ) |σ (si) ) is computed an exponential number of times in both cases, the overall speedup is by a constant factor.In some games, utility functions depend linearly on the number of agents taking each action.
This is true, e.g. for local-effect games [Leyton-Brown & Tennenholtz, 2003], where the utility function is defined as u(s i , D) = a∈S f si,a (D(a)) .
(u s i , D (s i ) (a→a ) = u s i , D (s i ) + f s i ,a D (s i ) (a→a ) (a) − f s i ,a D (s i ) (a) + f si,a D (si) (a→a ) (a ) − f si,a D (si) (a ).
(30) Thus the summation in Equation (29) can be evaluated once rather than |∆ (s i ) | times.
Nash proved [1951] that all finite symmetric games have at least one symmetric equilibrium.
We seek such an equilibrium here, specializing to the case in which all agents have the same action choices: ∀i, ∀j, S i = S j = S. All agents use the same mixed strategy: σ i = σ j ≡ σ * .
In order to compute a symmetric equilibrium, the continuation method must be seeded with a symmetric equilibrium of the perturbed (λ = 1) game.
This is accomplished by giving all agents the same bonus, so in the perturbed initial equilibrium all agents take the same action.
Then since the path-following algorithm is symmetric (i.e. the operation of propagation along the path commutes with the permutation of agent identities), the path-following algorithm takes a symmetric perturbed equilibrium to a symmetric equilibrium of the unperturbed game.Since all agents have the same strategies, each pure action profile is equally likely, so for anys ∈ S(D (si) ) P r D (s i ) |σ (si) * = S(D (s i ) ) P r s (si) |σ (si) * ,(31)whereP r s (s i ) |σ (si) * = a∈S (s i ) (σ (si) * (a)) D (s i ) (a) .
(32)The classes vary exponentially in size.
Sizes are given byS D (si) = n!
a∈S (s i ) D (si) (a) !
(33)which is the multinomial coefficient.
The largest classes are those in which agents are distributed as uniformly as possible across the nodes of the projected graph (relative to the unprojected graph, this corresponds to having just as many agents choosing each action in ν(s i ) as choose all the other actions combined).
Furthermore, the Jacobian simplifies, since we need no longer consider individual agent identities in V , so instead of considering V i,i s i ,s i (σ), we consider V * s,s (σ * ), which equals V i,i s i ,s i (σ) for any i = i .
We replace the strategy improvement mapping of Equation (12) withσ * = R(σ * + V * (σ * )).
(34)We can thus compute the Jacobian asV * si,s i (σ * ) = D (s i ) ∈∆ (s i ) u s i , D s i , s i , D (s i ) P r D (s i ) |σ (s i ) *(35)where P r(D (32), the results for all other projected distributions (nodes) can be computed by using Equation (36) at each subsequent step on the path.
Generating the Hamiltonian path corresponds to finding a combinatorial Gray code for compositions; an algorithm with constant amortized running time is given by Klingsberg [1982].
To provide some intuition, it is easy to see that a simple, "lawnmower" Hamiltonian path exists for any lower-dimensional projection of H ∆ (s i ) , with the only state required to compute the next node in the path being a direction value for each dimension.P r D (si) (a→a ) |σ (s i ) * = σ (s i ) * (a )D (s i ) (a) σ (si) * (a) D (si) (a ) + 1 P r D (si) |σ (s i ) * .
(36Theorem 2 Computation of the Jacobian for symmetric action-graph games using Equations (35), (31), (32) and (36) takes time that is O(poly(n I )poly(|S|)).
Proof.
Recall from Equation (28) that when all agents have the same action choices, ∆(si) = Θ n S (s i ) −1 .
(37)The summation for an element of the Jacobian V * therefore has Θ n S (s i ) −1terms.
If the utility function is taken to be straightforward to compute, then the initial summand requires O(n) time.
Through dynamic programming the computation of subsequent summands can be done in constant time.
Since there are O(|S| 2 ) independent entries in the Jacobian to be computed, the computation of the symmetric Jacobian takes time O(poly(n I )poly(|S|)).
This paper introduced action-graph games, which compactly represent both strict and context-specific independencies between players' utility functions.
We showed how the structure of this graph can affect the computation of the Jacobian of the payoff function, the bottleneck step of the Govindan and Wilson continuation algorithm.
We presented algorithms for computing both general and symmetric equilibria of AGGs.
We showed that in the general case, computation of the Jacobian grows exponentially with the action graph's maximal in-degree rather than with its total number of nodes, yielding exponential savings.
We also showed that the Jacobian can be computed in polynomial time for symmetric AGGs when the action graph's maximal in-degree is constant, and described two dynamic programming techniques to further speed up this case.The full version of this paper will include two sections that could not be included here.
First, a game is k-symmetric if agents have one of k types and all agents of a given type affect other agents in the same way (see e.g. Example 3).
Nash's proof that a symmetric equilibrium exists in every finite symmetric game implies that a k-symmetric equilibrium exists in every finite k-symmetric game.
We will extend our results to the k-symmetric case, showing that the Jacobian can still be computed in polynomial time for constant k. Second, we will provide an implementation and experimental evaluation of our algorithms, derived from the publicly available implementation of Blum et al. [2003].
