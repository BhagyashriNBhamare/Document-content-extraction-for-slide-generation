We have developed an unsupervised framework for simultaneously extracting and normalizing attributes of products from multiple Web pages originated from different sites.
Our framework is designed based on a probabilistic graphical model that can model the page-independent content information and the page-dependent layout information of the text fragments in Web pages.
One characteristic of our framework is that previously unseen attributes can be discovered from the clue contained in the layout format of the text fragments.
Our framework tackles both extraction and normalization tasks by jointly considering the relationship between the content and layout information.
Dirichlet process prior is employed leading to another advantage that the number of discovered product attributes is unlimited.
An unsupervised inference algorithm based on variational method is presented.
The semantics of the normalized attributes can be visualized by examining the term weights in the model.
Our framework can be applied to a wide range of Web mining applications such as product matching and retrieval.
We have conducted extensive experiments from four different domains consisting of over 300 Web pages from over 150 different Web sites, demonstrating the robustness and effectiveness of our framework.
The World Wide Web (WWW) contains a huge number of online stores selling million of different kinds of products.
While online stores can reduce the geographical barrier and the time constraint for shopping, it becomes problematic for Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
WOODSTOCK '97 El Paso, Texas USA Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00.
a user to retrieve, analysis, and compare products.
For example, Figure 1 shows a sample of a portion of a Web page about some product information of a digital camera which consists of several product attributes such as resolution, flash mode, etc.
Traditional search engines whose retrieval methods treat every term in a Web document in a uniform fashion often result in ineffective product attribute information extraction and analysis.
For example, in the digital camera domain, if we supply the query terms "auto white balance", existing search engines may just match the terms in Web pages and return products without attribute content "auto white balance" but with attribute content "auto ISO".
In fact, "auto white balance" and "auto ISO" refer to the values of the attribute "white balance" and "light sensitivity", which should be regarded as two different attributes of a digital camera, and they should be properly differentiated.
Manually identifying product attributes is tedious and time consuming, and practically infeasible for the massive amount of Web sites.
As a result, it raises the need for automated methods which can identify the attributes of products effectively from Web pages.
If the product attribute information is extracted from multiple Web sites, another desirable task is that the product attributes can be automatically normalized and preferably the semantic meaning of normalized attributes can be obtained.
This can improve the indexing of product Web pages, and support further intelligent tasks such as attribute search or product matching.Existing information extraction approaches aim at extracting precise text fragments from documents [11].
In particular wrapper learning techniques have been developed to extract information from semi-structured documents such as Web pages [18].
For example, by collecting training examples, which consists of certain product attributes, from some Web pages in the site shown in Figure 1, one can learn a wrapper for automatically extracting information from the remaining pages in the same Web site.
However, one major limitation of existing wrapper learning methods is that they are supervised method and hence it requires manual effort in preparing training examples for every product attribute.
Moreover, the learned wrapper can only be applied to the Web site where the training examples come from.
For instance, the learned wrapper for Figure 1 cannot be applied to the Web site site shown in Figure 2 because of essentially different layout formats of the two sites.
A separate human work is needed to prepare a new set of training examples.
As a consequence, existing wrapper construction or learning methods are not scalable if we wish to extract information from numerous different Web sites.Recently, Zhu et al. have developed a system which can segment Web pages and label the elements of Web pages from different sources [19].
Their method analyzes the layout format of Web pages and employ an integrated approach of Hierarchical Conditional Random Fields (HCRF) and Semi-Conditional Random Fields (Semi-CRF) for segmenting and labeling the text fragments in a single framework.
Their approach is template dependent and can be applied to multiple Web sites for information extraction.
Though this integrated method can partially solve the problem of wrapper learning, one limitation is that it is a supervised learning method and one has to define the set of possible product attributes in advance and provide training examples for each attribute.
In other words, it cannot handle the discovery of previously unseen attributes.To reduce the human work involved, several unsupervised wrapper learning techniques have been proposed [5] by making use of the layout format of Web pages which are generated by templates.
Since the extraction is template dependent, the data extracted from different sites, even in the same domain, may not be synchronized.
For example, a field extracted from a particular site may contain both book title and author, whereas in another site, book title and author correspond to two different extracted fields.
Chuang et al. proposed an unsupervised wrapper learning technique which can construct wrappers to extract synchronized data from multiple sources [4].
The objective is to identify the optimal segmentation of the text in Web pages.
For example, the field containing both book title and author can be automatically segmented into two separate fields or attributes However, their method requires to train a field model for each field.
For example, there are two different field models for book title and author.
These field models are required to be trained from manually prepared training examples, or developed by human experts in advance costing substantial human effort.
Moreover, it cannot handle previously unseen fields of records.
They proposed a heuristic methodology for training the field models for previously unseen fields in an unsupervised manner.
The idea is to consider each group of aligned segments created by an unsupervised wrapper as a single field, and train a field model for each group using HMM with a predefined labeling rule.
However, such method can only apply to a Web page that contains multiple records.
For a Web pages with a single record, such as the ones in Figures 1 and 2, there exists neither group of aligned segments, nor a single group in which the aligned segments refer to different fields.Another limitation of existing unsupervised wrapper methods is that the extracted fields from different Web sites are not normalized, and hence requiring human work to judge whether two extracted fields refer to the same attribute in a domain.
For example, one may not know that the extracted text fragments "fireworks" are "portrait" refer to two different attribute values of the same attribute "shooting mode" in the digital camera domain.
Normalization of attribute is defined as clustering attribute values with similar semantic meaning.
It is useful for many applications such as storing attribute values of product records into structured database, retrieving and matching of products, etc.
Chuang et al. proposed a clustering method to match the extracted data based on the tokens of the data in a separate step [4].
However, since their method mainly considers the tokens, it is not able to normalize the text fragments "fireworks" and "portrait" to the same attribute.
Moreover, the clustering algorithm requires to fix the number of clusters in advance.
In practice, the number of attributes in a domain is unknown, and new features are found in a domain from time to time resulting in an unlimited number of attributes.The requirement of training examples, the incapability of discovering unseen attributes, and the lack of normalization of extracted attributes with similar semantic meaning are the problems of existing approaches.
In this paper, we aim at addressing these problems by developing an unsupervised learning framework for jointly extracting and normalizing product attributes from multiple Web sites.
Consider the Web pages shown in Figures 1 and 2.
These two Web pages are collected from two different Web sites in the digital camera domain describing two different digital cameras.
Naturally, they have different layout formats because they come from different Web sites.
We define attribute and attribute value as a field of a product and a value for a particular field respectively.
For example one attribute of a digital camera is "sensor resolution" and the attribute values are "effective sensor resolution 10,100,000 pixels" and "sensor resolution 10 megapixels" for the products in Fig- ures 1 and 2 respectively.
To extract the product attribute values from these two pages, one can make use of two wrappers, which must be previously learned for each individual Web site, to accomplish the task.
However, as described before, human work is needed to prepare training examples for wrapper learning and the attributes to be extracted are required to be defined.Very often, users may have some prior knowledge about the content of some attributes of interest in the domain.
For example, users may know that some terms such as "'megapixel" and "ISO" are frequently used to describe a digital camera.
Such prior knowledge can be easily collected, for example, by scanning one Web page about digital cameras and collecting a few terms in a list.
We can utilize the prior knowledge and infer from the content of the text fragments in Web pages that the text fragment "sensor resolution 10 megapixels" in Figures 2 likely refers to an attribute value.
However, there may be some previously unseen attributes.
For example, from the layout format of the Web page in Figure 2, it can be inferred that the text fragment "white balance auto, daylight, cloudy, tungsten, fluorescent, fluorescent H, custom" should be an attribute value because the layout format of this text fragment share certain similarity to the extracted text fragment "sensor resolution 10 megapixels".
It likely corresponds to an attribute value of a previously unseen attribute "white balance".
Similarly, more attribute values, which correspond to some previously unseen attributes such as "shutter speed", can be discovered from both Figures 1 and 2.
This shows that there is mutual influence between the content and layout format of product attributes in Web pages.
This provides useful clues for extracting attribute values of previously unseen attributes.The above scenario demonstrates the possibility of making use of the layout format of text fragments for extraction.
The next issue is the requirement of human effort to interpret the semantic meanings of the attribute values of the previously unseen attributes.
For example, suppose there is an extracted text fragment "fluorescent" in other Web pages.
The text fragment "white balance auto, daylight, cloudy, tungsten, fluorescent, fluorescent H, custom" is extracted from the Web page shown in Figure 2.
Suppose that these two text fragments are automatically clustered to the same group representing an attribute.
One can easily observe that they refer to the same attribute corresponding to "white balance".
This allows better understanding and interpretation of the semantic meaning of the normalized attribute because of some indicative terms such as "white balance" appeared in the majority of the text fragments in the same group.After product attributes are extracted and normalized, a product can be effectively represented.
It is useful for indexing the terms and conducting other intelligent tasks such as product matching and comparison.
We have developed an unsupervised learning framework for jointly extracting and normalizing product attributes from multiple Web sites.
For example, the text fragments "fireworks" are "portrait" are samples of extracted and normalized text fragments in the digital camera domain using our method.
These two fragments do not have words in common, but actually they refer to the product attribute "shooting mode" in the digital camera domain.
Unlike existing methods which conduct the extraction and normalization tasks in separate steps unavoidably leading to the accumulation of errors in these two steps, we propose a single framework which can conduct extraction and normalization tasks simultaneously resulting in a solution optimizing both tasks.
We also demonstrate in our mathematical formulation, that considering both the content information and the layout information can resolve the conflict between the two tasks.Our framework considers the page-independent content information and the page-dependent layout information in a single framework.
As illustrated in the above motivating example, the mutual influence between the content and the layout format of text fragments provides useful clues for both of the tasks of attribute extraction and normalization.
We design a probabilistic graphical model to model the relationship between the content and layout information for solving the extraction and normalization tasks simultaneously.
We employ Dirichlet process prior leading to another characteristic that the number of attributes to be discovered need not to be fixed and can be unlimited.
This can handle product attributes not known in advance and new attributes can be discovered.The semantic meaning of the extracted and normalized attributes can be visualized by a set of weighted terms in the model.
This can significantly help users understand and interpret the attributes.
Our framework can be applied to applications such as improving product searching based on attributes and Web online product matching.
We have conducted extensive experiments from four different domains consisting of over 300 Web pages from over 150 Web sites.
The experimental results show that our framework is robust and effective.
In a product domain D, we have a set of reference attributes, denoted by A to describe the products.
Let ai be the i-th attribute in A. For example, in the digital camera domain, reference attributes of digital camera may include "resolution", "white balance", "light sensitivity", etc.
There exists a special element denoted as a representing "not-anattribute".
Since the number of attributes is unknown and hence the size of A denoted by |A| is between 0 and ∞.
Each product r in D, is then characterized by the attribute values of the reference attributes.
Let vi(r) be the attribute value of the reference attribute ai for product r. For instance, the attribute value of the reference attribute "resolution" shown in Figure 1 is '10.0 Megapixel".
Given a collection of product Web pages C collected from a set of Web sites S. Let ci(s) be i-th page collected from the site s. Each page contains a single product p. Within the Web page ci(s), we can collect a set of text fragments X(ci(s)).
For example, "resolution 10,100,000 pixels" and "optical sensor type CCD" are samples of text fragments collected from the page shown in Figure 1.
Let xj(ci(s)) be the j-th text fragment in the Web page ci(s).
Essentially, each x in X(ci(s)) can be represented by a four-field tuple (C, L, T, A).
C refers to the content information of the text fragment such as the tokens contained in "optical sensor type CCD".
L refers to the layout information of the text fragment.
For example, the text fragment "Feature" is grey and in larger font size.
T , defined as the target information, is a binary variable which is equal to 1 if the underlying text fragment is an attribute value, and 0 otherwise.
For example, the values of T for the text fragments "Feature" and "optical sensor type CCD" are 0 and 1 respectively.
A defined as the attribute information, refers to the reference attribute that the underlying text fragment belongs to.
It is a realization of A and hence it must be equal to one of the elements in A. For example, the values of A for the text fragments "resolution 10,100,000 pixels" and "white balance auto, daylight, cloudy, tungsten, fluorescent, fluorescent H, customoptical" should be equal to the reference attributes "resolution" and "optical sensor" included in A respectively.α π k L n L θ s N S Z n T n C n o o C G 0 G T 0 θ C k θ T kIn practice, the content information C and the layout information L of a text fragment can be observed from Web pages.
However, the target information T and the attribute information A cannot be observed.
As a result, given the observation of C and L, product attribute extraction can be formulated as the prediction for the value of T for each text fragment in Web pages aiming at discovering all text fragments corresponding to certain attribute values.
Formally, for each text fragment, we aim at finding T = t * such that t * = argmax t {P (T = t|C, L)}.
Attribute normalization can be defined as the prediction of the value of A for each text fragment, so that one can understand the reference attribute to which the underlying text fragment refers.
Formally, for each text fragment, we aim at finding A = a * such that a * = argmax a {P (A = a|C, L)}.
When T = 1, P (A = a|C, L) > 0 for some a ∈ A \ {a} and P (A = a|C, L) = 0.
When T = 0, P (A = a|C, L) = 1.
Obviously, P (T |C, L) and P (A|C, L) are dependent since P (A|T = 0, C, L) 񮽙 = P (A|C, L).
As a result, conducting product attribute extraction and normalization separately may lead to conflict solutions degrading the performance of both tasks.
In our framework, we aim at predicting the values of T and A such that the joint probability P (T, A|C, L) can be maximized leading to a solution optimizing both tasks.
Our model can be regarded as an extension of Dirichlet mixture model.
Each mixture component, which refers to a reference attribute in our framework, consists of its own distribution about text fragments.
Dirichlet process prior is employed so that our framework can handle unlimited number of reference attributes.
Figure 3 shows the plate diagram representation of our model.
Shaded nodes and unshaded nodes represent the observable and unobservable variables respectively.
The edges represent the dependence between variables and the plates represent the repetition of variables.
We adopt the stick breaking construction representation of Dirichlet process in our presentation [15].
Suppose we have a collection of N different text fragments collected from S different Web pages.
Each generation of a text fragment is modeled as an independent and identical event.
The n-th text fragment xn consists of an unobservable variable Zn depending on the variables π = {π1, π2, . . .}.
Zn represents the index of the mixture component from which that the underlying text fragment is generated.
Essentially, we use Zn to replace An for clarity and An = az n where ai ∈ A. Next, the content information of the text fragment, denoted as Cn, is generated according to P C (Cn|θ C zn ), where P C (·|θ C k ) is the probability distribution about the content C given the variable θ C k ; and k refers to the k-th mixture component.
On the other hand, the target information Tn is generated by P T (Tn|θ T zn ), where P T (·|θ T k ) is the probability distribution about the target information T given the variable θ T k .
Since the layout format of the text fragments in a Web page is page-dependent, we have a set of layout distributions, namely, θ L s , for generating the layout format of the text fragments in page s.
As shown in the running example in Section 1.1, there is mutual influence between the layout information and the target information of a text fragment.
Tn together with θ L s will generate the layout information Ln of the n-th text fragment according to and G T 0 act as the prior distributions of the content information and the target information respectively.
For example, suppose we model the content of the text fragments by a mixture model of tokens, G C 0 can be a Dirichlet distribution which is the conjugate prior of a mixture model, P C (·|θ C k ) a multinomial distribution, and θ C k is the set of parameters of multinomial distribution in component k. Similarly, Since T is a binary variable, it can be modeled as a Bernoulli trial.
Therefore, P T (·|θ T k ) can be a binomial distribution with parameter θ T k and G T 0 can be a Beta distribution, which is the conjugate prior of a binomial distribution.P L (Ln|Tn, θ L s(xn) ), where P L (·|Tn, θ L s ) isRecall that the Dirichlet process is represented by the stick breaking construction in the graphical model depicted in Figure 3.
In the stick breaking construction, we have a one-unit length stick and we break a π k portion from the remaining portion of the stick according to Beta(1, α) in the k-th break, where Beta(α1, α2) is the Beta distribution, with parameters α1 and α2.
The process repeats for infinite times and hence the k-th piece of the broken sticks can represent the proportion of k-th component in the mixture.
Therefore, Dirichlet process prior can support an infinite number of mixture components, which refer to the product attributes in our framework.
Zn is then drawn from the distribution π.
In summary, the generation process can be described as follows:˜ π k |α ∼ Beta(1, α) π k = ˜ π k Q k−1 i=1 (1 − ˜ π k ) Zn|π ∼ π θ T k |G T 0 ∼ G T 0 θ T k |G C 0 ∼ G C 0 Tn|θ T k ∼ P T (θ T Zn ) Cn|θ C k ∼ P C (θ C Zn ) Ln|Tn, θ L s ∼ P (Ln|Tn, θ L s(xn) )The joint probability for generating a particular text fragment xn given the parameters α, G C 0 , G T 0 , and θ L s can then be expressed as follows:P (Cn, Ln, Tn, Zn, π1, π2, . . . , θ C 1 , θ C 2 , . . . , θ T 1 , θ T 2 , . . . |α, G C 0 , G T 0 , θ L s ) = ∞ Q i=1 {P L (Ln|Tn, θ T s(xn) ) h P C (Cn|Zn, θ C i )P T (Tn|Zn, θ T i ) i χ {Zn=i} P (Zn = i|π1, π2, . . .)P (θ C i |G C 0 )P (θ T i |G T 0 )} ∞ Q i=1 P (πi|α, π1, . . . , πi−1)(1)where χ {Zn=i} = 1 if Zn = i and 0 otherwise.
For simplicity, we let O, U , and ϕ be the set of observable variables, which include all Cn and Ln, the set of unobservable variables, which include all Tn, Zn, θ C k , θ T k , and π k , and the set of model parameters, which include α, G C 0 , G T 0 , θ L s respectively.
Given a set of N text fragment X and the parameters ϕ, the inference problem is then defined as follows:u * = argmax u {P (U = u|O, ϕ)} = argmax u {log P (U = u|O, ϕ)}(2)Since the computation of log P (U |O, ϕ) = log R P (U , O|ϕ)dO involves the marginalization of P (U ,O|ϕ), that is defined in Equation 1, over the unobservable variables, exactly solving Equation 2 is intractable.
As a result, approximation methods such as Markov Chain Monte Carlo (MCMC) algorithm are required.
In this paper, we develop a variational method to tackle this problem.
Recall that the objective of the inference is to compute P (U |O, ϕ), however, it is intractable.
The main idea of our method is to design a tractable distribution Q(U |ν), which is called the variational distribution of U characterized by a set of variational parameters denoted as ν.
The designed Q(U |ν) should be as closer to P (U |O, ϕ) as possible.
The distance between any two probability distributions P and Q can be measured by Kullback-Leibler(KL) divergence defined as asD(Q||P ) = P x∈X Q(x) log Q(x) log P (x)where X refers to all possible events.
Therefore, our method aims at minimizing the following KL-divergence by altering the set of variational parameters:D(Q(U |ν)||P (U |O, ϕ)) = EQ[log Q(U |ν)] − EQ[log P (U |O, ϕ)] = EQ[log Q(U |ν)] − EQ[log P (U , O|ϕ)] + log P (O|ϕ)(3)Since D(Q||P ) ≥ 0, we have:log P (O|ϕ) ≥ EQ[log P (U , O|ϕ)] − EQ[log Q(U |ν)](4)The left-hand-side (LHS) is the log likelihood of the observation of all text fragments X given the model parameters, the right-hand-side (RHS) is the lower bound of the likelihood function.
The minimization of D(Q(U |ν) || P(U | O, ϕ) ) becomes the maximization of the bound on the RHS given the model parameters.
Using the original variable notation used in Figure 3, the bound can be expanded as follows:∞ P k=1 {EQ[log P (π k |α)] + EQ[log P (θ C k |G C 0 )] + EQ[log P (θ T k |G T 0 )]} + N P n=1 {EQ[log P (Zn|π1, π2, . . .)] + EQ[log P (Cn|Zn, θ C 1 , θ C 2 , . . .)] +EQ[log P (Tn|Zn, θ T 1 , θ T 2 , . . .)] + EQ[log P (Ln|Tn, θ L s )]} −EQ[log Q(T , Z, θ C , θ T , π|ν)](5)where T , Z, θ C , θ T , and π represent the collection of variables Tn, Zn, θ C k , θ T k , and π k respectively.
As described in Section 2.2, P (π k |α) can be set to the Beta distribution Beta (1, α).
In our framework, we model the content information of text fragments as a mixture model of tokens in the set of vocabularies V .
Hence P C (·|θ C k ) is a multinomial distribution and G C 0 can be defined as the Dirichlet distribution G C 0 (·|µ C ) where µ C is the set of hyper parameters.
Tn follows the binomial distribution P T (·|θ C k ) and hence G T 0 is the Beta distribution G T 0 (·|µ T ) where µ T is the hyper parameter.
The layout information is modeled by a set of Bernoulli trials, denoted as Fs.
The outcome of each Bernoulli trial is whether the underlying text fragment possesses the fs-th formatting feature in page s depending on the value of θ L s and Tn, where 1 ≤ fs ≤ |Fs|.
Therefore, P (Ln|Tn, θ L s ) is represented by a set of binomial distributions.
According to stick-breaking process, we can express: P (Zn|π1, π2, .
.
.)
= Q ∞ i=1 (1 − πi) χ Zn>i π χ Zn=i iNext, we make use of the truncated stick-breaking process [6] and define Q(T , Z, θ C , θ T , π) as follows:Q(T , Z, θ C , θ T , π) = K−1 Q k=1 Qπ(π k |τ k,0 , τ k,1 ) K Q k=1 Q θ T (θ T k |δ k,0 , δ k,1 ) K Q k=1 Q θ C (θ C k |ζ) N Q n=1 QT (Tn|ωn) N Q n=1 QZ (Zn|φn) (6)where K is the truncation level; Qπ(·|τ k,0 , τ k,1 ) is the Beta distribution Beta(τ k,0 , τ k,1 );Q θ T (θ T k |δ k,0 , δ k,1 ) is the Beta distribution Beta(δ k,0 , δ k,1 ); Q θ C (θ C k |ζ)is the Dirichlet distribution with parameter set ζ; QT (Tn|ωn) is the binomial distribution with parameter ωn; and QZ (Zn|φn,1, . . . , φn,K ) is the multinomial distribution with parameter set φn,1, . . . , φn,K.
In the truncated stick breaking process, QZ(Zn|φn) = 0 for Zn > K. Under this setting, all the terms in Equation 5 can be expressed in explicit form.To maximize Equation 5, we can take the first derivative with respect to each of the variational variables and set to 0.
Next we obtain the following optimal conditions:τ k,0 = (1 − α) + P N n=1 φ n,k τ k,1 = α + P N n=1 P K j=k+1 φn,j(7)δ k,0 = µ T 0 + P N n=1 ωnφ n,k δ k,1 = µ T 1 + P N n=1 (1 − ωn)φ n,k(8)ζ k,j = µ C j + P N n=1 wn,j φ n,k(9)φ n,k ∝ exp { P K−1 j=1 [Ψ(τj,1) − Ψ(τj,0 + τj,1)] +Ψ(τ k,0 ) − Ψ(τ k,0 + τ k,1 ) + P |V | j=1 wn,j [Ψ(ζ k,j ) − Ψ( P |V | h=1 ζ k,h )] +ωn(Ψ(δ k,0 ) − Ψ(δ k,0 + δ k,1 )) +(1 − ωn)(Ψ(δ k,1 ) − Ψ(δ k,0 + δ k,1 ))} (10)where wn,j = 1 if text fragment xn contains j-th token in the vocabulary V , and 0 otherwise; Ψ(γ), which is called digamma function, is the first derivative of the log Gamma function.ωn = 1 1+e −h(φ n,k ,δ k,0 ,δ k,1 ,θ L s )(11)whereh(φ n,k , δ k,0 , δ k,1 , θ L s ) = K P k=1 φ n,k (Ψ(δ k,0 ) − Ψ(δ k,1 )) + P L l=1 u n,l (log θ L s,l − log(1 − θ L s,l ))and u n,f = 1 if xn contains the f -th layout format in Fs in page s = s(xn), and 0 otherwise.
Given the model parameters, one can then apply the steepest ascent algorithm, which is an iterative algorithm to update each variable at a time, until convergence.
Essentially, the attribute that xn belongs to can be decided by the values of φ n,k for k = 1, 2, . . ., each of which represents how likely that xn is generated from the k-th mixture component.
It can be observed that the value of φ n,k depends on three different aspects in Equation 10.
The first aspect is the prior proportion of the k-th components, which is characterized by the value of the variational parameters τ k,0 and τ k,1 .
The second aspect is the content of xn, which is denoted by wn,j , and the token distribution in k-th component, which is characterized by ζ k,j .
The third aspect is likelihood that xn belongs to an attribute, which is characterized by ωn, and the prior distribution that a text fragment longs to an attribute value in the k-th component, which is characterized by δ k,0 and δ k,1 .
On the other hand, the probability that xn is an attribute value is represented by the value of ωn, which depends on other three aspects.
The first aspect is φ n,k which is the probability that xn belongs to the k-th component.
The second aspect is the prior information about how likely a text fragment in the k-th component is an attribute value, characterized by the factor Ψ(δ k,0 ) − Ψ(δ k,1 ).
The third aspect is the layout of xn, which is characterized by f n,l , and the factor about the layout format of an attribute, which is characterized by (log θ L s,f − log (1 − θ L s,f )).
Interestingly, it shows that both normalization and extraction decision have mutual influence in the optimal condition according to Equations 10 and 11.
In particular, Equation 11 is in the form of logistic regression, which is discriminative in nature, considering a factor related to the k-th component in the mixture, as well as the layout format of the text fragment.
Consequently, our model can resolve the conflict between the extraction and normalization tasks and achieve an optimal solution.
As described in the previous section, we can automatically achieve the optimal extraction and normalization of product attributes by satisfying the optimal condition stated in Equations 7-11 given the model parameters.
We have developed a method which can automatically determine the model parameters and initialize a steepest ascent algorithm, achieving an unsupervised extraction and normalization.As exemplified in Section 1.1, our framework can consider the page-dependent layout format of text fragments to enhance extraction.
However, the layout information of an unseen Web page is unknown and hence we cannot predefine or estimate the values of θ L s,f .
As a result, we develop an Expectation-Maximization (EM) algorithm based on our variational method to estimate the values of all θ T s,f in page s. By taking the first derivative of Equation 5 with respect to each θ L s,f , we can obtain the following formula:θ F s,f = 1 N K P N n=1 P K k=1 ωnu n,f(12)This is the optimal value of each θ L s,f given other model parameters and the current set of variational parameters.
The E-step and M-step are defined as follows: E-step:Apply steepest ascent algorithm until convergence to achieve the optimal conditions depicted in Equations 7-11.
M-step:Calculate each θ T s,f using Equation 12.
To initialize the EM algorithm, we are required to estimate φ n,k and ωn for the n-th text fragment.
To achieve this, we make use of the prior knowledge, which is in the form of a list of a few terms, denoted as κ, related to product attributes.
Let κi be the i-th term in the list.
Notice that the terms are not required to be categorized into different attributes.
In practice, this list can be easily obtained, for example, by scanning one Web page containing a product in the underlying domain and highlighting the product attributes.
For each κi, we select the i-th component in our model and set a higher value of ζi,j if κi is equal to the vj ∈ V , and zero otherwise.
In particular, we set to 10 for such ζ k,j .
Next, for these components, we set δi,0 = 6 and δi,1 = 4 which essentially means that 6 out of 10 text fragments in this component will be a text fragment related to attribute values.
δ k,0 and δ k,1 are set to 4 and 6 respectively for other components.
ωn can then be calculated according to Equation 11.
Notice that these values are used in initialization only.
Updated values will be automatically calculated via the EM algorithm.For the model parameters, α is the scaling parameter between 0 and 1 in the Dirichlet process, which essentially affects the number of normalized attributes in the normalization process.
Since we apply our framework to the domains, for example, digital camera, in which each product contains a number of attributes, we set α to a value that favors large number of normalized attributes.
In particular we set α to 0.1.
µ T j refers to the prior knowledge about how likely a text fragment will be an attribute value.
We treat it as an uninformative prior and set µ T 0 = µ T 1 = 1.
Similarly, µ C j are treated as uninformative as all µ C j are set to 1.
The truncation level K of truncated stick breaking process is set to a relatively large value such that attributes are normalized as fine as possible.
In particular, K is set to 500.
We have conducted extensive experiments on four different domains, namely, digital camera, MP3 player, camcorder, and restaurant domains to evaluate our framework.
We have collected 85 Web pages from 41 sites, 96 Web pages from 62 sites, and 111 Web pages from 61 sites in the digital camera, MP3 player, and camcorder domain respectively.
In these three domains, each Web page contains one product and a number of product attributes.
The dataset of the restaurant domain contains 29 Web pages from the LA-weekly Restaurant Guide 1 .
Each page contains attributes including names, addresses, phone numbers, and review from customers for one or more restaurants.
We use a simple method which considers the line separators such as the HTML tags <BR>, <P>, <LI>, etc. to collect all text fragments from all Web pages.
The layout format of each text fragment is automatically recorded during the collection process.
For evaluation purpose, two human accessors were invited to annotate each text fragment by indicating whether it is a valid product attribute value and normalize all identified attribute values to appropriate reference attributes.
If there is a disagreement on the judgment of the two human accessors, it is resolved by a discussion among them.In each domain, we conducted 10 runs of experiments.
For each run, we randomly selected one page in the domain and selected the text fragments corresponding to attribute values.
The tokens in these text fragments were used to initialize our algorithm as stated in Section 3.2.
In practice, this could simply be done in Web browser by highlighting the portion containing attribute values.
Next, we applied our framework to all other remaining pages to simultaneously extract and normalize product attributes.
The performance of both extraction and normalization in each run were recorded to evaluate our framework.
We evaluate the performance of product attribute normalization.
The attribute normalization results are compared with the manually annotated answers.
We adopt the pairwise precision and recall, which are commonly used in clustering, as the evaluation metric.
Pairwise recall is defined as the number of pairs of text fragments, which are correctly predicted as referring to the same reference attribute by the system, divided by the actual number of pairs of text fragments referring to the same reference attribute.
Pairwise precision is defined as the number of pairs of text fragments, which are correctly predicted as referring to the same reference attribute by the system, divided by the total number of pairs of text fragments, which are predicted as referring to the same reference attribute.
Pairwise F1-measure is defined as the harmonic mean of equal weighting of pairwise recall and precision.
We first collected all correctly extracted text fragments by our framework in each run of our experiments.
The pairwise precision, recall, and F1-measure of the product attributes were then calculated.
We conducted the evaluation for the digital camera, MP3 player, and camcorder domains because the products in these domain contain a large number of attributes, and some of these attributes are previously unseen.
The restaurant domain is not evaluated since each restaurant only consists of a few attributes including names, phone number, addresses, customer reviewers, and credit card information which can be easily predefined.
We design a baseline approach for comparison.
For each pair of the text fragments correctly extracted by our framework, we compute the edit-distance as described in [2].
Attribute normalization is then conducted by invoking the agglomerative clustering.
This baseline approach only considers the text content of text fragments.
Table 1 shows the attribute normalization performance of our framework and the baseline approach.
Each row of the table corresponds to a run of the experiment and the last row is the average performance.
Each cell records the performance of our framework and the performance of the baseline approach is shown in brackets.
Each column refers to the extraction performance in a domain.
Our framework achieves a better results compared with the baseline approach.
In particular, the average F1-measure are 0.78, 0.68, and 0.81 in the digital camera, MP3 player, and camcorder domains respectively.
It shows that our framework can effectively normalize text fragments with similar semantic meaning to the same reference product attribute.
The baseline approach has a relatively low recall since it can only consider the token content of the text fragments.
In contrast, each mixture component of our framework has its own distribution of terms, so that tokens related to the reference attributes will also be considered.
Therefore, our framework can normalize attributes with no common token between text fragments such as "Night Portrait" and "Candle Light", which refers to the reference attribute "shooting mode" of a digital camera.
Table 2 shows the top 5 weighted terms in 10 largest normalized attributes in the digital camera domain.
It can be observed that the semantic meaning of the attributes can be easily interpreted from the terms.
The output of attribute normalization can be very useful for supporting other intelligent applications such as product attribute indexing and product retrieval.
We evaluate the extraction performance of our framework in the digital camera, MP3 player, camcorder, and restaurant domains.
The system extracted attributes are compared with the attributes extracted by human as described above.
We adopt the commonly used recall and precision as the evaluation metrics.
Recall is defined as the number of correctly extracted text fragments corresponding to attribute values divided by the actual number of text fragments corresponding to attribute values.
Precision is defined as the number of correctly extracted text fragments corresponding to attribute values divided by the total number of text fragments extracted by the system.
F1-measure defined as the harmonic mean of recall and precision is also used.
Table 3 shows the attribute extraction performance of our framework.
Each row of the table depicts the extraction performance in a run.
The last row shows the average extraction performance.
Our approach obtains promising results in the four domains.
The average F1-measure are 0.95, 0.69, 0.60, and 0.58 in the restaurant, digital camera, MP3 player, and camcorder domains respectively.
Notice that our framework is an unsupervised approach and does not require human effort to prepare training examples for every Web site.
Surprisingly, in the restaurant domain, our framework achieves a performance which is comparable to the supervised method stated in [9].
Moreover, our framework can extract product attributes reasonably well from over 300 Web pages which are originated from over 150 Web sites in the other three domains.
Various information extraction techniques have been proposed to extract attributes from semi-structured documents including Web pages [11,18].
For example, Conditional Random Fields (CRF) [7] have been applied to extract information from Web documents achieving the state-of-theart performance.
Sarawagi and Cohen developed a semiMarkov CRF model which can assign labels to segments of a sequence [12].
Sutton et al. proposed a dynamic CRF models for labeling sequence data [14].
Zhu proposed an integrated model based on hierarchical CRF and semi-CRF for detecting records and extracting attributes from raw Web pages [19].
However, one shortcoming of these supervised methods is that human effort is needed to prepare training examples.
Moreover, the attributes to be extracted are pre-defined and hence it cannot discover unseen attributes.
Wong and Lam aimed at reducing the human work of preparing training examples by automatically adapting extraction knowledge learned from a source Web site to new unseen sites and discover new attributes [17].
In this paper, we propose an unsupervised framework and thus no training example is needed.
Probst et al. [10] proposed a semi-supervised algorithm to extract attribute value pairs from text description.
Their approach aims at handling free text descriptions by making use of natural language processing techniques.
Hence, it cannot be applied to Web documents which are composed of mixing HTML tags and free texts.
The objective of entity resolution shares certain resemblances with our goal of product attribute normalization.
It aims at classifying whether two references refer to the same entity.
Singla and Domingos developed an approach to entity resolution based on Markov Logic Network [13].
Bhattacharya and Getoor proposed an unsupervised approach for entity resolution based on Latent Dirichlet Allocation (LDA) [1].
One limitation of these approaches is that the entities are required to be extracted in advance and cannot be applied to raw data.A common drawback of existing methods is that the extraction and normalization tasks are conducted in two separate steps, leading to conflict solutions and degrading overall performance.
Approaches based on CRF have been proposed to collaboratively conduct information extraction and mining [8,16].
However, conducting the attributes to be extracted have to be known in these approaches and previously unseen attributes cannot be handled.Dirichlet process mixtures have been studied and applied in image analysis, language modeling [3,15].
Our framework extend the Dirichlet process mixture model and shows that the mutual the content and layout information of text fragments can be considered jointly to achieve an optimal solution in product attribute extraction and normalization.
We have developed an unsupervised framework which aims at simultaneously extracting and normalizing product attributes from Web pages collected from different sites.
Our method can effectively consider the page-independent content information and the page-dependent layout information of the text fragments of Web pages.
We have developed a graphical model, which employs Dirichlet process prior, to model the generation of text fragments in Web pages.
An unsupervised inference algorithm based on variational method is derived.
We formally show that content and layout information can collaborate and improve both extraction and normalization performance.
Extensive experiments on four different domains have been conducted to show the robustness and effectiveness of our approach.
