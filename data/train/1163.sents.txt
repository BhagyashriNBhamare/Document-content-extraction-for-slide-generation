We consider the problem of optimizing time averages in systems with independent and identically distributed behavior over renewal frames.
This includes scheduling and task processing to maximize utility in stochastic networks with variable length scheduling modes.
Every frame, a new policy is implemented that affects the frame size and that creates a vector of attributes.
An algorithm is developed for choosing policies on each frame in order to maximize a concave function of the time average attribute vector, subject to additional time average constraints.
The algorithm is based on Lyapunov optimization concepts and involves minimizing a "drift-plus-penalty" ratio over each frame.
The algorithm can learn efficient behavior without a-priori statistical knowledge by sampling from the past.
Our framework is applicable to a large class of problems, including Markov decision problems.
Consider a stochastic system that regularly experiences times when the system state is refreshed, called renewal times.
The goal is to develop a control algorithm that maximizes the time average of a reward process associated with the system, subject to time average constraints on a collection of penalty processes.
The renewal-reward theorem is a simple and elegant technique for computing time averages in such systems (see, for example, [1] [2]).
However, the renewal-reward theorem requires random events to be independent and identically distributed (i.i.d.) over each renewal frame.
While this i.i.d. assumption may hold if a single control law is implemented repeatedly, it is often difficult to choose in advance a single control law that optimizes the system subject to the desired constraints.
This paper investigates the situation where the control policies used may differ from frame to frame, and are designed to dynamically solve the problem of interest.This renewal problem arises in many different applications.
One application of interest is a task processing network.
For example, consider a network of wireless devices that repeatedly collaborate to accomplish tasks (such as reporting sensor data to a destination, or performing distributed computation on data).
Tasks are performed one after the other, and for each task we must decide what modes of operation and communication to use, possibly allowing some nodes of the network to remain idle to save power.
It is then important to make decisions that maximize the time average utility associated with task processing, subject to time average power constraints at each node.
Alternatively, one may want to minimize time average power, subject to constraints on utility and on the Michael J. Neely is with the Electrical Engineering department at the University of Southern California, Los Angeles, CA.
This material is supported in part by one or more of the following: the DARPA IT-MANET program grant W911NF-07-0028, the NSF Career grant CCF-0747525, and continuing through participation in the Network Science Collaborative Technology Alliance sponsored by the U.S. Army Research Laboratory.
"left-over" communication rates available for data that is not associated with the task processing.This paper develops a general framework for solving such problems.
To do so, we extend the theory of Lyapunov optimization from [3].
Specifically, work in [3] considers discrete time queueing networks and develops a simple drift-pluspenalty rule for making optimal decisions.
These decisions are made in a greedy manner every slot based only on the observed traffic and channel conditions for that slot, without requiring a-priori knowledge of the underlying probability distribution.
However, the work in [3] assumes all slots have fixed length, the random network condition is observed at the beginning of each slot and does not change over the slot, and this condition is not influenced by control actions.
The general renewal problem treated in the current paper is more complex because each frame may have a different length and may contain a sequence of random events.
The frame length and the random event sequence may depend on the control decisions made over the course of the frame.
Rather than making a single decision every slot, every frame we must specify a policy, being a contingency plan for making decisions over the course of the frame in reaction to the resulting system events.This paper solves the general problem with a conceptually simple technique that chooses a policy to minimize a driftplus-penalty ratio every frame.
We first develop algorithms for minimizing the time average of a penalty process subject to a collection of time average constraints.
We then consider maximization of a concave function of a vector of time average attributes subject to similar constraints.
This utility maximization problem is challenging because of the variable frame length.
We overcome this challenge with a novel transformation together with a variation of Jensen's inequality.While this paper focuses on task processing applications, we note that our renewal framework can also handle Markov decision problems.
Specifically, suppose the system operates according to either a continuous or discrete time Markov chain with control-dependent transition probabilities.
If the chain has a recurrent state, then renewals can be defined as re-visitations to this state, and the same drift-plus-penalty ratio technique can be applied.
However, the drift-plus-penalty ratio may be difficult to optimize for Markov decision problems with high dimension (see also [4]).
Prior work on learning algorithms for Markov decision problems is in [5], and related work in [6] [7][8] [9] considers learning for optimization of energy and delay in queueing systems.
The works [5]- [9] use stochastic approximation theory and two-timescale convergence analysis.
The Lagrange multiplier updates in [5]- [9] are analogous to the virtual queue updates we use in this paper.
However, the Lyapunov optimization framework we use is different and does not require a two- timescale approach.
It also provides more explicit bounds on convergence times and deviations from optimality, and allows a broader class of problems such as task processing problems.t[0]=0 t[1] t[2] t[3] t[4] T[0] T[1] T[2] T[3]The Lyapunov optimization technique that we use in this paper is based on our previous work in [3][10] [11][12] that develops the drift-plus-penalty method for stochastic network optimization, including opportunistic scheduling for throughpututility maximization [3][10] [12] and average power minimization [11] (see also [13]).
Alternative "fluid-based" stochastic optimization techniques for queueing networks are developed in [14][15] [16] [17], and dual and primal-dual algorithms for systems without queues, based on tracking a corresponding static optimization problem, are in [18][19] [20].
Our current paper considers the more complex renewal problem, and leverages ideas in [4] [21], where [4] considers a frame-based Lyapunov framework for Markov decision problems involving network delay, and [21] develops a ratio rule for utility optimization in wireless systems with variable length frames and time-correlated channels.Recent work in [22] considers a task processing system where multiple wireless "reporting nodes" select data formats (e.g., "voice" or "video") in which to deliver sensed information.
The work [22] also uses a renewal structure.
However, it assumes a single random event occurs at the beginning of each renewal frame, and the event and frame size are not influenced by control actions.
More general problems can be treated using the theory developed in the current paper.
Consider a system that operates over renewal frames.
Specifically, consider the timeline of non-negative real times t ≥ 0, and suppose this timeline is segmented into successive frames of duration {T [0], T [1], T [2], . . .}, as shown int[r] = r−1 i=0 T [i]The interval of all times t such that t[r] ≤ t < t[r + 1] is defined as the rth renewal frame, defined for each r ∈ {0, 1, 2, . . .}.
At the beginning of each renewal frame r, the controller selects a policy π [r] from an abstract policy space P, and implements the policy over the duration of the frame.
There may be random events that arise over the renewal frame (with distributions that are possibly dependent on the policy), and the policy specifies a contingency plan for reacting to these events.
The policy incurs a vector of penal- ties y[r] = (y 0 [r], y 1 [r], . . . , y L [r]) and attributes x[r] = (x 1 [r], . . . , x M [r]) for some integers L ≥ 0, M ≥ 0 (whereT [r] = ˆ T (π[r]) (1) y l [r] = ˆ y l (π[r]) ∀l ∈ {0, 1, . . . , L} (2) x m [r] = ˆ x m (π[r]) ∀m ∈ {1, . . . , M }(3)We assume the values of[ ˆ T (π[r]), (ˆ y l (π[r])), (ˆ x m (π[r]))] for frame r are conditionally independent of events in previous frames given the particular policy π = π [r], and are identically distributed over all frames that use the same policy π.Consider now a particular control algorithm that chooses policies π[r] ∈ P every frame r according to some well defined (possibly probabilistic) rule, and define the following frame-average expectations, defined for integers R > 0:T [R] = 1 R R−1 r=0 E {T [r]} , y l [R] = 1 R R−1 r=0 E {y l [r]}(4)where we recall thatT [r], y l [r], x m [r] depend on the policy π[r] by (1)-(3).
Define x m [R]similarly, and define the infinite horizon frame-average expectations T , y l , x m by:(T , y l , x m ) = lim R→∞ (T [R], y l [R], x m [R])where we temporarily assume the limits are well defined.
The first type of problem we consider uses only penalties y[r]: We must choose a policy π[r] ∈ P every frame r to minimize the ratio y 0 /T subject to constraints on y l /T :Minimize: y 0 /T (5) Subject to: y l /T ≤ c l ∀l ∈ {1, . . . , L} (6) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .}(7)where c l for l ∈ {1, . . . , L} are a given collection of realvalued (possibly negative) constants.
The motivation for looking at the ratio y l /T is that it defines the time average penalty associated with the y l [r] process.
To see this, suppose the following limits converge to constants y av l and T av with probability 1:lim R→∞ 1 R R−1 r=0 y l [r] = y av l , lim R→∞ 1 R R−1 r=0 T [r] = T av (w.p.1)Under very mild conditions, the existence of the limits y av l and T av implies the frame-average expectations also have well defined limits, with y l = y av l and T = T av .
This holds, for example, whenever y l [r] and T [r] are deterministically bounded by finite constants, or when more general conditions hold that allow the Lebesgue dominated convergence theorem to be applied [23].
Then the time average penalty per unit time associated with y l [r] (sampled only at renewal times for simplicity) satisfies with probability 1:lim R→∞ R−1 r=0 y l [r] R−1 r=0 T [r] = lim R→∞ 1 R R−1 r=0 y l [r] 1 R R−1 r=0 T [r] = y l TTherefore, the value y l /T indeed represents the limiting penalty per unit time associated with the process y l [r].
The problem (5)- (7) seeks only to minimize a time average subject to time average constraints.
The second problem we consider, more general than the first, seeks to maximize a concave and entrywise non-decreasing function φ(γ) of the time average attribute vector ratio x/T , where x = (x 1 , . . . , x M ):Maximize: φ(x/T ) (8) Subject to: y l /T ≤ c l ∀l ∈ {1, . . . , L} (9) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .} (10)where φ(γ) is a given concave and entrywise non-decreasing utility function defined overγ = (γ 1 , . . . , γ M ) ∈ R M .
We assume x such that for all π[r] ∈ P and all m ∈ {1, . . . , M } we have: y min 0 ≤ E {ˆy{ˆy 0 (π[r])|π[r]} ≤ y max 0 0 < T min ≤ E ˆ T (π[r])|π[r] ≤ T max x min m ≤ E {ˆx{ˆx m (π[r])|π[r]} ≤ x maxγ min m = min[x min m /T min , x min m /T max ] γ max m = max[x max m /T max , x max m /T max ]Define the hyper-rectangle R by:R = {γ ∈ R M |γ min m ≤ γ m ≤ γ max m ∀m ∈ {1, . . . , M }} (11)Then for any algorithm that chooses policies π[r] ∈ P for all frames r, it is not difficult to show that x m [R]/T [R] ∈ R for all R ∈ {1, 2, 3, . . .}, where T [R], x m [R], T [R]E ˆ T (π[r]) 2 |π[r] ≤ σ 1 E ˆ y l (π[r]) 2 |π[r] ≤ σ 1 ∀l ∈ {1, . . . , L} E ˆ x m (π[r]) 2 |π[r] ≤ σ 1 ∀m ∈ {1, . . . , M } C. Optimality of i.i.d We now state the problem (5)-(7) more precisely, using lim sups which do not require existence of a well defined limit:Minimize: lim sup R→∞ y 0 [R] T [R](12)Subject to: lim sup R→∞y l [R] T [R] ≤ c l ∀l ∈ {1, . . . , L} (13) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .}(14)Assume that the constraints (13)- (14) are feasible, and define ratio opt as the infimum ratio in (12) over all algorithms that satisfy these constraints.Define an i.i.d. algorithm as one that, at the beginning of each new frame r ∈ {0, 1, 2, . . .}, chooses a policy π [r] by independently and probabilistically selecting π ∈ P according to some distribution that is the same for all frames r. E {ˆy{ˆy 0 (π * [r])} ≤ E ˆ T (π * [r]) (ratio opt + δ) (15) E {ˆy{ˆy l (π * [r])} ≤ E ˆ T (π * [r]) (c l + δ) ∀l ∈ {1, . . . , L} (16) Proof:The proof is similar to results in [11] [13], and is omitted for brevity.
Here we develop an algorithm to treat the problem (5)- (7).
To treat the constraints y l /T ≤ c l , which are equivalent to the constraints y l ≤ c l T , we define virtual queues Z l [r] for l ∈ {1, . . . , L}, with finite initial condition and with update equation:Z l [r+1] = max[Z l [r]+y l [r]−c l T [r], 0]∀l ∈ {1, . . . , L} (17)The intuition is that if we can stabilize the queue Z l [r], then the time average of the "service process" c l T [r] is greater than or equal to the time average of the "arrival process" y l [r] (see also [11] for application to virtual power queues for meeting time average power constraints).
LetZ[r] = (Z 1 [r], . . . , Z L [r]) be the vector of virtual queues, and define the following quadratic Lyapunov functionL(Z[r]): L(Z[r]) = 1 2 L l=1 Z l [r] 2The value L(Z [r]) is a scalar measure of the size of the queue backlogs.
The intuition is that if we can take actions that consistently push this value down, then queues can be stabilized.
Define the frame-based conditional Lyapunov drift ∆(Z [r]) by:∆(Z[r]) = E {L(Z[r + 1]) − L(Z[r])|Z[r]}Lemma 2: Under any control decision for choosing π[r] ∈ P, we have for all r and all possible Z[r]:∆(Z[r]) ≤ B + L l=1 Z l [r]E {y l [r] − c l T [r]|Z[r]}(18)where B is a constant that satisfies for all r and all possible Z[r]:B ≥ 1 2 L l=1 E (y l [r] − c l T [r]) 2 |Z[r](19)Such a constant B exists by the boundedness assumptions in Section II-B.
Proof: Squaring (17) yields:Z l [r + 1] 2 ≤ (Z l [r] + y l [r] − c l T [r]) 2 = Z l [r] 2 + (y l [r] − c l T [r]) 2 +2Z l [r](y l [r] − c l T [r])Taking conditional expectations, dividing by 2, and summing over l ∈ {1, . . . , L} yields the result.
Our Drift-Plus-Penalty Ratio Algorithm is designed to minimize a sum of the variables on the right-hand-side of the drift bound (18) and a penalty term, divided by an expected frame size, as in [21].
The penalty term uses a non-negative constant V that will be shown to affect a performance tradeoff:• (Policy Selection) Every frame r ∈ {0, 1, 2, . . .}, observe the virtual queues Z[r] and choose a policy π[r] ∈ P to minimize the following expression:E V ˆ y 0 (π[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] E ˆ T (π[r])|Z[r](20)• ( E V ˆ y 0 (π[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] E ˆ T (π[r])|Z[r] ≤ C + inf π∈P   E V ˆ y 0 (π) + L l=1 Z l [r]ˆ y l (π)|Z[r] E ˆ T (π)|Z[r]  In Section V-B it is shown that the infimum of (20) over π ∈ P is the same as the infimum over the extended class of probabilistically mixed strategies that choose a random π ∈ P according to some distribution (exactly what i.i.d. policies do every frame).
Thus, if policy π[r] is a C-additive approximation, then: E V ˆ y 0 (π[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] ≤ E ˆ T (π[r])|Z[r] C + E{Vˆy0E{Vˆ E{Vˆy0(π * [r])+ P L l=1 Z l [r]ˆ y l (π * [r])} E{ˆTE{ˆ E{ˆT (π * [r])}(21)a) For all l ∈ {1, . . . , L} we have:lim sup R→∞ y l [R]/T [R] ≤ c l ∀l ∈ {1, . . . , L}(22)lim supR→∞ R−1 r=0 y l [r] R−1 r=0 T [r] ≤ c l (w.p.1)(23)where "w.p.1" stands for "with probability 1."
b) For all integers R > 0 we have:y 0 [R] T [R] ≤ ratio opt + (B/T [R] + C) V + E {L(Z[0])} V RT [R](24)and hence:lim sup R→∞ y 0 [R]/T [R] ≤ ratio opt + (B/T min + C)/V (25)where B is defined in (19), and ratio opt is the optimal solution to (12)- (14).
Thus, the algorithm satisfies all constraints, and the value of V can be chosen appropriately large to make (B/T min + C)/V arbitrarily small, ensuring that the time average penalty is arbitrarily close to its optimal value ratio opt .
The tradeoff in choosing a large value of V comes in the size of the Z l [r] queues and the number of frames required for E {Z l [R]} /R to approach zero (which affects convergence time of the algorithm, see (33) in the proof).
In particular, it can be shown from (30) that there are constants F 1 , F 2 such that (see [24]):E {Z l [R]} R ≤ F 1 + V F 2 R + L l=1 E {Z l [0] 2 } R 2Proof: (Theorem 1) Consider any frame r ∈ {0, 1, 2, . . .}.
Combining (18) and (21) yields: (26), and letting δ → 0 yields:∆(Z[r]) + V E {ˆy{ˆy 0 (π[r])|Z[r]} ≤ B + E ˆ T (π[r])|Z[r] C + E{Vˆy0E{Vˆ E{Vˆy0(π * [r])+ P L l=1 Z l [r]ˆ y l (π * [r])} E{ˆTE{ˆ E{ˆT (π * [r])} − L l=1 Z l [r]c l E ˆ T (π[r])|Z[r](26)∆(Z[r]) + V E {ˆy{ˆy 0 (π[r])|Z[r]} ≤ B +EˆT +Eˆ +EˆT (π[r])|Z[r] [C + V ratio opt ](27)Taking expectations of the above yields:E {L(Z[r + 1])} − E {L(Z[r])} + V E {ˆy{ˆy 0 (π[r])} ≤ B + E ˆ T (π[r]) [C + V ratio opt ](28)Summing the above over r ∈ {0, . . . , R − 1} for some integer R > 0 and dividing by R yields:E {L(Z[R])} − E {L(Z[0])} R + V y 0 [R] ≤ B + T [R][C + V ratio opt ](29)Rearranging terms in the above and using the fact that E {L(Z[R])} ≥ 0 yields the result of part (b).
To prove part (a), from (27) there is a constant F such that:∆(Z[r]) ≤ F(30)Thus, the drift of a quadratic Lyapunov function is bounded by a constant.
Further, the second moments of per-frame changes in Z l [r] are bounded because of the second moment assumptions on y l [r] and T [r].
It follows that (see [24]):lim R→∞ E {Z l [R]} /R = 0(31)lim R→∞ Z l [R]/R = 0 (w.p.1)(32)Now from the queue update (17) we have for any frame r:Z l [r + 1] ≥ Z l [r] + y l [r] − c l T [r]Summing the above over r ∈ {0, . . . , R − 1} for some integer R > 0 yields:Z l [R] − Z l [0] ≥ R−1 r=0 [y l [r] − c l T [r]] Taking expectations, dividing by R, and using E {Z l [0]} ≥ 0 yields for all integers R > 0:E {Z l [R]} R ≥ y l [R] − c l T [R]Thus:y l [R] T [R] ≤ c l + E {Z l [R]} RT [R] ≤ c l + E {Z l [R]} RT min(33)Taking limits of the above and using (31) proves (22).
A similar argument uses (32) to prove (23).
Under a mild "Slater-type" assumption that ensures the constraints (13) are achievable with "-slackness," the queues Z l [R] can be shown to be strongly stable, in the sense that the time average expectation is bounded by O(V ).
If further mild fourth moment boundedness assumptions hold for y l [r] and T [r] then the same bound (25) can be shown to hold for pure time averages with probability 1 [24].
Consider now the problem (8)-(10), which seeks to maximize φ(x/T ) subject to y l /T ≤ c l for all l ∈ {1, . . . , L}.
We transform this problem of maximizing a function of a time average ratio into a problem of the type (5)-(7).
The following variation on Jensen's inequality is crucial in this transformation:Lemma 3: (Variation on Jensen's Inequality) Let φ(γ) be any continuous and concave function defined over γ ∈ R for some closed and bounded hyper-rectangle R. Let (T [r], γ[r]) be a sequence of arbitrarily correlated random vectors for r ∈ {0, 1, 2, . . .}.
Assume that T [r] > 0, γ[r] ∈ R for all r, and:0 < T min ≤ E {T [r]} ≤ T max < ∞ ∀r ∈ {0, 1, 2, . . .}Then for any R > 0:1 R R−1 r=0 E {T [r]φ(γ[r])} 1 R R−1 r=0 E {T [r]} ≤ φ 1 R R−1 r=0 E {T [r]γ[r]} 1 R R−1 r=0 E {T [r]}Furthermore, assuming that the limits T φ(γ) and T γ defined below exist, we have:T φ(γ)/T ≤ φ(T γ/T )(34)where:T φ(γ) = lim R→∞ 1 R R−1 r=0 E {T [r]φ(γ[r])} T γ = lim R→∞ 1 R R−1 r=0 E {T [r]γ[r]}Proof: See [13].
Now define an auxiliary vector γ[r] = (γ 1 [r], . . . , γ M [r]), to be chosen in the set R defined in (11) on every frame r.Lemma 4: (Equivalent Transformation) The problem (8)-(10) is equivalent to the following transformed problem:Maximize: T φ(γ)/T (35) Subject to: x m ≥ T γ m ∀m ∈ {1, . . . , M } (36) y l /T ≤ c l ∀l ∈ {1, . . . , L} (37) γ[r] ∈ R ∀r ∈ {0, 1, 2, . . .} (38) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .} (39) Proof: We briefly sketch the proof: Let π * [r], γ * [r]be a policy that optimally solves the above transformed problem, and assume for simplicity it yields well defined time averagesT * , y * l , x * m , T * φ(γ * ), T * γ * , util * = T * φ(γ * )/T * .
Then the policy π * [r] also satisfies all constraints of problem (8)- (10), and yields:φ(x * /T * ) ≥ φ(T * γ * /T * ) ≥ T * φ(γ * )/T * = util *where the first inequality above holds by (36) and the entrywise non-decreasing property of φ(γ), and the second holds by (34).
Thus, the optimal utility of problem (8)-(10) is greater than or equal to that of the transformed problem.
A similar argument shows it is also less than or equal to the optimal utility of the transformed problem.
The transformed problem (35)-(39) has the structure of the problem (5)- (7) G m [r + 1] = max[G m [r] + T [r]γ m [r] − x m [r], 0] (40) Define G[r] = (G 1 [r], . . . , G M [r]).
The drift-plus-penalty ratio to minimize every frame r is then:E −V ˆ T (π[r])φ(γ[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] E ˆ T (π[r])|Z[r] + E M m=1 G m [r][ ˆ T (π[r])γ m [r] − ˆ x m (π[r])]|Z[r] E ˆ T (π[r])|Z[r]It is easy to see that the above can be minimized by separately choosing γ[r] ∈ R and π[r] ∈ P to minimize their respective terms, and thatˆTthatˆ thatˆT (π [r]) cancels out of the auxiliary variable decisions.
The resulting algorithm is thus to observe Z[r] and G[r] every frame r ∈ {0, 1, 2, . . .} and perform the following:• (Auxiliary Variables) Choose γ[r] ∈ R to maximize:V φ(γ[r]) − M m=1 G m [r]γ m [r]• (Policy Selection) Choose π[r] ∈ P to minimize:E L l=1 Z l [r]ˆ y l (π[r]) − M m=1 G m [r]ˆ x m (π[r])|Z[r] E ˆ T (π[r])|Z[r]• (Virtual Queue Update) Update Z [r] by (17) and G [r] by (40).
The auxiliary variable update is a simple deterministic maximization of a concave function over a hyper-rectangle, and can be separated into M optimizations of single-variable concave functions over an interval if the utility function has the formφ(γ) = M m=1 φ m (γ m ).
The policy selection step is again an optimization of a ratio of expectations and can be done as described in Section V.
Here we show how to minimize the ratio of expectations given in (20) (and also in the policy selection stage of the previous section).
These problems can be written more generally as choosing a policy π[r] ∈ P to minimize the ratio: E {a(π)} E {b(π)}θ * = inf π∈P E {a(π)} E {b(π)}If the expectation E {b(π)} is the same for all π ∈ P (such as when the frame size is independent of the policy), then θ * is obtained by infimizing the numerator E {a(π)}.
This is typically easier (often involving learning for stochastic shortest path computations [25] [4]).
Otherwise, the following simple lemma is useful.
Lemma 5: We have:inf π∈P E {a(π) − θ * b(π)} = 0(41)Further, for any real number θ, we have:inf π∈P E {a(π) − θb(π)} < 0 if θ > θ * (42) inf π∈P E {a(π) − θb(π)} > 0 if θ < θ * (43) Proof: See [13].
Lemma 5 immediately leads to the following simple bisection algorithm: Suppose we have upper and lower bounds θ min and θ max , so that we know θ min ≤ θ * ≤ θ max .
Then we can define θ = (θ min + θ max )/2, and compute the value of inf π∈P E {a(π) − θb(π)}.
If the result is 0, then θ = θ * .
If positive, then θ < θ * , and otherwise θ > θ * .
We can then refine our upper and lower bounds.
This leads to a simple iterative algorithm where the distance between the upper and lower bounds decreases by a factor of 2 on each iteration.
It thus approaches the optimal θ * value exponentially fast.
Each step of the iteration involves minimizing an expectation, rather than a ratio of expectations.
Note that for any set of policies S, Lemma 5 implies that inf π∈S E {a(π) − θb(π)} = 0 if and only if θ = inf π∈S E {a(π)} /E {b(π)}.
Now suppose we have a set of policies P pure that we call pure policies, and that the policy space P consists of all pure policies as well as all "mixtures" (or convex combinations) of pure policies, being policies that choose a pure policy in P pure with some particular probability distribution.
More generally, define Ω as the set of all vectors (E {a(π)} , E {b(π)}) achievable over π ∈ P pure , and suppose the set of all (E {a(π)} , E {b(π)}) achievable over π ∈ P is equal to the convex hull of Ω.
Recall that θ * is the infimum ratio of E {a(π)} /E {b(π)} over π ∈ P. Then:0 = inf π∈P E {a(π) − θ * b(π)} = inf (a,b)∈Conv(Ω) [a − θ * b] = inf (a,b)∈Ω [a − θ * b] = inf π∈P pure E {a(π) − θ * b(π)}where the third inequality holds because the infimum of a linear function over the convex hull of a set is equal to the infimum over the set itself.
It follows that θ * is also the infimum ratio of E {a(π)} /E {b(π)} over π ∈ P pure .
This means that to achieve the infimum ratio over policies π ∈ P, it suffices to restrict our search to pure policies.
Suppose at the beginning of each frame, we observe a vector η[r] of initial information that can affect the penalties and frame size.
Suppose that {η[r]} ∞ r=0 is i.i.d. over frames.
Each policy π ∈ P first observes η [r] and then chooses a sub-policy π ∈ P η [r] , where P η [r] is a space that possibly depends on η [r].
To minimize E {a(π)}, it suffices to observe η [r] and choose π ∈ P η [r] to minimize the conditional expectation E {a(π )|η[r]}.
However, this is not necessarily true for minimizing the ratio E {a(π)} /E {b(π)}.
A correct approach is the following: If θ * is known, we can simply choose π ∈ P η[r] to minimize:E {a(π ) − θ * b(π )|η[r]}If θ * is unknown, we can carry out the bisection routine.
Let θ be the midpoint in the current iteration.
We must compute:inf π∈P E {a(π) − θb(π)} = E inf π ∈P η[r] E {a(π ) − θb(π )|η[r]}(44) The infimizing decision π can be made by observing η [r], without requiring knowledge of its probability distribution.
However, the value in (44) cannot be computed without knowledge of this distribution.
Instead, suppose we have Wi.i.d.
samples {η w } W w=1.
We can then approximate the value in (44) by the function val(θ) defined below:val(θ) = 1 W W w=1 inf π ∈Pη w E {a(π ) − θb(π )|η w } (45)By the law of large numbers, val(θ) approaches the exact value of (44) with a large choice of W .
The bisection routine can be carried out using the val(θ) approximation, being sure to use the same samples at each step of the iteration (but different samples on each frame r).
Note that val(θ) is nonincreasing in θ, so the bisection will converge provided that it is initialized so that val(θ min ) ≥ 0 and val(θ max ) ≤ 0.
If we cannot independently generate W samples, we use the W past observed values of η[r] from previous frames.
There is a subtle issue here, as these past values have influenced system performance and are thus correlated with the current a(π) and b(π) functions.
However, a delayed queue argument similar to that given in [26] shows these past values can still be used.
Note that constraints of the form y l ≤ 0 are equivalent to y l /T ≤ c l in the special case c l = 0, and thus can be handled using the framework of this paper.
Now consider the following problem structure:Minimize: y 0 Subject to: y l /T ≤ c l ∀l ∈ {1, . . . , L} π[r] ∈ P ∀r ∈ {0, 1, 2, . . .}Such a problem has a different structure than the problem (5)-(7), and is easier to solve as it does not require a ratio of expectations.
It can be solved using the same virtual queues Z l [r] in (17), but every frame r observing Z[r] and selecting a policy π[r] ∈ P to minimize the following expression:E{VˆyE{Vˆ E{Vˆy 0 (π[r]) + L l=1 Z l [r][ˆ y l (π[r]) − c l ˆ T (π[r])]|Z[r]}Analysis is omitted for brevity (see Exercise 7.3 in [13]).
The following is an alternative algorithm for the original problem (5)-(7) that does not require a ratio minimization (and hence does not require a bisection step): Use the same virtual queues Z l [r] in (17).
Define θ[0] = 0, and define θ[R] for R ∈ {1, 2, 3, . . .} by:θ[R] = R−1 r=0 y 0 [r]/ R−1 r=0 T [r](46)Every frame r, observe Z[r] and θ [r] and select a policy π[r] ∈ P to minimize the following expression:E{V [ˆ y 0 (π[r]) − θ[r] ˆ T (π[r])]|Z[r], θ[r]}(47)+E{ L l=1 Z l [r][ˆ y l (π[r]) − c l ˆ T (π[r])]|Z[r], θ[r]}It is shown in Exercise 7.5 of [13] that all constraints are met, and that if θ[r] converges to a constant with probability 1, then with probability 1:lim R→∞ R−1 r=0 y 0 [r]/ R−1 r=0 T [r] ≤ ratio opt + O(1/V )The disadvantage is that the convergence time is not as clear as that given in part (b) of Theorem 1.
Further, use of the time average (46) makes it difficult to adapt to changes in system parameters, so that it may be better to approximate (46) with a moving average or an exponentially decaying average.
Here we provide a simple task processing example.
An infinite sequence of tasks must be processed one at a time with the help of a network of 5 wireless devices.
This applies, for example, in scenarios similar to [22] where each new task represents an event that is sensed by the wireless devices (each at different sensing qualities [27]), and we must select which device reports the event information.
The renewal structure is shown in Fig. 2.
At the beginning of each new task r, a period of 0.5 time units is expended to communicate control information about the task.
Each of the 5 devices expends 0.5 units of energy in this control phase.
At the end of this phase, the network controller obtains a vector η[r] of parameters for task r.
The vector η[r] has the form: η[r] = [(qual 1 [r], T tran 1 [r]), . . . , (qual 5 [r], T tran 5 [r])]where for each l ∈ {1, . . . , 5}, qual l [r] is a real number representing the information quality if device l is chosen to process task r, and T tran l [r] is the transmission time required for device l to transmit the corresponding information to a receiving station.
The controller must choose one of the 5 devices to process the task, and must also choose the amount of idle time at the end of the frame (chosen within the interval [0,5]), so that the policy decision π[r] has the form:π[r] = (l[r], Idle[r]) ∈ {1, 2, 3, 4, 5} × {I ∈ R|0 ≤ I ≤ 5}Define P tran as the power expenditure associated with wireless transmission.
The chosen device l[r] expends P tran × T tran l [r] units of energy in the transmit phase, while all other devices l = l[r] expend no energy in this phase.
None of the devices expend energy in the idle phase, which helps to limit the average power expenditure in the system.The goal is to maximize the quality of information (q.o.i) per unit time subject to an average power constraint of 0.25 at each device.
Definê y 0 (π[r]) as −1 times the q.o.i. obtained for task r, ˆ y l (π [r]) as the energy expended by device l on task r, andˆTandˆ andˆT (π [r]) as the frame duration for task r: We simulate the drift-plus-penalty ratio algorithm for 10 6 frames, using the bisection method with W past samples of η [r] as in (45) [0, l] for l ∈ {1, 2, 3, 4, 5} (so that device 5 tends to have the highest quality, while device 1 tends to have the lowest).
We initialize θ min = −5V , θ max= 5 l=1 Z l [r]3.
Each step of the bisection computes val(θ) according to a simple deterministic optimization, and the bisection routine is run for each frame until θ max −θ min < 0.001.
Using V = 100, the resulting q.o.i per unit time is plotted in Fig. 3.
This increases to its optimal value as W is increased.
However, in this example, W does not need to be very large for accurate results: Even W = 1 produces a value that is near optimal (note that the y-axis in Fig. 3 distinguishes utility only in the 3rd significant digit).
All It can be seen that devices {2, . . . , 5} are utilized to their maximum power constraints because these tend to give the highest quality, while average power for device 1 is slack.The alternative algorithm of Section V-E, which does not require a bisection routine and amounts to a simple deterministic optimization for (47) every frame, achieves similar time average power expenditures to the above.
It also achieves utility as shown in Fig. 3, being the constant that does not depend on W (as no sampling from the past is needed).
Its utility is slightly larger than that of the bisection algorithm, and is approached by the bisection algorithm as W increases.
It appears that this algorithm is simpler and yields "automatic learning" by using the time average value θ[r], but it might have trouble adapting if system parameters change.
