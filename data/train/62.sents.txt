Relevance Feedback formulations have been proposed to refine query result in Content-Based Image Retrieval (CBIR) in the past few years.
Many of them focus on a learning approach to solve the feedback problem.
In this paper, we present an Expectation Maximization (EM) approach to estimate the user's target distribution through user's feedback.
Furthermore, we describe how to use Maximum Entropy display to fully utilize user's feedback information.
We detail the process and also demonstrate the result through experiments.
In Content-Based Image Retrieval Systems, low level features vectors, which may include texture, color and shape, are extracted from the image for storage, manipulation, and retrieval purpose.
Many systems use the one-shot approach during retrieval where a query is given in the form of a feature vector and the result is calculated based on the distance between feature vectors of query and images in database, whil the similarity of images is based on this distance measure.In order to understand the user's need in a search, a relevance feedback approach is used.
Every time, the retrieval system presents the user a set of images, the user then selects those, he thinks, which are relevant and gives feedback to the retrieval system.
Based on the feedback, the system can capture the user's need more accurately.In this paper, we describe some current approaches in Section 2.
In Section 3, we point out some problems that exist in current approaches.
We then propose a relevance feedback architecture that use EM to estimate the user's target and maximum entropy display to utilizes the user's distinguishing power in order to narrow down the retrieval process.
In Section 4, we verify the correctness of our algorithm and compare our approach with Rui's approach using the precision versus recall measure.
We then give some final remarks and conclude.
Relevance feedback is used to capture users' searching criteria.
In each pass, the retrieval system presents user a set of images, user then gives feedback to the system, indicating which one is relevant.
The system makes some update to the parameter and presents user another set of images.
This process is illustrated in Fig. 1.
Base on this architecture, Rui et.
al.[5] formulated a weight updating method to capture user's preference on different features, such as color or texture.
Cox et.
al. [1] formulated a Bayesian Learning approach to learn which image is more likely to be user's target based on the feedback.
In the following section, we review these two approaches in detail.
In [5], objects in image database are modelled asO = O(D, F, R),(1)where D is the raw image data, F = {f i } is a set of low level visual features, such as color, texture, and shape, and R = {r ij } is the set of representations for f i , which is defined asr ij = [r ij1 , ..., r ijk , ..., r ijK ].
(2)Moreover, the feature vector is organized in a hierarchical manner.
The overall similarity of two images O a and O b is defined asS(O a , O b ) = 񮽙 i W i S(f a i , f b i ),(3)S(f a i , f b i ) = 񮽙 j W ij S(r a ij , r b ij ),(4)S(r a ij , r b ij ) = m(r a ij , r b ij , W ijk ),(5)where m is the distance measure function, while W i , W ij and W ijk are the weights associated with each features, its representation and each dimension respectively.
For each feedback, they will follow the two procedures described below to update the weight in order to capture user's interest in different features.
Suppose RT is the set of most similar images according to the overall similarity function Eq.
3, for each feature representation ij, the system retrieves a set of similar images, RT ij , according to that particular feature representation.
The weight, W ij , is updated according toW ij = 񮽙 W ij + R, RT ij l ∈ RT and l = 1, ..., N, W ij , otherwisewhere N is the number of most similar images and R is the degree of relevance indicated by the user.
For the set of relevant images indicated by user's feedback, the system computes the standard deviation, σ ijk , in each dimension, and the weight for each dimension is updated as Eq.
(6).
We can see that if σ ijk is large, then the dimension is not ideal for discriminating relevant and irrelevant images, so its weight is updated as follows,W ijk = 1 σ ijk .
(6) In [1], each image is associated with a probability of being the user's target.
The retrieval process consists of two steps.
In each pass, the system selects a set of images and presents to user.
Through the feedback, the system updates the likelihood measure to the query of each image accordingly.
The probability is updated using the Bayes' rule as follows,P (T = T i |H t ) = P (A t |T = T i , D t , S t−1 )P (T = T i |H t−1 ) 񮽙 n j=1 P (A t |T j , D t , S t−1 )P (T j |H t−1 ) .
(7)The meaning of Eq.
(7) is that the probability of T i being the target image at iteration t is equal to product of the probability of T i being the target at iteration t − 1 and the probability of user give such feedback at iteration t provided that T i is the target, over the summation of probability of other images.Moreover, as each image is associated with a probability of being the target, [1] proposed a maximum entropy display strategy to select image presenting to user.
As a result, the system is expected to get most information gain from user's feedback.
Besides, [3] also details the procedure to apply this strategy.
This method inspires the display model of our proposal.
Expectation Maximization (EM) algorithm was first proposed in 1977 [2] .
It was used to solve the maximumlikelihood from incomplete data.
Given a mixture of Gaussians, the EM algorithm estimates the parameter of each mixture, say, mean and variance.
Our approach is based on EM algorithm to estimate the parameter of user's target distribution.Nigam et.
al. [4] and Wu et.
al. [8] also used the EM algorithm to classify documents and images respectively.
The EM approaches utilize the information contained in unlabeled data (irrelevant data) to help estimating user's target distribution more efficiently and accurately.
More recently, Wang et.
al. [7] proposed a Wiener filter approach to learn user's feedback in an optimal fashion.
Tian et.
al. [6] proposed the Support Vector Machine (SVM) to classify out the user's target.
All these newly suggested methods tends to model the feedback process as a learning process and apply algorithms in computer learning to help.
In [5], the weight updating method is a distance based similarity measure.
The weight is a measure of how important a particular feature, or dimension is in the query process.
It makes use of the weight in calculating the distance measure.
While in [1], a global update of probability to all images in database is used.
It is not parametric based, and the global updating processes seems to be the 0-7803-7278-6/02/$10.00 ©2002 IEEE computational bound when the size of image database grows.
In our proposal, we estimate the parameter of user's target distribution.
With these parameters, we can capture user's need more accurately, and the process of selecting images to display becomes easier.
Many other approaches always overlook the process of selecting images to display.
If we keep displaying the most similar images to user, we have no way to capture user's need in a broader sense.
In [1], the mostinformative display updating scheme try to achieve this, since each images is associated with a probability value, a maximum entropy display is used to select images.
However, when the size of image database is large, the number of permutation is huge, so a sampling approach is used to choose images that maximize the entropy.
In our proposal, we have estimated the user's target distribution parameters, thus we can select images located in the boundary to display, as illustrated in Fig. 2.
This is analogous to the maximum entropy selection.
Through this way, we can have the most information gain from user's feedback.
Moreover, our estimation strategy is related to our display selection strategy greatly.
Since we select images located around the boundary region, our estimation is not simply calculating the variance of relevant images directly.
Instead, we use a new approach, which will be discussed in section III-D.2.
We propose to use a statistical learning method, expectation maximization (EM) to estimate the distribution of user's search target, which will fully utilize the distinguish power by user in classifying relevant and irrelevant images.
Let DB = {I i } n i=1 be a set of database objects, and a set of feature parameters be θ = {θ i } m i=1 .
For each image in the database, we perform low level feature extraction to map it to a high dimensional data point by function f , which extracts a real-valued d-dimensional vector as,f : I × θ → R d ,(8)where θ i means a specific feature, for example, the color histogram, the co-occurrence matrix based texture feature or the Fourier descriptor.
Then an image will be mapped to a high dimensional vector, R d .
We assume the user's searching target distribution is a cluster in the high dimensional space.
Our goal is to estimate this distribution as accurately as possible, based on the user's feedback.
We focus on one feature at this moment first.
For each dimension under this feature, we estimate the mean, µ and variance, δ of the user's target distribution.
In each pass, we will give user a set of images to choose.
The user then indicates whether a specific image is relevant or not.
Let R + and R − be the set of relevant images and the set of irrelevant images in each pass respectively.
Let Rel( 񮽙 I i ) be the measure of how relevant an image 񮽙 I i and be defined as.Rel t+1 ( 񮽙 I i ) = Rel t ( 񮽙 I i ) + 1 񮽙 I i ∈ R + (9) Rel t+1 ( 񮽙 I i ) = Rel t ( 񮽙 I i ) − 1 񮽙 I i ∈ R −(10)We only consider images of Rel( 񮽙 I) > 0 and Rel( 񮽙 I) < 0.
The images are divided into two classes, the relevant one and the irrelevant one.
we indicate it as I + and I − respectively.
Using equation Eq.
(9) and Eq.
(10), we can resolve the conflict between successive feedbacks given by user.
After the user indicated some relevant and irrelevant images, we estimate the mean and variance of user's target distribution in each dimension by the EM algorithm.
In Fig. 2, we try to fit the Gaussian distribution having the data points selected by user located in the boundary region; in other words, we are going to maximize the expression: Eq.
11.
The reason why we use this expression is that most of the images we give to user to distinguish will fall in the area of medium likelihood (boundary case), so we fit our maximum likelihood function in order to make images appear in the medium likelihood region.E = 񮽙 Ii∈I + 񮽙 j=1 P (I i |θ j ) × ( 1 √ 2πδ j − P (I i |θj))(11)0-7803-7278-6/02/$10.00 ©2002 IEEEP (I i |θj) = 1 √ 2πδ j exp − (I ij −µ j ) 2 2δ 2 j (12)where j is the subscript for dimension and θ j is the combination of µ and δ for a particular dimension j. I + is the set of relevant images and I ij is the value of feature vector of image I i in dimension j.For finding the mean, the obvious way is to find the average of all relevant data, i.e.,µ j = 񮽙 i∈I + I ij |I + | .
(13)In order to find the best fitting δ j , we differentiate E with respect to δ j as follows.dE dδ j = 0(14)񮽙 i∈I + ( −1 πδ 3 exp − (I ij −µ j ) 2 2δ 2 j + (I ij − µ j ) 2 2πδ 5 exp − (I ij −µ j ) 2 2δ 2 j + 1 πδ 3 exp − (I ij −µ j ) 2 δ 2 j − (P ij − µ j ) 2 πδ 5 exp − (I ij −µ j ) 2 δ 2 j ) = 0(15) As this maximum likelihood objective function is hard to solve, we use EM algorithm to estimate the δ j .
We make use of the parameter from the previous step to derived the new parameter value.
We substitute oldj P (I ij |θ oldj ), and come out with an update equation for δ j :δ 2 j = 񮽙 i∈I + ((I ij − µ j ) 2 2 1 4 π − 3 4 − δ 1 2 oldj P 1 2 old (I ij − µ j ) 2 2 1 2 π − 1 2 ) 񮽙 i∈I + (2 1 4 π − 3 4 − δ 1 2 oldj P 1 2 old 2 1 2 π −1 )(16) Since we have captured the µ and δ of user's target distribution, we proceed to select images that lie in the boundary case to display, and let user determine whether they are relevant or irrelevant.
We choose images that are ±kδ away from the µ such that P (µ±kδ) = 1 √ 2πδ−P (µ±kδ).
After solving this equation, we found that k is equal to 1.1774.
In our experiment, we choose points around µ + kδ or µ − kδ in each dimension to display.
This is analogous to the maximum entropy display as we choose the ambiguous images for user to classify, thus we fully utilize the power of user in distinguishing different image classes.
Here, we propose a set of experiments to verify the correctness and measure the performance of our proposed algorithm.
We would like to make sure of the convergence property is met.
Moreover, we compare our proposed method with the Rui's method in terms of accuracy.1.
We generate a mixture of Gaussians with class labels and store in a text file.
2.
Base on our proposed algorithm, the program selects 18 data points in each iteration, and presents their class label to user.
3.
The user choose one class as his target, if he find data points come from his target, he gives feedback to system indicating that they are relevant.
4.
After several iteration, we see if the estimated distribution parameters converge towards the parameters used to generate the user's target class 5.
We use Root-Mean-Square error to measure the difference between actual and estimated µ and δ.
In the experiments we focus only on synthetic data sets.
These data sets are generated by Matlab as mixture of Gaussians.
We specify the mean and variance for each class and use Matlab random function to generate.
Our experiments were performed on program written by C++ running on Sun Ultra 5/400 with 256Mb ram.
The synthetic data sets are mixture of Gaussians, and the parameters are listed in Table 1.
The µ of each class is uniformly distributed within the range, and the δ of each class is value lies within the range indicated of probability 0.68 (the two values are the ±1 standard deviation of the gaussian function used to generate δ).
To test the convergence property, we make sure that the Root-Mean-Square (RMS) error decreases in each iteration.
Tables 2-4 demonstrate the RMS error of estimated mean and standard deviation along each iteration.
The fields indicated as not applicable are those with fewer than 3 relevant samples given.
It is because our algorithm starts to estimate the mean and variance when 2 and 3 relevant data points are accumulated respectively.0-7803-7278-6/02/$10.00 ©2002 IEEE The data below for each dimension is an average of 4 test cases for that particular dimension.
We have implemented the intraweight updating version (Eq.
6) of Rui's approach to compare with our EM approach to see how we can improve the retrieval accuracy by estimating user's target.
We carry out this experiment using synthetic data, which is a mixture of Gaussians with the parameter same as the 4 dimensional case in the previous experiment.According to the intraweight update equation, we update the weight base on variance of retrieved relevant data points.
After several iterations (normally 6 to 7), we compute a K-nn(K nearest neighbors) search incorporating the weight measure to analyze the precision versus recall measure.
For our proposed algorithm, as we can estimate the µ and δ of the target distribution, we again perform a K-nn search starting from the mean µ while dropping those data points away from the µ more than two δ.
Fig. 6,7,8,9 shows the precision versus recall graph for 4 test cases.
We have done 9 cases, in 4 of them, EM outperforms Rui's approach, while in other 5 cases, EM ties or performs a little better than Rui's approach.
In this paper, we proposed an approach for CBIR to estimate the user's target through learning from user's feedback via EM algorithm.
We have demonstrated the correctness and accuracy of our algorithm.
We proposed a display selection strategy that utilizes the information given by user in dividing image classes in contrast to the K-nn search approach for result display.
Moreover, we try to solve the conflicts between successive feedbacks from user.
Also, our method is based on the parameter estimation of target distribution, we do not need to perform a global update each image in the database accordingly.However, our algorithm also has weaknesses.
Since we need to accumulate up to 3 relevant data points before estimating the standard deviation, user might find this too long.
Moreover, we use maximum entropy strategy for display, thus, user might feel that the system cannot present the most relevant data in the retrieval process.
This research is supported in part by an Earmarked Grant from the Hong Kong's University Grants Committee (UGC), CUHK #4407/99E.
