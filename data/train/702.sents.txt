Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2].
It extends the theory of support vector machine to unsupervised learning.
Despite its good performance, there are three major problems with maximum margin clustering that question its efficiency for real-world applications.
First, it is computationally expensive and difficult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples.
Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset.
Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions.
In this paper, we propose "generalized maximum margin clustering" framework that addresses the above three problems simultaneously.
The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins.
It significantly improves the computational efficiency by reducing the number of parameters.
Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data.
Finally, we show a formal connection between maximum margin clustering and spectral clustering.
We demonstrate the efficiency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository.
Data clustering, the unsupervised classification of samples into groups, is an important research area in machine learning for several decades.
A large number of algorithms have been developed for data clustering, including the k-means algorithm [3], mixture models [4], and spectral clustering [5,6,7,8,9].
More recently, maximum margin clustering [1,2] was proposed for data clustering and has shown promising performance.
The key idea of maximum margin clustering is to extend the theory of support vector machine to unsupervised learning.
However, despite its success, the following three major problems with maximum margin clustering has prevented it from being applied to real-world applications:• High computational cost.
The number of parameters in maximum margin clustering is quadratic in the number of examples.
Thus, it is difficult to scale to large-scale datasets.
Figure 1 shows the computational time (in seconds) of the maximum margin clustering algorithm with respect to different numbers of examples.
We clearly see that the computational time increases dramatically when we apply the maximum margin clustering algorithm to even modest numbers of examples.
• Requiring clustering boundaries to pass through the origins.
One important assumption made by the maximum margin clustering in [1] is that the clustering boundaries will pass through the origins.
To this end, maximum margin clustering requires centralizing data points around the origins before clustering data.
It is important to note that centralizing data points at the origins does not guarantee clustering boundaries to go through origins, particularly when cluster sizes are unbalanced with one cluster significantly more popular than the other.
• Sensitive to the choice of kernel functions.
Figure 2(b) shows the clustering error of maximum margin clustering for the synthesized data of two overlapped Gaussians clusters (Figure 2(a)) using the RBF kernel with different kernel width.
We see that the performance of maximum margin clustering depends critically on the choice of kernel width.
The same problem is also observed in spectral clustering [10].
Although a number of studies [8,9,10,6] are devote to automatically identifying appropriate kernel matrices in clustering, they are either heuristic approaches or require additional labeled data.In this paper, we propose "generalized maximum margin clustering" framework that resolves the above three problems simultaneously.
In particular, the proposed framework reformulates the problem of maximum margin clustering to include the bias term in the classification boundary, and therefore remove the assumption that clustering boundaries have to pass through the origins.
Furthermore, the new formulism reduces the number of parameters to be linear in the number of examples, and therefore significantly reduces the computational cost.
Finally, it is equipped with the capability of unsupervised kernel learning, and therefore, is able to determine the appropriate kernel matrix and clustering memberships simultaneously.
More interestingly, we will show that spectral clustering, such as the normalized cut algorithm, can be viewed as a special case of the generalized maximum margin clustering.
The remainder of the paper is organized as follows: Section 2 reviews the work of maximum margin clustering and kernel learning.
Section 3 presents the framework of generalized maximum margin clustering.
Our empirical studies are presented in Section 4.
Section 5 concludes this work.
The key idea of maximum margin clustering is to extend the theory of support vector machine to unsupervised learning.
Given the training examples D = (x 1 , x 2 , . . . , x n ) and their class labels y = (y 1 , y 2 , . . . , y n ) ∈ {−1, +1} n , the dual problem of support vector machine can be written as:max α∈R n α e − 1 2 α diag(y)Kdiag(y)α s. t. 0 ≤ α ≤ C, α y = 0 (1)where K ∈ R n×n is the kernel matrix and diag(y) stands for the diagonal matrix that uses the vector y as its diagonal elements.
To apply the above formulism to unsupervised learning, the maximum margin clustering approach relaxes class labels y to continuous variables, and searches for both y and α that maximizes the classification margin.
This leads to the following optimization problem:min y,λ,ν,δ t s. t. (yy ) • K e + ν − δ + λy (e + ν − δ + λy) t − 2Cδ e 0 ν ≥ 0, δ ≥ 0where • stands for the element wise product between two matrices.
To convert the above problem into a convex programming problem, the authors of [1] makes two important relaxations.
The first one relaxes yy into a positive semi-definitive (PSD) matrix M 0 whose diagonal elements are set to be 1.
The second relaxation sets λ = 0, which is equivalent to assuming that there is no bias term b in the expression of classification boundaries, or in other words, classification boundaries have to pass through the origins of data.
These two assumption simplify the above optimization problem as follows:min M,ν,δ t s. t. M • K e + ν − δ (e + ν − δ) t − 2Cδ e 0 ν ≥ 0, δ ≥ 0, M 0 (2)Finally, a few additional constraints of M are added to the above optimization problem to prevent skewed clustering sizes [1].
As a consequence of these two relaxations, the number of parameters is increased from n to n 2 , which will significantly increase the computational cost.
Furthermore, by setting λ = 0, the maximum margin clustering algorithm requires clustering boundaries to pass through the origins of data, which is unsuitable for clustering data with unbalanced clusters.
Another important problem with the above maximum margin clustering is the difficulty in determining the appropriate kernel similarity matrix K. Although many kernel based clustering algorithms set the kernel parameters manually, there are several studies devoted to automatic selection of kernel functions, in particular the kernel width for the RBF kernel, [8] recommended choosing the kernel width as 10% to 20% of the range of the distance between samples.
However, in our experiment, we found that this is not always a good choice, and in many situations it produces poor results.
Ng et al. [9] chose kernel width which provides the least distorted clusters by running the same clustering algorithm several times for each kernel width.
Although this approach seems to generate good results, it requires running seperate experiments for each kernel width, and therefore could be computationally intensive.
Manor et al. in [10] proposed a self-tuning spectral clustering algorithm that computes a different local kernel width for each data point x i .
In particular, the local kernel width for each x i is computed as the distance of x i to its kth nearest neighbor.
Although empirical study seems to show the effectiveness of this approach, it is unclear how to find the optimal k in computing the local kernel width.
As we will see in the experiment section, the clustering accuracy depends heavily on the choice of k. Finally, we will briefly overview the existing work on kernel learning.
Most previous work focus on supervised kernel learning.
The representative approaches in this category include the kernel alignment [11,12], semi-definitive programming [13], and spectral graph partitioning [6].
Unlike these approaches, the proposed framework is designed for unsupervised kernel learning.i.e., σ in exp − xi−xj 2 2 2σ 2 .
Shi et al.
We will first present the proposed clustering algorithm for hard margin, followed by the extension to soft margin and unsupervised kernel learning.
In the case of hard margin, the dual problem of SVM is almost identical to the problem in Eqn.
(1) except that the parameter α does not have the upper bound C. Following [13], we further convert the problem in (1) into its dual form:min ν,y,λ 1 2 (e + ν + λy) T diag(y)K −1 diag(y)(e + ν + λy) s. t. ν ≥ 0, y ∈ {+1, −1} n(3)where e is a vector with all its elements being one.
Unlike the treatment in [13], which rewrites the above problem as a semi-definitive programming problem, we introduce variables z that is defined as follows: z = diag(y)(e + ν) Given that ν ≥ 0, the above expression for z is essentially equivalent to the constraint |z i | ≥ 1 or z 2 i ≥ 1 for i = 1, 2, . . . , n. Then, the optimization problem in (3) is rewritten as follows:min z,λ 1 2 (z + λe) T K −1 (z + λe) s. t. z 2 i ≥ 1, i = 1, 2, . . . , n(4)Note that the above problem may not have unique solutions for z and λ due to the translation invariance of the objective function.
More specifically, given an optimal solution z and λ, we may be able to construct another solution z and λ such that: z = z + e, λ = λ − .
Evidently, both solutions result in the same value for the objective function in (4).
Furthermore, with appropriately chosen , the new solution z and λ will be able to satisfy the constraint z 2 i ≥ 1.
Thus, z and λ is another optimal solution for (3).
This is in fact related to the problem in SVM where the bias term b may not be unique [14].
To remove the translation invariance from the objective function, we introduce an additional term C e (z e) 2 into the objective function, i.e.min z,λ 1 2 (z + λe) T K −1 (z + λe) + C e (z e) 2 s. t. z 2 i ≥ 1, i = 1, 2, . . . , n(5)where constant C e weights the important of the punishment factor against the original objective.
It is set to be 10, 000 in our experiment.
For the simplicity of our expression, we further define w = (z; λ) and P = (I n , e).
Then, the problem in (4) becomes min w∈R n+1w T P T K −1 P w + C e (e 0 w) 2 s. t. w 2 i ≥ 1, i = 1, 2, . . . , n(6)where e 0 is a vector with all its elements being 1 except its last element which is zero.
We then construct the Lagrangian as followsL(w, γ) = w T P T K −1 P w + C e (e 0 w) 2 − n i=1 γ i (w I i n+1 w − 1) = w P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 w + n i=1 γ iwhere I i n+1 is an (n + 1) × (n + 1) matrix with all the elements being zero except the ith diagonal element which is 1.
Hence, the dual problem of (6) ismax γ∈R n n i=1 γ i s. t. P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 0 γ i ≥ 0, i = 1, 2, .
.
.
, n (7) Finally, the solution w can be computed using the KKT condition, i.e.,P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 w = 0 n+1In other words, the solution w is proportional to the eigenvector of matrixP T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1for the zero eigenvalue.
Since w i = (1 + ν i )y i , i = 1, 2, . . . , n and ν i ≥ 0, the class labels {y i } n i=1 can be inferred directly from the sign of {w i } n i=1 .
Remark I It is important to realize that the problem in (5) is non-convex due to the nonconvex constraint w 2 i ≥ 1.
Thus, the optimal solution found by the dual problem in (7) is not necessarily the optimal solution for the prime problem in (5).
Our hope is that although the solution found by the dual problem is not optimal for the prime problem, it is still a good solution for the prime problem in (5).
This is similar to the SDP relaxation made by the maximum margin clustering algorithm in (2) that relaxes a non-convex programming problem into a convex one.
However, unlike the relaxation made in (2) that increases the number of variables from n to n 2 , the new formulism of maximum margin does not increase the number of parameters (i.e., γ), and therefore will be computational more efficient.
This is shown in Figure 1, in which the computational time of generalized maximum margin clustering is increased much slower than that of the maximum margin algorithm.Remark II To avoid the high computational cost in estimating K −1 , we replace K −1 with its normalized graph Laplacian L(K) [15], which is defined asL(K) = I −D 1/2 KD 1/2where D is a diagonal matrix whose diagonal elements are computed as D i,i = n j=1 K i,j , i = 1, 2, . . . , n.
This is equivalent to defining a kernel matrix˜Kmatrix˜ matrix˜K = L(K) † where † stands for the operator of pseudo inverse.
More interesting, we have the following theorem showing the relationship between generalized maximum margin clustering and the normalized cut.
Theorem 1.
The normalized cut algorithm is a special case of the generalized maximum margin clustering in (7) if the following conditions hold, i.e., (1) K −1 is set to be the normalized Laplacian ¯ L(K), (2) all the γs are enforced to be the same, i.e., γ i = γ 0 , i = 1, 2, . . . , n, and (3) C e ≥ 1.
Proof sketch: Given the conditions 1 to 3 in the theorem, the new objective function in (7) becomes: max γ≥0 γ s.t. ¯ L(K) γI n and the solution for this problem is the largest eigenvector of ¯ L(K).
We extend the formulism in (7) to the case of soft margin by considering the following problem:min ν,y,λ,δ 1 2 (e + ν − δ + λy) T diag(y)K −1 diag(y)(e + ν − δ + λy) + C δ n i=1 δ 2 i s. t. ν ≥ 0, δ ≥ 0, y ∈ {+1, −1} n (8)where C δ weights the importance of the clustering errors against the clustering margin.
Similar to the previous derivation, we introduce the slack variable z and simplify the above problem as follows:min z,δ,λ 1 2 (z + λe) T K −1 (z + λe) + C e (z e) 2 + C δ n i=1 δ 2 i s. t. (z i + δ i ) 2 ≥ 1, δ i ≥ 0, i = 1, 2, . . . , n(9)By approximating (z i + δ i ) 2 as z 2 i + δ 2 i , we have the dual form of the above problem written as:max γ∈R n n i=1 γ i s. t. P K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 0 0 ≤ γ i ≤ C δ , i = 1, 2, . . . , n(10)The main difference between the above formulism and the formulism in (7) is the introduction of the upper bound C δ for γ in the case of soft margin.
In the experiment, we set the parameter C δ to be 100, 000, a very large value.
As already pointed out, the performance of many clustering algorithms depend on the right choice of the kernel similarity matrix.
To address this problem, we extend the formulism in (10) by including the kernel learning mechanism.
In particular, we assume that a set of m kernel similarity matrices K 1 , K 2 , . . . , K m are available.
Our goal is to identify the linear combination of kernel matrices, i.e., K = m i=1 β i K i , that leads to the optimal clustering accuracy.
More specifically, we need to solve the following optimization problem:max γ,β n i=1 γ i s. t. P m i=1 β i K i −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 0 0 ≤ γ i ≤ C δ , i = 1, 2, . . . , n, m i=1 β i = 1, β i ≥ 0, i = 1, 2, . . . , m(11)Unfortunately, it is difficult to solve the above problem due to the complexity introduced by (m i=1 β i K i ) −1 .
Hence, we consider an alternative problem to the above one.
We first introduce a set of normalized graph Laplacian ¯L 1 , ¯ L 2 , . . . , ¯ L m .
Each Laplacian L i is constructed from the kernel similarity matrix K i .
We then defined the inverse of the combined matrix asK −1 = m i=1 β i ¯ L i .
Then, we have the following optimization problem Figure 3: Data distribution of the three synthesized datasets By solving the above problem, we are able to resolve both γ (corresponding to clustering memberships) and β (corresponding to kernel learning) simultaneously.max γ,β n i=1 γ i s. t. m i=1 β i P ¯ L i P + C e e 0 e 0 − n i=1 γ i I i n+1 0 0 ≤ γ i ≤ C δ , i = 1, 2, . . . , n, m i=1 β i = 1, β i ≥ 0, i = 1, 2, . . . , m( We tested the generalized maximum margin clustering algorithm on both synthetic datasets and real datasets from the UCI repository.
Figure 3 gives the distribution of the synthetic datasets.
The four UCI datasets used in our study are "Vote", "Digits", "Ionosphere", and "Breast".
These four datasets comprise of 218, 180, 351, and 285 examples, respectively, and each example in these four datasets is represented by 17, 64, 35, and 32 features.
Since the "Digits" dataset consists of multiple classes, we further decompose it into four datasets of binary classes that include pairs of digits difficult to distinguish.
Both the normalized cut algorithm [8] and the maximum margin clustering algorithm [1] are used as the baseline.
The RBF kernel is used throughout this study to construct the kernel similarity matrices.
In our first experiment, we examine the optimal performance of each clustering algorithm by using the optimal kernel width that is acquired through an exhaustive search.
The optimal clustering errors of these three algorithms are summarized in the first three columns of Table 1.
It is clear that generalized maximum margin clustering algorithm achieve similar or better performance than both maximum margin clustering and normlized cut for most datasets when they are given the optimal kernel matrices.
Note that the results of maximum margin clustering are reported for a subset of samples(including 80 instances) in UCI datasets due to the out of memory problem.
In the second experiment, we evaluate the effectiveness of unsupervised kernel learning.
Ten kernel matrices are created by using the RBF kernel with the kernel width varied from 10% to 100% of the range of distance between any two examples.
We compare the proposed unsupervised kernel learning to the self-tuning spectral clustering algorithm in [10].
One of the problem with the self-tuning spectral clustering algorithm is that its clustering error usually depends on the parameter k, i.e., the number of nearest neighbor used for computing the kernel width.
To provide a full picture of the self-tuning spectral clustering, we vary k from 1 and 15 , and calculate both best and worst performance using different k.
The last three columns of Table 1 summarizes the clustering errors of generalized maximum margin clustering and self-tuning spectral clustering with both best and worst k. First, observe the big gap between best and worst performance of self-tuning spectral clustering with different choice of k, which implies that this algorithm is sensitive to parameter k. Second, for most datasets, generalized maximum margin clustering achieves similar performance as self-tuning spectral clustering with the best k. Furthermore, for a number of datasets, the unsupervised kernel learning method achieves the performance close to the one using the optimal kernel width.
Both results indicate that the proposed algorithm for unsupervised kernel learning is effective in identifying appropriate kernels.
In this paper, we proposed a framework for the generalized maximum margin clustering.
Compared to the existing algorithm for maximum margin clustering, the new framework has three advantages: 1) it reduces the number of parameters from n 2 to n, and therefore has a significantly lower computational cost, 2) it allows for clustering boundaries that do not pass through the origin, and 3) it can automatically identify the appropriate kernel similarity matrix through unsupervised kernel learning.
Our empirical study with three synthetic datasets and four UCI datasets shows the promising performance of our proposed algorithm.
