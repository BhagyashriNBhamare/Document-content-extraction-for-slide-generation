This paper studies the problem of computing the most frequent element (the mode) by means of a distributed algorithm where the elements are located at the nodes of a network.
Let k denote the number of distinct elements and further let m i be the number of occurrences of the element e i in the ordered list of occurrences m 1 > m 2 ≥ ... ≥ m k.
We give a deterministic distributed algorithm with time complexity O(D+k) where D denotes the diameter of the graph, which is essentially tight.
As our main contribution, a Monte Carlo algorithm is presented which computes the mode in O(D + F 2 /m 2 1 · log k) time with high probability, where the frequency moment F is dened as F = P k i=1 m i.
This algorithm is substantially faster than the deterministic algorithm for various relevant frequency distributions.
Moreover , we provide a lower bound of Ω(D + F 5 /(m 5 1 B)), where B is the maximum message size, that captures the eect of the frequency distribution on the time complexity to compute the mode.
A fundamental requirement of decentralized systems such as, e.g., wireless sensor networks or peer-to-peer networks, is that statistical data about these systems can be acquired in a distributed fashion.
A network component may want to analyze relevant data which has been accumulated in the network in order to learn about the state of the system.
For example, a participant in a peer-to-peer network might be interested in the most popular les or in the total number of available les.
All queries must be performed eciently as nodes may have limited resources such as a small energy supply in the case of sensor networks, or because the size of the network may forbid any operation that requires a single entity to communicate repeatedly with a large number of other entities in the system.Fortunately, many aggregation functions can be computed in the network itself.
Distributive (max, min, count, sum) and algebraic (plus, minus, average, variance) aggregation functions can be computed as follows [15]: In a rst step, a spanning tree is constructed on which the aggregations are performed.
The root sends a message along the spanning tree, asking the leaves to start the aggregation.
The inner nodes of the spanning tree wait until all their children have sent their values, and subsequently forward the aggregated value to their respective parent.
The time complexity of these operations is O(D) where D denotes the diameter of the spanning tree.
Even order statistics and percentiles can be computed eciently by using a k-selection algorithm [10], which computes the k th smallest element in the network.Although a wide range of queries can be computed by combining these aggregation functions, there are essential queries that cannot be answered using any of these functions, e.g., How many disjoint elements are there in the network?
or Which element occurs most often among all elements?
.
While it has been shown that the number of disjoint elements can be approximated eciently [6], less is known about the complexity of nding the mode, i.e., the element which occurs most often, in a distributed fashion.
An algorithm to compute the mode distributively is a useful tool for popularity analyses in large networks.This paper presents a deterministic algorithm which computes the mode in time O(D + k) for general distributions, where D is the network diameter and k is the total number of distinct elements; this is essentially tight up to logarithmic factors.
Our main result however is a distributed Monte Carlo algorithm to nd the mode in general graphs.
This algorithm is especially suited for skewed distributions which naturally arise in various contexts.
For example, the frequencies of terms on webpages, les in le-sharing networks etc., are distributed according to a power law [2,12,17].
We point out that the time complexity of our algorithm is fairly low for such distributions, but our bound on the time complexity is more general and holds for all distributions.
Most of our results are expressed in terms of the frequency moments F = P k i=1 m i , where m i denotes the number of occurrences of the element ei in the ordered list of occurrences, i.e., m1 > m2 ≥ . . . ≥ m k .
1 The proposed algorithm nds the mode in O(D +F 2 /m 2 1 ·log k) time with probability at least 1 − 1/k c for a constant c ≥ 1.
Moreover, we show that it is generally hard to compute the mode for arbitrary distributions by proving a lower bound of Ω(D+F5/(m 5 1 B)), where B is the maximum message size.The remainder of this paper is organized as follows.
In the subsequent section (Section 2), related work is reviewed.
The model used in this work is introduced in Section 3.
Our deterministic and randomized algorithms are both discussed in Section 4, together with their respective analyses.
In Section 5, the aforementioned lower bound is proven.
As an example for skewed distributions, we give the time complexity of the randomized algorithm in the case where the frequencies of the elements follow power laws in Section 6.
Finally, the paper concludes in Section 7.
As mentioned before, while certain aggregation functions such as the sum, minimum or maximum etc. can be computed eciently both in distributed and non-distributed settings, other functions are more dicult.
In a recent paper by Kuhn et al. [10] a randomized algorithm for the kselection problem, where the goal is to nd the k th smallest element, is presented.
In a graph of diameter D consisting of n nodes, where each node holds a single element, the time complexity is O(D log D n) with high probability, which matches the lower bound of Ω(D log D n) also derived in this work.
Thus, nding the median is asymptotically more difcult than computing distributive and algebraic functions.Flajolet et al. [6,8] have studied the problem of determining the number of distinct elements in a multiset.
In their probabilistic LogLog algorithm, all elements are hashed into suciently long binary strings in such a way that all bits closely resemble random, independent and uniformly distributed bits.
As the number of elements that hash to a value with a prex of x 0-bits is k/2 x in expectation, where k denotes the number of distinct elements, the basic idea of the algorithm is that the length of the longest prex consisting of 0-bits can be used as an approximation for log 2 k. By repeating this step m times and by correcting a systematic bias in the asymptotic limit, the resulting estimate is asymptotically unbiased and the standard error is approximately 1.30/ √ m.
A simple reduction from the set disjointness problem shows that the bit complexity to nd the true value k is at least Ω(k) [14].
This implies that, while nding the exact number of distinct elements is in general hard, an accurate estimate can be computed eciently.
The problem of nding the mode, the most frequent element in a list for random access machines has been studied by Munro et al. [5,13].
In [5], they give an algorithm which needs n log n/m + o(n log n/m) + O(n) comparisons in the worst-case, where n is the total number of elements in the list and m is the frequency of the mode.
This is asymptotically optimal up to lower order terms.
Farzan et al. [7] investigate a cache-oblivious model, i.e., a random access memory model with a memory hierarchy where the cache sizes are unknown.
The paper presents an optimal randomized algorithm and a near-optimal deterministic algorithm to compute the mode which minimize the number of cache misses.Alon et al. [1] have studied the space complexity of approximating the frequency moments F = P k i=1 m i in a streaming model (i.e., only one linear pass through the input is done).
Note that the frequency moment F∞ = lim →∞ (F ) 1// = max 1≤i≤k m i is the number of occurrences of the mode.
In particular, in this work the lower bound technique used in [11] is adapted to show that for any xed k ≥ 6 and γ < 1/2, given an input sequence I of at most n elements taken from the set N = {1, 2, ..., n}, any randomized algorithm that outputs a number Z k such thatP[|Z k − F k | > 0.1F k ]< γ uses at leastΩ(n 1−5/k ) memory bits.
This implies a lower bound of Ω(n) on the bit complexity to compute the frequency of the mode.
It also follows that computing the mode itself requires Ω(n) bits, as the frequency of the mode can be computed trivially in O(D) time once the mode is known.While preparing the camera-ready version, we found a paper by Charikar, Chen, and Farach-Colton studying the space complexity of nding frequent elements in a streaming model [3].
Using a dierent algorithm but somewhat similar techniques, they give an algorithm with a space complexity of O ` (k+F2/m 2 k )·log n ´ that nds k elements with frequency at least (1 − ε)m k with high probability, where m k is the frequency of the k th -most frequent element.
Together with some of the ideas of this paper, the techniques of [3] can be adapted for the distributed scenario yielding an alternative distributed algorithm with the same time complexity as the algorithm introduced in this paper.
Similarly, the techniques of this paper could also be used to solve the problem of [3] with the same space complexity.
We provide a deterministic algorithm which is essentially asymptotically optimal for arbitrary frequency distributions.
Furthermore, we derive general upper and lower bounds on the time complexity taking the frequency distribution of the elements into account.
We are given a connected graph G = (V, E) of diameter D with node set V and edge set E.
The diameter of a graph is dened as the length of the longest shortest path between any two nodes.
Each node v ∈ V stores one or more elements e i .
There are K possible elements, i.e., each element e i is chosen from an alphabet A, where |A| = K. For simplicity, we will assume that the alphabet is the set {1, ..., K}, henceforth denoted by [K]; this also implies that the elements can be ordered.
Each element e i ∈ A can appear multiple times, and we dene m i to be the frequency of the element e i , where m 1 > m 2 ≥ ... ≥ m k .
We assume that the frequencies mi are chosen from the set [M ].
Let k denote the total number of distinct elements, and let m = P k i=1 m i be the total number of elements.
This paper will often refer to the following denition.
Definition 3.1.
(Frequency Moments) The th frequency moment F of a multiset containing m i elements of type ei ∈ [K] is dened as F = P k i=1 m i .
Observe that F0 = k is the number of distinct elements in the multiset, and F 1 = m is the total number of elements.
For the sake of simplicity, when describing the algorithm we will assume that the nodes know the frequency moments F0 and F2 as well as the frequency m1 of the mode.
Estimates of these quantities can be obtained eciently in parallel to the computation of the mode, as we will show in Section 4.3.
The nodes compute the mode by exchanging messages.
Two nodes can directly send a message to each other if and only if they are connected by an edge in G.
We consider a classic synchronous message passing model as, e.g., described in [15].
Our algorithms are described in terms of synchronous rounds.
In every round, each node can receive a message from all adjacent nodes, perform some local computations, and send messages to all adjacent nodes.
The time complexity of an algorithm is the number of rounds needed until every node terminates.
Note that all our results also hold if we allow asynchronous communication since every synchronous algorithm can be reformulated as an asynchronous algorithm of the same time complexity [15].
We restrict the size of each message to B ∈ O(log K + log M + log n) bits.
The main obstacle for computations in the described model is edge congestion that is caused by the bound on the size of the messages.
In fact, with arbitrarily large messages, a single convergecast a simple ooding-echo operationwould suce to accumulate all elements at a single node, which could subsequently solve any problem locally.Finally, in this paper it is assumed that a breadth-rst search spanning tree rooted at the node initiating the algorithm has been pre-computed.
As computing such a tree only takes 2D time and all our time bounds are at least linear in D, this assumption is not critical.
First, the deterministic algorithm to compute the mode is presented, together with the proof of the lower bound for arbitrary frequency distributions.
In the second subsection, the randomized algorithm ALG mode whose running time crucially depends on the frequency distribution is described and analyzed.
Finally, it is proven that ALG mode has the same time complexity when a small quasi-random family of hash functions is used, which permits the use of small messages.
There is a straightforward deterministic algorithm to nd the mode executed on the pre-computed spanning tree.
We assume that there is a total order on all the elements, i.e., e 1 > . . . > e k .
The algorithm starts at the leaves of the tree which send element-frequency pairs ei, mi to their parents in increasing order starting with the smallest element that they possess.
Any inner node v stores these pairs received from its children and sums up the frequencies for each distinct element.
Node v forwards e i , m i , where m i is the accumulated frequency of ei in the subtree rooted at v, to its parent as soon as v has received at least one pair ej, mj from each of its children such that e j ≥ e i .
Any node v sends e i , m i to its parent at time t ≤ h + i where h is the height of the subtree rooted at v.
This claim clearly holds for the leaves as each leaf can send the i th smallest element e i at latest at time i. Inductively, a node v thus receives at least the i th smallest element after h+i−1 time, after which it can forward the element including the accumulated frequency to its parent.
Observe that there is no congestion, as node v has already sent all smaller element-frequency pairs in earlier rounds.
Thus, the algorithm terminates after at most O(D +k) steps.
Note that this algorithm does not only compute the mode and its frequency m 1 , but also the frequencies of all other elements.Similarly to the lower bound for the number of distinct elements, a lower bound for the mode problem follows by reduction from the well-known set disjointness problem.
It has been shown that two entities each holding a set of elements of cardinality k/2 must exchange Ω(k) bits in order to determine whether the two sets are disjoint, even using randomization [9,11,16].
This bit complexity implies a time lower bound of Ω(k/B), as by denition each message can contain at most B bits.
Since there are distributions where determining if the sets are disjoint requires Ω(k/B) time and computing the mode for these distributions solves the disjointness problem, 2 it follows that computing the mode also requires Ω(k/B) time.
As D is also a natural lower bound in the distributed model, the time lower bound for the mode problem isΩ(D + k/B).
Although this simple deterministic algorithm is optimal up to a factor B, it is worth investigating other algorithms which take the distribution of the elements into account.
In particular, the skewness of the distribution can reasonably be expected to aect the eciency of an algorithm to compute the mode.
In the following, we present a randomized algorithm whose time complexity is considerably lower for various distributions.
In this section the randomized algorithm ALG mode is introduced, followed by a thorough analysis of its time complexity.
In order to compute the mode, ALG mode makes extensive use of hash functions.
Our analysis is organized in two parts.
First, we assume that the hash functions are chosen independently and uniformly from the set of all possible hash functions mapping the elements e i ∈ [K] to a hash value in the required range R.
We require the size of the range R to be only 2; however, selecting any of these 2 K possible hash functions at random still entails a large communications overhead as choosing one hash function requires the communication of K bits.
Therefore, we subsequently show that ALG mode still works if a random hash function is selected from a much smaller set.
We rst summarize the basic mechanism underlying the algorithm ALG mode .
Each node in the graph stores a local counter c(e i ), which is initially 0, for each of its t elements e1, . . . , et.
The algorithm uses hash functions that map each element randomly to one of two bins with equal probability.
All nodes use the same hash function to compute and subsequently forward the number of its elements that mapped to the rst and the second bin.
Each counter c(e i ) is incremented by the number of elements that have been mapped Algorithm 1 countElementsInBins(h) at node v with mul-tiset e 1 , . . . , e t : 1: c0 = |{ei : h(ei) = −1}| 2: c1 = |{ei : h(ei) = +1}| 3: if Γ(v) = ∅ then 4:send c 0 , c 1 to p(v) 5: else 6: for all vj ∈ Γ(v) in parallel do 7:c (j) 0 , c (j) 1 = countElementsInBins(h) 8: send c 0 , c 1 + P j∈Γ(v) c (j) 0 , c (j) 1 to p(v)to the same bin as element ei.
The idea is to determine the mode by repeating this procedure using dierent hash functions.
Since the mode is likely to end up in the larger bin more often than the other elements, the counter c(e 1 ) will grow faster than the counters of the other elements.
After a rst phase, which reduces the set of candidates for the mode, the frequency of each remaining candidate is computed separately in a second phase.
The time complexity is bounded by the time required to nd a small set of candidates and by the time to check these candidates.We will now study each step of the algorithm in greater detail.
The root node, i.e., the node interested in computing the mode, selects r 1 hash functions h 1 , . . . , h r 1 where hi : A → {−1, +1}, i.e., each hash function maps every element to one of the two bins.
The parameter r1 will be determined later in the analysis of the algorithm.
In the following, we will represent the two bins as a tuple c 0 , c 1 , where c i denotes the number of elements that have been mapped to bin i ∈ {0, 1}.
All nodes then accumulate the mappings by means of a ooding-echo procedure on the spanning tree using the function countElementsInBins parameterized by the hash function: Once the leaves have received information about the hash function, their elements e1, . . . , et are hashed and added to the two bins, i.e., c0 is set to the number of elements that mapped to the rst bin and c 1 is set to the number of the remaining elements.
This tuple is sent to the parent node, which accumulates the tuples from all its children and adds its own tuple.
The resulting tuple is forwarded recursively up to the root.
Let p(v) and Γ(v) denote the parent and the set of children of node v in the spanning tree, respectively.
The sum of two tuples c 0 , c 1 and c 0 , c 1 is dened as c 0 , c 1 , where c i = c i + c i for i ∈ {0, 1}.
This subroutine is summarized in Algorithm 1.
Once the root has computed the nal tuple c0, c1, this tuple is distributed down the spanning tree.
Any node that receives this distribute message forwards it to its children and updates its local counters according to the following rule: For all elements ei that mapped to the larger of the two bins, its counter c(e i ) is increased by |c 0 − c 1 |.
These steps can be carried out in parallel for all r 1 hash functions, i.e., the root can issue one of the r 1 procedure calls in each communication round.
Once the r1 results have been obtained and the tuples have all been distributed, Phase (1) of the algorithm is completed.In the second phase, the r 2 elementsthe parameter r 2 will also be specied laterwith the largest counters are accumulated at the root using the procedure getPotentialModes.
In this procedure, the nodes always forward the element e i , including c(e i ), if its counter is the largest among all those whose element has not been sent yet.
Moreover, In Phase (1), ALG mode executes r1 iterations, where a randomly chosen hash function hi ∈ {h1, h2, ..., hr 1 } assigns all the elements e i ∈ A to one of the two bins in each iteration, i.e., h 1 , ..., h r 1 : A → {−1, +1}.
First, we need to prove a bound on the number r 1 of required hash functions to substantially reduce the set of candidates.Throughout the rest of this section, we will make use of the following helper lemma.
Lemma P h Y ≥ λ p F2[Y ] i ≤ e −λ 2 /2 .
Proof.
It holds that P h Y ≥ λ p F 2 [Y ] i ≤ γ>0 E[e γY ] e γλ √ F 2 [Y ] = Q k i=1 E[e γY i ] e γλ √ F 2 [Y ] ≤ Q k i=1 e γy i +e −γy i 2 e γλ √ F 2 [Y ] = Q k i=1 cosh(γyi) e γλ √ F 2 [Y ] ≤ Q k i=1 e γ 2 y 2 i /2 e γλ √ F 2 [Y ] = e γ 2 P k i=1 y 2 i /2 e γλ √ F 2 [Y ] ≤ e −λ 2 /2 since (e x + e −x )/2 = cosh(x) ≤ e x 2 /2 and by setting γ = λ/ p F 2 [Y ].
Note that F 2 [Y ] = Var(Y ).
The goal of Phase (1) is to reduce the set of elements that could potentially be the mode to a small set of size r 2 .
The following lemma bounds the number r1 of hash functions required to ensure that the counter of the mode is larger than the counter of a large fraction of all elements.Lemma 4.2.
If r 1 ∈ O(F 2 /m 2 1 log(k/ε)) then ∀e i : m i < m 1 /2 it holds that c(e i ) < c(e 1 ) with probability at least 1−ε.Proof.
First, we focus only on the events where the mode e1 and the element e with the maximum frequency among all elements whose frequency is less than half of the frequency e 1 of the mode are put into dierent bins.
All other elements are added randomly to one of the bins, and this procedure is repeated r1 times.
Alternatively, we can say that there are (k − 2)r1 elements α1, . . . , α (k−2)r 1 that are placed randomly into the two bins.
It holds thatP r 1 (k−2) i=1 α 2 i < r 1 · F 2 .
Before the elements α 1 , . . . , α r 1 (k−2) are put into the bins, it holds that c(e 1 ) > c(e ) + r 1 · m 1 /2.
In order to ensure that c(e 1 ) > c(e ) after all elements have been placed in one of the bins, the probability that the other elements compensate this imbalance of at least r1 · m1/2 must be small.
Let the Bernoulli variable Z i indicate into which bin the element α i is placed.
In particular, in case Z i = −1, the element is put into the bin where the mode is, and if Zi = 1, the element is placed into the other bin.
By setting r1 = 8F2/m 2 1 ln(2k/ε) and applying Lemma 4.1 we get thatP 2 4 (k−2)r 1 X i=1 αiZi ≥ r1 · m1/2 3 5 < e − r 2 1 ( m 1 2 ) 2 2 P(k−2)r 1 i=1 α 2 i < e − r 1 m 2 1 8F 2 = ε 2k .
In order to ensure that the elements e 1 and e are often placed into dierent bins, the number of rounds is increased to r1 = 32F2/m 2 1 ln(2k/ε).
Let the random variable U denote the number of times that the two elements are placed into the same bin.
Using a simple Cherno bound we get that the probability that 32F 2 /m 2 1 ln(2k//) rounds do not suce to bound the probability of failure to ε/(2k), because e1 and e are put into dierent bins less than 8F2/m 2 1 ln(2k/ε) times, is itself bounded byP[U > 24F2/m 2 1 ln(2k/ε)] < e −F 2 /m 2 1 ln(2k/ε) < ε/(2k).
Thus, the probability that c(e 1 ) > c(e ) is at least 1 − ε/k.
Let Υ = {e i : m i < m 1 /2} denote the set of all elements for which we want to prove that their counters are lower than the counter of the mode.
The probability that any element e * in this set has a counter larger than the mode isP[∃e * ∈ Υ : c(e * ) > c(e 1 )] < P e∈Υ P[c(e) > c(e 1 )] < k · (ε/k) = ε, which concludes the proof.Note that technically we cannot determine r 1 unless we know F 2 , m 1 , and k.
In the following section, we show that these quantities can all be estimated eciently.
Using this bound on the number of hash functions, we are now in the position to prove the following theorem.
Theorem 4.3.
The time complexity of ALG mode to compute the mode with probability at least 1 − ε on an arbitrarygraph G of diameter D is O " D + F 2 m 2 1 log k ε « .
Proof.
In Phase (1) of the algorithm, r 1 hash functions are applied to all elements and the sum of elements hashed to each bin is accumulated at the root using a simple oodingecho procedure.
It is important to see that all these hash functions can be handled in parallel as opposed to computing the resulting bin sizes for each hash function sequentially, i.e., the number of communication rounds required is bounded by O(D + r1) and not O(D · r1).
Each result is distributed back down the tree in order to allow each node to updates its counters for all its stored elements, which requires O(D) time.
Hence, the time complexity of the rst phase is bounded by O(D + r 1 ).
In Phase (2), the r2 elements with the largest counters are accumulated at the root, and the element with the highest number of occurrences out of this set is returned as the mode.
In order to estimate F 2 , the algorithm described by Alon et al. [1] can be used, which employs a set of four-wise independent hash functions mapping elements to either −1 or 1.
A set H of hash functions is called four-wise independent if for any four elements e 1 , . . . , e 4 , the values h(e i ) of a randomly chosen hash function h from the set are statistically independent.
Hence, for any elements e 1 , . . . , e 4 and any choice c1 . . . , c4 ∈ {−1, 1} there are |H|/16 hash functions h ∈ H for which it holds that h(ei) = ci for all i = {1 . . . , 4}.
It is shown that, by using s := 32 lg(1/ε ) λ 2 of these hash functions h1, . . . , hs, where each function hj is used to computeX j := P k i=1 h j (e i ) · m i, these values X j can be used to compute an estimatê F2 which deviates from F2 by at most λF2 with probability 1 − ε .
The transition from the streaming model to our model is straightforward: The values X j can be computed by usig two counters c −1 and c +1 that sum up the values that map to −1 and +1 (just like in our algorithm), as it holds thatXj = P k i=1 h j (ei) · mi = c+1 − c−1.
Since λ is a constant, we can aggregate all counters and thus all values X j for all hash functions j ∈ {1, . . . , s} at the node that wishes to approximate F 2 in O(D + log(1/ε )) time and compute the estimatê F2 locally.
As mentioned in Section 2, there is a probabilistic algorithm to estimate k = F 0 eciently [6,8].
The algorithm uses hash functions that map elements to bit strings.
The key idea is that the length of the longest prex consisting of 0-bits can be used as an approximation for log 2 k. Several runs of this procedure using randomly chosen hash functions are used in order to bound the variance.
In the distributed version of this algorithm, the lengths of the longest prex of 0-bits for each hash function are accumulated and then used to compute an estimatê k, which also takes O(D) time.
3 Thus, estimatorsˆF2estimatorsˆ estimatorsˆF2 andˆkandˆ andˆk for both F2 and k can be computed in parallel to the computation of the mode.
Since the variances are bounded, we can in fact compute estimates for which it holds thatˆFthatˆ thatˆF 2 ≥ F 2 andˆkandˆ andˆk ≥ k with a certain probability 1 − ε by modifying the original estimates.
Given the estimatorsˆF2estimatorsˆ estimatorsˆF2 andˆkandˆ andˆk, an estimatorˆm1estimatorˆ estimatorˆm1 for m1 can be obtained as follows.
We know that after r1 = 32F2/m 2 1 log(2k/ε) time, the rst phase of the algorithm may terminate.
After each distribution of c 0 , c 1 , h i we determine the frequency of any element e i whose counter is currently the largest in O(D) time and use this frequency as the new estimator forˆm1forˆ forˆm1, if this frequency is greater than any frequency encountered before.
Aggregation rounds are performed iteratively as long as 32ˆF32ˆ 32ˆF 2 / ˆ m 2 1 log 2 ˆ k/ε < T , where T denotes the number of aggregation rounds executed so far.
Once this inequality does no longer hold, we can conclude that the algorithm must have executed enough aggregation rounds, asˆm asˆ asˆm 1 ≤ m 1 .
The algorithm does not run much longer than needed, sincêm 1 ≥ m 1 /2 after 32ˆF32ˆ 32ˆF 2 / ˆ m 2 1 log(2 ˆ k/ε) ≥ r 1 rounds.Note that, since all algorithms exhibit a certain error probability, the number of rounds r 1 of the rst phase must be increased slightly by a small constant factor to ensure that the mode is still computed correctly with probability at least 1 − ε.
We dispense with the analysis of the exact factor.
ALG mode makes extensive use of hash functions.
In Section 4.2, we assumed that the hash functions can be chosen uniformly at random from the set of all 2 K possible hash functions.
However, in order to select one of these hash functions at random, K bits have to be communicated, which is not allowed in our model where we restrict message sizes to at most O(log K + log M + log n) bits.
In the following, we show that our algorithm still works if a random hash function is chosen from a much smaller set.We rst need a few denitions.
Let S ⊆ [K] × [M ] be a subset of all possible element-frequency pairs and let h : [K] → {−1, +1} be a hash function.
We dene the imbalance λS(h) of h with respect to S asλS(h) = P (e i ,m i )∈S h(e i ) · m i p F2[S] ,(1)where F2 [S] is the second frequency moment of the set S.We call a hash function h λ-good with respect to a setS ∈ [K] × [M ] if |λ S (h)| ≤ λ.Let H be a family of hash functions h : [K] → {−1, 1}.
Further, let H e,e ⊆ H be the set of hash functions h for which h(e) = h(e ).
We call H a quasi-random hash family with parameters δ, ε ∈ (0, 1) if the following conditions hold.Let 1 = p 2 ln(5(2 + δ)K 2 /ε)/δ.
(I) 2-independence: For all e, e ∈ [K], ˛ ˛ H e,e ˛ ˛ ≥ (1 − δ) · ˛ ˛ H ˛ ˛ /2.3 It has been pointed out that the somewhat ideal properties of the hash functions assumed in the original work are not required and that by slightly modifying the algorithm it suces to use a set of linear hash functions [1].
(II) Exponentially small imbalance: For all e, e ∈ [K],S ⊆ ` [K] \ {e, e } ´ × [M ], and all integers ≤ 1 , the number of hash functions h ∈ H e,e that are not δ-good with respect to S is at most (2 + δ) · e −(δ) 2 /2 .
(III) Independence of imbalance sign: For all e, e ∈[K], S ⊆ ([K] \ {e, e }) × [M ], and all integers ≤ 1, let Λ e,e ,S () be the set of hash functions h ∈ H e,e that are not (δ − 1)-good but that are δ-good with respect to S.
We have˛ ˛ ˛ ˛ ˛ ˘ h ∈ Λ e,e ,S () ˛ ˛ λ S (h) · h(e) < 0 ¯˛ ˛ − ˛ ˛ ˘ h ∈ Λ e,e ,S () ˛ ˛ λ S (h) · h(e) > 0 ¯˛ ˛ ˛ ˛ ˛ ≤ δ · " e −δ 2 (−1) 2 /2 − e −δ 2 2 /2 " · |H e,e |.
Note that the family of all 2 K hash functions clearly is a quasi-random family for all δ and ε (cf. Lemma 4.1).
However, we will show that there are exponentially smaller quasi-random hash families and that our algorithm achieves the same bounds if the hash functions h1, . . . , hr 1 are chosen uniformly from any quasi-random family H with appropriate values of δ and ε.
We proceed as follows: We rst show that choosing O(F 2 /m 2 1 · log(k/ε)) hash functions from a quasi-random family results in a time complexity of O(D + F2/m 2 1 · log(k/ε)) rounds if the algorithm is allowed to err with probability at most ε.
Subsequently, we prove that quasi-random hash families of size O(poly(K, M )) exist, allowing to run ALG mode with messages of size O(log M + log K + log n).
Assume that we are given a setS ⊆ [K] × [M ] of elements e 1 , . . . , e k ∈ [K] with frequencies m 1 ≥ . . . ≥ m k ∈ [M ]and a quasi-random hash family H.
The hash functions h 1 , . . . , h r 1 in ALG mode are chosen independently and uniformly at random from H. Let ei be an element that occurs less than half as often as the mode, i.e., mi < m1/2.
As above, let H e 1 ,e i ⊆ H be the set of hash functions h for which h(e 1 ) = h(e i ).
As in Section 4.2 for random hash functions, we again need to choose the number of rounds r1 such that c(e1) > c(ei) with suciently large probability.
Let Se i = S \ {(e1, m1), (ei, mi)} and let H e i = H e 1 ,e i ∩ {h 1 , . . . , h r 1 } be the set of chosen hash functions h for which h(e 1 ) = h(e i ).
The dierence ∆ e i between c(e 1 ) and c(e i ) after r 1 rounds can be computed as∆ e i = |H e i | · (m 1 − m i ) + N e i ,(2)whereN e i = X h∈H e i h(e 1 )· X (e,m)∈Se i h(e)·m = X h∈H e i h(e 1 )·λ Se i (h)· p F 2 [S e i ].
Thus, we have to show that N e i > −|H e i |(m 1 − m i ) > −|H e i | ·S e i (h) > c1 · ˛ ˛ He i ˛ ˛ 3 5 < c −|H e i | 2 .
Proof.
For simplicity, assume that 1/δ ∈ Z such that every integer λ = δ for some integer .
Then by using Condition (II) of the quasi-randomness denition, we obtain that for every λ ∈ N, the number of hash functions h ∈ H e 1 ,e i that is not λ-good is at most (2 + δ) · |H e 1 ,e i | · e −λ 2 /2 .
The probability that we want to bound is maximized if for all λ ∈ N we have thatβ λ = ˛ ˛ ˘ h ∈ H e 1 ,e i ˛ ˛ h is not λ-good ¯˛ ˛ ˛ ˛ He 1 ,e i ˛ ˛ = ( (2 + δ) · e −λ 2 /2 if λ ≥ p 2 ln(2 + δ) 1otherwise.
Let H = He 1 ,e i \ Λ∞ and t = ˛ ˛ H ˛ ˛ , and letp := P 2 4 X h∈H λ 2 Se i (h) > c 1 · t 3 5We havep ≤ P 2 4 X h∈H˚|λ h∈H˚h∈H˚|λ Se i (h)| ˇ 2 > c 1 · t 3 5 < γ>0 E » e γ P h∈H l |λ S e i (h)| m 2 - e γc 1 t = Q h∈H E » e γ l |λ S e i (h)| m 2 - e γc 1 t ≤ P ∞ λ=1 (β λ − β λ−1 ) · e γλ 2 e γc 1 !
t ≤ 0 @ (2 + δ) · P ∞ λ=1 " e −(λ−1) 2 /2 − e −λ 2 /2 " · e γλ 2 e γc 1 1 A t = 0 @ (2 + δ) · P ∞ λ=1 e λ 2 (γ−1/2) · " e λ−1/2 − 1 " e γc 1 1 A t .
There are constants c 1 and γ such that the above expression is at most c −t 2 for a constant c 2 > 1.
The claim now follows because for a hash function h that is chosen uniformly at random from He 1 ,e i , P ˆ h ∈ Λ∞˜≤ Λ∞˜Λ∞˜≤ ε/(5K 2 ) and thereforet = Ω(|H e i |) with probability e −Θ(|H e 1 ,e i |) .
We can now bound the value of Ne i with high probability.Lemma 4.5.
If δ < 1/(8c · K · ln 3/2 (K/ε)) and |He i | ≥ c · F 2 [S e i ]/m 2 1 · ln(k/ε) for a suciently large constant c ≥ 1, we get P h Ne i ≤ − m1 2 · |He i | i < ε k .
Proof.
Note that K ≥ k > F2[Se i ]/m 2 1 .
In order to simplify the analysis, we dene˜λdene˜ dene˜λ Se i (h) = δ( − 1) for everyh ∈ Λ + () and˜λand˜ and˜λ S e i (h) = −δ( − 1) for every h ∈ Λ − ().
Instead of N e i ,we now consider the random variable˜N variable˜ variable˜N ei = X h∈H e i h(e 1 ) · ˜ λ Se i (h) · p F 2 [S e i ](3)and getNe i > ˜ Ne i − δ · ˛ ˛ He i ˛ ˛ · p F2[Se i ],since we change no λ Se i (h)-value by more than δ.
We set d() = ˛ ˛ |Λ − ()| − |Λ + ()| to the dierence between the sizes of two symmetric sets of the described partition of H e 1 ,e i .
Since H is a quasi-random family, we have d() ≤ δ(e −δ 2 (−1) 2 /2 − e −δ 2 2 /2 )|H e 1 ,e i | (independence of imbalance sign).
For each 1 ≤ ≤ 1, remove d() hash functions from the larger of the two sets Λ + () and Λ − () and add them to a set Λ .
We then have|Λ + ()| = |Λ − ()| for 1 ≤ ≤ 1 .
Let H 1 = S 1 =1 Λ + () ∪ Λ − () be the hash func- tions in some Λ + () or Λ − () and let H 2 = H e 1 ,e i \(H 1 ∪Λ ∞ )be the set of hash functions from the sets Λ .
We deneX e i = X h∈H 1 ∩H e i h(e 1 ) · ˜ λ Se i (h) Ye i = X h∈H 2 ∩H e i h(e1) · ˜ λS e i (h) Z e i = X h∈Λ∞∩He i h(e 1 ) · λ Se i (h) ,and then have˜Nhave˜ Together with Lemma 4.4 and because λS e i (h)/ ˜ λS e i (h) ≥ 1 for every h ∈ H1, we obtain that for every constant c2 > 1, there is a constant c 1 > 0 such thathave˜N e i = (X e i + Y e i + Z e i ) · p F 2 [S e i ].
Because |Λ + ()| = |Λ − ()| for 1 ≤ ≤ 1 , for every hash function h ∈ H1, there is a corresponding hash function h ∈ H1 such that˜λSthat˜ that˜λS e i (h) = − ˜ λS e i (hP h X e i < −α · p c 1 · |H e i | i < e −α 2 /2 + c −|H e i | 2 .
Choosing α = p 2 ln(5k/ε) and an appropriate constant c 2 (note that |H e i | ≥ c ln(k/ε)), we haveP " X e i < − s 2 · c 1 · ln " 5k ε « · |H e i | # < 2ε 5k .
(4)In order to bound the value of Y e i , remember that|Λ | = d() ≤ δ(e −δ 2 (−1) 2 /2 − e −δ 2 2 /2 )|H e 1 ,e i |.
Hence, the probability that we choose at least one hash function h from H2 with |λSe i (h)| > λ is at most |H e i | · δ · e −λ 2 /2 .
Hence, there is a constant c 3 > 0 such that P " max h∈H 2 ∩H e i ˛ ˛ λ S e i (h) ˛ ˛ > c 3 · s ln " k + |H e i | ε « # ≤ ε 5k .
(5) Let us now consider the size of the set H 2 .
By Condition (III) of the quasi-randomness denition, we have|H 2 | ≤ δ · e · |H e 1 ,e i |.
The probability for choosing a hash function h ∈ H 2 is therefore at most δe.
Using Cherno and E ˆ |H ei ∩ H2|˜≤ H2|˜H2|˜≤ e/ p ln(K/ε), we conclude that there is a constant c4 > 0 such that P " ˛ ˛ He i ∩ H2 ˛ ˛ > c4 · s ln " |H e 1 | ε « # ≤ ε 5k .
(6)Combining Inequalities (5) and (6), we obtainP » Ye i < −c3c4 · ln " k ε «- ≤ 2ε 5k .
(7)Finally, by the denition of 1, we have thatP ˆ Ze i < 0 ˜ ≤ P ˆ |Λ∞| > 0 ˜ ≤ 2(1 + δ) · |H e i | · e −δ 2 2 1 /2 ≤ ε 5k .
(8) Let ν := δ · |He i | + s c 1 · ln " k ε « · |He i | + c3c4 · ln " k ε « !
for some constant c 1 .
Combining Inequalities (3), (4), (7), and (8), we get thatP h N e i < −ν · p F 2 [S e i ] i < ε k (9) for a constant c 1 > 0.
Using 1 ≤ F 2 [S e i ]/m 2 1 ≤ k ≤ K, we obtain δ · |He i | · p F2[Se i ] < p F 2 [S e i ] · |H e i | 8cK ln 3/2 (K/ε) ≤ m 2 1 · |H e i | 8c p F 2 [S e i ] ln 3/2 (K/ε) < m 1 8 · |H e i |.
For the second term in the sum of Inequality (9), we have sc 1 · ln " k ε « · |He i | · F2[Se i ] ≤ p c · c 1 · F 2 [S e i ] ln(k/ε) m 1 ≤ m 1 8 · |H e i |if c is chosen suciently large.
Finally, the third term of Inequality (9) can be bounded byc3c4 · ln " k ε « · p F2[Se i ] ≤ m1 8 · |He i | (10) with p F2[Se i ] ≥ m1and c large enough.
Combining Inequalities (9) (10) completes the proof.Next, an upper bound on the time complexity of ALG mode is derived if the hash functions are chosen from a quasirandom family.
The following theorem shows that we obtain the same asymptotic running time as with hash functions that are chosen uniformly from all possible hash functions.
Theorem 4.6.
If the hash functions are chosen from a quasi-random family H with parameters δ < 1/(8c · K · ln 3/2 (log(K/ε)) and ε ∈ (0, 1), ALG mode needs O(D + F 2 /m 2 1 log(k/ε)) rounds to compute the mode with probability at least 1 − ε.Proof.
We need to show that when using r 1 = O(D + F 2 /m 2 1 log(k/ε)) hash functions, the counters c(e i ) of all elements ei with multiplicity mi < m1/2 are smaller than the counter c(e1) of the mode with probability at least 1−ε.
This follows almost immediately from Equation (2) and Lemma 4.5.
It only remains to show that |H e i | = Ω(r 1 ) with high probability.
This follows immediately from Condition (I) (2-independence) in the quasi-randomness denition by using a Cherno bound.Since we only allow messages of size O(log K + log M + log n), ALG mode is ecient only when applied to a quasirandom family H of size polynomial in K, M , and n.
In the following, using the probabilistic method we show that indeed, small quasi-random hash families exist.
We prove that if we choose suciently many random hash functions, they form a quasi-random family with positive (in fact high) probability.Theorem 4.7.
For all parameters δ ∈ (0, 1) and ε ∈ (0, 1), there is a quasi-random hash family H of size |H| = O((K + M + log(1/δ) + log log(1/ε))/δ 6 ).
Proof.
Let H = ˘ h 1 , . . . , h q ¯ be q hash functions that are chosen independently and uniformly at random from the set of all 2 K possible hash functions h : [K] → {0, 1}.
We need that H is a quasi-random family with parameters δ and ε with positive probability.
Let us now examine the three conditions of the quasi-randomness denition.2-independence: For˜ < K 2 · 2 K · 2 M · 1 · e −δ 2 2e −δ 2 2 1 /2 (1−δ)q/16 .
Independence of imbalance sign:We assume that H satises Conditions (I) and (II).
Let e, e ∈ [K], S ⊆ ` [K]\{e, e } ´ ×[P ˆ Y > δ · Φ · q ˜ = P » Y > δ β · |Λ| - < 2e −δ 2 |Λ|/(4β 2 ) = 2e −δ 2 Φ·q/(4β) .
Based on the assumption that H satises Condition (II), we haveβ Φ ≤ (2 + δ)e −δ 2 (−1) 2 /2 ` e −δ 2 (−1) 2 /2 − e −δ 2 2 /2 ´ 2 = 2 + δ ` 1 − e −δ 2 (−1/2) ´ 2 · e −δ 2 (−1) 2 /2 .
The bound on β/Φ is largest if = 1.
We then get β/Φ ≤ (2 + δ)/(δ 2 /2 − δ 4 /4) 2 .
Hence, we obtainP ˆ H does not satisfy Condition (III) ˜ < K 2 · 2 K · 2 M · 1 · 2e − δ 2 4 · " δ 2 2 − δ 4 4 « 2 ·q .
If we chooseq ≥ C δ 6 · K + M + log p ln(K/ε) δ !!
for a suciently large constant C, the right-hand sides of the inequalities become less than 1/3 and it is therefore possible to satisfy Conditions (I)-(III).
This concludes the proof.
1 · log(k/ε)) rounds for a given error probability ε, and that there indeed exist suciently small sets of hash functions yielding the desired message size.Remark: A natural generalization of our models presented so far is to allow weighted elements, i.e., each element ei ∈ A has a corresponding weight wi.
The task of nding the mode would then translate into nding the element which maximizes the product of the frequency and weight, i.e., the mode is denes as the element ei maximizing wimi.
It is easy to see that our algorithms can be generalized to the case of weighted elements as well.
In this section, we show that for every m1, F5 ∈ N with F5 ≥ m 5 1 , there is a graph G with diameter D and a frequency distribution with maximum frequency m 1 and a 5 th frequency moment F 5 such that nding the mode requires Ω(D + F 5 /(B · m 5 1 )) rounds.
Here B denotes the number of bits that can be transmitted in a single message.To prove our lower bound, we use a reduction from the set disjointness problem, a well-known problem from communication complexity theory [11].
Assume that two nodes u 1 and u2 have sets of elements S1 and S2 with |S1| = |S2| = such that |S1 ∩ S2| ∈ {0, 1}.
In [16], Razborov showed that in order to distinguish between the case where S 1 and S 2 are disjoint and the case where they intersect in exactly one element, u 1 and u 2 have to exchange Ω() bits even if we allow them to err with probability ε ≤ ε0 for a constant ε0.
In order to derive a lower bound on the space complexity for approximating the frequency moments F of a distribution in a streaming model, Alon et al. extended Razborov's lower bound to a scenario with more than two nodes [1] to distinguish between the case where the sets are pairwise disjoint and the case where they intersect in exactly one element is Ω(/d 3 ).
This also holds for randomized algorithms with error probability ε < 1/2.
We prove the lower bound for computing the mode in two steps.
We rst look at a special type of frequency distributions where the frequencies m 2 , . . . , m k are equal up to a factor of 2 and then generalize the result to arbitrary frequency distributions.
In the following, we will assume that all the nodes know about the frequency distribution are bounds There is a total of m1 + k − 1 elements (element e1 occurs m1 times, all other k − 1 elements occur only once).
We distribute these elements among the m 1 leaf nodes such that every leaf node receives e 1 exactly once and (k − 1)/m 1 of the other elements.
Let S i be the set of elements of leaf node ui.
For i = j, we have Si ∩ Sj = {e1}.
we can apply Theorem 5.1 to show that the total number of bits the nodes u 1 , . . . , u d have to communicate in order to nd e 1 is Ω(k /m 1 /m 3 1 ) = Ω(k /m 4 1 ).
This is due to the fact that any algorithm ALG which computes e1 can be used to solve the problem of Theorem 5.1 as follows.
If ALG terminates without returning a value e 1 , we know that the sets S 1 , . . . , S d are pairwise disjoint.
If ALG returns a value e 1 , we can test whether e 1 indeed is in all sets S i be exchanging a logarithmic number of additional bits.
Note that we have 1 + (k − 1)/m1 ≥ m 4 1 (corresponding to the condition ≥ d 4 in Theorem 5.1) because we assumed that The inequality follows because we assume that F5 = m 5 1 + k − 1 ≥ 2m 5 1 .
An interesting and widely studied distribution is the power-law distribution p(x) ∝ x −α for some constant α > 0 [2,12,17], that is, by normalization, m i = 1/i α .
Let m = P k i=1 m i .
It holds that n ∈ Θ(k 1−α ) for α < 1, m ∈ Θ(log k) for α = 1 and m ∈ Θ(1) for α > 1.
For ALG mode , we hence obtain the following upper bounds on the running time T :T ∈ 8 > < > :O ` D + k 1−2α · (log k + log (1/ε)) ´ , if α < 1/2 O ` D + log k · (log k + log (1/ε)) ´ if α = 1/2 O ` D + log k + log (1/ε) ´ , if α > 1/2.
We observe an interesting threshold phenomenon.
If α < 1/2, our randomized algorithm needs polynomial time whereas for α ≥ 1/2, the mode can be determined in polylogarithmic time.
Our lower bound on the time T needed to nd the mode becomes Ω ` D + k 1−5α /(B log k) ´ if α < 1/5.
For α ≥ 1/5, we do not obtain a non-trivial lower bound.
Hence, indeed, there seems to exist a value α 0 ≥ 1/5 such that the time complexity of every algorithm is polynomial in k if α < α0.
This paper has shown that the mode can be computed deterministically in time O(D+k) in arbitrary graphs and that there are distributions for which this is tight up to a logarithmic factor.
In an eort to exploit properties of the actual frequency distribution, we presented a randomized Monte Carlo type algorithm which nds the mode with high probability in time O(D + F2/m 2 1 · log k).
We did not prove that this is tight; however, the lower bound of Ω(D + F 5 /(m 5 1 B)) rounds shows that the general dependence of the upper bound on the skewness of the distribution is correct.
We believe that at least up to polylogarithmic factors the upper bound is the correct bound for all distributions.
An improvement of the lower bound would most likely also solve an open problem from [1]: It is shown that in a streaming model, F 0 , F1, and F2 can be approximated in polylogarithmic space, whereas polynomial space is needed to approximate F for ≥ 6.
It is conjectured in [1] that approximating F 3 , F 4 , and F 5 also requires polynomial time.We formulated all our results for a message passing model where communication is constricted by congestion on edgesnote that we do not bound congestion at nodesand by the adjacency relationships of the underlying network graph.
However, our general approach directly applies to all distributed models (e.g. gossiping [4]) where aggregation can be studied.
The time complexity of our algorithm is then equal to the time complexity of computing O(F2/m 2 1 · log k) independent basic aggregation functions such as computing the sum or the maximum of all elements.
