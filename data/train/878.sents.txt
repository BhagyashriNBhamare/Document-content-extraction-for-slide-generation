Program slicing is a decomposition technique that elides program components not relevant to a chosen computation, referred to as a slicing criterion.
The remaining components form an executable program called a slice that computes a projection of the original pro-gram's semantics.
Using examples coupled with fundamental principles, a tutorial introduction to program slicing is presented.
Then applications of program slicing are surveyed, ranging from its first use as a de-bugging technique to current applications in property verification using finite state models.
Finally, a summary of research challenges for the slicing community is discussed.
1 A Tribute to The Little Lisper[26] Is Column 3 a slice of Column 1 with respect to Column 2?
Original Slicing Program Criterion Slice x = 42 x x = 42 d = 23 d d = 23 x = 42 x = 42 d = 23 d d = 23 Yes, Yes, Yes, regardless of the slicing criterion a program is always a slice of itself, albeit not always the best slice.
A slice is computed with respect to a slicing criterion which consists of a selected variable and a program location.
In the examples, the variable is listed in the middle column at the line that is the program location.
Are the following slices?
x = 42 d = 23 d d = 23 x = 42 x = 42 d = 23 x * On sabbatical leave from Loyola College in Maryland Yes, Yes, at the location contained in the slicing criterion , the slice and the original program compute the same value for the selected variable (this variable is henceforth termed of interest).
Statements not contributing to the computation of this variable can be elided.
How about a = 23 b = 42 b = 42 c = b + 2 c c = b + 2 a = 12 a = 12 x = 23 c = a + 2 c c = a + 2 Yes, Yes, to preserve semantics (value computed) of the chosen variable at the location in the slicing criterion, variables used to assign it a value are also of interest and assignments to them included in the slice.
Next a = 42 a = 42 x = 2 b = 23 + a b = 23 + a y = 3 c = b + 2 c c = b + 2 Yes, variables used in assignments to variables of interest are of interest and assignments to them are included in the slice.
Here, the assignment c = b + 2, causes variable b to be of interest and thus b = 23 + a is included in the slice.
This makes a of interest and subsequently causes a = 42 to be included in the slice.
And a = 10 b = 42 a = 20 a = 20 c = a + 2 c c = a + 2 Yes, only reaching assignments to a variable of interest are included in the slice.
In this case the definition a = 20 kills the previous definition a = 10.
while ( true ) x = 1 y = 2 y y = 2Programs that do not terminate still have (semantically) interesting slices because non-termination can be sliced out.while (P) while (P) { { if (Q) if (Q) break break b = 12 a = a + 1 a = a + 1 } a }The execution of the break affects the number of times the loop executes; thus, a = a + 1 is control dependent on the break and consequently the if statement.if (b) goto L2 L1: L1: y = 1 y = 1 goto L3 goto L3 z = 2 L2: x = 3 goto L1 L3: L3: print(x) print(y) ySimilar to a break statement, a goto affects the flow of control and induces control dependences.
Principal 4 -Union of Slices.
The slice taken with respect to a set of criteria is the same as the union of the slices taken separately with respect to each of the criteria [59].
Principal 4 is illustrated in Figure 1, which also illustrates Principal 3.
Here slicing is used used to extract two of the three interesting subprograms from the word count program.
(The three subprograms compute the number of characters, lines, and words in the program's input).
The center two columns show slices taken with respect to chars and words.
These two smaller programs compute only one of the three outputs.
The final column shows the slice computed with respect to a criteria that contains both chars and words, which is the same as the union of the two individual slices.
The slices in Section 1 are formally syntax preserving static backward slices [19,38,59].
This section introduces several other variants of program slicing.
Backward slices answer the question "what program components might effect a selected computation?"
The dual of backward slicing, forward slicing [10,38], answers the question "what program components might be effected by a selected computation?"
A forward slice captures the impact of its slicing criteria [17].
Forward slicing gets it's name from the ability to compute a forward slice by traversing data and control dependence edges in the forward direction.
In the following example the impact of prod (i.e., those computations potentially influenced by the value assigned to prod at the point) is captured by a forward slice.
sum = 0 prod = 1 prod prod = 1 i = 1 while ( i < 11) { sum = sum + i prod = prod * i prod = prod * i i = i + 1 } This forward slice illustrates, among other things, that a change to the initialization of prod potentially affects (only) the computation of prod found in the loop.
Because a forward slice is often not an executable program, one of the challenges posed by forward slicing is defining the semantics captured by a forward slice [10].
A static backward slice preserve the meaning of the variable(s) in the slicing criterion for all possible inputs to the program.
A dynamic slice does so only for a single input [1,32,60].
The slicing criterion is augmented to include a particular program input.
In the following, using the input 42, the middle two statements can be elided.Input< 42 > read(a) read(a) if (a < 0) a = -a x = 1/a x x = 1/a Conditioned slicing can be viewed as filling the gap between static and dynamic slicing.
A conditioned slice preserves the semantics of the slicing criterion only for those inputs that satisfy a boolean condition [18,20,22].
In the following code when the input value for a is positive, the middle two statements can be elided.read ( Generally conditions can be placed at arbitrary program locations.
In principal, such conditions can be expressed as restrictions on the allowed inputs.
= EOF) { { { { chars++ chars++ chars++ if (c == '\n') lines++ if (isletter(c))) if (isletter(c))) if (isletter(c))) { { { if (!
inword) if (!
inword) if (!
inword) { { { words++ words++ words++ inword = T inword = T inword = T } } } } } } else else else inword = F inword = F inword = F read(c) read(c) read(c) read(c) } } } } The aforementioned slicing techniques involve two requirements: a semantic requirement whereby the slice captures some projection of the original program's semantics and a syntactic requirement whereby the slice is constructed by deleting components from the original program.
Amorphous slicing relaxes the syntactic requirement and thus allows additional transformations to be applied [34].
There are amorphous variants of static, dynamic, and conditioned slicing.
To illustrate transformation's impact on a slice, the first example below applies constant propagation and then traditional slicing, which can then remove the first statement.
This section considers six applications of slicing: four traditional applications and two recent applications.
The first of the traditional applications is slicing's original application, debugging.
This is followed by the application of slicing to testing and then to maintenance.
The final traditional application is in the clustering of equivalent programs statements.
The two recent applications are slicing in support of verifying state-based models and in determining the impact of a database schema change.
Many of these applications exploit relationships between slices.
Program slicing was introduced by Mark Weiser as debugging aid [58,59].
Consider a program that outputs a wrong answer.
A programmer trying to ascertain what went wrong would have less code to consider if they worked with the slice taken with respect to the errant output.An extension of this idea further reduces the search space for the defect: consider the situation where a programmer knows that a program produced one correct and one errant output.
Slices on these outputs can be used to narrow the search for the defect as it is unlikely that the statements that contribute to the correct output contain the defect [47].
In the code below, the slice on the left is taken with respect to chars, which procures a faulty output (it has a seeded initialization defect).
The slice in the middle is taken with respect to lines, which produces the expected output.
Looking at the statements from the first slice that are not in the second, provides candidate statements likely to contain the error.
Of course, there is no guarantee that the fault lies in the difference; it might be an error of omission or one error might mask another in the slice with the correct output.
This application, known as program dicing [47,48], also shows that the study of the relationship between slices can be advantageous.
The aim of regression testing is to ensure that a change to a software system does not introduce new errors in the unchanged part of the program [53].
Testing effort can be reduced if fewer tests cases are run on a simpler program.
Program slicing can be used to partition tests cases into those that need to be re-run, as they may have been affected by a change, and those that can be ignored, as their behavior can be guaranteed to be unaffected by the change [7,9,53].
A partition is formed by finding all affected statements: those statements whose backward slice includes a new or edited statement.
This set can be efficiently computed as the forward slice taken with respect to the new and edited statements.
Only tests that execute an affected statement must be rerun.Slicing can be used to further reduce the cost by reducing the size of the program that must be tested.
This is done by applying semantic differencing [8] to Certified, the program that previous passed the test suite, and Modified, an updated version of Certified.
In the example below Certified has an initialization error (line = 1) that is fixed in Modified.
The program Differences captures (in an executable program) the computations of the changed code.Running Differences on selected tests reduces cost by running fewer tests on a simpler program.
Certified Modified Differences inword = F inword = F chars = 0 chars = 0 lines = 1 lines = 0 lines = 0 words = 0 words = 0 read(c) read(c) read(c) while(c !
= EOF) while(c !
= EOF) while(c !
= EOF) { { { chars++ if (c == '\n') if (c == '\n') lines++ lines++ if (isletter(c))) if (isletter(c))) { { if (!
inword) if (!
inword) { { words++ words++ inword = T inword = T } } } } else else inword = F inword = F read(c) read(c) read(c) } } } Maintaining a large software system is a challenging task.
Most programs spend 70% or more of their life time in the software maintenance phase where they are corrected and enhanced.
Slicing, in the form of Decomposition Slicing [30], reduces the effort required to maintain software.
The decomposition slice, taken with respect to variable v from function f , is the union of the slices taken with respect to v at each definition of v and at the end of f .
Decomposition slicing is another instance of a program slicing technique in which the relationships between slices is advantageously exploited.
Returning to the slice shown in the third column of Figure 1, observe that the if statements inside the main while loop do not appear in the slices on chars or lines; they are solely contained within one slice.
The same is true of the if statement in the slice on lines.
Such statements are called independent.
Furthermore, when all the assignments to a variable are solely within its decomposition slice then the variable is also called independent.
The key insight behind the approach is that changes made to independence statements and variables cannot impact computations in other decomposition slices.
The ability to delimit changes also impacts the amount of regression testing needed [29,30].
Clustering groups a set into subsets where the elements of the subsets are similar to each other, but the subsets are dissimilar.
A dependence cluster is a set of program points (statements), S, that mutually depend upon one another and for which there is no other mutually dependent set that contains S.
This definition is parameterized by an underlying transitive depends relation.One possible definition for depends is that two statements, s 1 and s 2 , depend on each other if they have the same slice [15].
In practice, same size can be used to approximate same slice.
Empirically this approximation is 99.5% accurate [15].
Using this definition, 30 of 45 programs contained a dependence cluster of at least 20% of the code and one of the programs included a cluster composed of 94% of the code.Equivalent decomposition slices can also be used as the depends relation to construct clusters [28].
Variables that have equivalent decomposition slices form equivalence classes.
An empirical study of 67 C programs found that the percentage of equivalent decomposition slices ranged from 50 to 60% (p < 0.005).
The negative side of large dependence can be seen in their impact on software maintenance.
For example, consider trying to reuse a small component that turns out to be part of a large dependence cluster.
On the other hand, having large clusters has a 'good' side: any test coverage method used for one variable in an equivalence class will apply to all the variables in that class.
Furthermore, in support of program comprehension, knowing that a collection of variables belong to a cluster means that they capture the same abstraction.
This observation can significantly reduce comprehension cost.
Recently, finite-state models have been used to specify a number of non-trivial program properties (e.g., the dead-lock freeness of a multi-threaded program).
However, analyzing them is computationally expensive.
Hatcliff, et al. [36] and later Dwyer, et al. [23] use static backward slicing to reduce the cost of property checking.
The experiments make use of Indus, a library of Java program analysis and transformations tools [39].
In the earlier study, Hatcliff, et al. apply program slicing techniques to remove irrelevant code and reduce the size of the corresponding model.
The study of slicing's impact considers two properties: Prop I.i states that when the main thread sends the shutdown value, stage i will eventually shutdown.
Prop II.i states that stage i only shuts down after the shutdown signal has been sent from the main thread.
Figure 2 compares the running time for checking the two properties (I and II) using models generated first from the original program, then from the sliced program, and finally from the sliced program with remaining queue variables abstracted using a classic signs abstract interpretation over the abstract domain {pos, zero, neg, ⊤}.
The cost savings from slicing comes primarily from slicing away of successive pipeline stages.
Slicing based on Prop I.1 and Prop II.1 removes stages two, three and four.
Generating these slice takes less than a second.In later work, Dwyer, et al. [23] compare slicing with call-graph reduction and partial order reduction (POR), which exploits the independence of transitions to create equivalence classes of paths such that only a single path from each equivalence class need be explored.
The motivation for applying slicing is that while existing static analysis attempts to remove unreachable/non-accessed program components, it does not eliminate code fragments that are reachable but where the intertwined code is irrelevant to the property begin checked -a task to which slicing is well suited.
They observe that on average slicing provides a factor of four improvement for nontrivial model checks.
With one exception, slicing always yields a greater reduction than POR.
Dwyer, et al. conclude that "slicing yields non-trivial additional reduction over partial-order reduction" and that "the reduction [due to slicing] is orthogonal to other reductions."
In the second recent application, Maule, et al. study the impact on the source code of a relational database schema change [49].
An important compromise in the approach is the amount of calling-context information processed.
Context-sensitivity is a measure of how precise the calling context of a procedure is represented during analysis.
A common solution is kCFA analysis using a call-string approach to handle context where only the last (or first) k calls are tracked by the analysis.
Maule, et al. record call strings that represent no more than the last k call-sites; e.g., for k = 2, the most recent two call cites are included.They employ program slicing to reduce the cost of identifying statements affected by a database scheme change.
In a case study using the content management system itPublisch, which contains 78,133 lines of code with 417 query executions sites, program slicing reduced the part of the program that had to be considered by 63% (from 191,173 instructions to 70,050 instructions).
Figure 3 shows the execution times of their analysis for different values of k.
It is clear that slicing provides a significant improvement.
Furthermore, while the use of slicing always provides benefit, this benefit increases with k. Statistically, it increases as k increases at a rate of 18 seconds per unit increase in k (R 2 = 0.94).
This is because when k = 1 constant costs of the dataflow analysis that underlies slicing consumes 56% of the runtime.
However, by k = 9, this has dropped to just 16%.
This section considers two current challenges for programs slicing and then speculates as to some future challenges.
The two current challenges include the implementation of slicing tools and the size of the resulting slices.
Each subsection provides first a broad overview and then considers one particular aspect in greater detail.
The first slicers were data-flow based.
They sliced by solving data flow equations [46,59].
This approach was inefficient and gave way to an approach that involves first building a dependence graph to cache the dependence information implicitly considered in the data-flow equations [33,38,39,42,45,51].
The latter approach is preferred when a large number of slices is to be computed [16,43].
Having a dependence graph also has the advantage of supporting other dependence based algorithms [5].
Recently, improvements in dataflow analysis have led to more efficient data-flow based slicers [3].
Other approaches have also been considered.
Ward, for example, implements slicing on top of the FermaT transformation tool which provides a rigorous mathematical foundation [56,57].
Ward's approach is based on set theory and mathematical logic, and is thus independent of any representation technique.As a representative example of this challenge, the problem of slicing parallel programs is considered in greater detail [37].
This requires considering three dependences beyond control and data dependence.
b = b * b } } { { c = b + c c = b + c d = d + c } } b = c + 1 b b = c + 1These two interference dependences arise as b may hold one of three values at c = b + c (11, 21, or 121), depending on the value of P, and the execution order of this assignment and the if statement.The remaining two dependence kinds are control dependences.
The first, parallel dependence, connects the statement that initiates a task and the first statement of that task.
The second, a synchronization dependence, connects statement s 1 to s 2 if the start or termination of s 1 depends on the start or termination of s 2 [37].
The second current challenge facing program slicing is reducing the size of a slice.
In almost all applications of program slicing, the smaller the slice the better.
Empirical study places the size of the average backward slice at about one third of a program [13].
While this is a significant reduction, the remaining code can still be too large to comprehend as a unit.
This observation led to the introduction of techniques such as dynamic slicing [60] and amorphous slicing [34].
One recent addition, thin slicing, reconsiders the fundamental dependences that slicing must capture [54].
A thin slice considers only producer statements, defined in terms of direct uses of variables and is thus always a subset of the traditional slices.
A direct use of a location l is a use of l excluding those that require pointer dereferencing: in the Java statement v = z.f, z is not directly used, but o.f is directly used, assuming that z points to o. Again, the crucial part of the definition is that it ignores pointers.
A producer statement is then defined transitively as the variable of the criterion plus the other statements that write a value to a location used by a producer.The following shows a thin slice, where statements in the 'full' slice but not the thin slice are called explainer statements.
They capture the usual notions of control dependence and heap-based data dependence.
In the example, there is a heap-based data dependence from w.f = y to v = z.f because z and w both point to the object created at x = new A().
When all of the explainers are transitively included, a traditional slice is obtained.
Thin slices have been shown useful in comprehension [54] where they reduce the number of statements that need to be examined in order to find an error by a factor of 3 (as compared to traditional slices), and reduce the number of statements that need to be examined for comprehension by a factor of 9 (as compared to traditional slices).
This section concludes by considering future trends that provide research challenges for those working on and around program slicing.
The increasingly dynamic nature of modern programming languages is an issue for all static analysis.
This trend is clearly evident over the past 20 years in the transition from imperative languages to object-oriented languages to agent-based languages.
In each case, it becomes less possible to predict (statically) which program elements will interact.
Analysis of such programs will inevitably, become more specialized considering only certain classes of inputs or execution environments.
Weiser's initial study demonstrated that programmers intuitively sliced programs while debugging [58].
Early slicing tools attempted to mimic this behavior.
To date, these tools have failed to capture the complete intuition that programmers appear to bring to the task.
Until such time as they do, slicing will continue to be used more as a building block in analysis tools rather than a discipline in and of itself.
The first slicing algorithms applied to programs and produced slices that were executable programs.
This helped defined the term slice.
For better or for worse, what is sliceable and what is a slice have broadened (e.g., the expansion of 'program' slicing to finite state models was described in Section 3.5).
Working in this domain, Korel, et al. report a significant reduction in the size of state-based models [41].
More recently, architecture descriptions written in the UML have been sliced.
A Model Dependence Graph [44] supports slicing by representing UML use cases, classes, objects, and their interconnection similar to how programs are represented in Program Dependence Graph [24].
Fifty years ago programs were composed of assembler instructions.
Modern programs of higher level syntactic entities such as statements and functions.
Looking forward, perhaps future programs will be composed of semantic entities.
Perhaps a library of slices could be selected by a programmers and 'woven' together by a compiler.
This will provide programmers with more "bang for the buck" (assuming the US dollar recovers) or perhaps better stated as more "bang for the keystroke."
Program slicing has been applied to a range of maintenance tasks.
This paper attempts to provide the intuition behind computing a program slice and considers several representative applications.
Several surveys on various types and applications of program slicing have been written and provide excellent sources for further information on program slicing [12,14,40,55].
Finally, this paper lays out some challenges and future work for program slicing researchers.
Our special thanks to the following for their comments and suggestions Francoise Balmas, Andrea De Lucia, Mark Harman, Michael Hind, Jim Lyle, Rajib Mall, Thomas Reps, Frank Tip, Paolo Tonella, Neil Walkinshaw, and Martin Ward.
Dave Binkley is supported by EPSRC grant GR/F010443 to the CREST Centre.
