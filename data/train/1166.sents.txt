Information leakage traditionally has been defined to occur when uncertainty about secret data is reduced.
This uncertainty-based approach is inadequate for measuring information flow when an attacker is making assumptions about secret inputs and these assumptions might be incorrect ; such attacker beliefs are an unavoidable aspect of any satisfactory definition of leakage.
To reason about information flow based on beliefs, a model is developed that describes how attacker beliefs change due to the attacker's observation of the execution of a probabilistic (or determin-istic) program.
The model leads to a new metric for quantitative information flow that measures accuracy rather than uncertainty of beliefs.
Qualitative security properties, such as noninterference [10], typically either prohibit any flow of information from a high security level to a lower level, or they allow any amount of flow so long as it passes through some release mechanism.
For a program whose correctness requires flow from high to low, the former property is too restrictive and the latter can lead to unbounded leakage of information.
Quantitative flow properties, such as "at most k bits leak per execution of the program", allow information flows but at restricted rates.
Such properties are useful when analyzing programs whose nature requires that some-but not too much-information be leaked.
Examples of these programs This work was supported by the Department of the Navy, Office of Naval Research, ONR Grant N00014-01-1-0968; Air Force Office of Scientific Research, Air Force Materiel Command, USAF, grant number F49620-03-1-0156; and National Science Foundation grants 0208642, 0133302, and 0430161.
Michael Clarkson is supported by a National Science Foundation Graduate Research Fellowship; Andrew Myers is supported by an Alfred P. Sloan Research Fellowship.
Opinions, findings, conclusions, or recommendations contained in this material are those of the authors and do not necessarily reflect the views of these sponsors.
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.include guards, which sit at the boundary between trusted and untrusted systems, and password checkers.Defining the quantity of information flow is more difficult than it might seem.
Consider a password checker PWC that sets an authentication flag a after checking a stored password p against a (guessed) password g supplied by the user.PWC : if p = g then a := 1 else a := 0For simplicity, suppose that the password is either A, B, or C. Suppose also that the user is actually an attacker attempting to discover the password, and he believes the password is overwhelmingly likely to be A but has a minuscule and equally likely chance to be either B or C. (This need not be an arbitrary assumption on the attacker's part; perhaps the attacker was told by a usually reliable informant that the password is A.) If the attacker experiments by executing PWC and guessing A, he expects the outcome to be that a is equal to 1.
Such a confirmation of the attacker's belief does seem to convey some small amount of information.
But suppose that the informant was wrong: the real password is C.
The outcome of this experiment has a equal to 0, from which the attacker infers that A is not the password.
Common sense dictates that his new belief is that B and C each have a 50% chance of being the password.
The attacker's belief has greatly changed-he is surprised to discover the password is not A-so this outcome of his experiment seems to convey a larger amount of information than the previous outcome.
Thus, the information conveyed by executing PWC depends on what the attacker believes.How much information flows from p to a in each of the above experiments?
Answers to this question have traditionally been based on change in uncertainty [5,20,11,1,16,2,17]: information flow is measured by the reduction in uncertainty about secret data.
Observe that, in the case where the password is C, the attacker initially is quite certain (though wrong) about the value of the password and after the experiment is rather uncertain about the value of the password; the change from "quite certain" to "rather uncertain" is an increase in uncertainty.
So according to a reduction in uncertainty metric, no information flow occurred, which flatly contradicts our intuition.The problem with metrics based on uncertainty is twofold.
First, they do not take accuracy into account.
Accuracy and uncertainty are orthogonal properties of the attacker's belief-being certain does not make one correct-and as the password checking example illustrates, the amount of information flow depends on accuracy rather than on uncertainty.
Second, uncertainty-based metrics are concerned with some unspecified agent's uncertainty rather than an attacker's.
The unspecified agent is able to observe a probability distribution over secret input values but cannot observe the particular secret input used in the program execution.
If the attacker were the unspecified agent, there would be no reason in general to assume the probability distribution the attacker uses is correct.
Because the attacker's probability distribution is therefore subjective, it must be treated as a belief.
Beliefs are thus an essential-though until now uninvestigated-component of information flow.This paper presents a new way of measuring information flow, based on these insights.
Section 2 gives basic representations and notations for beliefs and programs.
Section 3 describes a model of the interaction between attackers and systems; it also describes how attackers update beliefs by observing execution of programs.
Section 4 defines a new quantitative flow metric, based on information theory, that characterizes the amount of information flow due to changes in the accuracy of an attacker's belief.
The model and metric are formulated for use with any programming language (or even any state machine) that can be given a denotational semantics compatible with the representation of beliefs, and Section 5 illustrates with a particular programming language (while-programs plus probabilistic choice).
Section 6 discusses related work, and Section 7 concludes.
A belief is a statement an agent makes about the state of the world, accompanied by some measure of how certain the agent is about the truthfulness of the statement.
We begin by developing mathematical structures for representing beliefs.
A frequency distribution is a function δ that maps a program state to a frequency, where a frequency is a nonnegative real number.
A frequency distribution is essentially an unnormalized probability distribution over program states; frequency distributions are often better than probability distributions as the basis for a programming language semantics [21].
Henceforth, we write "distribution" to mean "frequency distribution".
The set of all program states is State, and the set of all distributions is Dist.
The structure of State is mostly unimportant; it can be instantiated according to the needs of any particular language or system.
For our examples, states map variables to values, where Var and Val are both countable sets.v ∈ Var σ ∈ State Var → Val δ ∈ Dist State → R +We write a state as a list of mappings; e.g.(g → A, a → 0)is a state in which variable g has value A and a has value 0.
The mass in a distribution δ is the sum of frequencies:δ σ δ(σ)A probability distribution has mass 1, but a frequency distribution may have any non-negative mass.
A point mass is a probability distribution that maps a single state to 1.
It is denoted by placing a dot over that single state:˙ σ λσ .
if σ = σ then 1 else 0 Execution of program S is described by a denotational semantics in which the meaning [[S]] of S is a function of type State → Dist.
This semantics describes the frequency of termination in a given state: if [[S]]σ = δ, then the frequency of S, when begun in σ, terminating in σ should be δ(σ ).
This semantics can be lifted to a function of type Dist → Dist by the following definition:[[S]]δ σ δ(σ) · [[S]]σThus, the meaning of S over a distribution of inputs is completely determined by the meaning of S given a state as input.
By defining programs in terms of how they operate on distributions we permit analysis of probabilistic programs.
Section 5 shows how to build such a semantics.Our examples use while-programs extended with a probabilistic choice construct.
Let metavariables S, v, E, and B range over programs, variables, arithmetic expressions, and Boolean expressions, respectively.
Evaluation of expressions is assumed side-effect free, but we do not otherwise prescribe their syntax or semantics.
The syntax of the language is:S ::= skip | v := E | S; S | if B then S else S | while B do S | S p SThe operational semantics for the deterministic subset of this language is standard.
Probabilistic choice S 1 p S 2 executes S 1 with probability p or S 2 with probability 1 − p.
We need a way to identify secret data; confidentiality labels serve this purpose.
For simplicity, assume there are only two such labels: a label L that indicates lowconfidentiality (public) data, and a label H that indicates high-confidentiality (secret) data.
Assume that State is a product of two domains State L and State H , which contain the low-and high-labeled data, respectively.
A low state is an element σ L ∈ State L ; a high state is an element σ H ∈ State H .
The projection of state σ ∈ State onto State L is denoted σ L; this is the part of σ visible to the attacker.
Projection onto State H , the part of σ not visible to the attacker, is denoted σ H.Each variable in a program is labeled to indicate the confidentiality of the information stored in that variable; for example, x L is a variable that contains low information.
For convenience, let variable l be labeled L and variable h be labeled H. Var L is the set of variables in a program that are labeledL, so State L = Var L → Val.
The low projection σ L of state σ is: σ L λv ∈ Var L .
σ(v)States σ and σ are low-equivalent, written σ ≈ L σ , if they have the same low projection:σ ≈ L σ (σ L) = (σ L)Distributions also have projections.
Let δ be a distribution and σ L a low state.
Then (δ L)(σ L ) is the combined frequency of those states whose low projection isσ L : 1 δ L λσ L ∈ State L .
σ | (σ L) = σ L δ(σ )High projection and high equivalence are defined by replacing occurrences of L with H in the definitions above.
Many belief representations have been proposed [13].
To be usable in our framework, a belief representation must support certain natural operations.
Let b and b be beliefs about sets of possible worlds W and W , respectively, where a world is an elementary outcome about which beliefs can be held.
1 Formula x∈D | R P is a quantification in which is the quantifier (such as ∀ or Σ), x is the variable that is bound in R and P , D is the domain of x, R is the range, and P is the body.
We omit D, R, and even x when they are clear from context; an omitted range means R = true.2.
Belief update b|U is the belief that results when b is updated to include new information that the actual world is in set U ⊆ W of possible worlds.3.
Belief distance D(b b ) is a real number r ≥ 0 quantifying the difference between b and b .
While the results in this paper are, for the most part, independent of any particular representation, the rest of this paper uses distributions to represent beliefs.
High states are the possible worlds for beliefs, and a belief b is a probability distribution over high states, i.e. b = 1.
Whereas distributions correspond to positive measures, beliefs correspond to probability measures.
Probability measures are well-studied as a belief representation [13], and they have several advantages here: they are familiar, quantitative, support the operations required above, and admit a programming language semantics (as shown in Section 5).
There is also a nice justification for the numbers they produce: roughly, b(σ) characterizes the amount of money an attacker should be willing to bet that σ is the actual state of the system [13].
For belief product ⊗, we employ a distribution product ⊗ of two distributions δ 1 : A → R + and δ 2 : B → R + , with A and B disjoint:δ 1 ⊗ δ 2 λ(σ 1 , σ 2 ) ∈ A × B .
δ 1 (σ 1 ) · δ 2 (σ 2 )It is easy to check that if b and b are beliefs, b ⊗ b is too.For belief update |, we use distribution conditioning:δ|U λσ .
if σ ∈ U then δ(σ) σ ∈U δ(σ ) else 0For belief distance D we use relative entropy, an information-theoretic metric [14] for the distance between distributions.D(b b) σ b(σ) · log b(σ) b (σ)The base of the logarithm in D can be chosen arbitrarily; we use base 2 and write lg to indicate log 2 , making bits the unit of measurement for distance.
The relative entropy of b to b is the expected inefficiency (that is, the number of additional bits that must be sent) of an optimal code that is constructed by assuming an inaccurate distribution over symbols b when the real distribution is b [14].
Like an analytic metric, D(b b) is always at least zero and D(b b) equals zero only when b = b .
2 Relative entropy has the property that if b(σ) > 0 andb (σ) = 0, then D(b b) = ∞.
An infinite distance between beliefs would cause difficulty in measuring change in accuracy.
To avoid this anomaly, beliefs may be required to satisfy certain restrictions.
For example, an attacker's belief b might be restricted such that:(min σ H b(σ H )) ≥ · 1 |State H|for some > 0, which ensures that b is never off by more than a factor of from a uniform distribution; we call such beliefs admissible.
Other admissibility restrictions may be substituted for this one when stronger assumptions can be made about attacker beliefs.
We formalize as an experiment how an attacker, an agent that reasons about beliefs, revises his beliefs from interaction with a system, an agent that executes programs.
The attacker should not learn about the high input to the program but is allowed to observe (and perhaps influence) low inputs and outputs.
Other agents (a system operator, other users of the system with their own high data, an informant upon which the attacker relies, etc.) might be involved when an attacker interacts with a system; however, it suffices to condense all of these to just the attacker and the system.We are chiefly interested in the program S with which the attacker is interacting, and conservatively assume that the attacker knows the source code of S. For simplicity of presentation, we assume that S always terminates and that it never modifies the high state.
Section 3.4 discusses how both restrictions can be lifted without significant changes.
Formally, an experiment E is a tuple:E = S, b H , σ H , σ Lwhere S is the program, b H is the attacker's belief, σ H is the high projection of the initial state, and σ L is the low projection of the initial state.
The protocol for experiments, which uses some notation defined below, is summarized in An attacker's prebelief, describing his belief at the beginning of the experiment (step 1), may be chosen arbitrarily (subject to the admissibility requirement in Section 2.4) or may be informed by previous experiments.
In a series of experiments, the postbelief from one experiment typically becomes the prebelief to the next.
The attacker might even choose a prebelief b H that contradicts his true subjective probability distribution for the state, and this gives our analysis additional power by allowing the attacker to conduct experiments to answer questions such as "What would happen if I were to believe b H ?"
.
The system chooses σ H (step 2(a)), the high projection of the initial state, and this part of the state might remainAn experiment E = S, b H , σ H , σ L is conducted as fol- lows.1.
The attacker chooses a prebelief b H about the high state.2.
(a) The system picks a high state σ H (b) The attacker picks a low state σ L .3.
The system executes the program S, which produces a state σ ∈ Γ(δ ) as output, whereδ = [[S]]( ˙ σ L ⊗ ˙ σ H ).
The attacker observes the low projection of the output state: o = σ L. Figure 1.
Experiment Protocol constant from one experiment to the next or might vary.
For example, Unix passwords do not usually change frequently, but the PINs on RSA SecurID tokens change each minute.
We conservatively assume that the attacker chooses all of σ L (step 2(b)), the low projection of the initial state, since this gives additional power in controlling execution of the program.
3 The attacker's choice of σ L is likely to be influenced by b H , but for generality, we do not require there be such a strategy.
H = (([[S]]( ˙ σ L ⊗ b H )|o)) HProgram S is executed (step 3) only once in each experiment; multiple executions are modeled by multiple experiments.
The meaning of S given input ˙ σ L ⊗ ˙ σ H is an output distribution δ :δ = [[S]]( ˙ σ L ⊗ ˙ σ H )From δ the attacker makes an observation, which is a low projection of an output state.
Probabilistic programs may yield many possible output states, but in a single execution of the program, only one output state is actually produced.
So in a single experiment, the attacker is allowed only a single observation.
The choice of a state from a distribution is modeled by sampling operator Γ, where Γ(δ) generates a state σ (from the domain of δ) with probability δ(σ)/δ.
To emphasize the fact that the choice is made randomly, assignment of a sample is written σ ∈ Γ(δ), using ∈ instead of =.
The observation o resulting from δ is:o ∈ Γ(δ ) LThe formula the attacker uses for postbelief b H (step 4) involves two operations.
The first is to use the semantics of S along with prebelief b H as the distribution on high input.
This "thought experiment" allows the attacker to generate a prediction of the output distribution.
We define prediction δ A to correlate the output state with the high input state:probability p H b H b H1 b H2 A 0.98 1 0 B 0.01 0 0.5 C 0.01 0 0.5 Table 1.
Beliefs about p H p g a δ A δ A |o 1 δ A |o 2 A A 0 0 0 0 A A 1 0.98 1 0 B A 0 0.01 0 0.5 B A 1 0 0 0 C A 0 0.01 0 0.5 C A 1 0 0 0 . . . 0 0 0δ A = [[S]]( ˙ σ L ⊗ b H )The second operation is to incorporate any additional inferences that can be made from the observation by conditioning prediction δ A on observation o.
The result is projected to H to produce the attacker's postbelief b H :b H = (δ A |o) HHere, conditioning operator | is a specialization of distribution conditioning operator δ|U .
The specialization removes all mass in distribution δ that is inconsistent with observation o, then normalizes the result:δ|o δ|{σ | σ L = o} = λσ .
if (σ L) = o then δ(σ) (δL)(o) else 0 Adding confidentiality labels to the password checker of Section 1 yields:PWC : if p H = g L then a L := 1 else a L := 0An analysis of PWC in terms of our experiment model allows the informal reasoning in Section 1 to be made precise, as follows.The attacker starts by choosing prebelief b H , perhaps as specified in the column labeled b H of Table 1.
Next, the system chooses initial high projection σ H , and the attacker chooses initial low projection σ L .
In the first experiment in Section 1, the password was A, so the system chooses σ H = (p → A).
Similarly, the attacker chooses σ L = (g → A, a → 0).
(The initial value of a is actually irrelevant, since it is never used by the program and a is set along all control paths.)
Next, the system executes PWC .
Output distribution δ is a point mass at the state σ = (p → A, g → A, a → 1); the semantics in Section 5 will validate this intuition.
Since σ is the only state that can be sampled from δ , the attacker's observation o 1 is σ L = (g → A, a → 1).
In the final step of the protocol, the attacker infers a postbelief.
He conducts a thought experiment, predicting an out- Table 2.
The ellipsis in the final row of the table indicates that all states not shown have frequency 0.
This distribution is intuitively correct: the attacker believes that he has a 98% chance of being authenticated, whereas 1% of the time he will fail to be authenticated because the password is B, and another 1% because it is C.
The attacker conditions prediction δ A on observation o 1 , obtaining δ A |o 1 , also shown in Table 2.
Projecting to high yields the attacker's postbelief b H1 , shown in Table 1.
This postbelief is what the informal reasoning in Section 1 suggested: the attacker is certain that the password is A.put distribution δ A = [[PWC ]]( ˙ σ L ⊗ b H ), given inThe second experiment in Section 1 can also be formalized by an experiment.
In it, b H and σ L remain the same as before, but σ H becomes (p → C).
Observation o 2 is therefore the point mass at (g → A, a → 0).
The prediction δ A remains unchanged, and conditioned on o 2 it becomes δ A |o 2 , shown in Table 2.
Projecting to high yields the new postbelief b H2 in Table 1.
This postbelief again agrees with the informal reasoning: the attacker believes that there is a 50% chance each for the password to be B or C.
The formula the attacker uses to infer a postbelief in step 4 is an application of Bayesian inference, which is a standard technique in applied statistics for making inferences when uncertainty is made explicit through probability models [9].
Let belief revision operator B yield the postbelief from an experimentE = S, b H , σ H , σ L : B(E) (([[S]]( ˙ σ L ⊗ b H )|o)) H where o ∈ Γ(δ ) L δ = [[S]]( ˙ σ L ⊗ ˙ σ H )Because it uses Γ, operator B produces values by sampling, so we write b H ∈ B(E).
To select a particular b H from B, we provide observation o:B(E, o) (([[S]]( ˙ σ L ⊗ b H )|o)) HThe fundamental Bayesian method of updating a hypothesis Hyp based on an observation obs is Bayes' rule:Pr(Hyp|obs) = Pr(Hyp)Pr(obs|Hyp) Hyp Pr(Hyp )Pr(obs|Hyp )In our model, the attacker's hypothesis is about the values of high states, so the domain of hypotheses is State H. Therefore Pr(Hyp), the probability the attacker ascribes to a particular hypothesis σ H , is modeled by b H (σ H ).
The probability Pr(obs|Hyp) the attacker ascribes to an observation given the assumed truth of a hypothesis is modeled by the program semantics: the probability of an observa-tion o given an assumed high input σ H is ([[S]]( ˙ σ L ⊗ ˙ σ H ) L)(o).
Given experiment E = S, b H , σ H , σ L , instantiat- ingBayes rule on these probability models yields B(E, o), which is Pr(σ H |o):B(E, o) = b H (σ H ) · ([[S]]( ˙ σ L ⊗ ˙ σ H ) L)(o) σ H b H (σ H ) · ([[S]]( ˙ σ L ⊗ ˙ σ H ) L)(o)With this instantiation, we can show that how an attacker updates his belief according to our experiment protocol is equivalent to Bayesian updating.Theorem 1B(E, o)(σ H ) = B(E, o)Proof.
In Appendix B. Section 3.1 invokes two simplifying assumptions about program S: it never modifies high input, and it always terminates.
We now dispense with these mostly minor technical issues.To eliminate the first assumption, note that if S were to modify the high state, the attacker's prediction δ A would correlate high outputs with low outputs.
However, to calculate a postbelief (in step 4), δ A must correlate high inputs with low outputs.
So our experiment protocol requires the high input state be preserved in δ A .
Informally, we can do this by copying the high input state and requiring that the copy be immutable.
Thus, the copy is preserved in the final output state, and the attacker can again establish a correlation between high inputs and low outputs.
Technical details are given in Appendix A.To eliminate the second assumption, note that program S must terminate for an attacker to obtain a low state as an observation when executing S.
There are two ways to model the observation in the case of nontermination, depending on whether the attacker can detect nontermination.
If the attacker has an oracle that decides nontermination, then nontermination can be modeled in the standard denotational style with a state ⊥ that represents the divergent state.
Details of this approach are given in Appendix A.
An attacker that cannot detect nontermination is more difficult to model.
At some point during the execution of the program, he will stop waiting for the program to terminate and declare that he has observed nontermination.
However, he may be incorrect in doing so-leading to beliefs about nontermination and instruction timings.
The interaction of these beliefs with beliefs about high inputs is complex; we leave this for future work.
The informal analysis of PWC in Section 1 suggests that information flow corresponds to an improvement in the accuracy of an attacker's belief.
Recall that the more accurate belief b is with respect to high state ˙ σ H , the less the distance D(b ˙ σ H ).
We use change in accuracy, as measured by distance, to quantify information flow.
Given an experiment E = S, b H , σ H , σ L , an outcome is a pair E, b H such that b H ∈ B(E).
Q(E, b H ) D(b H ˙ σ H ) − D(b H ˙ σ H )Thus the amount of information flow (in bits) in Q corresponds to the improvement in the accuracy of the attacker's belief.With an additional definition from information theory, a more consequential characterization of Q is possible.
Let I δ (F ) denote the information contained in event F drawn from probability distribution δ:I δ (F ) − lg Pr δ (F )Information is sometimes called "surprise" because I measures how surprising an event is; for example, when an event that has probability 1 occurs, no information (i.e. 0 bits) is conveyed because the occurrence is completely unsurprising.For an attacker, the outcome of an experiment involves two unknowns: the initial high state σ H and the probabilistic choices made by the program.
Letδ S = [[S]]( ˙ σ L ⊗ ˙ σ H )L be the system's distribution on low outputs, andδ A = [[S]]( ˙ σ L ⊗ b H )L be the attacker's distribution on low outputs.
I δ A (o) measures the information contained in o about both unknowns, but I δ S (o) measures only the probabilistic choices made by the program.
4 For programs that make no probabilistic choices, δ A contains information about only the initial high state, and δ S is a point mass at some state σ such that σ L = o.
So the amount of information I δ S (o) is 0.
For probabilistic programs, I δ S (o) is generally not equal to 0; subtracting it removes all the information contained in I δ A (o) that is solely about the outcomes of probabilistic choices, leaving information about high inputs only.The following theorem states that Q measures the information about high input σ H contained in observation o.Theorem 2 Q(E, b H ) = I δ A (o) − I δ S (o)Proof.
In Appendix B.As an example, consider the experiments involving PWC in Section 3.2.
The first experiment E 1 has the attacker correctly guess the password A, so:E 1 = PWC , b H , (p → A), (g → A, a → 0)where b H (and the other beliefs about to be used) is defined in Table 1.
Only one outcome, E 1 , b H1 , is possible from this experiment.
Calculating Q(E 1 , b H1 ) yields a flow of 0.0291 bits from the outcome.
The small flow makes sense because the outcome has only confirmed something the attacker already believed to be almost certainly true.
In experiment E 2 the attacker guesses incorrectly.E 2 = PWC , b H , (p → C), (g → A, a → 0)Again, only one outcome E 2 , b H2 is possible from this experiment, and calculating Q(E 2 , b H2 ) yields an information flow of 5.6439 bits.
This higher information flow makes sense, because the attacker's postbelief is much closer to correctly identifying the high state.
The attacker's prebelief b H ascribed a 0.02 probability to the event [p = A], and the information of an event with probability 0.02 is 5.6439.
This suggests that Q is a reasonable measure for the information about high input contained in the observation.Another interpretation of the number produced by our definition of Q is possible: the amount of information flow Q is the improvement in expected inefficiency of the attacker's optimal code for the high input.
This is because relative entropy can be interpreted in terms of coding efficiency (see Section 2.4).
We have yet to fully investigate this interpretation.
The information flow in experiment E 2 might seem surprisingly high.
At most two bits are required to store password p in memory, so how can the program leak more than five bits?
In brief, this occurs because the attacker's belief is so erroneous that a large amount of information is required to correct it.
This also illuminates the difference between measuring information flow based on uncertainty versus based on accuracy.Consider how an uncertainty-based approach would analyze the program, if the attacker's belief were used as the input distribution over high inputs.
The attacker's initial uncertainty about p is H(b H ) = 0.1614 bits, where H is the information-theoretic measure of entropy, or uncertainty, in a probability distribution δ.H(δ) − σ δ(σ) · lg δ(σ)Maximum entropy is achieved by uniform distributions [14], so the maximal uncertainty about p is lg 3 ≈ 1.6 bits, the same number of bits required to store p.
In the second experiment, the attacker's final uncertainty about p is Usually, FLIP sets l to be h, so the attacker will expect this to be the case.
Executions in which this occurs will cause his postbelief to be more accurate, but may cause his uncertainty to either increase or decrease, depending on his prebelief; when uncertainty increases, an uncertainty metric would mistakenly say that no flow has occurred.With probability 0.01, FLIP produces an execution that fools the attacker and sets l to be ¬h, causing his belief to become less accurate.
The decrease in accuracy results in misinformation, which is a negative information flow.
When the attacker's prebelief is almost completely accurate, such executions will make him more uncertain.
But when the attacker's prebelief is uniform, executions that result in misinformation will make him less uncertain; when uncertainty decreases, an uncertainty metric would mistakenly say that flow has occurred.
Table 3 demonstrates this phenomenon concretely.
The quadrant labels refer to Figure 2.
For each quadrant, the attacker's prebelief b H , observation o, and the resulting postbelief b H is given in the top half of the table.
In the bottom half, increase in accuracy is calculated using the information flow metric Q(E, b H ), and reduction in uncertainty is calculated using the difference in entropy H(b H ) − H(b H ).
Finally, recall that when the attacker guessed a password incorrectly in Section 1, his belief became more accurate and more uncertain.
Table 4 gives the exact changes in his accuracy and uncertainty, using guess g = A and password p = C.In summary, uncertainty is inadequate as a metric for information flow if input distributions represent attacker beliefs.
By Theorem 2, information flows when an attacker's belief becomes more accurate, but an uncertainty metric can mistakenly measure a flow of zero or less.
Inversely, misinformation flows when an attacker's belief becomes less accurate, but an uncertainty metric can mistakenly measure a positive information flow.
Hence, in the presence of beliefs, accuracy is the correct metric for information flow.
p b H : A 0.98 B 0.01 C 0.01 b H : A 0 B 0.5 C 0.5 Increase in accuracy +5.6439 Reduction in uncertainty −0.8245 Since an experiment on a probabilistic program S can produce many outcomes, it is reasonable to assume that quantitative information flow properties will discuss expected flow over those outcomes.
So we define expected flow Q E over all outcomes from experiment E:Q E (E) E o∈δ L [Q(E, B(E, o))] = o (δ L)(o) · Q(E, ([[S]]( ˙ σ L ⊗ b H )|o) H)) where δ = [[S]]( ˙ σ L ⊗ ˙ σ H )gives the distribution on outcomes and E δ [X] is the expected value of an expression X with respect to distribution δ.Expected flow is useful in analyzing probabilistic programs.
Consider a faulty password checker:FPWC :if p = g then a := 1 else a := 0; a := ¬a 0.1 skipWith probability 0.1, FPWC inverts the authentication flag.
Can this program be expected to confound attackers-does FPWC leak less expected information than PWC ?
This question can be answered by comparing the expected flow from FPWC to the flow of PWC .
Table 5 gives information flows from FPWC for experiments E F 1 and E F 2 , which are identical to E 1 and E 2 from Section 4.1, except that theyE o Q(E, B(E, o)) Q E (E) E 1 (a → 1) 0.0291 0.0291 (a → 0) impossible E F 1 (a → 1) 0.0258 0.0018 (a → 0) −0.2142 E 2 (a → 1) impossible 5.6439 (a → 0) 5.6439 E F 2 (a → 1) −3.1844 2.3421 (a → 0)2.9561 execute FPWC instead of PWC .
Observe that, for both pairs of experiments, the expected flow of FPWC is less than the flow of PWC .
We have confirmed that the random corruption of a makes it more difficult for the attacker to increase the accuracy of his belief.Outcomes E F 1 , (a → 0) and E F 2 , (a → 1) correspond to an execution where the value of a is inverted.
The flow for these outcomes is negative, indicating that the program is giving the attacker misinformation.In general, calculating expected flow can require summing over all o ∈ State L , which might be a countably infinite set.
Thus, expected flow could be infeasible to calculate either by hand or by machine.
Fortunately, expected flow can be conservatively approximated by conditioning on a single distribution rather than conditioning on many observations.
Conditioning δ on δ L has the effect of making the low projection of δ identical to δ L , while leaving the high projection of δ unchanged.δ|δ L λσ .
δ(σ) (δ L)(σ L) · δ L (σ L)A bound on expected flow is then calculated as follows.Theorem 3 Let:E = S, b H , σ H , σ L δ = [[S]]( ˙ σ L ⊗ ˙ σ H ) e H = (([[S]]( ˙ σ L ⊗ b H ))|(δ L)) H Then: Q E (E) ≤ Q(E, e H )Proof.
In Appendix B. Uncertainty-based metrics typically consider the expected information flow over all experiments, rather than the flow in a single experiment.
An analysis, like ours, based on single experiments allows a more expressive language of security properties in which particular inputs or experiments can be tested.
Moreover, our analysis can be extended to calculate expected flow over all experiments.Rather than choosing particular high and low input states σ H and σ L , the system and the attacker may more generally choose distributions δ H and δ L over high and low states, respectively.
These distributions are sampled to produce the initial input state.
Taking the expectation in Q E with respect to σ H , σ L and o then yields the expected flow over all experiments.This extension also increases the expressive power of the experiment model.
A distribution over low inputs allows the attacker to use a randomized guessing strategy.
His distribution might also be a function of his belief, though we leave investigation of such attacker strategies as future work.
A distribution over high inputs could be used, for example, to determine the expected flow of the password checker when users' choice of passwords can be described by a distribution.
System designers are likely to want to limit the maximum possible information flow.
So we characterize the maximum amount of information flow that program S can cause in a single outcome as the maximum amount of flow from any outcome of any experiment E = S, b H , σ H , σ L on S:Q max (S) max E,b H | b H ∈B(E) Q(E, b H )Consider applying Q max (S) to PWC .
Assuming that b H satisfies the admissibility restriction in Section 2.4 and that the attacker guesses an incorrect password yields that PWC can leak at most − lg( · n−1 n ) bits per outcome, where n is the number of possible passwords.
If = 1, the attacker is forced to have a uniform distribution over passwords, representing a lack of belief for any particular value for the password.
Additionally, if n = 2 k for some k, then we obtain that for k-bit passwords, PWC can leak at most k − lg(2 k − 1) bits in a outcome; for k > 12 this is less than 0.0001 bits, supporting the intuition that password checking leaks little information.
Nothing precludes repetition of experiments.
The most interesting case has the attacker return to step 2(b) of the experiment protocol in Figure 1 after updating his belief in step 4; that is, the system keeps the high input to the program constant, and the attacker is allowed to check new lowRepetition # 1 2 b H : A 0.98 0 B 0.01 0.5 C 0.01 0.5 σ L (g) A B o(a) 0 0 b H : A 0 0 B 0.5 0 C 0.5 1 Q(E, b H )5.6439 1.0 Table 6.
Repeated experiments on PWC inputs based on the results of previous experiments.
Suppose that experiment E 2 from Section 4.1 is conducted and then repeated with σ L = (g → B).
Then the attacker's belief about the password evolves as shown in Table 6.
Summing the information flow for each repetition yields a total information flow of 6.6439.
This total corresponds to what Q would calculate for a single experiment, if that experiment changed prebelief b H to postbelief b H2 , where b H2 is the attacker's final postbelief in Table 6:D(b H ˙ σ H ) − D(b H2 ˙ σ H ) = 6.6439 − 0 = 6.6439This example suggests a general theorem stating that the postbelief from a series of experiments, where the postbelief from one experiment becomes the prebelief to the next, contains all the information learned during the series.
Let E i = S, b Hi , σ H , σ Li be the i th experiment in the series, and let r i = E i , bHi be an outcome from E i .
Let r 1 , . . . , r n be a series of n outcomes in which prebelief b Hi in experiment E i is postbelief b Hi−1 from outcome i − 1.
Let b H0 = b H1 be the attacker's prebelief for the entire series.Theorem 4 D(b H1 ˙ σ H ) − D(b Hn ˙ σ H ) = i | 1≤i≤n Q(r i )Proof.
In Appendix B.
The last piece required for our framework is a semantics [[S]] in which programs are functions that map distributions to distributions.
Here we build such a semantics in two stages.
First, we build a simpler semantics that maps states to distributions.
Second, we lift the simpler semantics so that it operates on distributions, as suggested by Section 2.2.
State → Dist.
The semantics is given in Figure 3.
We assume some semantics [[E]] : State → Val that gives meaning to expressions, and a semantics [[B]] : State → Bool that gives meaning to Boolean expressions.The statements skip, if, and while have essentially the same denotations as in the standard deterministic case.
5 State update σ [v → V ], where V ∈ Val, changes the value of v to V in σ.
The distribution update δ [v → E] in the denotation of assignment represents the result of substituting the meaning of E for v in all the states of δ:δ[v → E] λσ .
( σ | σ [v →[[E]]σ ]=σ δ(σ ))The sequential composition of two programs, written S 1 ; S 2 , is defined using intermediate states.
The frequency of S 1 ; S 2 , starting from σ, reaching a final state σ is the sum of the probabilities of all the ways that S 1 can reach some intermediate σ and then S 2 from that σ can reachσ .
Note that ([[S 1 ]]σ)(σ ) is the frequency that S 1 , begin- ning in σ, terminates in σ , because [[S 1 ]]σ produces a distribution that, when applied to σ , returns the frequency of termination in σ .
Similarly, ([[S 2 ]]σ )(σ ) is the frequency that S 2 , beginning in σ , terminates in σ .
The final program construct is probabilistic choice, S 1 p S 2 , where 0 ≤ p ≤ 1.
The semantics multiplies the probability of choosing a side S i with the frequency that S i produces a particular output state σ .
Since the same state σ might actually be produced by both sides of the choice, the frequency of its occurrence is the sum of the frequency from either side:p · ([[S 1 ]]σ)(σ ) + (1 − p) · ([[S 2 ]]σ)(σ ).
This formula is simplified to the definition in Figure 3 using · and + as pointwise operators:p · δ λσ .
p · δ(σ) δ 1 + δ 2 λσ .
δ 1 (σ) + δ 2 (σ)To show how to lift the semantics in Figure 3 and define [[S]] : Dist → Dist, we use the same intuition as for the sequential operator above.
There are many states σ in which S could begin execution, and all of them could potentially terminate in state σ.
So to compute ([[S]]δ)(σ), we take a weighted average over all input states σ .
The weights are δ(σ ), which describes how likely σ is to be used as the input state.
With σ as input, S terminates in state σ with frequency ( [[S]]σ )(σ).
Thus we define [[S]]δ as: Figure 3.
Semantics of programs in states Figure 4.
This lifted semantics corresponds directly to a semantics given by Kozen [15], which interprets programs as continuous linear operators on measures.
Our semantics uses an extension of the distribution conditioning operator | to Boolean expressions.
Whereas distribution conditioning produces a normalized distribution, Boolean expression conditioning produces an unnormalized distribution:[[S]]δ λσ .
σ δ(σ ) · ([[S]]σ )(σ) = σ δ(σ) · [[S]]σ [[skip]]σ = ˙ σ [[v := E]]σ = ˙ σ[v → E] [[S 1 ; S 2 ]]σ = λσ .
σ ([[S 1 ]]σ)(σ ) · ([[S 2 ]]σ )(σ ) [[if B then S 1 else S 2 ]]σ = if [[B]]σ then [[S 1 ]]σ else [[S 2 ]]σ [[while B do S]] = fix(λf : State → Dist .
λσ .
if [[B]]σ then λσ .
σ ([[S]]σ)(σ ) · f (σ )(σ ) else ˙ σ) [[S 1 p S 2 ]]σ = p · [[S 1 ]]σ + (1 − p) · [[S 2 ]]σ[[skip]]δ = δ [[v := E]]δ = δ[v → E] [[S 1 ; S 2 ]]δ = [[S 2 ]]([[S 1 ]]δ) [[if B then S 1 else S 2 ]]δ = [[S 1 ]](δ | B) + [[S 2 ]](δ | ¬B) [[while B do S]] = fix(λf : Dist → Dist .
λδ .
f ([[S]](δ | B)) + (δ | ¬B)) [[S 1 p S 2 ]]δ = [[S 1 ]]p · δ + [[S 2 ]](1 − p) · δδ|B λσ .
if [[B]]σ then δ(σ) else 0By producing unnormalized distributions as part of the meaning of if and while statements, we are tracking the frequency with which each branch of the statement is chosen.
We believe our work is the first to address and show the importance of attacker beliefs in quantifying information flow.
Perhaps the earliest published connection between information theory and information flow is Denning [5], which demonstrates the analysis of a few particular assignment and if statements by using entropy to calculate leakage.
Millen [20], using deterministic state machines, proves that a system satisfies noninterference exactly when the mutual information between certain inputs and outputs is zero.
He also proposes mutual information as a metric for information flow, but does not show how to compute the amount of flow for programs.Wittbold and Johnson [26] introduce nondeducibility on strategies, an extension of Sutherland's nondeducibility [22].
Wittbold and Johnson observe that if a program is run multiple times and feedback between runs is allowed, then information can be leaked by coding schemes across multiple runs.
A system that is nondeducible on strategies has no noiseless communication channels between high input and low output, even in the presence of feedback.The flow model (FM) is a security property first given by McLean [19] and later given a quantitative formalization by Gray [11], who called it the Applied Flow Model (AFM).
The FM stipulates that the probability of a low output may depend on previous low outputs, but not on previous high outputs.
Gray formalizes this in the context of probabilistic state machines, and he relates noninterference to the rate of maximum flow between high and low.
Browne [1] develops a novel application of the idea behind the Turing test to characterize information flow: a system passes Browne's Turing test exactly when for all finite lengths of time, the information flow over that time is zero.
Halpern and O'Neill [12] construct a framework for reasoning about secrecy that generalizes many previous results on qualitative and probabilistic, but not quantitative, security.Volpano [23] gives a type system that can be used to establish the security of password checking and one-way functions such as MD5 and SHA1.
Noninterference does not allow such functions to be typed, so this type system is an improvement over previous type systems.
However, the type system does not allow a general analysis of quantita-tive information flow.
Volpano and Smith [24] give another type system that enforces relative secrecy, which requires that well-typed programs cannot leak confidential data in polynomial time.Weber [25] defines n-limited security, which allows declassification at a rate that depends, in part, on the size n of a buffer shared by the high and low projections of a state.
Lowe [16] defines the information flow quantity of a process with two users H and L to be the number of behaviors of H that L can distinguish.
When there are n such distinguishable behaviors, H can use them to transmit lg n bits to L.
These both measure the size of channels rather than accuracy of belief.Di Pierro, Hankin, and Wiklicky [6] relax noninterference to approximate noninterference, where "approximate" is a quantified measure of the similarity of two processes in a process algebra.
Similarity is measured using the supremum norm over the difference of the probability distributions the processes create on memory.
They show how to interpret this quantity as a probability on an attacker's ability to distinguish two processes from a finite number of tests, in the sense of statistical hypothesis testing.
Finally, the paper explores how to build an abstract interpretation that allows approximation of the confinement of a process.
Their more recent work [7] generalizes this to measuring approximate confinement in probabilistic transition systems.Clark, Hunt, and Malacaria [3] apply information theory to the analysis of while-programs.
They develop a static analysis that provides bounds on the amount of information that can be leaked by a program.
The metric for information leakage is based on conditional entropy; the analysis consists of a dataflow analysis, which computes a use-def graph, accompanied by a set of syntax-directed inference rules, which calculate leakage bounds.
In other work [2], the same authors investigate other leakage metrics, settling on conditional mutual information as an appropriate metric for measuring flow in probabilistic languages; they do not consider relative entropy.
Mutual information is always at least 0, so unlike relative entropy it cannot represent misinformation.McIver and Morgan [17] calculate the channel capacity of a program using conditional entropy.
They add demonic nondeterminism as well as probabilistic choice to the language of while-programs, and they show that the perfect security (0 bits of leakage) of a program is determined by the behavior of its deterministic refinements.
They also consider restricting the power of the demon making the nondeterministic choices, such that it can see all data, or just low data, or no data.Evfimievski, Gehrke, and Srikant [8] quantify privacy breaches in data mining.
In their framework, randomized operators are applied to confidential data before the data is released.
A privacy breach occurs when release of the randomized data causes a large change in an attacker's probability distribution on a property of the confidential data.
They use Bayesian reasoning, based on observation of randomized data, to update the attacker's probability distributions.
Their distributions are similar to our beliefs, but have a strong admissibility restriction: the attacker's prebelief must be the same distribution from which the system generates the high input.
They also show that relative entropy can be used to bound the maximum privacy breach for a randomized operator.
This paper presents a model for incorporating attacker beliefs into analysis of quantitative information flow in programs.
Our theory reveals that uncertainty, the traditional metric for information flow, is inadequate: it cannot satisfactorily explain even the simple example of password checking.
Accuracy is the appropriate metric for information flow in the presence of attacker beliefs, and we have shown how to use it to calculate exact, expected, and maximum information flow.
A formal model of experiments we give enables precise descriptions of attackers' actions.
We have instantiated the model with a probabilistic semantics and have given several examples of applying the model and metric to the measurement of information flow.
To allow mutable high inputs, as discussed in Section 3.4, let the notation b 0 H mean the same distribution as b H , except that each state of its domain has a 0 as a superscript.
So, if b H ascribes probability p to the state σ, then b 0 H ascribes probability p to the state σ 0 .
We assume that S cannot modify states with a superscript 0.
In the case that states map variables to values, this could be achieved by defining σ 0 to be the same state as σ, but with the superscript 0 attached to variables; for example, if σ(v) = 1 then σ 0 (v 0 ) = 1.
Note that S cannot modify σ 0 if did not originally contain any variables with superscripts.Using this notation, the belief revision operator is extended to B !
, which allows S to modify the high state in ex-periment E = S, b H , σ H , σ L .
B !
(E) (([[S]]( ˙ σ L ⊗ b H ⊗ b 0 H )|o)) H 0 where o ∈ Γ(δ ) L δ = [[S]]( ˙ σ L ⊗ ˙ σ H )In the first line of the definition, the high input state is preserved by introducing the product with b 0 H , and the attacker's postbelief about the input is recovered by restricting to H 0 , the high input state with the superscript 0.
To allow nonterminating programs, let State ⊥ State∪ {⊥}, and ⊥ L ⊥.
Nontermination is now allowed as an observation, leading to an extended belief revision operator B !
⊥ :B !
⊥ (E) (out ⊥ (S, ˙ σ L ⊗ b H ⊗ b 0 H )|o) H 0 where o ∈ Γ(δ ) L δ = out ⊥ (S, ˙ σ L ⊗ ˙ σ H )Function out ⊥ (S, δ) produces a distribution that yields the frequency that S terminates, or fails to terminate, on input distribution δ:out ⊥ (S, δ) λσ : State ⊥ .
if σ = ⊥ then δ − [[S]]δ else ([[S]]δ)(σ)If S does not terminate on some input states in δ, then output distribution [[S]]δ will contain less mass than δ; otherwise, δ will equal [[S]]δ.
Missing mass corresponds to nontermination [21,18], so out ⊥ maps the missing mass to ⊥.
Theorem 1 Let E = S, b H , σ H , σ L .
B(E, o)(σ H ) = B(E, o)Proof.B(E, o) = Definition of B b H (σ H ) · ([[S]]( ˙ σ L ⊗ ˙ σ H ) L)(o) σ H b H (σ H ) · ([[S]]( ˙ σ L ⊗ ˙ σ H ) L)(o) = Definition of δ L, apply distribution to o b H (σ H ) · ( σ | σL=o ([[S]]( ˙ σ L ⊗ ˙ σ H )(σ)) σ H b H (σ H ) · ( σ | σL=o ([[S]]( ˙ σ L ⊗ ˙ σ H )(σ)) = Lemma 1.1 b H (σ H ) · ( σ | σL=o ([[S]]( ˙ σ L ⊗ ˙ σ H )(σ)) σ | σ L=o [[S]]( ˙ σ L ⊗ b H )(σ ) = Distributivity, one-point rule σ | σL=o ∧ σH=σ H σ H b H (σ H )·[[S]]( ˙ σ L ⊗ ˙ σ H )(σ) σ | σ L=o [[S]]( ˙ σ L ⊗ b H )(σ ) = Lemma 1.1 σ | σL=o ∧ σH=σ H [[S]]( ˙ σ L ⊗ b H )(σ) σ | σ L=o [[S]]( ˙ σ L ⊗ b H )(σ ) = Distributivity σ | σL=o ∧ σH=σ H [[S]]( ˙ σ L ⊗b H )(σ) σ | σ L=o [[S]]( ˙ σ L ⊗b H )(σ ) = Definition of δ L σ | σH=σ H (([[S]]( ˙ σ L ⊗ b H ))|o)(σ) = Definition of δ H, applying distribution to σ H ((([[S]]( ˙ σ L ⊗ b H ))|o) H)(σ H ) = Definition of B(E, o) B(E, o)(σ H ) Lemma 1.1 Let σ L = o. [[S]]( ˙ σ L ⊗ b H )(σ) = σ H b H (σ H ) · [[S]]( ˙ σ L ⊗ ˙ σ H )(σ)Proof.
[ [S]]( ˙ σ L ⊗ b H )(σ) = Definition of [[S]]δ σ ( ˙ σ L ⊗ b H )(σ ) · ([[S]]σ )(σ) = Definition of point mass σ | σ L=σ L b H (σ H) · ([[S]]σ )(σ) = Let σ = σ L , σ H , nesting, one-point rule σ H b H (σ H ) · [[S]]( ˙ σ L ⊗ ˙ σ H )(σ) Theorem 2 Let E = S, b H , σ H , σ L .
Q(E, b H ) = I δ A (o) − I δ S (o) Proof.
Q(E, b H ) = Definition of Q D(b H ˙ σ H ) − D(b H ˙ σ H ) =b H (σ H ) = b H (σ H ) · δ S (o) δ A (o)Proof.b H (σ H ) = Definition of B (([[S]]( ˙ σ L ⊗ b H )|o) H)(σ H ) = Definition of δ H σ | σH=σ H ([[S]]( ˙ σ L ⊗ b H )|o)(σ) = Definition of δ|o σ | σH=σ H ∧ σL=o [[S]]( ˙ σ L ⊗ b H )(σ) ([[S]]( ˙ σ L ⊗ b H ) L)(o) = One-point rule: σ = o, σ H [[S]]( ˙ σ L ⊗ b H )(o, σ H ) ([[S]]( ˙ σ L ⊗ b H ) L)(o) = Definition of δ A 1 δ A (o) · [[S]]( ˙ σ L ⊗ b H )(o, σ H ) = Definition of [[S]]δ 1 δ A (o) · σ ( ˙ σ L ⊗ b H )(σ ) · ([[S]]σ )( ˙ o ⊗ ˙ σ H ) = Definition of ⊗, point mass 1 δ A (o) · σ | σ L=σ L b H (σ H) ·([[S]]( ˙ σ L ⊗ ( ˙ σ H)))( ˙ o ⊗ ˙ σ H ) = High input is immutable 1 δ A (o) · σ | σ L=σ L ∧ σ H=σ H b H (σ H) ·([[S]]( ˙ σ L ⊗ ( ˙ σ H)))( ˙ o ⊗ ˙ σ H ) = One-point rule: σ = σ L , σ H 1 δ A (o) · b H (σ H ) · ([[S]]( ˙ σ L ⊗ ˙ σ H ))( ˙ o ⊗ ˙ σ H ) = High input is immutable, Definition of δ L 1 δ A (o) · b H (σ H ) · (([[S]]( ˙ σ L ⊗ ˙ σ H )) L)(o) = Definition of δ S b H (σ H ) · δ S (o) δ A (o)Note that the immutability of high input can be dispensed with using the technique of Section 3.4.
E = S, b H , σ H , σ L δ = [[S]]( ˙ σ L ⊗ ˙ σ H ) e H = (([[S]]( ˙ σ L ⊗ b H ))|(δ L)) H Then: Q E (E) ≤ Q(E, e H )Proof.Q E (E) = Definition of Q E E o∈δ L [Q(E, B(E, o))] = Definition of Q, let b H = B(E, o)) E o∈δ L [D(b H ˙ σ H ) − D(b H ˙ σ H )] = Linearity of E D(b H ˙ σ H ) − E o∈δ L [D(b H ˙ σ H )] ≤Jensen's inequality and convexity of D, see [4] D(b H ˙ σ H ) − D(E o∈δ L [b H ] ˙ σ H ) = Lemma 3.1 D(b H ˙ σ H ) − D(e H ˙ σ H ) =Definition of Q Q(E, e H ) Lemma 3.1 Let E, δ , e H be defined as in Theorem 3.
Let b H = B(E, o) and assume the range of o is always δ L. Then:E o [b H ] = e H Proof.
(by extensionality) E o [b H ](σ H ) = Definitions of E, b H ( o (δ L)(o) · B(E, o)(σ H ) = Definition of B(E, o) o (δ L)(o) · ((([[S]]( ˙ σ L ⊗ b H ))|o) H)(σ H ) = Definition of δ H, applying distribution to σ H o (δ L)(o) ·( σ | σ H=σ H (([[S]]( ˙ σ L ⊗ b H ))|o)(σ )) = Definition of δ|o, applying distribution to σ o (δ L)(o) ·( σ | σ H=σ H ∧ σ L=o ([[S]]( ˙ σ L ⊗ b H ))(σ ) ([[S]]( ˙ σ L ⊗ b H ) L)(o) ) = One-point rule o (δ L)(o) · ([[S]]( ˙ σ L ⊗ b H ))(o, σ H ) ([[S]]( ˙ σ L ⊗ b H ) L)(o) = Definition of δ L, applied to o o (δ L)(o) · ([[S]]( ˙ σ L ⊗ b H ))(o, σ H ) σ | σ L=o [[S]]( ˙ σ L ⊗ b H )(σ ) = Let σ = o, σ H , change of dummy: o := σ, definition of ≈ L σ | σH=σ H (δ L)(o) · ([[S]]( ˙ σ L ⊗ b H ))(σ) σ | σ ≈ L σ [[S]]( ˙ σ L ⊗ b H )(σ ) = Definition of δ|δ L , applied to σ σ | σH=σ H ([[S]]( ˙ σ L ⊗ b H )|(δ L))(σ) = Definition of δ H, applied to σ H (([[S]]( ˙ σ L ⊗ b H )|(δ L)) H)(σ H ) = Definition of e H e H (σ H ) Theorem 4 D(b H0 ˙ σ H ) − D(b Hn ˙ σ H ) = i Q(r i ) Proof.
i | 1≤i≤n Q(r i ) = Definition of Q i | 1≤i≤n D(b Hi ˙ σ H ) − D(b Hi ˙ σ H ) = Lemma 4.1 D(b H1 ˙ σ H ) − D(b Hn ˙ σ H )Lemma 4.1 Assume for f and f that ∀ i | 1≤i≤n f (i) = f (i − 1), n ≥ 2, and f (1) = f (0).
Then:( i | 1≤i≤n f (i) − f (i)) = f (1) − f (n) Proof.
i | 1≤i≤n f (i) − f (i) = f (i) = f (i − 1) i | 1≤i≤n f (i − 1) − f (i) = Distributivity ( i | 1≤i≤n f (i − 1)) − i | 1≤i≤n f (i) =Change of dummy: i := i − 1( i | 0≤i≤n−1 f (i)) − i | 1≤i≤n f (i) = Split off term, n ≥ 2 f (0) + ( i | 1≤i≤n−1 f (i)) −( i | 1≤i≤n−1 f (i)) − f (n) = Arithmetic, f (1) = f (0) f (1) − f (n) Stephen Chong participated in an early discussion about the distinction between attacker beliefs and reality.
Sigmund Cherem, Stephen Chong, Jed Liu, Kevin O'Neill, Nathaniel Nystrom, Riccardo Pucella, Lantian Zheng, and the reviewers provided helpful feedback on the paper.
Sebastian Hunt also provided insightful comments on this work.
