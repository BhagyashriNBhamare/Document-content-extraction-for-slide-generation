In this work, we study a novel top-k query type, called top-k,m queries.
Suppose we are given a set of groups and each group contains a set of attributes, each of which is associated with a ranked list of tuples, with ID and score.
All lists are ranked in decreasing order of the scores of tuples.
We are interested in finding the best combinations of attributes, each combination involving one attribute from each group.
More specifically, we want the top-k combinations of attributes according to the corresponding top-m tuples with matching IDs.
This problem has a wide range of applications from databases to search engines on traditional and non-traditional types of data (relational data, XML, text, etc.).
We show that a straightforward extension of an optimal top-k algorithm, the Threshold Algorithm (TA), has shortcomings in solving the top-k,m problem, as it needs to compute a large number of intermediate results for each combination and reads more inputs than needed.
To overcome this weakness, we provide here, for the first time, a provably instance-optimal algorithm and further develop optimizations for efficient query evaluation to reduce computational and memory costs and the number of accesses.
We demonstrate experimentally the scalability and efficiency of our algorithms over three real applications.
During the last decade, the topic of top-k query processing has been extensively explored in the database community due to its importance in a wide range of applications.
In this work, we identify a Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
SIGMOD '12, May 20-24, 2012, Scottsdale, Arizona, USA.
Copyright 2012 ACM 978-1-4503-1247-9/12/05 ...$10.00.
novel top-k query type that has wide applications and requires novel algorithms for efficient processing.
Before we give the definition of our problem, we describe an interesting example to motivate it.Assume a basketball coach plans to build a good team for an important game (e.g., Olympic games).
He would like to select a combination of athletes including forward, center, and guard positions, by considering their historical performance in games.
See Figure 1 for the example data, which comes from the NBA 2010-2011 pre-season.
Each tuple in Figure 1(a) is associated with a pair (game ID, score), where the score is computed by an aggregation of various scoring items provided by the NBA for this game.
To build a good team, one option is to select athletes with the highest score in each group (e.g., "Juwan Howard" in the "forward" group), or to calculate the average scores of athletes across all games.
However, both methods overlook an important fact: a strong team spirit is critical to the overall performance of a team.
Therefore a better way is to evaluate the performance of athletes by considering their combined scores in the same game.
We can formalize this problem as to select the top-k combinations of athletes according to their best top-m aggregate scores for games where they played together.
For example, as illustrated in Figure 1, F 2 C 1 G 1 is the best combination of athletes, as the top-2 games in which the three athletes played together are G02 and G05, and 40.27 (= 21.51 + 18.76) is the highest overall score (w.r.t. the sum of the top-2 scores) among all eight combinations.
We study in this paper a new query type called top-k,m query that captures this need of selecting the top combinations of different attributes, when each attribute is described by a ranked list of tuples.
Given a set of groups G 1 ,. . . ,G n where each group G i contains multiple attributes e i1 ,. . . ,e il i , we are interested in returning top-k combinations of attributes selected from each group.
As an example, recall Figure 1: there are three groups, i.e., forward, center, and guard, and one athlete corresponds to one attribute in each group.
Our goal is to select a combination of three athletes, each from a different group.We suppose that each attribute e is associated with a ranked list L e , where each tuple τ ∈ L e is composed of an ID ρ(τ) (taken out of an arbitrary set of identifier) and a score σ(τ) (from an arbitrary fully ordered set, say, the reals).
The list is ranked in descending order of the scores.
In the example NBA data, each athlete has a list containing the game ID and the corresponding score.
Given a combination of attributes, a match instance I of is a set of tuples based on some arbitrary join condition on tuple IDs ρ(τ) from lists.
For example, the game G02 (including the three tuples (G02, 8.91), (G02, 6.01), (G02, 6.59)) is a match instance by the equijoin of the game IDs.
In the top-k,m problem, we are interested in returning top-k combinations of attributes which have the highest Figure 1: Motivating example using 2010-2011 NBA data.
Our purpose is to select one athlete from each of the three groups.
The best combination is F 2 C 1 G 1 .
Values in bold font indicate tuples contributing to the score of this best combination.overall scores over their top-m match instances by a monotonic aggregate function.
Suppose that k = 1, m = 2, and that scores are aggregated using the sum function.
The top-2 match instances of F 2 C 1 G 1 are G02 and G05 and their overall score is 40.27, which is the highest overall score among the top-2 match instances of all combinations.
Therefore, we say that the answer of the top-1,2 query for the problem instance in Figure 1 is the combination F 2 C 1 G 1 .
Next we give more use-case scenarios of top-k,m queries, which shed some light on the generality and importance of top-k,m models in practice.Use-case 1.
Top-k,m queries are useful in XML databases.
A simple yet effective way to search an XML database is keyword search.
But in a real application it is often the case that a user issues a keyword query Q which does not return the desired answers due to the mismatch between terms in the query and in documents.
A common strategy for remedying this is to perform some query rewriting, replacing query terms with synonyms that provide better matches.
Interestingly, top-k, m queries find an application in this scenario.
Specifically, for each keyword (or phrase) q in Q, we generate a group G(q) that contains the alternative terms of q according to a dictionary which contains synonyms and abbreviations of q. For example, given a query Q = DB, UC Irvine, 2002, we can generate three groups: G 1 = {"DB", "database"}, G 2 = {"UCI", "UC Irvine"}, and G 3 = {"2002"}.
We assume that each term in G(q) is associated with a list of node identifiers (e.g., JDewey IDs [7]) and scores (e.g., information-retrieval scores such as tf-idf [4]).
See Figure 2 for an example XML tree and scores.
The goal of top-k,m queries is to find the top-k combinations (of terms) by considering the corresponding top-m search results in the XML database.
Therefore, a salient feature of the top-k,m model for the refinement of XML keyword query is that it guarantees that the suggested alternative queries have high quality results in the database within the top-m answers.Use-case 2.
Top-k,m queries also have applications in evidence combination mining in medical databases [2].
The goal is to predict or screen for a disease by mining evidence combination from clinical and pathological data.
In a clinical database, each evidence refers Figure 2: An example illustrating XML query refinement using the top-k,m framework.
The original query Q = DB, UC Irvine, 2002 is refined into DB, UCI, 2002.
Each term is associated with an inverted list with the IDs and weights of elements.
Underlined numbers in the XML tree denote term scores.to a group and different degrees of the evidence act as different attributes.
For example queasiness, headaches, vomit, and diarrhea are four pieces of evidence (referring to groups) for acute enteritis, and each evidence has different degrees (referring to attributes), e.g., very low, low, middle, high, very high.
Each tuple in a list associated to an attribute consists of the patient ID and the probability the patient catches the disease.
The goal of the top-k, m query is to find the top-k combinations of different degrees of evidences ordered by the aggregate values of the probabilities of top-m patients for which there is the highest belief this disease is involved.Use-case 3.
Top-k,m queries may finally have applications in package recommendation systems, e.g., the trip selection problem.
Consider a tourist who is interested in planing a trip by choosing one hotel, one shopping mall, and one restaurant in a city.
Assume that we have survey data provided by users who made trips before.
The data include three groups and each group have multiple attributes (i.e., names of hotels, malls, or restaurants), each of which is associated with a list of users' IDs and grades.
Top-k,m queries recommend top-k trips which are combinations of hotels, malls, and restaurants based on the aggregate value of the highest m scores of the users who had the experience of this exact trip combination.Generally speaking, top-k,m queries are of use in any context where one is interested in obtaining combinations of attributes associated with ranked lists.
In addition, note that the model of top-k,m queries offers great flexibility in problem definitions to meet the various requirements that applications may have, in particular in the adjustment of the m parameter.
For example, in the application to XML keyword search, a user is often interested in browsing only the top few results, say 10, which means we can set m = 10 to guarantee the search quality of the refined keywords.
In another application, e.g., trip recommendation, if a tourist wants to consider the average score of all users, then we can define m to be large enough to take the scores of all users into accounts.
(Of course, in this case, the number of accesses and the computational cost are higher.)
The literature on top-k query processing in relational and XML databases is particularly rich [5-8, 12, 13].
We stress the essential difference between top-k queries (e.g., as in [8]) and our top-k,m problem: the latter returns the top-k combinations of attributes in groups, but the former returns the top-k tuples (objects).
Therefore, a top-k,m problem cannot be reduced to a top-k problem through a careful choice of the aggregate function.
In addition, contrarily to the top-k problem, we argue that top-k,m queries cannot be transformed into a SQL (nested) query, since SQL queries return tuples but our goal is to return attribute combinations based on ranked inverted lists, which is not something that the SQL language permits.
To the best of our knowledge, this is the first top-k work focusing on selecting and ranking sets of attributes based on ranked lists, which is a highly non-trivial extension of the traditional top-k problem.
An extended discussion about the difference between top-k,m queries and the existing top-k queries can be found in Section 3.
To answer a top-k,m query, one method, called extended TA (for short ETA hereafter), is to compute all top-m results for each combination by the well-known threshold algorithm (TA) [8] and then to pick the top-k combinations.
However, this method has one obvious shortcoming: it needs to compute top-m results for each combination and reads more inputs than needed.
To address this problem, we develop a set of provably optimal algorithms to efficiently answer top-k,m queries.Following the model of TA [8], we allow both random access and sorted access on lists.
Sorted accesses mean to perform sequential scan of lists and random access can be performed only on objects which are previously accessed by sorted access.
For example, consider Figure 1 again.
When the first tuple (G05, 7.21) of C 1 is read, the random access enables us to quickly locate all tuples whose ID are G05 (e.g., (G05, 7.54) in F 2 ).
We propose an algorithm called ULA (Upper and Lower bounds Algorithm) to avoid the needs to compute top-m results of combinations (Section 4) and thus to significantly reduce the computation costs compared to ETA.We then bring to light some key observations and develop an optimized algorithm called ULA + , which minimizes the number of accesses and consequently reduces the computational and memory costs.
The ULA algorithm needs to compute bounds (lower and upper bounds) for each combination, which may be expensive when the number of combination is large.
In ULA + , we avoid the need to compute bounds for some combinations by carefully designing the conditions to prune away useless combinations without reading any tuple in the associated lists.
We also propose a native structure called KMG graph to avoid the useless sorted and random accesses in lists to save computational and memory costs.We study the optimal properties of our algorithms with the notion of instance optimality [8] that reflects how well a given algorithm performs compared to all other possible algorithms in its class.
We show that two properties dictate the optimality of top-k,m algorithms in this setting: including (1) the number of attributes in group G i , namely |G i |, which is part of the input of the problem; and (2) whether wild guesses are allowed.
Following [8], wild guesses mean random access to objects which have not been seen by sorted access.
Only if each |G i | is treated as a constant and there are no wild guesses is our algorithm guaranteed to be instance-optimal.
In addition, we show that the optimality ratio of our algorithms is tight in a theoretical sense.
Unfortunately, if either |G i | is considered variable or wild guesses exist (uncommon cases in practice), our algorithms are not optimal.
But we show that in these cases no instance-optimal algorithm exists.To demonstrate the applicability of the top-k,m framework, we apply it to the problem of XML keyword refinement.
We show how to judiciously design the aggregation functions and join predicates to reflect the semantic of XML keyword search.
We then adapt the three algorithms: ETA, ULA and ULA + (from the most straightforward to the highly optimized one) to efficiently answer an XML top-k,m problem (Section 5).
We verify the efficiency and scalability of our algorithms using three real-life datasets (Section 6), including NBA data, YQL tripselection data, and XML data.
We find that our top-k,m algorithms result in order-of-magnitude performance improvements when compared to solutions based on the baseline algorithm.
We also show that our XML top-k,m approach is a promising and efficient method for XML keyword refinement in practice.To sum up, this article presents a new problem with important applications, and a family of provably optimal algorithms.
Comprehensive experiments verify the efficiency of solutions on three real datasets.
Given a set of groups G 1 ,. . . ,G n where each group G i contains multiple attributes e i1 ,. . . ,e il i , we suppose that each attribute e is associated with a ranked list L e , where each tuple τ ∈ L e is composed of an ID ρ(τ) and a score σ(τ).
The list is ranked by the scores in descending order.
Let = (e 1i , . . . , e n j ) ∈ G 1 × · · · × G n denote an element of the cross-product of the n groups, hereafter called combination.
For instance, recall Figure 1, every three athletes from different groups form a combination (e.g., {Lebron James, Chris Bosh, Dwyane Wade}).
Given a combination , a match instance I is defined as a set of tuples based on some arbitrary join condition on IDs of tuples from lists.
Each tuple in a match instance should come from different groups.
For example, in Figure 1, given a combination {Juwan Howard, Eddy Curry, Dwyane Wade}, then {(G01, 9.31), (G01, 3.81), (G01, 3.38)} is a match instance for the game G01.
Furthermore, we define two aggregate scores: tScore and cScore: the score of each match instance I is calculated by tScore, and the top-m match instances are aggregated to obtain the overall score, called cScore.
More precisely, given a match instance I defined on ,tScore(I ) = F 1 (σ(τ 1 ), . . . , σ(τ n ))where F 1 is a function: R n →R and τ 1 , . . . , τ n form the matching instance I .
Further, given an integer m and a combination , cScore(, m) = maxI 1 ,...,I m distinct {F 2 (tScore(I 1 ), . . . , tScore(I m ))}where F 2 is a function R m →R and I 1 ,. . . , I m are any m distinct match instances defined on the combination .
Intuitively, cScore returns the maximum aggregate scores of m match instances.
Following common practice [8], we require both F 1 and F 2 functions to be monotonic, i.e., the greater the individual score, the greater the aggregate score.
This assumption captures most practical scenarios, e.g., if one athlete has a higher score (and the other scores remain the same), then the whole team is better.
Definition 1 (top-k,m problem).
Given groups G 1 , . . . , G n , two integers k, m, and two score functions F 1 , F 2 , the top-k,m problem is an (n + 4)-tuple (G 1 , . . . , G n , k, m, F 1 , F 2 ).
A solution is an ordered set S containing the top-k combinations = (e 1i , . . . , e n j ) ∈ G 1 × · · · × G n ordered by cScore(, m).
Example 2.
Consider a top-1, 2 query on Figure 1, and assume that F 1 and F 2 are sum.
The final answer S is {F 2 C 1 G 1 }.
This is because the top-1 match instance I 1 of F 2 C 1 G 1 consists of tuples (G02, 8.91), (G02, 6.01) and (G02, 6.59) of the game G02 with tScore 21.51 = 8.91 + 6.01 + 6.59.
And the second top instance I 2 consists of tuples whose game ID is G05 with tScore 18.76 = 7.54 + 7.21 + 4.01.
Therefore, the cScore of F 2 C 1 G 1 is 40.27 = 21.51 + 18.76, which is the highest score among all combinations.
Before we describe the novel algorithms for top-k,m queries, we pause to review some related works about top-k queries.
Top-k queries were studied extensively for relational and XML data [5-8, 12, 13, 22].
Notably, Fagin, Lotem, and Naor [8] present a comprehensive study of various methods for top-k aggregation of ranked inputs.
They identify two types of accesses to the ranked lists: sorted accesses and random accesses.
In some applications, both sorted and random accesses are possible, whereas, in others, some of the sources may allow only sorted or random accesses.
For the case where both sorted and random accesses are possible, a threshold algorithm (TA) (independently proposed in [10,20]) retrieves objects from the ranked inputs in a round-robin fashion and directly computes their aggregate scores by using random accesses to the lists where the object has not been seen.
Fagin et al. prove that TA is an instance-optimal algorithm.
In this paper, we follow the line of TA to support both sorted accesses and random accesses for efficient evaluation of top-k,m query.
We prove that our algorithm is also an instance-optimal algorithm and its optimality ratio is tight.There is also a rich literature for top-k queries in other environments, such as no random access [9,18], no sorted access on restricted lists [5,6], no need for exact aggregate score [11], or adhoc top-k queries [15,23].
For example, Mamoulis, Yiu, Cheng and Cheung [18] proposed a family of optimizations for top-k queries in the case of no random accesses.
They impose two phases (growing and shrinking) that any top-k algorithm should go through, and perform optimizations on the shrinking phase to reduce the number of accesses.
Theobald, Schenkel and Weikum [21] proposed a top-k query processor for efficient and self-tuning query expansion, which is related to the XML keyword refinement method described in Section 5.
In contrast to our work, Theobald et al. also support a non-fixed number of keywords in the refined query; however, no optimality guarantees are given.
Recently, Jin and Patel [13] proposed a novel sequential access scheme for top-k query evaluation, which outperforms existing schemes.
For more information about top-k query evaluation, readers may refer to an excellent survey [12] by Ilyas, Beskales, and Soliman.
In this article, we argue that top-k,m queries are new types of queries and that the existing top-k algorithms cannot be used to solve them.
For example, consider the ad-hoc top-k queries in [15,23].
Note the difference regarding the notion of "group" between those works and ours.
For ad-hoc queries, a group refers to a set of objects (rows) which satisfy certain predicates, while a group in this article means a set of attributes (columns).
Therefore, top-k,m queries, in a different approach from ad-hoc top-k queries, focus on the ranking of the combinations of attributes (not objects).
To the best of our knowledge, this is the first work focusing on selecting and ranking the set of attributes, which is a highly non-trivial extension of the traditional top-k problem.Top-k processing in XML databases has recently gained more attention since XML has become the preferred medium for formatting and exchanging data in many domains [1,7,19].
There are various types of problems on XML top-k processing, including top-k twig query processing [1,19], top-k keyword search [4], top-k probabilistic query processing [16] and top-k keyword cleansing [17].
In this article, we demonstrate how to apply the framework of top-k,m on the problem of XML top-k keyword refinement.
Note the difference between keyword cleansing [17] and keyword refinement: the former rewrites the query by fixing spelling errors, but the latter rewrites the query using semantic knowledge such as synonyms and abbreviations.
Both approaches are complementary in query processing and can be used together to improve search engine results.
In this section we begin our study of an efficient top-k,m algorithm which can stop earlier than the straightforward algorithm (i.e., ETA mentioned earlier), by avoiding the need to compute the exact top-m scores for each combination.
We propose a family of optimizations to improve the performance by reducing the number of accesses and computational and memory costs.
We also analyze the optimality properties for proposed algorithms.
As mentioned in the Introduction, given an instance of a top-k,m problem, following the practice in the top-k literature (e.g., [8]), we support both sorted and random access.
Sorted accesses read the tuple of lists sequentially and random accesses quickly locate tuples whose ID has been seen by sorted access (assuming the existence of an index to achieve this goal).
For example, in Figure 1, at depth 1 (depth d means the number of tuples seen under sorted access to a list is d), consider the combination "F 2 C 1 G 1 "; the tuples seen by sorted access are (G02, 8.91), (G05, 7.21), (G02, 6.59) and we can quickly locate all tuples (i.e., (G02, 6.01), (G05, 7.54), (G05, 4.01)) whose IDs are G02 or G05 by random accesses.
To answer a top-k,m query, one straightforward method (called extended TA, or ETA for short) is to first compute all top-m results for each combination by the well-known threshold algorithm TA [8] and then pick the top-k combinations.
However, this method has one obvious shortcoming: it needs to compute top-m results for each combination and reads more inputs than needed.
For example, in Figure 1, ETA needs to compute the top-2 scores for all eight combinations (see Figure 1(b)).
Indeed, this method is not instanceoptimal in this context.
To address this problem, we develop a set of provably optimal algorithms to efficiently answer top-k,m queries.
When designing an efficient top-k,m algorithm, informally, we observe that a combination cannot contribute to the final answer if there exist k distinct combinations whose lower bounds are greater than the upper bounds of .
To understand this, consider the top-1,2 query in Figure 1 again.
At depth 1, for the combination "F 2 C 1 G 1 ", we get two match instances G02 and G05 through the sorted and random accesses.
Then the lower bound of the aggregate score (i.e., cScore) of "F 2 C 1 G 1 " is at least 40.27 (i.e., (7.54 + 7.21 + 4.01) + (8.91 + 6.01 + 6.59)).
At this point, we can claim that some combinations are not part of answers.
This is the case of "F 2 C 2 G 1 ", whose cScore is no more than 38.62 (= 2 × (8.91 + 3.81 + 6.59)).
Since 38.62 < 40.27, F 2 C 2 G 1 cannot be the top-1 combination.
We next formalize this observation by carefully defining lower and upper bounds of combinations.
We start by presenting threshold values, which will be used to estimate the upper bounds for the unseen match instances.
Definition 3 (threshold value).
Let = (e 1i , . . . , e n j ) ∈ G 1 × · · · × G n be an arbitrary combination, and τ i the current tuple seen under sorted access in list L i .
We define the threshold value T of the combination to be F 1 (σ(τ 1 ), . . . , σ(τ n )), which is the upper bound of tScore for any unseen match instance of .
As an example, in Figure 1(a), consider the combination = "F 2 C 1 G 1 ", at depth 1.
The current tuples are (G02, 8.91), (G05, 7.21), (G02, 6.59).
Assume F 1 = sum, we have for threshold value T = 8.91 + 7.21 + 6.59 = 22.71.
Definition 4 (lower bound).
Assume one combination has seen m distinct match instances.
Then the lower bound of the cScore of is computed as follows:min =                  F 2 (tScore(I 1 ), . . . , tScore(I m ), 0, . . . , 0 m−m ) m < m max{F 2 (tScore(I i ), . . . , tScore(I j ) m )} m mWhen m < m, we use the minimal score (i.e., zero) of unseen m − m match instances to estimate the lower bound of the cScore.
On the other hand, when m m, min equals the maximal aggregate scores of m match instances.Definition 5 (upper bound).
Assume one combination has seen m distinct match instances, where there are m match instances (m m ) whose scores are greater than or equal to T .
Then the upper bound of the cScore of is computed as follows:max =                  F 2 (tScore(I 1 ), . . . , tS core(I m ), T , . . . , T m−m ) m < m max{F 2 (tScore(I i ), . . . , tScore(I j ) m )} m mIf m < m, it means that there is still a chance that we will see a new match instance whose tScore contributes to the final cScore.
Therefore, the computation of max should be padded with m − m copies of the threshold value (i.e., T ), which is the upper bound of tScore for all unseen match instances.
Otherwise, m m, meaning that the final top-m results are already seen and thus max =cScore(, m) now.Example 6.
This example illustrates the computation of the upper and lower bounds.
See Figure 1 again.
Assume that F 1 and F 2 are sum, and the query is top-1, 2.
At depth 1, the combination "F 2 C 1 G 1 " read tuples (G02, 8.91), (G05, 7.21), and (G02, 6.59) by sorted accesses, and (G05, 7.54), (G02, 6.01), (G05, 4.01) by random accesses.
m = m = 2.
Therefore, the current lower bound of "F 2 C 1 G 1 " is 40.27 (i.e., (7.54 + 7.21 + 4.01) + (8.91 + 6.01 + 6.59) = 18.76 + 21.51), since the two match instances of F 2 C 1 G 1 are G02 and G05.
The threshold T F 2 C 1 G 1 = 8.91 + 7.21 + 6.59 = 22.71 and m = 0, since 18.76 < 22.71 and 21.51 < 22.71.
Therefore, the upper bound is 45.42 (i.e., 22.71 + 22.71).
In fact, the final cScore of "F 2 C 1 G 1 " is exactly 40.27 which equals the current lower bound.
Note that the values of lower and upper bounds are dependent of the depth where we are accessing.
For example, at depth 2, the upper bound of "F 2 C 1 G 1 " decreases to 41.78 (i.e., 21.51 + 20.27) and the lower bound remains the same.The following lemmas show how to use the bounds above to determine if a combination can be pruned safely or confirmed to be an answer.Lemma 7 (drop-condition).
One combination does not contribute to the final answers if there are k distinct combinations 1 ,. . . , k such that max < min{ mini | 1 i k}.
Proof.
The aggregate score of the top-m match instances is no more then the upper bound of , i.e., cScore(, m) max .
And ∀i ∈ [1, k], cScore( i , m) min i , since the min i is the lower bound of i .
Therefore, cScore(, m) < min{cScore( i , m) | |1 i k}, which means that cannot be one of the top-k answers, as desired.Lemma 8 (hit-condition).
One combination should be an answer if there are at least N com − k (N com is the total number of the combinations) distinct combinations 1 ,. . . , Ncom−k , such that min max{ maxi | 1 i N com − k}.
Proof.
The aggregate score of the top-m match instances of is no less than the lower bound of , i.e., cScore(, m) min .
And ∀i ∈ [1, N com − k], max i cScore( i , m).
Therefore, cScore(, m) max{cScore( i , m) | 1 i N com − k}, meaning that the top-m aggregate score of is larger than or equal to that of other N com − k combinations.
Therefore must be one of the top-k,m answers.
Definition 9 (termination).
A combination can be terminated if meets one of the following conditions: (i) the drop-condition, (ii) the hit-condition, or (iii) has seen m match instances whose tScores are greater than or equals to the threshold value T .
Intuitively, one combination is terminated if we do not need to compute its lower or upper bounds any further.
The first two conditions in the above definition are easy to understand.
The third condition means that we have found top-m match instances of .
Note that we may not see the final top-m match instances when satisfy the drop-or hit-condition.
We are now ready to present a novel algorithm named ULA (Upper and Lower bounds Algorithm), that relies on the dynamic computing of upper and lower bounds of combinations (see Algorithm 1).
Input: a top-k,m problem instance with n groups G 1 , . . . , G n , where each group has multiple lists L i j ∈ G i .
Output: top-k combinations of attributes in groups.
(i) Do sorted access in parallel to each of the sorted lists L i j .
As a tuple τ is seen under sorted access in some list, do random access to all other lists in G j ( j i) to find all tuples τ such that ρ(τ) = ρ(τ ).
(ii) For each unterminated combination (by Definition 9), compute min and max , and check if can be terminated now.
(iii) If there are at least k combinations which meet the hitcondition, then the algorithm halts.
Otherwise, go to step (i).
(iv) Let Y be a set containing the k combinations (breaking ties arbitrarily) when ULA halts.
Output Y.In the last step of the algorithm, note that the set Y is unordered by cScore.
In the case where the output set should be ordered by cScore, we need to continuously maintain the lower and upper bounds of objects in Y until their order is clear.Example 10.
We continue the example of Figure 1 to illustrate the ULA algorithm.
First, in step (i) (at depth 1), ULA performs sorted accesses on one row for each list and does the corresponding random accesses.
In step (ii) (at depth 1 again), it computes the lower and upper bounds for each combination, and then three combinationsF 1 C 2 G 2 , F 2 C 2 G 1and F 2 C 2 G 2 are safely terminated, since their upper bounds (i.e., max F 1 C 2 G 1 = 39.42, max F 2 C 2 G 1 = 38.62 and max F 2 C 2 G 2 = 39.64) are less than the lower bound of F2 C 1 G 1 ( min F 2 C 1 G 1 = 40.27).
Next, we go to step (i) again (at depth 2), as there is no combination satisfying the hit-condition in step (iii).
Finally, at depth 4, F 2 C 1 G 1 meets the hit-condition and the ULA algorithm halts.
To understand the advantage of ULA over ETA, note that ETA cannot stop at depth 4, since F 2 C 2 G 1 does not yet obtain its top-2 match instances.
Indeed, ETA stops at depth 5 with 54 accesses, whereas ULA performs only 50 accesses by depth 4.
In this subsection, we present several optimizations to minimize the number of accesses, memory cost, and computational cost of the ULA algorithm by proposing an extension, called ULA + .
Pruning combinations without computing the bounds.The ULA algorithm has to compute the lower and upper bounds for each combination, which may be an expensive operation when the number of combinations is large.
We next propose an approach which prunes away many useless combinations safely without computing their upper or lower bounds.We sort all lists in the same group by the scores of their top tuples.
Notice that all lists are sorted by decreasing order.
Intuitively, the combinations with lists containing small top tuples are guaranteed not to be part of answers, as their scores are too small.
Therefore, we do not need to take time to compute their accurate upper and lower bounds.
We exploit this intuitive observation by defining the precise condition under which a combination can be safely pruned without computing its bounds.
We first define a relationship between two combinations called dominating.Given a group G in a top-k,m problem instance, let L e and L t be two lists associated with attributes e, t ∈ G, we sayL e dominates L t , denoted L e L t if L e .
σ(τ m ) L t .
σ(τ 1 ), where τ i denote the ith tuple in the list.
That is, the score of the mth tuple in L e is greater than or equal to the score of the first tuple in L t .
Definition 11 (domination).
A combination = {e 1 , . . . , e n } is said to dominate another combination ξ = {t 1 , . . . , t n } (denoted ξ) if for every 1 k n, either e i = t i or L e i L t i holds, where e i and t i are two (possibly identical) attributes of the same group G i .
For example, in Figure 3, there are two groups G 1 and G 2 .
We say that the combination "A 2 B 1 " dominates "A 3 B 2 ", because in the group G 1 , 7.1 > 6.3 and in G 2 , 8.2 > 8.0.
In fact, "A 2 B 1 " dominates all combinations of attributes from A 3 to A n in G 1 and from B 2 to B n in G 2 .
Note that the lists in each group here are sorted by the scores of the top tuples.Lemma 12.
Given two combinations and ξ, if dominates ξ then the upper bound of is greater than or equal to that of ξ.Proof.
If dominates ξ, then for every attribute e in ξ, if e , then there is an attribute t in , s.t. the m-th tuple in the list L e has a larger score than the first tuple in L t .
Therefore, the upper bound of m match instances of is greater than or equal to that of ξ.
More formally, ξ ⇒ ∀i,L e i .
σ(τ m ) L t i .
σ(τ 1 ) ⇒ F 1 (L e 1 .
σ(τ m ),. . ., L en .
σ(τ m )) F 1 (L t 1 .
σ(τ 1 ),. . ., L tn .
σ(τ 1 )), since F 1 is monotonic.
So m × (F 1 (L e 1 .
σ(τ m ), . . . , L en .
σ(τ m ))) m × (F 1 (L t 1 .
σ(τ 1 ), . . . , L tn .
σ(τ 1 ))).
Note that max m × (F 1 (L e 1 .
σ(τ m ), . . . , L en .
σ(τ m ))),since the threshold value and the scores of the unseen match instances of are no less thanF 1 (L e 1 .
σ(τ m ), . . . , L en .
σ(τ m )).
In addi- tion, it is easy to verify that ξ max m×(F 1 (L t 1 .
σ(τ 1 ), . . . , L tn .
σ(τ 1 ))).
Therefore, max ξ max holds, as desired.According to Lemma 12, if meets the drop-condition (Lemma 7), it means the upper bound of is small, then any combination ξ which is dominated by (i.e., ξ's upper bound is even smaller) can be pruned safely and quickly.To apply Lemma 12 in our algorithm, the lists are sorted in descending order by the score of the first tuple in each list, which can be done off-line.
We first access m tuples sequentially for each list and perform random accesses to obtain the corresponding match instances.
Then we consider two phases.
(i) Seed combination selection.
As the name indicates, seed combinations are used to trigger the deletion of other useless combinations.
We pick the lists in descending order, and construct the combinations to compute their upper and lower bounds until we find one combination which meets the drop-condition, then is selected as the seed combination.
n-2 n-1 … … … … … … … Example 13.
See Figure 3.
Assume the query is top-1, 2 and F 1 = F 2 = sum.
The lists are sorted in descending order according to the score of the first tuple.
We access the lists in descending order to find the seed combination, which is ξ = (A 2 , B 1 ) (ξ max = 2 × (7.1 + 8.2) = 30.6 < min ,={A 1 , B 1 }).
In G 1 , ∀i ∈ [3, n] L A 2 L A i (e.g., L A 2 L A 3 , since 7.1 > 6.3).
Similarly, in G 2 , ∀i ∈ [2, n] L B 1 L B i .
Therefore all combinations (A i , B j ) (∀i ∈ [3, n], j ∈ [2, n]), as well as (A 2 , B j ) and (B 1 , A i ) are dominated by ξ and can be pruned quickly.
Therefore there are (n − 2)(n − 1) + (n − 1) + (n − 2)=n 2 − n − 1 combinations pruned without the (explicit) computation of their bounds, which can significantly save memory and computational costs.Note that in the ULA + algorithm (which will be presented later), we perform the two phases above as a preprocessing procedure to filter out many useless combinations.Reducing the number of accesses.
We now propose some further optimizations to reduce the number of accesses at three different levels: (i) avoiding both sorted and random accesses for specific lists; (ii) reducing random accesses across two lists; and (iii) eliminating random accesses for specific tuples.Claim 14.
During query processing, given a list L, if all the combinations involving L are terminated, then we do not need to perform sorted accesses or random accesses upon the list L any longer.Claim 15.
During query processing, given two lists L e and L t associated with two attributes e and t in different groups, if all the combinations involving L e and L t are terminated, then we do not need to perform random accesses between L e and L t any longer.Claim 16.
During query processing, given two lists L e and L t associated with two attributes e and t in different groups, consider a tuple τ in list L e .
We say that the random access for the tuple τ from L e to L t is useless, if there exists a group G (e G and t G) such that ∀s ∈ G, either of the two following conditions is satisfied: (i) the list L s does not contain any tuple τ , s.t. ρ(τ) = ρ(τ ); or (ii) the combination involving s, e and t is terminated.It is not hard to see Claim 14 and 15 hold.
To illustrate Claim 16, let us consider three groups G1, G2 and G3 in Figure 4, where G3 contains only two lists.
The list L s does not contain any tuple whose ID is x and the combination is terminated.
Therefore, according to Claim 16, the random access between L e and L t for tuple x is unnecessary.
This is because no match instances of x can contribute to the computation of final answers.
Note that it is common in real life that some objects are not contained in some list.
For example, think of a player who missed some games in the NBA pre-season.
Furthermore, to maximize the elimination of useless random accesses implied in Claim 16, in our algorithm, we consider the Small First Access (SFA) heuristic to control the order of random accesses, that is, we first perform random accesses to the lists in groups with less attributes.
In this way, the random access across lists in larger groups may be avoided if there is no corresponding tuple in the list of smaller groups.
As shown in our experimental results, Claim 16 and the SFA heuristic have significant practical benefits to reduce the number of random accesses.
Summarizing, Claim 14 through 16 imply three levels of granularity to reduce the number of accesses.
In particular, Claim 14 eliminates both random accesses and sorted accesses, Claim 15 aims at preventing unnecessary random accesses, while Claim 16 comes in to avoid random accesses for some specific tuples.In order to exploit the three optimizations in the processing of our algorithm, we carefully design a native data structure named top-k,m graph (called KMG hereafter).
Figure 5(a) shows an example KMG for the data in Figure 1.
Formally, given an instance Π of the top-k,m problem, we can construct a node-labeled, weighted graph G defined as (V, E, W, C), where (1) V is a set of nodes, each v ∈ V indicating a list in Π, e.g., in Figure 5, node F 1 refers to the list F 1 in Figure 1; (2) E ⊆ V × V is a set of edges, in which the existence of edge (v,v ) means that random accesses between v and v are necessary; (3) for each edge e in E, W(e) is a positive integer, which is the weight of e.
The value is the total number of unterminated combinations associated with e; and finally (4) C denotes a collection of subsets of V, each of which indicates a group of lists in Π, e.g., in Figure 5,C={{F 1 , F 2 }, {C 1 , C 2 }, {G 1 , G 2 }}.
A path of length |C| in G that spans all subsets of C corresponds to a combination in Π.Based on the above claims, we propose three dynamic operations in KMG: (i) decreasing the weight of edges by 1 if one of combinations involving the edge is terminated; (ii) deleting the edge if its weight is 0, which means that random accesses between the two lists are useless (implied by Claim 15); and (iii) removing the node if its degree is 0, which indicates that both sorted and random accesses in this list are useless (implied by Claim 14).
Optimized top-k,m algorithm.
We are now ready to present the ULA + algorithm based on KMG, which combines all optimizations implied by Claim 14 to 16.
This algorithm is shown as Algorithm 2.
Input: a top-k,m problem instance with multiple groups and each group G has multiple attributes and each attribute n is associated with a list L n .
Output: top-k combinations of attributes in groups.
(i) Find the seed combination and prune all useless combinations dominated by according to the approach in Section 4.4.
(ii) Initialize a KMG G for the remaining combinations.
(iii) Do sorted accesses in parallel to lists with nodes in G. (iv) Do random accesses according to the existing edges in G (note that we need to first access the smaller group based on SFA strategy).
In addition, given a tuple τ ∈ L n , n ∈ G, if there is another group G such that each node n in G (where ∃ edge (n, n ) ∈ G) does not contain the tuple with the same ID of τ, then we can immediately stop all random accesses for τ (implied by Claim 16).
(v) Compute min and max for each unterminated combination and determine if is terminated now by Definition 9 using min and max .
If yes, decrease the weights of all edges involved in by 1.
In addition, remove an edge if its weight is zero and remove a node v ∈ G if the degree of v is zero.
Example 17.
We present an example with the data of Figure 1 to illustrate ULA + .
Consider a top-1,2 query again.
Firstly, in step (i), ULA + performs sorted accesses to two rows of all lists, and finds a seed combination, e.g., F 2 C 1 G 2 , as maxF 2 C 1 G 2 = 40.18 < min F 2 C 1 G 1 = 40.27.
Because L C 1 L C 2 , the combination F 2 C 1 G 2 dominates F 2 C 2 G 2 .
Therefore, both F 2 C 1 G 2 and F 2 C 2 G 2 can be pruned in step (i).
Then ULA + constructs a KMG (see Figure 5(a)) for non-pruned combinations in step (ii).
Note that there is no edge between F 2 and G 2 , since both F 2 C 2 G 2 and F 2 C 1 G 2 have been pruned.
By depth 2, ULA + computes min and max for each unterminated combination in step (iii).
Then F 1 C 2 G 1 and F 1 C 2 G 2 meet the drop-condition (e.g., maxF 1 C 2 G 1 = 37.6 < min F 2 C 1 G 1 ), and we decrease the weights by 1 for the corresponding edges, e.g., w(F 1 , G 1 ) = 1.
In addition, node C 2 should be removed, since all the combinations containing C 2 are terminated, (see Figure 5(b)) in step (iv).
At depth 3, F 1 C 1 G 2 is terminated, since maxF 1 C 1 G 2 = 36.48 < min F 2 C 1 G 1, and we decrease the weights of (F 1 , C 1 ), (F 1 , G 2 ) and (C 1 , G 2 ) by 1 and remove the node G 2 (see Figure 5(c)).
Finally, ULA + halts at depth 4 in step (vi) and F 2 C 1 G 1 is returned as the final result in step (vii).
To demonstrate the superiority of ULA + , we compare the numbers of accessed objects for three algorithms: ETA accesses 54 tuples (at depth 5) and ULA accesses 50 tuples (at depth 4) , while ULA + accesses only 37 tuples (at depth 4) .
We next consider the optimality of algorithms.
We start by defining the optimality measures, and then analyze the optimality in different cases.
Some of the proofs are omitted here due to space limitation; and most proofs are highly non-trivial.
Competing algorithms.
Let D be the class of all databases.
We define A to be all deterministic correct top-k, m algorithms running on every database D in class D .
Following the access model in [8], an algorithm A ∈ A can use both sorted accesses and random accesses.Cost metrics.
We consider the number of tuples seen by sorted access and random access as the dominant computational factor.
Let cost(A , D) be the nonnegative performance cost measured by running algorithm A over database D, which represents the amount of the tuples accessed.Instance optimality.
We use the notions of instance optimality.
We say that an algorithm A ∈ A is instance-optimal if for every A ∈ A and every D ∈ D there exist two constants c and csuch that cost(A , D) c × cost(A , D) + c .
Following [9], we say that an algorithm makes wild guesses if it does random access to find the score of a tuple with ID x in some list before the algorithm has seen x under sorted access.
For example, in Figure 1, we can see tuples whose IDs are G04 only at depth 3 under sorted and random accesses.
But wild guesses can magically find G04 in the first step and obtain the corresponding scores.
In other words, wild guesses can perform random jump on the lists and locate any tuple they want.
In practice, we would not normally implement algorithms that make wild guesses.
We prove the instance optimality of ULA (and ULA + ) algorithm, provided the size of each group is treated as a constant.
This assumption is reasonable as it is mainly about assuming that the schema of the database is fixed.Theorem 18.
Let D be the class of all databases.
Let A be the class of all algorithms that correctly find top-k,m answers for every database and that do not make wild guesses.
If the size of each group is treated as a constant, then ULA and ULA + are instance-optimal over A and D.Proof.
According to the definition of instance optimality, the main goal of this proof is to show that for every A ∈ A and every D ∈ D there exist two constants c and c such that cost(ULA, D) c × cost(A , D) + c .
We obtain the values of c and c as follows.Assume that an optimal algorithm A halts by sorted access at most up to depth d.
Since A needs to access at least one tuple in each list (otherwise we can easily make A err), the cost of A is at least (d + n i=1 g i − 1)C s , where C s denotes the cost of one sorted access and g i is the number of attributes in the group G i .
We shall show that ULA halts on D by sorted access at most up to depth d + m.
Then the cost of ULA is at most: The following part of the proof aims at showing ULA halts by depth d + m (if the optimal algorithm stops by depth d).
Let Y be the output set of A .
There are now two cases, depending on whether or not A has seen the exact top-m match instances for each combination when it halts.Cost (d + m)( n i=1 g i )C s + (d + m) n i=1 {g i ( n j=1 g j − g i )}C r = (d + m)( n i=1 g i )C s + (d + m) i j (g i g j )C rCase 1: If A has seen the exact top-m match instances for each combination, then ULA also halts by depth d < d + m, as desired.Case 2: If A has not seen the exact top-m match instances for each combination, then there are still two subcases depending on whether or not the lower bound of each combination ∈ Y is larger than the upper bound of the combinations not in Y when A halts.Subcase 2.1: For any combination ∈ Y and combination ξ Y, min ξ max , that is, all the combinations in Y meet hit-condition, and the size of Y is k, so ULA halts by depth d < d + m, as desired.Subcase 2.2: There exists one combination ∈ Y and one combination ξ Y such that min < ξ max .
At this point, our ULA algorithm cannot stop immediately.
But since A is correct without seeing the remaining tuples after depth d, we shall prove that ULA algorithm accesses at most more m depths (i.e., min ξ max at that moment), otherwise we can easily make A err.Given a list L i in a combination , let σ i denote the seen minimal score (under sorted or random accesses) in L i at depth d. Assume that A has seen m (m m) match instances for .
Let ω = F 1 (σ 1 , . . . , σ || ) denote the possible minimal tScore.
Then we define an mScore as follows.
mScore(, m) = F 2 (tScore(I 1 ), . . . , tScore(I m ), ω, . . . , ωm−m )Assume that A has seen m (m m) match instances for ξ.
Let λ ξ i denote the unseen possible maximal score (λ ξ i σ(τ)) below τ in list i by depth d + (m − m ) of ξ.
Let ϕ = F 1 (λ ξ 1 , . . . , λ ξ |ξ| ) denote the possible maximal tScore.
Then hScore is defined as:hScore(ξ, m) = F 2 (tScore(I ξ 1 ), . . . , tScore(I ξ m ), ϕ, . . . , ϕ m−m )Let us call a combination big if its mScore is larger than at least N com − k hScore of other combinations.
We now show that every member of Y is big.
Define a database D to be just like D, except object unseen by A .
In D , assign unseen objects V 1 ,. . . , V m with the score σ i under τ in each list L i ∈ ( ∈ Y), and assign unseen objects U 1 ,. . . ,U m with the score λ ξ i under τ in a list L j ∈ ξ(ξ Y).
Then A performs exactly the same, and gives the same output and accesses the same objects, for databases D and D .
Then by the correctness of A , it follows that all combinations in Y is big.Therefore by depth d + m, ULA would get at least m match instances, and the lower bound of is no less than mS core, and the upper bound of ξ is no more than hS core.
Since mS core(,m) hS core(ξ,m), so by depth d + m, min ξ max .
Therefore, ULA halts by depth d + m, as desired.The next theorem shows that the upper bound of the optimality ratio of ULA is tight, provided the aggregation functions F 1 and F 2 are strictly monotone (the proof is omitted due to space limitation, it can be found in a technical report that cannot be referenced due to the anonymous review).
Theorem 19.
Assume that F 1 and F 2 are strictly monotonic functions.
Let C r and C s denote the cost of one random access and one sorted access respectively.
There is no deterministic algorithm that is instance-optimal for top-k,m problem, with optimality ratio less than T + KC r /C s , (which is the exact ratio of ULA), where T = n i=1 g i , K = i j (g i g j ), and g i denotes the number of lists in group G i .
When we consider the scenarios when an algorithm makes wild guesses, unfortunately, our algorithms are not instance-optimal, but we can show that in this case no instance-optimal algorithm exists.
Note that this appears a somewhat surprising finding, because the TA algorithm for top-k problems can guarantee instance optimality even under wild guesses for the data that satisfies the distinct property.
In contrast, the ULA algorithm for top-k,m problem is not instanceoptimal even for distinct data.
The intuition for this disparity is that top-k problem needs to return the exact k objects, forcing all algorithms (including those with wild guesses) to go through the list to verify the results, but an algorithm for top-k,m search can correctly return k combinations without seeing their m objects by quickly locating a match instance to instantly boost the lower bound.Theorem 20.
Let D be the class of all databases.
Let A be the class of all algorithms (wild guesses are allowed) that correctly find top-k,m answers for every database.
There is no deterministic algorithm that is instance-optimal over A and D.Finally, we consider the case (not so common in practice) when the number of attributes in each group is treated as a variable.
While our algorithm is not instance-optimal in this case, we can show that no instance-optimal algorithm exists.Theorem 21.
Let D be the class of all databases.
Let A be the class of all algorithms that correctly find top-k,m answers for every database.
If the number of elements in each group is treated as a variable, there is no deterministic algorithm that is instance-optimal over A and D.
In this section, we study XML keyword query refinement using the top-k,m framework.
We show how to judiciously define the aggregate functions and the join predicates in the top-k,m framework to reflect the semantics of XML keyword search and adapt the aforementioned three algorithms, i.e., ETA, ULA, and ULA + .
Given a set of keywords and an XML database D, we study how to automatically rewrite the keywords to provide users better and more relevant search results on D, as in real applications users' input may not have answers or the answers are not good.
In particular, we rewrite the users' queries by two operations: transformation by rules and deletion.
We assume that there exists a table to contain simple rules in the form of A→B, where A and B are two strings.
For example, "UC Irvine"→"UCI", "Database"→"Data base".
These rules can be obtained from existing dictionaries, query log analysis [14], or manual annotation.
Given a query q = {q 1 , . . . , q n }, we scan all keywords sequentially and perform substring match by rules to generate groups.
1 For example, assume that q={UC Irvine, Database}; then the two groups are G 1 ={UC Irvine, UCI} and G 2 ={Database, DB}.
We assume that each node in an XML database is assigned with its JDewey identifier [7], which gives the order numbers to nodes at the same level and inherits the label of their ancestors as their prefix.
In Figure 2, for example, school(1.2.4) shows that the label of its parent is 1.2 and school is the fourth node in level 3 from left to right.
One good property of JDewey is that the number is a unique identifier among all nodes in the same tree depth.In general, to convert the problem of XML keyword refinement to the top-k,m framework, given a keyword query q = {q 1 , . . . , q n }, we first produce a set of groups G 1 ,. . . ,G t where each element w i j ∈ G i is a keyword associated with an inverted list composed of binary tuples τ=ρ(τ), σ(τ), where ρ(τ) is the JDewey label and σ(τ) is the score of the node (e.g., tf-idf).
Then, the XML keyword refinement problem is to return top-k combinations of keywords that have the best aggregate scores in their top-m search results.We now present a widely adopted approach (e.g., [7,19]) to formally define tScore and cScore in the XML top-k,m problem.
In the XML tree data model, LCA is the lowest common ancestor of multiple nodes and SLCA [1] is the root of the subtree containing matches to all keywords without a descendant whose subtree contains all keywords.
In particular, given a combination and tuples τ 1 . . . τ || from different groups, one match instance (i.e., keyword search result) is formed by the SLCA nodeñnode˜nodeñ = slca(ρ(τ 1 ), . . . , ρ(τ || )).
Letx i =σ(τ i ) × d(l i − ˜ l), where l i denotes the depth of node ρ(τ i ), ˜ l is the depth ofñof˜ofñ, and d(·) is a decreasing function to leverage the score of SLCA at different levels (e.g., d(x) = 0.9 x in our implementation).
We define tScore to compute the score of one match instance I as 1 In cases where one word (or a set of words) appears in multiple rules, we need to design an algorithm to generate the mapping from words to groups.
tScore(I ) = min(x 1 , . . . , x || ).
Intuitively, tScore assigns a greater score to an SLCA subtree with smaller sizes (l i − ˜ l) and higher weights (σ(τ i )).
Given a combination and an integer m, and the tScores of any m match instances I 1 , . . . , I m on , we define that cScore(, m) = α |G|−|| × max{tScore(I 1 ) + · · · + tS core(I m ))} where |G| is the number of groups, || is the number of attributes in the combination , and α is a damping constant (e.g., α = 0.7 in our experiments).
The choice of the component α |G|−|| is to give a penalty for deletion operation in keyword refinement in the sense that more deletions (i.e., smaller ||) lead to smaller scores.Compared to the regular top-k,m problem (in Section 4), XML top-k,m follows the same framework to return the top-k combinations ordered by cScore.
But the concrete definitions of tScore and cScore are changed to cater for the tree-specific XML data.
Therefore, to address the challenge from SLCA computation, we convert SLCA subtree computation to ID equi-join in each level.
In particular, inspired by [7], we put the nodes into segments by the length of their labels and nodes are ordered by their scores at each level.
See Figure 6.
For example, in the list of "DB", data are clustered into two segments by label lengths (i.e., 5 and 4).
The dashed boxes show columns corresponding to levels in XML trees, and numbers in one column uniquely identify nodes at that level (this is because of the feature of JDewey labels).
In Figure 6, column 4 contains all the 4th JDewey numbers of the nodes.
Note that the complete order of one column from different length segments can be reconstructed online, as we can maintain a cursor for each segment and pick one number with the highest score for all the cursors at each iteration.
Therefore, to compute SLCA node, in column i, if two numbers have the same value v, then they share the same prefix path and their LCA subtree is uniquely identified by v. Furthermore, since we access the data in a bottom-up manner in the sense that we first access the column with greater column number, we guarantee that the first seen LCA is the smallest LCA node.
We now describe how to adapt the previous three top-k,m algorithms to work with XML trees.
First, the application of ETA on XML (called XETA) is obvious.
For each combination of keywords, we compute their exact top-m search answers and return the top-k combinations by sorting the final cScore.
Clearly, this approach has to find the top-m search results for all the combinations, which is usually prohibitively expensive.Second, to apply ULA on XML top-k,m, we iteratively access the JDewey numbers by columns in a bottom-up manner, while continuously computing the lower and upper bounds, until all top-k combinations are found.
To compute the upper bound, the tricky issue here is that we access the numbers by columns and do not know the maximal values in other columns.
However, this issue can be solved by collecting the scores of top nodes in each column in the preprocessing phase.
More precisely, given a term (an attribute) e i and column l, let y l e i = max{z l , . . . , z M }, where M denotes the maximal length of nodes in the list of e i and z j = s j ×d( j−l) (l j M), where s j is the top score of nodes in length j and d(·) has been previously defined.
For example, see Figure 6, y 4 DB = max(0.8 × 0.9 1 , 0.9 × 0.9 0 ) = 0.9, where d(x) = 0.9 x .
Suppose that we are accessing the JDewey label τ in column l, and the score of current number (representing an LCA node) can be computed as x l =σ(τ) × d(n − l), where n is the total number of components in τ.
Given a combination , the threshold value T l can be defined as: } (1 i ||).
For example, in Figure 6, consider a combination "DB".
(Note that a single word can be also considered as a combination due to deletion operation.)
Assume that we are accessing the second tuple in column 4, i.e., the current number in list "DB" is 15 and the score x 4 DB = 0.7 × 0.
T l = min{t l e 1 , .9 (5−4) = 0.63, then T 4 = t 4 DB = max{x 4 DB , y 1 DB , y 2 DB , y 3 DB } = max{0.63, 0.9 4 , 0.9 3 , 0.9 2 } = 0.81, where y i DB = max{0.8 × 0.9 (5−i) , 0.9 × 0.9 (4−i) }.
We present the XULA algorithm to address top-k,m queries for XML keyword refinement in Algorithm 3.
We iteratively access numbers from different columns in a bottom-up manner.
Input: an XML top-k,m problem instance.
Output: top-k combinations of keywords.
(i) Initialize a variable l = H, the height of XML trees.
For each combination , initialize an empty set S to store SLCA nodes and their scores.
Example 22.
Given a query q = DB, UC Irvine, 2002, we generate three groups G 1 = {"DB", "database"}, G 2 = {"UCI", "UC Irvine"}, and G 3 = {"2002"}.
See Figure 6.
Let d(x) = 0.9 x and α = 0.7.
Consider a top-1,2 query.
In step (i), l=5, depth=1, and there are 17 combinations (not 4, due to the deletion operation) and ∀S = ∅.
In step (ii), the sorted accesses find four (trivial) LCA nodes (single node) (e.g., 1.1.3.4.2 in DB) and add them to the corresponding S .
In step (iii), we compute the upper and lower bounds for all 17 combinations.
For example, max DB = 0.9α (3−1) + 0.9α (3−1) = 0.882, min DB = 0.8α (3−1) =0.392.
Then the conditions in step (iv) and (v) are not satisfied.
Then we are in step (ii) again (l=5, depth=2) and add two new LCA nodes to S DB and S UCI respectively.
In step (iii), at this moment, five combinations satisfy the drop-condition and are pruned.
For example, max DB,UCIrvine = 0.7 = 2α (3−2) × min(0.9, 0.5), which is smaller than min DB = 0.735 = (0.7+0.8)α (3−1) .
Thus, the combination "{DB, UC Irvine}" is pruned.
Now all nodes in column 5 have been accessed.
Subsequently, we access nodes in column 4, 3 and 2.
Finally, in column 2, we find the result is the combination "{DB, UCI, 2002}", which has two SLCA nodes (i.e., 1.3.10 and 1.2) and its lower bound is 0.567 + 0.729 = 1.296, which is greater than the upper bounds of all other combinations.
Finally, to optimize the XULA algorithm, we can reuse the optimizations in ULA + , except Claim 16.
This is because XML keyword refinement supports the deletion operation and Claim 16 does not hold when an attribute is allowed to be deleted (recall Figure 4).
We have to omit the details of the optimized XULA algorithm (called XULA + ) here due to the space limitation, but note that XULA + is implemented and tested in our experiments.
In this section, we report an extensive experimental evaluation of our algorithms, using three real-life datasets.
Our experiments were conducted to verify the efficiency and scalability of all three top-k,m algorithms ETA, ULA and ULA + ; and their variants for XML keyword query refinement.Implementation and environment.
All the algorithms were implemented in Java and the experiments were performed on a dual-core Intel Xeon CPU 2.0GHz running Windows XP operating system with 2GB RAM and a 320GB hard disk.Datasets.
We use three datasets including NBA 2 , Yahoo! YQL 3 , and DBLP to test the efficacy of top-k,m algorithms in the real world.
Figure 7 summarizes the characteristics of the three datasets.
NBA and Yahoo! YQL datasets were employed to evaluate the topk,m algorithms, while DBLP dataset was utilized to test the XML top-k,m algorithms.NBA dataset.
We downloaded the data of 2010-2011 pre-season in NBA for the "Point Guard", "Shooting Guard", "Small Forward", "Power Forward" and "Center" positions.
The original dataset contains thirteen dimensions, such as opponent team, shots, assists and score.
We normalized the score of the data into [0,10] by assigning different weights to each dimension.
There are five groups, and the average size of each group is about 6.
YQL dataset.
We downloaded data about the hotels, restaurants, and entertainments from Yahoo! YQL 3 .
The goal of the top-k,m queries is to recommend the top-k combinations of hotels, restaurants, and entertainments according to users' feedback.
There are three groups, and the average size of each group is around 12.
DBLP dataset.
The size of DBLP is about 127M.
In order to generate meaningful query candidates, we obtained 724 synonym rules about the abbreviations and full names for computer science conferences and downloaded Babel 4 data including 9, 136 synonym pairs about computer science abbreviations and acronyms.Choosing the XML queries.
Regarding to the real-world user queries, the most recent 1, 000 queries are selected from the query log of a DBLP online demo [3], out of which 219 frequent queries (with an average length of 3.92 keywords) are selected to form a pool of queries that need refinement.
Finally, we picked 186 queries that have meaningful refined results to test our algorithms.
Here we show 5 sample XML keyword refinement as follows.
Q 1 :{thomason, huang} is refined by adopting "thomason → thomas".
Q 2 :{philipos, data, base} can be refined as {philipos, database}.
ULA ULA+ (PL) ULA+ (AR) ULA+ (RO) ULA+ (PL & AR) ULA+ (PL & RO) ULA+ (AR & RO) ULA+ (PL & AR & RO) (k=10, m=30)Percentage of accessed number Figure 9: The performance of different optimizations Q 3 :{XML, key, word, application, 2008} is refined by deleting "2008", followed by a merging of "key" and "word".
Q 4 :{jimy, kevin} which is refined by substituting "jimmy" for "jimy".
Q 5 :{world, wild, web, search, engine, 2007}, which is refined by either adopting "world, wild, web" → "www" or deleting "2007".
Metrics.
Our performance metrics are (1) running time: the cost of the overall time in executing top-k,m queries; (2) access number: the total number of tuples accessed by both sorted access and random access and (3) number of processed combinations: the total number of combinations processed in memory.
We inspected the results returned from all tested algorithms and found that their results are all the same, which verifies the validity of our algorithms.
Each experiment was repeated over 10 times and the average numbers are reported here.Experimental results on NBA and YQL dataset.
Here we illustrate the performance of algorithms (ETA, ULA and ULA + ) on NBA and YQL dataset by varying parameters k, m, and the data size.
In addition, we also deeply study the performance of different optimizations.Scalability with database size.
We evaluated the scalability of our algorithms with varying the number of tuples from 10K to 100K in both datasets.
As shown in Figure 10(a)(b), both ULA and ULA + expose an amazingly stable performance without any significant fluctuation both in running time and number of accessed tuples while ETA scales linearly with the size of the database in NBA dataset.
And in general, the execution time of ULA + outperforms ETA by 1-2 orders of magnitude, which verifies the efficiency of the optimizations.
In addition, as we can see in Figure 10(e)(f), the results in YQL datasets are similar to that in NBA dataset.Performance vs. range of k.
In Figure 10(c)(g) we tested the number of accessed tuples for both random accesses (i.e., ULA(R) and ULA + (R)) and sorted accesses (i.e., ULA(S) and ULA + (S)) by varying k while fixing m on both NBA and YQL datasets.
As shown, the number of random accesses are greater than that of sorted accesses for both ULA and ULA + .
In addition, ULA + has less accesses than ULA because of the effects of optimizations.
Note that, the number of accessed tuples in the ETA algorithm is the same over all k values (i.e., 82K on NBA dataset and 70K on YQL dataset, without shown in the figures), because ETA has to obtain the exact top-m match instances for each combination independent of k.Performance vs. range of m.
The results with increasing m from 3 to 30 in NBA data and from 10 to 50 in YQL data are shown in Figure 10(d)(h), respectively.
In general, both ULA and ULA + are 1 to 2 orders of magnitude more efficient than ETA method.
In addition, ULA + is more efficient than ULA, which verifies the effects of our optimizations.Effect of the optimizations in ULA + .
We then performed experiments to investigate the effects of four different optimizations in ULA + .
We fixed the parameters k = 10, m = 30 and the number of tuples is 100K.
First, to evaluate the approach of pruning useless combinations introduced in Section 4.4, we plotted Figure 8, which shows that the number of combinations processed in memory by our optimized algorithm is far less than that of ULA when the average number of lists in each group is increased from 10 to 50.
More than 60% combinations are pruned without computing their bounds, thus significantly reducing the computational and memory costs.
Second, to evaluate the effects of Claim 14 to 16, Figure 9 is plotted to evaluate the performance of different optimizations in terms of the number of accessed tuples.
In particular, ULA + (PL) uses Claim 14 to prune the whole lists to avoid useless accesses; ULA + (AR) applies Claim 15 to avoid random accesses in some lists; and ULA + (RO) employs Claim 16 to prevent random accesses for some tuples.
In Figure 9, the second, third and fourth bars show the results to measure three optimizations individually, while the others are actually a combination of multiple optimizations.
As shown, the combination of all optimizations has the most powerful pruning capability, reducing the accesses for almost 80%.
Experimental results on DBLP dataset.
Finally, we run the experiments to test the scalability and efficiency of XETA, XULA and XULA + algorithms on DBLP dataset.
In Figure 11(a)(b), we varied the size of DBLP dataset from 20% to 100% while keeping k=3, m=2.
As expected, both XULA and XULA + perform better than XETA and scale well in both running time and number of accesses.
In Figure 11(c)(d), we varied k from 1 to 5 while fixing m = 2 and 100% data size.
As shown, both XULA and XULA + are far more efficient than XETA, and XULA + accesses 74.2% less objects than XULA and saves more than 35.1% running time, which indicates the effects of our optimizations.
We proposed a new problem called top-k,m query evaluation.
We developed a family of efficient algorithms, including ULA and ULA + .
We analyzed different classes of data and access models, such as group size and wild guesses and their implication on the optimality of query evaluation.
We then showed how to adapt our algorithms to the context of XML keyword query refinement.
As for future work, extending our problem and algorithms with more access models and query types, e.g., non-random access model and non-monotone weight functions, while preserving optimal property, is a challenging item for future research.
