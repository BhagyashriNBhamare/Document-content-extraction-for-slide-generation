Management tasks in datacenters are usually executed in-band with the data plane applications, making them susceptible to faults and failures in the data plane.
In this paper, we introduce power line communication (PLC) to datacenters as an out-of-band management channel.
We design PowerMan, a novel datacenter management network that can be readily built into existing datacenter power systems.
With commercially available PLC devices, we implement a small 2-layer PowerMan prototype with 12 servers.
Using this real testbed, as well as large-scale simulations, we demonstrate the potential of PowerMan as a management network in terms of performance, reliability , and cost.
A typical datacenter [29,65,67] contains more than thousands of servers, switches, storage units, etc.
Datacenter operations and management tasks [42,52,85] include device installation, bring-up/restart, configuration, monitoring, diagnostics, and Software Defined Networking (SDN) applications [58], etc.
At such scale, delivering management traffic is a critical task.In existing datacenters, management traffic is usually carried in-band with the data plane traffic.
Separate service queues and/or VLANs [11] may be reserved for reliable and timely delivery of management messages.
However, this approach introduces fate sharing [85] between the data plane traffic and management traffic: failures in data plane network will cut off management traffic to the exact network regions at fault, rendering important and relevant management tasks, such as diagnostics and recovery, impossible.Therefore, an out-of-band management network (MN) is desirable for datacenter operations.
A practical out-ofband MN for datacenters should be:• Survivable: MN should be always available, and should survive faults and failures in the datacenter, in order to perform diagnostic and recovery tasks.
• Scalable: MN should be scalable enough to access all the devices in the datacenter.
• Deployable: MN should be deployable at low cost, and compatible with existing infrastructure.
Prior proposals do not meet these requirements simultaneously.
Out-of-band MNs can be constructed as a parallel electrical network 1 using the same networking equipments as the data plane.
To reach all devices, this parallel network needs a port count larger than the data plane network; because this fabric not only accesses all the servers like a data plane network, it also needs to reach the management ports of all the switches and other devices.
Thus, the cost is prohibitive to build an parallel high port count electric fabric as a MN.Non-electrical communication channels in datacenters, such as WiFi [36,47,84,85] and free space optics (FSO) [41,48], are usually built to accommodate dynamic data plane traffic demands.
As out-of-band MNs (parallel to data plane network), deploying them results in significant changes to datacenter infrastructure (e.g., raising the ceiling [48,84], installing reflective surfaces [36,41,48,84], etc).
Furthermore, it is also expensive to build a wireless or FSO fabric that reaches the port count required by MNs with current technologies ( §6.3).
We believe, for a datacenter MN, power line communication (PLC) technology is an appealing option.
PLC [39], proposed in 1900s [66], allows communication between devices connected by power lines.
PLC is known to be challenging [60,69].
However, over short distances and among limited nodes, current PLC modems for home-use can support Gigabit connections using OFDM [61] in PHY layer and CSMA/CA [33] in the MAC layer, providing Ethernet networking to home appliances, e.g., smart TVs, WiFi extender, home networking, etc.
We believe these emerging technologies [24,56,81]) open up the opportunity of building a low-cost PLC network with necessary bandwidth and latency for management applications in datacenters.First of all, a MN using PLC technology naturally meets the survivability requirement, as power system is foundational to any datacenter.
A built-in PLC management network is always available, long-term survivable, reaching every device, and independent of the data plane.
Furthermore, deployment of PLC network reuses the power system wiring, and requires no change to the existing room designs or floor plan, which is economic.Yet, one question remains: can a MN using PLC also meet the other two requirements-being scalable and deployable in existing datacenters?
To answer this question, we build a PLC testbed ( §3) with commodity-offthe-shelf (COTS) PLC modems designed for households (using OFDM in PHY layer and CSMA/CA in the MAC layer).
Our experiments show that task completion times and user experience of real management applications on PLC network is comparable to that on a Gigabit electrical network.
However, we also conclude that a MN directly using COTS PLC devices cannot meet the above two requirements: 1) additional in-rack wiring may exceed existing rack designs; 2) due to PLC signal interference, the network can only scale to 6 nodes within a small range (usually 100s of meters [13]) on a single power circuit.To tackle these problems, we design PowerMan ( §4), a MN that can be constructed with existing PLC technology, to support datacenters with more than 10 5 servers.
As shown in Figure 1, PowerMan redesigns and replaces two key components in existing datacenter power systems ( Figure 2): a power supply unit (PSU) for servers and switches, and a power distribution unit (PDU).
• PowerMan PSU lowers the wiring complexity by increasing the integration level.
It combines a normal PSU module with a PLC modem module, and acts as a network interface to the server OS.
Using PowerMan PSU, PLC network can be deployed easily in the current server racks.
• PowerMan PDU addresses the scalability issue.
Due to available carrier frequencies and signal quality constraints, COTS PLC modems designed for home-use only support communication within 64 nodes in the To scale beyond this, PowerMan PDU eliminates signal interference on the boundary of PLANs using low pass filters, enabling the reuse of the same carrier frequencies across different PLANs.
By connecting multiple PLANs into a tree topology, we can scale the PLC network to reach potentially >10 5 servers, providing datacenter-wide coverage.
We have implemented a 2-layer PowerMan prototype ( §5) connecting 12 servers across two racks.
The prototype is built with existing PLC technology in an academic datacenter without any modification to the existing infrastructure, e.g., room plan, power line wiring, and ceiling height.
We demonstrate the potential of PowerMan as a datacenter MN (in terms of performance ( §6.1), availability ( §6.2), and cost ( §6.3)) by running real management applications in our small testbed as well as largescale simulations.
Our key contributions are:• We introduce PLC as an out-of-band channel for datacenter MN.
To validate the idea, we build a real smallscale PLC testbed to quantify the throughput, latency, and packet loss conditions for management applications ( §3).
We find that, due to various sources of interference in datacenter [60], PLC testbed exhibits lower performance than advertised (e.g. ≤50Mbps TCP throughput (measured) v.s. 1000Mbps PHY bit-rate (advertised)).
We further expose the wiring complexity and scalability issues that cannot be addressed with existing PLC devices.
• We design PowerMan to address the wiring complexity and scalability problems identified above.
We validate the design by implementing a PowerMan prototype.
On the prototype, our experiments with production traces show <24ms average flow completion time (FCT) and >10Mbps throughput for the 1-to-N/N-to-1 management traffic patterns.
Experiments with real management tasks show that, compared to a Gigabit electrical network, the completion times of all tasks are prolonged by <40.62% on PowerMan, with a minimum of 1.57% (66.43s → 67.47s) for a Humanin-the-Loop task, and a maximum of 40.62% (32ms → 45ms) for a SDN task.
We also confirm PowerMan's utility at large scale with simulations, and find that for a PowerMan with 120K servers, the round-trip time (RTT) for management applications is ∼40ms.
• Cost comparisons with other technologies show that, apart from saving infrastructure modification costs, PowerMan can be constructed with low equipment cost (1/2∼1/3 of the cost of related designs at the same scale).
PowerMan is also power-efficient in operation: its power consumption is 6%∼9% of other technologies.Caveat: PowerMan is suitable for many management applications given its performance characteristics.
However, we acknowledge that, for some applications, delivering control messages with low latency is crucial: fine-grained load balancing [62,82] and flow scheduling [20,30] need to configure data plane on millisecond time scale.
PowerMan alone is not suitable for such applications.
For them, we suggest dual-homing the controller with access to both the data plane network and PowerMan network.
Latency-sensitive traffic can use the fast data plane, while PowerMan can serve other management applications.
We believe PowerMan is also valuable as a back-up/diagnostic network to fall-back on in case of failures.
The power system [35,44] is the most fundamental system in a datacenter.
A typical DCPS is shown in Figure 2, and it is composed of:• The main switch board (MSB) directs electricity from one or more sources of supply to several smaller regions of usage.
It feeds into all loads in the datacenter.
• The uninterruptible power supply (UPS) provides consistent power to critical loads without interruption.
It contains energy storage, which supplies power to the load when the utility power is down.
• A power distribution unit (PDU) is an electrical distribution device, and it can be free-standing or rackmounted.
The PDU houses circuit breakers that are used to create multiple branch circuits from a single feeder circuit, and can also contain transformers, surge protection devices, and power monitoring/controls.
• A power supply unit (PSU) rectifies AC power from the connected PDU to DC power.
For reliability, critical servers and switches are usually equipped with two PSUs in case of failure.
DCPS is often classified as belonging to "Tier I-IV" [71] depending on the power distribution, UPS, redundancy, etc. [15,18] For example, in a Tier-III DCPS [35,44], each critical load device has two power distribution paths (including redundancy components), and the power system in Figure 2 is replicated for each PSU.
In what follows, for clarity, our design of PLC networking is limited to the primary power system depicted in Figure 2 by default, and we discuss how all tiers of DCPS can adopt PowerMan in §4.4.
PLC uses electrical wires to simultaneously carry high frequency data signals and 50∼60Hz AC power transmission.
PLC has been widely used in power systems for protection, telemetering, and industrial control applications [39,40,45,83] with data rate of a few Kbps.In recent years, we witness a rapid growth of PLC appliances for home networking, due to its ubiquity (home power systems provide sockets in every room) and ease of deployment (no new wire needed).
The home networking market drives PLC technology to reach higher bandwidth, in order to support popular use cases such as broadband Internet access, video streaming, gaming, etc.
Through standardization efforts from the US HomePlug Powerline Alliance and European Home System Consortium, vendors have converged to use Orthogonal Frequency Division Multiplexing (OFDM) [61] as the modulation scheme in PHY layer, and CSMA/CA [33] as the MAC layer protocol.
Adopting the HomePlug protocols (HomePlug 1.0 [56]/AV [24]/AV2 [81]), PLC modems and adapters can form a communication network providing Ethernet connectivity to TVs, gaming console, and PCs.
Currently, many vendors offer PLC modems with up to 1200Mbps PHY layer bit-rate [8,16,17], and up to 64 devices in one PLC network [13,16,17].
In academia, there have been continuous efforts in PHY [60,69] and MAC [74,75,76,77,78] layers for PLC to achieve higher throughput and lower latency.
Orthogonal to prior work, we focus on the application of PLC in the context of datacenter MN.
We believe PLC is a suitable candidate for MN as a built-in communication network in DCPS for the following reasons:• Power system is the last-to-fail system in datacenters, and is independent of the data plane network.
A MN within the power system therefore can survive data plane failures, and is ready for immediate diagnosis and recovery.
• Power system reaches every device in datacenters, providing full visibility for management applications.
• PLC reuses the wires in the power system, and there is no need to change the existing room plans, ceiling height, and rack dimensions.
This compatibility with existing datacenter designs greatly reduces the deployment cost.In the following, leveraging the technology advances in household PLC appliances, we are motivated to: 1) understand performance characteristics of PLC networks ( §3), 2) expand PLC network from home-scale to datacenter-scale ( §4), and 3) evaluate our design for datacenter MN using PLC ( §6).
15th USENIX Symposium on Networked Systems Design and Implementation 563 In this section, we first build a small-scale PLC testbed.
Then, we perform a series of experiments and measurements to learn the communication characteristics of the PLC channel in a datacenter environment.
We describe the devices we use and how they are connected to build the testbed.Server & switch:We use Huawei FusionServer RH1288 with Intel E5-2630 and 64GB memory (1 Rack Unit).
Each server has a NetXtreme BCM5719 Ethernet Network interface card (NIC) with 4×1 GbE ports.
The servers are all connected to a Gigabit Ethernet switch via their first Ethernet interface (Eth0).
PLC modem: We obtained 16 Netgear Powerline 1000 (PL1000) PLC modems (US$ 30.3 per piece) via local home appliance vendors.
As in Figure 3, each modem has one built-in power plug and one RJ-45 port for Ethernet connection.
The max power consumption of PL1000 is 3.73 watts (0.49 watts in standby mode).
It is compatible with HomePlug AV protocols.
For OFDM, it uses frequencies in the range from 2 MHz to 86 MHz.
PDU: The rack-mounted servers and Ethernet switch are plugged into the in-rack PDU with no empty sockets.
We use a separate Thomson TM-EC6 8-socket power extension cord for PLC modems.
Interconnection & wiring: We connect the PLC modems to the power extension cord, and then plug them into the power outlets on the in-rack PDU.
Each server is connected to one PLC modem via its second Ethernet interface (Eth1).
In summary, as shown in Figure 3, we build a PLC testbed using commodity components.
Each server is both connected to an electrical ToR Ethernet switch via Eth0, and to a PLC modem via Eth1.
These modems are connected via a power strip, forming a PLC network.Through building the testbed, we identify the first difficulty for practical deployment of PLC networking in datacenters: wiring.
This is because each of these external PLC modems requires an additional power socket slot and a network cable, resulting in 2× socket count on the in-rack PDU and 1.5× space for cabling.
As the current rack design does not anticipate the usage of PLC devices, we find it difficult to organize the additional cables, and the PLC modems have to be attached to a power Next, we measure its performance using both synthetic traffic and real management applications.
We first investigate how many PLC modems can coexist in a PLC network.
We add PLC modems to the power strip one by one (IP addresses and subnet masks are assigned beforehand), and then monitor the indicator lights on the modems for successful connections.
Finally, we verify the connection on the servers via ping utility.
We observe that the network can accommodate at most 6 PLC modems.
When there are more than 6 modems in the network, the first 6 modems are connected.
Setting: For the flow size, we adopt 2 realistic flow size distributions used in prior work [21,22,25,43]: one from a web search cluster [21] and the other from a data mining cluster [43], respectively.
We also include a uniform distribution for reference.
All distributions, shown in Figure 4 are capped at 25KB, as we are mainly interested in management applications, which tend to have shorter flow sizes.
We use the following traffic patterns: • 1-to-N: This pattern occurs in management applications where a master pushes configurations to slaves.
• N-to-1: This pattern occurs in monitoring applications where a server collects statistics from clients.
Among the 6 connected servers, we create traffic patterns using a traffic generator [6], which is a client/server application for generating user-defined traffic.
The server listens for incoming requests on the specified ports, and replies with a flow with the requested size for each request.
The client connects to a list of servers, and generates requests to randomly chosen servers.
For each request, it samples from the input request size and fanout distributions to determine the request size and how many flows to generate in parallel for the request.
All packets use the same default priority.In each experiment, we use a different combination of patterns and distributions, and each client generates 25K requests.
We measure the flow completion time (FCT), throughput, and packet loss rate for each flow, and Table 1 summarizes the results.
The average round-trip time (RTT, grouped by traffic patterns) is shown in Figure 5.
We then repeat the experiments using the Gigabit electrical network (via Eth0), and the results are shown in the parentheses in Table 1.
Results: We make the following observations:• Latency: The average FCT on PLC testbed is around 2 order-of-magnitude (OoM) larger than that on the electrical network.
We observe the same trend for 99th percentile FCT.
For RTT, the smallest one (2.2ms, from 1-to-1 pattern) is also around 2 OoM larger (∼20us on the electrical network).
• Throughput:The advertised 1000Mbps bandwidth is in fact the maximum PHY bit-rate, and we cannot obtain more than 50Mbps TCP throughput on the testbed, which matches field-tested results [23].
The throughput is about 1 OoM less than that on the electrical network.
• Packet loss rate: The packet loss rates are less than 0.5% for PLC testbed across all cases, while the electrical network shows near-zero packet loss rates.Implications: As expected, the PLC testbed we constructed shows much lower throughput and longer latency compared to the Gigabit electrical network.
This is because PLC is an "extremely harsh environment" [60] for the high-bandwidth, high-frequency communication signals, as critical channel parameters (e.g., noise, impedance, and attenuation) are highly unpredictable and varied with time, frequency and location [69].
As a result, the PLC network is clearly inappropriate for time-critical tasks (e.g., fine-grained load balancing [62,82] and flow scheduling [20,30]).
However, the PLC testbed is shown to deliver <10ms average FCT and >10Mbps throughput for N-to-1/1-to-N patterns, which are common for management applications.
Thus, for latency-insensitive management tasks (e.g., device installation, bring-up/restart, configuration, monitoring, diagnostics, etc), the PLC network remains attractive, due to its other benefits such as survivability in case of data plane failure, compatibility with existing datacenter design, and economy.Therefore, we proceed to evaluate the end-to-end application performance of the PLC testbed with latencyinsensitive management tasks.
Based on the communication model, we choose two management platforms for on-premise datacenters and cloud virtual clusters: push-based Ansible [1]), and pull-based Chef [3].
As SDN is an important class of applications, we also include a SDN controller, Ryu [12].
We perform tasks with real usage scenarios.
• Ansible [1] is an automation engine for clusters.
Ansible is push-based and agentless: from the master node, it manages slave nodes through SSH connections.
We deploy Ansible 1.7.2 on our testbed, and perform one automated task and one Human-in-the-Loop (HitL) task:• AnsibleLAMP: An automated LAMP deployment with two web severs, two load balancers, and two database servers.
The playbook is based on Ansible official examples [2].
• AnsibleHitL: A HitL setting with an operator checking configurations of servers.
Via Ansible adhoc commands [7], in each experiment, the operator sequentially executes df, route, and lsmod on all servers.
• Chef [3] is an automation platform for cluster management.
Chef is a pull-based: clients poll a centralized master periodically for updates.
On our testbed, we install a Chef Server 12.11 in standalone mode on one of the servers, and the rest are installed with Chef Client 12.17.
We perform two automated tasks described by Chef cookbooks.
• ChefReload: This cookbook [4] automatically reloads the Apache service on all servers.
Figure 6: Management Applications on PLC testbed electrical network.
We use percentage increase in completion time as the metric: for a task, denote its completion time on Gigabit electrical network as T e and on PLC testbed T p , the metric is defined as:T p −T e T e ×100%.
Results: In Figure 6, we observe encouraging application performance delivered by the PLC network.
Overall, we find that using PLC results in less than 30% increase in completion time compared to the Gigabit electrical network for all the tasks, and this increase is mainly due to the latency introduced by the PLC network.
In the best case, we notice that, for AnsibleHitL task, the PLC network performs almost the same as the electrical network (only 1.34% longer).
This is because human response time is the main contributor of latency in this task.
In the worst case, the completion time of RyuFWConf is increased by 30%, which is because it performs only HTTP query/response and network latency contributes the most to the completion time.
In summary, our results of endto-end application performance on PLC network is promising for latency-insensitive management tasks.
We conclude: 1) It is possible to use commodity PLC modems to form a PLC network that provides Ethernet connectivity for all connected servers.
2) PLC performance is promising for management applications: it provides <10ms average FCT and >10Mbps throughput for N-to-1/1-to-N patterns, which are common for management applications; the management tasks also have similar user experience.
3) This PLC network, however, cannot be directly used in real datacenters due to the deployability (wiring) and scalability problems.Therefore, we are motivated to tackle the wiring and scalability issues, so that the PLC technology can be deployed in real datacenters.
To tackle the wiring and scalability issues, we design PowerMan.
To ensure deployability, our guiding principle is to respect the existing datacenter designs, and preserve the floor plan, room design, rack dimensions, and power line wiring.
To this end, PowerMan only replaces two types of components in existing DCPS: PSU and PDU.
In the PLC testbed, each server needs two network cables: one for data plane connectivity, and one for PLC [65,67], it is infeasible to accommodate this additional wiring with existing rack design.
To address the problem, we design a novel PSU for rack-mounted servers and switches in datacenters (Figure 7).
The key idea is to increase the integration level in the PSU to reduce external wiring.We present two designs of PowerMan PSU to fit different deployment scenarios: Full-Integration and Bumpin-the-Wire.
Full-Integration is designed for new installation of datacenters, as the datacenter operator has the freedom to customize the hardware configuration of each server/switch.
We combine a normal PSU module (nPSU in Figure 7), a PLC modem module, and a network interface module in the PowerMan PSU.
It connects to the mainboard of the server via a PCIe interface, and appears as another NIC to the OS, which allows users to use familiar networking stack to access the PLC network.The Bump-in-the-Wire design is for incremental deployment in existing datacenters, and it leverages the integrated NIC on the mainboard of rack-mounted servers, which is exposed as the management port on the server back panel.
The PLC modem attaches to the PSU externally, and acts as a "bump" in the power cable from the PDU socket to the PSU.
The power to the server is fed into its PSU through the PLC modem via a bypass circuit.
The PLC modem connects to the management port via a RJ-45 network cable, so that the integrated NIC can access the PLC network.
This network cable travels a short distance from the power port to the management port on the back panel, and thus does not tangle with other in-rack cables.Via PowerMan PSU, a server can connect to a PLC MN without complicated wiring and additional power sockets, thus is compatible with the design and dimensions of the current racks.
The scale of PLC network on our testbed is limited to 6 nodes.
We refer the PLC network within the PDU as PLC LAN (PLAN).
Manufacturers of more advanced models claim that the scale can be as large as 64 nodes [13,16,17].
However, this is still too small for production datacenters [65,67].
The main reason for such limited scalability is that these devices are designed for home-use, where network size is not the main concern.To scale, we design a novel rack PDU (Figure 8).
The key idea is to remove cross-PDU PLC signal interference but maintains network connectivity.
PowerMan PDU achieves this with two main components, a low pass filter (LPF) and a PLC gateway.We keep the circuit of a normal PDU, and add a LPF between the circuit and the external power line.
Since the OFDM frequencies used in the PLC modem is ≥ 1.8MHz [60] and the AC power frequency is 50∼60Hz, a LPF with appropriate cut-off frequency (between 60Hz to 1.8MHz) can greatly attenuate the outgoing and incoming high frequency PLC signals, thus effectively eliminating the interference from/to other PLANs.While the PLC signals are mostly eliminated across the LPF, the network connectivity is preserved using a PLC gateway.
The PLC gateway consists of a packetforwarding hardware gateway and two PLC modems.
One modem is connected to the PLC network inside the PDU, and the other is connected to the PLC network on the external, upper-layer PDU.
The PLC gateway is therefore connecting the PDU's PLAN and the upper-layer PDU's PLAN by forwarding packets between them, with no PLC signal interference.PowerMan PDU replaces the rack PDU and retains the same cable and socket count.
It acts as a switch for the PLC network devices on the same rack.
With the new PowerMan PDUs developed, we can now connect them and scale the PLC network to support real datacenters.
We leverage DCPS to interconnect the PLC devices.
Since PDUs in DCPS are connected in a tree topology (Figure 2), we also choose to use the same topology to scale.
Other topologies (e.g. ring, mesh, hypercube, etc.) requires changing the wiring of the power system.
Take ring topology as an example, each PDU connects to more than one other PDUs, requiring an additional power cable for each PDU.
Other topologies also requires a different power allocation scheme, both inside the PDU and across PDUs.As shown in Figure 9, we construct the PowerMan PDUs into a (k−1)-ary tree topology, where k is the number of PLC devices supported in a PLAN.
For our current PLC modems, k=6; up to k=64 have been reported for other COTS PLC modem models [13].
With height [44] h, this topology can connect (k−1) h PowerMan PSUs.
With a tree height of h=3 and k=64, 250K PSUs can be connected.
PowerMan leverages the redundancy in existing DCPS to achieve high availability.
As mentioned in §2, DCPS can be classified into 4 tiers [71], and all can be integrated with PowerMan.
• Tier-I DCPS have a single path for power distribution without redundant components, and PowerMan can be integrated as in Figure 9.
• Tier-II adds redundant components to this design (N + 1), improving availability, and PowerMan can be integrated into the main distribution path as for Tier-I DCPS, the PDUs in the redundant components should also be replaced with PowerMan PDUs.
• Tier-III datacenters have one active and one alternate distribution path for utilities.
Each path has redundant components and are concurrently maintainable, providing redundancy during maintenance.
PowerMan can be integrated into both distribution paths.
As an example, Figure 10 showcases how PowerMan can be integrated with Tier-III DCPS [44].
This architecture is configured with two sides, A and B. Each side can include multiple UPSs, and either side can handle 100% load.
If one side has a problem, the load Figure 11: Two-Layer PowerMan Prototype is automatically switched to the other side.
Automatic load transfer switches can reside upstream of the UPS for maintenance isolation purpose.
This design ensures a high level of system availability even during maintenance or component failure.
PowerMan can therefore be replicated in both sides; the controller node where management applications are located should also connect to the root PowerMan PDUs on both sides, so that when failure (either PowerMan or DCPS) in either side happens, management applications can still access the servers and switches.
• Tier-IV DCPS have two simultaneously active power distribution paths, redundant components in each path, and are supposed to tolerate any single equipment failure without impacting the load.
PowerMan can be integrated in the same way as Tier-III.
Embedded in DCPS, PowerMan share the redundancy and availability mechanisms, thus is expected survive even partial power outages in Tier-II (or higher) DCPS.
We implement a PowerMan prototype to validate the design, and its schematic is shown in Figure 11.
We have not yet constructed a PowerMan PSU that can be fit into our rack-mounted servers, but its functionality can be emulated using the same setting as §3.1: each server connects to a PLC modem via one of its NIC ports (Eth1).
PDU has two components: a LPF and a PLC gateway.
For the LPF, instead of implementing LPF circuits and installing them on the power extension cords, we identify that power extension cords with surge protection can serve as low cost alternatives 2 .
This is because surge pro- We note that the use of surge protector as LPF is only for prototyping, and real deployment of PowerMan should use properly designed LPF in the PDU.
We obtained Targus SmartSurge 6 power extension cords from local vendors, and our testing shows that two PLC modems cannot establish connection across two such cords, which indicates that they have the correct cut-off frequency.
In this way, the PLC modems can form a PLAN within the power extension cord that they are attached to, without interference of PLC signals from other PLANs.Next, we implement the PLC gateway with two PLC modems and a rack-mounted server.
The server connects to the two modems via Eth1 and Eth2 ports.
The modems are attached to different power extension cords with surge protection.
Therefore, their signals are isolated, and can only propagate within their own PLANs.
With routing rules correctly configured, the server acts as a packet forwarding gateway between the two PLANs.We construct 3 prototype PowerMan PDUs, which form a tree topology with 2 layers, as shown in Figure 11&12.
In Layer-0, the prototype has two racks, and each rack forms a PLAN on its own PDU.
The two Layer-0 racks are connected to a Layer-1 PLAN via their PLC gateways.
In addition to the gateways of the two racks, we connect another two servers to act as gateways on the Layer-1 PLAN.
The routing tables and IP addresses are properly configured in all the servers and gateways, so that each server can reach all the other servers on this PowerMan PLC network.
In this section, we evaluate three aspects of PowerMan: performance, reliability, and cost.
• Experiments with production traces show <24msaverage FCT and >10Mbps throughput for 1-to-N/Nto-1 traffic patterns.frequency signals.
• Experiments with real management applications demonstrate that, compared to the Gigabit Etherent, the completion times of all tasks are only prolonged by <40.62% on PowerMan.
• By simulating a year of operation, PowerMan is shown to achieve >99.9977% availability (leveraging the redundancy in DCPS) at the scale of 250K servers.
• Apart from saving infrastructure modification costs, PowerMan can be constructed with low initial cost (1/2∼1/3 of the cost of other technologies at the same scale), with 6%∼9% operating power usage.
On the PowerMan prototype, we perform the same set of experiments as in §3.2.
Experiments with Production Traces: In addition to the setting in §3.2.2, our experiments here include another parameter: distance, which refers to the number of PLC gateways (i.e., hops) between the servers and clients.
For example, for 1-to-5 pattern with distance=1, a client in Rack 1 will only send requests to the traffic generator server hosted in gateways in Layer-1 PLAN 3 .
To understand this parameter in real PowerMan deployments, for a controller node connected to the root with tree height h=3, its distance to all PSUs is merely 2.
We summarize the results from the experiments on the prototype in Table 13.
We make the following observations.
• Latency: Compared to Table 1, we see on average 3.04ms increase in FCT if distance increases by 1, and 4.13ms if distance increases by 2.
This corresponds to our RTT measurements on the prototype in Figure 14: when distance increases from 0 to 1, the RTT increases on average 2.19ms, and 2.92ms from 1 to 2.
• Throughput: Increasing distance by 1 (2) decreases the throughput by 3.27Mbps (6.80Mbps) on average.
Still, the prototype provides >10Mbps for 1-to-N/Nto-1 patterns.
• Packet loss: interestingly, increasing distance lowers the packet loss rate: 1 (2) increase in distance decreases the packet loss rate by 0.05% (0.05%) on average.
This is because the inter-PLAN flows converge at the gateway, and from there, are forwarded to their destinations.
This store-and-forward behavior for flows across PLAN results in lower packet loss rate compared to the flows within a PLAN running CSMA/CA.
In summary, PowerMan prototype demonstrates <24ms average FCT and >10Mbps throughput for common management application traffic patterns (1-to-N/N-to-1) for distance=2.
This indicates that, a PowerMan with tree height h=3 can support management applications with 3 The setting for the results in Table 1 can be considered as distance=0 (within the same PLAN).
Experiments with Real Management Applications: Next, we evaluate the end-to-end applications performance.
We perform the tasks in §3.2.2 again on both PowerMan prototype and the Gigabit electrical network.
We scale the set of tasks in §3.2.3 so that they can cover all 10 servers in the testbed.
For example, the AnsibleLAMP task now configures 4 web servers, 2 load balancers, and 4 database servers.
We assign one of the gateway server in Layer-1 PLAN as the master node for Ansible, Chef, and Ryu, which is the darkened gateway in Figure 11.
We plot the results in Figure 15.
As expected, due to the need of traversing one PLC gateway, the completion times increase for all the tasks.
Among them, for AnsibleHitL, the PLC network performs almost the same with the electrical network (only 1.57% slower) as distance increases.Also, as explained in §3.2.2, network latency dominates the completion times of the two Ryu tasks, so their metrics increase the most, i.e., 40.62% and 40% respectively.
Furthermore, using PowerMan results in <30% increase in comple- Figure 16: Large Scale Simulations: RTT and throughput tion times for Chef tasks.
Overall, adding one more tier in PowerMan prototype compared to §3's testbed results in less than 5% increase in task completion time.
The current prototype is still too small to reveal PowerMan's performance in actual deployments.
Using ns-3 [50] simulator, we perform simulations at the scale of real datacenters [65,67] to infer the user experience of PowerMan in actual deployments.
Setting: Since each PowerMan PDU corresponds to a PLAN that uses CSMA/CA ( §2.2), we simulate PLAN using the CSMA network implementation in ns-3 with parameters in [73,79].
We interconnect PLANs with point-to-point links, which corresponds to the PLC gateways in our design.
We assume that a controller connects to the root of the PowerMan tree topology with a 1Gbps network interface.
We fix the tree height h=3, so the distance from the controller to every PSU is 2.
We first tune the parameters to fit the results in Figure 5&14, so that the RTT within a PLAN is 8ms and the latency across a point-to-point link is 3ms.
Then we run the simulations for different scale of the network (number of servers) from 125 (k=6) to 12167 (k=24).
Results: We create 1-to-1 and 1-to-N patterns from the controller using TCP connections, and measure the RTT and throughput per-server for different network scales.
The results are plotted in Figure 16.
For 1-to-1 connection from the controller to a server, we observe consistent throughput >328.64Mbps.
For 1-to-N pattern, we create connections from the controller to all servers, and see that the per server throughput quickly drops as more and more servers shares the out-going bandwidth of the controller.
At maximum network scale (12167), the perserver throughput is 29.8Kbps.
For latency, we observe that the average RTT is smaller than 40ms even when network scales beyond 10 5 servers.
This is as expected as the overall distance is only 2.
We use availability to characterize the system reliability of PowerMan, which is the percentage of reachable servers.
The key component in PowerMan PDU and PSU is the PLC modem, so we model availability of the entire system at the resolution of an PLC modem.
We use a Poisson process [27] to characterize the failure process of is the key metric in this model, and it is a common measure of reliability of a hardware component [27,53].
A higher MTBF means the component is more reliable.
Our PLC testbed and prototype have been running for 2 months without failure (1440 hours); using this as reference, we vary the MTBF of PLC modem from 500 to 9000 hours.
The MBTF of packet forwarding gateway is assumed to be 3000 hours [37].
We implement an event-driven failure simulation, modeling the entire network of PLC modems in PowerMan.
In each run, we vary the scale of network (by increasing k from 5 to 64, h=3), the MTBF of PLC modem (from 1000 to 9000), and simulate a year of PowerMan operation with the failure model describe above.
We plot the average availability of PowerMan in Figure 17.
We observe that PowerMan is highly available at large scale: For k=64 (network scale is 250K), the availability is 99.9943% (using the least reliable modem with MBTF=500hrs).
High availability provides consistent global visibility to management applications, allowing them to perform monitoring and diagnostic tasks.In Figure 17, we also plot the availability of a PowerMan in a DCPS with Dual Bus redundancy as shown in Figure 10.
In this setting, we have PLC networks replicated in both sides, and the controller attaches to the roots of both trees.
We can see that, by integrating with the redundant power systems, PowerMan can achieve higher availability for varying network scales.Since PowerMan is embedded in DCPS, servicing/replacing components is similar to that in a typical DCPS.
Tier II-IV DCPSs are designed with redundancy ( §4.4), so when parts of the system fail, the operations can continue, as back-up units will take over.
In the meantime, faulty components can be repaired/replaced.
PowerMan adopts the same recovery strategy.
Next we compare the construction, equipment, and operational costs of PowerMan and other related designs that can be used as out-of-band MNs.
The comparison is done at the same scale of 16000 servers.
We compare with these proposals for datacenters: 3D-Beamforming (3DBF) [84], Firefly [48], Diamond [36], and FatTree [19].
We emphasize that this is not a direct comparison: these designs are complete datacenter networks with both data plane and in-band control plane, and we [41,47]) have various requirements on the datacenter interior designs.
For example, reflective surfaces (static [36,48,84] or mechanically controlled [41]) must be installed for connectivity.
In addition,3DBF has ceiling height requirements [84], which may incur room modifications in deployment.
Furthermore, Diamond also requires the spacing between racks.
This limits the number of racks per room, and Diamond deployments may need more rooms to hold the same number of servers.In contrast, PowerMan leverages the wiring in existing DCPS to achieve scalable connectivity for MN, and only replaces the PDUs and PSUs in the DCPS.
Thus, constructing PowerMan should incur no cost in modifications of room design or floor plan, which greatly reduces the cost of deployment compared to the other proposals.
Equipment cost: Next, we compare the equipment costs in Table 2, and we explain the assumptions as follows.
For PowerMan, we assume PSU uses Design 1, which incurs no cable cost.
The tree topology of PowerMan is configured as h=3,k=27.
Each PLC modem is $30 5 .
Each Gateway is $500.
For other designs, we consider the cost of NICs on the server, switches, wireless radios and cables.
We adopt the conservative estimates in [36], and make the following cost assumptions: each wireless radio component costs $60 [85], each 40-port switch costs $1040, each NIC port costs $5 [46], each FSO device port costs $150 [48], and an average cost of $1 per meter for cabling [48] and $1 per square meter of absorbing paper.
We assume the reflectors used [36,48,84] have negligible cost as equipments.Overall, PowerMan can be constructed with 1/2∼1/3 of the cost of other proposals at the same scale, confirming PowerMan as a cost-effective option for MN.
Power consumption: Power consumption is an important component of operational cost.
In Figure 18, we compare the operational power consumption of different designs.
We assume each NIC consumes 5 Watts (W) [36], each PLC modem 3.73W ( §3.1), each switch 170W [36], and each gateway 300W 6 .
For wireless de- 5 We use retail price here.
The per-unit price are dependent on many factors: quantity, availability, distance, etc.
With large quantity, the price tend to decrease.
6 We use a rack-mounted server as the packet-forwarding hardware [48] and the WiFi module in Diamond 60W [36].
In general, PowerMan consumes much lower power at the same scale, using 6%∼9% power of other designs.
This is because PowerMan is mostly composed of PLC modems with lower power usage.
In this section, we discuss the limitations of PowerMan and experiences from constructing and operating the PLC testbed and PowerMan prototype.
Interference in DCPS: The major contributor to the loss of performance ( §3.2) is high-frequency noise from sources of interference in DCPS, including lighting, cooling, mechanical system, etc.
As pointed out in [60], PLC is an extremely harsh environment for the highbandwidth, high-frequency communication signals, as critical channel parameters (e.g. noise, impedance, and attenuation) are highly unpredictable [69].
Given the severity of the interference, in the design of PowerMan, we aim to limit the PLC signal within each PDU (i.e. within a PLC LAN (PLAN)) which reduces the signals exposure to interference, as the signal only travels short distances within the PDU.
The LPF in each layer also removes high frequency noises from non-PLC sources.
Low Throughput Alternatives to PLC: Our experiments and simulations (6) exhibit low throughput for various traffic patterns, and as we have discussed, the reasons include noise, signal attenuation, MAC layer overhead, etc.
Due to the low throughput of PowerMan, it is natural to consider using low cost, low bandwidth WiFi or Ethernet devices as alternatives.
Compared to low cost alternatives, we believe PLC is advantageous in the three goals we outlined for an out-of-band MN ( §1): survivability, deployability, and scalability.
• Survivability: PLC can leverage the robustness in existing power systems.
Power system is the last-to-fail system in datacenters, and is independent of the data plane network.
Embedded in DCPS, PowerMan can survive data plane failures, or even power system failures in Tier-II to IV datacenters, and is ready for immediate diagnosis and recovery.
Other low cost alternatives do not share this quality.
For example, a separate WiFi network requires additional monitoring and management systems to achieve the same level of robustness of power system.in the prototype, thus the high power usage.
This can be reduced with a typical packet-forwarding device.
• Deployability: PowerMan reuses wiring in existing DCPS, thus there is no need to change ceiling height, and rack dimensions.
This compatibility with existing datacenter designs greatly reduces the deployment cost.
In contrast, WiFi-based solutions require changes in the rack dimensions to accommodate antennae of servers and access points.
Ethernet-based solutions require additional rack space and cabling.
• Scalability: PowerMan reaches every device in the datacenter, as it reuses wiring in DCPS.
Ethernetbased solutions with the same topology as PowerMan can reach the same port count, but at the cost of much more cabling.
Like PLC, WiFi also suffers from the interference, which is more difficult to manage than that in a wired network.
PowerMan is able to use signal filters on the border of two PLANs to eliminate interference between them.
Such is not so easy in a wireless network, as there is no clear border between two broadcast domains.
For WiFi-based solutions, handling interference requires careful planning of antennae direction, AP radio power, location, and channel selection.
At the scale of a modern datacenter, the management of WiFi-based solution is challenging.
DC datacenters: Many modern datacenters are using DC power [31,55,68].
Our design can also work on such DCPS, because PowerMan is a design that utilizes power lines, which is the same in both DC and AC power systems.
The carrier frequencies in PLC devices (assuming compliance with HomePlug standards) come from OFDM circuitries, and are not the 50-60Hz AC power.
Security Concerns: Datacenter MN is a high value target, and a MN using PLC may be vulnerable to onpremise attacks.
PowerMan can adopt security mechanisms on MAC, network, transport, application layers.
For example, in the PLC MAC layer, HomePlug 1.0 [56] supports 56-bit DES encryption, and later versions (HomePlug AV/AV2 [24,81]) support 128-bit AES.
Cooling: Even with intensive experiments on PowerMan prototype, we have not yet witnessed any overheating issues for PLC modems.
This is because: 1) the PLC modems have low power profile, and 2) the PLC modems are placed outside of the servers.
Bump-in-theWire PSU design may benefit from the same reasons; but it is still important to investigate the heat dissipation of the Full-Integration design inside a rack-mounted server or switch as future work.
We summarize the related work in three broad categories: datacenter management, alternative datacenter networking architectures, and PLC networking.
Datacenter Management: There is vast literature on the management and control planes of datacenter networks [20,42,49,52,70,85].
They often assume that the management traffic can be delivered, and PowerMan complements these works with an out-of-band MN that offers necessary latency and bandwidth, while being survivable, scalable, and deployable.
Datacenter Networking Architectures: Datacenter networks in production usually use the Clos network [19,43,54,65,67] to achieve high bisection bandwidth.
Using flexible networking technology such as optical switching [32,34,38,57,59,64,80], FSO [41,48], and 60GHz wireless radios [36,47], dynamic network topologies are proposed to mitigate traffic hotspots and changing demands.
We differ from them in our technology choice.
In terms of datacenter MN, Angora [85] proposed using 60GHz wireless radio to construct a datacenter "facility network", which is a MN but with much stricter latency requirements.
In contrast, PowerMan is the first attempt to employ PLC in the datacenter MN setting, and as our cost comparisons ( §6.3) suggest, PowerMan has lower initial cost and operating power consumption than the other technologies at the same scale.
PLC Networking: In PLC PHY [60,69] and MAC [51,74,75,76,77,78] layer, many efforts have been made to improve the bandwidth, reliability, and latency [83].
In comparison, PowerMan focuses on the application of PLC in MN, exploring networking ( §3.1) and scalability ( §4.3) for datacenter management.
PowerMan can benefit from all PHY and MAC layer optimizations (e.g. parameter setting, dynamic bandwidth allocation scheme), as they improve the PLANs in PowerMan.
This paper has introduced PLC as an out-of-band management channel for datacenters.
We build a smallscale PLC testbed, and demonstrate the potential of PLC with deployment of actual management applications.
In the process, we identified the wiring and scalability issues which prevent deployment of PLC in datacenters.To tackle these problems, we design PowerMan, a datacenter MN using PLC that can be implemented using commercially available PLC devices.
We build a PowerMan prototype on a small testbed of 12 servers.
Using experiments and large-scale simulations, we evaluate its performance, reliability, and cost-effectiveness.
For future work, we plan to 1) investigate custom PLC devices with optimized PHY/MAC layers to improve latency, throughput, scalability, and reliability; 2) integrate PSU with single-board computer, so as to provide isolation from local OS-related failures.
ber of contending flows at the controller from N (total number of servers) to k−1 (number of nodes in a PLAN).
This can be observed in Figure 22, which summarizes the tail latencies (99th percentile FCTs).
Tail latencies capture the worst performing flows whose completion times are prolonged by events such as packet loss, reordering, frame collision.
Using LA to reduce the number of contending flows at the receiver decreases the occurrences of tail latency events, which improves the completion times.
Finally, a further optimization is to compress local information before sending.
Compression of locally collected information can reduce total traffic volume, and it would be beneficial if the computation overhead on the gateway is acceptable.
sAs discussed in §3.2.2, two common traffic patterns of management application is 1-to-N (e.g. configuration tasks) and N-to-1 (e.g. monitoring tasks).
The experiments in §3.2&6.1 shows that such patterns perform poorly on PowerMan.
In the following, we propose application-layer traffic optimizations to reduce the completion times of these two patterns on PowerMan.
We assume the controller is located at the root of the tree.
Accelerating 1-To-N PatternAs shown in Figure 16, PowerMan has low per-server bandwidth at large scale.
This is because the controller node needs to maintain connection to all servers, so the per-server bandwidth is constrained by the interface capacity of the controller.
Low per-server bandwidth can prolong completion times of configuration tasks.
Figure 19: Accelerating Management Application Traffic PatternsWe propose to construct an application-layer multicast (ALM) overlay network [26] in PowerMan for management tasks with 1-to-N distribution pattern, where the gateway in each PDU act as a distribution agent in corresponding PLAN.
As shown in Figure 19, the distribution from controller to all server is divided into multiple distributions within different PLANs.We evaluate the performance of ALM.
We use the production traces in §3.2.2.
For baseline performance, we create 1-to-N traffic patterns using the traffic generator with different numbers of receivers.
For ALM, we modify the traffic generator to include an implementation of ALM agent, and enable the agents in the gateways.
We collect the FCTs and the results are plotted in Figure 20.
We observe that ALM reduces the FCT for 1-to-N pattern, and the performance gap increases with the number of total receivers.
For 10 receivers, the FCT is reduced by 15.38% on average.
The main reason is that the total traffic volume is reduced with ALM, as copies of the flow are created by the agent in each gateway, which reduces the traffic volume at higher layers.
Accelerating N-to-1 PatternInformation collection tasks include monitoring, diagnostics, and measurement, which exhibit N-to-1 pattern We then evaluate the performance of LA.
For baseline, we create N-to-1 patterns using the same flow size distributions as above.
For LA, we implemented a LA agent for the traffic generator, and enable them on all gateways.
We collect the FCTs and plot the average in Figure 21 with respect to the number of senders.
We can see that, although LA in general outperforms baseline, the performance gap is smaller than that of Figure 20.
This is because the total traffic volume is not reduced with LA.
However, LA on PowerMan effectively reduces the num- As discussed in §3.2.2, two common traffic patterns of management application is 1-to-N (e.g. configuration tasks) and N-to-1 (e.g. monitoring tasks).
The experiments in §3.2&6.1 shows that such patterns perform poorly on PowerMan.
In the following, we propose application-layer traffic optimizations to reduce the completion times of these two patterns on PowerMan.
We assume the controller is located at the root of the tree.
As shown in Figure 16, PowerMan has low per-server bandwidth at large scale.
This is because the controller node needs to maintain connection to all servers, so the per-server bandwidth is constrained by the interface capacity of the controller.
Low per-server bandwidth can prolong completion times of configuration tasks.
We propose to construct an application-layer multicast (ALM) overlay network [26] in PowerMan for management tasks with 1-to-N distribution pattern, where the gateway in each PDU act as a distribution agent in corresponding PLAN.
As shown in Figure 19, the distribution from controller to all server is divided into multiple distributions within different PLANs.We evaluate the performance of ALM.
We use the production traces in §3.2.2.
For baseline performance, we create 1-to-N traffic patterns using the traffic generator with different numbers of receivers.
For ALM, we modify the traffic generator to include an implementation of ALM agent, and enable the agents in the gateways.
We collect the FCTs and the results are plotted in Figure 20.
We observe that ALM reduces the FCT for 1-to-N pattern, and the performance gap increases with the number of total receivers.
For 10 receivers, the FCT is reduced by 15.38% on average.
The main reason is that the total traffic volume is reduced with ALM, as copies of the flow are created by the agent in each gateway, which reduces the traffic volume at higher layers.
Information collection tasks include monitoring, diagnostics, and measurement, which exhibit N-to-1 pattern We then evaluate the performance of LA.
For baseline, we create N-to-1 patterns using the same flow size distributions as above.
For LA, we implemented a LA agent for the traffic generator, and enable them on all gateways.
We collect the FCTs and plot the average in Figure 21 with respect to the number of senders.
We can see that, although LA in general outperforms baseline, the performance gap is smaller than that of Figure 20.
This is because the total traffic volume is not reduced with LA.
However, LA on PowerMan effectively reduces the num-
