While distributed denial-of-service (DDoS) attacks become stealthier and more disruptive, real-world network operators often ignore academic DDoS defense research and instead rely on basic defense techniques that cannot adequately defend them.
In fact, prior to the deployment of a DDoS defense solution, a network operator must understand its impact specifically on their network.
However, without a sound empirical analysis of the solution, which is often the case even for the most cited academic work, the network operator may fear its poor defense efficacy or its adverse effects on legitimate traffic.
In this work, we elaborate on the critical missing gaps in DDoS defense evaluation and propose a new evaluation platform to help produce the missing defense analytics.
To identify the impact of a defense solution in realistic network settings, our platform offers to emulate a mini Internet topol-ogy with realistic IP address space allocation and generate representational, closed-loop background traffic specific to particular networks.
As such, our platform fulfills the prominent missing gaps in current DDoS research.
In the end, we conduct some experiments to demonstrate the correctness and efficiency of our platform.
Advanced distributed denial-of-service (DDoS) attacks, such as the CrossFire attack [14] and CICADAS [15], seriously challenge the efficacy of the rudimentary DDoS defense strategies typically deployed by network operators.
In particular, the most commonly deployed DDoS defense systems consist of two main components: a simple threshold-based DDoS detection/classification system, such as FastNetMon [9], and a coarsely grained mitigation solution such as Remotely Triggered Black Hole (RTBH) [18].
Unfortunately, these simple defense strategies struggle against the aforementioned, advanced DDoS attacks.
A threshold-based detection solution rarely finds an appropriate balance between false positive and false negative rates, and a coarsely grained mitigation solution, by definition, filters legitimate traffic.Despite over two decades of research to improve DDoS defense, network operators continue to choose these basic DDoS defense strategies regardless of their limitations.
To draw insights into this discrepancy, Kokulu et al. recently surveyed security operation centers and found that network operators must obtain a thorough quantitative analysis of any security solution prior to deployment [17].
Without a comprehensive performance test of a potential security system in an environment similar to the network in which the system will deploy, the network operator cannot justify the risk associated with the adoption of new research defense systems.
Clearly, to increase real-world DDoS defense deployment, the research community should conduct rigorous defense evaluation of DDoS defense solutions.
However, by surveying well-received DDoS defense papers, we found that even highly-cited solutions frequently fail to evaluate their approach under realistic deployment scenarios.
We highlight a few prominent missing gaps in evaluating DDoS defense solutions below.Lack of insights into advanced attacks in action.
While researchers continue to discover advanced DDoS attacks [14,15,31,32], researchers rarely have the opportunity to study these attacks in action.
To better defend against real-world attacks that leverage advanced attack mechanisms [6,10], researchers should be able to run and study these attacks in a high-fidelity network environment.
Preparing such a network environment is not a trivial task.
For example, to correctly study pulsing attacks, such as CICADAS [15], that exploit TCP congestion control, the network must provide a traffic generator [30] to provide realistic background traffic that honors network condition changes (e.g., congestion control).
Lack of closed-loop networks for DDoS detection.
As the most valuable outcome of a DDoS detection solution is to facilitate DDoS mitigation, we must evaluate the collaboration of different detection and mitigation solutions.
In other words, rather than evaluate DDoS detection and mitigation separately, we need to keep all DDoS defense components in the loop to evaluate each component individually and the entire defense as a whole, thus a closed-loop evaluation.
For example, a detection solution must work with abrupt network changes caused by the mitigation effort.
Indeed, Vishwanath et al. [36] found many network systems, including DDoS detection solutions, can present biased conclusions if evaluated under non-closed-loop networks.Lack of collateral damage analysis in filter-based mitigation.
Despite the network community is slowly adopting fine-grained mitigation solutions for disseminating filtering rules (e.g., BGP Flowspec [24]), the limited hardware filter space [7,12] is insufficient in mitigating DDoS attacks that do not contain common packet signatures [13,32].
Network operators often use limited filters to mitigate attacks but at the cost of disabling access to non-attacking networks.
Soldo et al. [34] proposed a set of filter generation methods to study the trade-off between limited filters and collateral damage.
However, the performance of such these methods is highly dependent on the IP address locality.
Therefore, we must provide an emulation environment with realistic IP addresses, only then can we evaluate such DDoS mitigation solutions with confidence.To facilitate sound empirical evaluation of DDoS defense solutions, we propose, design, and develop an emulation platform for evaluating DDoS defense solutions with high fidelity.
Referred to as the DDoS SandBox (or simply, the SandBox), it has the following capabilities:1.
Generation of inferred Internet topologies at the level of autonomous systems (ASes); 2.
Assignment of realistic IP address spaces to ASes; 3.
Routing and packet forwarding functions of each AS; 4.
Packet-level mimicry of real network traffic that honors network condition changes (e.g., congestion); and 5.
A simple usage model with high experiment portability.
Dummynet [5] and Modelnet [35] are two early network emulation systems that support unmodified applications.
They have fixed components that render the discussed missing gaps above difficult to close.
For example, Modelnet employs Click [16], a software router, to configure and route traffic in the emulation network.
This fixed design choice imposes burdens in deploying different routing implementations, which makes BGP-related DDoS defense techniques [1,26,28] difficult to evaluate.
Later emulation platforms, such as described in [2,3,19,27,37], unanimously adopted Linux namespaces [22] to provide each process its own abstracted and isolated system resources with a single system kernel.
For example, one type of Linux namespace, the Linux network namespace [21], can assign a unique network stack to each process, which allows the process to have its own routing table.
Thus, network emulation tools can easily spawn "hosts" by assigning processes different network namespaces.
Notably, mininet [19], a popular network emulation tool, also relies on Linux namespaces.
Mininet then utilizes cgroup [20] to partition system resources to each process (e.g., set a maximum CPU utilization for a host -a process in its own namespaces).
A more recent work, Containernet [27], extends mininet to support Docker.
It encourages users to create self-containing software images to mitigate deployment issues.A namespace-based emulation system offers low overhead when compared to virtual machines (VM) and many VMbased testbeds; the former emulates everything with a single operating system (OS) kernel while the latter emulates OS kernels but could incur additional interface translation and storage overheads.
Our SandBox leverages Containernet to realize its capabilities.
Specifically, the Docker support in Containernet allows us to create self-contained software images quickly, and its underlying mininet programming interfaces enable us to program the links between emulation nodes.
These features create a solid foundation for us to close the missing gaps in a foreseeable amount of time.
Using DDoS SandBox.
The DDoS SandBox facilitates network operators to evaluate a defense solution in an emulation environment that mimics a real network.
Meanwhile, DDoS researchers can also utilize the SandBox to gain insights into different DDoS attacks and defense solutions.
Emulating a distributed system with a wide range of applications is a challenging task.
Experimenters often run into issues such as fixing software dependencies or configuring network routers manually in an experiment.
Thus, in the SandBox, we reduce the deployment friction by automating as many components as possible.
In the meantime, the system remains fully customizable.
For example, users can drop into a shell terminal of an arbitrary router node, and add or remove its network interfaces as they wish.A mini Internet.
Both advanced attacks and defense research projects require the emulation system to provide support for basic network functions (e.g., correct router ICMP behavior for traceroute).
Ideally, the system should provide a mini Internet that is functionally equivalent to the real Internet.
Only then can we study the latest DDoS attacks and evaluate defense solutions in a closed-loop environment.
This includes but is not limited to 1) realistic AS-level IP space assignments based on the real-world BGP announcements, 2) BGP routing, and 3) closed-loop background traffic that reacts to the network conditions in real-time.
Elastic emulation fidelity.
Today, anyone can have easy access to bare-metal servers with 100-core processors and 100s GBytes of memory from major cloud providers [33].
The modern hardware can easily emulate a small to medium network at high fidelity.
However, emulation fidelity is not only about scalability.
An experiment may require specific hardware for emulation (e.g., using a programmable switch for DDoS detection in the data plane).
The emulation system must also consider supporting additional physical hardware, and as such, we can both offload the emulation load and increase the overall emulation fidelity.
We introduce the system inputs and main SandBox components at a high level, as shown in Fig. 1 [4], and traffic traces (e.g., sFlow or a pcap trace, of a network).
Typically, a user will input a benign traffic trace to generate the AS-level topology and background traffic in which the user can later inject attack traffic using the DDoS repository of attacks (which we describe further in the Node Images paragraph).
However, the user can also leverage attack traces to study a specific instance of an attack.
We reduce the input effort from users by automating data processing tasks for the public datasets.
Additionally, users can feed network and experiment specifications to increase the emulation fidelity.
For example, network operators can specify their intra-AS topologies and their upstream AS's partial intra-AS topologies to evaluate a defense solution of choice.
We also see that the residential IP sends 50 KB of data to Amazon in the first second, then 10 KB of data in the next second.
In return, Amazon.com sends 500 KB and then 1 MB of data back in two seconds.
Finally, each traffic mimicry agent creates corresponding network sockets and communicates with the destination specified in each flow snapshot.
We choose Containernet as the PoC driver to implement the blueprint produced by Topology Generator.
Specifically, we use Containernet to instantiate node images in Docker (e.g., the reference router image) and link instantiated nodes.
The implementation also allows us to pass hardware network interfaces to a router node to achieve high-performance traffic control (TC) [23,29] support.
Goals.
To conduct a preliminary evaluation of our PoC, we first validate the correctness of Topology Generator via traceroute tests, and we then evaluate the single-host system scalability by analyzing network instantiation time with respect to the number of routers in the network.
We plan on testing other aspects of the DDoS SandBox, such as the Traffic Mimicker, in our future work.Setup.
To create the AS topology, we feed a sampled sFlow trace from an IXP in the United States as our only private information input and two public information datasets (shown in Fig. 1 Fig. 2.
This traceroute (without TC policies) shows the route packets take between a randomly selected educational network and an arbitrary IP at a major cloud provider.
The packets generated at the educational network flow through its upstream AS and Internet2 to reach the cloud IP.
This reflects the real-world Internet AS topology, and we can find a corresponding ASlevel path on bgpview.io.
Furthermore, note that the SandBox provides a realistic IP assignment to each AS based on its real-world prefix ownership.
Network Instantiation Time.
We benchmark the scalability of the DDoS SandBox by measuring the time it takes to: 1) create the reference router nodes, 2) populate configuration files for each node, 3) set up veth devices for each node, and 4) allocate IP addresses to each node.
The main factor that affects network instantiation time is the number of nodes (or routers in this case) that need to be created in the SandBox.
Note, there are many factors other than the number of nodes that can affect the system instantiation time, albeit to a smaller degree -for example, if an AS has a high degree of neighboring connections, the system may spend more time in creating veth devices and generating Quagga routing configuration files.
Fig. 3 shows the network instantiation The two machines spend a similar amount of time to instantiate networks that have a limited amount of routers (i.e., less than 120).
In fact, due to its high processor frequency, the 3-core Hyper-V VM performs slightly faster than the 96-core Amazon EC2 VM.
However, for networks with more than 120 routers, the instantiation time for the Hyper-V machine increases exponentially due to its limited memory space, whereas the Amazon EC2 VM continues to follow a linear trend in instantiation time.
Fig. 3 clearly shows that the SandBox can instantiate relatively large-scale networks within a relatively short amount of time.
The DDoS SandBox is an ongoing project that is under active development.
We are integrating more reference node images into the SandBox, including Traffic Mimicker, and reducing the required efforts to include physical devices.
We plan to experiment with the scalability of the SandBox across multiple servers and investigate different approaches to extrapolate experiment results to the Internet scale.
We are investigating alternatives to Containernet such as Container Network Interface (CNI) plugins [8] that have better support and system compatibility in the long run.
Our ultimate goal for the DDoS SandBox is to run sound empirical DDoS attack and defense experiments.
We hope results derived from the SandBox can draw more attention from network operators, and pave the way for the deployment of various defense systems.
We will first implement the advanced DDoS attacks and well-received defense solutions, and provide them as reference node images in the SandBox for users to run general DDoS experiments.
To the best of our knowledge, this is the first attempt to bridge the missing gaps for sound empirical DDoS experiments, and we have shown initial success in bridging those gaps in our PoC.
Our PoC system is open source, and for more design details, the code is available on: https://github.com/DDoS-SandBox.
