Web applications suffer from poor reliability.
Practitioners commonly rely on fast failure detection to recover their applications quickly to reduce the effects of the failures on other users.
In this paper, we present a technique for detecting user-visible failures by analyzing Web logs.
Our technique applies a first-order Markov model to infer anomalous browsing behavior discovered in Web logs as indicators that users have encountered failures.
We implemented our technique in a tool called REBA (REcursive Byesian Analysis of Web Logs).
We evaluated our technique using REBA applied to the Web site of NASA.
The results demonstrate that our technique can detect user-visible failures with reasonable cost.
Web applications suffer from poor reliability.
Practitioners commonly rely on fast detection of user-visible failures (i.e., failures encountered by users but not detected by in-house testing), and expect to recover their applications quickly to reduce the effects of the failures on other users.
Research shows that early detection could mitigate about 65% of failures [13].
However, fast failure detection in Web applications is challenging-it can require as much as 75% of failure recovery time [5].
One possible way for practitioners to quickly detect failures is to seek users' feedback.
However, most users are reluctant to give feedback.
Moreover, even if users do provide feedback, it may not be useful for detecting failures.
Our industrial collaborator, Clinical Guard, an online medical device supplier, reported that in three months of 2008, their application operators sent enquiries to 200 customers whose browsing histories showed anomalous browsing paths, which operators believed might be generated because these customers encountered failures.
The operators received only nine responses.
Of these, only one identified the failure and provided information that was helpful in detecting the failure.
The remaining eight users just mentioned that they encountered failures when they tried to buy products through the Web site.Previous research has addressed failure detection in Web applications.
One approach concentrates on performance diagnosis by detecting failures of network services [2,7].
Many failures identified by these techniques downgrade the performance but do not affect the functionality of a Web application.
Another approach investigates detecting failures at the application level [6,11].
These techniques trace runtime execution paths that consist of components invoked in response to user's requests, and applies statistical analysis to infer whether failures occurred with respect to anomalous interactions of these components.The techniques can successfully detect application level failures, but may not be effective for detecting failures caused by software bugs [11].
A third approach analyzes users' browsing behavior to detect user-visible failures [3].
The technique reported in Reference [3] assumes that users refresh their browsers when they encounter failures.
The technique detects abrupt changes in the hit rates of a Web page and notifies human operators to perform further checking.
The technique can detect failures caused by software bugs for which users respond with refreshing browsers.
However, the technique could miss failures if the number of refreshing responded for failures is not enough to reach the threshold of abrupt (e.g., users respond with other behavior than refreshing for failures).
To address the limitations of these existing techniques, we developed a technique for detecting user-visible failures in Web applications.
Our technique assumes that the HCI rational principle holds (i.e., users must respond if the output of a sequence of interactions with software is unsatisfactory) [4].
Web users normally browse Web pages following certain browsing patterns [10,12,14].
Users' reaction to failures may break these browsing patterns and thus, has a low likelihood of occurrence.
Our technique applies a first-order Markov model to model users' browsing behavior, uses the data in Web logs to train the Markov model, and applies the trained model to infer unusual browsing paths (i.e., sequences of requests from users with low probability of occurrence).
If an unusual browsing path is detected, our technique reports that the user has encountered a failure.We implemented our technique in a tool called REBA (REcursive Byesian Analysis of Web Logs), and evaluated the technique using REBA applied to NASA's Web site.
We built the Markov model using user-session data retrieved from a one-month Web log.
We used Bayesian inference to estimate the parameters of the model: the user-session data from the first day were used in setting the prior distribution to estimate the parameters, the user-session data from the second to the tenth days were used as training data to estimate the parameters, and the user-session data for the remaining days were used to test the model.
Our study shows that our technique detects approximately 71% of the failures with a 26% false-positive rate.
We analyzed the results using the measures of ROC (Receiver Operating Characteristic) curve [9], a broadly-used graphical plot for evaluating and visualizing the performance of statistical models, and the analysis indicated that our model can detect failures with reasonable cost.
• A new technique for detecting user-visible failures in Web applications that builds a probabilistic model of user-browsing behavior• A prototype tool, REBA, that implements our technique• The results of an empirical study on a real-world application that demonstrates the effectiveness of our technique In this section, we discuss the assumptions of our technique (Section 2.1), present our model (Section 2.2), describe the way of using the model to detect failures (Sections 2.3), and discuss important features of our technique (Section 2.4).
Our technique assumes that HCI rational principle holds that a user will respond when the output of a sequence of interactions with the software is not satisfactory [4].
Research has shown that users follow browsing patterns to use a Web site [10,12,14].
We assume that when users encounter failures, their reactions to the failures often break browsing patterns and the probability of the occurrence of such response is low.
Thus, if the occurrence probability of several successive requests is low, the user making these requests may have encountered failures.
A Markov model describes a system consisting of a set of N distinct states [15].
The system transits from the source state to the sink state.
A Markov model can be represented by a directed graph whose nodes represent states and edges represent transitions (i.e., the start node of an edge represents the source state and the end node represents the sink state).
A Markov model transits from a source state to a sink state according to a conditional probability, which is called the transition probability.
If the transition probability to the next state is conditionally dependent on only the current state, the model is at the first-order.
Our technique builds a first-order Markov model to analyze user browsing behavior.
The technique employs user-session data retrieved from Web logs to train this model, and then uses the model to compute the occurrence probabilities of sequences of user requests.
If a sequence's probability is lower than a threshold value, the sequence is reported as one in which a user encountered a failure.In our Markov model, a state represents a Web page, and a transition represents navigation from one page to another.
The direction of a transition represents the direction of the navigation.
For example, if a user first visits page A and then visits page B, the corresponding transition in our model is from the state representing A to the state representing B.
If a user refreshes page A, this refreshing operation corresponds to the self-loop transition from A's state back to itself.States and transitions of our model have associated occurrence numbers.
A state s has a state occurrence number, denoted by Occ(s), which is the number of times that s was visited; a transition s i s j has a transition occurrence number, denoted by Occ(s i s j ), which is the number of times that the system made the transition s i s j .
Our technique uses a trained Markov model to estimate transition probabilities of a sequence of transitions to infer anomalous browsing paths, which are viewed as indicators that the user has encountered failures.In our model, there are two types of transition probabilities: outgoing and incoming.
Given a transition s i s j , the outgoing transition probability (OTP) is the probability that from state s i the system will transit to state s j ; the incoming transition probability (ITP) is the probability that when the system is in s j , the system transited to s j directly from s i .
For example, given the Home page H and another Web page A, the OTP of the transition from H to A is the probability that a user navigates from the Home page to page A; while the ITP is the probability that when a user is browsing page A, the user navigated to the page directly from the home page.
OPT and ITP are usually different.
For example, users may go from any Web page to the Home page, but not vise versa.To check whether a user encountered a failure in a specific request, our technique checks the outgoing occurrence probability and the incoming occurrence probability of a fixed number 1 + w of subsequent requests (i.e., this request and its w successive requests).
Given a sequence T of requests and a trained Markov model M , the outgoing occurrence probability (OOP) is the occurrence probability of T computed using OTPs, whereas the incoming occurrence probability (IOP) is computed using ITPs.
If the minimum of OOP and IOP of the subsequence of requests is less than a threshold, our technique views T as an anomalous browsing path and reports that the user encountered failures.
For example, given a user session, if we want to check whether a user encountered failures in the fourth request, the model estimates the OOP and the IOP of the sequence of transitions from the 4th request to the (4 + w)th request, where w is a predefined value.
If the minimum of OOP and IOP of this sequence of transitions is less than a specific threshold, the technique reports that the user encountered failures in the 4th request.
We discuss three important features of our technique.Choice of the model.
A first-order Markov model may not perfectly model a user's browsing behavior, because the latter may not satisfy the first order Markov property.
The next page a user visits may depend not only on the current page but also on earlier pages.
Although the dependence could be captured by a higher order Markov model, it would entail additional complexity and computational cost.
Our empirical results (see Section 4) suggest that a first-order model is adequate for revealing many failures in Web pages.Computing OOP and IOP.
We use Bayesian estimation to compute OTP and ITP.
We set the prior to compute the estimator.
The way of setting prior ensures that OOP and IOP of a sequence of requests are not dependent on a specific page.
For example, given two pages that have 1000 links and 1 link, respectively, the probabilities of visiting the page with more links are not necessarily lower than the page has less links.
Rather, the probabilities are dependent on the number of links and the frequencies of these links are visited (See Section 3.3 for more details).
The threshold value.
Our model raises an alarm when the occurrence probability of a sequence of requests is lower than a threshold value.
The specific value of the threshold depends on applications and the loss functions.
Suppose the practitioner chooses the same loss function for detection rate and false positive rate, the threshold value can be determined by the output of the model training.Limitations of our technique.
Our technique does not handle two types of user-browsing behavior.
First, users sometimes immediately leave a Web site when they find a failure in their first request.
In this case, it is difficult for our technique to distinguish whether the user left the site due to a failure.
Second, a user may not take any unusual actions after encountering a failure because the user did not notice or care about it.
In this case also, our technique cannot detect the failure.
Our technique employs Bayesian inference to estimate parameters of the Markov model (i.e., outgoing transition probabilities and incoming transition probabilities).
In this section, we introduce notations (Section 3.1), present estimating transition probabilities (Section 3.2), discuss setting parameters of the prior distribution (Section 3.3), and describe updating the model (Section 3.4).
Without loss of generality, we consider in detail only estimation of outgoing transition probabilities.
The treatment of incoming transition probabilities is similar and is omitted.
Let D = (u 1 , ..., u n ) be a set of user sessions.
For a user session u q , let k q i = (k q i1 , ..., k q im ) be the occurrence numbers of transitions leaving from s i to m states, Our technique uses Bayesian inference to learn a first-order Markov model.
Bayesian inference views parameters of a model as random variables and infers the distributions of these random variables by following the process of Bayes Theorem.
To estimate a parameter, a common Bayesian method is to (1) assume a distribution of the parameter, (2) set a conjugate prior distribution (i.e., a distribution in the same distribution family as the assumed distribution), (3) update the conjugate prior using the training data, where the updated prior is the posterior distribution, and (4) estimate the parameter using the posterior distribution.
One common estimator used in Beyesian estimation is the posterior mean, which is the mean of the estimated posterior distribution.
Given a Markov model with m states, according to Bayes' Theorem, we haveP [θ i |D] = P [D|θ i ]P [θ i ] P [D](1)where and q = (q 1 , ..., q m ) as its conjugate prior, where q j is the prior estimate of the forward transition probability of s i s j after s i is visited α times.
The distribution function of a random vector θ = (θ 1 , ..., θ m ) governed by a Dirichlet distribution with parameters α and q = (q 1 , ..., q m ) isDir α,q (θ) = 1 Z m ∏ i=1 θ αqi−1 i(2)whereθ i , q i , α ≥ 0, ∑ θ i = 1, ∑ q i = 1, and Z = ∏ m i=1 Γ(αqi) Γ(α)is a constant normalization factor.Thus, according to Equation 2, we have the priorP [θ i ] = 1 Z m ∏ j=1 θ αqj −1 ij(3)Now, given the probability structure θ i = (θ i1 , ..., θ im ), we assume the independence of the training user sessions.
This assumption is reasonable because two user sessions record two different browsing histories, which, particularly generated by two different users, are independent.
Thus, we have the likelihood of selecting the data set D asP [D|θ i ] = P [(u 1 , ..., u n )|θ i ] = P [u 1 |θ i ]...P [u n |θ i ](4)We also have the likelihood of collecting user session u q , in which transition s i s j occurred k ij q times, regarding to θ ij .
As shown in Equation 5, the likelihood is the marginal probability of random variable θ ij , which views other variables in θ i as 0.
P [u q |θ i ] = θ k q ij ij (5)Substitute Equation 4 using Equation 5, we haveP [D|θ i ] = P [u 1 |θ i ]...P [u n |θ i ] = m ∏ j=1 n ∏ q=1 θ k q ij ij = m ∏ j=1 θ ∑ n q=1 k q ij ij = m ∏ j=1 θ nij ij (6)Then substitute Equation 1 using Equations 3 and 6, we haveP [θ i |D] = ∏ m j=1 θ nij ij · 1 Z ∏ m j=1 θ αqj −1 ij P [D] = 1 Z · P [D] m ∏ j=1 θ (nij +αqj −1) ij (7)Now, we have Equation 7 as the posterior distribution.
We use the expectation of this distribution (i.e., the mean posterior) to estimate transition probabilities of the outgoing transitions of state s i .
P [θ i |D] = 1 Z ′ m ∏ j=1 θ (α ′ q ′ j −1) ijwhich represents a Dirichlet distribution with a parameter α ′ and a parameter vectorq ′ = (q ′ 1 , ..., q ′ m ), that is P [θ i |D] ∼ Dir α ′ q ′ (θ i )The expectation of a specific variable at j dimension of θ i is the mean posterior estimator of transition s i s j 's forward transition probability, denoted byˆθbyˆ byˆθ ij .
We havêhavê θ ij = E(P [θ ij |D]) = E(Dir α ′ q ′ (θ ij ) = q ′ j = n ij + αq j n i + α(8) Our technique uses the information extracted from the Web logs to estimate α and q i .
Let N be the sum of hits of all Web pages, and again let m be the number of states in the model.
For each transition s i s j , we let α = N .
In setting the parameters q ij of Equation 8, the prior outgoing transition probability of s i s j , we distinguish two cases.
The first case is that s j has incoming transitions.
We set each q ij to the normalized value of the average occurrence frequency of these transitions.
(Normalization is necessary so the q ij sum to one.)
The second case is that s j has no incoming transitions.
In this case, the soonest we could observe a transition to s j will be when the (N + 1)st page hit occurs.
We assume "optimistically" that the probability of this event is 1 N +1 , which would be the maximum likelihood estimate of s j 's frequency if the event did occur.
Without knowing the order of previous page hits, we also assume that the transition to s j is equally likely to come from any state.
Therefore we set each q ij to 1 (N +1)m divided by a normalization constant.
For I ≥ 0, let s 1 , ..., s I be the states from which transitions to s j were observed.
Equation 9 gives the value of q ij in each case:q ij = q ′ ij ∑ m j=1 q ′ ij(9)whereq ′ ij = { 1 I ∑ I k=1 Occ(s k sj )Occ(s k ) , I > 0;1 (N +1)·m , otherwise.
Our model is updated in two cases.
To evaluate the effectiveness of our technique, we implemented it and performed an empirical study to evaluate the effectiveness of our technique.
This section presents the study.
We introduce the experiment subject (Section 4.1), describe the empirical setup (Section 4.2), and present and analyze the results (Section 4.3).
We used one month of log data for the NASA Web site as the subject for our study.
The log data consists of 1,891,714 HTTP requests from real users.
After filtering trivial sessions that consisted of only one user request, we constructed two types of user sessions: non-error sessions, which contained no requests with error status code, and error sessions, which contained requests with error status code.Because the subject is a real-world application, we could not control the experimental environment and thus, did not know whether or not a user encountered a failure.
Our only information was the requests that were not successfully handled by the Web server-the errors that were recorded in the Web log.
Therefore, we used the error sessions retrieved from the Web log to evaluate the detection rate and used non-error sessions to evaluate the false-positive rate of our technique.
We implemented our technique in a tool called REBA (REcursive Byesian Analysis of Web Logs).
REBA consists of a preprocessing component implemented in Jython 2.2.1 and a detection-model component implemented in Java 1.6.
In our implementation of REBA, we set the size of the sliding windows to 4 (see Section 2.3 for details), which covers the state to be checked and its three successive states (i.e., w = 3).
Our observations indicate that after encountering a failure, many users make two or three more requests and then give up.
Setting w = 3 means that to check whether a user encountered failures in a specific request s i , our technique checks a sequence T of up to four requests including s i .
If a request is followed by fewer than three successive requests, the sequence T contains fewer than 4 requests.
For example, if s i has only one succeeding request (e.g., a user tried only one more request after encountering a failure) then T = (s i , s i+1 ).
We used the following process to perform the study using REBA:1.
Set the prior for estimating parameters.
We used the first day's non-error sessions, consisting of 572 user sessions, to set the prior for estimating parameters in our model.
2.
Train the model.
We trained our model with the non-error sessions from the second day to the tenth day, which consisted of 2,404 user sessions.
3.
Investigate the detection rate of the model.
We tested our model using all error sessions of the remaining days of the month, which consisted of 500 user sessions.
We checked the rate that these sessions are reported as error sessions.
4.
Investigate the false-positive rate of the model.
We tested our model using all non-error sessions of the remaining days of the month, which were 7,491 user sessions.
We checked the rate that these sessions were reported as error sessions.
5.
Update the model.
In both Step 3 and Step 4, if a testing session was not reported as encountering errors, the model was updated using this session as new training data.
In this section, we present the results of the study and analyze the results.
Table 2 shows several statistically important values of detection rates with false-positive rates.
In this table, the first column is the threshold value, where 1 ≤ w ≤ 3.
The second column is the detection rate, and the third column is the false-positive rate.
For example, the second column shows that the detection rate is 0.70 with a false-positive rate of 0.26.
Figure 1 shows the receiver operating characteristic (ROC) curve of the study results.
An ROC curve is a broadly-used graphical plot for evaluating and visualizing the performance of classifier models [9].
In the figure, the horizontal axis represents the false-positive rate, and the vertical axis represents the detection rate.
There are two important measures of an ROC curve.
One measure of an ROC curve is the area under the curve (AUC).
As a rough guide of how the AUC relate to the performance of a model, an area of 0.90-1 is excellent, 0.80-0.90 is good, 0.70-0.80 is fair, 0.60-0.70 is poor, and 0.50-0.60 is a failure.
(See details at http://gim.unmc.edu/dxtests/roc3.htm.)
The other measure of an ROC curve is the Equal Error Rate (EER), which is used as the decision boundary when detection and false-positive have the same loss function.
Geometrically, the EER is the intersection of the ROC curve and the diagonal from the top left to the bottom right of the graph.
In Figure 1, the bold curve is the ROC curve and the AUC of this ROC curve is approximately 0.83; the intercept point of the ROC curve and diagonal is the EER, and the EER is a detection rate of 0.71 with a falsepositive rate of 0.26.
The analysis of ROC indicates that our model can detect failures with reasonable cost.
We believe that the actual false-positive rate was lower than our empirical results.
We evaluated the falsepositive rate using non-error sessions whose status codes were not errors in Web logs.
These sessions had no HTTP errors.
However, user-visible failures caused by application failures, such as incorrect outputs or Web page display problems, are not recorded in Web logs.
Thus, there could be other types of failures than HTTP errors in these non-error sessions.
We expect that these non-error sessions contains failures and thus, the true false-positive rate is lower than our experiment results.Our empirical results may also under-estimate the detection rate.
The error session for evaluating detection rates were constructed based on the status code in Web logs.
However, there are cases that an error status was recorded in Web logs but no failures actually occurred on the client side.
For example, suppose an HTML template file is not used by a Web page, and the template file is removed from the Web site but the code of using the template still remains in the HTML code.
If a user sends a request to this Web page, there is an error log entry of the template file in the Web logs, but no failure occurs on the client side.
In this case, our study reported that a failure was encountered by users but was not detected by our technique.
However, no failure actually occurred, and thus, the empirical results are lower than the real detection rate.
In our study, our model was updated in both Step 3 andStep 4 if a testing session was not reported as encountering failures (Step 5 of the process we used indicates this updating).
This process means that some error sessions were used to train the model because some error sessions were not reported as errors.
Nevertheless, our model still detected failures with a reasonable false positive rate.
This result demonstrates the robustness of the model.
There are three approaches to detect failures in Web applications.
The first set approach, which is the closely related work to ours and inspires ours, was presented by Bodik and colleagues [3].
Their technique addresses a special case of our technique, and our technique generalizes theirs and can detect more failures than theirs.
Their technique detects failures by analyzing Web logs to find abrupt changes of hits on a Web page as an indication of users encountering failures.
The technique assumes that users refresh their browsers when they encounter failures, and refreshing operations are recorded in Web logs as repeated requests to the same resource.
However, this assumption does not always hold because users can have behaviors different from refreshing Web pages when they encounter failures.
Our empirical results show that the frequency of users refreshing their browsers when they encounter failures is low: refreshing occurred in only 119 out of 500 error sessions.
Like their technique, ours assumes that users' browsing behaviors change when they encounter failures.
However, our technique does not presume any user's behavior.
Rather, our technique constructs a first-order Markov model to compute the probability of a browsing path to infer whether a user's browsing behavior is anomalous.The second approach applies statistical analysis to components' runtime information to infer anomalous execution paths (e.g., Chen and colleagues [6] and Kiciman and Fox [11]).
Our technique differs from these in three ways.
First, our technique can detect source code bugs that cause user-visible changes, which theirs is not always effectively detect failures caused by such software bugs [11].
Second, our technique is more efficient than these techniques.
Ours does not require any instrumentation, but theirs rely on code instrumentation to monitor applications' runtime behavior.
Third, our technique is more scalable.
Their techniques can suffer scalability issues because the number of underlying components can grow exponentially as the systems grow.
This growth occurs particularly when users' requests invoke a large number of objects of dynamic binding, which is an essential programming technique in multi-layer Web applications.
In contrast, our technique requires only Web logs and is independent of the size of the application.
However, theirs can detect failures that are not caused by software bugs such as performance failures, but ours does not ensure to detect such failures.The third approach applies statistical analysis to infer failures that downgrade the performance of multi-tier Web applications (e.g., Barham and colleagues [2] and Cohen and colleagues [7]).
Their statistical models analyze data from a variety of sources, including networking, hardware, operating systems, databases, middle-ware, and application components.
These techniques address failures that reduce the system's response.
In contrast, our technique focuses on detecting application-level failures.Another set of techniques are related to ours in that they also study user-browsing behavior.
These techniques aim at such goals as optimizing Web caching (e.g. Padmanabhan and Mogul [14]), personalizing Web sites (e.g., Mobasher and colleagues [12], or studying user behavior patterns [1].
Our technique differs from these in two ways.
First, these techniques aim at discovering the majority of user browsing behavior, whereas ours uncovers the minority-they discover patterns, but we find anomalies.
Second, although both types of techniques use a fixed-size sliding window to cover 1 + w user requests, theirs uses the window in the w + 1 mode: using the w requests to predict the (w + 1)th request.
In contrast, our technique uses a fixed-size sliding window in the 1 + w mode: using the w requests to estimate the one before the w requests.
In our future work, we plan to improve our technique from three aspects.
First, we need improve the detection power of our model.
As shown in the ROC curve in Section 4.3, the slope of the false positive shows straight line segments, which indicates our model needs further sources of evidence for detection.
A potential improvement of the model is to use a semi-Markov model to characterize the time that the user spends in each web page to infer additional indicators.
We could also introduce more hidden states into the model to group users by their intention of using the Web site.
Second, we need further evaluate our technique.
In this work, we had only one cast study and could only use "hard errors" (i.e., errors reported by HTTP error code) as the "ground truth" to classify whether users encountered errors.
However, the goal of the technique is to detect failures caused by software bugs that do not necessarily generate an error code.
Users' responses to software bugs might be different from responses to hard errors, and such differences could potentially weaken the validate of the experimental results of the current work.
However, it requires to have "known" data for improving the evaluation.
Thus, we plan to collect user-interaction data of using multiple real Web sites in the controlled environment.
One possible solution is to recruit user to use Web sites with seeded bugs and collect user-interaction data of using such Web sites.
Third, we plan to introduce more human-computer interaction techniques into our model.
For instance, our current model assumes an ideal environment where users perform their tasks without interruption.
However, users may abandon their tasks by interruptions such as phone calls and emails.
In such cases, our model could raise false alarms.
A more sophisticated user model could help to handle more realistic users' environments.
