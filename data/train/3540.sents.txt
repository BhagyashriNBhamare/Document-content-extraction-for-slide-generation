Clos topologies have been widely adopted for large-scale data center networks (DCNs), but it has been difficult to support incremental expansions for Clos DCNs.
Some prior work has claimed that the structure of Clos topologies hinders incremental expansion.
We demonstrate that it is indeed possible to design ex-pandable Clos DCNs, and to expand them while they are carrying live traffic, without incurring packet loss.
We use a layer of patch panels between blocks of switches in a Clos DCN, which makes physical rewiring feasible, and we describe how to use integer linear programming (ILP) to minimize the number of patch-panel connections that must be changed, which makes expansions faster and cheaper.
We also describe a block-aggregation technique that makes our ILP approach scalable.
We tested our "minimal-rewiring" solver on two kinds of fine-grained expansions using 2250 synthetic DCN topologies, and found that the solver can handle 99% of these cases while changing under 25% of the connections.
Compared to prior approaches, this solver (on average) reduces the average number of "stages" per expansion from 4 to 1.29, and reduces the number of wires changed by an order of magnitude or more-a significant improvement to our operational costs, and to our exposure (during expansions) to capacity-reducing faults.
Prior work has described DCN designs that support incremental expansion, and techniques for conducting expansions.
Our work focuses on Clos topologies, the de-facto standard for large-scale DCNs; most prior work on expansions has used non-Clos designs.DCell [15] and BCube [14] are built using iterative structures.
As a result, they can only supports expansions at a very coarse granularity, which could lead to substantial stranded DCN capacity after expansion.
Similar iteratively-designed DCN structures are also proposed in [16,21,22].
JellyFish [28], Space Shuffle [31], Scafida [18], and Expander [29,9] were designed to support fine-grained incremental expansion using random-graph-based DCN topologies.
However, these topologies have not been widely adopted for industrial-scale data centers, possibly due to the increased complexity of cabling and routing and congestion control when deploying large-scale DCNs.Random Folded Clos [4] is a variant of Clos that supports fine-grained incremental expansion.
It maintains a layered structure, but builds inter-layer links based on random graphs.
However, Random Folded Clos is only designed for homogeneous DCNs, where all blocks are of the same size and the same port speed.
Further, Random Folded Clos is not non-blocking, with reduced capacity when compared to Fat Trees.
In contrast, our minimal-rewiring solver can be applied to heterogeneous DCNs, and its topology solution preserves the non-blocking property of Clos topologies.Similar to our minimal-rewiring DCN topology solver, optimization-based approaches were also adopted in LEGUP [8] and REWIRE [7].
However, neither paper looked at the topology-design problem in the presence of a patch-panel layer, and could have daunting cabling complexity.
Further, LEGUP uses a branch-and-bound algorithm [3], and REWIRE uses simulated annealing [24] to search for the optimal topology.
Both algorithms scale poorly, as their convergence time grows exponentially with the problem size.Condor [25] allowed designers to express goals for DCN topologies via a constraint-based language, and used a constraint solver to generate topologies.
Condor addressed the challenge of multi-phase expansions, but their approach was unable to solve expansions for many arbitrary configurations of Clos networks due to computational complexity.
The Clos topology was initially designed for telephone networks [6].
Years later, it was proposed for DCNs [1] and subsequently became the de-facto standard for largescale DCNs, at Google [27], Facebook [10], Cisco [5], Microsoft [13], etc..
There are many advantages to Clos networks.
First, Clos networks are non-blocking, and thus have high throughput.
Second, Clos networks can be built using identical and inexpensive commodity switches, and thus are cost-effective.
Third, Clos networks have many of redundant paths for each source-destination pair, and thus are highly failure-resilient.
Clos-based data centers exhibit a layered architecture (see Fig. 1).
The lower layer contains a number of server blocks 1 .
The upper layer contains a number of spine blocks, used to forward traffic between server blocks.
A DCN topology interconnects each server block to each spine block.
Because we want to support technology evolution within a single network, each of the server blocks could have a different number of spine-facing uplinks, a different uplink rate, etc 2 .
Connecting server blocks and spine blocks by direct wires 3 is highly inefficient.
First,a large-scale data center typically has tens of thousands of DCN links.
Second, the server blocks and the spine blocks of a data center may be deployed at different locations on a data center floor, due to space constraints, so some direct links would have to run a long way across the data center.
Third, moving, adding, or removing a long link during an expansion requires significant human labor, creates a risk of error, and because the new links might have dramatically different lengths requires a large inventory of cables of various lengths.To overcome these difficulties, we introduce a patch panel [19,26] layer between server blocks and spine blocks (Fig. 1).
Patch panels are much cheaper than other DCN components (e.g., switches, optical transceivers, long-reach fibers).
All the interconnecting ports of the server and spine blocks can be connected to the front side of the patch panels via fiber bundles, and all the connecting links can be established or changed using fiber jumpers on the back side 4 .
These patch panels are co-located.
As a result, a DCN topology can be wired and modified without walking around the data center floor or requiring the addition or removal of existing fiber.
Also, as discussed in [27], deploying fibers in bundles greatly reduces cost and complexity; using patch panels means we can deploy bundles once, without having to change them during an expansion.
This patch-panel layer makes it much easier for us to support rapid expansions without excessive capacity reduction.Patch panels allow us to divide a DCN topology into two layers, physical and logical.
As shown in Fig. 1, each server and spine block is connected to the patch-panel layer; we call this the physical topology.
When a new block is first deployed, we deploy its corresponding physical topology.
Changing physical links is not easy, as it involves moving fiber bundles across different patch panels.
Hence, in this paper, readers can view physical links as fixed once deployed.Logical topology defines how server blocks connect to spine blocks, abstracting out the patch-panel layer.
All DCN topologies discussed in literatures refer to the logical topology.
Many DCN performance metrics have been defined in terms of logical topologies, including network bandwidth, failure resiliency, incremental expandability, etc.
However, except for Condor [25], no prior work has studied the logicaltopology design problem in presence of patch panels.
As discussed in §5.2.1, in addition to optimizing the performance metrics listed above, we also need to enforce additional physical constraints, so that the resulting logical topology is compatible with the underlying physical topology.
During an expansion, the existing physical topology remains unchanged, and we only add new bundles of physical links for the newly added/upgraded server/spine blocks.
Note that adding new physical links does not impact any ongoing traffic.
In contrast, some of the links in the existing logical topology, which could be carrying significant traffic, will have to change, so we must ensure there is no traffic loss caused by changes to the logical topology.As shown in Fig. 2 (a), we could add a large amount of capacity to a data center during each expansion, which would allow us to do expansions infrequently.
However, this expansion strategy would lead to much more stranded network capacity -capacity installed but not usable -as the traffic demand could be far less than the capacity provided.
By doing fine-grained expansions, we reduce the mean amount of stranded capacity (see Fig. 2 (b)).
We support two types of expansions, at the granularity of a server block (Fig. 3).
The first type adds a new server block, to allow more servers to be added to an existing data center.
The second type increases the uplink count ("radix") of an existing server block.
Typically, the uplinks of a server block are not initially fully populated with optical transceivers, because transceivers are expensive and a new server block has a low bandwidth requirement (as not all of its servers are connected).
As a block's bandwidth requirement increases, we need to populate more uplinks.
Note that as we expand the server-block layer, additional spine blocks will also be needed, so expansions generally involve adding server blocks and spine blocks at the same time.
Fig. 4 depicts our pipeline for updating a logical topology during a live expansion.
It guarantees that:• No traffic loss due to routing packets into "black holes."
• All wiring changes are made correctly.
• No congestion due to the temporary capacity reduction.To avoid black holes when changing a set of logical links, we must redirect traffic on these links to other paths.
We instruct our SDN-based control plane to "drain" these links.
After we verify that there is no longer traffic on the target drained links, we can proceed to rewire the links.Rewiring links via patch-panel changes is the most laborintensive and error-prone step.
Typically, thousands of links need to be rewired during one expansion, creating the possibility of multiple physical mistakes during the rewiring process.
To check for errors, we perform a cable audit.
In cable audit, we use the Link Layer Discovery Protocol to construct a post-wiring topology, and then cross-validate this against the target logical topology.
We also run a bit-error-rate test (BERT) for all the new links, to detect links with issues.
This audit results in automated tickets to repair faulty links, followed by a repeat of the audit.During DCN expansion, we must drain some fraction of the logical links.
While Clos networks are resilient to some missing links, draining too many links simultaneously could result in dropping the network's available capacity below traffic demand.
We therefore set a residual-capacity threshold, based on measured demand plus some headroom.
We then divide an expansion into stages such that, during each stage, the residual capacity remains above this threshold.
In our original approach, we partitioned the set of patch panels into C groups (as illustrated by different colors in Fig.
1), and only rewired links in one group of patch panels per stage.
Then, at each stage, we would still have approximately 1 − 1/C of the pre-expansion capacity available.Note that the expansion pipeline migrates the network, in stages, from an existing (old) topology to some new logical topology that connects the new blocks to the existing ones, subject to a set of constraints on the logical connectivity ( §5.2.1 formalizes those constraints).
However, there are many ways to construct a logical topology that meets our constraints, and our original, simple solution to these constraints typically required a lot of rewiring for an expansion.
This, in turn, forced us to divide expansions into C stages, to ensure that we preserved at least 1 − 1/C of the pre-expansion capacity.As a result, these expansions took a long time, especially if the pre-expansion network was highly utilized.
In our experience, each stage takes considerable time, including draining, rewiring, cable auditing, BERTing, and undraining.
If we were to expand a network with a requirement to preserve 90% residual capacity, at least 10 stages would be needed 5 .
The root cause of the length of our original expansion pipeline is that it does not attempt to optimize the difference (in terms of wiring changes) between pre-expansion and post-expansion topologies.
If we were able to minimize this difference, we could finish an expansion in fewer stages.
This is related to the "incremental expansibility" property of a DCN topology.
This property is easy to achieve for random-graph topologies such as Jellyfish [28].
However, for Clos networks, none of the existing topology solutions exhibits this property.
This motivated us to look for a better approach, which we describe in the rest of this paper.
In this section, we describe a new "minimal rewiring" topology-design methodology for Clos networks, which not only achieves high network capacity and failure resiliency, but also minimizes rewiring for expansions.
Our approach relies on Integer Linear Programming (ILP) to compute a logical topology.
Our results compare well to existing Clos topology solutions [27,5,13,33].
While we initially considered an ILP formulation at the granularity of switch and patch-panel ports, the resulting scale made solving infeasible.
Instead, we use multiple phases, first solving a block-level ILP formulation ( §5.2) and then performing a straightforward mapping onto a port-level solution ( §5.4).
To rigorously formulate the minimal-rewiring topologydesign problem, we introduce the following definitions and mathematical notations.Server Block: We use a server block as a unit of deploying network-switching capacity.
On the order of 1000 servers can be connected to a server block via ToR switches.
To avoid a single point of failure, we further divide a server block into independent middle blocks, typically four 6 (see Fig. 1); each middle block is controlled by a different routing controller.
The uplinks of each ToR switch are evenly spread among the middle blocks in its server block.
Even if one middle block is down, servers in the server block are 5 In fact, since failed links and inexact load balance can cause topology imperfections, more than 10 stages would be required to ensure 90% residual capacity.
6 Our results hold for any number of middle blocks.
E n , S m , O k Server block n, spine block m, patch panel k E t n Middle block t in server block n G k (E t n ) Physical link-count for E t n via patch panel k G k (S m ) Physical link-count for S m via patch panel k b k (E t n , S m )Reference logical-topology link-count between E t n and S m via patch panelO k d k (E t n , S m )Desired logical-topology link-count between E t n and S m via patch panel O k p n,mMean number of links between a server block and a spine block q t n,mMean number of links between a middle block and a spine block n g Server block group index; or a set containing all the server block indices in the n g -th group m gSpine block group index; or a set containing all the spine block indices in the m g -th group k gPatch panel group index; or a set containing all the patch panel indices in the k g -th group E n g , S m g , O k g Server block group n g , spine block group m g , patch panel group k g E t n gAll the t-th middle blocks in E n g b k g (E t n g , S m g ) Reference logical-topology link-count between E t n g and S m g via patch panel groupO k g d k g (E t n g , S m g ) Desired logical-topology link-count between E t n g and S m g via patch panel group O k g x + max{0, x}still accessible via its other middle blocks.
We assume that a DCN has N server blocks, each of which is represented by E n , n = 1, 2, ..., N.
We denote the middle blocks within server block E n by E t n ,t = 1, 2, 3, 4.
Spine Block: Spine blocks forward traffic among different server blocks.
We use S m , m = 1, 2, ..., M to represent a spine block, where M is the total number of spine blocks.
Physical Topology: Assume there are K patch panels, each of which is represented by O k , k = 1, 2, ..., K.
We use G k (E t n ) to represent the total number of physical links between the middle block E t n and the patch panel O k .
Then, the physical topology of server block E n can be characterized by{G k (E t n ), k = 1, ..., K,t = 1, 2, 3, 4} 7 .
Similarly, we use G k (S m ) to represent the total number of physical links between spine block S m and patch panel O k .
Then, the physical topology of spine block S m can be characterized by {G k (S m ), k = 1, ..., K}.
Note that our networks are heterogeneous: different server blocks and spine blocks could have different physical topologies (see Table 2 for details).
Reference Topology: Our goal is to minimize rewiring with respect to the old logical topology, called the reference topology.
We let b k (E t n , S m ) be the total number of referencetopology links between middle block E t n and spine block S m that connect via patch panel O k .
Since the new server and spine blocks do not exist in the reference topology, for these blocks we simply set b k (E t n , S m ) = 0.
Logical Topology: Our objective is to compute a new logical topology for the given physical topology.
We use d k (E t n , S m ) to represent the total number of logical links between middle block E t n and spine block S m that connect via patch panel O k .
As we will show shortly, as long as {d k (E t n , S m )} k,m,n,t satisfies a set of physical topology constraints, a polynomial-time algorithm can be used to map {d k (E t n , S m )} k,m,n,t to point-to-point configurations in the patch panels.
Hence, the objective of the block-level ILP formulation is to compute {d k (E t n , S m )} k,m,n,t .
Our ILP formulation consists of a set of constraints ( §5.2.1) and an objective ( §5.2.2).
Recall that our objective is to compute {d k (E t n , S m )} k,m,n,t .
We must impose a set of constraints on the solution, not only to ensure the compatibility of the logical topology with the underlying physical topology, but also to guarantee both high throughput and high failure resiliency.
Physical Topology Constraints: Recall that d k (E t n , S m ) is the total number of logical links between middle block E t n and spine block S m connected via patch panel O k .
Clearly, it must be no larger than the total number of physical links G k (E t n ) between O k and E t n , and the total number of physical links G k (S m ) between O k and S m , i.e.,0 ≤ d k (E t n , S m ) ≤ min{G k (E t n ), G k (S m )}.
(1)To ensure high uplink bandwidth for all the server blocks, we require that each "populated" server block port 8 must connect to a spine block port.
This is guaranteed byM ∑ m=1 d k (E t n , S m ) = G k (E t n ).
(2)Note that constraint (2) requires that the total number of spine block ports must be no smaller than the total number of server block ports on each patch panel.
Hence, it is possible that a physically-connected spine block port might not connect to any server block port, which can be expressed as:N ∑ n=1 4 ∑ t=1 d k (E t n , S m ) ≤ G k (S m ).
(3)Capacity Constraints: In order to achieve high DCN capacity, which depends on load balance, we require the uplinks of each server block to be evenly distributed among all spine blocks.
Specifically,p n,m ≤ K ∑ k=1 4 ∑ t=1 d k (E t n , S m ) ≤ p n,m ,(4)where p n,m is the mean number of links between a server block and a spine block.
p n,m = |E n ||S m |/(|S 1 |+· · ·+|S M |) 9 , where |E n | (or |S m |) is the total number of ports in E n (or S m ), p n,m is the largest integer that is no larger than p n,m , and p n,m is the smallest integer that is no smaller than p n,m .
Constraint (4) ensures high DCN capacity.
In the ideal case where all p n,m 's are integers, constraint (4) ensures that traffic between any two server blocks E n 1 and E n 2 can burst at full rate (min{|E n 1 |, |E n 2 |}).
Specifically, E n 1 and E n 2 can communicate at rate min{p n 1 ,m , p n 2 ,m } through the m-th spine block, and thus the total rate would be ∑ M m=1 min{p n 1 ,m , p n 2 ,m } = min{|E n 1 |, |E n 2 |}.
In the general case where some p n,m 's are not integers, there must be some imbalance in the logical topology.
Constraint (4) minimizes this imbalance.
Constraints: While commodity switches are highly reliable, an entire middle block can fail as a unit due to software bugs in its routing controller.
We also bring down an entire middle block occasionally to upgrade the switch stack softwares.
In order for our DCN to be failure-resilient, we need to make sure that throughput remains as high as possible even under middle-block failures.
This can be achieved by requiring the middle block links to be evenly distributed among the spine blocks.
Specifically,q t n,m ≤ K ∑ k=1 d k (E t n , S m ) ≤ q t n,m ,(5)where q t n,m is the mean number of links between a middle block and a spine block 10 .
q t n,m = |E t n ||S m |/(|S 1 | + · · · + |S M |) = p n,m /4 (assuming 4 middle blocks per server block).
Constraint (5) minimizes the capacity impact under middle block failures.
In the ideal case where q t n,m 's are all integers, the throughput impact is exactly 25%.
In the general case where some q t n,m 's are not integers, there must be some imbalance in the traffic between the middle blocks and the spine blocks.
Constraint (5) minimizes this imbalance.
Note that Constraint (5) cannot subsume (4), because all the decision variables are integers.
In our ILP-based formulation, it is easy to add a minimalrewiring objective function.
Specifically, our block-level minimal-rewiring solver can be formulated as:min K ∑ k=1 N ∑ n=1 4 ∑ t=1 M ∑ m=1 (b k (E t n , S m ) − d k (E t n , S m )) + , subject to (1) − (5),(6)where x + = max{0, x}.
This objective function computes the total number of links to be rewired for changing the old (reference) topology to the new topology.
We demonstrate the benefit of minimal rewiring using a simple example, comparing our minimal-rewiring approach against the rotation striping approach described in [33].
Rotation striping can be used to design Clos topologies for homogeneous DCNs of arbitrary size,whereas minimalrewiring works with various block sizes.Consider a DCN configuration with N server blocks and M spine blocks.
Assume that each server block has X ports, and that there is only one patch panel 11 .
Then, rotation striping can be expressed as Algorithm 1.
Input: DCN configuration parameters N, M, X Output: A DCN topology 1 Label all the server block ports with different indices from 1, 2, ..., NX.
Note that the set of ports {(n − 1)X + 1, (n − 1)X + 2, ..., nX} corresponds to the n-th server block.
2 Connect port e ∈ {1, 2, ..., NX} to the e/M-th port of the ((e − 1)%M + 1)-th spine block.Algorithm 1: Rotation striping algorithm from [33] We can quantify the rewiring ratio for a solution as the fraction of wires between server blocks and spine blocks in the pre-existing topology that must be disconnected during an expansion procedure.Consider an expansion in which we add 1 server block and 1 spine block.
It is easy to check that with rotation striping, only the first M server-block ports connect to their original peers, and thus the rewiring ratio would be (NX − M)/NX.On the other hand, using minimal-rewiring 12 , we can show that only XN/(M + 1) links need to be rewired, which corresponds to a rewiring ratio of 1/(M + 1).
This means that even if our expansion is executed in just a single stage, the capacity reduction is just 1/(M + 1).
To the best of our knowledge, with the exception of Condor [25], none of the prior literature has incorporated patchpanel layer constraints, and Condor's constraint-satisfaction approach was unable to find solutions in most cases.
In practice, we usually need more than one patch panels in order to connect all the server blocks to all the spine blocks, since the number of ports on each patch panel is limited.
If one ignores the patch-panel layer, then the resulting DCN topology will be very likely not compatible with the underlying physical topology.Consider rotation striping in an example with two patch panels (see Fig. 5).
Each server block has six links, with three links connecting to the first patch panel and the other three connecting to the second one.
There are four spine blocks, with two of them connecting to the first patch panel and the other two connecting to the second one.
If we apply rotation striping here, there should be four logical links between the first server block and the first two spine blocks.
Note that these four links can only be created through the first patch panel, because the first two spine blocks only connect to the first patch panel.
However, this is impossible, as there are only three physical links between the first server block and the first patch panel.
Our approach incorporates the patch-panel layer via three physical constraints (1)-(3).
These three constraints ensure that any solution {d * k (E t n , S m )} k,m,n,t of (6) can be mapped to port-to-port configurations in the patch panels as we describe below.
(Note that (6) may yield multiple solutions.)
Our ILP formulation tells us the block-level link count between each middle block E t n and each spine block S m in each patch panel O k , as in (6), but not how individual ports must be connected.
We thus developed a straightforward algorithm to compute these port-to-port mappings.The algorithm's input consists of the block-level link counts b k (E t n , S m ) and d * k (E t n , S m ) for the pre-expansion and post-expansion topologies, respectively.
We use two passes, both of which iterate over all pairs of middle blocks and spine blocks:Pass 1: disconnect links as necessary: For each patch panel O k , note that an expansion changes the link count between middle block E t n and spine block S m fromb k (E t n , S m ) to d * k (E t n , S m ).
Therefore, if b k (E t n , S m ) ≤ d * k (E t n , S m ), we simply preserve all pre-existing links; ifb k (E t n , S m ) > d * k (E t n , S m ), we can disconnect any (b k (E t n , S m ) −d * k (E t n , S m )) of the pre-existing links.
This pass disconnects∑ N n=1 ∑ 4 t=1 ∑ M m=1 (b k (E t n , S m ) − d * k (E t n , S m )) + links.Pass 2: connect new links: After the first pass, min{b k (E t n , S m ), d * k (E t n , S m )} links remain between E t n and S m .
For any block pair E t n and S m with less than d * k (E t n , S m ) links, we can arbitrarily pick d * k (E t n , S m ) − b k (E t n , S m ) nonconnected ports from E t n and S m respectively, and interconnect them.
Feasibility is guaranteed by the physical topology constraints (1)-(3).
We use integer linear programming to formulate our minimal-rewiring solver, because our topology-design problem is NP-Complete.
Specifically, the decision variables d k (E t n , S m ) contain three dimensions (middle-block dimension E t n , spine-block dimension S m , and patch-panel di-mension k ), and the constraints (2)(3)(5) are essentially for the 2-marginal sums ∑ k d k (E t n , S m ), ∑ E t n d k (E t n , S m ) and ∑ S m d k (E t n , S m ).
In the literature, this is called the ThreeDimensional Contingency Table (3DCT) problem, and has been proven to be NP-Complete [20].
Having failed to find a polynomial-time algorithm for our problem, we decided to use ILP, as there are many readily-available commercial ILP solvers, e.g., Gurobi [17], Google Optimization Tools [12].
However, our problem size is so large that none of the existing commercial solvers scales well.
For example, one DCN configuration we evaluate (see §9.7.1) contains 77 server blocks of three kinds, 68 spine blocks of two kinds, and 256 patch panels.
Without any optimization, this leads to about 400000 decision variables, and the ILP solver ran for a day without generating a solution.
As shown in §9.3, for the 4500 trials we ran without optimization, we could only solve 32% within a 3-hour deadline.
(Longer timeouts yield little improvement.)
To improve the scalability of our minimal-rewiring solver, we developed a block aggregation technique.
Block aggregation significantly reduces the total number of decision variables in (6), and thus greatly improves solver scalability.
The idea behind block aggregation is to group patch panels, server blocks, spine blocks, and then aggregate decision variables within each group.Patch-Panel Group: Two patch panels k 1 , k 2 belong to the same group if and only if they have the same number of physical links to each middle block and each spine block, i.e.,G k 1 (E t n ) = G k 2 (E t n ), G k 1 (S m ) = G k 2 (S m )for any n,t, m. Server-Block Group: Two server blocks n 1 , n 2 belong to the same group if and only if they have the same physical topology, i.e.,G k (E t n 1 ) = G k (E t n 2) for any k,t. Spine-Block Group: Two spine blocks m 1 , m 2 belong to the same group if and only if they have the same physical topology, i.e., G k (S m 1 ) = G k (S m 2 ) for any k.Assume that these definitions yield K g patch-panel groups, N g server-block groups, and M g spine-block groups.
Then, we can define an aggregated decision variable d k g (E t n g , S m g ) for the k g -th patch-panel group, the n g -th server block group, and the m g -th spine block group as follows:d k g (E t n g , S m g ) = ∑ k∈k g ∑ n∈n g ∑ m∈m g d k (E t n , S m ).
(7)Here, we have abused the notation, and used k g , n g , and m g to represent both the indices and the actual groups.
With the aggregated decision variables d k g (E t n g , S m g ), the original constraints (1)-(5) also need to be aggregated:0 ≤ d k g (E t n g , S m g ) ≤ |k g ||n g ||m g | min{G k (E t n ), G k (S m )}, (8) M g ∑ m g =1 d k g (E t n g , S m g ) = |k g ||n g |G k (E t n ),(9)N g ∑ n g =1 4 ∑ t=1 d k g (E t n g , S m g ) ≤ |k g ||m g |G k (S m ),(10)|n g ||m g |p n,m ≤ K g ∑ k g =1 4 ∑ t=1 d k g (E t n g , S m g ) ≤ |n g ||m g |p n,m ,(11)|n g ||m g |q t n,m ≤ K g ∑ k g =1 d k g (E t n g , S m g ) ≤ |n g ||m g |q t n,m .
(12)In the aggregated constraints, k, n, m are arbitrary patch panel, server block, and spine block indices, drawn from the k g -th patch-panel group, the n g -th server-block group, and the M g -th spine-block group, respectively.
In fact, the valuesG k (E t n ), G k (S m ), p n,m , q t n,m are all the same, as long as k, n, m are chosen in the same group, respectively.
Here, we also view k g , n g , and m g as groups, and have used |k g |, |n g |, |m g | to represent the sizes of these groups.With block aggregation, we can thus rewrite the optimization problem (6) as follows:min K g ∑ k g =1 N g ∑ n g =1 4 ∑ t=1 M g ∑ m g =1 (b k g (E t n g , S m g ) − d k g (E t n g , S m g )) + , subject to (8) − (12),(13)whereb k g (E t n g , S m g ) = ∑ k∈k g ∑ n∈n g ∑ m∈m g b k (E t n , S m ).
Compared to (6), the total number of decision variables in (13) is significantly reduced, from Θ(NKM) to Θ (N g K g M g ).
Thus, the complexity of solving (13) is significantly lower than that of (6).
After obtaining a solution d * k g (E t n g , S m g ) for (13), we still need to decompose the solution to d * k (E t n , S m ).
Specifically, we need to solve the following problem.min K ∑ k=1 N ∑ n=1 4 ∑ t=1 M ∑ m=1 (b k (E t n , S m ) − d k (E t n , S m )) + , subject to ∑ k∈k g ∑ n∈n g ∑ m∈m g d k (E t n , S m ) = d * k g (E t n g , S m g ) and (1) − (5).
(14)If there were no constraints (1)- (5) in (14), we could easily compute a solution d * k (E t n , S m ) in polynomial time using an algorithm similar to the one in §5.4.
Because of these constraints, solving (14) becomes much more challenging.
In fact, it is not trivial to prove that (14) always has a solution.
In the literature, (14) is closely related to the Integer-Decomposition property [2].
In general, the IntegerDecomposition property does not always hold (an example is provided in [32]).
Fortunately, thanks to our problem structure, we are able to rigorously prove that (14) indeed has a solution.
Specifically, we build an integer decomposition theory specifically for our problem, and prove that a decomposition satisfying (1)-(5) can be found iteratively in polynomial time.
The details are available in [32].
Given that (14) has solutions, the next question is to find one that minimizes the objective function.
The simplest approach is to directly solve it using integer programming.
However, this will destroy the complexity-reduction of block aggregation, as (14) is of exactly the same size as (6).
Our approach is to decompose (14) into K g + N g + M g smaller ILP problems.
Specifically, we can decompose patch-panel groups, spine-block groups, and server-block groups by solving three ILPs in separate steps.
In each step, different block groups can be completely decoupled, and thus the three ILPs can be further decomposed into K g , N g , and M g ILPs, respectively.
These smaller ILP problems are much easier to solve compared to (14).
However, as shown in §9.3, computing these smaller ILP problems sequentially can still be slow.
To further improve scalability, we map these smaller problems to polynomial-complexity min-cost-flow problems.
This min-cost-flow-based decomposition can guarantee a solution that satisfies all constraints, but not rewiring optimality.
For details, please refer to [32] .
Theoretically, and as confirmed in §9.4, the total number of rewires would be higher with block aggregation.
We are essentially breaking one optimization problem (6) into two problems (13) and (14).
Solving (13) and (14) will generate a solution satisfying all constraints in (6), but which might not be optimal wrt. the minimal-rewiring objective (6).
Whenever the solver without block aggregation succeeds, it always achieve the smallest (best) rewiring ratio.
However, scalability without block aggregation is poor.
For a total of 4500 synthesized DCN expansion configurations, only 32% can be solved within a 3-hour limit.
(Longer timeouts yield little improvement.)
Breaking (6) into smaller ILPs does not completely solve the solver scalability issue.We may still encounter some intractable ILPs while solving (14) (see §9.3).
Our polynomial-time min-cost-flow based variabledeaggregation algorithm solves the scalability issue.
However, it may also introduce some sub-optimality in the rewiring ratio (see §9.4 for the detailed comparison).
Thus, we face a tradeoff between solver scalability and rewiring optimality, which we address in the next section.
While block aggregation makes it feasible to solve most DCN expansion configurations, it creates a tradeoff between solver scalability and rewiring optimality, depending on how one chooses a strategy for block aggregation.
Blockaggregation strategies define choices for each of several aggregation layers (patch-panel, server-block, or spine-block), and for the decomposition technology (ILP or a min-costflow approximate algorithm) applied at each layer.How can we choose the best strategy among all the options, given that the tradeoff between optimality and solver run-time is unknown when we start a solver?
We observe that since we care more about finding a solution within an elapsed time limit, and less about the total computational resources we use, our best approach is to run, in parallel, a solver instance for each of the options, and then choose the best result, based on a scoring function, among the solvers that complete within a set deadline.
We can define scores based simply on rewiring ratio, or on residual bandwidth during expansion, or some combination.
Fig. 6 shows the parallel-solver architecture.
The introduction of minimal rewiring requires several changes to the DCN expansion pipeline shown in Fig. 4.
Without minimal rewiring, it is fairly easy for an experienced network engineer to determine the number of stages required for an expansion; because almost all logical links need to be rewired, we must use C stages to maintain a residual capacity of 1 − 1/C.
With minimal rewiring, however, one cannot know the number of rewired links before running the solver; based on results in §9.4, the fraction could range from 0 to 30%.
Therefore, we add an automated expansion planner step to the front of the pipeline in Fig. 4.
The planner first uses the minimal-rewiring solver to generate a post-expansion topology, then iteratively finds the lowest C, starting at C = 1, which preserves the desired residual capacity threshold during the expansion (recall from §4 that this threshold is a function of forecasted demand, existing failures, and some headroom).
For each C, the planner divides the patch panels into C groups, tentatively generates the C intermediate topologies that would result from draining the affected links within a group, and evaluates those topologies against the threshold.
If capacity cannot be preserved for all intermediate topologies, the planner increments C and tries again.
Once a suitable C is found, the rest of the pipeline is safe to execute.Without minimal rewiring, we simply drain all links for the patch panels covered by a stage.
Minimal rewiring allows us to preserve more capacity because we only have to drain a subset of the links, rather than an entire patch panel.
However, an operator can accidentally rewire the wrong link, or dislodge one inadvertently, a problem that does not occur when we drain entire panels.
Therefore, we built a link-status monitor that alerts the operator if an active (undrained) link is disconnected.
Since Clos networks by design tolerate some link failures, this allows the operator enough time to repair the problem without affecting users.
We have been successfully using this minimal-rewiring approach for our own DCNs since early 2017 13 .
In order to demonstrate its benefits over a wide range of scales, we evaluated our approach using a set of synthetic DCN topologies (including some similar to our real DCNs), and show the effect of block aggregation on solver run-time and rewiring ratio.
We also show that our approach preserves most of the capacity of the original network during expansions, based on several workloads and under some failure scenarios.
To evaluate our approach over a wide range of initial topologies, we synthesized thousands of configurations that were consistent with our past deployment experience.
In particular, since we must support heterogeneity among server blocks and spine blocks, we synthesized configurations using three types of server blocks and two types of spine block, in various combinations.
Every configuration includes exactly one border block, analogous to a server block but used for external connectivity.Our synthesized DCN configurations always have 256 patch panels, located in 4 sets, each with 64 patch panels.
We use patch panels with 128 server-block-facing ports and 128 spine-block-facing ports, so the entire patch-panel layer supports up to 64 512-port server blocks.
Table 2 lists the physical-topology parameters for the different block types.
For example, a Type-1 server block has 512 up links, evenly distributed among all 256 patch panels.
Type-2 & Type-3 server blocks are "light" versions of the Type-1 server block, with fewer uplinks; they can be upgraded to Type-1 server blocks.
Note that a Type-3 server block only connects to 128 patch panels.
We divide the 256 patch panels into two partitions, and connect the Type-3 server blocks to the two partitions by rotation.
We also connect the Type-1 & Type-2 spine blocks by rotation.
We generate a total of 2250 initial configurations (preexpansion "reference topologies" as defined in §5.1) from all possible combinations of {3, 6, ..., 30} Type-1 server blocks, {2, 4, ..., 20} Type-2 server blocks, {3, 6, ..., 30} Type-3 server blocks, and {8, 16} Type-1 spine blocks, with the remainder of the necessary spine-block ports as Type-2 spine blocks (the total number of server-block and spineblock ports must match).
We omit any configuration that would require more patch-panel ports than we have available.These pre-expansion topologies can be generated using our minimal rewiring solver, by simply ignoring the objective function.
We run all the configurations in parallel, and allocate 4 CPUs and 16G RAM for each configuration.
With block aggregation enabled, all 2250 topologies can be computed within 10 seconds.
As shown in Fig. 3, we support two types of DCN expansions.
We construct two benchmarks for each of our 2250 reference topologies:Benchmark Suite 1: We upgrade two Type-2 server blocks in the reference topology to Type-1 server blocks.Benchmark Suite 2: We expand the reference topology by one Type-3 server block.We end up with 2250 × 2 = 4500 total target topologies.
For each of these, we ran the minimal-rewiring solver with three aggregation strategies: 1.
No aggregation.
2.
Block aggregation, decomposing the spine-block and server-block layers using ILP, while decomposing the patch-panel layer using MIN COST FLOW (See technical report [32] for details) .
3.
Block aggregation, decomposing all three layers using MIN COST FLOW.
plots success rate for our minimal-rewiring solver with different aggregation strategies, grouped by the total number of server blocks; the bars show the fraction of topologies solved for each group.
With the third aggregation strategy, we can solve all test cases within 10 seconds; this strategy only needs to solve one ILP for the aggregated problem, while using polynomial algorithms for all decompositions.
The first strategy scales poorly, and can only solve 32% of the test cases even if we increase the deadline to 3 hours; even for small DCNs (11-20 server blocks), it sometimes times out.
The second strategy can solve 67% of the test cases, but we start seeing timeouts for DCNs with 31-40 server blocks.
For all strategies, setting timeouts above 3 hours yields little improvement.
The differences between the second and third strategies shows that variable deaggregation using ILP could take significant run time.
(If we also use ILP to deaggregate the patch-panel layer with the second strategy, only 3% of the test cases can be solved.)
However, deaggregation via ILP is still useful, because as we discuss next, it can generate more-optimal rewiring solutions.
For the same set of test cases, Fig. 8 plots the rewiring ratios, again grouped by the number of server blocks; here, the bars show the mean value for each group.
The third aggregation strategy, while it has the best run time, also leads to the highest (worst) rewiring ratio, often close to 20%; this motivates our use of ILP for decomposition whenever we can tolerate the run time.
9.5 Effectiveness of the parallel solver §7 described how we use a parallel solver to strike a balance between scalability and rewiring optimality.
For each of our benchmarks, we ran, in parallel, solvers with the three different aggregation strategies, with a 3-hour timeout.
Because the third strategy works quickly for all instances, this parallel approach always succeeds.
It also achieves the best rewiring ratio available within a 3-hour budget.
Fig. 9 plots the CDF of the parallel solvers rewiring ratio.
For about 82% of the DCN configurations in the first benchmark suite, and about 93% in the second suite, we get solutions with ratios under 20%.
(The first suite tends to yield a higher ratio, because the total number of newly added server-block physical links in the first suite is twice that in the second suite.)
Note that the rewiring ratio for our prior approach was always 1.0 -we always replaced all patch-panel jumpers.
Fig. 9 shows that minimal rewiring saves us a lot of cost and time; the median rewiring ratio is about 22× better for the first suite and about 38× better for the second suite.
In addition to solver scalability and rewiring optimality, we also must preserve sufficient capacity of the intermediate topologies during expansion.
This requires us to quantify "capacity."
While our DCNs experience a variety of traffic patterns, we specifically evaluate the maximum achievable capacity under one-to-all and one-to-one traffic, which we rigorously define as:One-to-all capacity: T 1-all .
Assume that a source server block E n is sending traffic to all other server blocks, with its demand proportional to the ingress bandwidth of the destination blocks.
We increase the traffic until some DCN links are fully utilized.
Let T n 1-all be the ratio between the egress traffic under these assumptions and the best-case egress DCN bandwidth of the server block E n .
T n 1-all characterizes the one-to-all DCN capacity for server block E n .
We can then define T 1-all = min N n=1 T n 1-all as the one-to-all capacity of the entire DCN.One-to-one capacity: T 1-1 .
Assume that a server block E n 1 is sending traffic to another server block E n 2 .
We increase the traffic until some DCN links are fully utilized.
Let T n 1 ,n 2 1-1 be the ratio between the traffic sent and the minimum, over the two server blocks E n 1 and E n 2 , of their ingress capacity.
T n 1 ,n 2 1-1 characterizes the one-to-one DCN capacity for the server blocks E n 1 and E n 2 .
We can then defineT 1-1 = min n 1 =n 2 T n 1 ,n 2 1-1as the one-to-one capacity of the entire DCN.We evaluate T 1-all and T 1-1 under two different scenarios: Steady-state no-failures capacity: Recall that we imposed constraint (4) when computing a topology, which ensures high capacity in the non-expansion steady state, without any failures.
Ideally, if the p n,m 's in (4) were all integers, we would have T 1-all = 1 and T 1-1 = 1.
(Neither T 1-all or T 1-1 can be larger than 1).
Fig. 10 plots CDFs of T 1-all and T 1-1 based on the 4500 post-expansion topologies (benchmark suite 1 + benchmark suite 2).
Note that both T 1-all and T 1-1 are fairly close to 1.Steady-state capacity under middle block failure: Recall that we imposed constraint (5), to ensure the highest possible capacity if one middle block fails.
Ideally, for our typical case of 4 middle blocks, if the q t n,m 's in (5) were all integers, we would have T 1-all = 0.75 and T 1-1 = 0.75, no matter which middle block fails.
(Given one failure, neither T 1-all or T 1-1 can be larger than 0.75.
Fig. 10 also plots the CDFs of T 1-all and T 1-1 under middle block failures.
Note that both T 1-all and T 1-1 are fairly close to 0.75.
During an expansion, we must disconnect some links, which reduces DCN capacity.
We are interested in the residual capacity of the DCN in this state which clearly depends on the rewiring ratio.
Fig. 11 shows scatter plots of T 1-all and T 1-1 for each (rewiring ratio, residual capacity) pair, based on the two benchmark suites.
This figure assumes we do all expan- sion in a single stage, even if the residual capacity is lower than we can accept in reality.The residual capacity decreases approximately linearly with the rewiring ratio.
Note that the residual capacity decreases faster than the rewiring ratio increase, because, the rewired DCN links might not be evenly distributed among different server blocks.
As a result, some server blocks could suffer more capacity reduction than others.
In practice, we might not be able to do an expansion in just one stage while preserving sufficient residual capacity.
§8 described an expansion planner that determines the number of stages required.
We evaluated the number of stages saved by using minimal rewiring.Prior to minimal rewiring, we typically did 4-stage expansions.
If the topologies were perfect, with 4 stages we could preserve a residual one-to-all capacity of 0.75, but in practice we cannot achieve perfect balance; we have found it feasible and sufficient to preserve a residual capacity of 0.7.
Table 3 shows how many stages are needed to preserve a residual one-to-all capacity of 0.7, with minimal rewiring, for each of the 4500 benchmarks, based on solver strategy.
With strategies 1 and 2, many test cases require four stages, because the solvers time out before finding 1-or 2-stage expansions.
With strategy 3, almost all cases require at most 2 stages.
The parallel solver finds 1-or 2-stage expansions for all test cases (because the few cases for which strategy 3 requires four stages are handled better by the other strategies), and usually does better than strategy 3 (because if the other strategies succeed within the deadline, they yield better rewiring ratios.)
Overall, the parallel solver needs an average of 1.29 stages, vs. 4 stages for our prior approach.
A concrete (arbitrary, but realistic) example demonstrates the benefits of minimal rewiring.
Assume a pre-expansion DCN with 30 Type-1, 20 Type-2, 27 Type-3 server blocks, 1 border block, and 16 Type-1, 52 Type-2 spine blocks, which we expand by one Type-3 server block.
Without minimal rewiring, we must rewire all the 28056 logical links, in four stages.
With minimal rewiring, we need to rewire only 6063 of 28056 links (ratio = 0.216), in two stages.
Due to the scale of this example, only the third aggregation strategy succeeds.
Table.
4 shows how this maintains mid-expansion capacity (in italics) higher than our prior approach (in bold), and completes in two stages rather than four.
We have demonstrated that it is, in fact, feasible to do finegrained expansions of heterogeneous Clos DCNs, at large scale, while preserving substantial residual capacity during an expansion, and with a significant reduction in the amount of rewiring (compared to prior approaches).
We described how we use a patch-panel layer to reduce the physical complexity of DCN expansions.
We then described an ILP formulation that allows us to minimize the amount of rewiring, a block-aggregation approach that allows scaling to large networks, and a parallel solver approach that yields the best tradeoff between elapsed time and rewiring ratio.
Our overall approach flexibly handles heterogenous switch blocks, and enforces "balance constraints that guarantee both high capacity and high failure resiliency.
We evaluated our approach on a wide range of DCN configurations, and found on average that it allows us to do expansions in 1.29 stages, vs. 4 stages as previously required.
We thank Google colleagues for their insights and feedback, including Bob Felderman, David Wetherall, Keqiang He, Yadi Ma, Jad Hachem, Jayaram Mudigonda, and Parthasarathy Ranganathan.
We also thank our shepherd Ankit Singla, and the NSDI reviewers.
