We present a new cluster scheduler, G, aimed at jobs that have a complex dependency structure and heterogeneous resource demands.
Relaxing either of these challenges, i.e., scheduling a DAG of homogeneous tasks or an independent set of heterogeneous tasks, leads to NP-hard problems.
Reasonable heuristics exist for these simpler problems, but they perform poorly when scheduling heterogeneous DAGs.
Our key insights are: (() focus on the long-running tasks and those with tough-to-pack resource demands, () compute a DAG schedule , oine, by rst scheduling such troublesome tasks and then scheduling the remaining tasks without violating dependencies.
ese oine schedules are distilled to a simple precedence order and are enforced by an online component that scales to many jobs.
e online component also uses heuristics to compactly pack tasks and to trade-o fairness for faster job completion.
Evaluation on a-server cluster and using traces of production DAGs at Microso, shows that G improves median job completion time by and cluster throughput by .
Heterogeneous DAGs are increasingly common in dataparallel clusters.
We use DAG to refer to a directed acyclic graph where each vertex represents a task and edges encode input-output dependencies.
Programming models such as Dryad, SparkSQL and Tez compile user scripts into job DAGs [, , , , , ].
Our study of a large production cluster in Microso shows that jobs have large and complex DAGs; the median DAG has a depth of ve and thousands of tasks.
Furthermore, there is a substantial variation in task durations (sub-second to hundreds of seconds) and the resource usage of tasks (e.g., compute, memory, network and disk bandwidth).
In this paper, we consider the problem of scheduling such heterogeneous DAGs eeciently.Given job DAGs and a cluster of machines, a cluster scheduler matches tasks to machines online.
is matching has tight timing requirements due to the scale of modern clusters.
Consequently, schedulers use simple heuristics.
e heuristics leave gains on the table because they ignore crucial aspects of the problem.
For example, critical path-based schedulers [] only consider the critical path as determined by predicted task runtime and schedule tasks in the order of their critical path length.
When DAGs have many parallel chains, running tasks that use dierent resources together can lead to a better schedule because it allows more tasks to run at the same time.
As another example, multi-resource packers [] aim to run the maximal number of pending tasks that t within the available resources.
When DAGs are deep, locally optimal choices do not always result in the fastest completion time of the whole DAG.
Hence, intuitively, considering both variation in resource demands and dependencies may result in better schedules for heterogeneous DAGs.By comparing the completion times of jobs in the production cluster with those achieved by an oracle, we estimate that the median job can be sped up by up to .
We observe that individual DAGs have fewer tasks running relative to the optimal schedule at some point in their lifetime.
e cluster has lower overall utilization because (a) resources are idle even when tasks are pending due to dependencies or resource fragmentation, and (b) fewer jobs are released because users wait for the output of previous jobs.
Given the large investment in such clusters, even a modest increase in utilization and job latency can have business impact [, , ].
We note that the optimal schedule for heterogeneous DAGs is intractable [, ].
Prior algorithmic work exists especially on simpler versions of the problem [, , , , , , ].
However, we are yet to nd one that holds in the practical setting of a data-parallel cluster.
Specically, the solution has to work online, scale to large and complex DAGs as well as many concurrent jobs, cope with machine-level fragmentation as opposed to imagining one cluster-wide resource pool, and handle multiple objectives such as fairness, latency and throughput.In this paper, we describe a cluster scheduler G that eeciently schedules heterogeneous DAGs.
To identify a good schedule for one DAG, we observe that the pathologically bad schedules in today's approaches mostly arise due to two reasons: (a) long-running tasks have no other work to overlap with them, which reduces parallelism, and (b) the tasks that are runnable do not pack well with each other, which increases resource fragmentation.
Our approach is to identify the potentially troublesome tasks, such as those that run for a very long time or are hard to pack, and place them rst onto a virtual resource-time space.
is space would have d + dimensions when tasks require d resources; the last dimension being time.
Our intuition is that placing the troublesome tasks rst leads to a good schedule since the remaining tasks can be placed into resultant holes in this space.At job submission time, G builds a preferred schedule for a job as shown in Figure .
Aer identifying a subset of troublesome tasks, the remaining tasks are divided into the parent, child and sibling subsets.
G rst places the troublesome tasks onto a virtual resourcetime space and then places the remaining subsets.
Realizing this idea has a few challenges.
Which choice of troublesome tasks leads to the best schedule?
Further, since troublesome tasks are placed rst, when a task is considered for placement some of its parent tasks and some of its children tasks may already have been placed in the virtual space.
How to guarantee that every task can be placed without violating dependencies?
Our answers are in §.
G's online component schedules the tasks of each DAG in the order of their starting time in the virtual resource-time space.
Furthermore, across the many DAGs that may be running in the cluster, the online component respects dierent objectives-low job latency, high cluster throughput and fairness.
ese objectives can translate to discordant actions.
For example, a fairness scheme such as DRF [] may want to give resources to a certain job but the shortest-job-rst heuristic that reduces job latency may pick a dierent job.
Similarly, the task that is most likely to reduce resource fragmentation [] may not start early in the virtual resource-time space.
Our reconciliation heuristic intuitively picks tasks by consensus (e.g., based on a weighted combination of the scores received by a task from each objective).
However, to maintain predictable performance, we limit unfairness to an operator-congured threshold.We have implemented G as extensions to Apache YARN and Tez and have experimented with jobs from TPC-DS, TPC-H and other benchmarks on a server cluster.
Furthermore, we evaluate G in simulations on , DAGs from a production cluster.To summarize, our key contributions are: .
A characterization of the DAGs seen in production at Microso and an analysis of the performance of various DAG scheduling algorithms ( §).
.
A novel DAG scheduler that combines multiresource packing and dependency awareness ( §).
.
An online inter-job scheduler that mimics the preferred per-job schedules while bounding unfairness ( §) for many fairness models [, , ].
.
In our extended tech report [], we develop a new lower bound on the completion time of a DAG.
Using this, we show that the schedules built by G's oine component are within .
times the theoretically optimal schedule (OPT) for half of the production DAGs; three quarters are within .
times and the worst is .
times OPT.
.
Our experiments show that G improves the completion time of half of the DAGs by to across the various workloads.
Production DAGs improve relatively more because those DAGs are more complex and have diverse resource demands.
e gains accrue from running more tasks at a time; the cluster's job throughput (e.g., makespan) also improves by about .
While we present our work in the context of cluster scheduling, DAGs are a powerful and general abstraction for scheduling problems.
Scheduling the network transfers of a multi-way join or the work in a geo-distributed analytics job etc. can be represented as DAGs.
We oer early results in § from applying G to scheduling the DAGs that arise in distributed build systems [, ] and in request-response workows [, ].
Let each job be represented as a directed acyclic graph G = {V , E}.
Each node in V is a task with demands for various resources.
Edges in E encode precedence constraints between tasks.
Many jobs can simultaneously run in a cluster.
Given a set of concurrent jobs {G}, the cluster scheduler maps tasks to machines while respecting capacity constraints and task dependencies.
e goals of a typical cluster scheduler are high performance (measured using job throughput, average job completion time and overall cluster utilization) while oering fairness (measured w.r.t how resources are divided).
We use the DAG shown in Figure to illustrate the issues in scheduling DAGs.
Each node represents a task: the node labels represent the duration (top) and the demands for two resources (bottom).
Assume that the capacity is for both resources.
Let ε represent a value approaching zero.
Time Worst-case Intuitively, a good schedule would overlap the longrunning tasks shown with a dark background.
e resulting optimal schedule (OPT) is listed in the table below the gure.
OPT overlaps the execution of all the long-running tasks, t , t and t , and nishes in T.OPT t → t → {t , t} → {t , t , t} T − CPSched t → t → t → t → t → t T O(n) × OPT Tetris t → t → t → t → t → t T O(d) ×Since such long-running or resource-intensive tasks can be present anywhere in the DAG, greedy schedulers oen perform poorly as we show next.Critical path-based schedulers (CPSched) pick tasks along the critical path (CP) in the DAG.
e CP for a task is the longest path from the task to the job output.
e table shows the task execution order with CPSched.
CPSched ignores the resources needed by tasks.
In this example, CPSched performs poorly because it does not schedule tasks o the critical path early (e.g., t , t , t ) even though doing so reduces resource fragmentation by overlapping long-running tasks.Packers, such as Tetris [], match tasks to machines so as to maximize the number of simultaneously running tasks.
Tetris greedily picks the task with the highest value of the dot product between task's demand vector and the CP of t, t, t is T(+ε), T(+ ε) and T(+ε) respectively.
Tasks can run simultaneously only if their total demand is below capacity.
available resource vector.
e table also shows the task execution order with Tetris.
Tetris does not account for dependencies.
Its packing heuristic only considers the tasks that are currently schedulable.
In this example, Tetris performs poorly because it will not choose locally inferior packing options (such as running t instead of t ) even though that can lead to a better global packing.
G comes close to the optimal schedule for this example.
When searching for troublesome subsets, it will consider the subset {t , t , t } because these tasks run for much longer.
As shown in Figure , the troublesome tasks will be placed rst.
Since there are no dependencies among them, they will run at the same time.
e parents ({t , t , t }) and any children are then placed before and aer the troublesome tasks respectively in a compact manner while maintaining inter-task dependencies.Online: Consider two jobs that have the DAG shown in Figure .
Figure illustrates the online schedule when resources are to be divided evenly between these jobs (e.g., slot fairness []).
e oine schedule computed by G for each of the jobs, which overlaps the long-running tasks (t , t , t ), is shown on top.
e online component distills these schedules into a precedence order over tasks.
For example, the order for both jobs is: t , t , t , t , t , t .
Figure , bottom, shows a time-lapse of the task execution.Capacity Scheduler (CS) [], a widely used cluster scheduler, checks for which DAG the next available slot has to be allocated, and then picks (in a breadth-rst order) a runnable task from the designated DAG that ts the available resources.
Figure shows that CS results in an average job completion time (JCT) and makespan of .
T and T respectively.
Fairness causes the scheduler to interleave the tasks of the two jobs.
Tetris happens to produce a similar schedule to CS.
Note that this online schedule is far from the preferred per-DAG schedule, only a few of the long-running tasks overlap.
Similar to CS, most production schedulers, including Spark, schedule tasks based on some topological ordering of the DAG while using fairness to decide which job to give resources to next.
Hence, they behave similarly.Figure also shows that CPSched has an average JCT and makespan of .
T and T respectively.
is is because CPSched nishes the t tasks of both the jobs late; because t has a small CP length.
erefore the t tasks from both jobs do not overlap with any other long task.Finally, the gure shows that G has an average JCT and makespan of T. G achieves this by just enforcing a precedence order within each DAG.
In particular, note that all of the schedulers are work-conserving; they leave resources idle only if no schedulable task can t Tetris' packing score for each task, in descending order, is t=.
, t=.
, t=.
, t=.
, t=.
and t=.
.
in those resources.
e key dierence, among the schedulers, is the order in which they consider the tasks for scheduling.
Another dierence is whether this order is computed based only on the runnable tasks (e.g., ordering runnable tasks on their CP length, packing score or on their breadth-rst position) versus ordering based on a global optimization.
Informally, G's gains arise from looking at the entire DAG and choosing a globally optimal schedule.
To understand the problem with actual DAGs and at scale, we examine (a) the production jobs from a cluster of tens of thousands of servers at Microso, (b) jobs from a server Hive [] cluster and (c) jobs from a Condor cluster [].
Structural properties: As a preliminary, Figure illustrates some production DAGs at Microso.
Each circle denotes a stage.
By stage, we mean a collection of tasks that perform the same computation on dierent data (e.g. all map tasks).
e size of the circle corresponds to the number of tasks in logarithmic scale, the circle's color corresponds to the average task duration in linear scale and the edge color denotes the type of dependency.
We see W-shaped DAGs (bottom le) that join multiple datasets, inverted V-shaped DAGs (middle) that perform dierent analysis on a dataset, and more complex shapes (right) wherein multiple datasets are analyzed leading to multiple outputs.
Note also the varying average durations of the tasks (circle colors); the resource variations are not shown for simplicity.
Further, note cycles in the DAGs which are possibly due to self-joins and range-partitions.
Figure plots a CDF of various structural properties of the DAGs from the Microso cluster.
Since the x-axis is in log scale, we put the probability mass for x = at x = .
.
We see that the median DAG has a depth of ve.
To compare, a map-reduce job has depth of one.
A quarter of the DAGs have depth above ten.While of the DAGs are trees (i.e., no cycle aer ignoring the direction of dependency), we see that many have cycles (half of the DAGs have at least cycles); the average number of tasks in a cycle is (not shown in the e DAGs can be quite large; the median job has thousands of tasks and tens of stages.
To compare, a mapreduce job has two stages.We also see that most of the edges are not barriers (e.g., those labeled "can be local").
Note the gap between the orange stars line and the black squares line in Figure which correspond to counts of all edges and barriers respectively.
A barrier edge indicates that every task in the parent stage should nish before any task in the child stage begins.We observe that DAGs can be cut into portions such that all tasks aer the cut can only begin aer every task before the cut has nished.
An example cut is shown with a red dashed line on the DAG in Figure (le bottom).
Cuts oen arise because a dataset, perhaps newly generated by upstream tasks, has to be partitioned before downstream tasks can begin.
Cuts are convenient because the optimal schedule for the DAG is a concatenation of the optimal schedules of the cut portions of that DAG.
We observe that of the production DAGs can be split into four or more parts.
e median (th percentile) task in-degree and outdegree are () and () respectively.
For a map-reduce job with m mappers and r reducers, the median in-degree will be if m ≥ r and m otherwise.
e larger out-degree is because stages that read from the le-system are data reductive; hence, the query optimizer creates fewer downstream tasks overall.Overall, we conclude that DAGs are both large and have complex structures.
Similar to prior work [, ], we observed substantial variability in the usage of various resources; the details are in [].
Potential for improvement: To quantify potential gains, we compare the runtime of production DAGs to two measures.
e rst, CPLength, is the duration of the DAG's critical path.
If the available parallelism is innite, the DAG would nish within CPLength.
e second, TWork, is the total work in the DAG normalized by the cluster share of that DAG (a formula is in Table .)
If there were no dependencies and perfect packing, a DAG would nish within TWork.
Figure plots a CDF of the relative gap between the runtime of a DAG and these measures.
Half of the jobs have a gap of over for both CPLength and TWork.
Understanding the gap: A careful reader would notice that about of the DAGs nish faster than some measures.
is is because our production scheduler occasionally gives jobs more than their fair share if the cluster has spare resources; hence, measures which assume that the cluster share will be the minimum guaranteed for the DAG can be larger than the actual completion time.
We will ignore such DAGs for this analysis.Suppose OPT is the optimal completion time for a DAG given a certain cluster share.
We know that actual runtime is larger than OPT and that the above measures are smaller than OPT.
Now, the gap could be due to one of two reasons.
(() e measure is loose (i.e., well below OPT).
In practice, we found this to be the case because CPLength ignores all the work o the critical path and TWork ignores dependencies.
() e observed runtimes of DAGs are innated by runtime artifacts such as task failures, stragglers and performance interference from other cluster activity [, ].
To correct for (), we discount the eeects of runtime artifacts on the above computed DAG runtime as follows.
First, we chose the fastest completion time from a group of recurring jobs.
It is unlikely that every execution suffers from failures.
Second, to correct for stragglers-one or a few tasks holding up job progress-we deduct from completion time the periods when the job ran fewer than ten concurrent tasks.
Note that both these changes reduce the gap; hence they under-estimate the potential gain.Further, to correct for ((), we develop a new improved lower bound NewLB that uses the specic structure of data-parallel DAGs.
Further details are in []; but intuitively NewLB leverages the fact that all the tasks in a job stage (e.g., a map or reduce or join) have similar dependencies, durations and resource needs.
e gap relative to NewLB is smaller, indicating that the newer bound is tighter, but the gap is still over for half of the jobs.
at is, they take over two times longer than they could.To summarize, (() production jobs have large DAGs that are neither a bunch of unrelated stages nor a chain of stages, and () a packing+dependency-aware scheduler can oer substantial improvements.
Lemma (Dependencies).
Any scheduling algorithm, deterministic or randomized, that does not account for the DAG structure (e.g., only schedules currently runnable tasks) is Ω(d) times OPT where d is the number of resources.
e proof, for deterministic algorithms, follows from designing an adversarial DAG for any scheduler [].
We extend this to randomized algorithms by using Yao's minimax principle [].
Lemma applies to the following multi-resource packers [, , , ] since they ignore dependencies.Lemma (Resource Variation).
Schedulers that ignore resource heterogeneity have poor worst-case performance.
For example, critical path scheduling can be Ω(n) times OPT where n is the number of tasks in a DAG.
Combining these two principles, we conjecture that it is possible to nd similar examples for any scheduler that ignores dependencies or ignores resource usages.To place these results in context, note that d is about (cores, memory, network, disk) and can be larger when tasks require resources at other servers or on many network links.
Further, the median DAG has hundreds of tasks (n).
e key intuition here is that DAGs are hard to schedule because of their complex structure and because of discretization issues when tasks use multiple resources (fragmentation, task placement etc.) G is close to OPT on all of the described examples and is within .
times OPT for half of the production DAGs (see §).
Acquiring an annotated DAG is non-trivial.
Much prior work has similar requirements as G (see Table in []).
ere are two parts to this: the structure of the DAG and the task proles (resource needs and durations).
DAG structure: In order to launch a task only aer parent tasks nish, every DAG scheduler is aware of the DAG structure.
Furthermore, the DAG is oen known before the job starts.
Runtime changes to the DAG, if they happen, only aect small portions of a DAG.
For example, our scheduler adds an aggregation tree in front of a reduce stage depending upon runtime conditions.
Task resource demands and durations: G requires each task to be annotated with the demands for any resource that could be congested; the other resources do not aect scheduling.
Here, we consider four resources (cores, memory, disk and network bandwidth).
12th USENIX Symposium on Operating Systems Design and Implementation 85 Schedulers such as Yarn, Mesos, Hive and Spark ask users to annotate their tasks with cores and memory requirements; for example, [ core, GB] is the default in Hadoop .
.
G requires annotations for more resources as well as the durations of tasks.
ere are some early eeorts to obtain these proles (tasks' demands and durations) automatically.
For example, in the production cluster at Microso, up to of the resources in the examined cluster are used by recurring jobs; the same script executes periodically on newly arriving data.
Recurring jobs can be identied based on the job name (e.g., LogMiner date [ time]) and prior work shows that the task proles of these jobs can be estimated from history (aer normalizing for the size of input Novel ideas in G Cluster scheduling is the problem of matching tasks to machines.
Most production schedulers today do so in an online manner and have very tight timing constraints since clusters have thousands of servers, many jobs that each have many pending tasks and tasks that nish in seconds or less [, ].
Given such stringent time budget, carefully considering large DAGs seems daunting.As noted in §, a key design decision in G is to divide this problem into two parts.
An oine component constructs careful schedules for a single DAG.
We call these the preferred schedules.
A second online component enforces the preferred schedules of the various jobs running in the cluster.
We elaborate on each of these parts below.
Figure shows an example of how the two parts may inter-operate in a YARN-style architecture.
Dividing a complex problem into parts and independently solving each part oen leads to a sub-optimal solution.
While we have no guarantees for our particular division, we note that it scales to large clusters and outperforms the stateof-art in experiments.To nd a compact schedule for a single DAG, our idea is to place the troublesome tasks, i.e. those that can lead to a poor schedule, rst onto a virtual space.
Intuitively, this maximizes the likelihood that any holes, un-used parts C(s, G), P(s, G), D(s, G), A(s, G), U(s, G)Children, parents, descendants, ancestors and unordered neighbors of s in G; note that of the resource-time space, can be lled by other tasks.
However, nding the best choice of troublesome tasks is as hard as nding a good schedule for the DAG.
We use an eecient search strategy that mimics dynamic programming: it picks subsets that are more likely to be useful and avoids redundant exploration.
Furthermore, placing troublesome tasks rst can lead to dead-ends.
We deene dead-end to be an arrangement of a subset of the DAG in the virtual space on which the remaining tasks cannot be placed without violating dependencies.
Our strategy is to divide the DAG into subsets of tasks and place one subset at a time.
While intra-subset dependencies are handled directly during schedule construction, intersubset dependencies are handled by restricting the order in which the various subsets are placed.
We prove that the resultant placement has no dead-ends.
e online component has to co-ordinate between some potentially discordant directives.
Each job running in the cluster oers a preferred schedule for its tasks (constructed as above).
Fairness models such as DRF may dictate which job (or queue) should be served next.
e set of tasks that is advantageous for packing (e.g., maximal use of multiple resources) can be dierent from both the above choices.
We oer a simple method to reconcile these various directives.
Our idea is to compute a realvalued score for each pending task that incorporates the above aspects soly.
at is, the score trades-o violations on some directives if the other directives weigh strongly against it.
For example, we can pick a task that is less useful from a packing perspective if it appears much earlier on the preferred schedule.
One key novel aspect is bounding the extent of unfairness.
e oine component of G is described next; the online component is described in Section .
U(s, G) = V − A(s, G) − D(s, G) − {s} G builds the schedule for a DAG in three steps.
Second, tasks in a subset are packed greedily onto the virtual space while respecting dependencies ( §.)
.
ird, G carefully restricts the order in which dierent Func: BuildSchedule: Input: G: a DAG, m: number of machines Output: An ordered list of tasks t ∈ G S best ← ∅ // best schedule for G thus far foreach sets {T, S, P, C} ∈ CandidateTroublesomeTasks(G) do Space S ← CreateSpace(m) //resource-time space S ← PlaceTasks(T, S , G) // trouble goes first S ← TrySubsetOrders({SCP, SPC, CSP, PSC}, S , G) if S < S best then S best ← S //keep the best schedule; return OrderTasks(G, S best )Figure : Pseudocode for constructing the schedule for a DAG.
Helper methods are in Figure .
subsets are placed such that the troublesome tasks go rst and there are no dead-ends ( §.)
.
G picks the most compact schedule aer iterating over many choices for troublesome tasks.
e resulting schedule is passed on to the online component ( §).
To identify troublesome tasks, G computes two scores per task.
e rst, LongScore, divides the task duration by the maximum across all tasks.
Tasks with a higher score are more likely to be on the critical path and can beneet from being placed rst because other work can overlap with them.
e second, FragScore, reects the packability of tasks in a stage (e.g., a map or a reduce).
It is computed by dividing the total work in a stage (TWork deened in Table ) by the time a greedy packer takes to schedule that stage.
Tasks that are more dicult to pack would have a lower FragScore.
Given thresholds l and f , G picks tasks with LongScore ≥ l or FragScore ≤ f .
Intuitively, this biases towards selecting tasks that are more likely to hurt the schedule because they are long or dicult to pack.
Each value of {l , f } leads to a choice of troublesome tasks T which leads to a schedule (aer placing the tasks in T rst and then the other subsets); G iterates over dierent values for the l and f thresholds and picks the most compact schedule.To speed up this search, (() rather than choose the threshold values arbitrarily, G picks values that are discriminative, i.e. those that lead to dierent choices of troublesome tasks, and () G remembers the set of troublesome tasks that were already explored (by previous settings of the thresholds) so that only one schedule is built for each troublesome set.
Note also that the dierent choices of troublesome tasks can be explored in parallel.
Further improvements are in §...As shown in Figure (line ), the set T is a closure over the chosen troublesome tasks.
at is, T contains the troublesome tasks and all tasks that lie on a path in the DAG between two troublesome tasks.
e parent and child subsets P, C consist of tasks that are not in T but have a descendant or ancestor in T respectively.
e subset S consists of the remaining tasks.
Input: DAG G; Output: list L of sets T, S, P, C // choose a candidate set of troublesome tasks; per choice, divide G into four setsL ← ∅ ∀v ∈ G, LongScore(v) ← v .
durationmax v ′ ∈G v ′ .
duration ∀v ∈ G, v in stage s, FragScore(v) ← TWork(s))ExecTime(s) foreach l ∈ δ , δ, . . . do foreach f ∈ δ, δ, . . . do T ← {v ∈ GLongScore(v) ≥ l or FragScore(v) ≤ f } T ← Closure(T)if T ∈ L then continue // ignore duplicates;P ← v∈T A(v , G); C ← v∈T D(v , G); S ← V − T − P − C;L ← L ∪ {T, S, P, C} Given a subset of tasks and a partially occupied space, how best to pack the tasks while respecting dependencies?
G uses the following logic for each of the subsets T, P, S and C.
One can choose to place the parents rst or the children rst.
We call these the forward and backward placements respectively.
More formally, the forward placement recursively picks a task all of whose ancestors have already been placed on the space and puts it at the earliest possible time aer its latest nishing ancestor.
e backward placement is analogously deened.
Intuitively, both placements respect dependencies but can lead to different schedules since greedy packing yields dierent results based on the order in which tasks are placed.
Traversing the tasks in either placement has n log n complexity for a subset of n tasks and if there are m machines, placing tasks greedily has n log(mn) complexity.
For each division of DAG into subsets T, S, P, C, G considers these orders: TSCP, TSPC, TPSC or TCSP.
at is, in the TSCP order, it rst places all tasks in T, then tasks in S, then tasks in C and nally all tasks in P. Intuitively, this helps because the troublesome tasks T are always placed rst.
Further, other orders may lead to dead-ends.
For example, consider the order TPCS; by the time some task t in the subset S is considered for placement, parents of t and children of t may already have been placed since they may belong to the sets P and C respectively.
Hence, it may be impossible to place t without violating dependencies.
We prove that the above orders avoid dead-ends and are the only orders beginning with T to do so.Note also that only one of the forwards or backwards placements (described in §.)
are appropriate for some subsets of tasks.
For example, tasks in P cannot be placed forwards since some descendants of these tasks may already have been placed (such as those in T).
As noted above, the forwards placement places a task aer its last nishing ancestor but ignores descendants and can hence violate dependencies if used for P; because by deenition Func: PlaceTasks(V , S , G): Inputs: V: subset of tasks to be placed, S: space (partially filled), G: a DAG Output: a new space with tasks in V placed atop S return min (PlaceTasksF(V , S , G), PlaceTasksB(V , S , G)) Func: PlaceTasksF: // forwards placement, inputs and outputs same as PlaceTasks S ← Clone(S) finished placement set F ← {v ∈ Gv already placed in S} while true doready set R ← {v ∈ V − F P(v , G) ⊆ F} if R = ∅ then break // all done; v ′ ← task in R with longest runtime t ← max v∈P(v ,G) EndTime(v , S)// place v ′ at earliest time ≥ t when its resource needs can be metF ← F ∪ v ′ Func:PlaceTasksB: // only backwards, analogous to PlaceTasksF.
Input: G: a DAG, S in : space with tasks in T already placed Output: A space that has the most compact placement of all tasks.
S , S , S , S ← Clone(S in ) return min( // pick the most compact among all feasible orders PlaceTasksF(C, PlaceTasksB(P, PlaceTasks(S, S , G), G), G), //SPC PlaceTasksB(P, PlaceTasksF(C, PlaceTasks(S, S , G), G), G), //SCP PlaceTasksB(P, PlaceTasksB(S, PlaceTasksF(C, S , G), G), G), //CSP PlaceTasksF(C, PlaceTasksF(S, PlaceTasksB(P, S , G), G), G) //PSC );Figure : Pseudocode for the description in §.
, §.
.
every task in the parent subset P has at least one descendant task.
Analogously, tasks in C cannot be placed backwards.
Tasks in S can be placed in one or both placements, depending on the inter-subset order.
Finally, since the tasks in T are placed onto an empty space they can be placed either forwards or backwards; details are in Fig Intuitively, the proof (omitted for space) follows from (() all four subsets are closed and hence intra-subset dependencies are respected by the placement logic in §.
whether in the forward or in the backward placement, () the inter-subset orders and the corresponding restrictions to only use forwards and/or backwards placements specied in §.
ensure that dependencies across subsets are respected and, () every other order that begins with T can lead to dead-ends.
We note a few enhancements.
First, as noted in §.
, it is possible to partition a DAG into parts that are totally ordered.
Hence, any schedule for the DAG is a concatenation of per-partition schedules.
is lowers the complexity of schedule construction.
of the production DAGs can be split into four or more parts.
Second, and along similar lines, whenever possible we reduce complexity by reasoning over stages.
Stages are collections of tasks and are to times fewer in number than tasks.
ird, schedule computation can be sped up in a few ways.
Parallelizing the search will help the most, i.e. examine dierent choices for troublesome tasks T in parallel.
Working over more compact representations (e.g., scaling down the DAG and the cluster by a corresponding amount) will also help.
Fourth, jobs that are short-lived, or only use a small amount of resources, or do not have complex DAG structures, will bypass the oine portion of G. Fiih, the complexity of schedule construction is independent of the sizes of the subsets T, S, P, C that G divides the DAG into.
However, if T is very large, the approach of placing troublesome tasks rst and other tasks carefully around them is unlikely to help.
We prune such choices of T without further exploration.
Among the schedules built by G for production DAGs, the median DAG has of its tasks considered troublesome; these tasks contribute to of the work in that job.
Finally, note that it is possible to recursively employ this logic: i.e., given a DAG G, pick a troublesome subset T, let G ′ be the sub-DAG over tasks in T, repeat the logic on G ′ .
We defer further examination of this approach to future work.
Given the preferred schedules for each job, we describe how the G inter-job scheduler matches tasks to machines online.
Recall the example in Figure .
e scheduling procedure is triggered when a machine m reports its vector of available resources to the cluster-wide resource manager.
Given a set of runnable jobs (and their tasks), the scheduler returns a list of tasks to be allocated on that machine.
e challenge is to enforce the per-job order computed in § while also packing tasks for cluster eeciency, ensuring low JCTs, and enforcing fairness.
Enforcing preferred schedules.
Using the per-DAG schedule constructed in §, a t pr i S core is computed for each task t by (() ranking tasks in increasing order of their start time in the schedule and () dividing the rank by the number of tasks in the DAG so that the result is between (for the task that begins rst) and .
As noted below, G preferentially schedules tasks with a higher t pr i S core value rst.
Packing eciency.
G borrows ideas from [] to improve packing eeciency.
For every task, it computes a packing score pScore t as a dot product between the task demand vector and the machine's available resource vector.
To favor local placement, when remote resources are needed, pScore t is reduced by multiplying with a remote penalty rp (∈ [, ]).
Sensitivity analysis on the value of rp is in §... Job completion time.
G estimates the remaining work in a job j similar to []; srpt j is a sum over the remaining tasks to schedule in j, the product of their duration and resource demands.
A lower score implies less work remaining in the job j. Bounding unfairness.
G trades o fairness for better performance while ensuring that the maximum unfairness is below an operator congured threshold.
Specically, G maintains deecit counters [] across jobs to measure unfairness.
e deecit counters are updated as follows.
When a task t from a group g is scheduled, its deecit increases by factor t × (fairShare g − ) and the deecit of all the other groups g ′ increases by factor t × fairShare g ′ .
is update lowers the deecit counter of g proportional to the resources allocated to it and increases the deecit counters of other groups to remember that they were treated unfairly.
Further, by varying the value of factor t , G can support dierent fairness schemes: e.g., factor t = mimics slot fairness and factor t = demand of the dominant resource of g mimics DRF [].
Combining schedule order, packing, completion time and fairness.
G attempts to simultaneously consider the above four aspects; as shown in Figure , some of the aspects vary with the task while others vary across jobs.
First, G combines the performance related aspects into a single score, i.e., perfScore t = pScore t ⋅ t pr i S core − ηsrpt j .
η is a parameter that is automatically updated based on the average srpt and pScore across jobs and tasks.
Subtracting η ⋅ srpt j prefers shorter jobs.
Sensitivity analysis on the value of η is in §... Intuitively, the combined value perfScore t soly enforces the various objectives.
For example, if a task t is preferred by all individual objectives (belongs to shortest job, is most packable, is next in the preferred schedule), then it will have the highest perfScore t .
When the objectives are discordant, colloquially, the task preferred by a majority of objectives t will have the highest perfScore t .
Next, to trade-o performance while bounding unfairness, let the most unfairly treated group (the one with the highest deecit counter) be g unfair .
If the deecit counter of g unfair is below the unfairness threshold, then G picks the task with the maximum perfScore from among all groups; else it picks the task with the maximum perfScore from g unfair .
e unfairness threshold is κC where κ (< ) is a tunable parameter and C is the cluster capacity.Further details, including a pseudo-code, are in [].
We have implemented the runtime component ( §) in the Apache YARN resource manager (RM) and the schedule constructor ( §) in the Apache Tez application master (AM).
Our (unoptimized) schedule constructor nishes in tens of seconds on the DAGs used in experiments; this is in the same ballpark as the time to compile and query-optimize these DAGs.
Recall from §.
.
that G requires a more detailed annotation of DAGs than existing systems: specically, it needs task durations and estimates of network and disk usages; the usages of cores and memory are already available [, , ].
Our approach is to construct estimates for the average task in each stage using a combination of historical data and prediction.
ese estimates are used by the ofine portion of G ( §).
As noticed by prior work, recurring jobs are common in our production clusters and historical usages, aer normalizing for the change in data volume, are predictive for such job groups [].
e online portion of G ( §) reenes these estimates based on the actual work of a task (e.g., by noting its input size) and based on the executions of earlier tasks; since (a) tasks in the same stage oen run in multiple waves due to capacity limits and (b) running tasks issue periodic progress reports [, ].
In our evaluation, we execute the jobs once and use the actual observed usage (from job history) to compute the necessary annotations.
We normalize both the duration and usage estimates by the tasks' input size, as appropriate.
A sensitivity analysis that introduces dierent amounts of error to the estimates and shows their eeect on performance is in §...We observe that G is rather robust to estimation error because relatively small dierences in tasks' duration and usages do not change the schedule.
For example, while it is useful to know that reduce and join tasks are network-heavy as opposed to map tasks which have no network usage, it is less useful to know precisely how much network usage a reducer or a join task will have; the actual usage would vary, at runtime, in any case due to contention, thread or process scheduling, etc.
Similarly, while it is useful to know that tasks in a certain stage will take ten times longer, on average, and hence it is better to overlap those tasks with unrelated work, it is less useful to know the exact duration of a task; again, the exact durations will vary because of contention, machine-specic slowdowns etc. [].
Naively implementing our runtime component ( §) would improve schedule quality at the cost of delaying scheduling.
We use bundling to oset this issue.Some background: e matching logic in typical schedulers is heartbeat-based [].
When a machine heartbeats to the RM, the allocator () maintains an ordering over pending tasks, (() picks the rst appropriate task to allocate to that machine, () adjusts its data structures (such as, resorting/rescoring) and () repeats these steps until all resources on the node have been allocated or all allocation requests have been satised.A naive implementation of the runtime component would examine all the pending tasks; thereby increasing the time to match.Instead, we propose to bundle the allocations.
Specically, rather than breaking the loop aer nding the rst schedulable task (step above), we keep along a bundle of tasks that can all be potentially scheduled on the machine.
At the end of one pass, we assign multiple tasks by choosing from among those in the bundle.
e bundle amortizes the cost of examining the pending tasks.
We can allocate multiple tasks in one pass as opposed to one pass per task.
It is also easy to see that bundling admits non-greedy choices and that the pass can be terminated early when the bundle has good-enough tasks.
We have refactored the Yarn scheduler with congurable choices for (() which tasks to add to the bundle, () when to terminate bundling and () which tasks to pick from the bundle.
From conversations with Hadoop committers, these code-changes help improve matching eeciency and code readability.
We note that a cluster scheduler performs other roles besides matching tasks to machines.
Several of these roles such as handling outliers and failed tasks dierently [, ], delay scheduling [], reservations [, ] or supporting heterogeneous clusters where only some servers may have GPUs [] are implemented as preconditions to the main schedule loop, i.e. they are checked rst, or are implemented by partitioning the tasks that will be considered in the scheduling loop.
Since G's changes only aect the inner core of the schedule loop (e.g., given a set of pending tasks, which subset to allocate to a machine), our implementation co-exists with these features.
Our key evaluation results are as follows.
(() In experiments on a server cluster, relative to Tez jobs running on YARN, G improves completion time of half of the jobs by to across various benchmarks.
of the jobs improve by to .
e extent of gains depends on the workload (complexity of DAGs, resource usage variations etc.).
() On over , DAGs from production clusters, the schedules constructed by G are faster by for half of the DAGs.
A quarter of the DAGs improve by .
Further, by comparing with our new lower bound, these schedules are optimal for of the jobs and within of optimal for of the jobs.
() By examining further details, we show that the gains are from better packing dependent tasks.
Makespan (and cluster throughput) improve by a similar amount.
More resources are used, on average, by G and trading o short-term unfairness improves performance.
(() We also compare with several alternative schedulers and oer a sensitivity analysis to cluster load, various parameter choices, and annotation errors.
Our experimental cluster has servers with two quadcore Intel E processors (hyperthreading enabled), GB RAM, drives, and a Gbps network interface.
e network has a congestion-free core [].
Hive jobs (E-Hive).
We also use K DAGs from a private production system in our simulations.
In each experimental run, job arrival is modeled as a Poisson process with average inter-arrival time of s for minutes.
Each job is picked at random from the corresponding benchmark.
We built representative inputs and varied input size from GBs to tens of TBs such that the average query completes in a few minutes and the longest query nishes in under ten minutes on the idle cluster.
A typical experiment run has about jobs.
e results presented are the median over three runs.
We experimentally compare G with the following baselines: (() Tez breadthrst order of tasks in the DAG running atop YARN's Capacity Scheduler (CS), () Tez + CP critical path length based order of tasks in the DAG atop CS and () Tez + Tetris breadth-rst order of tasks in the DAG atop Tetris [].
To tease apart the gains from the ofine and online components, we also oer results for (() Tez + G + CS and (() Tez + G + Tetris which use the ofine constructed schedules at the job manager (to request containers in that order) but the online components are agnostic to the desired schedule (either the default capacity scheduler or Tetris respectively).
Using simulations, we also compare G against the following schemes: () BFS breadth rst order, () CP critical path order, () Random order, (() StripPart [], (() All of the above schemes except (() are workconserving.
()-() and (() pick only from among the runnable tasks but vary in the specic heuristic.
(() and ((() perform more complex schedule construction, as we will discuss later.
Metrics: Improvement in JCT is our key metric.
Between two schemes, we measure the normalized gap in JCTs.
at is, the dierence in the runtime of a job divided by the job runtime; the normalization lets us compare jobs with very dierent runtimes.
We also measure makespan, i.e., the time to nish a given set of jobs, Jain's fairness index [], and the actual usages of various resources in the cluster.
(Figure a).
e other schemes take over longer.
G runs more tasks by reducing fragmentation and by overbooking resources such as network and disk that do not lose goodput when demand exceeds capacity (unlike say memory).
Comparing Figure b with Figures c, d, the average allocation of all resources is higher with G. Occasionally, G allocates over of the network and disk.
One caveat about our measurement methodology here: we take the peak usage of a task and assume that the task held on to those resources for the entirety of its lifetime; hence, the usages are over-estimates for all schemes.
Tez + Tetris, the closest alternative, has fewer tasks running at all times because (a) it does not overbook (resource usages are below in Figure c) and (b) it has a worse global packing for a DAG because it ignores dependencies and packs only the runnable tasks.
Tez + CP is impacted negatively by two eeects: (a) ignoring disk and network usage leads to arbitrary over-allocation (the "total" resource usage is higher because, due to saturation, tasks hold on to allocations for longer) and (b) due to fragmentation, many fewer tasks run on average.
Overall, G gains by increasing the task throughput.
Makespan: To evaluate makespan, we make one change to the experiment setup-all jobs arrive within the rst few minutes.
Everything else remains the same.
Table shows the gap in makespan for dierent cases.
Due to careful packing, G sustains high cluster resource utilization which in turn enables jobs to nish quickly: makespan improves relative to Tez and over relative to alternatives.
Fairness: Can we improve performance while also being fair?
Intuitively, fairness may hurt performance since fairly dividing resources may lower overall utilization or slow-down some jobs.
To evaluate fairness, we make one change to the experiment set up.
e jobs are evenly and randomly distributed among two queues and the scheduler has to divide resources evenly.
Table reports the gap in performance (median JCT) for each scheme when run with two queues vs. one.
Tez, Tez + DRF and Tez + Tetris lose over in performance relative to their one queue counterparts.
e table shows that with two queues, G has a small gain (perhaps due to experimental noise).
Hence, relatively, G performs even better than the alternatives if given more queues.
But why?
Table also shows Jain's fairness index computed over s, s and s windows.
We see that G is less fair at short timescales but is indistinguishable at larger time windows.
is is because G bounds unfairness ( §); it leverages short-term slack from precise fairness to make scheduling choices that improve performance.
Value of enforcing preferred schedules online: Recall that G's online component enforces the preferred schedules constructed by the oine component.
To tease apart the value of this combination, we consider alternatives wherein the job managers use the preferred schedules (to request containers in that order) but the cluster scheduler is agnostic; i.e. it simply runs the default capacity scheduler or Tetris (we call these Tez + G + CS and Tez + G + Tetris respectively).
We nd that G oers and better median JCT compared to Tez + G + Tetris and Tez + G + CS.
is experiment was conducted on a smaller server cluster with dierent hardware so these numbers are not directly comparable with the remaining experiments; we oer them merely as G T+C T+T G T+C T+T TPC-DS . . . . . .
TPC-H . . . . . .
BigBench . . . . . .
E-Hive . . . . . .
We use simulations to compare a wider set of algorithms ( §.
.)
on the much larger DAGs that ran in the production clusters.
We mimic the actual dependencies, task durations and resource needs from the cluster.
Figure compares the schedules constructed by G with the schedules from other algorithms.
Table reads out the gaps at various percentiles.
We observe that G's gains at the end of schedule construction are about the same as those at runtime (Figure ).
is is interesting because the runtime component only soly enforces the desired schedules from all the jobs running simultaneously in the cluster.
It appears that any loss in performance from not adhering to the desired schedule is made up by the gains from better packing (across DAGs) and trading o some short-term unfairness.
Second, G's gains are considerable compared to the alternatives.
CP and Tetris are the closest.
e reason is that G looks at the entire DAG and places the troublesome tasks rst, leading to a more compact schedule overall.ird, when tasks have unit durations and nicely shaped demands, CG (Coman-Graham []) is at most times optimal.
However, it does not perform well on the heterogeneous DAGs seen in production.
Some recent extensions of CG to handle heterogeneity ignore fragmentation when resources are divided across machines [].
Fourth, StripPart [] combines resource packing and task dependencies and has the best-known approximation ratio: O(log n) on a DAG with n tasks [].
e key idea is to partition tasks into levels such that all depen-92 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association Impact of misestimations: We oer to each scheduler an inaccurate task duration and resource usage vector but have the underlying execution use the true values.
Hence, the schedulers match tasks to machine based on imperfect estimates.
Once scheduled, the tasks may nish aer a dierent duration or use dierent amounts of resources.
When the total resource demand crosses machine capacity, we delay the completion of tasks further by a proportional amount.
Figure shows a CDF of the change in the completion time of the production DAGs for dierent schedulers.
Each line denotes a different amount of error.
For example, the red triangle line labeled [−.
−.]
corresponds to picking a random number in that range for each stage and then changing the task durations and resource needs fractionally by that random number (−.
indicates a lower value).
We see that the impact of mis-estimates is rather small; G changes roughly similarly to the other schedulers.
Under-estimates tend to speed up the job because the scheduler over-allocates tasks but over-allocation can also slow-down jobs.
Over-estimates delay jobs because the scheduler wastes resources; it may refrain from allocating a task when its needs appear larger than the available resources at a machine.
Overall, G appears robust to mis-estimations.
We evaluate G's eeectiveness in scheduling DAGs that arise in distributed compilation jobs [, , ] and Internet service workows [].
Distributed build systems speed up the compilation of large code bases [, ].
Each build is a DAG with de- pendencies between the various tasks (compilation, linking, test, code analysis).
e tasks have dierent runtimes and dierent resource proles.
Figure a shows that G is () faster than Tetris (CP) when scheduling the build DAGs from a production distributed build system [].
Each bar is centered on the median gain for DAGs of a certain size; the error bars are quartiles.
We also examine the DAGs that arise in datacenterside workows for Internet-services [].
For instance, a search query translates into a workow of dependent RPCs at the datacenter (e.g., spell check before index lookup, video and image lookup in parallel).
e RPCs use dierent resources, have dierent runtimes and often run on the same server pool [].
Over several workows from a production service, Figure b shows that G improves upon alternatives by about .
ese encouraging early results hint that G may be more broadly useful.
To structure the discussion, we ask four questions: (Q) does a scheme consider both packing and dependencies, (QQ) does it make realistic assumptions, (QQ) is it practical to implement in cluster schedulers and, (Q) does it consider multiple objectives such as fairness?
G is unique in positively answering these four questions.
Q1 NO.
Substantial prior work ignores dependencies but packs tasks with varying demands for multiple resources [, , , , ].
e best results are when the demand vectors are small [].
Other work considers dependencies but assumes homogeneous demands [, ].
A recent multi-resource packing scheme, Tetris [], succeeds on the three other questions but does not handle dependencies.
Hence, we saw in § that Tetris performs poorly when scheduling DAGs (can be up to d times o, see []).
Tetris can also be arbitrarily unfair.
Q1 YES, Q2 NO.
e packing+dependencies problem has been considered at length under job-shop scheduling [, , , ].
Most results assume knowledge of job arrival times and proles [].
For the case with unknown future job arrivals (the version considered here), no algorithms with bounded competitive ratios are known [, ].
Some notable work assumes only two resources [], applies for a chain but not a general DAG [] or assumes one cluster-wide resource pool [].
Q3 NO.
Several of the schemes listed above are complex and hence do not meet the tight timing requirements of cluster schedulers.
VM allocators [] also consider multi-resource packing.
However, cluster schedulers have to support roughly two to three orders of magnitude higher rate of allocation (tasks are more numerous than VMs).
We note that these fairness schemes neither pack nor are DAG-aware.
G can incorporate these fairness methods as one of the multiple objectives and trades o bounded unfairness for performance.
DAGs are a common scheduling abstraction.
However, we found that existing algorithms make key assumptions that do not hold in the case of cluster schedulers.
Our scheduler, G, is an eecient online solution that scales to large clusters.
We experimentally validated that it substantially improves the scheduling of DAGs in both synthetic and emulated production traces.
e core technical contributions are: (() construct a good schedule for a DAG by placing tasks out-of-order on to a virtual resource-time space, and () use an online heuristic to soly enforce the desired schedules and simultaneously manage other concerns such as packing and fairness.
Much of these innovations use the fact that job DAGs consist of groups of tasks (in each stage) that have similar durations, resource needs, and dependencies.
We intend to contribute our G implementation to Apache YARN/Tez projects.
For early discussions, we would like to thank Ganesh Ananthanaryanan and Peter Bodik.
For feedback that helped improve this paper, we thank the anonymous reviewers, our shepherd Phil Levis, Mohammad Alizadeh, Chris Douglas, Hongzi Mao, Ishai Menache and Malte Schwarzkopf.
For operating the cluster that motivated and inspired this work, we thank the Cosmos/ SCOPE production team at Microso.
