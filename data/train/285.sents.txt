We present and evaluate a new, simple, pipelined algorithm for large, irregular all-gather problems, useful for the implementation of the MPI Allgatherv collective operation of MPI.
The algorithm can be viewed as an adaptation of a linear ring algorithm for regular all-gather problems for single-ported, clustered multiprocessors to the irregular problem.
Compared to the standard ring algorithm, whose performance is dominated by the largest data size broadcast by a process (times the number of processes), the performance of the new algorithm depends only on the total amount of data over all processes.
The new algorithm has been implemented within different MPI libraries.
Benchmark results on NEC SX-8, Linux clusters with InfiniBand and Gigabit Ethernet, Blue Gene/P, and SiCortex systems show huge performance gains in accordance with the expected behavior.
The all-gather problem is a basic collective communication operation, in which each participant of a predefined group wants to broadcast personal data to all other group members.
In the MPI standard, this functionality is embodied in the regular MPI Allgather collective, in which each process contributes the same amount of data, and in the irregular MPI Allgatherv collective, where the amount of data can be freely chosen for the different processes [8].
For both MPI collectives, all participating processes know the sizes of the data to be broadcast by all other processes.
The irregular all-gather operation is used for instance in linear algebra kernels for matrix multiplication and LU factorization [1].
The regular all-gather problem has been intensively studied (theoretically under the term gossiping, but is also known as broadcast-to-all, all-to-all-broadcast, as well as many other names) [5,6], and many algorithms have been proposed and/or implemented as part of MPI libraries for various systems and communication models [1-3, 7, 9, 10].
The more challenging, irregular all-gather problem has received much less attention, and MPI libraries typically use the same algorithm for both MPI Allgather and MPI Allgatherv.
For irregular problems with considerable differences between the amount of data contributed by the processes, this can have huge performance drawbacks.
For extreme cases, the resulting performance loss can amount to orders of magnitude (cf. Section 3).
In this paper, we present an algorithm for large, irregular all-gather problems.
The underlying idea is quite simple and can be viewed as an adaptation to the irregular problem of a ring-based algorithm for regular all-gather problems for single-ported, clustered multiprocessors.
The algorithm has been implemented for several MPI libraries, and evaluated on diverse systems, namely NEC SX-8, two Linux clusters, IBM Blue Gene/P, and SiCortex 5832.
We demonstrate significant performance improvements over a standard MPI Allgatherv algorithm, depending on the amount of irregularity in the benchmark scenarios.
In the following, p is the number of participating (MPI) processes, numbered consecutively from 0 to p − 1.
We let m i denote the size of the data contributed by process i, and m = p−1 i=0 m i the total amount of data that eventually has to be gathered by all processes.
For large data, we assume that the time for transmitting a message of size m ′ is simply O(m ′ ).
For most of the following discussion, a detailed communication cost model is unnecessary.
A basic (folklore) algorithm for large, regular all-gather problems is the linear ring.
The algorithm performs p − 1 communication rounds.
In each round process i sends (starting with its own data) an already known block of data of size m ′ to process (i + 1) mod p and receives an unknown block of data from process (i − 1) mod p. For regular problems where all blocks are of the same size m i = m ′ , the completion time of the ring algorithm is O((p−1)m ′ ) = O(m−m ′ ).
The number of communication start-ups (latency) scales linearly with p.
This is unproblematic for large m ′ , but for small problems, an algorithm with a logarithmic number of start-ups is clearly preferable [1,3,10].
The linear ring algorithm is straightforward to implement.
For systems with single-ported, bidirectional communication capabilities (where each process can at the same time send data to another process and receive data from a possibly different process) it can use the system communication bandwidth to full capacity.
For irregular all-gather problems, where the data sizes m i can vary arbitrarily over the processes, the algorithm can however perform poorly.
The running time is determined by the largest amount of data m ′ = max p−1 i=0 m i , which has to be sent along the ring in each round, and is therefore O((p − 1)m ′ ).
In particular, (p − 1)m ′ can be much larger than the total amount of data m. Node j Node j − 1 Fig. 1.
The linear ring algorithm on a cluster of SMP nodes with different number of MPI processes per node.
The processes are (virtually) ranked such that one process at each node receives data from another node, and one process sends data to another node in each round.
We first observe that the linear ring algorithm can also be used for the regular all-gather problems on clustered multiprocessors (like clusters of SMP nodes) with a single-ported communication network.
In that case the ring is organized such that exactly one process i per SMP node has its predecessor (i − 1) mod p on another SMP node, and exactly one process j per SMP node has its successor (j + 1) mod p on another node.
To accomplish this, a (virtual) reranking of the MPI processes might be necessary.
The clustered, linear ring algorithm is now communication-bandwidth optimal, because in each round one process on each node receives a block of data and one process sends a block of data.
This holds also for the case where the number of MPI processes per cluster node is not identical, and is illustrated in Figure 1.
In [11] it is observed that regular collective communication problems like the all-gather problem induce corresponding irregular problems over a set of nodes in a clustered system.
Therefore, if the communication capabilities of processors and nodes in a cluster are similar (for instance, single ported), an algorithm for solving a regular problem on a clustered system (with possibly different number of processes per cluster node) can be used to solve its irregular counterpart over a set of processors.
This observation can be exploited to convert the clustered linear ring algorithm into an algorithm for the irregular all-gather problem.To accomplish this the data of process i of size m i is associated with a virtual cluster node, and divided into b i = max(1, ⌈m i /B⌉) blocks of size at most B. Each block is associated with a virtual processor in the node.
The total number of blocks is b = p−1 i=0 b i (note that b ≥ p).
Every actual process with data size m i will play the role of a cluster node with b i virtual processors.
The linear ring algorithm with regular blocks of size (at most) B now solves the problem in b − 1 instead of p− 1 communication rounds.
The resulting, pipelined (or blocked ) ring algorithm is illustrated in Figure 2.
Compared to the linear ring, the advantage of the pipelined ring algorithm is that (more) regular blocks are sent and received in each round, for a total time of O((b − 1)B).
A small value for B increases the number of start-ups, and a large value increases the possible round up error.
Therefore a proper balancing needs to be applied to find an optimal value for the block size parameter.
We note that for extremely irregular all-gather problems where only one process has all the data, the pipelined ring algorithm is equivalent to a linear broadcast pipeline.
For regular problems where m i = m ′ for all i, the Process i block size B can be set to m ′ , in which case the algorithm is identical to the standard, linear ring.
Thus, by choosing B properly, the pipelined ring algorithm should never perform worse than the linear ring algorithm.Process i + 1 Process i − 1 j − 1 j j + 1 j + k − 1 We note that for partially full blocks, only the actual data are sent and received (see again Figure 2).
In particular, the empty blocks which arise for processes with m i = 0 are neither sent nor received.
Nevertheless, they contribute to the total number of communication rounds.
We estimate the optimal block size B as follows, assuming that z denotes the number of processes with m i = 0:-If z = 0 we take B = min p−1 i=0 m i (as long as this is not too small).
This ensures that all processes are both sending and receiving blocks in (almost) all rounds.
-If z 񮽙 = 0 we try to minimize the time needed for b − 1 communication rounds.Assuming that the remainders in the m i /B terms are equally distributed, we get an average padding of B/2 for all partially full blocks.
We can therefore simplify b = p−1 i=0 max(1, ⌈m i /B⌉) to b = m B + p+z 2 .
Assuming linear communication costs, where sending and receiving messages of size m ′ takes time α + βm ′ , the estimated total running time is (b − 1)(α + βB).
Minimizing this term gives an (approximated) optimal block size of B = 2αm β(p+z−2) We have benchmarked the new MPI Allgatherv implementations with the following distributions of contiguous data over the p MPI processes.
A base count c (which is varied over some interval) is used as seed for the following distributions: In distributions (2) and (3) the same total amount of data m = c is gathered by all processes, so similar running times can be expected (comparable to the regular distribution with p times smaller data size).
The case for distributions (1), (4), (5) and (6) is analogous, where the total amount of data is m = pc.We compare our implementations of the new MPI Allgatherv algorithm with implementations of the standard linear ring algorithm that is still used in many MPI libraries [9].
The reported running times are minimum times for the last process to finish over a (small) number of iterations [4].
The pipelined ring has been implemented for MPI/SX for the NEC SX-series of parallel vector computers.
It has been benchmarked with the distributions described above on 30 SX-8 nodes at HLRS in Stuttgart, with 1 and 8 MPI processes per node, respectively.
Selected results are shown in Figure 3.
For the extreme broadcast distribution (2) the pipelined ring outperforms the standard linear ring by more than a factor of 10 on 30 SX-8 nodes.
For 32 MBytes with a fixed block size B of 1 MByte an improvement of a factor 32×29 29+31 ≈ 15 would have been best possible.
Significant improvements can also be observed for the other distributions.
The performance of the standard ring and the pipelined ring are similar for the regular (1) and the half full (4) distributions.
Running on a randomly permuted communicator instead of MPI COMM WORLD gives almost identical results.
This is a desirable property of an algorithm for a symmetric (i.e. non-rooted) collective operation like MPI Allgatherv [12].
To show the effect of the block size B, the algorithm has also been integrated into NEC's MPI/PC version and evaluated on an Intel Xeon based SMP cluster with InfiniBand interconnect.
The running time is compared to the standard, nonpipelined algorithm for B = 32K, 64K, 128K, 512K, 1024K.
Results are shown in Figure 4.
For the spike distribution (3) the pipelined algorithm is faster for all block sizes.
However, the best block size depends not only on the size of the problem but also on the distribution of data over the processes.
This can be seen in the case of the decreasing distribution (5) where a too small block size makes the pipelined algorithm perform worse than the standard ring.
We ran the benchmarks on a Linux cluster at Argonne National Laboratory with 24 nodes, each with two dual-core 2.8 GHz AMD Opteron CPUs (total .
For small problem sizes, the pipelined algorithm performs only slightly better than the standard algorithm, but as problem size increases, the difference in performance becomes considerable.
Figure 6(right) shows the distribution of communication and idle times for the two algorithms.
As expected, the standard algorithm suffers because many processes remain idle for a long time, whereas in the pipelined algorithm, communication is more balanced.
We also collected traces of the program execution and plotted them using the Jumpshot tool, as shown in Figure 7.
The penalty due to idle time incurred by the standard algorithm is clearly visible as the yellow bars.
Benchmarks were also performed on the SiCortex 5832 system at Argonne.
This machine has 972 nodes, each with 6 cores, for a total of 5832 processors.
The nodes are connected by a Kautz graph network.
In some of our experiments the native SiCortex MPI implementation failed.
We therefore implemented the standard linear ring algorithm ourselves and compared it with the pipelined algorithm.
Figure 8 shows the results for a test run with a geometric curve dis- Finally, we performed the tests on one rack of the IBM Blue Gene/P at Argonne National Laboratory (4096 cores).
The native implementation of MPI Allgatherv in the Blue Gene/P's MPI library uses a very fast hardware-supported algorithm, which outperforms both standard ring and pipelined ring implementations.
Therefore, to fairly compare pipelined and non-pipelined algorithms, we implemented both these algorithms.
Figure 8 shows the results.
The pipelined algorithm performs even better on this machine.
We described a simple, pipelined ring algorithm for large, irregular all-gather problems.
The algorithm was implemented within different MPI libraries and benchmarked on various systems, and in all cases showed considerable improvements over a commonly used linear ring algorithm for problems with significant irregularity in the individual message sizes.
Determining the best possible pipeline block size for all distributions of input data still requires more (experimental) work.
On regular problem instances the pipelined algorithm performs similarly to the linear ring, which is bandwidth optimal for that case.
Ring algorithms can likewise be implemented to be largely independent on process placement in an SMP system.
This is an important property for users expecting (self-) consistent performance of their MPI library [12].
