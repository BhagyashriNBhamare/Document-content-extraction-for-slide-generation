Virtualized servers run a diverse set of virtual machines (VMs), ranging from interactive desktops to test and development environments and even batch workloads.
Hy-pervisors are responsible for multiplexing the underlying hardware resources among VMs while providing them the desired degree of isolation using resource management controls.
Existing methods provide many knobs for allocating CPU and memory to VMs, but support for control of IO resource allocation has been quite limited.
IO resource management in a hypervisor introduces significant new challenges and needs more extensive controls than in commodity operating systems.
This paper introduces a novel algorithm for IO resource allocation in a hypervisor.
Our algorithm, mClock, supports proportional-share fairness subject to minimum reservations and maximum limits on the IO allocations for VMs.
We present the design of mClock and a prototype implementation inside the VMware ESX server hypervisor.
Our results indicate that these rich QoS controls are quite effective in isolating VM performance and providing better application latency.
We also show an adaptation of mClock (called dmClock) for a distributed storage environment, where storage is jointly provided by multiple nodes.
The increasing trend towards server virtualization has elevated hypervisors to first class entities in today's datacenters.
Virtualized hosts run tens to hundreds of virtual machines (VMs), and the hypervisor needs to provide each virtual machine with the illusion of owning dedicated physical resources: CPU, memory, network and storage IO.
Strong isolation is needed for successful consolidation of VMs with diverse requirements on a shared infrastructure.
Existing products such as VMware ESX server hypervisor provide guarantees for CPU and memory allocation using sophisticated controls such as reservations, limits and shares [3,44].
However, the current state of the art in storage IO resource allocation is much more rudimentary, limited to providing proportional shares [20] to different VMs.IO scheduling in a hypervisor introduces many new challenges compared to managing other shared re- sources.
First, virtualized servers typically access a shared storage device using either a clustered file system such as VMFS [11] or NFS volumes.
A storage device in the guest OS or a VM is just a large file on the shared storage device.
Second, the IO scheduler in the hypervisor runs one layer below the elevator-based scheduling in the guest OS.
Hence, it needs to handle issues such as locality of accesses across VMs, high variability in IO sizes, different request priorities based on the applications running in the VMs, and bursty workloads.In addition, the amount of IO throughput available to any particular host can fluctuate widely based on the behavior of other hosts accessing the shared device.
Unlike CPU and memory resources, the IO throughput available to a host is not under its own control.
As shown in the example below, this can cause large variations in the IOPS available to a VM and impact application-level performance.Consider the simple scenario shown in Figure 1, with three hosts and five VMs.
Each VM is running a DVDStore [2] benchmark, which is an IO-intensive OLTP workload.
The system administrator has carefully provisioned the resources (CPU and memory) needed by VM 5, so that it can serve at least 400 orders per second.
Initially, VM 5 is running on host 3, and it achieves a transaction rate of roughly 500 orders/second.
Later, as four other VMs (1 -4), running on two separate hosts sharing the same storage device, start to consume IO bandwidth, the transaction rate of VM 5 drops to 275 orders per second, which is significantly lower than expected.
Other events that can cause this sort of fluctuation are: (1) changes in workloads (2) background tasks scheduled at the storage array, and (3) changes in SAN paths between the hosts and storage device.PARDA [20] provided a distributed control algorithm to allocate queue slots at the storage device to hosts in proportion to the aggregate IO shares of the VMs running on them.
The local IO scheduling at each host was done using SFQ(D) [24] a traditional fair-scheduler, which divides the aggregate host throughput among the VMs in proportion to their shares.
Unfortunately, as aggregate throughput fluctuates downwards, or as the value of a VM's shares is diluted by the addition of other VMs to the system, the absolute throughput for a VM falls.
This open-ended dilution is unacceptable in many applications that require minimum resource requirements to function.
Lack of QoS support for IO resources can have widespread effects, rendering existing CPU and memory controls ineffective when applications block on IO requests.
Arguably, this limitation is one of the reasons for the slow adoption of IO-intensive applications in virtualized environments.Resource controls such as shares (a.k.a. weights), reservations, and limits are used for predictable service allocation with strong isolation [8,34,43,44].
Shares are a relative allocation measure that specify the ratio in which the different VMs receive service.
Reservations and limits are expressed in absolute units, e.g. CPU cycles/sec or megabytes of memory.
The general idea is to allocate the resource to the VMs in proportion to their shares, subject to the constraints that each VM receives at least its reservation and no more than its limit.
These controls have primarily been employed for allocating resources like CPU time and memory pages where the resource capacity is known and fixed.For fixed-capacity resources, one can combine shares and reservations into one single allocation for a VM.
This allocation can be calculated whenever a new VM enters or leaves the system, since these are the only events at which the allocation is affected.
However, enforcing these controls is much more difficult when the capacity fluctuates dynamically, as is the case for the IO bandwidth of shared storage.
In this case the allocations need to be continuously monitored (rather than only at VM entry and exit) to ensure that no VM falls below its minimum.
A brute-force solution is to emulate the method used for fixed-capacity resources by recomputing the allocations periodically.
However this method relies on accurately being able to predict future capacity based on the current state.Finally, limits provide an upper bound on the absolute resource allocations.
Such a limit on IO performance is desirable to prevent competing IO-intensive applications, such as virus scanners, virtual-disk migrations, or backup operations, from consuming all the spare bandwidth in the system, which can result in high latencies for bursty and ON-OFF workloads.
There are yet other reasons cited by service providers for wanting to explicitly limit IO throughput; for example, to avoid giving VMs more throughput than has been paid for, or to avoid raising expectations on performance that cannot generally be sustained [1,8].
In this paper, we present mClock, an IO scheduler that provides all three controls mentioned above at a per-VM level ( Figure 2).
We believe that mClock is the first scheduler to provide such controls in the presence of capacity fluctuations at short time scales.
We have implemented mClock, along with certain storage-specific optimizations, as a prototype scheduler in the VMware ESX server hypervisor and showed its effectiveness for various use cases.We also demonstrate dmClock, a distributed version of the algorithm that can be used in clustered storage systems, where the storage is distributed across multiple nodes (e.g., LeftHand [4], Seanodes [6], IceCube [46], FAB [30]).
dmClock ensures that the overall allocation to each VM is based on the specified shares, reservations, and limits even when the VM load is nonuniformly distributed across the storage nodes.The remainder of the paper is organized as follows.
In Section 2 we discuss mClock's scheduling goal and its comparison with existing approaches.
Section 3 presents the mClock algorithm in detail, along with storagespecific optimizations.
Distributed implementation for a clustered storage system is discussed in Section 3.2.
Detailed performance evaluation using a diverse set of workloads is presented in Section 4.
Finally we conclude with some directions for future work in Section 5.
The work related to QoS-based IO resource allocation can be divided into three broad areas.
First is the class of algorithms that provide proportional allocation of IO [18] and Latency-rate scheduling [33]) proposed in the networking literature, adapted to handle various storage-specific concerns such as concurrency, minimizing seek delays and improving throughput.The goal of these algorithms is to allocate throughput or bandwidth in proportion to the specified weights of the clients.
Second is the class of algorithms that provide support for latency-sensitive applications along with proportional sharing.
These algorithms include SMART [28], BVT [14], pClock [22], Avatar [49] and service curve based techniques [12,27,31,36].
Third is the class of algorithms that support reservation along with proportional allocation, such as Rialto [25], ESX memory management [44] and other reservation based CPU scheduling methods [17,34,35].
Table 1 provides a quick comparison of mClock with existing algorithms in the three categories.
We first discuss a simple example describing the scheduling policy of mClock.
As mentioned earlier, three parameters are specified for each VM in the system: a share or weight represented by w i , a reservation r i , and a limit l i .
We assume these parameters are externally provided; determining the appropriate parameter settings to meet application requirements is an important but separate problem, outside the scope of this paper.
We also assume that the system includes an admission control component that ensures that the system capacity is adequate to serve the aggregate minimum reservations of all admitted clients.
The behavior of the system if the assumption does not hold is discussed later in the section, along with alternative approaches.Consider a simple setup with three VMs: one supporting remote desktop (RD), one running an Online Transaction Processing (OLTP) application and a Data Migration (DM) VM.
The RD VM has a low throughput requirement but needs low IO latency for usability.
OLTP runs a transaction processing workload requiring high throughput and low IO latency.
The data migration workload requires high throughput but is insensitive to IO latency.
Based on these requirements, the shares for RD, OLTP, and DM can be assigned as 100, 200, and 300 respectively.
To provide low latency and a minimum degree of responsiveness, reservations of 250 IOPS each are specified for RD and OLTP.
An upper limit of 1000 IOPS is set for the DM workload so that it cannot consume all the spare bandwidth in the system and cause high delays for the other workloads.
The values chosen here are somewhat arbitrary, but were selected to highlight the use of various controls in a diverse workload scenario.First consider how a conventional proportional scheduler would divide the total throughput T of the storage device.
Since throughput is allocated to VMs in proportion to their weights, an active VM v i will receive a throughput T × (w i / ∑ j w j ), where the summation is over the weights of the active VMs (i.e. those with at least one pending IO).
If the storage device's throughput is 1200 IOPS in the above example, RD will receive 200 IOPS, which is below its required minimum of 250 IOPS.
This can lead to a poor experience for the RD user, even though there is sufficient system capacity for both RD and OLTP to receive their reservations of 250 IOPS.
In our model, VMs always receive service between their minimum reservation and maximum limit (as long as system throughput is at least the aggregate of the reservations of active VMs).
In this case, mclock would provide RD with its minimum reservation of 250 IOPS and the remaining 950 IOPS would be divided between OLTP and DM in the ratio 2 : 3, resulting in allocations of 380 and 570 IOPS respectively.
Figure 3 shows the IOPS allocation to the three VMs in the example above, for different values of the system throughput, T. For T between 1500 and 2000 IOPS, the throughput is shared between RD, OLTP, and DM in proportion to their weights (1 : 2 : 3), since none of them will exceed their limit or fall below the reservation.
If T ≥ 2000 IOPS, then DM will be capped at 1000 IOPS because its share of T /2 is higher than its upper limit, and the remainder is divided between RD and OLTP in the ratio 1 : 2.
If the total throughput T drops below 1500 IOPS, the allocation of RD bottoms out at 250 IOPS, and similarly at T ≤ 875 IOPS, OLTP also bottoms out at 250 IOPS.
Finally, for T < 500 IOPS, the reservations of RD and OLTP cannot be met; the available throughput will be divided equally between RD and OLTP (since their reservations are the same) and DM will receive no service.
The last case should be rare if the admission controller estimates the overall throughput conservatively.The allocation to a VM varies dynamically with the current throughput T and the set of active VMs.
At any time, the VMs are partitioned into three sets: reservation-clamped (R), limit-clamped (L ) or proportional (P), based on whether their current allocation is clamped at the lower or upper bound or is in between.
If T is the current throughput, we define T P = T − ∑ j∈R r j − ∑ j∈L l j .
The allocation γ i made to active VM v i for T P ≥ 0, is given by:γ i =    r i v i ∈ R l i v i ∈ L T P × (w i / ∑ j∈P w j ) v i ∈ P (1) and ∑ i γ i = T.(2)When the system throughput T is known, the allocations γ i can be computed explicitly.
Such explicit computation is sometimes used for calculating CPU time allocations to virtual machines with service requirement specifications similar to these.
When a VM exits or is powered on at the host, new service allocations are computed.
In the case of a storage array, T is highly dependent on the presence of other hosts and the workload presented to the storage device.
Since the throughput varies dynamically, the storage scheduler cannot rely upon service allocations computed at VM entry and exit times.
The mClock scheduler ensures that the goals in Eq.
(1) and (2) are satisfied continuously, even as the system's throughput varies, using a novel, lightweight tagging scheme.Clearly, a feasible allocation is possible only if the aggregate reservation ∑ j r j does not exceed the total system throughput T .
When T P < 0, the system throughput is insufficient to meet the reservations; in this case mClock simply gives each VM throughput proportional to its reservation.
This may not always be the desired behavior.
VMs without a reservation may be starved in this case, but this problem can be easily avoided by adding a small default reservation for all VMs.
In addition, one can add priority control to meet reservations based on priority levels.
Exploring these options further is left to future work.
A number of approaches such as Stonehenge [23], SFQ(D) [24] and Argon [41] have been proposed for proportional sharing of storage between applications.
Wang and Merchant [45] extended proportional sharing to distributed storage.
Argon [41] and Aqua [48] propose service-time-based disk allocation to provide fairness as well as high efficiency.
Brandt et al. [47] have proposed Hierarchical Disk Sharing, which uses hierarchical token buckets to provide isolation and bandwidth reservation among clients accessing the same disk.
However, measuring per-request service times in our environment is difficult because multiple requests will typically be pending at the storage device.Overall, none of these algorithms offers support for the combination of shares, reservations, and limits.
Other methods for resource management in virtual clusters [16,39] have been proposed, but they mainly focus on CPU and memory resources and do not address the challenges raised by variable capacity that mClock does.
Several existing algorithms provide support for controlling the response time of latency-sensitive applications, but not strict latency guarantees or explicit latency targets.
In the case of CPU scheduling, BVT [14], SMART [28], and lottery scheduling [37,43] provide proportional allocation, latency-reducing mechanisms, and methods to handle priority inversion by exchanging tickets.
Borrowed Virtual Time [14] and SMART [28] can give a short-term advantage to latency-sensitive applications by shifting their virtual tags relative to the other applications.
pClock [22] and service-curve based methods [12,27,31,36] decouple latency and throughput requirements, but like the other methods also do not support reservations and limits.
For CPU scheduling and memory management, several approaches have been proposed for integrating reservations with proportional-share allocations [17,34,35].
In these models, clients either receive a guaranteed fraction of the server capacity (reservation-based clients) or a share (ratio) of the remaining capacity after satisfying reservations (proportional-share-based clients).
A standard proportional-share scheduler can be used in conjunction with an allocator that adjusts the weights of the active clients whenever there is a client arrival or departure.
Guaranteeing minimum allocations for CPU time is relatively straightforward since its capacity (in terms of cycles/sec) is fixed and known, and allocating a given proportion would guarantee a certain minimum amount.
The same idea does not apply to storage allocation where system throughput can fluctuate.In our model the clients are not statically partitioned into reservation-based or proportional-sharebased clients.
Our model automatically modifies the entitlement of a client when service capacity changes due to changes in the workload characteristics or due to the arrival or departure of clients.
The entitlement is at least equal to the reservation and can be higher if there is sufficient capacity.
Since 2003, the VMware ESX Server has provided reservations and proportional-share controls for both CPU and memory resources in a commercial product [8,42,44].
These mechanisms support the same rich set of controls as in mClock, but do not handle varying service capacity.
Finally, operating system based frameworks like Rialto [25] provide fixed reservations for known-capacity CPU service, while allowing additional service requests to be honored on an availability basis.
Rialto requires recomputation of an allocation graph on each new arrival, which is then used for CPU scheduling.
Tag-based scheduling underlies many previously proposed fair-schedulers [10,13,15,18]: all requests are assigned tags and scheduled in order of their tag values.
For example, an algorithm can assign tags spaced by increments of 1/w i to successive requests of client i; if all requests are scheduled in order of their tag values, the clients will receive service in proportion to w i .
In order to synchronize idle clients with the currently active ones, these algorithms also maintain a global tag value commonly known as global virtual time or just virtual time.
In mClock, we extend this notion to use multiple tags based on three controls and dynamically decide which tag to use for scheduling, while still synchronizing idle clients.The intuitive idea behind the mClock algorithm is to logically interleave a constraint-based scheduler and a weight-based scheduler in a fine-grained manner.
The constraint-based scheduler ensures that VMs receive at least their minimum reserved service and no more than the upper limit in a time interval, while the weight-based scheduler allocates the remaining throughput to achieve proportional sharing.
The scheduler alternates between phases during which one of these schedulers is active to Maximum service allowance (Limit) for v i Table 2: Symbols used and their descriptions maintain the desired allocation.
mClock uses two main ideas: multiple real-time clocks and dynamic clock selection.
Each VM IO request is assigned three tags, one for each clock: a reservation tag R, a limit tag L, and a proportional share tag P for weight-based allocation.
Different clocks are used to keep track of each of the three controls, and tags based on one of the clocks are dynamically chosen to do the constraint-based or weight-based scheduling.The scheduler has three main components: (i) Tag Assignment (ii) Tag Adjustment and (iii) Request Scheduling.
We will explain each of these in more detail below.
Tag Assignment: This routine assigns R, L and P tags to a request r from VM v i arriving at time t. All the tags are assigned using the same underlying principle, which we illustrate here using the reservation tag.
The R tag assigned to this request is the higher of the arrival time or the previous R tag + 1/r i .
That is:R r i = max{R r−1 i + 1/r i , Current time}(3)This gives us two key properties: first, the R tags of a continuously backlogged VM are spaced 1/r i apart.In an interval of length T , a backlogged VM will have about T × r i requests with R tag values in that interval.
Second, if the current time is larger than this value due to v i becoming active after a period of inactivity, the request is assigned an R tag equal to the current time.
Thus idle VMs do not gain any idle credit for future service.
Similarly, the L tag is set to the maximum of the current time and(L r−1 i + 1/l i ).
The L tags of a backlogged VM are spaced out by 1/l i .
Hence, if the L tag of the first pending request of a VM is less than the current time, it has received less than its upper limit at this time.
A limit tag higher than the current time would indicate that the VM has received its limit and should not be scheduled.
The proportional share tag P r i is also the larger of the arrival time of the request and (P r−1 i + 1/w i ) and subsequent backlogged requests are spaced by 1/w i .
Tag Adjustment: Tag adjustment is used to calibrate the proportional share tags against real time.
This is required whenever an idle VM becomes active again.
In virtual time based schedulers [10,15] this synchronization is done using global virtual time.
The initial P tag value of a freshly active VM is set to the current time, but the spacing of P tags after that is determined by the relative weights of the VMs.
After the VM has been active for some time, the P tag values become unrelated to real time.
This can lead to starvation when a new VM becomes active, since the existing P tags are unrelated to the P tag of the new VM.
Hence existing P tags are adjusted so that the smallest P tag matches the time of arrival of the new VM, while maintaining their relative spacing.
In the implementation, when a VM is acti- vated, we assign it an offset equal to the difference between the effective value of the smallest existing P tag and the current time.
During scheduling, the offset is added to the P tag to obtain the effective P tag value.
The relative ordering of existing P tags is not altered by this transformation; however, it ensures that the newly activated VMs compete fairly with existing VMs.
Request Scheduling: mClock needs to check three different tags to make its scheduling decision instead of a single tag in previous algorithms.
As noted earlier, the scheduler alternates between constraint-based and weight-based phases.
First, the scheduler checks if there are any eligible VMs with R tags no more than the current time.
If so, the request with smallest R tag is dispatched for service.
This is defined as the constraintbased phase.
This phase ends (and the weight-based phase begins) at a scheduling instant when all the R tags exceed the current time.During a weight-based phase, all VMs have received their reservations guaranteed up to the current time.
The scheduler therefore allocates server capacity to achieve proportional service.
It chooses the request with smallest P tag, but only from VMs which have not reached their limit (whose L tag is smaller than the current time).
Whenever a request from VM v i is scheduled in a weight-based phase, the R tags of the outstanding requests of v i are decreased by 1/r i .
This maintains the condition that R tags are always spaced apart by 1/r i , so that reserved service is not affected by the service provided in the weight-based phase.
Algorithm 1 provides pseudo code of various components of mClock.
There are several storage-specific issues that an IO scheduler needs to handle: IO bursts, request types, IO size, locality of requests and reservation settings.
Burst Handling.
Storage workloads are known to be bursty, and requests from the same VM often have a high spatial locality.
We help bursty workloads that were idle to gain a limited preference in scheduling when the system next has spare capacity.
This is similar to some of the ideas proposed in BVT [14] and SMART [28].
However, we do it in a manner so that reservations are not impacted.To accomplish this, we allow VMs to gain idle credits.
In particular, when an idle VM becomes active, we compare the previous P tag with current time t and allow it to lag behind t by a bounded amount based on a VM-specific burst parameter.
Instead of setting the P tag to the current time, we set it equal to t − σ i * (1/w i ).
Hence the actual assignment looks like:P r i = max{P r−1 i + 1/w i , t − σ i /w i }The parameter σ i can be specified per VM and determines the maximum amount of credit that can be gained by becoming idle.
Note that adjusting only the P tag has the nice property that it does not affect the reservations of other VMs; however if there is spare capacity in the system, it will be preferentially given to the VM that was idle.
This is because the R and L tags have strict priority over the P tags, so adjusting P tags cannot affect the constraint-based phase of the scheduler.
Request Type.
mClock treats reads and writes identically.
In practice writes show lower latency due to write buffering in the disk array.
However doing any re-ordering of reads before writes for a single VM can lead to an inconsistent state of the virtual disk on a crash.
Hence mClock schedules all IOs within a VM in a FCFS order without distinguishing between reads and writes.
IO size.
Since larger IO sizes take longer to complete, differently-sized IOs should not be treated equally by the IO scheduler.
We propose a technique to handle largesized IOs during tagging.
The IO latency with n random outstanding IOs with an IO size of S each can be written as:Lat = n(T m + S/B peak )(4)Here T m denotes the mechanical delay due to seek and disk rotation and B peak denotes the peak transfer bandwidth of a disk.
Converting the latency observed for an IO of size S 1 to an IO of a reference size S 2 , keeping other factors constant would give:Lat 2 = Lat 1 * (1 + S 2 T m × B peak )/(1 + S 1 T m × B peak )(5)For a small reference IO size of 8KB and using typical values for mechanical delay T m = 5ms and peak transfer rate, B peak = 60 MB/s, the numerator = Lat 1 *(1 + 8/300) ≈ Lat 1 .
So, for tagging purposes, a single request of IO size S is treated as equivalent to:(1 + S/(T m × B peak )) IO requests.
Request Location.
mClock can detect sequentiality within a VM's workload, but in most virtualized environments the IO stream seen by the underlying storage may not be sequential due to a high degree of multiplexing.
mClock improves the overall efficiency of the system by scheduling IOs with high locality as a batch.
A VM is allowed to issue IO requests in a batch as long as the requests are close in logical block number space (e.g., within 4 MB).
Also the size of batch is bounded by a configurable parameter (set to 8).
This optimization impacts the time granularity over which reservations are met.
The batching of IOs is limited to a small number, typically 8.
so for N VMs, the delay in meeting reservations can be 8N IOs.
A typical number of VMs/host is 10-15, so this can delay reservation guarantees in the short term by the time taken to do roughly 100 IOs.
Note that the benefit of batching and improved efficiency is distributed among all the VMs instead of giving it just to the VM with high sequentiality.It may be preferable to allocate the benefit of locality to the concerned VM; this is deferred to future work.
Reservation Setting.
Admission control is a well known and difficult problem for storage devices due to their stateful nature and dependence of the throughput on the workload.
We propose the simple approach of using the worst case IOPS from a storage device as an upper bound on sum of reservations for admission control.
For example, an enterprise FC disk can service 200 to 250 random IOPS and a SATA disk can do roughly 80-100 IOPS.
Based on the number and type of disk drives backing a storage LUN, one can obtain a conservative estimate of reservable throughput.
This is what we have used to set parameters in our experiments.
Also in order to set the reservations to meet an application's latency for a certain number of outstanding IOs, we use Little's law:IOPS = Outstanding IOs/Latency(6)Thus, for an application that typically keeps 8 IOs outstanding and requires 25 ms average latency, the reservation should be set to 8 / 0.025 = 320 IOPS.
Cluster-based storage systems are emerging as a costeffective, scalable alternative to expensive, centralized disk arrays.
By using commodity hardware (both hosts and disks) and using software to glue together the storage distributed across the cluster, these systems allow for lower cost and more flexible provisioning than conventional disk arrays.
The software can be designed to compensate for the reliability and consistency issues introduced by the distributed components.
Several research prototypes (e.g., CMU's Ursa Minor [9], HP Labs' FAB [30], IBM's Intelligent Bricks [46]) have been built, and several companies (such as LeftHand [4], Seanodes [6]) are offering iSCSIbased storage devices using local disks at virtualized hosts.
In this section, we extend mClock to run on each storage server, with minimal communication between the servers, and yet provide per-VM globally (clusterwide) proportional service, reservations, and limits.
dmClock runs a modified version of mClock at each server.
There is only one modification to the algorithm to account for the distributed model in the Tag-Assignment component.
During tag assignment each server needs to determine two things: the aggregate service received by the VM from all the servers in the system and the amount of service that was done as part of reservation.
This information will be provided implicitly by the host running a VM by piggybacking two integers ρ i and δ i with each request that it forwards to a storage server s j .
Here δ i denotes number of IO requests from VM v i that have completed service at all the servers between the previous request (from v i ) to the server s j and the current request.
Similarly, ρ i denotes the number of IO requests from v i that have been served as part of constraint-based phase between the previous request to s j and the current request.
This information can be easily maintained by the host running the VM.
The host forwards the values of ρ i and δ i along with v i 's request to a server.
(Note that for the single server case, ρ and δ will always be 1.)
In the Tag-Assignment routine, these values are used to compute the tags as follows:R r i = max{R r−1 i + ρ i /r i , t} L r i = max{L r−1 i + δ i /l i , t} P r i = max{P r−1 i + δ i /w i , t}Hence, the new request may receive a tag further into the future, to reflect the fact that v i has received additional service at other servers.
The greater the value of δ , the lower the priority the request has for service.
Note that this does not require any synchronization among the storage servers.
The remainder of the algorithm remains unchanged.
The values of ρ and δ may, in the worst case, be inaccurate by up to 1 request at each of the other servers.
However, the dmClock algorithm does not require complex synchronization between the servers [32].
In this section, we present results from a detailed evaluation of mClock using a prototype implementation in the VMware ESX server hypervisor [7,40].
The changes required were small: the overall implementation took roughly 200 lines of C code in order to modify an existing scheduling framework.
The resulting scheduler is lightweight, which is important because it is on the critical path for IO issues and completions.
We examine the following key questions about mClock:(1) Why is mClock needed?
(2) Can mClock allocate service in proportion to weights, while meeting the reservation and limit constraints?
(3) Can mClock handle bursts effectively and reduce latency by giving idle credit?
(4) How effective is dmClock in providing isolation among dynamic workloads in a distributed storage environment?
We implemented mClock by modifying the SCSI scheduling layer in the IO stack of VMware ESX server hypervisor to construct our prototype.
The ESX host was a Dell Poweredge 2950 server with 2 Intel Xeon 3.0 GHz dual-core processors, 8GB of RAM and two Qlogic HBAs connected to an EMC CLARiiON CX3-40 storage array over FC SAN.
We used two different storage volumes: one hosted on a 10 disk RAID 0 disk group and another on a 10 disk, RAID 5 disk group.
The host was configured to keep 32 IOs pending per LUN at the array, which is the default setting.We used a diverse set of workloads, using different operating systems, workload generators, and configurations, to verify that mClock is robust under a variety of conditions.
We used two kinds of VMs: (1) Linux (RHEL) VMs, each with a 10GB virtual disk, one VCPU and 512 MB memory, and (2) Windows server 2003 VMs, each with a 16GB virtual disk, one VCPU and 1 GB of memory.
The disks hosting the operating systems for VMs were on a different storage LUN.Three parameters were configured for each VM: a minimum reservation r i IOPS, a global weight w i , and maximum limit l i IOPS.
The workloads were generated using Iometer [5] in the Windows server VMs and our own micro-workload generator in the Linux RHEL VMs.
For both cases, the workloads were specified using IO sizes, the percentage of reads, the percentage of random IOs, and the number of concurrent IOs.
We used 32 concurrent IOs per workload in all experiments, unless otherwise stated.
In addition to these micro-benchmark workloads, we used macrobenchmark workloads generated using Filebench [26].
Figure 5: mClock limits the throughput of VM2 and VM3 to 400 and 500 IOPS as desired.
First we show the need for the limit control by demonstrating that pure proportional sharing cannot guarantee the specified number of IOPS and latency to a VM.
We experimented with three workloads similar to those in the example of Section 2: RD, OLTP and DM.RD is a bursty workload sending 32 random IOs (75% reads) of 4KB size every 250 ms. OLTP sends 8KB random IOs, 75% reads, and keeps 16 IOs pending at all times.
The data migration workload DM does 32KB sequential reads, and keeps 32 IOs pending at all times.
RD and OLTP are latency-sensitive workloads, requiring a response time under 30ms, while DM is not sensitive to latency.
Accordingly, we set the weights in the ratio 2:2:1 for the RD, OLTP, and DM workloads.
First, we ran them with zero reservations and no limits in mClock, which is equivalent to running them with a standard fair scheduler such as SFQ(D) [24].
The throughput and latency achieved is shown in Figures 4(a) and (b), between times 60 and 140sec.
Since RD was not fully backlogged, and OLTP had only 16 concurrent IOs, the work-conserving scheduler gave all the remaining queue slots (16 of them) to the DM workload.
As a result, RD and OLTP got less than the specified proportion of IO throughput, while DM received more.
Since the device queue was always heavily occupied by IO requests from DM, the latency seen by RD and OLTP was higher than desirable.
We also experimented with other weight ratios (which are not shown here for lack of space), but saw no significant improvement, because the primary cause of the poor performance seen by RD and OLTP was that there were too many IOs from DM in the device queue.To provide better throughput and lower latency to RD and OLTP workloads, we changed the upper limit for DM to 300 IOs (from unlimited) at t = 140sec.
This caused the OLTP workload to see a 100% increase in throughput and the latency was reduced by half (36 ms to 16 ms).
The RD workload also saw lower latency, while its throughput remained equal to its demand.
This result shows that using limits with proportional sharing can be quite effective in reducing contention for critical workloads, and this effect cannot be produced using proportional sharing alone.Next, we did an experiment to show that mClock effectively enforces limits in a more dynamic setting with workloads arriving at different times.
Using Iometer on Windows Server VMs, we ran three workloads (VM1, VM2, and VM3), each generating 16KB random reads.
We set the weights in the ratio 1:1:2, with limits of 400 IOPS on VM2 and 500 IOPS on VM3.
We began with just VM1 and a new workload was started every 60 seconds.
The storage device had a capacity of about 1600 random reads per second.
Without the limits and based on the weights alone, we would expect the applications to receive 800 IOPS each when VM1 and VM2 are running, and 400, 400, and 800 IOPS respectively when VM1, VM2, and VM3 are running together.
Figure 5 shows the throughput obtained by each of the workloads.
When we added the VM2 (at time 60sec), it received only 400 IOPS based on its limit, and not the 800 IOPS it would have received based on the weights alone.
When we started VM3 (at time 120sec), it received only its maximum limit, 500 IOPS, again smaller than its throughput share based on the weights alone.
This shows that mClock is able to limit the throughput of VMs based on specified upper limits.
To test the ability of mClock to enforce reservations, we used a combination of 5 workloads, VM1 -VM5, all generated using Iometer on Windows Server VMs.
Each workload maintained 32 outstanding IOs, all 16 KB random reads, at all times.
We set their shares to the ratio 1:1:2:2:2.
VM1 required a minimum of 300 IOPS, VM2 required 250 IOPS, and the rest had no minimum requirement.
To demonstrate again the working of mClock in a dynamic environment, we began with just VM1, and a new workload was started every 60 seconds.
Figures 6(a) shows the overall throughput observed by the host using SFQ(D=32) and mClock.
As the number of workloads increased, the overall throughput from the array decreased because the combined workload spanned larger numbers of tracks on the disks.
Figures 6(b) and (c) show the throughput obtained by each workload using SFQ(D=32) and mClock respectively.
When we used SFQ(D), the throughput of each VM decreased with increasing load, down to 160 IOPS for VM1 and VM2, while the remaining VMs received around 320 IOPS.
In contrast, mClock provided 300 IOPS to VM1 and 250 IOPS to VM2, as desired.
Increasing the throughput allocation also led to a smaller latency (as expected) for VM1 and VM2, which would not have been possible just using proportional shares.
In the experiments above, we used mostly homogeneous workloads for ease of exposition and understanding.
To demonstrate the effectiveness of mClock with a non-homogeneous combination of workloads, we experimented with workloads having very different IO characteristics.
We used four workloads, generated using Iometer on Windows VMs, each keeping 32 IOs pending at all times.
The workload configurations and the resource control settings (reservations, limits, and weights) are shown in Table 3.
Figures 7(a) and (b) show the throughputs allocated by SFQ(D) (weight-based allocation) and by mClock for these workloads.
mClock was able to restrict VM2 to 700 IOPS, as desired, when only two VMs were doing IOs.
Later, when VM4 became active, mClock was able to meet the reservation of 250 IOPS for it, whereas SFQ only provided around 190 IOPS.
While meeting these constraints, mClock was able to keep the allocation in proportion to the weights of the VMs; for example, VM1 got twice as many IOPS as VM3 did.We next used the same workloads to demonstrate how an administrator may determine the reservation to use.
If the maximum latency desired and the maximum concurrency of the application is known, then the reservation can be simply estimated using Little's law as the ratio of the concurrency to the desired latency.
In our case, if it is desired that the latency not exceed 65ms, the reservation can be computed as 32/0.065 = 492, since the number of concurrent IOs from each application is 32.
First, we Table 4: mClock provided low latencies to VM1 and VM2 and throughputs close to the reservation when the reservations were changed from r i = 1 to 512 IOPS.ran the four VMs together with a reservation r i = 1 each, and weights in the ratio 1:1:2:2.
The throughput (IOPS) and latency received by each in this simultaneous run are shown in Table 4.
Note that workloads received IOPS in proportion to their weights, but the latencies of VM1 and VM2 were much higher than desired.
We then set the reservation (r i ) for each VM to be 512 IOPS; the results are shown in the last column of Table 4.
Note that first two VMs received higher IOPS of around 500 instead of 330 and 390, which is close to their reservation targets.
The latency is also close to the expected value of 65ms.
The other VMs saw a corresponding decline in their throughput.
The reservation targets of VM1 and VM2 were not entirely met because the overall throughput was slightly smaller than the sum of reservations.
This experiment demonstrates that mClock is able to provide a strong control to storage admins to meet their IOPS and latency targets for a given VM.
Next, we experimented with the use of idle credits given to a workload for handling bursts.
Recall that idle credits allow a workload to receive service in a burst only if the workload has been idle in the past and the reservations for all VMs have been met.
This ensures that if an application is idle for a while, it gets preference when next there is spare capacity in the system.
In this experiment, we used two workloads generated with Iometer on Win- Table 5: The bursty workload (VM1) saw an improved latency when given a higher idle credit of 64.
The overall throughput remained unaffected.VM σ =1dows Server VMs.
The first workload was bursty, generating 128 IOs every 400ms, all 4KB reads, 80% random.
The second was steady, producing 16 KB reads, 20% of them random and the rest sequential, with 32 outstanding IOs.
Both VMs had equal shares, no reservation, and no limit imposed on the throughput.
We used idle-credit (σ ) values of 1 and 64 for our experiment.
Table 5 shows the IOPS and average latency obtained by the bursty VM for the two settings of the idle credit.
The number of IOPS were almost equal in either case because idle credits do not impact the overall bandwidth allocation over time, and VM1 had a bounded request rate.
VM2 also saw almost the same IOPS for the two settings of idle credits.
However, we notice that the latency seen by the bursty VM1 decreased as we increased the idle credits.
VM2 also saw a similar or a slightly smaller latency, perhaps due to the increase in efficiency of doing several IOs at a time from a single VM, which are likely to be spatially closer on the storage device.In the extreme, however, a very high setting of idle credits can lead to high latencies for non-bursty workloads by distorting the effect of the weights (although not the reservations or limits), and so we limit the setting to a maximum of 256 IOs in our implementation.
This result indicates that using idle credits is an effective mechanism to help lower the latency of bursts.
To test mClock with more realistic workloads, we experimented with two Linux RHEL VMs running OLTP workload using Filebench [26].
Each VMs was configured with 1 VCPU, 512 MB of RAM, 10GB database disk, and 1 GB log virtual disk.
To introduce throughput fluctuation another Windows 2003 VM running Iometer was used.
The Iometer workload produced 32 concurrent, 16KB random reads.
We assigned the weights in the ratio 2:1:1 to the two OLTP workloads and the Iometer workload, respectively, and gave a reservation of 500 IOPS to each OLTP workload.
We initially started the two OLTP workloads together and then the Iometer workload at t = 115s.
Figures 8(a) and (b) show the IOPS received by the three workloads as measured inside the hypervisor, with and without mClock.
Without mClock, as soon as the Iometer workload started, OLTP2 started missing its reservation and received around 250 IOPS.
When run with mClock, both the OLTP workloads were able to achieve their reservations of 500 IOPS.
This shows that mClock can protect critical workloads from a sudden change in the available throughput.
The applicationlevel metrics -the number of operations/sec and the transaction latency reported by Filebench -are summarized in Figure 8(c).
Note that mClock was able to provide higher operations/sec and lower latency per operation in OLTP VMs, even with an increase in the overall IO contention.
In this section, we present results of a dmClock implementation in a distributed storage system.
The system consisted of multiple storage servers (nodes) -three in our experiment.
Each node was implemented using a virtual machine running RHEL Linux with a 10GB OS disk and a 10GB experimental disk, from which the data was served.
Each experimental disk was placed on a different LUN backed by RAID-5 group with six disks.
Thus, each experimental disk could do roughly 1500 IOPS for a random workload.
A single storage device shared by all clients, was then constructed by striping across all the storage nodes.
This configuration represents a clustered-storage system where there are multiple storage nodes, each with dedicated LUNs used for servicing IOs.We implemented dmClock as a user-space module in each server node.
The module receives IO requests containing IO size, offset, type (read/write), the δ and ρ parameters, and data in the case of write requests.
The module can keep up to 16 outstanding IOs (using 16 threads) to execute the requests, and the requests are scheduled on these threads using the dmClock algorithm.
The clients were run on a separate physical machine.
Each client generated an IO workload for one or more storage nodes and also acted as a gateway, piggybacking the δ and ρ values onto each request sent to the storage nodes.
Each client workload consisted of 8KB random reads with 64 concurrent IOs, uniformly distributed over the nodes it used.
We used our own workload generator here because of the need to add appropriate δ and ρ values to each request.In first experiment, we used three clients, {c 1 , c 2 , c 3 }, each accessing all three storage nodes.
The weights were set in the ratio 1:4:6, with no upper limit on the IOPS.
We experimented with two different cases: (1) No reservation per client, (2) Reservations of 800, 1000 and 100 for clients {c 1 , c 2 , c 3 } respectively.
These values were used to highlight a use case where the allocation based on reservations may be higher than the allocation based on weights or shares for some clients.
The output for these two cases is shown in Figure 9 (a) and (b).
Case (a) shows the overall IO throughput obtained by three clients without reservations.
As expected, each client received total service in proportion to its weight.
In case (b), dmClock was able to meet the reservation goal of 800 IOPS for c 1 , which would have been missed with a proportional share scheduler.
The remaining throughput was divided between clients c 2 and c 3 in the ratio 2:3 as they respectively received around 1750 and 2700 IOPS.
Figure 9(b) also shows the IOs done during the two phases of the algorithm.Next, we experimented with non-uniform accesses from clients.
In this case we used two clients c 1 ,c 2 and two storage servers.
The reservations were set to 800 and 1000 IOPS and the weights were again in the ratio 1:4.
c 1 sent IOs to the first storage node (S 1 ) only and we started c 2 after approximately 40 seconds.
Fig- ure 10 shows the IOPS obtained by the two clients with time.
Initially, c 1 got the full capacity from server S 1 and when c 2 was started, c 1 was still able to get an allocation close to its reservation of 800 IOPS.
The remaining capacity was allocated to c 2 , which received around 1400 IOPS.
A distributed weight-proportional scheduler [45] would have given approximately 440 IOPS to c 1 and the remainder to c 2 , which would have missed the minimum requirement of c 1 .
This shows that even when the access pattern is non-uniform in a distributed environment, dmClock is able to meet reservations and assign overall IOPS in the ratio of weights to the extent possible.
In this paper, we presented a novel IO scheduling algorithm, mClock, that provides per-VM quality of service in presence of variable overall throughput.
The QoS requirements for a VM are expressed as a minimum reservation, a maximum limit, and a proportional share.
A key aspect of mClock is its ability to enforce such controls even with fluctuating overall capacity, as shown by our implementation in the VMware ESX server hypervisor.
We also presented dmClock, a distributed version of our algorithm that can be used in clustered storage system architectures.
We implemented dmClock in a distributed storage environment and showed that it works as specified, maintaining global per-client reservations, limits, and proportional shares, even though the schedulers run locally on the storage nodes.The controls provided by mClock should allow stronger isolation between VMs.
Although we have shown the effectiveness for hypervisor IO scheduling, we believe that the techniques are quite generic and can be applied to array-level scheduling and to other resources such as network bandwidth allocation as well.In our future work, we plan to explore further how to set these parameters to meet application-level SLAs.
We would like to thank our shepherd, Jon Howell, and the anonymous reviewers for their comments, which helped improve this paper.
We thank Carl Waldspurger for valuable discussions and feedback on this work.
Many thanks to Chethan Kumar for providing us with motivational use cases and Ganesha Shanmuganathan for discussions on the algorithm.
Part of the work was done while the first author was a PhD student at Rice University [19].
The support of the National Science Foundation under Grants CNS-0541369 and CNS-0917157 is gratefully acknowledged.
A preliminary version of the dmClock algorithm appeared as a brief announcement in PODC 2007 [21].
