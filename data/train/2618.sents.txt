Automated management is critical to the success of cloud computing, given its scale and complexity.
But, most systems do not satisfy one of the key properties required for automation: predictability, which in turn relies upon low variance.
Most automation tools are not eeective when variance is consistently high.
Using automated performance diagnosis as a concrete example, this position paper argues that for automation to become a reality, system builders must treat variance as an important metric and make conscious decisions about where to reduce it.
To help with this task, we describe a framework for reasoning about sources of variance in distributed systems and describe an example tool for helping identify them.
Many in the distributed systems community [, ] recognize the need for automated management in cloud infrastructures, such as large distributed systems and datacenters.
ey predict that the rapidly increasing scale and complexity of these systems will soon exceed the limits of human capability.
Automation is the only recourse, lest they become completely unmanageable.
In response to this call to arms, many research papers have been published on tools for automating various tasks, especially performance diagnosis [, , , -, , , , , ].
Most focus on layering automation on top of existing systems, without regard to whether they exhibit a key property needed for automation-predictability.
Most systems do not, especially the most complex ones that need automation the most.
is limits the utility of automation tools and the scope of tasks that can be automated.Our experiences using an automated performance diagnosis tool, Spectroscope [], to diagnose problems in distributed storage systems, such as Ursa Minor [] and Bigtable [], bear out the inadequacy of the predictability assumption.
ough Spectroscope has proven useful, it has been unable to reach its full potential due to high variance in performance resulting from poorly structured code, high resource contention, and hardware issues.
e predictability of a distributed system is aected by variance in the metrics used to make determinations about it.
As variance increases, predictability decreases as it becomes harder for automation tools to make conndent, or worse, correct determinations.
ough variance cannot be eliminated completely due to fundamental nondeterminism (e.g., actions of a remote system and bit errors), it can be reduced, improving predictability.To aid automation, system builders could be encouraged to always minimize variance in key metrics.
is policy dovetails nicely with areas in which predictability is paramount.
For example, in the early s, the US Postal Service slowed down mail delivery because they decided consistency of delivery times was more important than raw speed.
When asked why this tradeo was made, the postmaster general responded: "I began to hear complaints from mailers and customers about inconsistent rst-class mail delivery. . . .
We learned how important consistent, reliable delivery is to our customers" [].
In scientiic computing, inter-node variance can drastically limit performance due to frequent synchronization barriers.
In real-time systems, it is more important for programs to meet each deadline than run faster on average.
Google has recently identiied low response-time variance as crucial to achieving high performance in warehouse-scale computing [].
Of course, in many cases, variance is a side eeect of desirable performance enhancements.
Caches, a mainstay of most distributed systems, intentionally trade variance for performance.
Many scheduling algorithms do the same.
Also, reducing variance blindly may lead to synchronized "bad states, " which may result in failures or drastic performance problems.
For example, Patil et al. describe an emergent property in which load-balancing in GIGA+, a distributed directory service, leads to a large performance dropo as compute-intensive hash bucket splits on various nodes naturally become synchronized [].
Some variance is intrinsic to distributed systems and cannot be reduced without wholesale architectural changes.
For example, identical components, such as disks from the same vendor, can diier signiicantly in performance due to fault-masking techniques and manufacturing eeects [,].
Also, it may be diicult to design complex systems to exhibit low variance because it is hard to predict their precise operating conditions [, ].
In practice, there is no easy answer in deciding how to address variance to aid automation.
ere is, however, a wrong answer-ignoring it, as is too oen being done today.
Instead, for the highly touted goal of automation to become a reality, system builders must treat variance as a rst-class metric.
ey should strive to localize the sources of variance in their systems and make conscious decisions about which ones should be reduced.
Such explicit decisions about variance properties will result in more robust systems and will vastly improve the utility of automation tools that rely on low variance to work.
e rest of this paper is organized as follows.
Section discusses how automated performance diagnosis tools are aected by high variance.
Section identiies three types of variance that can be found in distributed systems and what should be done about them.
Section proposes a mechanism system builders can use to identify the sources of variance in their systems.
Section discusses open questions and Section concludes.
Tools that automate aspects of performance diagnosis each assume a unique model of system behaviour and use deviations from it to predict diagnoses.
Most do not identify the root cause directly, but rather automatically localize the source of the problem from any of the the numerous components in the system to just the speciic components or functions responsible.
Diierent tools exhibit diierent failure modes when variance is high, depending on the underlying techniques they use.
Tools that rely on thresholds make predictions when important metrics chosen by experts exceed pre-determined values.
eir failure mode is the most unpredictable, as they will return more false positives (inaccurate diagnoses) or false negatives (diagnoses not made when they should have been), depending on the value of the threshold.
A low threshold will result in more false positives, whereas increasing it to accommodate the high variance will mask problems, resulting in more false negatives.
False positives perhaps represent the worst failure mode, due to the amount of developer eeort wasted [].
To avoid costly false positives, some tools use statistical techniques to avoid predicting when the expected false positive rate exceeds a pre-set one (e.g., ).
e cost of high variance for them is an increase in false negatives.
Some statistical tools use adaptive techniques to increase their conndence before making predictions-e.g., by collecting more data samples.
e cost of high variance for them is increased storage/processing cost and an increase in time required before predictions can be made.Many tools use machine learning to automatically learn the model (e.g., metrics and values) that best predicts performance.
e false positive rate and false negative rate are controlled by selecting the model that best trades generality (which usually results in more false negatives) with speciicity (which results in more false positives) .
Real tools use a combination of the techniques described above to make predictions.
is section lists four such tools and how they are aected by variance.
Table provides a summary and lists additional tools.Magpie []: is tool uses an unsupervised machine learning algorithm (clustering) to identify anomalous requests in a distributed system.
Requests are grouped together based on similarity in request structure, performance metrics, and resource usage.
It expects that most requests will fall into one of several "main" clusters of behaviour, so it identiies small ones as anomalies.
A threshold is used to decide whether to place a request in the cluster deemed most similar to it, or whether to create a new one.
High variance in the values of the features used and use of a low threshold will yield many small clusters, resulting in an increase in false positives.
Increasing the threshold will result in more false negatives.Spectroscope []: is tool uses a combination of statistical techniques and thresholds to identify the changes in request processing most responsible for an observed performance change.
It relies on the expectation that requests that take the same path through a distributed system's components should incur similar performance costs and that the request topologies observed (e.g., components visited and functions executed by individual requests) should be similar across executions of the same workload.
High variance in these metrics will increase the number of false positives and false negatives.
Experiments run on Bigtable [] in three diierent Google datacenters show that ----of all unique paths observed satisfy the similar paths expectation, leaving much room for improvement [].
ose paths that do not satisfy it suuer from a lack of enough instrumentation to tease out truly unique paths and high contention with co-located processes.
Peer comparison [, ]: ese diagnosis tools are intended to be used on tightly coupled distributed systems, such as Hadoop and PVFS.
ey rely on the expectation that every machine in a given cluster should exhibit the same behaviour.
As such, they indict a machine as exhibiting a problem if its performance metrics diier signiicantly from others.
resholds are used to determine the degree of diierence tolerated.
High variance in metric distributions between machines will result in more false positives, or false negatives, depending on the threshold chosen.
Reis is known as the bias-variance tradeo.
FPs / FNs Tool FPs / FNs Magpie [] ⇑ / ⇑ DARC [] -/ ⇑ Spectroscope [] ⇑ / ⇑ Distalyzer [] -/ ⇑ Peer comp.
[] ⇑ / ⇑ Pinpoint [] -/ ⇑ NetMedic [] -/ ⇑ Shen [] -/ ⇑ Oliner [] -/ ⇑ Sherlock [] -/ ⇑ Variance in distributed systems is an important metric that directly aects potential for automated diagnosis.
To reduce it, two complementary courses of action are necessary.
During the design phase, system builders should make conscious decisions about which areas of the distributed system should be more predictable (exhibit low variance w/regard to important metrics).
Since the complexity of distributed systems makes it unlikely they will be able to identify all of the sources of variance during design [, , ], they must also work to identify sources of variance during development and testing.
To help with the latter, this section describes a nomenclature for variance sources that can help system builders reason about them and understand for which ones variance should be reduced.
Variance from intentional and intrinsic sources may be a given, so the quality of predictions made by automation tools in these areas will suuer.
However, it is important to guarantee their variance does not impact predictions made for other areas of the system.
is may be the case if the data granularity used by an automation tool to make predictions is not high enough to distinguish between a high variance source and surrounding areas.
For example, problems in the soware stack of a component may go unnoticed if an automation tool does not distinguish it from a high-variance disk.
To avoid such scenarios, system builders should help automation tools account for high variance sources directly-for example, by adding markers around them that are used by automation tools to increase their data granularity.
To illustrate a variance-oriented mindset, this section proposes one potential mechanism, called VarianceFinder, for helping system builders identify the main sources of variance in their systems during development and testing.
e relatively simple design outlined here focuses on reducing variance in response times for distributed storage systems such as Ursa Minor [], Bigtable [], and GFS [].
However, we believe this basic approach could be extended to include other performance metrics and systems.VarianceFinder utilizes end-to-end traces (Section .
.)
and follows a two-tiered approach.
First, it shows the variance associated with aspects of the system's overall functionality that should exhibit similar performance (Section .
.)
.
Second, it allows system builders to select functionality with high variance and identiies the components, functions, or RPCs responsible, allowing them to take appropriate action (Section .
.)
.
We believe this tiered approach will allow system builders to expend eeort where it is most needed.
To identify sources of variance within a distributed system, a ne-grained instrumentation mechanism is needed.
End-to-end tracing satisses this requirement, as it captures the detailed control ow of individual requests within and across the components of a distributed system with as little as overhead.
Many implementations exist, all of which are relatively similar [, , , ].
Note that nodes in this graph indicate instrumentation points reached by the request, whereas edges are annotated with performance metrics-in this case the latency between executing successive instrumentation points.
To identify functionality that should exhibit similar performance, VarianceFinder utilizes an informal expectation, common in distributed storage systems, that requests that take the same path through the system should incur similar performance costs.
For example, system builders generally expect requests whose metadata and data hit in a NFS server's cache to perform similarly, whereas they do not expect this for requests that take diierent paths because some miss in cache and others hit in it.
VarianceFinder groups request--ow graphs that exhibit the same structure-i.e., those that represent identical activities and execute the same trace points-into categories and calculates average response times, variances, and squared coeecients of variation (C ) for each.
C , which is dened as ( σ µ ) , is a normalized measure of variance and captures the intuition that categories whose standard deviation is much greater than the mean are worse oenders than those whose standard deviation is less than or close to the mean.
In practice, categories with C > are said to have high variance around the mean, whereas those with C < exhibit low variance around the mean.
e rst-tier output from VarianceFinder consists of the list of categories ranked by C value.
System builders can click through highly-ranked categories to see a graph view of the request structure, allowing them to determine whether it is important.
For example, a highly-ranked category that contains requests likely will be deemed important, whereas one that contains rare requests for the names of mounted volumes likely will not.
Once the system builder has selected an important highlyranked category, he can use VarianceFinder to localize its main sources of variance.
is is done by highlighting the highest-variance edges along the critical path of the category's requests.
Figure illustrates the overall process.
In some cases, an edge may exhibit high variance because of another edge-for example, an edge spanning a queue might display high variance because the component to which it sends data also does so.
To help system builders understand these dependencies, clicking on a highlighted edge will reveal other edges that have non-zero covariance with it.Knowing the edges responsible for the high variance allows the system builder to investigate the relevant areas of the system.
Variance from sources that he deems inadvertent should be reduced or eliminated.
Alternatively, he might decide that variance from certain sources should not or cannot be reduced because they are intentional or intrinsic.
In such cases, he should add tight instrumentation points around the source to serve as markers.
Automation tools that use these markers to increase their data granularity-especially those that use end-to-end traces directly [, , ]-will be able to make better predictions about areas surrounding the high-variance source.
Adding instrumentation can also help reveal previously unknown interesting behaviour.
e system builder might decide that an edge exhibits high variance because it encompasses too large of an area of the system, merging many dissimilar behaviours (e.g., cache hits and cache misses).
In such cases, extra trace points should be added to disambiguate them.
is paper argues that variance in key performance metrics needs to be addressed explicitly during design and implementation of distributed systems, if automated diagnosis is to become a reality.
But, much additional research is needed to understand how much variance can and should be reduced, the diiculty of doing so, and the resulting reduction in management eeort.To answer the above questions, it is important that we work to identify the breakdown of intentional, inadvertent, and intrinsic variance sources in distributed systems and datacenters.
To understand if the eeort required to reduce variance is worthwhile, the beneets of better automation must be quantiied by how real people utilize and react to automation tools, not via simulated experiments or fault injection.
If this tradeo falls strongly in favour of automation and intrinsic variance sources are the largest contributers, architectural changes to datacenter and hardware design may be necessary.
For example, system builders may need to increase the rate at which they adopt and develop strong performance isolation [] or insulation [] techniques.
Also, hardware manufacturers, such as disk drive vendors, may need to incorporate performance variance as a rst-class metric and strive to minimize it.Similarly, if (currently) intentional sources are the largest contributors, system builders may need to re-visit key design tradeos.
For example, they may need to consider using datacenter schedulers that emphasize predictability and low variance in job completion times [] instead of ones that dynamically maximize resource utilization at the cost of predictability and low variance [].
Finally, automated performance diagnosis is just one of many reasons why low variance is important in distributed systems and datacenters.
For example, strong service-level agreements are diicult to support without expectations of low variance.
As such, many of the arguments posed in this paper are applicable in a much broader sense.
ough automation in large distributed systems is a desirable goal, it cannot be achieved when variance is high.
is paper presents a framework for understanding and reducing variance in performance metrics so as to improve the quality of automated performance diagnosis tools.
We imagine that there are many other tools and design patterns for reducing variance and enhancing predictability.
In the interim, those building automation tools must consider whether the underlying system is predictable enough for their tools to be eeective.
We thank Michelle Mazurek, Ilari Shafer, and Soila Kavulya for their feedback.
We thank the members and companies of the PDL Consortium (including Actiio, APC, EMC, Emulex, Facebook, Fusion-io, Google, Hewlett-Packard Labs, Hitachi, Intel, Microso Research, NEC Labs, NetApp, Oracle, Panasas, Riverbed, Samsung, Seagate, STEC, Symantec, VMWare, and Western Digital) for their interest, insights, feedback, and support.
is research was sponsored in part by a Google research award, NSF grant CNS----, and by Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC).
