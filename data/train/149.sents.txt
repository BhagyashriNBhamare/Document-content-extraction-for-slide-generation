The sum capacity of the Gaussian vector broadcast channel is the saddle point of a Gaussian mutual information game in which the transmitter maximizes the mutual information by choosing the best transmit covariance matrix subject to a power constraint, and the receiver minimizes the mutual information by choosing a least-favorable noise covariance matrix subject to a diagonal constraint.
This result has been established using a decision-feedback equalization approach under the assumption that the least-favorable noise co-variance matrix is non-singular.
This paper generalizes the above result to the case where the least-favorable noise is singular.
In particular, it is shown that the least-favorable noise is not unique, and different least-favorable noise covariance matrices are related to each other by a linear estimation relation.
A communication model with a single transmitter sending independent information to multiple receivers at the same time is referred to as a broadcast channel.
Although the capacity region of a general broadcast channel is still an unsolved problem, recent progress has been made in the special case of the sum capacity of Gaussian vector broadcast channels [1] [2] [3] [4].
A Gaussian vector broadcast channel with K receivers can be modeled as follows:(1.1)Y i = H i X + Z i , i= 1, · · · , Kwhere X is a vector-valued transmit signal, Y i is the vector-valued receive signal, H i is the channel matrix, and Z i is the additive Gaussian vector noise at receiver i.
The solution to the Gaussian vector broadcast channel sum capacity problem is based on two key ideas.
First, an achievability result can be obtained using a precoding technique that effectively pre-subtracts multi-user interference at the transmitter.
The precoding technique is known as "writing-on-dirty-paper" [5].
Second, a converse theorem can be obtained using the idea that the broadcast channel capacity is bounded by a cooperative bound with a least-favorable noise.The least-favorable-noise converse is based on the following: the sum capacity of the broadcast channel is clearly bounded by the capacity of the vector channel with cooperative receivers: max I(X; Y 1 · · · Y K ).
Further, Sato [6] observed that the sum capacity of a broadcast channel depends only on the marginal distribution of the noises and not on their correlation.
This is because receivers of a broadcast channel cannot cooperate.
So, they are ignorant of the actual noise correlation.
Therefore, the sum capacity of a broadcast channel must be bounded by the minimum mutual information minimized over all possible joint distributions of the noises.
For a Gaussian vector broadcast channel with a convex input constraint, restricting the above minimax problem to joint Gaussian input and noise distributions is without loss of generality.
Further, it can be shown that this upper bound is tight.
The achievability of the minimax mutual information expression can be established using several different methods.
In [2], it is shown that if a decision-feedback equalizer is used as a joint receiver in the broadcast channel, the least-favorable noise would induce a feedforward matrix that is diagonal.
In addition, the feedback section can be moved to the transmitter as a precoder.
Thus, with a leastfavorable noise, no receiver cooperation in a decision-feedback equalizer is necessary and min I(X; Y 1 · · · Y K ) is achievable.
The above minimum can be further maximized over all possible input distributions.
Since min-max is equal to max-min in a convex-concave function, this implies that (1.2) is indeed the sum capacity of a Gaussian vector broadcast channel.
However, the proof in [2] appears to apply only to least-favorable noise covariances that are non-singular.
In [3] and [4], a completely different approach is taken.
Focusing on sum power constrained channels, [4] showed that the precoding region for a Gaussian vector broadcast channel is precisely a "dual" multiple access channel with the transmitter and receivers interchanged.
Based on this duality, [4] showed that the minimization for the least-favorable noise and the maximization for the dual multiple access channel sum capacity have the same solution.
In addition, [4] gives an explicit solution for the least-favorable noise.
The derivation in [4] is most transparent if the least-favorable noise is non-singular.
To circumvent the singular noise issue, [4] used a sequence of non-singular noise covariance to approximate the singular noise, thus proving the result in full generality.
Strictly speaking, however, this solution is not constructive when the least-favorable noise is singular.The singular noise issue is circumvented in [3] using a different technique.
Although also based on duality, the treatment in [3] involves a transformation of the noise covariance matrix for the broadcast channel to an input cost constraint for the multiple access channel.
This approach resolves the singularity issue, but it does not characterize the entire class of least-favorable noise.It should be noted that the duality approach [3] [4] applies only to Gaussian vector broadcast channels with a power constraint.
The decision-feedback equalization approach [2], on the other hand, applies to Gaussian vector broadcast channels with arbitrary convex constraints, and is therefore more general.
However, neither approach has explicitly dealt with the issue of singular least-favorable noise so far.Singular least-favorable noise is not necessarily an unlikely event.
In fact, the least-favorable noise is almost always singular in a broadcast channel where the number of receive dimensions is larger than the number of transmit dimensions.
The main purpose of this paper is to give a complete characterization of the leastfavorable noise for an arbitrary channel.
A combination of the decision-feedback equalization approach and the duality approach is taken.
The main results are, first, the decision-feedback equalizer approach can be generalized to the case of singular least-favorable noise.
Second, least-favorable noises are not unique, and they can be characterized via a duality between the broadcast channel and the multiple access channel.
Without loss of generality, consider a Gaussian vector broadcast channel with two receivers Y = HX + Z, whereH T = [H T 1 H T 2 ], Y T = [Y T 1 Y T 2 ] and Z T = [Z T 1 Z T 2 ].
The objective of this section is to show that(2.1) min Szz 1 2 log |HS xx H T + S zz | |S zz |is achievable even if the minimizing S zz is singular.
Recall that the minimization is over all S zz with fixed block diagonal terms.
The fixed block diagonals can be assumed to be identity matrices with no loss of generality.
This is a generalization of the earlier result in [2] where the case for non-singular S zz is solved.
The earlier result showed that if S zz satisfies the Karush-Kuhn-Tucker (KKT) condition for the minimization problem(2.2) S −1 zz − (HS xx H T + S zz ) −1 = 񮽙 Ψ 1 0 0 Ψ 2 񮽙 ,then there exists a decision-feedback equalizer (DFE) with a diagonal feedforward matrix.
When the channel matrix H is full rank, the minimizing noise covariance matrix is always non-singular.
However, this is not the case when H has more rows than columns.
This situation corresponds to the case where there are more receiver antennas than transmit antennas in the broadcast channel.
To extend the achievability result to accommodate singular least-favorable noises, both the KKT condition and the design of the DFE need to be generalized.
The decision-feedback equalizer that is required to accommodate singular noise differs from the conventional structure in one crucial aspect.
A conventional DFE never processes more output dimensions than input dimensions.
For example, in a channel with two transmit antennas and three receive antennas, a conventional DFE always reduces the two-by-three channel to a two-by-two channel by receiver joint processing.
However, in a broadcast channel, receivers cannot cooperate and joint processing is not possible.
Thus, a non-trivial generalization of the conventional DFE structure is required.Without loss of generality, S xx is assumed to be fixed and full-rank.
Let S xx = V ΣV T be an eigenvalue decomposition.
It is convenient to set H 񮽙 = HV √ ΣM to be the effective channel, and let the input covariance matrix be simply an identity matrix.
The choice of M will be made later.Suppose that S zz is a low-rank solution to the minimization problem (2.1).
Decompose S zz in the following form:(2.3) S zz = 񮽙 U 1 U 2 񮽙 񮽙 S ˜ z ˜ z 0 0 0 񮽙 񮽙 U T 1 U T 2 񮽙 ,whereS ˜ z ˜ z is invertible and [ U 1 U 2 ]is an orthonormal matrix.
The null space of S zz must be a subspace of the null space of the channel, (because otherwise the capacity would be infinity).
Thus, it must be possible to express the channel matrix H 񮽙 as:(2.4) H 񮽙 = U 1 ˜ H.The minimization problem now becomesmin S˜z˜zS˜zS˜z˜ S˜z˜z 1 2 log | ˜ H ˜ H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | (2.5) s.t. U 1 S ˜ z ˜ z U T1 has identities on the diagonal.
A necessary condition for the least-favorable noise is(2.6) S −1 ˜ z ˜ z − ( ˜ H ˜ H T + S ˜ z ˜ z ) −1 = U T 1 񮽙 Ψ 1 0 0 Ψ 2 񮽙 U 1 .
The objective of this section is to show that if the noise covariance matrix satisfies the above condition, then there exists a decision-feedback equalizer whose feedforward matrix is diagonal.
The development of decision-feedback equalizer follows that of [2].
The difference between the new proof and the original treatment is highlighted here.
The derivation of the decision-feedback equalizer is based on minimum meansquare error (MMSE) estimation.
The key observation is that MMSE estimator is not unique when both the channel and the noise are rank deficient.
Consider the MMSE estimation of X given Y = HX + Z.
The MMSE estimation is a matrix multiplicationˆXmultiplicationˆ multiplicationˆX = W Y, where W satisfies a normal equation:(2.7) W S yy = S xy .
Because both S zz and H are low rank, S yy is also low rank.
It is not difficult to verify that(2.8) W = ˜ H T ( ˜ H ˜ H T + S ˜ z ˜ z ) −1 U T 1 + SU T 2satisfies the normal equation for any choice of S.
A different DFE can be designed for each choice of S.
The rest of the section is devoted to showing that there exists a choice of S (along with a choice of M ) that makes the DFE feedforward matrix diagonal.
The first step in the design of a decision-feedback equalizer is noise whitening.
Define(2.9) ˆ H = S − 1 2 ˜ z ˜ z ˜ H.The MMSE estimator matrix W can be re-written as(2.10) W = S xxˆHxxˆ xxˆH T ( ˆ H ˆ H T + I) −1 S − 1 2 ˜ z ˜ z U T 1 + SU T 2The structure of the feedforward matrix involves a Cholesky factorization ofR b = E[(X − ˆ X)(X − ˆ X) T ].
It turns out that R b is independent of the choice of S. Using the matrix inversion lemma, it can be shown that(2.11) R b = ( ˆ H T ˆ H + I) −1 .
Further, W can be re-written as follows:(2.12) W = ( ˆ H T ˆ H + I) −1 ˆ H T S − 1 2 ˜ z ˜ z U T 1 + SU T 2 .
In a decision-feedback equalizer, half of the Cholesky factorizationR b = G −1 ∆ −1 G −Tis placed in the feedback section, and the remaining half is placed in the feedforward section.
Thus, the feedforward matrix has the form:(2.13)F = ∆ −1 G −T ˆ H T S − 1 2 ˜ z ˜ z U T 1 + S 񮽙 U T 2 .
The goal of this section is to use the generalized least-favorable noise condition (2.6) to show that F can be made diagonal.
Using the matrix inversion lemma,U T 1 񮽙 Ψ 1 0 0 Ψ 2 񮽙 U 1 = S −1 ˜ z ˜ z − ( ˜ H ˜ H T + S ˜ z ˜ z ) −1 (2.14) = ˆ H(I + ˆ H ˆ H T ) −1 ˆ H T .
With some algebra (see [2] for details), it is possible to prove that, by an appropriate choice of M , the Cholesky factorization of R b can be made to give:(2.15) ∆ − 1 2 G −T ˆ H T S − 1 2 ˜ z ˜ z = 񮽙 √ Ψ 1 0 0 √ Ψ 2 񮽙 .
Therefore, by choosing(2.16) S 񮽙 = ∆ − 1 2 񮽙 √ Ψ 1 0 0 √ Ψ 2 񮽙 U 2 ,the feedforward matrix becomes (2.17)F = ∆ − 1 2 񮽙 √ Ψ 1 0 0 √ Ψ 2 񮽙 (U 1 U T 1 + U 2 U T 2 ),which is diagonal since (2.18)U 1 U T 1 + U 2 U T 2 = [ U 1 U 1 ] 񮽙 U T 1 U T 2 񮽙 = I.To summarize, when the least-favorable noise is singular, it must satisfy a modified KKT condition (2.2).
The decision-feedback equalizer structure with the singular noise is not unique.
However, among the class of decision-feedback equalizers, there exists one whose feedforward matrix is diagonal.
This is true for any fixed S xx .
Thus, in a Gaussian vector broadcast channel, the minimum mutual information I(X; Y 1 , Y 2 ) is an achievable sum rate, even when the least favorable noise covariance matrix is singular.
3.1.
Uplink-Downlink Duality.
The previous section shows that whenever the noise covariance matrix satisfies the least-favorable noise condition, (2.2) or (2.6), the mutual information I(X; Y) is achievable in a broadcast channel even without receiver cooperation.
However, it does not give an explicit method to compute the least-favorable noise.
Neither (2.2) nor (2.6) appears to have a closedform solution.
Further, although I(X; Y) is a convex function of S zz , the numerical computation of the least-favorable noise is not necessarily easy, especially when the minimizing noise covariance matrix is singular.
Fortunately, there is one important special case for which the computation is easy.
This is when the transmit covariance S xx is being maximized at the same time.Consider a Gaussian vector broadcast channel with a power constraint tr(S xx ) ≤ P .
Sato's outer bound states that the sum capacity is less than min max I(X; Y), where the maximization is over the power constraint and the minimization is over all possible noise correlations.
The achievability result in the previous section states that a sum rate of max min I(X; Y) is achievable.
Since the mutual information is concave in S xx and convex in S zz , the minimization and maximization operations can be interchanged.
Thus, the sum capacity of a Gaussian vector broadcast channel is precisely(3.1) C = max Sxx min Szz 1 2 log |HS xx H T + S zz | |S zz | ,where tr(S xx ) ≤ P and S zz has fixed diagonal terms.
Surprisingly, this minimax problem has a special structure that gives it a particularly simple solution.
The new ingredient is the uplink-downlink duality 1 .
Uplink-downlink duality refers to the observation that the achievable rate region of a broadcast channel using "writing-on-dirty-paper" precoding is the same as the capacity region of a "dual" multiple access channel with the roles of the input and output terminals interchanged and with a sum power constraint across all input terminals.
This duality was established in [4] and [3].
In this section, a different derivation of duality is given.
The main purpose is to illustrate that the minimax problem can be solved efficiently via duality.To simplify matters, let's first assume that H is square and invertible, and also that the maximizing S xx and the least-favorable S zz are both full rank.
Further, assume that each receiver in the broadcast channel is equipped with a single-antenna.
The starting point of the new derivation is the KKT condition for the minimax problem (3.1).
Because the objective function in (3.1) is concave in S xx and convex in S zz , the KKT condition completely characterizes the saddle point.
The KKT condition is:H T (HS xx H T + S zz ) −1 H = λI (3.2) S −1 zz − (HS xx H T + S zz ) −1 = Ψ (3.3)where λ is the dual variable associated with the power constraint and Ψ is a diagonal matrix of dual variables associated with the diagonal constraint on the noise covariance matrix.
Multiplying (3.3) by H T on the left and H on the right and substituting in (3.2), we obtain(3.4) H T S −1 zz H = H T ΨH + λI.The above is equivalent to(3.5) H(H T ΨH + λI) −1 H T = S zz .
Observe that (3.5) is precisely the KKT condition for a multiple access channel with diagonal matrix Ψ as the transmit covariance matrix and tr(ΨS zz ) ≤ P as the input constraint.
Since S zz has 1's on the diagonal, the constraint is equivalent to a sum power constraint on Ψ.
After a proper scaling of the power constraint, Ψ, λ and S zz , it can be shown that (3.5) is precisely the KKT condition for the following 1 Uplink-downlink duality can be generalized beyond the power constrained broadcast channels.
Duality exists whenever the input constraint is a linear covariance constraint.
See [7] for a detailed discussion.optimization problem max D 1 2 log |H T DH + I| (3.6) s.t. D is diagonal trace(D) ≤ P, D ≥ 0,(where D = Ψ/λ.)
Thus, there is a duality between a multiple access channel and a broadcast channel.
The solution to the minimax problem (3.1) can be obtained from the solution to a maximization problem (3.6).
Since (3.6) is much easier to solve numerically (see e.g.[8] [9]), uplink-downlink duality gives an efficient way to solve the minimax problem (3.1).
The duality result suggests that the least favorable noise in the minimax problem can be computed via a multiple access channel.
In particular, (3.5) gives an explicit formula for the covariance matrix of the least-favorable noise.
This formula has also appeared in both [3] and [4].
However, (3.5) is derived assuming that the channel matrix H is square and invertible and both the least-favorable noise and the maximizing input covariance matrices are full rank.
In the general case, this is not necessarily true.
In particular, the maximizing input covariance matrix D of the dual multiple access channel may not have non-zero entries everywhere on its diagonal.
When the diagonals of D are all non-zero, the KKT condition for the maximization problem (3.6) ensures that the diagonal terms of H(H T ΨH + λI) −1 H T are all 1's.
Otherwise, slack variables need to be introduced and H(H T ΨH + λI) −1 H T does not necessarily have 1's on its diagonal.
Thus, it cannot be a valid choice of the least-favorable noise.The purpose of the rest of the section is to show that a least-favorable noise covariance can be obtained by adding a positive semi-definite matrix to H(H T ΨH + λI) −1 H T so that the sum of the two matrices has 1's on the diagonal.
Such a positive semi-definite matrix is not unique, so the least-favorable noise covariance matrix is not unique.
In fact, in some cases, the class of least favorable noises can be further enlarged by taking an additional step.
Consider the candidate leastfavorable noise, re-labeled as S (0) zz :(3.7) S (0) zz = H(H T ΨH + λI) −1 H T .
It is easy to see that the water-filling input covariance matrix S xx with respect to the channel H and the noise S (0) zz is:(3.8) S xx = (λI) −1 − (H T ΨH + λI) −1 .
Now, S xx may be rank deficient.
In this case, the dimension of H must be reduced, and the least-favorable noise must be re-computed using the reduced channel.
The re-computed noise covariance could have fewer 1's on its diagonal, thus enlarging the class of positive semi-definite matrices that can be added to its diagonal terms.
For the rest of the proof, it is assumed that this channel reduction step has already taken place.
The proof is fairly lengthy.
To simplify the algebra, the broadcast channel considered here is assumed to have only a single antenna at each receiver.
This special case exhibits the essential features of the least-favorable noise.The first step of the proof is to verify that even though its diagonals may not be 1's, the candidate noise covariance matrix (3.7) satisfies the least-favorable noise condition.
Assuming that channel has already been reduced, S zz can be re-written as(3.9) S (0) zz = U 1 S ˜ z ˜ z U T 1where S ˜ z ˜ z is invertible and columns of U 1 are orthonormal vectors.
Further, H can be re-written as(3.10) H = U 1 ˜ H,where˜Hwhere˜ where˜H is invertible.
The strategy is to solve the minimax problem over S ˜ z ˜ z and to show that the solution is S (0)zz = H(H T ΨH + λI) −1 H T .
The minimax problem is now max Sxx min S˜z˜zS˜z˜S˜z˜z 1 2 log | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | (3.11) s.t. tr(S xx ) ≤ P U 1 S ˜ z ˜ z U T 1 has 1 񮽙 s on diagonal S xx ≥ 0,The KKT condition of the minimax problem is:˜ H T ( ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z ) −1 ˜ H = λI (3.12) S −1 ˜ z ˜ z − ( ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z ) −1 = U T 1 ΨU 1 (3.13)Pre-multiplying the second equation above by˜Hby˜ by˜H T and post-multiplying by˜Hby˜ by˜H, it is not difficult to verify that (3.14) H(H T ΨH + λI) −1 H T = U 1 S ˜ z ˜ z U T 1 = S (0) zz .
Thus, the candidate S (0)zz satisfies the least-favorable noise condition.
The rest of the proof is devoted to showing that the least-favorable noise condition remains to be satisfied if a positive semi-definite matrix, denoted as S 񮽙 zz , is added to S (0) zz to makes its diagonal terms 1.
The proof is divided into several parts.
First, without loss of generality, the rows of H can be re-arranged so that the upper-left sub-matrix of S (0) zz has 1's on its diagonal.
This implies that Ψ only has positive entries on its upper-left diagonal, and S 񮽙 zz has non-zero entries only in the lower-right corner:(3.15) Ψ = 񮽙 Ψ 񮽙 0 0 0 񮽙 , S 񮽙 zz = 񮽙 0 0 0 S 񮽙 .
Let n be the number of receivers in the broadcast channel.
So, Ψ, S zz and S 񮽙 zz are n × n matrices.
Let the dimension of Ψ 񮽙 be k × k. So, the dimension of S is (n − k) × (n − k).
Also, let the rank of S (0) zz be r. Now, observe that k ≥ r.
The reason for this has to do with the channel reduction step mentioned before.
Note that from (3.8) the optimal S xx can be expressed as:S xx = (λI) −1 − (H T ΨH + λI) −1 (3.16) = (λI) −1 H T √ Ψ(I + √ ΨHH T √ Ψ) −1 √ ΨH(λI) −1 .
Thus, the rank of S xx is the same as the rank of H T ΨH.
The channel reduction step guarantees that S xx is full rank.
This implies that H T ΨH must be full rank.For this to be true, Ψ must have non-zero diagonal entries in at least r 񮽙 positions, where r 񮽙 is the rank of H. So, k ≥ r 񮽙 .
But, after the channel reduction, the rank of H is the same as the rank of S (0) zz .
This proves that k ≥ r.
In general, k can be strictly larger than r. Physically, this implies that in a broadcast channel, the number of active receivers can be larger than the number of transmit dimensions.
Also note that since the rank of S (0) zz is r and the rank of S 񮽙 zz is (n − k), the rank of S (0) zz + S 񮽙 zz is at most (n − k + r), which is not full rank if k is strictly larger than r.The next step in the proof involves the decomposition of S (0) zz + S 񮽙 zz along the direction U 1 .
The strategy is the following.
First, find an n × (n − k) matrix of orthonormal column vectors, denoted as U 2 , extending the space spanned by the columns of U 1 , such that S 񮽙 zz can be expressed as:(3.17) S 񮽙 zz = 񮽙 0 0 0 S 񮽙 = 񮽙 U 1 U 2 񮽙 񮽙 S 11 S 12 S 21 S 22 񮽙 񮽙 U T 1 U T 2 񮽙 .
Recall that U 1 contains r column vectors and U 2 contains n − k column vectors.
So, [ U 1 U 2 ] contains n − k + r vectors, which do not necessarily span the whole space.
(Note that this U 2 may be different from the U 2 in section 2.)
Partition U 1 and U 2 into sub-matrices:U 1 = 񮽙 u T 11 u T 12 񮽙 U 2 = 񮽙 u T 21 u T 22 񮽙 .
(3.18)Two useful facts about S ij and u ij are derived next.
First, (3.19) S 11 − S 12 S −1 22 S 21 = 0.
This is because(3.20) 񮽙 U T 1 U T 2 񮽙 񮽙 U 1 U 2 񮽙 = I, so, from (3.17), (3.21) 񮽙 S 11 S 12 S 21 S 22 񮽙 = 񮽙 u T 11 u T 12 u T 21 u T 22 񮽙 񮽙 0 0 0 S 񮽙 񮽙 u 11 u 21 u 12 u 22񮽙 .
This allows S ij to be solved explicitly: Now, using (3.30), the difference between the two can now be simplified:S 11 = u T(3.36) 񮽙 U T 1 ΨU 1 −U T 1 ΨU 1 S 12 S −1 22 −S −1 22 S 21 U T 1 ΨU 1 S −1 22 S 21 U T 1 ΨU 1 S 12 S −1 22 񮽙 .
To prove (3.31), it remains to show that the above is equal to(3.37) 񮽙 U T 1 U T 2 񮽙 Ψ 񮽙 U 1 U 2 񮽙 = 񮽙 U T 1 ΨU 1 U T 1 ΨU 2 U T 2 ΨU 1 U T 2 ΨU 2 񮽙 .
Comparing (3.36) with (3.37), it is clear that the two are equal if the following holds:(3.38) ΨU 2 = −ΨU 1 S 12 S −1 22 .
Recall that Ψ has non-zero entries only in its upper-left diagonal.
So, the left-hand side of (3.38) is(3.39) ΨU 2 = 񮽙 Ψ 񮽙 0 0 0 񮽙 񮽙 u 21 u 22 񮽙 = Ψ 񮽙 u 21 .
The right-hand side of (3.38) is By (3.25), the left-hand side is equal to the right-hand side.
This establishes the least-favorable noise condition (3.31).
(3.40) −ΨU 1 S 12 S −1 22 = −Ψ 񮽙 u 11 u T 12 u −T 22 .
The previous section shows that the least-favorable noise in a Gaussian minimax mutual information game is of the type S (0) zz + S 񮽙 zz .
Note that adding the term S 񮽙 zz does not change the mutual information I(X; Y).
This can be seen using the following argument.
The value of the minimax expression with (S xx , S (0) zz ) is (3.41) C = 1 2 log | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | .
With (S xx , S (0) zz + S 񮽙 zz ),|S 22 | · | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z + S 11 − S 12 S −1 22 S 21 | |S 22 | · |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z + S 11 − S 12 S −1 22 S 21 | = | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | .
Therefore the capacity expressions (3.41) and (3.42) have the same value.In some sense, S (0) zz is the "smallest" possible noise in the class of least-favorable noises.
This is because after the channel reduction step, HS xx H T is strictly positive everywhere in the span of the U 1 space.
Thus, for capacity not to become infinity, the noise covariance must also be strictly positive in the space spanned by U 1 .
Now, S (0) zz is the only noise covariance entirely in the U 1 space that satisfies the KKT condition.
Thus, S (0) zz must be the "smallest" noise covariance that satisfies the KKT condition.
Szz is not, of course, the only such noise.
The previous section shows that (S xx , S (0) zz + S 񮽙 zz ) also satisfies the least-favorable noise condition.
However, the previous section did not explicitly prove that S xx is the water-filling covariance matrix for S (0) zz +S 񮽙 zz .
But, because the addition of S 񮽙 zz can only reduce the minimax capacity, the fact that it does not shows that S xx must be the maximizing covariance for S (0) zz + S 񮽙 zz .
Thus, the entire class of (S xx , Szz + S 񮽙 zz ) are saddle-points of the minimax problem.
u + Z 񮽙 u as noise.
Clearly the second receiver cannot do better than the first one.
However, the second receiver does as well as the first receiver if the linear estimation of z 0 1 + z 񮽙 1 given z 񮽙 2 is exactly z 񮽙 1 .
Since z 0 1 and z 񮽙 1 are independent, this condition is equivalent to E[z 񮽙 1 |z 񮽙 2 ] = 0.
But, the covariance matrix of E[z 񮽙 1 |z 񮽙 2 ] is S 11 − S 12 S −1 22 S 21 .
Thus, the covariance matrix of the additional noise S 񮽙 zz must satisfy the condition (3.46) S 11 − S 12 S −1 22 S 21 = 0.
The above argument shows that the entire class of least-favorable noises are related to each other via a linear estimation relation 2 .
As the previous section shows, this relation is crucial in the derivation of the least-favorable condition.
This paper deals with the least-favorable noise in a Gaussian vector broadcast channel.
The least-favorable noise is not necessarily non-singular.
The achievability proof for the broadcast channel sum capacity is extended to the case of singular least-favorable noises.
The proof is based on the fact that the decision-feedback equalizer is not unique when the noise is singular.
Among all possible equalizers, there exists one with a diagonal feedforward matrix.In the second part of the paper, a new derivation for the duality between the multiple access channel and the broadcast channel is given.
The duality relation gives a natural characterization of the entire class of least-favorable noises.
The least-favorable noise is not unique, and all least-favorable noises are related to each other by a linear estimation relation. )
zz + S 񮽙 zz satisfies the least-favorable noise condition.
Starting from (3.13) (3.30) S −1The strategy is to simplify the above using Schur's complement formula: and the second matrix inversion in (3.31) can be expanded similarly: zz + S 񮽙 zz satisfies the least-favorable noise condition.
Starting from (3.13) (3.30) S −1The strategy is to simplify the above using Schur's complement formula: and the second matrix inversion in (3.31) can be expanded similarly:
