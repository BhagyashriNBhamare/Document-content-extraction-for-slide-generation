We introduce a novel stochastic local search algorithm for the vertex cover problem.
Compared to current exhaustive search techniques , our algorithm achieves excellent performance on a suite of problems drawn from the field of biology.
We also evaluate our performance on the commonly used DIMACS benchmarks for the related clique problem, finding that our approach is competitive with the current best stochastic local search algorithm for finding cliques.
On three very large problem instances, our algorithm establishes new records in solution quality.
Finding a minimum vertex cover of a graph is a well-known NP-hard problem [1].
Given an undirected graph G = (V, E), a vertex cover is defined as a subset of the vertices C ⊆ V , such that every edge of G has an endpoint in C, i.e. for all (u, v) ∈ E : u ∈ C or v ∈ C.
The task then is to find a vertex cover of minimum size, or, for the corresponding NP-complete decision problem k-vertex cover, to decide whether a vertex cover of size k exists.Applications of the vertex cover problem arise in network security, scheduling and VLSI design [2].
For example, finding a minimum vertex cover in a network corresponds to locating an optimal set of nodes on which to strategically place controllers such that they can monitor the data going through every link in the network.
Algorithms for minimum vertex cover can also be used to solve the closely related problem of finding a maximum clique, which has a range of applications in biology, such as identifying related protein sequences [3].
Over the past decade, numerous algorithms have been proposed for solving the vertex cover problem, including evolutionary algorithms [4], ant colony system approaches [5] and complete search [6].
A recent approach of the latter kind that has proven useful in biological applications is the work of Abu-Khzam et al. [3].
In this work, we introduce a stochastic local search algorithm for vertex cover, dubbed COVER (Cover Edges Randomly), and show that it achieves excellent performance on a large variety of benchmarks.
For the protein sequencing problems used by Abu-Khzam et al. [3], COVER is several orders of magnitude faster at finding the optimal solution than their approach.
On a suite of "hard" benchmarks with hidden optimal solutions [7], COVER performs very well and establishes a new record on the largest instance.
Furthermore, we compare the performance of COVER against state-of-the-art solvers for the related independent set and clique problems, showing that we achieve competitive results on a commonly used benchmark set.
We also explore the importance of knowing the target solution size k for our algorithm.The remainder of this article is organized as follows.
We first give an overview of related work, including algorithms for the clique and independent set problems.
We then describe the general idea of stochastic local search, and the specific details of our algorithm.
In Sec. 5, we describe the empirical evaluation of our algorithm conducted on a wide range of benchmarks, after which we conclude.
Various complete algorithms for k-vertex cover have been derived from the theory of fixed-parameter tractability (FPT) [6,8,9,3].
The characterizing feature of FPT algorithms is that their run-time is bounded by f (k) · p(n), where the dependence f (k) on the parameter k may be arbitrary, but the dependence p(n) on the graph size n is polynomial.
FPT algorithms generally work by first reducing the problem at hand in what is called a kernelization phase (transforming the problem to an equivalent problem with smaller parameter k), and then performing a bounded tree search on the remaining problem kernel.
Recently, FPT algorithms have been implemented in a parallel fashion [9,3].
Besides FPT methods, there are a number of heuristic approaches to the vertex cover problem.
Evans describes an evolutionary approach, and also reviews previous evolutionary algorithms for the problem [4].
Ant colony systems have been employed by Shyu et al. [10] and Gilmour and Dras [5].
Looking beyond work on vertex cover, a wide range of heuristic algorithms have been proposed for related problems, which we review in the following.
A clique of a graph G = (V, E) is a subset of the vertices K ⊆ V such that all vertices in K are pairwise connected.
An independent set of a graph G = (V, E) is a subset of the vertices S ⊆ V such that no two vertices in S are connected.
A maximum clique (maximum independent set) is a clique (independent set) of maximum cardinality for a given graph.Cliques and independent sets are closely related to vertex covers.
In particular, a vertex set S is an independent set of G iff V \ S is a vertex cover of G, and a vertex set K is a clique of G iff K is an independent set of the complementary graph G, in which two vertices are connected iff they are unconnected in G.Hence, the problems of computing maximum cliques or maximum independent sets can be reduced to the computation of minimum vertex covers.
In particular, all these problems are NP-hard, and the associated decision problems are NP-complete.
Many of the algorithms that have been proposed for computing maximum cliques or maximum independent sets in the past have been evaluated on the set of benchmark problems from the Second DIMACS Implementation Challenge in 1992-1993 [11].
The instances in this benchmark set are taken from a variety of application domains and also include examples that are specifically engineered to be "hard".
They can be considered the standard benchmark for algorithms that compute cliques or independent sets.For the maximum independent set, a recently proposed heuristic approach is the Widest Acyclic Orientation algorithm by Barbosa and Campos, which is competitive with the algorithms used in the original DIMACS challenge [12].
Even better results are obtained with the recent QSH algorithm by Busygin, Batenko and Pardalos [13], which outperforms the maximum clique algorithms used during the DIMACS challenge.
However, QSH fares badly on two classes of the DIMACS benchmarks.For maximum clique, the recently published DLS-MC algorithm by Pullan and Hoos seems to deliver the best results, clearly dominating previously published algorithms on the DIMACS benchmark set [14].
DLS-MC stands for Dynamic Local Search -Maximum Clique and works by iteratively growing a candidate solution (initially one vertex) and conducting a plateau search where no further improvement is possible.
When neither improvement nor new plateau steps are possible, search restarts from a single vertex.
The DLS-MC algorithm has since evolved into the Phased Local Search algorithm [15], which eliminates the need for tuning a parameter while producing similar results.
Stochastic Local Search (SLS) methods are a popular means of solving notoriously hard combinatorial problems.
They can be very effective while usually being conceptually simple [16].
For example, SLS algorithms are state of the art for solving Boolean Satisfiability problems [17].
In the following, we give a short description of SLS using the terminology of the textbook by Hoos and Stützle [16].
An SLS algorithm operates by searching in a space of candidate solutions for a problem π, where a candidate solution may not satisfy all of the constraints required by a solution.Starting from an initial candidate solution, an SLS algorithm iteratively performs a small step to a neighbouring candidate solution by perturbing its current candidate solution.
These steps, as well as the initialization of the search, may be randomized.
The perturbation steps are only based on local information and on some memory state m of the algorithm (for example a taboo list).
Pseudo code for a general SLS algorithm for a decision problem is shown in Alg.
1.
In line 1 of Alg.
1, a candidate solution s and an initial memory state m of the algorithm are computed.
In line 1, a termination criterion is used to determine whether the search should be terminated.
In line 1, the step function replaces the current candidate solution s by a new candidate solution from the neighbourhood of s, while replacing the memory state m with a corresponding new memory state.
If after terminating the search the candidate solution s is in the set S * (π) of solutions to π, then s is returned; otherwise, the algorithm fails.In the following section, we describe the COVER algorithm as a specific instance of an SLS algorithm by elucidating the exact choices we made for the nature of candidate solutions as well as the initialization, termination and step functions.
COVER is an SLS algorithm for k-vertex cover, i. e. it takes as input a graph G = (V, E) and a parameter k, and searches for a vertex cover of size k of G. Its candidate solutions are subsets of the vertices V of size k (which are not necessarily vertex covers).
The step to a neighbouring candidate solution consists of exchanging two vertices: a vertex u that is in the current candidate solution C is taken out of C, and a vertex v which is not currently in C is put into C.The initial candidate solution is constructed greedily.
In detail, COVER builds C by iteratively adding vertices that have a maximum number of incident edges which are not covered by C, i. e. they have no endpoint in C, until the cardinality of C is k.
When several vertices satisfy the criterion for inclusion in C, COVER selects one of them randomly, with uniform probabilities.
Favouring vertices of high degree is a common heuristic for vertex cover algorithms.
In fact, we find that some benchmark problems are even solved by this initialization step alone, e. g. the p-hat class of the DIMACS benchmark set.The termination criterion COVER uses is straightforward: at each step, it tests whether its current candidate solution is a vertex cover of G.
The algorithm terminates when either a vertex cover is found, or when a maximum number of steps, denoted by MAX ITERATIONS, has been reached.The most influential part of an SLS algorithm is the definition of its step function.
COVER uses several heuristic criteria to choose which two vertices to exchange in C, but also utilizes a substantial element of randomness, thus striking a balance between guided search and the diversity that is necessary to escape local optima.
This balance is achieved with a simple division of responsibilities: the vertex to be taken out of C is chosen mainly according to heuristics, while the vertex to be put into C is chosen almost randomly.When choosing possible candidates for inclusion in C, COVER selects uniformly at random an edge e that is not covered (following the strategy popularized by Selman and Kautz for SAT [18]).
The vertex added to C is then chosen from one of the endpoints of e, ensuring that e will be covered in the successive candidate solution.
When choosing which one of the two endpoints of e to include and which vertex to take out of the current candidate solution, COVER uses a heuristic based on an edge weighting scheme: with each edge of G, we associate a positive real number.
Intuitively, these weights indicate for each edge how "difficult" it is to cover it -i. e. how difficult it is to find a candidate solution that contains one of the endpoints of that edge.In the beginning, all edge weights are initialized to a small constant (0.05).
In each of the following iterations, COVER adds 1 to the weights of all edges that are not covered.
We then derive vertex weights from the weights of edges incident to the vertex.
We say that a vertex v potentially covers an incident edge (v, u) if u is not in the current candidate solution C. Let the weight of a vertex v, weight(v), be the weighted sum of all edges that vertex v potentially covers.
An exchange of two vertices a and b, where a is taken out of C and b is put into C then results in a gain defined as weight(b) − weight(a) + δ, where δ is the weight of the edge between a and b if they are connected, and zero otherwise.
COVER tries to maximize this gain, thereby covering the more "difficult" edges of higher weight with greater priority.
Using weights to direct the search of stochastic local search algorithms is popular practice, and a similar approach has led to very good results for the Boolean Satisfiability problem [19].
COVER also employs a taboo list of size 2, keeping track of the vertices last inserted into C and last removed from C.
This prevents it from immediately reversing a decision made in the last iteration.
Moreover, COVER remembers for each vertex the iteration count at which it was last removed from C.
When inserting a vertex into C, COVER favours vertices that have not been in the cover recently.
In particular, this "time stamp" criterion is used to break ties between all insertion candidates that result in maximum gain and are not taboo.
When removing vertices from the candidate solution, COVER chooses randomly with uniform probability between all removal candidates.Pseudo code for the algorithm is given in Alg.
2.
We evaluate the performance of COVER on an extensive set of benchmarks from three different sources.
In Sec. 5.1, we use graphs representing biological real-world problems, which were kindly provided to us by Abu-Khzam et al. [3].
In Sec. 5.2, we report results on the BHOSLIB benchmark suite [7], a set ofAlgorithm 2 COVER(G, k, MAX ITERATIONS)1: initialize C greedily with |C| = k 2: initialize weights 3: iteration number = 1 4: while exists uncovered edge and iteration number < MAX ITERATIONS do 5: choose uncovered edge e = (u1, u2) randomly 6: choose vertices u ∈ {u1, u2} and v ∈ C according to max gain criterion 7:C = C\{v} 8: C = C ∪ {u} 9:taboo list = {v, u} 10:u.time stamp = iteration number 11:update weights 12:increase iteration number by 1 13: end while graphs with "hidden optimal solutions" that are specifically designed to be hard to solve.
In Sec. 5.3, we evaluate our performance on the (complement) graphs of the Second DIMACS Implementation Challenge for the maximum clique problem [11].
For each benchmark set, we compare against the best results we could find in literature.
Unfortunately, we have to compare against a different system for each benchmark set, as we could not obtain the respective programs to run them on the other benchmark sets.Our experiments were run on a machine with a 2.13 GHz CPU with 2 GB RAM.
For comparing different algorithms on the DIMACS benchmark suite, three machine benchmarks are available from the DIMACS web site.
When run on our machine, we obtained run-times of 0.51 seconds for r300.5, 3.01 seconds for r400.
5, and 11.31 seconds for r500.
5.
Due to the random elements of COVER, we ran the algorithm 100 times, with different random seeds, on each instance of each experiment described in this section.
In all cases, the MAX ITERATIONS parameter was set to 100,000,000.
For each instance, we report the following information:-solution quality: This is denoted as a triple a-b-c, where a is the number of runs (out of 100) in which the algorithm found a vertex cover with the minimal (or lowest known) cardinality k * ; b is the number of runs in which a vertex cover of size k * was not found, but a vertex cover of size k * + 1 was found; and c is the number of runs where only vertex covers of cardinality k * + 2 or worse could be found.
-median run-time and 1st quartile run-time: We ordered the outcomes of the 100 runs by the cardinality of the solution found (the lower, the better) and, in case of vertex covers of equal cardinality, by search time.
Median run-time is the run-time for the median element in this sequence (i. e., the 50th-best result), and 1st quartile run-time is the run-time for the element at the 1st quartile (i. e., the 25th-best result).
Thus, median run-time is indicative of a "typical run" of the algorithm, and 1st quartile run-time is indicative of the typical performance one might obtain by running the algorithm repeatedly, with four restarts.
If the median (or 1st quartile) run did not achieve an optimal solution, the run-time result is reported in parentheses.
The graphs used in this section originate from phylogeny and correspond to protein sequencing data [3].
The task here is to find maximum sets of closely correlated protein sequences, which can be directly cast as a (weighted) maximum clique problem.
Our input graphs are obtained from the biological problems as follows: first a weighted graph is constructed with vertices corresponding to protein sequences, and weighted edges between vertices corresponding to the extent of correlation between two sequences.
Then, for a chosen threshold all edges with weights below the threshold are removed.
The complement of this graph is the input for a vertex cover algorithm.
The problems we use here were obtained from Abu-Khzam et al., who use them in their paper on parallel FPT algorithms for vertex cover [3].
Our run-time results, as well as the results obtained by the algorithm of AbuKhzam et al. which we will denote by P-FPT (parallel FPT ) in the following, are shown in Tab.
1.
COVER found optimal solutions in all runs on all graphs except for the globin-15 instance, where 98 of the 100 runs found optimal solutions, with the other two finding solutions of size k * + 1 and k * + 3, respectively.
Table 1.
Results on biological problems.
|V | and |E| denote the number of vertices and edges of the input graph, k * the minimum vertex cover size, as determined by Abu-Khzam et al.
The performance metrics for COVER (solution quality, 1st quartile run-time, median run-time) are explained in detail at the start of this section.
The last column denotes the run-time of the P-FPT algorithm.
All run-times are in seconds.The difference in run-time between the two approaches is compelling.
A problem that took P-FPT more than two hours to solve is solved by COVER in less than one second.
Of course, our local search algorithm may fail to find an optimal solution, while P-FPT searches exhaustively, guaranteeing optimality.
However, as the results show, COVER reliably finds optimal vertex covers.The approach used by Abu-Khzam et al. has reportedly delivered valuable results in collaboration projects with biologists: they report that, based on the cliques they derived from microarray data, "neurobiologists have identified what appear to be both network structures and gene roles in intra-cellular transport that were previously unrecognized".
The vast difference in run-time between the two approaches prompts the question of what could be gained by using a local search algorithm like COVER in practice.It is noteworthy that for many application domains of vertex cover, a nearoptimal solution may be sufficient.
In the protein sequencing domain, for example, choosing a certain correlation threshold has a somewhat arbitrary influence on the size of the greatest clique, which has more impact on the resulting solutions than the fact that the algorithm might be slightly suboptimal.
Our algorithm is likely to provide solutions that are very close to the optimal, even for problems where the optimal solution is difficult to find.
Hence, for solving large real-world problems, an incomplete algorithm like ours might prove to be more fruitful than a complete, but prohibitively slow algorithm.
The BHOSLIB problems ("Benchmarks with Hidden Optimal Solutions") [7] result from translating binary Boolean Satisfiability problems that were generated randomly according to the model RB [20].
The satisfiability versions of these benchmarks are guaranteed to be satisfiable, and the model parameters were set to such values that the instances are in the phase transition area of model RB.
They have been proven to be hard both theoretically and in practice [20].
The full BHOSLIB set of instances we use here is available on the Internet [7].
Some of these instances were also used in the 2004 SAT competition [21].
The problem instances are grouped by size into 8 groups, with 5 graphs per group, where all graphs of a group have the same number of vertices and edges.
In addition to the 40 instances that form the actual benchmark suite, there is a single "challenge problem", a very large graph with 4,000 vertices and 572,774 edges.
The minimum vertex cover for this instance has size 3,900.
The first two instances from each of the groups 3-8 (the frb40 -frb59 graphs) were used in the 2004 SAT competition.
From the 6th group (frb53 ) onwards, none of the 55 solvers in the 2004 SAT competition was able to solve either of the two instances within a time limit of 10 minutes.
For the very large instance, the best solution found up to now was a vertex cover of size 3,904, using a run-time of 3,743 seconds on a Pentium IV 3.4GHz/512MB machine [7].
This solution was obtained by translating the problem to a propositional logic formula extended with cardinality atoms, and using a dedicated solver [22].
The results obtained by COVER on these benchmarks are displayed in Tab.
2.
The increasing difficulty of the instances is apparent both in the increasing runtimes, as the graph sizes grow, and in the fact that COVER does not find optimal solutions consistently, i.e. in all runs.
However, COVER does find optimal solutions for each graph at least once, and the sizes of the vertex covers it finds never exceed the minimum by more than 1.
For comparison, Gilmour and Dras recently developed a series of ant colony system algorithms for the vertex cover problem, evaluated on the BHOSLIB benchmarks [5].
They do not report run-times or solution results for individual graphs, but only the average vertex cover size found over all BHOSLIB graphs.
(We obtained the detailed results shown in Tab.
2 from personal communications.)
The best result they achieve, using the CKACS algorithm, is an average vertex cover size of 975.875, while 967.25 is the optimal value.
This means that the vertex covers found by CKACS, on average, have 8.625 more vertices than an optimal solution.
In comparison, COVER achieves an average vertex cover size of 967.50 on the BHOSLIB suite, i. e. the vertex covers it finds are only off by 0.25 on average.On the challenge problem, COVER does not find an optimal solution.
Indeed, the designer of the BHOSLIB benchmark set conjectures that this problem will not be solved on a PC in less than a day within the next two decades [7].
However, the COVER algorithm finds a solution of size 3,903 within 71 seconds, surpassing the best solution known so far in terms of both quality and run-time.
The DIMACS benchmark set is taken from the Second DIMACS Implementation Challenge (1992)(1993) [11], a competition targeting the maximum clique, graph colouring, and satisfiability problems.
The maximum clique benchmarks from this competition have since been used in many publications as a reference point for new algorithms [4,[12][13][14][15].
The benchmark set comprises 80 problems from a variety of applications.
For example, the C-fat family is motivated by fault diagnosis, the johnson and hamming graphs by coding theory, the keller group is based on Keller's conjecture on tilings using hypercubes, and the MANN graphs derive from the Steiner Triple Problem [11].
In addition, there are graphs generated randomly according to various models.
For example, the brock family is generated by explicitly incorporating low-degree vertices into the cover, in order to defeat algorithms that search greedily with respect to vertex degrees [23].
The sizes of the graphs range from less than 30 vertices and ∼200 edges to more than 3000 vertices and ∼5,000,000 edges.The results for COVER are shown in Tab.
3 and 4.
For comparison, the tables also contain the results obtained by Pullan and Hoos with the DLS-MC algorithm [14].
DLS-MC was also run 100 times with the same limit on iterations as COVER.
The times reported are the ones published by Pullan and Hoos [14], and refer to a 2.
2GHz Pentium IV machine with 512 MB RAM, which executed the DIMACS machine benchmarks r300.
5 (r400.
5, r500.
5) in 0.72 (4.47, 17.44) seconds.
The run-times are thus roughly comparable, our machine being 30-35% faster according to this measure.
The "avg."
column shows the mean run-time for COVER across all runs where optimal solutions were found, for graphs where both algorithms found optimal solutions.
This allows a direct comparison with the corresponding column for DLS-MC, taken from the article by Pullan and Hoos and determined by the same method.
Note that this only compares the run-time for the cases where an optimal solution was found, and thus ignores runs where the found vertex cover was sub-optimal.
Unfortunately, a direct comparison of our median run-time criterion (which we consider more indicative of actual performance because it is also influenced by sub-optimal runs) is not possible with the published results on DLS-MC.
In 75 of the 80 benchmarks, COVER finds a vertex cover of the putative minimum size for that instance.
Note that it is only for some graphs of the brock family that COVER never finds optimal results.
For the brock graphs, the cardinality of the vertex covers found by COVER can become as large as k * + 5 in the worst case.
This is not surprising, as COVER favours vertices of high degree, which generally is a helpful heuristic for finding minimum vertex covers.
The brock graphs, however, were explicitly designed to counteract this approach.Of the 75 instances where COVER finds an optimal solution, in 69 cases it does so consistently, i. e. in all 100 runs.
For the remaining instances, there are occasional sub-optimal runs, but COVER always finds vertex covers of cardinality k * + 2 or less.
For MANN a81, the putatively hardest problem in this benchmark set, COVER finds an optimal solution in 4 runs, is off by 1 in 3 runs, and off by 2 in the remaining 93 runs.
1000 127754 932 100 Comparing against the results of the DLS-MC algorithm, we find that the two algorithms are largely competitive.
In fact, many of the benchmarks seem to be too easy for a state-of-the-art solver nowadays.
On the graph families c-fat, DSJC, gen, hamming, johnson, p hat, san and sanr, both DLS-MC and COVER consistently find optimal solutions within extremely short run-times.
However, on MANN a45 and MANN a81, COVER significantly outperforms the DLS-MC algorithm.
On these graphs, DLS-MC does not find an optimal solution.
On the hard MANN a81 instance, DLS-MC indeed only finds solutions that are of distance 2 or more from the optimum, while COVER finds optimal solutions for both graphs.
In fact, to our knowledge COVER is the first algorithm to find covers of this quality for the two MANN graphs.On the other hand, on the brock family DLS-MC shows far better results than COVER.
This can be explained by the fact that DLS-MC uses a parameter called penalty-delay, which Pullan and Hoos hand-tuned for each graph to achieve the best possible performance.
While for almost all other graphs this parameter was set to a value between 1 and 5, it was set to 15 and 45 for the larger brock graphs, encouraging DLS-MC to quite drastically change its usual behaviour in these cases [14].
We conclude that, despite the fact that COVER is designed for vertex cover problems and DLS-MC is designed for clique problems, COVER is competitive with DLS-MC on clique benchmarks.
COVER furthermore has the advantage of requiring no parameters, while achieving excellent results but for one special class of artificial graphs.
To further understand the run-time complexity of COVER, we conduct a set of experiments aimed at determining the importance of knowing k, the target cover size.
Most state-of-the-art solvers, including the ones we compared against in this paper, are, like COVER, solving the k-vertex cover problem (or k-clique, respectively).
In practice, however, we do not usually know the optimal value for k. Instead, we want to find a minimum (or close to minimum) vertex cover of unknown size.
The question thus arises whether COVER can be used efficiently for finding a minimum vertex cover by iteratively searching for various decreasing values of k. Specifically, we are interested in determining how much run-time is spent searching for several values of k as opposed to just searching with a known optimal value k * .
We expect that it is much easier to find solutions that are suboptimal than ones that are optimal, and that indeed only the last few runs where k is close to k * substantially influence run-time.
Graph Run-time(s) k * k * + 1 k * + 2 k * + 3 > k * + 3 Globin7To test this hypothesis, we extend COVER to an iterative version COVER-I, which runs without a parameter k, as follows.
First, COVER-I greedily computes a vertex cover for the input graph.
This is done much in the same way as COVER computes an initial candidate solution.
Instead of stopping when the size of the candidate solution reaches a prespecified parameter, however, COVER-I keeps adding vertices until the candidate solution is indeed a vertex cover.
The size k of this vertex cover is thus an upper bound for the optimal value k * .
COVER-I then iteratively calls COVER as a subroutine, decreasing k each time COVER succeeds in finding a solution within the usual limit of 100,000,000 iterations.
When COVER fails to find a solution, COVER-I stops and returns the last solution found.For our experiment, we select a representative set of graphs containing instances from all three benchmark suites in varying sizes.
The results are displayed in Tab.
5.
The run-time column shows total run-time for COVER-I for the given graphs, summed up for 25 different random seeds, to give an impression of the relative difficulty of these instances.
The column k * shows the percentage of total run-time spent on the final iteration (producing the optimal solution), with columns k * + 1, k * + 2 etc. referring to the previous iterations.The results largely confirm our expectations.
For small graphs, where the search times for the optimal value k * are already short, run-time is sometimes spread out fairly evenly across iterations; but for the larger graphs, the amount of time spent in the ultimate iteration dominates the total run-time of COVER-I.
We have presented a stochastic local search algorithm for the vertex cover problem, COVER, and evaluated its performance on a wide variety of benchmarks.
COVER is surprisingly effective while being conceptually simple and not requiring any instance-dependent parameters.
For biological real-world problems, COVER finds optimal solutions in just a fraction of the time needed by a complete search, which leads us to believe that COVER is a valuable approach for practical problems.
On the hard BHOSLIB benchmark set, COVER vastly improves on existing results and sets a new record for the 20-year challenge problem.Compared to the state-of-the-art solver DLS-MC for the maximum clique problem, COVER shows competitive results on the DIMACS suite.
We emphasize the fact that unlike DLS-MC and many algorithms proposed in the literature previously, COVER has not been tuned in any way to the benchmark sets we evaluated it on.
The excellent performance of COVER is further underlined by the fact that it sets a new record in solution quality on two large benchmark instances of the DIMACS set.
However, COVER did not perform well on the brock family of graphs from the DIMACS test set.
An obvious direction of future work is therefore to develop further techniques to more reliably escape local minima during search.
NICTA is funded by the Australian Government's Backing Australia's Ability initiative, in part through the Australian Research Council.
We thank Abdul Sattar for his input on the direction of this project and Duc Nghia Pham for helpful discussions.
We also thank Stephen Gilmour, Wayne Pullan and Michael Langston for providing problem graphs, results, and helpful suggestions.
