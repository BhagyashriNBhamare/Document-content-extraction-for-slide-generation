IDS research still needs to strengthen mathematical foundations and theoretic guidelines.
In this paper, we build a formal framework, based on information theory, for analyzing and quantifying the effectiveness of an IDS.
We firstly present a formal IDS model, then analyze it following an information-theoretic approach.
Thus, we propose a set of information-theoretic metrics that can quantitatively measure the effectiveness of an IDS in terms of feature representation capability, classification information loss, and overall intrusion detection capability.
We establish a link to relate these metrics, and prove a fundamental upper bound on the intrusion detection capability of an IDS.
Our framework is a practical theory which is data trace driven and evaluation oriented in this area.
In addition to grounding IDS research on a mathematical theory for formal study, this framework provides practical guidelines for IDS fine-tuning, evaluation and design, that is, the provided set of metrics greatly facilitates a static/dynamic fine-tuning of an IDS to achieve optimal operation and a fine-grained means to evaluate IDS performance and improve IDS design.
We conduct experiments to demonstrate the utility of our framework in practice.
As an essential component of the defense-in-depth strategy, intrusion detection systems (IDSs) have achieved more and more attention in both academic and industry.
A number of IDSs have been developed in the last two decades [8].
Research work in the IDS field mainly focuses on how to construct a new detector based on some new idea so that the IDS can detect certain attacks with reasonable accuracy (in terms of false positives and false negatives).
These are important topics, of course.
However, very little work has been conducted on the fundamental theory.
As a result, unlike cryptography, which now has a solid mathematical ground based on probability theory and the random oracle model, the IDS community still lacks a mathematical foundation that can be used to reason about the effectiveness of an IDS formally and practically.
It is definitely necessary to base IDS research on a solid mathematical background [19] that can lead to a better understanding, evaluation, and design of an IDS.
Such a theoretic framework should be mathematically sound, and useful in analyzing and quantifying the effectiveness of an IDS in both theory and practice.In this paper, we investigate a novel theoretic framework for IDS research based on information theory.
The basic observation is that, the intrusion detection process is actually a series of data processing and transformation procedures.
This motivates us to use information theory, which is successfully applied in the area of communication (which is also a data/signal processing and transformation process), to study the efficiency of the intrusion detection process.
We significantly extend our previous work on an information-theoretic measure of the intrusion detection capability [10], which only treats the IDS as a black box and measures the overall performance.
In this paper, we further look into the basic components and architecture of an IDS, and apply information-theoretic analysis on the detailed intrusion detection procedure.
Specifically, we make the following contributions in this paper:1.
We present a formal model of an IDS in Section 2 using an eight-tuple representation containing four data structures and four algorithms, which are used in three procedures, i.e., feature selection, profiling, and detection.
This IDS model unifies signature-based and anomaly-based IDSs, thus, we can reason about all these IDSs using the same analytical approach.
We also show how existing realistic IDSs such as PAYL [31] and Snort [26] fit into our model.2.
We perform a fine-grained information-theoretic analysis on the IDS model in Section 4.
The detection procedure can be considered as a Markov transition chain in which two algorithms, i.e., a data reduction and representation algorithm R and a classification algorithm C, are sequentially applied.
This establishes a connection between intrusion detection and information theory.
Further, we present a series of information-theoretic metrics that can quantitatively measure the effectiveness of an IDS and its components.
We define the measures of feature representation capability (C R ), classification information loss (L C ), as well as the overall intrusion detection capability (C ID , [10]).
We establish a link among these metrics, and prove a fundamental upper bound of C ID .
The task of the IDS is to faithfully reflect the ground truth about intrusion information in observed data.
If we assume the original ground truth information is 1 (normalized), when the data reduction and representation algorithm R is applied, this information is reduced to C R .
After the classification algorithm C is performed, there is further L C amount of information loss.
The end result is C ID , the overall capability of the IDS.
We also discuss how the metrics can be used in a robust way to tolerate uncertainties and possible estimation errors of parameters in practice.3.
This framework provides practical guidelines for fine-tuning, evaluation and design of IDSs.
With the help of C ID , one can select the optimal operating point (where C ID is maximized) for an IDS, and we provide a concrete example for dynamically fine-tuning PAYL [31].
With the whole set of metrics, we provide a fine-grained analysis and quantification on the effectiveness of an IDS and its components.
This yields a guideline for IDS design improvement, in particular, whether and how the feature representation or classification algorithm is (the bottleneck) to be improved.
Experiments are conducted to show the utility of our framework in Section 5.
Note that in this paper we are not dealing with other important IDS performance issues, such as resilience to stress [25] and ability to resist attacks directed at the IDS [24,23].
These are different research topics beyond the scope of this paper.
Also we are not trying to address cost related issues in IDS analysis because cost factor is subjective, but we are building an objective theoretic framework.
Finally, we need to point out that although our technique/framework may be applicable to other domains (e.g., to analyze a general classifier), we focus on the intrusion detection (specifically network-based intrusion detection) field.
In order to formally reason and analyze an IDS, we firstly present a formal model of the IDS.
Briefly, an IDS is represented as an eight-tuple (D, Σ, F, K, S, R, P, C), in which the first four items are data structures, and the last four are algorithms.
Note that whenever we analyze and evaluate any IDS, we cannot talk about it without dealing with its data source.
After all, our IDS model and framework are data trace driven and evaluation oriented.D: the data source that an IDS will examine and analyze.
Essentially this is a stream of consecutive data units.
Since each IDS has its own unit of analysis, e.g., packet level or flow level for a network-based IDS (NIDS), without loss of generality, wedefine D = (D 1 , D 2 , ...)where D i is an analysis unit of data for the target IDS and D i ∈ {d 1 , d 2 , ...}, d j is the possible data unit.
For example, an NIDS uses network traffic (packet stream), so the data source is a packet stream P = (P 1 , P 2 , ...).
For a host-based IDS (HIDS) using system call sequence, the data source is a system call stream C = (C 1 , C 2 , ...).
In this paper, we mainly take network data as our example, and packet as our data unit.Σ: a finite set of data states indicating whether the data unit D i is normal or anomalous (or further what type of intrusion).
For convenience, we define an IDS oracle Oracle IDS which accepts any query with data unit D i , and outputs an indication whether the unit is normal or anomalous.
The IDS oracle knows the ground truth so it will always tell the truth 3 .
Then for every data unit D i , its state is Oracle IDS (D i ).
The space of this state set is finite.
For anomaly detection, Σ = {N ormal, Anomalous}, or simply Σ = {N, A}, or Σ = {0, 1} where 0 denotes normal and 1 denotes anomalous.
For misuse detection, we can let Σ = {N ormal, AttackT ype 1 , AttackT ype 2 , ...}, or Σ = {N, A 1 , A 2 , ...}.
F: a feature vector contains a finite number of features, formally F =< f 1 , f 2 , ..., f n >.
Every feature is a meaningful attribute of a data unit.
For example, f 1 could be the protocol type (TCP, UDP, ICMP, etc.), f 2 could be the port number.
Each feature has its own meaningful domain (called feature space) which is a set of discrete or continuous values (either numerical or nominal).
The full range of F is the product of the ranges of all the features.
We denote it as Range(F ) = f 1 × f 2 ... × f n .
K: the knowledge base about the profiles of normal/anomalous data.
This knowledge base consists of profiling model (stored in some data structures) of normal and/or attack information.
The detailed structure of K is possibly different for every IDS.
It could be a tree, a Markov model, a Petri net, a rule set, a signature base, etc.
For a signature-based NIDS, K is its rule set which contains only the attack profiling model (i.e., intrusion signatures).
For an anomaly NIDS, K is mainly the profile of the normal traffic.
Any activity that deviates the normal profile is considered as anomaly.S: feature selection algorithm.
Given some D and the corresponding states Oracle IDS (D) (note sometimes only partial or even no such state information available), this algorithm should return several features f i for the IDS to use.
Although there is some preliminary effort to automatically generate worm signature [14,22] for misuse IDSs as part of their features, generally speaking S still highly depends on domain knowledge and is normally conducted manually.
The automatic selection or generation of features for both anomaly and misuse IDSs remains a grand challenge.
The quality of features is one of the most important factors that will affect the effectiveness of an IDS.
R: data reduction and representation algorithm.
When processing data, the IDS will firstly reduce the data and represent it in the feature space.
This is a mapping/transition function, mapping the given data to a proper feature vector representation, namely R : D → F. P: profiling algorithm, which is the procedure of generating the profiling knowledge base K. Given all the feature vector representations of data and their corresponding states, this algorithm will return the profiling knowledge base K.C: classification algorithm.
It is a mapping/transition function that maps the feature vector representation of given data to some states (it will also use the profiling base K in classification decision).
Formally, C : F → Σ.Most IDSs work in three steps.1.
Feature selection procedure ( Fig.1(a)).
When we are developing an IDS, this is one of the first steps.
Once the proper feature set is defined, it will be used in the following procedures.
Normally, the feature selection procedure is conducted once, only during development.2.
Profiling procedure ( Fig.1(b), sometimes also called training procedure 4 ).
We will run P (also involving R) on a sufficiently large amount of training data and get the profiling knowledge base K. Normally this procedure is performed once, only during development/training.
In some situation, this procedure can be performed dynamically/periodically to update K.3.
Detection procedure (Fig.1(c)).
In this procedure, the IDS is used to detect intrusions in the data stream.
This is the most important and frequently used procedure.
We will perform an information-theoretic analysis on this procedure in Section 4.
Our IDS model unifies anomaly detection and misuse detection.
In Appendix 9, we examine two representative IDSs, i.e., PAYL(Payload Anomaly Detection [31]) and Snort [26], to show how real world IDSs fit into our model.
Prior to introducing our information-theoretic framework for IDSs, we will first review a few basic concepts in information theory [6] to assist readers to follow our analysis.Entropy: The entropy (or self-information) H(X) of a discrete random variable X is defined as H(X) = − x p(x) log p(x).
This definition, also known as Shannon entropy, measures the uncertainty of X.
A smaller value of H(X) indicates that X is less uncertain (more regular).
The definition of entropy can also be easily extended to the case of jointly distributed random variables.
The detection procedure (Fig.1(c)) of an IDS is the most important process for us to analyze.
For simplicity, we will assume an anomaly NIDS with Σ = {N, A} in all the following analysis (the analysis can be extended to an IDS with more than two states).
We Y i = C(R(D i )).
Note that here we assume there is always an IDS output (decision) corresponding to each input.
Although a real IDS only needs to output alerts when there is an intrusion, this does not affect our analysis.Thus, the detection process is the Markov chain of X o → Z o → Y (data→representation→alert) as shown in Fig.2(a), which we refer to as the original model.
The input data are processed in sequence through two algorithms, R and C.
The mapping from X o to Z o is the result of R.
The mapping from Z o to Y is the result of C.
The simple observation of this Markov chain data processing procedure motivates us to use information theory to analyze the process.
Intuitively, we can roughly consider R as an encoding algorithm that uses feature vector to encode the original data unit.
And then, C, as a decoding algorithm, decodes the feature representation to an output of the IDS.
We should point out that although R and C resemble encoding and decoding procedures, they are not exactly the strict encoding and decoding schemes.
In information theory, either encoding or decoding needs an encoding/decoding table containing all possible codewords for all possible source codes, so it can ensure a perfect encoding and decoding (without error or ambiguity).
In the case of intrusion detection, we cannot enumerate all the possible input data units (source codes) and feature representations (code words), nor can we afford to store such a huge encoding/decoding table.
As a result, both R and C algorithms can only work roughly correct, i.e., these algorithms may not guarantee errorless information transmission.
We can analyze and quantify the effectiveness of this information transmission using information-theoretic metrics.It is still a little hard to practically measure the effectiveness of the intrusion detection process based on the original model in Fig.2(a), because this model involves too many states in X o and Z o .
We can hardly enumerate all the states and practically measure the transition probabilities.
However, we notice that the purpose of an IDS is not to identify the original input data unit, but to identify the state of the data unit.
That is, we are interested in only limited states of the data, i.e., Σ.
We can group the input data to their states.
This greatly simplifies the original model and our practical analysis.
Similar idea can also be applied to the feature representation.
Thus, we will introduce two simplified models step by step in the next paragraphs.
First, we introduce a new random variable X to replace X o in our analysis.
X takes values in Σ, which represents the state of all possible input data unit to the IDS, with certain probabilities.
X is the state stream (X 1 , X 2 , ...) whereX i = Oracle IDS (D i ).
As the first step of our simplification, we ignore the intermediate feature representation process (that is, we ignore Z o , and only consider X, Y ).
We treat an IDS as a black box and thus, introduce our first simplified model, i.e., the abstract model in Fig.2(b), as firstly shown in [10].
In this abstract model, Σ = {N, A}.
We can denote transition probabilities between X and Y using false positive rate (F P , P (Y = A|X = N ), denoted as α) and false negative rate (F N ,P (Y = N |X = A), denoted as β).
Thus, we have an abstract model of intrusion detection with a very simple Markov transition matrix between X and Y .
The capability of an IDS to classify the input events correctly (i.e., faithfully reflect the "truth" about the input) can be measured using (normalized) mutual information, which captures the reduction of original uncertainty (intrusion or normal) given that the IDS alerts are known.
H(X) .
We can easily deriveC ID = I(X;Y ) H(X) = H(X)−H(X|Y ) H(X) = 1 − H(X|Y ) H(X) .
Since 0 ≤ H(X|Y ) ≤ H(X), we get 0 ≤ C ID ≤ 1.
Intuitively, C ID is interpreted as how much (normalized) ground truth information an IDS can identify.
For example, C ID = 0.8 means that the IDS identifies 0.8 bit of ground truth information assuming the original ground truth contains information 1.
It indicates how well an IDS can distinguish normal from anomaly and distinguish anomaly from normal.
In other words, it is an objective trade-off between F P and F N .
C ID has several nice properties [10]: (1) it naturally takes into account all the important aspects of detection capability (if we expand the equation of C ID ), i.e., false positive rate, false negative rate, positive predictive value (P P V , or Bayes detection rate [3]), negative predictive value (N P V ), and base rate (the probability of intrusion P (X = A), denoted as B); (2) it objectively provides an intrinsic measure of intrusion detection capability; (3) C ID yields a series of related information-theoretic metrics, which will be discussed soon.
This gives a fine-grained measure of the basic architecture and components of an IDS; (4) it is very sensitive to IDS operation parameters such as α, β, which can demonstrate the effect of the subtle changes of an IDS.
[10] has showed that C ID is more sensitive than some existing metrics (P P V, N P V ), however, comparison with the probability of error P e = P r(Y = X) (which is another metric to define how Y is different from X) is missing.
Now we demonstrate that C ID is also more sensitive to operation parameters than P e in reasonable situations in which the base rate is very low [3].
For Σ = {N, A}, we can derive P e = Bβ + (1 − B)α.
Similarly we can express the formula of C ID using B, α, β.
Since both P e and C ID have the same scale (value range [0,1]), it is fair to compare their sensitivities.
To compare the sensitivities of C ID and P e , we perform a differential analysis of B, α, β to study the effect of changing these parameters on P e and C ID .
For most IDSs and their operation environments, base rate and false positive rate are very low [3] so we can assume B 1 and α 1.
Fig.3 shows the partial differential analysis (in absolute value) on different metrics.
We only need to care about the absolute value of the derivatives.
A larger derivative value shows more sensitivity to changes.
For all the cases in Fig.3(a)(b)(c), a change in B, α or β results in very tiny change in P e .
Only when α > 0.1, the derivative of P e on α begins to be greater than that of C ID .
But for real world IDSs, it is very unlikely to have a false positive higher than 10%.
(For example, it is quite reasonable to have more than one million packets per day in an enterprise network.
If a packet level IDS has a false positive rate of 10%, this will generate more than 100,000 false positives per day!)
Clearly, from Fig.3 we can see that C ID is more sensitive to changes in B, α, β than P e (several orders of magnitude more sensitive).
In every situation C ID has the highest sensitivity compared to P e , except in (b) when α > 0.1 (which is unlikely, in practice every IDS should have a much smaller false positive rate than 10%).
For realistic situations, its derivative is always higher (several orders of magnitude) than P e .
In the previous abstract model, we F i , denoted as L(F i ) ∈ {N, U, A}.
State N means the feature representation vector is from and only from the data unit which is normal.
If the feature vector is from and only from data unit which is anomaly, then this is labeled as A.Those feature vectors that can be from both normal and anomalous data have the state U (means undistinguishable).
Formally,L(F i ) = N ⇔ ∀D j , R(D j ) = F i , Oracle IDS (D j ) = N L(Fi) = A ⇔ ∀Dj , R(Dj ) = Fi, Oracle IDS (Dj ) = A L(Fi) = U ⇔ ∃D1 = D2, R(D1) = Fi, R(D2) = Fi, Oracle IDS (D1) = N, Oracle IDS (D2) = AWe use a new random variable Z to replace Z o .
Z denotes the clustered feature representation state, and Z ∈ {N, U, A}.
Thus, we can slightly change the original transition model (Fig.2(a)) to a new one (Fig.2(c)).
In this clustered model, we can perform a fine-grained information-theoretic analysis on the intrusion detection procedure.
Instead of viewing the IDS as a black box in the abstract model ( Fig.2(b)) , we will analyze and reason about the basic architecture and components of an IDS.
Here we have three random variables X, Z, Y , which form a Markov chain in the order X → Z → Y .
In this detailed model, we first consider the transition from X to Z.
Here we can define a metric which measures the capability of feature representation.
The definition is also the normalized mutual information, similar to the definition of C ID .
Definition 2.
We define the feature representation capability C R as the normalized mutual information between X and Z, i.e.,C R = I(X;Z) H(X) .
Clearly C R is also a measure of the capability of R. Similar to C ID , 0 ≤ C R ≤ 1.
A larger C R means a better feature representation capability.
If C R = 1, then we say the IDS has an ideal feature representation capability.
Intuitively this is saying that there is no information loss during the first transition from X to Z.If there are some feature vectors with state U , it is hard to distinguish whether they are from normal or anomalous data only given the feature vectors (note that for C, the feature representation vector is the only input).
Intuitively, when transition from X to Z, we lose the "information".
Information-theoretically, we will have a smaller C R .
Ideally, if the feature set has a perfect feature representation capability, we will have no feature vector with state U , which also means P (Z = U |X = x) = 0 for ∀x ∈ Σ.
In this case, we get the identical distribution of X and Z, so we get C R = 1.
Then the model is much simplified as well as the abstract model in Fig.2(b).
Now let us consider the transition from Z to Y .
In order to measure how good C is, we expect that there will be less information loss after the classification algorithm.
= I(X, Y ; Z) − I(Y ; Z), we can also write L C as L C = I(X;Z|Y ) H(X) .
Since I(X; Z|Y ) = H(X|Y ) − H(X|Y, Z) ≤ H(X|Y ) ≤ H(X), we know that 0 ≤ L C ≤ 1.
We always expect that C does a good job so as to have less information loss.
Thus, a smaller L C means a better classification algorithm.
If L C = 0, then we say the IDS has an ideal classification algorithm C (so ideal classification information loss).
Now we have two new metrics C R and L C which can measure the feature representation capability and the classification information loss.
The following theorem provides a nice relationship between these two metrics and C ID .
Theorem 1.
The intrusion detection capability C ID is equal to the feature representation capability C R minus the classification information loss L C , i.e.,C ID = C R − L C .
Proof.
Since X, Z, Y form a Markov chain in the order X → Z → Y , the conditional distribution of Y depends only on Z and is conditionally independent on X.
We can get I(X; Y |Z) = 0 in this case (because X and Y are conditionally independent given Z).
Using the chain rule, we can expand mutual information in two different ways.
Applying the fact that I(X; Y |Z) = 0, we can getI(X; Y ) = I(X; Z) − I(X; Z|Y ).
Divided by H(X), we get C ID = C R − L C .
We already know C ID is the fraction of ground truth information identified by the IDS.
If we assume the original ground truth information is 1 (normalized), when R is applied, this information will be reduced to C R .
After C is performed, we will further lose L C amount of information due to the classification algorithm.
So finally we can get C ID amount of information.
If both C R and L C are ideal, then the IDS has an ideal intrusion detection capability (C ID = 1).
From this theorem, clearly we have C R ≥ L C because C ID ≥ 0.
Also we can obtain the following corollary easily.
This establishes an upper bound of C ID for an IDS.
For any given IDS, C ID can never exceed C R .
Once the feature set and R are given, the upper bound of C ID is also established no matter how good C is.
Now we can perform a fine-grained evaluation of IDSs using a set of informationtheoretic metrics, C R , L C , as well as C ID .
We can compare different IDSs, not only in terms of the overall performance, but also the performance of their specific components.The overall measure, C ID , is surely very useful.
We can fine-tune an IDS to some configuration that maximizes the C ID so that we have an optimal operation point.
Section 5 will show a concrete example to demonstrate the static and dynamic finetuning of an IDS based on C ID .
C R can help us evaluate whether the features in use have a good representation capability or not, independent of the classification algorithm.
An ideal feature set should have no information loss during the process, i.e., there should be no "undistinguishable" feature representation vector.
Once there exist some, they will definitely be classified to one output category although they are actually from two different input category (normal and anomaly).
Here we lose the information, and the lost information will never come back or be complemented by further process (classification algorithm).
If we do find some undistinguishable state (conflicts), we need to further reconsider/reselect the features (refine F).
For example, we can carefully add more features so that the existing undistinguishable state will become distinguishable.
(The original distinguishable states are still distinguishable).
Thus, we can improve C R and avoid information loss in the first process (X → Z).
Note that only simply adding more features does not guarantee increasing accuracy (decreasing L C ) in the testing data, which is known as "overfitting" problem in machine learning literature, because the change of the feature set may also affect the accuracy of the classification algorithm.
As a result, when adding more features, we increase the upper bound of C ID (i.e., C R ), but we still need to do some possible adjustment/change on classification algorithm to make sure that L C does not increase, so that we can improve the final C ID .
In most cases when we compare two different IDSs, they can have different feature sets and different classification algorithms.
With our framework, we can tell their finegrained performance difference.
For example, the reason why one IDS is less capable (lower C ID ) than another one can be mainly because its poor feature representation capability or classification information loss.
Knowing the exact reason will point out future improvement direction (bottleneck) of IDS design.
We have shown this using an example in experiment 4 of Section 5.
In practical evaluation, C ID and C R are easy to measure because the distribution, transition probabilities from X to Y in Fig.2(b) and the ones from X to Z in Fig.2(c) are easy to obtain in evaluation data.
We may not need to directly calculate L C , but simply apply Theorem 1 to compute L C = C R − C ID .
Feature F and algorithm R requirement: Feature selection is very important for any IDS.
C R is the first quantitative measure of its representation capability.
If features are not carefully selected, the information will be lost when R is applied.
Once C R becomes lower, C ID will also decrease no matter how good L C is.An IDS will not have a good representation capability if different types of data are represented in the same feature vector.
It will misclassify some events because in the first transition process (X → Z), these different type of events cannot be distinguished from each other in terms of the feature vector representation (e.g., for Snort, some normal packets may match the same rule of some attack; for PAYL, the frequency vector of byte sequence for some attacks may be within the range of normal profile).
A lower feature representation capability C R normally implies two possible reasons, either features are not well selected or R is not well designed.
So we are left with two possible ways to improve C R .
(1) Re-select the feature set or at least carefully add more features (this implies a better feature selection algorithm S).
For example, a context-aware Bro [27] is better than the one without considering context because it essentially adds new features (about the context).
(2) Well implemented data reduction and representation algorithm R will also improve C R than poorly designed R. For instance, in network intrusion detection, when using full assembling, protocol parsing R, an IDS may achieve better C R .
Traffic normalization [11] is another good example of a better R.Knowledge base K requirement: Since knowledge base K is used in the procedure of C, an accurate (complete and general) K is an important factor to improve the performance of classification algorithm C, so as to improve L C .
For a signature based IDS, it is important to make sure the signature set is accurate and covering as many as possible known attacks.
This directly affects the quality of K, and C. For anomaly detection, an exact profiling of large amount of normal data is the key to improve the quality of K and C.Realtime requirement on algorithms: The four algorithms in an IDS have different realtime requirement.
S, P are off-line algorithms, so there is fewer runtime speed requirement.
However, for algorithm R and C, they are mostly used online, so they should be efficiently implemented.Finally, we should note that most of the implications are not surprising facts.
They can be used as a sanity test for the correctness of any IDS model and theory.
Our IDS model and information-theoretic framework nicely confirm them.
Static situation: When evaluating IDSs, we should always have the data set with detailed ground truth knowledge.
Thus, from the evaluation data we can easily find out the base rate (fraction of intrusion) and measure all the transition probabilities (α, β, etc.) in Fig.2 (b) and (c).
Error bound and confidence: Machine learning researchers have given some bounds with certain confidence on the estimation of true error based on an observed error over a sample of data [21].
Given an observed sample error e s , with approximately N % (e.g. 99%) probability, the true error e t will lie in the interval e s ± z N e s (1−e s ) n , where n is the number of records in sample data, z N is a constant related to the confidence interval N % we want to reach.
For example, if we want approximately 99% confidence intervals then z N = 2.58.
Since the possible difference between testing data and real data is a general problem for every data-centric evaluation research, we are not trying to solve this problem in this paper.
In practice, we can assume the transition probabilities are relatively stable (such as α, β) with reasonable high confidence, if the testing data is a representative sample of the real situation.Base rate estimation: In the real world, the base rate may vary in different situations.
Here we give a heuristic approach to estimate the base rate.
Once we have the estimated FP (α), FN (β), we can approximately estimate the base rate in real traffic as follows.
All we need is an alert rate (r a ) of the IDS (the fraction of generated alerts over total data).
As we know this alert rate can be computed as r a = B(1−β)+(1−B)α.
So we can approximately estimated the base rate as B = ra−α 1−β−α , which provides us a good estimation of the real base rate.
It is easy to prove that this is an unbiased estimator for B.
In the next section, we will show how to use this estimation to dynamically fine-tune an IDS to keep it working on optimal operation points.Towards a robust consideration: Our framework can also be easily analyzed with a robust consideration.
For a robust evaluation with uncertain parameters in real world, we consider the real B, α, β can deviate from our estimation to some certain degree (a range).
Thus, we release the assumptions in all above sections.
Now instead of calculating C ID , C R , L C with a static setting of B, α, β, we use a range of these parameters (to tolerate largest possible estimation error bound), and among all possible results, we take the worst values (stands for the worst cases with all possible situation of B, α, β as we expect) as the final resulting metric.
By doing this, we are actually finding the best performing IDS against the worst situation (with the worst possible estimation error bound), instead of finding the best performing IDS on average (this is similar to the idea in [5]).
Thus, we can make sure that this final measure (say, C ID ) is robust in sense that it is the low bound in all cases of possible estimated range of parameters.
This will help if one is really concerned about the (large) estimation errors and uncertainties of the parameters in practice.
The IDS is guaranteed to be better than this robust metric given the largest possible estimation error bound.
In this section, we describe the experiments we conducted to show how our informationtheoretic framework is useful for fine-tuning an IDS in the real world, and we also show how a fine-grained measurement of IDSs is helpful for improving IDS design.
Fine-tuning an IDS is an important and non-trivial task, especially for anomaly-based IDSs.
We can use C ID as a yardstick to find an operation point yielding the best tradeoff between F P and F N (best is in terms of the intrinsic ability of the IDS to classify input data).
Specifically, we firstly change the threshold of the anomaly IDS so that we can achieve different F P and F N pairs, and create an ROC curve.
Then, we can calculate corresponding C ID for every point in the ROC.
We select the point with the highest C ID , and the threshold corresponding to this point provides the optimal threshold for use.To demonstrate this, we select an anomaly IDS, PAYL [31], as our example.
PAYL requires a threshold for determining whether the observed byte frequencies vary significantly from a trained model.
For example, a threshold of 256 allows each character in an observed payload to vary within one standard deviation of the model.
We collected a HTTP trace at a web server from our campus backbone network.
Since PAYL only handles the HTTP requests from client to the server, we filter out all the outgoing HTTP responses.
The trace data set only consists of incoming HTTP requests, approximately 7.5 million packets.
We also filtered the trace set through to remove known attacks, and equally split the trace into three sets: training set, testing set 1, and testing set 2.
We injected numerous HTTP attacks into the testing set, using tools such as Nikto [29].
In our first experiment, we train PAYL on the training set, and test it on testing set 1.
The purpose is to choose an optimal threshold as the static operation point for PAYL in our testing environment.
The base rate in testing set 1 is B = 0.00081699.
The result is shown in Fig.4(a).
We see that for the testing trace, as the threshold drops, C ID reaches a peak and then drops, while the ROC curve (shown in the top graph) continues to slowly increase.
The maximum point of C ID corresponds to < α = 0.0016053, 1 − β = 0.9824 >, and the corresponding threshold is 480.
This tells us that PAYL works optimal in sense of intrusion detection capability at this threshold in our testing data.
Without our information-theoretic guideline C ID , it is not clear how to choose an optimal operation point from the ROC curve.Experiment 1 finds optimal threshold in testing set 1.
If the base rate in testing set 1 is representative to the real situation, then it is perfect.
However, in real world situation, the base rate may vary from time to time.
If we fix the operation point at a certain threshold, then in other testing data, we may not always achieve optimal C ID when the base rate varies.
To address this problem, we introduce a new dynamic finetuning scheme that can be adaptive to any real situation.
In previous section, we have discussed an unbiased estimation of base rate, i.e., B = r a −α 1−β−α .
If we divide the time series into many intervals, at each interval n, we estimate B n , and then choose optimal operation point at this base rate to maximize C ID .
By dynamically fine-tuning the IDS, we ensure the IDS always operates on optimal points.
Thus, we get a self-adaptive, self-tuning version of the IDS, which is very useful in practice.We conduct a second experiment to investigate the effectiveness of dynamic finetuning.
In experiment 2, we use the same training set in experiment 1.
For the testing set, we inject different amount of attacks into testing set 2, and generate two new testing set 2A and 2B.
They contain the same normal data but different amount of attacks, so their base rates are different from testing set 1.
Specifically, B 2A = 0.00077256, which is only slightly different from testing set 1, B 2B = 0.00044488, which is almost half of that in testing set 1.
We modified PAYL to deploy a dynamic fine-tuning using C ID as the guideline.
And we denote this scheme as C ID Auto scheme.
We compare the results to the cases when we fix the threshold at some certain values from 416 to 544.
We also compare with the original automatic threshold adjusting scheme provided by PAYL (denoted as P aylAuto).
This scheme is to adjust the threshold in testing to control the alarm rate below certain value (0.001 in PAYL's setting).
Once the alarm rate is stable low for some time, then the threshold is fixed during the rest of the testing.The results of experiment 2 are shown in Fig.4(b).
C ID Auto outperforms all other schemes in all cases, i.e., it outputs the highest C ID in both two testing sets 2A and 2B.
Fixing threshold at 480 as in experiment 1 still achieves satisfied result but not the optimal one because the base rate varies.
Fixing threshold at 480 scheme gives the second highest C ID in test 2B, less than C ID Auto.
Fixing threshold at 448 achieves the second highest C ID in test 2A, still less than C ID Auto.
In both testing sets, the P aylAuto scheme runs with a final stable threshold at 376, and this scheme has the highest F P and lowest C ID (which is not good).
Our experiments clearly demonstrate the usefulness of C ID in dynamic fine-tuning of IDSs.
In this section, we will show how our framework can be used for a fine-grained evaluation of IDSs and how we can improve the design.
As a motivating example, we will use several machine learning based IDSs in this experiment, because they have a clear architecture and we can easily manipulate them as we want to change features or classification algorithms.
Thus, it is much easier for readers to understand the usefulness of our framework.In [17], Lee and Stolfo proposed three categories of features to be used by IDSs, i.e., 9 basic features of individual TCP connections, 13 content features within a connection suggested by domain knowledge, and 19 traffic features computed using a two-second time window.
Using these total 41 features, they processed data set from 1998 DARPA Intrusion Detection Evaluation program [18].
The processed data are available known as KDD cup 1999 data set [1].
Every connection record is labeled as N (normal) or A (anomalous).
The training set has 494,020 connection records.
The testing data set has 311,029 records, among which 250,436 are anomalous.
Obviously we can see that the distribution (of normal and anomalous data) in the testing data is not good because the base rate is so high (about 0.8) and obviously not a reasonable operation environment.
Only for the purpose of providing a reasonable base rate, we artificially duplicate the normal connections 4000 times, so that the base rate B = 250436/(4000 * 60593 + 250436) ≈ 0.001 is more reasonable.
Note that our duplicating normal data does not affect other parameters such as α, β.We have noticed the critique [20] on the DARPA data set and the limit of the KDD data set.
However, since our purpose is not to design a new IDS nor to conduct an official testing evaluation, we merely take them as a (public available) platform to demonstrate our framework.
In this sense, we think the data are still valid to achieve our goal.
We also plan to conduct more experiments using more real world IDSs on real world data to demonstrate our framework in the future.In experiment 3, we use all 41 features (denoted as feature set 1).
For the classification algorithm C, we choose three different machine learning algorithms, i.e., decision tree (specifically, we use C4.5 [21]), Naive Bayes classifier, and SVM (Support Vector Machine [30]).
All of them have been successfully applied to intrusion detection [2,13].
Since they are standard machine learning classification algorithms that are well documented in [21,30], we skip the details of these algorithms in this paper.
The result of experiment 3 is shown in Table 1.
Form Table 1, we can see that in the transition X → Z, these 41 feature set does not provide an ideal feature representation capability (i.e., C R = 0.9644 < 1).
Specifically, we measure the transition probabilities P (Z = U |X = N ) ≈ 0.12 and P (Z = U |X = A) ≈ 0.030319.
When further analyzing the state U in detail, we surprisingly find that all of the U states are caused by snmpgetattack (one kind of R2L attack), i.e., 7,593 (out of total 7,741) snmpgetattack connection records have the same feature representations with some normal data.
In other words, only from these feature representation, one cannot distinguish these snmpgetattack from normal traffic.
So we already have information loss at the data reduction and representation process.
For the classification process, SVM has the lowest classification information loss (L C = 0.4002, much lower than other two algorithms).
Thus, SVM finally outputs C ID = 0.5642, which means on average, SVM can achieve slightly more than half of the original ground truth 'information'.
The other two algorithms get less than half.
Experiment 3 clearly shows that the feature set in use still has room to improve because the feature representation capability is not ideal (a simple possible solution to improve C R is to add one more feature which can distinguish these snmpgetattack from normal traffic, e.g., SNMP protocol or not).
We are the first to provide a quantitative measure of such capability.
We also quantitatively compare the classification information loss of different machine learning algorithms such as SVM, decision tree and Naive Bayes.
The result shows SVM has the least classification information loss in the data set.In practice, more likely we will have IDSs with different feature sets and different classification algorithms.
In these cases, we can first compare them using the overall intrusion detection capability (C ID ).
Moreover, we can further (fine-grained) compare their feature representation capability and classification information loss.
It will help us understand why an IDS is better, i.e., mainly due to its C R or L C .
This not only helps us evaluate IDSs (especially when the IDSs have similar overall C ID ), but also indicates the direction for further improvement and tuning of IDS design.
To demonstrate this point, we conduct experiment 4.
We choose two different feature sets and two different classification algorithms to form two IDSs: one uses feature set 2 (including 9 basic features and 13 content features) and C4.5 classification algorithm, the other uses feature set 3 (including 9 basic features and 19 traffic features) and Naive Bayes classifier.
For feature set 2, we get the transition probabilities P (Z = U |X = N ) ≈ 0.2021, P (Z = U |X = A) ≈ 0.1892 in testing set, and C R = 0.8092.
For feature set 3, we get the transition probabilities P (Z = U |X = N ) ≈ 0.12, P (Z = U |X = A) ≈ 0.030319, and C R = 0.9644.
The experiment result is shown in Table 2.
We can see that the two IDSs have similar C ID (IDS1 is slightly better).
But by further exploring the components of these IDSs, we find IDS1 has a much worse C R but a better L C .
On the contrary, IDS2 has a better C R but the classification algorithm is very poor (causing larger classification information loss).
This fine-grained analysis indicates the bottleneck and further improvement direction for IDS2 is mainly on classification algorithm, while for IDS1 is primarily a better feature set (since its C R is too low compared to that of IDS2).
Following this direction, we do another experiment with indicated improvements.
When IDS1 improves its feature set (classification algorithm unchanged) by simply adding more traffic features to become feature set 1, we can get a better C ID = 0.4258, which is higher than the original 0.4002.
By improving IDS2's classification algorithm (use c4.5 to substitute Naive Bayes in our experiment, feature set unchanged), we can improve the C ID from 0.3875 to 0.4255.
The above example clearly demonstrates that fine-grained analysis can indicate our further (component) improvement direction for the design of IDSs.
Intrusion detection is a field of active research for more than two decades.
However, there is still little work on fundamental (theoretical) research, and there is still a huge gap between theory and practice.For theoretical studies of intrusion detection, in 1987, Denning [9] was the first to systematically introduce an intrusion detection model, and also proposed several statistical models to build normal profiles.
Later, Helman and Liepins [12] studied some statistical foundations of audit trail analysis for intrusion detection.
Axelsson [4] pointed out that results from detection and estimation theory may be used in the IDS research.
However, it is unclear how these similarities can benefit IDS evaluation and design.
Song et al. [28] used ACL2 theorem prover for the analysis of IDSs that employ declarative rules for attack recognition by proving the specifications satisfy the policy with various assumptions.
This approach is only useful for a certain type of IDSs, i.e., specification-based intrusion detection.
In contrast, our framework is general to all types of IDSs.
Recently, Di Crescenzo et al. [7] proposed a theory for IDSs based on both complexity-theoretic notions and well-known notions in cryptography (such as computational indistinguishability).
Cardenas et al. [5] proposed a framework for IDS evaluation (not analysis) by viewing it as a multi-criteria optimization problem and gave two approaches: expected cost, and a new trade-off curve (IDOC) considering both the detection rate and the Bayes detection rate.
Different from these existing work, our framework is an objective (without taking subject cost factors), natural and fine-grained approach with information-theoretic grounding.
Besides, we established a clear and detailed IDS model, and provided an entire framework to analyze components inside an IDS and improve the design of IDSs.Information-theoretic metrics have been widely applied in many fields.
For instance, in the machine learning area, there are well-known algorithms (such as C4.5 [21]) that use information gain as a criterion to select features.
[15] proposed to use entropy as a measure of distributions of packet features (IP addresses and ports) to identify and classify anomaly network traffic volumes.
Lee et al. [16] applied information theoretic measurement to describe the characteristics of audit data set, suggested the appropriate anomaly detection model, and explained the performance of the models.
This paper is a significant improvement and extension on our previous work [10], in which C ID was firstly proposed as a measure of the overall intrusion detection capability by viewing the whole IDS as a black box.
An overall measure of the IDS is useful, but it cannot measure the performance of each component of the IDS.
In this paper, we looked into the detailed processes within an IDS and performed a white box information-theoretic analysis on the components of the IDS.
Thus, we built a complete framework.
In addition, we demonstrated fine-tuning an IDS in both static and dynamic cases.
We also showed how to use our framework to evaluate IDSs in a fine-grained way and improve the design of IDSs with experiments.
In the paper, we established a formal framework for analyzing IDSs based on information theory.
As a practical theory, it is data trace driven and evaluation oriented.
Within our framework, the analysis of anomaly based and signature based IDSs can be unified.
In addition to providing a better understanding of IDSs grounded on information theory, the framework also facilitates a static/dynamic fine-tuning of an IDS to achieve optimal operation, a better or finer-grained means to evaluate IDS performance and improve IDS design.
Our framework provided intrusion detection research a solid mathematic basis and opened the door for the study of many open problems.This paper is only a preliminary start in the field.
There are many topics for possible future work.
One is to use more information theory, e.g., channel capacity models, to further study the effect of multiple processes/layers/sensors of the IDS architecture.
Thus, we can analyze and improve both internal and external designs of the IDSs by extending our current framework.
We will also further study robust ways of applying the framework.
Table 3 shows how PAYL and Snort 5 fit into our model.
F A character frequency vector < f re0, f re1, ..., f re255 >, here f rei is the frequency of char i in the payload of the packet.
K For each specific observed length i of each port j, M ij stores the mean and standard deviation of the frequency for each distinct byte.
S Manually examines exploits and finds out the importance of byte frequency.
R Scans each incoming payload of the packet, computes its byte value distribution.
P Runs R on large normal data set to generate normal profile (K) of the frequency.
C For each new payload distribution given by R, compares against model M ij .
If their Mahalanobis distance significantly deviates from the normal threshold, flags the packet as anomalous and generates an alert.
Snort model D Packet sequence (P 1 , P 2 , ...) Σ {N, A 1 , A 2 , ...}.
N is the normal state.
A i is the type of attack that can be detected by Snort, e.g., WEB-IIS .
asp HTTP header buffer overflow attempt.
Currently Snort can detect over three thousand attacks.
F Feature vector such as <srcIP, dstIP, dstPort, payload containing '|3A|' or not, payload containing '|00|' or not, ...>.
Many of the features are boolean values to indicate whether the payload contains some substring (part of the intrusion signatures) or not.
K The rule set of Snort.
S Manually examines exploits and finds out most common strings in intrusions.
R Grinders the packet, preprocesses (defragmentation, assembling, etc.).
If possible, uses string matching to explore feature space according to the rule set.
Due to the implementation of Snort, R does not need to represent the packet into the whole feature space.
Instead, R can stop when the packet is represented to some subset of features (matching a certain rule).
P Manually extracts signatures of intrusions and generates the rule set.
C Although C is not clearly separated from R in the implementation of Snort, we can consider it as a simple exact matching in knowledge base K (as 1-nearest neighbor searching in the rule set for the exact rule).
This work is supported in part by NSF grant CCR-0133629 and Army Research Office contract W911NF0510139.
The contents of this work are solely the responsibility of the authors and do not necessarily represent the official views of NSF and the U.S. Army.
