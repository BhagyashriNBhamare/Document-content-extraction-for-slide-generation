We offer a new metric for big data platforms, COST, or the Configuration that Outperforms a Single Thread.
The COST of a given platform for a given problem is the hardware configuration required before the platform out-performs a competent single-threaded implementation.
COST weighs a system's scalability against the overheads introduced by the system, and indicates the actual performance gains of the system, without rewarding systems that bring substantial but parallelizable overheads.
We survey measurements of data-parallel systems recently reported in SOSP and OSDI, and find that many systems have either a surprisingly large COST, often hundreds of cores, or simply underperform one thread for all of their reported configurations.
"You can have a second computer once you've shown you know how to use the first one."
The published work on big data systems has fetishized scalability as the most important feature of a distributed data processing platform.
While nearly all such publications detail their system's impressive scalability, few directly evaluate their absolute performance against reasonable benchmarks.
To what degree are these systems truly improving performance, as opposed to parallelizing overheads that they themselves introduce?Contrary to the common wisdom that effective scaling is evidence of solid systems building, any system can scale arbitrarily well with a sufficient lack of care in its implementation.
The two scaling curves in Figure 1 present the scaling of a Naiad computation before (system A) and after (system B) a performance optimization is applied.
The optimization, which removes parallelizable overheads, damages the apparent scalability despite resulting in improved performance in all configurations.
* Michael Isard was employed by Microsoft Research at the time of his involvement, but is now unaffiliated.
† Derek G. Murray was unaffiliated at the time of his involvement, but is now employed by Google Inc.
Figure 1: Scaling and performance measurements for a data-parallel algorithm, before (system A) and after (system B) a simple performance optimization.
The unoptimized implementation "scales" far better, despite (or rather, because of) its poor performance.While this may appear to be a contrived example, we will argue that many published big data systems more closely resemble system A than they resemble system B.
In this paper we take several recent graph processing papers from the systems literature and compare their reported performance against simple, single-threaded implementations on the same datasets using a high-end 2014 laptop.
Perhaps surprisingly, many published systems have unbounded COST-i.e., no configuration outperforms the best single-threaded implementation-for all of the problems to which they have been applied.
The comparisons are neither perfect nor always fair, but the conclusions are sufficiently dramatic that some concern must be raised.
In some cases the singlethreaded implementations are more than an order of magnitude faster than published results for systems using hundreds of cores.
We identify reasons for these gaps: some are intrinsic to the domain, some are entirely avoidable, and others are good subjects for further research.We stress that these problems lie not necessarily with the systems themselves, which may be improved with time, but rather with the measurements that the authors provide and the standard that reviewers and readers demand.
Our hope is to shed light on this issue so that future research is directed toward distributed systems whose scalability comes from advances in system design rather than poor baselines and low expectations.
name twitter rv [13] uk-2007-05 [5, 6] nodes 41,652,230 105,896,555 edges 1,468,365,182 3,738,733,648 size 5.76GB 14.72GB Table 1: The "twitter rv" and "uk-2007-05" graphs.
Graph computation has featured prominently in recent SOSP and OSDI conferences, and represents one of the simplest classes of data-parallel computation that is not trivially parallelized.
Conveniently, Gonzalez et al. [10] evaluated the latest versions of several graph-processing systems in 2014.
We implement each of their tasks using single-threaded C# code, and evaluate the implementations on the same datasets they use (see Table 1).
1 Our single-threaded implementations use a simple Boost-like graph traversal pattern.
A GraphIterator type accepts actions on edges, and maps the action across all graph edges.
The implementation uses unbuffered IO to read binary edge data from SSD and maintains pernode state in memory backed by large pages (2MB).
PageRank is an computation on directed graphs which iteratively updates a rank maintained for each vertex [19].
In each iteration a vertex's rank is uniformly divided among its outgoing neighbors, and then set to be the accumulation of scaled rank from incoming neighbors.
A dampening factor alpha is applied to the ranks, the lost rank distributed uniformly among all nodes.
Figure 2 presents code for twenty PageRank iterations.scalable system cores twitter uk-2007-05 GraphChi [12] 2 3160s 6972s Stratosphere [8] 16 2250s -X-Stream [21] 16 1488s -Spark [10] 128 857s 1759s Giraph [10] 128 596s 1235s GraphLab [10] 128 249s 833s GraphX [10] 128 419s 462s Single thread (SSD) 1 300s 651s Single thread (RAM) 1 275s - Table 2 compares the reported times from several systems against a single-threaded implementations of PageRank, reading the data either from SSD or from RAM.
Other than GraphChi and X-Stream, which reread edge data from disk, all systems partition the graph data among machines and load it in to memory.
Other than GraphLab and GraphX, systems partition edges by source vertex; GraphLab and GraphX use more sophisticated partitioning schemes to reduce communication.No scalable system in Table 2 consistently outperforms a single thread, even when the single thread repeatedly re-reads the data from external storage.
Only GraphLab and GraphX outperform any single-threaded executions, although we will see in Section 3.1 that the single-threaded implementation outperforms these systems once it re-orders edges in a manner akin to the partitioning schemes these systems use.
The connected components of an undirected graph are disjoint sets of vertices such that all vertices within a set scalable system cores twitter uk-2007-05 Stratosphere [8] 16 950s -X-Stream [21] 16 1159s -Spark [10] 128 1784s ≥ 8000s Giraph [10] 128 200s ≥ 8000s GraphLab [10] 128 242s 714s GraphX [10] 128 251s 800s Single thread (SSD) 1 153s 417s Table 3: Reported elapsed times for label propagation, compared with measured times for singlethreaded label propagation from SSD.are mutually reachable from each other.In the distributed setting, the most common algorithm for computing connectivity is label propagation [11] ( Figure 3).
In label propagation, each vertex maintains a label (initially its own ID), and iteratively updates its label to be the minimum of all its neighbors' labels and its current label.
The process propagates the smallest label in each component to all vertices in the component, and the iteration converges once this happens in every component.
The updates are commutative and associative, and consequently admit a scalable implementation [7].
Table 3 compares the reported running times of label propagation on several data-parallel systems with a single-threaded implementation reading from SSD.
Despite using orders of magnitude less hardware, singlethreaded label propagation is significantly faster than any system above.
The single-threaded implementations we have presented were chosen to be the simplest, most direct implementations we could think of.
There are several standard ways to improve them, yielding single-threaded implementations which strictly dominate the reported performance of the systems we have considered, in some cases by an additional order of magnitude.
Our single-threaded algorithms take as inputs edge iterators, and while they have no requirements on the order in which edges are presented, the order does affect performance.
Up to this point, our single-threaded implementations have enumerated edges in vertex order, whereby all edges for one vertex are presented before moving on to the next vertex.
Both GraphLab and GraphX instead partition the edges among workers, without requiring that all edges from a single vertex belong to the same worker, which enables those systems to exchange less data [9,10].
A single-threaded graph algorithm does not perform explicit communication, but edge ordering can have a pronounced effect on the cache behavior.
For example, the edge ordering described by a Hilbert curve [2], akin to ordering edges (a, b) by the interleaving of the bits of a and b, exhibits locality in both a and b rather than just a as in the vertex ordering.
Table 4 compares the running times of single-threaded PageRank with edges presented in Hilbert curve order against other implementations, where we see that it improves over all of them.Converting the graph data to a Hilbert curve order is an additional cost in pre-processing the graph.
The process amounts to transforming pairs of node identifiers (edges) into an integer of twice as many bits, sorting these values, and then transforming back to pairs of node identifiers.
Our implementation transforms the twitter rv graph in 179 seconds using one thread, which can be a performance win even if pre-processing is counted against the running time.
The problem of properly choosing a good algorithm lies at the heart of computer science.
The label propagation algorithm is used for graph connectivity not because it is a good algorithm, but because it fits within the "think like a vertex" computational model [15], whose implementations scale well.
Unfortunately, in this case (and many others) the appealing scaling properties are largely due to the algorithm's sub-optimality; label propagation simply does more work than better algorithms.Consider the algorithmic alternative of Union-Find with weighted union [3], a simple O(m log n) algorithm which scans the graph edges once and maintains two integers for each graph vertex, as presented in Figure 4.
mentations of label propagation, faster than the fastest of them (the single-threaded implementation) by over an order of magnitude.There are many other efficient algorithms for computing graph connectivity, several of which are parallelizable despite not fitting in the "think like a vertex" model.
While some of these algorithms may not be the best fit for a given distributed system, they are still legitimate alternatives that must be considered.
Having developed single-threaded implementations, we now have a basis for evaluating the COST of systems.
As an exercise, we retrospectively apply these baselines to the published numbers for existing scalable systems.
Figure 5 presents published scaling information from PowerGraph [9], GraphX [10], and Naiad [16], as well as two single-threaded measurements as horizontal lines.
The intersection with the upper line indicates the point at which the system out-performs a simple resourceconstrained implementation, and is a suitable baseline for systems with similar limitations (e.g., GraphChi and X-Stream).
The intersection with the lower line indicates the point at which the system out-performs a feature-rich implementation, including pre-processing and sufficient memory, and is a suitable baseline for systems with similar resources (e.g., GraphLab, Naiad, and GraphX).
From these curves we would say that Naiad has a COST of 16 cores for PageRanking the twitter rv graph.Although not presented as part of their scaling data, GraphLab reports a 3.6s measurement on 512 cores, and achieves a COST of 512 cores.
GraphX does not intersect the corresponding single-threaded measurement, and we would say it has unbounded COST.
The published works do not have scaling information for graph connectivity, but given the absolute performance of label propagation on the scalable systems relative to single-threaded union-find we are not optimistic that such scaling data would have lead to a bounded COST.Instead, Figure 6 presents the scaling of two Naiad implementations of parallel union-find [14], the same examples from Figure 1.
The two implementations differ in their storage of per-vertex state: the slower one uses hash tables where the faster one uses arrays.
The faster implementation has a COST of 10 cores, while the slower implementation has a COST of roughly 100 cores.The use of hash tables is the root cause of the factor of ten increase in COST, but it does provide some value: node identifiers need not lie in a compact set of integers.
This evaluation makes the trade-off clearer to both system implementors and potential users.
Several aspects of scalable systems design and implementation contribute to overheads and increased COST.
The computational model presented by the system restricts the programs one may express.
The target hardware may reflect different trade-offs, perhaps favoring capacity and throughput over high clock frequency.
Finally, the implementation of the system may add overheads a single thread doesn't require.
Understanding each of these overheads is an important part of assessing the capabilities and contributions of a scalable system.To achieve scalable parallelism, big data systems restrict programs to models in which the parallelism is evident.
These models may not align with the intent of the programmer, or the most efficient parallel implementations for the problem at hand.
Map-Reduce intentionally precludes memory-resident state in the interest of scalability, leading to substantial overhead for algorithms that would benefit from it.
Pregel's "think like a vertex" model requires a graph computation to be cast as an iterated local computation at each graph vertex as a function of the state of its neighbors, which captures only a limited subset of efficient graph algorithms.
Neither of these designs are the "wrong choice", but it is important to distinguish "scalability" from "efficient use of resources".
The cluster computing environment is different from the environment of a laptop.
The former often values high capacity and throughput over latency, with slower cores, storage, and memory.
The laptop now embodies the personal computer, with lower capacity but faster cores, storage, and memory.
While scalable systems are often a good match to cluster resources, it is important to consider alternative hardware for peak performance.Finally, the implementation of the system may introduce overheads that conceal the performance benefits of a scalable system.
High-level languages may facilitate development, but they can introduce performance issues (garbage collection, bounds checks, memory copies).
It is especially common in a research setting to evaluate a new idea with partial or primitive implementations of other parts of the system (serialization, memory management, networking), asserting that existing techniques will improve the performance.
While many of these issues might be improved with engineering effort that does not otherwise advance research, nonetheless it can be very difficult to assess whether the benefits the system claims will still manifest once the fat is removed.There are many good reasons why a system might have a high COST when compared with the fastest purpose-built single-threaded implementation.
The system may target a different set of problems, be suited for a different deployment, or be a prototype designed to assess components of a full system.
The system may also provide other qualitative advantages, including integration with an existing ecosystem, high availability, or security, that a simpler solution cannot provide.
As Section 4 demonstrates, it is nonetheless important to evaluate the COST, both to explain whether a high COST is intrinsic to the proposed system, and because it can highlight avoidable inefficiencies and thereby lead to performance improvements for the system.
While this note may appear critical of research in distributed systems, we believe there is still good work to do, and our goal is to provide a framework for measuring and making the best forward progress.There are numerous examples of scalable algorithms and computational models; one only needs to look back to the parallel computing research of decades past.
Borůvka's algorithm [1] is nearly ninety years old, parallelizes cleanly, and solves a more general problem than label propagation.
The Bulk Synchronous Parallel model [24] is surprisingly more general than most related work sections would have you believe.
These algorithms and models are richly detailed, analyzed, and in many cases already implemented.Many examples of performant scalable systems exist.
Both Galois [17] and Ligra [23] are shared-memory systems that significantly out-perform their distributed peers when run on single machines.
Naiad [16] introduces a new general purpose dataflow model, and out-performs even specialized systems.
Understanding what these systems did right and how to improve them is more important than re-hashing existing ideas in new domains compared against only the poorest of prior work.We are now starting to see performance studies of the current crop of scalable systems [18], challenging some conventional wisdom underlying their design principles.
Similar such studies have come from previous generations of systems [22], including work explicitly critical of the absolute performance of scalable systems as compared with simpler solutions [20,4,25].
While it is surely valuable to understand and learn from the performance of popular scalable systems, we might also learn that we keep making, and publishing, the same mistakes.Fundamentally, a part of good research is making sure we are asking the right questions.
"Can systems be made to scale well?"
is trivially answered (in the introduction) and is not itself the right question.
There is a substantial amount of good research to do, but identifying progress requires being more upfront about existing alternatives.
The COST of a scalable system uses the simplest of alternatives, but is an important part of understanding and articulating progress made by research on these systems.
