While there has been a tremendous interest in processing data that has an underlying graph structure, existing distributed graph processing systems take several minutes or even hours to mine simple patterns on graphs.
This paper presents ASAP, a fast, approximate computation engine for graph pattern mining.
ASAP leverages state-of-the-art results in graph approximation theory, and extends it to general graph patterns in distributed settings.
To enable the users to navigate the tradeoff between the result accuracy and latency, we propose a novel approach to build the Error-Latency Profile (ELP) for a given computation.
We have implemented ASAP on a general-purpose distributed dataflow platform and evaluated it extensively on several graph patterns.
Our experimental results show that ASAP outperforms existing exact pattern mining solutions by up to 77×.
Further, ASAP can scale to graphs with billions of edges without the need for large clusters.
The recent past has seen a resurgence in storing and processing massive amounts of graph-structured data [1,3].
Algorithms for graph processing can broadly be classified into two categories.
The first, graph analysis algorithms, compute properties of a graph typically using neighborhood information.
Examples of such algorithms include PageRank [46], community detection [31] and label propagation [65].
The second, graph pattern mining algorithms, discover structural patterns in a graph.
Examples of graph pattern mining algorithms include motif finding [44], frequent sub-graph mining (FSM) [60] and clique mining [19].
Graph mining algorithms are used in applications like detecting similarity between graphlets [49] in social networking and for counting pattern frequencies to do credit card fraud detection.Today, a deluge of graph processing frameworks exist, both in academia and open-source [20, 24, 25, 34- 36, 40, 42, 43, 45, 50, 53, 54, 58, 64].
These frameworks typically provide high-level abstractions that make it easy for developers to implement many graph algorithms.
A vast majority of the existing graph processing * Equal contribution.
frameworks however have focused on graph analysis algorithms.
These frameworks are fast and can scale out to handle very large graph analysis settings: for instance, GraM [59] can run one iteration of page rank on a trillionedge graph in 140 seconds in a cluster.
In contrast, systems that support graph pattern mining fail to scale to even moderately sized graphs, and are slow, taking several hours to mine simple patterns [29,55].
The main reason for the lack of the scalability in pattern mining is the underlying complexity of these algorithmsmining patterns requires complex computations and storing exponentially large intermediate candidate sets.
For example, a graph with a million vertices may possibly contain 10 17 triangles.
While distributed graph-processing solutions are good candidates for processing such massive intermediate data, the need to do expensive joins to create candidates severely degrades performance.
To overcome this, Arabesque [55] proposes new abstractions for graph mining in distributed settings that can significantly optimize how intermediate candidates are stored.
However, even with these methods, Arabesque takes over 10 hours to count motifs in a graph with less than 1 billion edges.In this paper, we present ASAP1, a system that enables both fast and scalable pattern mining.
ASAP is motivated by one key observation: in many pattern mining tasks, it is often not necessary to output the exact answer.
For instance, in FSM the task is to find the frequency of subgraphs with an end-goal of ordering them by occurrences.
Similarly, motif counting determines the number of occurrences of a given motif.
In these scenarios, it is sufficient to provide an almost correct answer.
Indeed, our conversations with a social network firm [4] revealed that their application for social graph similarity uses a count of similar graphlets [49].
Another company's [4] fraud detection system similarly counts the frequency of pattern occurrences.
In both cases, an approximate count is good enough.
Furthermore, it is not necessary to materialize all occurrences of a pattern2.
Based on these use cases, we build a system for approximate graph pattern mining.Approximate analytics is an area that has gathered attention in big data analytics [6,13,32], where the goal is to let the user trade-off accuracy for much faster results.
The basic idea in approximation systems is to execute the exact algorithm on a small portion of the data, referred to as samples, and then rely on the statistical properties of these samples to compose partial results and/or error characteristics.
The fundamental assumption underlying these systems is that there exists a relationship between the input size and the accuracy of the results which can be inferred.
However, this assumption falls apart when applied to graph pattern mining.
In particular, running the exact algorithm on a sampled graph may not result in a reduction of runtime or good estimation of error ( § 2.2).
Instead, in ASAP, we leverage graph approximation theory, which has a rich history of proposing approximation algorithms for mining specific patterns such as triangles.
ASAP exploits a key idea that approximate pattern mining can be viewed as equivalent to probabilistically sampling random instances of the pattern.
Using this as a foundation, ASAP extends the state-of-the-art probabilistic approximation techniques to general patterns in a distributed setting.
This lets ASAP massively parallelize instance sampling and provide a drastic reduction in runtimes while sacrificing a small amount of accuracy.
ASAP captures this technique in a simple API that allows users to plugin code to detect a single instance of the pattern and then automatically orchestrates computation while adjusting the error bounds based on the parallelism.Further, ASAP makes pattern mining practical by supporting predicate matching and introducing caching techniques.
In particular, ASAP allows mining for patterns where edges in the pattern satisfy a user-specified property.
To further reduce the computation time, ASAP leverages the fact that in several mining tasks, such as motif finding, it is possible to cache partial patterns that are building blocks for many other patterns.
Finally, an important problem in any approximation system is in allowing users to navigate the tradeoff between the result accuracy and latency.
For this, ASAP presents a novel approach to build the Error-Latency Profile (ELP) for graph mining: it uses a small sample of the graph to obtain necessary information and applies Chernoff bound analysis to estimate the worst-case error profile for the original graph.The combination of these techniques allows ASAP to outperform Arabesque [55], a state-of-the-art exact pattern mining solution by up to 77× on the LiveJournal graph while incurring less than 5% error.
In addition, ASAP can scale to graphs with billions of edges-for instance, ASAP can count all the 6 patterns in 4-motifs on the Twitter (1.5B edges) and UK graph (3.7B edges) in 22 and 47 minutes, respectively, in a 16 machine cluster.We make the following contributions in this paper:• We present ASAP, the first system to our knowledge, that does fast, scalable approximate graph pattern mining on large graphs.
( §3) • We develop a general API that allows users to mine any graph pattern and present techniques to automatically distribute executions on a cluster.
( §4) • We propose techniques that quickly infer the relationship between approximation error and latency, and show that it is accurate across many real-world graphs.
( §5) • We show that ASAP handles graphs with billions of edges, a scale that existing systems failed to reach.
( §6) We begin by discussing graph pattern mining algorithms and then motivate the need for a new approach to approximate pattern mining.
We then describe recent advancements in graph pattern mining theory that we leverage.
Mining patterns in a graph represent an important class of graph processing problems.
Here, the objective is to find instances of a given pattern in a graph or graphs.
The common way of representing graph data is in the form of a property graph [52], where user-defined properties are attached to the vertices and edges of the graph.
A pattern is an arbitrary subgraph, and pattern mining algorithms aim to output all subgraphs, commonly referred to as embeddings, that match the input pattern.
Matching is done via sub-graph isomorphism, which is known to be NP-complete.
Several varieties of graph pattern mining problems exist, ranging from finding cliques to mining frequent subgraphs.
We refer the reader to [7,55] for an excellent, in-depth overview of graph mining algorithms.
A common approach to implement pattern mining algorithms is to iterate over all possible embeddings in the graph starting with the simplest pattern (e.g., a vertex or an edge).
We can then check all candidate embeddings, and prune those that cannot be a part of the final answer.
The resulting candidates are then expanded by adding one more vertex/edge, and the process is repeated until it is not possible to explore further.
The obvious challenge in graph pattern mining, as opposed to graph analysis, is the exponentially large candidate set that needs to be checked.Distributed graph processing frameworks are built to process large graphs, and thus seem like an ideal candidate for this problem.
Unfortunately when applied to graph mining problems, they face several challenges in managing the candidate set.
Arabesque [55], a recently proposed distributed graph mining system, discusses these challenges in detail, and proposes solutions to tackle several of them.
However, even Arabesque is unable to scale to large graphs due to the need to materialize candidates and exchange them between machines.
As an example, Arabesque takes over 10 hours to count motifs of size 3 in a graph with less than a billion edges on a cluster of 20 machines, each having 256GB of memory.
Approximate processing is an approach that has been used with tremendous success in solving similar problems in both the big data analytics [6,32] and databases [22,26,27], and thus it is natural to explore similar techniques for graph pattern mining.
However, simply extending existing approaches to graphs is insufficient.The common underlying idea in approximate processing systems is to sample the input that a query or an algorithm works on.
Several techniques for sampling the input exists, for instance, BlinkDB [6] leverages stratified sampling.
To estimate the error, approximation systems rely on the assumption that the sample size relates to the error in the output (e.g., if we sample K items from the original input, then the error in aggregate queries, such as SUM, is inversely proportional to √ K).
It is straightforward to envision extending this approach to graph pattern mining-given a graph and a pattern to mine in the graph, we first sample the graph, and run the pattern mining algorithm on the sampled graph.
Figure 1a depicts the idea as applied to triangle counting.
In this example, the input graph consists of 10 triangles.
Using uniform sampling on the edges we obtain a graph with 50% of the edges.
We can then apply triangle counting on this sample to get an answer 1.
To scale this number to the actual graph, we can use several ways.
One naive way is to double it, since we reduced the input by half.
To verify the validity of the approach, we evaluated it on the Twitter graph [39] for finding 3-chains and the UK webgraph [17] graph for triangle counting.
The relation between the sample size, error and the speedup compared to running on the original graph ( These results show the fundamental limitations of the approach.
We see that there is no relation between the size of the graph (sample) and the error or the speedup.
Even very small samples do not provide noticeable speedups, and conversely, even very large samples end up with significant errors.
We conclude that the existing approximation approach of running the exact algorithm on one or more samples of the input is incompatible with graph pattern mining.
Thus, in this paper, we propose a new approach.
Graph theory community has spent significant efforts in studying various approximation techniques for specific patterns.
The key idea in these approaches is to model the edges in the graph as a stream and sample instances of a pattern from the edge stream.
Then the probability of sampling is used to bound the number of occurrences of the pattern.
There has been a large body of theoretical work on various algorithms to sample specific patterns and analysis to prove their bounds [8,11,21,38,47,48,56].
While the intuition of using such sampling to approximate pattern counts is straightforward, the technical details and the analysis are quite subtle.
Since sampling once results in a large variance in the estimate, multiple rounds are required to bound the variance.
Consider triangle counting as an example.
Naively, one would design an technique that uniformly samples three edges from the graph without replacement.
Since the probability of sampling one edge is 1/m in a graph of m edges, the probability of sampling three edges is 1/m 3 .
If the sampled three edges form a triangle, we estimate the number of triangles to be m 3 (the expectation); otherwise, the estimation is 0.
While such a sampling technique is unbiased, since m is large in practice, the probability that the sampling would find a triangle is very low and the variance of the result is very large.
Obtaining an approximated count with high accuracy, would require a large number of trials, which not only consumes time but also memory.Neighborhood sampling [48] is a recently proposed approach that provides a solution to this problem in the context of a specific graph pattern, triangle counting.
The basic idea is to sample one edge and then gradually add more edges until the edges form a triangle or it becomes impossible to form the pattern.
This can be analyzed by Bayesian probability [48].
Let's denote E as the event that a pattern is formed, E 1 , E 2 , . . ., E k are the events that edges e 1 , e 2 , . . ., e k are sampled and stored.
Thus the probability of a pattern is actually sampled can be calculated asPr(E) = Pr(E 1 ∩ E 2 · · · ∩ E k ) = Pr(E 1 ) × Pr(E 2 |E 1 ) · · · × Pr(E k |E 1 , . . ., E k−1 ).
Intuitively, compared to the naive sampling, neighborhood sampling increases the probability that each trial would find an instance of the given pattern, and thus requires fewer estimations to achieve the same accuracy.
To illustrate neighborhood sampling, we will revisit the triangle counting example discussed earlier.
To sample a triangle from a graph with m edges, we need three edges:• First edge l 0 .
Uniformly sample one edge from the graph as l 0 .
The sampling probability Pr(l 0 ) = 1/m.
• Second edge l 1 .
Given that l 0 is already sampled, we uniformly sample one of l 0 's adjacent edges (neighbors) from the graph, which we call l 1 .
Note that neighborhood sampling depends on the ordering of edges in the stream and l 1 appears after l 0 here.
The sampling probability Pr(l 1 |l 0 ) = 1/c, where c is the number l 0 's neighbors appearing after l 0 .
• Third edge l 2 .
Find l 2 to finish if edges l 2 , l 1 , l 0 form a triangle and l 2 appears after l 1 in the stream.
If such a triangle is sampled, the sampling probability isPr(l 0 ∩ l 1 ∩ l 2 ) = Pr(l 0 ) × Pr(l 1 |l 0 ) × Pr(l 2 |l 0 , l 1 ) = 1/mc.
The above technique describes the behaviors of one sampling trial.
For each trial, if it successfully samples a triangle, converting probabilities to expectation, e i = mc will be the estimate of the triangles in the graph.
For a total of r trials, 1 r r e i is output as the approximate result.
Figure 2 presents an example of a graph with five nodes.
While the neighborhood sampling algorithm described above has good theoretical properties, there are a number of challenges in building a general system for large-scale approximate graph mining.
First, neighborhood sampling was proposed in the context of a specific graph pattern (triangle counting).
Therefore, to be of practical use, ASAP needs to generalize neighborhood sampling to other patterns.
Second, neighborhood sampling and its analysis assume that the graph is stored in a single machine.
ASAP focuses on large-scale, distributed graph processing, and for this it needs to extend neighborhood sampling to computer clusters.
Third, neighborhood sampling assumes homogeneous vertices and edges.
Real-world graphs are property graphs, and in practice pattern mining queries require predicate matching which needs the technique to Pattern Mining graphA.patterns("a->b->c", "100s") graphB.fourClique("5.0%","95.0%") Embeddings ( be aware of vertex and edge types and properties.
Finally, as in any approximate processing system, ASAP needs to allow the end user to trade-off accuracy for latency and hence needs to understand the relation between run-time and error in a distributed setting.
In this work, we design ASAP, a system that facilitates fast and scalable approximate pattern mining.
Figure 3 shows the overall architecture of ASAP.
We provide a brief overview of the different components, and how users leverage ASAP to do approximate pattern mining in this section to aid the reader in following the rest of this paper.User interface.
ASAP allows the users to tradeoff accuracy for result latency.
Specifically, a user can perform pattern mining tasks using the following two modes 1 :• Time budget T.
The user specifies a time budget T, and ASAP returns the most accurate answer within T with a error rate guarantee e and a configurable confidence level (default of 95%).
• Error budget .
The user gives an error budget and confidence level, and ASAP returns an answer within in the shortest time possible.
Before running the algorithm, ASAP first returns to the user its estimates on the time or error bounds it can achieve 6 .
After user approves the estimates, the algorithm is run and the result presented to the user consists of the count, confidence level and the actual run time 7 .
Users can also optionally ask to output actual (potentially large number of) embeddings of the pattern found.
Development framework.
All pattern mining programs in ASAP are versions of generalized approximate pattern mining 2 we describe in detail in §4.
ASAP provides a standard library of implementations for several common patterns such as triangles, cliques and chains.
To allow developers to write program to mine any pattern, ASAP further provides a simple API that lets them utilize our approximate mining technique ( § 4.1.2).
Using the API, developers simply need to write a program that finds a single instance of the pattern they are interested in, which we refer to as estimator in the rest of this paper.
In a nutshell, our approximate mining approach depends on running multiple such estimators in parallel.Error-Latency Profile (ELP).
In order to run a user program, ASAP first must find out how many estimators it needs to run for the given bounds 3 .
To do this, ASAP builds an ELP.
If the ELP is available for a graph, it simply queries the ELP to find the number of estimators 4 .
Otherwise, the system builds a new ELP 5 using a novel technique that is extremely fast and can be done online.
We detail our ELP building technique in §5.
Since this phase is fast, ASAP can also accommodate graph updates; on large changes, we simply rebuild the ELP.System runtime.
Once ASAP determines the number of estimators necessary to achieve the required error or time bounds, it executes the approximate mining program using a distributed runtime built on Apache Spark [62,63].
We now present how ASAP enables large-scale graph pattern mining using neighborhood sampling as a foundation.
We first describe our programming abstraction( § 4.1) that generalizes neighborhood sampling.
Then, we describe how ASAP handles errors that arise in distributed processing( § 4.2).
Finally, we show how ASAP can handle queries with predicates on edges or vertices( § 4.3).
To extend the neighborhood sampling technique to general patterns, we leverage one simple observation: at a high level, neighborhood sampling can be viewed as consisting of two phases, sampling phase and closing phase.
In the sampling phase, we select an edge in one of two ways by treating the graph as an ordered stream of edges: (a) sample an edge randomly; (b) sample an edge that is adjacent to any previously sampled edges, from the remainder of the stream.
In the closing phase, we wait for one or more specific edges to complete the pattern.The probability of sampling a pattern can be computed from these two phases.
The closing phase always has a probability of 1 or 0, depending on whether it finds the edges it is waiting for.
The probability of the sampling phase depends on how the initial pattern is formed and is a choice made by the developer.
For a general graph pattern with multiple nodes, there can be multiple ways to form the pattern.
For example, there are two ways to sample a four-clique with different probabilities, as shown in Figure 4.
(i) In the first case, the sampling phase finds three adjacent edges, and the closing phase waits for rest three edges to come, in order to form the pattern.
The sampling probability is 1mc 1 c 2, where c 1 is the number of the first edge's neighbors and c 2 represents the neighbor count of the first and the second edges.
(ii) In the second case, the sampling phase finds two disjoint edges, and the closing phase waits for other four edges to form the pattern.
The sampling probability in this case is 1 m 2 .
We now show how neighborhood sampling, when captured using the two phases, can extend to general patterns.
Definition 4.1 (General Pattern).
We define a "general pattern" as a set of k connected vertices that form a subgraph in a given graph.First, let's consider how an estimator can (possibly) find any general patterns.
We show how to sample one general pattern from the graph uniformly with a certain success probability, taking 2 to 5-node patterns as examples.
Then, we turn to the problem of maintaining r ≤ 1 pattern(s) sampled with replacement from the graph.
We sample r patterns and a reasonably large r will yield a count estimate with good accuracy.
For the convenience of the analysis, we define the following notations: input graph G = (V, E) has m edges and n vertices, and we denote the occurrence of a given pattern in G as f (G).
A pattern p = {e i , e j , . . . } contains a set of ordered edges, i.e., e i arrives before e j when i < j.
When describing the operation of an estimator, c(e) denotes the number of edges adjacent to e and appearing after e, and c i is c(e 1 , . . ., e i ) for any i ≥ 1.
For a given a pattern p * with k * vertices, the technique of neighborhood sampling produces p * with probabilityPr[p = p * , k = k * ].
The goal of one estimator is to fix all the vertices that form the pattern, and complete the pattern if possible.Lemma 4.2.
Let p * be a k-node pattern in the graph.
The probability of detecting the pattern p = p * depends on k and the different ways to sample using neighborhood sampling technique.
(1) When k = 2, the probability that p = p * after processing all edges in the graph by all possible neighborhood sampling ways isPr[p = p * , k = 2] = 1 m (2) When k = 3, the probability that p = p * is Pr[p = p * , k = 3] = 1 m · c 1 13th USENIX Symposium on Operating Systems Design and Implementation 749(3) When k = 4, the probability that p = p * is Pr[p = p * , k = 4] = 1 m 2 (Type-I) or 1 m · c 1 · c 2 (Type-II) (4) When k = 5, the probability that p = p * is Pr[p = p * , k = 5] = 1 m 2 · c 1 (Type-I) or = 1 m 2 · c 2 (Type-II.
a) or = 1 m · c 1 · c 2 · c 3 (Type-II.
b)Proof.
Since a pattern is connected, the operations in the sampling phase are able to reach all nodes in a sampled pattern.
To fix such a pattern, the neighborhood sampling needs to confirm all the vertices that form the pattern.Once the vertices are found, the probability of completing such a pattern is fixed.
When k = 2, let p * = {e 1 } be an edge in the graph.
Let E 1 be the event that e 1 is found by neighborhood sampling.
There is only one way to fix two vertices of the pattern-uniformly sampling an edge from the graph.
By reservoir sampling, we claim thatPr[p = p * , k = 2] = Pr[E 1 ] = 1 m When k = 3, we need to fix one more vertex beyond the case of k = 2.
As shown in [48], we need to sample an edge e 2 from e 1 's neighbors that occur in the stream after e 1 .
Let E 2 be the event that e 2 is found.
SincePr[E 2 |E 1 ] = 1 c(e 1 ) , Pr[p = p * , k = 3] = Pr[E 1 ] · Pr[E 2 |E 1 ] = 1 m · c(e 1 )When k = 4, we require one more step from the case of k = 2 or the case of k = 3, from extending neighborhood sampling.
By extending from the case of k = 2 (denoted as Type-I), two more vertices are needed to fix a 4-node pattern.
In Type-I, we independently find another edge e * 2 that is not adjacent to the sampled edge e 1 .
Let E * 2 be the event that e * 2 is found.
SincePr[E * 2 |E 1 ] = 1 m , Pr[p = p * , k = 4] = Pr[p = p * , k = 2] * Pr[E * 2 |E 1 ] =1 m 2 (Type-I) When extending from the case k = 2 (denoted as Type-II), one more vertex is needed to fix a 4-node pattern.
In Type-II, we sample a "neighbor" e 3 that comes after e 1 ande 2 .
Let E 3 be the event that e 3 is found.
Since e 3 is sampled uniformly from the neighbors of e 1 and e 2 and is appearing after e 1 , e 2 ,Pr[E 3 |E 1 , E 2 ] = 1 c(e 1 ,e 2 ) .
Thus, Pr[p = p * , k = 4] = Pr[p = p * , k = 3] · Pr[E 3 |E 1 , E 2 ] = 1 m · c(e 1 ) · c(e 1 , e 2 ) (Type-II)When k = 5, we again need one more step from the case k = 3 or the case k = 4.
By extending from k = 3 (denoted as Type-I), we require two separate vertices to fix a 5-node pattern.
In Type-I, we independently sample another edge e * 3 that is not adjacent to e 1 , e 2 .
Let E * 3 be the event that e * 3 is found.Pr[E * 3 |E 1 , E 2 ] = 1 m .
Therefore, Pr[p = p * , k = 5] = Pr[p = p * , k = 3] * Pr[E * 3 |E 1 , E 2 ] = 1 m 2 · c(e 1 ) (Type-I)When extending from the case k = 4, we need to consider the two types separately.
By extending Type-I of case k = 4 (denoted as Type-II.
a), we need one more vertex to construct a 5-node pattern and thus we sample a neighboring edge e 4 .
Let E 4 be the event that e 4 is found.
Since e 4 is sampled from the neighbors of e 1 , e 2 ,Pr[p = p * , k = 5] = Pr[p = p * , k = 4] * Pr[E 4 |E 1 , E * 2 ] = 1 m 2 · c(e 1 , e 2 ) (Type-II.
a)Similarly, by extending Type-II of case k = 4 (denoted as Type-II.
b),Pr[p = p * , k = 5] = 1 m · c(e 1 ) · c(e 1 , e 2 ) · c(e 1 , e 2 , e 3 ) Lemma 4.3.
For pattern p * with k * nodes, let's define˜t define˜ define˜t = 1 Pr[p=p * ,k=k * ] if p ∅ 0 if p = ∅ Thus, E[ ˜ t] = f (G).
Proof.
By Lemma 4.2, we know that one estimator samples a particular pattern p * with probabilityPr[p = p * , k = k * ].
Let p(G) be the set of a given pattern in the graph,E[ ˜ t] = p * ∈p(G) ˜ t(p ∅)· Pr[p = p * , k = k * ] = |p(G)| = f (G)The estimated count is the average of the input of all estimators.
Now, we consider how many estimators are needed to maintain an error guarantee.Theorem 4.4.
Let r ≥ 1, 0 < ≤ 1, and 0 < δ ≤ 1.
There is an O(r)-space bounded algorithm that return anapproximation to the count of a k-node pattern, with probability at least 1 − δ.
For a certain , when k = 4, we need r ≥ C 1 m 2 f (G) Type-I estimators, or r ≥ C 2 m∆ 2 f (G) Type-II estimators for some constants C 1 and C 2 , to achieve -approximation in the worst case; When k = 5, we needr ≥ C 3 m 2 ∆ f (G) Type-I estimators, or r ≥ C 4 m 2 ∆ f (G) Type-II.
a es- timators, or r ≥ C 5 m∆ 3f (G) Type-II.
b estimators, for some constants C 3 , C 4 , C 5 in the worst case.
Uniformly sample one vertex from the graph.
SampleEdge: ()→(e, p)Uniformly sample one edge from the graph.ConditionalSampleVertex: (subgraph)→(v, p)Uniformly sample a vertex that appears after a sampled subgraph.
ConditionalSampleEdge: (subgraph)→(e, p)Uniformly sample an edge that is adjacent to the given subgraph and comes after the subgraph in the order.
ConditionalClose: (subgraph, subgraph)→bool ean Given a sampled subgraph, check if another subgraph that appears later in the order can be formed.
Proof.
Let's first consider the case k = 4.
Let X i for i = 1, . . ., r be the output value of i-th estimator.
Let ¯ X =1 r r i=1 X i be the average of r estimators.
By Lemma 4.3, we know that E[X i ] = f (G) and E[ ¯ X] = f (G).
From the properties of graph G, we have c(e) ≤ ∆ for ∀e ∈ E, where ∆ is the maximum degree (note that in practice ∆ isn't a tight bound for the edge neighbor information).
In Type-I, X i ≤ m 2 and we construct random variables Yi = X i m 2 such that Y i = [0, 1].
Let Y = r i=1 Y i and E[Y ] = f (G)r m 2 .
Thus the probability that the estimated number of patterns has a more than relative error off its expectation · ln 2 δ .
Similarly, this lower bound of r holds for Pr[ · ln( 2 δ ) Type-II.
b estimators.
Since each estimator stores O(1) edges, the total memory is O(r).
f (G) is Pr[ ¯ X > (1 + ) f (G)] ≤ δ 2 , which is at most Pr[ r i=1 Y i > (1 + )E[Y ]] ≤ e − 2 2+ E[Y] ≤ e − 2 3 E[Y] ≤ δ¯ X < (1 − ) f (G)].
In Type-II, X i ≤ 6m∆ 2 .
Let Y i = X i 6m∆ 2 such that Y i = [0, 1].
Let Y = r i=1 Y i and E[Y ] = f (G)r6m∆ ASAP automates the process of computing the probability of finding a pattern, and derives an expectation from it by providing a simple API that captures two phases.
The API, shown in Table 1, consists of the following five functions:• SampleVertex uniformly samples one vertex from the graph.
It takes no input, and outputs v and p, where v is the sampled vertex, and p is the probability that sampled v, which is the inverse of the number of vertices.
• SampleEdge uniformly samples one edge from the graph.It also takes no input, and outputs e and p, where e is the sampled edge, and p is the sampling probability, which is the inverse of the number of edges of the graph.
• ConditionalSampleVertex conditionally samples one vertex from the graph, given subgraph as input.
It outputs v and p, where v is the sampled vertex and p is the probability to sample v given that subgraph is already sampled.
• ConditionalSampleEdge(subgraph) conditionally samples one edge adjacent to subgraph from the graph, given that subgraph is already sampled.
It outputs e and p, where e is the sampled edge and p is the probability to sample e given subgraph.
• ConditionalClose(subgraph, subgraph) waits for edges that appear after the first subgraph to form the second subgraph.
It takes the two subgraphs as input and outputs yes/no, which is a boolean value indicating whether the second subgraph can be formed.
This function is usually used as the final step to sample a pattern where all nodes of a possible instance have been fixed (thereby fixing the edges needed to complete that instance of the pattern) and the sampling process only awaits the additional edges to form the pattern.These five APIs capture the two phases in neighborhood sampling and can be used to develop pattern mining algorithms.
To illustrate the use of these APIs, we describe how they can be used to write two representative graph patterns, shown in Figure 5.
Chain.
Using our API to write a sampling function for counting three-node chains is straightforward.
It only includes two steps.
In the first step, we use SampleEdge () to uniformly sample one edge from the graph (line 1).
In the second step, given the first sampled edge, we use ConditionalSampleEdge (subgraph) to find the second edge of the three-node chain, where subgraph is set to be the first sampled edge (line 2).
Finally, if the algorithm cannot find e 2 to form a chain with e 1 (line 3), it estimates the number of three-node chains to be 0; otherwise, since the probability to get e 1 and e 2 is p 1 · p 2 , it estimates the number of chains to be 1/(p 1 · p 2 ).
Four clique.
Similarly, we can extend the algorithm of sampling three node chains to sample four cliques.
We first sample a three-node chain (line 1-2).
Then we sample an adjacent edge of this chain to find the fourth node (line 4).
Again, during the three steps, if any edges were not USENIX Association 13th USENIX Symposium on Operating Systems Design and Implementation 751SampleThreeNodeChain (e1, p1) = SampleEdge() (e2, p2) = ConditionalSampleEdge(Subgraph(e1)) if (!
e2) return 0 else return 1/(p1.p2) (e1, p1) = SampleEdge() (e2, p2) = ConditionalSampleEdge(Subgraph(e1)) if (!
e2) return 0 (e3, p3) = ConditionalSampleEdge(Subgraph(e1, e2)) if (!
e3) return 0 subgraph1 = Subgraph(e1,e2,e3) subgraph2 = FourClique(e1,e2,e3)-subgraph1 if (ConditionalClose(subgraph1,subgraph2)) return 1/(p1.p2.p3) else return 0 sampled, the function would return 0 as no cliques would be found (line 3 and 5).
Given e 1 , e 2 and e 3 , all the four nodes are fixed.
Therefore, the function only needs to wait for all edges to form a clique (line 8-9).
If the clique is formed, it estimates the number of cliques to be 1/(p 1 · p 2 · p 3 ); otherwise, it returns 0 (line 10).
Figure 4(a) illustrates this sampling procedure (CliqueType1).
Capturing general graph pattern mining using the simple two phase API allows ASAP to extend pattern mining to distributed settings in a seamless fashion.
Intuitively, each execution of the user program can be viewed as an instance of the sampling process.
To scale this up, ASAP needs to do two things.
First, it needs to parallelize the sampling processes, and second, it needs to combine the outputs in a meaningful fashion that preserves the approximation theory.For parallelizing the pattern mining tasks, ASAP's runtime takes the pattern mining program and wraps it into an estimator3 task.
ASAP first partitions the vertices in the graph across machines and executes many copies of the estimator task using standard dataflow operations: map and reduce.
In the map phase, ASAP schedules several copies of the estimator task on each of the machines.
Each estimator task operates on the local subgraph in each machine and produces an output, which is a partial count.
ASAP's runtime ensures that each estimator in a machine sees the graph's edges and vertices in the same order, which is important for the sampling process to produce correct results.
Note that although every estimator in 3Since each program is providing an estimate of the final answer.
each partition sees the graph in the same order, there is no restriction on what the order might be (e.g., there is no sorting requirement), thus ASAP uses a random ordering which is fast and requires no pre-processing of the graph.
Once this is completed, ASAP runs a reduce task to combine the partial counts and obtain the final answer.
This is depicted in fig. 6.
This massively parallel execution is one of the reasons for huge latency reduction in ASAP.
Since the input to the reduce phase is simply an array of numbers, ASAP's shuffle is extremely lightweight, compared to a system that produces exact answers (and needs to exchange intermediate patterns).
Handling Underestimation.
Only summing up the partial counts in the reduce phase underestimates the total number of instances, because when vertices are partitioned to the workers, the instances that span across the partitions are not counted.
This results in our technique underestimating the results, and makes the theoretical bounds in neighborhood sampling invalid.
Thus, ASAP needs to estimate the error incurred due to distributed execution and incorporate that in the total error analysis.We use probability theory to do this estimation.
We enforce that the vertices in the graph are uniformly randomly distributed across the machines.
ASAP is not affected by the normal shortcomings of random vertex partitioning [35] as the amount of data communication is independent of partitioning scheme used.
In this case random vertex partitioning is in fact simple to implement, and allows us to theoretically analyze the underestimation.The theoretical proof for handling the underestimation is outside the scope of this paper.
Intuitively, we can think of the random vertex partitioning into w workers as uniform vertex coloring from w available colors.
Vertices with the same color are at the same worker and each worker estimates patterns locally on its monochromatic vertices.
By doing this coloring, the occurrence of a pattern has been reduced by a factor of 1/ f (w), where f is a function of the number of workers and the pattern.
For instance, a locally sampled triangle has three monochromatic vertices and the probability that this happens among all triangles is 1/w 2 .
Thus by the linearity of expectation, each such triangle is scaled by f (w) = w 2 .
A rigorous proof on the maximum possible w with small errors (in practice w can be >> 100), can be shown using concentration bounds and Hajnal-Szemerédi Theorem [47].
Similarly, each monochromatic 4-clique is scaled by f (w) = w 3 and f (w) can be computed for any given pattern.
Predicate Matching.
In property graphs, the edges and vertices contain properties; and thus many real-world mining queries require that matching patterns satisfy some predicates.
For example, a predicate query might ask for the count of all four cliques on the graph where every vertex in the clique is of a certain type.
ASAP supports two types of predicates on the pattern's vertices and edges all and atleast-one.
For "all" predicate, queries specify a predicate that is applied to every vertex or edge.
For example, such query may ask for "four cliques where all vertices have a weight of atleast 10".
To execute such queries, ASAP introduces a filtering phase where the predicate condition is applied before the execution of the pattern mining task.
This results in a new graph which consists only of vertices and edges that satisfy the predicate.
On this new graph, ASAP runs the pattern mining algorithm.
Thus, the "all" predicate query does not require any changes to ASAP's pattern mining algorithm.The "atleast-one" predicate allows specifying a condition that atleast one of the vertices or edges in the pattern satisfies.
An example of such a query is "four cliques where atleast one edge has a weight of 10".
To execute such predicate queries, we modify the execution to take two passes on the edge list.
In the first pass, edges that match the predicate are copied from the original edge list to a matched edge list.
Every entry in the matched list is a tuple, (edge, pos), where pos is the position in the original list where the matched edge appears.
In the second pass, every estimator picks the first edge randomly from the matched list.
This ensures that the pattern found by the estimator (if it finds one) satisfies the predicate.
For the second edge onwards, the estimator uses the original list but starts the search from the position at which the first matched edge was found.
This ensures that ASAP's probability analysis to estimate the error holds.
Motif mining.
Another query used in many real-world workloads is to find all patterns with a certain number of vertices.
We define these as motif queries; for example a 3-motif query will look for two patterns, triangles and 3-chains.
Similarly a 4-motif query looks for six patterns [51].
For motif mining we notice that several patterns have the same underlying building block.
For example, in 4-motifs, 3-chains are used in many of the constituent patterns.
To improve performance, ASAP saves the sampling phase's state for the building block pattern.
This state includes (i) the currently sampled edges, (ii) the probability of sampling at that point, and (iii) the position in the edge list up to which the estimator has traversed.
All the patterns that use this building block are then executed starting from the saved state.
This technique can significantly speedup the execution of motif mining queries and we evaluate this in Section 6.2.
Refining accuracy.
In many mining tasks, it is common for the user to first ask for a low accuracy answer, followed by a higher accuracy.
For example, users performing exploratory analysis on graph data often would like to iteratively refine the queries.
In such settings, ASAP caches the state of the estimator from previous runs.
For instance, if a query with an error bound of 10% was executed using 1 million estimators, ASAP saves the output from these estimators.
Later, when the same pattern is being queried, but with an error bound of 5% that requires 3 million estimators, ASAP only needs to launch 2 million, and can reuse the first 1 million.
A key feature in any approximate processing system is allowing users to trade-off accuracy for result latency.
To do this for graph mining, we need to understand the relation between running time and error.In ASAP's general, distributed graph pattern mining technique described earlier, the only configurable parameter is the number of estimator processes used for a mining task.
By using r estimators and making r sufficient large, ASAP is able to get results with bounded errors.
Since an estimator takes computation and memory resource to sample a pattern, picking the number of estimators r provides a trade-off between result accuracy and resource consumption.
In other words, setting a specific number of estimators, N e , results in a fixed runtime and an error within a certain bound.
As an example, fig. 7 depicts the relation between the number of estimators, runtime and error for triangle counting run on the Twitter graph [39].
To enable the user to traverse this trade-off, ASAP needs to determine the correct number of estimators given an error or time budget.
The time complexity of our approximation algorithm is linearly related to the number of edges in the graph and the number of estimators.
Given a graph and a particular pattern, we find the computation time is dominated by the number of estimators when the number of estimators is large enough.
From fig. 7, we see that the estimator-time 13th USENIX Symposium on Operating Systems Design and Implementation 753Algorithm 1 BuildTimeProfile(T * )1: P ← ∅ // store points for the profile 2: T ← 0, t ← 0, α ← α * // α * can be a reasonable random start 3: while T + t <= T * do 4: t ← run approximation algorithm with α estimators 5:P.
add((α, t)) 6:α ← 2α 7:T ← T + t curve is close to linear when the number of estimators is greater than 0.5M. Thus we propose using a linear model to relate the running time to the number of estimators.
When the number of estimators is small, the computation time is also affected by other factorsand thus the curve is not strictly linear.
However, for these regions, it is not computationally expensive to profile more exhaustively.
Therefore, to build the time profile, we exponentially space our data collection, gathering more points when the number of estimators is small and fewer points as the number of estimators grows.
We use a profiling budget T * to bound the total time spent on profiling.
Algorithm 1 shows the pseudo code.
ASAP starts from using a small number of estimators (α ← α * ), and doubles α each time until the total profiling time exceeds the profiling cost T * .
In practice, we have found that setting T * in the minute granularity gives us good results.
Since error profile is non-linear ( fig. 7), techniques like extrapolating from a few data points is not directly applicable.
Some recent work has leveraged sophisticated techniques, such as experiment design [57] or Bayesian optimization [12] for the purpose of building non-linear models in the context of instance selection in the cloud.
However, these techniques also require the system to compute the error for a given setting for which we need to know the ground-truth, say, by running the exact algorithm on the graph.
Not only is this infeasible in many cases, it also undermines the usefulness of an approximation system.In ASAP, we design a new approach to determine the relationship between the number of estimators N e and error .
Our approach is based on two main insights: first, we observe that for every pattern based on the probability of sampling, a loose upper bound for the number of estimators required can be computed using Chernoff bounds.
For instance for triangle counting, the sampling probability is 1/mc where m is the number of edges and c is the degree of first chosen edge( § 2.3.1).
This probability bound can be translated to an estimator of form N e > K * m * ∆ 2 P (Theorem 3.3 [48]) where K is a constant, m is the number of edges, ∆ is the maximum degree and P is the ground truth or the exact number of triangles.
At a high level, the bound is based on the fact that the maximum degree vertex leads to the worst case scenario where we have the minimum probability of sampling.
Similar bounds exist for 4-cliques and other patterns [48].
These theoretical bounds provide a relation between the number of estimators (N e ), error bound () and ground truth (P) in terms of the graph properties such as m and ∆.
The second insight we use is that for smaller graphs we can get a very close approximation to the ground truth by using a very large number of estimators.
This is useful in practice as this avoids having to run the exact algorithm to get a good estimate of the ground truth.
Based on these two insights, the steps we follow are: (a) We first uniformly sample the graph by edges to reduce it to a size where we can obtain a nearly 100% accurate result.
In our experiments, we find that 5 − 10% of the graph is appropriate according to the size of the graph.
(b) On the sampled graph, we run our algorithm with a large number of estimators (N gt ) to findˆPfindˆ findˆP s , a value very close to the ground truth for the sampled graph.
(c) UsingˆPUsingˆ UsingˆP s as the ground truth value and the theoretical relationship described above, we compute the value of other variables on the sampled graph.
For example, in the sampled graph, it is easy to compute m s and ∆ s , and then infer K by running varying number of estimators.
(d) Finally we scale the values m s , ∆ s andˆPandˆ andˆP s to the larger graph to compute N e .
We note that the scaledˆPscaledˆ scaledˆP might not be close to P for the larger graph.
But as we use the worst case bound to computê P s , the computed value of N e offers a good bound in practice for the larger graph.
The ELP building process in ASAP is designed to be fast and scalable.
Hence, it is possible to extend our pattern mining technique to evolving graphs [37] by simply rebuilding the ELP every time the graph is updated.
However, in practice, we don't need to rebuild the ELP for every update.
and that it is possible to reuse an ELP for a limited number of graph changes.
Thus we use a simple heuristic where are a fixed number of changes, say 10% of edges, we rebuild the ELP.
The general problem of accurately estimating when a profile is incorrect for approximate processing systems is hard [5] and in the future we plan to study if we can automatically determine when to rebuild the ELP by studying changes to the smaller sample graph we use in § 5.2.
We evaluate ASAP using a number of real-world graphs and compare it to Arabesque, a state-of-the-art distributed graph mining system.
Overall, our evaluations show that:• Compared to Arabesque, we find ASAP can improve performance by up to 77× with just 5% loss of accuracy for counting 3-motifs and 4-motifs.
• We find that ASAP can also scale to much larger graphs (up to 3.7B edges) whereas existing systems fail to complete execution.
Nodes Edges Degrees CiteSeer [30] 3,312 4732 2.8 MiCo [30] 100,000 1,080,298 22 Youtube [41] 1,134,890 2,987,624 8 LiveJournal [41] 3,997,962 34,681,189 17 Twitter [39] 41.7 million 1.47 billion 36 Friendster [61] 65.5 million 1.80 billion 28 UK [16,17] 105.9 million 3.73 billion 35 Table 2: Graph datasets used in evaluating ASAP.
• Our techniques to build error profile and time profile (ELP) are highly accurate across all the graphs while finishing within a few minutes.Implementation.
We built ASAP on Apache Spark [63], a general purpose dataflow engine.
The implementation uses GraphX [34], the graph processing library of Spark to load and partition the graph.
We do not use any other functionality from GraphX, and our techniques only use simple dataflow operators like map and reduce.
As such, ASAP can be implemented on any dataflow engine.
Datasets and Comparisons.
[41].
For all other evaluations, we use the large graphs.
Our experiments were done on a cluster of 16 Amazon EC2 r4.2xlarge instances, each with 8 virtual CPUs and 61GiB of memory.
While all of these graphs fit in the main memory of a single server, the intermediate state generated ( §2) during pattern mining makes it challenging to execute them.
Arabesque, despite being a highly optimized distributed solution, fails to scale to the larger graphs in our cluster.
We note that Arabesque (or any exact mining system) needs to enumerate the edges significantly more number of times compared to ASAP which only needs to do it once or twice, depending on the query.
Patterns and Metrics.
For evaluating ASAP, we use two types of patterns, motif s and cliques.
For motifs, we consider 3-motifs (consisting of 2 individual patterns), and 4-motifs (consisting of 6 individual patterns) and for cliques, we consider 4-cliques.
For our experiments, we run 10 trials for each point and report the median, and error bar in the ELP evaluation.
We do not include the time to load the graph for any of the experiments for ASAP and Arabesque.
We use total runtime as the metric when raw performance is evaluated.
When evaluating ASAP on its ability to provide errors within the requested bound, we need to know the actual error so that it can be compared with ASAP's output.
We compute actual error as |t−t r e al | t r e al , where t r eal is the ground truth number of a specific pattern in a given graph.
Since this requires us to know the ground-truth, we use simpler, known patterns, such as triangles and chains, where the ground-truth can be obtained from verified sources for such experiments.Note that the actual error is only used for evaluation purposes.
Unless otherwise stated, the ASAP evaluations were done with an error target of 5% at 95% confidence.
We first present the overall performance numbers.
To do so, we perform comparisons with Arabesque and evaluate ASAP's scalability on larger graphs.
We do not include ELP building time in these numbers since it is a one-time effort for each graph/task and we measure this in § 6.3.
Comparison with Arabesque.
In this experiment, we compare Arabesque and ASAP on the 4 smaller graphs (Table 2).
In each of these systems, we load the graph first, and then warm up the JVM by running a few test patterns.
Then we use each system to perform 3-motif and 4-motif mining, and measure the time taken to complete the task.
In Arabesque, we do not consider the time to write the output.
Similarly, for ASAP we do not output the patterns embeddings.
The results are depicted in figs. 8a and 8b.
We see that ASAP significantly outperforms Arabesque on all the graphs on both the patterns, with performance improvements up to 77× with under 5% loss of accuracy.
The performance improvements will increase if the user is able to afford a larger error (e.g., 10%).
We also noticed that the performance gap between Arabesque and ASAP increases with larger graph and/or more complex patterns.
In this experiment, mining the more complex pattern (4-motif) on the largest graph (LiveJournal) provides the highest gains for ASAP.
This validates our choice of using approximation for large-scale pattern mining.
Scalability on Larger Graphs.
We repeat the above experiment on the larger graphs.
Since Arabesque fails to execute on these graphs on our cluster, we also provide performance numbers that were reported by its authors [55] as a rough comparison.
The results are shown in Table 3.
When mining for 3-motif, ASAP performs vastly superior on the Twitter, the Friendster, and the UK graphs.
Arabesque's authors report a run time of approximately 11 hours on a graph with a similar number of edges.
This translates to a 258× improvement for ASAP.
In the case 4These graph datasets in Arabesque are not publicly available.
of 4-motifs, ASAP is easily able to scale to the more complex pattern on larger graphs.
In comparison, Arabesque is only able to handle a much smaller graph with less than 200 million edges.
Even then, it takes over 6 hours to mine all the 4-motif patterns.
These results indicate that ASAP is able to not only outperform state-of-the-art solutions significantly, but do so in a much smaller cluster.
ASAP is able to effortlessly scale to large graphs.
We next evaluate the advanced pattern mining capabilities in ASAP described in § 4.3.
Motif mining.
We first evaluate the impact of ASAP's optimization when handling motif queries for multiple patterns.
We use the Twitter graph and study a 4-motif query that looks for 6 different patterns.
In this case ASAP caches the 3-node chain that is shared by multiple patterns.
As shown in with another query that has a 5% error bound.
We find that the running time goes from 2.5min to 1.5min (40% improvement) when our caching technique is enabled.
Here, we evaluate the effectiveness of the ELP building techniques in ASAP, described in §5.
Time Profile.
To evaluate how well our time profiling technique ( § 5.1) works, we run three patterns-3-chains, triangles, and 4-cliques-on the three large graphs.
In each graph, we obtain the time vs. estimator curve by exhaustively running the mining task with varying number of estimators and noting the time taken to complete the task.
We then use our time profiling technique which uses a small number of points instead of exhaustive profiling to obtain ASAP's estimate.
We plot both the curves in fig. 9 for each of the three graphs.
In these figures, the colored lines represent the actual (exhaustively profiled) curve, and the black line shows ASAP's estimate.
From the figure we can see that the time profile estimated by ASAP very closely tracks the actual time taken, thereby showing the effectiveness of our technique.
Error Profile.
We repeat the experiment for evaluating ASAP's error profile building technique.
Here, we exhaustively build the error profile by running a different number of estimators on each graph, and note the error.
Then we use ASAP's technique of using a small portion of the graph to build the profile.
We show both in fig. 10.
We see that the actual errors are always within the estimated profile.
This means that ASAP is able to guarantee that the answer it returns is within the requested error bound.
We also note that in real-world graphs, the worst-case bounds are never really reached.
In edge cases, where the number of patterns in the graphs are high like the chains in UK graph, the overestimation may be large, and one concern might be that we run more estimators than required.
We are working on techniques that can help us determine a tighter bound for the number of estimators in the future but as discussed in § 6.1, even with this overestimation we get significant speedups in practice.
This experiment confirms that ASAP's heuristic of using a very small portion of the graph and leveraging the Chernoff bound analysis ( § 5.2) is a viable approach.
Error rate Confidence.
In Figure 11, we evaluate the cumulative distribution function (CDF) of 100 independent runs on the UK graph with 3% error target and 99% confidence.
We can see that 100/100 actual results are not worse than 3% error and 74/100 results are within 2% error.
Thus the actual results are even better than the theoretical analysis for 99% confidence.
ELP Building Time.
Finally, we evaluate the time taken for building the profiling curves.
For this, we use the UK graph and configure ASAP to use 1% of the graph to build the error profile.
The results are shown in table 5 for different patterns, which shows that the time to build the profiles is relatively small, even for the largest graph.
how configurations with different numbers of machines impact the accuracy.
In Fig. 12, we consider two scenarios: strong-scaling, where we fix the total number of estimators used for the entire graph, and increase the number of machines used; and weak-scaling where we fix the number of estimators used per-machine and thus correspondingly scale the number of estimators as we add more machines.
We run the triangle counting task with the Twitter graph on different cluster sizes of 4, 8, 12, and 16 machines.
From the figure we see that in the strong-scaling regime, adding more machines has no impact on the accuracy of ASAP and that we are able to correctly adjust the accuracy as more graph partitions are created.
In the weak-scaling case we see that the accuracy improves as we increase more machines, which is the expected behavior when we have more estimators.
Finally, we evaluate the generality of ASAP's techniques by applying to mine 5-motifs, consisting of 21 individual patterns.
This choice was influenced by our conversations with industry partners, who use similar patterns in their production systems.
Due to the complexity of the patterns, we used a larger cluster for this experiment, consisting of 16 machines, each with 16 cores and 128GB memory.
Due to space constraints, and also because of the absence of a comparison, we only provide ASAP's performance on two representative patterns in table 6.
As we see, ASAP is able to handle complex patterns on large graphs easily.
A large number of systems have been proposed in the literature for graph processing [20,23,34,35,40,42,50,53,54,58,64].
Of these, some [40,42,54] are single machine systems, while the rest supports distributed processing.
By using careful and optimized operations, these systems can process huge graphs, in the order of a trillion edges.
However, these systems have focused their attention mainly on graph analysis, and do not support efficient graph pattern mining.
Some systems implement very specific versions of simple pattern mining (e.g., triangle count).
They do not support general pattern mining.
Similar to graph processing systems, a number of graph mining systems have also been proposed.
Here too, the proposals contain a mix of centralized systems and distributed systems.
These proposals can be classified into two categories.
The first category focuses on mining patterns in an input consisting of multiple small graphs.
This problem is significantly easier, since the system only finds one instance of the pattern in the graph, and is trivially incorporated in ASAP.
Since this approach can be massively parallelized, several distributed systems exist that focus specifically on this problem.
The stateof-the-art in distributed, general purpose pattern mining systems is Arabesque [55].
While it supports efficient pattern mining, the system still requires a significant amount of time to process even moderately sized graphs.
A few distributed systems have focused on providing approximate pattern mining.
However, these systems focus on a specific algorithm, and hence are not general-purpose.
In distributed data processing, approximate analysis systems [6,13,32] have recently gained popularity due to the time requirements in processing large datasets.
Following the approximate query processing theory in the database community, these systems focus on reducing the amount of data used in the analysis process in the hope that the analysis time is also reduced.
However, as we show in this work, applying the exact algorithm on a sampled graph does not yield desired results.
In addition, doing so complicates, or even makes it infeasible to provide good time or error guarantees.
Theory community has invested a significant amount of time in analyzing and proposing approximate graph algorithms for several graph analysis tasks [9,10,15,18,28,33].
None of these are aimed at distributed processing, nor do they propose ways to understand the performance profile of the algorithms when deployed in the real world.
We leverage this rich theoretical foundation in our work by extending these algorithms to mine general patterns in a distributed setting.
We further devise a strategy to build accurate profiles to make the approach practical.
We present ASAP, a distributed, sampling-based approximate computation engine for graph pattern mining.
ASAP leverages graph approximation theory and extends it to general patterns in a distributed setting.
It further employs a novel ELP building technique to allow users to trade-off accuracy for result latency.
Our evaluation shows that not only does ASAP outperform state-of-the-art exact solutions by more than a magnitude, but it also scales to large graphs while being low on resource demands.
