1.0 Background and Introduction.
The University Assessment Committee (UAC) was established at the University of Central Florida (UCF) in 1996 to support a process of continual self-evaluation and improvement.
The primary purpose of the UAC is to oversee and assist academic and administrative units in conducting ongoing assessment to improve student-learning, student development, and university services and operations.
More than 210 academic programs and 120 administrative units are involved in conducting assessment at the UCF.
Each year, faculty and staff collect data, report results of the previous year's assessment, and develop assessment plans for the upcoming year.
This process includes the annual submission of an assessment report made up of the following components: 1) results of the previous year's assessment, 2) proposed or actual changes based on these results, and 3) a new assessment plan to measure the impact of these changes (which includes measurement of the effect of change made).
Undergraduate programs incorporate Academic Learning Compacts student learning outcomes assessment (as specified by the Florida Board of Governors) into their assessment results reporting.
The UCF Office of Excellence and Assessment Support (OEAS) is currently the authority and administrative unit that oversees all program assessment and improvement processes on campus.
In October 2005, CS, IT, CpE and EE were merged into the School of EECS.
The focus of the merger was to integrate fully the degree programs and operations of the school.
Leadership of the School of EECS consisted of representatives from all programs/disciplines within the School.In Spring and Summer of 2006, the new EECS director and associate director began to identify and redefine the objectives, missions, and operational guidelines of the School.
New mission, vision, goals, objectives, and outcomes were discussed at length, and they were agreed upon and finalized throughout the remainder of the year.The cycle of assessment began anew in late 2006/early 2007 with the true merging of curriculum, outcomes, objectives, mission, and vision for a unified School of EECS.
Some of the more important details of these activities will be discussed in the remainder of this section.
The first order of business of the new School was planning for the upcoming accreditation review in 2008.
This began in earnest in the Fall 2006.
Since each degree program already had assessment coordinators (stemming from University OEAS requirements for degree programs), it became School policy that this group would be responsible for planning and coordinating all assessment and accreditation activities for the School.
In the remainder of the discussion, we shall refer to this group as the Coordinators.In 2006 ABET was organized into three commissions, the Engineering Accreditation Commission (EAC), the Computing Accreditation Commission (CAC), and the Technology Accreditation Commission (TAC).
In addition, the accreditation criteria defined by each commission were structured around the traditional (a-k) model that had long been used by ABET for engineering programs.
For the School of EECS, this meant CpE and EE would be accredited by the EAC, while CS and IT would be accredited by CAC.
The new accreditation criteria defined by EAC and CAC imposed some significant challenges for the School.
The ABET commissions were now clearly distinguishing between program objectives and program outcomes.
In past accreditation reviews these terms and concepts were used almost interchangeably by each program and by the UAC.
For the 2008 review we had to define a set of objectives that would define the professional skills we expected our majors to be practicing 3-5 years post graduation.
Program outcomes, on the other hand, were skills and knowledge we could measure during the educational process.
The operating principle was that by achieving the desired level of performance with respect to program outcomes at graduation, it was highly probable that our graduates would achieve the professional objectives we defined for our respective programs.In late Fall 2006, each program within the School defined their program objectives.
In addition to having a defined set of program objective, ABET required that our assessment process incorporated appropriate mechanisms for measuring our success in achieving these objectives.
This could be done partially through our Industrial Affiliates Board, but to quantify success required that we implement an Industrial and Alumni Surveys.
These surveys were developed on-line to make it more convenient for those completing the survey, and to automate the collection and analysis of the results.
We were successful in collecting good data from these surveys just in time for writing our Self Study Reports in Spring 2008.
Computer Engineering and Electrical Engineering had historically defined and had been assessing the set of ABET (a-k) outcomes as their program outcomes.
However, the situation was quite different for Computer Science and Information Technology.Because of the newness of CAC accreditation criteria for CS and IT, ABET gave these degree programs a choice.
New programs (e.g. IT) would have to be accredited under the new CAC (an) outcomes.
This rule definitely applied to our IT program because it was going up for accreditation for the first time (under CAC).
CS, on the other hand, was renewing accreditation and could choose either the old CSAB criteria or the new CAC (a-k) criteria.
CS chose the new CAC (a-k) criteria to be consistent with all other degree programs in the School.One of the major complications this decision created for both CS and IT was that the program outcomes we had previously defined and had been assessing through OEAS processes did not align well with CAC(a-k) or CAC(a-n).
We determined (see Mock Visit below) that some of our program outcomes should have been defined as program objectives.
Furthermore, several of the new CAC outcomes were not covered at all by our (then current) program outcomes.Fortunately, through our normal OEAS cycle, both CS and IT had sufficient time to redefine the respective program outcomes to bring them more into a one-to-one alignment with CAC outcomes.
Ironically, as we learned later, ABET/CAC recognized that such disconnects would occur and were not so concerned that CS and IT program outcomes did not fully align with the new CAC outcomes.
Their primary concern was that whatever program outcomes we had defined were all being assessed and on a regular repeatable basis.
Historically it had been a turf battle between computer science and computer engineering as to which program should offer software engineering and programming courses.
This was one issue that could finally be laid to rest because of the merger.
Indeed, course duplication among CS, CpE and EE was eliminated and degree program requirements were modified to avoid duplication of courses.
Where necessary, common course pre-requisites were changed to ensure students from different degree programs were properly prepared and on a level playing field.There were essentially three areas where most of the course duplication occurred: operating systems, software engineering, and computer architecture.
Computer and Electrical Engineering did have one or two courses they could merge.
In each case a single course was created and the duplicate eliminated.
The surviving course carried the designation of the "home" program in the catalog, either CS or CpE or EE.
The home program was responsible for teaching and administering the common course.
The greatest concern among the Coordinators was the timing of the curriculum unification relative to the preparation activities for accreditation.
Specifically, the unified or common courses were scheduled to be implemented in Fall 2007, just one year before our accreditation review.
Assessment data collected in the common courses had to be factored into several sets based on the majors of students taking the course; for example, CS students were analyzed separately from CpE students.
To complicate the situation further, assessment of course content had to be made with respect to different sets of degree program outcomes.To mitigate some of the problems of assessing common or merged courses, we designed our ABET Course Description (CD) documents and Assessment Report (AR) documents so that each course would define set of course outcomes (typically 4 -6) that characterized the most important content and learning objectives for that course.
Each course outcome would be assessed and mapped independently to a set of program outcomes based on what majors were taking the course.
Using this approach enabled the course instructors to collect one set of assessment data (for course outcomes) and then map this data to one or more sets of program outcomes.
(This topic will be addressed again in Section 3.0)Another concern created by course mergers was the fact that in the initial offering of common courses, majors from one program might be placed at a disadvantage because their background was not in agreement with the new pre-requisites for the common course; pre-requisites were waived for the first academic year of the merger to prevent students from having to delay their graduation by going back and taking pre-requisite courses -to compensate for lack of prerequisites, instructors and TAs would offer extra hours for tutorial and remedial training.This problem was manifest in some of the upper level common courses.
For example, if a common course was home to CS there was usually a large amount of programming required.
CpE students entering this course for the first time, without having had all the new pre-requisites, would not have had the same programming experience and breadth of programming language knowledge that CS majors would have had.
This disparity would likely be reflected in the assessment data for CpE students and could jeopardize meeting CpE program outcomes.
While this problem would disappear with time (as students filtered through the new pre-requisite structure), the Coordinators were concerned that the smoothing effect of time would not occur quickly enough for accreditation.
Fortunately, this concern was not realized in fact.
The Program Improvement Process that is used by all programs in the School of EECS is depicted in Figure 1.
As shown, Educational Objectives assessment and refinement has been instituted to occur on a 3 year cycle.
Meanwhile, the assessment and refinement of Program Outcomes has been instituted to occur annually.
As depicted in the upper left corner of Figure1, the program improvement process begins with Definition of Educational Objectives.
The EECS process of defining program Educational Objectives involves all of our constituents in five essential ways:A) Directly through participation as members of our Industrial Affiliates Board, B) Indirectly by the relationships our faculty have with our constituents through consulting and research projects, C) Directly through Undergraduate Student Forums held once per semester, D) Directly through participating in our Industrial Survey; and finally, E) Directly by responding to our Alumni Survey.The Educational Objectives are then approved by faculty vote at an EECS faculty meeting.
Next, Educational Objectives are used to create Program Outcomes that meet or exceed ABET requirements by the Curriculum Oversight and Review Committees (there is one CORC for each degree program in the School).
The Curriculum Oversight and Review Committees also map courses in a given degree program to the corresponding Program Outcomes.
The goal is to ensure that every Program Outcome is covered by at least one, and ideally two, required courses in the curriculum.
Conversely, every required course defines a map to a subset of Program Outcomes which it supports.
The Program Outcomes and course mappings are then approved by faculty vote at an EECS faculty meeting.Each required course in the curriculum is assigned a Course Custodian.
This person is a regular permanent faculty member who ensures that: 1.
course content delivered is complete and consistent with respect to the course description document, 2.
course assessment is conducted and documented in the course assessment report, and 3.
any instructor questions are answered in a consistent fashion regardless of which instructor delivers the course.The Custodian is responsible for writing the official Course Description Document.
This document defines learning outcomes for the course and maps each course outcome to the applicable Program and ABET outcomes (if they differ); the number of course outcomes varies slightly, but typically falls between 4 and 6.
In addition, the course description defines direct assessment measures for each course outcome along with a threshold that defines the expected level of performance for students passing the course; more precisely, we expect that the median assessment score for a given course outcome is greater than or equal to the defined threshold.Next, the course is delivered once the semester begins.
The instructor delivering a course may or may not be the Course Custodian.
If not, the instructor is required to meet with the Custodian to ensure that all requirements for course content and course assessment are understood.
As dictated by the Course Description Document, the instructor conducts assessment throughout the semester, maintaining a spreadsheet of assessment data for each student.
This spreadsheet is summarized in an Assessment Report which is finalized at the end of the course.
The Assessment Report provides details documenting the performance of each passing student with respect to each course outcome as well as the percent of students achieving the desired performance threshold for each course outcome.
In addition, the Assessment Report contains a table mapping the performance on course outcomes to a performance metric for Program outcomes.
The course is considered to have met the learning goals of a given Program outcome if more than 50% of the course outcomes mapping to the given Program outcome meet or exceed their performance thresholds.
Finally, the course instructor analyzes the assessment data for each course outcome and writes a recommendation for actions that might be taken in future offerings of the course to improve performance on outcomes for which assessment goals have not been met.
The Assessment Report is submitted to the Course Custodian.Next each instructor's Assessment Report is reviewed by the Course Custodian, if they are not one and the same person.
The Course Custodian provides uniformity and continuity from semester to semester even if various faculty members happen to teach the course in different semesters.
After reviewing the Assessment Reports for a given course the Custodian makes his own summary report that provides input to the Curriculum Oversight and Review Committee (CORC), and also to the relevant Technical Area Committee (TAC).
The responsibility of the Curriculum Oversight and Review Committee is to review all of the Assessment Reports collected during a given term or assessment cycle -no less frequently than once per year.
The CORC has the authority to directly approve changes whose scope is limited to an individual course.
It may consider changes at the curriculum level but must submit such recommendations to the School Faculty for official approval prior to implementation.Whether at the course or curriculum level, the CORC must clear its decisions and recommendations with the relevant TACs.
The TACs have jurisdiction over a set of courses within a particular sub-discipline or technical area.
Any changes to content or course outcomes must meet with the approval of the relevant TAC.
Once the CORC actions and recommendations have been cleared by the TACs, course changes pass on to the Course Custodian for immediate implementation, while recommendations for curriculum changes are submitted for consideration at the next School Faculty meeting.Finally, the processes of assessment and refinement of Educational Objectives occurs on a threeyear cycle and are shown on the top portion of Figure 3-1.
In particular, surveys of Industry and Government Employers of Graduates, Alumni themselves, and more directed survey feedback of the Industrial Advisory Board (IAB) are collected.
These are provided to the Curriculum Oversight and Review Committees which interpret assessment whether or not Educational Objectives are being met.
Based on achievement of the Educational Objectives, along with input from constituents, the CORCs may recommend changes to the Educational Objectives.
Constituent input is actively sought via IAB meetings and Undergraduate Student Forums held at least once per year.
Recommendations by the CORCs go before the School Faculty for final approval.Membership to the Curriculum Oversight and Review Committees are made by the EECS Director.
Each CORC must have a student representative from the relevant degree program.
The CORC includes program coordinators, assessment coordinators and at least one other faculty member, preferably a Full Professor, from each program (with these faculty member positions rotating annually) plus at least 1 representative(s) from the EECS Industry Advisory Board.
The greatest challenge posed by the new accreditation criteria was to demonstrate that each program was regularly assessing their program outcomes and then also demonstrating how this data was used to effect program changes.
In this section we describe the data collection process in more detail and give examples of the various instruments we used to collect the data and present the results.
As mentioned earlier, the Course Description document was the official standard that defined course content and how it was to be assessed.
The CD contained the following elements: Course title and number Pre-requisites Course Custodian Text books and references Course topic outline Course outcomes, the instruments used to assess each outcome and the performance threshold for each outcome.
For each degree program, a mapping from course outcomes to program outcomes.For all required courses in each School program, the Course Descriptions contained the same information and format.
Furthermore, the instruments used to assess each course outcome consisted of specific questions on exams, quizzes and homework problems and/or separately graded deliverables on outside projects and programming assignments.
These instruments were explicitly identified for each course outcome.
During delivery of a course, an Excel Spreadsheet was maintained to capture the assessment data.
Table 1 illustrates the structure and organization of this spreadsheet for quizzes and exams.
A similar table is used for collecting data from homework or project assignments.
The columns of this table correspond to questions on a given exam or quiz.
The first row allows the instructor to define the mapping from questions to course outcomes -as shown, question 1 maps to course outcome #1.
Not all questions may map directly to course outcomes.
The rightmost columns of the table are used respectively to tally total points over all questions, and total points accumulated for each course outcome measured by the exam.
Rows at the bottom of the table specify, respectively, the maximum points possible on each question, the assessment threshold for each question, the mean and standard deviation of scores for each question.
The table entries marked "#1 Threshld" and "#3 Threshld" give the tally of the Threhld across all questions mapping to course outcomes #1 and #3, respectively.
The percentage of students whose scores for a give course outcome exceeds the Threshld for that outcome is a measure of how well the given course outcome is being met at the point in the course at which the exam was given.Of course, the percentage of students exceeding the threshold for a given question or outcome cannot be an absolute measure of success or failure.
The difficulty of the questions mapping to a given course outcome is obviously a factor when interpreting the significance of this metric.
The instructor can use the mean and stdev in the "totals" columns and in the question columns to gauge whether the questions were too difficult or the students just were not mastering the material.These statistics can be tracked over multiple offerings of the course by the same instructor to determine whether or not the performance thresholds should be adjusted.
These statistics can also be very strong indicators about the relative performance of different instructors in the same course.
Table 2 illustrates the table used to map course outcomes to a given set of program outcomes.
If a course serves multiple degree programs, there would be one of these tables for each program being assessed by the given course.
Entries in this table lists the percentage of students meeting or exceeding the performance threshold for a given course outcome identified by the row.
This percentage is computed by summing the total scores and thresholds across all exams, quizzes and other assignments that measure the given course outcome (see Table 1).
If a given course outcome maps to one or more program outcomes (it may map to none), the same performance percent is entered into each column corresponding to a program outcome supported or addressed by the given course outcome.
As depicted in Table 2, course outcome #1 maps to program outcomes #1 and #3.
The Totals row is computed as a pair (N/D), where D denotes the number course outcomes contributing to the program outcome identified by the column.
N denotes the number of performance percentages from the contributing rows that exceed 50%.
If N/D is interpreted as a fraction and its value is greater than 0.5, then we conclude that more than half of the course outcomes supporting the given program outcome indicate that assessment goals are being met by this course for the given program outcome.
As an example, the Totals row is computed assuming X and Y are greater than 50% and Z is less than 50%.
A table such as this can also be used to measure achievement relative to ABET outcomes (a-k).
The only difference would be that the columns would identify the ABET outcomes.
The last table we wish to discuss is the Program Performance (or Rollup) Table.
This is illustrated by Table 3.
The structure of this table is identical to Table 2.
The only differences are: (a) the rows identify courses in the curriculum for which assessment was conducted; (b) entries in the table for a given course (row) are copied from the Totals row in Table 2 for that course.The Totals row of Table 3 is specified as a fraction (N/D) where N is the sum of the numerators of fractions given in a column and D is the sum of the denominators of the fractions given in the column.
This aggregate fraction gives the total number of course outcomes for which the assessment performance criteria were met divided by the total number of course outcomes contributing to a give program outcome.
As for Table 2, if this final fraction is greater than 0.5, then we conclude that assessment data indicates the given program outcome is being achieved by the degree program.
Several hurdles had to be surmounted during our preparation of the Self Study reports.
First and foremost we began the Self Study too late.
Our advice to those who read this in time, you must start at least two months earlier than the anticipated preparation time to allow for internal reviews and unforeseen dependencies on other organizations external to the program.
Most of all, program administrators must lead the charge and oversee the project of producing the Self Study report.First point: The Self Study requires the involvement of many people at all levels of the academic organization.
The College contributed enormously to the effort in producing our individual Self Study documents.
They contributed data about student demographics across all degree programs, as well as information about funded research, library resources, facilities, and budget related matters.
The College collected course descriptions for all courses taught by supporting programs (e.g. Physics, Mathematics, Chemistry, English, etc.)Second point: It is typical that information of different types is known and managed by different individuals.
Matters of budget and the day-to-day operation of a program or academic unit are known only by the chair or director of that academic unit.
Matters concerning research and the graduate program are know and maintained by faculty assigned those administrative responsibilities.
The oversight of the undergraduate program is usually performed by the Undergraduate Coordinator who is different from the Graduate Coordinator, etc.
In summary, information that must go into the Self Study is known only to certain individuals and these individuals must participate in the writing of the document.Third point.
The faculty have to produce several documents in specific formats for inclusion within the Self Study: Curriculum Vitae, Course Descriptions and Assessment Reports.
Getting faculty to respond in a timely manner is always a problem.
It helps if the director or chair of the academic program applies pressure to ensure these documents are produced on time and within in the constraints defined by ABET.Final point.
We made a small error in planning our Self Study effort.
We thought we could assign the writing of different criteria to different individuals and then copy and paste to create the final documents for each degree program.
This approach did not work for many criteria because what EAC and CAC requirements was different.
In the end, the most efficient way to produce the Self Study was to have the program coordinator be responsible for writing and managing the assembly of the Self Study.
That person was also responsible for enlisting the support of others to write the various sections of the document that required specialized and compartmentalized knowledge.The bottom line is that writing the Self Study is a major project that requires sufficient planning and enlisting the commitment of various key people to contribute to the writing effort.
One of our successful projects in support of both the accreditation visit and our on-going assessment process was the development of a publically-available website that presented our assessment process, data and results as well the statement of mission and program outcomes for each degree program in the School.
We also provided password protected access to the Self Studies for each degree program.
Giving accreditation reviewers access to this website at the time we submitted our Self Study afforded them the luxury of quickly accessing any information that went into our Studies over the six month period before the on-campus visit.We continue to develop this website for maintaining our assessment process and data.
It will be a very powerful tool to use in preparing for our accreditation review in 2014.
If you decide to tackle such a project, you should plan on hiring a webmaster full-time.
The reader may access this website at http://abet.eecs.ucf.edu/ The School hired ABET consultants (former Program Evaluators and Team Leads) to spend three days simulating very accurately the process the actual ABET review teams would be doing six months later.
These consultants were brought in at the end of April, after we had completed our Self Study.
This was a very worth-while effort and enabled us to make important changes to our Self Studies before submitting them to ABET.
The benefits of the Mock review for CpE program included the following revisions/changes to our ABET Self Study report and other related documents.
(c) The Mock reviewers pointed out that our student/faculty ratio was too high.
Since we could not simply change or improve the ratio drastically over the next months prior to the visit, the recommendation was to highlight the strengths of the School of EECS faculty where a number of faculty in other programs can cross teach CpE classes.
For instance, a CS faculty can also teach classes in CpE program and vice versa.
The Mock reviewers identified several defects in our Self Study documents.
• They pointed out that our program outcomes were not well-defined.
Some our outcomes should have been included among our program objectives (professionally oriented).
They also observed that our program outcomes were in poor alignment with the new CAC outcomes (a-k).
Because we had learned this information in early April, we were able to revise the outcomes to be in better alignment with CAC (a-k).
• They pointed out that CS might get a weakness because of lack of assessment data for some of our program outcomes.
This defect was anticipated, in part, because our OEAS assessment plan for measuring some of the program outcomes had not been fully implemented.
Up until the last completed annual assessment cycle, CS had used only a Senior Exit Survey, the CS Foundation Exam, and an Alumni Survey to measure program outcomes.
The CS Foundation Exam was the only direct measure we had been using, but it was very narrowly focused on two or three of our program outcomes.
The Senior Exit Survey was an indirect measure (opinion and recall by graduating seniors) and was extremely unreliable.
We had realized that direct measures had to be implemented in several key courses to collect more reliable data.
Unfortunately, those direct measures had only recently been implemented (Summer of 2007) and no data from these measures had been available to report in our OEAS 2006-07 assessment report.As it turned out, the accreditation Review team did give one program a conditional weakness on our assessment data.
The primary reason was not that some program outcomes were not being assessed, but rather that we had only been collecting good assessment data (according to our unified assessment process) for one assessment cycle.
ABET requires that you must be able to demonstrate a pattern of regular and periodic assessment and then to demonstrate how this data is used to improve your program.
Thus we had to show that we had been operating our assessment process for at least two cycles -which we could not.
The remedy was that we continue to collect assessment data (as we have done) and report that to ABET before they make a final ruling on our accreditation status.
• Our IT program suffered the same defects as CS.
At most one assessment cycle's worth of data had been collected for existing courses and there were a number of curriculum deficiencies that had only recently been rectified.
New courses had to be defined and approved to cover the missing content in our IT degree program.
These courses could not go on-line until Fall 2008 (the time of the Review).
Thus, the Mock reviewers recommended we withdraw our application for IT accreditation.
This we did.
• Another observation made by the Mock reviewers was that we had not made clear the mapping from course and/or program outcomes to CAC outcomes (a-k).
The remedy was to add mapping tables ( Table 2) that defined the mapping from course outcomes to CAC outcomes in every course Assessment Report.
In addition, we included a version of Table 3 in our Self Study showing how the curriculum performed relative to CAC outcomes.
• Finally, the Mock reviewers expressed concern that we would be able to demonstrate "closing the loop", meaning that we had to demonstrate a documented trail of meetings and decisions by the CORCs showing how assessment data was used to effect curriculum changes.
Meeting this concern was a challenge because of timing, but we did succeed in documenting changes (instituted by the CORC) to three required courses to ensure that all program and CAC outcomes would be assessed on the next assessment cycle.
An important task for the School was preparing a room for the Review Teams to use while they were on campus.
This room had to provide computers with internet access, a printer and a paper shredder.
This room also served as a repository for all the course binders and textbooks for all three degree programs (IT had withdrawn from consideration by this point).
The important advice we offer to our readers is that putting together all the course display materials is a major effort.
We hired a very competent person, working 20hrs/week, beginning in August of 2008 and continuing through the Site Visit in November of 2008, to assemble course display materials and organize the display room for the Review team.
This person also had to be knowledgeable about many details concerning the three degree programs and how they related (e.g. common courses, elective courses, etc.) The result of our effort is shown in Figures 2 and 3 below.Some details should be brought to attention in Figure 2.
On the left side of the room we placed a bookcase with binders for elective courses only.
Some faculty had even gone to the trouble to conduct assessments on these courses.
On the pillar on the right side of the room we posted a diagram of our assessment process.
This was a good decision because on one occasion the to answer their questions.
Two important items should not be overlooked or given short shrift in preparing for the accreditation visit.
Preparing labs is critical and particular with attention given to safety and security.
Of course labs should be clean and should be equipped with the latest hardware and software.The second item is to continue ABET training sessions for all faculty.
These sessions ensured that all faculty members knew important details about the assessment process and knew where to find important information, such as program outcomes and objectives.
All of our preparations paid off.
The actual Site Visit went very smoothly.
An important detail that should be given special consideration is the scheduling of interviews with faculty and students.
Make sure that you know the wishes of the program evaluators in this regard ahead of time and then leave ample time in their schedule to meet with faculty individually and/or as a group.
The evaluators are particularly pleased when there is a good turnout by students at their interview time.
Ensuring good turnout may require active marketing during the two weeks prior to the visit.
There was one Concern for CpE on Criterion 6 (Faculty).
The reason for this concern was the high student/faculty ratio (approximately 36:1) and large class sizes.
To address the high student/faculty ratio and large class sizes noted during the 2008 ABET Accreditation, the Computer Engineering (CpE) program pursued approval for additional full-time faculty lines.
The reasons behind insufficient number of full time CpE faculty were due to faculty retirements and unforeseen faculty departures in the last decade or so.
Two new faculty lines were approved in the Software Engineering area and a search is currently underway.
As discussed earlier, Computer Science was given a conditional Weakness regarding our ability to demonstrate that our assessment process was repeatable and being conducted on a regular basis.
This weakness stemmed primarily from the fact that CS could only produce one assessment cycle's worth of data.
The Weakness could be down-graded to a Concern by showing ABET that we could produce the assessment data for the next cycle (Fall 2008-Spring 2009.
This we are doing.
We have just filed a report to ABET based on the Fall 2008 data and its analysis by the CORC.Computer Science also had some Concerns.
One Concern related to being able to support and assess CAC outcomes (e)(g) and (h).
In the past we had partially covered those outcomes by requiring CS students to take a course in Ethics in Science and Technology offered by the Philosophy department.
However, to quantitatively assess whether or not these specific outcomes were being adequately covered in this course we would have needed to place an extra burden on the Philosophy instructors teaching this course.
Instead, we agreed to drop the Philosophy course from our requirements and replace it with one of our own IT courses that covered the ethical and societal issues defined by these ABET outcomes.
Furthermore, it was decided to alter the content of one of our existing CS courses to specifically include exercises focusing on issues defined by these ABET outcomes.
While the Review team recognized our intent, these changes had not been implemented at the time of the review.
Thus, the program received a Concern in this area.
