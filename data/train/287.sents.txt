Achieving high performance for distributed I/O on a wide-area network continues to be an elusive holy grail.
Despite enhancements in network hardware as well as software stacks, achieving high-performance remains a challenge.
In this paper, our worldwide team took a completely new and non-traditional approach to distributed I/O, called ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing, by utilizing application-specific transformation of data to orders-of-magnitude smaller meta-data before performing the actual I/O.
Specifically, this paper details our experiences in deploying a large-scale system to facilitate the discovery of missing genes and constructing a genome similarity tree by encapsulating the mpiBLAST sequence-search algorithm into ParaMEDIC.
The overall project involved nine different computational sites spread across the U.S. generating more than a petabyte of data, that was "teleported" to a large-scale facility in Tokyo for storage.
With the rapid growth in the scale and complexity of scientific applications over the past few decades, the requirements for compute, memory and storage resources are now greater than ever before.
With the onset of petascale and exascale computing, issues related to managing such grand-scale resources, particularly related to data I/O, need to be carefully studied.
For example, applications including genomic sequence search and the emergent field of metagenomics, large-scale data mining, data visual analytics and communication profiling on ultra-scale parallel computing platforms, generate massive amounts of data that needs to be managed for later processing or archival.To add to the complexity of this problem is the issue of resource locality.
While system sizes have certainly grown over the past few years, most researchers do not have local access to systems of the scale required by their applications.
Therefore, researchers access such large systems remotely to perform the required computations and move the generated data to their local systems after the computation is complete.
Similarly, many applications tend to require multiple resources simultaneously for efficient execution.
For example, applications that perform large computations and generate massive amounts of output data are becoming increasingly common.
While several large-scale supercomputers provide either the required compute power or the storage resources, very few provide both.
Thus, data generated at one site often has to be moved to a different site for storage and/or analysis.To alleviate issues related to moving such massive data across sites, there has been a lot of monetary and intellectual investment in high-speed distributed network connectivity [2,5,33].
However, the utility of these investments is limited in the light of three primary observations: (i) in spite of the effort put into high-speed distributed networks, such infrastructure is scarce and does not provide end-to-end connectivity to a very high percentage of the scientific community, (ii) the amount of data generated by many applications is so large that even at 100% network efficiency, the I/O time for these applications can significantly dominate their overall execution time and (iii) based on recent trends and published results, existing distributed I/O mechanisms have not been able to achieve a very high network utilization for "real data" on high-speed distributed networks, particularly for single stream data transfers [3,36].
To resolve such issues on a global scale, we proposed a new non-traditional approach for distributed I/O known as ParaMEDIC (Parallel Metadata Environment for Distributed I/O and Computing) [11,12].
ParaMEDIC uses application-specific semantic information to process the data generated by treating it as a collection of high-level abstract objects, rather than as a generic bytestream.
It uses such information to transform the data into orders-of-magnitude smaller metadata before transporting it over the distributed environment and regenerating it at the target site.
All data transformation, movement, and regeneration is done while the application is executing, giving the illusion of an ultra-fast teleportation device for large-scale data over distributed environments.At a high-level ParaMEDIC is similar to standard compression algorithms.
However, the term "compression" typically has a connotation that the data is dealt as a generic byte-stream.
Since ParaMEDIC uses a more abstract application-specific representation of the data to achieve a much larger reduction in the data size, we use the terminology of "metadata transformation" in this case.Because ParaMEDIC utilizes application semantics to generate metadata, it loses some portability compared to traditional bytestream based distributed I/O.
For example, an instance of ParaMEDIC's metadata transformation in the context of the mpiBLAST sequence search application is described in Section 3.2.
However, by giving up some portability, ParaMEDIC can potentially attain tremendous savings in the amount of actual distributed I/O performed, consequently resulting in substantial performance gains.
Further, through the use of a generic framework with an application plugin model, different applications can utilize the overall framework in an easy and flexible manner.In this paper, we demonstrate how we used ParaMEDIC to tackle two large-scale computational biology problems-discovering missing genes and adding structure to genetic sequence databases-on a worldwide supercomputer.
The overall worldwide supercomputer comprised nine different supercomputers distributed at seven sites across the U.S., and one large-scale storage facility located in Japan.
The overall experiment consisted of sequence searching the entire microbial genome database against itself generating more than a petabyte of data that was transported to Tokyo for storage.
We present several insights gained from this large scale run which will be of immense value to other researchers performing such large global-scale distributed computation and I/O.
In this section we discuss details on different aspects of computational biology with a focus on the compute and storage requirements of large-scale applications in this domain.
With the advent of rapid DNA sequencing, the amount of genetic sequence data available to researchers has increased exponentially [9].
The GenBank database, a comprehensive database that contains genetic sequence data for more than 260,000 named organisms, has exhibited exponential growth since its inception over 25 years ago [13].
This information is available for researchers to search new sequences against and infer homologous relationships between sequences or organisms.
This helps progress a wide range of projects, from assembling the Tree of Life [18] to pathogen detection [20] and metagenomics [21].
Unfortunately, the exponential growth of sequence databases necessitates faster search algorithms to sustain reasonable search times.
The Basic Local Alignment Search Tool (BLAST), which is the de facto standard for sequence searching, uses heuristics to prune the search space and decrease search time with an accepted loss in accuracy [7,8].
mpiBLAST parallelizes BLAST using several techniques including database fragmentation, query segmentation [16], parallel input-output [27], and advanced scheduling [38].
As shown in Figure 1, mpiBLAST uses a master-worker model and performs a scatter-search-gather-output execution flow.
During the scatter, the master splits the database and query into multiple pieces and distributes them among worker nodes.
Each worker then searches the query segment against the database fragment that it was assigned.
The results are gathered by the Master, formatted, and output to the user.
Depending on the size of the query and that of the database, such output generated can be large (Table 1).
Thus, for environments with limited I/O capabilities, such as distributed systems, the output step can cause significant overheads.
Genome annotation is the process of associating information with a genomic sequence.
Part of this process includes determining (by computational analysis) the location and structure of protein-encoding and RNAencoding genes, also known as making gene calls.
It is important to be as accurate and as sensitive as possible in making gene calls: avoiding false positives and missing real genes.
Gene prediction in prokaryotes (bacteria and archaea) typically involves evaluating the coding potential of genomic segments which are delimited by conserved nucleotide motifs.
The most widely used gene finding programs [17,28] build a sequence model based on statistical properties of genes known to be (very likely) real genes in a given genome.
This model is then used to evaluate the likelihood that an individual segment codes for a gene.
In using this method it is almost always the case that some genes with anomalous composition are missed.
Another popular method for locating genes is to compare genomic segments with a database of gene sequences found in similar organisms.
If the sequence is conserved, the segment being evaluated is likely to be a coding gene (this is the "similarity method").
Genes that do not fit a given genomic pattern and do not have similar sequences in current annotation databases may be systemically missed.
One way to detect missed genes is to use the similarity method and compare raw genomes against each other, as opposed to comparing a raw genome to a database of known genes.
If gene a in genome A and gene b in genome B have been missed and a is similar to b, then this method will find both.
However, this involves performing an all-to-all comparison of the entire microbial genome database against itself.
This task is heavily compute and data intensive, requiring thousands of compute processors and generating more than a petabyte of output data that needs to be stored for processing.
One of the major issues with sequence searching is the structure of the sequence database itself.
Currently, these databases are unstructured and represented as a flat file and each new sequence that is discovered is simply appended to the end of the file.
Without more intelligent structuring, a query sequence has to be compared to every sequence in the database forcing the best-case to take just as long as the worst-case.
By organizing and providing structure to the database, searches can be performed more efficiently by discarding irrelevant portions entirely.One way to structure the sequence database is to create a sequence similarity tree where "similar" sequences are closer together in the tree than dissimilar sequences.
The connections in the tree are created by determining how "similar" the sequences are to each other determined through sequence searches.
To create every connection, however, the entire database has to be searched against itself resulting in an output size of N 2 values (where N is the number of sequences in the database).
In our previous work [11,12], we provided a detailed description of ParaMEDIC.
Here we present a brief summary of this work.
ParaMEDIC provides a framework for decoupling computation and I/O in applications relying on large quantities of both.
Specifically, it does not hinder application computation.
However, as the output data is generated, the framework differs from traditional distributed I/O; it uses application-semantic information to process the data generated by treating it as a collection of high-level application-specific objects rather than as a generic byte-stream.
It uses such information to transform the data into orders-ofmagnitude smaller metadata before transporting it over the distributed environment and regenerating it at the target site.
As shown in Figure 2, ParaMEDIC provides several capabilities including support for data encryption and integrity as well as data transfer in distributed environments (either directly via TCP/IP communication or through global file-systems).
However, the primary semantics-based metadata creation is done by the application plugins.
Most application plugins are specific to each application, and thus rely on knowledge of application semantics.
These plugins provide two functionalities: (i) processing output data generated by the application to create metadata and (ii) converting metadata back to the final output.
Together with application-specific plugins, ParaMEDIC also provides application-independent components such as data compression, data integrity, and data encryption.
These can be used in conjunction with the application-specific plugins or independently.
The amount of computation required in ParaMEDIC is higher than what is required by the original application; after the output is generated by the application processes, it has to be further processed to generate the metadata, sent to the storage site, and processed yet again to regenerate the final output.
However, the I/O cost achieved can potentially be significantly reduced by using this framework.
In other words, ParaMEDIC trades (a small amount of) additional computation for (potentially large) reduction in I/O cost.
With respect to the additional computational cost incurred, ParaMEDIC is quite generic with respect to the metadata processing required by the different processes.
For many applications, it is possible to tune the amount of post-processing performed on the output data, with the general trend being-the more the post-processing computation, the better the reduction in the meta-data size.
That is, an application plugin can perform more processing of the output data to reduce the I/O cost.
In a cluster environment, most of the mpiBLAST execution time is spent on the search itself as the BLAST string-matching algorithm is computationally intensive, compared to which the cost of formatting and writing the results is minimal, especially when many advanced clusters are configured with high-performance parallel file-systems.
However, in a distributed environment, the output typically needs to be written over a wide-area network to a remote file-system.
Hence, the cost of writing the results can easily dominate the execution profile of mpiBLAST and become a severe performance bottleneck.
By replacing the traditional distributed I/O framework with ParaMEDIC (as shown at the top of Figure 3), we can provide large reduction in the amount of data I/O performed.
For example, as we will see in Section 5, a mpiBLAST-specific instance of ParaMEDIC reduces the volume of data written across a wide-area network by more than two orders of magnitude.
ParaMEDIC then re-runs mpiBLAST at the storage site by taking as input the same query sequence and the temporary database to generate and write output to the storage system.
The overhead in rerunning mpiBLAST at the storage site is quite small as the temporary database that is searched is substantially smaller, with only about 250 sequences in it, compared to the several millions of sequences in large DNA databases.
In this paper, to accommodate the compute and storage requirements of the computational biology applications discussed in Section 2, we utilize a world-wide supercomputer that, in aggregate, provides the required compute power and storage resources.
The worldwide supercomputer comprises nine high-performance computing systems at seven different sites across the U.S. and a largescale storage facility in Japan, to create a single high-performance distributed computing system.
The specifics of each individual system are in Several systems in our worldwide supercomputer operate in batch-mode.
Users submit jobs to system queues and are scheduled to execute on the available resources.
That is, compute resources are not available in a dedicated manner, but become available when our job is scheduled for execution and become unavailable when our job terminates.To handle this issue, we segment the overall work to be performed into small tasks that can be performed independently (i.e., sequentially, concurrently or out-of-order).
The management of tasks is done by a centralized server running on a dedicated resource.
As each job is executed, it contacts this server for the next task, computes the task, transfers the output to metadata and transmits the metadata to the storage site.
This approach has two benefits.
First, the clients are completely stateless.
That is, if a client job terminates before it has finished its computation or metadata transmission to the storage site, the servers handle this failure by reassigning the task to a different compute client.
The second advantage is if the metadata corresponding to a tasks is either not received completely or is corrupted, the server just discards the data and reassigns the task to another compute node.
Thus, I/O failures are never catastrophic.
One of the key impediments to large-scale distributed systems is system heterogeneity.
Many distributed systems, such as the one used in this paper, cannot obtain a homogeneous environment in either hardware or software and efficient use of the system requires overcoming this obstacle.
The worldwide supercomputer used in this paper contains six different CPU architectures (e.g., IBM PowerPC 970FX, IBM PowerPC 440, AMD Opteron, SiCortex MIPS64, Intel Xeon, Intel Itanium2), five different network interconnects (e.g., Gigabit Ethernet, 10-Gigabit Ethernet, InfiniBand, IBM proprietary 3D toroidal network, and SiCortex Kautz graph), and eight variations of the Linux operating system.To deal with this issue, all data being transferred over the network has to be converted to an architecture-independent format.Since the total amount of data that is generated and has to be moved to the storage site in enormous, this can significantly impact on traditional distributed I/O.
However, with ParaMEDIC, only metadata generated by processing the actual output is transferred across the wire.
Since this metadata is orders-of-magnitude smaller as compared to the actual output, such byte manipulation to deal with heterogeneity has minimal impact on overall performance.
In traditional file I/O, there are two levels of parallelism.
First, multiple I/O servers are aggregated into a parallel file-system to take advantage of the aggregate storage bandwidth of these servers.
Second, multiple compute clients, that process different tasks, write data to such file-systems in parallel as well.
Most parallel file-systems are optimized for such access to give the best performance.With ParaMEDIC, there are three I/O components: (i) compute clients that perform I/O, (ii) post-processing servers that process the metadata to regenerate the original output and (iii) I/O servers which host the file-system.
Similar to the traditional I/O model, the first and third components are already parallelized.
That is, multiple streams of data being written in parallel by different compute clients and the I/O servers parallelize each stream of data that is being written to them.
However, in order to achieve the best performance, it is important that the second component, post-processing servers, be parallelized as well.Parallelizing the post-processing servers adds its own overhead and complexity mainly with respect to synchronization between the different parallel processes.
To avoid this, we utilize an embarrassingly parallel approach for these servers.
Each incoming stream of data is allocated to a separate process till a maximum number of processes is reached, after which the incoming data requests are queued till a process becomes available again.
Thus, different processes do not have to share any information and can proceed independently.
The advantage of this approach is its simplicity and lack of synchronization required between different parallel post-processing servers.
The disadvantage, however, is that the number of data streams generated from the post-processing servers is equal to the number of incoming data streams.
That is, if only two tasks are active at one time, only two streams of data are written to the actual storage system.
Thus, the performance might not be optimal.
However, in most cases, we expect the number of incoming streams to be sufficiently large to not face such performance issues.
As seen in Table 2, the computational sites are between 9,000 and 11,000 kilometers away from the storage site.
At these distance, communication latencies are in tens of milliseconds.
Such large latencies can severely limit the effectiveness of a synchronous remote file-systems that can be used for distributed I/O, since each synchronization operation has to make round-trip hops on the network.
To overcome the bottleneck incurred by such high-latency, our worldwide supercomputer utilizes a lazy asynchronous I/O approach.
By caching the output data locally before performing the actual output, clients can perform their computations while disconnected from the remote file-system.
After a substantial amount of data is generated a bulk transfer of the metadata occurs, thereby maximally utilizing the bandwidth available between the sites and mitigating the effect of high-latency.
An issue with this approach of disconnected computation is fault-tolerance.
Once a task is assigned to a compute client, the server is completely disconnected from this client.
After the computation is complete, the client reconnects and sends the generated metadata.
Though, this two-phase synchronization model is more error prone and requires additional state information in the server, it makes the compute clients truly stateless even when the actual computation is going on.
In this paper, we use ParaMEDIC to search the entire microbial genome database against itself.
Several supercomputing centers within the U.S. perform the computation, while the data generated is stored at a large storage resource in Tokyo.
This section Figure 4 illustrates the amount of data transmitted between the compute and the storage sites for different number of post-processing threads, and the factor of reduction in the amount of data.
Each post-processing thread processes one segment of data which has the output for 10,000 query sequences.
Most segments have approximately similar output sizes, so the amount of data communicated over the distributed network increases linearly with the number of segments, and hence the number of post-processing threads.
ParaMEDIC, on the other hand, processes the generated data and converts it into metadata before performing the actual transfer.
Thus, the actual data that is transferred over the network is much smaller.
For example, with 288 threads, mpiBLAST communicates about 100GB of data, while ParaMEDIC only communicates about 108MB-a 900-fold reduction.
We also illustrate the I/O time in Figure 5.
As shown, ParaMEDIC outperforms mpiBLAST by more than two orders-of-magnitude.
This result is attributed to multiple aspects.
First, given the shared network connection between the two sites, the achievable network performance is usually much lower than within the cluster.
Thus, with mpiBLAST transferring the entire output over this network, its performance would be heavily impacted by the network performance.
Second, the distance between the two sites causes the communication latency to be high.
Thus, file-system control messages tend to take a long time to be exchanged, resulting in further loss in performance.
Third, for mpiBLAST, since the wide-area network is a bottleneck, the number simultaneously transmitted data streams does not matter; communication is serialized in the network.
However, with ParaMEDIC, since the wide-area network is no longer a bottleneck, it can more effectively utilize the parallelism in the data streams to better take advantage of the storage bandwidth available at the storage site, as desribed in more detail in Section 5.2.
Figure 6(a) illustrates the storage bandwidth utilization achieved by mpiBLAST, ParaMEDIC, and the MPI-IO-Test benchmark, that is used as an indication of the peak performance capability of the I/O subsystem.
We notice that the storage utilization of mpiBLAST is very poor compared to ParaMEDIC.
This is because, for mpiBLAST, the I/O is limited by the wide-area network bandwidth.
Thus, though more than 10,000 processors are performing the compute part of the task, the network connecting the On the other hand, ParaMEDIC uses more than 90% of the storage system capability (shown by MPI-IO-test).
When the number of processing threads is low (x-axis in figure), ParaMEDIC uses about half the storage capability.
However, as the number of processing threads increases, the I/O utilization of ParaMEDIC increases as well.
In Figure 7(a), we remove the file-system network bottleneck and directly perform I/O on the local nodes.
Storage utilization achieved in this case is significantly higher than going over the network.
Even in this case, ParaMEDIC completely uses the storage capability with more than 90% efficiency.
Figure 7(b) shows the percentage breakup of the time spent.
Similar to the case with the Lustre file-system, a significant portion of the time is still spent on I/O.
Thus, again, ParaMEDIC can be expected to scale and fully use even faster storage resources.
Though this paper only deals with enhancing the mpiBLAST application through ParaMEDIC, the idea is relevant for many other applications as well.
For example, applications that have natively been built for distributed environments such as SETI@home [35] and other BOINC applications [1] can easily use similar ideas, and can benefit aspects in which such techniques are possible.
In the field of communication profiling with MPE [4], we have also done some preliminary work that utilizes metadata transformation of profiled data through ParaMEDIC.
Specifically, based on the observation that most scientific applications have a very uniform and periodic communication pattern, we perform a fourier transform on the data to identify this periodicity and use this as an abstract block.
The metadata comprises one complete abstract block and just the differences for all other blocks.
Our preliminary numbers in this field have demonstrated between 2 and 5-fold reduction in the I/O time using ParaMEDIC.
Work on other application fields including earthquake modeling and remote visualization is ongoing as well, with promising preliminary results.
Efficient I/O access for scientific applications in distributed environments has been an ongoing subject of research for various parallel and distributed file-systems [14,22,32,34].
There has also been work on explicit data transfer protocols such as GridFTP [6].
Other efforts include providing remote data access through MPI-IO [29].
RIO [19] introduced a proof-of-concept implementation that allows applications to access remote files though ROMIO [37].
RFS [25] enhanced the remote write performance with active buffering, by optimizing overlap between applications computation and I/O.
Studies have also been done in translating MPI-IO calls into operations of lower level data protocols such as Logistic Network [26].
However, all these approaches deal with data as a byte-stream.
Conversely, our approach focuses on aggressively reducing the amount of I/O data to be communicated by taking advantage of application semantics and dealing with data as high-level abstract units, rather than a stream of bytes.Semantic-based data transformation is not new.
Several semantic compression algorithms have been investigated in compressing relational databases [10,23,24].
Leveraging the table semantics, these algorithms first build descriptive models of the database using data mining techniques such as clustering, and strip out data that can be regenerated.
In the multimedia field, context-based coding techniques (similar to semantics-based approaches) have been widely used in various video compression standards [15,30,31].
With aid of context modeling, these techniques efficiently identify redundant information in the media.
Although sharing the same goal of reducing data to store or transfer with ParaMEDIC, these data compression studies do not address the remote I/O issue.Thus, ParaMEDIC utilizes ideas from different fields to provide a novel approach for distributed I/O.
Rapid growth of computational power is enabling computational biology to tackle increasingly large problems such as discovering missing genes and providing structure to genetic sequence databases.
However, as the problems grow larger, so does the data consumed and produced by the applications.
For many applications, the required compute power and storage resources cannot be found at a single location, precipitating the transfer of large amounts of data across the wide-area network.
ParaMEDIC mitigates this issue by pursuing a non-traditional approach to distributed I/O.
By trading computation for I/O, ParaMEDIC utilizes application semantics information to transform the output to orders-of-magnitude smaller metadata.
In this paper, we presented our experiences in solving large-scale computational biology problems by utilizing nine different high-performance compute sites within the U.S. to generate a petabyte of data, that was transferred to a large-scale storage facility in Tokyo using ParaMEDIC's distributed I/O capability.
We demonstrated that ParaMEDIC can achieve a performance improvement of several orders of magnitude compared to traditional I/O.
In future, we plan to evaluate semantic-based compression for other applications.
We would like to thank the following people for their technical help in managing the large-scale run and other experiments associated with this paper: (i) R. Kettimuthu
