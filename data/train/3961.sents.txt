In this paper we explore several contexts where an adversary has an upper hand over the defender by using special hardware in an attack.
These include password processing, hard-drive protection, cryptocurrency mining , resource sharing, code obfuscation, etc.
We suggest memory-hard computing as a generic paradigm, where every task is amalgamated with a certain procedure requiring intensive access to RAM both in terms of size and (very importantly) bandwidth, so that transferring the computation to GPU, FPGA, and even ASIC brings little or no cost reduction.
Cryptographic schemes that run in this framework become egalitarian in the sense that both users and attackers are equal in the price-performance ratio conditions.
Based on existing schemes like Argon2 and the recent generalized-birthday proof-of-work, we suggest a generic framework and two new schemes: • MTP, a memory-hard Proof-of-Work based on the memory-hard function with fast verification and short proofs.
It can be also used for memory-hard time-lock puzzles.
• MHE, the concept of memory-hard encryption, which utilizes available RAM to strengthen the en-cryption for the low-entropy keys (allowing to bring back 6 letter passwords).
Historically attackers have had more resources than defenders, which is still mostly true.
Whether it is secret key recovery or document forgery, the attackers are ready to spend tremendous amount of computing power to achieve the goal.
In some settings it is possible to make most attacks infeasible by simply setting the key length to 128 bits and higher.
In other settings the secret is limited and the best the defender can do is to increase the time needed for the attack, but not to render the attack impossible.Passwords, typically stored in a hashed form, are a classical example.
As people tend to choose passwords of very low entropy, the security designers added unique salts and then increased the number of hash iterations.
In response the attackers switched to dedicated hardware for password cracking, so that the price of single password recovery dropped dramatically, sometimes by a few orders of magnitude.A similar situation occurred in other contexts.
The Bitcoin cryptocurrency relies on continuous preimage search for the SHA-256 hash function, which is much cheaper on custom ASICs, consuming up to 30,000 times less energy per solution than most efficient x86 laptops [2].
Eventually, the original concept of an egalitarian cryptocurrency [25] vanished with the emergence of huge and centralized mining pools.Related problems include password-based key derivation for hard-drive encryption, where the data confidentiality directly depends on the password entropy, and where offline attack is exceptionally easy once the drive is stolen.
Similar situation arise in the resource sharing and spam countermeasures.
In the latter it is proposed that every user presents a certain proof (often called proof-of-work), which should be too expensive for spammers to generate on a large scale.
Yet another setting is that of code obfuscation, in which powerful reverseengineering/de-compilation tools can be used in order to lift the proprietary code or secrets embedded in the software.
Our idea is to remedy the disparity between ordinary users and adversaries/cheaters, where latter could use botnets, GPU, FPGA, ASICs to get an advantage and run a cheaper attack.
We call it egalitarian computing as it should establish the same price for a single computation unit on all platforms, so that the defender's hardware is optimal both for attack and defence.
Equipped with egalitarian crypto schemes, defenders may hope to become to be on par with the most powerful attackers.The key element of our approach is large (in size) and intensive (in bandwidth) use of RAM as a widely available and rather cheap unit for most defenders.
In turn, RAM is rather expensive on FPGA and ASIC 1 , and slow on GPU, at least compared to memoryless computing tasks.
All our schemes use a lot of memory and a lot of bandwidth -almost as much as possible.We suggest a single framework for this concept and concrete schemes with an unique combination of features.In the future, adoption of our concept could allow a homogenization of computing resources, a simplified security analysis, and relaxed security requirements.
When all attackers use the same hardware as defenders, automated large-scale attacks are no longer possible.
Shorter keys, shorter passwords, faster and more transparent schemes may come back to use.
The idea of extensive memory use in the context of spam countermeasures dates back at least to 2003 [5,13] and was later refined in [15].
Fast memoryintensive hash functions were proposed first by Percival in 2009 [27] an later among the submissions of the Password Hashing Competition.
Memory-intensive proofsof-work have been studied both in theory [16] and practice [6,32].
Paper structure We describe the goals of our concept and give a high level overview in Section 2.
Then we describe existing applications where this approach is implicitly used: password hashing and cryptocurrency proofs of work (Section 3).
We present our own progress-free Proof-of-Work MTP with fast verification, which can also serve as a memory-hard time-lock puzzle, in Section 4.
The last Section 5 is devoted to the novel concept of memory-hard encryption, where we present our scheme MHE aimed to increase the security of password-based disk encryption.
Our goal is to alter a certain function H in order to maximize its computational cost on the most efficient architecture -ASICs, while keeping the running time on the native architecture (typically x86) the same.
We ignore the design costs due to nontransparent prices, but instead estimate the running costs by measuring the time-area product [8,31].
On ASICs the memory size M translates into certain area A.
The ASIC running time T is determined by the length of the longest computational chain and by the ASIC memory latency.Suppose that an attacker wants to compute H using only a fraction αM of memory for some α < 1.
Using some tradeoff specific to H , he has to spend C(α) times as much computation and his running time increases by the factor D(α) (here C(α) may exceed D(α) as the attacker can parallelize the computation).
In order to fit the increased computation into time, the attacker has to placeC(α)D(α) additional cores on chip.
Therefore, the time-area product changes from AT 1 to AT α asAT α = A · (α + βC(α) D(α) )T · D(α) = = AT 1 (αD(α) +C(α)β ), (1)where β is the fraction of the original memory occupied by a single computing core.
If the tradeoff requires significant communication between the computing cores, the memory bandwidth limit Bw max may also increase the running time.
In practice we will have It is known [19] that any function whose computation is interpreted as a directed acyclic graph with T vertices of constant in-degree, can be computed using O( T log T ) space, where the constant in O() depends on the degree.
However, for concrete hash functions very few tradeoff strategies have been published, for example [9].
D(α) ≥ C(α) · Bw/Bw max , Our idea is to combine a certain computation H with a memory-hard function F .
This can be done by modifying H using input from F (amalgamation) or by transforming its code to an equivalent one (obfuscation).
The amalgamation is used as follows.
The execution of H is typically a sequence of smaller steps H i , i < T , which take the output V i−1 from the previous step and produce the next output V i .
For our purpose we need another primitive, a memory-hard function F , which fills the memory with some blocks X[i], i < T .
We suggest combining H with F , for example like:H 񮽙 = H 񮽙 T • H 񮽙 T −1 • ··· • H 񮽙 1 , where H 񮽙 i (V i−1 ) = H(V i−1 ⊕ X[i − 1]).
Depending on the application, we may also modify X [i] as a function of V i−1 so that it is impossible to precompute F .
The idea is that any computation of H 񮽙 should use T blocks of memory, and if someone wants to use less, the memory-hardness property would impose computational penalties on him.
This approach will also work well for any code that uses nonces or randomness produced by PRNG.
PRNG could then be replaced by (or intermixed with the output of) F.The obfuscation principle works as follows.
Consider a compiler producing an assembly code for some function H .
We make it to run a memory-hard function F on a user-supplied input I (password) and produce certain number of memory blocks.
For each if-statement of the form if x then A else B the compiler computes a memory-hard bit b i which is extracted from the block X[i] (the index can also depend on the statement for randomization) and alters the statement as if x ⊕ b i then A else B for b i = 0 and if x ⊕ b i then B else A for b i = 1.
This guarantees that the program will have to run F at least once (the bits b i can be cached if this if-statement is used multiple times, ex.
in a loop).
Accessing the memory block from a random memory location for each conditional statement in practice would slow down the program too much, so compiler can perform a tradeoff depending on the length of the program, the number of conditional statements in it and according to a tunable degree of required memory-hardness for a program.
Memory-hard bits could be mixed into opaque predicates or other code obfuscation constructs like code-flattening logic.We note that in order for a program to run correctly, the user needs to supply correct password for F , even though the source code of the program is public.
A smart decompiler, however, when supplied with the password, can obtain clean version of the program by running F only once.Our schemes described in the further text use the amalgamation principle only, so we leave the research directions in obfuscation for future work.
In this section we outline several applications, where memory-hard functions are actively used in order to achieve egalitarian computing.
The typical setting for the password hashing is as follows.
A user selects a password P and submit it to the authentication server with his identifier U.
The server hashes P and unique salt S with some function F , and stores (U, S, F(P, S)) in the password file.
The common threat is the password file theft, so that an attacker can try the passwords from his dictionary file D and check if any of them yields the stolen hash.
The unique S ensures that the hashes are tried one-by-one.
Following massive password cracking attacks that use special hardware [23,30], the security community initiated the Password Hashing Competition [3] to select the hash function that withstands the most powerful adversaries.
The Argon2 hash function [10] has been recently selected as the winner.
We stress that the use of memoryhard function for password hashing does not make the dictionary attacks infeasible, but it makes them much more expensive in terms of the single trial cost.
We use Argon2 in our new schemes described in Sections 4 and 5.
Here we outline the key elements of the Argon2 design that are used in our scheme.
For more details and their rationale we refer the reader to [10].
Argon2 takes P, S, and possibly some additional data U as inputs.
It is parametrized by the memory size M, number of iterations t, and the available parallelism l. X[1] = H(P, S); X[i] = F(X[i − 1], X[φ (i)]), i > 1; Out → H(X[M]).
(2)The indexing function φ (i) is defined separately for each of two versions of Argon2: 2d and 2i.
The Argon2d version, which we use, compute it as a function of the previous blockX[i − 1].
The authors proved [10] that all the blocks are generated distinct assuming certain collision-resistant-like properties of F.
They also reported the performance of 0.7 cpb on the Haswell CPU with 4 threads, and 1.6 cpb with 1 thread.Tradeoff security of Argon2 Using the tradeoff algorithm published in [9], the authors report the values C(α) and D(α) up to α = 1/7 with t = 1.
It appears that C(α) is exponential in α, whereas D(α) is linear.α 1 2 1 3 1 4 1 5 1 6 1 7 C(α) 1.5 D(α) 1.5 2.8 5.5 10.3 17 27 Table 1: Time and computation penalties for the ranking tradeoff attack for Argon2d.
A proof-of-work scheme is a challenge-response protocol, where one party (Prover) has to prove (maybe probabilistically) that it has performed a certain amount of computation following a request from another party (Verifier).
It typically relies on a computational problem where a solution is assumed to have fixed cost, such as the preimage search in the Bitcoin protocol and other cryptocurrencies.
Other applications may include spam protection, where a proof-of-work is a certificate that is easy to produce for ordinary sender, but hard to generate in large quantities given a botnet (or more sophisticated platform).
The proof-of-work algorithm must have a few properties to be suitable for cryptocurrencies:• It must be amortization-free, i.e. producing q outputs for B should be q times as expensive;• The solution must be short enough and verified quickly using little memory in order to prevent DoS attacks on the verifier.
• The time-space tradeoffs must be steep to prevent any price-performance reduction.
• The time and memory parameters must be tunable independently to sustain constant mining rate.
• To avoid a clever prover getting advantage over the others the advertised algorithm must be the most efficient algorithm to date (optimization-freeness).
• The algorithm must be progress-free to prevent centralization: the mining process must be stochastic so that the probability to find a solution grows steadily with time and is non-zero for small time periods.
• Parallelized implementations must be limited by the memory bandwidth.As demonstrated in [11], almost any hard problem can be turned into a proof-of-work, even though it is difficult to fulfill all these properties.
The well-known hard and NP-complete problems are natural candidates, since the best algorithms for them run in (sub)exponential time, whereas the verification is polynomial.
The proof-ofwork scheme Equihash [11] is built on the generalizedbirthday, or k-XOR, problem, which looks for a set of n-bit strings that XOR to zero.
The best existing algorithm is due to Wagner [34].
This problem is particularly interesting, as the time-space tradeoff steepness can be adjusted by changing k, which does not hold, e.g., in hard knapsacks.Drawbacks of existing PoW We briefly discuss existing alternatives here.
The first PoW schemes by Dwork and Naor [14] were just computational problems with fast verification such as the square root computation, which do not require large memory explicitly.
The simplest scheme of this kind is Hashcash [7], where a partial preimage to a cryptographic hash function is found (the so called difficulty test).
Large memory comes into play in [13], where a random array is shared between the prover and the verifier thus allowing only large-memory verifiers.
This condition was relaxed in [15], where superconcentrators [28] are used to generate the array, but the verifier must still hold large memory in the initialization phase.
Superconcentrators were later used in the Proof-of-Space construction [16], which allows fast verification.
However, the scheme [16] if combined with the difficulty test is vulnerable to cheating (see Section 4.4 for more details) and thus can not be converted to a progress-free PoW.
We note that the superconcentrators make both [15] and [16] very slow.Ad-hoc but faster schemes started with scrypt [27], but fast verification is possible only with rather low amount of memory.
Using more memory (say, using Argon2 [10]) with a difficulty test but verifying only a subset of memory is prone to cheating as well (Section 4.4).
The scheme [11] is quite promising, but the reference implementation reported is quite slow, as it takes about 30 seconds to get a proof that certifies the memory allocation of 500 MB.
As a result, the algorithm is not truly progress-free: the probability that the solution is found within the first few seconds is actually zero.
It can be argued that this would stimulate centralization among the miners.
In addition, the memory parameter does not have sufficient granularity and there is no correlation between the allocated memory and the minimal time needed to find the proof.Finally, we mention schemes Momentum [21] and Cuckoo cycle [32], which provide fast verification due to their combinatorial nature.
They rely on the memory requirements for the collision search (Momentum) or graph cycle finding (Cuckoo).
However, Momentum is vulnerable to a sublinear time-space tradeoff [11], whereas the first version of the Cuckoo scheme was recently broken in [6].
We summarize the properties of the existing proof-ofwork constructions in Table 2.
The AT cost is estimated for the parameters that enable 1-second generation time on a PC.
In this section we present a novel proof-of-work algorithm MTP (for Merkle Tree Proof) with fast verification, which in particular solves the progress-free problem of [11].
Our approach is based on the memory-hard function, and the concrete proposal involves Argon2.
Since fast memory-hard functions F such as Argon2 perform a lengthy chain of computations, but do not solve any NP-like problem, it is not fast to verify that Y is the output of F. Checking some specific (say, last) blocks does not help, as explained in detail in the further text.
We thus have to design a scheme that lower bounds the time-area product for the attacker, even if he computes a slightly modified function.
Consider a memory-hard function F that satisfies Equation (2) (for instance, Argon2) with a single pass over the memory producing T blocks and a cryptographic hash function H (possibly used in F ).
We propose the following non-interactive protocol for the Prover (Figure 1) in Algorithm 1, where L and d are security parameters.
The average number of calls to F is T + 2 d L.Algorithm 1 MTP: Merkle-tree based Proof-of-Work.
Prover's algorithm Input: Challenge I, parameters L, d. .
.
.
, X[T ] in the memory.2.
Compute the root Φ of the Merkle hash tree (see Appendix A).3.
Select nonce N.4.
Compute Y 0 = H(Φ, N)where G is a cryptographic hash function.
Output:5.
For 1 ≤ j ≤ L: i j = Y j−1 (mod T ); Y j = H(Y j−1 , X[i j ]).
Proof (Φ, N, Z ).
The verifier, equipped with F and H, runs Algorithm 2.
Input: Proof (Φ, N, Z ), parameters L, d. 1.
Compute Y 0 = H(Φ, N).2.
Verify all block openings using Φ.
Z for 1 ≤ j ≤ L: X[i j ] = F(X[i j − 1], X[φ (i j )]); Y j = G(Y j−1 , X[i j ]).
Output: Yes/No.
Let the computation-space tradeoff for H and the default memory value T be given by functions C(α) and D(α) (Section 2).
Memory savings Suppose that a cheating prover wants to reduce the AT cost by using αT memory for some α < 1.
First, he computes F (I) and Φ, making C(α)T Table 2: Review of existing proofs of work.
Litecoin utilizes scrypt with 128KB of RAM followed by the difficulty test).
M/less -memoryless; constr.
-constrained.calls to F.
Then for each N he has to get or recompute L blocks using only αT stored blocks.
The complexity of this step is equal to the complexity of recomputing random L blocks during the first computation of F .
A random block is recomputed by a tree of average size C(α) and depth D(α).
Therefore, to compute the proofof-work, a memory-saving prover has to make C(α)(T + 2 d L) calls to F, so his amount of work grows by C(α).
The second cheating strategy is to compute a different function F 񮽙 񮽙 = F .
More precisely, the cheater produces some blocks X[i 񮽙 ] (which we call inconsistent as in [16]) not as specified by Equation (2) (e.g. by simply computing X[i 񮽙 ] = H(i 񮽙 )).
In contrast to the verifiable computation approach, our protocol allows a certain number of inconsistent blocks.
Suppose that the number of inconsistent blocks is εT , then the chance that no inconsistent block is detected by L opened blocks isγ = (1 − ε) L .
Therefore, the probability for a proof-of-work with εM inconsistent blocks to pass the opening test is γ.
In other words, the cheater's time is increased by the factor 1/γ.
We note that it does not make sense to alter the blocks after the Merkle tree computation, as any modified block would fail the opening test.Overall cheating penalties Let us accumulate the two cheating strategies into one.
Suppose that a cheater stores αT blocks and additionally allows εT inconsistent blocks.
Then he makes at leastC(α + ε)(T + 2 d L) γ(3)calls to F.
The concrete values are determined by the penalty function C(), which depends on F .
Both honest prover and cheater can parallelize the computation for 2 t different nonces.
However, the latency of cheater's computation will be higher, since each block generates a recomputation tree of average depth D(α + ε).
Now we can explain in more details why the composition of F and the difficulty test is not a good proof-of-work even if some internal blocks of H are opened.
Suppose that the proof is accepted if H(X [T ] would then modify the previous block, or X[φ (T )], or an earlier block and then propagate the changes.
A single inconsistent block is just too difficult to catch 2 .
As a concrete application, we suggest a cryptocurrency proof-of-work based on Argon2d with 4 parallel lanes.
We aim to make this PoW unattractive for botnets, so we suggest using 2 GB of RAM, which is very noticeable (and thus would likely alarm the user), while being bearable for the regular user, who consciously decided to use his desktop for mining.
On our 1.8 GHz machine a single call to 2-GB Argon2d runs in 0.5 seconds, but the Merkle tree computation is more expensive, as we have to hash 2 GB of data splitted into 1 KB blocks.
We suggest using Blake2b for H, as it is already used in Argon2d, but restrict to 128-bit output, so that the total running time is about 3 seconds.
In this case a single opening has 16 · 21 bytes of hashes, or 1.3 KB in total.
We suggest L = 70, so that the entire proof consists of 140 blocks and their openings, or 180 KB in total.
Let us figure out the cheating advantage.
The C() and D() functions are given in Table 1).
Assuming certain ratio between the area needed to implement Blake2b and the area needed for DRAM, we get the following lower bound on the ASIC-equipped cheater.Proposition 1 For L = 70 and 2 GB of RAM the timearea product can be reduced by the factor of 12 at most, assuming that each Blake2b core occupies an equivalent of 2 16 bytes.Proof.
Assuming that each core occupies 2 16 bytes, we obtain β = 2 −15 in terms of Equation (1).
Since the cheater has the success chance γ = (1 − ε) L , Equation (1) is modified as follows:AT α = AT 1 αD(α + ε) +C(α + ε)/2 15 (1 − ε) L .
(4)Consider three options:• α, ε < 1/12.
Then C(α + ε) ≥ 4660 (Table 1) and we haveAT α ≥ AT 1 · 4660 32768 ≥ AT 1 · 0.12.
•α < 1/12, 1/6 ≥ ε > 1/12.
Then C(α + ε) ≥ 20, (1 − ε) L > 1/441, and we haveAT α ≥ AT 1 · 20 · 441 32768 ≥ AT 1 · 0.27.
• α < 1/12, 1/6 ≤ ε.
Then (1 − ε) L > 2 15 , and the time-area product increases.
• α > 1/12.
Then AT α ≥ AT 1 · 1/12.
This ends the proof.
We conclude that a cheater can gain at most 12x-advantage, whereas he can still be detected in the future by memory-rich verifiers.
Tradeoffs are also not helpful when implementing this Proof-of-Work on ASIC.
Altogether, our proposal should reduce the relative efficiency of potential ASIC mining rigs and allow more egalitarian mining process.
Even if someone decides to use large botnets (10,000 machines and more), all the botnets machines would have to use the same 2 GB of memory, otherwise they would suffer large penalty.
We note that if ε = 0, i.e. the prover is honest, then his maximal advantage is max 1 αD(α) ≤ 2.
The paradigm of inherently sequential computation was developed by [12] in the application to CPU benchmarking and [29] for timestamping, i.e. to certify that the document was generated certain amount of time in the past.
Rivest et al. suggested time-lock puzzles for this purpose.In our context, a time-lock puzzle solution is a proof-ofwork that has lower bound on the running time assuming unlimited parallelism.The verifier in [20,29] selects a prime product N = pq and asks the prover to compute the exponent 2 2 D (mod N) fpr some D ≈ N.
It is conjectured that the prover who does not know the factors can not exponentiate faster than do D consecutive squarings.
In turn, the verifier can verify the solution by computing the exponent 2 D modulo φ (N), which takes log(D) time.
So far the conjecture has not been refuted, but the scheme inherently requires a secret held by the verifier, and thus is not suitable for proofs-of-work without secrets, as in cryptocurrencies.Time-lock puzzles without secrets were suggested by Mahmoody et al. [22].
Their construction is a graph of hash computations, which is based on depth-robust graphs similarly to [16].
The puzzle is a deterministic graph such that removing any constant fraction of nodes keeps its depth above the constant fraction of the original one (so the parallel computation time is lower bounded).
A Merkle tree is put atop of it with its root determining a small number of nodes to open.
Therefore, a cheater who wants to compute the graph in less time has to subvert too many nodes and is likely to be caught.
As [16], the construction by Mahmoody et al., if combined with the difficulty filter, is subject to the grinding attack described above.The MTP-Argon2 construction can be viewed as a time-lock puzzle and an improvement over these schemes.
First, the difficulty filter is explicitly based on the grinding attack, which makes it a legitimate way to solve the puzzle.
Secondly, it is much faster due to high speed of Argon2d.
The time-lock property comes from the fact that the computation chain can not be parallelized as the graph structure is not known before the computation.Suppose that MTP-Argon2 is parallelized by the additional factor of R so that each core computes a chain of length about T /R. Let core j compute j-th (out of R) chain, chronologically.
Then bu step i each core has computed i blocks and has not computed T /R − i blocks, so the probability that core j requests a block that has not been computed is( j − 1)(T /R − i) ( j − 1)T /R + i ≤ ( j − 1)(T /R − i) jT /R .
Summing by all i, we obtain that core j misses at leastT (1−1/ j) 2R, so the total fraction of inconsistent blocks is about 0.5 − ln R 2R .
Therefore, ε quickly approaches 0.5, which is easily detectable.
We thus conclude that a parallel implementation of MTP-Argon2 is likely to fail the Merkle tree verification.5 Memory-hard encryption on lowentropy keys In this section we approach standard encryption from the memory-hardness perspective.
A typical approach to hard-drive encryption is to derive the master key from the user password and then use it to encrypt chunks of data in a certain mode of operation such as XTS [24].
The major threat, as to other password-based security schemes, are low-entropy passwords.
An attacker, who gets access to the hard drive encrypted with such password, can determine the correct key and then decrypt within short time.A countermeasure could be to use a memory-hard function for the key derivation, so that the trial keys can be produced only on memory-rich machines.
However, the trial decryption could still be performed on special memoryless hardware given these keys.
We suggest a more robust scheme which covers this type of adversaries and eventually requires that the entire attack code have permanent access to large memory.
We assume the following setting, which is inspired by typical disk-encryption applications.
The data consists of multiple chunks Q ∈ Q, which can be encrypted and decrypted independently.
The only secret that is available to the encryption scheme E is the user-input password P ∈ P, which has sufficiently low entropy to be memorized (e.g., 6 lowercase symbols).
The encryption syntax is then as follows:E : P × S × Q → C ,where S ∈ S is associated data, which may contain salt, encryption nonce or IV, chunk identifier, time, and other secondary input; and C ∈ C is ciphertext.
S serves both to simplify ciphertext identification (as it is public) and to ensure certain cryptographic properties.
For instance, unique salt or nonce prevents repetition of ciphertexts for identical plaintexts.
We note that in some settings due to storage restriction the latter requirement can be dropped.
Decryption then is naturally defined and we omit its formal syntax.In our proposal we do not restrict the chunk size.
Even though it can be defined for chunks as small as disk sectors, the resistance to cracking attacks will be higher for larger chunks, up to a megabyte long.A typical attack setting is as follows.
An attacker obtains the encrypted data via some malicious channel or installs malware and then tries different passwords to decrypt it.
For the sake of simplicity, we assume that the plaintext contains sufficient redundancy so that a successful guess can be identified easily.
Therefore, the adversary tries D passwords from his dictionary D ⊂ P. Let T be the time needed for the fastest decryption operation that provides partial knowledge of plaintext sufficient to discard or remember the password, and A 0 be the chip area needed to implement this operation.
Then the total amount of work performed by the adversary isW = D · T · A 0 .
At the same time, the time to encrypt T 񮽙 for a typical user should not be far larger than T .
Our goal is to maximize W with keeping T 񮽙 the same or smaller.The memory-hard functions seem to serve perfectly for the purpose of maximizing W .
However, it remains unclear how to combine such function F with E to get memory-hard encryption (MHE).
Now we formulate some additional features that should be desirable for such a scheme:• The user should be able to choose the requested memory size A independently of the chunk length |Q|.
Whereas the chunk length can be primarily determined by the CPU cache size, desirable processing speed, or the hard drive properties, the memory size determines the scheme's resistance to cracking attacks.
• The memory can be allocated independently for each chunk or reused.
In the former case the user can not allocate too much memory as the massive decryption would be too expensive.
However, for the amounts of memory comparable to the chunk size the memory-hard decryption should take roughly as much as memoryless decryption.
If the allocated memory is reused for distinct chunks, much more memory can be allocated as the allocation time can be amortized.
However, the decryption latency would be quite high.
We present both options in the further text.
• Full ciphertext must be processed to decrypt a single byte.
This property clearly makes T larger since the adversary would have to process an entire chunk to check the password.
At the same time, for disk encryption it should be fine to decrypt in the "all-ornothing" fashion, as the decryption time would still be smaller than the user could wait.
• Encryption should be done in one pass over data.
It might sound desirable that the decryption should be done in one pass too.
However, this would contradict the previous requirement.
Indeed, if the decryption can be done in one pass, then the first bytes of the plaintext can be determined without the last bytes of the ciphertext 3 .
• Apart from the memory parameter, the total time needed to allocate this memory should be tunable too.
It might happen that the application does not have sufficient memory but does have time.
In this case, the adversary can be slowed down by making several passes over the memory during its initialization (the memory-hard function that we consider support this feature).
Our next and final requirement comes from adversary's side.
When the malware is used, the incoming network connection and memory for this malware can be limited.
Thus, it would be ideal for the attacker if the memoryintensive part can be delegated to large machines under attacker's control, such as botnets.
If we just derived the secret-key K for encryption as the output of the memoryhard hash function F , this would be exactly this case.
An adversary would then run F for dictionary D on his own machine, produce the set K of keys, and supply them to malware (recall that due to low entropy there would be only a handful of these keys).
Thus the final requirement should be the following:• During decryption, it should be impossible to delegate the entire memory-hard computation to the external device without accessing the ciphertext.
Therefore, there could be no memory-hard precomputation.
Our scheme is based on a recent proposal by Zaverucha [35], who addresses similar properties in the scheme based on Rivest's All-or-Nothing transform (ANT).
However, the scheme in [35] does not use an external memory-hard function, which makes it memory requirements inevitably bound to the chunk size.
Small chunks but large memory is impossible in [35].
Our proposal is again based on the All-or-Nothing transformation, though we expect that similar properties can be obtained with deterministic authenticated encryption scheme as a core primitive.
The chunk length q (measured in blocks using by F ) and memory size M ≥ q are the parameters as well as some blockcipher E (possibly AES).
First, we outline the scheme where the memory is allocated separately for each chunk.
The reader may also refer to Figure 2.
The underlying idea is to use both the header and the body blocks to produce the ciphertext.
In tun, to recompute the body blocks both the ciphertext and the header must be available during trial decryption.
The version of the MHE scheme which allocates the same memory for multiple chunks is very similar.
The S input is ignored at the beginning, so that the header memory blocks do not depend on the data.
Instead, we set K 0 = H(X 0 , S), so that the body blocks are affected by S and M, and thus are different for every chunk.
In this case the body blocks have to be stored separately and should not overwrite the header blocks for t > 1.
Let us verify that the scheme in Algorithm 3 satisfies the properties we listed earlier:• The allocated memory size M can be chosen independently of the chunk length q (as long as M > q).
• The body memory blocks are allocated and processed for each chunk independently.
In addition, the header blocks are also processed independently for each chunk in the single-chunk version.
• In order to decrypt a single byte of the ciphertext, an adversary would have to obtain K 1 , which can be done only by running F up to the final block, which requires all C 񮽙񮽙 i , which are in turn must be derived from the ciphertext blocks.
• Encryption needs one pass over data, and decryption needs two passes over data.
• The total time needed to allocate and fill the header is tunable.
• The computation of the body memory blocks during decryption can not be delegated, as it requires knowledge both of the header and the ciphertext.
It in [18].
might be possible to generate the header on an external machine, but then random access to its blocks to decrypt the ciphertext is required.We note that properties 1, 5, and 6 are not present in [35].
Security First, we address traditional CPA security.
We do not outline the full proof here, just the basic steps.
We assume that the adversary does not have access to the internals of Argon2, and that blockcipher E is a secure PRF.
Next, we assume collision-resistance of the compression function F used in F .
Given that, we prove that all the memory blocks are distinct, which yields the CPA security for C 񮽙 .
From the latter we deduce the CPA security for the final ciphertext.
We note that in the case when the collision-resistance of F can not be guaranteed, we may additionally require that X i undergo hashing by a cryptographic hash function H 񮽙 before encryption, so that the plaintext blocks are still distinct.
All these properties hold up to the birthday bound of the blockcipher.
Next, we figure out the tradeoff security.
The genuine decrypting user is supposed to spend M memory blocks for F and q memory blocks to store the plaintext and intermediate variables (if the ciphertext can be overwritten, then these q blocks are not needed).
Suppose that an adversary wants to use αM memory for header and body.
Then each missing block, if asked during decryption, must be recomputed making C(α) calls to F.
The best such strategy for Argon2, described in [9], yields C(α) that grows exponentially in 1/α.
For example, using 1/5 of memory, an adversary would have to make 344 times as many calls to F, which makes a memoryreducing encryption cracking inefficient even on special hardware.Algorithm 3 Memory-hard encryption with independent memory allocation (for each chunk).
Input: Password P, memory size M, associated data S, chunk Q, number of iterations t, memory-hard function F (preferably Argon2), blockcipher E, cryptographic hash function H (e.g. SHA-3).1.
Run F on (P, S) with input parameters M and t but fill only M − q blocks (the header) in the last iteration.
Let X 0 be the last memory block produced by F .2.
Produce K 0 = H(X 0 ) -the first session key.3.
Generate a random session key K 1 .4.
Generate the remaining blocks X 1 , X 2 , .
.
.
, X q (body) for F as follows.
We assume that each chunk M consists of smaller blocks m 1 , m 2 , .
.
.
, m q of length equal to the block size of F .
For each i ≥ 1:• Encrypt X i−1 by E in the ECB mode under K 1 and get the intermediate ciphertext block C 񮽙 i .
• Add the chunk data: C 񮽙񮽙 i = C 񮽙 i ⊕ m i .
• Encrypt C 񮽙񮽙 i under K 0 in the CBC mode and produce the final ciphertext block C i .
• Modify the memory: X i−1 ← X i−1 ⊕C 񮽙񮽙 i .
• Generate the block X i according to the specification of F .
In Argon2, the modified X i−1 and some another block X[φ (X i−1 )] would be used.5.
After the entire chunk is encrypted, encrypt also the key K 1 :C t+1 = E K 0 (H(X t ) ⊕ K 1 ).
Output: C 1 ,.
.
.
,C t+1 .
Performance We suggest taking l = 4 in Argon2 in order to fill the header faster using multiple cores, which reportedly takes 0.7 cpb (about the speed of AES-GCM and AES-XTS).
The body has to be filled sequentially as the encryption process is sequential.
As AES-CBC is about 1.3 cpb, and we use two of it, the body phase should run at about 4 cpb.
In a concrete setting, suppose that we tolerate 0.1 second decryption time (about 300 Mcycles) for the 1-MB chunk.
Then we can take the header as large as 256 MB, as it would be processed in 170 Mcycles + 4 Mcycles for the body phase.
We have introduced the new paradigm of egalitarian computing, which suggests amalgamating arbitrary computation with a memory-hard function to enhance the security against off-line adversaries equipped with powerful tools (in particular with optimized hardware).
We have reviewed password hashing and proofs of work as applications where such schemes are already in use or are planned to be used.
We then introduce two more schemes in this framework.
The first one is MTP, the progress-free proof-of-work scheme with fast verification based on the memory-hard function Argon2, the winner of the Password Hashing Competition.
The second scheme pioneers the memory-hard encryption -the security enhancement for password-based disk encryption, also based on Argon2.
so that the verifier can make all the computations.
If G is collision-resistant, it is hard to open any block in more than one possible way.
