Modern computing systems generate large amounts of log data.
System administrators or domain experts utilize the log data to understand and optimize system behaviors.
Most system logs are raw textual and unstructured.
One main fundamental challenge in automated log analysis is the generation of system events from raw textual logs.
Log messages are relatively short text messages but may have a large vocabulary , which often result in poor performance when applying traditional text clustering techniques to the log data.
Other related methods have various limitations and only work well for some particular system logs.
In this paper, we propose a message signature based algorithm logSig to generate system events from textual log messages.
By searching the most representative message signatures, logSig categorizes log messages into a set of event types.
logSig can handle various types of log data, and is able to incorporate human's domain knowledge to achieve a high performance.
We conduct experiments on five real system log data.
Experiments show that logSig outperforms other alternative algorithms in terms of the overall performance.
Each log message consists of a sequence of terms.
Some of the terms are variables or parameters for a system event, such as the host name, the user name, IP address and so on.
Other terms are plain text words describing semantic information of the event.
For example, three sample log messages of the Hadoop system [3] describing one type of events about the IPC (Inter-Process Communication) subsystem are listed below:The three messages contain many different words(or terms), such as the date, the hours, the handler name, and the port number.
People can identify them as the same event type because they share a common subsequence: "INFO: org.apache.hadoop.ipc.Server: IPC Server:starting ".
Let's consider how the three log messages are generated by the system.
The Java source code for generating them is described below: logger = Logger.getLogger ("org.apache.hadoop.ipc.Server"); logger.info("IPC Server "+handlerName+": starting");where logger is the log producer for the IPC subsystem.
Using different parameters, such as handlerName, the code can output different log messages.
But the subsequence "INFO: org.apache.hadoop.ipc.Server: IPC Server : starting " is fixed in the source code.
It will never change unless the source code has been modified.Therefore, the fixed subsequence can be viewed as a signature for an event type.
In other words, we can check the signatures to identify the event type of a log message.
Other parameter terms in the log message should be ignored, since messages of the same event type can have different parameter terms.
Note that some parameters, such as the handlerName in this example, consist of different numbers of terms.
Consequently, the position of a message signature may vary in different log messages.
Hence, the string matching similarity proposed in [6] would mismatch some terms.
Another method IPLoM proposed in [18] also fails to partition log messages using the term count since the length of handlerName is not fixed and three log messages have different numbers of terms.Given an arbitrary log message, we do not know in advance which item is of its signature, or which term is its parameter.
That is the key challenge we aim to address in this paper.
In this paper, we first describe the drawbacks of traditional text clustering techniques for event generation from log messages.
We show that, it is difficult for the bag-of-word model to accurately partition log messages.
We also analyze that, the string kernel based approach would be inefficient when the log vocabulary size is large.
In addition, we discuss some limitations of related approaches proposed in previous literatures.Then, we propose logSig algorithm to generate system events from textual log messages.
logSig algorithm tries to find k message signatures to match all given messages as much as possible, where k is specified by the user.
We conduct experiments on five real system log data.
Experiments show that logSig outperforms other alternative algorithms in terms of the overall performance.The rest of the paper is organized as follows: Section 2 describes the related work about system event generation from textual logs.
Then, we formulate the problem of the system events generation in Section 3.
In Section 4, we present an overview of the logSig algorithm.
Section 5 discusses some detailed implementation issues.
Section 6 proposes two approaches for incorporating the domain knowledge to improve the accuracy of the logSig algorithm.
In Section 7, we present the experimental studies on five real system log data.
Finally, Section 8 concludes our paper and discusses the future work.
It has been shown in [24] that log messages are relatively short text messages but could have a large vocabulary size.
This characteristic often leads to a poor performance when using the bag-of-words model in text mining on log data.
The reason is that, each single log message has only a few terms, but the vocabulary size is very large.
Hence, the vector space established on sets of terms would be very sparse.
The string kernel can be used to extract deep semantic information (e.g., the order of terms) to improve the performance of the clustering algorithm.
It maps a string to a high dimensional vector to represent all possible term orders.
However, because of the large vocabulary size, the dimensionality of the transformed space would be very high.
Although the kernel trick does not have to explicitly create those high dimensional vectors, clustering algorithms would still be influenced by the high dimensionality due to the Curse of Dimensionality [25].
The related work about the log data analysis can be broadly summarized into two categories.
One category is on system event generation from raw log data [6] [9] [18] [26] and the other category is on analyzing patterns from system events [22] [30] [13] [15] [10] [29] [14].
Our work in this paper belongs to the first category.
A word matching similarity measurement is introduced in [6] for clustering the log messages.
One problem is that, if most terms of a log message are parameter terms, then this type of log messages may not have many matched common words.
This method is denoted as StringMatch in this paper.
[18] develops a 4-steps partitioning method IPLoM for clustering the log messages based on inherent characteristics of the log format.
However, the method can only be useful for strictly formatted log data.
The logSig algorithm proposed in this paper can handle various types of log data without much prior knowledge about the log format.
The goal of this paper is to identify the event type of each log message according to a set of message signatures.
Given a log message and a set of signatures, we need a metric to determine which signature best matches this log message.
Therefore, we propose the Match Score metric first.
Notations: Let D be a set of log messages, D = {X1, ..., XN }, where Xi is the ith log message, i = 1, 2, ..., N .
Each Xi is a sequence of terms, i.e., Xi = wi 1 wi 2 ....wi n i .
A message signature S is also a sequence of terms S = wj 1 wj 2 ....wj n .
Given a sequence X = w1w2...wn and a term wi, wi ∈ X indicates wi is a term in X. X −{wi} denotes a subsequence w1...wi−1wi+1...wn.
|X| denotes the length of the sequence X. LCS(X, S) denotes the Longest Common Subsequence between two sequences X and S. Definition 1.
(Match Score) Given a log message Xi and a message signature S, the match score is computed by the function below:match(Xi, S) = |LCS(Xi, S)| − (|S| − |LCS(Xi, S)|) = 2|LCS(Xi, S)| − |S|.
Intuitively, |LCS(Xi, S)| is the number of terms in Xi matched with S. |S| − |LCS(Xi, S)| is the number of terms in Xi not matched with S. match(Xi, S) is the number of matched terms minus the number of not-matched terms.
We illustrate this by a simple example below: Example 1.
A log messages X = abcdef and a message signature S = axcey.
The longest common subsequence LCS(X, S) = ace.
The matched terms are "a","c","e", shown by underline words in Table 1.
"x" and "y" in S are not matched with any term in X. Hence, match(X, S) = |ace|− |xy| = 1.
Note that this score can be negative.
match(Xi, S) is used to measure the degree of the log message Xi owning the signature S.
If two log messages Xi and Xj have the same signature S, then we regard Xi and Xj as of the same event type.
The longest common subsequence matching is a widely used similarity metric in biological data analysis [7] [19], such as RNA sequences.
If all message signatures S1, S2,...,S k are known, identifying the event type of each log message in D is straightforward.
But we don't know any message signature at the beginning.
Therefore, we should partition log messages and find their message signatures simultaneously.
The optimal result is that, within each partition, every log message matches its signature as much as possible.
This problem is formulated below.Problem 1.
Given a set of log messages D and an integer k, find k message signatures S = {S1, ..., S k } and a k-partition C1,...,C k of D to maximizeJ(S, D) = k 񮽙 i=1 񮽙 X j ∈C i match(Xj, Si).
The objective function J(S, D) is the summation of all match scores.
It is similar to the k-means clustering problem.
The choice of k depends on the user's domain knowledge to the system logs.
If there is no domain knowledge, we can borrow the idea from the method finding k for k-means [11], which plots clustering results with k.
We can also display generated message signatures for k = 2, 3, .
.
until the results can be approved by experts.Comparing with k-means clustering problem Problem 1 is similar to the classic k-means clustering problem, since a message signature can be regarded as the representative of a cluster.
People may ask the following questions: Why we propose the match function to find the optimal partition?
Why not use the LCS as the similarity function to do k-means clustering?
The answer for the two questions is that, our goal is not to find good clusters of log messages, but to find the message signatures of all types of log messages.
K-means can ensure every two messages in one cluster share a subsequence.
However, it cannot guarantee that there exists a common subsequence shared by all (or most) messages in one cluster.
We illustrate this by the following example.Example 2.
There are three log messages X1: "abcdef, X2: "abghij" and X3: "xyghef".
Clearly, LCS(X1, X2)=2, LCS(X2, X3)=2, and LCS(X1, X3)=2.
However, there is no common subsequence that exists among all X1, X2 and X3.
In our case, it means there is no message signature to describe all three log messages.
Hence, it is hard to believe that they are generated by the same log message template.
Problem 1 is an NP-hard problem, even if k = 1.
When k = 1, we can reduce the Multiple Longest Common Subsequence problem to the Problem 1.
The Multiple Longest Common Subsequence problem is a known NP-hard [17].
Lemma 1.
Problem 1 is an NP-hard problem when k = 1.
Proof: Let D = {X1, ..., XN }.
When k = 1, S = {S1}.
Construct another set of N sequences Y = {Y1, ..., YN }, in which each term is unique in both D and Y. Let D 񮽙 = D∪Y, J(S, D 񮽙 ) = 񮽙 X j ∈D match(Xj, S1) + 񮽙 Y l ∈Y match(Y l , S1)Let S * 1 be the optimal message signature for D 񮽙 , i.e.,S * 1 = arg max S 1 J({S1}, D 񮽙 ).
Then, the longest common subsequence of X1,...,XN must be an optimal solution S * 1 .
This can be proved by contradiction as follows.
Let S lcs be the longest common subsequence of X1,...,XN .
Note that S lcs may be an empty sequence if there is no common subsequence among all messages.
Case 1: If there exists a term wi ∈ S * 1 , but wi / ∈ S lcs .
Since wi / ∈ S lcs , wi is not matched with at least one message in X1,...,XN .
Moreover, Y1,...,YN are composed by unique terms, so wi cannot be matched with any of them.
In D 񮽙 , the number of messages not matching wi is at least N + 1, which is greater than the number of messages matching wi.
Therefore,J({S * 1 − {wi}}, D 񮽙 ) > J({S * 1 }, D 񮽙 ),which contradicts with S * 1 = arg maxS 1 J({S1}, D 񮽙 ).
Case 2: If there exists a term wi ∈ S lcs , but wi / ∈ S * 1 .
Since wi ∈ S lcs , X1,...,XN all match wi.
The total number of messages that match wi in D 񮽙 is N .
Then, there are N remaining messages not matching wi: Y1,...,YN .
Therefore,J({S lcs }, D 񮽙 ) = J({S * 1 }, D 񮽙 ),which indicates S lcs is also an optimal solution to maximize objective function J on D 񮽙 .
To sum up the two cases above, if there is a polynomial time-complexity solution to find the optimal solution S * 1 in D 񮽙 , the Multiple Longest Common Subsequence problem for X1,...,XN can be solved in polynomial time as well.
However, Multiple Longest Common Subsequence problem is an NP-hard problem [17].
Lemma 2.
If when k = n Problem 1 is NP-hard, then when k = n + 1 Problem 1 is NP-hard, where n is a positive integer.
Proof-Sketch: This can be proved by contradiction.
We can construct a message Y whose term set has no overlap to the term set of messages in D in a linear time.
Suppose the optimal solution for k = n and D is C = {C1, ..., C k }, then the optimal solution for k = n + 1 and D ∪ {Y } should be C 񮽙 = {C1, ..., C k , {Y }}.
If there is a polynomial time solution for Problem 1 when k = n + 1, we could solve Problem 1 when k = n in polynomial time.
In this section, we first present an approximated version of Problem 1 and then present our logSig algorithm.
logSig algorithm consists of three steps.
The first step is to separate every log message into several pairs of terms.
The second step is to find k groups of log messages using local search strategy such that each group share common pairs as many as possible.
The last step is to construct message signatures based on identified common pairs for each message group.
The first step of logSig algorithm is converting each log message into a set of term pairs.
For example, there is a log message collected from FileZilla [2] client:2010-05-02 11:34:06 Command: mkdir ".
indexes"We extract each pairwise of terms and preserve the order of two terms.
Then, the converted pairs are as follows:{2010-05-02,11:34:06}, {2010-05-02,Command:}, {2010-05-02, mkdir}, {2010-05-02, ".
indexes" } {11:34:06,Command:}, {11:34:06,mkdir}, {11:34:06,".
indexes"}, {Command:,mkdir}, {Command:,".
indexes"}, {mkdir,".
indexes"}The converted term pairs preserve the order information of message terms.
On the other hand, the computation on the discrete term pairs is easier than a sequence.
A similar idea was proposed in string kernel [16] for text classification.
Their output is a high dimensional vector, and our output is a set of pairs.
The second step is to partition log messages into k groups based on converted term pairs.
Notations: Let X be a log message, R(X) denotes the set of term pairs converted from X, and |R(X)| denotes the number of term pairs in R(X).
Problem 2.
Given a set of log messages D and an inte-ger k, find a k-partition C = {C1, ..., C k } of D to maximize objective function F (C, D):F (C, D) = k 񮽙 i=1 | 񮽙 X j ∈C i R(Xj )|.
Object function F (C, D)is the total number of common pairs over all groups.
Intuitively, if a group has more common pairs, it is more likely to have a longer common subsequence.
Then, the match score of that group would be higher.
Therefore, maximizing function F is approximately maximizing J in Problem 1.
Lemma 4 shows the average lower bound for this approximation.Lemma 3.
Given a message group C, it has n common term pairs, then the length of the longest common subsequence of messages in C is at least 񮽙 √ 2n񮽙.
Proof-sketch: Let l be the length of a longest common subsequence of messages in C. Let T (l) be the number of term pairs that generated by that longest common subsequence.Since each term pair has two terms, this sequence can generate at most񮽙 l 2 񮽙 pairs.
Hence, T (l) ≤ 񮽙 l 2 񮽙 = l(l − 1)/2.
Note that each term pair of the longest common subsequence is a common term pair in C. Now, we already know T (l) = n, so T (l) = n ≤ l(l − 1)/2.
Then, we have l ≥ ≥ √ 2n񮽙.
Lemma 4.
Given a set of log messages D and a k-partitionC = {C1, ..., C k } of D, if F (C, D) ≥ y, y is a constant, wecan find a set of message signatures S such that on average: J(S, D) ≥ |D| · 񮽙 2y k 񮽙 Proof-sketch: Since F (C, D) ≥ y, The logSig algorithm applies the local search strategy to solve Problem 2.
It iteratively moves one message to another message group to increase the objective function as large as possible.
However, unlike the classic local search optimization method, the movement is not explicitly determined by objective function F (·).
The reason is that, the value of F (·) may only be updated after a bunch of movements, not just after every single movement.
We illustrate this by the following example.Example 3.
Message set D is composed of 100 "ab" and 100 "cd".
Now we have 2-partition C = {C1, C2}.
Each message group has 50% of each message type as shown in Table 2.
The optimal 2-partition is C1 has 100 "ab" and C2 X X X X X X X X X term pair group C1 C2"ab" 50 50 "cd" 50 50has 100 "cd", or in the reverse way.
However, beginning with current C1 and C2, F (C, D) is always 0 until we move 50 "ab" from C2 to C1, or move 50 "cd" from C1 to C2.
Hence, for first 50 movements, F (C, D) cannot guide the local search because no matter what movement you choose, it is always 0.
Therefore, F (·) is not proper to guide the movement in the local search.
The decision of every movement should consider the potential value of the objective function, rather than the immediate value.
So we develop the potential function to guide the local search instead.
Notations: Given a message group C, R(C) denotes the union set of term pairs from messages of C. For a term pair r ∈ R(C), N (r, C) denotes the number of messages in C which contains r. p(r, C) = N (r, C)/|C| is the portion of messages in C having r .
Definition 2.
Given a message group C, the potential of C is defined as φ(C),φ(C) = 񮽙 r∈R(C) N (r, C)[p(r, C)] 2 .
The potential value indicates the overall "purity" of term pairs in C. φ(C) is maximized when every term pair is contained by every message in the group.
In that case, for each r, N (r, C) = |C|, φ(C) = |C| · |R(C)|.
It also means all term pairs are common pairs shared by every log message.
φ(C) is minimized when each term pair in R(C) is only contained by one message in C.
In that case, for each r,N (r, C) = 1, |R(C)| = |C|, φ(C) = 1/|C|.
Definition 3.
Given a k-partition C = {C1, ..., C k } of a message set D, the overall potential of D is defined as Φ(D), Φ(D) = k 񮽙 i=1 φ(Ci),where φ(Ci) is the potential of Ci, i = 1, ..., k. Objective function F computes the total number of common term pairs in each group.
Both Φ and F are maximized when each term pair is a common term in its corresponding message group.
Let's consider the average case.Lemma 5.
Given a set of log messages D and a k-partition Lemma 5 implies, in the average case, if we try to increase the value of F to be at least y, we have to increase the overall potential Φ to be at least y · |D|/k.
As for the local search algorithm, we mentioned that Φ is easier to optimize thanC = {C1, ..., C k } of D, if F (C, D) ≥ y,F .
Let ΔiX − → j Φ(D) denote the increase of Φ(D) by moving X ∈ D from group Ci into group Cj , i, j = 1, ..., k, i 񮽙 = j. Then, by Definition 3, ΔiX − → j Φ(D) = [φ(Cj ∪ {X}) − φ(Cj)] −[φ(Ci) − φ(Ci − {X})],where φ(Cj ∪ {X}) − φ(Cj) is the potential increase brought by inserting X to Cj , φ(Ci)−φ(Ci−{X}) is the potential loss brought by removing X from Ci.
Algorithm 1 is the pseudocode of the local search algorithm in logSig.
Basically, it iteratively updates every log message's group according to ΔiX − → j Φ(D) to increase Φ(D) until no more update operation can be done.
Parameter: D : log messages set; k: the number of groups to partition; Result: C : log message partition;1: C ← RandomSeeds(k) 2: C 񮽙 ← ∅ // Last iteration's partition 3: Create a map G to store message's group index 4: for C i ∈ C do 5:for X j ∈ C i do 6: G[X j ] ← i 7:end for 8: end for 9: while C 񮽙 = C 񮽙 do 10:C 񮽙 ← C 11: for X j ∈ D do 12: i ← G[X j ]13:j * = arg max j=1,.
.
,k Δ iX − → j Φ(D) if i 񮽙 = j * then 15:C i ← C i − {X j } 16: C j * ← C j * ∪ {X j } 17: G[X j ] ← j * 18: end if 19: end for 20: end while 21: return C Why choose this Potential Function?
Given a message group C, let g(r) = N (r, C)[p(r, C)] 2 , then φ(C) = 񮽙 r∈R(C) g(r).
Since we have to consider all term pairs in C, we define φ(C) as the sum of all g(r).
As for g(r), it should be a convex function.
Figure 1 shows a curve of g(r) by varying the number of messages having r, i.e., N (r, C).
The reason for why g(r) is convex is that, we hope , |C| = 100 to give larger awards to r when r is about to being a common term pair.
That is because, if N (r, C) is large, then r is more likely to be a common term pair.
Only when r becomes a common term pair, it can increase F (·).
In other words, r has more potential to increase the value of objective function F (·), so the algorithm should pay more attention to r first.In the experimental section, we will empirically compare the effectiveness of our proposed potential function Φ with the objective function function F in the local search.
The final step of logSig algorithm is to construct the message signature for each message group.
Recall that a message signature is a sequence of terms that has a high match score to every message in the corresponding group.
So it could be constructed by highly frequent term pairs identified in the second step.Lemma 6.
Let S be an optimal message signature for a message group C, the occurrence number of every term wj ∈ S must be equal or greater than 񮽙|C|/2񮽙.
The proof of Lemma 6 is similar to the proof of Lemma 1.
If there exists a term w 񮽙 j ∈ S only appearing in less than 񮽙|C|/2񮽙 messages, we can have: J({S − {w 񮽙 j }}, C) > J({S}, C), then S is worse than S − {w 񮽙 j }.
Thus, S is not optimal.Lemma 6 indicates that we only need to care about those terms which appear in at least one half of the messages in a group.
By scanning every message in a group, we could obtain the optimal sequence of those terms.
Since log messages are usually very short, there are only a few term whose occurrence number is equal or greater than 񮽙|C|/2񮽙.
Therefore, there are not many candidate sequences generated by those terms.
We enumerate each one of them and select the best one to be the message signature.
In this section, we discuss some detailed issues about the implementation of the logSig algorithm.Term Pair Histogram To efficiently compute the potential value of each message group Ci ∈ C, a histogram is maintained for each group.
The histogram is implemented by a hash table, whose key is a term pair and the value is the number of messages containing that term pair in the group.
Then, for a term pair r, c(r, Ci) and p(r, Ci) can be obtained in a constant time complexity.Approximately Computing ΔΦ(D) The straightforward way to compute ΔiX − → j Φ(D) is O( 񮽙 k l=1 |R(C l )|), which is the total number of term pairs in the entire data set.
In log data, this number is even tens of times greater than |D|.
Hence, this computation cost is not affordable for each single log message.Our approximated solution is to consider the change of N (r, C), for each r ∈ R(X).
The reason is that, |C| is large.
The impact to |C| by inserting or removing one message could be ignored comparing to the impact to N (r, C).
|C)| ≈ |C| + 1 ≈ |C| − 1.
So |C| can be treated as a constant in one exchange operation in local search.
Then,∂φ(C) ∂N(r, C) = ∂[[N (r, C)] 3 /|C| 2 ] ∂N(r, C) = 3[N (r, C)] 2 |C| 2 = 3[p(r, C)] 2 .
ΔiX − → j Φ(D) ≈ 3 · 񮽙 r∈R(X) [[p(r, Cj)] 2 − [p(r, Ci)] 2 ].
By utilizing the histograms, p(r, Cj) and p(r, Ci) can be obtained in a constant time.
Hence, for each log message X ∈ D, the time complexity of finding the largest ΔiX− → j Φ(D) is at most O(k · |R(X)|).
Although ΔiX − → j Φ(D)is would underestimate the actual change of Φ(D), it can save a lot of computation cost.
In the local search algorithm, exchanging messages' groups only increases the overall potential Φ(D).
There is no operation to decrease the overall potential.
Φ(D) is not infinite, which is less than or equal to |R(D)| · |D|.
Therefore, the local search algorithm would converge.Similar to other local search based optimization algorithms, logSig may converge into a local optima as well.
However, logSig's potential function provides a more reliable heuristic to guide the optimization process.
It is much less likely to stop at a local optima.Time Complexity and Space Complexity The time complexity of converting log messages into term pairs is O(|D| · L 2 ), where L is the maximum length of log messages.
For every Xj ∈ D, L 2 ≥ |R(Xj )|.
For the local search, each iteration goes through every message.
So the time complexity of an iteration is O(k · L 2 · |D|).
Let t be the number of iterations, so the time complexity of the local search is O(t · k · L 2 · |D|).
Our experiments shows that t is usually 3 to 12 for most system log data.The space cost of the algorithm is mainly determined by the term pair histograms created for every message group.
The total space complexity is the total number of term pairs, O(|R(D)|).
In real world applications, analyzing system behaviors mainly relies on the generated system events, so the event generation algorithm should be reliable.
Current approaches for grouping log messages into different event types are totally unsupervised.
In practice, for most sorts of logs (e.g., Hadoop system logs), there are some domain knowledge about a fixed catalog of Java exceptions.
In addition, the log generation mechanisms implicitly create some associations between the terminologies and the situations.
Incorporating domain knowledge into the clustering process could improve the performance of event generation.
In this section, we present two approaches for incorporating domain knowledge to improve the accuracy of logSig algorithm.
Some terms or words in log messages share some common features which can help our algorithm identify the log message type.
The features are domain knowledge from human experts.
For example, "2011-02-01" and "2011-01-02" are different terms, but they are both dates; "192.168.0.12" and "202.119.23.10" are different terms, but they are both IP addresses.logSig algorithm allows users to provide a list of features.
Each feature is described by a regular expression.
An additional feature layer is built to incorporate those features for representing log messages.
The feature layer is created by regular expression matching.
For example, we have following regular expressions 1 :Timestamp : \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} FilePath : "(\w|\\)+"Then, we could scan the log message X1 to create the feature layer Y1 as shown in Table 3.
The feature layer Y1 contains In reality, domain experts usually identify the event type of a log message by scanning keywords or phrases.
For example, as for SFTP/FTP logs, they would be sensitive to those phrases: "Command", "No such file", "Error", "Transfer failed" and so on.
Those sensitive phrases should be included in the generated message signature to express system events.
On the other hand, domain experts also know that some trivial phrases should be ignored, such as the message IDs and the timestamps.
Those trivial phrases should not be included in any message signature.
To sum up, those knowledge can be transferred as constraints on message signatures.
Those constraints can help the event generation algorithm to improve its accuracy.
Unlike traditional constraints in semi-supervised clustering, such as Must-Link or CannotLink [28], our constraints are placed in the subsequences (or phrases) of messages, not on the messages themselves.Constraint-based logSig Algorithm To incorporate constraints on message phrases, Problem 1 can be revised as follows:Problem 3.
Given a set of log messages D, a set of sensitive phrases PS and trivial phrases PT , find k message signatures S = {S1, ..., S k } with a k-partition C1,...,C k of D to maximize:J 񮽙 (S, D, PS, PT ) = λ k 񮽙 i=1 񮽙 X j ∈C i match(Xj, Si) + (1 − λ) k 񮽙 i=1 [ 񮽙 Ps∈P S match(Ps, Si) − 񮽙 Ps∈P T match(Pt, Si)],where λ is a user-defined parameter between 0 and 1.
The revised optimization problem can be solved by constraintbased logSig Algorithm.
The basic idea of constraint-based logSig is to increase the weight of term pairs in R(PS) and decrease the weight of term pairs in R(PT ).
Let wr denote the weight of pair r.
The only revised part in logSig algorithm is ΔiX − → j Φ(D).
By multiplying the weight wr, it becomes:ΔiX − → j Φ(D) ≈ 3 񮽙 r∈R(X) [[p(r, Cj)] 2 − [p(r, Ci)] 2 ] · wr, and wr = ⎧ ⎨ ⎩ 1.0, r / ∈ R(PS), r / ∈ R(PT ) λ 񮽙 , r ∈ R(PS), r / ∈ R(PT ) 1/λ 񮽙 , r / ∈ R(PS), r ∈ R(PT ) ,where λ 񮽙 ≥ 1 is a user-defined parameter that can be approximately derived from the original parameter λ.
λ 񮽙 is utilized to increase(and decrease) the importance of term pairs in sensitive phrases(and trivial phrases).
The choice of λ 񮽙 depends on users' confidence in those sensitive phrases and trivial phrases.
In experiments, we let λ 񮽙 = 10.0.
We show the performances of the algorithm when varying λ 񮽙 from 0.0 to 20.0 and explain why we choose 10 in the experimental section.
We implement our algorithm and other comparative algorithms in Java 1.6 platform.
Table 5 summarizes our experimental enviroment.
We collect log data from 5 different real systems, which are summarized in Table 6.
Logs of FileZilla [2], PVFS2 [4] Apache [1] and Hadoop [3] are collected from the server machines/systems in the computer lab of a research center.
Log data of ThunderBird [5] is collected from a supercomputer in Sandia National Lab.
The true categories of log messages are obtained by specialized log parsers.
For instance, FillZilla's log messages are categorized into 4 types: "Command ", "Status", "Response", "Error ".
Apache error log messages are categorized by the error type: "Permission denied ", "File not exist" and so on.
The vocabulary size is an important characteristic of log data.
Figure 2 exhibits the vocabulary sizes of the 5 different logs along with the data size.
It can be seen that some vocabulary size could become very large if the data size is large.
We compare our algorithm with 7 alternative algorithms in this experiment.
Those algorithms are described in Table 7.
6 of them are unsupervised algorithms which only look at the terms of log messages.
3 of them are semi-supervised algorithms which are able to incorporate the domain knowledge.
IPLoM [18] and StringMatch [6] are two methods proposed in recent related literatures .
VectorModel [23], Jaccard [25], StringKernel [16] are traditional methods for text clustering.
VectorModel and semi-StringKernel are implemented by k-means clustering algorithm [25].
Jaccard and StringMatch are implemented by k-medoid algorithm [12], since they cannot compute the centroid point of a cluster.
As for Jaccard, the Jaccard similarity is obtained by a hash table to accelerate the computation.
VectorModel and StringKernel use Sparse Vector [23] to reduce the computation and space costs.
Message signature based method proposed in this paper semi-logSig logSig incorporating domain knowledge semi-StringKernel Weighted string kernel based k-means semi-JaccardWeighted Jaccard similarity based k-medoid semi-logSig, semi-StringKernel and semi-Jaccard are semi-supervised versions of logSig, StringKernel and Jaccard respectively.
To make a fair comparison, all those semisupervised algorithms incorporate the same domain knowledge offered by users.
Specifically, the 3 algorithms run on the same transformed feature layer, and the same sensitive phrases PS and trivial phrases PT .
Obviously, the choices of features, PS and PT have a huge impact to the performances of semi-supervised algorithms.
But we only compare a semisupervised algorithm with other semi-supervised algorithms.
Hence, they are compared under the same choice of features, PS and PT .
The approaches for those 3 algorithms to incorporate with features, PS and PT are described as follows:Feature Layer: Replacing every log message by the transformed sequence of terms with features.PS and PT : As for semi-StringKernel, replacing Euclidean distance by Mahalanobis distance [8]:DM (x, y) = 񮽙 (x − y) T M (x − y).
where matrix M is constructed according to term pairs PS, PT and λ 񮽙 .
As for semi-Jaccard, for each term, multiply a weight λ 񮽙 (or 1/λ 񮽙 ) if this term appears in PS ( or PT ).
Jaccard, StringMatch and semi-Jaccard algorithms apply classic k-medoid algorithm for message clustering.
The time complexity of k-medoid algorithm is very high: O(tn 2 ) [27], where t is the number of iterations, n is the number of log messages.
As a result, those 3 algorithms are not capable of handling large log data.
Therefore, for the accuracy comparison, we split our log files into smaller files by time frame, and conduct the experiments on the small log data.
The amounts of log messages, features, term pairs in PS and PT are summarized in Table 8.
In Section 7.5, larger logs are used to test the scalability.
Table 4 shows the accuracy comparison of generated system events by different algorithms.
The accuracy is evaluated by F-measure (F1 score) [23], which is a traditional metric combining precision and recall.
Since the results of k-medoid, k-means and logSig depend on the initial random seeds, we run each algorithm for 10 times, and put the average F-measures into Table 4.
From this table, it can be seen that StringKernel and logSig outperform other algorithms in terms of the overall performance.
Jaccard and VectorModel apply the bag-of-word model, which ignores the order information about terms.
Log messages are usually short, so the information from the bag-ofword model is very limited.
In addition, different log messages have many identical terms, such as date, username.
That's the reason why the two methods cannot achieve high F-measures.
IPLoM performs well in ThunderBird log data, but poorly in other log data.
The reason is that, the first step of IPLoM is to partition log message by the term count.
One type of log message may have different numbers of terms.
For instance, in FileZilla logs, the length of Command messages depends on the type of SFTP/FTP command in the message.
But for ThunderBird, most event types are strictly associated with one message format.
Therefore, IPLoM could easily achieve the highest score.Due to the Curse of dimensionality [25], k-means based StringKernel is not easy to converge in a high dimensional space.
Figure 2 shows that, 50K ThunderBird log messages contain over 30K distinct terms.
As a result, the transformed space has over (30K) 2 = 900M dimensions.
It is quite sparse for 50K data points.
Generated message signatures are used as descriptors for system events, so that users can understand the meanings of those events.
Due to the space limit, we cannot list all Table 9 shows generated signatures of FileZilla and Apache Error by semi-logSig, in which features are indicated by italic words.
As for FileZilla log, each message signature corresponds to a message category, so that the F-measure of FileZilla could achieve 1.0.
But for Apache Error log, Only 4 message signatures are associated with corresponding categories.
The other 2 signatures are generated by two ill-partitioned message groups.
They cannot be associates with any category of Apache Error logs.
As a result, their "Associated Category" in Table 9 are "N/A".
Therefore, the overall F-measure on Apache error log in Table 4 is only 0.7707.
All those algorithms have the parameter k, which is the number of events to create.
We let k be the actual number of message categories.
String kernel method has an additional parameter λ, which is the decay factor of a pair of terms.
We use StringKernel λ to denote the string kernel method using decay factor λ.
In our experiments, we set up string kernel algorithms using three different decay factors: StringKernel0.8, StringKernel0.5 and StringKernel0.3.
As for the parameter λ 񮽙 of our algorithm logSig, we set λ 񮽙 = 10 based on the experimental result shown by Figure 6.
For each value of λ 񮽙 , we run the algorithm for 10 times, and plot the average F-measure in this figure.
It can be seen that, the performance becomes stable when λ 񮽙 is greater than 4.
To evaluate the effectiveness of the potential function Φ, we compare our proposed logSig algorithm with another logSig algorithm which uses the objective function F to guide its local search.
Figure 7 shows the average Fmeasures of the two algorithms on each data set.
Clearly, our proposed potential function Φ is more effective than F in all data sets.
In addition, we find logSig algorithm using F always converges within 2 or 3 iterations.
In other words, F is more likely to stop at a local optima in the local search.
The reason for this has been discussed in Section 4.2.2.
Scalability is an important factor for log analysis algorithms.
Many high performance computing systems generate more than 1Mbytes log messages per second [21].
Fig- ure 3, Figure 4 and Figure 5 show the average running time comparison for all algorithms on the data sets with different sizes.
We run each algorithm 3 times and plot the average running times.
IPLoM is the fastest algorithm.
The running times of other algorithms depend on the number of iterations.
Clearly, k-medoid based algorithms are not capable of handling large log data.
Moreover, StringKernel is not efficient even though we use Sparse Vector to implement the computation of its kernel functions.
We keep track of its running process, and find out the low speed convergence is mainly due to the high dimensionality.
Figure 8 shows the scalability of logSig algorithm on ThunderBird logs and Apache Error logs.
Its actual running time is approximated linear with the log data size.
In this paper, we show the drawbacks of traditional methods and previous log message analyzing methods.
To address the limitations of existing methods, we propose logSig, a message signature based method for events creation from system log messages.
logSig utilizes the common subsequence information of messages to partition and describe the events generated from log messages.As for the future work, we will integrate the structural learning techniques into our framework to capture the structural information of log messages.
We hope those structures could improve the performance of the logSig algorithm.
The work is supported in part by NSF grants IIS-0546280 and HRD-0833093.
