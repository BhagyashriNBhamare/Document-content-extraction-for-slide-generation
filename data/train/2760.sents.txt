Understanding the GPU utilization of Deep Learning (DL) workloads is important for enhancing resource-efficiency and cost-benefit decision making for DL frameworks in the cloud.
Current approaches to determine DL workload GPU utilization rely on online profiling within isolated GPU devices, and must be performed for every unique DL workload submission resulting in resource under-utilization and reduced service availability.
In this paper, we propose a prediction engine to proactively determine the GPU utilization of heterogeneous DL workloads without the need for in-depth or isolated online profiling.
We demonstrate that it is possible to predict DL workload GPU utilization via extracting information from its model computation graph.
Our experiments show that the prediction engine achieves an RMSLE of 0.154, and can be exploited by DL schedulers to achieve up to 61.5% improvement to GPU cluster utilization.
Deep Learning (DL) has begun to make significant impact across many fields of computing.
Growth in volumes of data, in the complexity of model usage, as well as innovations in DL architectures have all led to increasing practitioner demand.
This in turn has resulted in new ways of satisfying this demand including, for example, exploiting compute accelerators such as Graphic Processing Units (GPUs) for training and inference computation [1][2][3].
Prominent cloud providers now provision GPU 1 cluster resources as part of their IaaS [4][5][6][7].
An important goal for such services is ensuring specified Service Level Agreements (SLA) and Quality of Service (QoS) are met in a resource-efficient manner [8,9].
Efforts to ensure SLA and QoS guarantees for cloudbased DL often result, however, in considerable GPU underutilization [10,11].
This is because orchestration frameworks such as Kubernetes [12] or YARN [13] prohibit explicit GPU sharing for workloads, including those for DL.
Such underutilization decreases resource-efficiency as well as service availability, requiring additional expensive GPU devices to address the demand.Numerous approaches have been proposed to improve GPU resource-efficiency.
These include exploiting workload performance metrics such as execution time [8,10,14], communication [15,16], and GPU utilization patterns [17][18][19].
These information allow providers to make high-quality cluster scheduling decisions to address issues such as interference between co-located workloads, network latency and resource under-utilization.
Since GPU rental can be expensive 2 , it is also beneficial for consumers to understand GPU utilization requirements of their DL workloads prior to execution to determine potential cost.
As such, understanding DL workload has been identified as an important topic for designing resource-efficient cloud-based DL systems [21][22][23].
A challenge confronting resource-efficient cloud-based DL, though, is that current approaches to obtain the GPU utilization of DL workloads are reactive, i.e. online workload profiling is undertaken during execution.
Online profiling entails executing every unique submitted DL workload within an isolated GPU (or entire machine) ensuring accurate metric collection [14,24], and often exploits stress testing using micro-benchmarks to identify bottlenecks [18].
Such reactive profiling approaches results in both reduced service availability and resource-efficiency due to the necessity for dedicated unreserved devices.
This is particularly important due to the growing trend of AutoML, where DL workloads are of different configurations and/or model architecture [10].
An alternative and potentially more efficient approach is instead to proactively predict DL workload GPU utilization prior to execution.
However, no work yet exists capable of doing so rapidly or accurately; such prediction is difficult when considering diverse DL model architectures, DL frameworks (differing graph optimizations), datasets, and hardware heterogeneity-all which alter GPU utilization patterns.In this paper, we have designed a GPU utilization prediction engine for heterogeneous DL workloads.
In contrast to current approaches, our prediction engine proactively provides estimated values of unseen workloads based on historical profiles which can be used by DL cluster schedulers to reduce performance interference between workloads to increase resource efficiency as depicted in Figure 1.
Our research contributions are specifically:• We identify relevant DL model features (Floating Point Operations Per second (FLOPs), number of convolution layers, and input data size from DL workload computation graphs) that can be used to proactively predict GPU utilization without a need for online profiling.
We show that DL workload GPU utilization can be predicted with an RMSLE of 0.154.
• We demonstrate empirically that the prediction engine when integrated into a DL cluster scheduler supporting co-location can minimize performance interference due to GPU over-allocation, achieving up to 61.5% increase in GPU cluster utilization and up to 33.6% makespan reduction over existing approaches.
Understanding and achieving high resource utilization for heterogeneous workloads (including DL) in cloud computing is an important topic [8,10,14,18,[24][25][26][27][28][29][30][31][32][33][34][35][36][37].
DL workload are represented as Deep Neural Networks (DNN) as a Directed Acyclic Graph (DAG) or computation graph in execution.
Each graph node is an operation (i.e. layer), containing parameter information and has access to its predecessor and successor.
Since model parameters are typically stored as floating point values, larger models result in a higher number of Floating Point Operations (FLOPs), and increased GPU device memory requirements.
It has recently been shown that increasing DNN model depth and width improves accuracy in DL [38,39].
For example, in the ImageNet image classification challenge, the ResNet50 model with 4.1 GFLOPS and EfficientNet-B7 with 37 BFLOPs, achieved 93.0% and 97.1% Top-5 accuracy, respectively.GPUs are well-suited for DL computation due to their ability to perform iterative matrix multiplication, utilizing thousands of processing units, where a group of processing units is referred to as a Streaming Multiprocessor (SM).
SM executes kernels in Single-Instruction-Multiple-Thread (SIMT) paradigm 3 , hence each kernel defines the number of threads required, which are called thread blocks.
Each thread block is composed of a number of warps 4 , where each warp contains 32 threads [40].
Thus, a complex DL model with a large number of layers (i.e. kernels), results in a significant amount of 3 Kernel-In this context, GPU implementation of DNNs operations 4 A warp is the basic scheduling unit within an Nvidia GPU.
GPU utilization is known to be non-trivial to calculate [41].
To determine how the GPU is utilized during kernel execution, users can obtain relevant metrics from an NVIDIA System Management Interface (nvidia-smi), and through this observe the GPU and Memory utilization.
GPU utilization is defined as the percentage of time over the past sample period that one or more kernels are executed on the GPU.
Whereas Memory utilization is defined as the percentage of time over the past sample period that global (device) memory was read from or written to.
Although both measurements are not the actual utilization of the SM core (chip area containing the floating-point, integer and tensor units 5 ), nor the amount of read/write bytes from device memory and caches, they are a good estimate of the amount of warps required to keep the GPU busy.
Thus, recent online profiling approaches use one or more of these metrics for decision making [10,19,37].
GPU profiling is performed by executing workload on a dedicated GPU located in an isolated machine [14,18,24], with various device metrics obtainable by profiling tools 6 .
Profiling can be categorized as one of two types: Coarse grained profiling obtains the number of kernels, kernel configuration, GPU/memory utilization, and kernel execution time.
Based on our own experimentation, obtaining a good coarse grained profile requires several minutes of profiling depending on workload.
Fine-grained profiling requires accessing hardware performance counters per kernel including SM Efficiency, Achieved Occupancy and Bytes read/write from DRAM.
This method is more intensive, and based on our experiments, minutes to hours of profiling may be required depending on the metrics measured and workload complexity.
MobileNetV2 [42] [default, channel: 2 i 0...n ] ResNet [43] [18, 34, 50, 18 -bottleneck], GoogLeNet [44] [default] VGGNet [45] [19] ResNeXt [46] [11 -cardinality: 2, width :16] SqueezeNetV1 [47] [1.0] DenseNet [48] [121, 161, 169] ShuffleNetV2 [49] [0.5, 1.0, 2.0] MNASNet [50] [0.5, 1.0, 1.3], PyramidNet [51] [depth: [48,84] [57] were used for CV and NLP domains.
NLP dataset sentence length was set as 200, and vocabulary 10000 with batch sizes [16,32,64].
CV model batch sizes set as [64,128,256,512] 3 Related WorkMany existing cloud DL systems already profile workload to improve resource-efficiency, obtaining metrics such as training progress [58], workload communication distribution [15,16] and workload execution time [8,59].
In terms of profiling GPU utilization, Gandiva [10] focuses on DL workload time-sharing on GPUs, and conducts online workload profiling within isolated machines to determine suitable colocation and migration strategies.
Thinakaran el al. [37] also conduct online workload profiling on isolated machines for co-location strategies to harvest under-utilized resources.
Xu et al. [19] leverage characteristics of virtualized GPUs, and propose an interference-aware scheduler.
It exploits workload usage patterns such as GPU, memory, and vCPU utilization obtained from isolated profiling, to predict performance slowdown from co-located DL workloads.
Wang et al [22] obtain DL workload features and infrastructure features to decide a training regime (AllReduce, Parameter server/worker).
Qi et al [60] is the closest to our work, where they utilize model and device features, and profile per-layer execution time to predict overall training time.
To create our prediction engine, it is first necessary to study the relationship between DL workload characteristics and GPU utilization.
We have profiled 13 prominent computer vision (CV) and natural language processing (NLP) models architecture 7 , including convolution neural networks (CNNs) and recurrent neural networks (RNNs), with varying config-7 Models obtained from github: pytorch/vision, kuangliu/pytorch-cifar dyhan0920/PyramidNet-PyTorch [61], allenlp/allennlp [62] Table 1.
We attempted to capture as many unique DL models configurations as possible within the memory constraints of our GPU devices, resulting in a total of 81 different DL workloads executed.Micro-benchmarks were conducted using a dedicated machine with 4 x Nvidia GeForce 1080 GPUs and Intel i7-6850K CPU.
We used Nvidia Docker 2 container runtime, CUDA Toolkit 10.0, cuDNN 7.5, and PyTorch 1.1 DL Framework [63].
All micro-benchmarks were executed in isolation and ran multiple times to ensure metric consistency, with metrics collected using nvidia-smi.
Figure 2a depicts the GPU utilization of micro-benchmark models, with DL workload ranging between 20 -90% GPU utilization for their default parameter configurations, and batch size at 64 and 16 for vision and language models respectively.
It is observable that model complexity relates to higher GPU utilization, particularly FLOPs 8 , as shown in Figure 2b.
There exist several models with low FLOPs and high GPU utilization, which we postulate is likely resultant of model architectures producing high number of intermediate feature maps (e.g ResNet).
These findings are supported by prior studies in the field of neural architecture search and model compression [38,42,64,65].
DL models with greater complexity in terms of FLOPs and high number of feature maps, result in more kernels launched and longer kernel execution time.
We show the difference between two models in a 30ms period as shown in Figure 3a extracted via Nvidia Nsight System.
We also conducted experiments within two different GPU architectures -Pascal (1080) and Turing (2080) -with the same workload parameters, and observed that they follow the similar utilization proportions with Pascal exhibiting higher utilization as shown in Figure 3b.
We intend to investigate further with different hardware architectures.
Based on these observations, it appears possible to exploit DL model architecture structure to predict GPU utilization.8 calculated with respect to input data size Overview.
Our prediction engine extracts key DL workload features described in Table 2 via a process 9 that iterates through the DL model as shown in Figure 4.
These features are normalized and provided as numerical inputs to a machine learning model.
It requires an offline training stage on historical DL workload profiles.
Such profiles can be acquired via developers running benchmarks or DL workloads momentarily being the sole occupant of a GPU device, which we believe provides advantages over -and can be combined with -existing reactive approaches [10,19,37], as after successful model training, there is no need for isolated profiling for unique DL workload entering the system.
The machine learning model can be periodically retrained after collecting additional profiles.
Conv Features.
The prediction engine iterates through the model and calculates the FLOPs for each operation based on its inputs, output shape, and parameters.
For example, a standard general matrix multiplication in FLOPs is calculated by inputs shape × outputs shape × batch size.
Note that we modeled LSTM cell as two linear layers, as they perform matrix multiplication between the cell weights and inputs (e.g. 9 Features are currently limited to PyTorch, we plan to integrate into Open Neural Network Exchange formatting in future work.
input embedding, hidden states).
Once the inputs are split into gates, gate computation can be modeled as activations.
Finally, we counts the number of layer types.
We believe that these features are sufficient due to their prominence within all current vision and language models.Model Training.
81 samples of features and utilization profiles were derived from experiments in Section 4, and is of similar size comparable to prior work [19].
The training procedure consists of: 1) Random shuffling and splitting samples into 80% training and 20% testing sets.
2) Applying established regression models on training set to investigate prediction effectiveness -we perform naive grid search on each regressor to train the model with 5-fold cross validation to find best parameters i.e. n estimators and max depth for tree-based regressors.
3) Testing our models with 20% testing set to ensure the model can generalize to unseen data.
Model accuracy was determined via measuring regressor Root Mean Square Log Error (RMSLE)-an established measure for regression accuracy when the under-prediction error is enlarged.
This is suitable for utilization prediction: whilst overestimating GPU utilization is not ideal in terms of maximizing resource efficiency, it is preferable than underestimation leading to unintended GPU over-allocation,performance interference, cost budget underestimation.We observe that the random forest model achieves RMSLE of 0.154, and outperforms gradient boosting machine models.
We postulate that this may be due to our experiments containing a large proportion of CNN models, leading to random forests better fitted towards a set of data points as they naturally have higher variance than gradient boosting methods, where boosting are composed of weak learners [69].
We investigate the benefit of our proposed approach in colocation enabled DL workload scheduling (i.e. GPU sharing).
While we present scheduling as a case study, the prediction engine has other applications in power management and cost modelling which we intend to expand upon in future work.Co-location.
Safe co-location of DL workloads is desirable to improve resource-efficiency, with issues in performance interference and slowdown stemming from GPU overallocation.
Prior studies [10,19] indicate that co-location impacts Job Completion Time (JCT).
To reinforce these findings, we conducted an initial analysis of JCT slowdown for 276 execution combinations of co-located DL models each set to run for 5 epochs.
Figure 5a shows that increases JCT correlates positively with cumulative GPU utilization with typical slowdown between 10%-200% JCT slowdown 10 per DL model.
Experiment.
We integrated the prediction engine into a prototype co-location DL scheduler, deployed within a laboratory cluster (12 x 2080 GPU cluster, AMD Ryzen 1920 x 12 Core Processor, Kubernetes 1.15.2).
The scheduler uses a modified First Fit Decreasing bin-packing algorithm to maximize GPU utilization and minimize GPU over-allocation, allows three jobs maximum co-located per GPU, and perform resubmission in the event of job OOM.
We proactively provide predicted GPU utilization values at job submission time.
We compared our approach against slot-based (disallowed co-location) -i.e. the default kubernetes scheduler and a reactive online profiling approach, using a 1 minute period to profile in isolation until we can obtain training progress time and utilization value prior to co-location similar to [10].
Experiments were conducted using a mixture of DL workloads samples from Table 1.
Each experiment scheduled 100 DL workloads to all approaches five times each.
Workloads are configured to execute between 2 minutes to 2 hours by defining the appropriate epoch limit, scaled from prior work [15].
10 Normalized JCT slowdown is calculated as Results.
As summarized in Table 4, our proactive approach achieved average 69.6% GPU utilization in comparison to slot-based (43.1%) and reactive (47.1%).
This is due to reduced time waiting for an isolated machine to obtain an accurate metric using online profiling, hence increasing utilization overall.
We foresee that the reactive approach will achieve improved GPU utilization in a cluster with longer running DL workloads (i.e. where profiling time is small as a proportion of overall execution time), but would still require occupying a reserved GPU that may be otherwise available to other Cloud customers.
In all experiment runs, our approach achieved the lowest makespan with a 33.5% improvement against slotbased, and a 26.5% improvement over the reactive approach.
We found that slot-based scheduling achieved the lowest average JCT of 1869s due to disallowing co-location, followed by prediction (2024s) and reactive (2193s).
T colo − T solo T solo ,Summary.
From our preliminary study, we have found that our prediction engine appears to be promising approach to benefit DL schedulers supporting co-location, and believe that DL schedulers should encompass both reactive and proactive GPU utilization profiling together.
Though we have conducted experiments on Pascal and Turing GPU architecture, we did not integrate distributed workload (i.e. AllReduce, Parameter server and workers) in our case study.
We will discuss potential approach to handle distributed workload in Section 8.
Existing resource-efficient decisions for DL in the cloud made by providers and consumers are performed by understanding and exploiting workload metrics.
However, they can only do so by performing online profiling.
In this paper, we have proposed a prediction engine capable of predicting GPU utilization for heterogeneous DL workloads without the need for online profiling.
We have identified several model features that relate to GPU utilization (most notably model FLOPs), and demonstrated prediction with RMSLE of 0.154.
Based on these findings, we have experimented with several regression techniques and demonstrated the practicality of the prediction engine within a preliminary DL scheduler implementation for co-location, showing that it can be used to achieve an overall cluster GPU utilization of 69.6% in comparison to slot-based (43.1%) and conventional reactive (47.1%) approaches using online profiling.
While our study of DL model utilization is promising, there are several directions that we believe would be highly interesting to the community.
We note that there are DNN architectures that were excluded in this study, such as GAN and GNN.
This opens an intuitive path for further feature exploration.Hardware.
It is established that accelerator performance is affected by the number of processing elements, cache size and memory bandwidth.
We may be able to further generalise our approach to other GPU and accelerator architectures such as FPGAs.
Note that, we have yet to establish to what extent GPU architectures affect the prediction methods needed.DL Compilers.
It is known that when executing convolution, cudnn selects the best algorithm from winograd, fft and GEMM for the layer configuration, therefore it would be interesting for our approach to pre-determine these decisions to achieve higher utilization prediction accuracy.
Furthermore, existing works leverage DL compilers such as TVM [70] for computation graph optimization to reduce the number of kernels launched and reduce latency.
It may be worth to consider the intermediate outputs from these compilers to extract further information.Training Procedure.
Since we shuffle and split our samples, it is possible for models within same architecture to reside within the train and test set.
For example: ResNet50 -batch 64 in training set and ResNet50 -batch 256 in testing set.
We believe that in our current context it is acceptable as when using AutoML to search for best hyperparameters, it is possible that a DL model will be deployed with different batch sizes.
In the future, we will investigate the robustness of our approach by randomly shuffling the samples w.r.t model architecture type (i.e. all ResNet must be in the same set).
Distributed Workload.
Our prediction engine was designed for single GPU DL models, as recent analysis of production cluster traces [11,71] have shown that 86.6% of all DL workloads are currently trained within a single GPU.
However, it is likely that distributed DL workload will become increasingly prominent, and will result in performance bottlenecks due to network latency.
By considering the intricate relationship between infrastructure i.e. Network I/O, CPU, Memory, and models impact on GPU utilization would be crucial towards achieving high performance and scalability in distributed DL training or inference.Co-location Scheduling.
Our prototype cluster scheduler has identified that there are potentially large performance gains possible via intelligent co-location of DL workload.
We intend to improve upon this scheduler, and believe that optimization methods such as Mixed Integer Programming can be leveraged to derive a co-location aware optimization objective under resource constraints.
We would like to thank the anonymous HotCloud reviewers and our shepherd Arvind Krishnamurthy for their highly constructive comments and valuable feedback.
