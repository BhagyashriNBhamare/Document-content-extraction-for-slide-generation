In storage systems, delta compression is often used as a complementary data reduction technique for data deduplica-tion because it is able to eliminate redundancy among the non-duplicate but highly similar chunks.
Currently, what we call 'N-transform Super-Feature' (N-transform SF) is the most popular and widely used approach to computing data similarity for detecting delta compression candidates.
But our observations suggest that the N-transform SF is compute-intensive: it needs to linearly transform each Rabin fingerprint of the data chunks N times to obtain N features, and can be simplified by exploiting the fine-grained feature locality existing among highly similar chunks to eliminate time-consuming linear transformations.
Therefore, we propose Finesse, a fine-grained feature-locality-based fast resemblance detection approach that divides each chunk into several fixed-sized subchunks, computes features from these subchunks individually, and then groups the features into super-features.
Experimental results show that, compared with the state-of-the-art N-transform SF approach, Finesse accelerates the similarity computation for resemblance detection by 3.2×∼3.5× and increases the final throughput of a deduplicated and delta compressed prototype system by 41%∼85%, while achieving comparable compression ratios.
Data deduplication, a popular data reduction technique, usually identifies duplicate data at the chunk level (e.g., 8KB size) by using secure fingerprints (e.g., SHA1) to uniquely and globally represent data chunks in storage systems [34,44].
Hence, deduplication-based storage systems only store one physical instance referred to by any other duplicates, which helps improve storage space efficiency [18,25,44] or network bandwidth efficiency [26,29].
Recently, delta compression has also gained increasing attention due to its ability to eliminate data redundancy among non-duplicate but highly similar chunks, which can be used post-deduplication as a complementary technique to further eliminate redundancy.
For example, if chunk A 2 is similar to chunk A 1 (the base chunk), the delta compression approach only stores or transfers the differences (delta) and the mapping relation between A 2 and A 1 , removing the redundant data to improve storage space efficiency [21,30,36,37,41] or network bandwidth efficiency [8,29,42,43].
Several studies [29,30,37,38] suggest that delta compression is able to achieve about 2× additional compression ratio beyond deduplication and local compression in backup storage workloads.For delta compression in deduplication-based storage systems, resemblance detection is the first key step in its workflow, which identifies delta compression candidates.
This is because a higher similarity degree in the detected chunks implies more space savings from delta compression.
Currently, the most commonly used chunk-level resemblance detection approach computes the 'super-features' (SF for short) [5,17,29] based on the Rabin fingerprints [28] of data contents, to detect highly similar chunks.
Figure 1 gives an example of the general workflow for this SF-based delta compression approach: 1 ⃝ computing the similarity of chunks, namely, computing features and grouping features into SFs (detailed in Section 3), 2 ⃝ detecting similar chunks according to their SFs (any two chunks having a SF in common are considered highly similar [6]), 3 ⃝ delta encoding the two similar chunks, i.e., calculating their differences, also called 'delta'.
For decompression, the input chunk is recovered by decoding the 'delta' with the base chunk.To achieve high delta compression efficiency, some recent works on delta compression [17,19,29] recommend grouping four or more features into one SF to reduce false positives in resemblance detection, and using three or more SFs to detect more highly similar chunks for delta compression.
But according to observations in our delta compressed prototype system, computing the similarity of data chunks, namely, generating their SFs, is quite time-consuming.
Specifically, to ensure high similarity detection efficiency, Rabin fingerprints are calculated byte-by-byte on data chunks (similar to Content-Defined Chunking [26,40,43]), and are each then linearly transformed N times to calculate N-dimensional hash value sets.
Finally the N maximal values, one from each of the N dimensions, are selected as features.
Thus, the traditional SF approach needs to linearly transform each Rabin fingerprint of data chunks N times, which we refer to as 'N-transform SF' to distinguish it from our approach in the remainder of the paper.Consistent with the backup stream locality observed by many studies on deduplication [13,16,18,22,33,35,44], we observe that there also exists fine-grained locality among similar chunks.
This locality refers to the fact that the corresponding subregions (subchunks) of chunks and their features also appear in the same order among the similar chunks with a very high probability, which is referred to as feature locality in this paper.
Based on this key observation, we argue that a collection of features, exactly one extracted from each subchunk of a chunk, can also be used for representing the similarity of a chunk for generating SFs, which is much less compute-intensive than the N-transform SF since it eliminates the time-consuming linear transformations.In this paper, we propose Finesse, a fast resemblancedetection approach that exploits the fine-grained feature locality of similar chunks.
Specifically, Finesse simplifies computing the similarity by first dividing each chunk into several subchunks and then quickly computing features from each subchunk, finally grouping these features into SFs.
Experimental results based on six datasets show that, compared with the baseline N-transform SF approach, Finesse accelerates the similarity computation by 3.2×∼3.5× and increases the throughput of a delta compression prototype system by 41%∼85%, while achieving comparable and even higher compression ratios.
Data reduction has gained increasing attention and popularity in storage and file-transfer systems due to the explosive growth of digital data.
Compared with local compression (e.g., LZ [34]), data deduplication is able to identify and eliminate redundancy globally at at a much larger granularity (i.e., chunk-or file-level) in large-scale storage systems.
Thus it is widely studied and used in large-scale backup storage [18,29,33,44], primary storage [10,24,31], and HPC storage [23].
Meanwhile, delta compression, another data reduction technique that removes redundancy among non-duplicate but highly similar chunks, is able to help maximize the compression ratio when combined with deduplication and local compression in backup storage [30], storage replication [29], database storage [41], etc.
Shilane et al. [29,30] suggest that delta compression can achieve an additional 2× compression ratio beyond data deduplication in their production backup storage systems.
Similar results are also observed in other scenarios, such as, database storage [41,42] and migratory compression [19].
While greatly improving storage efficiency, delta compression also introduces extra compute and I/O overheads.
SIDC [29] suggests that the issue of on-disk large-sized similarity indexing faced by delta compression can be addressed by exploiting (caching) backup stream locality, in a way similar to data deduplication systems [44].
Ddelta [38] and Edelta [39] have been proposed to accelerate the delta encoding process by using the idea of CDC-based deduplication and exploiting fine-grained locality of the backup data streams.
Resemblance detection is the first step needed for delta compression to compute the similarity of data chunks and find compression candidates.
As mentioned earlier, the 'N-transform SF' approach is currently the most popular method for chunk-level resemblance detection.
It was first proposed by Broder [6] and is based on "Broder's theorem" [5], which evaluates the resemblance between two sets, as detailed below: being the corresponding sets of the hashes of the elements of A and B respectively, where H is chosen uniformly and randomly from a min-wise independent family of permutations [2,7].
An element in the set is mapped to an integer.
Let min(S) denote the smallest element of the set of integers S. Then:Pr[min(H(A))) = min(H(B))] = |A ∩ B| |A ∪ B| .
Broder's theorem states that the probability of the two sets A and B having the same minimum hash element is the same as their Jaccard similarity coefficient [14].
Based on this theorem, Broder proposed a resemblance detection approach called super-features [6,29] that extracts a fixed number of features from a chunk.
Specifically, this SF-based approach [29] (referred to as N-transform SF in this paper) Require: chunk content, Str; length of the chunk, L; randomly value pair for linear transformation, m i and a i ;Ensure: N features, Feature[N]; 1: function FEATURE-EXTRACT-N-TRANSFORM SF(Str, L) 2: Feature[0, · · · , N -1] ← 0; 3:for m = 0 to L -1 do 4:FP ← RabinFunction(Str, m); for i = 0 to N -1 do end for 12: end function computes data similarity by extracting features from Rabin fingerprints (a rolling hash algorithm [28]) and then grouping these features into SFs to detect resemblance for data reduction.
For example, Feature i of a chunk (length = L), is uniquely generated with a randomly pre-defined value pair m i & a i (i.e., linear transformation) and L Rabin fingerprints (as used in Content-Defined Chunking [26,40,43] with a sliding window size of 48 bytes as follows:Feature i = Max L j=1 {(m i · Rabin j + a i ) mod 2 32 } (1)Where Rabin j is the Rabin fingerprint of the sliding window located at position j) Thus chunks that have one or more such features (maximal values) in common are likely to be very similar, but small changes to the data are unlikely to perturb the maximal values [5,29].
Algorithm 1 provides a detailed pseudo-code implementation of extracting features by N-transform SF.
Then a super-feature of this chunk, SF x , can be calculated by several such features as follows:SF x = Rabin(Feature x·k , ..., Feature x·k+k-1 )(2)For example, to generate three SFs with k=4 features each, we must first generate N=12 features, namely, features 0...3 for SF 0 , features 4...7 for SF 1 , etc.
For similar chunks that differ only in a tiny fraction of bytes, most of their features will be identical and thus so are their SFs [6].
More specifically, this N-transform SF approach is able to maximally detect the highly similar chunks for two reasons.
1 ⃝ The matching of one SF means that almost all the features grouped in this SF are identical and thus grouping features into SFs reduces false positives for resemblance detection.
2 ⃝ Multiple SFs are computed to increase the probability of detecting highly similar chunks.
Meanwhile, this N-transform SF approach needs to linearly transform Rabin fingerprints of the data chunks N times, which is time-consuming and slows the whole post-deduplication delta compression process.It is worth noting that there are also some other coarsegrained resemblance detection approaches [4,9,11,15 41] for matching similar files or large data blocks (e.g., size of 16MB), which extract features from non-overlapped strings (or chunks) and thus may suffer from high false positives.
In this paper, we focus on improving the most popular N-transform SF approach for the chunk-level resemblance detection in post-deduplication delta compression scenario [30].
F BN = 0xffa23dc4 񮽙 񮽙 F A1 = 0xffc35b5e F A2 = 0xffed1921 F AN = 0xffa23dc4 F A1 = F B1 F A2 = F B2 F AN = F4 Finesse Design and Implementation As analyzed above, the root cause of the relatively high computation overhead of the N-transform SF approach is its linearly transforming the whole chunk's Rabin fingerprints N times (i.e., compute multiple rounds of transformations on each fingerprint) to extract N features.
According to our observation of delta compression on backup workloads, the features extracted from the subchunks inside individual chunks can also be used for resemblance detection, which means that we eliminate the linear transformations and thus simplify the feature computation.Computing features from subchunks is motivated by our observation that the fine-grained stream locality widely exists in the detected similar chunks.
Figure 2 provides an example of this locality: the subregions (subchunks) of individual chunks also appear in the same order among their highly similar chunks with a very high probability, meaning that these subchunks are also very similar to each other.
Table 1 studies this locality on six deduplicated backup datasets (the detailed experimental environment and workload characteristics can be found in Section 5), which demonstrates that most of the corresponding subchunk pairs Table 1: A study of the repeatability of subchunks and their features (i.e., the fine-grained locality) in the identified similar chunks in six deduplicated backup datasets.
Here the identified chunks are all divided into 12 equal-sized subchunks and then we verify the locality shown in Figure 2 subChunkSize ← L N ;3:Feature[0, · · · , N -1] ← 0; 4:for m = 0 to N -1 do for i = 0 to subChunkSize -1 do end for 12: end function in the detected similar chunks have the same features, accounting for 87.22% on average, although many of them are non-duplicate, accounting for 41.07% on average.
Therefore, grouping some of these features into SFs by exploiting this fine-grained locality of similar chunks may also potentially enable maximal detection of highly similar chunks.More importantly, this fine-grained locality-based resemblance detection approach has the potential to greatly reduce the execution time of computing features while achieving comparable resemblance detection efficiency relative to the N-transform SF approach, which is comprehensively evaluated and demonstrated in in Section 5.
In this subsection, we discuss some implementation issues of Finesse, including the feature extraction and grouping strategies, overhead analysis, and other design considerations.Feature Extraction.
To exploit the fine-grained feature locality of similar chunks for extracting features, Finesse first divides a chunk into several fixed-sized subchunks, and then computes features on each subchunk based on the Rabin fingerprints of the data contents, in the same way as the traditional N-transform SF approach, which is detailed in Algorithm 2.
Note that a chunk can be divided into variable-sized subchunks, similar to Content-Defined Chunking [26,40,43], but the feature grouping process will become very complicated since the subchunks' sizes and the number of features will become unknown.
In addition, our preliminary results suggest that extracting features on fixedsized subchunks is able to achieve nearly the same compression ratio as N-transform SF, and thus we use the fixedsized subchunks for feature extraction.Feature Grouping.
The grouping strategy in Finesse is different from traditional N-transform SF since the way features are extracted is changed in Finesse.
Specifically, Finesse first divides the subchunks and their corresponding features into several contiguous sets of the same size.
Then the biggest features (with the largest hash values from each of the sets) are grouped to constitute the first SF, the secondbiggest features of the sets are grouped to form the second SF, and so on and so forth.
This grouping strategy in Finesse ensures that the grouped features in each SF are selected uniformly and consistently all over the chunks, which achieves grouping efficiency similar to N-transform SF.To better illustrate this grouping strategy, we provide a detailed example with three SFs and four features per SF in Finesse as shown in Figure 3.
We first divide the chunk into twelve subchunks to extract 12 features F 0 ...F 11 Note that we tried other grouping strategies for Finesse but the performance differences were small or ever worse.
And our final evaluation result suggests Finesse using this grouping strategy achieves nearly the same delta-compression ratio as the classic N-transform SF.Computational Overhead.
As discussed above, the computational overhead of grouping features in Finesse is insignificant compared with computing features.
Thus, we only analyze the computational overhead on computing features.
Specifically, to generate N features from one chunk, for each Rabin fingerprint on the data chunk contents:• N-transform SF needs at least 3×N operations, includ- ing N multiply, N add, and N conditional branch operations, to select N maximal values (i.e., features) after linear transformation as discussed in Section 3.
• Finesse only needs one operation, i.e., one 'conditional branch', to select one maximal value (one feature) in each subchunk.
Therefore, Finesse greatly reduces the computation overhead for feature extraction and thus accelerates the whole resemblance-detection process.Limitations.
Note that Finesse has one limitation in that it does not detect "similar" chunks with very different sizes.
This is because Finesse divides a chunk into several equalsized subchunks and the features will be totally different if the two "similar" chunks are of very different sizes.
But in the delta-compression scenario, detecting chunks with similar sizes is reasonable since "similar" chunks with very different sizes (detected by the N-transform SF approach) may result in a low delta-compression ratio [17].
For example, two non-similar chunks that only have a small region in common may have many features and SFs in common and thus be considered to be similar chunks by the N-transform SF approach.
Experimental Platform.
We implement delta compression in an open-source deduplication prototype system called Destor [12,13] on the Ubuntu 12.04.2 operating system running on a quad-core Intel i7-4770 processor at 3.4 GHz and two 1TB 7200RPM hard disks.
Another Intel E5-2620 processor at 2.4 GHz is also used for performance comparison.
Data Reduction Configurations.
In our prototype system, deduplication is configured with Rabin-based chunking with the expected chunk size of 8KB as used in LBFS [26] and an in-memory SHA1 fingerprint table for duplicate detection.For the post-deduplication delta compression, the nonduplicate chunks are processed in three steps: resemblance detection, base chunk reading, and delta encoding.
The resemblance detection step for both Finesse and N-transform SF is configured to compute 3 SFs and 4 features per SF for matching highly similar chunks as suggested by SIDC [29] and MC [19] (to trade off the space savings and the computation & indexing overheads).
In addition, a chunk may have multiple similar chunks, and our prototype system selects the first matched chunk as its base, which is also known as "FirstFit" [17].
For the base chunk reading step, delta compression needs to read for each matched similar chunk its base chunk from the disk for delta encoding.
Here we use a base chunk cache with LRU and a size of 400MB to reduce base chunk I/Os.
For delta encoding, we employ the classic Xdelta [20] to calculate the delta of the similar chunks for space saving.
Performance Metrics.
We evaluate resemblance detection performance of Finesse using two metrics, Delta Compression Ratio (DCR) and Delta Compression Efficiency (DCE).
DCR is measured by total size before delta compression total size after delta compression , reflecting the total space saved by resemblance detection and then delta compression.
DCE is used to estimate the similarity degree between the similar chunks detected by Finesse, i.e., the chunk data size after delta compression the chunk data size before delta compression .
It is worth noting that DCR focuses on the overall space savings while DCE emphasizes the detected resembling chunks themselves.
Thus higher DCE means lower probability of false positives for detecting similar chunks.In addition, Similarity Computing Speed is measured by the processing speed at which the input data are calculated to obtain SFs for resemblance detection.
System Throughput is measured by the throughput with which the input data are deduplicated and then delta compressed.
We run each experiment five times to get the stable and the average results of the deduplication throughput.
Evaluated Datasets.
Six datasets are used for evaluation as shown in Table 2.
These datasets represent various typical workloads, including website snapshots, tarred source code files, database snapshots, and virtual machine images.
Resemblance Detection Efficiency.
Table 3 provides the delta compression results of all the similar chunks detected (i.e., they have a super-feature in common) by Finesse and N-transform SF respectively.
Generally, evaluation results in Table 3 suggest that Finesse achieves comparable compression ratio (with difference of -3.21%∼+7.36%) to the Ntransform SF approach in the metrics of DCR and DCE.
In addition, the resemblance detection performance of Finesse is sensitive to the datasets due to the different ways in which the files of each workload are evolved (i.e., modified) during backups.
Thus the six workloads have different levels of the fine-grained locality as studied in Table 1 (see Section 4.1).
For example, Finesse achieves higher DCR on datasets TAR and RDB, and lower DCR on datasets SYN and VMB.
Meanwhile, Finesse achieves higher DCE than Ntransform SF on all the six datasets.
There are two reasons.
1 ⃝ The N-transform SF approach may obtain all the features from one subregion of the chunk, which can lead to possible false positive resemblance detection and thus lower DCE.
In contrast, Finesse's SF grouping strategy ensures that the features grouped for each SF are coming from multiple subchunks of a chunk.
2 ⃝ N-transform SF may detect "similar" chunks with the very different sizes, which can result in poor delta compression efficiency as discussed in Section 4.2.
Speed of Computing SFs.
While Finesse achieves comparable compression ratios to that of N-transform SF, it greatly accelerates the similarity computation as shown in Figure 4.
Finesse improves this speed by an average of 3.5× and 3.2× respectively on the i7-4770 and E5-2620 CPUs.
This is because it requires much fewer operations on computing features as discussed in Section 4.
Note that the SF computing speed is not sensitive to the datasets because the time on computing features is decided by the size of the data chunks (i.e., scan all the bytes to calculate features and SFs ).
System Throughput.
To understand the impact of the resemblance detection approaches on the total throughput of the composite data reduction system combining deduplication and delta compression, we construct and evaluate the throughputs of two such systems with Finesse and Ntransform SF as their delta compression components respectively.
In our prototype system of both Finesse and Ntransform SF, we pipeline the deduplication subtasks (i.e., chunking, fingerprinting, and indexing) and delta compression subtasks (i.e., resemblance detection, reading base chunk, and delta encoding) for high system throughput.
Figure 5 shows the evaluation results comparing these two systems.
The system based on the Finesse approach outperforms the one based on N-transform SF by 41%-85% in total system throughput.
This is because in the delta compression phase after deduplication, Finesse is running 3× faster than N-transform SF for resemblance detection.
In this paper, we propose Finesse, a much faster resemblance detection approach than the state-of-the-art N-transform SF approach.
The key idea behind Finesse is to exploit the finegrained feature locality of highly similar chunks by dividing data chunk into multiple subchunks and extract features from each subchunk, thus reducing the computation overhead of resemblance detection.
Our experimental results based on six datasets demonstrate the superior performance of Finesse in terms of delta compression ratio, delta compression efficiency, speed of computing SFs, and throughput of the composite data reduction prototype system combining deduplication and delta compression.
We are grateful to our shepherd Geoff Kuenning and the anonymous reviewers for their insightful comments and feedback on this work.
This research was partly supported
