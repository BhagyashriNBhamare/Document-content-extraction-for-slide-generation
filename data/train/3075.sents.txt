Efficient namespace metadata management is increasingly important as next-generation storage systems are designed for peta and exascales.
New schemes have been proposed; however, their evaluation has been insufficient due to a lack of an appropriate namespace metadata benchmark.
We describe MimesisBench, a novel names-pace metadata benchmark for next-generation storage systems, and demonstrate its usefulness through a study of the scalability and performance of the Hadoop Distributed File System (HDFS).
There are no metadata-intensive benchmarks with realistic workloads for next-generation storage systems [1,13].
A few existing tools [7,9] are useful as microbenchmarks, but do not reproduce realistic workloads.The storage community has long-acknowledged the need for benchmarks based on realistic workloads, and programs like SPECsfs2008 [12] and Filebench [6] are extensively used for this purpose in traditional storage systems.
However, emerging Big Data workloads like MapReduce [2] have not been properly synthesized yet.We present MimesisBench, a novel metadata-intensive storage benchmark suitable for Big Data workloads.
MimesisBench consists of a workload modeling tool, a workload generator, and a workload profile from a large cluster at Yahoo.
More workloads will be added in the future.The model on which MimesisBench is based [3] allows it to generate type-aware workloads, in which specific types of file behavior can be isolated or modified for 'what-if' and sensitivity analysis.
These types of files are modeled autonomically, using unsupervised statistical clustering.
The model also supports multidimensional workload scaling.MimesisBench's Hadoop-based implementation allows it to be used in any storage system that is compatible with Hadoop (e.g., HDFS, Ceph, CassandraFS, Lustre).
We have released the benchmark and workloads as open source so that other researchers can benefit from it 1 .
This paper makes two contributions.
First, we extend a model for temporal locality and popularity in object request streams [3] to: (1) include other operations in addition to regular accesses to objects (opens), (2) support pre-existing files (created before the benchmark), and (3) support a realistic hierarchical namespace.
Second, we use this model to implement a metadata-intensive storage benchmark to issue realistic workloads on distributed storage systems.
This benchmark can be used to evaluate the performance of storage systems without having to deploy a large cluster and its applications.
We extend a model we proposed for generating accesses to objects (e.g., opens to files) [3], which is able to reproduce temporal correlations in object request streams that arise from the long-term popularity of the objects and their short-term temporal correlations.
This model is suitable for Big Data workloads because it supports highly dynamic populations, it is fast and scalable to millions of objects, and it is workload-agnostic so it can be used to model emerging workloads.Objects or files in a stationary segment of a request stream are modeled as a set of delayed renewal processes [11] (one per object).
Each object in the stream is characterized by its time of first access, its access interarrival distribution, and its active span (time during which an object is accessed).
With this approach, the system-wide popularity distribution asymptotically emerges through explicit reproduction of the per-object request arrivals and active span [3].
However, this model is unscalable, as it models each object independently.To reduce the model size, a lightweight version uses unsupervised statistical clustering (k-means) to identify groups of objects with similar behavior and significantly reduce the model space by modeling "types of objects" instead of individual objects.
As a result, the clustered model is suitable for synthetic workload generation.
The model described above cannot be used directly to test a storage system since it only reproduces accesses (e.g., opens) to an object (i.e., file) and not other operations that are also critical in a storage system like creates and deletes.
We propose the following extensions to the original model to make it suitable for namespace metadata benchmarking: (1) additional storage system operations, (2) pre-existing files, and (3) realistic namespaces.
We first extend our model to include file creations, deletions and list operations in addition to regular opens.
We focus on these operations because together they constitute more than 95% of the namespace metadata operations in MapReduce clusters [2], thus accounting for the vast majority of the workload.The list operations are like opens and only read or access the namespace.
Thus, we model both list and opens together as accesses to files.
A parameter keeps track of the percentage of read operations that constitute opens and those that are list.On the other hand, creates and deletes write or modify the namespace, and are characterized independently.
(a) Original model (b) Extended model Figure 1: Operations on a single file, with the original and the extended model.
The file's activation time is given by its creation time plus the delay to its first access; the deactivation time is given by its activation time plus its active span.
Figure 1 shows how the original and extended models generate operations on a file of a particular type (cluster).
The extended model requires the following additional per-cluster statistical information: distribution of create interarrivals, distribution of delay to deletion (relative to the object's last access in the stream, which is given by the time of first access + active span), and percentage of accesses that are open operations (list are the remaining percentage).
In addition, the activation time is relative to the creation stamp of a file (or to the beginning of the test, for files that already exist at t 0 ).
Table 1 lists the parameters required by the extended model to generate events to files.This extension does not affect the access patterns preserved by the original model, which are characterized by the per-cluster access interarrival distribution and the per-cluster distribution of active spans.
When benchmarking storage systems, we must consider that their performance depends on the state of the underlying file system [1,4], namely the pre-existing files and the structure of the hierarchical namespace (the latter is discussed in the next subsection).
We extend our model to keep track of the number of Array with percentage of files at each depth in hierarchy files (within each file type) that were created sometime before the beginning of the modeled trace.We could infer if a file exists prior to the captured trace of namespace events by making the assumption that any file accessed in the trace, but not created during it, is a pre-existing file.
However, this approach would lead to an inaccurate model if the trace contains many operations on files that do not exist (e.g., due to users incorrectly entering the name of a file).
To avoid this problem we can use a namespace metadata trace that contains a snapshot of the namespace (file and directory hierarchy), in addition to the set of events that operate atop that namespace (e.g., open a file, list directory contents) [1].
The traces we analyzed consist of access logs obtained by parsing the name node audit logs, plus namespace snapshots obtained with Hadoop's Offline Image Viewer tool.
Earlier, we proposed [1] a statistical model for generating realistic namespace hierarchies, and implemented a namespace generation module (NGM) based on it.
To the best of our knowledge, this is the only available tool that can generate large realistic namespaces 2 .
Prior to issuing the workload, the NGM is used to generate a realistic directory structure, which preserves the following statistical properties of the original: number of directories, distribution of directories at each depth, and distribution of subdirectories per directory.To integrate the files to this directory structure, we add a parameter to the model, the percentage of files at each depth of the hierarchy, and proportionally assign files to each depth according to this parameter.
Our model can be used to generate realistic synthetic workloads to evaluate the namespace metadata management subsystems.
Dimensions related to data input/output behavior-like a correlation between file size and popularity or the length of data read/write operations-are out of the scope of our model.We model stationary segments of object request streams.
Workloads consisting of a few stationary segments can be divided using the approach in [15].
However, in practice, benchmark runs tend to issue the workload of a period of one hour or less, so the need to consider non-stationary segments is not critical.We represent arrivals with a sequence of interarrival times X = (X 1 , X 2 , ...), where X is a sequence of independent, identically distributed random variables.
In practice, there could be autocorrelations in the arrival process, leading to bursty behavior at different timescales.In [3] we briefly discuss how ON/OFF processes can be used to capture burstiness.We assume that each arrival process of the accesses to a file in the storage system is ergodic, so its interarrival distribution can be deduced from a single realization of the process (i.e., the trace of events representing the segment being modeled).
In addition, instead of forcing a fit to a particular interarrival distribution, we use the empirical distribution of the interarrivals inferred from the arrivals observed in the trace being modeled.Finally, there may be unknown, but important, behavior in the original workloads that our model does not capture.
Some of these dimensions, like spatial locality, may be added to our model in the future.
However, there is a trade-off of increased complexity due to adding these dimensions.
Similar to Hadoop's DFSIO [16] and S-live [9], MimesisBench is a MapReduce job in which a set of mappers simultaneously issue requests to the storage layer.
Each mapper is in charge of issuing the operations on files of a particular type (as encapsulated by the model).
The cluster used to run MimesisBench must have at least k nodes available to run a mapper task each, so that the full workload can be issued simultaneously and the mappers do not interfere with each other during their operation.A run of MimesisBench has two phases: First, the preexisting files are created; next, the workload is issued.In each phase, a job coordinator parses the parameters and generates configuration files for each worker.
In addition, the job coordinator of the first phase creates the hierarchical namespace based on an input parameter file that has been pre-generated with the Namespace Generation Module (NGM).
In the first phase, the workers create the target number of files for each type.
A parameter tells the workers to use a flat namespace (create all files in a single, configurable path) or a hierarchical one.
Files of each type are created at different levels of the namespace hierarchy, proportionally to the configuration parameter of files at each depth (which indicates what percentage of files are located at each depth).
The subdirectories at each depth are assigned to a file type, proportionally to the weight of the cluster (w j ) to which the file belongs.In the second phase, the workers issue the load.
Each load generator worker reads the configuration for the specific file type that it has been assigned and waits in a time-based barrier 3 to start issuing the load corresponding to the files that belong to the type it is in charge of.
Two data structures are used to keep track of the files and events: a PriorityBlockingQueue of files (sorted by the timestamp of the next event-create, open, etc.-of that file) and a FIFO BlockingQueue of events to be issued 4 .
Three threads coordinate access to these data structures: a file introduction thread adds files to the priority queue (using the cluster's create interarrival distribution) 5 , another thread continuously polls the priority queue and adds details of the next event to be issued to the back of the FIFO queue.
Finally, a consumer thread pulls the information of the next event to be issued from the FIFO queue and schedules it to be issued at the proper time, using Java's ScheduledExecutorService.All file create operations create files of size zero.
This allows us to ignore the effect of writing bytes to a file, handled by the data nodes, and concentrate on evaluating the namespace metadata server (name node in HDFS) performance.A configurable maximum allowed drift is used to abort a run of the benchmark if the events are falling behind from their original schedule.
In that case, more mappers would be needed to issue the workload.A collector reducer task gathers the stats from the workers and generates aggregate results (see Table 2).
n new = n + w 1 × n (1) w 1 new × n new = 2(w 1 × n) (2) w j new × n new = w j × n, j ∈ {2, ..., k}(3)Time.
Interarrivals can be accelerated by multiplying the random variable by a scaling factor between 0 and 1.
A scaling factor of 1 reproduces the original workload, while a scaling factor of 0 provides maximum stress on the system by issuing all the operations as fast as possible (0 millisecond wait between operations).
Interarrivals can also be slowed down by multiplying the random variable by a constant greater than 1.
Active span.
The active span random variable can also be multiplied by a user-defined constant.
Modifying the active span has the effect of modifying the number of accesses of the files, thus modifying their popularity.
The user can choose to isolate the workload of a particular file type or cluster (i.e., turn-off the other types of files).
This can be used to analyze how a particular type of file affects the performance of the system.Summary Table 3 shows a summary of the features of MimesisBench and other related tools.
We demonstrate the usefulness of MimesisBench by using it to evaluate the performance and scalability of the HDFS name node across several dimensions.We modeled a 1-day (12/1/2011) namespace metadata trace from a Hadoop cluster at Yahoo.
The trace came from a 4100-node production cluster [10].
It contains 60.9 million opens events that target 4.3 million distinct files, and 4 PB of used storage space.
For a detailed workload characterization of this cluster see [2].
We modeled the trace using 30 file types or clusters (i.e., k = 30 for the k-means clustering algorithm).
We chose this value of k because it was the smallest for which we could obtain a close approximation of the file popularity distribution (Kolmogorov-Smirnov distance between the real and synthetic CDFs < 4%).
Our performance testbed had 32 nodes, each with 2 Xeon 2.4GHz quad-core processors and 24GB RAM.
Figure 2 shows the effect on the operation latency as interarrivals are accelerated.
In general, read operations are faster than write operations because acquiring an exclusive lock is not necessary to read the namespace.
The latency of the operations is not initially affected by issuing them faster.
This shows, that the performance of the name node is not degraded as more operations are issued per second.
However, when the clients issue more than 16,800 operations per second (c ≥ 0.2), the name node's performance starts degrading rapidly.
This information can be used to determine whether the name node can properly support an increased workload in the future.
Figure 3 shows the impact a hierarchical namespace (versus a flat one) has on the name node.
The performance degrades significantly faster on a flat namespace than on a hierarchical one.
The hierarchical namespace can serve up to 19,696 ops/sec versus 10,284 ops/sec for the flat namespace.
These results show that using benchmarks that create files in a flat namespace (see Table 3) is not desirable as they place a heavier and unrealistic burden on the locking mechanisms of the metadata server.
In this case, the problem is with the locking used to log namespace write operations used for auditing purposes.
We isolated the workload of two file types with different access patterns and observe the effects on latency and throughput (see Table 4).
We chose these two clusters because they represent two extreme, read-mostly (cluster 29) and write-heavy (cluster 17), yet realistic, workloads.We ran several tests with events issued at normal speed * Creates hierarchies with a given depth and width.
Characteristics like subdirectories per directory and directories per depth, are not supported.
(interarrival scaling factor = 1) and at full speed (interarrival scaling factor = 0).
To ensure a fair comparison, we adjusted the number of mappers issuing the workload so that the total number of operations issued in both tests was roughly the same.
At 30 mappers for cluster 17, and 12 mappers for cluster 29, the number of operations issued in the tests was 3 559 101 ± 0.9%.
Figure 4 shows the latency of open events.
The latency of the open events degrades significantly more in a write-heavy workload: When operations are issued at full speed, the latency of opens in the write-heavy workload increases 3.9x in cluster 17 versus 1.4x in cluster 29.
In addition, at maximum issuing speed (interarrival scaling = 0) the name node can serve 8 times more operations when the workload constitutes only reads: 53,233 vs. 6,453 ops/sec for clusters 29 and 17, respectively.
Table 4: Characteristics of the two clusters whose workload was isolated.
We chose these two clusters because they represent two extreme, read-mostly and write-heavy, workloads.
A common use for benchmarks is to evaluate a new design against an old design.
In this subsection, we show the results of one example of this type of evaluation.The HDFS-5239 [8] Jira allows the namespace lock fairness to be changed from fair (default) to unfair.
This Table 4).
Standard deviation < 0.5 msecs.change may be desirable in large clusters, as an unfair lock has a higher throughput than a fair lock.
Table 5 shows the name node throughput improvement for the following scenarios: full workload on a hierarchical namespace, full workload on a flat namespace, a realistic read-mostly workload (cluster 29), and a realistic write-heavy workload (cluster 17).
The improvement when the full workload is issued is around 10%.
However, for a read-mostly workload, the improvement is as high as 94%.
This is a result of the bottleneck that exists in the logging mechanism used for write operations.
Benchmarks are often used to evaluate new designs against an old design, or against another competing new design.
An improvement of 10% in performance may be desirable to push in a large system, as long as we can trust the results of the benchmark.
For this reason, it is important to have a small variability in the results produced by a benchmark.
For example, it is not reasonable to trust a 10% performance improvement if the coefficient of variation, c v , of the results is 8%.
Our experiments had a very small standard deviation and corresponding coefficient of variation (or the ratio of the standard deviation to the mean).
For all the sets of experiments we ran, the greatest observed coefficient of variation was 2.38%, with an average coefficient of variation of 1.08%.
These results suggest that MimesisBench is suitable as a tool to evaluate expected performance improvements of new designs.
Some prior tools provide a subset of the features of MimesisBench, as summarized in Table 3.
Mdtest issues metadata intensive workloads.
However, it does not support realistic workloads or namespaces.
In addition, mdtest interfaces with the storage system via system calls and has not been ported to the interfaces of next-generation storage systems, nor does it allow distributed workload generation.Filebench [6] uses a POSIX-compliant interface and includes traditional workloads like web server, file server, and database server.
Emerging Big Data workloads have not been included as pre-defined workloads.SPECsfs2008 [12] is used as a standard to enable comparison of file server throughput and response times across different vendors and configurations.
It supports NFSv3 and CIFS APIs and comes with a pre-defined workload based on enterprise NFS and CIFS workloads.For HDFS, the Hadoop developer community has designed two benchmarks that can test I/O operations and do simple stress-tests on storage layer: DFSIO [16] and S-live [9].
However, these benchmarks do not reproduce realistic workloads and can only be used as microbenchmarks.
Another Apache tool, NNBench [7] was created to benchmark the HDFS name node; however, it can only issue one type of operation at a time, and does not work atop a realistic namespace.Tarasov et al. [14] found that traditional workloads, like those provided with Filebench, are a poor replacement for virtual machine workloads.
We consider another emerging workload: MapReduce clusters.Impressions [4] generates realistic file system images; however, it is not readily coupled with a workload generator to easily reproduce workloads that operate on the generated namespace.
Furthermore, the generative model used by Impressions to create the file system hierarchy is not able to reproduce the distributions observed in our analysis, nor is it able scale to the large hierarchies observed in the Big Data systems we have studied.Chen et al. [5] proposed the use of multi-dimensional statistical correlation (k-means) to obtain storage system access patterns and design insights in user, application, file, and directory levels.
However, the clustering was not leveraged for workload generation or benchmarking.In earlier work, we developed Mimesis [1], a synthetic trace generator for namespace metadata traces.
However, it was too CPU-intensive to issue operations at real-time and as such is inapplicable for benchmarking real systems (though its synthetic traces could be used in tracebased evaluations).
In addition, its model does not reproduce file popularity.
We presented MimesisBench, a metadata-intensive storage benchmark suitable for Big Data workloads.
MimesisBench consists of a workload-generating software and a workload from a Yahoo Big Data cluster.MimesisBench is extensible and more workloads can be added in the future.
It is based on a novel model that allows it to generate type-aware workloads, in which specific type of file behavior can be isolated or modified for 'what-if' and sensitivity analysis.
In addition, it supports multi-dimensional workload scaling.MimesisBench is implemented on top of the Apache Hadoop framework, which allows it to be used in any storage system that is compatible with Hadoop.
We have released the benchmark and workload as open source.A study of the performance and scalability of HDFS was presented to show the usefulness of metadataintensive benchmarking using MimesisBench.
We thank our anonymous reviewers for their many insightful comments and suggestions.
This work was partially completed during C. Abad's PhD studies at UIUC and during her internship at Yahoo.
R. Campbell and C. Abad were supported in part by AFRL grant FA8750-11-2-0084.
Y. Lu is partially supported by NSF grant CNS-1150080.
