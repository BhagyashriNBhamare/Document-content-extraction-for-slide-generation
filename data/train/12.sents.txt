The existence of incentive-compatible computationally-efficient protocols for combinatorial auctions with decent approximation ratios is the paradigmatic problem in computational mechanism design.
It is believed that in many cases good approximations for combinatorial auctions may be unattainable due to an inherent clash between truthfulness and computational efficiency.
However, to date, researchers lack the machinery to prove such results.
In this paper, we present a new approach: We take the first steps towards the development of new technologies for lower bounding the VC-dimension of k-tuples of disjoint sets.
We apply this machinery to prove the first computational-complexity inapproximability results for incentive-compatible mechanisms for combinatorial auctions.
These results hold for the important class of VCG-based mechanisms , and are based on the complexity assumption that NP has no polynomial-size circuits.
We believe that our approach holds great promise.
Indeed, subsequently to our work, Buchfuhrer and Umans [10], and, independently, Dughmi et al. [20], strengthened our results via extensions of our techniques, thus proving conjectures we presented in [32] The field of algorithmic mechanism design [34] is about the reconciliation of bounded computational resources and strategic interaction between selfish participants.
In combinatorial auctions, the paradigmatic problem in this area, a set of items is sold to bidders with private preferences over subsets of the items, with the intent of maximizing the social welfare (i.e., the sum of bidders' utilities from their allocated items).
Researchers constantly seek auction protocols that are both incentive-compatible and computationally-efficient, and guarantee decent approximation ratios.
Sadly, to date, huge gaps exist between the state of the art approximation ratios obtained by unrestricted, and by truthful, algorithms.
It is believed that this could be due to an inherent clash between the truthfulness and computational-efficiency requirements, that manifests itself in greatly degraded algorithm performance.
Such tension between the two desiderata was recently shown to exist in [36] for a different mechanism design problem called combinatorial public projects [40].
However, in the context of combinatorial auctions, due to their unique combinatorial structure, the algorithmic game theory community currently lacks the machinery to prove this [38].
The celebrated class of Vickrey-Clarke-Groves (VCG) mechanisms [43,11,23] is the only known universal technique for the design of deterministic incentive-compatible mechanisms (in certain interesting cases VCG mechanisms are the only truthful mechanisms [8,19,27,37,36]).
While a naive application of VCG is often computationally intractable, more clever uses of VCG are the key to the best known (deterministic) approximation ratios for combinatorial auctions [16,25].
For these reasons, the exploration of the computational limitations of such mechanisms is an important research agenda (pursued in [15,19,27,33,36]).
Recently, it was shown [36] that the computational-complexity of VCG-based mechanisms is closely related to the notion of VC dimension.
[36] was able to make use of existing VC machinery to prove computational hardness results for combinatorial public projects.
However, for combinatorial auctions, these techniques are no longer applicable.
This is because, unlike combinatorial public projects, the space of outcomes in combinatorial auctions does not consist of subsets of the universe of items, bur rather of partitions of this universe (between the bidders).
This calls for the development of new VC machinery for the handling of such problems.We formally define the notion of the VC dimension of collections of partitions of a universe 1 and present techniques for establishing lower bounds on this VC dimension.
We show how these results can be used to prove computational-complexity inapproximability results for VCG-based mechanisms for combinatorial auctions.
(We note that these results actually hold for the more general class of mechanisms that are affine maximizers [27,37].)
Our inapproximability results depend on the computational assumption that SAT does not have polynomial-size circuits.
Informally, our method of lower bounding 2 the approximability of VCG-based mechanisms via VC arguments is the following: We consider well-known auction environments for which exact optimization is NP-hard.
We show that if a VCG-based mechanism approximates closely the optimal social welfare, then it is implicitly solving optimally a smaller, but still relatively large, optimization problem of the same nature -an NP-hard feat.
We establish this by showing that the subset of outcomes (partitions of items) considered by the VCG-based mechanism "shatters" a relatively large subset of the items.Our results imply the first computational complexity lower bounds for VCG-based mechanisms, and truthful mechanisms in general, for combinatorial auctions (with the possible exception of a result in [27] for a related auction environment).
In fact, we show that in some auction en-1 By a partition in this paper we mean an ordered k-tuple of disjoint subsets which, however, may not exhaust the universe.
2 We use the term lower bound as a general reference to an inapproximability result.
Hence, a lower bound of 1 2 means (as we are looking at a maximization problem) that no approximation better than 1 2 is possible.
This use is similar to that of Hastad in [24].
vironments these results actually apply to all truthful algorithms.
We illustrate our techniques via 2-bidder combinatorial auctions.
It is also our belief that the notion of the VC dimension of k-tuples of disjoint sets is of independent interest, and suggests many new and exciting problems in combinatorics.
Combinatorial auctions have been extensively studied in both the economics and the computer science literature [9,12,13].
It is known that if the preferences of the bidders are unrestricted then no constant approximation ratios are achievable (in polynomial time) [30,35].
Hence, much research has been devoted to the exploration of restrictions on bidders' preferences that allow for good approximations, e.g., for subadditive, and submodular, preferences constant approximation ratios have been obtained [16,18,21,22,29,44].
In contrast, the known truthful approximation algorithms for these classes have non-constant approximation ratios [14,16,17].
It is believed that this gap may be due to the computational burden imposed by the truthfulness requirement.
However, to date, this belief remains unproven.
In particular, no computational complexity lower bounds for truthful mechanisms for combinatorial auctions are known.Vickrey-Clarke-Groves (VCG) mechanisms [43,11,23], named after their three inventors, are the fundamental technique in mechanism design for inducing truthful behaviour of strategic agents.
Nisan and Ronen [33,34] were the first to consider the computational issues associated with the VCG technique.
In particular, [33] defines the notion of VCG-Based mechanisms.
VCG-based mechanisms have proven to be useful in designing approximation algorithms for combinatorial auctions [16,25].
In fact, the best known (deterministic) truthful approximation ratios for combinatorial auctions were obtained via VCG-based mechanisms [16,25] (with the notable exception of an algorithm in [5] for the case that many duplicates of each item exist).
Moreover, Lavi, Mu'alem and Nisan [27] have shown that in certain interesting cases VCG-based mechanisms are essentially the only truthful mechanisms (see also [19]).
Dobzinski and Nisan [15] tackled the problem of proving inapproximability results for VCGbased mechanisms by taking a communication complexity [45,26] approach.
Hence, in the settings considered in [15], it is assumed that each bidder has an exponentially large string of preferences (in the number of items).
However, real-life considerations render problematic the assumption that bidders' preferences are exponential in size.
Our intractability results deal with bidder preferences that are succinctly described, and therefore relate to computational complexity.
Thus, our techniques enable us to prove lower bounds even for the important case in which bidders' preferences can be concisely represented.The connection between the VC dimension and VCG-based mechanisms was observed in [36], where a general (i.e., not restricted to VCG-based mechanisms) inapproximability result was presented, albeit in the context of a different mechanism design problem, called combinatorial public projects (see also [40]).
The analysis in [36] was carried out within the standard VC framework, and so it relied on existing machinery (namely, the Sauer-Shelah Lemma [39,41] and its probabilistic version due to Ajtai [1]).
To handle the unique technical challenges posed by combinatorial auctions (specifically, the fact that the universe of items is partitioned between the bidders) new machinery is required.
Indeed, our technique can be interpreted as an extension of the Sauer-Shelah Lemma to the case of partitions (Lemma 2.12 in Sec. 2).
made between the the VC dimension and the approximability of combinatorial auctions.
Subsequently to this work [32], Buchfuhrer and Umans [10], and, independently, Dughmi et al. [20], strengthened our results via extensions of our techniques.
Specifically, both works show that no constant approximation ratios are achievable for n-bidder case of the combinatorial auction environment we discuss in Sec. 3, thus proving a conjecture we presented in [32].
The techniques in these two works are quite different, as we briefly discuss below.One of the main challenges in dealing with partitions of items is the possibility of unallocated items (as we shall later explain).
Our lower bounds for VCG-based mechanisms fall into two categories: (1) lower bounds that rely on the assumption that the auctioneer allocates all items.
(2) lower bounds for the general case that items may remain unallocated.
While case (1) enables a straightforward use of the Sauer-Shelah Lemma, case (2) requires heavier machinery.The work of Dughmi et al. [20] strengthens our results for case (2) via a clever and subtle generalization of our proof techniques.
The main ingredients in the proof in [20] are: (i) An analogue of the Sauer-Shelah Lemma that incorporates sets of partial k-colorings, generalizing our Lemma 2.12, that can be regarded as the case of partial 2-colorings.
(ii) Our lower bounds are based on a collection of sets such that every pair of them are "epsilon-close to orthogonal" (see Theorem 2.14).
[20] get sharper lower bounds by considering collections of sets such that every -tuple of them are epsilon-close to orthogonal.Buchfuhrer and Umans [10] overcome the problem of unallocated items via a clever "reduction" from case (2) to case (1).
They use a counting argument that shows that, for any VCG-based mechanism that obtains a "good" approximation ratio, there must be a reasonably large subset of items that are fully allocated in exponentially many different ways.
This enables an indirect application of the Sauer-Shelah Lemma.
In Sec. 2 we present our approach to analyzing the VC dimension of partitions.
In Sec. 3 we prove our inapproximability results for combinatorial auctions.
We conclude and present open questions in Sec. 4.
The main hurdle in combinatorial auctions stems from the fact that the outcomes are partitions of the set of items.
Approximation algorithms for combinatorial auctions can be thought of as functions that map bidders' preferences to partitions of items.
This motivates our extension of the standard notion of VC dimension to the VC dimension of partitions.
This section presents this notion of VC dimension and lower bounding techniques.
In Sec. 3 we harness this machinery to prove computational complexity lower bounds for combinatorial auctions.
We focus on partitions that consist of two disjoint subsets (our definitions can easily be extended to k-tuples).
Our formal definition of a partition of a universe is the following:Definition 2.1 (partitions) A partition T = (T 1 , T 2 ) of a universe U = {1, ..., m} is a pair of two disjoint subsets of U , i.e. T 1 , T 2 ⊆ [m] and T 1 ∩ T 2 = ∅.
Observe that we do not require that every element in the universe appear in one of the two disjoint subsets that form a partition.
This definition of partitions will later enable us to address crucial aspects of combinatorial auctions.
We refer to partitions that do exhaust the universe (i.e., cover all elements in the universe) as "covering partitions" (we shall refer to not-necessarily-covering partitions as "general partitions").
Definition 2.2 (covering partitions) A partition (T 1 , T 2 ) of a universe U is said to cover U if T 1 ∪ T 2 = U .
C(U )is defined to be the set of all partitions that cover U .
For every subset E of a universe U , we can define (in an analogous way) what a partition of E is, and denote by P (E) the set of all partitions of E and by C(E) the set of all partitions of E that cover E.Definition 2.3 (projections) The projection of a partition (S 1 , S 2 ) ∈ P (U ) on E ⊆ U , denoted by (S 1 , S 2 ) |E , is the partition (S 1 ∩ E, S 2 ∩ E) ∈ P (E).
For any collection of partitions R ⊆ P (U ) we define R's projection on E ⊆ U , R |E , to be R |E = {(T 1 , T 2 )| ∃(S 1 , S 2 ) ∈ R s.t. (S 1 , S 2 ) |E = (T 1 , T 2 )}.
Observe that if a partition (S 1 , S 2 ) of E ⊆ U is in C(E), then for any E ⊆ E (S 1 , S 2 ) |E ∈ C(E ).
We are now ready to define the notions of shattering and VC dimension in our context: R ⊆ P (U ) if C(E) ⊆ R |E .
Observe that that if E ⊆ U is shattered by a collection of partitions R ⊆ P (U ) then so are all subsets of E. Definition 2.5 (VC dimension) The VC dimension V C(R) of a collection of partitions R ⊆ P (U ) is the cardinality of the biggest subset E ⊆ U that is shattered by R.We now introduce the useful concept of α-approximate collections of partitions.
Informally, a collection R of partitions is α-approximate if, for every partition S of the universe (not necessarily in R), there is some partition in R that is "not far" (in terms of α) from S.
We are interested in the connection between the value of α of an α-approximate collection of partitions and its VC dimension.
This will play a major role in the proofs of our results for combinatorial auctions.Definition 1 (α-approximate collections of partitions) Let R be a collection of partitions of a universe U .
R is said to be α-approximate if for every partition S = (S 1 , S 2 ) ∈ P (U ) there exists some partitionT = (T 1 , T 2 ) ∈ R such that |S 1 ∩ T 1 | + |S 2 ∩ T 2 | ≥ α(|S 1 | + |S 2 |).
When dealing with collections of covering partitions it is possible to use existing VC machinery to lower bound their VC dimension.
Specifically, a straightforward application of the Sauer-Shelah Lemma [39,41] implies that: Lemma 2.6 (lower bounding the VC dimension of covering partitions) For every R ⊆ C(U ) it holds that V C(R) = Ω( log |R| log |U | ).
Proof:Let R 1 = {S 1 | ∃S 2 s.t. (S 1 , S 2 ) ∈ R}.
Because R only consists of covering partitions it must be that |R 1 | = |R|.
We now recall the Sauer-Shelah Lemma:Lemma 2.7 ( [39,41]) For any family Z of subsets of a universe U , there is a subset E of U of size Θ( log |Z| log |U | ) such that for each E ⊆ E there is a Z ∈ Z such that E = Z ∩ E.The Sauer-Shelah Lemma, when applied to R 1 implies the existence of a set E (as in the statement of the lemma) of size Ω( log |R 1 | log |U | ) (that is, a large set that is shattered in the traditional sense).
The fact that all partitions in R are covering partitions now immediately implies that V C(R) = Ω( log |R 1 | log |U | ) = Ω( log |R| log |U | ).
Lemma 2.6 enables us to prove a lower bounds on the VC dimension of ( 1 2 + )-approximate collections of covering partitions.Theorem 2.8 Let R ⊆ C(U ).
If R is ( 1 2 + )-approximate (for any small constant > 0) then there exists some constant α > 0 such that V C(R) ≥ m α .
Proof: [sketch]We use a probabilistic construction argument: Consider a partition of U , S = (S 1 , S 2 ), that is chosen, uniformly at random, out of all possible partitions in C(U ).
Observe that (by the Chernoff bounds), for every partition T = (T 1 , T 2 ) ∈ R, the probability that|S 1 ∩ T 1 | + |S 2 ∩ T 2 | ≥ ( 1 2 + )(|S 1 | + |S 2 |) = ( 1 2 + )m is exponentially small in m.Hence, for R to be ( 1 2 + )-approximate it must contain exponentially many partitions in C(U ).
We can now apply Lemma 2.6 to conclude the proof.
Observe that the proof Lemma 2.6 heavily relied on the fact that the partitions considered were covering partitions.
Dealing with collections of general partitions necessitates the development of different techniques for lower bounding the VC dimension.
We shall now present such a method, that can be regarded as an extension of the Sauer-Shelah Lemma [39,41] to the case of collections of general partitions: Definition 2.9 (distance between partitions) Given a universe U , two partitions in P (U ), (T 1 , T 2 ) and ( The proof follows the basic idea of [3,6,31].
Our novel observation is that the same proof strategy applies with our new definition of distance.T 1 , T 2 ), are said to be b-far (or at distance b) if |T 1 ∩ T 2 | + |T 1 ∩ T 2 | ≥ b. [Sketch] Fix > 0, k, m.
We wish to prove that t(, k, m) ≥ k α , for some constant α > 0.
We shall bound t(, k, m) by induction ( shall remain fixed throughout the proof and the induction is on k and m).
Let R be some collection of partitions as in the statement of the lemma.
Arbitrarily partition R into pairs.
Since the partitions that make up each pair are at least m-far there must exist (via simple counting) an element e ∈ U , such that in at least k 2 pairs(T 1 , T 2 ), (T 1 , T 2 ), e ∈ T 1 ∩ T 2 or e ∈ T 1 ∩ T 2 .
Let R ⊆ R be the collection of all partitions (T 1 , T 2 ) in R in which e ∈ T 1 .
Let R ⊆ R be the collection of all partitions (T 1 , T 2 ) in R in which e ∈ T 2 .
By the arguments above we are guaranteed that |R | ≥ k 2 and |R | ≥ k 2 .
Let I be all the subsets of U that are shattered by R.
We wish to lower bound |I|.
Let R −e be all the partitions of U \ {e} we get by removing e from T 1 for every partition (T 1 , T 2 ) ∈ R .
Let I be all the subsets of U \ {e} shattered by R −e .
As there are at least k 2 sets in R , by definition |I | ≥ t(, k 2 , m − 1).
Similarly, let R −e be all the partitions of U \ {e} we get by removing e from T 1 for every partition (T 1 , T 2 ) ∈ R .
Let I be all the subsets of U \ {e} shattered by R −e .
As there are at least k 2 sets in R , by definition |I | ≥ t(, k 2 , m − 1).
We claim that |I| ≥ |I | + |I |.
To see why this is true consider the following argument: All sets in I \ I and in I \ I are distinct and belong to I. Let S be a set in I ∩ I .
Observe that this means that not only is S in I, but so is S ∪ {e}.
So,I ≥ |I \ I | + |I \ I | + 2|I ∩ I | = |I | + |I |.
Hence, t(, k, m) ≥ 2 × t(, k 2 , m − 1).
We now use the induction hypothesis to conclude the proof.Lemma 2.12 and Observation 2.11 imply the following important corollary:Corollary 2.13 For every constant α > 0, and (sufficiently small) constant > 0, there exists a β > 0 such that, if R ⊆ P (U ) and it holds that: (1) |R| ≥ e m α , and (2) every two partitions in R are m-far, then V C(R) ≥ m β .
We shall now discuss the connection between the value of α of an α-approximate collection of partitions and its VC dimension.
We prove the following theorem:Theorem 2.14 Let R ⊆ P (U ).
If R is ( 3 4 + )-approximate (for any constant > 0) then there exists some constant α > 0 such that V C(R) ≥ m α .
Proof: To prove the theorem we use the following claim:Claim 2.15 For every small constant δ > 0, there is a family F of partitions (T 1 , T 2 ) in C( [m]) and a constant α > 0 such that |F | = e αm and every two partitions in F are at least 1−δ 2 m-far.
We will prove the claim for partitions T = (T 1 , T 2 ) whereT 1 ∪ T 2 = [m].
For such partitions, the distance between partition T = (T 1 , T 2 ) and T = (T 1 , T 2 ) is just the size of the symmetric difference of T 1 and T 2 .
The existence of the desired collection now follows from the existence of good codes, see e.g. [42].
For completeness we include the standard construction to show the existence of F .
Let T = (T 1 , T 2 ) and T = (T 1 , T 2 ) be two partitions in C( [m]) chosen at random in the following way: For each item j ∈ [m] we choose, uniformly at random, whether it will be placed in T 1 or in T 2 .
Similarly, we choose, uniformly at random, whether each item j shall be placed in T 1 or T 2 .
Using standard Chernoff arguments it is easy to show that the probability that there are at least m+δ 2 that appear in either T 1 ∩ T 1 or T 2 ∩ T 2 is exponentially small in .
Observe that this immediately implies (by our definition of distance) that the probability that the distance between T and T is less than 1−δ 2 m is exponentially small i δ.
Hence, a family F of exponential size must exist.Lemma 2.16 Let > 0.
Let R ⊆ P (U ) such that R is ( 3 4 + )-approximate.
Then, there is a subset of R, R , of size exponential in m such that every two elements of R are at least αm-far (for some constant α > 0).
By Claim 2.15 we know that, for our universe of U , there exists an exponential-sized family of partitions in C(U ), F , such that every two partitions in F are at least 1−δ 2 m-far (for some arbitrarily small δ > 0).
Fix some T, T ∈ F .
By definition of F , T and T are identical only on at most 1+δ 2 m elements (that is, only for at most 1+δ 2 m items j,either j ∈ T 1 ∩ T 1 or j ∈ T 2 ∩ T 2 ).
Let R T and R T represent two partitions in R that obtain 3 4 + "approximations" for T and T ', respectively (because R is ( 3 4 + )-approximate such partitions must exist).
Even if we assume that both R T and R T are identical on all elements on which T and T are identical, we are still left with 1−δ 2 m elements.
Observe that for each such element, if R T and R T are identical on it, it holds that it can only contribute to the approximation obtained by one of them.
This implies that to obtain the promised approximation R T and R T must differ on quite a lot (a constant fraction) of the elements in U .
This, in turn, implies that there is some α > 0 such that R T and R T are αm-far.
Corollary 2.13 now concludes the proof of the theorem.
In this section we present the connection between our results for collections of partitions in Sec. 2 and the problem of social-welfare-maximization in combinatorial auctions.
We use the VC dimension framework developed in the previous section to present a general technique for proving computational complexity lower bounds for VCG-based mechanisms.
2-bidder combinatorial auctions.
We consider auction environments of the following form: There is a set of items 1 . . . , m that are sold to 2 bidders, 1 and 2.
Each bidder i has a private valuation function (sometimes simply referred to as a valuation) v i that assigns a nonnegative real value to every subset of the items.
v i (S) can be regarded as i's maximum willingness to pay for the bundle of items S. Each v i is assumed to be nondecreasing, i.e., ∀S ⊆ T it holds that v i (S) ≤ v i (T ).
The objective is find a partition of the items (S 1 , S 2 ) between the two bidders that maximizes the social welfare, i.e., the expression Σ i v i (S i ).
It is known that optimizing the social welfare value in 2-bidder combinatorial auctions is computationally intractable even for very restricted classes of valuation functions.
In particular, [29] shows that this task is NP-hard even for the simple class of capped additive valuations: Definition 2 (additive valuations) A valuation function a is said to be additive if there exist per-item values a i1 , . . . , a im , such that for every bundle S ⊆ [m], a(S) = Σ j∈S a ij .
S ⊆ [m], v(S) = min{a(S), B}.
Intuitively, a bidder has a capped additive valuation if his value for each bundle of items is simply the additive sum of his values for the items in it, up to some maximum amount he is willing to spend.
This class of valuations shall be used throughout this section to illustrate our impossibility results (as we aim to prove inapproximablity results the restrictedness of this class works to our advantage).
Maximal-in-range mechanisms.
Mechanisms that rely on the VCG technique to ensure truthfulness (VCG-based mechanisms) are known to have the useful combinatorial property of being maximal-in-range [15,33,37]: 3 Maximal-in-range mechanisms are mechanisms that always exactly optimize over a (fixed) set of outcomes.
In our context, this means that for every maximal-in-range mechanism M there exists some R M ⊆ P ( [m]) such that M always outputs an optimal outcome in R M (with respect to social-welfare maximization).
We refer to R M as M 's range.Definition 4 (maximal-in-range mechanisms) A mechanism M is maximal-in-range if there is a collection of partitions R M ⊆ P ( [m]) such that for every pair of valuations,(v 1 , v 2 ), M outputs a partition (T 1 , T 2 ) ∈ argmax (S 1 ,S 2 )∈R M v 1 (S 1 ) + v 2 (S 2 ).
It is know that every maximal-in-range mechanism can be made incentive compatible via the VCG technique [33,34].
This suggests a general way for the design of truthful mechanisms for combinatorial auctions: Fix the range R M of a maximal-in-range mechanism M to be such that (1) optimizing over R M can be done in polynomial time, and (2) the optimal outcome in R M always provides a "good" approximation to the globally-optimal outcome.
This approach was shown to be useful in [16,25].
Observe that the maximal-in-range mechanism in which R M contains all possible partitions of items is computationally intractable even for capped additive valuations.
In contrast, the fact that bidders' valuations are nondecreasing implies the following general upper bound: Observation 3.1 (the trivial upper bound) For any 2-bidder combinatorial auction, the maximalin-range mechanism M for which R M = {([m], ∅), (∅, [m])} provides a 1 2 -approximation to the optimal social welfare.That is, the maximal-in-range mechanism that bundles all items together and allocates them to the bidder with the highest value provides a 1 2 -approximation to the optimal social welfare.
This mechanism is easy to implement in a computationally-efficient manner as it only requires learning the value of each bidder for the bundle of all items.Is the trivial upper bound optimal?
Naturally, we are interested in the question of whether a more clever choice of range than {( [m], ∅), (∅, [m])} can lead to better approximation ratios (without jeopardizing computational efficiency).
Let us consider 2-bidder combinatorial auctions with capped additive valuations.
For this restricted case, a non-truthful PTAS exists [4].
Can a similar result be obtained via a maximal-in-range mechanism?
We show that the answer to this question is No by proving that the approximation ratios obtained by computationally-efficient VCG-based mechanisms are always bounded away from 1.
We stress that these are the first computational complexity lower bounds on the approximability of VCG-based mechanisms for combinatorial auctions.
In fact, as we shall later show, in certain cases these bounds extend to all incentive-compatible mechanisms.
We now present our method of proving lower bounds on the approximability of VCG-based mechanisms using the VC framework.
On a high level, our technique for proving that a maximal-in-range mechanism M cannot obtain an α-approximation consists of three steps:• Observe that M 's range must be an α-approximate collection of partitions.
• Conclude (from our results in Sec. 2) the existence of a shattered set of items of size m α (if α is sufficiently high).
• Show a non-uniform reduction from NP-hard 2-bidder combinatorial auctions with m α items to the optimization problem solved by M .
We illustrate these three steps by proving a lower bound of 3 4 for 2-bidder combinatorial auctions with capped additive valuations (which naturally extends to the more general classes of submodular, and subadditive, valuations).
We stress that our proof technique can be applied to prove the same lower bound for practically any NP-hard 2-bidder combinatorial auction environment.
Essentially, our only requirement from the class of valuations is that it be expressive enough to contain the class of 0/1-additive valuations defined below.Definition 5 (0/1-additive valuations) A valuation v is said to be 0/1-additive if it is an additive valuation in which all the per-item values are in {0, 1}.
We make the following observation: Observation 3.2 Any α-approximation maximal-in-range mechanism for 2-bidder combinatorial auctions with 0/1-additive valuations has a range that is an α-approximate collection of partitions.
A 0/1-additive valuation can be regarded as an indicator function that specifies some subset of the universe (that contains only the items that are assigned a value of 1).
Hence, pairs of such valuations that specify disjoint subsets correspond to partitions of the universe.
Now, it is easy to see that, by definition, the range of an α-approximation maximal-in-range mechanism must be an α-approximate collection of partitions.
Observation 3.2 enables us to make use of Theorem 2.14 to conclude that:Theorem 3.3The range of any ( 3 4 + )-approximation maximal-in-range mechanism for 2-bidder combinatorial auctions with 0/1-additive valuations shatters a set of items of size m α (for some constant α > 0).
We can now exploit the existence of a large shattered set of items to prove our lower bound by showing a non-uniform reduction from an NP-hard optimization problem:Theorem 3.4 No polynomial-time maximal-in-range mechanism obtains an approximation ratio of 3 4 + for 2-bidder combinatorial auctions with capped additive valuations unless NP ⊆ P/poly.
Proof: Let M be a mechanism as in the statement of the theorem.
Since 0/1-additive valuations are a special case of capped additive valuations, by Theorem 3.3 there exists a constant α > 0 such that R m shatters a set of items E of size m α .
Therefore, given an auction with m α items and capped additive valuation functions v 1 , v 2 we can identify each item in this smaller auction with some unique item in E, and construct valuation functions v 1 , v 2 , such that v i is identical to v i on E and assigns 0 to all other items.
Observe that this means that M will output for v 1 , v 2 the optimal solution for v 1 , v 2 (as M 's range contains all partitions in C(E)).
We now have a non-uniform reduction from an NP-hard problem (social-welfare maximization in the smaller auction) to the optimization problem solved by M .
Recall that the trivial upper bound provides an approximation ratio of 1 2 .
We leave the problem of closing the gap between this upper bound and our lower bound open.
We conjecture that the trivial upper bound is, in fact, tight.
This conjecture is motivated by the following result.The allocate-all-items case.
We now consider the well-studied case that the auctioneer must allocate all items [19,27].
Observe that, in this case, the range of a maximal-in-range mechanism can only consist of covering partitions, for which stronger results are obtained in Sec. 2.
This enables us to use our technique to prove the following theorem:Theorem 3.5 For the allocate-all-items case, no polynomial-time maximal-in-range mechanism obtains an approximation ratio of 2 − for 2-bidder combinatorial auctions with capped additive valuations unless NP ⊆ P/poly.
If bidders have subadditive valuations, and all items are allocated, then maximal-in-range mechanisms are the only truthful mechanisms [19].
Since capped additive valuations are a special case of subadditive valuations, the lower bound in Theorem 3.5 holds for all truthful mechanisms in this more general environment.
In Appendix A, we show that, for a superclass of capped additive valuations, it is possible to relax the computational assumption in the statement of Theorem 3.5 to the assumption that NP is not contained in BPP.
This is achieved by using Ajtai's [1] probabilistic version of the Sauer-Shelah Lemma.
We believe that our work opens a new avenue for proving complexity-theoretic inapproximability results for maximal-in-range mechanisms for auctions.
In particular, the following important questions remain wide open: (1) Can our inapproximability results be made to hold for all truthful mechanisms?
So far, despite much work on this subject [37,27,28,8,19,36], very little is known about characterizations of truthfulness in combinatorial auctions (and in other multi-parameter environments).
(2) Our computational complexity results depend on the assumption that SAT does not have polynomial-size circuits.
Can this assumption be relaxed by proving probabilistic versions of our VC machinery (see Appendix A)?
