Networks are expected to provide reliable performance under a wide range of operating conditions, but existing traffic engineering (TE) solutions optimize for performance or robustness, but not both.
A key factor that impacts the quality of a TE system is the set of paths used to carry traffic.
Some systems rely on shortest paths, which leads to excessive congestion in topologies with bottleneck links, while others use paths that minimize congestion, which are brittle and prone to failure.
This paper presents a system that uses a set of paths computed using Räcke's oblivious routing algorithm, as well as a centralized controller to dynamically adapt sending rates.
Although oblivious routing and centralized TE have been studied previously in isolation, their combination is novel and powerful.
We built a software framework to model TE solutions and conducted extensive experiments across a large number of topologies and scenarios, including the production backbone of a large content provider and an ISP.
Our results show that semi-oblivious routing provides near-optimal performance and is far more robust than state-of-the-art systems.
Two roads diverged in a wood, and I -I took the one less traveled by, And that has made all the difference.-Robert FrostNetworks are expected to provide good performance even in the presence of unexpected traffic shifts and outright failures.
But while there is extensive literature on how to best route traffic through a network while optimizing for objectives such as minimizing congestion [3,9,13,14,15,22,24,26,47], current traffic engineering (TE) solutions can perform poorly when operating conditions diverge from the expected [32,42].
The tension between performance and reliability is not merely a hypothetical concern.
Leading technology companies such as Google [18,24] and Microsoft [22,32] have identified these properties as critical issues for their private networks.
For example, a central goal of Google's B4 system is to drive link utilization to 100%, but doing this means that packet loss is "inevitable" when failures occur [24].
Meanwhile a different study of availability at Google identified "no more than a few minutes of downtime per month" as a goal, where downtime is defined as packet loss above 0.1%-2% [18].
Stepping back, one can see that there are two fundamental choices in the design of any TE system: (i) which forwarding paths to use to carry traffic from sources to destinations, and (ii) which sending rates to use to balance incoming traffic flows among those paths.
Any TE solution can be viewed in terms of these choices, but there are also practical considerations that limit the kinds of systems that can be deployed.
For example, setting up and tearing down end-to-end forwarding paths is a relatively slow operation, especially in wide-area networks, which imposes a fundamental lower bound on how quickly the network can react to dynamic changes by modifying the set of forwarding paths [25].
On the other hand, modifying the sending rates for an existing set of forwarding paths is a relatively inexpensive operation that can be implemented almost instantaneously on modern switches [32].
Another important consideration is the size of the forwarding tables required to implement a TE solution, as there are limits to how many paths can be installed on each switch [7,22,42].
These considerations suggest that a key factor for achieving reliable performance is to select a small set of diverse forwarding paths that are able to route a range of demands under a variety of failure scenarios.
Unfortunately, existing TE solutions fail to meet this challenge.
For example, using k-shortest paths works well in simple settings but leads to excessive congestion in topologies with shortcut links, which become bottlenecks.
Using kedge-disjoint paths between pairs of nodes does not fare much better, since paths between different node pairs still contend for bandwidth on bottleneck links [24,32].
Using 15th USENIX Symposium on Networked Systems Design and Implementation 157 Path weights w p Figure 1: Semi-oblivious TE system model a constraint solver to compute forwarding paths that optimize for given scenarios and objectives effectively avoids bottlenecks, but it can also "overfit" to specific scenarios, yielding a brittle and error-prone solution.
In addition, it is difficult to impose a budget on the number of paths used by the solver, and common heuristics for pruning the set of paths degrade performance [42].
Our approach.
We present SSSSS, a new TE system based on two key ingredients.
First, it uses a set of forwarding paths computed using oblivious routing1 [38,39], rather than shortest, edge-disjoint, or optimal paths, as in current approaches [22,24,32].
The paths computed by oblivious routing enjoy three important properties: they are low-stretch, diverse, and naturally balance load.
Second, it dynamically adapts sending rates [9,22,26,28,47] on those paths to fit current demands and react to failures.
While these ideas have been explored in isolation previously [3,7,20], their combination turns out to be surprisingly powerful, and allows SSSSS to achieve near-optimal performance.
Our work is the first practical implementation and comprehensive evaluation of this combined approach, called semi-oblivious routing.Through extensive experiments with data from the production networks of a large content provider (anonymized as BigNet) and a major ISP, as well as large-scale simulations, we demonstrate that SSSSS achieves performance that is near-optimal, competitive with state-of-the-art solutions [22,24], and better than the worst-case scenarios predicted in the literature.
SSSSS also achieves a level of robustness that improves on solutions explicitly designed to be fault tolerant [32,42].
Contributions.
Our contributions are as follows: 1.
We identify a general model for TE systems, and survey various approaches to wide-area TE ( §2).
2.
We present SSSSS's design and discuss key properties that affect performance and robustness ( §3).
3.
We demonstrate the deployability of SSSSS on a production network, and develop a framework for modeling and evaluating TE systems ( §4).
4.
We conduct an extensive evaluation comparing SSSSS with other systems in a variety of scenarios ( §5- §6).
Overall, SSSSS is a promising approach to TE that is based on solid theoretical foundations and offers attractive 1We use the term oblivious routing to refer to Räcke's algorithm, and not other demand-oblivious approaches.
Load balanced Diverse Low-stretch Capacity awareGlobally optimized [24] ?
?
SPF / ECMP × × × 񮽙 CSPF 񮽙 × × 񮽙 k-shortest paths (KSP) × × ? 񮽙
Edge-disjoint KSP × × MCF × × Robust MCF [42] × VLB [45] × × 񮽙 × B4SSSSS / Oblivious [39] 񮽙 񮽙 񮽙 񮽙?
: difficult to generalize without considering topology and/or demands.
performance and robustness.
This section develops a general model that captures the essential behavior of TE systems and briefly surveys related work in the area.
Abstractly, a TE system can be characterized in terms of two fundamental choices: which forwarding paths to use to carry traffic, and how to spread traffic over those paths.
This is captured in the two phases of the model shown in Fig. 1: (i) path selection and (ii) rate adaption.
The first phase maps the network topology to a set of forwarding paths connecting each pair of nodes.
Typically this phase is executed only infrequently-e.g., when the topology changes-since updating end-to-end forwarding paths is a relatively slow operation.
In fact, in a wide-area network it can take as long as several minutes to update end-to-end paths due to the time required to update switch TCAMs on multiple geo-distributed devices.
In the second phase, the system takes information about current demands and failures, and generates a weighted set of paths that describe how incoming flows should be mapped onto those paths.
Because updating path weights is a relatively fast operation, this phase can be executed continuously as conditions evolve.
For example, the system might update weights to rebalance load when demands change, or set the weight of some paths to zero when a link breaks.
The main challenge studied in this paper is how to design a TE system that selects a small set of paths in the first phase that is able to flexibly handle many different scenarios in the second phase.
The central thesis of this paper is that path selection has a large impact on the performance and robustness of TE systems.
Even for systems that incorporate a dynamic rate adaption phase to optimize for specific performance objectives, the set of paths selected is crucial as it defines the state space for optimization.
Desirable path properties include:A. low stretch for minimizing latency, B. high diversity for ensuring robustness, and C. good load balancing for achieving performance.Unfortunately, current TE systems fail to guarantee at least one of these properties, as shown in Table 1.
For example, approaches based on k-shortest paths (KSP) fail to provide good load balancing properties in many topologies due of two fundamental reasons.
First, KSP is not capacity-aware.
Note that wide-area topologies evolve over time to meet growing demands [18], leading to heterogeneous link capacities as shown in Fig. 2.
As KSP does not consider capacities, it often over-utilizes low-capacity links that lie on many shortest paths.
Using inverse of capacity as link weight is a common technique to handle this, but it can lead to increased latency due to capacity heterogeneity.
Second, because KSP computes paths between each pair of nodes independently, it does not consider whether any given link is already heavily used by other pairs of nodes.
Hence, lacking this notion of globally optimized path selection, even if one shifts to using seemingly more diverse edge-disjoint k-shortest paths, the union of paths for all node pairs may still overutilize bottleneck links.In general, to achieve low stretch and good load balancing properties, a path selection algorithm must be capacity aware and globally optimized.
To illustrate, consider the topology in Fig. 3 where unit flows arrive in the order f 1 , f 2 , and f 3 .
In Fig. 3a, we use the shortest paths to route, as in KSP, and thus link (G, E) becomes congested.
In Fig. 3b, we greedily assign the shortest path with sufficient capacity to each flow in order of arrival, as in CSPF, which leads to a locally optimal but globally suboptimal set of paths since some paths have high latency.
Finally, in Fig. 3c we depict the globally optimal set of paths.
The challenge is to compute a set of paths that closely approximates the performance of these optimal paths while remaining feasible to implement in practice.
The textbook approach to TE merges the two phases in our model and frames it as a combinatorial optimization problem: given a capacitated network and a set of demands for flow between nodes, find an assignment of flows to paths that optimizes for some criterion, such as minimizing the maximum link utilization (MLU).
This is known as the multi-commodity flow (MCF) problem in the literature, and has been extensively studied.
If flows are restricted to use a single path between node pairs, then the problem is NP-complete.
But if fractional flows are allowed, then optimal solutions can be found in strongly polynomial time using linear programming (LP) [44].
Another approach, which has been widely used in practice, is to tune link weights in distributed routing protocols, such as OSPF and ECMP, so they compute a good set of forwarding paths [14,15], and not perform any rate adaptation.
This approach is simple to implement as it harnesses the capabilities of widely-deployed conventional protocols, but optimizing link weights for ECMP is NP-hard.
Moreover, it often performs poorly when failures occur, or during periods of re-convergence after link weights are modified to reflect new demands.
COYOTE [8] aims to improve performance of such distributed approaches by carefully manipulating the view of each switch via fake protocol messages.Several recent centralized TE systems explicitly decouple the phases of path selection and rate adaption.
SWAN [22] distributes flow across a subset of k-shortest paths, using an LP formulation that reserves a small amount of "scratch capacity" for configuration updates.
The system proposed by Suchara et al. [42] (henceforth referred as "R-MCF") performs a robust optimization that incorporates failures into the LP formulation to compute a diverse set of paths offline.
It then uses a simple local scheme to dynamically adapt sending rates at each source.
Recent work by Chang et al. [6] also used robust optimization to validate designs that provide performance and robustness guarantees that are better than worst-case bounds.
FFC [32] recommends (p, q) link-switch disjoint paths and spreads traffic based on an LP to ensure resilience to up to k arbitrary failures.
B4 [24] selects paths greedily based on demands.
It uses BwE [28] and heuristic optimizations to divide flows onto paths to improve utilization while ensuring fairness.Another line of work has explored the space of oblivious approaches that provide strong guarantees in the presence of arbitrary demands [2,3,7].
Valiant Load Balancing (VLB) routes traffic via randomly selected intermediate nodes.
Originally proposed as a way to balance load in parallel computers [45], VLB has recently been applied in a number of other settings including WANs [48].
However, the use of intermediate nodes increases path length, which can dramatically increase latency-e.g., consider routing traffic from New York to Seattle via Paris.Oblivious routing, which generalizes VLB, computes a probability distribution on low-stretch paths and forwards traffic according to that distribution no matter what demands occur when deployed-in other words, it is oblivious to the demands.
Remarkably, there exist oblivious routing schemes whose congestion ratio is never worse than O(log n) factor of optimal.
One such scheme, proposed in a breakthrough paper by Räcke [38], constructs a set of tree-structured overlays and then uses these overlays to construct random forwarding paths.
While the O(log n) congestion ratio for oblivious routing is surprisingly strong for a worst-case guarantee, it still requires overprovisioning capacities by a significant amount.
Applegate and Cohen [3] developed an LP formulation of optimal oblivious routing.
They showed that in contrast to the O(log n) overprovisioning suggested by Räcke's result, in most cases, it is sufficient to overprovision the capacity of each edge by a factor of 2 or less.
While better than the worst-case bounds, it is still not competitive with the state-of-the-art.
This lead us to explore augmenting oblivious routing for path selection with dynamic rate adaption in order to achieve better performance.
Our design for SSSSS follows the two-phase system model introduced in the preceding section: we use oblivious routing ( §3.1) to select forwarding paths, and we use a constraint optimizer ( §3.2) to continuously adapt the sending rates on those paths.
This approach ensures that the paths used in SSSSS enjoy the properties discussed in §2 by construction-i.e., they are low-stretch, diverse, and load balanced.Performing a robust, multi-objective optimization to compute paths based on anticipated demands is challenging in practice in the presence of resource constraints [6,32].
Moreover, if the actual conditions differ from what was predicted-e.g., due to failures, or in an ISP where customers may behave in ways that are difficult to anticipate-performance will suffer in general [3].
In contrast, because the paths in oblivious routing are computed without knowledge of the demands, they avoid overfitting to any specific scenario, which makes the system naturally robust.
Finally, SSSSS comes pre-equipped with a simple mechanism for imposing a budget on the total number of paths used, which allows it to degrade gracefully in the presence of resource constraints, unlike many other approaches.
The core of SSSSS's oblivious path selection is based on a structure we call a routing tree that implicitly defines a unique path for every node pair in the network G(V, E).
A routing tree comprises: (i) a logical tree T(V t , E t ) whose leaves correspond to nodes of G, i.e., there is a one-to-one mapping m ′ V : V → V t , and (ii) a mapping m E : E t → E * that assigns to each edge e T of T a corresponding path in G, such that edges sharing a common endpoint in T are mapped to paths sharing a common endpoint in G.
One can obtain a path Path T (u, v) from u to v in G by finding the corresponding leaves m ′ V (u) and m ′ V (v) of T, identifying the edges of the unique path in T that joins these two leaves, and concatenating the corresponding physical paths based on m E in G. Generalizing this idea, a randomized routing tree (RRT) is a probability distribution over routing trees.
The corresponding oblivious routing scheme computes a u−v path by first sampling a routing tree T, then selecting the path Path T (u, v).
One way to think of oblivious routing is as a hierarchical generalization of VLB, where the network is recursively partitioned into progressively smaller subsets, and one routes from u to v by finding the smallest subset in the hierarchy that contains them both, and constructing a path through a sequence of random intermediate destinations within this subset.
Räcke's breakthrough discovery [39] was an efficient, iterative algorithm for constructing RRTs.We illustrate how the set of paths selected by SSSSS have the required properties, in contrast to other well known path selection algorithms, such as ECMP, KSP, edge-disjoint k-shortest paths (EDKSP), VLB and MCF using a representative WAN topology (Hibernia Atlantic).2 Fig. 4 shows the paths selected by various algorithms for all node pairs and uses a color-coding to indicate load (i.e., the sum of weights of paths using each link).
The inset images show the latencies of paths selected by each algorithm for different node pairs.A.
SSSSS's paths have low stretch.
The central ingredient in Räcke's construction of RRTs is a reduction from oblivious routing to the problem of computing low-stretch routing trees, defined as follows.
The input is an undirected graph G whose edges are assigned positive lengths ℓ(e) and weights w(e).
The length of a path P, ℓ(P), is defined to be the sum of its edge lengths, and the average stretch of a routing tree T is defined to be the ratio of weighted sumsstretch(T) = 񮽙 e=(u,v) w(e)ℓ(Path T (u, v)) 񮽙 e=(u,v) w(e)ℓ(e), where both sums range over all edges of G.
The problem is to select T so as to minimize this quantity, which can be interpreted as the (weighted) average amount by which we inflate the length of an edge e when we join its endpoints using the path determined by T.For every instance of the low-stretch routing tree problem, there exists a solution with average stretch O(log n), and a randomized algorithm, due to Fakcharoenphol, Rao, and Talwar (FRT) [10], efficiently finds such a tree.
The algorithm works by computing all-pairs shortest paths in G to define a metric space, then hierarchically decomposing this metric space into clusters of geometrically decreasing diameter, each with a distinguished vertex called the cluster center.
The topology of the routing tree is defined to be the Hasse diagram of this hierarchical decomposition ordered by inclusion, and the paths associated to its edges are shortest paths between the corresponding cluster centers.
Thus, to route from a source u to a destination v, one constructs a path from u by bubbling up through the hierarchy, taking shortest paths to centers of increasingly large clusters, until one reaches the center of a cluster containing both u and v; let's call this center the least common ancestor (LC A u,v ); then one reverses this process to route from that cluster center to v.
If both u and v belong to a cluster C, then the length of the path thus constructed is bounded by a constant times the diameter of C.
This explains why paths tend to avoid the lengthy detours that can plague VLB, especially when the source and destination are near one another as shown in insets in Fig. 4.
In practice, oblivious routing is often competitive with shortest-path based approaches in terms of latency.
Also note that while MCF optimizes for congestion, it may pick long detours to avoid bottlenecks.B.
SSSSS uses diverse paths for robustness.
VLB achieves robustness by routing through random intermediaries, which avoids treating any particular link as critical.
Oblivious routing generalizes VLB by allowing for a hierarchy of random intermediate destinations rather than just one.
A u−v path is constructed by concatenating paths through a sequence of intermediate destinations representing their ancestors in the sampled routing tree T, up to and including LC A u,v .
A well-chosen RRT will have the property that the detour through LC A u,v rarely consumes much more capacity than directly taking a shortest path.
This allows routing with RRTs to attain aggregate utilization that is nearly as efficient as shortestpath routing, worst-case load balancing that matches or improves VLB, as well as good robustness properties.One can quantify robustness by generalizing the concept of a SRLG3 and grouping u−v paths, Π(u, v), by the edges they share, such that an edge failure can break all the paths in the shared risk group.
We define risk, r euv , of an edge e with respect to a node pair (u, v) as the fraction of Π(u, v) paths using e.
If e is not used by (u, v), then r euv is undefined.
For highest resilience, Π(u, v) consists of pairwise edge-disjoint paths, and for any edge e, r euv ≤ 1 |Π(u,v) | .
A fragile Π(u, v) has paths sharing some common edge e ′ , and r e ′ uv = 1.
Thus, low risk implies high resilience to faults, and low impact on congestion when reacting to failures as more paths are available to share the load.
A robust set of paths will have less high risk edges.
Fig. 5 shows the distribution of risk when using up to 4 paths per node pair.
Ideally, EDKSP should have the entire mass at 0.25.
But, a closer look at the topology reveals that for most node pairs, only two edgedisjoint paths exist, implying risk of 0.5 for all edges in those paths, as illustrated in Fig. 6.
KSP always finds 4 (u, v) paths which differ slightly and these differing edges have low risk, but the significant number of overlapping edges in these paths have high risk.
Interestingly, the set of paths computed using MCF also tend to be brittle.
On this topology, both oblivious routing and VLB compute diverse sets of paths, and thus are robust to failures.
SSSSS's path selection algorithm is a capacity-aware iterative algorithm that constructs a sequence of instances of the low-stretch routing tree problem, with the same graph topology but varying edge lengths, and solves each in- stance using FRT.
In a given iteration, the root is selected randomly and the length of each edge is multiplied by an exponential function of the "cumulative usage", relative to capacity, of that edge in previously computed routing trees4.
The tree computed in each iteration is thus penalized for re-using edges that have been heavily utilized in previous trees, and consequently, the ensemble of all routing trees in the sequence (with suitable probabilities) balances load among edges in a way that ensures O(log n) congestion ratio regardless of the traffic matrix (TM).
To illustrate this, consider the nodes Boston and Halifax in Fig. 4.
There is a direct "shortcut" link connecting these nodes, as well as a slightly longer path to the north.
Shortest path based algorithms overload the shorter link and ignore the detour, while MCF balances load equally on the two paths.
Oblivious routing distributes load unequally, preferring the direct link but reducing its load by using the detour for a fraction of traffic.We note another interesting observation based on the Seattle-Miami paths depicted in Fig. 6.
Paths selected by KSP are identical for most part, and the only variation occurs at nodes in close proximity within the US northeast.
In our experience, this phenomenon occurs often, and failures of such shared edges can adversely affect performance.
In contrast, oblivious routing and VLB select a more diverse set of paths which are edge-disjoint with higher probability.
Another intuitive way to look at this is that most path selection algorithms compute paths greedily for individual node pairs while oblivious routing globally optimizes paths considering all pairs simultaneously, like MCF.
For instance, even though EDKSP computes diverse paths for individual pairs, when paths for all pairs are considered together, the shortcut links can become overloaded, as they may be used by many pairs.4This is an instance of the multiplicative weights update method [4], a general iterative method for solving programs such as packing and covering LPs.
The method has a tunable parameter ǫ which governs the trade-off between the approximation accuracy and the number of iterations required.
Our implementation uses ǫ = 0.1.
G(V, E) Input graph Input Π The base set of paths allowed in G D Predicted traffic matrix Π(s, t)The set of all s to t paths in Π d (s,t) Demand from s to t specified by D Auxiliary cap(e) Capacity of link e U e Expected utilization of link e Z Expected maximum link utilization ep(P) End-w P = 1 ∀e ∈ E : U e ≤ Z U e = 񮽙 P ∈Π:e ∈P w P · d ep(P) cap(e)∀P ∈ Π : w P ≥ 0 These observations on properties of paths selected by oblivious routing motivate using a static set of paths while dynamically adjusting the distribution of traffic over those paths as the demand varies and/or network elements fail and recover.
This combination of a static set of paths and time-varying adaptation of flow rates on those paths has been called semi-oblivious routing [20].
From a worstcase standpoint, this approach is not significantly better than oblivious routing.
Hajiaghayi et al. [20] proved that any semi-oblivious routing scheme that uses polynomially many forwarding paths must suffer a congestion ratio of Ω(log n/log (log (n))) in the worst case.
However, the proof of the lower bound involves constructing highly unnatural TMs and topologies such as recursive seriesparallel graphs and grids satisfying specific properties.
In contrast, WAN topologies grow in a planned manner, and capacities are augmented based on forecasted demands.
Hence, real-world topologies and TMs are implicitly correlated.
This raises the question of whether it is possible for semi-oblivious routing schemes to approach or match the performance of optimal MCF in practice.In SSSSS, we select the static set of paths, Π, using Räcke's algorithm to obtain a distribution over routing trees, taking the union of the path sets defined by each routing tree in the support of this distribution, pruning this distribution to the paths with the highest weights to respect path budget constraints, and then re-normalizing.
To distribute flow over paths, we solve a variation of MCF using a linear program (LP).
This is similar to the usage of LP in SWAN and FFC, for instance, but with different objective function and constraint set.
In SSSSS the LP formulation (Table 2) is used to minimize MLU by balancing traffic over the allowed base set of paths.
The output variables w P express the relative weight of paths for each source-destination pair.
The constraints ensure that the weights sum to 1 (i.e., all flows are assigned some path) and that capacity constraints are respected.
We discuss two implementations to understand and evaluate TE systems.
First, we describe a real-world deployment in a production WAN.
We highlight a number of practical issues that arose in that deployment ( §4.1).
Second, we present an implementation using Y [29], a general framework for rapid prototyping and evaluation of TE approaches ( §4.2).
We discuss how we calibrated Y's simulator against a hardware testbed and the production WAN.
To better understand the practical challenges associated with bringing SSSSS to production, we deployed a TE system, which dynamically load-balances traffic over a static set of paths, on BigNet's inter-datacenter network (IDN), which is similar to Google's B4 [24] and Facebook's EBB [11].
Architecture.
IDN consists of four identical planes (topologies), each of which can be programmed independently.
The backbone routers at each datacenter site are connected to an aggregation layer similar to FatCat [41], which distributes outgoing traffic across the planes equally using ECMP.
IDN employs a hybrid control model with distributed LSP agents as well as a centralized controller.
It supports two traffic classes (high and low priority) which can be managed using different TE algorithms.
This architecture facilitates experimenting with different TE algorithms on a subset of planes while the other planes provide a safe fallback.Controller.
The IDN controller allows the routes for each plane to be updated every 15 seconds.
The inputs to the controller are obtained from a state snapshotter service that captures the live state of the network including: (i) configured components for IDN from a central repository for network information [43], (ii) live link state information from Open/R [12], (iii) any operational overrides (link, router, or plane drains), and (iv) real-time TM estimated from sFlow [37] Failures.
In the event of a failure, traffic is routed along pre-programmed backup paths until the IDN controller computes a new routing scheme.
In addition to data-plane faults, failures can also arise due to control-plane errors, such as router misconfiguration or control-plane having an inconsistent view of the network.
We proactively test resilience using a fault-injector that can introduce both kinds of failures in a controlled manner.State churn and update time.
To quantify the operational overheads of different TE approaches, we measure path churn and the time required to compute an updated routing scheme.
Churn is undesirable for several reasons: it increases CPU and memory load on routers and adds significant complexity to the management infrastructure.
Fig. 7a shows the number of paths that would be changed every hour when running MCF and CSPF5.
Likewise, routing schemes that are expensive to compute impose a burden on the controller.
Fortunately, the LP that SSSSS uses for rate adaptation is less complex than the LP used to solve MCF-the time needed to solve each problem instance is two orders of magnitude less than MCF (order of 100ms vs. 10s), as shown in Fig. 7b, using Gurobi [19] for optimization on a 16-core machine with 2.6 GHz CPU.
Furthermore, because SSSSS only updates path weights, which takes just a few milliseconds, it is more responsive than MCF, which requires updating whole paths, which can take tens of seconds [24,32].
We evaluate the performance of a wide range of TE approaches under a variety of workloads and operational scenarios using Y [29], which consists of a TE simulator and a SDN prototype.
Although numerous simulators and emulators have been proposed over the years [5,21,30,34,36,46], Y is designed specifically to evaluate TE algorithms.
With Y, TE algorithms are implemented as modules against a general interface.
Table 3 contains a partial list of TE algorithms that we have implemented and made available under an opensource license6.
For clarity, we report results from only a subset of these.Simulator.
Y's simulator can model diverse operational conditions and record detailed statistics.
It requires three inputs: (i) topology, (ii) a timeseries of actual TMs to simulate network load, and (iii) corresponding predicted TMs.
For each predicted TM, it computes the routing scheme based on the algorithm (OOOOOOO uses actual TMs) and then simulates the flow of traffic where each source generates traffic based on the actual TM.
We choose the fluid model [27] to simulate traffic owing to its scalability without sacrificing accuracy of macroscopic behavior.
In case the actual TM is unsatisfiable using the routing scheme, Y still admits the entire demand at each source.
However, it assigns each flow its max-min fair share at each oversubscribed link and drops any traffic exceeding the flow's fair share for that link.
ible, it is critical that they accurately correspond to results from real deployments.
We validate the accuracy of Y's simulator with: (i) a small-scale hardware testbed, and (ii) BigNet's large backbone network (LBN) ( §5).
We use the SDN backend to emulate Internet2's Abilene backbone network [1] on a testbed of 12 switches and replay traffic based on NetFlow traces collected from the actual Abilene network.
As shown in Fig. 8, the simulation results closely match observed results in the hardware testbed.
We also implement a centralized approximation of the distributed TE algorithm used in production in LBN.
The network and demands are highly dynamic, and the production TE scheme reacts to such changes at a very fine time scale.
As a result, we are able to only approximate its behavior.
Still, the values reported by Y closely match those seen in production.
We evaluate SSSSS in a production setting using multiple criteria.
For the setting, we use data from BigNet's large backbone network (LBN).
LBN is one of the largest global deployments and carries a mix of traffic ranging from real-time video streaming and messaging to massive data synchronization globally.
For criteria, we focus on four key questions: How close is SSSSS to optimal in terms of performance ( §5.1)?
What is the impact on latency for not choosing strictly shortest paths ( §5.2)?
How is performance impacted under failures ( §5.3) and other operational constraints ( §5.4)?
§6 explores whether these results generalize to other settings, using large-scale simulations over a diverse set of network scenarios.Overview of BigNet's WAN.
The network models a common content-provider design, with connections between several large datacenters across Asia, Europe, and the US as well as connectivity to numerous Points-ofPresence (PoPs) around the globe.
The topology has hundreds of routers and thousands of high-speed interconnecting links, varying vastly in capacity and latency.
This heterogeneity largely stems from the way the net- work evolved over many years.
The topology exhibits clustered structure, with clusters following geographic constraints imposed by continents and links between clusters running over transoceanic paths, similar to the topology that we used for illustration in §3.
Traffic on LBN exhibits multiple strong diurnal patterns, modulated by the activities of billions of users in different time zones.
Being a global system, different parts of the network experience peak loads at different times.
Overall traffic patterns over the WAN can be split into two major categories: (i) traffic between datacenters, and (ii) traffic from datacenters to PoPs.
Inter-datacenter traffic typically consists of various replication workflows, such as those related to cache consistency or bulk traffic for Hadoop and database replication.
A significant amount of this traffic is delay tolerant, and could be routed over non-shortest paths between the datacenters.
However, the traffic from datacenters to PoPs is latency sensitive, as it represents content routed to BigNet users.Methodology.
We collect production data from LBN consisting of accurate network topology, link capacities, link latencies, aggregate site-to-site TMs, and paths used by traffic in production.
Using this data, we perform highfidelity simulations with Y and present results based on the statistics reported.
Traffic with different latency requirements are routed separately in production to avoid excessive path stretch for latency-sensitive traffic.
For simplicity, we choose to route both types of traffic using the same TE scheme.
Traffic on BigNet's WAN is growing at a rapid pace, and so the network also evolves with it.
We present results based on the network state and demands for a month in late 2016.
For each hourly snapshot of the network under regular operating conditions (i.e. without any failures), we measure various performance statistics (with a path budget k = 4, same as reported in B4 [24]) using different TE approaches.
Fig. 9 shows the (i) throughput normalized to total demand, (ii) maximum congestion (fractional link utilization), and (iii) normalized traffic dropped due to congestion over a period of half a week.
CSPF and OOOO-񮽙񮽙񮽙7 dynamically compute paths with sufficient capacity for each flow, and thus avoid congestion.
Remarkably, only oblivious routing and SSSSS are able to achieve 100% throughput, while other centralized TE approaches aren't able to do so and introduce bottlenecks.As expected, OOOOOOO (which uses MCF to minimize MLU) achieves the lowest maximum congestion, which varies between 0.40 and 0.67 following a diurnal pattern.
We find that oblivious routing performance remains within a factor of 2 as had been previously studied [3], while SSSSS is closest to optimal with maximum congestion within 16% of OOOOOOO, on average and within 41% in the worst case.Clearly, SSSSS's path selection plays a crucial role in it being so competitive.
To gain further insight, we examine the distribution of congestion and expected link utilizations, i.e., how much traffic each link would have carried if packets weren't dropped due to capacity constraints.
Figs. 10a and 10b show the corresponding complementary CDFs.
We observe that ECMP8, FFC*9, R-MCF and KSP+MCF (an approximation of SWAN's path selection and rate adaptation)10 scheduled ∼10% of links to carry traffic exceeding their capacity-ECMP even oversubscribed a link 80×!
We find that these bottleneck links usually appear in the shortest paths between many pairs of nodes.
In contrast, Räcke's algorithm iteratively samples paths while avoiding overloading any link, and SSSSS load balances over these paths to reduce congestion even further.
On scaling up the demands, we do expect to see congestion loss with all the approaches, including SSSSS.
From Fig. 10a, we also note that SSSSS maintains a lower congestion consistently for all links and has a 95 th -percentile congestion of 0.57-same as OOOOOOO.Even though FFC*, KSP+MCF and R-MCF dynamically load balance traffic over a set of paths, they perform suboptimally.
This could be because the paths selected under the budget constraint did not provide enough flexi-7OOOOOOO does not have any budget or other operational constraints.
8Using RTTs as link weights for computing shortest paths.
9Our implementation configures FFC to handle single link failures by combining edge-disjoint k-shortest paths with fault-tolerance LP.
10We implement a version that uses k-shortest paths as tunnels and uses MCF to assign path weights.
bility to eliminate bottlenecks for any traffic splitting ratio, and this was further exacerbated as the inadmissible fraction of demands also contributed to congestion before being dropped.
To validate this, we measure performance with increasing budget in Figs. 11a and 11b.
KSP+MCF and R-MCF, indeed, become near-optimal when the sets of paths become diverse enough.
However, FFC* doesn't improve beyond a point as the number of disjoint paths is very limited.
Even though the "working set" [22] of paths for KSP+MCF is small, it needs 8× as many total number of paths as SSSSS to achieve similar performance.
Fig. 10c shows the distribution of latency as a fraction of total demand that is delivered within a given latency11.
To compute latency experienced by traffic along a path, we simply sum the measured RTTs for each hop along the path.
TE approaches which route over shortest paths while respecting capacity constraints, like CSPF, have the least latency.
Oblivious routing doesn't ensure that the shortest paths are necessarily selected.
However, as we showed in §3.1, the paths computed have low stretch.
SSSSS uses the same set of low-stretch paths.
Intuitively, longer paths can increase congestion as the same set of packets contribute to congestion at more links.
As SSSSS optimizes for congestion, it also indirectly favors shorter paths.
We find that SSSSS is competitive with other shortest-path based approaches.
Even if we ignore dropped traffic and normalize y−values in Fig. 10c with throughput, the median latency for SSSSS (58.3ms) is similar to KSP+MCF (62.7ms).
We find that for any node pair, SSSSS finds a path with latency within 1.09× the shortest path, on average.
Furthermore, if we include factors such as buffering at routers, which depends on congestion, we expect to see better latency for SSSSS as it has better congestion guarantees.
There is a trade-off between performance and robustness, and often TE systems that optimize for performance 11Assuming dropped traffic has infinite latency, curves reach a maximum y-value equal to throughput.
tend to overfit and become brittle.
In Fig. 12, we evaluate the robustness of TE approaches to network failures.Here, we fail a unique link every hour and note the impact on performance.
We implement a simple recovery mechanism which re-normalizes path weights to shift traffic from failed paths on to unaffected paths, if available.
The recovery method is fast as it does not need setting up new paths, and it decreases loss due to failure at the cost of congestion.
Usually, this increases throughput, but there are exceptions as illustrated in §B.1.
We see this with R-MCF in Fig. 12.
OOOOOOO knows failures in advance and reacts by setting up globally optimal paths instantaneously.
FFC* is always able to find backup paths as it uses disjoint paths, and thus avoids loss due to failure.
However, these paths are suboptimal for achieving the best throughput as congestion causes packet drops.
SSSSS continues to deliver ∼100% throughput.
Although maximum congestion increases because of recovery, SSSSS remains within 18% of OOOOOOO, on average and within 71% in the worst case.
SSSSS's high resilience can be attributed to the fact that the paths it uses are diverse and have low risk, as we saw earlier in §3.1.
This ensures that, in most cases, SSSSS has sufficient options to re-route traffic and load balance efficiently without overloading any link.
Various operational constraints need to be accounted for while deploying a TE system.
We describe one such constraint-path-split quantization.
So far, our evalu-ation has assumed that traffic could be split in arbitrary proportions.
This is usually not the case, and path weights are quantized.
Most routers support splitting by allowing to specify a certain number, typically up to 64, of next-hop groups [24].
This means that path weights should be multiples of the path-split quantum, 1 64 .
Fig. 11c shows the impact of quantization on performance (at path budget of 4).
We approximate traffic split ratio generated by different TE schemes to be multiples of the path-split quantum using a greedy approach.
SSSSS degrades gracefully when quantization becomes restrictive and performs well for practical settings when path-split quantum ≥ 1 64 .
The evaluation on LBN in the preceding section showed that SSSSS achieves near-optimal performance and robustness for the topology and workload in a large production network.
We also obtained similar results for experiments conducted using data from a major ISP (omitted from paper due to space constraints).
In this section, we show that these performance ( §6.1) and robustness ( §6.2) results generalize to a wider range of scenarios.Methodology.
We evaluate 17 TE algorithms over 262 ISP and inter-DC WAN topologies using Y.
We model a diverse set of operational conditions by varying demands, failures, TM prediction, and path budget.
We present a subset of our experimental data that illustrates our main results over the scenarios described next.Topologies.
We select 28 topologies, shown in §C, from ITZ and other real-world networks, to overlap with ones used to evaluate TE approaches in the literature [24,26].
TM Generation.
We use Y to generate TMs based on the gravity model [40], which assigns a weight w i to each host i and assumes that i → j demand ∝ w i · w j .
We sample w i from a heavy-tailed Pareto distribution obtained by fitting real-world TMs.
We model diurnal and weekly variations by randomly perturbing the Fourier coefficients of the observed time-series and temporal variations at a finer scale by using the Metropolis-Hastings algorithm to sample from a Markov chain on the space of TMs, whose stationary distribution is the gravity model with Pareto-distributed weights described above.
The algorithm updates w i at each time step to model gradual variation over time.
The following experiments scale TMs such that the minimum possible MLU for the first TM is 0.4, which matches the average MLU observed in traditional overprovisioned WANs [22,24].
each pair of hosts, we perform an independent time series prediction where, at each time step t, the demand is predicted using demands observed in [t − k, t).
We optimize the size of the sliding window (k) separately using cross-validation.
Regression and random forest models are trained using the previous k time steps as k features.FFT fit finds a function with a bounded number of nonzero Fourier coefficients, while polynomial fit finds a bounded-degree polynomial function that minimizes the absolute difference between predicted and actual TMs over the past k time steps.
This best-fit function is evaluated in the current time step to yield the predicted demand.
Finally, Y exposes an error parameter to assess the sensitivity of TE algorithms to inaccuracy in prediction.
We start by evaluating basic properties of TE approaches including the effective capacity of the network and the performance ratio with respect to OOOOOOO.Network capacity.
During normal operation, the MLU of a network is usually below 1, meaning there is spare capacity in the network.
Given a routing scheme and a TM, we can define network capacity as the factor by which the TM can be scaled up before it experiences congestion.
This spare capacity could be used to handle unexpected surges in traffic, or to schedule background traffic.
As OOOOOOO minimizes MLU, it has the highest possible network capacity.
Fig. 13a shows network capacity for different TE approaches, normalized with respect to OOOOOOO.
As expected, oblivious routing, which can use up more bandwidth on more links to route the same TM, is unable to admit a significant fraction of TMs that OOOOOOO can handle.
We find that SSSSS has the highest network capacity and is near-optimal owing to efficient load balancing over a diverse set of paths.Performance ratio.
Another way to compare TE approaches is to measure how far they are from OOOOOOO, with respect to minimizing MLU for a given set of TMs.
Here, we follow the metric defined by Applegate and Cohen [3], but use throughput after accounting for congestion loss, if any, instead of using the demand TM to compute congestion and performance ratio.
Fig. 13b remain optimal in 75-80% of the cases but SSSSS has closest to optimal performance ratio ( §C.1).
Similar to §5.3, we systematically inject failures in the network to study the trade-off between performance and robustness.
Fig. 15 shows the distribution of loss in throughput over all possible single link failures.
With base demands, failures are the main cause of loss.
As demand scales up, loss due to congestion becomes significant.
SSSSS performs better in both cases by being more fault tolerant as well as spreading traffic evenly to avoid congestion.
Although TE approaches like FFC* and R-MCF are designed to be fault tolerant, they do not achieve the best throughput.
This is because FFC* relies on disjoint paths to be available to reroute traffic; the number of such paths is limited in real-world topologies.
For instance, GÉANT's topology has nodes with degree 1.
These nodes have a single edge-disjoint path to any other node; the failure of any edge along this path leads to loss of traffic.
Using (p, q) link-switch disjoint paths also doesn't improve the robustness much.
Moreover, FFC* is not congestion-optimal by design and incurs high loss due to congestion as demands increase.
R-MCF relies on using a large number of paths for fault tolerance and applying a budget deteriorates its performance [42].
Typically, SLAs refer to availability of a network in terms of "nines".
This can also be translated in terms of throughput and given a failure characteristic [17,18,33], a network operator could be interested in questions such as "what is the probability that throughput is greater than 99.9%?"
Fig. 14 compares TE approaches on how likely are they to achieve different levels of SLA under various operational conditions.
In addition to the scenario where single link failures can happen with uniform probability under regular load (Fig. 14a), we perform two more experiments where we study robustness under increased load (Fig. 14b), and concurrent failures (Fig. 14c).
We find that SSSSS consistently outperforms other TE approaches.
Oblivious TE is robust under both single and concurrent link failures at base demands, but its resilience deteriorates for increased load.
SSSSS benefits from the robust set of paths selected by oblivious routing, and also load balances efficiently even during increased load.
Thus, SSSSS is highly robust and achieves SLAs with highest probability under diverse operational conditions.
In TE, there is a fundamental trade-off between performance and robustness.
Most systems are designed to optimize for one or the other, but few manage to achieve both.
This challenge is further exacerbated by operational restrictions such as the number of paths, overhead due to churn, quantized splitting ratio imposed by hardware, etc.
This paper presents SSSSS, a new approach that navigates these trade-offs by combining careful path selection with dynamic weight adaptation.
As shown through a detailed evaluation on a production backbone network, SSSSS achieves near-optimal performance in terms of congestion and load balancing metrics, is competitive with shortest-path based approaches in terms of latency, and is also robust, allowing traffic to be re-routed around failures without introducing bottlenecks while respecting operational constraints.
Our large-scale evaluation shows that these performance and robustness guarantees hold across a broader class of networks.
More generally, our experiences designing and implementing SSSSS suggests lessons that are broadly applicable to TE systems including the importance of capacity-aware and globally optimized selection of low-stretch and diverse paths, as well as the consideration of operational constraints when building a practical TE system.
