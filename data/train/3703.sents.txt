Despite years of intensive research, Byzantine fault-tolerant (BFT) systems have not yet been adopted in practice.
This is due to additional cost of BFT in terms of resources, protocol complexity and performance, compared with crash fault-tolerance (CFT).
This overhead of BFT comes from the assumption of a powerful adversary that can fully control not only the Byzantine faulty machines, but at the same time also the message delivery schedule across the entire network, effectively inducing communication asynchrony and partitioning otherwise correct machines at will.
To many practitioners, however, such strong attacks appear irrelevant.
In this paper, we introduce cross fault tolerance or XFT, a novel approach to building reliable and secure distributed systems and apply it to the classical state-machine replication (SMR) problem.
In short, an XFT SMR protocol provides the reliability guarantees of widely used asynchronous CFT SMR protocols such as Paxos and Raft, but also tolerates Byzantine faults in combination with network asynchrony, as long as a majority of replicas are correct and communicate synchronously.
This allows the development of XFT systems at the price of CFT (already paid for in practice), yet with strictly stronger resilience than CFT-sometimes even stronger than BFT itself.
As a showcase for XFT, we present XPaxos, the first XFT SMR protocol, and deploy it in a geo-replicated setting.
Although it offers much stronger resilience than CFT SMR at no extra resource cost, the performance of XPaxos matches that of the state-of-the-art CFT protocols .
Tolerance to any kind of service disruption, whether caused by a simple hardware fault or by a large-scale * Work done while being a PhD student at EURECOM.
disaster, is key for the survival of modern distributed systems.
Cloud-scale applications must be inherently resilient, as any outage has direct implications on the business behind them [24].
Modern production systems (e.g., [13,8]) increase the number of nines of reliability 1 by employing sophisticated distributed protocols that tolerate crash machine faults as well as network faults, such as network partitions or asynchrony, which reflect the inability of otherwise correct machines to communicate among each other in a timely manner.
At the heart of these systems typically lies a crash fault-tolerant (CFT) consensus-based state-machine replication (SMR) primitive [36,10].
These systems cannot deal with non-crash (or Byzantine [29]) faults, which include not only malicious, adversarial behavior, but also arise from errors in the hardware, stale or corrupted data from storage systems, memory errors caused by physical effects, bugs in software, hardware faults due to ever smaller circuits, and human mistakes that cause state corruptions and data loss.
However, such problems do occur in practice -each of these faults has a public record of taking down major production systems and corrupting their service [14,4].
Despite more than 30 years of intensive research since the seminal work of Lamport, Shostak and Pease [29], no practical answer to tolerating non-crash faults has emerged so far.
In particular, asynchronous Byzantine fault-tolerance (BFT), which promises to resolve this problem [9], has not lived up to this expectation, largely because of its extra cost compared with CFT.
Namely, asynchronous (that is, "eventually synchronous" [18]) BFT SMR must use at least 3t + 1 replicas to tolerate t non-crash faults [7] instead of only 2t + 1 replicas for CFT, as used by Paxos [27] or Raft [34], for example.The overhead of asynchronous BFT is due to the extraordinary power given to the adversary, which may control both the Byzantine faulty machines and the entire network in a coordinated way.
In particular, the classical BFT adversary can partition any number of otherwise correct machines at will.
In line with observations by practitioners [25], we claim that this adversary model is actually too strong for the phenomena observed in deployed systems.
For instance, accidental non-crash faults usually do not lead to network partitions.
Even malicious non-crash faults rarely cause the whole network to break down in wide-area networks and geo-replicated systems.
The proverbial all-powerful attacker as a common source behind those faults is a popular and powerful simplification used for the design phase, but it has not seen equivalent proliferation in practice.In this paper, we introduce XFT (short for cross fault tolerance), a novel approach to building efficient resilient distributed systems that tolerate both non-crash (Byzantine) faults and network faults (asynchrony).
In short, XFT allows building resilient systems that• do not use extra resources (replicas) compared with asynchronous CFT;• preserve all reliability guarantees of asynchronous CFT (that is, in the absence of Byzantine faults); and• provide correct service (i.e., safety and liveness [2]) even when Byzantine faults do occur, as long as a majority of the replicas are correct and can communicate with each other synchronously (that is, when a minority of the replicas are Byzantine-faulty or partitioned because of a network fault).
In particular, we envision XFT for wide-area or georeplicated systems [13], as well as for any other deployment where an adversary cannot easily coordinate enough network partitions and Byzantine-faulty machine actions at the same time.As a showcase for XFT, we present XPaxos, the first state-machine replication protocol in the XFT model.
XPaxos tolerates faults beyond crashes in an efficient and practical way, achieving much greater coverage of realistic failure scenarios than the state-of-the-art CFT SMR protocols, such as Paxos or Raft.
This comes without resource overhead as XPaxos uses 2t + 1 replicas.
To validate the performance of XPaxos, we deployed it in a geo-replicated setting across Amazon EC2 datacenters worldwide.
In particular, we integrated XPaxos within Apache ZooKeeper, a prominent and widely used coordination service for cloud systems [19].
Our evaluation on EC2 shows that XPaxos performs almost as well in terms of throughput and latency as a WAN-optimized variant of Paxos, and significantly better than the best available BFT protocols.
In our evaluation, XPaxos even outperforms the native CFT SMR protocol built into ZooKeeper [20].
Finally, and perhaps surprisingly, we show that XFT can offer strictly stronger reliability guarantees than state-of-the-art BFT, for instance under the assumption that machine faults and network faults occur as independent and identically distributed random variables, for certain probabilities.
To this end, we calculate the number of nines of consistency (system safety) and availability (system liveness) of resource-optimal CFT, BFT and XFT (e.g., XPaxos) protocols.
Whereas XFT always provides strictly stronger consistency and availability guarantees than CFT and always strictly stronger availability guarantees than BFT, our reliability analysis shows that, in some cases, XFT also provides strictly stronger consistency guarantees than BFT.The remainder of this paper is organized as follows.
In Section 2, we define the system model, which is then followed by the definition of the XFT model in Section 3.
In Section 4 and Section 5, we present XPaxos and its evaluation in the geo-replicated context, respectively.
Section 6 provides simplified reliability analysis comparing XFT with CFT and BFT.
We overview related work and conclude in Section 7.
For space reasons, the full correctness proof of XPaxos is given in [31].
Machines.
We consider a message-passing distributed system containing a set Π of n = |Π| machines, also called replicas.
Additionally, there is a separate set C of client machines.Clients and replicas may suffer from Byzantine faults: we distinguish between crash faults, where a machine simply stops all computation and communication, and non-crash faults, where a machine acts arbitrarily, but cannot break cryptographic primitives we use (cryptographic hashes, MACs, message digests and digital signatures).
A machine that is not faulty is called correct.
We say a machine is benign if the machine is correct or crash-faulty.
We further denote the number of replica faults at a given moment s by• t c (s): the number of crash-faulty replicas, and• t nc (s): the number of non-crash-faulty replicas.Network.
Each pair of replicas is connected with reliable point-to-point bi-directional communication channels.
In addition, each client can communicate with any replica.The system can be asynchronous in the sense that machines may not be able to exchange messages and obtain responses to their requests in time.
In other words, network faults are possible; we define a network fault as the inability of some correct replicas to communicate with each other in a timely manner, that is, when a message exchanged between two correct replicas cannot be delivered and processed within delay ∆, known to all replicas.
Note that ∆ is a deployment specific parameter: we discuss practical choices for ∆ in the context of our georeplicated setting in Section 5.
Finally, we assume an eventually synchronous system in which, eventually, network faults do not occur [18].
Note that we model an excessive processing delay as a network problem and not as an issue related to a machine fault.
This choice is made consciously, rooted in the experience that for the general class of protocols considered in this work, a long local processing time is never an issue on correct machines compared with network delays.To help quantify the number of network faults, we first give the definition of partitioned replica.Definition 1 (Partitioned replica).
Replica p is partitioned if p is not in the largest subset of replicas, in which every pair of replicas can communicate among each other within delay ∆.
If there is more than one subset with the maximum size, only one of them is recognized as the largest subset.
For example in Figure 1, the number of partitioned replicas is 3, counting either the group of p 1 , p 4 and p 5 or that of p 2 , p 3 and p 5 .
The number of partitioned replicas can be as much as n − 1, which means that no two replicas can communicate with each other within delay ∆.
We say replica p is synchronous if p is not partitioned.
We now quantify network faults at a given moment s as Problem.
In this paper, we focus on the deterministic state-machine replication problem (SMR) [36].
In short, in SMR clients invoke requests, which are then committed by replicas.
SMR ensures• safety, or consistency, by (a) enforcing total order across committed client's requests across all correct replicas; and by (b) enforcing validity, i.e., that a correct replica commits a request only if it was previously invoked by a client;• liveness, or availability, by eventually committing a request by a correct client at all correct replicas and returning an application-level reply to the client.
This section introduces the XFT model and relates it to the established crash-fault tolerance (CFT) and Byzantine-fault tolerance (BFT) models.
Classical CFT and BFT explicitly model machine faults only.
These are then combined with an orthogonal network fault model, either the synchronous model (where network faults in our sense are ruled out), or the asynchronous model (which includes any number of network faults).
Hence, previous work can be classified into four categories: synchronous CFT [16,36], asynchronous CFT [36,27,33], synchronous BFT [29,17,6], and asynchronous BFT [9,3].
XFT, in contrast, redefines the boundaries between machine and network fault dimensions: XFT allows the design of reliable protocols that tolerate crash machine faults regardless of the number of network faults and that, at the same time, tolerate non-crash machine faults when the number of machines that are either faulty or partitioned is within a threshold.To formalize XFT, we first define anarchy, a very severe system condition with actual non-crash machine (replica) faults and plenty of faults of different kinds, as follows:Definition 2 (Anarchy).
The system is in anarchy at a given moment s iff t nc (s) > 0 and t c (s)+t nc (s)+t p (s) > t.Here, t is the threshold of replica faults, such that t ≤ n−1 2 .
In other words, in anarchy, some replica is noncrash-faulty, and there is no correct and synchronous majority of replicas.
Armed with the definition of anarchy, we can define XFT protocols for an arbitrary distributed computing problem in function of its safety property [2].
Definition 3 (XFT protocol).
Protocol P is an XFT protocol if P satisfies safety in all executions in which the system is never in anarchy.Liveness of an XFT protocol will typically depend on a problem and implementation.
For instance, for deterministic SMR we consider in this paper, our XPaxos USENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 487Maximum number of each type of replica faults non-crash faults crash faults partitioned replicas Asynchronous CFT (e.g., Paxos [28]) consistency 0 n n − 1availability 0 n−1 2 (combined)Asynchronous BFT (e.g., PBFT [9]) consistency n−1 3 n n − 1 availability n−1 3 (combined)(Authenticated) Synchronous BFT (e.g., [29]) consistency n − 1 n 0 Table 1: The maximum numbers of each type of fault tolerated by representative SMR protocols.
Note that XFT provides consistency in two modes, depending on the occurrence of non-crash faults.availability n − 1 (combined) 0 XFT (e.g., XPaxos) consistency 0 n n − 1 n−1 2 (combined) availability n−1 2 (combined)protocol eventually satisfies liveness, provided a majority of replicas is correct and synchronous.
This can be shown optimal.
Table 1 illustrates differences between XFT and CFT/BFT in terms of their consistency and availability guarantees for SMR.
State-of-the-art asynchronous CFT protocols [28,34] guarantee consistency despite any number of crashfaulty replicas and any number of partitioned replicas.
They also guarantee availability whenever a majority of replicas (t ≤ n−1 2 ) are correct and synchronous.
As soon as a single machine is non-crash-faulty, CFT protocols guarantee neither consistency nor availability.
Optimal asynchronous BFT protocols [9,22,3] guarantee consistency despite any number of crash-faulty or partitioned replicas, with at most t = n−1 3 non-crashfaulty replicas.
They also guarantee availability with up to n−1 3 combined faults, i.e., whenever more than twothirds of replicas are correct and not partitioned.
Note that BFT availability might be weaker than that of CFT in the absence of non-crash faults -unlike CFT, BFT does not guarantee availability when the sum of crashfaulty and partitioned replicas is in the range [n/3, n/2).
Synchronous BFT protocols (e.g., [29]) do not consider the existence of correct, but partitioned replicas.
This makes for a very strong assumption -and helps synchronous BFT protocols that use digital signatures for message authentication (so called authenticated protocols) to tolerate up to n − 1 non-crash-faulty replicas.In contrast, XFT protocols with optimal resilience, such as our XPaxos, guarantee consistency in two modes: (i) without non-crash faults, despite any number of crash-faulty and partitioned replicas (i.e., just like CFT), and (ii) with non-crash faults, whenever a majority of replicas are correct and not partitioned, i.e., provided the sum of all kinds of faults (machine or network faults) does not exceed n−1 2 .
Similarly, it also guarantees availability whenever a majority of replicas are correct and not partitioned.It may be tempting to view XFT as some sort of a combination of the asynchronous CFT and synchronous BFT models.
However, this is misleading, as even with actual non-crash faults, XFT is incomparable to authenticated synchronous BFT.
Specifically, authenticated synchronous BFT protocols, such as the seminal Byzantine Generals protocol [29], may violate consistency with a single partitioned replica.
For instance, with n = 5 replicas and an execution in which three replicas are correct and synchronous, one replica is correct but partitioned and one replica is non-crash-faulty, the XFT model mandates that the consistency be preserved, whereas the Byzantine Generals protocol may violate consistency.
2 Furthermore, from Table 1, it is evident that XFT offers strictly stronger guarantees than asynchronous CFT, for both availability and consistency.
XFT also offers strictly stronger availability guarantees than asynchronous BFT.
Finally, the consistency guarantees of XFT are incomparable to those of asynchronous BFT.
On the one hand, outside anarchy, XFT is consistent with the number of non-crash faults in the range [n/3, n/2), whereas asynchronous BFT is not.
On the other hand, unlike XFT, asynchronous BFT is consistent in anarchy provided the number of non-crash faults is less than n/3.
We discuss these points further in Section 6, where we also quantify the reliability comparison between XFT and asynchronous CFT/BFT assuming the special case of independent faults.
The intuition behind XFT starts from the assumption that "extremely bad" system conditions, such as anarchy, are very rare, and that providing consistency guarantees in anarchy might not be worth paying the asynchronous BFT premium.In practice, this assumption is plausible in many deployments.
We envision XFT for use cases in which an adversary cannot easily coordinate enough network partitions and non-crash-faulty machine actions at the same time.
Some interesting candidate use cases include:• Tolerating "accidental" non-crash faults.
In systems which are not susceptible to malicious behavior and deliberate attacks, XFT can be used to protect against "accidental" non-crash faults, which can be assumed to be largely independent of network faults.
In such cases, XFT could be used to harden CFT systems without considerable overhead of BFT.
• Wide-area networks and geo-replicated systems.XFT may reveal useful even in cases where the system is susceptible to malicious non-crash faults, as long as it may be difficult or expensive for an adversary to coordinate an attack to compromise Byzantine machines and partition sufficiently many replicas at the same time.
Particularly interesting for XFT are WAN and geo-replicated systems which often enjoy redundant communication paths and typically have a smaller surface for network-level DoS attacks (e.g., no multicast storms and flooding).
• Blockchain.
A special case of geo-replicated systems, interesting to XFT, are blockchain systems.
In a typical blockchain system, such as Bitcoin [32], participants may be financially motivated to act maliciously, yet may lack the means and capabilities to compromise the communication among (a large number of) correct participants.
In this context, XFT is particularly interesting for so-called permissioned blockchains, which are based on statemachine replication rather than on Bitcoin-style proof-of-work [40].4 XPaxos Protocol XPaxos is a novel state-machine replication (SMR) protocol designed specifically in the XFT model.
XPaxos specifically targets good performance in geo-replicated settings, which are characterized by the network being the bottleneck, with high link latency and relatively low, heterogeneous link bandwidth.
In a nutshell, XPaxos consists of three main components:• A common-case protocol, which replicates and totally orders requests across replicas.
This has, roughly speaking, the message pattern and complexity of communication among replicas of stateof-the-art CFT protocols (e.g., Phase 2 of Paxos), hardened by the use of digital signatures.
• A novel view-change protocol, in which the information is transferred from one view (system configuration) to another in a decentralized, leaderless fashion.
• A fault detection (FD) mechanism, which can help detect, outside anarchy, non-crash faults that would leave the system in an inconsistent state in anarchy.The goal of the FD mechanism is to minimize the impact of long-lived non-crash faults (in particular "data loss" faults) in the system and to help detect them before they coincide with a sufficient number of crash faults and network faults to push the system into anarchy.XPaxos is orchestrated in a sequence of views [9].
The central idea in XPaxos is that, during common-case operation in a given view, XPaxos synchronously replicates clients' requests to only t +1 replicas, which are the members of a synchronous group (out of n = 2t + 1 replicas in total).
Each view number i uniquely determines the synchronous group, sg i , using a mapping known to all replicas.
Every synchronous group consists of one primary and t followers, which are jointly called active replicas.
The remaining t replicas in a given view are called passive replicas; optionally, passive replicas learn the order from the active replicas using the lazy replication approach [26].
A view is not changed unless there is a machine or network fault within the synchronous group.In the common case (Section 4.2), the clients send digitally signed requests to the primary, which are then replicated across t + 1 active replicas.
These t + 1 replicas digitally sign and locally log the proofs for all replicated requests to their commit logs.
Commit logs then serve as the basis for maintaining consistency in view changes.The view change of XPaxos (Section 4.3) reconfigures the entire synchronous group, not only the leader.
All t + 1 active replicas of the new synchronous group sg i+1 try to transfer the state from the preceding views to view i + 1.
This decentralized approach to view change stands in sharp contrast to the classical reconfiguration/view-change in CFT and BFT protocols (e.g., [27,9]), in which only a single replica (the primary) leads the view change and transfers the state from previous views.
This difference is crucial to maintaining consistency (i.e., total order) across XPaxos views in the presence of non-crash faults (but in the absence of full anarchy).
This novel and decentralized view-change scheme of XPaxos guarantees that even in the presence of non-crash faults, but outside anarchy, at least one correct replica from the new synchronous group sg i+1 will be able to transfer the correct state from previous views, as it will be able to contact some correct replica from any old synchronous group.Finally, the main idea behind the FD scheme of XPaxos is the following.
In view change, a non-crashfaulty replica (of an old synchronous group) might not transfer its latest state to a correct replica in the new synchronous group.
This "data loss" fault is dangerous, as it may violate consistency when the system is in anarchy.
However, such a fault can be detected using digital signatures from the commit log of some correct replicas (from an old synchronous group), provided that these correct replicas can communicate synchronously with correct replicas from the new synchronous group.
In a sense, with XPaxos FD, a critical non-crash machine fault must occur for the first time together with sufficiently many crash or partitioned machines (i.e., in anarchy) to violate consistency.In the following, we explain the core of XPaxos for the common case (Section 4.2), view-change (Section 4.3) and fault detection (Section 4.4) components.
We discuss XPaxos optimizations in Section 4.5 and give XPaxos correctness arguments in Section 4.6.
Because of space limitations, the complete pseudocode and correctness proof have been included in [31].
Figure 2 shows the common-case message patterns of XPaxos for the general case (t ≥ 2) and for the special case t = 1.
XPaxos is specifically optimized for the case where t = 1, as in this case, there are only two active replicas in each view and the protocol is very efficient.
The special case t = 1 is also highly relevant in practice (see e.g., Spanner [13]).
In the following, we first explain XPaxos in the general case, and then focus on the t = 1 special case.
Notation.
We denote the digest of a message m by D(m), whereas m σ p denotes a message that contains both D(m) signed by the private key of machine p and m. For signature verification, we assume that all machines have public keys of all other processes.
The common-case message pattern of XPaxos is shown in Figure 2a.
More specifically, upon receiving a signed request req = REPLICATE, op, ts c , c σ c from client c (where op is the client's operation and ts c is the client's timestamp), the primary (say s 0 ) (1) increments sequence number sn and assigns sn to req, (2) signs a message prep = PREPARE, D(req), sn, i σ s 0 and logs req, prep into its prepare log PrepareLog 0 [sn] (we say s 0 prepares req), and (3) forwards req, prep to all other active replicas (i.e, the t followers).
Each follower s j (1 ≤ j ≤ t) verifies the primary's and client's signatures, checks whether its local sequence number equals sn − 1, and logs req, prep into its prepare log PrepareLog j [sn].
Then, s j updates its local sequence number to sn, signs the digest of the request req, the sequence number sn and the view number i, and sends COMMIT, D(req), sn, i σ s j to all active replicas.Upon receiving t signed COMMIT messages -one from each follower -such that a matching entry is in the prepare log, an active replica s k (0 ≤ k ≤ t) logs prep and the t signed COMMIT messages into its commit log CommitLog s k [sn].
We say s k commits req when this oc-curs.
Finally, s k executes req and sends the authenticated reply to the client (followers may only send the digest of the reply).
The client commits the request when it receives matching REPLY messages from all t + 1 active replicas.A client that times out without committing the requests broadcasts the request to all active replicas.
Active replicas then forward such a request to the primary and trigger a retransmission timer, within which a correct active replica expects the client's request to be committed.
When t = 1, the XPaxos common case simplifies to involving only 2 messages between 2 active replicas (see Figure 2b).
Upon receiving a signed request req = REPLICATE, op, ts c , c σ c from client c, the primary (s 0 ) increments the sequence number sn, signs sn along the digest of req and view number i in message m 0 = COMMIT, D(req), sn, i σ s 0 , stores req, m 0 into its prepare log (PrepareLog s 0 [sn] = req, m 0 ), and sends the message req, m 0 to the follower s 1 .
On receiving req, m 0 , the follower s 1 verifies the client's and primary's signatures, and checks whether its local sequence number equals sn − 1.
If so, the follower updates its local sequence number to sn, executes the request producing reply R(req), and signs message m 1 ; m 1 is similar to m 0 , but also includes the client's timestamp and the digest of the reply: m 1 = COMMIT, D(req), sn, i, req.ts c , D(R(req)) σ s 1 .
The follower then saves the tuple req, m 0 , m 1 to its commit log (CommitLog s 1 [sn] = req, m 0 , m 1 ) and sends m 1 to the primary.The primary, on receiving a valid COMMIT message from the follower (with a matching entry in its prepare log), executes the request, compares the reply R(req) with the follower's digest contained in m 1 , and stores req, m 0 , m 1 in its commit log.
Finally, it returns an authenticated reply containing m 1 to c, which commits the request if all digests and the follower's signature match.
Intuition.
The ordered requests in commit logs of correct replicas are the key to enforcing consistency (total order) in XPaxos.
To illustrate an XPaxos view change, consider synchronous groups sg i and sg i+1 of views i and i + 1, respectively, each containing t + 1 replicas.
Note that proofs of requests committed in sg i might have been logged by only one correct replica in sg i .
Nevertheless, the XPaxos view change must ensure that (outside anarchy) these proofs are transferred to the new view i+1.
techniques [9,22,12] where the entire view-change is led by a single replica, usually the primary of the new view.
Instead, in XPaxos view change, every active replica in sg i+1 retrieves information about requests committed in preceding views.
Intuitively, with correct majority of correct and synchronous replicas, at least one correct and synchronous replica from sg i+1 will contact (at least one) correct and synchronous replica from sg i and transfer the latest correct commit log to the new view i + 1.
In the following, we first describe how we choose active replicas for each view.
Then, we explain how view changes are initiated, and, finally, how view changes are performed.
To choose active replicas for view i, we may enumerate all sets containing t + 1 replicas (i.e., 2t+1 t+1 sets) which then alternate as synchronous groups across views in a round-robin fashion.
In addition, each synchronous group uniquely determines the primary.
We assume that the mapping from view numbers to synchronous groups is known to all replicas (see e.g., Table 2).
The above simple scheme works well for small number of replicas (e.g., t = 1 and t = 2).
For a large number of replicas, the combinatorial number of synchronous groups may be inefficient.
To this end, XPaxos can be modified to rotate only the leader, which may then resort to deterministic verifiable pseudorandom selection of the set of f followers in each view.
The exact details of such a scheme would, however, exceed the scope of this paper.
If a synchronous group in view i (denoted by sg i ) does not make progress, XPaxos performs a view change.
Only an active replica of sg i may initiate a view change.An active replica s j ∈ sg i initiates a view change if (i) s j receives a message from another active replica that does not conform to the protocol (e.g., an invalid signature), (ii) the retransmission timer at s j expires, (iii) s j does not complete a view change to view i in a timely manner, or (iv) s j receives a valid SUSPECT message for view i from another replica in sg i .
Upon a view-change initiation, s j stops participating in the current view and sends SUSPECT, i, s j σ s j to all other replicas.
Upon receiving a SUSPECT message from an active replica in view i (see the message pattern in Figure 3), replica s j stops processing messages of view i and sends m = VIEW-CHANGE, i + 1, s j ,CommitLog s j σ s j to the t + 1 active replicas of sg i+1 .
A VIEW-CHANGE message contains the commit log CommitLog s j of s j .
Commit logs might be empty (e.g., if s j was passive).
Note that XPaxos requires all active replicas in the new view to collect the most recent state and its proof (i.e., VIEW-CHANGE messages), rather than only the new primary.
Otherwise, a faulty new primary could, even outside anarchy, purposely omit VIEW-CHANGE messages that contain the most recent state.
Active replica s j in view i + 1 waits for at least n − t VIEW-CHANGE messages from all, but also waits for 2∆ time, trying to collect as many messages as possible.Upon completion of the above protocol, each active replica s j ∈ sg i+1 inserts all VIEW-CHANGE messages it has received into set VCSet i+1 s j .
Then s j sends VC-FINAL, i + 1, s j ,VCSet i+1 s j σ s j to every active replica in view i+1.
This serves to exchange the received VIEW-CHANGE messages among active replicas.
Every active replica s j ∈ sg i+1 must receive VC-FINAL messages from all active replicas in sg i+1 , after which s j extends the value VCSet i+1 s j by combining VCSet i+1 * sets piggybacked in VC-FINAL messages.
Then, for each sequence number sn, an active replica selects the commit log with the highest view number in all VIEW-CHANGE messages, to confirm the committed request at sn.Afterwards, to prepare and commit the selected requests in view i + 1, the new primary ps i+1 sends NEW-VIEW, i + 1, PrepareLog σ ps i+1 to every active replica in sg i+1 , where the array PrepareLog contains the prepare logs generated in view i + 1 for each selected request.
Upon receiving a NEW-VIEW message, every active replica s j ∈ sg i+1 processes the prepare logs in PrepareLog as described in the common case (see Section 4.2).
Finally, every active replica s j ∈ sg i+1 makes sure that all selected requests in PrepareLog are committed in view i + 1.
When this condition is satisfied, XPaxos can start processing new requests.
XPaxos does not guarantee consistency in anarchy.
Hence, non-crash faults could violate XPaxos consistency in the long run, if they persist long enough to eventually coincide with enough crash or network faults.
To cope with long-lived faults, we propose (an otherwise optional) Fault Detection (FD) mechanism for XPaxos.Roughly speaking, FD guarantees the following property: if a machine p suffers a non-crash fault outside anarchy in a way that would cause inconsistency in anarchy, then XPaxos FD detects p as faulty (outside anarchy).
In other words, any potentially fatal fault that occurs outside anarchy would be detected by XPaxos FD.Here, we sketch how FD works in the case t = 1 (see [31] for details), focusing on detecting a specific noncrash fault that may render XPaxos inconsistent in anarchy -a data loss fault by which a non-crash-faulty replica loses some of its commit log prior to a view change.
Intuitively, data loss faults are dangerous as they cannot be prevented by the straightforward use of digital signatures.Our FD mechanism entails modifying the XPaxos view change as follows: in addition to exchanging their commit logs, replicas also exchange their prepare logs.
Notice that in the case t = 1 only the primary maintains a prepare log (see Section 4.2).
In the new view, the primary prepares and the follower commits all requests contained in transferred commit and prepare logs.With the above modification, to violate consistency, a faulty primary (of preceding view i) would need to exhibit a data loss fault in both its commit log and its prepare log.
However, such a data loss fault in the primary's prepare log would be detected, outside anarchy, because (i) the (correct) follower of view i would reply in the view change and (ii) an entry in the primary's prepare log causally precedes the respective entry in the follower's commit log.
By simply verifying the signatures in the follower's commit log, the fault of a primary is detected.
Conversely, a data loss fault in the commit log of the follower of view i is detected outside anarchy by verifying the signatures in the commit log of the primary of view i. Although the common-case and view-change protocols described above are sufficient to guarantee correctness, we applied several standard performance optimizations to XPaxos.
These include checkpointing and lazy replication [26] to passive replicas (to help shorten the state transfer during view change) as well as batching and pipelining (to improve the throughput).
Below, we provide a brief overview of these optimizations; the details can be found in [31].
Checkpointing.
Similarly to other replication protocols, XPaxos includes a checkpointing mechanism that speeds up view changes and allows garbage collection (by shortening commit logs).
To that end, every CHK requests (where CHK is a configurable parameter) XPaxos checkpoints the state within the synchronous group.
Then the proof of checkpoint is lazily propagated to passive replicas.
Lazy replication.
To speed up the state transfer in view change, every follower in the synchronous group lazily propagates the commit log to one passive replica.
With lazy replication, a new active replica, which might be the passive replica in the preceding view, may only need to retrieve the missing state from others during a view change.
Batching and pipelining.
To improve the throughput of cryptographic operations, the primary batches several requests when preparing.
The primary waits for B requests, then signs the batched request and sends it to every follower.
If primary receives less than B requests within a time limit, the primary batches all requests it has received.
Consistency (Total Order).
XPaxos enforces the following invariant, which is key to total order.
Lemma 1.
Outside anarchy, if a benign client c commits a request req with sequence number sn in view i, and a benign replica s k commits the request req with sn in view i > i, then req = req .
A benign client c commits request req with sequence number sn in view i only after c has received matching replies from t + 1 active replicas in sg i .
This implies that every benign replica in sg i stores req into its commit log under sequence number sn.
In the following, we focus on the special case where: i = i + 1.
This serves as the base step for the proof of Lemma 1 by induction across views which we give in [31].
Recall that, in view i = i + 1, all (benign) replicas from sg i+1 wait for n − t = t + 1 VIEW-CHANGE messages containing commit logs transferred from other replicas, as well as for the timer set to 2∆ to expire.
Then, replicas in sg i+1 exchange this information within VC-FINAL messages.
Note that, outside anarchy, there exists at least one correct and synchronous replica in sg i+1 , say s j .
Hence, a benign replica s k that commits req in view i + 1 under sequence number sn must have had received VC-FINAL from s j .
In turn, s j waited for t + 1 VIEW-CHANGE messages (and timer 2∆), so it received a VIEW-CHANGE message from some correct and synchronous replica s x ∈ sg i (such a replica exists in sg i as at most t replicas in sg i are non-crash-faulty or partitioned).
As s x stored req under sn in its commit log in view i, it forwards this information to s j in a VIEW-CHANGE message, and s j forwards this information to s k within a VC-FINAL.
Hence req = req follows.
Availability.
XPaxos availability is guaranteed if the synchronous group contains only correct and synchronous replicas.
With eventual synchrony, we can assume that, eventually, there will be no network faults.
In addition, with all combinations of t + 1 replicas rotating in the role of active replicas, XPaxos guarantees that, eventually, view change in XPaxos will complete with t + 1 correct and synchronous active replicas.
In this section, we evaluate the performance of XPaxos and compare it to that of Zyzzyva [22], PBFT [9] and a WAN-optimized version of Paxos [27], using the Amazon EC2 worldwide cloud platform.
We chose georeplicated, WAN settings as we believe that these are a better fit for protocols that tolerate Byzantine faults, including XFT and BFT.
Indeed, in WAN settings (i) there is no single point of failure such as a switch interconnecting machines, (ii) there are no correlated failures due to, e.g., a power-outage, a storm, or other natural disasters, and (iii) it is difficult for the adversary to flood the network, correlating network and non-crash faults (the last point is relevant for XFT).
In the remainder of this section, we first present the experimental setup (Section 5.1), and then evaluate the performance (throughput and latency) in the fault-free scenario (Section 5.2) as well as under faults (Section 5.3).
Finally, we perform a performance comparison using a real application, the ZooKeeper coordination service [19] (Section 5.4), by comparing native ZooKeeper to ZooKeeper variants that use the four replication protocols mentioned above.
In a practical deployment of XPaxos, a critical parameter is the value of timeout ∆, i.e., the upper bound on the communication delay between any two correct machines.
If the round-trip time (RTT) between two correct machines takes more than 2∆, we declare a network fault (see Section 2).
Notably, ∆ is vital to the XPaxos viewchange (Section 4.3).
To understand the value of ∆ in our geo-replicated context, we ran a 3-month experiment during which we continuously measured the round-trip latency across six Amazon EC2 datacenters worldwide using TCP ping Table 3: Round-trip latency of TCP ping (hping3) across Amazon EC2 datacenters, collected during three months.
The latencies are given in milliseconds, in the format: average / 99.99% / 99.999% / maximum.
(hping3).
We used the least expensive EC2 micro instances, which arguably have the highest probability of experiencing variable latency due to virtualization.
Each instance was pinging all other instances every 100 ms.The results of this experiment are summarized in Ta- ble 3.
While we detected network faults lasting up to 3 min, our experiment showed that the round-trip latency between any two datacenters was less than 2.5 sec 99.99% of the time.
Therefore, we adopted the value of ∆ = 2.5/2 = 1.25 sec.
We compare XPaxos with three protocols whose common-case message patterns when t = 1 are shown in Figure 4.
The first two are BFT protocols, namely (a speculative variant of) PBFT [9] and Zyzzyva [22], and require 3t +1 replicas to tolerate t faults.
We chose PBFT because it is possible to derive a speculative variant of the protocol that relies on a 2-phase common-case commit protocol across only 2t + 1 replicas (Figure 4a; see also [9]).
In this PBFT variant, the remaining t replicas are not involved in the common case, which is more efficient in a geo-replicated settings.
We chose Zyzzyva because it is the fastest BFT protocol that involves all replicas in the common case (Figure 4b).
The third protocol we compare against is a very efficient WAN-optimized variant of crash-tolerant Paxos inspired by [5,23,13].
We have chosen the variant of Paxos that exhibits the fastest write pattern (Figure 4c).
This variant requires 2t + 1 replicas to tolerate t faults, but involves t + 1 replicas in the common case, i.e., just like XPaxos.To provide a fair comparison, all protocols rely on the same Java code base and use batching, with the batch size set to 20.
We rely on HMAC-SHA1 to compute MACs and RSA1024 to sign and verify signatures computed using the Crypto++ [1] library that we interface with the various protocols using JNI.
We run the experiments on the Amazon EC2 platform which comprises widely distributed datacenters, interconnected by the Internet.
Communications between datacenters have a low bandwidth and a high latency.
We run the experiments on mid-range virtual machines that In the case t = 1, Table 4 gives the deployment of the different replicas at different datacenters, for each protocol analyzed.
Clients are always located in the same datacenter as the (initial) primary to better emulate what is done in modern geo-replicated systems where clients are served by the closest datacenter [37,13].
3 To stress the protocols, we run a microbenchmark that is similar to the one used in [9,22].
In this microbenchmark, each server replicates a null service (this means that there is no execution of requests).
Moreover, clients issue requests in closed-loop: a client waits for a reply to its current request before issuing a new request.
The benchmark allows both the request size and the reply size to be varied.
For space limitations, we only report results for two request sizes (1kB, 4kB) and one reply size (0kB).
We refer to these microbenchmarks as 1/0 and 4/0 benchmarks, respectively.
We first compare the performance of protocols when t = 1 in replica configurations as shown in Table 4, using the 1/0 and 4/0 microbenchmarks.
The results are shown in Figures 5a and 5b.
In each graph, the X-axis shows the throughput (in kops/sec), and Y-axis the latency (in ms).
As we can see, in both benchmarks, XPaxos achieves a significantly better performance than PBFT and Zyzzyva.
This is because, in a worldwide cloud environment, the network is the bottleneck and the message patterns of BFT protocols, namely PBFT and Zyzzyva, tend to be expensive.
Compared with PBFT, the simpler message pattern of XPaxos allows better throughput.
Compared with Zyzzyva, XPaxos puts less stress on the leader and replicates requests in the common case across 3 times fewer replicas than Zyzzyva (i.e., across t followers vs. across all other 3t replicas).
Moreover, the performance of XPaxos is very close to that of Paxos.
Both Paxos and XPaxos implement a round-trip across two replicas when t = 1, which renders them very efficient.
Next, to assess the fault scalability of XPaxos, we ran the 1/0 micro-benchmark in configurations that tolerate two faults (t = 2).
We use the following EC2 datacenters for this experiment: CA (California), OR (Oregon), VA (Virginia), JP (Tokyo), EU (Ireland), AU (Sydney) and SG (Singapore).
We place Paxos and XPaxos active replicas in the first t + 1 datacenters, and their passive replicas in the next t datacenters.
PBFT uses the first 2t + 1 datacenters for active replicas and the last t for passive replicas.
Finally, Zyzzyva uses all replicas as active replicas.We observe that XPaxos again clearly outperforms PBFT and Zyzzyva and achieves a performance very close to that of Paxos.
Moreover, unlike PBFT and Zyzzyva, Paxos and XPaxos only suffer a moderate performance decrease with respect to the t = 1 case.
In this section, we analyze the behavior of XPaxos under faults.
We run the 1/0 micro-benchmark on three replicas (CA, VA, JP) to tolerate one fault (see also Table 4).
The experiment starts with CA and VA as active replicas, and with 2500 clients in CA.
At time 180 sec, we crash the follower, VA.
At time 300 sec, we crash the CA replica.
At time 420 sec, we crash the third replica, JP.
Each replica recovers 20 sec after having crashed.
Moreover, the timeout 2∆ (used during state transfer in view change, Section 4.3) is set to 2.5 sec (see Section 5.1.1).
We show the throughput of XPaxos in function of time in Fig- ure 6, which also indicates the active replicas for each view.
We observe that after each crash, the system performs a view change that lasts less than 10 sec, which is very reasonable in a geo-distributed setting.
This fast execution of the view-change subprotocol is a consequence of lazy replication in XPaxos that keeps passive replicas updated.
We also observe that the throughput of XPaxos changes with the views.
This is because the latencies between the primary and the follower and between the pri-mary and clients vary from view to view.
To assess the impact of our work on real-life applications, we measured the performance achieved when replicating the ZooKeeper coordination service [19] using all protocols considered in this study: Zyzzyva, PBFT, Paxos and XPaxos.
We also compare with the native ZooKeeper performance, when the system is replicated using the built-in Zab protocol [20].
This protocol is crash-resilient and requires 2t + 1 replicas to tolerate t faults.We used the ZooKeeper 3.4.6 codebase.
The integration of the various protocols inside ZooKeeper was carried out by replacing the Zab protocol.
For fair comparison to native ZooKeeper, we made a minor modification to native ZooKeeper to force it to use (and keep) a given node as primary.
To focus the comparison on the performance of replication protocols, and avoid hitting other system bottlenecks (such as storage I/O that is not very efficient in virtualized cloud environments), we store ZooKeeper data and log directories on a volatile tmpfs file system.
The configuration tested tolerates one fault (t = 1).
ZooKeeper clients were located in the same region as the primary (CA).
Each client invokes 1 kB write operations in a closed loop.
Figure 7 depicts the results.
The X-axis represents the throughput in kops/sec.
The Y-axis represents the latency in ms. In this macro-benchmark, we find that Paxos and XPaxos clearly outperform BFT protocols and that XPaxos achieves a performance close to that of Paxos.
More surprisingly, we can see that XPaxos is more efficient than the built-in Zab protocol, although the latter only tolerates crash faults.
For both protocols, the bottleneck in the WAN setting is the bandwidth at the leader, but the leader in Zab sends requests to all other 2t replicas whereas the XPaxos leader sends requests only to t followers, which yields a higher peak throughput for XPaxos.
In this section, we illustrate the reliability guarantees of XPaxos by analytically comparing them with those of the state-of-the-art asynchronous CFT and BFT protocols.
For simplicity of the analysis, we consider the fault states of the machines to be independent and identically distributed random variables.We denote the probability that a replica is correct (resp., crash-faulty) by p correct (resp., p crash ).
The probability that a replica is benign is p benign = p correct + p crash .
Hence, a replica is non-crash-faulty with probability p non-crash = 1 − p benign .
Besides, we assume there is a probability p synchrony that a replica is not partitioned, where p synchrony is a function of ∆, the network, and the system environment.
Finally, the probability that a replica is partitioned equals 1 − p synchrony .
Aligned with the industry practice, we measure the reliability guarantees and coverage of fault scenarios using nines of reliability.
Specifically, we distinguish nines of consistency and nines of availability and use these measures to compare different fault models.
We introduce a function 9of(p) that turns a probability p into the corresponding number of "nines", by letting 9of(p) = − log 10 (1 − p).
For example, 9of(0.999) = 3.
For brevity, 9 benign stands for 9of(p benign ), and so on, for other probabilities of interest.Here, we focus on comparing consistency guarantees, which is less obvious than comparing availability, given that XPaxos clearly guarantees better availability than any asynchronous CFT or BFT protocol (see Table 1).
The availability analysis can be found in [31].
We start with the number of nines of consistency for an asynchronous CFT protocol, denoted by 9ofC(CFT ) = 9of(P[CFT is consistent]).
As P[CFT is consistent] = p n benign , a straightforward calculation yields:9ofC(CFT ) = −log 10 (1− p benign )−log 10 ( n−1 ∑ i=0 p i benign ) ,which gives 9ofC(CFT ) ≈ 9 benign −log 10 (n) for values of p benign close to 1, when p i benign decreases slowly.
As a rule of thumb, for small values of n, i.e., n < 10, we have 9ofC(CFT ) ≈ 9 benign − 1.
In other words, in typical configurations, where few faults are tolerated [13], a CFT system as a whole loses one nine of consistency from the likelihood that a single replica is benign.We now quantify the advantage of XPaxos over asynchronous CFT.
From Table 1, if there is no non-crash fault, or there are no more than t faults (machine faults or network faults), XPaxos is consistent, i.e.,P[XPaxos is consistent] = p n benign + t= n−1 2 ∑ i=1 n i p i non-crash × t−i ∑ j=0 n − i j p j crash × p n−i− j correct × t−i− j ∑ k=0 n − i − j k × p n−i− j−k synchrony × (1 − p synchrony ) k .
To quantify the difference between XPaxos and CFT more tangibly, we calculated 9ofC(XPaxos) and 9ofC(CFT ) for all values of 9 benign , 9 correct and 9 synchrony (9 benign ≥ 9 correct ) between 1 and 20 in the special cases where t = 1 and t = 2, which are the most relevant cases in practice.
For t = 1, we observed the following relation (the t = 2 case is given in [31]):9ofC(XPaxos t=1 ) − 9ofC(CFT t=1 ) =      9 correct − 1, 9 benign > 9 synchrony ∧ 9 synchrony = 9 correct , min(9 synchrony , 9 correct ), otherwise.Hence, for t = 1, we observe that the number of nines of consistency XPaxos adds on top of CFT is proportional to the nines of probability for a correct or synchronous machine.
The added nines are not directly related to p benign , although p benign ≥ p correct must hold.
Example 1.
When p benign = 0.9999 and p correct = p synchrony = 0.999, we have p non-crash = 0.0001 and p crash = 0.0009.
In this example, 9 × p non-crash = p crash , i.e., if a machine suffers a faults 10 times, then one of these is a non-crash fault and the rest are crash faults.
In this case, 9ofC(CFT t=1 ) = 9 benign − 1 = 3, whereas 9ofC(XPaxos t=1 ) − 9ofC(CFT t=1 ) = 9 correct − 1 = 2, i.e., 9ofC(XPaxos t=1 ) = 5.
XPaxos adds 2 nines of consistency on top of CFT and achieves 5 nines of consistency in total.
Example 2.
In a slightly different example, let p benign = p synchrony = 0.9999 and p correct = 0.999, i.e., the network behaves more reliably than in Example 1.9ofC(CFT t=1 ) = 9 benign − 1 = 3, whereas 9ofC(XPaxos t=1 ) − 9ofC(CFT t=1 ) = p correct = 3, i.e., 9ofC(XPaxos t=1 ) = 6.
XPaxos adds 3 nines of consistency on top of CFT and achieves 6 nines of consistency in total.
Recall that (see Table 1) SMR in asynchronous BFT model is consistent whenever no more than one-third of the machines are non-crash-faulty.
Hence,P[BFT is consistent] = t= n−1 3 ∑ i=0 n i (1− p benign ) i × p n−i benign .
We first examine the conditions under which XPaxos has stronger consistency guarantees than BFT.
Fixing the value t of tolerated faults, we observe thatP[XPaxos is consistent] > P[BFT is consistent] is equivalent to p 2t+1 benign + t ∑ i=1 2t + 1 i p i non-crash × t−i ∑ j=0 2t + 1 − i j p j crash × p 2t+1−i− j correct × t−i− j ∑ k=0 2t + 1 − i − j k p 2t+1−i− j−k synchrony × (1 − p synchrony ) k > t ∑ i=0 3t + 1 i p 3t+1−i benign (1 − p benign ) i .
In the special case when t = 1, the above inequality simplifies to p correct × p synchrony > p 1.5 benign .
Hence, for t = 1, XPaxos has stronger consistency guarantees than any asynchronous BFT protocol whenever the probability that a machine is correct and not partitioned is larger than the power 1.5 of the probability that a machine is benign.
This is despite the fact that BFT is more expensive than XPaxos as t = 1 implies 4 replicas for BFT and only 3 for XPaxos.In terms of nines of consistency, again for t = 1 (t = 2 is again given in [31]), we calculated the difference in consistency between XPaxos and BFT SMR, for all values of 9 benign , 9 correct and 9 synchrony ranging between 1 and 20, and observed the following relation: 9 benign − 9 correct + 1, 9 benign > 9 synchrony ∧ 9 synchrony = 9 correct , 9 benign − min(9 correct , 9 synchrony ), otherwise.Notice that in cases where XPaxos guarantees better consistency than BFT (p correct × p synchrony > p 1.5 benign ), it is only "slightly" better and does not yield additional nines.Example 1 (cont'd.).
Building upon our example, p benign = 0.9999 and p synchrony = p correct = 0.999, we have 9ofC(BFT t=1 ) − 9ofC(XPaxos t=1 ) = 9 benign − 9 synchrony + 1 = 2, i.e., 9ofC(XPaxos t=1 ) = 5 and 9ofC(BFT t=1 ) = 7.
BFT brings 2 nines of consistency on top of XPaxos.Example 2 (cont'd.).
When p benign = p synchrony = 0.9999 and p correct = 0.999, we have 9ofC(BFT t=1 ) − 9ofC(XPaxos t=1 ) = 1, i.e., 9ofC(XPaxos t=1 ) = 6 and 9ofC(BFT t=1 ) = 7.
XPaxos has one nine of consistency less than BFT (albeit the only 7th).
In this paper, we introduced XFT, a novel fault-tolerance model that allows the design of efficient protocols that tolerate non-crash faults.
We demonstrated XFT through XPaxos, a novel state-machine replication protocol that features many more nines of reliability than the best crash-fault-tolerant (CFT) protocols with roughly the same communication complexity, performance and resource cost.
Namely, XPaxos uses 2t + 1 replicas and provides all the reliability guarantees of CFT, but is also capable of tolerating non-crash faults, as long as a majority of XPaxos replicas are correct and can communicate synchronously among each other.As XFT is entirely realized in software, it is fundamentally different from an established approach that relies on trusted hardware for reducing the resource cost of BFT to 2t + 1 replicas only [15,30,21,39].
XPaxos is also different from PASC [14], which makes CFT protocols tolerate a subset of Byzantine faults using ASC-hardening.
ASC-hardening modifies an application by keeping two copies of the state at each replica.
It then tolerates Byzantine faults under the "fault diversity" assumption, i.e., that a fault will not corrupt both copies of the state in the same way.
Unlike XPaxos, PASC does not tolerate Byzantine faults that affect the entire replica (e.g., both state copies).
In this paper, we did not explore the impact on varying the number of tolerated faults per fault class.
In short, this approach, known as the hybrid fault model and introduced in [38] distinguishes the threshold of non-crash faults (say b) despite which safety should be ensured, from the threshold t of faults (of any class) despite which the availability should be ensured (where often b ≤ t).
The hybrid fault model and its refinements [11,35] appear orthogonal to our XFT approach.Specifically, Visigoth Fault Tolerance (VFT) [35] is a recent refinement of the hybrid fault model.
Besides having different thresholds for non-crash and crash faults, VFT also refines the space between network synchrony and asynchrony by defining the threshold of network faults that a VFT protocol can tolerate.
VFT is, however, different from XFT in that it fixes separate fault thresholds for non-crash and network faults.
This difference is fundamental rather than notational, as XFT cannot be expressed by choosing specific values of VFT thresholds.
For instance, XPaxos can tolerate, with 2t + 1 replicas, t partitioned replicas, t non-crash faults and t crash faults, albeit not simultaneously.
Specifying such requirements in VFT would yield at least 3t + 1 replicas.
In addition, VFT protocols have more complex communication patterns than XPaxos.
That said, many of the VFT concepts remain orthogonal to XFT.
It would be interesting to explore interactions between the hybrid fault model (including its refinements such as VFT) and XFT in the future.Going beyond the research directions outlined above, this paper opens also other avenues for future work.
For instance, many important distributed computing problems that build on SMR, such as distributed storage and blockchain, deserve a novel look at them through the XFT prism.
We thank Shan Lu and anonymous OSDI reviewers for their invaluable comments that helped us considerably to improve the paper.
Shengyun Liu's work was supported in part by the National Key Research and Development Program (2016YFB1000101) and the China Scholarship Council.
This work was also supported in part by the EU H2020 project SUPERCLOUD (grant No. 643964) and the Swiss Secretariat for Education, Research and Innovation (contract No. 15.0025).
