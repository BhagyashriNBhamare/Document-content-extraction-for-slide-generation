With the advent of high performing NVMe SSDs, the bottleneck of system performance is shifting away from the traditional storage device.
In particular, the I/O stack software layers have already been recognized as a heavy burden on the overall I/O.
Efforts to alleviate this burden have been considered [1,2,3,4].
Recently, the spotlight has been on the CPU.
With computing capacity as well as the means to get the data to the processor now being limited, recent studies have suggested that processing power be pushed into where the data is residing [5,6,7,8].
With devices such as 3D XPoint [9,10,11] in the horizon, this phenomenon is expected to be aggravated.In this paper, we focus on another component related to such changes.
In particular, it has been observed that the bandwidth of the network that connects clients to storage servers is now being surpassed by storage bandwidth [12,13].
Figure 1 shows the changes that are happening.
We observe that the changes in the storage interface is allowing storage bandwidth to surpass that of the network.
As shown in Table 1, recent developments in SSDs have resulted in individual SSDs providing read and write bandwidth in the 5GB/s and 3GB/s range, respectively, which surpasses or is close to that of 10/25/40GbE (Gigabit Ethernet) that comprise the majority of networks being supported today.Based on this observation, in this paper, we revisit the organization of disk arrays.
Specifically, we target write performance in all-flash arrays, which we interchangeably refer to as SSD arrays, that are emerging as a solution for high-end storage [14,15,16,17,18,19,20].
As shown in Table 2, most major storage vendors carry such a solution and these products employ plenty of SSDs to achieve large capacity and high performance [16,17,18,19].
Figure 2 shows how typical all-flash arrays would be connected to the network and the host.
Our goal is to provide high, sustained, and consistent write performance in such a storage environment.
Even though SSD arrays employ a large number of SSDs, reports have shown that contrary to expectations, they do not provide high, stable performance.
In contrast, large latency variations and oscillating bandwidth for I/O requests are commonly reported [21,22,23,24,25].
The major source of such ineffectiveness is the effect of garbage collection (GC) operations within the SSD devices [26,27].
Figure 3 shows our preliminary experimental results where we observe the write bandwidth of a RAID-0 configured SSD array comprising four identical 400GB capacity PCIe SSDs (with specifications of 128KB sequential read and write bandwidth of 2.2GB/s and 900MB/s, respectively) running the synthetic FIO workload [28].
The measurements are made at the storage host, without going through the network, and the numbers reported are averages of 5 executions of the same workload with Figure 2: Organization of a typical network connected all-flash array measurements starting with clean empty SSDs (cleaned for every execution).
We observe that, at first, the performance is at its peak at roughly 3.5GB/s, which is quite close to the ideal 3.6GB/s write bandwidth.
However, after some time, performance drops considerably, dropping even below the line along the x-axis at 1.25GB/s, which is the bandwidth of today's typical 10GbE network.
This drop is due to GC.
After the performance drop, performance stabilizes at low bandwidth and then increases, but still oscillates considerably.
We observe that with just four SSDs the storage system bandwidth can be considerably higher than typical network bandwidth.
However, we also observe that due to fluctuations in SSD performance due to GC, performance can drop considerably.
In conclusion, we need to find a way to stabilize performance such that services are provided in a steady and expected manner.Efforts have been made to alleviate such performance variations at various layers of the system [27,29,30,31,32,33].
Despite such effort, performance instability due to GC still remains a problem since GC is a necessary operation for flash memory based storage devices.In this paper, we present a storage organization for SSD arrays that totally eliminates garbage collection (GC) within an SSD (that is, internal GC does not occur).
This is based on two key, simple observations: 1) host-end storage systems are connected through the network, and 2) the storage device is no longer the bottleneck, but rather, the network bandwidth is.
These two observations imply that network bandwidth performance should be the target performance goal for SSD arrays.
We show that this can be achieved, not as occasional peak performance, but as sustained, consistent performance as we eliminate all internal GC.The remainder of this paper is organized as follows.
In Section 2, we discuss the organization that we propose.
In the days of HDDs, where disk bandwidth was limited, it was logical to organize disks as an array so that individual disks could be accessed in parallel to increase bandwidth.
However, with contemporary SSDs, as individual device bandwidth start to surpass network bandwidth, parallel organization of many SSDs may no longer be advantageous.Based on this observation, we propose to organize the array of SSDs in a serial manner.
This organization allows only one SSD to service all write requests, though SSDs in the array will take turns as the write servicing SSD.
As we will show later, such management allows clients to observe consistent SSD write bandwidth performance.
The goal of such an organization is to provide, at all times, 1) consistent unfluctuating SSD write performance, 2) at peak network bandwidth performance.
This can be achieved by forbidding GC within SSDs.
We now discuss the organization in detail.
In describing our technique, let us for now assume that the ideal SSD write bandwidth as specified on the product is greater than the network bandwidth.
This will generally be true with contemporary high-end SSDs and network configurations as we discussed earlier (for example, 3 GB/s for PCIe SSDs shown in Table 1 versus 1.25 GB/s for 10GbE).
If this is not the case, we can increase the number of the so-called front-end SSDs, which we describe later, to serve our purpose.
For simplicity, but without loss of generality, let us assume that we have one front-end SSD satisfying the above assumption and concentrate on writes.
In this case, the network will always be the bottleneck if the SSD can be used at its maximum.
This is what we strive for.
Let us assume that we have n SSDs, where n is the number of SSDs in an all-flash array, for example, as specified in Table 2.
Given these assumptions, the array of SSDs in our proposed organization can be viewed as a serial sequence of SSDs, where data is written to only one SSD at a time.As the SSD bandwidth is higher than the network bandwidth, there is no loss of performance as one SSD can absorb all write requests.
We refer to the SSD serving the write request as the front-end, while the rest of the SSDs are referred to as the back-end as depicted in Figure 4(a).
Logically, such an organization can be viewed as a sequential list of blocks as depicted in Figure 4(b).
However, physically, each SSD is controlled independently, which is different from a typical log-structured layout.Let us now see how we can manage the SSDs to achieve consistent, maximal performance.
Assume at first that all SSDs are empty.
As writes come in, all data are written only in log-structured manner without any GC.
Hence, the SSD can be written with maximum performance.
However, the front-end SSD will eventually fill up with a mixture of valid and invalid flash pages.
Then, the front-end is replaced with a new, clean SSD.
(Initially, the selected SSDs will be empty.
However, with time, the selected SSD will contain a mix of valid and invalid data.)
Evidently, if this keeps going, all the SSDs will fill up.
Our past experience from logstructured designs tell us that some form of garbage collection is required.Given such a scenario, let us now discuss how our proposed organization is different from the traditional logstructured approach.
• First, only the front-end SSD is actively performing writes.
All back-end SSDs are free to serve read requests if there are any targeted towards these SSDs ( Figure 5(a)).
• Second, no GC occurs within the frond-end SSD so that performance is maximized.
• Third, when the front-end SSD fills up, it relieves its role as a front-end SSD to one of the back-end SSDs ( Figure 5(b)).
• Fourth, GC of an SSD occurs not within SSDs, but only among back-end SSDs.
Hence, we refer to this GC as external GC (xGC) in contrast to the traditional internal GC that happens within an SSD.
.
When all valid data is moved, then the old front-end is cleaned by issuing a TRIM command to the entire SSD ( Figure 5(c)).
This makes GC a deterministic and simple activity.
There are two major benefits to our approach.
The first is on performance and other is on the efficiency of the SSD.
We discuss each of these below.
The performance benefits of serial management is as follows, which we quantify in Section 3.
Writes: As writes are being done in a log-structured manner, that is, a sequential manner, and no GC occurs performance is optimized.
Reads: As writes to the front-end SSD are of recent data, most of the reads targeted towards the frontend SSD will be absorbed by the cache in either the client and/or the host without affecting the front-end SSD.
Similarly, reads targeted towards the backend SSDs will not be affected by the front-end SSD write activities.
Furthermore, as read latency is greatly affected by GC, and as we rid of internal GC, read performance is stabilized.
The second benefit offered by serial management is the simplicity that it brings to the design of the FTL (Flash Translation Layer) within the SSD.
Recall that most high-end commodity SSDs employ large amounts of DRAM as buffer space and require substantial spare flash memory to be used as OPS (Over Provisioning Space) [34].
In addition, for high performance, techniques such as page-level mapping must be employed over simpler, resource thrifty block-level mapping within the FTL.
These kinds of restrictions impose a burden on the SSD controller in terms of cost and performance.
As there is no need to perform internal GC (though in practice, we believe some form of minimal internal GC may be needed), serial management alleviates these burden on the FTL in the following manner (though these aspects are not quantified in this study):1.
Less internal resources are required for SSDs used in our organization.
This is because block-level mapping, with large block sizes, may be adopted as write requests always arrive in append only manner.
Hence, less memory space is required to manage the blocks within the FTL.2.
SSD capacity increases, in effect, as with no internal (or minimal) GC, no (or less) OPS space is needed to perform GC.3.
Longer lifetime can be expected for SSDs used in our organization.
This is because with internal GC the same valid page may be copied multiple times within an SSD if it, by chance, happens to be in the victim block multiple times.
In contrast, with our technique a valid page is written once and never copied within an SSD as there is no internal GC.4.
Wear-leveling becomes simple and even for all blocks with serially managed SSDs resulting in more predictable SSD lifetime management.
This is so as an erasure happens only when a TRIM command is issued to the entire SSD.
Hence, all blocks wear out evenly for the entire lifetime of the SSD.
Most previous studies on an array of disks have attempted to increases access parallelism such as in RAID configurations.
In contrast, the Gecko study by Shin et al. takes a similar approach as ours in that they consider serial management of storage devices [35].
Gecko views the chain of HDDs as a log with new writes being made to the tail of the log.
This is, in essence, similar to the traditional log-structured approach, and the general differences between the traditional log-structured approach and ours was described in Section 2.1.
Some other specific differences are as follows.
Gecko was designed for HDD, while ours is for high-end SSDs that have bandwidth similar or surpassing that of the network.
Gecko does not forbid the replacement of HDDs with SSDs, but then, does not consider the peculiarities of SSDs.
In particular, our technique completely separates GC writes from first-class writes allowing firstclass writes to make full use of the bandwidth.
Gecko, on the other hand, intermixes them, which incurs contention at the disk that holds the tail.
Furthermore, GC writes in Gecko are for segment cleaning and is different from SSD internal GC, which is not controlled in Gecko.
Hence, the effect of internal GC, which is the key performance distracting factor and which our technique is obviating, still remains with Gecko.One similarity between the two is how metadata is managed.
Similarly to Gecko, we maintain a logical to physical address map as well as an inverse map in memory.
Such mappings are kept for each SSD, and they occupy around 0.2% of the total capacity of the SSDs.
In this section, we discuss the initial implementation of our proposed serial management scheme.
We present the experimental setup and the benchmarks used in our evaluations, and then discuss the results.
Experimental Platform: For the storage server, we use a Dell R730 equipped with a Xeon E5-2609 CPU, 64GB DRAM, and four Intel 750 400GB NVMe SSDs.
Depending on the experiment target, the four SSDs are configured in parallel manner as a volume of RAID-0 or in the serial manner that we propose resulting in 1.6TB storage capacity.
The RAID-0 volume with a chunk size of 64KB is created using LVM2 [36].
For the host system, we use an x86 compatible PC with an i5-6600k CPU, 16GB DRAM, and local storage.
The host is directly connected to the storage server via a 10Gb/s Ethernet card.
The operating systems for the storage server and the host are Linux kernel-4.4.43 and kernel-4.3.3, respectively, and the Ext4 file system is used.Benchmark Parameters: Two types of workloads are used for our experiments.
In Section 3.2, we use a simple FTP application to isolate the effect of the network.
For this workload, we make use of 10 threads with each thread transmitting a 10 GB file.In Section 3.3, where the stability of performance is considered, we use the synthetic FIO benchmark workload.
The benchmark is executed with one thread and the thread issues random writes with a queue depth of 32 and to 10 files.
Over the entire experiments, we make use of two different FIO workloads.
The first is the aging workload that writes for 15 minutes, where the block size is 256KB with a footprint of 1200GB.
After aging, we use a second workload where 64KB sized random writes are issued for 30 minutes on a 200GB footprint.
In this section, we observe how the network affects the performance of the parallel configuration, that is, RAID-0 versus the serial configuration that we propose.
For both cases, we connect the storage array to the host via 10Gb/s network connection and have 10 threads in the host individually send 10GB files to write to storage via Figure 6: Performance of RAID-0 and serially configured storage connected to the host via 10GbE network the network.
We observe the throughput at the storage layer as writing a total of 100GBs saturates the network and storage bandwidth.
Figure 6 is what we observe at the storage end for RAID-0 and the serial configuration that we propose with the y-axis being the throughput in MB/s.
We see that for RAID-0 (the dotted line) the bandwidth fluctuates considerably, periodically reaching the peak and zero values.
This phenomenon occurs because as the data arrives through the network, RAID-0 processes the requests in bulks; once that is done, it has to wait for the new data to arrive through the network resulting in zero bandwidth.
However, the results reveal that on average, 900MB/s throughput is maintained at the storage layer.
This is below the perfect available network bandwidth, but very close to the practical peak bandwidth.In contrast, for our serial configuration (red solid line), we also observe fluctuations but at a much smaller scale than the RAID-0 case.
We find that the average bandwidth is around 890MB/s for this case, which is slightly below the 900MB/s we saw for RAID-0.
However, recall that for RAID-0 all four SSDs are simultaneously being used whereas for our proposed configuration only one is being used.
We see that matching the storage bandwidth with the network bandwidth is a logical choice.
While this was not possible with past disk technology, with current high-end SSDs, this is easily achieved.
We showed in the previous section that the network is the bottleneck with high bandwidth SSDs.
In this section, we remove the network aspect, which is the source of performance fluctuation in the previous experiments, and concentrate on storage performance.
Recall from Figure 3 how RAID-0 performance drops after some time due to GC operations.
We conduct a similar experiment with our proposed serial configuration on a locally connected storage end with the workload and setting as described in Section 3.1.
The average throughput results obtained Figure 7.
The line in the middle demarcates the results during and after aging.
The demarcating line represents a sudden drop in throughput that is due to a slight pause in generating the workload during the experiments.Overall, we see that performance during and after aging is consistent.
Some points of interest in Figure 7 are as follows.
First, we see that there are slight differences in the bandwidth observed when front-end and back-end SSDs are swapped as a new front-end is selected.
This is because most commercial SSDs do not come with exactly the same performance.
We find that performance changes according to the performance characteristics of each SSD selected as the front-end.
This, we find, is consistent for each of the SSDs.
Second, we see that xGC does not affect performance.
Even though GC is happening, this has no bearing on performance.
Finally, we attain overall high, sustained performance that we set out to attain even though we do observe occasional small dips in performance.
Based on the observation that storages devices are no longer the performance bottleneck, but that the network is, this paper proposes a serial organization for SSD arrays.
Such serial organization allowed us to completely avoid internal garbage collection of SSDs such that high performance of write requests can be sustained.
Experimental results showed that this is a promising approach.
We are currently looking into specific details concerning the implementation of this approach and other possible design issues that we might have overlooked.
To consider the scalability issue, we also plan to implement a prototype all-flash array with many more SSDs.
