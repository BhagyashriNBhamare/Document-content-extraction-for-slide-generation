We investigate variants of Lloyd's heuristic for clustering high dimensional data in an attempt to explain its popularity (a half century after its introduction) among practitioners, and in order to suggest improvements in its application.
We propose and justify a clusterability criterion for data sets.
We present variants of Lloyd's heuristic that quickly lead to provably near-optimal clustering solutions when applied to well-clusterable instances.
This is the first performance guarantee for a variant of Lloyd's heuristic.
The provision of a guarantee on output quality does not come at the expense of speed: some of our algorithms are candidates for being faster in practice than currently used variants of Lloyd's method.
In addition, our other algorithms are faster on well-clusterable instances than recently proposed approximation algorithms, while maintaining similar guarantees on clustering quality.
Our main algo-rithmic contribution is a novel probabilistic seeding process for the starting configuration of a Lloyd-type iteration.
Overview.
There is presently a wide and unsatisfactory gap between the practical and theoretical clustering literatures.
For decades, practitioners have been using heuristics of great speed but uncertain merit; the latter should not be surprising since the problem is NP-hard in almost any formulation.
However, in the last few years, algorithms researchers have made considerable innovations, and even obtained polynomialtime approximation schemes (PTAS's) for some of the most popular clustering formulations.
Yet these contributions have not had a noticeable impact on practice.
Practitioners instead continue to use a variety of heuristics (Lloyd, EM, agglomerative methods, etc.) that have no known performance guarantees.There are two ways to approach this disjuncture.
The most obvious is to continue developing new techniques until they are so good-down to the implementations-that they displace entrenched methods.
The other is to look toward popular heuristics and ask whether there are reasons that justify their extensive use, but elude the standard theoretical criteria; and in addition, whether theoretical scrutiny suggests improvements in their application.
This is the approach we take in this paper.As in other prominent cases [48,42], such an analysis typically involves some abandonment of the worst-case inputs criterion.
(In fact, part of the challenge is to identify simple conditions on the input, that allow one to prove a performance guarantee of wide applicability.)
Our starting point is the notion that (as faster, and also simpler, than the PTAS of Kumar et al. [30] (applying the separation condition to both algorithms; the latter does not run faster under the condition).
The problem of minimizing the k-means cost is one of the earliest and most intensively studied formulations of the clustering problem, both because of its mathematical elegance and because it bears closely on statistical estimation of mixture models of k point sources under spherically symmetric Gaussian noise.
We briefly survey the most relevant literature here.
The k-means problem seems to have been first considered by Steinhaus in 1956 [49].
A simple greedy iteration to minimize cost was suggested in 1957 by Lloyd [32] (and less methodically in the same year by Cox [9]; also apparently by psychologists between 1959-67 [50]).
This and similar iterative descent methods soon became the dominant approaches to the problem [35,33,12,31] (see also [19,20,24] and the references therein); they remain so today, and are still being improved [1,43,45,28].
Lloyd's method (in any variant) converges only to local optima however, and is sensitive to the choice of the initial centers [38].
Consequently, a lot of research has been directed toward seeding methods that try to start off Lloyd's method with a good initial configuration [18,29,17,23,47,5,36,44].
Very few theoretical guarantees are known about Lloyd's method or its variants.
The convergence rate of Lloyd's method has recently been investigated in [10,22,2] and in particular, [2] shows that Lloyd's method can require a superpolynomial number of iterations to converge.The k-means problem is NP-hard even for k = 2 [13].
Recently there has been substantial progress in developing approximation algorithms for this problem.
Matoušek [34] gave the first PTAS for this problem, with running time polynomial in n, for a fixed k and dimension.
Subsequently a succession of algorithms have appeared [41,4,11,15,16,21,30] with varying runtime dependency on n, k and the dimension.
The most recent of these is the algorithm of Kumar, Sabharwal and Sen [30], which presents a linear time PTAS for a fixed k.
There are also various constant-factor approximation algorithms for the related k-median problem [26,7,6,25,37], which also yield approximation algorithms for k-means, and have running time polynomial in n, k and the dimension; recently Kanungo et al. [27] adapted the k-median algorithm of [3] to obtain a (9 + )-approximation algorithm for k-means.
However, none of these methods match the simplicity and speed of the popular Lloyd's method.
Researchers concerned with the runtime of Lloyd's method bemoan the need for n nearest-neighbor computations in each descent step [28] !
Interestingly, the last reference provides a data structure that provably speeds up the nearest-neighbor calculations of Lloyd descent steps, under the condition that the optimal clusters are well-separated.
(This is unrelated to providing performance guarantees for the outcome.)
Their data structure may be used in any Lloyd-variant, including ours, and is well suited to the conditions under which we prove performance of our method; however, ironically, it may not be worthwhile to precompute their data structure since our method requires so few descent steps.
We use the following notation throughout.
For a point set S, we use ctr(S) to denote the center of mass of S. Let partition X 1 ∪ · · · ∪ X k = X be an optimal k-means clustering of the input X, and let c i = ctr(X i ) and c = ctr(X).
So ∆ 2 k (X) = k i=1 x∈X i x − c i 2 = k i=1 ∆ 2 1 (X i ).
Let n i = |X i |, n = |X|, andr 2 i = ∆ 2 1 (X i ) n i, that is, r 2 i is the "mean squared error" in cluster X i .
Define D i = min j =i c j − c i .
We assume throughout that X is -separated for k-means, that is, ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X), where 0 < ≤ 0 with 0 being a suitably small constant.
We use the following basic lemmas quite frequently.Lemma 2.1 For every x, y∈X x − y 2 = ∆ 2 1 (X) + nx − c 2 .
Hence {x,y}⊆X x − y 2 = n∆ 2 1 (X).
Lemma 2.2 Consider any set S ⊆ R d and any partition S 1 ∪ S 2 of S with S 1 = ∅.
Let s, s 1 , s 2 denote respectively ctr(S), ctr(S 1 ), ctr(S 2 ).
Then, (i)∆ 2 1 (S) = ∆ 2 1 (S 1 ) + ∆ 2 1 (S 2 ) + |S 1 ||S 2 | |S| s 1 − s 2 2 , and (ii) s 1 − s 2 ≤ ∆ 2 1 (S) |S| · |S 2 | |S 1 | .
Proof : Let a = |S 1 | and b = |S 2 | = |S| − |S 1 |.
We have∆ 2 1 (S) = x∈S 1 x − s 2 + x∈S 2 x − c 2 = ∆ 2 1 (S 1 ) + as 1 − s 2 + ∆ 2 1 (S 2 ) + bs 2 − s 2 (by Lemma 2.1) = ∆ 2 1 (S 1 ) + ∆ 2 1 (S 2 ) + ab a+b · s 1 − s 2 2 .
The second equality follows from Lemma 2.1 by noting that s is also the center of mass of the point set where a points are located at s 1 and b points are located at s 2 , and so the optimal 1-means cost of this point set is given by as 1 − s 2 + bs 2 − s 2 .
This proves part (i).
Part (ii) follows by substitutings 1 − s = s 1 − s 2 · b/(a + b)in part (i) and dropping the ∆ 2 1 (S 1 ) and ∆ 2 1 (S 2 ) terms.
We first consider the 2-means case.
We assume that the input X is -separated for 2-means.
We present an algorithm that returns a solution of cost at most 1 + f () ∆ 2 2 (X) in linear time, for a suitably defined function f that satisfies lim →0 f () = 0.
An appealing feature of our algorithm is its simplicity, both in description and analysis.
In Section 4, where we consider the k-means case, we will build upon this algorithm to obtain both a linear time constant-factor (of the form 1 + f ()) approximation algorithm and a PTAS with running time exponential in k, but linear in n, d.The chief algorithmic novelty in our 2-means algorithm is a non-uniform sampling process to pick two seed centers.
Our sampling process is very simple: we pick the pair x, y ∈ X with probability proportional to x − y 2 .
This biases the distribution towards pairs that contribute a large amount to ∆ 2 1 (X) (noting that n∆ 2 1 (X) = {x,y}⊆X x − y 2 ).
We emphasize that, as improving the seeding is the only way to get Lloyd's method to find a high-quality clustering, the topic of picking the initial seed centers has received much attention in the experimental literature (see, e.g., [44] and references therein).
However, to the best of our knowledge, this simple and intuitive seeding method is new to the vast literature on the k-means problem.
By putting more weight on pairs that contribute a lot to ∆ 2 1 (X), the sampling process aims to pick the initial centers from the cores of the two optimal clusters.
We define the core of a cluster precisely later, but loosely speaking, it consists of points in the cluster that are significantly closer to this cluster-center than to any other center.
Lemmas 3.1 and 3.2 make the benefits of this approach precise.
Thus, in essence, we are able to leverage the separation condition to nearly isolate the optimal centers.
Once we have the initial centers within the cores of the two optimal clusters, we show that a simple Lloyd-like step, which is also simple to analyze, yields a good performance guarantee: we consider a suitable ball around each center and move the center to the centroid of this ball to obtain the final centers.
This "ball-k-means" step is adopted from Effros and Schulman [16], where it is shown that if the k-means cost of the current solution is small compared to ∆ 2 k−1 (X) (which holds for us since the initial centers lie in the cluster-cores) then a Lloyd step followed by a ball-k-means step yields a clustering of cost close to ∆ 2 k (X).
In our case, we are able to eliminate the Lloyd step, and show that the ball-k-means step alone guarantees a good clustering.1.
Sampling.
Randomly select a pair of points from the set X to serve as the initial centers, picking the pair x, y ∈ X with probability proportional to x − y 2 .
LetˆcLetˆ Letˆc 1 , ˆ c 2 denote the two picked centers.2.
"Ball-k-means" step.
For eachˆceachˆ eachˆc i , consider the ball of radiusˆcradiusˆradiusˆc 1 − ˆ c 2 /3 aroundˆcaroundˆ aroundˆc i and compute the centroid ¯ c i of the portion of X in this ball.
Return ¯ c 1 , ¯ c 2 as the final centers.Running time The entire algorithm runs in time O(nd).
Step 2 clearly takes only O(nd) time.
We show that the sampling step can be implemented to run in O(nd) time.
Consider the following two-step sampling procedure: (a) first pick centerˆccenterˆ centerˆc 1 by choosing a point x ∈ X with probability equal toP y∈X x−y 2 P x,y∈X x−y 2 = ∆ 2 1 (X) + nx − c 2 /2n∆ 2 1 (X)(using Lemma 2.1); (b) pick the second center by choosing point y ∈ X with probability equal toy − ˆ c 1 2 / ∆ 2 1 (X) + nc − ˆ c 1 2.
This two-step sampling procedure is equivalent to the sampling process in step 1, that is, it picks pair x 1 , x 2 ∈ X with probabilityx 1 −x 2 2 P {x,y}⊆X x−y 2 .
Each step takes only O(nd) time since ∆ 2 1 (X) can be precomputed in O(nd) time.Analysis The analysis hinges on the important fact that under the separation condition, the radius r i of each optimal cluster is substantially smaller than the inter-cluster separation c 1 − c 2 (Lemma 3.1).
This allows us to show in Lemma 3.2 that with high probability, each initial centerˆccenterˆ centerˆc i lies in the core (suitably defined) of a distinct optimal cluster, say X i , and hence c 1 − c 2 is much larger than the distancesˆcdistancesˆdistancesˆc i − c i for i = 1, 2.
Assuming thatˆcthatˆ thatˆc 1 , ˆ c 2 lie in the cores of the clusters, we prove in Lemma 3.3 that the ball aroundˆc aroundˆ aroundˆc i contains only, and most of the mass of cluster X i , and therefore the centroid ¯ c i of this ball is very "close" to c i .
This in turn implies that the cost of the clustering around ¯ c 1 , ¯ c 2 is small.Lemma 3.1 max(r 2 1 , r 2 2 ) ≤ 2 1− 2 c 1 − c 2 2 = O( 2 )c 1 − c 2 2 .
Proof : By part (i) of Lemma 2.2 we have ∆ 2 1 (X) = ∆ 2 2 (X) + n 1 n 2 n · c 1 − c 2 2 which is equivalent ton n 1 n 2 · ∆ 2 2 (X) = c 1 − c 2 2 ∆ 2 2 (X) ∆ 2 1 (X)−∆ 2 2 (X).
This implies that r 2 1 · n n 2 + r 2 2 · n n 1≤ 2 1− 2 c 1 − c 2 2 .
Let ρ = 100 2 1− 2 .
We require that ρ < 1.
We define the core of cluster X i as the set X cori = x ∈ X i : x − c i 2 ≤ r 2 i ρ .
By Markov's inequality, |X cor i | ≥ (1 − ρ)n i for i = 1, 2.
Lemma 3.2 Pr [{ˆc{ˆc 1 , ˆ c 2 } ∩ X cor 1 = ∅ and {ˆc{ˆc 1 , ˆ c 2 } ∩ X cor 2 = ∅] = 1 − O(ρ).
Proof : To simplify our expressions, we assume that all the points are scaled by1 c 1 −c 2 (so c 1 − c 2 = 1).
By part (i) of Lemma 2.2, we have ∆ 2 1 (X) = ∆ 2 2 (X) + n 1 n 2 n · c 1 − c 2 2 which implies that ∆ 2 1 (X) ≤ n 1 n 2 n(1− 2 ).
Let c i denote the center of mass of X cor i .
Applying part (ii) of Lemma 2.2 (taking S = X i and S 1 = X cor i ) we get that c i − c i 2 ≤ ρ 1−ρ · r 2 i .
B = {x,y}⊆X x − y 2 = n∆ 2 1 (X) ≤ n 1 n 2 1− 2 .
By the above bounds on c i − c i and Lemma 3.1, we getc 1 − c 2 ≥ 1 − 2 ρ (1−ρ)(1− 2 ).
So A = 1 − O(ρ) n 1 n 2 , and A/B = 1 − O(ρ) .
So we may assume that each initial centerˆccenterˆ centerˆc i lies inX cor i .
LetˆdLetˆ Letˆd = ˆ c 1 − ˆ c 2 and B i = {x ∈ X : x − ˆ c i ≤ ˆ d/3}.
Recall that ¯ c i is the centroid of B i , and we return ¯ c 1 , ¯ c 2 as our final solution.Lemma 3.3 For each i, we have X cori ⊆ B i ⊆ X i .
Hence, ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i .
Proof : By Lemma 3.1 and the definition of X cor i , we know thatˆcthatˆthatˆc i − c i ≤ θc 1 − c 2 for i = 1, 2 where θ =√ ρ(1− 2 ) ≤ 1 10 .
So 4 5 ≤ ˆ d c 1 −c 2 ≤ 6 5 .
For any x ∈ B i we have x − c i ≤ ˆ d 3 + ˆ c i − c i ≤ c 1 −c 2 2 , so x ∈ X i .
Also for any x ∈ X cor i , x − ˆ c i ≤ 2θc 1 − c 2 ≤ ˆ d 3 , so x ∈ B i .
Now by part (ii) of Lemma 2.2, with S = X i and S 1 = B i , we obtain that ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i since |B i | ≥ |X cor i | for each i.Theorem 3.4The above algorithm returns a clustering of cost at most∆ 2 2 (X) 1−ρ with probability at least 1 − O(ρ) in time O(nd), where ρ = Θ( 2 ).
Proof : The cost of the solution is at mosti,x∈X i x − ¯ c i 2 = i ∆ 2 1 (X i ) + n i ¯ c i − c i 2 ≤ ∆ 2 2 (X) 1−ρ .
We now consider the k-means setting.
We assume that∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X).
We describe a linear time constant-factor approximation algorithm, and a PTAS that returns a (1 + ω)-optimal solution in timeO 2 O(k/w) nd .
The algorithms consist of various ingredients, which we describe separately first for ease of understanding, before gluing them together to obtain the final algorithm.Conceptually both algorithms proceed in two stages.
The first stage is a seeding stage, which performs the bulk of the work and guarantees that at the end of this stage there are k seed centers positioned at nearly the right locations.
By this we mean that if we consider distances at the scale of the inter-cluster separation, then at the end of this stage, each optimal center has a (distinct) initial center located in close proximity -this is precisely the leverage that we obtain from the k-means separation condition (as in the 2-means case).
We shall employ three simple seeding procedures with varying time vs. quality guarantees that will exploit this condition and seed the k centers at locations very close to the optimal centers.
In Section 4.1.1, we consider a natural generalization of the sampling procedure used for the 2-means case, and show that this picks the k initial centers from the cores of the optimal clusters.
This sampling procedure runs in linear time but it succeeds with probability that is exponentially small in k.
In Section 4.1.2, we present a very simple deterministic greedy deletion procedure, where we start off with all points in X as the centers and then greedily delete points (and move centers) until there are k centers left.
The running time here is O(n 3 d).
Our deletion procedure is similar to the reverse greedy algorithm proposed by Chrobak, Kenyon and Young [8] for the k-median problem.
Chrobak et al. show that their reverse greedy algorithm attains an approximation ratio of O(log n), which is tight up to a factor of log log n.
In contrast, for the k-meansproblem, if ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X), we show that our greedy deletion procedure followed by a clean-up step (in the second stage) yields a 1 + f () -approximation algorithm.Finally, in Section 4.1.3 we combine the sampling and deletion procedures to obtain an O(nkd+k 3 d)-time initialization procedure.
We sample O(k) centers, which ensures that every cluster has an initial center in a slightly expanded version of the core, and then run the deletion procedure on an instance of size O(k) derived from the sampled points to obtain the k seed centers.Once the initial centers have been positioned sufficiently close to the optimal centers, we can proceed in two ways in the second-stage (Section 4.2).
One option is to use a ball-k-means step, as in 2-means, which yields a clustering of cost 1 + f () ∆ 2 k (X) due to exactly the same reasons as in the 2-means case.
Thus, combined with the initialization procedure of Section 4.1.3, this yields a constant-factor approximation algorithm with running time O(nkd + k 3 d).
The entire algorithm is summarized in Section 4.3.
The other option, which yields a PTAS, is to use a sampling idea of Kumar et al. [30].
For each initial center, we compute a list of candidate centers for the corresponding optimal cluster as follows: we sample a small set of points uniformly at random from a slightly expanded Voronoi region of the initial center, and consider the centroid of every subset of the sampled set of a certain size as a candidate.
We exhaustively search for the k candidates (picking one candidate per initial center) that yield the least cost solution, and output these as our final centers.
The fact that each optimal center c i has an initial center in close proximity allows us to argue that the entire optimal cluster X i is contained in the expanded Voronoi region of this initial center, and moreover that |X i | is a significant fraction of the total mass in this region.
Given this property, as argued by Kumar et al. (Lemma 2.3 in [30]), a random sample from the expanded Voronoi region also (essentially) yields a random sample from X i , which allows us to compute a good estimate of the centroid of X i , and hence of ∆ 2 1 (X i ).
We obtain a (1 + ω)-optimal solution in time O 2 O(k/ω) nd with constant probability.
Since we incur an exponential dependence on k anyway, we just use the simple sampling procedure of Section 4.1.1 in the first-stage to pick the k initial centers.
Although the running time is exponential in k, it is significantly better than the running time of O 2 (k/ω) O (1) nd incurred by the algorithm of Kumar et al.; we also obtain a simpler PTAS.
Both of these features can be traced to the separation condition, which enables us to nearly isolate the positions of the optimal centers in the first stage.
Kumar et al. do not have any such facility, and therefore need to sequentially "guess" (i.e., exhaustively search) the various centroids, incurring a corresponding increase in the run time.
This PTAS is described in Section 4.4.
The following lemma, which is a simple extension of Lemma 3.1 to the k-means case and is proved via an almost identical argument, will be used repeatedly.
4.1 Seeding procedures used in stage I We pick k initial centers as follows: first pick two centersˆccentersˆ centersˆc 1 , ˆ c 2 as in the 2-means case, that is, choose x, y ∈ X with probability proportional to x − y 2 .
Suppose we have already picked i centersˆccentersˆ centersˆc 1 , . . . , ˆ c i where 2 ≤ i < k.
Now pick a random point x ∈ X with probability proportional to min j∈{1,...,i} x − ˆ c j 2 and set that as centerˆccenterˆ centerˆc i+1 .
Running time The sampling procedure consists of k iterations, each of which takes O(nd) time.
This is because after sampling a new pointˆcpointˆ pointˆc i+1 , we can update the quantity min j∈{1,...,i+1} x − ˆ c j for each pointx in O(d) time.
So the overall running time is O(nkd).
Analysis Let 2 ρ < 1 be a parameter that we will set later.
As in the 2-means case, we define the core of cluster X i as X cori = x ∈ X i : x − c i 2 ≤ r 2 i ρ .
We show that under our separation assumption, the above sampling procedure will pick the k initial centers to lie in the cores of the clusters X 1 , . . . , X k with probability 1 − O(ρ) k .
We also show in Lemma 4.5 that if more than k, but still O(k), points are sampled, then with constant probability, every cluster will contain a sampled point that lies in a somewhat larger core, that we call the outer core of the cluster.
This analysis will be useful in Section 4.1.3.
Proof :The key observation is that for any pair of distinct clusters X i , X j , the 2-means separation condition holds, that is,∆ 2 2 (X i ∪ X j ) = ∆ 2 1 (X i ) + ∆ 2 1 (X j ) ≤ 2 ∆ 2 1 (X i ∪ X j ).
This is because∆ 2 k−1 (X) ≤ =i,j ∆ 2 1 (X ) + ∆ 2 1 (X i ∪ X j ) = ∆ 2 k (X) + ∆ 2 1 (X i ∪ X j ) − ∆ 2 2 (X i ∪ X j ) .
So ∆ 2 1 (X i ∪ X j ) − ∆ 2 2 (X i ∪ X j ) ≥ 1 2 − 1 ∆ 2 k (X) ≥ 1 2 − 1 ∆ 2 2 (X i ∪ X j ).
So using Lemma 3.2 we obtain thatx∈X cor i ,y∈X cor j x − y 2 = 1 − O(ρ){x,y}⊆X i ∪X jx − y 2 .
Summing over all pairs i, j yields the lemma.
Now inductively suppose that the first i centers pickedˆcpickedˆ pickedˆc 1 , . . . , ˆ c i lie in the cores of clusters X j 1 , . . . , X j i .
We show that conditioned on this event, centerˆccenterˆ centerˆc i+1 lies in the core of some cluster X where / ∈ {j 1 , . . . , j i } with probability 1 − O(ρ).
Given a set S of points, we use d(x, S) to denote min y∈S x − y. PrˆPrˆc i+1 ∈ / ∈{j 1 ,...,j i } X cor | ˆ c 1 , . . . , ˆ c i lie in the cores of X j 1 , . . . , X j i = 1 − O(ρ).
Proof : For notational convenience, re-index the clusters so that {j 1 , . . . ,j i } = {1, . . . , m}.
LetˆCLetˆ LetˆC = {ˆc{ˆc 1 , . . . , ˆ c i }.
For any cluster X j , let p j ∈ {1, . . . , i} be the index such that d(c j , ˆ C) = c j − ˆ c p j .
Let A = k j=m+1 x∈X cor j d(x, ˆ C) 2 , and B = k j=1 x∈X j d(x, ˆ C) 2 .
Observe that the probability of the event stated in the lemma is exactly A/B.
Let α denote the maximum over all j ≥ m + 1 of the quantitymax x∈X cor j x − c j /d(c j , ˆ C).
For any point x ∈ X cor j , j ≥ m + 1, we have d(x, ˆ C) ≥ (1 − α)d(c j , ˆ C).
Note that by Lemma 4.1, α ≤ / √ ρ(1− 2 ) 1−/ √ ρ(1− 2 ) ≤ 2 √ ρ(1− 2 )< 1 for a small enough ρ.
Therefore,A = k j=m+1 x∈X cor j d(x, ˆ C) 2 ≥ k j=m+1 (1 − ρ)(1 − α) 2 n j d(c j , ˆ C) 2 ≥ (1 − ρ − 2α) k j=m+1 n j d(c j , ˆ C) 2 .
On the other hand, for any point x ∈ X j , j = 1, . . . , k, we haved(x, ˆ C) ≤ x − ˆ c p j .
Also note that for j = 1, . . . , m, ˆ c p j lies in X cor j , so c j − ˆ c p j ≤ r j √ ρ .
Therefore, B ≤ k j=1 x∈X j x − ˆ c p j 2 ≤ k j=1 ∆ 2 1 (X j ) + n j c j − ˆ c p j 2 ≤ 1 + 1 ρ ∆ 2 k (X) + k j=m+1 n j d(c j , ˆ C) 2 .
Finally, for any j = m+1, . . . k, if we assign all the points in cluster X j to the pointˆcpointˆ pointˆc p j , then the increase in cost is exactly n j c j − ˆ c p j 2 and at least∆ 2 k−1 (X) − ∆ 2 k (X).
Therefore 1 2 − 1 ∆ 2 k (X) ≤ n j d(c j , ˆ C) 2for any j = m + 1, . . . , k, andB ≤ 1+ 2 /ρ 1− 2 k j=m+1 n j d(c j , ˆ C) 2 .
Comparing with A and plugging in the value of α, we get thatA = 1 − O(ρ + √ ρ ) B.
If we set ρ = Ω( 2/3 ), we obtain A/B = 1 − O(ρ).
Next, we analyze the case when more than k points are sampled.
Let ρ 1 = ρ 3 .
Define the outer core of X i to be X outi = {x ∈ X i : x − c i 2 ≤ r 2 i ρ 1 }.
Note that X cor i ⊆ X out i .
Let N = 2k 1−5ρ + 2 ln(2/δ) (1−5ρ) 2where 0 < δ < 1 is a desired error tolerance.
We prove in Lemma 4.4 that at every sampling step, there is a constant probability that the sampled point lies in the core of some cluster whose outer core does not contain a previously sampled point.
The crucial difference between this lemma and Lemma 4.3, is that Lemma 4.3 only shows that the "good" event happens conditioned on the fact that previous samples were also "good", whereas here we give an unconditional bound.
Using this, Lemma 4.5 shows that if we sample N points from X, then with some constant probability, each outer core X out i will contain a sampled point.
The proof is based on a straightforward martingale analysis.
α ≤ ρ 1 /ρ < 1.
Then for any point x ∈ X cor j , j ≥ m + 1, we have d(x, ˆ C) ≥ (1 − α)d(c j , ˆ C) and as in Lemma 4.3, A = k j=m+1 x∈X cor j d(x, ˆ C) 2 ≥ (1 − ρ − 2α) k j=m+1 n j d(c j , ˆ C) 2 .
On the other hand, again arguing as in Lemma 4.3, we haveB = k j=1 x∈X j d(x, ˆ C) 2 ≤ 1+ 2 /ρ 1 1− 2 k j=m+1 n j d(c j , ˆ C) 2 .
Therefore A/B ≥ 1 − ρ + 2 ρ 1 ρ + 2 ρ 1 + 2 .
Since ρ 1 = ρ 3 , taking ρ = √ gives A/B ≥ 1 − 5ρ.
Lemma 4.5 Suppose we sample N pointsˆxpointsˆ pointsˆx 1 , . . . , ˆ x N from X using the above sampling procedure.
Then, Pr[∀j = 1, . . . , k, there exists somêx i ∈ X out j ] ≥ 1 − δ.Proof : Let Y t be a random variable that denotes the number of clusters that do not contain a sampled point in their outer cores, after t points have been sampled.
We want to bound Pr[Y N > 0].
Consider the following random walk on the line with W t denoting the (random) position after t time steps: W 0 = k, and W t+1 = W t with probability 5ρ and W t −1 with probability 1−5ρ.
Notice that Pr[Y N > 0] ≤ Pr[W N > 0], because as long as W t > 0, any outcome that leads to a left move in the random walk can be mapped to an outcome (in the probability space corresponding to the sampling process) where the outer core of a new cluster is hit by the currently sampled point.
So we boundPr[W N > 0].
Define Z t = W t + t(1 − 5ρ).
Then E Z t+1 |Z 1 , . . . , Z t ≤ Z t , so Z 0 , Z 1 , .
.
.
forms a supermartingale.
Clearly |Z t+1 − Z t | ≤ 1 for all t.
So by Azuma's inequality (see, e.g., [39]), Pr[Z N − Z 0 > 2N ln(2/δ)] ≤ δ which implies that W N ≤ k + 2N ln(2/δ) − N (1 − 5ρ) with probability at least 1 − δ.
Plugging the value of N shows thatN (1 − 5ρ) − 2N ln(2/δ) ≥ k.Corollary 4.6 (i) If we sample k pointsˆcpointsˆ pointsˆc 1 , . . . , ˆ c k , then with probability1 − O(ρ) k , where ρ = Ω( 2/3 ), for each i there is a distinct centerˆccenterˆ centerˆc i ∈ X cor i , that is, ˆ c i − c i ≤ r i / √ ρ.
(ii) If we sample N pointsˆxpointsˆ pointsˆx 1 , . . . , ˆ x N , where N = 2k 1−5ρ + 2 ln(2/ρ) (1−5ρ) 2 and ρ = √ , then with probability1 − O(ρ), for each i there is a distinct pointˆxpointˆ pointˆx i ∈ X out i , that is, ˆ x i − c i ≤ r i / ρ 3 .
We maintain a set of centersˆCcentersˆ centersˆC that are currently used to cluster X. For any point x ∈ R d , let R(x) ⊆ X denote the points of X in the Voronoi region of x (given the set of centersˆCcentersˆ centersˆC).
We refer to R(x) as the Voronoi set of x. Initializê C ← X. Repeat the following steps until | ˆ C| = k.B1.
Compute T = cost of clustering X around the centers inˆCinˆ inˆC = x∈ˆCx∈ˆ x∈ˆC y∈R(x) y − x 2 .
Also for every x ∈ ˆ C, compute T x = cost of clustering X aroundˆCaroundˆ aroundˆC \ {x} = z∈ˆC\{x}z∈ˆ z∈ˆC\{x} y∈R −x (z) y − z 2 , where R −x (z) denotes the Voronoi set of z given the center setˆCsetˆ setˆC \ {x}.
C for which T x − T is minimum and setˆCsetˆ setˆC ← ˆ C \ {y}.
B3.
Recompute the Voronoi sets R(x) = R −y (x) ⊆ X for each (remaining) center x ∈ ˆ C.
Now we "move" the centers to the centroids of their respective (new) Voronoi sets, that is, for every set R(x), we updatê C ← ˆ C \ {x} ∪ {ctr(R(x))}.
Running time There are n − k iterations of the B1-B3 loop.
Each iteration takes O(n 2 d) time: computing T and the sets R(x) for each x takes O(n 2 d) time and we can then compute each T x in O(|R(x)|d) time (since while computing T , we can also compute for each point its second-nearest center inˆCinˆ inˆC).
Therefore the overall running time is O(n 3 d).
Analysis Let ρ be a parameter such thatρ ≤ 1 10 , / ρ(1 − 2 ) ≤ 1 14 .
Recall that D i = min j =i c j − c i .
Define d 2 i = ∆ 2 k (X)/n i .
We will use a different notion of a cluster-core here, but the notion will still capture the fact that the core consists of points that are quite close to the cluster-center compared to the inter-cluster distance, and contains most of the mass of the cluster.
Let B(x, r) = {y ∈ R d : x − y ≤ r} denote the ball of radius r centered at x. Define the kernel of X i to be the ball Z i = B(c i , d i / √ ρ) and the core ofX i as X cor i = X i ∩ Z i .
Observe that r i ≤ d i , so by Markov's inequality |X cor i | ≥ (1 − ρ)n i .
Also, since ∆ 2 k−1 (X) − ∆ 2 k (X) ≤ n i D 2 i we have that d 2 i ≤ D 2 i · 2 1− 2 .
Therefore, X cor i = X ∩ Z i .
We prove that,at the start of every iteration, for every i, there is a (distinct) center x ∈ ˆ C that lies in Z i .
Clearly (*) holds at the beginning, sincê C = X and X cor i = ∅ for every cluster X i .
First we show (Lemma 4.7) that if x ∈ ˆ C is the only center that lies in a slightly enlarged version of the ball Z i for some i, then x is not deleted .
Lemma 4.8 then makes the crucial observation that even after a center y is deleted, if the new Voronoi region R −y (x) of a center x ∈ ˆ C captures points from X cor i , then R −y (x) cannot "extend" too far into some other cluster X i , that is, for x ∈ R −y (x) ∩ X j where j = i, y − c i is not much larger than y − c j .
It will then follow that invariant (*) is maintained.
C is the only center in B c i , 4d i √ ρ for some cluster X i , then x ∈ ˆ C after step B2.Proof : Since property (*) holds, we also know that x ∈ Z i and so X cor i ⊆ R(x) ∩ X i .
If x is deleted in step B2 then all points in X cor i will be reassigned to a center at least 4d i √ ρ away from c i .
So the cost-increaseT x − T is at least A = 5(1−ρ) ρ · n i d 2 i = 5(1−ρ) ρ · ∆ 2 k (X).
Now since | ˆ C| > k, there is some j (j could be i) such that the Voronoi region of c j (with respect to the optimal center-set) contains at least two centers fromˆCfromˆ fromˆC.
We will show that deleting one of these centers will be less expensive than deleting x. Let z ∈ ˆ C be the center closest to c for = 1, . . . , k. Note that z ∈ Z .
Let y ∈ ˆ C, y = z j be another center in the Voronoi region of c j .
Suppose we delete y.
We can upper bound the cost-increase T y − T by the cost-increase due to the reassignment where we assign all points in R y ∩ X to z for = 1, . . . , k. For any .
Hence, the cost-increase of the reassignment is at mostx ∈ R(y) ∩ X we have x − z ≤ x − c + c − z ≤ x − c + d √ ρ .
For = j,y − c ≤ x − c + x − y ≤ x − c + x − z ≤ 2x − c + c − z ≤ 2x − c + d √ ρ .
Therefore, D ≤ 4x − c + 2d √ ρ which implies that √ 1− 2 − 2 √ ρ d ≤ 4x − cB = k =1 x ∈R(y)∩X x − z 2 ≤ x ∈R(y)∩X j 2 x − c j 2 + d 2 j ρ + =j x ∈R(y)∩X β 2 x − c 2 ≤ max(2, β 2 )∆ 2 k (X) + 2 ρ · n j d 2 j = max(2, β 2 ) + 2 ρ ∆ 2 k (X).
Any ρ satisfying the bounds stated in Section 4.1.2 ensures that A > B (since β < 4 3 and ρ < 3 7 ).
Thus, x is not the cheapest center to delete, which completes the proof.Lemma 4.8 Suppose center y ∈ ˆ C is deleted in step B2.
Let x ∈ ˆ C \ {y} be such that R −y (x) ∩ X cor j = ∅ for some j.
Then for any x ∈ R −y (x) ∩ X , = j we have x − c j ≤ x − c + max(d +6d j ,4d +3d j ) √ ρ .
Proof : Suppose that y lies in the Voronoi region of center c i (wrt. optimal centers).
LetˆCLetˆ LetˆC = ˆ C \ {y}.
There must be a center z i ∈ ˆ C such that z i − c i ≤ 4d i √ ρ .
If y / ∈ Z i , this follows from property (*) otherwise this follows from Lemma 4.7.
For any = i, we know by property (*) that there is some center z ∈ ˆ C that lies in Z .
Let x be a point in R −y (x) ∩ X cor j .
Then,x − c j ≤ x − x + x − c j ≤ x − z j + x − c j ≤ z j − c j + 2x − c j ≤ z j − c j + 2d j √ ρ .
Now considering the point x , we have (since it could be that = i).
x − c j ≤ x − x + x − c j ≤ x − z + x − c j ≤ x − c + z − c + x − c j ≤ x − c + z − c + z j − c j + 2d j √ ρ .
Lemma 4.9 Suppose that property (*) holds at the beginning of some iteration in the deletion phase.
Then (*) also holds at the end of the iteration, i.e., after step B3.Proof : Suppose that we delete center y ∈ ˆ C that lies in the Voronoi region of center c i (wrt. optimal centers) in step B2.
LetˆCLetˆ LetˆC = ˆ C \ {y} andR (x) = R −y (x) for any x ∈ ˆ C .
Fix a cluster X j .
Let S = {x ∈ ˆ C : R (x) ∩ X cor j = ∅} and Y = x∈S R (x).
We show that there is some set R (x), x ∈ ˆ C whose centroid ctr(R (x)) lies in the ball Z j , which will prove the lemma.
By Lemma 4.8 and noting that d 2≤ 2 1− 2 · D 2 for every , for any x ∈ Y ∩ X where = j, we have x − c j ≤ x − c + √ ρ(1− 2 ) · max(D + 6D j , 4D + 3D j ).
Also D j , D ≤ c j − c ≤ x − c j + x − c .
Substituting for D j , D we get that y − c j ≤ βy − c where β = 1+7/ √ ρ(1− 2 ) 1−7/ √ ρ(1− 2 ).
Using this we obtain thatA = x ∈Y x − c j 2 ≤ β 2 k =1 x ∈Y ∩X x − c 2 ≤ β 2 ∆ 2 k (X).
We also have A = x∈S x ∈R (x) y−c j 2 = x∈S ∆ 2 1 (R (x))+|R (x)||ctr(R (x))−c j 2 ≥ |Y | min x∈S ctr(R (x))−c j 2 .
Since X cor j ⊆ Y we have |Y | ≥ (1 − ρ)n j , so we obtain that min x∈S ctr(R (x)) − c j ≤ β √ 1−ρ · d i .
The bounds on ρ ensure that ρβ 2 1−ρ ≤ 1, so that min x∈S ctr(R (x)) − c j ≤ d j √ ρ .
Corollary 4.10 After the deletion phase, for every i, there is a centerˆccenterˆ centerˆc i ∈ ˆ C withˆcwithˆwithˆc i −c i ≤√ ρ(1− 2 )·D i .
We now combine the sampling idea with the deletion procedure to obtain an initialization procedure that runs in time O(nkd + k 3 d) and succeeds with high probability.
We first sample O(k) points from X using the sampling procedure.
Then we run the deletion procedure on an O(k)-size instance consisting of the centroids of the Voronoi regions of the sampled, points, with each centroid having a weight equal to the mass of X in its corresponding Voronoi region.
The sampling process will ensure that with high probability, every cluster X i contains a pointˆcpointˆ pointˆc i that is close to its center c i .
This will allow us to argue that the ∆ 2 k (.)
cost of the sampled instance is much smaller than its ∆ 2 k−1 (.)
cost, and that the optimal centers for the sampled instance lie near the optimal centers for X.
We can then use the analysis of the previous section to argue that after the deletion procedure the k centers are still quite close to the optimal centers for the sampled instance, and hence also close to the optimal centers for X.Fix ρ 1 = √ .
C1.
Sampling.
Sample N = 2k 1−5ρ 1 + 2 ln(2/ρ 1 )(1−5ρ 1 ) 2 points from X using the sampling procedure of Section 4.1.1.
Let S denote the set of sampled points.C2.
Deletion phase.
For each x ∈ S, let R(x) = {y ∈ X : y − x = min z∈ˆSz∈ˆ z∈ˆS y − z} be its Voronoi set (wrt. the sampled points).
We now ignore X, and consider a weighted instancê S obtained as follows: setˆSsetˆ setˆS ← {ˆx{ˆx = ctr(R(x)) : x ∈ S}, and assign eachˆxeachˆ eachˆx a weight w(ˆ x) = |R(x)|.
Run the deletion procedure of Section 4.1.2, on this new instance to obtain k centersˆccentersˆ centersˆc 1 , . . . , ˆ c k .
Step Analysis Recall that ρ 1 = √ .
Let ρ 2 = ρ 3 1 .
Let X cor i = {x ∈ X i : x − c i 2 ≤ r 2 i ρ 1 }.
Let X out i = {x ∈ X i : x − c i 2 ≤ r 2 i ρ 2} denote the outer core of cluster X i .
By part (ii) of Corollary 4.6 we know that with probability 1 − O(ρ 1 ), every cluster X i contains a sampled point in its outer core after step C1.
So assume that this event happens.
LetˆsLetˆ Letˆs 1 , . . . , ˆ s k denote the optimal k centers forˆSforˆ forˆS andˆcandˆ andˆc 1 , . . . , ˆ c k be the centers returned by the deletion phase.
Lemma 4.11 shows that the k-means separation condition also holds forˆSforˆ forˆS, and the optimal centers forˆSforˆ forˆS are close to the optimal centers for X.
This will imply that the centers returned by the deletion phase are close to the optimal centers for X.Lemma 4.11 (i) ∆ 2 k ( ˆ S) = O( 2 )∆ 2 k−1 ( ˆ S).
(ii) For every optimal center c i of X, there is a centerˆscenterˆ centerˆs i such thatˆsthatˆthatˆs i − c i ≤ D i 25 + r i √ ρ 1 .
Proof : For each sampled point x ∈ S recall that R(x) ⊆ X denotes its Voronoi set (wrt.
S).
For j = 1, . . . , k, let z j ∈ S be a sampled point inX out j , so z j − c j ≤ r j √ ρ 2.
Consider an optimal (k − 1)-clustering ofˆSofˆ ofˆS.
We can obtain a (k − 1)-clustering of X from this by assigning all the points in R(x), where x ∈ S, to the center to which ctr(R(x)) ∈ ˆ S is assigned.
The cost-increase in doing so is exactlyA = x∈S ∆ 2 1 (R(x)), so ∆ 2 k−1 (X) ≤ ∆ 2 k−1 ( ˆ S) + A.
Since y − x 2 ≤ y − z j 2 ≤ 2 y − c j 2 + r 2 j ρ 2 for any y ∈ R(x) ∩ X j , we obtain that A ≤ 2 1 + 1 ρ 2 ∆ 2 k (X).
To upper bound ∆ 2 k ( ˆ S), consider the following k-clustering ofˆSofˆ ofˆS: for eachˆxeachˆ eachˆx = ctr(R(x)) ∈ ˆ S where x ∈ S ∩ X i , assignˆxassignˆ assignˆx to center c i .
To bound the cost of this assignment, first note that for a point y ∈ R(x) ∩ X j where x ∈ X i and j = i, we havey − c i ≤ y − x + x − c i ≤ y − x + x − c j ≤ 2y − x + y − c j ≤ 3y − c j + 2z j − c j .
We also have z j − c j ≤ r j √ ρ 2 ≤ D j √ ρ 2 (1− 2 ) and D j ≤ y − c i + y − c j , which implies that y − c i ≤ βy − c j where β = 3+2/ √ ρ 2 (1− 2 ) 1−2/ √ ρ 2 (1− 2 ).
Thus,∆ 2 k ( ˆ S) ≤ k i=1 x∈S∩X i |R(x)||ctr(R(x)) − c i 2 ≤ k i=1 x∈S∩X i y∈R(x) y − c i 2 ≤ k i=1 x∈S∩X i y∈R(x)∩X i y − c i 2 + j =i,y∈R(x)∩X j β 2 y − c j 2 ≤ β 2 ∆ 2 k (X).
Combining the two bounds we get,∆ 2 k−1 (S) ≥ ∆ 2 k−1 (X) − A ≥ 1 2 − 2 − 2 ρ 2 ∆ 2 k (X) ≥ 1// 2 − 2 − 2/ρ 2 β 2 · ∆ 2 k ( ˆ S).
Sinceρ 1 = √ and ρ 2 = ρ 3 1 , we get that ∆ 2 k ( ˆ S) = O( 2 )∆ 2 k−1 ( ˆ S).
This proves part (i).
Consider any center c i .
Supposê s j −c i > D i /25+ r i √ ρ 1for every pointˆspointˆ pointˆs j .
Then the cost of clusteringX around the centersˆscentersˆ centersˆs 1 , . . . , ˆ s k is at least 1−ρ 1 625 · n i D 2 i = Ω( −2 )∆ 2 k (X).
On the other hand, we also have that the cost of this clustering for X is at most ∆ 2 k ( ˆ S) + A = O( −3/2 )∆ 2 k (X), which contradicts with the earlier bound.
Proof :LetˆDLetˆ LetˆD i = min j =î s j − ˆ s i .
Then (1 − 2θ) ≤ ˆ D i D i ≤ 1 + 2θ) where θ ≤ 1 25 + √ ρ 1 (1− 2 ) .
Since ρ 1 = √ , for small enough, we have that θ < 1 22 .
Choosing ρ for the deletion phase suitably, by Corollary 4.10, we can ensure that the deletion phase returns a pointˆcpointˆ pointˆc i such thatˆcthatˆthatˆc i − ˆ s i ≤ ˆ D i20 .
Thus, using Lemma 4.11ˆcthatˆthatˆc i − ˆ s i ≤ ˆ D i Given k seed centersˆccentersˆ centersˆc 1 , . . . , ˆ c k located sufficiently close to the optimal centers after stage I, we use two procedures in stage II to obtain a near-optimal clustering: the ball-k-means step, which yields a 1 + f () -approximation algorithm, or the centroid estimation step, based on a sampling idea of Kumar et al. [30], which yields a PTAS with running time exponential in k. Definêd i = min j =î c j − ˆ c i .
Recall that D i = min j =i c j − c i .
(A) Ball-k-means step.
Let B i be the points of X in a ball of radiusˆdradiusˆ radiusˆd i /3 aroundˆcaroundˆ aroundˆc i , and ¯ c i be the centroid of B i .
Return ¯ c 1 , . . . , ¯ c k as the final centers.Lemma 4.13 (Ball-k-means) Suppose that for each i, there is a centerˆccenterˆ centerˆc i such thatˆcthatˆthatˆc i − c i ≤ D i /10.
Let ρ = 36 2 1− 2 and Y i = x ∈ X i : x − c i 2 ≤ r 2 i ρ .
Then Y i ⊆ B i ⊆ X i , and ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i .
The proof of the above lemma is essentially identical to that of Lemma 3.3, and hence is omitted.
(B) Centroid estimation.
For each i, we will obtain a set of candidate centers for cluster X i as follows.
Fix β = 1 1+144 2 .
Define the expanded Voronoi region ofˆcofˆ ofˆc i as follows: for any x ∈ X, letˆcletˆ letˆc(x) denote the centersuch that x − ˆ c(x) = min j x − ˆ c j .
Let R i ⊆ X = {x ∈ X : x − ˆ c i ≤ x − ˆ c(x) + ˆ c i − ˆ c(x)/4}.
Sample 4βω points independently and uniformly at random from R i , where ω is a given input parameter, to obtain a random subset S i ⊆ R i .
Compute the centroid of every subset of S i of size 2 ω ; let T i be the set consisting of all these centroids.
Select the candidates ¯ c 1 ∈ T 1 , . . . , ¯ c k ∈ T k that yield the least-cost solution, and return these as the final centers.Lemma 4.14 (Centroid-estimation) Suppose that for each i, there is a centerˆccenterˆ centerˆc i such thatˆcthatˆthatˆc i − c i ≤ D i /10.
Then X i ⊆ R i, where R i is as defined in the centroid-estimation procedure, and |X i | ≥ β|R i |.
Proof : For any j = i, we have4 5 · c i − c j ≤ ˆ c i − ˆ c j ≤ 6 5 · c i − c j .
Hence, 4D i 5 ≤ ˆ d i ≤ 6D i 5 .
Consider any x ∈ X i that lies in the Voronoi region ofˆcofˆ ofˆc j (sô c(x) = ˆ c j ).
We have x − c i ≤ x − c j , therefore x − ˆ c i ≤ x − ˆ c j + D i +D j 10 ≤ x − ˆ c i + c i − c j /5 ≤ x − ˆ c j + ˆ c i − ˆ c j /4; so x ∈ R i .
Suppose |X i | ≤ β|R i |.
Let a j = |R i ∩X j | |R i | .
So a i 1−a i ≤ β 1−β .
Consider the clustering where we arbitrarily assign some a j 1−a i points of X i to center c j for each j = i. For any x ∈ X i and j = i, we have x − c j 2 ≤ 2(x − c i 2 + c i − c j 2 ).
So the cost of reassigning points in X i is at most2∆ 2 1 (X i ) + 2n i 1−a i · j =i a j c i − c j 2 ≤ 2∆ 2 1 (X i ) + 2β 1−β · j =i a j |R i ||c i − c j 2 .
We also know that for any y ∈ R i ∩ X j , y − c i ≤ y − ˆ c(y) + ˆ c i − ˆ c(y) 4 + D i 10 ≤ y − ˆ c j + y − ˆ c i 2 + D i 10 =⇒ y − c i 2 ≤ y − c j + 1.5D i + D j 10 .
Since D i , D j ≤ c i − c j , this in turn implies that y − c i ≤ 2y − c j + c i − c j /2, which implies that c i − c j ≤ 6 · y − c j .
Therefore, we can bound a j |R i ||c i − c j 2 by 36 · y∈R i ∩X j y − c j 2 .
Hence, the cost of this clustering is at most max 2, 1 + 72β1−β ∆ 2 k (X) ≤ 1 + 1 2 2 ∆ 2 k (X).
The cost of this clustering is also at least ∆ 2 k−1 (X).
This is a contradiction to the assumption that∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X).
This algorithm uses the initialization procedure of Section 4.1.3 followed by a ball-k-means step, and hence runs in time O(nkd + k 3 d).
D1.
Execute the seeding procedure of Section 4.1.3 to obtain k initial centersˆccentersˆ centersˆc 1 , . . . , ˆ c k .
D2.
Run the ball-k-means step of Section 4.2 to obtain the final centers.By Lemma 4.12, we know that with probability 1 − O( √ ), for each c i , there is a distinct centerˆccenterˆ centerˆc i such thatˆcthatˆthatˆc i − c i ≤ D i /10.
Therefore, by Lemma 4.13, for each c i , we have ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i .
Hence, by mimicking the proof of Theorem 3.4, we obtain the following theorem.Theorem 4.15 Assuming that ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X) for a small enough , the above algorithm returns a solution of cost at most 1− 2 1−37 2 · ∆ 2 k (X) with probability 1 − O(√ ) in time O(nkd + k 3 d).
The PTAS combines the sampling procedure of Section 4.1.1 (we could also use the seeding procedure of Section 4.1.3) with the centroid estimation step described in Section 4.2.
E1.
Use the procedure in Section 4.1.1 to pick k initial centersˆccentersˆ centersˆc 1 , . . . , ˆ c k .
E2.
Run the centroid estimation procedure of Section 4.2 to obtain the final centers.The running time is dominated by the exhaustive search in the centroid estimation procedure, which takes time O 2 (4k/βω) nd .
We show that the cost of the final solution is at most (1 + ω)∆ 2 k (X), with probability γ k for some constant γ.
By repeating the procedure O(γ −k ) times, we can boost this to a constant.Theorem 4.16 Assuming that ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X)for a small enough , there is a PTAS for the k-means problem that returns a (1 + ω)-optimal solution with constant probability in timeO(2 O(k(1+ 2 )/ω) nd).
Proof : By appropriately setting ρ in the sampling procedure, we can ensure that with probability Θ(1) k , it returns centersˆccentersˆ centersˆc 1 , . . . , ˆ c k such that for each i, [30] shows that for every i, with constant probability, there is some candidate point c i ∈ T i such thatˆ c i − c i ≤ D i /10 (part (i) ofx∈X i x − c i 2 ≤ (1 + ω)∆ 2 1 (X i ).
The cost of the best-candidate solution is at most the cost of the solution due to the points c 1 ∈ T 1 , . . . , c k ∈ T k , which is at most (1 + ω)∆ 2 k (X).
The overall success probability for one call of the procedure is γ k for some constant γ < 1, so by repeating the procedure O(γ −k ) times we can obtain constant success probability.
We now show that our separation condition implies, and is implied by, the condition that any two nearoptimal k-clusterings disagree on only a small fraction of the data.
Let cost(x 1 , . . . , x k ) denote the cost of clustering X around the centers x 1 , . . . , x k ∈ R d .
We use R(x) to denote the Voronoi region of point x (the centers will be clear from the context).
Let S 1 S 2 = (S 1 \ S 2 ) ∪ (S 2 \ S 1 ) denote the symmetric difference of S 1 and S 2 .
Theorem 5.1 Suppose that X ⊆ R d is -separated for k-means for a small enough .
The following hold:(i) If there are centersˆccentersˆ centersˆc 1 , . . . , ˆ c k such that cost(ˆ c 1 , . . . , ˆ c k ) ≤ α∆ 2 k−1 (X), where 0 < α ≤ 1−401 2 400 , then for eachˆceachˆ eachˆc i there is a distinct optimal center c σ(i) such that |R(ˆ c i ) X σ(i) | ≤ 161 2 |X σ(i) |;(ii) IfˆXIfˆ IfˆX is a point set obtained by perturbing each x ∈ X i by a distance of (at most)∆ k−1 (X) √ n (in any direction) then ∆ 2 k ( ˆ X) = O( 2 )∆ 2 k−1 ( ˆ X).
Proof :Let ρ = α 2 + 1 −1 .
Note that ρ ≥ 400 2 1− 2 .
Define X cor i = x ∈ X i : x − c i ≤ r i√ ρ , and .
Also by Lemma 4.14,let d 2 i = 2 ∆ 2 k−1 (X)/n i .
Note that r 2 i ≤ d 2 i ≤ 2 1− 2 · D 2 i .
cost(ˆ c 1 , . . . , ˆ c k ) > 1 ρ − 1 n i d 2 i = α∆ 2 k−1 (X) givingwe have |X i | ≥ β|R(ˆ c i )| where β = 1 1+144 2 .
Therefore, we get that |R(ˆ c i ) X i | ≤ 2ρ 1 + 1 β − 1 |X i | ≤ 161 2 |X i | for ≤ 12 .
For a point x ∈ X, we usê x to denote its perturbed image inˆXinˆ inˆX.
Note that for any y ∈ R d we havêhavê x − y 2 ≤ 2 x − y 2 + 2 ∆ 2 k−1 (X) n .
Consider the k-clustering ofˆXofˆ ofˆX where we assign all the perturbed points of X i to c i .
The cost of this clustering forˆXforˆ forˆX is at most 2∆ 2 k (X) + 2 2 ∆ 2 k−1 (X).
Conversely, one can obtain a (k − 1)-clustering of X from an optimal (k − 1)-clustering ofˆXofˆ ofˆX by assigning each x ∈ X to the center to whichˆxwhichˆ whichˆx is assigned.
Thus we get that ∆ 2 k−1 (X) ≤ 2∆ 2 k−1 ( ˆ X) + 2 2 ∆ 2 k−1 (X).
Combining the two bounds, we get that ∆ 2 k ( ˆ X) ≤ γ∆ 2 k−1 ( ˆ X) where γ = 8 2 1−2 2 = O( 2 ).
Theorem 5.2 Let ≤ 1 3 .
Suppose that for every k-clusteringˆXclusteringˆ clusteringˆX 1 , . . . , ˆ X k of X of cost at most α 2 ∆ 2 k (X), (i) there exists a bijection σ such that ∀i, | ˆ X i X σ(i) | ≤ |X σ(i) |; AND/OR (ii) there is a bijection σ such thatk i=1 | ˆ X i X σ(i) | ≤ k−1 |X|.
Then, X is α-separated for k-means.
Proof : Let R 1 , . . . , R k−1 be an optimal (k − 1)-means solution.
We will construct a refinement of R 1 , . . . , R k−1 and argue that this has large Hamming distance to X 1 , . . . , X k , and hence has cost at least α 2 ∆ 2 k (X).
Since the cost of R 1 , . . . , R k−1 is at least the cost of any refinement of it, this will imply that ∆ 2 k−1 (X) ≥ α 2 ∆ 2 k (X).
Let R k−1 be the largest cluster.
We start with an arbitrary refinement R 1 , . . . , R k−2 , ˆ X k−1 , ˆ X k wherêX k−1 ∪ ˆ X k = R k−1 , ˆ X k−1 , ˆ X k = ∅.
If the cost of this k-clustering is at least α 2 ∆ 2 k (X) then we are done.
So assume that this is not the case, and let σ be the claimed bijection.
For part (i), we introduce a large disagreement by splittingˆXsplittingˆ splittingˆX k−1 ∩ X σ(k−1) andˆXandˆ andˆX k ∩ X σ(k) into two equal-sized halves, A k−1 ∪ B k−1 and A k ∪ B k respectively, and "mismatching" them.
More precisely, we claim that the clustering R 1 , . . . , R k−2 , X k−1 = ( ˆ X k−1 \ A k−1 ) ∪ A k , X k = ( ˆ X k \ A k ) ∪ A k−1 has large Hamming distance.
For any bijection σ , if σ (i) = σ(i) for i ≤ k − 2, then|R i X σ (i) | ≥ |R i ∩ X σ(i) | ≥ (1 − )|R i |; otherwise, σ (k) ∈ {σ(k − 1), σ(k)}, so |X k X σ (k) | ≥ 1− 2 |X k | ≥ |X k | since X k \ X σ(k−1) ⊇ B k , X k \ X σ(k) ⊇ A k−1 .
For part (ii), since |R k−1 | ≥ |X| k−1 , we have | ˆ X k−1 ∩ X σ(k−1) | + | ˆ X k ∩ X σ(k) | ≥ 1− k−1 |X|.
After the above mismatch operation, for any bijection σ , the total disagreement is at least|X k−1 X σ (k−1) | + |X k X σ (k) | ≥ 1 2 | ˆ X k−1 ∩ X σ(k−1) | + | ˆ X k ∩ X σ(k) | ≥ 1− 2(k−1) |X| ≥ k−1 |X|.
We initiate a mathematical analysis of Lloyd-style methods that attempts to explain the performance of these heuristics.
We show that if the data satisfies a natural "clusterability" or "separation" condition, then various Lloyd-style methods perform well and return a near-optimal clustering.
Our chief algorithmic contribution is a novel and efficient sampling procedure for seeding Lloyd's method with initial centers, such that if the data satisfies our separation condition then (even) a single Lloyd-type descent step suffices to yield a constant-factor approximation.
It may have struck the reader that there is something too good about our performance guarantees.
Since we need to use only one round of Lloyd's method, we cannot possibly be taking full advantage of the algorithm, in particular, its capacity to start out with a seeding that is unbalanced across clusters and correct it by shifting centers from one cluster to another.
The extent to which Lloyd's method is successful at doing so is, in fact, unclear, and for this reason there is much literature exploring the merits of different seeding procedures.
Nevertheless we feel that Lloyd's method is better than we have accounted for, and that our results fall short of explaining (or predicting) the performance of Lloyd-style methods; instead, they suggest that our separation condition is perhaps too stringent (and too restrictive as a measure of data-clusterability).
If so, then the main open question that emerges from our work is to demonstrate a condition weaker than ours, for which the initial seeding is not necessarily close to an optimal solution, but yet Lloyd's algorithm can be shown to converge in a small number of rounds to a near-optimal solution.An orthogonal research direction is to explore further implications of our separation condition (or similar ones) for the k-means and possibly other clustering problems.
For instance, it might be possible to obtain stronger, or more general, algorithmic results.
Nissim et al. [40] have obtained a result in this vein: they exploit the robustness of our separation condition to design secure, privacy-preserving ways of computing a near-optimal k-means solution when the data satisfies our separation condition.
