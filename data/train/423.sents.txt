This work describes a grapheme-based approach of English-to-Chinese (E2C) transliteration, which includes many-to-many alignment models and conditional random fields using accessor variety (AV) as an additional feature based on source graphemes.
Experimental results indicate that the AV of a given English segment can generally improve effectiveness of E2C transliteration.
As a subfield of computation linguistics, transliteration refers the phonetic translation of proper nouns and technical terms across languages.
Several terms are used interchangeably in the contemporary research literature for the conversion of names between two languages, such as transliteration, transcription, and sometimes Romanization, especially if Latin scripts are used for target strings [1].
This work adopts the same definition of transliteration as during the NEWS 2009 workshop at ACL-IJCNLP 2009 [2] to narrow down "transliteration" to the conversion of a given name in the source language (a text string in the source writing system or orthography) to a name in the target language (another text string in the target writing system or orthography), such that the target language name must fit three specific requirements as follows: Phonemically equivalent to the source name;  Conforms to the phonology of the target language;  Matches the user intuition of the equivalent of the source language name in the target language, considering the culture and orthographic character usage in the target language.
Among the numerous applications of transliteration of named entities include machine translation, corpus alignment, cross-language information retrieval, information extraction, and automatic lexicon acquisition.
Transliteration modeling approaches can be classified as phoneme-based, grapheme-based, and a hybrid of phoneme and grapheme.Numerous studies have focused on the phoneme-based approach [3][4][5].
Assume that E denotes an English name and C represents its Chinese transliteration.
The phoneme-based approach converts E into an intermediate phonemic representation p, and then converts p into its Chinese counterpart C. Doing so transforms both the source and target names into comparable phonemes, making it relatively easy to determine the phonetic similarity between the two names.
The grapheme-based approach, also known as direct orthographical mapping (DOM), has attracted much attention, too [6][7][8][9][10][11][12][13].
This approach treats the transliteration as a statistical machine translation problem under monotonic constraint, and aims to obtain the bilingual orthographical correspondence directly to reduce the possible errors introduced in multiple conversions.
The hybrid approach attempts to utilize both phoneme and grapheme information for transliteration.
Oh and Choi [14] proposed a strategy to include both phoneme and grapheme features in a single learning process, and subsequently several investigators compared selections between purely phonemic or graphemic features and mixtures of those features [15][16][17][18][19][20][21][22].
This work presents a grapheme-based approach of English-to-Chinese (E2C) transliteration by using many-tomany alignment (M2M-aligner) [15] and conditional random fields (CRF) [23] with an additional feature of accessor variety (AV) [24].
The remainder of this paper is organized as follows.
Section 2 briefly introduces related works involving CRF, M2M-aligner, and AV.
Section 3 then explains the concept of transliteration using M2M-aligner and CRF.
Next, Section 4 summarizes the experimental results, followed by a discussion.
Conclusions are finally drawn in Section 5, along with recommendations for future research.
Yang et al. [10] proposed a two-step CRF model for direct orthographical mapping (DOM) based machine transliteration, in which the first CRF segments a source word into chunks and the second CRF maps the chunks to a word in the target language.
Reddy and Waxmonsky [12] presented a phrase-based translation system that characters are grouped into substrings to be mapped atomically into the target language, which showed how substring representation can be incorporated into a CRF model with local context and phonemic information.
Shishtla et al. (2009) [7] adopted a statistical transliteration technique that consists of alignment model of GIZA++ [25] and CRF model.
Aramaki and Abekawwa [9] who focused on fast decoding and easy implementation also used GIZA++ and CRF for sequential labeling of DOM-based transliteration, while Oh et al. [19] applied target grapheme and phoneme to CRF, maximum entropy model [26], and margin infused relaxed algorithm [27].
The approach of this work is similar to the techniques of Shishtla et al. and Aramaki and Abekawwa, yet this work focuses on the additional AV feature of CRF and uses M2M-aligner, which will be described in the next section, instead of GIZA++.
Jiampojamarn et al. argued that previous work has generally assumed one-to-one alignment for simplicity, but letter strings and phoneme strings are not typically in the same length, so null phonemes or null letters must be introduced to make one-to-one-alignments possible [15].
Furthermore, two letters frequently combine to produce a single phoneme (double letters), and a single letter can sometimes produce two phonemes (double phonemes).
For example, the English word "ABERT" with its Chinese transliteration "阿贝特", which Jiampojamarn et al. referred as "phonemes", is aligned as follows.The letters "BE" are an example of the double letter problem which mapping to the single phoneme " 贝 ."
These alignments provide more accurate grapheme-to-phoneme relationships for a phoneme prediction model.
Hence the M2M-aligner is for alignments between substrings of various lengths and based on the expectation maximization (EM) algorithm.
For more details of the algorithm, readers are encouraged to explore previous works [15][16][17] [28].
Despite the ambiguity between Chinese transliteration and phoneme, the above paragraph of the opinion of Jaimpojamarn et al. indicates a particular problem of E2C transliteration, that the training data comprised pairs of names written in source and target scripts lacks explicit grapheme-level alignment.
This work uses M2M-aligner as an unsupervised method for generating alignments of the training data, which provide hypotheses of DOM without null graphemes.
Feng et al. developed an accessor variety (AV) to evaluate the likelihood that a character substring is a Chinese word [24].
Several works have adopted another measurement method called boundary entropy or branching entropy (BE) [29][30][31][32][33][34][35].
This determination is closely related to a particular perspective of n-gram and information theory of cross entropy or perplexity.
According to Zhao and Kit [36], AV and BE both assume that the border of a potential Chinese word is located where the uncertainty of successive characters increases.
That work assumed that AV and BE are discrete and continuous versions, respectively, of the fundamental work of Harris [37]; in addition, AV was adopted as an additional feature of CRF-based Chinese Word Segmentation (CWS).
The AV of a string s is defined as)} ( ), ( min{ ) ( s R s L s AV av av  (1)In (3), L av (s) and R av (s) denotes the number of distinct preceding and succeeding characters, except when the adjacent character is absent due to a sentence boundary; the pseudo-character of the beginning or end of a sentence is then accumulated indistinctly.
Feng et al. also developed more heuristic rules to remove strings that contain known words or adhesive characters [24].
For a strict meaning of unsupervised features and for simplicity, this work does not include those additional rules.
Unlike previous works of CRF-based transliteration involving DOM [7][9] [10][12] [19] that usually report only one configuration of CRF and assume the initial alignments of name pairs for training have been prepared by GIZA++ (which often needs external data and is relatively complicated to use) or by human annotators, this section provide an easy-to-reproduce automatic procedure using EM-based M2M-aligner and CRF, along with thorough experiments of different feature sets and context depths for CRF configurations.
The alignment models maximize the likelihood of the observed (source, target) word pairs by using the EM algorithm.
However, the initialization often inhibits the performance of the EM algorithm.
To obtain better alignment results, this work sets the "maxX" parameter, referring to the maximum size of sub-alignments in the source side is 8, and set the "maxY" parameter, referring to the maximum size of sub-alignments in the target side is 1, since one of the well-known a priori of Chinese is that almost all Chinese characters are monosyllabic, which reflects the situation of "double phoneme" mentioned in Section 2.
b. Notably, this work follows the definition of grapheme described by Oh and Choi [38] to prevent from confusion of phoneme, grapheme, character, and letter, that graphemes refer to the basic units (or the smallest contrastive units) of written language: for example, English has 26 graphemes or letters or characters, Korean has 24, and German has 30.
In the proposed labeling scheme, the following configurations are used in E2C transliteration, as shown in Table III.
Additionally, C i refers to the input characters bound individually to the prediction label at its current position i. Consider Table II as an example.
If the current position is at label "B 纳", features generated by C -1 , C 0 and C 1 are "A" "N" and "A", respectively.
Notably, a prediction label may either consist of a positioning tag and a Chinese character, or simply be the positioning tag.
In particular, while the first, the second, the fifth and the sixth "standard runs" (i.e. using only the parallel names provided by the corpus for transliteration task to ensure a fair evaluation) appended Chinese characters to all of the positioning tags, the third and fourth standard runs attached Chinese characters to the tag B only, but introduce an additional tag E representing the ending position of the chunk.
The necessity of AV is primarily on the demand for semi-supervised learning.
Since AV can be extracted from large corpora without any manual segmentation or annotation, hidden variables underlying frequent surface patterns of languages may be captured via an inexpensive and unsupervised algorithm such as suffix array.
Unsupervised feature selection of AV or similar features has generally improved effectiveness of supervised CWS on cross-domain and unlabeled data [40], and this work consequently considers that AV of un-segmented English names from training, development, and test data might help enhancing E2C transliteration.This work extends the findings of Zhao and Kit [41] to a unified representation of AV features.
The representation accommodates both the character position of a string and the string's likelihood ranking by the logarithm.
Formally, the ranking function for a string, s, with a score, x, counted by AV can be expressed as1 2 2 , ) (     r r x if r s f(2)The logarithm ranking mechanism in (4) is inspired by Zipf's law to alleviate the potential data sparseness of infrequent strings.
Rank r and the corresponding character positions of a string are then concatenated as feature tokens.
To clarify the appearance of feature tokens, Table IV presents a sample representation of AV.For instance, consider strings with two characters, in which one of the strings "RA" is ranked r = 5.
Therefore, the column of two-character feature tokens has "R" denoted as 5B and "A" denoted as 5E.
If another two-character string, "AR," competes with "RD" at the position of "R" with a higher rank of r = 5, then 5E is selected to represent the feature of the token at a certain position.
Notably, when string "RA" conflicts with string "AN" at position "A" with the same rank of r = 5, the corresponding character position with the ranking of the leftmost string, which is 5E in this case, is applied arbitrarily.
1 C 0 , C -1 , C 1 C -2 , C 2 C 0 C 1 , C -1 C 0 C -2 C 1 , C 1 C 2 No B, I B and I 2 C 0 , C -1 , C 1 C -2 , C 2 C 0 C 1 , C -1 C 0 C -2 C 1 , C 1 C 2Yes B, I B and I3 C 0 , C -1 , C 1 C -2 , C 2 C 0 C 1 , C -1 C 0 C -2 C 1 , C 1 C 2 No B, I, E B 4 C 0 , C -1 , C 1 C -2 , C 2 C 0 C 1 , C -1 C 0 C -2 C 1 , C 1 C 2 Yes B, I, E B 5 C 0 , C -1 , C 1 C 0 C 1 , C -1 C 0 No B, I, E B, I and E 6 C 0 , C -1 , C 1 C 0 C 1 , C -1 C 0Yes B, I, E B, I and E This section summarizes the experimental results.
Experiments were conducted on English-Chinese name pairs of shared tasks NEWS-2009 (NEWS09) [2] and NEWS-2010 (NEWS10) [42].
Evaluation metrics and evaluation scripts of NEWS are adopted as well.
The following notation is further assumed: N: total number of source words in the test set;  n i : number of reference transliterations for i-th name in the test set (n i ≧ 1);  r i,j : j-th reference transliteration for i-th name in the test set;  c i,k : k-th candidate transliteration (system output) for i-th name in the test set (1 ≦ k ≦ 10);  K i : number of candidate transliterations produced by a transliteration system.
Word Accuracy in Top-1 (ACC), also known as Word Error Rate, measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system.
ACC = 1 means that all top candidates are correct transliterations i.e. they match one of the references, and ACC = 0 means that none of the top candidates are correct.
otherwise 0; : if 1 1 1 ,1 , ,            N i i j i j i c r r N ACC (5)Fuzziness in Top-1 (Mean F-score) measures how different, on average, the top transliteration candidate is from its closest reference.
F-score for each source word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references.
Precision and Recall are calculated based on the length of the Longest Common Subsequence between a candidate and a reference as  ) , ( 2 1 ) , ( r c ED r c r c LCS    (6)where ED is the edit distance and |x| is the length of x. For example, the longest common subsequence between "abcd" and "afcde" is "acd" and its length is 3.
The best matching reference, that is, the reference for which the edit distance has the minimum is taken for calculation.
If the best matching reference is given by 4          k i j i k i j i i c r c r RR (11) 1 1    N i i RR N MRR (12)MAP ref measures tightly the precision in the n-best candidates for i -th source name, for which reference transliterations are available.
If all of the references are produced, then the MAP is 1.
Let's denote the number of correct candidates for the i -th source word in k -best list as ) , ( k i num .
MAP ref is then given byMAP ref  1 N 1 n i i N  num(i, k) k1 n i       (13) The accuracy and F score between development sets (Dev) and test sets (Test) from NEWS10 and NEWS09 were then compared.
Table V and VI list the results of the proposed system.Many pilot tests have been undertaken with both the training set and the development set, followed by evaluation of the development set for optimizing feature combinations and M2M and Wapiti CRF parameters (with Wapiti's default value of Gaussian prior as 10,000).
Sixes configurations of standard runs are selected from them as representative comparisons.
The fifth and the sixes standard runs use only unigram and bigram features within the window size of the context in one character.
Comparing the fifth standard run with the sixth standard run clearly reveals that AV features significantly improve performances in terms of the accuracy and F-score on the development sets.
The third and the fourth standard runs indicate that increasing the window size of the context and decreasing the variety of prediction labels degrades performances slightly.
The first and the second standard runs label Chinese character thoroughly with less specific positioning tags intentionally, to facilitate the tradeoff between effectiveness and efficiency of CRF training phases.
Notably, the second standard runs on test sets are the only places where AV features fail to improve performances of E2C transliteration.Since improvements on the test sets are not as good as expected, NEWS data is then carefully investigated.
Despite the possibility of over-fitting because of CRF training parameters, one particular phenomenon has been noticed: the development sets contain phrasal named entities that are unseen in the training sets and unused in the test sets.
Specifically, E2C word pairs are occasionally impure transliteration and aligned in very different character lengths, such as the name pair of "COMMONWEALTH OF THE BAHAMAS" and "巴哈马 / 联邦."
A few E2C name pairs even consist of pure translations, especially for short Chinese names, such as "ARAL SEA" and "咸 / 海."
Such names can lead to noisy alignments during the training phases.
Li et al. also noted that place and company names are sometimes translated in combination of transliteration and meanings, for example, "VICTORIA FALL" becomes "维多利亚 / 瀑布", and then suggested that directed orthographical mapping can be easily extended to handle such name translations [5].
In fact, the M2M parameter "maxX" of this work has been designed for these phrasal structure to be relatively larger and less symmetrical to the parameter "maxY" than those in previous works that normally set both X and Y to 2 as default values.
This work analyzes this phenomenon, which is referred to "semi-semantic transliteration" for convenience, by acquiring NEWS Chinese to English (C2E) backtransliteration corpus.The C2E experiments, however, encounter a serious problem of CRF L-BFGS training requirement in terms of space complexity.
Therefore the experimental results of C2E transliterations are incomplete and erroneous, since C2E transliteration with the proposed method produces too many labels and features to train a CRF model with the whole training set.
In our experience, even a computer with 24GB memory capacity is insufficient for such training.
Similar challenges have been noted in the report of NEWS-2010 and reasoned that C2E transliteration is a one-to-many mapping while E2C is a many-to-one mapping [42].
Cohn et al. [43] shows that the time complexity of a single iteration, which is required for a typical CRF training using L-BFGS, is O(L 2 NTF), where L is the number of labels, N is the number of sequences, T is the average length of the sequences, and F is the average number of activated features of each labeled clique.
Meanwhile, it is still an issue to state the precise bound on the number of iterations required for certain tasks.
Moreover, efficient CRF implementations usually cache the feature values for every possible clique labeling of the training data, which leads to a memory space requirement of O(L 2 NTF), too.
According to those analytical results, this work proposes a contribution rate C of the CRFbased transliteration method.
For example, the contribution rate of ACC on the test set C ACC Test indicates that for each standard run how many ACC point of the test set a unit of computational effort in terms of log 2 (L 2 F total ) can provide, where F total is the number of all activated features, which equals to NTF approximately.
Judging by the contribution rates of each run, Table VII and Table VIII shows that the first and the second standard runs are usually more efficient than others, except for F-Score.
This work approximates a phonological context for E2C transliteration with AV as additional graphemic features.
As the standard runs are limited by the use of corpus, most systems are implemented under the direct orthographic map.
Experimental results indicate that without an explicit phonemic representation of the English source names, using AV features of a given segment can approximate the local phonological context affecting the rendition of a specific segment in Chinese.
This work also suggests appropriate parameters of M2M-aligner and optimal configurations of CRF labeling scheme and context depth for E2C transliteration tasks.To resolve the limitations of this work, we recommend that future research investigate the feasibility of applying different approaches to recognize semi-semantic transliteration with efficient memory usages.
This research was supported in part by the National Science Council under grant NSC 100-2631-S-001-001, and the research center for Humanities and Social Sciences under grant IIS-50-23.
Ted Knoy is appreciated for his editorial assistance.
The authors would like to thank anonymous reviewers for their constructive criticisms.
