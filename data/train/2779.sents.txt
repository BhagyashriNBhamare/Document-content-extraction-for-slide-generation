Fast event ordering is critical for delay-sensitive edge computing applications that serve massive geographically distributed clients.
Using a centralized cloud to determine the event order suffers from unsatisfactory la-tency.
Naive edge-centric solutions, which designate one edge node to order all the events, have scalability and single point of failure issues.
To address these problems, we propose EdgeCons, a novel consensus algorithm optimized for edge computing networks.
EdgeCons achieves fast consensus by running a sequence of Paxos instances among the edge nodes and dynamically distributing their leadership based on the recent running history.
It also guarantees progressiveness by incorporating a reliable, backend cloud.
A preliminary evaluation shows that EdgeCons works more efficiently than the state-of-the-art consensus algorithms, in the context of achieving fast event ordering in edge computing networks.
Edge computing (aka.
fog computing [4], cloudlet [33] and MEC [31]) is a concept that has gained lots of attention recently [32,35,43].
The basic idea of edge computing is to provide elastic resources like cloud computing at the edge of network, such that the requests for the resources from client devices will be handled by edge computing nodes at places closer to the users.
Modern applications and services, such as real-time video processing, cognitive analytics in critical missions and online video games, can benefit a lot from the decentralized edge computing paradigm.
Clients connected to the edge will experience faster service response.
The nearby access saves network bandwidth and lowers the peak workload to the cloud.
The deployment of applications can adapt to the geographic distribution of users and make the most efficient usage of the resources on the edge nodes.
As edge computing possesses so many advantages, we expect more and more applications and services be deployed on edge computing platforms.When massive client devices are connected to the edge, a large volume of events will be generated at the edge that need to be synchronized in a consistent order among all the edge nodes.
Event ordering is fundamental for the correctness of distributed systems [17].
A cloudcentric solution employs cloud servers to order all the events.
The drawback is that it may take a long time to determine the order of the events, as massive events have to go to the cloud in the first place.
Because many modern applications and services are human-centric, latency has a huge impact on the user experience.
For example, lower latency will greatly improve the user experience of games which highly rely on the fast reaction of the players [6].
Therefore, in this paper, we study how to achieve fast event ordering for large-scale delay-sensitive distributed applications in edge computing networks.It is quite challenging to achieve this goal.
In contrast to cloud networks and the cloud servers, edge computing networks are usually heterogeneous, and the edge computing nodes are less reliable.
Events transmitted over such a network are subject to unpredictable delivery time and packet loss rate, not to mention the large amount of events received by all the edge nodes for ordering.
To be noted that, while edge nodes are less reliable, edge node failures are still rare.
Therefore, opportunistically using a local edge computing network to order the events has performance gain compared to barely using a remote cloud.
In our previous study [41], we have shown that the Amazon EC2 cloud leads to much higher latency than a locally-built edge computing network.
A naive edgecentric solution can simply select an edge node to order all the events.
The drawback is that the designated edge node will become the performance bottleneck of the system and a single point of failure.
Using timestamps to order the events is also a bad idea, which has been thoroughly discussed in the literature [29,18].
To the best of our knowledge, we are the first to study the consensus approaches for edge computing networks.
Most existing researches of edge computing focus on the client-to-edge interactions, such as how to accommodate computation and storage tasks for the clients on edge nodes.
Our problem concentrates on event ordering in the scope of interconnected edge nodes, and we will show the traits of this problem via an online gaming application.
Existing consensus protocols can hardly meet the latency requirements imposed by this problem.
Multi-Paxos [19] runs Paxos instances with a designated leader.
Since edge nodes are not as reliable and powerful as cloud servers, the leader edge node will encounter the same issues as in the naive edge-centric solution.
Mencius [24] addresses the issues by assigning the leadership evenly among the system nodes.
The main problem of Mencius is that a slow system node will deteriorate the overall system performance.
E-Paxos [25] avoids this problem by opportunistically executing Fast Paxos [21] instances instead of Paxos instances.
Nevertheless, it suffers from heavier network burdens than Multi-Paxos and Mencius for achieving consensus.To this end, we design EdgeCons, a consensus algorithm that achieves fast event ordering for large-scale distributed applications in edge computing networks.
Similar to Mencius, EdgeCons runs a sequence of Paxos instances on the edge, but it distributes the leadership of the Paxos instances based on the recently running history of the consensus process.
EdgeCons also guarantees the progressiveness of consensus, which is indispensable for the edge computing scenario.
It achieves so by employing a backend cloud that never fails or becomes networkpartitioned as a reliable conflict resolver in the system.To summarize, we make the following contributions.
• We formulate the problem of achieving fast event ordering for large-scale delay-sensitive distributed applications in edge computing networks, and propose realistic application scenarios related to this problem.
• We design a novel consensus solution to this problem, which dynamically distributes the leadership of a Paxos instance sequence among the edge nodes based on the recent running history, and guarantees progressiveness by incorporating a reliable, backend cloud.
• Preliminary simulation results reveal that our solution works more efficiently than existing consensus solutions in the literature, in the context of achieving fast event ordering in edge computing networks.
Before digging into our solution, we briefly introduce some preliminaries on edge computing and consensus.
Edge Computing.
Edge computing aims at serving the end users at the edge of network, providing better network conditions than cloud computing.
It has attracted a lot of research effort recently [2,14,26,36,28,1,15,10,7,34,40,22,44,12,11,42,23].
are further backed by a cloud at the core of network.
It is worth mentioning that Figure 1 only showcases one typical architecture of edge computing among many others.
Other edge computing architectures may contain less than or more than two levels of edge nodes.
In addition, some architectures may have no cloud at the backend.
Consensus.
Consensus is a fundamental problem in distributed computing: How to achieve overall system reliability in the presence of a number of faulty nodes?
It has been studied for decades but remains a hot research topic in academia [18,19,21,20,3,24,25,30,16,46,8,45,38].
Among existing consensus solutions, we focus on the Paxos-based ones.
On one hand, to the best of our knowledge, most, if not all, consensus solutions that achieve strong consistency under the typical FLP [9] setting are essentially variants of Paxos, such as Raft [30].
On the other hand, other consensus solutions, such as the blockchain-based ones [27,39], cannot guarantee strong consistency, thus violating our design goal.
Note that an execution of the Paxos algorithm is usually called a Paxos "instance" in the literature.
There are two types of phases in a classical Paxos instance: Phase 1 is for electing the leader, while Phase 2 is for deciding a value on behalf of the entire system.As the Paxos-based solutions employ the so-called "state machine replication" method, consensus is also related to the research problem of how to build replicated and reliable services for the users across a large geographic area.
Among the literature on consensus, three Paxos-based consensus solutions, i.e., Multi-Paxos [19], Mencius [24] and E-Paxos [25], are more related to our solution than the others.
Multi-Paxos runs a sequence of Paxos instances with a designated leader.
Mencius improves Multi-Paxos by distributing the leadership evenly among the nodes.
E-Paxos opportunistically runs Fast Paxos [21] instances instead of Paxos instances, and improves the system performance by delaying the resolution of conflicts.
EdgeCons shares some similarities with Mencius.
Nevertheless, it is different from all the aforementioned consensus solutions, because it is optimized for the application scenarios found in edge computing environments.
In-depth discussion will be provided in the following part of the paper.
In this section, we first propose an application scenario for which EdgeCons is designed, and then discuss the approach of EdgeCons in detail.
To achieve a better understanding on the problem EdgeCons tries to solve, consider the following scenario.
A game company operates a massive multi-player online gaming service to the players located in a city.
The players can connect their own devices, such as smartphones and AR devices, to a remote game server through wireless links.
The latency perceived by the players is critical in achieving satisfactory gaming experience, and a large number of players may play the game simultaneously.
As such, it is not a good choice to perform the main part of the computation for each player on the remote server, no matter it is a cloud server or an edge node.
This is because a cloud server cannot provide low enough latency [41,13], while an edge node cannot afford the heavy computation for so many players.
We hence make the following assumptions for this scenario.
• The main part of computation for each player is done locally on the player's client device, and the main task of the remote gaming service is to order the input from all the online players.
More specifically, the client device runs a piece of gaming software that forwards the player's input to the remote gaming service and receives the ordered input by all the online players.
The gaming software also performs computation on the ordered input and renders the result to the player.
The computation performed on the client device is deterministic, similar to what has been discussed in the literature [8], such that all the players have consistent views of the game.
• The input forwarded to the remote gaming service is of a small size.
It may include the data collected from the touch screen, the accelerator, and/or the GPS equipped on the client device, but does not contain any multimedia data such as the voice of the player.
The output of the remote gaming service is thus also of a small size, compared to the video stream output typically seen by existing remote gaming solutions.
We also assume that the client devices are powerful enough to perform the computation of the game locally [37].
As such, the game company cannot adopt user-side solutions, as long as it wants to guarantee strong consistency on the event order across the system.
The best design choice that the game company can make, from our point of view, is to build and utilize an edge computing network for input ordering.
To be more specific, the company sets or rents several edge nodes in the city, and deploys the input-ordering service on them.
The edge nodes are well scattered, such that their service regions cover the whole city with modest overlapping, and they can communicate with each other through a wide area network (WAN).
The players connect their client devices to the nearest edge nodes, interacting with the input-ordering service.
As discussed in Section 1, naive solutions cannot work properly or efficiently in such a situation, while existing consensus algorithms likewise cannot fully exploit the potential of the edge computing network.
This motivates us to design EdgeCons, a novel consensus algorithm for achieving fast event ordering in edge computing networks.
As mentioned previously, EdgeCons shares some similarities with Mencius.
In particular, EdgeCons distributes the leadership of the Paxos instances among the edge nodes, and when an edge node works as the leader, it starts from the state in which it has already run Phase 1 for the initial round.
Therefore, all the leader edge nodes skip Phase 1 and start from Phase 2, which improves the system performance.Nevertheless, EdgeCons is different from Mencius in the following aspects.
First, EdgeCons does not distribute the leadership evenly like Mencius, but based on the running history of the system.
This allows EdgeCons to distribute the leadership more wisely, and makes the system performance better.
Second, unlike Mencius, EdgeCons relaxes the assumption that the network links between the edge nodes are FIFO.
The edge nodes are free to use non-FIFO links, such as links based on UDP, and the messages transmitted via those links may be reordered during the transmission.
Last, EdgeCons assumes that there is a cloud behind the edge network that never fails or becomes network-partitioned.
By incorporating such a backend cloud into the system, EdgeCons breaks the assumption made by the FLP paper [9], and guarantees the progressiveness of the consensus process.We summarize the rules of EdgeCons as follows.
• EdgeCons divides the consensus process into epochs.
executed from Phase 2; Phase 1 is considered already executed by the leader for the initial round.
• Unlike Mencius, which distributes the leadership of the Paxos instances evenly among all the nodes, EdgeCons distributes the leadership dynamically according to the running history of the system.
More specifically, EdgeCons counts the number of the effective Paxos instances under the leadership of each edge node in the previous N epoch epochs (N epoch is a predefined positive value), denoted by N ef,i for edge node i (i = 1, .
.
, n), and distributes the leadership for the next epoch accordingly, proportional to the N ef,i values.
Note that a Paxos instance is considered effective if and only if it has made the system agree on a proposed value.
• To distribute the leadership for the next epoch, EdgeCons arranges the upcoming Paxos instances in a way that the Paxos instances are assigned to each edge node as evenly-scattered in the epoch as possible, such that they are separated by the same or similar numbers of Paxos instances assigned to the others.
Because of the deterministicity of this algorithm, the edge nodes can learn the distribution of leadership for the upcoming epoch independently, without the need of interacting with the other edge nodes.
• EdgeCons also involves a backend cloud into the algorithm.
The cloud resides at the core of network, and is therefore less efficient than the edge nodes in exchanging the messages of events.
Nevertheless, it is far more reliable than the edge nodes.
We assume that the cloud never fails or becomes network-partitioned, so it is always reachable to the edge nodes.
• In addition to the Paxos instances assigned to the edge nodes, there are also quasi-Paxos instances assigned to the cloud in each epoch, which work as follows.
When an edge node has failed to make the system agree on a value it intends to propose for N fail times (N fail is a pre-defined positive value), it transmits the value to the cloud.
The cloud collects all such values, orders them by their arrival time, and announces them to the edge nodes in the next quasi-Paxos instance.
• When a quasi-Paxos instance is over, the cloud will initiate the next quasi-Paxos instance by sending the values it has collected in-between the two quasi-Paxos instances to all the edge nodes.
In case that no value has been collected, the cloud will send a SKIP message to the edge nodes.
The edge nodes must accept the values (or the SKIP message), and reply an OKAY message to the cloud.
Having collected the OKAY messages from a majority of the edge nodes, the cloud considers that the current quasi-Paxos instance is over, and starts the next quasi-Paxos instance.
• The edge nodes cannot skip the quasi-Paxos instances.In other words, they cannot execute the Paxos instances posterior to each of the quasi-Paxos instances they have encountered, until they have received either the ordered values or a SKIP message from the cloud.The consensus instances assigned to the cloud are called quasi-Paxos instances, because they are similar to at first glance, but fundamentally different from the Paxos instances in Multi-Paxos.
Being highly reliable, the cloud can actually guarantee consensus without collecting the OKAY messages from a majority of the edge nodes.
We design EdgeCons in a way that the cloud collects the OKAY messages only for the performance considerations.
Moreover, the cloud is not included in any quorum, no matter the consensus instance is a Paxos one or a quasi-Paxos one.
The main purpose of involving the backend cloud, as it can be seen, is to help the edge nodes propose values when their Paxos instances have been frequently skipped by the others for whatever reasons.
This guarantees the progressiveness of the consensus process, and sets a logical "upper bound" on the latency perceived by the clients.
Note that this statement does not violate the FLP result [9], because EdgeCons makes a different assumption by involving the cloud.
The cloud is barely an arbitrator, but not a replica as the edge nodes, because it does not maintain a local state for the event log.
Finally, in EdgeCons, the leader edge node proposes at most one value in a Paxos instance, but the cloud can propose potentially many values in a quasi-Paxos instance.Since the values are of small data amount, proposing many values does not make a big difference in the transmission time with proposing one value.It is also worth mentioning that the quasi-Paxos instances should be well distributed in each epoch, such that the cloud executes them continuously, and the edge nodes cross them naturally without being blocked for a long time.
In addition, EdgeCons makes a strong assumption that the cloud never fails or becomes networkpartitioned.
In fact, clouds in the real world sometimes do experience temporary outages [5].
However, we believe that it is still possible to build a highly reliable cloud-based arbitrator in practice, e.g., by employing the clouds from different companies (Google, Amazon, Microsoft, etc.) and utilizing Paxos to coordinate them.
Figure 2 depicts a simple example of how EdgeCons works.
Due to the space limit, the figure only shows the first two epochs of the consensus process, and each epoch only contains 24 Paxos instances and 2 quasi-Paxos instances.
In spite of its simplicity, Figure 2 depicts some important aspects of EdgeCons.
In Epoch 1, for example, the leadership of the Paxos instances is distributed in a round-robin way as Mencius.
When Epoch 1 ends, Edge Node A has contributed 6 effective Paxos instances, Edge Node B has contributed 2 and Edge Node C has contributed 4.
Consequently, in the upcoming Epoch 2, Edge Node A possesses half of the Paxos instances, Edge Node B possesses one sixth and Edge Node C possesses one third.
Since Epoch 1 is the only epoch before Epoch 2, EdgeCons can only refer to this epoch when determining the leadership distribution in Epoch 2.
For the following epochs, however, more epochs can be referred to as long as N epoch is larger than 1.
To examine the efficiency of EdgeCons, we conduct a first-step simulation experiment with the following settings.
The round-trip time (RTT) between the client and the edge is 10 ms, and the edge nodes have a RTT of 10 ms to the others, except a slow one, which has a RTT of 40 ms to the others.
The cloud has a RTT of 60 ms to the edge.
Each edge node can only transmit no more than 10,000 messages to the others in one second.
The workload is that 2,000 events have been sent from the client to each edge node in one second, with their intervals uniformly distributed.
Two cases, i.e., a system with 5 edge nodes and another with 7 edge nodes, are tested.
The edge nodes in EdgeCons will ask the cloud to propose a value after two failures, those in Mencius will try to skip a Paxos instance after waiting for 80 ms, and those in E-Paxos will try to urge the execution of a consensus instance after waiting for 80 ms. With these settings, we compare the user-perceived latency under different consensus algorithms.
Figure 3 depicts the results.
It can be seen from Figure 3 that EdgeCons achieves the best performance in both cases.
Note that the edge network is assumed non-FIFO: Mencius cannot piggyback SKIP messages on other messages.
Consequently, Mencius sends more messages than EdgeCons, especially when there is a slow edge node in the system.
EPaxos also needs to deal with more messages, and it is likely to encounter more conflicts than EdgeCons.
MultiPaxos suffers from the problem that the leader is the performance bottleneck.
It is also worth mentioning that in the second case of the experiment, Mencius, E-Paxos and Multi-Paxos are overwhelmed by the workload, which lasts for one second in the simulation.
With the increase of the workload duration, the user-perceived latency of these three algorithms will increase dramatically.
In this paper, we propose a novel consensus protocol, EdgeCons, for achieving fast event ordering in edge computing networks.
A preliminary evaluation reveals that EdgeCons works better than the state-of-the-art consensus algorithms.We will improve EdgeCons in the following two directions in our future work.
First, the current leadership distribution method has a drawback.
If an edge node has experienced a network congestion but later recovers, it may take a long time to re-gain its "leadership share".
For this reason, a deterministic, randomized method that can tune the leadership share in a timely manner should be designed in the future.
Second, how to monitor the system effectively, such that EdgeCons can quickly adjust the number of the quasi-Paxos instances in the upcoming epochs, in response to the dramatic changes on the workload received by the edge, is a challenging task and requires more considerations.
