Deterministic replay, which provides the ability to travel backward in time and reconstruct the past execution flow of a multiprocessor system, has many prominent applications.
Prior research in this area can be classified into two categories: hardware-only schemes and software-only schemes.
While hardware-only schemes deliver high performance, they require significant modifications to the existing hardware which makes them difficult to deploy in real systems.
In contrast, software-only schemes work on commodity hardware, but suffer from excessive performance overhead and huge logs caused by tracing every single memory access in the software layer.
In this paper, we present the design and implementation of a novel system, Samsara, which uses the hardware-assisted virtualization (HAV) extensions to achieve efficient and practical deterministic replay without requiring any hardware modification.
Unlike prior software schemes which trace every single memory access to record interleaving, Samsara leverages the HAV extensions on commodity processors to track the read-set and write-set for implementing a chunk-based recording scheme in software.
By doing so, we avoid all memory access detections, which is a major source of overhead in prior works.
We implement and evaluate our system in KVM on commodity Intel Haswell processor.
Evaluation results show that compared with prior software-only schemes, Samsara significantly reduces the log file size to 1/70th on average, and further reduces the recording overhead from about 10×, reported by state-of-the-art works, to 2.3× on average.
Modern multiprocessor architectures are inherently nondeterministic: they cannot be expected to reproduce the past execution flow exactly, even when supplied with the same inputs.
The lack of reproducibility complicates software debugging, security analysis, and faulttolerance.
It greatly restricts the development of parallel programming.Deterministic replay helps reconstruct nondeterministic processor executions.
It is extensively used in a wide range of applications.
For software debugging, it is the most effective way to reproduce bugs, which helps the programmer understand the causes of the bug [1,33].
For security analysis, it can help the system administrator analyze the intrusions and investigate whether a specific vulnerability was exploited in a previous execution [17,18,7,11,37].
For fault-tolerance, it provides the ability to replicate the computation on processors for building the hot-standby system or data recovery [6,43,42,32].
In the multiprocessor environment, memory accesses from multiple processors to a shared memory object may interleave in any arbitrary order, which become a significant source of non-determinism and pose a formidable challenge to deterministic replay.
To address this problem, most of the existing research focuses on how to record and replay the memory access interleaving using either a pure hardware scheme or a pure software scheme.Hardware-only schemes record memory access interleaving efficiently by embedding special hardware components into the processors and redesigning the cache coherence protocol to identify the coherence messages among processors [38,22,21,15,9,29,14,35,23,30].
The advantage of such a scheme is that it allows efficient recording of memory access interleaving in a multiprocessor environment.
On the down side, it requires extensive modifications to the existing hardware, which significantly increases the complexity of the circuits and makes them largely impractical in real systems.In contrast, software-only schemes achieve deterministic replay on the existing hardware by modifying the OS, the compiler, the runtime libraries or the virtual machine manager (VMM) [19,13,8,34,33,34,25,2,26,20,41,4].
Among them, virtualization-based deterministic replay is one of the most promising approaches which provides full-system level replay by leveraging the concurrent-read, exclusive-write (CREW) protocol to serialize and log the total order of the memory access interleaving [13,8,27].
While these schemes are flexible, extensible, and user-friendly, they suffer serious performance overhead (about 10× compared to the native execution) and generate huge logs (approximately 1 MB/s on a four core processor after compression).
The poor performance can be ascribed to the numerous page fault VM exits led by tracing every single memory access in the software layer.To summarize, it is inherently difficult to record memory access interleaving efficiently by software alone without proper hardware support.
Although there is no commodity processor with dedicated hardware-based record and replay capability, some advanced hardware features in these processors are available to boost the performance of the software-based deterministic replay systems.
Therefore, we argue that the software scheme can be a viable approach in the foreseeable future if it can take advantages of advanced hardware features.In this paper, the main goal is to implement a software approach that can take full advantages of the latest hardware features in commodity processors to record and replay memory access interleaving efficiently without introducing any hardware modifications.
The emergence of hardware-assisted virtualization (HAV) provides the possibility to meet our requirements.
Although HAV cannot be used for tracing memory access interleaving directly, we have found a novel use of it to track the read-set and write-set, and bypass the time-consuming process in traditional software schemes.
Specifically, we abandon the inefficient CREW protocol that records the dependence between individual instructions, and instead use a chunk-based strategy that records processors' execution as a series of chunks.
By doing so, we avoid all memory access detections, and instead obtain each chunk's read-set and write-set by retrieving the accessed and the dirty flags of the extended page table (EPT).
These read and write sets are used to determine whether a chunk could be committed, and the determinism is ensured by recording the chunk size and the commit order.
To further improve the system performance, we propose a decentralized three-phase commit protocol, which significantly reduces the performance overhead by allowing chunk commits in parallel while still ensuring serializability.We implement our prototype, Samsara, which, to the best of our knowledge, is the first software-based deterministic replay system that can record and replay memory access interleaving efficiently by leveraging the HAV extensions on commodity processors.
Experimental results show that compared with prior software schemes based on the CREW protocol, Samsara reduces the log file size to 1/70th on average (from 0.22MB/core/second to 0.003MB/core/second) and reduces the recording overhead from about 10× to 2.3× compared to the native execution.Our main contributions are as follows:• We present a software-based deterministic replay system that can record and replay memory access interleaving efficiently by leveraging the HAV extensions.
It improves the recording performance dramatically with a log size much smaller than all prior approaches.
• We design a decentralized three-phase commit protocol, which further improves the performance by enabling the chunk commit in parallel while ensuring serializability.
• We build and evaluate our system in KVM on Intel Haswell processor, and we plan to open-source our system to the community.The rest of the paper is organized as follows.
Section 2 describes the general architecture and shows how Samsara achieves deterministic replay.
Section 3 illustrates how to record and replay the memory access interleaving.
Section 4 presents the optimization and the implementation details.
We evaluate Samsara in section 5.
Section 6 reviews related work and section 7 concludes the paper.
In this section, we present the system overview of Samsara.
We first outline the overall architecture of Samsara.
Then, we discuss how it records and replays all non-deterministic events.
Samsara implements the deterministic replay as an extension to VMM, which has access to the entire virtual machine and can take full advantage of the HAV extensions, as illustrated in Figure 1.
The architecture of Samsara consists of four principal components, namely, the Controller, the record and replay component, the DMA recorder, and the log record daemon as shown in orange boxes in the figure.
The controller is in charge of all policy enforcement.
It provides a control interface to users, manages the record and replay component in KVM, and is in charge of the log transfer.
The record and replay component acts as a part of VMM working in the kernel space being responsible for recording and replaying all non-deterministic events, especially the memory access interleaving.
The DMA recorder records the contents of DMA events as part of QEMU.
Finally, we optimize the performance of logging by utilizing a user-space log record daemon.
It runs as a background process that supports loading and storing log files.Samsara implements deterministic replay by first logging all non-deterministic events during the recording phase and then reproducing these events during the replay phase.
Before recording, the controller initializes a snapshot of the whole VM states.
Then all nondeterministic events and the exact points in the instruction stream where these events occurred will be logged by the record and replay component during recording.
Meanwhile, it transfers these log data to the userspace log record daemon, which is responsible for the persistent storage and the management of the logs.
The replay phase is initialized by loading the snapshot to restore all VM states.
During replay, the execution of the virtual processors is controlled by the record and replay component which ignores all external events.
Instead each recorded event will be injected at the exact same point as in the recorded execution.
Non-deterministic events fall into three categories: synchronous, asynchronous, and compound.
The following illustrates what events will be recorded and how recording and replaying is done in our system.
Synchronous Events.
These events are handled immediately by the VM when they occur.
They always take place at the exact same point where they appear in the instruction stream, such as I/O events and RDTSC instructions.
The key observation is that they will be triggered by the associated instructions at the fixed point if all previous events are properly injected.
Therefore, we just record the contents of these events.
During replay, we merely inject logged data to where the I/O (or RDTSC) instruction is trapped into the VMM.Asynchronous Events.
These events are triggered by external devices, such as external interrupts, so they may appear at any arbitrary time from the point of view of the VM.
Their impact to the state of the system is deterministic, but the timing of their occurrences is not.
To replay them, all such events must be identified with a three-tuple timestamp (including program counter, branch counter, and the value of ECX) like the approach in ReVirt [12].
The first two are used to uniquely identify the instruction where the event appears in the instruction stream.
However, the x86 architecture introduces the REP prefixes to repeat a string instruction the number of times specified in the ECX.
Therefore, we also need to log the value of ECX which stores how many iterations remain at the time of this event takes place [12].
During replay, we leverage a hardware performance counter to guarantee that the VM stops at the recorded timestamp to inject them.Compound Events.
These events are nondeterministic in both their timing and their impact on the system.
DMA is an example of such events: the completion of a DMA operation is notified by an interrupt which is asynchronous, and the data copy process is initialized by a series of I/O instructions which are synchronous.
Hence, it is necessary to record both the completion time and the content of a DMA event.Memory Access Interleaving.
In the multiprocessor environment, memory accesses from multiple processors to a shared memory object may interleave in any arbitrary order, which become a significant source of nondeterminism.
More specifically, if two instructions both access the same memory object and at least one of them is write, then the access order of these two instructions should be recorded during the recording phase.
Unfortunately, the number of such events is orders of magnitude larger than all the other non-deterministic events combined.
Therefore, how to record and replay these events is the most challenging problem in a replay system.
How to record and replay memory access interleaving efficiently is the most significant challenge we face during the design and implementation of Samsara.
In this section, we describe on how Samsara uses HAV extensions to overcome this challenge.
Firstly, we show the specific design of our chunk-based strategy, then we discuss two technical issues when implementing this strategy in software: how to obtain the read-set and write-set efficiently and how to reduce the commit overhead.
Finally, we give a brief description on how to replay the memory access interleaving in Samsara.
Previous software-only schemes leverage CREW protocol to serialize and log the total order of the memory access interleaving [19], which produces huge log size and excessive performance overhead because every single memory access needs to be checked for logging before execution.
Therefore, chunk-based approach has been proposed on the hardware-based replay system to reduce the log size [21].
In this approach, each processor executes instructions grouped into chunks.
Thus, it just needs to record the total order of chunks.
However, this approach is not directly applicable to a software-only replay system, because tracing every single memory access to obtain the read-set and write-set during chunk execution in software will still be as time-consuming as directly logging the memory access interleaving itself.
To eliminate this performance overhead, we find HAV extension extremely useful.
Instead of tracing every single memory access, HAV offers a fast shortcut to track the read-set and write-set, which can be used to implement the chunk-based approach in software layer.To implement a chunk-based recording scheme, we need to divide the execution of virtual processors into a series of chunks.
In our system, a chunk is defined as a finite sequence of machine instructions.
Similarly to the database transaction, chunk execution must satisfy the atomicity and serializability requirements.
Atomicity requires that the execution of each chunk must be "all or nothing".
Serializability requires that the concurrent execution of chunks have to result in the same system state as if these chunks were executed serially.To enforce serializability, firstly, we must guarantee no update within a chunk is visible to other chunks until it commits.
Thus, on the first write to each memory page within a chunk, we create a local copy on which to perform the modification by leveraging copy-on-write (COW) strategy.
When a chunk completes execution, it either gets committed, copying all local data back to the shared memory, or gets squashed, discarding all local copies.
Moreover, an efficient conflict detection strategy is necessary to enforce serializability.
Particularly, an executing chunk must be squashed and re-executed when its accessed memory pages have been modified by a newly committed chunk.
To optimize recording performance, we leverage lazy conflict detection.
Namely, we defer detection until chunk completion.
When a chunk completes, we obtain the read-set and write-set (R&W-set) of this chunk.
We intersect all write-sets of other concurrent chunks with this R&W-set afterwards.
If the intersection is not empty, which means there are collisions, then this chunk must be squashed and re-executed.
Note that the write-write conflict must be detected even if there is no read in these chunks.
Specifically, the conflict detection is implemented at the page-level granularity, therefore any attempts to make the write-conflicting chunks serial may overwrite uncommitted data and cause a lost update.
Finally, there are certain instructions that may violate atomicity because they lead to externally observable behaviors (e.g., I/O instructions may modify device status and control activities on a device).
Once any of such instructions has been executed in a chunk, this chunk could no longer be rolled back.
Therefore, we truncate a chunk when any of such instructions is encountered.
Then the execution of such instructions must be deferred until this chunk can be committed.
Figure 2 illustrates the execution flow of our chunkbased approach.
First, we make a micro-checkpoint of the status of a virtual processor at the beginning of each chunk.
During chunk execution, the first write to each memory page will trigger a COW operation that creates a local copy.
All the following modifications to this page will be performed on this copy until chunk completion.
A currently running chunk will be truncated when an I/O operation occurs or if the number of instructions executed within this chunk reaches the size limit.
When a chunk completes, we obtain its R&W-set.
Then the con-flict detection is done by intersecting its own R&W-set with all W-sets of other chunks which just committed during this chunk execution.
If the intersection is empty (as C1 or C2 in Figure 2), this chunk can be committed.
Finally, we record the chunk size and the commit order which together are used to ensure that this chunk will be properly reconstructed during replay.
Otherwise (as C3 in Figure 2), all local copies will be discarded and we rollback the status of the virtual processor with the micro-checkpoint we made at the beginning and reexecute this chunk.In our design, there are two major challenges: 1) how to obtain the R&W-set (section 3.2); 2) how to commit the chunks in parallel while ensuring serializability (section 3.3).
The biggest challenge in the implementation of a chunkbased scheme in software is how to obtain the R&W-set efficiently.
Hardware-based schemes achieve this by tracing each cache coherence protocol message.
However, doing so in software-only schemes will result in serious performance degradation.Fortunately, the emergence of HAV provides the possibility to reduce this overhead dramatically.
HAV extensions enable efficient full-system virtualization utilizing the help from hardware capabilities.
Take Intel Virtualization Technology (Intel VT) as an example.
It provides hardware support for simplifying x86 processor virtualization.
The EPT that provided in HAV is a hardwareassisted address translation technology, which can be used to avoid the overhead associated with software managed shadow page tables.
Intel Haswell microarchitecture also introduces the accessed and dirty flags for EPT, which enables hardware to detect which page has been accessed or updated during execution.
More specifically, whenever the processor uses an EPT entry as part of the address translation, it sets the accessed flag in that entry.
In addition, whenever there is a write to a guest-physical address, the dirty flag in the corresponding entry will be set.
Therefore, by utilizing these hardware features, we can obtain the R&W-set by gathering all leaf entries where the accessed or the dirty flag is set, which can be archived by a simple EPT traversal.Moreover, the tree-based design of EPT makes it possible to further improve performance.
EPT uses a hierarchical, tree-based design which allows the subtrees corresponding to some unused part of the memory to be absent.
A similar feature is also present for the accessed and the dirty flags.
For instance, if the accessed flag of one internal entry is 0, then the accessed flags of all page entries in its subtrees are definitely 0.
Hence, it is not necessary to traverse these subtrees.
In practice, due to locality of reference, the access locations of most chunks are adjacent.
Thus, we usually just need to traverse a tiny part of EPT, which incurs negligible overhead.
Apart from obtaining the R&W-set, chunk commit is another time-consuming process.
In this section, we discuss how to optimize this part using a decentralized three-phase commit protocol.
Some hardware-based solutions add a centralized arbiter module to processors to ensure that one chunk gets committed at a time, without overlapping [21].
However, when it comes to software-only schemes, an arbiter will be slow.
Thus, we propose a decentralized commit protocol to perform chunk commit efficiently.The chunk commit process includes at least three steps in our design: 1) conflict detection that determines whether this chunk can be committed, 2) update broadcast that notifies other processors which memory pages are modified, 3) update write-back that copies all updates back to shared memory.
A na¨ıvena¨ıve design of the decentralized commit protocol is shown in Figure 3 a).
Without a centralized arbiter, we leverage a system-wide lock to enforce serializability.
Each virtual processor maintains three bitmaps: an access bitmap, a dirty bitmap, and a conflict bitmap.
The first two bitmaps help mark which memory pages were accessed or updated dur- ing the chunk execution (same as the R&W-set).
Each bit in the conflict bitmap indicates whether its corresponding memory page was updated by other committing chunks.
To detect conflict, we just need to intersect the first two bitmaps with the last one.
If the intersection is empty which means this chunk can be committed, this virtual processor broadcasts its W-set to notify others which memory pages have been modified by performing a bitwise-OR operation between the other virtual processors' conflict bitmaps and its own dirty bitmap.
Then it copies its local data back to the shared memory.
Finally, it clears its three bitmaps before the succeeding chunk starts.
This whole commit process is performed while holding this lock.
However, lock contention turns out to cause significant performance overhead.
In our experiments, it contributes to nearly 40% of the time spent on committing the chunks.
To address this issue, we redesign the commit process to reduce the lock granularity.
We observe that the write-back operation involves serious performance degradation due to lots of page copies, and all these pages committed concurrently by different chunks have no intersection, which is already guaranteed by conflict detection.
Based on this observation, we move this operation out of the synchronized block to reduce the lock granularity, as shown in Figure 3 b).
This not only reduces the cost of the locking operation substantially, but also increases parallelism because multiple chunks can now commit concurrently.However, one side effect of this design is that chunks may get committed out-of-order, thereby violating serializability.
One example is shown in Figure 4.
C1 writes A, then finishes its execution first and starts to commit.
Then, C2 starts committing as well and finishes before C1.
Meanwhile C3 starts to execute and happens to read A immediately.
Unfortunately, C1 may not accomplish its commit process in such a short period, thus C3 fetches the obsolete value of A. Suppose C3 reads A again and gets a new value after C1 completes its commit.
Then C3 gets two different values of the same memory object, which violates serializability.
To maintain serializability, we need to guarantee that before starting C3, P1 waits until all the other chunks which start committing prior to the commit point of C2 (e.g., C1 and C4) complete their commit.We develop a decentralized three-phase commit protocol to support parallel commit while ensuring serializability.
To eradicate out-of-order commits, we introduce a global linked list, commit order list, which maintains the order and information of each current committing chunk.
Each node of this list contains a commit flag field to indicate whether the corresponding chunk has completed its commit process.
Moreover, this list is kept sorted by the commit order of its corresponding chunk.
A lock is used to prevent multiple chunks from updating this list concurrently.
This protocol consists of three phases as shown in Figure 3 c):1) The pre-commit phase: In this phase, each processor must register its commit information by inserting an info node at the end of the commit order list.
The commit flag of this info node will be initialized to 0, which means this chunk is about to be committed.2) The commit phase: In this phase, the memory pages updated by this chunk will be committed (i.e., written back to shared memory).
Then the processor must set the commit flag of its info node to 1 at the end of this phase, which means it has completed its commit process.
Chunks can commit in parallel in this phase, because pages committed by different chunks have no intersection.3) The synchronization phase: In this phase, this virtual processor is blocked until all the other chunks which start committing prior to the commit point of its preceding chunk have completed their commit.
To enforce this, it needs to check all commit flags of those chunk info nodes which are ahead of its own node.
If at least one flag is 0, then this processor must be blocked.
Otherwise, the processor removes its own info node from the commit order list and begins executing the next chunk.
In practice, this blocking almost never happens, because a virtual processor tends to exit to QEMU to emulate device operations before executing the next chunk, which happens to provide sufficient time for other chunks to complete their commit.This design noticeably improves performance via reducing the lock granularity.
In brief, only the conflict de-tection and the update broadcast operation are protected by a system-wide lock.
Furthermore, It also reduces the time spent on waiting for the lock, because the shorter the time a chunk holds a lock, the lower the probability that other chunks requesting it have to wait is.
The most important characteristic is that this protocol can satisfy the serializability requirement because it strictly guarantees that the processor starting to commit a chunk first will execute the subsequent chunk preferentially.
The following of this section presents a formal proof on how our decentralized three-phase commit protocol ensures serializability.Assume for the sake of contradiction that it does not guarantee serializability.
Then there exists a set of chunks C 0 , C 1 . . .C n−1 which obey our three-phase commit protocol and produce a non-serializable schedule.
In order to know whether this chunk schedule is serializable or not, we can draw a precedence graph.
This is a graph in which the vertices are the committed chunks and the edges are the dependencies between these committed chunks.
A dependence C i → C j exists only if one of the following is true: 1) C i executes Store(X) before C j executes Load(X); 2) C i executes Load(X) before C j executes Store(X); 3) C i executes Store(X) before C j executes Store(X).
A non-serializable chunk schedule implies a cycle in this graph, and we will prove that our commit protocol cannot produce such a cycle.
Assume that a cycle exists in the precedence graph like this: C 0 → C 1 → C 2 → . . . → C n−1 → C 0 , for each chunk C i , we define T i to be the time when C i has been committed, and the corresponding processor begins executing its next chunk C i+1 .
Then for chunks such that C i → C j , T i < T j .
This is because the commit order list maintains the total order of these current committing chunks on all processors, and the three-phase commit protocol guarantees that all chunks will be processed in FIFO order.
Specifically, The pre-commit phase guarantees that the chunk will be inserted in the commit order list in execution order, and the synchronization phase guarantees that the chunk will be blocked until all the other chunks which start committing prior to it have completed their commits.
Moreover, the conflict detection ensures that an executing chunk will be squashed and re-executed later when there are collisions between it and a newly committed chunk, therefore, will not affect the commit order.
Then for this cycle, we have: T 0 < T 1 < T 2 < . . . < T n−1 < T 0 , which is a manifest contradiction.
Hence, our three-phase commit protocol can ensure serializability.
It is relatively simple and efficient to replay memory access interleaving under a chunk-base strategy.
Unlike the CREW protocol which must restrict every single memory access to reconstruct the recorded memory access interleaving, we just need to make sure that all chunks will be re-built properly and executed in the original order.
In other words, our replay strategy is more coarse-grained.
When we design the replay mechanism of Samsara, a design goal is to maintain the same parallelism as the recoding phase.
Since the atomicity and the serializability have already been guaranteed in recording phase, both the conflict detection and the update broadcast operations are no longer required during replay.
We just need to ensure that all the preceding chunks have been committed successfully before the current chunk starts.
More specifically, during replay, the processors generate chunks according to the order established by the chunk commit log.
Then they use the chunk size in that log to determine when they need to truncate these chunks.
Here, we use the same approach as above to confirm that a chunk can be truncated at the recorded timestamp.
During chunk execution, the COW operation is also required to guarantee that the other concurrently executing chunks will not access the latest data updated by this chunk.
To ensure chunk commit in the original order, we will block the commit of a chunk until all the preceding chunks have been committed successfully.
This section describes several optimizations for our chunk-based strategy to improve the recording performance and some implementation details of Samsara.
In our chunk-based strategy, a copy-on-write (COW) operation will be triggered to create a local copy on the first write to each memory page within a chunk.
In the original design, these local copies will be destroyed at the end of this chunk.
However, we find that these COW operations can cause a significant amount of performance overhead, especially when recording computation intensive applications.
By analyzing the memory access patterns, we observe that the write accesses of successive chunks exhibit great temporal locality with a history-similar pattern, which means they incline to access roughly the same set of pages.
Particularly, when a rollback occurs, the reexecuted chunk will follow a similar instruction flow and access the exact same set of pages in most instances.Based on this observation, we decide to retain local copies at the end of each chunk and use them as a cache of hot pages.
By doing so, when a processor modifies a page which already has a copy in the local cache, it acts just like it does in the unmodified VM with hardware acceleration, and no other operations will be necessary.However, this design may cause chunks to read outdated data.
One example is shown in Figure 5: chunk C4 reads z from its local cache, and meanwhile this page is modified to z' by another committed chunk C2 and copied back to the shared memory.
This does not cause any collision, but unfortunately, chunk C4 reads the outdated data z.These outdated copies can be simply detected by checking the corresponding bit in the conflict bitmap for each local copy.
However, the crucial issue remains as how to deal with these outdated copies.
We can either update local copies with the latest data in the shared memory or simply discard these outdated copies which have been modified by other committed chunks.
These two strategies both have their own advantages and shortcomings: the former reduces the number of COW operations but leads to relatively high overhead due to frequent memory copy operations, while the latter avoids this overhead but still retains some COW operations.
We combine the merits of these two strategies as follows: we update outdated copies when a rollback occurs, and discard them when a chunk is committed.This optimization is essentially equivalent to adding a local cache to buffer the hot pages which are modified by successive chunks.
In the current implementation, we limit this cache to a fixed size (0.1% of the main memory size) with a modified LRU replacement policy.
The chunk size is also a critical factor to the performance of the replay system.
If the chunk size is too small, its execution time will not be long enough to amortize the cost of a chunk commit.
On the other hand, if the chunk size is too large, the corresponding processor may experience repeated rollbacks due to the increased risk of collision during its commitment, which will eventually cause starvation.
Moreover, different applications or even different execution regions in the same application can exhibit varying memory access patterns, which makes it difficult to seek the sweet spot for chunk size, because the optimal size might not be a constant, but rather change during execution.
Therefore, one of the major challenges in our implementations is how to adjust chunk size adaptively to achieve a good balance.Therefore, we propose an adaptive additiveincrease/multiplicative-decrease (AIMD) algorithm to adjust the chunk size dynamically during runtime.
In this algorithm, a processor will increase its chunk size by a fixed amount after each successful commit to probe for longer execution time.
When collision is detected, the processor decreases its chunk size by a multiplicative factor.
The idea is similar to the feedback control algorithm in TCP congestion avoidance [10].
The decrease must be multiplicative because it is the only effectual way to ensure that at every step the fairness either increases or stays the same [10].
In a nutshell, when a conflict takes place, this adaptive AIMD algorithm ensures that all processors will quickly converge to use equal chunk size.
Therefore, each chunk has same probability to be committed or squashed.
One limitation of this algorithm is that it is less effective for some I/O intensive workloads due to the frequent chunk truncations caused by the large number of concurrent I/O requests.
In our decentralized commit protocol, the conflict detection and the update broadcast operation are both protected by a system-wide lock to enforce the serialization requirement.
Since the conflict bitmap will be modified by other chunks due to the update broadcast operation, while being read by its own chunk for the conflict detection, it has to acquire this system-wide lock whenever one chunk try to set the corresponding bits in other chunks' conflict bitmap to broadcast its updates.
Similarly, it has to wait for this lock to be released whenever its own chunk needs to read this bitmap for conflict detection.Double buffering mitigates this problem and can further increase parallelism.
Instead of using a single bitmap, we use two bitmaps simultaneously to implement double buffering.
One of them serves as a write bitmap and the other as the read bitmap.
Both bitmaps can be accessed at any time.
By doing so, we avoid locking the bitmap while reading and writing to it.
We switch these two bitmaps when the succeeding chunk starts its conflict detection, so it can read the bitmap directly when other chunks are free to set this bitmap simultaneously.
In our design, only this switch operation is protected by a lock, and neither bitmap requires any locking at all.
This section discusses our evaluation of Samsara.
We first illustrate the experimental setup and our workloads.
Then we evaluate different aspects of Samsara and compare it with a CREW approach.
All the experiments are conducted on a Dell Precision T1700 Workstation with a 4-core Intel Core i7-4790 processor (running at 3.6GHz, with 256KB L1, 1MB private L2 and 8MB shared L3 cache) running Ubuntu 12.04 with Linux kernel version 3.11.0 and QEMU-1.2.2.
The host machine has 12GB memory.
The Guest OS is an Ubuntu 14.04 with Linux kernel version 3.13.1.
To evaluate our system on a wide range of applications, we choose two sets of benchmarks that represent very different characteristics, including both computation intensive and I/O intensive applications.The first set includes eight computation intensive applications chosen from PARSEC and SPLASH-2 benchmark suites (four from each): blackscholes, bodytrack, raytrace, and swaptions form PARSEC [5]; radiosity, water nsquared, water spatial, and barnes from SPLASH-2 [36].
We choose both PARSEC and SPLASH-2 suites because each of them has its own merits, and no single benchmark can represent the characteristics of all types of applications.
PARSEC is a well-studied benchmark suite composed of emerging multithreaded programs from a broad range of application domains.
In contrast, SPLASH-2 is composed mainly of high-performance computing programs which are commonly used for scientific computation on distributed shared-address-space multiprocessors.
These eight applications come from different areas of computing and are chosen because they exhibit diverse characteristics and represent the different worst-case applications due to the burdensome shared memory accesses.
Blackscholes Bodytrack Raytrace swap�ons radiosity water_nsquared water_spa�albarnes kernel_buildpbzip2Log Size (MB/core/s) chunk commit order Figure 7: The proportion of each type of nondeterministic events in a log file (without compression).
Although there are applications in the first set that perform certain amount of I/O operations, most of them are disk read only.
In the other set of benchmarks, we select two more I/O intensive applications (kernel-build and pbzip2) to further evaluate how well Samsara handle I/O operations.
Kernel-build is a parallel build of the Linux kernel version 3.13.1 with the default configuration.
In order to achieve maximum degree of parallelism we use the -j option of make.
Usually, make -j n+1 produces a relatively high performance on a VM with n virtual processors.
This is because the extra process makes it possible to fully utilize the processors during network delays and general I/O accesses such as loading and saving files to disk [13].
Pbzip2 is a parallel file compressor which uses pthreads.
We use pbzip2 to decompress a 111MB Linux-kernel source file.
Log size is an important consideration of the replay systems.
Usually, recording non-deterministic events will generate huge space overhead which limits the duration of the recording.
The log size of some prior works is approximately 2 MB/1GHz-processor/s [38].
Some can support only a few seconds' recording which is difficult to satisfy long-term recording needs [38].
Experiment results show that Samsara produces a much smaller log size which is orders of magnitude smaller than the ones reported by prior work in softwarebased schemes, and even smaller than some reported in hardware-based schemes.
Figure 6 shows the compressed log sizes generated by each core for all the applications.
The experiments indicate that Samsara generates logs at an average rate of 0.0027 MB/core/s and 0.0031 MB/core/s for recording two and four cores, respectively.
For comparison, the average log size with a single core, which does not need to record memory interleaving, is 0.0024 MB/s.
To compare the log size of Samsara and the previous software or hardware approaches, this experiment was designed to be as similar as possible to the ones in the previous papers.
SMP-ReVirt generates logs at an average rate of 0.18MB/core/s when recording the workloads in SPLASH-2 and kernel-build on two dualcore Xeons [13].
DeLorean generates logs at an average rate of 0.03MB/core/s when recording the workloads in SPLASH-2 on eight simulated processors [21].
We achieve a significant reduction in the log size because the size of the chunk commit log is practically negligible compared with other non-deterministic events.
Figure 7 illustrates the proportions of each type of nondeterministic events in each log file.
In most workloads, the interleaving log represents a small fraction of the whole log (approximately 9.36% with 2 cores and 19.31% with 4 cores).
For the I/O intensive applications, this proportion is higher, because the large number of concurrent I/O requests leads to more chunk truncations.Another reason is we avoid recording all disk reads.
In Samsara, we use QEMU's qcow2 (QEMU Copy On Write) disk format to create a write protected base image and an overlay image on top of it to perform disk modifications during recording and replay.
By doing so, we can present the same disk view for replay without logging any disk reads or creating another copy of the whole disk image.In summary, the use of chunk-based strategy makes it possible to significantly reduces the log file size by 98.6% compared to the previous software-only schemes.
The log size in our system is even smaller than the ones reported in hardware-based solutions, since we can further reduce the log size via increasing the chunk size which is impossible in hardware-based approaches due to the risk of cache overflow [21].
The performance overhead of a system can be evaluated in different ways.
One way is to measure the overhead of the system relative to the base platform (e.g., KVM) it runs on.
The problem with this approach is that the performance of different platforms can vary significantly and hence the overhead measured in this manner does not reflect the actual execution time of the system in real life.
Consequently, we decide to compare the performance of our system to native execution, as shown in Figure 8.
As shown in the figure, the average performance overhead introduced by Samsara is 2.3× for recording computation intensive applications on two cores, and 4.1× on four cores.
For I/O intensive applications, the overhead is 3.5× on two cores and 6.1× on four cores.
This overhead is much smaller than the ones reported by prior works in software-only schemes, which cause about 16× or even 80× overhead when recording similar workloads on two or four cores [8,27].
Samsara improves the recording performance dramatically because we avoid all memory access detections which are a major source of the overhead.
Further experiment reveals that only 0.83% of the whole execution time is spent on handling page fault VM exits in Samsara, while prior CREW approaches suffer from more than 60% execution time spent on handling page fault VM exits.Among the computation intensive workloads, barnes has a relatively high overhead (more than 3× on two cores), while retrace has a negligible overhead (about 0.3× on two cores).
After analyzing the shared memory access pattern of these two workloads, we find that retrace contains many more read operations than write.
Since Samsara does not trace any read accesses, these read operations do not cause any performance overhead.
In contrast, barnes contains a lot of shared mem- ory writes, and the unstructured communication pattern negates the effects of our hot page cache.
Moreover, our page-level conflict detection may cause false conflicts (i.e., false sharing in SMP-ReVirt), which may lead to unnecessary rollback and increase performance overhead.
When compared to computation intensive workloads, I/O intensive workloads incur relatively high overhead.
This is also caused by the large number of concurrent I/O requests, which keep the chunk size quite small.
Therefore, the execution time is not long enough to amortize the cost of the chunk commits in these workloads.
To further evaluate our chunk-based strategy in Samsara against prior software-only approaches, we implement the original CREW protocol [13] in our testbed.
Log Size: Figure 9 shows the comparison against CREW protocol in log file size, in which Samsara reduces the log file size by 98.6% (i.e., from 0.22MB/core/s to 0.003MB/core/s).
To understand the improvement that Samsara achieves, we measure the proportions of each type of non-deterministic events in the log file.
In this measurement, we find that nearly 99% of the events are memory access interleaving in CREW protocol, while only 10% of the events in Samsara are chunk commit orders.Performance Overhead: We also compare the performance overhead of Samsara and the CREW protocol.
The results in figure 10 illustrate that with four cores Samara reduces the overhead by up to 76.8% and the average performance improvement is 58.6% compared to the native execution.Time Consumed on Handling Page Fault VM Exits: To understand why Samsara improves the recording performance so dramatically, we evaluate the time consumed on handling page fault VM exits in both approaches, since it is one of the primary contributors to the performance overhead.
Figure 11 shows that 65.6% of the whole execution time is spent on handling page fault VM exits in the CREW protocol.
In contrast, this proportion is only 0.83% in Samsara due to the HAV and chunk-based strategy we used.
Benefits of Caching Local Copies: Experimental results show that the average performance benefits contributed by caching local copies are 15.2% for recording computation intensive applications on four cores.
For I/O intensive applications, the benefits increase to 22.1%.
The effect of this optimization is highly dependent on the amount of temporal locality the local cache can exploit and the frequency of write operations.
This explains why, for the applications, like water nsquared and water spatial, which perform few write operations and exhibit poor temporal locality, the benefits of this optimization are less (-1.24% and 2.76%).
Benefits of Double Buffering: The performance benefits contributed by double buffering are less significant.
Empirically, the average performance increase is 4.61% when recording computation intensive applications on four cores.
For I/O intensive applications, the improvement is 7.43%.
The improvement of the adaptive chunk size optimization is constrained by the frequent chunk truncations caused by I/O requests, thus is heavily applicationspecific.
Among all the applications we experiment with, only a small subset of the computation intensive applications (e.g., raytrace from PARSEC, radiosity from SPLASH-2) is shown to have statistically significant benefit from this optimization.
Deterministic Replay in Virtualization Environment: The idea of achieving deterministic replay based on virtualization environment was first proposed by Bressoud, et al. [6].
Similarly, ReVirt [12] can replay entire OSes deterministically and efficiently by recording all non-deterministic events within the VMM.
ReTrace [40] is a trace collection tool based on the deterministic replay of the VMware hypervisor.
However, both of them only work for uniprocessors and cannot be applied to multiprocessor environment.
SMP-Revirt [13] is the first deterministic replay system that records and replays a multiprocessor VM on commodity hardware by leveraging CREW protocol.
ReEmu [8] refines the CREW protocol with a seqlock-like design to achieve scalable deterministic replay in a parallel full-system emulator.
While these virtualization-based schemes are flexible, extensible, and user-friendly, they suffer serious performance degradation and generate huge logs.
In contrast, Samsara can leverage the latest HAV extensions in commodity multiprocessors to achieve efficient and practical deterministic replay.
A preliminary description of this work was in [31].
Hardware-based Deterministic Replay: Hardwarebased deterministic replay uses special hardware support for recording memory access interleaving.
These schemes require modifications to the existing hardware, which increases the complexity of the circuits.
FDR [38] records interleaving between pairs of instructions, and it improves the performance by implementing the Netzer's Transitive Reduction optimization [24] on hardware.
RTR [39] extended FDR by only recording the logical time orders between memory access instructions.
However, they still generate huge space overhead, which limits the duration of the recording.
Strata [22] redesigns the recording strategy and records a stratum when a dependence occurs.
Each stratum contains many memory operations issued by the corresponding processor since the last stratum is logged.
Delorean [21] goes even further on this idea.
Rather than logging individual dependence, it records memory access interleaving as series of chunks.
By doing so, it allows out-of-order execution of instructions.
IMMR [28] designs a chunk-based strategy for memory race recording in modern chip multiprocessors.
Rerun [16] introduces an intermediate approach where it traces each data access but does not record this dependence.
Instead, it records the number of instructions between two dependences.
However, Rerun does not scale well during replay.
To improve replay performance, Karma [3] is proposed as a chunk-based approach that aims to increase replay parallelism.
Compared to chunk-based strategies in hardware schemes, Samsara improves the recording performance in VMM without requiring any hardware modification.
Firstly, by leveraging HAV extensions, we avoid tracing every single memory access, instead perform a EPT traversal to obtain the read and write set.
Secondly, we remove the centralized arbiter in Delorean, and propose a decentralized three-phase commit protocol to perform chunk commit efficiently.
In this paper, we have made the first attempt to leverage HAV extensions to achieve an efficient and practical software-based deterministic replay system on commodity multiprocessors.
Unlike prior software schemes that trace every single memory access to record interleaving, we leverage the HAV extensions to track the read and write-set, and implement a chunk-based recording scheme in software.
By doing so, we avoid all memory access detections, which are a major source of overhead in the prior work.
In addition, we propose a decentralized three-phase commit protocol which significantly reduces the performance overhead by allowing chunk commits in parallel while still ensuring serializability.
By evaluating our system on real systems, we demonstrate that Samsara can reduce the recording overhead from 10× to 2.3× and reduce the log file size to 1/70th on average.
The authors would like to thank Jon Howell, our shepherd Andreas Haeberlen, and the anonymous reviewers for their insightful comments.
We also thank Yunqi Zhang for his valuable feedback on the earlier drafts of this paper.
This work was supported by the National Natural Science Foundation of China (Grant No. 61170056), the National Grand Fundamental Research 973 Program of China (Grant No. 2014CB340405).
