Recent deep learning (DL) models are moving more and more to dynamic neural network (NN) architectures, where the NN structure changes for every data sample.
However, existing DL programming models are inefficient in handling dynamic network architectures because of: (1) substantial overhead caused by repeating dataflow graph construction and processing every example; (2) difficulties in batched execution of multiple samples; (3) inability to incorporate graph optimization techniques such as those used in static graphs.
In this paper, we present "Cavs", a runtime system that overcomes these bottlenecks and achieves efficient training and inference of dynamic NNs.
Cavs represents a dynamic NN as a static vertex function F and a dynamic instance-specific graph G.
It avoids the overhead of repeated graph construction by only declaring and constructing F once, and allows for the use of static graph optimization techniques on pre-defined operations in F. Cavs performs training and inference by scheduling the execution of F following the dependencies in G, hence naturally exposing batched execution opportunities over different samples.
Experiments comparing Cavs to state-of-the-art frameworks for dynamic NNs (TensorFlow Fold, PyTorch and DyNet) demonstrate the efficacy of our approach: Cavs achieves a near one order of magnitude speedup on training of dynamic NN architectures, and ablations verify the effectiveness of our proposed design and optimizations.
Deep learning (DL), which refers to a class of neural networks (NNs) with deep architectures, is now a workhorse powering state-of-the-art results on a wide spectrum of tasks [53,54,30].
One reason for its widespread adoption is the variety and quality of software toolkits, such as Caffe [23], TensorFlow [1], PyTorch [36] and DyNet [33,34], which ease programming of DL models, and speed computation by harnessing modern computing hardware (e.g. GPUs), software libraries (e.g. CUDA, cuDNN [6]), and compute clusters [56,57,7].
One dominant programming paradigm, adopted by DL toolkits such as Caffe and TensorFlow, is to represent a neural network as a static dataflow graph [32,1], where computation functions in the NN are associated with nodes in the graph, and input and output of the computation map to edges.
It requires DL programmers to define the network architecture (i.e. the dataflow graph) using symbolic expressions, once before beginning execution.
Then, for a given graph and data samples, the software toolkits can automatically derive the correct algorithm for training or inference, following backpropagation [21] and auto-differentiation rules.
With proper optimization, the execution of these static dataflow graphs can be highly efficient; as the dataflow graph is fixed for all data, the evaluation of multiple samples through one graph can be naturally batched, leveraging the improved parallelization capability of modern hardware (e.g. GPUs).
Moreover, by separating model declaration and execution, it makes it possible for the graph to be optimized once at declaration time [1], with these optimizations benefiting the efficiency of processing arbitrary input data batches at execution time.While the dataflow graph has major efficiency advantages, its applicability highly relies on a key assumption -the graph (i.e. NN architecture) is fixed throughout the runtime.
This assumption however breaks for dynamic NNs, where the network architectures conditionally change with every input sample, such as NNs that compute over sequences of variable lengths [22,43], trees [45], and graphs [26].
Due to the growing interest in these sorts of dynamic models, recent years have seen an increase in the popularity of frameworks based on dynamic declaration [49,33,11], which declare a different dataflow graph per sample.
While dynamic declaration is convenient to developers as it removes the restriction that computation be completely specified before training begins, it exhibits a few limitations.
First, constructing a graph for every sample results in substantial overhead, which grows linearly with the number of input instances.
In fact, we find graph construction takes longer time than the computation in some frameworks (see §5.2).
It also prevents the application of complex static graph optimization techniques (see §3.4).
Moreover, since each sample owns a dataflow graph specifying its unique computational pattern, batching together similarly shaped computations across instances is non-trivial.
Without batching, the computation is inefficient due to its lack of ability to exploit modern computational hardware.
While some progress has been made in recent research [34,27], how to automatically batch the computational operations from different graphs remains a difficult problem.To address these challenges, we present Cavs, an efficient runtime system for dynamic NNs that exploits the recurrent and recursive nature of dynamic NNs.
Instead of declaring a dataflow graph per sample, it decomposes a dynamic NN into two components: a static vertex function F that is only declared (by the user) and optimized once before execution, and an input-specific graph G obtained via I/O at runtime.
Cavs inherits the flexibility of symbolic programming [1,12,33] for DL; it requires users to define F by writing symbolic expressions in the same way as in static declaration.
With F and G, the workflow of training or testing a dynamic NN is cast as scheduling the execution of F following the structure of the input graph G. Cavs will perform autodifferentiation, schedule the execution following dependencies in G, and guarantee efficiency and correctness.Cavs' design allows for highly efficient computation in dynamic graphs for a number of reasons.
First, it allows the vertex function only to be defined and constructed once for any type of structured data, hence avoiding the overhead of repeated dataflow graph construction.
Second, as the dataflow graph encoded by the vertex function is static throughout the runtime, it can benefit from various static graph optimizations [1,5,12,18]( §3.4), which is not the case in the scenario of dynamic declaration ( §2.2).
Moreover, it naturally exposes opportunities for batched computation, i.e. we are able to parallelize the execution of F over multiple vertices from different input graphs ( §3.2) with the support of our proposed memory management strategy ( §3.3).
To evaluate Cavs' performance, we compare it to several state-of-the-art systems supporting dynamic NNs.
We focus our experiments on GPU training, and verify that both Fold and DyNet suffer from substantial overhead caused by repeated graph preprocessing or construction, which is bypassed by Cavs ( §5.2).
In a comparison with unbatched dynamic graphs in PyTorch and DyNet, two widely-used dynamic NN libraries, we verify that batching is essential for efficient processing.
In a comparison with TensorFlow Fold and DyNet Autobatching, two libraries that allow for the use of dynamic NNs with automatic operation batching, we find that Cavs' has significant performance advantages; on static graphs it performs equivalently or slightly better, and on dynamic NNs with difficult-to-batch workloads (e.g. Tree-LSTM [45] and Tree-FC [27]), Cavs demonstrates near one order of magnitude speedups across multiple dataset and hyper-parameter settings ( §5.1).
We further investigate the effectiveness of our design choices: Cavs benefits from not only our proposed memory management strategy, but also various optimizations on graph execution, which were originally for static dataflow graphs and not applicable in dynamic declaration.To summarize, we make three primary contributions in this paper: (1) We propose a novel representation for dynamic NNs, based on which we design four APIs and implement the Cavs runtime system ( §3.1); (2) We propose several novel strategies in Cavs for efficient training and inference of dynamic NNs: the batching policy ( §3.2), a memory management mechanism to guarantee the memory coalescing ( §3.3), and multiple graph execution optimizations ( §3.4); (3) We compare Cavs to state-of-theart systems for dynamic NNs ( §5).
We reveal the problems of existing systems, and report near 10x speedup for Cavs on various experimental settings.
We also verify the effectiveness of our proposed design strategies, and quantize their contributions to the final performance.
Successful NN models generally exhibit suitable architectures that capture the structures of the input data.
For example, convolutional neural networks [24,53], which apply fixed-structured operations to fixed-sized images, are highly effective precisely because they capture the spatial invariance common in computer vision domains [39,44].
However, apart from images, many forms of data are structurally complex and can not be readily captured by fixed-structured NNs.
Appropriately reflecting these structures in the NN design has shown effective in sentiment analysis [45], semantic similarity between sentence pairs [40], and image segmentation [26].
To see this, we will take the constituency parsing problem as an example.
Sentences in natural languages are often represented by their constituency parse tree [45,31], whose structure varies depending on the content of the sentence itself ( Fig. 1(a)).
Constituency parsing is an important problem in natural language processing that aims to determine the corresponding grammar type of all internal nodes given the parsing tree of a sentence.
Fig. 1(b) shows an example of a network that takes into account this syntactic structure, generating representations for the sentence by traversing the parse tree bottomup and combining the representations for each sub-tree using a dynamic NN called Tree Structured Long Shortterm Memory (Tree-LSTM) [45].
In particular, each node of the tree maps to a LSTM function [22].
The internal computations and parameters of the LSTM function is defined in Fig. 4.
At each node, it takes a variable number of inputs and returns to the parent node a vector representing the parsing semantics up to that point, until the root LSTM node returns a vector representing the semantics of the entire sentence.
The important observation is that the NN structure varies with the underlying parsing tree over each input sample, but the same LSTM cell is constant in shape and repeated at each internal node.
Similar examples can be found for graph input [25,26] and sequences of variable lengths [43,2].
We refer to these NNs that exhibit different structures for different input samples as dynamic neural networks, in contrast to the static networks that have fixed network architecture for all samples.
There is a natural connection between NNs and directed graphs: we can map the graph nodes to the computational operations or parameters in NNs, and let the edges indicate the direction of the data being passed between the nodes.
In this case, we can represent the process of training NNs as batches of data flowing through computational graphs, i.e. dataflow graphs [3,1,33].
Static declaration.
As mentioned previously, static declaration is one dominant programming paradigm for programming NNs [3,1,5].
Fig 2(a) summarizes its workflow, which assumes all data samples share a fixed NN structure declared symbolically in a dataflow graph D. Static declaration, using a single dataflow graph D, cannot express dynamic NNs with structures changing with data samples.
A primary remedy to this problem is to forgo the efficiency gains of static dataflow graphs and instead use a dynamic declaration framework.
Dynamic declaration.
Fig 2(b) illustrates the workflow of dynamic declaration.
By creating a unique dataflow graph D p k for each sample x p k according to its associated structure, dynamic declaration is able to express sampledependent dataflow graphs.
It however causes extra overhead on graph construction and puts constraints on runtime optimization, which usually lead to inefficient execution.
Particularly, since a dataflow graph D p k needs to be constructed per sample, the overhead is linearly increasing with the number of samples, and sometimes yields downgraded performance [27] ( §5.2), even for frameworks with optimized graph construction implementations [33].
Moreover, we can hardly benefit from any well-established dataflow graph optimization ( §3.4).
We will have to perform graph processing/optimization for each dataflow graph and every single sample; but incorporating this optimization itself has a non-negligible overhead.
More importantly, as we are unable to batch the computation of different structured graphs, we note in Fig 2(b) single-instance computation D p k (x p k ) would be very inefficient in the absence of batched computation.
Dynamic batching.
To address the batching problem, some recent effort, notably TensorFlow Fold [27] and DyNet [34], propose dynamic batching that dynamically groups similarly shaped operations from different graphs, and batch their execution whenever possible.Fold turns dynamic dataflow graphs into a static control flow graph to enable batched execution, but introduces a complicated functional programming-like interface and a large graph preprocessing overhead.
As we will show in §5.2, the graph construction sometimes slows down the computation by 4x.
DyNet proposes an auto-batching strategy that searches for batching opportunities by profiling every fine-grained operator, while this step itself has non-negligible overhead ( §5.2).
It is also not open to dataflow graph level optimizations.In summary, there are three major challenges that prevent the efficient execution of dynamic neural networks: (1) non-negligible graph construction overhead; (2) difficulties in parallel execution; (3) unavailability to graph execution optimization.
Our motivation for Cavs comes from a key property of dynamic NNs: most dynamic NNs are designed to exhibit a recursive structure; Within the recursive structure, a static computational function is being applied following the topological order over instance-specific graphs.
For instance, if we denote the constituency parsing tree in §2.1 as a graph G, where each node of the tree maps to a vertex in G, we note the Tree-LSTM can be interpreted as follows: a computational cell function, specified in advance, is applied from leaves to the root, following the dependencies in G. G might change with input samples, but the cell function itself is always static: It is parametrized by a fixed set of learnable parameters and interacts in the same way with its neighbors when applied at different vertices of G.These observations motivate us to decompose a dynamic NN into two parts: (1) a static computational vertex function F that needs to be declared by the program-/* (a) static declaration */ // all samples must share one graph declare a static dataflow graph D. for p = 1 → P:read the pth data batch mer once before runtime; (2) a dynamic input graph G that changes with every input sample 1 .
With this representation, the workflow of training a dynamic NN can be cast as scheduling the evaluation of the symbolic construct encoded by F, following the graph dependencies of G, as illustrated in Fig 2(c).
This representation exploits the property of dynamic NNs to address the aforementioned issues in the following ways: Minimize graph construction overhead.
Cavs only requires users to declare F using symbolic expressions, and construct it once before execution.
This bypasses repeated construction of multiple dataflow graphs, avoiding overhead.
While it is still necessary to create an I/O function to read input graphs G for each sample, this must be done by any method, and only once before training commences, and it can be shared across samples.
Batched execution.
With the proposed representation, Cavs transforms the problem of evaluating data samples [27,34] into a simpler form -scheduling the execution of the vertex function F following the dependencies in input graphs {G p k } k=1 .
For the latter problem, we can easily batch the execution of F on multiple vertices at runtime ( §3.2), leveraging the batched computational capability of modern hardware and libraries.
Open to graph optimizations.
Since the vertex function F encodes a dataflow graph which is static throughout runtime, it can benefit from various graph optimizations originally developed for static declaration, such as kernel fusion, streaming, and our proposed lazy batching, which are not effective in dynamic declaration.
{x p k } K k=1 .
batched computation: D({x p k } K k=1 ).
/* (b) dynamic declaration */ for p = 1 → P: read the pth data batch {x p k } K k=1 .
for k = 1 → K: declare a dataflow graph D p k for x p k .
single-instance computation: D p k (x p k ).
/* (c) our proposed vertex-centric model */ declare a symbolic vertex function F .
for p = 1 → P: read the pth data batch {x p k } K k=1 .
read their associated graphs {G p k } K k=1 .
compute F over {G p k } K k=1 with inputs {x p k } K k=1 .
{x p k } K k=1 (at the pth batch) on different dataflow graphs {D p k } k=1Based on this motivation, we next describe the Cavs system.
Cavs faces the following challenges in system design: (1) how to design minimal APIs in addition to the symbolic programming interface to minimize user code; (2) how to schedule the execution of F over multiple input graphs to enable batched computation; (3) how to manage memory to support the dynamic batching; (4) how to incorporate static graph optimization in Cavs's execution engine to exploit more parallelism.
Conventional dataflow graph-based programming models usually entangle the computational workflow in F with the structure in G, and require users to express them as a whole in a single dataflow graph.
Instead, Cavs separates the static vertex function F from the input graph G (see Fig 3).
While users use the same set of symbolic operators [1,11] to assemble the computational workflow in F, Cavs proposes four additional APIs, gather, scatter, pull, push, to specify how the messages shall be passed between connected vertices in G:• gather(child idx): gather accepts an index of a child vertex, gets its output, and returns a list of symbols that represent the output of the child.
• scatter(op): scatter reverses gather.
It sets the output of the current vertex as op.
If this vertex is gathered, the content of op will be returned.gather and scatter are motivated by the GAS model in graph computing [14] -both are vertex-centric APIs that help users express the overall computational patterns by thinking locally like a vertex: gather receives messages from dependent vertices, while scatter updates information to parent vertices (see discussion in §6).
However, in dynamic NNs, the vertex function F usually takes input from not only the internal vertices of G (internal data path in Fig 3), but also the external environment, e.g. an RNN can take inputs from a CNN feature extractor or some external I/O (external data path in Fig 3).
Cavs therefore provides another two APIs to express such semantics:• pull(): pull grabs inputs from the external of the current dynamic structure, e.g. another NN, or I/O.
• push(op): push reverses pull.
It sets the output of the current vertex as op.
If this vertex is pulled by others, the content of op will be returned.
# specify the computation8 h = ∑ N−1 k=0 h k 9 i = sigmoid(W (i) × x + U (i) × h + b (i) ) 10for k in range(N): Once F declared, together with an input graph G, they encode a recursive dataflow graph structure, which maps to a subgraph of the implicit full dataflow graph of the model that may needs to be explicitly declared in traditional programming models.
Via push and pull, Cavs allows users to connect any external static dataflow graph to a dynamic structure encoded by (F, G), to express more complex model architectures, such as the LRCN [9] (i.e. connecting a CNN to an RNN), or an encoder-decoder LSTM network [43] (i.e. connecting two different recursive structures).
With these four APIs, we present in Fig 4 an example user program how the Nary child-sum Tree-LSTM [45] can be simply expressed by using them and other mathematical operators.
Auto-differentiation.
Given a vertex function F Cavs derives ∂ F following the auto-differentiation rules: for each math expression such as s l = op(s r ) in F, Cavs generates a corresponded backward expression ∇s r = grad op(∇s l , s l , s r ) in ∂ F. For the four proposed operators, we note scatter is the gradient operator of gather in the sense that if gather collects inputs from child vertex written by scatter at the forward pass, a scatter needs to be performed to write the gradients for its dependent vertices to gather at the backward pass.
Hence, for an expression like s l = gather(child idx) in F, Cavs will generate a backward expression scatter(∇s l ) in ∂ F. Similarly, the gradient operator of scatter is gather.
The same rules apply for push and pull.
Expressiveness.
With these four APIs, Cavs can be seen as a middle ground between static and dynamic declaration.
In the best case that the NN is fully recursive (e.g. most recurrent or recursive NNs), it can be represented by a single vertex function and an input graph.
While in the worst case, that every sample has a unique input graph while every vertex in the graph has a unique way to interact with its neighboring vertices (i.e. the NN is dynamic but non-recursive), Cavs reduces to dynamic declaration that one has to define a vertex function for each vertex of each input graph.
Fortunately, dynamic NNs in this scenario are usually avoided because of the difficulties in design, programming and learning.11 f k = sigmoid(W ( f ) × x + U ( f ) × h k + b ( f ) ) 12 o = sigmoid(W (o) × x + U (o) × h + b (o) ) 13 u = tanh(W (u) × x + U (u) × h + b (u) ) 14 c = i ⊗ u + ∑ N−1 k=0 f k ⊗ c k 15 h = o ⊗ tanh(c) Once F is defined and G is obtained from I/O, Cavs will perform computation by scheduling the evaluation of F over data samples {x i } N i=1 and their input graphs {G i } N i=1 .
Forward pass.
For a sample x i with its input graph G i , the scheduler starts the forward pass from the input vertices of G i , and proceeds following the direction indicated by the edges in G i : at each sub-step, the scheduler figures out the next activated vertex in G i , and evaluates all expressions in F at this vertex.
It then marks this vertex as evaluated, and proceeds with the next activated vertex until reaching a terminal vertex (e.g. the loss function).
A vertex of G is activated if and only if all its dependent vertices have been evaluated.
Backward pass.
The backward pass is continued right after the forward.
The scheduler first resets the status of all vertices as not evaluated, then scans the graph in a reverse direction, starting from the ending point of the forward pass.
It evaluates ∂ F at each vertex until all vertices have been evaluated in the backward pass.To train a NN to convergence, the above process has to be iterated on all samples {x i } N i=1 and their input graphs{G i } N i=1, for many epochs.
We next describe our batched execution policy to speed the computation.
Batching policy.
Given a data batch{x k } K k=1 ⊆ {x i } N i=1and associated graphs {G k } K k=1 , this policy groups multiple vertices and performs batched evaluation of F in order to reduce kernel launches and exploit parallelism.
Specifically, a forward pass over a batch {x k } K k=1 are performed in multiple steps.
At each step t, Cavs analyzes {G k } K k=1 at runtime and determines a set V t that contains all activated vertices in graphs {G k } K k=1 .
It then evaluates F over these vertices by creating a batched execution task, with the task ID set to t 2 .
The task is executed by the Cavs execution engine ( §3.4).
Meanwhile, the scheduler records this task by pushing V t into a stack S. To perform backward pass, the scheduler pops out an element V t from S at each step -the execution engine will evaluate the derivative function ∂ F over vertices in V t , until all vertices of {G k } K k=1 are evaluated.
We note the batching policy is similar to the dynamic batching in Fold [27] and DyNet [33].
However, Cavs determines how to batch fully dynamically during runtime using simple breadth-first search with negligible cost (instead of analyzing full dataflow graphs before every iteration of the execution).
Since batched computation requires the inputs to an expression over multiple 1  2  3  0  1  2  3  3  0  1  2  3  Figure 5:The memory management at the forward pass of F (top-left) over two input trees (bottom-left).
Cavs first analyzes F and inputs -it creates four dynamic tensors {α n } 3 n=0 , and figures out there will be four batch tasks (dash-lined boxes).
Starting from the first task (orange vertices {0, 1, 2, 5, 6, 7, 8, 9}), Cavs performs batched evaluation of each expression in F. For example, for the pull expression α 0 = pull(), it indexes the content of α 0 on all vertices from the pull buffer using their IDs, and copies them to α 0 continuously; for scatter and push expressions, it scatters a copy of the output (α 3 ) to the gather buffer, and pushes them to the push buffer, respectively.
Cavs then proceeds to the next batching task (blue vertices).
At this task, Cavs evaluates each expression of F once again for vertices {3, 10, 11}.
(e.g. for a pull expression α 0 = pull(), it pulls the content of α 0 from pull buffer again; for a gather expression α 2 = gather(1) at vertex 3, it gathers the output of the second child of 3, which is 1); it writes results continuously at the end of each dynamic tensor.
It proceeds until all batching tasks are finished.vertices to be placed on a continuous memory buffer, we develop a new memory management support for it.
In static declaration [1,33], a symbol in the user program usually corresponds to a fixed-sized tensor object with a batch size dimension.
While in Cavs, each batching task V t is determined at runtime.
For the batched computation to be efficient, Cavs must guarantee for each batching task, the inputs to each expression of F over a group of runtime-determined vertices coalescing in memory.
Cavs proposes a novel data structure dynamic tensor to address this challenge (Fig 6).
A dynamic tensor is a wrapper of a multi-dimensional array [1,52].
It contains four attributes: shape, bs, a pointer p to a chunk of memory, and offset.
shape is an array of integers representing the specific shape of the tensor excluding the batch dimension.
It can be inferred from the user program and set before execution.
The batch size bs is dynamically set by the scheduler at runtime at the beginning of a batching task.
To access a dynamic tensor, one moves p forward with the value of offset, and reads/writes number of elements equal to bs · ∏ i shape [i].
Therefore, bs together with offset provide a view of the tensor, and the state of the tensor will vary based on their values.
Given a vertex function F, Cavs creates dynamic tensors {α n } N n=1 for each non-parameter symbol s n (n = 1, . . . , N) in F, and also {∇α n } N n=1 as their gradients, while it creates static tensors for model parameters.
Fig 5 illustrates how the memory is assigned during the forward pass by manipulating dynamic tensors.
In particular, in a training iteration, for a batching task V t , the scheduler sets bs of all {α n } N n=1 to M t = |V t | (the number of vertices in V t ).
The execution engine then performs batched evaluation of each expression in F. For an expression s l = op(s r ) 3 , Cavs first accesses α r (the dynamic tensor of the RHS symbol s r ) -it offsets α r .
p by α r .
offset, and reads a block of M t ∏ i α r .
shape [i] elements, and presents it as a tensor with batch size M t and other dimensions as α r .
shape.
It then applies batched computational kernels of the operator op over this memory block, and writes the results to α l (the dynamic tensor of the LHS symbol s l ) on the continuous block in between[α l .
p + α l .
offset, α l .
p + α l .
offset + M t ∏ i α l .
shape[i]].
Upon the completion of V t , the scheduler increases offset of all {α n } N n=1 by M t ∏ i α n .
shape [i], respectively.
It then starts the next task V t+1 .
Hence, intermediate results generated in each batching task at forward pass are stored continuously in the dynamic tensors, and their offsets are recorded.At the entrance of F, the vertices {v m } M t m=1 in V t need to interact with its dependent vertices in previous V t−1 to gather their outputs as inputs (L3 of Figure 4), or pull inputs from the external (L5 of Figure 4).
Cavs maintains memory buffers to enable this ( Figure 5).
It records the offsets of the dynamic tensors for each v m ∈ V t , and therefore during the execution of gather operator, the memory slices of specific children can be indexed.
As shown in Figure 5, gather and scatter share the same temporary buffer for memory re-organization, but push and pull operate on external memory buffers.Algorithm 1 summarizes the memory management during forward pass.
The backward execution follows an exactly reverse order of the forward pass ( §3.2 ), which we skip in the text.
With this strategy, Cavs guarantees memory continuity for any batched computation of F and ∂ F. Compared to dynamic batching in DyNet, Cavs performs memory movement only at the entrance and exit of F, instead of for each expression (operator).
We empirically find this significantly reduces overhead of memory operations ( §5.3).
Algorithm 1 Memory management at forward pass.1: function FORWARD({V t } T t=1 , {α n } N n=1 , F ) 2: for t = 1 → T do 3:for n = 1 → N do α n .
bs ← M t end for 4:for each expression like s l = op(s r ) in F do 5:if op ∈ {gather, pull} then 6:C ← ∏ i α l .
shape[i], q ← α l .
p + α l .
offset.
7: for v m ∈ V t (m = 1 → M t ) do 8: src ← IndexBuffer(op, m), dest ← q + (m − 1)C. 9:memcpy(dest, src,C).
end for 11:else if op ∈ {scatter, push} then 12:C ← ∏ i α r .
shape[i], q ← α r .
p + α r .
offset.
13: for v m ∈ V t (m = 1 → M t ) do 14: dest ← IndexBuffer(op, m), src ← q + (m − 1)C. 15:memcpy(dest, src,C).
end for 17: else 18: perform batched computation: α l = op kernel(α r ).
end if 20: end for 21:for n = 1 → N do α n .
offset+ = M t ∏ i α n .
shape[i] end for 22: end for 23: end function Since Cavs separates out a static dataflow graph encoded by F, we can replace the original F with an optimized one that runs more efficiently, as long as maintaining correctness.
We next described our optimization strategies.
Lazy batching and streaming 4 .
In addition to batched execution of F, the lazy batching and streaming explore potential parallelism for a certain group of finer-grained operators in F or ∂ F called lazy and eager operators.Definition.
An operator in F (∂ F) is a lazy operator if at the forward (backward) pass, for ∀v ∈ G, ∀G ∈ {G k } K k=1 , the evaluation of F (∂ F) at any parent (dependent) vertex of v does not rely on the evaluation of F at v.
It is an eager operator if the evaluation at v does not rely on the evaluation of F (∂ F) at any dependents (parents) of v. gather and scatter operator, respectively.
A node that has g as its dependent and is not on any path from g to s is a lazy operator.
A node that has s as its ancestor and is not on any path from g to s is an eager operator.
Fig 7 illustrates a forward dataflow graph of the vertex function of Tree-LSTM, with eager and lazy operators colored.
A property of them is that their evaluation is not fully subject to the dependency reflected by the input graph G. For instance, the pull operator in Fig 7 is eager and can be executed in prior -even before F has been evaluated at the vertices that gather tries to interact with; the push operator is lazy, so we can defer its execution without impacting the evaluation of F at parent vertices.
Similarly, in ∂ F, the gradient derivation for model parameters are mostly lazy -their execution can be deferred as long as the gradients of hidden states are derived and propagated in time.
Cavs leverages this property and proposes the lazy batching strategy.
It defers the execution of all lazy operators in F and ∂ F until all batching tasks {V t } T t=1 has finished.
It then performs a batched execution of these lazy operators over all vertices of {G k } K k=1 .
These operators includes, but is not limited to, the push operator that is doing memory copy, and operators for computing gradients of model parameters.
Lazy batching helps exploit more parallelism and significantly reduces kernel launches.
Empirically lazy batching brings 20% overall improvement ( §5.3).
To leverage the exhibited parallelization opportunity between eager operators and the operators on the path from gather to scatter (Figure 7), Cavs proposes a streaming strategy that pipelines the execution of these two groups of operators.
It allocates two streams, and puts the eager operators on one stream, and the rest (excluding lazy operators) on the other.
Hence, independent operators in two streams run in parallel, while for those operators that depend on an eager operator, this dependency is respected by synchronization barriers (Fig 7).
Automatic kernel fusion.
Given F, before execution, Cavs will run a fusion detector [20] to scan its corresponded dataflow graph and report all fuse-able subgraphs therein, i.e. all nodes in a fuse-able subgraph can be fused as a single operator that behaves equivalently but takes less execution time (e.g. with fewer kernel launches and I/O, or faster computation).
Currently, we only detect groups of directly linked elementwise operators, such as +, sigmoid, as shown in Fig 7, and we use a simple union-find algorithm to detect the largest possible fuse-able subgraphs.
Given a fuse-able subgraph, Cavs adopts de facto automatic code generation techniques [37,8,38,35] to generate lower-level kernel implementations.
Replacing the original fuse-able subgraphs with fused operators during execution is beneficial in many aspects: (1) it reduces the number of kernel launches; (2) on some devices such as GPUs, kernel fu-sion transform device memory access into faster device registers access.
We empirically report another 20% improvement with automatic kernel fusion ( §5.3).
Cavs is implemented as a C++ library and integrable with existing DL frameworks to enhance their support for dynamic NNs.
It is composed of three major layers (which is the case for most popular frameworks [3,1,33]): (1) a frontend that provides device-agnostic symbolic programming interface; (2) an intermediate layer that implements the core execution logic; (3) a backend with device-specific kernels for all symbolic operators.
Frontend.
In addition to the four APIs, Cavs provides a macro operator VertexFunction.
Users instantiate it by writing symbolic expressions and specifying methods to read input graphs.
It encapsulates scatter/gather semantics, so users can continue using higher level APIs.
To construct more complex NN architectures (e.g. encoder-decoder LSTM [43], LRCN [9])), users employ push and pull to connect multiple vertex functions, or to external structures.
Intermediate Layer.
Cavs has its core runtime logic at this layer, i.e. the batching scheduler, the memory management, and the execution engine, etc.
Backend.
Following practice [1,33,12], we implement device-specific operator kernels at this layer.
Cavs has optimized implementations for the four proposed operators (gather, scatter, pull, push).
Specifically, gather and pull index different slices of a tensor and puts them together continuously on memory; scatter and push by contrast splits a tensor along its batch dimension, and copy different slices to different places.
Cavs implements customized memcpy kernels for there four operators, so that copying multiple slices from (or to) different places is performed within one kernel.
Distributed Execution.
While Cavs's implementations are focused on improving the efficiency on a single node, they are compatible with most data-parallel distributed systems for deep learning [56,7,1], and can also benefit distributed execution on multiple nodes.
In this section, we evaluate Cavs on multiple NNs and datasets, obtaining the following major findings: (1) Cavs has little overhead: on static NNs, Cavs demonstrates equal performance on training and inference with other systems; On several NNs with notably difficult-tobatch structures, Cavs outperforms all existing frameworks by a large margin.
(2) We confirm the graph construction overhead is substantial in both Fold [27] and dynamic declaration [33], while Cavs bypasses it by loading input graphs through I/O.
(3) We verify the effectiveness of our proposed design and optimization via ablation studies, and discuss Cavs' advantages over other DL systems for dynamic dataflow graphs.
Environment.
We perform all experiments in this paper on a single machine with an NVIDIA Titan X (GM200) GPU, a 16-core CPU, and CUDA v8.0 and cuDNN v6 installed.
As modern DL models are mostly trained using GPUs, we focus our evaluation on GPUs, but note Cavs' design and implementation do not rely on a specific type of device.
We mainly compare Cavs to TensorFlow v1.2 [1] with XLA [18] and its variant Fold [27], PyTorch v0.3.0 [11], and DyNet v2.0 [33] with autobatching [34], as they have reported better performance than other frameworks [5,50] on dynamic NNs.
We focus on metrics for system performance, e.g. time to scan one epoch of data.
Cavs produces exactly the same numerical results with other frameworks, hence the same per-epoch convergence Models and dataset.
We experiment on the following models with increasing difficulty to batch: (a) Fixed-LSTM language model (LM): a static sequence LSTM with fixed steps for language modeling [42,43,55].
We train it using the PTB dataset [48] that contains over 10K different words.
We set the number of steps as 64, i.e. at each iteration of training, the model takes a 64-word sentence from the training corpus, and predicts the next word of each word therein.
Obviously, the computation can be by nature batched easily, as each sentence has exactly the same size.
(b) Var-LSTM LM: that accepts variable-length inputs.
At each iteration the model takes a batch of natural sentences with different length from PTB, and predicts the next words; (c) Tree-FC: the benchmarking model used in [27] with a single fullyconnected layer as its cell function.
Following the same setting in [27], we train it over synthetic samples generated by their code [47] -each sample is associated with a complete binary tree with 256 leaves (therefore 511 vertices per graph); (d) Tree-LSTM: a family of dynamic NNs widely adopted for text analysis [26,51].
We implement the binary child-sum Tree-LSTM in [45], and train it as a sentiment classifier using Stanford sentiment treebank (SST) dataset [40].
The dataset contains 8544 training sentences, each associated with a human annotated grammar tree, and the longest one has 54 words.
We first verify the viability of our design on the easiestto-batch case: Fixed-LSTM language model.
We compare Cavs to the following three strong baselines: (1) CuDNN [6]: a CuDNN-based fixed-step sequence LSTM, which is highly optimized by NVIDIA using handcrafted kernels and stands as the best performed implementation on NVIDIA GPUs; (2) TF: the official implementation of Fixed-LSTM LM in TensorFlow repository [46] based on static declaration; (3) DyNet: we implement a 64-step LSTM in DyNet based on dynamic declaration -we declare a dataflow graph per sample, and train with the autobatching [34] enabled; (4) Cavs with batching policy, and all input samples have a same input graph -a 64-node chain.
We train the model to converge, and report the average time per epoch in Fig 8(a)(e), where in (a) we fix the hidden size h of the LSTM unit as 512 and vary the batch size bs, and in (e) we fix bs = 64 and vary h. Empirically, CuDNN performs best in all cases, but note it is highly inflexible.
Cavs performs slightly better than TF in various settings, verifying that our system has little overhead handling fully static graphs, though it is specialized for dynamic ones.
We also conclude from Fig 8 that batching is essential for GPU-based DL: bs = 128 is nearly one order of magnitude faster than bs = 1 regardless of used frameworks.
For Cavs, the batching policy is 1.7x, 3.8x, 7.0x, 12x, 15x, 25x, 36x faster than nonbatched at bs = 2, 4, 8, 16, 32, 64, 128, respectively.Next, we experiment with Var-LSTM, the most commonly used RNN for variable-length sequences.
We compare the following three implementations (CuDNNbased LSTM cannot handle variable-length inputs): (1) TF: an official TensorFlow implementation based on the dynamic unroll approach described in §6; (2) DyNet: an official implementation from DyNet benchmark repository based on dynamic declaration [10]; (3) Cavs: where each input sentence is associated with a chain graph that has number of vertices equal to the number of words.
We vary h and bs, and report the results in Figure 8(b)(f), respectively.
Although all three systems perform batched computation in different ways, Cavs is consistently 2-3 times faster than TF, and outperforms DyNet by a large margin.
Compared to TF, Cavs saves computational resources.
TF dynamically unrolls the LSTM unit according to the longest sentence in the current batch, but it cannot prevent unnecessary computation for those sentences that are shorter than the longest one.We then turn to Tree-FC, a dynamic model for benchmarking.
Since vanilla TensorFlow is unable to batch its computation, we compare Cavs to (1) DyNet and (2) Fold, a specialized library built upon TensorFlow for dynamic NNs, with a depth-based dynamic batching strategy.
To enable the batching, it however needs to preprocess the input graphs, translate them into intermediate representations and pass them to lower-level TensorFlow control flow engine for execution.
We report the results in Figure 8(c)(g) with varying bs and h, respectively.
For all systems, we allocate a single CPU thread for graph preprocessing or construction.
Cavs shows at least an order of magnitude speedups than Fold and DyNet at h ≤ 512.
Because the size of the synthetic trees is large, one major advantage of Cavs over them is the alleviation of graph preprocessing/construction overhead.
With a single CPU thread, Fold takes even more time on graph preprocessing than computation ( §5.3).
Finally, we compare three frameworks on Tree-LSTM in Figure 8(d)(h): Cavs is 8-10x faster than Fold, and consistently outperforms DyNet.
One difference in this experiment is that we allocate as many CPU threads as possible (32 on our machine) to accelerate graph preprocessing for Fold, otherwise it will take much longer time.
Further, we note DyNet performs much better here than on Tree-FC, as the size of the input graphs in SST (maximally 54 leaves) is much smaller than the synthetic ones (256 leaves each) in Tree-FC experiments.
We observe DyNet needs more time on graph construction for large input graphs, and DyNet's dynamic batching is less effective on larger input graphs, as it has to perform frequent memory checks to support its dynamic batching, which we will discuss in §5.3.
We also compare Cavs with PyTorch -its per-epoch time on Tree-LSTM is 542s, 290x slower than Cavs when the batch size is 256.
Compared to other systems, PyTorch cannot batch the execution of dynamic NNs.
In this section, we investigate the graph construction overhead in Fold and DyNet.
To batch computation of different graphs, Fold analyzes the input graphs to recognize batch-able dynamic operations, then translates them into intermediate instructions, with which, TensorFlow generates appropriate control flow graphs for evaluation -we will treat the overhead caused in both steps as Fold's graph construction overhead.
DyNet, as a typical dynamic declaration framework, has to construct as many dataflow graphs as the number of samples.
Though DyNet has optimized its graph construction to make it lightweight, the overhead still grows with the training set and the size of input graphs.
By contrast, Cavs takes constant time to construct a small dataflow graph encoded by F, then reads input graphs through I/O.
To quantify the overhead, we separate the graph construction from computation, and visualize in Figure 9(a) how it changes with the average number of leaves (graph size) of input graphs on training Tree-FC, with fixed bs = 64, h = 512.
We compare (1) Cavs (2) Fold-1 which is Fold with one graph processing thread and (3) DyNet.
We plot for one epoch, both the (averaged) absolute time for graph construction and it percentage of the overall time.
Clearly we find that all three systems take increasingly more time when the size of the input graphs grows, but Cavs, which loads graphs through I/O, causes the least overhead at all settings.
In terms of the relative time, Fold unfortunately wastes 50% at 32 leaves, and 80% when the tree has 1024 leaves, while DyNet and Cavs take only 10% and 20%, respectively.
We also wonder how the overhead is related with batch size when there is fixed computational workload.
We report in Figure 9(b) the same metrics when training Tree-LSTM with varying bs.
We add another baseline Fold-32 with 32 threads for Fold's graph preprocessing.
As Fold-1 takes much longer time than others, we report its time at bs = 1, 16, 32, 64, 128, 256 here (instead of showing in Figure 9 works, while Cavs successfully overcomes this barrier.
Apart from the graph construction we report in Table 1 the computation-only time.
Cavs shows maximally 5.4x/9.7x and 7.2x/2.4x speedups over Fold/DyNet on Tree-FC and Tree-LSTM, respectively.
The advantages stem from two main sources: an optimized graph execution engine, and a better-suited memory management strategy, which we investigate next.
Graph Execution Engine.
To reveal how much each optimization in §3.4 contributes to the final performance, we disable lazy batching, fusion and streaming in Cavs and set this configuration as a baseline (speedup = 1).
We then turn on one optimization at a time and record how much speedup it brings.
We train Fixed-LSTM and Tree-LSTM, and report the averaged speedups one computation-only time in one epoch over the baseline configuration in Fig 10, with bs = 64 but varying h. Lazy batching and fusion consistently deliver nontrivial improvement -lazy batching is more beneficial with a larger h while fusion is more effective at smaller h, which are expected: lazy batching mainly parallelizes matrixwise operations (e.g. matmul) commonly with O(h 2 ) or higher complexity, while fusion mostly works on elementwise operations with O(h) complexity [19].
Streaming, compared to the other strategies, is less effective on Tree-LSTM than on Fixed-LSTM, as we have found the depth of the input trees in SST exhibit high variance, i.e. some trees are much deeper than others.
In this case, many batching tasks only have one vertex to be evaluated.
The computation is highly fragmented and the efficiency is bounded by kernel launching latency.
Lazy batching and fusion still help as they both reduce kernel launches ( §3.4).
Streaming, which tries to pipeline multiple kernels, can hardly yield obvious improvement.
Memory Management.
Cavs' performance advantage also credits to its memory management that reduces memory movements while guarantees continuity.
Quantitatively, it is difficult to compare Cavs to Fold, as Fold relies on TensorFlow where memory management is highly coupled with other system aspects.
Qualitatively, we find Cavs requires less memory movement (e.g. memcpy) during dynamic batching.
Built upon the tf while operator, whenever Fold performs depth- Hidden Size (h)1.01.11.21.31.41.5Speedup ( based batching at depth d, it has to move all the contents of nodes in the dataflow graphs at depth d −1 to a desired location, as the control flow does not support cross-depth memory indexing.
This results in redundant memcpy, especially when the graphs are highly skewed.
By contrast, Cavs only copies contents that are necessary to the batching task.
DyNet has a specialized memory management strategy for dynamic NNs.
Compared to Cavs, it however suffers substantial overhead caused by repeated checks of the memory continuity -whenever DyNet wants to batch operators with same signatures, it checks whether their inputs are continuous on memory [34].
The checking overhead increases with bs and is more prominent on GPUs.
Thanks to the simplicity of both systems, we are able to profile the memory-related overhead during both training and inference, and separate it from computation.
We compare them on TreeLSTM, and report the breakdown time per epoch in Table 3 under different bs.
We observe the improvement is significant (2x -3x) at larger bs, especially during inference where DyNet has its continuity checks concentrated.
DL programming models.
In addition to §2.2, we summarize in Table 2 the major programming models and frameworks for dynamic NNs, and their pros and cons, in contrast to Cavs.
Within static frameworks, there are also efforts on adapting static declaration to support sequence RNNs, such as static unrolling [17], bucketing [15] and dynamic unrolling [16].
The ideas are to pad zero at the end of samples so that they have the same structure (i.e. same length) for batched computation.
However, they all result in unnecessary computation and can not express more complex structures than sequences.
Asynchronous model-parallelism [13] enables the concurrent execution of different graphs similar to batched execution in Cavs, it however may suffer from insufficient cache re-usage and overhead by multiple kernel launches (on GPUs).
Execution optimization.
A variety of developed techniques from other areas (e.g. kernel fusion, constant folding) have been adapted to speed the computation of DL dataflow graphs [1,5,12,18].
Cavs separates the static vertex function from the dynamic-varying input graph, so it benefits from most of the aforementioned optimizations.
We learn from these strategies and reflect them in Cavs' execution engine.
We further propose lazy batching and concurrent execution to exploit more parallelism exposed by our APIs.Graph-based systems.
The vertex-centric programming model has been extensively developed in graph computing [29,14,4,41].
Cavs draws insights from the GAS model [14], but is fundamentally different: gather and scatter in Cavs are fully symbolic -they allow backpropagation through them; graph computing systems compute on large natural graphs, while Cavs addresses problems that each sample has a unique graph and the training is iterative on batches of samples.
In terms of system design, Cavs also faces different challenges, such as scheduling for batched execution of different graphs, guaranteeing the memory continuity.
There are also some graph-based ML systems, such as GraphLab [28], but they do not handle instance-based graphs, and do not offer batching advantages for dynamic DL workloads.
We present Cavs, an efficient system for dynamic neural networks.
With a novel representation, designed scheduling policy, memory management strategy, and graph execution optimizations, Cavs avoids substantial graph construction overhead, allows for batched computation over different structured graphs, and can benefit from well-established graph optimization techniques.
We compare Cavs to state-of-the-art systems for dynamic NNs and report a near one order of magnitude speedup across various dynamic NN architectures and settings.
