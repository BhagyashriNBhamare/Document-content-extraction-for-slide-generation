Recent advances in large-margin classification of data residing in general metric spaces (rather than Hilbert spaces) enable classification under various natural metrics, such as edit and earthmover distance.
The general framework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004] left open the question of computational efficiency and providing direct bounds on classification error.
We design a new algorithm for classification in general metric spaces, whose runtime and accuracy depend on the doubling dimension of the data points.
It thus achieves superior classification performance in many common scenarios.
The algorithmic core of our approach is an approximate (rather than exact) solution to the classical problems of Lipschitz extension and of Nearest Neighbor Search.
The algorithm's generalization performance is established via the fat-shattering dimension of Lipschitz classifiers.
A recent line of work extends the large-margin classification paradigm from Hilbert spaces to less structured ones, such as Banach or even metric spaces [HBS05, vLB04, DL07].
In this metric approach, data is presented as points with distances but without requiring the additional structure of inner products.
The potentially significant advantage is that the metric can be carefully suited to the type of data, e.g. earthmover distance for images, or edit distance for sequences.However, much of the existing machinery of generalization bounds [CV95, SS02] depends strongly on the inner-product structure of the Hilbert space.
von Luxburg and Bousquet [vLB04] developed a powerful framework of large-margin classification for a general metric space X .
First, they show that the natural hypotheses (classifiers) to consider in this context are maximally smooth Lipschitz functions; indeed, they reduce classification (of points in a metric space X ) to finding a Lipschitz function (f : X → R) consistent with the data, which is a classic problem in Analysis, known as Lipschitz extension.
Next, they establish error bounds in the form of expected-loss.
Finally, the computational problem of evaluating the classification function is reduced, assuming zero training error, to exact 1-nearest neighbor search.
This matches a common classification heuristic, see e.g. [CH67], and the analysis of [vLB04] may be viewed as a rigorous explanation for the empirical success of this heuristic.An important question left open by the work of [vLB04] is the efficient computation of the classifier.
Specifically, exact nearest neighbor search in general metrics might require time that is linear in the sample size, and it is algorithmically nontrivial to deal with training error.
In particular, the task of choosing which points will be misclassified by the hypothesis (i.e. optimizing the bias-variance tradeoff) remains to be addressed.Our contribution.
We solve the problems delineated above by showing that data with a low doubling dimension admits accurate and computationally efficient classification.
In fact, this is the first time in which the doubling dimension of the data points is tied to either classification error or algorithmic runtime.
(Previously, the doubling dimension of the space of classifiers was controlled by the VC dimension of the classifier space [BLL09].)
We first give an alternate generalization bound for Lipschitz classifiers, which directly bounds the classification error, rather than expected loss.
(A similar bound can in fact be derived from the analysis of [vLB04].)
Our bound is based on an elementary analysis of the fat-shattering dimension, see Section 3.
We then present our main contribution, and give an efficient computational implementation of the Lipschitz classifier.
In Section 4 we prove that once a Lipschitz classifier has been chosen, the classifier can be computed (evaluated) quickly on any new point x ∈ X , by utilizing approximate nearest neighbor search (which is known to be fast when points have a low doubling dimension).
In Section 5 we further show how to quickly compute a near-optimal classifier (in terms of classification error bound), even when the training error is nonzero.
In particular, this necessitates the optimization of the number of incorrectly labeled examples -and moreover, their identity -as part of the bias-variance tradeoff.
In Section 6 we give an example to illustrate the potential power of our approach.
We use standard notation and definitions throughout.Metric spaces.
A metric ρ on a set X is a positive symmetric function satisfying the triangle inequality ρ(x, y) ≤ ρ(x, z) + ρ(z, y); together the two comprise the metric space (X , ρ).
The diameter of a set A ⊆ X , is defined by diam(A) = sup x,y∈A ρ(x, y).
The Lipschitz constant of a function f : X → R, denoted by f Lip , is defined to be the smallest L > 0 that satisfies |f (x) − f (y)| ≤ Lρ(x, y) for all x, y ∈ X .
Doubling dimension.
For a metric (X , ρ), let λ be the smallest value such that every ball in X can be covered by λ balls of half the radius.
The doubling dimension of X is ddim(X ) = log 2 λ.
A metric is doubling when its doubling dimension is bounded.
Note that while a low Euclidean dimension implies a low doubling dimension (Euclidean metrics of dimension d have doubling dimension O(d) [GKL03]), low doubling dimension is strictly more general than low Euclidean dimension.The following packing property can be demonstrated via a repetitive application of the doubling property: For set S with doubling dimension ddim(X ), if the minimum interpoint distance in S is at least α, and diam(S) ≤ β, then |S| ≤ ⌈β/α⌉ ddim(X )+1 (see, for example [KL04]).
Learning.
Our setting in this paper is a generalization of PAC known as probabilistic concept learning [KS94].
In this model, examples are drawn independently from X × {−1, 1} according to some unknown probability distribution P , and the learner, having observed n such pairs (x, y) produces a hypothesis h :X → {−1, 1}.
The generalization error is the probability of misclassifying a new point drawn from P :P {(x, y) : h(x) 񮽙 = y} .
The quantity above is random (since it depends on a random sequence) and we wish to upper-bound it in probability.
Most bounds of this sort contains a sample error term (corresponding in statistics to bias), which is the fraction of observed examples misclassified by h and a hypothesis complexity term (corresponding to variance in statistics) which measures the richness of the class of all admissible hypotheses [Was06].
Keeping in line with the literature, we ignore the measure-theoretic technicalities associated with taking suprema over uncountable function classes.
In this section, we take a preliminary step towards our efficient classification algorithm by deriving generalization bounds for Lipschitz classifiers over doubling spaces.
As noted by [vLB04] Lipschitz functions are the natural object to consider in an optimization/regularization framework.
The basic intuition behind our proofs is that the Lipschitz constant plays the role of the inverse margin in the confidence of the classifier.
As in [vLB04], small Lipschitz constant corresponds to large margin, which in turn yields low hypothesis complexity and variance.
In retrospect, our generalization bound (Corollary 5 below) can be derived as a consequence of [vLB04, Theorem 18] in conjunction with [BM02, Theorem 5(b)].
We apply tools from generalized Vapnik-Chervonenkis theory to the case of Lipschitz classifiers.
Let F be a collection of functions f : X → R and recall the definition of the fat-shattering dimension [ABCH97, BS99]: a set X ⊂ X is said to be γ-shattered by F if there exists some function r : X → R such that for each label assignment y ∈ {−1, 1}X there is an f ∈ F satisfying y(x)(f (x) − r(x)) ≥ γ > 0 for all x ∈ X.
The γ-fat-shattering dimension of F , denoted by fat γ (F ), is the cardinality of the largest set γ-shattered by F .
For the case of Lipschitz functions, we will show that the notion of fat-shattering dimension may be somewhat simplified.
We say that a set X ⊂ X is γ-shattered at zero by a collection of functions F if for eachy ∈ {−1, 1} X there is an f ∈ F satisfying y(x)f (x) ≥ γ for all x ∈ X.(This is the definition above with r ≡ 0.)
We write fat 0 γ (F ) to denote the cardinality of the largest set γ-shattered at zero by F and show that for Lipschitz function classes the two complexity measures are the same.Lemma 1 Let F be the collection of all f : X → R with f Lip ≤ L.
Then fat γ (F ) = fat 0 γ (F ).
Proof: We begin by recalling the classic Lipschitz extension result, essentially due to McShane and Whitney [McS34, Whi34].
Any real-valued function f defined on a subset X of a metric space X has an extension f * to all of X satisfying f * Lip = f Lip .
Thus, in what follows we will assume that any function f defined on X ⊂ X is also defined on all of X via some Lipschitz extension (in particular, to bound f Lip it suffices to bound the restricted f | X Lip ).
Consider some finite X ⊂ X .
If X is γ-shattered at zero by F then by definition it is also γ-shattered.
Now assume that X is γ-shattered by F .
Thus, there is some function r : X → R such that for each y ∈ {−1, 1}X there is an f = f r,y ∈ F such that f r,y (x) ≥ r(x) + γ if y(x) = +1 and f r,y (x) ≤ r(x) − γ if y(x) = −1.
Let us define the functioñ f y on X and as per above, on all of X , by˜fby˜ by˜f y (x) = γy(x).
It is clear that the collectioñ collectioñ f y : y ∈ {−1, 1} X γ-fat-shatters X at zero; it only remains to verify that˜fthat˜ that˜f y ∈ F, i.e.,sup y∈{−1,1} X ˜ f y Lip ≤ sup y∈{−1,1} X f r,y Lip .
Indeed,sup y∈{−1,1} X ,x,x ′ ∈X f r,y (x) − f r,y (x ′ ) ρ(x, x ′ ) ≥ sup x,x ′ ∈X r(x) − r(x ′ ) + 2γ ρ(x, x ′ ) ≥ sup x,x ′ ∈X 2γ ρ(x, x ′ ) = sup y∈{−1,1} X ˜ f y Lip .
A consequence of Lemma 1 is that in considering the generalization properties of Lipschitz functions we need only bound the γ-fat-shattering dimension at zero.
The latter follows from the observation that the packing number of a metric space controls the fat-shattering dimension of Lipschitz functions defined over the metric space.
Let M (X , ρ, ε) be defined as the ε-packing number of X , the cardinality of the largest ε-separated subset of X .
Theorem 2 Let (X , ρ) be a metric space.
Fix some L > 0, and let F be the collection of all f :X → R with f Lip ≤ L.
Then for all γ > 0, fat γ (F ) = fat 0 γ (F ) ≤ M (X , ρ, 2γ/L).
Proof: Suppose that S ⊆ X is fat γ-shattered at zero.
The case |S| = 1 is trivial, so we assume the existence of x 񮽙 = x ′ ∈ S and f ∈ F such that f (x) ≥ γ > −γ ≥ f (x ′ ).
The Lipschitz property then implies that ρ(x, x ′ ) ≥ 2γ/L, and the claim follows.Corollary 3 Let metric space X have doubling dimension ddim(X ), and let F be the collection of realvalued functions over X with Lipschitz constant at most L.
Then for all γ > 0,fat γ (F ) ≤ Ldiam(X ) 2γ ddim(X )+1 .
Proof: The claim follows immediately from Theorem 2 and the packing property of doubling spaces.Equipped with these estimates for the fat-shattering dimension of Lipschitz classifiers, we can invoke a standard generalization bound stated in terms of this quantity.
For the remainder of this section, we take γ = 1 and say that a function f classifies an example (x i , y i ) correctly ify i f (x i ) ≥ 1.
(1)The following generalization bounds appear in [BS99]:Theorem 4 Let F be a collection of real-valued functions over some set X , define d = fat 1/16 (F ) and let and P be some probability distribution on X × {−1, 1}.
Suppose that (x i , y i ), i = 1, . . . , n are drawn from X × {−1, 1} independently according to P and that some f ∈ F classifies the n examples correctly, in the sense of (1).
Then with probability at least 1 − δ P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ 2 n (d log 2 (34en/d) log 2 (578n) + log 2 (4/δ)) .
Furthermore, if f ∈ F is correct on all but k examples, we have with probability at least 1 − δP {(x, y) : sgn(f (x)) 񮽙 = y} ≤ k/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)).
Applying Corollary 3, we obtain the following consequence of Theorem 4:Corollary 5 Let metric space X have doubling dimension ddim(X ), and let F be the collection of realvalued functions over X with Lipschitz constant at most L.
Then for any f ∈ F that classifies a sample of size n correctly, we have with probability at least 1 − δ P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ 2 n (d log 2 (34en/d) log 2 (578n) + log 2 (4/δ)) .
Likewise, if f is correct on all but k examples, we have with probability at least 1 − δP {(x, y) : sgn(f (x)) 񮽙 = y} ≤ k/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)).
(2)In both cases,d = fat 1/16 (F ) ≤ ⌈8Ldiam(X )⌉ ddim(X )+1 .
Given a labeled set (X, Y ) ⊂ X ×{−1, 1}, we construct our classifier in a similar manner to [vLB04, Lemma 12], via a Lipschitz extension of the labels Y to all of X .
Let S + , S − ⊂ X be the sets of positive and negative labeled points that the classifier correctly labels.
Our starting point is the same extension function used in [vLB04], namely, for all α ∈ [0, 1]f α = α min i y i + 2 d(x, x i ) d(S + , S − ) + (1 − α) max j y j − 2 d(x, x j ) d(S + , S − ) .
However, evaluating the exact value of f α (x) for each point x ⊂ X (or even the sign of f α (x) at this point) requires an exact nearest neighbor search, and in arbitrary metric space nearest neighbor search may require Θ(|X|) time.In this section, we give a classifier whose sign can be evaluated using a (1 + ε)-approximate nearest neighbor search.
There exists a search structure for an n point set that can be built in 2 O(ddim(X )) n log n time and supports approximate nearest neighbor searches in time 2 O(ddim(X )) log n+ε −O(ddim(X )) [CG06, HM06] (see also [KL04, BKL06]).
In constructing the classifier, we assume that the sample points have already been partitioned in a manner that yields a favorable bias-variance tradeoff, as in Section 5 below.
Therefore, the algorithm below takes as input a set of point S 1 ⊂ X that must be correctly classified, and a set of error points S 0 = X − S 1 that may be ignored in the classifier construction (but which affect the resulting generalization bound).
Theorem 6 Let X be a metric space, and fix 0 < ε ≤ 1 2 .
Given a labeled sample S = (x i , y i ) ∈ X ×{−1, 1}, i = 1, . . . , n, let S be partitioned into S 0 and S 1 , of sizes k and n − k, where S 0 contains points that may be misclassified, and S 1 contains points that may not be misclassified.
Define S + 1 , S − 1 ⊂ S 1 according to their labels and define L = 2/d(S + 1 , S − 1 ).
Then there exists a binary classification function h : X → {−1, 1} satisfying the following: (a) h(x) can be evaluated at each x ∈ X via a single (1 + ε)-nearest neighbor query.
In particular, h(x) can be evaluated in time 2 O(ddim(X )) log n + ε −O(ddim(X )) , after an initial computation of (2 O(ddim(X )) log n + ε −O(ddim(X )) )n time.
(b) With probability at least 1 − δP {(x, y) : h(x) 񮽙 = y} ≤ 2 k n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ))whered = ⌈8(1 + ε)Ldiam(X )⌉ ddim(X )+1 .
Proof: Let the distance functioñ d(·, ·) be the approximate distance between a point and a set (or between two sets), as determined by a fixed (1 + ε 4 )-nearest neighbor search structure.
Let˜fLet˜ Let˜f 1 (x) := min i y i + 2 ˜ d(x, x i ) ˜ d(S + 1 , S − 1 ) ,and let the classifier be h(x) := sgn( ˜ f 1 (x)).
h(x) can be evaluated via an approximate nearest neighbor query in time 2 O(ddim(X )) log n + ε −O(ddim(X )) , assuming that a search structure has been precomputed in time 2 O(ddim(X )) n log n, and˜dand˜ and˜d(S + 1 , S − 1 ) has been precomputed via O(n) nearest neighbor searches in time (2 O(ddim(X )) log n + ε −O(ddim(X )) )n.It remains to bound the generalization error of h. To this end, definef + 1 (x) = (1 + ε)f 1 (x) + ε = (1 + ε) min i y i + 2 d(x, x i ) d(S + 1 , S − 1 ) + ε, f − 1 (x) = (1 + ε)f 1 (x) − ε = (1 + ε) min i y i + 2 d(x, x i ) d(S + 1 , S − 1 ) − ε.Note that f + 1 (x) > f − 1 (x).
Both f + 1 (x) and f − 1 (x) correctly classify all labeled points of S 1 and have Lipschitz constant (1 + ε)L, so their classification bounds are given by Corollary 5 with this Lipschitz constant.We claim that h(x) always agrees with the sign of at least one of f + 1 (x) and f − 1 (x): If f + 1 (x) and f − 1 (x) disagree in their sign, then the claim follows trivially.
Assume then that the signs of f + 1 (x) and f − 1 (x) agree.
Suppose that f + 1 (x) and f − 1 (x) are positive, which implies that y j + 2d(x,xj) d(S + 1 ,S − 1 )> ε 1+ε for all j.
Now recall that˜fthat˜ that˜f 1 (x) = min i y i + 2˜ d(x,xi) ˜ d(S + ,S − ) ≥ min i y i + 2 (1+ε/4) 2 d(x,xi) d(S + ,S − ).
If y i = +1,then trivially h(x) is positive.
If y i = −1, we have that 2 d(x,xi) d(S + ,S − ) > ε 1+ε + 1 = 1+2ε 1+ε, and sõf 1 (x) ≥ min i y i + 2 (1+ε/4) 2 d(x,xi) d(S + ,S − ) > −1 + 1 (1+ε/4) 2 1+2ε 1+ε> 0, and we are done.
Suppose then that f + 1 (x) and f − 1 (x) are negative, which implies that y j + 2d(x,xj) d(S + 1 ,S − 1 )< − ε 1+ε for some fixed j.
Now it must be that y j = −1, and so 2d(x,xj) d(S + ,S − ) < − ε 1+ε + 1 = 1 1+ε .
Now recall that˜fthat˜ that˜f 1 (x) = min i y i + 2 ˜ d(x,xi) ˜ d(S + ,S − ) ≤ y j + 2(1 + ε/4) 2 d(x,xj) d(S + ,S − ) < −1 + (1 + ε/4) 2 1 1+ε< 0, and we are done.It follows that if h(x) misclassifies x, then x must be misclassified by at least one of f + 1 (x) and f − 1 (x).
Hence, the generalization bound of h(x) is not greater than the sum of the generalization bounds of f + 1 (x) and f − 1 (x).
In this section, we show how to efficiently construct a classifier that optimizes the bias-variance tradeoff implicit in Corollary 5, equation (2).
Let X be a metric space, and assume we are given a labeled sample S = (x i , y i ) ∈ X × {−1, 1}.
For any Lipschitz constant L, let k(L) be the minimal sample error of S over all classifiers with Lipschitz constant L.
We rewrite the generalization bound as follows:G(L) = P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ k(L)/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ))where d = ⌈8Ldiam(X )⌉ ddim(X )+1 .
This bound contains a free parameter, L, which may be tuned to optimize the bias-variance tradeoff.
More precisely, decreasing L drives the bias term (number of mistakes) up and the variance term (fat-shattering dimension) down.
For some optimal values of L, G(L) achieves a minimum value.
The following theorem gives our bias-variance tradeoff.Theorem 7 Let X be a metric space.
Given a labeled sample S = (x i , y i ) ∈ X × {−1, 1}, i = 1, . . . , n, there exists a binary classification function h : X → {−1, 1} satisfying the following properties:(a) h(x) can be evaluated at each x ∈ X in time 2 O(ddim(X )) log n, after an initial computation of O(n 2 log n) time.
(b) The generalization error of h is bound byP {(x, y) : sgn(f (x)) 񮽙 = y} ≤ c · inf L>0 k(L)/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)) .
for some constant c, and whered = d(L) = ⌈8Ldiam(X )⌉ ddim(X )+1 .
We proceed with a description of the algorithm.
We will first give an algorithm with runtime O(n 4.376 ), and then improve the runtime to O(n 2 log n).
Algorithm description.
Here we give a randomized algorithm that finds an optimal value L * , that is G(L * ) = inf L>0 G(L).
The runtime of this algorithm is O(n 4.376 ) with high probability.First note the behavior of k(L) as L increases.
k(L) may decrease when the value of L crosses some critical value: This critical value is determined by point pairs x i ∈ S + , x j ∈ S − (that is, L = 2 d(xi,xj) ) and implies that the classification function can correctly classify both these points.
There are O(n 2 ) critical values of L, and these can be determined by enumerating all interpoint distances between sets S + , S − ⊂ S.Below, we will show that for any given L, the value k(L) can be computed in randomized time O(n 2.376 ).
More precisely, we will show how to compute a partition of S into sets S 1 (with Lipschitz constant L) and S 0 (of size k(L)) in this time.
Given sets S 0 , S 1 ⊂ S, we can construct the classifier of Corollary 5.
Since there are O(n 2 ) critical values of L, we can calculate k(L) for each critical value in O(n 4.376 ) total time, and thereby determine L * .
Then by Corollary 5, we may compute a classifier with a bias-variance tradeoff arbitrarily close to optimal.It is left to describe how value k(L) is computed for any L in randomized time O(n 2.376 ).
Consider the following algorithm: Construct a bipartite graph G = (V + , V − , E).
The vertex sets V + , V − correspond to the labeled sets S + , S − ∈ S, respectively.
The length of edge e = (u, v) connecting vertices u ∈ V + and v ∈ V − is equal to the distance between the points, and E includes all edges of length less than 2/L.
(E can be computed in O(n 2 log n) time.)
Now, for all edges e ∈ E, a classifier with Lipschitz constant L necessarily misclassifies at least one endpoint of e. Hence, the problem of finding a classifier with Lipschitz constant L that misclassifies a minimum number of points in S is equivalent to finding a minimum vertex cover for bipartite graph G. By König's theorem, minimum bipartite vertex cover is itself equivalent to the maximum matching problem on bipartite graphs.
An exact solution to the bipartite matching problem may be computed in randomized time O(n 2.376 ) [MS04].
This solution immediately identifies sets S 0 , S 1 , which allows us to compute a classifier with a bias-variance tradeoff arbitrarily close to optimal.Improved algorithmic runtime.
The runtime given above can be reduced from randomized O(n 4.376 ) to deterministic O(n 2 log n), if we are willing to settle for a generalization bound G(L) within a constant factor of the optimal G(L * ).
The first improvement is in the runtime of the vertex cover algorithm.
It is well known that a 2-approximation to the minimum vertex cover on an arbitrary graph can be computed by a greedy algorithm in timeO(|V + + V − | + |E|) = O(n 2 ) [GJ77].
Hence, we may evaluate in O(n 2 ) time a function k ′ (L) which satisfies k(L) ≤ k ′ (L) ≤ 2k(L).
The second improvement uses a binary search over the values of L, which allows us to evaluate k ′ (L) for only O(log n) values of L, as opposed to all Θ(n 2 ) values above.
Now, we seek the a value of L for whichG ′ (L) = k ′ (L)/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)) is minimal.
Call this value L ′ .
Also note that for all L, G ′ (L) ≤ 2G(L), from which it follows that G ′ (L ′ ) ≤ 2G(L * ).
While we cannot efficiently find L ′ , we are able to use a binary search to find a value L for whichG ′ (L) ≤ 2G ′ (L ′ ) ≤ 4G(L * ).
In particular we seek the minimum value of L for which k ′ (L)/n ≤ 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)).
Now, decreasing L can only increase k ′ (L), so the solution to the inequality above necessarily yields an L for whichG ′ (L) ≤ 2G ′ (L ′ ) ≤ 4G(L * ).
The solution to the inequality can be computed through a binary search on all values of L. By Corollary 5, we can construct a classifier with a bias-variance tradeoff within a factor 4(1 + ε) of optimal.
The total runtime is O(n 2 log n).
To illustrate the potential power of our approach, we now analyze the doubling dimension of an earthmover metric X k that is often used in computer vision applications.
(k ≥ 2 is a parameter.)
Each point in X k is a multiset of size k in the unit square in the Euclidean plane, formally S ⊂ [0, 1] 2 and |S| = k (allowing and counting multiplicities).
The distance between such sets S, T (i.e. two points in X k ) is given by EMD(S, T ) = minπ:S→T 1 k s∈S s − π(s) 2 ,where the minimum is over all one-to-one mappings π : S → T .
In other words, EMD(S, T ) is the minimum-cost matching between the two sets S, T , where costs correspond to Euclidean distance.
The earthmover metric X above satisfies diam(X k ) ≤ √ 2, and ddim(X k ) ≤ O(k log k).
Proof: For the rest of this proof, a point refers to the unit square, not X k .
Fix r > 0 and consider a ball (in X k ) of radius r around some S. Let N be an r/2-net of the unit square [0, 1] 2 .
Now consider all multisets T of size k of the unit square which satisfy the following condition: every point in T belongs to the net N and is within (Euclidean) distance (k + 1/2)r from at least one point of S. Points in such a multiset T are chosen from a collection of size at most k · (k+1/2)r r/2 O(1) ≤ k O(1) (by the packing property in the Euclidean plane).
