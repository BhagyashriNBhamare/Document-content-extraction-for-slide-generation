We propose a new approach to fault tolerance that we call bounded-time recovery (BTR).
BTR is intended for systems that need strong timeliness guarantees during normal operation but can tolerate short outages in an emergency , e.g., when they are under attack.
We argue that BTR could be a good fit for many cyber-physical systems.
We also sketch a technical approach to providing BTR, and we discuss some challenges that still remain.
In everyday life, we are often willing to overlook small mistakes, as long as they are fixed quickly.
For instance, when a customer is shortchanged at the register but the clerk immediately notices the mistake and corrects it, the customer will typically not complain.
This makes sense because a) ensuring perfection -perhaps by hiring additional clerks that double-check the change before handing it over to the customer -would be cumbersome and expensive, and because b) the mistakes cause no actual damage, as long as they are noticed and fixed quickly.
The second point is crucial: if the customer notices the mistake after leaving the store, it will be difficult or impossible to get the money back.In this paper, we argue that allowing small mistakes could also be a useful approach to fault tolerance in distributed systems.
For instance, systems could provide the equivalent of the infamous "five-second rule": 1 when a fault occurs, the system is allowed to produce incorrect outputs for at most R seconds, but must recover and resume correct operation afterwards.
We refer to this property as bounded-time recovery (BTR).
At first glance, bounded-time recovery seems very weak, particularly if we consider adversarial faults, since it allows the system to output anything (or nothing) during the R-second recovery period.
However, the potential damage depends on how these outputs are used.
For instance, many distributed systems interact with some kind of physical system (say, an airplane), and this system often has some inertia or is able to tolerate some faults for other reasons.
For instance, the flight control system in an airplane can typically operate within a relatively large flight envelope [61] and is already equipped to handle small disturbances, e.g., because of air turbulence.
Be-cause of inertia, a short malfunction will not be enough to push the airplane out of this envelope and can thus be tolerated, as long as the system returns to correct operation quickly enough.
A similar argument holds true for many other cyber-physical systems (CPS) [64,72].
BTR has at least three potential advantages to offer over existing alternatives, two direct ones and one indirect one.
The direct ones are efficiency and support for timeliness.
BTR can be more efficient than, say, BFT because it provides weaker guarantees; for instance, detection requires fewer replicas than masking [36], and BTR can use the output of some replicas without waiting for the others to complete.
BTR can also guarantee that outputs are timely when an attack is absent, and that an attack can only affect the outputs for a bounded amount of time.
This is in direct contrast to most existing faulttolerance solutions, which tend to sacrifice liveness to preserve safety, and it is an important property for the applications we envision (e.g., CPS), where it is not enough if the system recovers "eventually": real, physical damage can occur if correct outputs are missing too long.
BTR's indirect advantage is an ability to offer more fine-grained responses to faults and attacks.
Since BTR guarantees timeliness, a BTR-enabled system must necessarily monitor the workload and the available resources very carefully; thus, when a fault does occur, it can make very detailed tradeoffs to utilize the remaining resources in the best possible way.
For instance, many CPS run a mixed workload with tasks of different criticality levels [67] -the CPS on an airplane might run flight control and the in-flight entertainment system.
Thus, when a fault occurs, the system can disable some of the less critical tasks and allocate their resources to the more critical ones.
This is in contrast to many existing fault-tolerance approaches that treat the workload as a "black box" (and thus protect either all of it or none of it), and that abstract away most of the details of the resources, e.g., the amount of available bandwidth or the network topology.Building a practical BTR technique involves several interesting challenges -e.g., with respect to scheduling, fault detection, and reconfiguration -and we discuss these in the rest of this paper, along with a sketch of a possible BTR technique.
However, our main goal is to make the case for BTR as an interesting addition to the fault-tolerance toolbox that should have useful, practical applications in systems such as CPS.
For the purposes of this paper, we use cyber-physical systems (CPS) as our running example.
A cyber-physical system contains both digital control and physical sensors and actuators; examples include factory or power plant control systems [54], avionic systems [24], building control systems [22], robots [66], and even self-driving vehicles [7].
Unlike classical embedded systems, which typically have a central controller, CPS are truly distributed systems, with a heterogeneous mix of different nodes that are connected by various networks: even a simple CPS such as a modern (non-self-driving) car contains about a hundred microprocessors [20].
Requirements: CPS require very high reliability: if they fail, or even take too much time to respond to a trigger from their environment, the result can be physical damage or even loss of life.
As discussed earlier, timing is critical: for instance, when a sensor indicates a pressure increase in some part of the system, the system may need to respond within seconds -e.g., by opening a safety valve -to prevent an explosion.
However, somewhat counterintuitively, perfect accuracy is not as critical: the physical part of the system has properties like inertia or thermal capacity, and thus can tolerate small mistakes or omissions, as long as they are fixed within a bounded amount of time.
Control algorithms can be designed to preserve stability under these conditions [56,64,72].
In other words, this domain requires stronger liveness, and potentially weaker safety, than classical fault-tolerance solutions tend to offer, making it suitable for BTR.Network: The strict timing requirements have consequences for the network: in the presence of an end-to-end deadline for multi-node data flows, queueing delays or packet drops can be devastating.
Hence, it is more common to see circuit-switched networks with strict bandwidth reservations, which enable predictable timing and prevent packet drops due to queue overflows.
Packets can still be dropped due to transmission errors, but forward error correction (FEC) can be used to minimize this risk where necessary.
Thus, stronger synchrony assumptions about the network seem reasonable in this domain.Processing power: CPUs in this domain are often far less powerful than CPUs in servers and workstations.
This is partly due to a desire to reduce cost (or size, weight, power consumption, ...), which encourages system designers to use the least powerful CPU that will do the job, at the lowest possible clock frequency.
Indeed, the impact on clock frequency is a common evaluation metric in this domain, e.g., for scheduling techniques.
Because of this, developers may be reluctant to accept the cost of techniques like BFT, especially if their strong safety guarantees are not strictly needed.
Existing fault-tolerance techniques commonly assume either an asynchronous (or at most weakly synchronous) system model [17,40] or a simple synchronous one in which processes take abstract "steps" and the details of the network (such as bandwidth and topology) are abstracted away.
Neither model is suitable for our purposes: to give timeliness guarantees, we need both synchrony and a more detailed view of the available resources.
Therefore, we define our own tentative system model that captures some of the special features of CPS that we have outlined above.System model: The system consists of a set of nodes and a set of links.
Nodes have a finite processing speed and access to a local clock.
For simplicity, we assume that the processing speeds are all the same; some nodes are sources or sinks, that is, they can receive inputs from, or accept outputs for, the physical world.
Each link is connected to some subset of the nodes and has a finite bandwidth.
We assume that losses are rare enough to be ignored, and that there is some solution to the babblingidiot problem [11] -e.g., that the bandwidth of each link is statically allocated between the nodes.
Some of these assumptions are very strong (for instance, there is a rich literature on clock synchronization alone [3,10,25,32,46]), and in a longer paper we would definitely discuss them in more detail.
Here, we can only briefly argue that they do seem reasonable for typical CPS hardware: for instance, buses like CAN support the use of FEC to mask packet corruption [11], and the MAC is often implemented in hardware and thus can enforce bandwidth allocations even if nodes are corrupted.Workload: For simplicity, we assume a static, periodic workload that can be described as a dataflow graph.
(This model clearly does not fit all possible CPS, but seems appropriate for many -e.g., [58].)
The system has a period P and releases a set of tasks during each period.
Each task requires some inputs from the sources and/or from other tasks, and it sends at least one output to a sink or another task.
Each output has a criticality level and a deadline by which it must arrive at the appropriate sink.Threat model: We assume Byzantine [47] faults -that is, there is an adversary who has compromised some subset of the nodes and has complete control over them.
Perhaps surprisingly, this model is considerably stronger than the one CPS typically use: although faults can result (and have resulted [44,48,63,73]) in substantial damage and many CPS must therefore undergo a strict certification process, this process is usually based on crash faults.
However, we note that there is growing concern about non-crash faults and attacks, both in the community [15,16] and from operators and the government [1].
We now offer a first intuitive (but still imperfect) definition of BTR.
Assume that, for each node in the system, there is some notion of its expected behavior -for instance, a function that maps a sequence of messages m in i that the node receives at times t in i to a sequence of messages m out j that the node should send at times t out j .
We say that the node is correct in an interval [t 1 ,t 2 ] if its actual behavior matches the expected behavior, and we say that a fault manifests on the node at time t if it is correct in [0,t) but not in [0,t ) for any t > t. Similarly, we say that the outputs of the system as a whole (e.g., its commands to the actuators) are correct in an interval [t 1 ,t 2 ] if they are consistent with the outputs of a system in which all nodes are correct.
Then we can define BTR as follows:Definition 3.1 (Bounded-time recovery).
A system offers recovery with a time bound R if its outputs are correct in any interval [t 1 ,t 2 ] such that no fault has manifested in [t 1 − R,t 2 ).
In other words, the system is allowed to produce incorrect outputs during an interval of length R whenever a fault manifests on a new node.In practice, BTR would obviously need a bound f on the number of nodes that can become faulty.
Note that, if an adversary controls k ≤ f nodes, he can trigger a new fault every R seconds and thus potentially force the system to produce bad outputs for kR seconds; thus, if the system has an overall deadline D after which damage can occur in the absence of correct outputs, it seems prudent to set R := D/ f rather than R := D. Also, our intuitive definition does not yet account for mixed-criticality, but it should not be difficult to extend it, e.g., by allowing a certain set of outputs to fail permanently if the number of faults rises above a certain level.
There are two special cases where BTR closely resembles existing fault-tolerance approaches: for R = 0, BTR is analogous to classical fault tolerance -as in BFT [17,47] -where all faults must be masked; without a hard upper bound on R, BTR closely resembles selfstabilization [28], where the system is simply required to return to correct operation eventually.
However, even with R = 0, BTR is not quite the same as BFT because a) it has a different system model, with finite resources and a partially connected network, and because b) it gives strong timing guarantees, even in the presence of compromised nodes.
Thus, an implementation of BTR always requires a set of detailed schedules for different scenarios to ensure that the timing guarantees can be met.The difficulty of BTR also depends on the magnitude of R and the amount of resources the system has available.
If R is large, the system can simply drop all of its current tasks as soon as a fault is detected, and then perform diagnosis and rescheduling at its leisure; if there are plenty of resources, the system can afford enough replicas for fault tolerance, which of course simplifies recovery (though not necessarily planning).
However, recall that CPS are often resource-constrained and tend to have strong timeliness requirements, so we expect the "easy" cases to be less common in practice.
Our approach to BTR is centered around the concept of a plan, which is basically a distributed schedule: it maps the tasks from the workload (and some additional tasks, such as replicas) to specific nodes, and it prescribes a schedule for each of the nodes.
At runtime, the system is either operating in one of several modes, which correspond to a particular plan, or it is executing a mode transition, in which the system starts, reconfigures, or shuts down tasks on some of the nodes in order to switch to a new plan.
(Mode transitions are triggered by detected faults and are intended to isolate the faulty nodes from the rest of the system.)
Together, the plans, and the conditions for switching between them, form the system's strategy for responding to faults.We use four components to achieve BTR: an offline planner, which computes a feasible strategy, based on the requested workload, the fault assumptions (such as an upper bound on the number of faulty nodes), and the desired recovery bound R; an online fault detector that looks for manifested faults and generates some kind of evidence; an evidence distributor that quickly and reliably gets the evidence to the nodes that need to know about it; and a mode switcher that executes the appropriate mode transition based on valid evidence of faults.
Next, we discuss each of the components in more detail.
Before the system can run a given workload, it must first find a strategy that can ensure BTR.
Some representation of the strategy is then installed in each node, so that correct nodes will have a consistent view of it at runtime.
Choosing the strategy offline seems safer than dynamic rescheduling at runtime because a) a centralized scheduler would be an obvious target for the adversary, and because b) to guarantee BTR, we would need a time bound on rescheduling, which seems difficult to obtain.The planner first augments the dataflow graph with additional tasks.
It adds 1) replicas; 2) checking tasks, which compare the outputs of the replicas to detect faults and generate evidence; and 3) verification tasks, which distribute and verify incoming evidence from other nodes.
These tasks all consume resources at runtime and must therefore be scheduled together with the workload tasks -there are no "extra resources" for BTR.Next, the planner computes a plan for each mode.
Each task is mapped to a node; this involves some "hard" constraints -for instance, no two replicas of the same task can run on the same node -but also some heuristics: for instance, putting replicas close to each other may save bandwidth, and putting checking tasks close to replicas can make it easier to detect omission faults.
The planner then tries to derive a schedule for each node and a resource allocation for each link.
If the system is not schedulable (e.g., because the current mode contains many faulty nodes, and thus few resources), the planner removes some of the less critical tasks and retries.
Challenges: So far, there has not been much work on real-time mixed-criticality scheduling for distributed systems; most of the work we are aware of is for single or multicore CPUs [12].
Moreover, BTR's scheduling problem involves dependencies, which are challenging to support -especially in mixed-criticality systems.
Dependencies can arise between tasks (e.g., checking tasks must run after the corresponding replicas) but also between plans.
For instance, if plan B would be activated if a fault is detected on node X while the system is running some other plan A, then B must obviously reassign the tasks that were running on X, but it should otherwise change as little as possible.
Any extra reassignments will consume resources (e.g., bandwidth for transferring state) and can thus prolong recovery.Finally, planning has an interesting strategic component.
Suppose, for instance, that the planner has already chosen a plan Π {X} for the case where node X has failed, and is now looking for a plan Π {X,Y } that can handle an extra fault on node Y.
If the planner was not careful when choosing Π {X} , it may be impossible to find a Π {X,Y } that can be activated quickly enough -for instance, a task with a lot of state may have been moved to a node whose only high-bandwidth connection to the rest of the system is via Y. Thus, computing a strategy is a bit like building a game tree for a game like chess.
Techniques like empirical game-theoretic analysis [68,69] may be useful for finding good strategies efficiently.
Fault detection and diagnosis in distributed systems are interesting problems in their own right [49], particularly when non-crash faults are considered [36,41].
However, BTR adds an interesting twist: since there are no trusted nodes, the compromised nodes can try to confuse the detector, e.g., by reporting nonexistent faults or by making false statements about the actions of other nodes.
Therefore, it is necessary to generate evidence of detected faults that other nodes can verify independently.
Challenges: Systems like PeerReview [37] can already generate evidence for asynchronous systems, but synchronous systems -and BTR particularly -present at least three additional challenges.
First, BTR additionally requires the detection of timing-related faults (such as doing the right thing at the wrong time).
Second, BTR requires a time bound on detection; this is difficult because an adversary can break the BTR guarantee simply by delaying the detector, e.g., by running a DoS attack against some of the nodes.
The strong assumptions in our system model should be enough to build a countermeasure, but even so, designing and proving the correctness of a concrete protocol that does this seems challenging.The third challenge is handling omission faults.
In contrast to commission faults, there is no direct way to prove that a faulty node failed to send -or correctly sign -a required unicast message.
Thus, a faulty node may be able to drain substantial resources from the system by constantly failing to send messages and then claiming that the problem is with the recipient.
One way to avoid this would be to allow both the sender and the recipient to declare (without further evidence) a problem with the path between them; the system could then a) switch to a mode that does not use this particular path, and b) keep track of which paths have been declared problematic.
If a node is on a large number of problematic paths, it may be possible to attribute the problem to that node.
Once a node has detected a fault, the resulting evidence must quickly be distributed to any other nodes that need to be aware of it.
(If the system's strategy is compositional, not all nodes will need to know about all faults, at least not immediately.)
The distribution process must a) compete for resources with the foreground tasks, b) be completed within bounded time, and c) prevent the adversary from causing delays via DoS, e.g., by flooding the system with bogus evidence.
Challenges: As a first approximation, we can achieve the above properties by reserving some amount of computation and bandwidth for evidence distribution, and by having each node validate incoming evidence before distributing it further.
If nodes are required to endorse evidence they distribute, invalid evidence can be counted as evidence against the signer.
However, a compromised node can still fabricate evidence that is improperly signed, or that can only be recognized as invalid after a lot of expensive computation -thus, there must be a way to quickly recognize and reject such cases.
When a node receives evidence of a new fault, it consults the strategy, picks the plan for the new fault pattern, and initiates a mode change to transition to this new plan.
This can involve starting new tasks or terminating existing ones, sending or receiving the state of migrating tasks, and adjusting the local schedule.At first glance, it may seem that some form of global agreement (e.g., via BFT) is needed to reconfigure the system.
Agreement would certainly suffice, but it does not seem necessary: since the new plan is a function of the set of faulty nodes, it is sufficient for the nodes to agree on the latter -but (if we ignore physical repair by the administrator) this set is append-only, and, if a node receives valid evidence of a fault on some other node X, it can safely add X to its local set.
Thus, as long as all new evidence reaches each correct node, the system should converge to a single, consistent plan.
Challenges:The key challenge here is not convergence itself but rather coordination and timing: if different nodes switch modes at different times, some confusion can briefly result, e.g., when a new task on some node X waits for an input from another node Y that has not yet completed (or even started) its mode transition.
Since BTR allows the system to produce incorrect outputs for a limited time, some brief confusion may even be acceptable, as long as evidence distribution and mode transitions are fast enough, but for quick recovery, a more sophisticated solution is needed.
So far, the literature on real-time systems and on noncrash fault tolerance has had few intersections -the former has mostly focused on crash faults, and the latter mostly on asynchronous systems.
This disconnect has been pointed out before us, e.g., by Aguilera and Walfish [4], and BTR is an attempt to bridge this gap.
Recovery: The idea of recovering systems from disruptions is not new; indeed, the term "bounded-time recovery" has been used in the database literature [60] to describe a real-time database that can recover from a failure within bounded time.
However, most of the recovery work we are aware of is application-specific, such as [18,19,35], or focuses exclusively on crash faults, such as [13,60].
Some systems also support simple forms of recovery, such as rebooting faulty machines [17] or application components [14].
BFT: There is a rich literature on protocols for tolerating Byzantine faults [2,17,26,34,45], and some of these protocols have been applied to time-critical distributed systems (e.g., in [39,42,52]).
Many classical BFT protocols are unsuitable for time-critical systems [62], but more recent protocols have improved in this respect [6,23,51], although they still do not provide "hard" timing guarantees.
In contrast, BTR focuses explicitly on timeliness, which requires different assumptions and a more detailed system model; it also offers different properties, e.g., less masking at lower cost.ZZ [71] reduces the normal-case overhead of BFT by running only f + 1 replicas by default, and by changing to agreement only if these replicas disagree.
BTR shares this reactive, detection-based approach, but ZZ does not offer timeliness or fine-grained recovery strategies.
Self-stabilization: One way to make a distributed system fault-tolerant is to ensure that it converges to a correct state even if it is started in an incorrect state.
This approach was first proposed by Dijkstra [28] and has led to a rich body of work on self-stabilizing systems (see [30,59]).
Much of the early work assumed that faults are benign and cannot handle malicious nodes that might constantly steer the system away from its goal.
Recent work [9,27,29,31,38,50] has extended the approach to the Byzantine setting, but this line of work tends to use a very different system model that does not consider scheduling, deadlines, task criticality, or complex network topologies; thus, an application in the context of CPS would be difficult.
CPS security: There is some existing work on faulttolerant real-time systems, such as Mars [43] and DeCoRAM [8], as well as on fault-tolerant and/or reconfigurable control systems [74].
However, most of this work has considered various types of "benign" faults, such as hardware defects, software bugs, or electromagnetic interference.
There is also an emerging security-oriented research trend within the control systems and CPS communities (see [33] and the references therein).
However, existing solutions either assume a centralized setting (e.g., [70]) or focus more on attacks on the sensors and actuators, and not on the controllers (e.g., [5,33,53,65]).
Accountability: PeerReview [37] can detect node misbehavior in distributed systems and produce evidence of it; however, with one exception, this line of work has focused exclusively on asynchronous systems.
The exception is TDR [21], which can detect time-related misbehavior -specifically, covert timing channels -but can neither offer recovery nor a time bound on detection.
Multi-mode systems: Many real-time embedded systems can operate in multiple modes that involve different sets of tasks, and transitioning between modes requires elaborate mode-change protocols (MCPs) [55,57] to prevent deadline misses and other disruptions.
Our recovery approach builds on MCPs, but, to our knowledge, all the existing work on MCPs assumes benign faults and cannot work reliably when the system is compromised.
We believe that there is room in the "fault-tolerance toolbox" for an approach that focuses more on timeliness and less on perfect safety, and we have argued that many cyber-physical systems (and perhaps other systems) could benefit from such a approach.
We have made a concrete proposal (BTR), and we have sketched a technique that can achieve it.
However, some interesting challenges remain, and we have started to address some of them in our ongoing work.
