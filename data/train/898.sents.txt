Forecasting has attracted a lot of research interest, with very successful methods for periodic time series.
Here, we propose a fast, automated method to do non-linear forecasting, for both periodic as well as chaotic time series.
We use the technique of delay coordinate embedding, which needs several parameters; our contribution is the automated way of setting these parameters, using the concept of 'intrinsic di-mensionality'.
Our operational system has fast and scalable algorithms for preprocessing and, using R-trees, also has fast methods for forecasting.
The result of this work is a black-box which, given a time series as input, finds the best parameter settings, and generates a prediction system.
Tests on real and synthetic data show that our system achieves low error, while it can handle arbitrarily large datasets.
Given the sequence of values taken by an observable over time, a forecasting system attempts to predict the observable's values over some future time period.
This has applicability in a wide range of fields, such as financial systems (exchange rates), physics (laser fluctuations) and biology (physiological data over time).
Many techniques have been suggested, but are either too simplistic for many realworld problems (such as linear models like ARMA), or require long training times and large number of parameters (such as Neural Networks).
Our approach uses a forecasting method called "Delay Coordinate Embedding".
This has the advantage of being firmly grounded in theory, and can handle periodic as well as chaotic datasets.
Its disadvantage was that its parameters had to be set manually.
Our F4 (Fractal FOREcasting) system provides automatic methods to do this, without any human intervention.
This results in a black-box which, given any time series, can automatically find the optimal parameters and build a prediction system.Our method, being completely automated, can operate in a sensor network setting, which is an area of increasing interest ( [12], [5], [3]).
Each sensor will be collecting time series data, and it will be doing forecasting, and thus be able to spot outliers and find patterns.
In such an environment, human intervention and manual setting of parameters is out of the question, because sensors may be operating in a humanhostile environment, with low communication bandwidth for shipping data and getting human feedback.Problem Definition (Predict-1):• Given a time series x1, x2, . . . , xt,• predict the value of xt+1.
Later in the paper, we shall generalize the problem to allow multiple time series and n-step-ahead predictions.
An example time series is shown in Figure 1(a).
The rest of the paper is organized as follows: Section 2 gives a background tutorial on certain techniques used by our algorithms.
Section 3 details our contributions, and gives the algorithms we use.
Section 4 gives our experiments and results.
Section 5 describes related work in the field of time series prediction, followed by the conclusions in Section 6.
We first introduce certain concepts.
Let the observation sequence be represented by the series xt, which gives the value of the time series at time t. Let us define the following vector: Here, τ is some real number greater than zero called the time delay, and L (called the lag length) is any integer greater than zero.
Definition 2.
Lag Plot: A plot of this (L + 1) dimensional vector space is called the Lag Plot for lag L.
This vector space is also called the Phase-Space.
We explain the technique we use with an example.
Fig- ure 1(a) shows a plot of the Logistic Parabola time series.
Let us draw its lag plot with a lag of L = 1, for a moment leaving aside the question of why L = 1 was chosen.
The lag plot is shown in Figure 1(b).
We see that a definite structure becomes apparent in the lag plot, which was hidden in the first plot.
This can be exploited for prediction purposes.
To predict xt+1 given xt, we find all the points in the lag plot whose X-values are the k-nearest-neighbors of xt (assuming a given k).
We take the Y-values of these points, and interpolate between them.
The result is our predicted value of xt+1.
This idea can be generalized to lag plots with L > 1, with KNN being done on the L dimensions representing xt−τ , . . . , xt−Lτ , followed by interpolation on the dimension representing xt.
As against this, we show the fit obtained by using an autoregressive (AR) model in figure 1(c).
AR attempts to minimize least-squares distance, which leads to very bad predictions on chaotic time sequences such as the Logistic Parabola, as shown.This soundness of this method is based on a certain result (generally known as Takens' Theorem), which is outside the scope of this paper.
It says that there is an optimal value of L, and increasing L beyond that value will not yield any better predictions.
Interested readers are referred to [27,26] for details.Hence, the problem of time series prediction can be divided into two subproblems:1.
Finding the right values for the lag length (Lopt) and number of nearest neighbors (kopt)2.
Prediction of future values given the current delay coordinate vectorThe value of the time delay (τ ) is usually τ = 1.
For setting the correct lag length Lopt, most current implementations either involve manual parameter setting or techniques requiring thresholding/bucketization.
This is one of the issues for which we propose solutions.
Again, the determination of kopt has till now been ad hoc.
Here we propose a method to automatically set the value of kopt.
The problem is finding the optimal value for lag.
Abarbanel( [1]) suggests a method called False Nearest Neighbors to estimate this.
We found this technique to be unsuitable for inclusion in a black-box prediction system, because it requires thresholding, and the results are sensitive to the threshold.
Our upcoming methods solve this problem of choosing L.
The fractal dimension of a cloud of points gives an estimate of the "intrinsic dimensionality" of the data in that space.
For example, if we consider points along the diagonal of a cube, the intrinsic dimensionality of these points is 1 (because they actually lie along a straight line), but the extrinsic dimensionality is 3 (because the individual points are expressed in 3D-coordinates).
Similarly, a parabola in two dimensions (as in Figure 1(b)) also has an intrinsic dimensionality of 1.
There are several fractal dimensions existing in literature.
Two of the most used ones are called the "correlation fractal dimension" and the "Hausdorff dimension".
The correlation integral is obtained by plotting the number of pairs of points (P (r)) within a radius r, against r, on a log-log plot.
For fractals, the plot consists of a horizontal part followed by an incline, and then another horizontal part.
The correlation integral is the slope of the middle part.
A faster way to compute it is through the Box-counting plot( [4]).
Figure 2 shows several example datasets, and their fractal dimensions.
Plots (a) and (b) show the 'Circle' dataset, where the fractal dimension is found to be 1.
This correctly captures the fact that given one dimension, the other dimension is automatically determined, so the intrinsic dimensionality is only 1.
Plot (c) shows a randomly spread cloud of points, whose fractal dimension is found to be 2 from plot (d).
Again, this is expected because the dimensions are mutually independent in this case.
Plots (e) and (f) show the fractal dimension plot for a set of points called the "Sierpinski Triangle".
Here, each dimension provides some information about the other, so the fractal dimension should be between 1 (for complete information) and 2 (for no information).
It is actually found to be approximately 1.56.
The reader is referred to [21] for more details.
We recap the problem: Given a time series x1, x2, . . . , xt, predict the value of xt+1.
We can generalize this problem as follows:Problem Definition (Predict-n):The training set (also called the historical data, for example, stocks) is a set of time series generated by the same physical system over separate periods of time.T S = X1, . . . , XNwhere Xi = xt i , xt i +1, . . . , x t i +(l i −1) .
Here, xt is the value of the time series at time t, and li is the length of sequence Xi.
Also, the forecasting system is provided with the query sequence Y = y1, . . . , y l from the same physical system, which must be forecasted; that is, we need to find y l+1 , y l+2 , . . . and so on.
Thus, we have several training sequences, and one query sequence.If we had only one training sequence, which was also the query sequence, then this would reduce to the Predict-1 problem.We automate the delay-coordinate-embedding approach in our forecasting system.
An advantage of this approach is its firm mathematical basis [27,26].
To automate the forecasting process, the system needs to be able to automatically .
This appears hard to predict, but on generating the two-dimensional lag-plot, shown in (b), we see the pattern in the data.
This pattern can be used for prediction.
Part (c) shows the AR fit, which is obviously not good.
The first figure shows the overview of the F4 system.
Given a training set and a query, it forecasts the future of the query time series.
The second figure expands the F4 system.
The training set of time series is fed into a preprocessor, which outputs the "best" values of lag-length and number of nearest neighbors(k) to use.
Then the time series are stored in an R-Tree using this lag-length.
When queries come, the its k-nearest neighbors in lag space are retrieved.
The points succeeding these neighbors are noted and interpolated to give the value for the prediction for the current query.
The preprocessor itself is our novel contribution, and the shaded portions can be off-the-shelf components.set the values of the parameters, such as the optimal lag length Lopt and optimal number of nearest neighbors kopt.The thrust of our research has been on formalizing methods for setting these parameters, without the use of thresholding/bucketization techniques, which introduce arbitrariness into the system.
Thus, we shall have a black box, which, given a time series, will generate a similarity search system based on this approach, with all parameters set automatically.
Given a cloud of points in an L-dimensional space, let us call its fractal dimension fL.
Now, with a given time series, we can form several clouds of points by considering L = 1 . . . Lmax.
Now we define the concept of a "Fractal Dimension vs Lag" (FDL) plot.Definition 3.
FDL plot of a sequence: This is a plot of fL versus L, for L = 1 . . . Lmax.Beyond a certain point, increasing the lag length adds no new information regarding the state space, so the FDL plot should flatten out.
The lag at which it settles could be used as the optimal lag length.
For example, Figure 6(a) shows the FDL plot for the LASER dataset.
We see that the plot starts flattening at lag L = 7.
Thus, the optimal lag length in this case is Lopt = 7.
Current algorithms for finding fractal dimension scale almost linearly with number of datapoints, so using this method is fast and scalable.Generating the FDL plot requires a value of Lmax.
We generate this value adaptively.
We find fL for L = 1, 2, . . . [L] is within delta=95% of the maximum fd return L(opt); } One possible method for estimating Lopt would be by using Cross-Validation.
In this case, the historical data is divided into a training set and a holdout set, and for each L = 1, 2, . . . , Lmax, the error is estimated over the holdout set.
The value of L at which this error is minimized is chosen to be Lopt.
But this approach requires us to do k-nearestneighbors for each value of L, which means that an R-Tree has to be built for each value.
This is an extremely timeconsuming operation, and cannot be done for large datasets.Hence, this technique cannot be used to find Lopt.
Our approach is more scalable and can be extended to very large datasets.
Our approach requires finding k nearest neighbors to a given delay vector.
Current implementations leave the value of k to be fixed manually.
The textbook approach is to have a dynamic assignment by using the technique of crossvalidation.
In this, we start by dividing the available data into a training and a holdout set.
Using the optimal lag, we build an R-Tree using the training set.
Then, starting with k as 1, we find the prediction error over the holdout set.
We keep incrementing k until the prediction error stops decreasing significantly.We propose a new method for setting kopt in this problem setting.
Our observation is that the minimum number of points needed to bracket one point in f dimensions is f + 1.
This leads us to the following conjecture.Conjecture 1.
The optimal number of nearest neighbors should depend linearly on the intrinsic dimensionality; that is,kopt = O(f )(2)To do a better interpolation, we need to bracket it in each dimension.
This leads to k = 2f .
We add a constant for handling noise, and that leads to the formula kopt = 2f + 1.
This is the technique we use.
The results seem to suggest that this provides pretty good prediction accuracy (better than cross-validation, and it is also faster).
It must still be mentioned that this is only a heuristic, which works well in practice.
Finally, this system depends on making predictions on the basis of past regularities in the data.
Thus, the data must be stored in a form making such similarity searches fast and efficient.
Storage of and similarity searches over delay coordinate vectors leads to several problems.
One is the problem of dimensionality curse: the larger the dimension of the delay coordinate vector, the more acute the problem.
Another problem is that of fast retrieval from the database.
We have already implemented a time series similarity search prototype, which uses the R-Tree spatial data access method ( [14]).
The dimensionality curse is tackled by the use of feature extraction, DFT ( [9]), DWT, segmented means ( [32,16]) and such methods.The other point of concern is the speed and scalability of the preprocessing step.
We show in Section 4.2 that this step is indeed fast.
We give order-of-magnitude computations, as well as experimental wall-clock times to prove this.
A related question is determining the correct method for interpolating between the predictions given by the chosen k nearest neighbors.
We tried out several interpolation methods: one uses plain averaging over the k predictions, another weights the predictions by the exponential of the negative of the distance squared (here, distance refers to the Euclidean distance between the current delay vector and the neighboring delay vector).
We settled on an SVD-based interpolation method, detailed in [25], where the least-squares fitting line through the nearest neighbor predictions is used as an approximation of the true predictor function.
The projection of the point-of-prediction on to this line gives the value of the prediction.Thus, the main aim of our work was to find formal methods for choosing values for lag length (Lopt) and number of nearest neighbors used for interpolation (kopt).
The success of the work can be judged by comparing the results produced by using these parameters against those when the parameters are chosen on an ad hoc basis.
We have compared them on several datasets, the results of which will be described in the next section.
We compared out algorithm to several parameter settings based on computation time required and prediction accuracy.
We used several datasets, both real and synthetic, on which to test our algorithm.
These were:• Synthetic (but realistic):-Lorenz equations (LORENZ): N = 20, 000 points.The equations are:˙ x = σ(y − x) (3) ˙ y = −xz + rx − y (4) ˙ z = xy − bz (5)where σ, b and r are constants.
These equations have been used to model convection currents.
• Real-world:-Laser Fluctuations (LASER): N = 10, 000 points, this is Time Series A from the Santa Fe competition(1992) Figure 4 gives snapshots of these datasets.
Several other datasets were also tested, but are not detailed due to space considerations; results in all cases were similar.
The following subsections will describe each of the datasets and the results we obtained on them.For prediction accuracy, we used the typical measure called Normalized Mean Squared Error [2].
N M SE = 1 σ 2 N N i=1 (xi − ˆ xi) 2 (6)where xi is the true value of the ith point in the series of length N , ˆ xi is the predicted value, and σ is the standard deviation of the true time series during the prediction interval.
The lower this value, the better the algorithm.We did experiments to answer the following questions:• how good (that is, accurate) are the proposed choices of Lopt and kopt• how fast is the preprocessing• how fast is the forecasting The LORENZ dataset is a synthetic dataset which models convection currents.
Results from the LORENZ dataset are shown in Figure 5.
The observations are:1.
The plot is seen to flatten out at a lag length of five.
This verifies Takens' theorem, and the mathematical justification for our technique.2.
NMSE was also computed for each lag length, and we see that plot 5(b) has its minimum at lag length of four.
This is close to the value that our approach gives.
The difference in NMSEs for these two lags is also not much different.3.
We also show a sample 150-step-ahead prediction sequence.
The prediction accuracy is excellent.4.
We ran AR with the same window size as out Lopt, that is, 5.
Our method is seen to clearly outperform the AR(5) model.
This is a dataset of fluctuations measured during the operation of a Laser.
It is more widely known as "Data Set A" from the Santa Fe competition [2].
We used the first 6000 points as the training set.
Our results on a particular stretch of the dataset are shown in Figure 6.
The observations are:1.
The FDL plot flattens out at around L = 7, which is chosen as Lopt.2.
The NMSE versus Lag plot shows that this choice is one of the smallest values of L which give good accuracy.3.
We also show a sample 100-step-ahead prediction sequence, where we predict using the AR model, using a window size of 7(= Lopt).
Our prediction is clearly better.
For the experiment, we generated the LORENZ dataset for different lengths of time.
Figure 7 (plots a and b) give the experimental results.
It can be seen that the preprocessing time varies almost linearly with the size of the training set N , and quadratically with L.
This verifies the assertions in Section 3.3.
Here we plot forecasting time versus database size.
Our method is found to clearly outperform sequential scan, without suffering from dimensionality curse problems.
The same experimental setting as before is used in this.
Figure 7 (plot c) gives the results when forecasting time is compared with database size (that is, the length of the time series).
It can be seen that the system easily outperforms plain Sequential Search.
The observations to note are:• F4 takes approximately constant time (about 1.5 seconds), which is interactive.
• F4 does not suffer from dimensionality curse problems, thanks to our careful design (using segmented means and R-Trees).
Work on time series can be broadly classified into three categories: Linear Prediction, Non-linear Prediction and Time-Series-Indexing related work.
There exist a large number of linear models, such as AR, MA, ARIMA, ARFIMA, and so on.
The basic ideas are illustrated here in the case of the ARMA(M,N) model, where the equation for xt is given by:xt = M m=1 amxt−m + N n=0 bnet−n(7)where the ams lead to internal dynamics, and the bns are weighing factors for external inputs et.
It has been widely used in all areas of time series analysis and discrete-time signal processing.
Reference texts for this are [7,22].
The downside of linear models is that they fail in the presence of non-linearities in the underlying process.
We have provided an intuitive reason for this with regard to the Logistic Parabola dataset ( figure 1(c)).
One nonlinear prediction method is the Delay Coordinate Embedding method, which has been described in Section 2.
Another approach has been through the use of Artificial Neural Networks [30,31,19,29,28].
Neural networks use the same idea: find a function f which gives the best fit for xt = f (xt−1, . . . , ft−w).
The only difference is in the method for estimating the function f .
A taxonomy of neural network structures for time-series processing is provided in [20].
But all these methods suffer from the traditional problems associated with neural networks: long training times and large number of parameters.
Hidden Markov Models(HMMs) ( [23]) have also been used ( [10,11]).
There is a similarity between these models and our technique, but the forward-backward algorithm for finding the parameters in such models is O(N 2 ) and hence not scalable to large datasets.
Also our approach seems to lead to a more natural problem description.Another method is called the Method of Analogues ( [18]).
This approach is simple and has few free parameters, but works only for low-dimensional chaotic attractors, and only for predictions over a short period of time.
Time sequences have been studied from several points of view, other than prediction.
[13] and [24] are some recent surveys on time series similarity and indexing.
One problem of particular relevance to the data-mining and database communities has been the storage and indexing of large time series datasets for supporting fast similarity search.
Similarity search is an important aspect of our technique too, and hence is relevant for us.
Since the sequences to be stored are typically n-dimensional data, spatial databases such as R-Trees( [14]) and variants have been used.
But such structures cannot efficiently handle very high-dimensional data.
Hence, techniques have been sought to reduce dimensionality: [17] used Singular Value Decomposition (SVD) to do this.
In [32], the authors use an approach called "Segmented Means" in which the time series is subdivided in equal-sized windows, and the means of the data in those windows are stored.
The authors give techniques to get carry out exact k-nearest-neighbor and range queries in such situations.
A similar technique is detailed in [16].
In [6], the authors attempt to find local correlations in the data, as opposed to global correlations, and then perform dimensionality reduction on the locally correlated clusters of data individually.
In a similar vein, [15] try to fit a set of constant value segments of varying lengths to the series, thus achieving data compression.Another aspect of time series research has been the problem of finding rules/patterns from the series.
One approach by [8] was to form subsequences of the data, which were clustered, and then rule finding methods were used to obtain rules from the sequence of clusters.
We focussed on building a fast and completely automated non-linear forecasting system.
The major contributions of this work are the following:• The automatic estimation of vital forecasting parameters, namely of the optimal lag Lopt and a good guess for the number of neighbors kopt.
The novelty is the use of 'fractal dimensions' for this purpose.
• Scalability: 'F4' is the first scalable, 'black-box' forecasting method, capable of handling arbitrarily large datasets, and with fast forecasting algorithms.
• Experiments on real and synthetic datasets, that show that our method is not only fast and scalable, but also accurate, achieving low prediction error.Additional, smaller contributions include the following:• The modularization of the forecasting problem: the 'F4' system ( Figure 3) can trivially use better/newer access methods, as they come along, although the chosen R-tree and 'segmented means' perform very well.
It can also trivially accommodate newer/better interpolation methods, once the k nearest neighbors have been retrieved.
