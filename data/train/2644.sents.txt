Modern cloud infrastructures are geo-distributed.
Geo-distribution offers many advantages but can increase the total cloud capacity required.
To achieve low la-tency, geo-distribution forfeits statistical multiplexing of demand that a single data center could benefit from.
Geo-distribution also complicates software design due to storage consistency issues.
On the other hand, geo-distribution can lower costs through eliminating redundancies at individual sites or exploiting regional differences in energy prices.
We discuss several factors that influence geo-distributed capacity provisioning, and quantify latency, availability, and capacity trade-offs that emerge.
We describe open research challenges in designing software that efficiently uses cloud capacity.
Most large online services are geo-distributed.
Cloud infrastructures make it easy for any application to become geo-distributed.
Many techniques have been developed for geo-distributed application design such as distributed failure detection and request re-routing [41,48,7,38,8], geo-distributed storage consistency management [36,46,15,16,14,12,47,31], data placement [1,49,43], and distributed consensus [11,28,2].
However, geo-distribution complicates allocation of compute resources compared to building a single large data center.
In this paper we explore multiple factors that affect geo-distributed capacity provisioning and discuss open challenges in realizing efficient resource allocation.Capacity implications of geo-distribution: Arguably, the most compelling reason to go geo-distributed is latency.
Geo-distribution creates a point of presence close to clients, thereby reducing latency.
Latency is critical for interactive online applications, and directly impacts user experience and revenue [21,30,9,22].
Latency expectations for cloud services are becoming more stringent due to increasing interactive features in cloud services such as auto-complete of search queries, cloud hosted mobile games, and growth in cloud hosted versions of traditionally client-based applications such as content editing.
Trends such as the use of edge data centers and hosting VMs at cellular base stations [44] will only lead to greater geo-distribution.
Latency gains come at the cost of potentially larger total data center capacity compared to a single very large data center.
Consider the demand over time seen at six data center locations, where each is serving the clients nearest to itself (Figure 1).
The demand is based on measured server utilization at one data center location hosting Microsoft's production applications spanning tens of thousands of servers.
It is re-played in the local timezone for each data center, and the peak magnitude is scaled in proportion to the population size [10] served by that location.
The peak of the sum of these demands (single data center capacity) is smaller than the sum of the peaks of these demands (capacity required for geodistributed data centers).
Table 1 shows the increase in capacity compared to a single data center, assuming the geo-distributed data centers are located at the publicly available US data center locations of Google [26], Amazon [6] Figure 1: Population based demand projection at 6 of Microsoft data center locations in the US.zones [4] at a single site cannot: earthquakes, civil unrest, or severe weather issues [23,39,3,20].
Even at a given level of availability, geo-distribution offers new design options by trading off local redundancy costs for spare capacity [29,19].
Availability gains also come at the cost of extra spare capacity added to make up for capacity lost in a failure.
If all data centers were equally sized and served equal demand, then 6 data centers would require an excess of 1/6 of the total capacity as spare, spread across data centers remaining after a failure.
This leads to a total of 20% excess spare capacity.
The actual excess will be higher because not all data centers are equally sized, and failure of a larger data center necessitates larger spare at remaining ones.A third reason to use geo-distribution is to exploit regional differences in energy prices or availability of renewable energy [45,33,42,35,34,17,18].
But to shut down servers when energy is expensive or unavailable from green sources at one location, we would have to add additional servers at another location.
If the entire workload is to be moved, this can lead to 100% excess.Problem: We see that geo-distribution leads to excess capacity.
Capacity is expensive, and its cost needs to be weighed against the advantages of lower latency, higher availability, and energy savings.
Our goal in this paper is to determine the lowest excess capacity required.
Determining the optimal capacity is non-trivial.
The spare created due to latency and availability reasons is not disjoint and some of it may overlap, depending on latency requirements after a failure.
While current server and network costs may be too high to justify buying excess capacity for reduced energy price, green incentives, or customer goodwill, the excess capacity already available due to latency and availability objectives could be used for that purpose.
Capacity allocation also depends on factors such as reduced the cost per unit capacity due to reduced local availability requirements [29] and revenue from sale of excess capacity on the spot market [5].
Optimized capacity allocation also impacts software design because existing geo-distribution methods do not use capacity efficiently.
For instance, routing client requests to the next-nearest data center after a failure may not allocate clients in the right proportion from a capacity provisioning perspective.
Distributed storage methods may need to account for capacity provisioning decisions when determining placement of data replicas.
We discuss such open challenges in making geo-distributed software more efficient in terms of capacity used.
We want to understand the minimum capacity required to realize the advantages of geo-distribution.
Consider n geo-distributed data centers, located based on proximity to clients and availability of power, network, land or other resources.
The clients are spread across m locations where each location could be a small geographic unit within which network latency variation is insignificant.
Response time consists of the service time within the data center and the network latency outside the data center.
Assuming that the service time (including queuing delay) is the same regardless of which data center is used, we focus on network latency.
Let l i j represent the latency when data center at location j serves a client at location i. Suppose client demand over time is known based on history, denoted by {d i (t)} for clients at location i in time slot t, where t ∈ {1, ...., T } and T is a period over which demand repeats (e.g., T=24 hours if demand has a diurnal pattern).
We wish to determine the capacity c j for the data center at location j, such that there is sufficient capacity to serve all of the client demand within a latency target, l, when no data centers have failed and within latency l (l ≥ l) when up to one data center has failed, while minimizing total cloud capacity.
We assume each data center is large enough to benefit from economies of scale applicable to multi-megawatt data centers and hence do not model any extra fixed costs aside from the capacity.The solution also requires specifying how clients are allocated to different data centers.
Let f i jk (t) denote the demand originating from client i that will be serviced by data center j when the k th data center has failed, during time slot t. Here, k = 0 represents no data center failed, and k = {1, ..., n} represents one of the n data centers having failed.Objective: The optimization objective thus becomes:min ∑ j∈{1,...,n} c jConstraints: At any given time, demand from a client location should be serviced by data centers that are within the latency target.
Before failure, this can be expressed mathematically as:∑ j:l i j ≤l f i j0 (t) ≥ d i (t), ∀i,t.(1)where the summation represents the total demand from client i allocated to data centers that are within a network latency of l from this client location.
After failure, the constraint is similar, except that the failed data center is not used in the summation and the latency constraint could be less stringent.
∑ j: j =k,l i j ≤l f i jk (t) ≥ d i (t), k > 0, ∀i,t.(2)The capacity at each data center should be sufficient to serve all of the demand allocated to it in any time slot.
Before failure, this implies:c j ≥ ∑ i f i j0 (t) ∀ j,t,(3)After each possible failure, this implies:c j ≥ ∑ i f i jk (t) k > 0, k = j, ∀ j,t.(4)The above optimization problem is a linear program (LP) and can be solved using any LP solver.We use the above optimization to explore some of the interesting latency, availability and capacity trade-offs.
Latency and Availability: As latency constraints become tighter, we expect greater spare capacity to be required, since more capacity would have to be placed closer to each client, leading to lower multiplexing of demand across client locations.
Figure 2 shows the excess capacity required, compared to a single very large data center, for different post-failure latency constraints with up to one data center failure.
The pre-failure latency (l) is set to the latency achieved when each client is mapped to the nearest data center, and is different for each set of data center locations.
Interestingly, the excess capacities required jointly for latency and availability for each set of locations are similar to the excess capacity present anyway to reduce latency, shown in Table 1.
For Amazon locations, increasing l to above 1.1l allows more data centers to be within range giving a large reduction in excess capacity at that latency.
Prior works have shown that tighter latency constraints at a given availability led to an increase in the number of locations but not in total capacity [19], because they ignored the statistical multiplexing of demand across locations over time.
This is significant since the increase in capacity can be leveraged for other uses such as higher availability and energy cost reduction.Energy Savings Opportunity: The geo-distributed system has idle capacity at any given time since the demand varies over time, both at each data center location and also globally.
Figure 3 shows the idle capacity at different times of the day summed over Microsoft's data center locations.
This capacity is available for use on the spot market for cloud resources [5], or for shifting load latency tolerant load to a location with lower energy price or greater renewable availability.
The load shifting opportunity depnds on how the spare is distributed across multiple sites.
Software Impact: Figure 4 shows the excess capacity required without exploiting time of day effects, i.e., assuming demand is always at its peak at each location.
Significant excess capacity results.
However, using efficient capacity complicates the allocation of client traffic.
If peak based capacity is provisioned, the client demand allocations to different data centers stay constant over time.
Re-allocation is required only in case of a failure.
But when time of day effects are exploited, client demand requires re-allocation even without failures, as demands in different regions change.
Figure 5 shows the optimal demand allocations at three different times of the day for Pennsylvania clients.
We see that the demand shifts between Virginia and Iowa data centers.
One of the mechanisms used by geo-load balancers to allocate clients to a data center is to set different DNS entries for the geo-distributed application.
In this method, DNS entries may have to be changed over 3 Open Challenges Optimization: The optimization above can be expanded to consider additional factors.
Data center construction takes time and demand curves over time are often not available for the time horizon for which capacity is built.
Errors in demand projection need to be accounted for.
While in today's clouds more than one data center failing at the same time is highly unlikely, multiple failures may become more practically relevant when the number of geo-distributed data centers grows much larger.
The capacity required for a given demand can also be optimized by running servers at higher utilization and increasing the queuing delays inside the data center, when tolerable.
The optimization could thus obtain a more efficient solution when considering queuing delays and network latencies jointly.
Heterogeneity and Right-Sizing: One size may not fit all applications, because multiple cloud applications share the infrastructure.
A key opportunity is to allow finer grained control of latency and availability objectives.
Different applications may have different latency requirements, or even multiple latency objectives for different percentiles of the client population.Availability requirements also vary across applications depending on factors such as whether an application faces external customers or is for internal use.
This opens up opportunities to also consider heterogeneous data center infrastructures, where some portion of the capacity is highly resilient and provisioned with power backups, redundant network connectivity, and highly reliable hardware, but other portions of the infrastructure economize on redundancy, environmental controls, or hardware quality.
Cost and availability are not linearly related and not all points in the trade-off space may be practical to build given available technology.
A few classes of such data centers are likely to suffice to meet a wide range of requirements but determining the most appropriate cost-reliability trade-offs is an open problem.Spatio-temporal variations: Failures do not necessarily happen when demand is at its peak.
Since the peak only happens for a small portion of the time, we could provision for meeting availability objectives only for a high percentile of the time and reduce overall capacity.
Failures and their impact may also be time dependent.
Failures may be more likely in winter months.
A failure during non-work hours may take longer to recover since the right staff may not be on-site.
Not all data centers may be equally likely to fail.
Spatial differences such as varying susceptibility to severe weather, nearness to earthquake zones, network backbone redundancy in the region can make certain locations much more reliable than others.
Exploiting these variations offers an opportunity to further optimize both the total capacity provisioned and its distribution across sites.
While nearness to clients may force us to place data centers in a less reliable region, the backup capacity may be preferrentially placed at more reliable sites.
For the infrastructure to be efficient, it is essential that software be designed to use it efficiently.
Several opportunities exist to improve geo-distributed software design.Request Routing: Existing geo-load balancing solutions are largely designed to route client traffic to the nearest available data center.
They may rely on maintaining Internet latency maps and set DNS entries in every regions to point to the nearest data center, or they may rely on IP anycast [40] to automatically find the route from the client to the nearest available application endpoint.
As we saw, the nearest data center may not lead to the most efficient capacity allocation.
Especially after a failure, routing to the nearest data center will not make efficient use of available capacity.
Compute capacity allocation information is typically not something that the network can easily access or make use of even if available.
Application layer intervention will likely be needed to ensure that clients are routed to the correct data center, either through logic hosted in edge data centers or even at the client as part of the application.
If coordination be-tween the application and cloud infrastructure is needed to realize the correct demand allocations, new APIs to facilitate that will be needed.
Cost structures that guide applications to behave efficiently are also important in this context.State Replication: Many geo-replication systems save a copy of the state at a remote site [12].
If the objective of the replication is only to improve data durability and protect it against data destruction at the primary site, replicating to any remote site suffices.
However, if the data will be actively used by the application from the replica site, it is essential to make sure that the replicas exist at the same site where the compute capacity will be allocated after a failure.
This allocation may depend on time of day since availability of spare capacity changes with temporal variations in demand.
If the data center fails at an off peak hour, the nearest data center to it may be able to serve all of its demand.
However, if the failure happens at peak demand time, the nearest data center may itself be at its peak demand, and many clients from the region may have to be routed to multiple different data centers.
At the very least, geo-replication solutions need to take input from the capacity provisioning system regarding where clients will be re-directed after a failure.
Since we do not know when the failure will happen, statically determining replication sites will likely require significantly conservative copying to too many possible backup sites.
On the other hand, backing up to just one site may mean that large amounts of data has to be migrated after a failure.
A good solution will carefully trade-off the network costs of data replication, increased consistency protocol overheads due to increased number of copies, support for concurrent access at multiple locations for shared data, the higher storage space requirement, and post-failure migration penalty.
It may even turn out that for data intensive applications with high network usage, minimizing network usage at the cost of increased excess capacity nearby is more cost effective.
Since many applications have user specific data, placement of replicas will also influence which users are to be re-directed to which data center and this can require coordination with the request routing mechanism.
Determining the best APIs to share the required information is an interesting research problem.Auto-scaling: Cloud applications can auto-scale the compute capacity they are using in response to client demand.
As long as demand follows expected patterns, the provisioned capacity at each data center would allow for this scaling.
However, if an application sees an unexpected demand spike, and the local data center is unable to satisfy the scaling needs, auto-scaling can re-direct this demand to other data centers and scale its number of instances across multiple data centers.
While existing auto-scaling algorithms only tune up (or down) the number of compute instances, leaving the storage layer scaling to the cloud provider, a global auto-scaling algorithm will have to consider additional factors aside from compute utilization, such as which data centers have the required data replicas and if additional data needs to be migrated to enable a scaling event.Virtualized Availability: The cloud infrastructure as constructed may be capable of meeting certain latency and availability targets if an application is geo-distributed to all the cloud locations.
However, not all applications may need that same latency and availability.
Based on actual needs, an application may determine which and how many of the cloud data centers it spreads itself across.
Using fewer data centers may not yield the lowest latency but can help reduce data replication complexity and networking costs.
Reduced spare capacity, or longer post-failure latency constraints can be used if the application can tolerate lower performance after a failure.
A good design can exploit the flexibility offered by geo-distribution to virtualize the cloud in the availability dimension, allowing applications to pay for what they need.
Generic capacity and placement planning for geodistributed facilities has been studied in depth in the context of the facility location problem [13,27].
Specifically for data centers, both placement and capacity optimizations have been considered in [19,24,25,32].
For instance, the work in [19] used sophisticated cost models for a geo-distributed cloud infrastructure to show that a larger number of locations provides lower latency, and that for a given availability target, a larger number of cheaper (reduced redundancy) data centers are more cost effective.
However, prior works have not considered the excess capacity created due to temporal variations in demand across sites and how this capacity can be leveraged to achieve higher availability for no additional cost.
Prior works [37] have turned servers off temporarily by exploiting multiple data center locations within a latency bound from a client but these works do not minimize the actual capacity built at each of those data centers, and do not determine the excess capacity required for availability reasons.
Also, we model availability in terms of supporting a data center failure, rather than assuming hard numbers quantifying the probability of failure.
Such numbers are typically only available for the power infrastructure, designated as tiers or number of 9s, and do not account for other causes of data center failures observed in practice such as electrical storms, software issues and operator errors [39,3,20].
