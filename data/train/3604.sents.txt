The application of deep learning models presents significant improvement to many Microsoft services and products.
In this paper, we introduce our experience and methodology of developing and applying the DeepCPU library for serving DL models in production at large scale with remarkable latency improvement and infrastructure cost reduction.
We describe two ways to use the library, through customized optimization or framework integration, targeting different scenarios.
Deep learning (DL) sits at the core of many essential products and services at Microsoft, such as web question and answering, web relevance ranking, advertising, language modeling, text translation, and conversational bot [2,3,4,5,7,8].
Many of these services are deployed in large scale, supporting millions of users and billions of requests.Such large scale DL inference faces threefold challenges to deploy a trained DL model at production.
First, users expect to receive an inference result with low latency.
The serving system needs to provide adequate quality of service, expressed as latency SLA (service level agreement), which is often a few milliseconds [11].
In practice, DL models are computationally expensive, incurring long latency, e.g., ranging from hundreds of milliseconds to seconds, that blocks their deployment [12,23].
Second, when the volume of requests exceeds the capacity of a single server, the DL service must scale horizontally.
An efficient serving system reduces the required replications and save thousands of machines and millions of cost.
Finally, these constraints come together with restriction on the deployment infrastructure.
In particular, it is strongly preferable to use existing commodity hardware, one main reason being the easier maintenance of the infrastructure and the agility of deployment.To tackle these challenges, we foremost rely on a large amount of CPUs for serving DL models and adopt a codevelopment methodology called SLT (scenario, library, and technique) to make the best use of the CPU resource for business critical scenarios while accelerating the iteration cycle of deployment and optimization.
In this paper, we present the SLT methodology and how it leads to DeepCPU, a DL inference library, which is deployed in production for many services on thousands of servers, and is tailored for DL scenarios with large number of users.
We show two ways of applying DeepCPU, either through customized end-to-end optimized DL serving solution or low-level interface integration into frameworks such as TensorFlow [9] and ONNX [6].
Our Table 1: DL scenarios and corresponding models.evaluation on production models demonstrates the ability of the DeepCPU library that addresses latency SLA violation problem on a single server and also improves the throughput so that the DL service scales horizontally.
This section highlights the SLT methodology.
Section 2.1 describes DL inference scenarios that are of interest in our production.
Section 2.2 introduces what DeepCPU library is.
Section 2.3 shows our performance optimization techniques.
We start the SLT methodology with a bird's-eye view of some major Microsoft scenarios that leverage DL models from the standpoint of latency SLA and resource consumption.
Table 2: Optimization results with and without DeepCPU on production models.
For similarity ranking models, cost is often a big concern.
Ranking model A takes 10ms to serve a query with batch size 1 on a single server, whereas the latency SLA is 5ms for batch size 150.
This is not scalable because even a fan-out solution requires thousands of machines to serve the large volume of request traffic.
[10,13].
Fundamental building blocks and common DL layers includes matrix-multiply kernels, high-way network [20], max pooling layer [16], Conv layer [15], MLP layer [18], etc.
DL layers for machine reading comprehension and conversation models includes variety of attention layers [14,19], seq2seq decoding with beam search [21], etc.
We build DeepCPU, a library of these components as building blocks with customized optimization.
We find that these components are highly reusable and allow faster implementation and decreased development cost to support new scenarios.
As an example, it takes < 200 lines of C++ code for running a Seq2Seq model end-to-end with the library.
Not only we support the library, but we also offer optimization techniques to optimize different components.
We perform three large categories of optimizations: Intra-op optimizations.
We provide i) more efficient matrix computation by combining Intel MKL [1] with customized cache-aware kernel computation to handle, large matrix computation, as well as small or tall-and-skinny matrixmultiplication.
ii) optimized common activation functions using continued fraction expansion [22], efficient parallelization, and SIMD vectorization.
Inter-op optimizations.
We perform operation fusion which fuses point-wise operation to avoid multiple scans of data and reduced data movement overhead.
Parallelism, scheduling, and affinity.
The parallelism, load balancing, and scheduling order are also critical to the performance of DL optimization on multicore CPU.
Existing frameworks such as TensorFlow are designed to handle generic DAG, which can lead to suboptimal parallelism decisions and cannot control per-op parallelism, while we consider the characteristics of the workload and perform global optimization by looking at model structure.
We also pin application threads to physical cores and make DL computation NUMA-aware and socket-aware to avoid expensive context switching and cross-socket communication overhead.
DeepCPU is currently released as C++ SDK to first party users.
There are two approaches to use the library.Customized optimization.
This approach requires rewriting the model runtime using the DeepCPU library.
After then we tune the performance such as thread settings, targeting at obtaining the ultimately optimized performance, because at large scale, every possible bit of hardware optimization space leads to major improvements.
This approach requires interaction with the model developer and requires some development efforts if the model changes drastically.
To achieve improved performance with less development work, we also integrate DeepCPU into existing DL frameworks.Framework integration.
We replace frequently used and costly operators, such as LSTM, GRU, Conv2D, Attention, with DeepCPU 's high-performance implementations in TensorFlow runtime.
This approach targets framework users directly, and it allows users to use existing frameworks to develop models while taking only a minimal amount of work to switch the operators to take the advantage of DeepCPU.
Meanwhile, we are closely working with ONNX team to power ONNX runtime [6] with DeepCPU technology, which allows frameworks that support ONNX IR, such as PyTorch [17], to also benefit from DeepCPU.For new scenarios and models, we often encourage to try the framework integration approach first, which allows fast deployment if that already gives satisfying performance results (e.g., meeting latency SLA).
Otherwise, we apply customized optimization to further boost the performance.
Table 2 shows a list of models we have optimized with Deep-CPU, with both latency and throughput improvement in comparison with TensorFlow on a server with two 2.20 GHz Intel Xeon E5-2650 V4 processors, each of which has 12-core with 128GB RAM.
Overall, we see 5-20 times latency improvement, which helps to change the model status from non-shippable to shippable.
Meanwhile, we have achieved up to 100 times throughput improvement and cost reduction.
These models have been running in production for the last two years on thousands of servers.
