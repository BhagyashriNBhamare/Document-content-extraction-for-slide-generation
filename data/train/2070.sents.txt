A large class of IoT applications read sensors, execute application logic, and actuate actuators.
However, the lack of high-level programming abstractions compromises correctness especially in presence of failures and unwanted interleaving between applications.
A key problem arises when operations on IoT devices or the application itself fails, which leads to inconsistencies between the physical state and application state, breaking application semantics and causing undesired consequences.
Transactions are a well-established abstraction for correctness, but assume properties that are absent in an IoT context.
In this paper, we study one such environment, smart home, and establish inconsistencies manifesting out of failures.
We propose an abstraction called transactuation that empowers developers to build reliable applications.
Our runtime, Relacs, implements the abstraction atop a real smart-home platform.
We evaluate programmability, performance, and effectiveness of transactuations to demonstrate its potential as a powerful abstraction and execution model.
Building reliable IoT applications that interact with the physical world on top of existing solutions is difficult.
Current IoT solutions (e.g., Smartthings [14] and OpenHAB [12]) provide simple abstractions that allow developers to easily read sensors and actuate actuators.
However, they lack high-level abstractions for writing reliable and fault-tolerant applications that can tolerate different types of failures that might happen.
Therefore, application programmers need to implement tedious and error-prone code for not only handling all kinds of failures happening in the physical world, but also to guarantee consistency between operations on application states (called soft states hereafter) and states of IoT devices (called hard states).
For instance, an actuation to turn on an alarm might * Work done at Samsung Research America.
Now at Microsoft Research.
† Work done at Samsung Research America.
Now at Uber Technologies.
‡ Work done at Samsung Research America.fail while the alarm state in an application might have been set to true.The use of serverless functions as a de facto platform for running IoT applications has exacerbated the reliability issues of these applications even further.
This is because serverless computing infrastructure can terminate running applications at any point [2].
This again leaves incomplete operations on some hard states (e.g., lock all doors) inconsistent with an operation on soft state inside the application (e.g., set the home state to safe after all doors are locked).
Transactions seem like the right mechanism for addressing the above issues.
Interestingly though, a transactional abstraction cannot fix these issues because of intrinsic properties of IoT devices (and their associated hard states).
A transactional abstraction is ideal for ensuring isolation and all-or-nothing guarantees among soft states.
Moreover, a transactional system can easily rollback soft states without other transactions or users noticing effects of a rolled back transaction.
However, rolling back a hard state has consequences.
The state might have already been observed by a user and rolling it back may be undesirable.
Or even worse, some states cannot be rolled back (e.g., undoing actuation of a water dispenser).
This paper proposes an abstraction called transactuation.
Transactuations hide the complexity of handling various failures and allow developers to easily maintain soft states to be consistent with respect to reads and writes to hard states -states of sensors and actuators.
Objectively, transactuations allow a developer to specify dependencies among operations on soft and hard states along with a sensing/actuating policy which specifies the conditions under which soft states can commit despite failures.We provide a runtime system called Relacs that implements the abstraction for the smart home environment.
Relacs transforms an application into a serverless function, and reliably executes the application in the cloud while enforcing transactuation specific semantics.
We note that while the focus of this paper is on smart homes, the transactuation abstraction is not particularly specific to smart homes, and can be applied to other IoT environments as well.Concretely, this paper has the following contributions: 1.
Study of smart-home applications.
Using static analysis, we conduct a comprehensive study of smart-home applications written for two popular platforms [12,14] and identify drawbacks of existing platforms in writing reliable and faulttolerant applications (Section 3).2.
Transactuations.
We present our abstraction that allows developers to simply write reliable IoT applications.
Transactuations preserve the dependencies between operations on hard states and soft states, which when broken, break application semantics (Section 4).3.
Relacs.
Our runtime, Relacs, enforces a serializable execution of transactuations without rolling back hard states (i.e., states of actuators) while enforcing the specified sensing and actuating policies (Section 4 and Section 5).4.
Evaluation.
We evaluate representative smart-home applications to reveal the correctness issues due to lack of appropriate abstractions.
Our evaluation further demonstrates that (a) Transactuations are an effective high-level abstraction for building reliable IoT applications and reduce lines of code significantly compared to manually handling failures.
(b) Relacs guarantees reliable execution of transactuations while imposing reasonable overheads over a baseline that does not provide consistency between operations on hard states and soft states (Section 6).
In this section, we first review existing smart-home platforms and their programming models.
We focus on smart homes as a case study of class of IoT environments that deal with real world state since many smart home applications and platforms are publicly available.
We then discuss different types of failures that occur in IoT environments.
To setup a smart home, a user installs centralized gateways, called smart-home hubs or simply hubs, to connect in-home devices (e.g., light bulbs, outlet strip, and motion sensor) that typically communicate through low-energy wireless protocols (e.g., Zigbee [17], ZWave [16], and Bluetooth Low Energy [4]).
The user then installs smart-home applications to create her desired home automation.
For instance, to turn on a balcony light when motion is detected outside.Currently, cloud-centric smart-home solutions (e.g., Smartthings [14]) are the most widely used architecture [28].
In this model, a hub is only responsible for collecting device events, and forwarding them to the cloud, where applications run.
The applications running in the cloud then process events and send actuation commands back to the hub, which forwards the commands to corresponding devices.
An alternative architecture is to run applications inside hubs.
OpenHAB [12] follows this hub-centric approach.
1 preferences { 2 input( sensor , " capa .
co2 " , req :true) 3 input( switches , " capa .
switch " , multi :true) 4 input( level , " number " , req :true) 5 } 6 def initialize () { 7 state .
active = false; 8 subscribe( sensor , " co2 " , handleLevel ) 9 } 10 def handleLevel ( evt ) { 11 def co2 = sensor .
currentValue (" co2 "); 12if ( Listing 1: CO 2 vent application that turns exhaust fans on when CO 2 level is high and turns off otherwise.
In most smart-home platforms, an application is written in a trigger-action programming model [45] where an application comprises event handlers.
Handlers can subscribe to changes in sensor/actuator states, updates to shared states, or timerbased events.
Handlers can issue the following operations:• Hard read: reading sensor/actuator values.
• Hard write: sending actuation commands to actuators.
• Soft read: reading application states from shared storage.
• Soft write: writing application states to shared storage.
In the remainder of this section, and for simplicity, we solely detail SmartThings [14] programming model.
Yet, we note that other platforms have very similar constructs.SmartThings uses capabilities, attributes, and commands to manage devices.
Each device has one or more capabilities, and each capability has one or more associated attributes and commands.
For example, a smart light bulb has two capabilities, switch and color.
The switch capability allows an application to control the bulb status via on/off commands.
The color capability has three attributes, color, hue and saturation that can be controlled via setColor, setHue, and setSaturation commands.Listing 1 shows a SmartThings application, named CO 2 vent, written in the Groovy language [5].
It reads CO 2 level from sensors, and turns on an exhaust fan if the level is high.
Similarly, it turns off an exhaust fan if the level is low.
A developer first declares mapping of variable names to capabilities in the preference section (lines 1-5).
Consequently, a variable is mapped to an array of devices with the same capability.
For example, variable switches (line 3) gets mapped to an array of exhaust fans having the switch capability.A developer then subscribes event handlers to value changes of some capabilities or timer schedules.
In line 8, she subscribes an event handler called handleLevel to co2 capability.
Observe that inside the handler, she can perform hard read on sensor data (sensor.currentValue() in line 11) and soft read on shared states (reading state.active in line 12 and 15).
Also, the developer can issue hard writes to list of actuators (line 13 and 16).
She can also perform soft writes to application states (assignments to state.active in line 14 and 17).
Previous work [20,33] have shown a variety of failures in IoT environments.
For instance, hubs can fail due to plug disconnection, hardware failure, and driver crash.
IoT devices can fail due to battery drainage, plug disconnection, and failure in a sensor subsystem.
Additionally, network loss occurs due to RF interference, concrete slab flooring and copper siding.
These failures lead to permanent or intermittent unavailability of devices in an IoT environment.Although, these failures are common, existing platforms do not provide a simple way to detect and handle them.
A failed hard read can produce a null or stale value that a developer needs to handle or explicitly validate its timestamp (freshness).
Detecting a failed hard write is even more difficult due to the asynchronous nature of IoT programming model.
For instance, a developer needs to subscribe to an event triggered by a hard write, and periodically check if the event is fired.
As shown in other systems [30,35,37], inserting failure detection and handling code for asynchronous environments is challenging and error prone.
Moreover, due to inherent eventdriven concurrency in applications, it is notoriously difficult to prevent interleaving and concurrency-related bugs in IoT platforms [40].
Existing smart-home solutions do not guarantee any consistency between soft reads/writes (i.e., reads/writes from/to shared storage) and reads/writes to hard states (i.e., sensor reads and actuation commands sent to actuators) in case of failures.
Application developers need to carry the burden and ensure the correctness of an application when a failure occurs.In this section, we present a systematic study of open source smart-home applications, using static analysis, in order to unearth various inconsistencies, that surface under failure, between operations on soft and hard states.
Listing 2 shows a simplified code excerpt from a smart security application.
This application associates a soft state named alarmActive with the status of an alarm.
If the application detects an intruder when the alarm is not active, it activates the alarm and sets alarmActive to true.
However, an inconsistency arises if the alarm is not activated properly.
For example, RF interference may cause an actuation command1 def intruderMotion ( evt ) { 2 ... 3 if ( isIntruder ( evt ) && !
state .
alarmActive ) { 4alarm .
strobe () ; 5 state .
alarmActive = true; 6 } 7... 8 } Listing 2: A simplified code excerpt from Smart Security application that detects an intruder using sensors, and activates an alarm if it has not been activated previously.to be lost.
This problem is so common that some brands (e.g., Fortrezz [15]) give warnings regarding RF interference, and explicitly ask consumers to not use the alarm in life supporting situations.
Observe that even though alarmActive is set, the states of the physical world and application have diverged.
Further, if the sensors detect the intruder again, the application will not retry to activate the alarm because as per the application's state the alarm is ringing.
Clearly, the developer does not anticipate such a failure, and this divergence is irreversible without manual intervention.
Such inconsistencies cause changes in application semantics and compromise correctness, and may severely affect smart-home users.
Moreover, stale hard reads may also break correctness of an application.
For example, recent CO 2 level events might never get delivered to the CO 2 vent application in Listing 1.
By reading a stale CO 2 level, the application may incorrectly turn off the exhaust fans.Besides device failures, similar issues arise if an application crashes.
For instance, an inconsistency arises if the smart security application fails between sending a command to set the alarm (line 4) and setting the active state to true (line 5).
Finally, applications may modify shared soft and hard states concurrently [40] which can cause canonical interleaving based inconsistencies [39].
As an example, the following quote from a disgruntled SmartThings customer [9] who got robbed during his vacation shows the impact of the inconsistency problem: "More importantly, we were robbed when we were out on vacation.
... The logs show the motion of the robbers, but it never sounded the alarm ... I no longer trust it to do what it is supposed to do when it is supposed to do."
In the previous section, we showed connections between hard states and soft states that are potential sources of inconsistencies due to hard read/write failure.
We call these connections between two operations on hard states or two operations between soft and hard states that are semantically associated, a dependency.
By identifying dependencies in an application, we can study the effects of failures on its correctness.In order to systematically analyze smart-home applications, and understand how failures can affect them, we categorize dependencies into four classes, using the following notations: we represent a hard read to device D as HR D , and denote a hard write to device D with value V as HW D (V).
A soft read from application state X is denoted as SR X , and a soft write to state X with value V is represented as SW X (V).1.
HR D → HW D (V): a dependency in this category captures the effect of a failure in a HR D .
The read might fail to return any value if device D is unavailable, or it might return a stale value.
In either case, it implies that the application may exercise the dependency incorrectly, thus breaking its semantic.
Such a dependency in an application can be because of a control dependence [27] or a data dependence.If the dependency is a control-dependence [27], the value of HR D controls the execution of HW D (V).
For example, in Listing 1, the dependency between lines 12 and 13, and also between lines 15 and 16 are control dependences.
The hard read in line 11 flows into the control statements in lines 12 and 15.
Therefore, a stale read at line 12 might incorrectly switch off the exhaust fans, and update the soft state even though the CO 2 levels are unsafe.
A read value can also flow into a hard write via data dependencies.
For example:a = HR D 1 ; c = foo(a, b); HW D 2 (c).2.
HR D → SW X (V): this dependency affects the execution of a soft write.
Analogous to HR D → HW D (V), this results in a missing soft write or an incorrect soft write, because of control and data dependences.
In turn, the incorrect soft write leads to unexpected program behavior when the state is read elsewhere.
In our running example, this dependency exists between lines 12 and 14, and also lines 15 and 17.3.
HW D (V) → SW X (V ): a hard write to soft write dependency is more subtle since SW X (V ) is not a control or a data dependence on a HW D (V).
Nevertheless, we observe that semantically tying a soft state with a hard state -meaning the soft state is an indicator of the hard state -is a common practice in many smart-home applications.
Developers use this technique mainly to save battery: by associating a soft state with an actuation, developers can use the soft state elsewhere in the code instead of reading hard states.For example, in the CO 2 Vent application, the developer implicitly creates a HW switches (ON) → SW active (true) dependency between lines 13 and 14, and also between lines 16 and 17.
Thus, a failure in turning on switches, even if temporary, leaves a permanent inconsistency.
Any subsequent change in the CO 2 level, even above the level, precludes turning on the exhaust fans.To find a HW → SW dependency in the code, we compute the postdominance relation [23]: a code point b postdominates a code point a, if b is executed on every path from a to the end of the analyzed entity, which in our case, is an event handler.
After computing postdominance instances, we manually look at all instances to confirm if the pair is semantically tied.
Accordingly, we infer a case for semantic error if the soft state is read elsewhere in the application.4.
SW X (V) → HW D (V ): this dependency has the samesemantic effect as HW D (V) → SW X (V ).
Note that all dependencies with soft reads (i.e., SR X → * ), are not directly related to device failures.
However, we still statically compute all such control and data dependences as an incorrect soft read can produce unintended behavior.
Concretely, a soft read can be on a state determined by an incorrect, inconsistent, or missing soft write originating from the dependencies described above.
We statically analyzed 147 SmartThings applications [19] and 35 OpenHAB applications chosen from IoTBench [10] by adding phases to the Groovy compiler.
The AST visitors, GroovyClassVisitor [8], allow us to build a call graph per entry point and an intermediate representation (IR) amenable to data and control-flow analysis.We analyzed the applications using inter-procedural data and control-flow analysis to understand the dependencies and their implications.
Our analysis yields two key benefits: (i) understand the implications and the extent of failures on a large set of smart-home applications, and (ii) mitigate or eliminate the problems with our programming abstraction, called transactuation.On average, the studied applications have three triggers, and manage a diverse set of devices (4-5 capabilities).
In order to get a holistic view of the home state, on average, the applications perform three hard reads.
They also perform between seven to nine hard writes on average.
This shows that many of these applications try to provide automation among a set of devices (e.g., turning on restroom light, preparing coffee, and playing music, when a user wakes up), instead of managing a single device.
Additionally, our analysis revealed that developers regularly use soft states to share states not only among handlers, but also among different applications.
These results indicate that smart-home applications are fairly complex, and their behavior could be complicated through the use of handlers triggered by events that read/write both hard and soft states.More specifically, we observed that, on average, applications have 3-10 instances of HR→HW, 1-2 instances of HR→SW and 1-2 instances of HW→SW dependencies.
We inspected these dependencies to find their potential implications on systems lacking appropriate abstractions to capture failures.
We categorized the implications as follows: (i) missing actuation, (ii) wrong actuation, (iii) inconsistent soft state, (iv) missing notification, and (v) wrong notification .
These implications can lead to unwanted outcomes, some of which have serious consequences such as security threats, health hazards, and missing critical alerts, e.g., a fire alarm not rung.
to space constraint, we only show a subset of them with unintended semantics and potential fixes in Table 1.
To address these implications, a developer needs to preserve the semantic invariants of the dependencies to avoid discrepancy between the physical and application realms.
One key trait of these applications is that their semantics tolerate different numbers of failed hard reads and writes.
For example, for HR→HW in the application that computes average humidity level and reacts accordingly, even if some hard reads are stale based on their timestamp (i.e., some humidity sensors fail), the application can proceed with correct semantics as long as some sensors function properly.
On the other hand, for HW→SW in the application that locks all doors and set the home state to safe, the developer needs to ensure that the home state is not set, even if only one door fails to be locked.
To summarize, the following two key aspects are missing in existing IoT abstractions: 1.
identifying the inherent connection between application semantics and number of failed operations, and 2.
recomputing application states to preserve invariants under failed hard reads/writes.
To address the issues discussed in the previous section, we introduce a new abstraction called transactuation that allows a developer to build a reliable smart-home application.
Transactuations provide the following two guarantees: (1) preserve dependencies between reads/writes to hard states and soft writes (i.e., HR→SW and HW→SW) even in cases of hardware and communication failures.
(2) ensure isolation among transactuations that execute concurrently.The concept of transactuations is very similar to database transactions.
Yet, due to the intrinsic nature of physical world, it is impossible to ensure similar transactional guarantees.
We note that transactuations are not meant to replace transactions completely.
Instead, they are designed to address a similar problem in a cyber-physical environment which inherently prevents us from making strong assumptions.
Precisely, transactuations and transactions differ as follows:1.
Atomic durability: atomic durability [36] guarantees that either all updates inside a transaction eventually become durable, or none of them becomes durable.
Since IoT devices can neither be locked nor rolled back (e.g., in case of some failures), transactuation cannot guarantee atomic durability of hard writes.
More specifically, unlike a transaction, a transactuation only guarantees atomic durability of soft writes but not hard writes inside it.
Thus, if a hard write fails, a transactuation still commits by forcing its soft states to be consistent with its hard states, as per developer specified policies (see Section 4.1).2.
Isolation & Atomic visibility: strong isolation models (e.g., serializability or snapshot isolation) requires a transaction to read a consistent snapshot of a system (e.g., the last committed state) and precludes a use of partially committed states.
A transactuation executes on the latest known consistent snapshot of the physical world, in isolation from other concurrent transactuations.
However, two concurrent transactuations can execute on different snapshots of the physical world in absence of any committing transactuation.
Additionally, (internal) atomic visibility ensures that effects of all updates in a transaction become visible to another transaction atomically [36].
Transactuations are also capable of guaranteeing internal atomic visibility: effects of a transactuation become atomically visible to other transactuation.
However, in a smart home domain, consumers will unavoidably observe the effect of a hard write operation the moment it gets executed in an actuator.
Thus, it is impossible to provide external atomic visibility.
For instance, one cannot expect that a smart-home user to observe all door locks become locked instantaneously.
Transactuations, further add to the definition of consistency based on consistency between hard reads/writes and soft writes.
Transactuations preserve two invariants as follows: (D1) A transactuation guarantees that if it executes, the staleness of its hard reads is bounded, as per the developer specified tolerance.
A developer leverages this invariant to ensure inconsistencies arising out of breaking HR→ * dependencies are detected, and appropriate actions are taken.
(D2) If writes to soft states are committed, it implies that sufficient number of hard writes as per developer specification have successfully executed.
A developer leverages this invariant to enforce consistency of HW→ SW dependencies.
Transactuations contain three pieces of logic which a developer writes as lambda expressions.
A lambda expression is a function that can be passed as an argument to another function [1,7,11].
In the rest of this paper, we refer to these lambda expressions as lambdas.
A transactuation can have the following three lambdas: perform lambda, onSuccess lambda, and onFailure lambda.
A developer cannot explicitly issue a hard read inside a perform lambda.
Instead, she has to specify a list of required hard states as an argument (i.e., sensorList) to perform() method.
The required hard states are read before perform lambda is executed, and a list of available hard states are accessible as key-value pairs to perform lambda, using sensors parameter of a perform lambda (line 5).
Disallowing explicit hard reads inside a transactuation prevents reading stale or null sensor values, which can break application semantics.To preserve consistency between hard reads and soft writes in case of a sensor unavailability, a developer can use a time window along with a sensing policy.
The time window specifies that the sensor list must be validated such that, after validation, the list of available sensors includes those that have received events close in time.
Specifically, a time window defines the duration when the transactuation triggering event and read hard states remain valid.
For instance, a window of 10 seconds has the following intent: a hard state passes validation if its most recent event and the transactuation triggering event are not more than 10 seconds apart.
A sensing policy is an acceptable level of hard-read failures that a transactuation can tolerate.
It specifies that under what condition a perform lambda can be executed over a returned list of window-validated sensors.
The perform lambda in turn may or may not execute depending on the sensing policy.
Transactuations support three sensing policies:• All: ensures that the perform lambda executes only if all hard states in the sensor list pass validation.
Consider an application that reads presence sensors of every user and turns on cameras if no one is present.
For privacy, all sensors need to pass validation.
If even one presence sensor fails, it should not risk turning on the cameras since it violates privacy.
• Any: guarantees the execution of the perform lambda as long as at least one hard state in the sensor list passes validation.
For example, an application that computes average humidity level from multiple sensors to control fans, executes accordingly with correct semantics, even if some sensors fail, but not all.
• None: states that the perform lambda executes over the returned validated list of hard states regardless of how many hard states are unavailable.Observe that a time window along with a sensing policy helps preserve HR→* dependency as per the developer's intention to preserve invariant (D1).
To preserve invariant (D2), a developer needs to specify an actuating policy.
The actuating policy is an acceptable level of hard-write failures that is tolerable.
To meet an actuating policy in case of a failure, soft writes inside a transactuation roll back to their initial values, and onFailure lambda executes.
Similar to a sensing policy, an actuating policy supports the following semantics:• All: states that modifications to soft states commit if all hard writes successfully finish.
An example of this policy is an application that locks all doors and sets home state to safe.
If even one door fails, the home state should not be set.
• Any: guarantees that soft state modifications inside a lambda commits if at least one hard write succeeds.
For example, an application that actuates all sirens and sets the flag ringing.
Even if only one siren rings, the flag should be set.
• None: states that soft writes commit despite of failures.onSuccess lambda.
An onSuccess lambda executes if the perform lambda of a transactuation succeeds (i.e., sensing and actuating policies are met).
A developer can assign an onSuccess lambda to a transactuation via onSuccess() as shown in line 17 of Listing 3.
onFailure lambda.
An onFailure lambda executes if a transactuation cannot meet its sensing or actuating policies.
It is assigned to a transactuation via onFailure() as depicted in line 25 of Listing 3.
When a developer has set up all the lambdas for a transactuation, she executes the transactuation by invoking execute() (line 29), which is an asynchronous call that executes the perform lambda in the background.Listing 3 illustrates the CO 2 Vent rewritten with the transactuation abstraction.
The perform lambda is parameterized with 5s time window.
The transactuation only reads one hard state, co2.
The lambda executes if the latest sensor update from co2, and the triggering event, which is also co2 fall in the 5 second time interval.
switches, which binds to an array of fans, requires the "all" policy if we want the soft writes to be consistent with the actuations.
The soft state active will be set to true only if all fans can be turned on, otherwise, active remains unchanged.
A transactuation can be chained to other transactuations by invoking it in their onSuccess and onFailure lambdas.
As we shall see in the next section, the runtime guarantees to execute chained transactuations sequentially: if a transactuation τ j is invoked in onSuccess lambda of τ i , τ j is guaranteed to see the updates τ i makes.
We call this ordered execution of transactuations as T-Chain.
This is particularly relevant in an asynchronous runtime where high latency operations can finish in arbitrary order, executing outside the critical path such as in worker threads [25,44].
Thus, if τ j wants to use a soft state written by τ i , τ j needs to be invoked in onSuccess lambda of τ i .
In addition, if τ j requires actuations of τ i to complete before it, these two transactuations must form a T-Chain.
In this section, we detail the design of our runtime, called Relacs, that execute smart-home applications, along with a supporting key-value store called Relacs Store.
All soft and hard states inside a transactuation are stored in a key-value store called Relacs Store.
It hides all complexities of working with sensors and actuators by allowing developers to not only perform read/write operations on soft states inside a transactuation, but also to issue hard reads/writes.
Conceptually, every state inside the Relacs Store maintains two values, speculative and final.
A speculative value means that the state has been updated logically in the Relacs Store, but is not confirmed to be final (i.e., issued to an IoT device).
For example, a transactuation that wants to unlock a door will have the speculative value of the door set to unlocked, before the actuation command succeeds.
When Relacs receives an ack event confirming the success of an actuation command, it updates the final value and discards the speculative value.
Along with setting the final value, the Relacs Store also logs the timestamp of the ack event for validating a time window of a transactuation reading that hard state.
In Section 5.2, we explain how speculative states help Relacs to speculatively execute transactuations.Since multiple hard writes on the same state can execute before the system receives an ack from the corresponding device, Relacs Store needs to record all versions of speculative values that have not been finalized yet.
When reading a state, Relacs Store returns the latest speculative value, or the final value if no speculative value exists.
For instance, consider the following transactuations: a transactuation τ i sets a lamp color to red.
While the lamp is changing its color, τ j changes the lamp color to green.
In this example, Relacs Store logs both speculative values.
Thus, if τ k tries to read the state of the lamp, Relacs Store returns green, even if the lamp has not completed executing the first actuation command to change its color to red.
A transactuation execution model comprises of the following three phases:1.
Hard read phase: to start executing a transactuation, the system first needs to determine if it can read the required hard states in the sensor list which satisfy the specified window and the sensing policy.
If so, the system proceeds to the next phase.
For a poll-based sensor, if Relacs fails to validate the window, it polls the sensor to check if it can get a fresh value.
For a push-based sensor, Relacs simply waits, as long as the window is valid, to receive an event from the sensor.
Observe that the window is valid as long as the specified time window has not passed since the transactuation triggering event.
If the window becomes invalid, and the list of received events fails staleness validation, it cannot execute the perform lambda, and proceeds to execute the onFailure lambda.2.
Speculative Commit Phase: since IoT devices cannot roll back, Relacs needs to make sure that a transactuation will definitely commit before performing real actuations.
Therefore, it employs a speculative execution model where a perform lambda first executes speculatively, without performing any real actuation.
Once the perform lambda finishes, it tries to speculatively commit like a normal transaction inside Relacs Store.
Therefore, new speculative values are committed for modified soft and hard states.
Additionally, committing new speculative values may trigger other handler functions subscribed to these states.
Finally, Relacs starts executing the onSuccess lambda of the transactuation when it commits.
Note that these lambdas triggered by speculative commit execute their transactuations speculatively.3.
Final Commit Phase: in the last phase, Relacs sends actuation commands that correspond to hard writes.
A transactuation τ i can start its final phase, when the following three conditions hold: first, all transactuations that precede τ i in the T-Chain finally commit.
Second, all transactuations updating states that τ i read, finally commit.
Third, no other finally committing τ j conflicts with τ i .
More specifically, the readset of τ i does not have any intersection with the writeset of some finally committing transactuation, and the writeset of τ i does not intersect with both readset and writeset of some finally committing transactuation.Relacs finally commits the transactuation when sufficient acks are received from actuators to satisfy its actuating policy.
If the transactuation times out without satisfying its actuating policy, all soft writes inside the transactuation roll back to their initial state, and the transactuation finally commits.
Next, onFailure lambda executes if it has been defined.
Moreover, all speculative transactuations invoked by the failed transactuation abort (e.g., chained transactuations), and transactuations that bear data dependencies with the failed transactuation need to re-execute.
Relacs is built atop serverless computing [32,42].
The runtime comprises two classes of functions namely application functions and system functions.
We explain these functions in detail here.Application Functions.
An application can comprise several handlers which are triggered when particular states in the Relacs Store change (publish-subscribe model), and each handler can comprise several transactuations.
An application submitted to run by Relacs system is transformed into a set of application functions to run on serverless instances as follows:1.
For each handler, Relacs transforms the logic of an embedded transactuation (i.e., perform lambda) into a transaction that can execute transactionally inside the Relacs Store.2.
The logic inside onSuccess lambda and onFailure lambda are transformed into stand-alone serverless functions called success and failure functions, respectively, hereafter.
If onSuccess lambda or onFailure lambda is comprised of transactuations with their own onSuccess lambda and onFailure lambda (T-Chain), the transformations are applied recursively.3.
Finally, every handler is transformed into a runnable stand-alone serverless function, called handler function.System Functions.
Relacs comprises a serverless function called updater function that is invoked whenever the state of a sensor or an actuator changes.
Upon receiving a notification, the updater updates the hard state corresponding to the event in Relacs Store, and launches an instance of subscribed handler function(s).
Final-committer is a designated function to perform the final commits.
It selects speculative transactuations that can finally commit without breaking the final commit rules, issues all of their actuation commands, and marks the actuations as issued.
When a successful actuation receives a notification (ack) from an IoT device, the updater function updates its corresponding state in Relacs Store, and marks the actuation command as done transactionally.In order to detect an actuation failure, Relacs has a failuredetector function that runs periodically, and checks whether an ack is received for an actuation command.
If after certain threshold no ack is received, the failure detector marks the actuation as failed.
If actuating policy is not met, the enclosing transactuation commits with rollback of soft writes, which triggers a re-executor function to re-execute transactuations that have data dependencies with the failed transactuation.
A function in serverless computing is not guaranteed to complete, and can terminate at any arbitrary point of execution.
Yet, Relacs guarantees applications to execute reliably despite failures as follows.Relacs ensures that all transactuations are executed exactlyonce even if an application function (handler, success, or failure) fails during its execution.
To this end, Relacs maintains two logs: function log and transactuation log.
Function log is a write-ahead log for application functions.
The function name along with ID of the triggering event is recorded in the function log before the function executes.
Transactuation log atomically records a transactuation name and the event ID during the speculative commit of a transactuation along with updates to soft/hard states.A system function called serverless checker runs periodically, and inspects the function log to execute functions which have failed.
In either case, the serverless checker invokes the failed functions again.
This might lead to duplicated executions of transactuations that have executed.
To prevent this, Relacs checks if a particular transactuation is in the transactuation log, and skips its execution if present.
1 Currently, the updater failure is treated as an equivalent of sensor or actuator failure and it is handled by transactuation semantics.
To address final committer failure, Relacs runs the final committer periodically to complete pending final commits by actuating unissued actuations.
To preclude contention between the periodic and the regular final committer that can run concurrently, Relacs uses leases and ETAGS à la Tuba [21] in the final committer to ensure correctness.
We implemented Relacs runtime and Relacs Store on top of Microsoft Azure.
We used Azure Function (serverless computing) to implement the runtime, and used Azure Cosmos DB to build Relacs Store.
All serverless functions were implemented with Azure Function.
Application functions are triggered by HTTP calls and system functions are triggered on Cosmos DB updates or periodic timers.
The parts of the protocol that need to update Relacs Store transactionally (including perform lambda) are transformed into Cosmos DB stored procedures [3].
Currently, Relacs has only been integrated with Samsung SmartThings.
SmartThings allows a developer to build a web service that connects with devices in a home [18].
We built a gateway that forwards actuation commands from Relacs to actuators and also polls sensor data.
As described, Relacs validates sensor failures through event timestamps and actuator failures through timeouts.
For sensor validation, as explained, if validation fails and a device is pollable, Relacs polls the device within the window constraints.
If a device is push-based but pollable, Relacs polls the device and if the validation fails again, it waits for its pushinterval within the time window.
However, if the device is purely push-based, Relacs cannot differentiate between inactivity and failure.
We inspected 188 SmartThings-compatible devices and found that 113 of them are pollable.
Likewise, actuation failures are detected with timeouts, first on initial ack from smart-home connector, followed by notification on final actuator state change.
Again, if the ack message is lost, Relacs can incorrectly rollback soft states.
However, transactuations can still help developers to prioritize home safety over convenience such as always setting a soft state to a conservative value; e.g., in Smart Security (Listing 2) to ensure that the alarm eventually rings.
In this section, we report our evaluation results on programmability, effectiveness of transactuations in enforcing correctness, and the overhead incurred by Relacs to provide transactuation semantics.We selected 10 SmartThings applications from the applications that we statically analyzed.
These applications are publicly available on SmartThings repository [19].
The applications cover the four most common categories-Security (Sc), Safety (Sf), Convenience (Cn), and Energy Efficiency (Ee).
Instead of using the original version that runs on SmartThings cloud, we implemented the following three versions of the applications, that run on Azure Functions, using Javascript Node JS [44].
This allows us to compare an application with transactuations against an application without transactuations in an apple-to-apple fashion.
• BE: we wrote a best-effort version (BE) of the applications without the transactuation abstraction.
The BE version follows the default semantics that ignores device failure, exactly-once execution, and isolation.
• BE+Con: since the BE version ignores potential failures in devices or applications, we implemented a best-effort with consistency (BE+Con) version of an application which adds code that keeps device states consistent with application states.
More specifically, BE+Con introduces both sensor window validation and soft state rollback code.
However, it ignores the isolation guarantee that transactuations provide.
• TN: we also implemented these applications with the transactuation abstraction (TN).
5 applications out of the evaluated 10 applications used T-Chain to establish order among hard and soft states.Experimental setup.
We set up SmartThings compatible devices and measured the round trip latency of four devices in a typical smart home: a door lock, a bulb, a power strip, and a smart power plug.
The door lock has a significant latency of nearly 3.6s on average and maximum of nearly 9.8s, over 100 trials.
The other devices incur an average latency of nearly 0.7s with the maximum at nearly 3.7s.
Since we had a limited set of devices, we parallelized our experiments by simulating the devices using latency data on a Raspberry Pi Model 3 [13].
It comes with a 1.2 GHz 32-bit quadcore ARM Cortex-A53 processor and 1 GB RAM.
In addition, the simulator also allowed us to easily inject failures for our experiments.
Table 2: Properties of each benchmark application including the number of hard reads and hard writes (* denotes an operation to an array of devices with a single command, for example, 2 (*) means 2 operations, each accessing a device group); the fault-tolerance policies for the TN configuration in a format of (sensing, actuating) (Col 4); and programability shown by LOC comparison among transactuation (TN), best effort (BE), and best effort with consistency (BE+Con) (Col 5).
In order to evaluate the programmability and convenience of using transactuation in contrast to manually writing failure handling code, we compare lines of code (LOC) of applications, using CLOC [6].
Table 2 shows the programmability evaluation (LOC) along with the number of hard reads and writes, and transactuation policies we employ for each application.
Observe that TN and BE versions are comparable in LOC despite no guarantees in the BE version, except in Ee1 where we introduce new soft states and four transactuations, each part of T-Chains, in order to ensure consistency.
BE+Con version requires substantial code to explicitly handle failures.
As mentioned earlier, BE+Con version validates sensor freshness similar to transactuation and may roll back soft states after determining the outcome of actuations for hard write to soft write dependencies.
Finally, although transactuations require more code in order to create T-Chains, it automatically handles failure, and simplifies writing reliable applications considerably.
Table 3 shows the applications that we evaluated with their inherent undesirable behaviors on transient or longer duration failures.
The second column shows the undesirable behaviors, and the third column shows the outcome of using transactuations.
The last column explains the mechanism transactuations use to resolve or mitigate the issue.
We considered different types of failures that transactuations can address (i.e., unavailable sensors and failed actuations), and injected these failures by dropping event or actuation messages.
Transactuation addresses these issues with three techniques.
First, sensor staleness validation prevents the execution of perform lambda and executes onFailure lambda that can notify a user.
Second, actuation losses are detected automatically and associated soft writes are rolled back to ensure consistency.
Third, when one actuation depends on another, we used an intermediate soft state to chain two transactuations each having actuations.
For example, in Sc3 (Smart Security) application, inconsistency between the alarm actuation and the soft write is resolved using roll back to eliminate the issue.
However, some applications need to use multiple chained transactuations to correctly address actuation dependencies.
To evaluate the overhead of transactuations, we measured execution time of the applications as follows.
We started timing when an application began executing, and stopped when every soft write committed and all actuations completed.
Our performance results are summarized in Figure 1.
Each value is the mean of 30 runs, with 95% confidence intervals.Failure-free.
We first compare the execution times of TN and BE versions without any injected failures.
The overhead of transactuations is attributed to (1) safeguarding against inconsistencies due to inherently concurrent execution, (2) providing fault tolerance, and (3) enforcing actuation orders of T-Chains.
We note that the final committer function imposes significant overhead on Relacs since it is invoked 2 automatically by CosmosDB updates.
For instance, we observed that its start may be delayed between zero to five seconds.
The periodic final committer which we set to run every second helps to mitigate this overhead.
Figure 1a shows that, on average (geomean), the TN version incurs 1.5 times slowdown compared to BE.
Observe that the Table 3: Applications with undesirable consequences on induced failures.
Column 3 shows failure avoidance or mitigation when written with transactuations.
Column 4 shows the internal mechanism used by the transactuations.
A checkmark implies that transactuation automatically resolves the issue.
With failure.
In this scenario, we conducted two experiments.
In each experiment, we used a dummy application that issued a dummy actuation, and updated a dummy soft state.
In the first experiment, the dummy actuation turned on a smart switch (low-latency actuation).
In the second one, it actuated a door lock (high-latency actuation).
We introduced an artificial data dependency (RAW) by forcing all benchmark applications to read the dummy soft state before executing their core logic.
Lastly, we injected a failure to the dummy actuation to trigger failure detection and handling in the dummy application and re-execution of the benchmark applications to repair the broken data dependency.
Because devices have different actuation latencies, the timeout thresholds to declare failed actuations are specific to each device.
More specifically, we used the maximum observed latency for each device (i.e., 4s for the smart switch and 10s for the door lock).
Figure 1b compares the execution time of the failure-free case against the two failure experiments.
The additional overhead we observe here is the failure detection overhead which includes the timeout (TN.FD.TO) and the overhead of triggering the re-executor function (TN.FD.TRIG).
Similar to the final committer, the re-executor is invoked automatically by Cosmos DB when actuations are marked as failed, thus it incurs similar overhead.
Observe that the failure experiments have two stacked bars of speculative commits.
The second bar shows the re-execution of transactuations with broken dependencies.As expected, introducing a failure results in longer execution times for the applications.
This slowdown is caused by the timeout threshold plus the re-executor triggering overhead (~2s).
Moreover, the difference between the middle and right bars for each application is the difference in timeout thresholds for low and high latency actuations (~6s).
Checking Correctness.
Soteria [22] employs model checking to identify contradicting interactions between IoT applications.
For example, water leak detection turns off a water valve while smoke detection attempts to turn on a fire sprinkler.
Prior work like DeLorean [24] models absolute and relative time to find timing bugs in event driven programs, e.g., door open at unsafe times.
In contrast, our work tackles a different problem, the lack of reliability and isolation, using a dynamic technique.
IoT analyses also use dynamic taint analyses like techniques to detect source of security breaches [46] and dynamic program slicing to explain behaviors [40].
We use static dependence analysis to report potential problems.Programming abstractions.
Using speculative execution for improving latency and performance is a common technique in many transactional and replicated systems.
These can be classified into two categories: systems [34,41,47] that hide the effects of speculation from applications, and work [29,31,43] that expose speculation results to applications.
While certain applications in the latter case can benefit by reading speculative values, they need to handle possible side effects of acting on misspeculated values.
With Relacs, effects of speculatively committed transactuations are exposed to other transactuations.
Yet, no transactuation can finally commit, and actuate devices until all transactuations that it speculatively read from finally commit.Planet [43] provides a mechanism to speculate on partial state of a transaction in distributed environments.
The abstraction allows a developer to continue based on a predictive outcome, and later receive a confirmation or an apology.
In contrast, we target a different environment and problem, and provide a simplified way to address device failure handling.Execution semantics and conflict detection.
IOTA [40] defines a calculus for programs in IoT domain.
They also define an execution semantics to eliminate races on actions against the same physical event.
Similar races can be resolved in our system by reordering transactuations according to programmer annotations similar to Zave et al. [48].
IOTA also shows offline analyses to detect device conflicts.
Conflict detection in a home can include static model checking [38] or dynamic analyses [48] to detect feature interactions [38] and accesses to the same device [26].
They detect commands due to single event or concurrent independent events to the same device, e.g., simultaneous turning on and off on a device.
The execution semantics of our system provides isolation naturally and can easily be enhanced to report device interactions by intersecting read-write sets of transactuations dynamically.
In this paper, we identified a fundamental problem that arises due to failures in IoT systems that interact with the physical world.
We analyzed smart-home applications, and showed how application semantics is broken due to different failures that occur in an IoT environment.
We introduced an abstraction, called transactuation, that allows a developer to build reliable IoT applications.
Our runtime, called Relacs, enforces the semantic guarantees of transactuations.
Our evaluation demonstrated programmability, performance, and effectiveness of the transactuation abstraction on top of our runtime.
We would like to thank our shepherd, Gernot Heiser and anonymous reviewers for their insightful and valuable feedback.
We would also like to thank Nitin Agrawal, Arani Bhattacharya, Juan Colmenares, Iqbal Mohomed, Marc Shapiro, Pierre Sutra, Ahmad Bisher Tarakji, and Ashish Vulimiri for their suggestions and helpful discussions.
