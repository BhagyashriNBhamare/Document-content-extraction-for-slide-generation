We study storage performance in over 450,000 disks and 4,000 SSDs over 87 days for an overall total of 857 million (disk) and 7 million (SSD) drive hours.
We find that storage performance instability is not uncommon: 0.2% of the time, a disk is more than 2x slower than its peer drives in the same RAID group (and 0.6% for SSD).
As a consequence, disk and SSD-based RAIDs experience at least one slow drive (i.e., storage tail) 1.5% and 2.2% of the time.
To understand the root causes, we correlate slowdowns with other metrics (workload I/O rate and size, drive event, age, and model).
Overall, we find that the primary cause of slowdowns are the internal characteristics and idiosyncrasies of modern disk and SSD drives.
We observe that storage tails can adversely impact RAID performance, motivating the design of tail-tolerant RAID.
To the best of our knowledge, this work is the most extensive documentation of storage performance instability in the field.
Storage, the home of Big Data, has grown enormously over the past decade [21].
This year Seagate projects to ship more than 240 exabytes of disk drives [20], SSD market has doubled in recent years [32], and data stored in the cloud has also multiplied almost exponentially every year [10].
In a world of continuous collection and analysis of Big Data, storage performance is critical for many applications.
Modern applications particularly demand low and predictable response times, giving rise to stringent performance SLOs such as "99.9% of all requests must be answered within 300ms" [15,48].
Performance instability that produces milliseconds of delay lead to violations of such SLOs, degrading user experience and impacting revenues negatively [11,35,44].
A growing body of literature studies the general problem of performance instability in large-scale systems, specifically calling out the impact of stragglers on tail latencies [7,13,14,34,45,50,52,54,56].
Stragglers often arise from contention for shared local resources (e.g., CPU, memory) and global resources (e.g., network switches, back-end storage), background daemons, scheduling, power limits and energy management, and many others.
These studies are mostly performed at server, network, or remote (cloud) storage levels.To date, we find no systematic, large-scale studies of performance instability in storage devices such as disks and SSDs.
Yet, mounting anecdotal evidence of disk and SSD performance instability in the field continue to appear in various forums ( §2).
Such ad-hoc information is unable to answer quantitatively key questions about drive performance instability, questions such as: How much slowdown do drives exhibit?
How often does slowdown occur?
How widespread is it?
Does slowdown have temporal behavior?
How long can slowdown persist?
What are the potential root causes?
What is the impact of tail latencies from slow drives to the RAID layer?
Answers to these questions could inform a wealth of storage systems research and design.To answer these questions, we have performed the largest empirical analysis of storage performance instability.
Collecting hourly performance logs from customer deployments of 458,482 disks and 4,069 SSDs spanning on average 87 day periods, we have amassed a dataset that covers 857 million hours of disk and 7 million hours of SSD field performance data.Uniquely, our data includes drive-RAID relationships, which allows us to compare the performance of each drive (D i ) to that of peer drives in the same RAID group (i = 1.
.
N ).
The RAID and file system architecture in our study ( §3.1) expects that the performance of every drive (specifically, hourly average latency L i ) is similar to peer drives in the same RAID group.Our primary metric, drive slowdown ratio (S i ), the fraction of a drive's latency (L i ) over the median latency of the RAID group (median(L 1.
.
N )), captures deviation from the assumption of homogeneous drive performance.
Assuming that most workloads are balanced across all the data drives, a normal drive should not be much slower than the other drives.
Therefore, we define "slow" (unstable) drive hour when S i ≥ 2 (and "stable" the otherwise).
Throughout the paper, we use 2x and occasionally 1.5x slowdown threshold to classify drives as slow.
(iii) Slowdown temporal behavior and extent ( §4.1.3, §4.1.4): We find that slowdown often persists; 40% and 35% of slow disks and SSDs respectively remain unstable for more than one hour.
Slowdown periods exhibit temporal locality; 90% of disk and 85% of SSD slowdowns occur on the same day of the previous occurrence.
Finally, slowdown is widespread in the drive population; our study shows 26% of disks and 29% of SSDs have experienced at least one slowdown occurrence.
(iv) Workload analysis ( §4.2): Drive slowdowns are often blamed on unbalanced workloads (e.g., a drive is busier than others).
Our findings refute this, showing that more than 95% of slowdown periods cannot be attributed to I/O size or rate imbalance.
(v) "The fault is (likely) in our drives": We find that older disks exhibit more slowdowns ( §4.3.2) and MLC flash drives exhibit more slowdowns than SLC drives ( §4.3.3).
Overall, evidence suggests that most slowdowns are caused by internal characteristics of modern disk and SSD drives.In summary, drive performance instability means the homogeneous performance assumption of traditional RAID is no longer accurate.
Drive slowdowns can appear at different times, persist, disappear, and recur again.
Their occurrence is "silent"-not accompanied by observable drive events ( §4.3.1).
Most importantly, workload imbalance is not a major root cause ( §4.2).
Replacing slow drives is not a popular solution ( §4.4.2- §4.4.3), mainly because slowdowns are often transient and drive replacement is expensive in terms of hardware and RAID rebuild costs.
(vi) The need for tail-tolerant RAID: All of the reasons above point out that file and RAID systems are now faced with more responsibilities.
Not only must they handle well-known faults such as latent sector errors and corruptions, now they must mask storage tail latencies as well.
Therefore, there is an opportunity to create "tail tolerant" RAID that can mask storage tail latencies online in deployment.In the following sections, we present further motivation ( §2), our methodology ( §3), the main results ( §4), an opportunity assessment of tail-tolerant RAID ( §5), discussion ( §6), related work ( §7) and conclusion ( §8).
Our work is highly motivated by the mounting anecdotes of performance instability at the drive level.
In the past several years, we have collected facts and anecdotal evidence of storage "limpware" [16,26] from literature, online forums supported by various storage companies, and conversations with large-scale datacenter operators as well as product teams.
We found many reports of storage performance problems due to various faults, complexities and idiosyncrasies of modern storage devices, as we briefly summarize below.Disk: Magnetic disk drives can experience performance faults from various root causes such as mechanical wearout (e.g., weak head [1]), sector re-reads due to media failures such as corruptions and sector errors [2], overheat from broken cooling fans [3], gunk spilling from actuator assembly and accumulating on disk head [4], firmware bugs [41], RAID controller defects [16,47], and vibration from bad disk drive packaging, missing screws, earthquakes, and constant "noise" in data centers [17,29].
All these problems can reduce disk bandwidth by 10-80% and increase latency by seconds.
While the problems above can be considered as performance "faults", current generation of disks begin to induce performance instability "by default" (e.g., with adaptive zoning and Shingled-Magnetic Recording technologies [5,18,33]).
The pressure to increase flash density translates to more internal SSD complexities that can induce performance instability.
For example, SSD garbage collection, a well-known culprit, can increase latency by a factor of 100 [13].
Programming MLC cells to different states (e.g., 0 vs. 3) may require different numbers of iterations due to different voltage thresholds [51].
The notion of "fast" and "slow" pages exists within an SSD; programming a slow page can be 5-8x slower compared toL i = S i = T 1Request latency (full stripe): programming fast page [23].
As device wears out, breakdown of gate oxide will allow charge moves across gate easily, resulting in faster programming (10-50%), but also higher chance of corruption [22].
ECC correction, read disturb, and read retry are also factors of instability [19].
Finally, SSD firmware bugs can cause significant performance faults (e.g., 300% bandwidth degradation in a Samsung firmware problem [49]).
Although the facts and anecdotes above are crucial, they do not provide empirical evidence that can guide the design of future storage systems.
The key questions we raised in the introduction (slowdown magnitude, frequency, scope, temporal behavior, root causes, impacts, etc.) are still left unanswered.
For this reason, we initiated this large-scale study.
In this section, we describe the RAID systems in our study ( §3.1), the dataset ( §3.2), and the metrics we use to investigate performance instability ( §3.3).
The overall methodology is illustrated in Figure 1.
RAID group: Figure 1 provides a simple illustration of a RAID group.
We study disk-and SSD-based RAID groups.
In each group, disk or SSD devices are directly attached to a proprietary RAID controller.
All the disk or SSD devices within a RAID group are homogeneous (same model, size, speed, etc.); deployment age can vary but most of them are the same.
RAID and file system design: The RAID layer splits each RAID request to per-drive I/Os.
The size of a per-drive I/O (a square block in Figure 1) can vary from 4 to 256 KB; the storage stack breaks large I/Os to smaller I/Os with a maximum size of the processor cache size.
Above the RAID layer runs a proprietary file system (not shown) that is highly tuned in a way that makes most of the RAID I/O requests cover the full stripe; most of the time the drives observe balanced workload.
The RAID systems in our study use small chunk sizes (e.g., 4 KB).
More than 95% of the RAID groups use a custom version of RAID-6 where the parity blocks are not rotated; the parity blocks live in two separate drives (P and Q drives as shown in Figure 1).
The other 4% use RAID-0 and 1% use RAID-4.
We only select RAID groups that have at least three data drives (D 1 .
.
D N where N ≥ 3 in Figure 1), mainly to allow us measure the relative slowdown compared to the median latency.
Our dataset contains RAID groups with 3-26 data drives per group.
Figure 2a shows the RAID width distribution (only data drives); wide RAID (e.g., more than 8 data drives) is popular.
Scale of dataset: A summary of our dataset is shown in Table 1.
Our dataset contains 38,029 disk and 572 SSD groups within deployment duration of 87 days on average (Figure 2b).
This gives us 72 and 1 million disk and SSD RAID hours to analyze respectively.
When broken down to individual drives, our dataset contains 458,482 disks and 4069 SSDs.
In total, we analyze 857 million and 7 million disk and SSD drive hours respectively.Data collection: The performance and event logs we analyze come from production systems at customer sites.
When the deployed RAID systems "call home", an autosupport system collects hourly performance metrics such as: average I/O latency, average latency per block, and number of I/Os and blocks received every hour.
All these metrics are collected at the RAID layer.
For each of Latency slowdown of Di compared to the median;Si = Li/L med T kThe k-th largest slowdown ("k-th longest tail"); Figure 2a.
Li, Si and T k are explained in Section 3.3.
T 1 = M ax of (S1.
.
N ), T 2 = 2nd M ax of (S1these metrics, the system separates read and write metrics.
In addition to performance information, the system also records drive events such as response timeout, drive not spinning, unplug/replug events.
Below, we first describe the metrics that are measured by the RAID systems and recorded in the auto-support system.
Then, we present the metrics that we derived for measuring tail latencies (slowdowns).
Some of the important metrics are summarized in Table 2.
Data drives (N ): This symbol represents the number of data drives in a RAID group.
Our study only includes data drives mainly because read operations only involve data drives in our RAID-6 with non-rotating parity.
Parity drives can be studied as well, but we leave that for future work.
In terms of write operations, the RAID small-write problem is negligible due to the file system optimizations ( §3.1).
Per-drive hourly average I/O latency (L i ): Of all the metrics available from the auto-support system, we at the end only use the hourly average I/O latency (L i ) observed by every data drive (D i ) in every RAID group (i=1.
.
N ), as illustrated in Figure 1.
We initially analyzed "throughput" metrics as well, but because the support system does not record per-IO throughput average, we cannot make an accurate throughput analysis based on hourly average I/O sizes and latencies.
Slowdown (S i ): To measure tail latencies, RAID is a perfect target because it allows us to measure the relative slowdown of a drive compared to the other drives in the same group.
Therefore, as illustrated in Figure 1, for every hour, we first measure the median group latency L med from L 1.
.
N and then measure the hourly slowdown of a drive (S i ) by comparing its latency with the median latency (L i /L med ).
The total number of S i is essentially the "#drive hours" in Table 1.
Our measurement of S i is reasonably accurate because most of the workload is balanced across the data drives and the average latencies (L i ) are based on per-drive I/Os whose size variance is small (see §3.1).
Stable vs. slow drive hours: Assuming that most workload is balanced across all the data drives, a "stable" drive should not be much slower than other drives.
Thus, we use a slowdown threshold of 2x to differentiate slow drive hours (S i ≥ 2) and stable hours (S i < 2).
We believe 2x slowdown threshold is tolerant enough, but conversations with several practitioners suggest that a conservative 1.5x threshold will also be interesting.
Thus, in some of our findings, we show additional results using 1.5x slowdown threshold.Conceptually, drives appear to behave similar to a simple Markov model in Figure 3.
In a given hour, a drive can be stable or slow.
In the next hour, the drive can stay in the same or transition to the other condition.Tails (T k ): For every hourly S 1.
.
N , we derive the k-th largest slowdown represented as T k .
In this study, we only record the three largest slowdowns (T 1 , T 2 and T 3 ).
T 1 represents the "longest tail" in a given RAID hour, as illustrated in Figure 1.
The total number of T 1 is the "#RAID hours" in Table 1.
The differences among T k values will provide hints to the potential benefits of tailtolerant RAID.Tail hours: A "tail hour" implies a RAID hour that observes T 1 ≥2 (i.e., the RAID group observes at least one slow drive in that hour).
This metric is important for fullstripe balanced workload where the performance will follow the longest tail (i.e., the entire RAID slows down at the rate of T 1 ).
From the above metrics, we can further measure other metrics such as slowdown intervals, extents, and repetitions.
Overall, we have performed an in-depth analysis of all the measured and derived metrics.
In many cases, due to space constraints, we aggregate some results whenever the sub-analysis does not show different behaviors.
For example, we merge read and write slowdowns as I/O slowdown.
In some graphs, we break down the slowdowns (e.g., to 2-4x, 4-8x, 8-16x) if their characterizations are different.
We now present the results of our study in four sets of analysis: slowdown and tail distributions and characteristics ( §4.1), correlations between slowdowns and workload-related metrics ( §4.2) and other available metrics ( §4.3), and post-slowdown analysis ( §4.4).
In this section, we present slowdown and tail distributions and their basic characteristics such as temporal behaviors and the extent of the problem.
We first take all S i values and plot their distribution as shown by the thick (blue) line in Figure 4 (steeper lines imply more stability).
Table 3 details some of the slowdown and percentile intersections.
Finding #1: Storage performance instability is not uncommon.
Figure 4 and Table 3b show that there exists 0.22% and 0.58% of drive hours (99.8 th and 99.4 th percentiles) where some disks and SSDs exhibit at least 2x slowdown (S i ≥ 2).
With a more conservative 1.5x slowdown threshold, the percentiles are 99.3 th and 98.7 th for disk and SSD respectively.
These observations imply that user demands of stable latencies at 99.9 th percentile [15,46,54] (or 99 th with 1.5x threshold) are not met by current storage devices.Disk and SSD slowdowns can be high in few cases.
Table 3a shows that at four and five nines, slowdowns reach ≥9x and ≥30x respectively.
In some of the worst cases, 3-and 4-digit disk slowdowns occurred in 2461 and 124 hours respectively, and 3-digit SSD slowdowns in 10 hours.
We next plot the distributions of the three longest tails (T 1−3 ) in Figure 4.
Storage tails appear at a significant rate.
The T 1 line in Figure 4 shows that there are 1.54% and 2.23% "tail hours" (i.e., RAID hours with at least one slow drive).
With a conservative 1.5x threshold, the percentiles are 95.4 th and 95.2 th for disk and SSD respectively.
These numbers are alarming for full-stripe workload because the whole RAID will appear to be slow if one drive is slow.
For such workload, stable latencies at 99 th percentile (or 96 th with 1.5x threshold) cannot be guaranteed by current RAID deployments.The differences between the three longest tails shed light on possible performance improvement from tailtolerant RAID.
If we reconstruct the late data from the slowest drive by reading from a parity drive, we can cut the longest tail.
This is under an assumption that drive slowdowns are independent and thus reading from the parity drive can be faster.
If two parity blocks are available (e.g., in RAID-6), then tail-tolerant RAID can read two parity blocks to cut the last two tails.Finding #3: Tail-tolerant RAID has a significant potential to increase performance stability.
The T 1 and T 2 values at x=2 in Figure 4a suggests the opportunity to reduce disk tail hours from 1.5% to 0.6% if the longest tail can be cut, and furthermore to 0.3% (T 3 ) if the two longest tails can be cut.
Similarly, Figure 4b shows that SSD tail hours can be reduced from 2.2% to 1.4%, and furthermore to 0.8% with tail-tolerant RAID.The T 1 line in Figure 4b shows several vertical steps (e.g., about 0.6% of T 1 values are exactly 2.0).
To understand this, we analyze S i values that are exactly 1.5x, 2.0x, and 3.0x.
We find that they account for 0.4% of the entire SSD hours and their corresponding hourly and median latencies (L i and L med ) are exactly multiples of 250 µs.
We are currently investigating this further with the product groups to understand why some of the deployed SSDs behave that way.
To study slowdown temporal behaviors, we first measure the slowdown interval (how many consecutive hours a slowdown persists).
Figure 5a plots the distribution of slowdown intervals.Finding #4: Slowdown can persist over several hours.
Figure 5a shows that 40% of slow disks do not go back to stable within the next hour (and 35% for SSD).
Furthermore, slowdown can also persist for a long time.
For example, 13% and 3% of slow disks and SSDs stay slow for 8 hours or more respectively.
Next, we measure the inter-arrival period of slowdown occurrences from the perspective of each slow drive.
Figure 5b shows the fraction of slowdown occurrences that arrive within X hours of the preceding slowdown; the arrival rates are binned by hour.Finding #5: Slowdown has a high temporal locality.
Figure 5b shows that 90% and 85% of disk and SSD slowdown occurrences from the same drive happen within the same day of the previous occurrence respectively.
The two findings above suggest that history-based tail mitigation strategies can be a fitting solution; a slowdown occurrence should be leveraged as a good indicator for the possibility of near-future slowdowns.
We now characterize the slowdown extent (i.e., fraction of drives that have experienced slowdowns) in two ways.
First, Figure 6a plots the fraction of all drives that have exhibited at least one occurrence of at least X-time slowdown ratio as plotted on the x-axis.
Finding #6: A large extent of drive population has experienced slowdowns at different rates.
Figure 6a depicts that 26% and 29% of disk and SSD drives have exhibited ≥2x slowdowns at least one time in their lifetimes respectively.
The fraction is also relatively significant for large slowdowns.
For example, 1.6% and 2.5% of disk and SSD populations have experienced ≥16x slowdowns at least one time.Next, we take only the population of slow drives (26% and 29% of the disk and SSD population) and plot the fraction of slow drives that has exhibited at least X slowdown occurrences, as shown in Figure 6b.
Finding #7: Few slow drives experience a large number of slowdown repetitions.
Figure 6b shows that that around 6% and 5% of slow disks and SSDs exhibit at least 100 slowdown occurrences respectively.
The majority of slow drives only incur few slowdown repetitions.
For example, 62% and 70% of slow disks and SSDs exhibit only less than 5 slowdown occurrences re- spectively.
We emphasize that frequency of slowdown occurrences above are only within the time duration of 87 days on average ( §3.2).
The previous section presents the basic characteristics of drive slowdowns.
We now explore the possible root causes, starting with workload analysis.
Drive slowdowns are often attributed to unbalanced workload (e.g., a drive is busier than other drives).
We had a hypothesis that such is not the case in our study due to the storage stack optimization ( §3.2).
To explore our hypothesis, we correlate slowdown with two workload-related metrics: size and rate imbalance.
We first measure the hourly I/O count for every drive (R i ), the median (R med ), and the rate imbalance (RI i = R i /R med ); this method is similar to the way we measure S i in Table 2.
If workload is to blame for slowdowns, then we should observe a high correlation between slowdown (S i ) and rate imbalance (R i ).
That is, slowdowns happen in conjunction with rate imbalance, for example, S i ≥ 2 happens during R i ≥ 2.
Figure 7a shows the rate imbalance distribution (RI i ) only within the population of slow drive hours.
A rate imbalance of X (on the x-axis) implies that the slow drive serves X times more I/Os.
The figure reveals that only 5% of slow drive hours happen when the drive receives 2x more I/Os than the peer drives.
95% of the slowdowns happen in the absence of rate imbalance (the rateimbalance distribution is mostly aligned at x=1).
To strengthen our conjecture that rate imbalance is not a factor, we perform a reverse analysis.
To recap, Figure 7a essentially shows how often slowdowns are caused by rate imbalance.
We now ask the reverse: how often does rate imbalance cause slowdowns?
The answer is shown in Figure 7b; it shows the slowdown distribution (S i ) only within the population of "overly" rate-imbalanced drive hours (RI i ≥ 2).
Interestingly, rate imbalance has negligible effect on slowdowns; only 1% and 5% of rateimbalanced disk and SSD hours experience slowdowns.
From these two analyses, we conclude that rate imbalance is not a major root cause of slowdown.
Next, we correlate slowdown with size imbalance.
Similar to the method above, we measure the hourly average I/O size for every drive (Z i ), the median (Z med ), and the size imbalance (ZI i = Z i /Z med ).
Figure 7c plots the size imbalance distribution (ZI i ) only within the population of slow drive hours.
A size imbalance of X implies that the slow drive serves X times larger I/O size.
The size-imbalance distribution is very much aligned at x=1.
Only 2.5% and 1.1% of slow disks and SSDs receive 2x larger I/O size than the peer drives in their group.
Reversely, Figure 7d shows that only 0.1% and 0.2% of size-imbalanced disk and SSD hours experience more than 2x slowdowns.Finding #8: Slowdowns are independent of I/O rate and size imbalance.
As elaborated above, the large majority of slowdown occurrences (more than 95%) cannot be attributed to workload (I/O size or rate) imbalance.
As workload imbalance is not a major root cause of slowdowns, we now attempt to find other possible root causes by correlating slowdowns with other metrics such as drive events, age, model and time of day.
Slowdown is often considered as a "silent" fault that needs to be monitored continuously.
Thus, we ask: are there any explicit events surfacing near slowdown occurrences?
To answer this, we collect drive events from our auto-support system.
drive events.
However, when specific drive events happen (specifically, "disk is not spinning" and "disk is not responding"), 90% of the cases lead to slowdown occurrences.
We rarely see storage timeouts (e.g., SCSI timeout) because timeout values are typically set coarsely (e.g., 60 seconds).
Since typical latency ranges from tens of microseconds to few milliseconds, a slowdown must be five orders of magnitude to hit a timeout.
Thus, to detect tail latencies, storage performance should be monitored continuously.
Next, we analyze if drive age matters to performance stability.
We break the the slowdown distribution (S i ) by different ages (i.e., how long the drive has been deployed) as shown in Figure 8.
For disks, the bold lines in Figure 8a clearly show that older disks experience more slowdowns.
Interestingly, the population of older disks is small in our dataset and yet we can easily observe slowdown prevalence within this small population (the population of 6-10 year-old disks ranges from 0.02-3% while 1-5 year-old disks ranges from 8-33%).
In the worst case, the 8th year, the 95 th percentile already reaches 2.3x slowdown.
The 9th year (0.11% of the population) seems to be an outlier.
Performance instability from disk aging due to mechanical wear-out is a possibility ( §2).
For SSD, we do not observe a clear pattern.
Although Figure 8b seemingly shows that young SSDs experience more slowdowns than older drives, it is hard to make such as a conclusion because of the small old-SSD population (3-4 year-old SSDs only make up 16% of the population while the 1-2 year-old is 83%).
Finding #10: Older disks tend to exhibit more slowdowns.
For SSDs, no high degree of correlation can be made between slowdown and drive age.
We now correlate slowdown with drive model.
Not all of our customers upload the model of the drives they use.
Only 70% and 86% of customer disks and SSDs have model information.
Thus, our analysis in this section is based on partial population.We begin by correlating SSD model and slowdown.
The SSD literature highlights the pressure to increase density, which leads to internal idiosyncrasies that can induce performance instability.
Thus, it is interesting to know the impact of different flash cell levels to SSD performance stability.Finding #11: SLC slightly outperforms MLC drives in terms of performance stability.
As shown in Figure 9, at 1.5x slowdown threshold, MLC drives only reaches 98.2 th percentile while SLC reaches 99.5 th percentile.
However, at 2x slowdown threshold, the distribution is only separated by 0.1%.
As MLC exhibits less performance stability than SLC, future comparisons with TLC drives will be interesting.Our dataset contains about 60:40 ratio of SLC vs. MLC drives.
All the SLC drives come from one vendor, but the MLC drives come from two vendors with 90:10 population ratio.
This allows us to compare vendors.Finding #12: SSD vendors seem to matter.
As shown by the two thin lines in Figure 9, one of the vendors (the 10% MLC population) has much less stability compared to the other one.
This is interesting because the instability is clearly observable even within a small population.
At 1.5x threshold, this vendor's MLC drives already reach 94.3 th percentile (out of the scope of Figure 9).
For disks, we use different model parameters such as storage capacity, RPM, and SAN interfaces (SATA, SAS, or Fibre Channel).
However, we do not see any strong correlation.
We also perform an analysis based on time of day to identify if night-time background jobs such as disk scrubbing cause slowdowns.
We find that slowdowns are uniformly distributed throughout the day and night.
We now perform a post-mortem analysis: what happens after slowdown occurrences?
We analyze this from two angles: RAID performance degradation and unplug/replug events.
A slow drive has the potential to degrade the performance of the entire RAID, especially for full-stripe workload common in the studied RAID systems ( §3.1), it is reasonable to make the following hypothesis: during the hour when a drive slows down, the RAID aggregate throughput will drop as the workload's intensity will be throttled by the slow drive.
Currently, we do not have access to throughput metrics at the file system or application levels, and even if we do, connecting metrics from different levels will not be trivial.
We leave cross-level analysis as future work, but meanwhile, given this constraint, we perform the following analysis to explore our hypothesis.
We derive a new metric, RIO (hourly RAID I/O count), which is the aggregate number of I/Os per hour from all the data drives in every RAID hour.
Then, we derive RIO degradation (RAID throughput degradation) as the ratio RIO lastHour to RIO currentHour .
If the degradation is larger than one, it means the RAID group serves less I/Os than the previous hour.Next, we distinguish stable-to-stable and stable-to-tail transitions.
Stable RAID hour means all the drives are stable (Si < 2).
Tail RAID hour implies at least one of the drives is slow.
In stable-to-stable transitions, RIO degradation can naturally happen as workload "cools down".
Thus, we first plot the distribution of stable-tostable RIO degradation, shown by the solid blue line in Figure 10.
We then select only the stable-to-tail transitions and plot the RIO degradation distribution, shown by the dashed red line in Figure 10.
Finding #13: A slow drive can significantly degrade the performance of the entire RAID.
Figure 10 depicts a big gap of RAID I/O degradation between stable-tostable and stable-to-tail transitions.
In SSD-based RAID, the degradation impact is quite severe.
Figure 10b example shows that only 12% of stable-to-stable transitions observe ≥2x RIO degradation (likely from workload cooling down).
However, in stable-to-slow transitions, there is 23% more chance (the vertical gap at x=2) that RIO degrades by more than 2x.
In disk-based RAID, RIO degradation is also felt with 7% more chance.
This finding shows the real possibilities of workload throughput being degraded and stable drives being under-utilized during tail hours, which again motivates the need for tailtolerant RAID.
We note that RAID degradation is felt more if user requests are casually dependent; RIO degradation only affects I/Os that are waiting for the completion of previous I/Os.
Furthermore, since our dataset is based on hourly average latencies, there is no sufficient information that shows every I/O is delayed at the same slowdown rate.
We believe these are the reasons why we do not see a complete collapse of RIO degradation.
When a drive slows down, the administrator might unplug the drive (e.g., for offline diagnosis) and later replug the drive.
Unplug/replug is a manual administrator's process, but such events are logged in our auto-support system.
To analyze unplug patterns, we define wait-hour as the number of hours between a slowdown occurrence and a subsequent unplug event; if a slowdown persists in consecutive hours, we only take the first slow hour.
Figures 11a-b show the wait-hour distribution within the population of slow disks and SSDs respectively.Finding #14: Unplug events are common.
Figures 11a-b show that within a day, around 4% and 8% of slow (≥2x) disks and SSDs are unplugged respectively.
For "mild" slowdowns (1.5-2x), the numbers are 3% and 6%.
Figure 11a also shows a pattern where disks with more severe slowdowns are unplugged at higher rates; this pattern does not show up in SSD.
We first would like to note that unplug is not the same as drive replacement; a replacement implies an unplug without replug.
With this, we raise two questions: What is the replug rate?
Do replugged drives exhibit further slowdowns?
To analyze the latter, we define recurhour as the number of hours between a replug event and the next slowdown occurrence.
Figures 11c-d show the recur-hour distribution within the population of slow disks and SSDs respectively.Finding #15: Replug rate is high and slowdowns still recur after replug events.
In our dataset, customers replug 89% and 100% of disks and SSDs that they unplugged respectively (not shown in figures).
Figures 11c-d answer the second question, showing that 18% and 35% of replugged disks and SSDs exhibit another slowdown within a day.
This finding points out that administrators are reluctant to completely replace slow drives, likely because slowdowns are transient (not all slowdowns appear in consecutive hours) and thus cannot be reproduced in offline diagnosis and furthermore the cost of drive replacement can be unnecessarily expensive.
Yet, as slowdown can recur, there is a need for online tail mitigation approaches.In terms of unplug-replug duration, 54% of unplugged disks are replugged within 2 hours and 90% within 10 hours.
For SSD, 61% are replugged within 2 hours and 97% within 10 hours.
It is now evident that storage performance instability at the drive level is not uncommon.
One of our major findings is the little correlation between performance instability and workload imbalance.
One major analysis challenge is the "silent" nature of slowdowns; they are not accompanied by explicit drive events, and therefore, pinpointing the root cause of each slowdown occurrence is still an open problem.
However, in terms of the overall findings, our conversations with product teams and vendors [4] confirm that many instances of drive performance faults are caused by drive anomalies; there are strong connections between our findings and some of the anecdotal evidence we gathered ( §2).
As RAID deployments can suffer from storage tails, we next discuss the concept of tail-tolerant RAID.
With drive performance instability, RAID performance is in jeopardy.
When a request is striped across many drives, the request cannot finish until all the individual I/Os complete ( Figure 1); the request latency will follow the tail latency.
As request throughput degrades, stable drives become under-utilized.
Tail-tolerant RAID is one solution to the problem and it brings two advantages.First, slow drives are masked.
This is a simple goal but crucial for several reasons: stringent SLOs require stability at high percentiles (e.g., 99% or even 99.9% [15,45,48,52]); slow drives, if not masked, can create cascades of performance failures to applications [16]; and drive slowdowns can falsely signal applications to back off, especially in systems that treat slowdowns as hints of overload [24].
Second, tail-tolerant RAID is a cheaper solution than drive replacements, especially in the context of transient slowdowns ( §4.1.3) and high replug rates by administrators ( §4.4.3).
Unnecessary replacements might be undesirable due to the hardware cost and the expensive RAID re-building process as as drive capacity increases.Given these advantages, we performed an opportunity assessment of tail-tolerant strategies at the RAID level.
We emphasize that the main focus of this paper is the large-scale analysis of storage tails; the initial exploration of tail-tolerant RAID in this section is only to assess the benefits of such an approach.
We explore three tail-tolerant strategies: reactive, proactive, and adaptive.
They are analogous to popular approaches in parallel distributed computing such as speculative execution [14] and hedging/cloning [6,13].
To mimic our RAID systems ( §3.2), we currently focus on tail tolerance for RAID-6 with non-rotating parity (Fig- ure 1 and §3.1).
We name our prototype ToleRAID, for simplicity of reference.Currently, we only focus on full-stripe read workload where ToleRAID can cut "read tails" in the following ways.
In normal reads, the two parity drives are unused (if no errors), and thus can be leveraged to mask up to two slow data drives.
For example, if one data drive is slow, ToleRAID can issue an extra read to one parity drive and rebuild the "late" data.Reactive: A simple strategy is reactive.
If a drive (or two) has not returned the data for ST x (slowdown threshold) longer than the median latency, reactive will perform an extra read (or two) to the parity drive(s).
Reactive strategy should be enabled by default in order to cut extremely long tails.
It is also good for mostly stable environment where slowdowns are rare.
A small ST will create more extra reads and a large ST will respond late to tails.
We set ST = 2 in our evaluation, which means we still need to wait for roughly an additional 1x median latency to complete the I/O (a total slowdown of 3x in our case).
While reactive strategies work well in cluster computing (e.g., speculative execution for mediumlong jobs), they can react too late for small I/O latencies (e.g., hundreds of microseconds).
Therefore, we explore proactive and adaptive approaches.Proactive: This approach performs extra reads to the parity drives concurrently with the original I/Os.
The number of extra reads can be one (P drive) or two (P and Q); we name them PROACTIVE 1 and PROACTIVE 2 respectively.
Proactive works well to cut short tails (near the slowdown threshold); as discussed above, reactive depends on ST and can be a little bit too late.
The downside of proactive strategy is the extra read traffic.Adaptive: This approach is a middle point between the two strategies above.
Adaptive by default runs the reactive approach.
When the reactive policy is triggered repeatedly for SR times (slowdown repeats) on the same drive, then ToleRAID becomes proactive until the slowdown of the offending drive is less than ST .
If two drives are persistently slow, then ToleRAID runs PROAC-TIVE 2 .
Adaptive is appropriate for instability that comes from persistent and periodic interferences such as background SSD GC, SMR log cleaning, or I/O contention from multi-tenancy.
Our user-level ToleRAID prototype stripes each RAID request into 4-KB chunks ( §3.2), merge consecutive perdrive chunks, and submit them as direct I/Os.
We insert a delay-injection layer that emulates I/O slowdowns.
Our prototype takes two inputs: block-level trace and slowdown distribution.
Below, we show ToleRAID results from running a block trace from Hadoop Wordcount benchmark, which contains mostly big reads.
We perform the experiments on 8-drive RAID running IBM 500GB SATA-600 7.2K disk drives.We use two slowdown distributions: (1) Rare distribution, which is uniformly sampled from our disk dataset (Figure 4a).
Here, tails (T 1 ) are rare (1.5%) but long tails exist (Table 3).
(2) Periodic distribution, based on our study of Amazon EC2 ephemeral SSDs (not shown due to space constraints).
In this study, we rent SSD nodes and found a case where one of the locally-attached SSDs periodically exhibited 5x read slowdowns that lasted for 3-6 minutes and repeated every 2-3 hours (2.3% instability period on average).
Figure 12 shows the pros and cons of the four policies using the two different distributions.
In all cases, PROAC We hope our work will spur a set of interesting future research directions for the larger storage community to address.
We discuss this in the context of performancelog analysis and tail mitigations.Enhanced data collection: The first limitation of our dataset is the hourly aggregation, preventing us from performing micro analysis.
Monitoring and capturing fine-grained data points will incur high computation and storage overhead.
However, during problematic periods, future monitoring systems should capture detailed data.
Our ToleRAID evaluation hints that realistic slowdown distributions are a crucial element in benchmarking tail-tolerant policies.
More distribution benchmarks are needed to shape the tail-tolerant RAID research area.
The second limitation is the absence of other metrics that can be linked to slowdowns (e.g., heat and vibration levels).
Similarly, future monitoring systems can include such metrics.Our current SSD dataset is two orders of magnitude smaller than the disk dataset.
As SSD becomes the front-end storage in datacenters, larger and longer studies of SSD performance instability is needed.
Similarly, denser SMR disk drives will replace old generation disks [5,18].
Performance studies of SSD-based and SMRbased RAID will be valuable, especially for understanding the ramifications of internal SSD garbage-collection and SMR cleaning to the overall RAID performance.Further analyses: Correlating slowdowns to latent sector errors, corruptions, drive failures (e.g., from SMART logs), and application performance would be interesting future work.
One challenge we had was that not all vendors consistently use SMART and report drive errors.
In this paper, we use median values to measure tail latencies and slowdowns similar to other work [13,34,52].
We do so because using median values will not hide the severity of long tails.
Using median is exaggerating if (N-1)/2 of the drives have significantly higher latencies than the rest; however, we did not observe such cases.
Finally, we mainly use 2x slowdown threshold, and occasionally show results from a more conservative 1.5x threshold.
Further analyses based on average latency values and different threshold levels are possible.Tail mitigations: We believe the design space of tailtolerant RAID is vast considering different forms of RAID (RAID-5/6/10, etc.), different types of erasure coding [38], various slowdown distributions in the field, and diverse user SLA expectations.
In our initial assessment, ToleRAID uses a black-box approach, but there are other opportunities to cut tails "at the source" with transparent interactions between devices and the RAID layer.
In special cases such as materials trapped between disk head and platter (which will be more prevalent in "slim" drives with low heights), the file system or RAID layer can inject random I/Os to "sweep" the dust off.
In summary, each root cause can be mitigated with specific strategies.
The process of identifying all possible root causes of performance instability should be continued for future mitigation designs.
Large-scale storage studies at the same scale as ours were conducted for analysis of whole-disk failures [37,40], latent sector errors [8,36], and sector corruptions [9].
Many of these studies were started based on anecdotal evidence of storage faults.
Today, as these studies had provided real empirical evidence, it is a common expectation that storage devices exhibit such faults.
Likewise, our study will provide the same significance of contribution, but in the context of performance faults.
Krevat et al. [33] demonstrate that disks are like "snowflakes" (same model can have 5-14% throughput variance); they only analyze throughput metrics on 70 drives with simple microbenchmarks.
To the best of our knowledge, our work is the first to conduct a large-scale performance instability analysis at the drive level.Storage performance variability is typically addressed in the context of storage QoS (e.g., mClock [25], PARDA [24], Pisces [42]) and more recently in cloud storage services (e.g., C3 [45], CosTLO [52]).
Other recent work reduces performance variability at the file system (e.g., Chopper [30]), I/O scheduler (e.g., split-level scheduling [55]), and SSD layers (e.g., Purity [12], Flash on Rails [43]).
Different than ours, these sets of work do not specifically target drive-level tail latencies.Finally, as mentioned before, reactive, proactive and adaptive tail-tolerant strategies are lessons learned from the distributed cluster computing (e.g., MapReduce [14], dolly [6], Mantri [7], KMN [50]) and distributed storage systems (e.g., Windows Azure Storage [31], RobuSTore [53]).
The applications of these high-level strategies in the context of RAID will significantly differ.
We have "transformed" anecdotes of storage performance instability into large-scale empirical evidence.
Our analysis so far is solely based on last generation drives (few years in deployment).
With trends in disk and SSD technology (e.g., SMR disks, TLC flash devices), the worst might be yet to come; performance instability can be more prevalent in the future, and our findings are perhaps just the beginning.
File and RAID systems are now faced with more responsibilities.
Not only must they handle known storage faults such as latent sector errors and corruptions [9,27,28,39], but also now they must mask drive tail latencies as well.
Lessons can be learned from the distributed computing community where a large body of work has been born since the issue of tail latencies became a spotlight a decade ago [14].
Similarly, we hope "the tail at store" will spur exciting new research directions within the storage community.
We thank Bianca Schroeder, our shepherd, and the anonymous reviewers for their tremendous feedback and comments.
We also would like to thank Lakshmi N. Bairavasundaram and Shiqin Yan for their initial help and Siva Jayasenan and Art Harkin for their managerial support.
This material is based upon work supported by NetApp, Inc. and the NSF (grant Nos.
CCF-1336580, CNS-1350499 and CNS-1526304).
