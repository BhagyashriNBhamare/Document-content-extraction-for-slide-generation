An accurate, tractable, analytic cache model for time-shared systems is presented, which estimates the overall cache miss-rate of a multiprocessing system with any cache size and time quanta.
The input to the model consists of the isolated miss-rate curves for each process, the time quanta for each of the executing processes, and the total cache size.
The output is the overall miss-rate.
Trace-driven simulations demonstrate that the estimated miss-rate is very accurate.
Since the model provides a fast and accurate way to estimate the effect of context switching, it is useful for both understanding the effect of context switching on caches and optimizing cache performance for time-shared systems.
A cache partitioning mechanism is also presented and is shown to improve the cache miss-rate up to 25% over the normal LRU replacement policy.
This paper presents an analytical model for the behavior of a cache in a multiprocessing system that can accurately estimate overall miss-rate for any cache size and any time quantum.
An evaluation method for miss-rate is essential to optimize cache performance.
Traditional cache performance evaluation is done by simulations [25,16,12], which provide accurate results, but simulation time is often long.
Hardware monitoring can dramatically speed up the process [26], however, it is limited to the particular cache configuration.
As a result, both simulations and hardware monitoring can only be used to evaluate the effect of context switches [14,10].
Moreover, simulations and monitoring rarely provide intuitive understanding making it difficult to improve cache performance.
To provide both performance prediction and insight into improving performance, analytical cache models are required.We use our model to determine the best cache partitioning so as to improve performance.
Partitioning is needed to mitigate the effects of conflicts among concurrently executing processes, especially for large caches.
In the past, caches were small and it was best to let each process consume the entire cache space, since process footprints were much larger than the cache.
In modern microprocessors, caches are much larger; some Level 1 (L1) caches range up to one MB [5], and L2 caches are up to several MB [2,13].
Large caches allow potential performance improvement by partitioning.
Since each process may not need the entire cache space, the effect of context switches can be mitigated by keeping useful data in the cache over context switches.
It is crucial for modern microprocessors to minimize inter-process conflicts by proper cache partitioning [21,9] or scheduling [17,23].
Our model requires information that is relatively easy to acquire.
The characteristics for each process are given by the miss-rate as a function of cache size when the process is isolated, which can be easily obtained either on-line or offline.
The time quantum for each process and cache size are also given as inputs to the model.
With this information, the model estimates the overall miss-rate for a given cache size when an arbitrary combination of processes is run.
The model provides good estimates for any cache size and any time quantum, and is easily applied to real problems since the input miss-rate curves are both intuitive and easy to obtain in practice.
Therefore, we believe that the model is useful for any study related to the effect of context switches on cache memory.After describing related research in Section 2, Section 3 derives an analytical cache model for time-shared systems.
Section 4 discusses cache partitioning based on the model and evaluates the model-based partitioning method by simulations.
Finally, Section 5 concludes the paper.
Several early investigations of the effects of context switches use analytical models.
Thiébaut and Stone [20] modeled the amount of additional misses caused by context switches for set-associative caches.
Agarwal, Horowitz and Hennessy [1] also included the effect of conflicts between processes in their analytical cache model and showed that inter-process conflicts are noticeable for a mid-range of cache sizes that are large enough to have a considerable number of conflicts but not large enough to hold all the working sets.
However, these models work only for long enough time quanta, and require information that is hard to collect on-line.
Mogul and Borg [14] studied the effect of context switches through trace-driven simulations.
Using a timesharing system simulator, their research shows that system calls, page faults, and a scheduler are the main sources of context switches.
They also evaluated the effect of context switches on cycles per instruction (CPI) as well as the cache miss-rate.
Depending on cache parameters, the cost of a context switch appears to be in the thousands of cycles, or tens to hundreds of microseconds in their simulations.Stone, Turek and Wolf [18] investigated the optimal allocation of cache memory between two competing processes that minimizes the overall miss-rate of a cache.
Their study focuses on the partitioning of instruction and data streams, which can be thought of as multitasking with a very short time quantum.
Their model for this case shows that the optimal allocation occurs at a point where the miss-rate derivatives of the competing processes are equal.
The LRU replacement policy appears to produce cache allocations very close to optimal for their examples.
They also describe a new replacement policy for longer time quanta that only increases cache allocation based on time remaining in the current time quantum and the marginal reduction in missrate due to an increase in cache allocation.
However, their policy simply assumes the probability for a evicted block to be accessed in the next time quantum as a constant, which is neither validated nor is it described how this probability is obtained.Thiébaut, Stone and Wolf applied their partitioning work [18] to improve disk cache hit-ratios [21].
The model for tightly interleaved streams is extended to be applicable for more than two processes.
They also describe the problems in applying the model in practice, such as approximating the miss-rate derivative, non-monotonic miss-rate derivatives, and updating the partition.
Trace-driven simulations for 32-MB disk caches show that the partitioning improves the relative hit-ratios in the range of 1% to 2% over the LRU policy.Our analytical model and partitioning differ from previous efforts that tend to focus on some specific cases of context switches.
Our model works for any specific time quanta, whereas the previous models focus only on long time quanta.
Also, our partitioning works for any time quanta, whereas Thiébaut's algorithms only works for very short time quanta.
Moreover, the inputs of our model (miss-rates) are much easier to obtain compared to footprints or the number of unique cache blocks that previous models require.
The analytical cache model estimates the overall cache missrate for a multi-processing system.
The cache size and the time quantum length for each job is known.
The cache size is given by the number of cache blocks, and the time quantum is given by the number of memory references.
Both are assumed to be constants (See Figure 1 (a)).
In addition, associated with each job is its miss-rate curve, i.e., the number of cache misses as a function of the cache size.This section explains the development of the model in several steps.
Heavy use is made of the individual, isolated miss-rate curve (iimr).
This curve is the miss-rate for a pro- cess as a function of cache size assuming no other processes are running.
There is much information that can be gleaned from this equation.
For example, we can compute the miss rate of a process as a function of time (Section 3.2.1) from the miss-rate of a process as a function of space.Observe that as a process executes, it either references an item in the cache, in which case its footprint size remains the same, or it gets a cache miss thereby increasing its footprint size.
In other words, we know how much cache is allocated to a process as a function of time: from the iimr curve, we compute the independent, isolated footprint as a function of time (iifp) (Section 3.2.2).
If one knows how much cache is allocated to a process when it begins executing its time quantum and how much more cache it will need during the execution of that time quantum, we can compute how much cache will be left for the next process that is about to begin its time quantum execution.
In other words, from the iifp curves of all the concurrent processes, we compute the individual, dependent footprint (dfp) as a function of time (Section 3.2.3).
At each time step, we know how much cache is allocated to the running process (from dfp(t)) and we know the miss rate for that size (from iimr(S)) for the executing process and so we can get the dependent miss rate as a function of time (dmr(t)) (Section 3.2.1).
Finally, integrating or summing the dmr(t) over time, gives the overall average miss rate for a given cache size, given time quantum sizes, and a given set of concurrent processes (Section 3.2.4).
The following subsection gives an overview of our assumptions.
The development of the cache model is then presented, following the outline given above.
Finally, this section ends with experimental verification of the model.
The memory reference pattern of each process is assumed to be represented by a miss-rate curve that is a function of the cache size.
Moreover, this miss-rate curve is assumed not to change over time.
Although real applications do have dynamically changing memory reference patterns, our results show that, in practice, an average miss-rate function works very well.
For abrupt changes in the reference pattern, multiple miss-rate curves can be used to estimate an overall miss-rate.
There is no shared address space among processes.
This assumption is true for common cases where each process has its own virtual address space and the shared memory space is negligible compared to the entire memory space that is used by a process.Finally, a round-robin scheduling policy with a fixed time quantum for each process is assumed (see Figure 1 (b)), an LRU replacement policy is used, and the cache is fully associative.
Although most real caches are set-associative, a model for fully-associative caches is very useful for understanding the effect of context switches because the model is simple.
Moreover, cache partitioning experiments demonstrate that the fully-associative model can also be applied to set-associative caches in practice (Section 4).
Elsewhere, we have extended the model to handle set-associative caches [19].
A model assuming many other scheduling methods and replacement policies can be similarly derived.We make use of the following notations:t the number of memory references from the beginning of a time quantum.x(t) the number of cache blocks that belong to a process after t memory references.m(x) the steady-state miss-rate for a process with cache size x.T the number of memory references in a time quantum.
The goal is to predict the average miss-rate for a multiprocess machine with a given cache size and set of processes.
Given the independent, isolated miss-rate of a process as a function of cache size, we compute its miss-rate as a function of time.
Let time t start at the beginning of a time quantum, not at the beginning of execution.
Since all time quanta for a process are identical by our assumptions, we consider only one time quantum for each process.Although the cache size is C, at certain times, it is possible that only part of the cache is filled with the current process' data ( Figure 2 (a) shows a snapshot of a cache at time t0).
Therefore, the effective cache size at time t0 can be thought of as the amount of the current process' data x(t0) in the cache at that time.
The probability of a cache miss in the next memory reference is given byPmiss(t0) = m(x(t0)).
(1)Once we have Pmiss(t0), it is easy to estimate the missrate over time during that time quantum.
The number of misses for the process over a time quantum can be expressed as a simple integral, Figure 2 (b), where the miss-rate is expressed as the number of misses divided by the number of memory references.miss-rate = 1 T Z T 0 Pmiss(t)dt = 1 T Z T 0 m(x(t))dt (2) We now estimate x(t), the amount of a process' data, i.e. its footprint, in a cache as a function of time.
Let us begin with the assumption that a process starts executing during a time quantum with an empty cache in order to estimate cache performance for cases when a cache gets flushed for every context switch.
Virtual address caches without process ID are good examples of such a case.
We show later how to estimate x(t) when the cache is not empty at the start of a time quantum.Consider x ∞ (t) as the amount of the current process' data at time t for an infinite size cache.
We assume that the process starts with an empty cache at time 0.
There are two possibilities for x ∞ (t) at time t + 1.
If the (t + 1) th memory reference results in a cache miss, a new cache block is brought into the cache.
As a result, the amount of the process's cache data increases by one block.
Otherwise, the amount of data remains the same.
Therefore, the amount of the process' data in the cache at time t + 1 is given byx ∞ (t + 1) = ( x ∞ (t) + 1 (t + 1) th reference misses x ∞ (t) otherwise.
(3)Since the probability for the (t + 1) th memory reference to miss is m(x ∞ (t)) from Equation 1, the expected value of x(t + 1) can be written byE[x ∞ (t + 1)] = E[x ∞ (t) · (1 − m(x ∞ (t))) + (x ∞ (t) + 1) · m(x ∞ (t))] = E[x ∞ (t) + 1 · m(x ∞ (t))] = E[x ∞ (t)] + E[m(x ∞ (t))].
(4)Assuming that m(x) is convex 1 , we can use Jensen's inequality [3] and rewrite the equation as a function of E[x ∞ (t)].
E[x ∞ (t + 1)] ≥ E[x ∞ (t)] + m(E[x ∞ (t)]).
(5)Usually, a miss-rate changes slowly.
As a result, for a short interval such as from x to x + 1, m(x) can be approximated as a straight line.
Since the equality in Jensen's inequality holds if the function is a straight line, we can approximate the amount of data at time t + 1 asE[x ∞ (t + 1)] E[x ∞ (t)] + m(E[x ∞ (t)]).
(6)We can calculate the expectation of x ∞ (t) more accurately by calculating the probability for every possible value at time t [19].
However, calculating a set of probabilities is computationally expensive.
Also, our experiments show that the above approximation closely matches simulation results.If we further approximate the amount of data x ∞ (t) to be the expected value E[x ∞ (t)], x ∞ (t) can be expressed with a differential equation:x ∞ (t + 1) − x ∞ (t) = m(x ∞ (t)),(7)which can be easily calculated in a recursive manner.To obtain a closed form solution, we can rewrite the discrete form of the differential equation 7 to a continuous form:dx ∞ dt = m(x ∞ ).
(8)Solving the differential equation by separating variables, the differential equation becomest = Z x ∞ (t) x ∞ (0) 1 m(x ) dx .
(9)We define a function M (x) as an integral of 1/m(x), which means that dM (x)/dx = 1/m(x), and then x ∞ (t) can be written as a function of t:x ∞ (t) = M −1 (t + M (x ∞ (0)))(10)where M −1 (x) represents the inverse function of M (x).
Finally, for a finite size cache, the amount of data in the cache is limited by the size of the cache C. Therefore, x φ (t),1 If a replacement policy is smart enough, the marginal gain of having one more cache block monotonically decreases as we increase the cache size.P i,1 P i,3 P i+1,2 ... P i-1,2 P i,2 P i+1,1 ... P i-1,1 MRU data LRU dataThe snapshot of a cache the amount of a process' data starting from an empty cache, is written byx φ (t) = M IN [x ∞ (t), C] = M IN [M −1 (t + M (0)), C].
(11) We now compute the amount of a process' data at time t when the cache is not flushed at a context switch, i.e., the dependent case.
To distinguish between the processes, a subscript i is used to represent Process i. For example, xi(t) represents the amount of Process i's data at time t.The estimation of xi(t) is based on round-robin scheduling (See Figure 1 (b)) and the LRU replacement policy.
Process i runs for a fixed length time quantum Ti.
For simplicity, processes are assumed to be of infinite length so that there is no change in the scheduling.
Also, the initial startup transient from an empty cache is ignored since it is negligible compared to the steady state.To estimate the amount of a process' data at a given time, imagine the snapshot of a cache after executing Process i for time t as shown in Figure 3.
Note that time is 0 at the beginning of the process' time quantum.
In the figure, the blocks on the left side show recently used data, and blocks on the right side show old data.
P j,k represents the data of Process j, and subscript k specifies the most recent time quantum when the data are referenced.
From the figure, we can obtain xi(t) once we know the size of all P j,k blocks.The size of each block can be estimated using the x φ i (t) curve from Equation 11, which is the amount of Process i's data when the process starts with an empty cache.
Since x φ i (t) can also be thought of as the amount of data that are referenced from time 0 to time t, x φ i (Ti) is the amount of data that are referenced over one time quantum.
Similarly, we can estimate the amount of data that are referenced over k recent time quanta to be x φ i (k · Ti).
As a result, the size of Block P j,k can be written asP j,k = 8 > > > < > > > :x φ j (t + (k − 1) · Tj) − x φ j (t + (k − 2) · Tj ) if j is executing x φ j (k · Tj) − x φ j ((k − 1) · Tj ) otherwise(12)where we assume that x φ j (t) = 0 if t < 0.
xi(t) is the sum of P i,k blocks that are inside the cache of size C in Figure 3.
If we define li(t) as the maximum integer value that satisfies the following inequality, then li(t) + 1 represents how many P i,k blocks are in the cache.l i (t) X k=1 N X j=1 P j,k = x φ i (t+(li(t)−1)·Ti)+ N X j=1,j񮽙 =i x φ j (li(t)·Tj) ≤ C (13)where N is the number of processes.
From li(t) and Figure 3, the estimated value of xi(t) is Figure 4 illustrates the relation between x φ i (t) and xi(t).
In the figure li(t) is assumed to be 2.
Unlike the cache flushing case, a process can start with some of its data left in the cache.
The amount of initial data xi(0) is given by Equation 14.
If the least recently used (LRU) data in a cache does not belong to Process i, xi(t) increases the same as x φ i (t).
However, if the LRU data belongs to Process i, xi(t) does not increase on a cache miss since Process i's block gets replaced.xi(t) = 8 > > > > > > > < > > > > > > > :x φ i (t + li(t) · Ti) if x φ i (t + li(t) · Ti)+ N X j=1,j񮽙 =i x φ j (li(t) · Tj ) ≤ C C − N X j=1,j񮽙 =i x φ j (li(t) · Tj ) otherwise(14)Define tstart(j, k) as the time when the k th MRU block of Process j (P j,k ) becomes the LRU part of a cache, and t end (j, k) as the time when P j,k gets completely replaced from the cache (See Figure 3).
tstart(j, k) and t end (j, k) specify the flat segments in Figure 4 and can be estimated from the following equations that are based on Equation 12.x φ j (tstart(j, k)+(k−1)·Tj )+ N X p=1,p񮽙 =j x φ p ((k−1)·Tp) = C. (15) x φ j (t end (j, k)+(k−2)·Tj)+ N X p=1,p񮽙 =j x φ p ((k−1)·Tp) = C.(16)tstart(j, lj (t) + 1) would be zero if Equation 15 is satisfied when tstart(j, lj (t) + 1) is negative, which means that the P (j, lj (t) + 1) block is already the LRU part of the cache at the beginning of a time quantum.
This section presents the overall miss-rate calculation.
When a cache uses virtual address tags and gets flushed for every context switch, each process starts a time quantum with an empty cache.
In this case, the miss-rate of a process can be estimated from the results of Section 3.2.1 and 3.2.2.
From Equation 2 and 11, the miss-rate for Process i can be written bymiss-rate φ i = 1 Ti Z T i 0 mi(M IN [M −1 i (t + Mi(0)), C])dt.
(17)If a cache uses physical address tags or has a process' ID with virtual address tags, it does not have to be flushed at a context switch.
In this case, the amount of data xi(t) is estimated in Section 3.2.3.
The miss-rate for Process i can be written bymiss-ratei = 1 Ti Z T i 0 mi(xi(t))dt(18)where xi(t) is given by Equation 14.
For actual calculation of the miss-rate, tstart(j, k) and t end (j, k) from Equation 15 and 16 can be used.
Since tstart(j, k) and t end (j, k) specify the flat segments in Figure 4, the miss-rate of Process i can be rewritten bymiss-ratei = 1 Ti { Z T i 0 mi(M IN [M −1 i (t + Mi(xi(0))), C])dt + l i (t)+1 X k=d i mi(x φ i (tstart(i, k) + (k − 1) · Ti)) · (M IN [t end (i, k), Ti] − tstart(i, k))}(19)where di is the minimum integer value that satisfies tstart(i, di) < Ti.
T i is the time that Process i actually grows.T i = Ti − l i (t)+1 X k=d i (M IN [t end (i, k), Ti] − tstart(i, k)).
(20)As shown above, calculating a miss-rate could be complicated if we do not flush a cache at a context switch.
If we assume that the executing process' data left in a cache is all in the most recently used part of the cache, we can use the equation for estimating the amount of data starting with an empty cache.
Therefore, the calculation can be much simplified as follows,miss-ratei = 1 Ti Z T i 0 mi(M IN [M −1 i (t + Mi(xi(0))), C])dt(21)where xi(0) is estimated from Equation 14.
The effect of this approximation is evaluated in the experiment section (cf. Section 3.3).
Once we calculate the miss-rate of each process, the overall miss-rate is straightforwardly calculated from those missrates.Overall miss-rate = P N i=1 miss-ratei · Ti P N i=1 Ti(22) Our cache model can be validated by comparing estimated miss-rate predictions with simulation results.
Several combinations of benchmarks are modeled and simulated for various time quanta.
First, we simulate cases when a cache gets flushed at every context switch, and compare the results with the model's estimation.
Cases without cache flushing are also tested.
For the cases without cache flushing, both the complete model (Equation 19) and the approximation (Equation 21) are used to estimate the overall miss-rate.
Based on the simulation results, the error caused by the approximation is discussed.
The results of the cache model and simulations are shown in Figure 5 in cases when a process starts its time quantum with an empty cache.
Four benchmarks from SPEC CPU2000 [7], which are vpr, vortex, gcc and bzip2, are tested.
The cache is a 32-KB fully-associative cache with 32-Byte blocks.
The miss-rate of a process is plotted as a function of the length of a time quantum, and shows a good agreement between the model's estimation and the simulation result.As inputs to the cache model, the average miss-rate of each process has been obtained from simulations.
Each process has been simulated for 25 million memory references, and the miss-rates of the process for various cache size have been recorded.
The simulation results were also obtained by simulating benchmarks for 25 million memory references with flushing a cache every T memory references.
As the result shows, the average miss-rate works very well.
Figure 6 shows the result of the cache model when two processes are sharing a cache.
The two benchmarks are vpr and vortex from SPEC CPU2000, and the cache is a 32-KB fully-associative cache with 32-Byte blocks.
The overall miss-rates are shown in Figure 6 (a).
As shown in the figure, the miss-rate estimated by the model shows a good agreement with the results of the simulations.
The figure also shows an interesting fact that a certain range of time quanta could be very problematic for cache performance.
For short time quanta, the overall miss-rate is relatively small.
For very long time quanta, context switches do not matter since a process spends most of its time in the steady state.
However, medium time quanta could severely degrade cache miss-rates as shown in the figure.
This problem occurs when a time quantum is long enough to pollute the cache but not long enough to compensate for the misses caused by context switches.
The problem becomes clear in Figure 6 (b).
The figure shows the amount of data left in the cache at the beginning of a time quantum.
Comparing Figure 6 (a) and (b), we can see that the problem occurs when the initial amount of data rapidly decreases.The error caused by our approximation (Equation 21) method can be seen in Figure 6.
In the approximation, we assume that the data left in the cache at the beginning of a time quantum are all in the MRU region of the cache.
In reality, however, the data left in the cache could be the LRU cache blocks and get replaced before other process' blocks in the cache, although the current process's data are likely to be accessed in the time quantum.
As a result, the approximated miss-rate is lower than the simulation result when the initial amount of data is not zero.
A four-process case is also tested in Figure 7.
Two more benchmarks, gcc and bzip2, from SPEC CPU2000 [7] are added to vpr and vortex, and the same cache configuration is used as the two process case.
The figure also shows a very close agreement between the miss-rate estimated by the cache model and the miss-rate from simulations.
The problematic time quanta and the effect of the approximation have changed.
Since there are more processes polluting the cache as compared to the two process case, a process experiences an empty cache in shorter time quanta.
As a result, the problematic time quanta become shorter.
On the other hand, the effect of the approximation is less harmful in this case.
This is because the error in one process' miss-rate becomes less important as we have more processes.
This section shows how the analytical cache model can be used to dynamically partition the cache.
A partitioned cache allocates cache space to particular processes.
This space is dedicated to the process and cannot be used to satisfy cache misses by other processes.
Using trace-driven simulations, we compare partitioning with the normal LRU.
The partitioning is based on the fully-associative cache model.
However, simulation results demonstrate that this implementation works for both fully-associative caches and setassociative caches.
The miss-rate curves for each process are generated off-line.
We record the miss-rate curve for each process to represent its memory reference pattern.
For various cache sizes, a single process cache simulator is applied to each process.
This information can be reused for any combination of processes as long as the cache configuration is the same 2 .
To incorporate the dynamically changing behavior of a process, a set of miss-rate curves, one for each time period, are 2 Note that for our fully-associative model, only the cache block size matters Figure 8: The implementation of on-line cache partitioning.produced.
At run-time, the miss-rate curve is mapped to the appropriate time quantum.
The overall flow of the partitioning scheme can be viewed as a set of four modules: off-line recording, scheduler information, allocation, and replacement ( Figure 8).
The scheduler provides the partition module with the set of executing processes and their start/end times.
The partition module uses the miss-rate information for the processes to calculate cache partitions at the end of each time quantum.
Finally, the replacement unit maps these partitions to the appropriate parts of the cache.The partition module decides the number of cache blocks that should be dedicated to a process (Di).
The Di most recently used cache blocks of Process i are kept in the cache over other process' time quanta, and Process i starts its time quantum with those cache blocks in the cache.
During its own time quantum, Process i can use all cache blocks that are not reserved for other processes (S = C − P N j=1,j񮽙 =i Dj).
In addition to LRU information, our replacement decision depends on the number of cache blocks that currently belong to each process (Xi), that is, the number of cache lines in the cache that currently contain memory of that process.
The LRU cache block of an active process (i) is chosen if its actually allocation (Xi) is larger than or equal to the desired one (Di + S ≤ Xi).
Otherwise, the LRU cache block of a dormant overallocated process is chosen.
For set-associative caches, there may be no cache block of the desired process in the set.
In this case, the LRU cache block of the set is replaced.For set-associative caches, the fully-associative replacement policy may result in replacing recently used data to keep useless data.
Imagine the case when a process starts to heavily access two or more addresses that happen to be mapped to the same set.
If the process already has many cache blocks in other sets, our partitioning will allocate only a few cache blocks in the accessed set for the process, causing lots of conflict misses.
To solve this problem, we can use better mapping functions [22,6] or a victim cache [8].
When a Process i first starts, Di is set to zero since there is no cache block that belongs to the process.
At the end of Process i's time quantum, the partition module updates the information such as the miss-rate curve(mi(x)) and the time quantum (Ti).
If there is any change, Di is also updated based on the cache model.
A cache partition specifies the amount of data in the cache at the beginning of a process' time quantum (Di), and the maximum cache space the process can use (C − P N j=1,j񮽙 =i Dj ).
Therefore, the number of misses for a process over one time quantum can be estimated from Equation 21:missi = Z T i 0 mi(M IN [M −1 i (t + Mi(Di)), C − N X j=1,j񮽙 =i Dj ])dt (23)where C is cache size, and N is the number of processes sharing the cache.The new value of Di is the integer, in the range [0, Xi], that minimizes the total number of misses that is given by the following quantity:N X p=1 Z Tp 0 mp(M IN [M −1 p (t + Mp(Dp)), C − N X q=1,q񮽙 =p Dq])dt.
(24) The case of eight processes sharing a 32-KB cache is simulated to evaluate model-based partitioning.
Seven benchmarks (bzip2, gcc, swim, mesa, vortex, vpr, twolf) are from SPEC CPU2000 [7], and one (the image understanding program (iu)) is from a data intensive systems benchmark suite [15].
The overall miss-rate with partitioning is compared to the miss-rate only using the normal LRU replacement policy.The simulations are carried out for fifty million memory references for each time quantum.
Processes are scheduled in a round-robin fashion with the fixed number of memory references per time quantum.
Also, the number of memory references per time quantum is assumed to be the same for the all eight processes.
Finally, two record cycles (P ), of ten million and one hundred thousand memory references, respectively, are used for the model-based partitioning.
The record cycle represents how often the miss-rate curve is recorded for the off-line profiling.
Therefore, a shorter record cycle implies more detailed information about a process' memory reference pattern.The characteristics of the benchmarks are illustrated in Fig- ure 9.
Figure 9 (a) shows the change of a miss-rate over time.
The x-axis represents simulation time.
The y-axis represents the average miss-rate over one million memory references at a given time.
As shown in the figure, bzip2, gcc, swim and iu show abrupt changes in their miss-rate, whereas other benchmarks have very uniform miss-rate characteristics over time.
Figure 9 (b) illustrates the miss-rate as a function of the cache size.
For a 32-KB fully-associative cache, benchmarks show miss-rates between 1% and 5%.
The results of cache partitioning for a fully-associative cache are shown in Figure 10.
In Figure 10 (a), the miss-rates are averaged over 50 million memory references and shown for various time quanta.
As discussed in the cache model, the normal LRU replacement policy is problematic for a certain range of time quanta.
In this case, the overall miss-rate increases dramatically for time quanta between one thousand and ten thousand memory references.
For this problematic region, the model-based partitioning improves the cache miss-rate by lowering it from 4.6% to 3.4%, which is about a 25% improvement.
For short time quanta, the relative improvement is about 7%.
For very long time quanta, the model-based partitioning shows the exact same result as the normal LRU replacement policy.
In general, it is shown by the figure that the model-based partitioning always performs at least as well as or better than the normal LRU replacement policy.
Also, the partitioning with a short record cycle performs better than the partitioning with a long record cycle.In our example of a 32-KB cache with eight processes ( ure 10), the problematic time quanta are in the order of a thousand memory references, which is very short for modern microprocessors.
As a result, only systems with very fast context switching, such as simultaneous multi-threading machines [24,11,4], can be improved for this cache size and workload.
However, longer time quanta become problematic if a cache is larger.
Therefore, conventional time-shared systems with very high clock frequency can also be improved by the same technique if a cache is large.
In the figure, the model-based partitioning with the long record cycle (P = 10 7 ) performs worse than LRU at the beginning of a simulation, even though it outperforms the normal LRU replacement policy overall.
This is because the model-based partitioning has only one average miss-rate curve for a process.
As shown in Figure 9, some benchmarks such as bzip2 and gcc have a very different miss-rate at the beginning.
Therefore, the average miss-rate curves for those benchmarks do not work at the beginning of the simulation, which results in worse performance than the normal LRU replacement policy.
The model-based partitioning with the short record cycle (P = 10 5 ), on the other hand, always outperforms the normal LRU replacement policy.
In this case, the model has correct miss-rate curves for all the time quanta, and partitions the cache properly even for the beginning of processes.
The result of cache partitioning for a set-associative cache is shown in Figure 11.
The same set of benchmarks are simulated with a 32-KB 8-way set-associative cache, and the same miss-rate curves generated for a 32-KB fully-associative cache are used.
In this case, a 16 entry victim cache is added.
In the figure, the model-based partitioning improves the miss-rate about 4% for short time quanta and up to 15% for mid-range time quanta.
The figure demonstrates that the model-based partitioning mechanism works reasonably well for set-associative caches.
An analytical cache model to estimate overall miss-rate when multiple processes are sharing a cache has been presented.
The model obtains the information about each process from its miss-rate curve, and combines it with parameters that define the cache configuration and schedule of processes.
Interference among processes under the LRU replacement policy is quickly estimated for any cache size and any time quantum, and the estimated miss-rate is very accurate.
A more important result is that the model provides not only the overall miss-rate but also a very good understanding of the effect of context switching.
For example, the model clearly shows that the LRU replacement policy is problematic for mid-range time quanta because the policy replaces the blocks of least recently executed process that are more likely to be accessed in the near future.The analytical model has been applied to the cache partitioning problem.
A model-based partitioning method has been implemented and verified by simulations.
Miss-rate curves are recorded off-line and partitioning is performed on-line according to the combination of processes that are executing.
Even though we have used an off-line profiling method to obtain miss-rate curves, it should not be hard to approximate the miss-rate curve on-line using a miss-rate monitoring technique.
Therefore, a fully on-line cache partitioning method can be developed based on the model.Only the cache partitioning problem has been studied in this paper.
However, as shown by the study of cache partitioning, our model can be applied to any cache optimization problem that is related to the problem of context switching.
For example, it can be used to determine the best combination of processes that can be run on each processor of a multi-processor system.
Also, the model is useful to identify areas in which further research in improving cache performance would be fruitful since it can easily provide the maximum improvement we can expect in the area.
