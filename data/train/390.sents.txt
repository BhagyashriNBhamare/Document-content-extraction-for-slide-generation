We distinguish four categories of routines that we want to include in ScaLAPACK: (1) functions for matrix types appearing in LAPACK but not yet supported in ScaLAPACK (discussed here), (2) functions for matrix types common to LAPACK and ScaLAPACK, but implemented only in LAPACK (discussed here), (3) improved algorithms for functions in LAPACK, which also need to be put in ScaLAPACK (see Sec. 3), and (4) new functions for both LAPACK and ScaLAPACK (see Sec. 4).
Table 1 compares the available data types in the latest releases of LAPACK and ScaLAPACK.
After the data type description, we list the prefixes used in the respective libraries, a blank entry indicates that the corresponding type is not supported.
The most important omissions in ScaLAPACK are as follows.
(1) There is no support for packed storage of symmetric (SP,PP) or Hermitian (HP,PP) matrices, nor the triangular packed matrices (TP) resulting from their factorizations (using ≈ n 2 /2 instead of n 2 storage); these have been requested by users.
The interesting question is what data structure to support.
One possibility is recursive storage as discussed in Sec. 4 [1,51,52,42].
Alternatively we could partially expand the packed storage into a 2D array in order to apply Level 3 BLAS (GEMM) efficiently.
Some preliminary ScaLAPACK prototypes support packed storage for the Cholesky factorization and the symmetric eigenvalue problem [17], but they are under development and have not been rigorously tested on all of the architectures to which the ScaLAPACK library is portable.
(2) ScaLAPACK only offers limited support of band matrix storage and does not specifically take advantage of symmetry or triangular form (SB,HB,TB).
(3) ScaLAPACK does not support data types for the standard (HS) or generalized (HG, TG) nonsymmetric EVDs; we discuss this further below.In Table 2, we compare the available functions in LAPACK and ScaLAPACK.
We list the relevant user interfaces ('drivers') by subject and the acronyms used for the software in the respective libraries.
In the ScaLAPACK column, we indicate what is currently missing.
We note that most expert drivers in LAPACK (which supply extra information, such as error bounds) and their specialized computational routines are missing from ScaLAPACK and do not include them explicitly in the table.We discuss our most important priorities for inclusion in ScaLAPACK:1.
The solution of symmetric linear systems (SYSV), combined with the use of symmetric packed storage (SPSV), will be a significant improvement with respect to both memory and computational complexity over the currently available LU factorization.
It has been requested by users and is expected to be used widely.
In addition to solving systems it is used to compute the inertia (number of positive, zero and negative eigenvalues) of symmetric matrices.2.
EVD and SVD routines of all kinds (standard -for one matrix -and generalized -for two matrices) are missing from ScaLAPACK.
We expect to exploit the MRRR algorithm for the SVD and symmetric EVD, and new algorithms of Braman/Byers/Mathias for the nonsymmetric EVD (see Sec. 3).3.
LAPACK provides software for the linearly constrained (generalized) least squares problem, and users in the optimization community will benefit from a parallel version.
In addition, algorithms for rank deficient standard least squares problems based on the SVD are missing from ScaLAPACK; it may be that a completely different algorithm based on the MRRR algorithm (see Sec. 3) may be more suitable for parallelism instead of the divide & conquer (D&C) algorithm that is fastest for LAPACK.4.
Expert drivers that provide error bounds, or other more detailed structural information about eigenvalue problems, should be provided.
In Section 2, we have given an outline of how ScaLAPACK needs to be extended in order to cover the functionalities of LAPACK.
However, LAPACK itself needs to be updated in order to include recent developments since its last release.
Whereever possible, we plan to extend these functionalities subsequently to ScaLAPACK.
The expected benefits higher accuracy and/or speed in the solution of linear systems and eigensolvers.
We list each set of improvements in our current priority order (highest first).
1.
The recent developments of extended precision arithmetic [63,9,20,19] in the framework of the new BLAS standard allow the use of higher precision iterative refinement to improve computed solutions.
We have recently shown how to modify the classical algorithm of Wilkinson [91,55] to compute not just an error bound measured by the infinity (or max) norm, but a componentwise relative error bound, i.e. a bound on the number of correct digits in each component.
Both error bounds can be compute for a tiny O(n 2 ) extra cost after the initial O(n 3 ) factorization [32].2.
Gustavson, Kågström and others have recently proposed a new set of recursive data structures for dense matrices [51,52,42].
These data structures represent a matrix as a collection of small rectangular blocks (chosen to fit inside the L1 cache), and then stores these blocks use ones of several "space filling curve" orderings.
The idea is that the data structure, and associated recursive matrix algorithms, are cache oblivious [43], that is they optimize cache locality without any explicit blocking of the sort conventionally done in LAPACK and ScaLAPACK, or any of the tuning parameters (beyond the L1 cache size).
Table 2: LAPACK codes and corresponding parallel version in ScaLAPACK.
Underlying LAPACK algorithm shown in parentheses.
"Missing" means both drivers and computational routines are missing.
"Missing driver" means that underlying computational routines are present.
The reported benefits of these data structures and associated algorithms to which they apply is usually slightly higher peak performance on large matrices, and a faster increase towards peak performance as the dimension grows.
Sometimes slightly modified tuned BLAS are use for operations on matrices assumed to be in L1 cache.
The biggest payoff by far is for factoring symmetric matrices stored in packed format, where the current LAPACK routines are limited to the performance of BLAS2, which do O(1) flops per memory reference, whereas the recursive algorithms can use the faster BLAS3, which do O(n) flops per memory reference, and so can be optimized to hide slower memory bandwidth and latenciesThe drawback of these algorithms is their use of a completely different and rather complicated data structure, which only a few expert users could be expected to use.
That leaves the possibility of copying the input matrices in conventional column-major (or row-major) format into the recursive data structure.
Furthermore, they are only of benefit for "one-sided factorizations" (LU , LDL T , Cholesky, QR), but none of the "two-sided factorizations" needed for the EVD or SVD.
(There is a possibility they might be useful when no eigenvectors or singular vectors are desired; see below.)
We will incorporate the factorization of symmetric packed matrices using the recursive data structures into LAPACK, copying the usual data structure in-place to the recursive data structure.
The copying costs O(n 2 ) in contrast to the overall O(n 3 ) operation count, so the asymptotic speeds should be the same.
We will explore the use of the recursive data structures for other parts of LAPACK, but for the purposes of ease of use, we will keep the same column-major interface data structures that we have now.3.
Ashcraft, Grimes and Lewis [7] proposed a variation of Bunch-Kaufman factorization for solving symmetric indefinite systems Ax = b by factoring A = LDL T with different pivoting.
The current Bunch-Kaufman factorization is backward stable for the solution of Ax = b but can produce unbounded L factors.
Better pivoting provides better accuracy for applications requiring bounded L factors, like optimization and the construction of preconditioners [55,27].
In addition, Livne [64] has developed alternative equilibration strategies for symmetric indefinite matrices whose accuracy benefits we need to evaluate.4.
A Cholesky factorization with diagonal pivoting [54,55] that avoids a breakdown if the matrix is nearly indefinite/rank-deficient is valuable for optimization problems (and has been requested by users), and is also useful for the high accuracy solution of the symmetric positive definite EVD, see below.
For both this pivoting strategy and the one proposed above by Askcraft/Grimes/Lewis, published results indicate that on uniprocessors (LAPACK), the extra search required (compared to Bunch-Kaufman) has a small impact on performance.
This may not be the case for distributed memory (ScaLAPACK), in which the extra searching and pivoting may involve nonnegligible communication costs; we will evaluate this.5.
Progress has been made in the development of new algorithms for computing or estimating the condition number of tridiagonal [34,53] or triangular matrices [41].
These algorithms play an important role in obtaining error bounds in matrix factorizations and we plan to evaluate and incorporate the most promising algorithms in a future release.
Algorithmic improvements to the current LAPACK eigensolvers concern both accuracy and performance.1.
Braman, Byers, and Mathias proposed in their SIAM Linear Algebra Prize winning work [21,22] an up to 10x faster Hessenberg QR-algorithm for the nonsymmetric EVD.
This is the bottleneck of the overall nonsymmetric EVD, for which we expect significant speedups.
Byers recently spent a sabbatical visiting PI Demmel where he did much of the software engineering required to convert his prototype into LAPACK format.
We also expect an extension of this work with similar benefits to the QZ algorithm for Hessenberg-triangular pencils, with collaboration from Mehrmann (see the attached letter).
We will also be able to exploit these techniques to accelerate our routines for (block) companion matrices; see Sec. 4.2.
An early version of an algorithm based on Multiple Relatively Robust Representations (MRRR) [71,72,74,73] for the tridiagonal symmetric eigenvalue problem (STEGR) was incorporated into LAPACK version 3.
This algorithm promised to replace the prior O(n 3 ) QR algorithm (STEQR) or ≈ O(n 2.3 ) divide & conquer (STEDC) algorithm with an O(n 2 ) algorithm.
In fact, it should have cost O(nk) operations to compute the nk entries of k n-dimensional eigenvectors, the minimum possible work, in a highly parallel way.
In fact the algorithm in LAPACK v.3 did not cover all possible eigenvalue distributions, and resorted to a slower and less accurate algorithm based on classical inverse iteration for "difficult" (highly clustered) eigenvalue distributions.
The inventors of MRRR, Parlett and Dhillon, have continued to work on improving this algorithm, and very recently have proposed a solution for the last hurdle [75] and now pass our tests for the most extreme examples of highly multiple eigenvalues.
(These arose in our tests from very many large "glued Wilkinson matrices", constructed so that large numbers of mathematically distinct eigenvalues agreed to very high accuracy, much more than double precision.
The proposed solution involves randomization, making small random perturbations to an intermediate representation of the matrix to force all eigenvalues to disagree in at least 1 or 2 bits.)
Given the solution to this last hurdle, we can propagate this algorithm to all the variants of the symmetric EVD (banded, generalized, packed, etc.) in LAPACK (this has NPACI funding through the end of 2004).
For this proposal we will go beyond this and parallelize this algorithm for the corresponding ScaLAPACK symmetric EVD routines.
Currently ScaLAPACK only has parallel versions of the oldest, least efficient (or least accurate) LAPACK routines, because we had been waiting for the completion of the sequential MRRR algorithm.
This final MRRR algorithm requires some care at load balancing because the Multiple Representations used in MRRR represent subsets of the spectrum based on how clustered they are, which may or may not correspond to a good load balance.
Initial work by our collaborators in this area is very promising [14].3.
The MRRR algorithm can and should also be applied to the SVD, replacing the current O(n 3 ) or ≈ O(n 2.3 ) bidiagonal SVD algorithms with an O(n 2 ) algorithm.
The necessary theory and a preliminary prototype implementation have been developed [49].
Grosser, the author of [49] visited us to help with this development, and we expect a visit from the same team to help again (see the attached letter from Lang, who was Grosser's adviser).4.
There are three phases in the EVD (or SVD) of a dense or band matrix: (1) reduction to tridiagonal (or bidiagonal) form, (2) the subsequent tridiagonal EVD (or bidiagonal SVD), and (3) backtransforming the eigenvectors (or singular vectors) of the tridiagonal (or bidiagonal) to correspond to the input matrix.
If many (or all) eigenvectors (or singular vectors) are desired the bottleneck had been phase 2.
But now that the MRRR algorithm promises to make phase 2 cost just O(n 2 ) in contrast to the O(n 3 ) costs of phases 1 and 3, our attention turns to these phases.
In particular, Howell and Fulton [44] recently devised a new variant of reduction to bidiagonal form for the SVD, that has the potential to eliminate half the memory references, by reordering the floating point operations (flops).
Howell and Fulton fortunately discovered this algorithm during the deliberations of the recent BLAS standardization committee (led by PI Dongarra, and participated in by PI Demmel), because they required new BLAS routines to accomplish this, which we added to the standard (routines GEMVT and GEMVER [19]).
We call these routine BLAS2.5, because they do many more than O(1) but fewer than O(n) flops per memory reference.
Preliminary tests indicate speed ups of up to nearly 2x.5.
For the SVD, when only left or only right singular vectors are desired, there are other variations on phase 1 to consider, that reduce both floating point operations and memory references [11,76].
Initial results indicate reduced operation counts by a ratio of up to .75, but at the possible cost of numerical stability for some singular vectors.
We will evaluate these for possible incorporation.6.
When few or no vectors are desired, the bottleneck shifts entirely to phase 1.
Bischof and Lang [15] have proposed a Successive Band Reduction (SBR) algorithm that will asymptotically (for large dimension n) change most of the BLAS2 operations in phase 1 to BLAS3 operations (see the attached letter from Lang).
They report speed ups of almost 2.4x.
This approach is not suitable when a large number of vectors are desired, because the cost of phase 3 is much larger per vector.
In other words, depending on how many vectors are desired, we will either use the SBR approach or the one-step reduction (the Howell/Fulton variant for the SVD, and the current LAPACK code for the symmetric EVD).
And if only left or only right singular vectors are desired, we might want to use the algorithms described in bullet 5.
This introduces a machine-dependent tuning parameter to choose the right algorithm; we discuss tuning of this and other parameters in Sec. 6.
It may also be possible to use Gustavson's recursive data structures to accelerate SBR; we will consider this.7.
Drmač and Veseli´cVeseli´c have made significant progress on the performance of the one-sided Jacobi algorithm for computing singular values with high relative accuracy [31,40].
In contrast to the algorithms described above, theirs can compute most or all of the significant digits in tiny singular values, when these digits are determined accurately by the input data, and when the above algorithms return only roundoff noise.
The early version of this algorithm introduced by PI Demmel in [33,31] was rather slower than the conventional QR-iteration-based algorithms, and so much slower than the MRRR algorithms discussed above.
But recent results reported by Drmač at [59] show that a combination of clever optimizations have finally led to an accurate algorithm that is faster than the original QRiteration-based algorithm.
Innovations include preprocessing by QR factorizations with pivoting, block application of Jacobi rotations, and early termination.
Two immediate applications include the (full matrix) SVD and the symmetric positive-definite EVD, by first reducing to the SVD by using the Cholesky-with-pivoting algorithm discussed earlier.8.
Analogous high accuracy algorithms for the symmetric indefinite EVD have also been designed.
One approach by Slapničar [79, 78] uses a J-symmetric Jacobi algorithm with hyperbolic rotations, and another one by Dopico/Molera/Moro [39] does an SVD, which "forgets" the signs of the eigenvalues, and then reconstructs the signs.
The latter can directly benefit by the Drmač/Veselič algorithm above.
We will investigate which of these two algorithm meets our criteria for inclusion; see the attached letter from Dopico.
In this section, we outline possible extensions of the functionalities available in LAPACK and ScaLAPACK.
These extensions are mostly motivated by users but also by research progress.1 [35], the efficient update of the SVD is a current research topic [50].
Furthermore, divide & conquer based techniques are promising for a general framework of updating eigendecompositions of submatrices, this will be a future research topic.2.
Semi-separable matrices are generalizations of the inverses of banded matrices, with the property that any rectangular submatrix lying strictly above or strictly below the diagonal has a rank bounded by a small constant.
Recent research has focused on methods exploiting semiseparability, or being a sum of a banded matrix and a semiseparable matrix, for better efficiency [26,86].
We will consider the development of such algorithms in a future release.
Most exciting is the recent observation of Gu, Bini and others that a companion matrix is banded plus semiseparable, and that this structure is preserved under QR iteration to find its eigenvalues.
This observation let us accelerate the standard method used in Matlab and other libraries for finding roots of polynomials from O(n 3 ) to O(n 2 ).
Our initial rough prototype of this code starts being faster than the highly tuned LAPACK eigensolver for n between 100 and 200, and becomes arbitrarily faster for larger n.
While the current algorithm (joint work with Gu) has been numerically stable on all examples we have tried so far, more work needs to be done to guarantee stability in all cases.
The same technique should apply to finding eigenvalues of block companion matrices, i.e. matrix polynomials, yielding speedups proportional to the degree of the matrix polynomial.3.
Eigenvalue problems for matrix polynomials [45] are common in science and engineering.
The most common case is the quadratic eigenvalue problem (λ 2 M + λD + K)x = 0, where typically M is a mass matrix, D a damping matrix, K a stiffness matrix, λ a resonant frequency, and x a mode shape.
The classical solution is to linearize this eigenproblem, asking instead for the eigenvalues of a system of twice the size: λ ·񮽙 0 I M D 񮽙 · 񮽙 y 1 y 2 񮽙 + 񮽙 I 0 0 K 񮽙 · 񮽙 y 1 y 2 񮽙 = 0where y 2 = x and y 1 = λx.
But there are a number of ways to linearize, and some are better at preserving symmetries in the solution of the original problem or saving time than others.
There has been a great deal of recent work on picking the right linearization and subsequent algorithm for its EVD to preserve desired structures, and we have had user requests to incorporate some of these structures.
In particular, for the general problem 񮽙 k i=0 λ i · A i · x = 0, the requested cases are symmetric (A i = A T i , arising in mechanical vibrations without gyroscopic terms), its even (A i = (−1) i A T i ) and odd (A i = (−1) i+1 A T i ) variations (used with gyroscopic terms and elsewhere), and palindromic (A i = A T k−i , arising in discrete time periodic and continuous time control).
Recent references include [4,5,6,66,67,68,81,82,84,12]; see the attached letter from Mehrmann.
[28,56,80,70,23,24,25,60,61,8] that could be included in a future release.5.
The eigenvalue and singular value decomposition of products and quotients of matrices plays an important role in control theory, we consider incorporating such functionalities from the software library SLICOT [13], using our improved underlying EVD algorithms (Mehrmann, see attached letter, was one of the designers of SLICOT).
Following another request from the control theory community, we could incorporate efficient solvers for Sylvester and Lyapunov equations that are also currently in SLICOT.6.
Multiple user requests concern the development of out-of-core versions of matrix factorizations.
ScaLA-PACK prototypes [17] are under development which implement out-of-core data management for the LU, QR, and Cholesky factorizations [37,36].
Users have asked for two kinds of parallel I/O: to a single file from a sequential LAPACK program (possible with sequential I/O in the reference implementation), and to a single file from MPI-based parallel I/O in ScaLAPACK.
First we discuss language interfaces to Sca/LAPACK, and then higher level issues related to their use on possibly heterogeneous clusters.
By exploiting features of modern programming languages and making Sca/LAPACK easier to use we will be in a position to capture a new generation of users who are not interested in using Fortran 77, the current implementation language.
Balanced against this are the cost in performance, memory usage or even reliability of some of these features, and the difficulty of building and maintaining one version of these very large libraries, let alone several versions in different languages.
Since we do not believe that we can simultaneously maximize performance, memory efficiency, ease of use, reliability, and ease of maintenance, we have decided on the following strategy: Maintain one core version in Fortran 77, and provide wrappers in other languages just for the driver routines.
Based on current user demand, these other languages will include Fortran 95 and C, as well as selected higher level languages like Matlab, Python and Mathematica (where ultimate ease of use is possible, such as typing "x = A\b" to solve Ax = b no matter what type, mathematical properties or data structure A has).
Users of the Fortran 77 version will get maximum performance and memory efficiency, but worst ease-of-use.
Users of the wrappers will have better ease of use and reliability, but worse performance and memory efficiency in some cases.
We, the developers, will have a tractable amount of code to maintain.We justify the above mentioned tradeoffs in more detail.
First consider memory usage.
Sca/LAPACK currently expects the user to pass in the right amount of workspace, since the (old) F77 standard does not include allocate/malloc.
The amount of workspace can be a complicated function of the problem to be solved, so the user can query each Sca/LAPACK driver routine to inquire what the optimal workspace actually is.
Thus, a wrapper in F95 or C can query the F77 driver to get the optimal workspace, try to allocate it, and then call the F77 driver again to solve the problem.
If the maximum workspace is not available, we could make drivers queryable for the minimum workspace and try again, and eventually return an error message if too little space is available.
This approach is reliable as long as we allocate workspace on the heap with allocate/malloc, and not on the stack, since there is generally no graceful way to recover from stack overflow.The details of the interface can affect the tradeoff between ease of use and performance.
For example, there are two F95 interfaces to LAPACK, LAPACK95 [10] and LAPACK3E [2].
LAPACK95 uses assumedsize arrays, so that passing in a submatrix can cause a copy of O(n 2 ) input data to a new contiguous space (and copying back again on output), costing both time and memory.
In contrast LAPACK3E supports the older Fortran 77 style interface that works with submatrices in-place.
Based on the larger usage of LAPACK95 over LAPACK3E, we will go with the LAPACK95 interface.
We also plan to drop support for the CLAPACK translation of LAPACK to C, and just supply a C wrapper.
We do not plan to provide a separate C++ wrapper.
Now we consider ease-of-use issues in a possibly dynamically changing cluster environment, or in one where the user does not know and does not wish to know the computational resources available, but may have other demands like reproducibility.
These enhancements will incorporate novel techniques for managing high latencies, low bisection bandwidths, and other characteristics of the modern computing environment, and will also be designed to adapt their behavior in response to information obtained from other system components.
Our primary goals are to develop a new generation of software libraries and algorithms needed for the effective and reliable use of capacity cluster and high performance capability computers, to validate the libraries and algorithms on important scientific and engineering applications, and to provide mechanisms for the integration of other software components frequently used in applications.We will develop parameterizable and annotatable software libraries that will permit performance tuning and dynamic algorithm selection for a broad range of architectures.
These extensions will take the form of performance models and assertions made by the user and library writer.
In addition, we will promote standards activities for key aspects of execution.
The libraries and algorithms will be designed and implemented for complex and dynamic environments and cover a wide range of scales in time and space.
Successful implementation along these lines requires:• predictability and robustness of accuracy and performance,• run-time resource management and algorithm selection,• support for a multiplicity of programming environments,• reproducibility and auditability of the computations• new algorithmic techniques for latency tolerant applications; and• scalable algorithms that expose enough concurrency to keep resources busy, and for latency hiding.Predictability in accuracy and performance is an often expressed requirement whose importance increases in the cluster environment.
Robustness in accuracy and performance addresses the issues associated with maintaining an expected level of accuracy or performance for changes in the computing environment, problem size, or other application parameters.
The library will acquire, at run-time, cluster and application information necessary to make decisions on which components and algorithms will best solve the underlying problem.The growing gap between the speed of microprocessors and memory technology, resulting in deep memory hierarchies imply that the memory subsystem is a critical performance factor posing increased challenges in the design and implementation of algorithm and software systems [38].
Novel latency tolerant algorithms that explores a wider range of the latency/bandwidth/memory space are needed.
We propose to identify algorithms and applications that would benefit by a latency tolerant approach and construct new algorithms where appropriate.
Our initial experiments show, for example, that up to 10x speedups are attainable by using different reduction algorithms and other communication primitives in the Basic Linear Algebra Communication Subroutines (BLACS) underlying ScaLAPACK [69,85] In a multithreaded execution, when a processor reaches a point where remote memory access is necessary, the request is sent out on the network and a context-switch occurs to a new thread of computation.
This effectively masks a long and unpredictable latency due to remote loads, thereby providing tolerance to remote access latency.
As a byproduct, we will develop standards to profile the degree of parallelism, granularity, precision, instruction set mix, interprocessor communication, latency etc.
These tools will develop and evolve as the cluster environment matures.For numerical libraries there are deeper issues relating to portability that we will address.
For example, while it is reasonable to assume IEEE arithmetic, hardware vendors provide variants on the standard resulting in the same computation producing different results when executed on different parts of the cluster [18].
Furthermore, repeatability is an issue for applications that are ill-conditioned or are sensitive to rounding errors.
We plan to develop high confidence algorithms and software that will address this issue.
The implementation strategies would allow the user to control the application to guarantee repeatability, perhaps at the expense of performance (Java's original goal was exact reproducibility, but backed away from this for floating point applications because of the performance penalties caused by different platforms having different width floating point registers, i.e. Intel and Sun).
To meet the above goals, we will develop• Parameterizable libraries.
We propose to design and construct libraries that are parameterized to allow their performance to be optimized over a range of current and future memory hierarchies, including the ones expected in computational clusters.
• Annotated libraries.
We will identify opportunities where information about algorithms contained in library functions can aid the compilation process and run-time environment.
At the library interface level, this includes memory-hierarchy tuning parameters and a performance model that depends on input parameters, such as problem size.
• Cluster environment.
We will pioneer new methods for packaging, distributing, and accessing algorithms and software technology for cluster applications.In summary, the new libraries will speed transfer of recent algorithmic technology in linear algebra, hierarchical methods, and other areas to large-scale computer users.
A new division of labor between compiler writers, library writers, and algorithm developers and application developers will emerge.
The capability of controlling large complex heterogeneous and dynamic applications over a distributed network will enable the assembly of multidisciplinary applications, parts of which are built by independent teams.
Our experience with automatics tuning of dense linear algebra with ATLAS [90] and sparse linear algebra with Sparsity [58,87] shows that their value arises from their ability to discover and use performance information by empirical methods that would otherwise be difficult to come by.
While many of the techniques used by ATLAS and Sparsity are specific to their particular operations, many others are more generally useful.
For instance, information about o the type and number of floating units, the pipeline length, the number TLB entries, etc. are all generally useful in code optimization.
During this stage of our work we plan to develop methodologies so we and other developers can query an ATLAS like system for this information in order to aid in developing automatic performance models for algorithm tuning and selection.There are a large number of tuning parameters that LAPACK and ScaLAPACK use.
There are over 1300 calls in LAPACK to the routine ILAENV which takes as input the name of the calling LAPACK subroutine (such as SGETRF which performs LU decomposition with partial pivoting) and parameters describing the size of the problem (such as the matrix dimension n) and returns a tuning parameter (such as a threshold n 2 , such that if n ≤ n 2 SGETRF will use an unblocked (BLAS2) routine, and if n > n 2 SGETRF will use a blocked BLAS3 routine.
The idea is that for small enough n the overhead of the BLAS3 routine outweighs the speeds and BLAS2 is faster.
Exactly where the n 2 threshold should be depends on the platform and BLAS implementation, and is best determined empirically.
We propose to develop an automatic tuning system analogous to ATLAS and Sparsity for this purpose.In the case of ScaLAPACK there are yet more parameters having to do with the number of processors, the dimensions determining the block cyclic layout of the data, and so on.
Tuning here (depending on the maximum available number of processors, not all of which we may want to use, and their computational and communication speeds) will require performance modeling to pick the best configuration.These are illustrations of the basic problem of designing numerical library software that addresses both computational time and space complexity issues on the user's behalf and in a manner as transparent to the user as possible.
The software intends to allow users to either link against an archived library of executable routines or benefit from the convenience of prebuilt executable programs without the hassle of properly having to resolve linker dependencies.
The user is assumed to call one routine from a serial environment while working on a single processor of the cluster.
The software executes the application.
If it is possible to finish executing the problem faster by mapping the problem into a parallel environment, then this is the thread of execution taken.
Otherwise, the application is executed locally with the best choice of a serial algorithm.
The details for parallelizing the user's problem such as resource discovery, selection, and allocation, mapping the data onto (and off of) the working cluster of processors, executing the user's application in parallel, freeing the allocated resources, and returning control to the user's process in the serial environment from which the procedure began are all handled by the software.
Whether the application was executed in a parallel or serial environment is presumed not to be of interest to the user but may be explicitly queried.
All the user knows is that the application executed successfully and, hopefully, in a timely manner.
Target systems are intended to be "Beowulf like".
There are essentially three components to the software: data collection routines, data movement routines, and application routines.Our software will dynamically make decisions on how to solve the user's problem by coupling the cluster state information with knowledge of the particular application.
Specifically, a decision is based upon the scheduler's ability to successfully predict that a particular subset of the available processors on the cluster will enable a reduction of the total time to solution when compared to serial expectations for the specific application and user parameters.
The relevant times are the time that is spent handling the user's data before and after the parallel application plus the amount of time required to execute the parallel application.
If the decision is to solve the user's problem locally (sequentially) then the relevant LAPACK routine is executed.
On the contrary, if the decision is to solve the user's problem in parallel then a process is forked that will be responsible for spawning the parallel job and the parent process waits for its return in the sequential environment.
The selected processors are allocated (in MPI), the user's data is mapped (block cyclically decomposed) onto the processors (the data may be in memory or on disk), the parallel application is executed (e.g. ScaLAPACK), the data is reverse mapped, the parallel process group is freed, and the solution and control are returned to the user's process.At runtime our software makes choices at the software and hardware levels for obtaining a best parameter set for the selected algorithm by applying expertise from the literature and empirical investigations of the core kernels on the target system.
The algorithm selection depends on the size of the input data and empirical results from previous runs for the particular operation on the cluster.
The overheads associated with this dynamic adaptation of the user's problem to the hardware and software systems available can be minimal.
Specifically, we will develop tools, libraries, networking protocols, and methodologies to facilitate:• Building, tuning, and testing of computational software libraries on multiple computing platforms and operating environments.
• Automatic cataloging of software libraries according to the target platforms and operating environments for which they were built.
• Accurate and precise identification of the characteristics of a computing platform and operating environment which affect the selection of computational software libraries.
• Automatically locating and incorporating appropriate versions of desired software components, into a program -where this binding may occur at either build time or run time.
• Distribution and location of software components in a way which preserves the integrity of those components and which utilizes existing distribution channels (e.g. mirrored web and ftp servers) with minimal disruption.
• Easy and nearly-automatic configuration of target platforms to support diverse applications for different computing environments.In order to facilitate development of software which is portable across a variety of computing platforms, we will provide a system for automatic compilation and testing of new or updated libraries.
This will consist of:• Facilities for submitting, over the Internet, new or updated libraries for compilation and testing,• An access control mechanism to prevent the use of this system by unauthorized users,• A set of computing platforms to which libraries can be submitted for compilation and testing, • A queuing system for submitting compilation and testing tasks to each platform and monitoring the progress of these operations,• A "sandbox" environment on each platform to allow for safe compilation and testing of untrusted codes without compromising the host environment,• Reporting of test results to the software author or maintainer, and• If compilation and tests are successful, automatic cataloging and filing of the compiled libraries.Our goals in establishing this system include: encouraging development of portable software libraries, encouraging development of testing methodologies as part of the development of software libraries, providing a mechanism by which libraries for diverse platforms can be automatically produced, and automatically cataloging libraries with sufficient precision to allow effective matching of libraries to target platforms.
Both PIs have a long and successful history of collaboration on LAPACK and ScaLAPACK, and will continue their past practices of periodic face-to-face meetings, design reviews, presentations of draft designs at conferences to get user inputs, and so on.
The difference between managing this effort and prior ones is our attempt to enlist the help of a number of outside people and organizations.
The ones listed below have agreed to help, and many of them have sent us letters of support which are attached to the proposal.
Support ranges from offering their software and ideas to help in programming and development (see the attached letters for details).
