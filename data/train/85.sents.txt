We introduce the maximum graph homomorphism (MGH) problem: given a graph G, and a target graph H, find a mapping ϕ : VG → VH that maximizes the number of edges of G that are mapped to edges of H.
This problem encodes various fundamental NP-hard problems including Maxcut and Max-k-cut.
We also consider the multiway uncut problem.
We are given a graph G and a set of terminals T ⊆ VG.
We want to partition VG into |T | parts, each containing exactly one terminal, so as to maximize the number of edges in EG having both endpoints in the same part.
Multiway uncut can be viewed as a special case of prela-beled MGH where one is also given a prelabeling ϕ : U → VH , U ⊆ VG, and the output has to be an extension of ϕ.
Both MGH and multiway uncut have a trivial 0.5-approximation algorithm.
We present a 0.8535-approximation algorithm for multiway uncut based on a natural linear programming relaxation.
This relaxation has an integrality gap of 6 7 0.8571, showing that our guarantee is almost tight.
For maximum graph homomorphism, we show that a 1 2 + ε0-approximation algorithm, for any constant ε0 > 0, implies an algorithm for distinguishing between certain average-case instances of the subgraph isomorphism problem that appear to be hard.
Complementing this, we give a 1 2 + Ω(1 |H| log |H|)-approximation algorithm.
Maximum Graph Homomorphism.
The input to the maximum graph homomorphism (MGH) problem consists of two graphs G = (V G , E G ) and H = (V H , E H ).
The objective is to find a mapping ϕ : V G → V H that maximizes the number of edges of G that are mapped to edges of H.
More formally, we want to maximize |{(u, v) ∈ E G : (ϕ(u), ϕ(v)) ∈ E H }|.
We will often refer to the mapping ϕ as a labeling, ϕ(u) as the label of u, and H as the label graph or target graph.
Let OPT (G, H) denote the value of an optimal solution.
Throughout, n will denote |V G | and k will denote |V H |.
We use variables u, v, w to denote vertices in V G and i, j, to denote vertices in V H .
We also consider a prelabeled version of maximum graph homomorphism where some of the nodes of G are already labeled, and we want to label the remaining vertices so as to maximize the objective function.
More precisely, in the prelabeled maximum graph homomorphism problem, in addition to the graphs G and H, we are given a prelabeling ϕ : U → V H where U ⊆ V G , and the goal is to find an extension ϕ of ϕ that maximizes|{(u, v) ∈ E G : (ϕ(u), ϕ(v)) ∈ E H }|.
In general, the label graph H may also contain self-loops.
However, note that if H has a self-loop, say at node i, then the unlabeled problem becomes trivial: we can simply map every vertex of G to label i to obtain OPT (G, H) = |E G |.
Thus, the problem with self-loops is only interesting in the prelabeled setting.The Multiway Uncut Problem.
In the multiway uncut problem, we are given a graph G = (V, E) and a set of k terminals T ⊆ V .
We want to find a partition of V into k subsets V 1 , . . . , V k such that each part V i contains a distinct terminal, so as to maximize the number of uncut edges, that is, the quantityk i=1 |{(u, v) ∈ E : u, v ∈ V i }|.
Notice that the multiway uncut problem is a special case of the prelabeled MGH problem, where the label graph H consists of k disconnected self loops and the prelabeling is a bijection ϕ : T → V H .
In this section, we consider the multiway uncut problem and present a 0.8535-approximation algorithm based on a natural linear programming (LP) relaxation.
The integrality gap of this relaxation is at least 6 7 0.8571, which shows that our guarantee is almost tight.
Since multiway uncut is a special case of the prelabeled maximum graph homomorphism problem, we will use the terminology of MGH for consistency: we have k labels i = 1, . . . , k, and the prelabeling ϕ is given by ϕ (t i ) = i for the i-th terminal t i ∈ T .
Note that we may assume that there are no edges between two labeled vertices since such edges contribute 0 to the value of any solution.
We consider the following LP relaxation.
We use u to index the vertices of G = (V, E), and i to index the labels.max (u,v)∈E i c i uv (MU-LP) s.t. i x i u = 1 for all u, x ϕ (t) t = 1 for all t ∈ T, c i uv = min(x i u , x i v ) for all (u, v) ∈ E(1)x i u , c i uv ≥ 0 for all u, v, i.Here x i u indicates if vertex u is assigned label i, and c i uv indicates if both endpoints of edge (u, v) are assigned label i.
The first constraint states that every node must be assigned a label, and the second enforces that this labeling is an extension of ϕ (i.e., the label of a terminal does not change).
The term i c i uv measures the similarity along edge (u, v).
Although (1) is not written as a linear constraint, it is easy to see that one can encode (1) using linear constraints.
One can show that the LP relaxation (MU-LP) is identical to the relaxation introduced by Calinescu et al. [5] for the multiway cut problem, i.e., any solution of value Val to (MU-LP) is a solution of value |E| − Val to the relaxation in [5].
For the multiway cut problem, Calinescu et al. showed that the integrality gap of the relaxation is at most 1.5 − 1 k , which was improved to 1.3438 [23], whereas Freund and Karloff [16] showed that the integrality gap is at least 8 7+1/(k−1) .
Our result shows that the integrality gap of (MU-LP) (which is now less than 1) is at most 0.8535, that is, there is always an integer solution of value at least 0.8535 times the optimum of (MU-LP).
This also holds in the weighted setting (non-negative edge weights) where the goal is to maximize the weight of the uncut edges.
The integrality-gap example in [16] also yields an integrality gap of 6k 2 −10k+4 7k 2 −13k+6 → 6 7 0.8571, as k → ∞, for (MU-LP) (with weighted edges).
Thus, our guarantee is very close to the best possible using this LP relaxation.A similar LP relaxation was used by Kleinberg and Tardos [24] for the uniform labeling problem.
We will use a randomized rounding procedure from [24], but we will need a more refined analysis of this procedure than that in [24].
The algorithm is simple: we return the better of the following two labelings.1.
The first labeling picks an arbitrary label i, and sets ϕ(u) = i for every vertex u ∈ T .
We call this the "trivial labeling".
2.
The second labeling is obtained via the randomized rounding procedure of Kleinberg and Tardos, which we describe below for completeness.
They also show how to derandomize the rounding, so we could use this and obtain a deterministic algorithm with the same performance guarantee.
We consider the randomized version for ease of exposition and analysis.
Let {x, c} be an optimal solution to (MU-LP).
The rounding proceeds in several rounds.
Initially all vertices in V \ T are unassigned.
In each round, we independently pick a label i ∈ {1, . . . , k} uniformly at random, and a threshold ρ uniformly in [0,1].
For each unassigned vertex u ∈ V , we assign u the label i (i.e., set ϕ(u) = i) if x i u ≥ ρ.
We repeat this until all the vertices in V \ T are assigned.
We call this the "LP labeling".
Analysis.
Let C uv = i c i uv .
We analyze the algorithm by considering a "hybrid labeling", where we choose the LP-labeling with probability λ and the trivial labeling with probability 1−λ, for some λ ∈ [0, 1].
We will compare the expected contribution of an edge (u, v) in the hybrid labeling against the LP-value C uv .
Let E 0 = {(u, v) ∈ E : u, v / ∈ T } and E 1 = {(u, v) ∈ E : u or v ∈ T }.
Note that E = E 0 ∪ E 1 since there are no edges with both endpoints in T .
The trivial labeling obtains a value of 1 for every edge in E 0 .
We now analyze the LP-labeling.
For an edge (u, v), let X uv denote the random variable that is 1 if u and v are assigned the same label in the LP-labeling, and 0 otherwise.
We will use "u → i" and "u → * " as a shorthand to denote that "u is assigned label i", and "u is assigned some label" respectively.
Let X i u be a random variable that is 1 if u → i in the LP-labeling, and 0 otherwise.
x i u .
Therefore Pr[u → * in the round] = 1 k · i x i u = 1 k .
Claim 3.2.
Pr[X i u = 1] = x i u .
Thus, for an edge (u, v) ∈ E 1 , E X uv = C uv .
Proof.
Pr[X i u = 1] = ∞ r=1 1−Pr[u → * before round r] ·Pr[u → i in round r] = ∞ r=1 1 − 1 k r−1 · x i u k = x i u .
For an edge (u, v) ∈ E 1 , where v ∈ T has label i, we have E X uv = Pr[X i u ] = x i u = c i uv = C uv .
Lemma 3.3.
For an edge (u, v) ∈ E 0 , we have E X uv ≥ Cuv 2−Cuv .
Proof.
We can lower bound E X uv by the probability that both u and v are assigned a label in the same round.
Observe that if both u and v are unassigned before a given round, then (a) the probability that u and v are both assigned in the round is 1 k i min(x i u , x i v ) = 1 k ·C uv , and (b) the probability that u or v is assigned in the round is 1 k k i=1 max(x i u , x i v ) = 1 k ·(2−C uv ), since i min(x i u , x i v )+ max(x i u , x i v ) == ∞ r=1 1 − 2 − C uv k r−1 · C uv k = C uv 2 − C uv .
Fact 3.1 and Claim 3.2 were proved in [24], but for edges in E 0 their analysis proves the weaker bound E X uv ≥ 1 − x u − x v 1 = 2C uv − 1 which only yields a 2 3 -approximation guarantee for the overall algorithm.
1 2 + √ 2 4 · (u,v)∈E C uv .
Thus the approximation ratio of the above algorithm is at least 1 2 + √ 2 4 0.8535.
Proof.
We prove the stated bound for the expected value of the random hybrid labeling; the theorem then follows.
For an edge (u, v) ∈ E 0 , we get an expected value of (at least) Cuv 2−Cuv in the LP-labeling by Lemma 3.3, and 1 in the trivial labeling.
So the expected contribution of this edge in the hybrid labeling is at leastC uv · λ 2−Cuv + 1−λ Cuv ≥ 1 2 + λ(1 − λ) C uv .
The last inequality follows since min C∈[0,1] λ 2−C + 1−λ C ≥ 1 2 + λ(1 − λ)by simple calculus.
For an edge (u, v) ∈ E 1 , using Claim 3.2, the (expected) contribution in the hybrid labeling is at least λC uv .
Therefore the expected total value of the hybrid labeling is at least min λ,1 2 + λ(1 − λ) · (u,v)∈E C uv .
Taking λ = 1 2 + √ 2 4 = 1 2 + λ(1 − λ) 0.8535 maximizes this expression and yields a solution of value at least 0.8535 · (u,v)∈E C uv .
As mentioned earlier, the rounding procedure can be derandomized to yield a deterministic algorithm with the same guarantee.Extensions.
We can also handle the weighted case where we have non-negative weights on the edges and we want to maximize the weight of the uncut edges.
The algorithm remains unchanged and the analysis requires only notational changes.
One can also consider the problem where we have non-negative profits {p i u } for assigning label i to node u, and we want to maximize the sum of the profits and the weight of the uncut edges.
This problem is the complement of the uniform labeling problem considered in [24].
We can reduce this to the no-profit setting by adding an edge (u, i) with weight p i u for every node u ∈ V and label i.
We now consider the maximum graph homomorphism (MGH) problem (with an arbitrary label graph H).
Recall that we are given graphs G and H, and the goal is to find a mapping ϕ : V G → V H that maximizes the number of edges of G mapped to edges of H.
In Section 4.1, we give a 1 2 + Ω( 1 k log k ) -approximation algorithm (where k = |V H |) for this problem.
In Section 4.2, we present some evidence suggesting that obtaining a 1 2 + Ω(1) -approximation algorithm may be inherently difficult.
We argue that such an approximation algorithm would yield an algorithm for distinguishing between certain average-case instances of the subgraph isomorphism problem.
In Section 4.3, we consider some extensions and refinements.
We show that any approximation guarantee for the unlabeled problem yields a corresponding guarantee for prelabeled MGH.
We also obtain a quasi-PTAS for the problem on dense graphs G (i.e., |E G | = Ω(|V G | 2 )).4.1 A 1 2 + Ω( 1 k log k ) -Approximation AlgorithmWe now present the 1 2 + Ω( 1 k log k ) -approximation algorithm.
Recall that k = |V H |.
We assume that H contains at least one edge and has no self-loops (otherwise the problem is trivial).
We start with some simple observations.
Observe that any cut (U, V G \ U ) of G yields a labeling ϕ of value equal to the size of the cut, since we can consider any edge (i, j) ∈ E H and map all the nodes in U to i, and all the nodes in V G \ U to j. Thus, since one can easily obtain a cut of value at least |E G | 2 (e.g., by using the greedy, or randomized, algorithm where we assign each vertex greedily, or independently and uniformly at random, to one of the two parts), there is a trivial 0.5-approximation algorithm for the maximum graph homomorphism problem.
Conversely, for bipartite graphs H, one can show that MaxCut(G) = OPT (G, H).
Fact 4.1.
Any cut of G yields a mapping ϕ of value equal to the size of the cut.Thus, OPT (G, H) ≥ MaxCut(G) ≥ |E G | 2 .
Claim 4.2.
If H is bipartite, the MGH problem on graphs G and H is equivalent to the Maxcut problem on G, that is, MaxCut(G) = OPT (G, H).
We improve upon this factor of 0.5 for any fixed graph H, by using a result of Charikar and Wirth [6].
They used the semidefinite program for Maxcut in [18], along with the RPR 2 rounding technique of [15] to obtain the following theorem.
Notice that the algorithm mentioned in the above theorem always returns a cut of value at least |E G | 2 .
Our algorithm for MGH simply uses the algorithm mentioned in Theorem 4.3 to obtain a cut of G; this induces a labeling of the same value and the algorithm returns this labeling.
The idea behind the algorithm is that if OPT (G, H) is small compared to |E G |, then |E G | 2 would be strictly larger than OPT (G,H) 2 .
Otherwise, we will show that there exists a bipartite subgraph H of H that captures more than half the edges of G, which in turn implies that G has a cut of value strictly larger than |E G | 2 .
Thus, using Theorem 4.3 we obtain a cut of G, and hence a labeling, of value strictly larger than|E G | 2 ≥ OPT (G,H) 2 .
Theorem 4.4.
There is a 1 2 + c k log k -approximation algorithm for MGH, where c > 0 is a constant independent of k.Proof.
Let G and H be the input graphs.
IfOPT (G, H) ≤ |E G | 1− 1 2k, then our algorithm returns a solution of value at least|E G | 2 ≥ OPT (G,H) 2(1−1/2k) ≥ OPT (G,H) 2 1 + 1 2k .
So suppose that OPT (G, H) ≥ |E G | 1 − 1 2k .
Consider an optimal mapping ϕ * .
For each edge (i, j) in H, let m ij = (u, v) ∈ E G : {ϕ * (u), ϕ * (v)} = {i, j} .
Thus, OPT (G, H) = (i,j)∈E H m ij .
We claim that there is a bipartite subgraphH of H such that OPT (G, H ) ≥ (i,j)∈H m ij ≥ |E G | 2 1 + 1 4k .
Consider the cut (U H , V H \ U H )where U H is a random subset of vertices of H of size k/2.
The probability that an edge is cut by such a partition isk 2 4 / k 2 = 1 2 1 + 1 k−1.
Therefore, the expected weight of the cut edges is(i,j)∈E H m ij · 1 2 1+ 1 k−1 = OPT (G,H) 2 1 + 1 k−1 ≥ |E G | 2 1 + 1 4k.
Thus, there exists such a partition of at least this value, and we can take H to be the associated bipartite subgraph of H.
Now by Claim 4.2, G must have a cut of value at least |E G |2 1 + 1 4k.
So applying Theorem 4.3, our algorithm finds a cut, and hence a labeling, of value at least|E G | 1 2 + c k log k .
The theorem follows since OPT (G, H) ≤ |E G |.
Given two graphs G and H, the subgraph isomorphism problem is the problem of deciding whether G is a subgraph of H.
The subgraph isomorphism problem is a well-known NP-complete problem.
We show that a ( 1 2 + ε 0 )-approximation algorithm for MGH, where ε 0 > 0 is an absolute constant, implies an algorithm for distinguishing between certain average-case instances of the subgraph isomorphism problem (this is defined precisely below).
This hints at an inherent difficulty in obtaining an approximation ratio better than 0.5 for MGH.The main technical result of this section (Lemma 4.5) is as follows.
For any > 0, if H is a triangle-free graph, and G is a random graph drawn from the distribution G n,p , for a suitable p = p() ∈ [0, 1] and large enough n, then OPT (G, H) ≤ |E G | 2 (1 + ) with high probability.
If however G is a subgraph of H, then OPT (G, H) = |E G |.
The gap between these two cases motivates the definition of a refutation problem for certain average-case instances of the subgraph isomorphism problem, which allows us to encode the difficulty of obtaining a better than 0.5-approximation algorithm for MGH.
Let n be the set of all triangle-free graphs on n vertices.
For p ∈ [0, 1], let n,p be the distribution over G ∈ n obtained by choosing a random graph G ∈ G n,p , and then considering the edges of G in a random order and deleting any edge that is part of a triangle.Refutation problem (with parameter c > 0).
Find a polynomial time algorithm A such that given a pair of random graphs G ∈ n,p G , H ∈ n,p H , where p G = c ln n n , p H p G , (a) A returns "yes" if H contains G as a subgraph, and (b) A returns "no" on most instances, more precisely Pr G,H [A(G, H) = "no"] ≥ 1 2 .
Intuitively, the refutation algorithm A refutes most tuples (G, H) as being "no" instances of the subgraph isomorphism problem, but always announces "yes" when G is a subgraph of H.
As mentioned earlier, with very high probability G will not be a subgraph of H, thus conditions (a) and (b) do not conflict.
We will show that a 1 2 + ε 0 -approximation algorithm for MGH yields such a refutation algorithm; thus the non-existence of such an algorithm implies that MGH cannot be approximated to a factor better than 0.5.
We mention a few remarks.
First, one could also define the refutation problem in terms of an approximation version of subgraph isomorphism by requiring (a'): A always return "yes" if G contains a subgraph of size |E G |(1 − ) that is isomorphic to a subgraph of H.
Such a modification was also considered by Feige (see Hypothesis 2 in [14]).
An algorithm satisfying (a'), (b) refutes average-case instances of the maximum common subgraph problem [22], and is also a refutation algorithm for the exact-version of the problem.
Thus, the non-existence of an algorithm satisfying (a'), (b) is a weaker hardness assumption (implying a ( 1 2 +ε 0 ) inapproximability for MGH).
Moreover, this version of the refutation problem might be more robust than the exact-version.
Second, we take p H p G to avoid the case where p H p G .
In this setting, the problem is closely related to the graph isomorphism problem on random graphs, which is known to be solvable on average in polynomial time; see [4], and §6 of the survey [17] and its references.Lemma 4.5.
For any ∈ (0, 1), there exist constants n 0 (), c 0 (), such that if G = (V G , E G ) is a random graph in G n,p , where n ≥ n 0 (), p = c ln k n , c ≥ c 0 (e), and H = (V H , E H ) is a simple triangle-free graph with k vertices, then (i) OPT (G, H) < cn ln k 4 (1 + /2) with probability at least 1 − e −n ln k , and(ii) OPT (G, H) < |E G | 2 (1 + ) with probability at least 1 − 2e −n ln k .
Proof.
Set n 0 () = 8 , c 0 () = 2048 7 2 .
Let m = p n 2be the expected number of edges in G. Fix a mapping ϕ : V G → V H .
We will show that with very high probability, mapping ϕ has value at most m 2 (1 + /2).
Applying the union bound over all mappings then yields that OPT (G, H) < m 2 (1 + /2) with high probability, proving part (i).
Since |E G | is strongly concentrated around its expectation, this will also prove part (ii).
Given the mapping ϕ, consider the following graph H : H also has n = |V G | vertices, and we include an edge (u, v) in H iff (ϕ(u), ϕ(v)) is an edge in H.
It is easy to see that H is also triangle-free: a triangle (v 1 , v 2 , v 3 ) in H implies that H has edges (ϕ(v 1 ), ϕ(v 2 )), (ϕ(v 2 ), ϕ(v 3 )), and (ϕ(v 3 ), ϕ(v 1 )), and therefore contains a triangle.
Since H is triangle-free, by Turán's Theorem [25] it has at most n 2 4 edges.
Let X(ϕ) denote the (random) value of the mapping ϕ for G. Observe that X(ϕ) is simply the number of edges of H that are also edges of G. For every pair u, v, (u, v) is in E G with probability p, so we haveE X(ϕ) = p · |E H | ≤ p · n 2 4 = cn ln k 4 .
Since X(ϕ)is the sum of independent indicator random variables, using Chernoff bounds, we get PrX(ϕ) ≥ cn ln k 4 (1 + /2) ≤ e −( 2 cn ln k)/48 ≤ e −2n ln k .
The number of mappings ϕ is k n .
So by the union bound, PrOPT (G, H) ≥ cn ln k 4 (1 + /2) = Pr ∃ϕ, X(ϕ) ≥ cn ln k 4 (1 + /2) ≤ e −n ln k .
The expected number of edges in G is p n 2 = cn ln k 2 (1 − 1 n ) ≥ cn ln k 2 (1 − 8 ).
Again using Chernoff bounds, we get that Pr|E G | ≤ cn ln k 2 (1 − /4) ≤e −(7 2 cn ln k)/2048 ≤ e −n ln k .
So using part (i), with probability at least 1−2e −n ln k it is the case that OPT (G, H) < cn ln k 4 (1 + /2) < |E G | 2 (1 + ).
Proof Sketch.
Let G and H be the two input graphs.
Let n be sufficiently large.
If we are in case (a), then OPT (G, H) = |E G |, so running A on (G, H) will produce a solution of value at least |E G | 1 2 + ε 0 .
Otherwise, we can use Lemma 4.5 to show that that OPT (G, H) < |E G | 1 2 + ε 0 with high probability; thus, one can use A to distinguish between the two cases.
Let G by obtained by deleting edges from G ∈ G n,p .
Lemma 4.5 shows that OPT (G, H) ≤ OPT (G , H) < cn ln n 4(1 + ε 0 /2) and |E G | ≥ cn ln n 2(1 − ε 0 /4), with high probability.
Although we delete edges from G , with high probability, the number of triangles in G is a negligible fraction of |E G |.
So we obtain that |E G | ≥ cn ln n 2 (1 − ε 0 /2) and therefore we have OPT (G, H) < |E G | 1 2 + ε 0 .
Prelabeled MGH.
Recall that in prelabeled MGH, we are given a prelabeling ϕ : U → V H , U ⊆ V G and the output has to be an extension of ϕ .
We can show that for any label graph H, an α-approximation algorithm for MGH on instances (G, H) (α could depend on H) gives an α 1+α -approximation algorithm for prelabeled MGH on instances (G, H).
Dense Graphs G.
We obtain much better results when G is dense, i.e., when |E G | = Ω(n 2 ) (n = |V G |).
One can adapt the techniques of Arora, Karger and Karpinski [3] to obtain a solution ϕ of value OPT (G, H) − n 2 in time O (nk) log k/ 2 (although MGH does not directly fall into the problem-class detailed in [3]).
Since OPT (G, H) ≥ |E G | 2 = Ω(n 2 ), we can obtain a quasi-PTAS by setting suitably.
This also yields a PTAS for any fixed graph H.Special graphs H.
When H if bipartite, by Claim 4.2 it follows that one can obtain a 0.878-approximation algorithm for MGH using the Maxcut algorithm of Goemans and Williamson [18].
One can also obtain an approximation ratio better than 0.5 if H has a dense subgraph.
Let ρ H = max U ρ(U ), where ρ(U ) = 2|{(u, v) ∈ E H : u, v ∈ U }| /|U | 2 .
Let U * ⊆ V H be such that ρ(U * ) = ρ H .
The randomized algorithm that maps each node of G to a node of U * chosen uniformly at random, returns a solution of expected value ρ(U * )|E G | and is thus a ρ H -approximation algorithm.
This immediately implies an approximation ratio of at least 2/3 if H contains a triangle.
