This paper focuses on 'user browsing graph' which is constructed with users' click-through behavior modeled with Web access logs.
User browsing graph has recently been adopted to improve Web search performance and the initial study shows it is more reliable than hyperlink graph for inferring page importance.
However, structure and evolution of the user browsing graph haven't been fully studied and many questions remain to be answered.
In this paper, we look into the structure of the user browsing graph and its evolution over time.
We try to give a quantitative analysis on the difference in graph structure between hyperlink graph and user browsing graph, and then find out why link analysis algorithms perform better on the browsing graph.
We also propose a method for combining user behavior information into hyper link graph.
Experimental results show that user browsing graph and hyperlink graph share few links in common and a combination of these two graphs can gain good performance in quality estimation of pages.
Web page quality estimation is considered as one of the major challenges for Web search engines [1].
For contemporary search engines, estimating page quality plays an important role in crawling, indexing and ranking processes.Currently, the task of page quality estimation is usually based on hyperlink structure analysis of the Web.
The success of PageRank [2] and other hyperlink analysis algorithms such as HITS [3] prove that it is possible to evaluate the importance of a Web page query-independently.
In these analyses, two basic assumptions are concluded by [4]: recommendation assumption and topic locality assumption.
It is assumed that if two pages are connected by a hyperlink, the page linked is recommended by the page which links to it (recommendation) and the two pages share a similar topic (locality).
Hyperlink analysis algorithms adopted by both commercial search engines (such as [5]) and researchers (such as [6]) all rely on these two assumptions.
However, contemporary WWW is filled with spam links and advertising links so the assumptions as well as the consequent algorithms don't work very well in this new situation.Recently, the wisdom of the crowd is paid much attention in Web search researches, e.g. [7], [8] and [9].
In their work, users' browsing behavior is usually considered as implicit feedback information for page relevance and importance.
For example, Liu et.
al. constructed 'user browsing graph' with search log data [10].
They proposed a page importance estimation algorithm called BrowseRank which performs on the user browsing graph.
It is believed that the link structure in user browsing graph is more reliable than hyperlink graph because users actually follow links in the browsing graph.Liu's initial study shows that the BrowseRank algorithm works better than hyperlink analysis algorithms such as PageRank and TrustRank [10].
However, there remain many questions to be answered for the user browsing graph.
We can infer that user browsing graph is different from the hyperlink graph in some aspects, but how the structures of these two graphs differ from each other?
How the user browsing graph evolves over time and even every day?
We've known that BrowseRank performs better than PageRank and TrustRank algorithms when the latter two algorithms are performed on hyperlink graph.
Then on the other hand, how these algorithms perform on the user browsing graph?
1 In this paper, we look into the structure and the evolution of the user browsing graph.
And furthermore we study how traditional link analysis algorithms perform on the user browsing graph.
We try to answer the following questions: How does the user browsing graph differ from the hyperlink graph?
How many edges are overlapped, reduced and complemented, respectively?
How does the user browsing graph evolve over time?
How much data are necessary to build a stable graph structure and how the graph changes day by day?
3.
How traditional hyperlink analysis algorithms perform on the user browsing graph?
Do they perform better on the browsing graph than on the hyperlink graph?The rest of the paper is organized as follows: Section 2 introduces user behavior dataset.
Section 3 describes the structure and evolution of user browsing graph.
Experimental results of applying link analysis algorithms on user browsing graph are reported in Section 4.
Conclusions and future work are given in Section 5.
With the development of search engine, Web browser toolbars become more and more popular recently.
Lots of search engines develop toolbar software to attract more user visits, e.g. Google, Yahoo and Live search.
Web users usually adopt toolbars to get instant access to search engine services and to get browser enhancement such as pop-up window blocking and download acceleration.
In order to provide value-add services to users, most toolbar services also collect anonymous click-through information from users' browsing behavior.
Previous work such as [8] adopts this kind of click-through information to improve ranking performance.
Our previous work [10] proposed a Web spam identification algorithm based on this kind of user behavior data.
In this paper, we also adopt Web access logs collected by search toolbar because this kind of data sources collect user behavior information at low cost without interrupting users' browsing behavior.
Information recorded in these logs is shown in Table 1.
Table 1 we can see that no privacy information was included in the log data.
The information shown can be easily recorded using browser toolbars by commercial search engine systems.
Therefore it is practical and feasible to obtain these types of information and to apply them in the construction of user browsing graph.
With the help of a widely-used commercial Chinese search engine, Web access logs were collected from Aug.3 rd , 2008 to Oct 6 th , 2008.
Over 2.8 billion click-through events on 8.5 million Web sites were recorded in these logs.
User browsing graph is constructed with users' behavior data recorded in Web access logs.
We can use UG(V,E) to represent the user browsing graph in which V is the vertex set and E is the edge set.
The construction process can be described as follows:1.
{} {}, = = E V 2.
For each record in the Web access log, if the source URL is A and the destination URL is B, then; ) , ( ; 1 ) , ( )}, , {( ) , ( }; { , }; { , + + = ∪ = ∉ ∪ = ∉ ∪ = ∉ B A Weight else B A Weight B A E E E B A if B V V V B if A V V V A ifAfter the construction process, V includes all Web pages visited by users during the time period in which access logs were collected; and E records the users' browsing behaviors.
Each edge in E is also assigned a weight value which represents how many times Web users visit B from A. For each edge (A,B) from UG(V,E), there exists some user who visits Web page B by following links located on page A.
It means that UG(V,E) can be regarded as a subset of the hyperlink graph on the Web (named HG(V,E)), because the latter graph includes all hyperlinks and pages on the Web.
However, HG(V,E) is constructed with the information collected by Web crawlers and it is not possible for any crawler to collect hyperlink graph of the whole Web because it is too huge and changing so fast.
It is also not necessary to retain the whole graph structure because Web users can only view part of it due to limited time.
Therefore, it is likely that the HG(V,E) graph maintained by search engines may not contain all vertexes and edges in UG(V,E).
In order to find out the differences between the two graphs, we constructed site-level UG(V,E) with web access data from Aug.3 rd , 2008 to Sept.2 nd , 2008 (totally 30 days).
The vertex set contains 4,252,495 Web sites and the edge set contains 10,564,205 edges.
We constructed site-level instead of page-level UG(V,E) because we believe that the site-level graph is more stable and we also want to avoid the data sparsity problem.
With the help of the same search engine which collected Web access log for us, we obtained hyperlink graph constructed by them which contains hyperlink relations of over 3 billion Web pages.
Then we extracted a sub-graph for all the Web sites in V of UG(V,E).
The sub-graph is the hyperlink graph for these Web sites and we found that although the hyperlink graph contains almost the same Web sites as the user browsing graph, structures of these two graphs are significantly different.
Table 2 shows differences in the edge set.
Table 2 we can see that user browsing graph and hyperlink graph share only a small proportion of edges in common.
There are only less than 1/4 of pages in user browsing graph that also appear in hyperlink graph.
The percentage of common pages in HG(V,E) is only 1.86% because it has much more pages.
We can see from Table 2 that most (98.14%) of the links in the hyperlink graph are not actually clicked by users.
This can be explained by the fact that Web pages usually provide too many hyperlinks for the user to click.
We can also find that over 75% of the links in the user browsing graph are not included in the hyperlink graph.
When we look into these links, we found that most of these links come from user clicks on search engines.
Table 3 shows the number of search engine oriented edges in UG(V,E) that are not included in HG(V,E).
In [10], user browsing graph is used to calculate the importance of Web pages.
For practical applications, an important issue is whether the page importance scores calculated off-line can be adopted for on-line search process.
If pages needed by users are not included in the user browsing graph, it is impossible to calculate their importance scores and search engines may not be able to show these pages to users.
Therefore, it is important to find out how user browsing graph evolves over time.
The graph cannot cover all pages that are required by users because new pages appear from time to time.
However, it is acceptable if only a small proportion of newly-appeared pages are not included in user browsing graph.
Figure 1 shows evolution of the user browsing graph in the time period during which the dataset was prepared.In Figure 1, we show how V and E in UG(V,E) evolves over time.We can see that on the first day all edges and vertexes were newly-appeared because both V and E are empty sets.
From the second day to the 15 th day (approximately), the percentage of newly-appeared edges and vertexes drops day by day.
After the 15 th day, about 25% of the edges and 30% of the vertexes appeared each day are new ones to the user browsing graph.
During the first 15 days, the percentage of newly-appeared edges and vertexes drops because the structure of the browsing graph is more and more complete day by day.
At the fifteenth day, the browsing graph contains 6.12 million edges and 2.56 million vertexes.
From then on, the amount of newly-appeared edges and vertexes are relatively stable and about 0.3 million new edges and 0.1 million new vertexes appear each day.
Therefore, it takes about 15 days to construct a stable user browsing graph and after that, small part (less than 5%) of the graph changes each day.Because of the relatively small size of user browsing graph, daily reconstruction of the user browsing graph is possible.
It means that most of the pages that users need can be included in the graph.
According to Section 3.2, links oriented by search engine result pages make up large part of the browsing graph; therefore, we look into this part of graph to see whether it changes faster or slower than the other parts.
Figure 1 shows that without search engine oriented links, the percentages of newly-appeared edges/vertexes rise up to 30% and 35%, respectively.
It means user clicks from search result pages don't change as fast as other user clicks.
It is necessary to retain this kind of links in the browsing graph because they provide valuable information (See Section 3.2) and they compose a relatively stable part of the graph.
From previous work in [10], we can see specifically-designed link analysis algorithm (named BrowseRank) can perform better on user browsing graph in page quality estimation than state-of-theart link analysis algorithms which performs on hyperlink graphs.
We want to find out whether this improvement comes from algorithm design or the differences in graph structure.
We also want to know how hyperlink analysis algorithms such as PageRank [2] and TrustRank [11] perform on user browsing graph.
If these algorithms perform better on user browsing graph, it is possible that browsing graph can replace hyperlink graph for page quality estimation because hyperlink graph is much huger in size and has brought much difficulty in computation complexity and information storage.
We didn't compare the performance of BrowseRank with PageRank and TrustRank on user browsing graph because we are not able to obtain stay time information (which is necessary for BrowseRank calculation) from our Web access logs described in Section 2.
In order to answer these questions, we build four Web graphs and compares how PageRank and TrustRank algorithms perform on them.
Details of these four graphs are shown in Table 4.
Constructed with over 3 billion pages (all pages in a certain search engine's index) and all hyperlinks among them Vertexes are from UG(V,E).
Edges among them are extracted from hyperlink relations in whole-HG(V,E).
Vertexes are from UG(V,E).
Edges among them are from UG(V,E) combined with those from extracted-HG(V,E).
After performing PageRank and TrustRank algorithms on these four graphs, we sampled 1680 Web sites according to user visit count and had 2 assessors annotate their quality scores.
About 39% of these sites are annotated as "high quality"; 19% are "spam" and the others are "ordinary".
After the annotation, we choose ROC curves and corresponding AUC values to evaluate the performance of link analysis algorithms.
It is a useful technique for organizing classifiers and it is adopted by several quality estimation researches such as [12].
Table 5 and 6 show the performances of link analysis algorithms on different graphs.
From Table 5 and 6 we can find several interesting results.
The first is that among the four link graphs, link analysis algorithms performs the worst on whole-HG(V,E) (the whole hyperlink graph).
This experimental result accords with the conclusion in [10] that link analysis algorithm performs better on user browsing graph than on the whole hyperlink graph.
Another finding is that the sub-graph of whole-HG(V,E) (extracted-HG(V,E)) is better at identifying both high quality and spam pages than the whole graph.
Whole-HG(V,E) is composed of much more hyperlinks than extracted-HG(V,E) but it results in performance loss in page quality estimation.
This may be explained by the fact that extracted-HG(V,E) can be regarded as the user-accessed part of Whole-HG(V,E).
It shares the same vertex set of UG(V,E) and therefore reduces possible noises in the whole hyperlink graph.
Another result is that although UG(V,E) and extracted-HG(V,E) share only a small proportion of edges in common (see Table 2), they get similar performances in page importance estimation.
A combination of UG(V,E) and extracted-HG(V,E) which contain all edges in them (CG(V,E)) result in best results.
It means both user browsing graphs and hyperlink graph can provide useful information that the other graph doesn't contain.
From these results, we find that information recorded in user browsing graph is important for page quality estimation.
Vertex set of UG(V,E), extracted-HG(V,E), CG(V,E) is the same and it is the user accessed part of Web.
Focusing in this part of Web can reduce possible noises in hyperlink graph.
As for the edge set, both hyperlink relations and user browsing relations are important and a combination can improve performance.
In this paper, we look into structure and evolution of user browsing graph.
First, quantitative analyses have been made to show the differences between the hyperlink graph and the user browsing graph in terms of structure and links.
Second, the evolution of the graph has been studied, and experimental analysis shows that about 15 days data is enough for constructing a stable graph.
Furthermore, the performances of traditional hyperlink analysis algorithms are studied in the user browsing graph.
And finally, we proposed a combination algorithm to integrate hyperlink relation and user browsing relation to build a novel web page structure, which improves the performance of page quality estimation.
Future study will focus on improving search effectiveness and efficiency with link analysis algorithms performing on user browsing graphs.
