"No one is so brave that he is not disturbed by something unexpected."
Julius Caesar The operating system is increasingly regarded as un-trustworthy.
Applications, hardware, and hypervisors are erecting defenses to insulate themselves from the operating system.
This paper explores the potential benefits if operating systems simply embraced these lowered expectations and deliberately varied API behavior.
We argue that, even for trusted or benign applications, diversity roughly within the specification can improve resilience to attack and improve robustness.
Malicious software tends to be brittle; a preliminary case study indicates that, for software of questionable origin, a somewhat hostile operating system may do more good than harm for system security.
This paper describes the architecture of Chameleon, an ongoing project to implement spectrum-behavior as an operating system feature.
A common goal of modern operating systems is to be predictable.
This improves compatibility among different instances of the system, including older programs running on newer systems.
But predictability poses problems.
Predictability allows vulnerabilities that are exploitable on one system to be exploitable on all systems of that type.
The paucity of different OS implementations is one facet of the software "monoculture" problem.One specific, limited form of unpredictability is diversity.
The intent of diversity is independence, which means that multiple instances yield the same result, but in such a way that the only common factor is the inputs.
One example is N-version programming [1], in which multiple teams create different software implementations to perform the same actions.
The results are considered more reliable when multiple versions produce the same results.
Most fault-tolerant system designs require sufficient software diversity that faults are independent, and can be masked by voting or Byzantine protocols [2,3].
At the system level, approaches to diversity generally involve randomness.
For example, address space layout randomization (ASLR) randomizes the placement of pages of a program in memory during execution.
A return-to-libc or ROP attack may fail when the attack relies on a buffer overflow causing a branch to a library function or gadget, as the address of that target will vary among instances of an operating system.
But this randomization is often insufficient against knowledgeable attackers.
A recent paper [4] demonstrated how, even without specific knowledge of the ASLR of a web server, one can quickly identify and exploit buffer overflows in it.
The technique relied on the fact that systems are commonly configured to restart daemons such as web servers automatically, and that ASLR implementations do not rerandomize the address space after restarting.
As a result, an attacker can incrementally explore the address space and probe application behavior.
Although fixes to ASLR may mitigate this specific attack, the underlying lesson is that diversity without unpredictability is not enough.
There is enough residual certainty that adversaries can craft attacks that will work reliably across multiple instances of a predictably diverse system.Strategies for less predictable operating systems are constrained by concerns for efficiency and reliability.
Yet consider what "efficient" and "reliable" mean for an operating system.
An operating system's job is to manage tasks that the system is authorized to run, "authorized" meaning "in conformance with a security policy."
For unauthorized tasks, such as those an attacker would execute to exploit vulnerabilities or otherwise misuse a system, the operating system should be as inefficient and unreliable as possible.
So for "good" users and uses, the operating system should work predictably; but for "bad" users or uses, the system should be unpredictable.
The latter case eliminates efficiency and reliability.
An extension is a spectrum of predictability, so that the less actions conform to the security policy, the more unpredictable the results of those actions should be.
This paper explores the benefits and feasibility of making OS APIs less predictable on a spectrum from diversity within the specification to active deception of dodgy software.
We argue that software robustness can actually be improved by being developed on a spectrumbehavior operating system.
Even within POSIX, mature, portable software packages already handle considerable variations in system call behavior.
Most of this maturity is the product of testing and bug reports across many platforms.
Moreover, hardware, compiler, and hypervi-sor tools to protect the application from a malicious OS are rapidly evolving [5][6][7][8].
Rather than require a software developer to manually test the software on multiple platforms, the development process could be facilitated by easily generating a range of different behaviors to test the software on-running the same test suite against different operating system behaviors.
In essence, the operating system is a chameleon, taking on attributes appropriate to the user and use to which it is put.Underlying this idea is the observation that systems tend to be fixed and do not adapt well to new conditions.
A motivated attacker can bring great resources to find attack variations that will succeed.
Despite the explosion of security software, malware remains at an average of 125 lines of code [9].
Thus, a "holy grail" of system design is the ability for the system to adapt with considerably less effort than the attacker must expend to explore the new system variants.
Unpredictable behavior can be such a mechanism for active defense against an attacker.Section 2 examines deception and diversity as mechanisms for introducing unpredictability into OSes.
Section 3 presents preliminary results that indicate varying OS behavior affects malware, which loses data and functionality.
Section 4 describes the design of Chameleon, a system that combines inconsistent and consistent deception with software diversity to provide a mechanism for active defense of computer systems and herd protection.
Chameleon leverages recent work that pushes an increasing portion of system code to user level [8,[10][11][12][13][14][15][16][17][18] as a means to more quickly and easily mix-and-match system behavior transparently to the application.
This section summarizes how deception and diversity have been used previously in software design, and highlights under-studied areas.
The ability to diversify behavior within a system is an essential building block for unpredictability.
We define the distinction between diversity and unpredictability as whether the variations stay within the API specification.One approach to diversifying software is at compile time.
Several projects [19,20] randomize selection of instructions at compile time, breaking unnecessarily predictable sequences of potentially-exploitable instructions.
Each instance of a system binary will have different, but functionally-equivalent instruction sequences.
Compile-time techniques can improve diversity, but cannot adapt to changing attacks.In addition to ASLR, several proposals have dynamically diversified other aspects of application behavior at runtime.
Several projects mitigate buffer overflows and other memory errors by randomizing system call mappings, global library entry points, stack placement, stack direction, and heap placement-often in conjunction with running multiple versions in parallel to detect divergence [21][22][23][24][25].
Holland et al. [26] proposed a strategy to randomize the ISA of a virtual environment, undermining portability of attacks leveraging lowlevel features, such as code injection attacks.
The Synthetix project [27] specialized code dynamically using automatic compiler analysis and programmer annotations, primarily to improve performance; specialization has been proposed as a mechanism to block attacks [28].
Program slicing has also been used to bound the cost and complexity of automatic diversification [29].
Finally, several projects have combined existing diverse implementations of file systems [30], databases [31], and language implementations [32].
As discussed above, dynamic diversity reduces predictability but is often limited to easily-randomized features of software.Although the focus of this work is not on diversity, we observe that much of the needed infrastructure for both diversity and deception are already being developed for other purposes.
Recent library OS designs [8, 10-13, 16,18] high-performance I/O systems [15][16][17], and other hardware access techniques [14] facilitate migration of kernel APIs into the application itself, in some cases implemented in higher level languages [13].
With some disciplined modularization of library OS subsystems, the otherwise daunting task of multi-version programming can be made feasible-a few hundred or thousand lines per component, possibly in different languages.
Our vision is to mix-and-match different implementations of different components, such that one can run many instances of an application, such as a web server, and only a few of instances will share the same combinations of vulnerabilities.
When the implementation effort is smaller and well-defined, a single graduate operating system course could easily generate dozens of functional implementations of each subsystem.
The art of deception has been successfully used in warfare for thousands of years.
Strategists such as Sun Tzu, Julius Caesar, and Napoleon Bonaparte advocated the use of deception as a way to confuse and stall the enemy, sap their morale, and decrease their maneuverability [33][34][35][36][37][38].
To a limited extent, deception has been an implicit technique for cyber warfare and defense.
The best known example is Cliff Stoll's use of deception to keep an intruder on an international telephone line for several hours, downloading a bogus but interesting file [39].
The authorities were able to trace the call, and broke up a spy ring.
Cheswick's response to Berferd is another classic in this area [40], and foreshadowed much of the honeypot work [41,42].
Zhao and Mannan [43] employed deception in system authentication by giving adversaries access to fake accounts in cases of password brute force attacks.
Sandboxes and virtual machines limit the actions of the attackers while giving the appearance of unfettered access to resources.Consistent deception strategies make the deceiver's system appear as indistinguishable as possible from another, real system.
The attacker does not perceive the deception and believes in a consistent false reality.
Stoll's actions were designed to make the attacker think he had found a system with classified documents on it.
Cheswick created a falsity of a system that was old, slow, and vulnerable.
Honeypots, honeynets, sandboxes, and virtual machines are designed to exhibit behavior consistent with production systems.Several technologies for providing deception have been studied.
Software decoys are agents that protect objects from unauthorized access [44][45][46][47][48] by creating a belief in the attacker's mind that the defended systems are not worth attacking or that the attack was successful.
The researchers considered tactics such as responding with common system errors and inducing delays to frustrate attackers.
The work used consistent deception.Red-teaming experiments at Sandia tested the effectiveness of network deception on attackers working in groups [49].
The network-level deception delayed attackers for a few hours, wearing down some groups to abandon the attack before the end of the experiments.Deception at the host level modifies system behavior when an attacker is logged in.
One implementation uses a wrapper that intercepts program execution requests and optionally runs a different program without the user detecting the switch [50].
But many command interpreters perform some of the requested actions directly, without invoking system calls and so bypassing the wrapper.Almeshekah and Spafford [51] further investigated the adversaries' biases and proposed a model to integrate deception-based mechanisms in computer systems.
Chameleon will extend this model, and investigate the utility of inconsistent deception.In all these cases, the fictional systems are predictable to some degree; they act as would real systems given the attacker's inputs.
Other inputs (such as hardware failures) introduce a degree of unpredictability with respect to the availability of the system, but do not affect the attacker's steps to compromise the system.True unpredictability requires randomness at a level that would cause the attacker to get inconsistent results, or inconsistent deception [52].
Neagoe and Bishop argued that an attacker will have no idea of whether she is exposed under a deception or a normal system that is malfunctioning, but will feel disoriented and may withdraw from the situation.
In this paper we present preliminary results from running keyloggers and botnets under inconsistent deception.
For instance, a keylogger in the inconsistent deceptive environment loses some keystrokes and records some false keystrokes.Iago attacks [53] are a good example of how such deception might work.
An Iago attack occurs when an untrusted system attacks a trusted program by returning system call results that the trusted program cannot robustly guard against-ultimately causing the trusted program to violate its security policies.
We believe similar techniques can be employed for active system defense.
Several papers have demonstrated that abnormal sequences of system calls or even function calls can indicate the presence of malware [54][55][56].
When malware is suspected, we propose to use Iago attack-style deception as active defense.
Somayaji and Forrest [57] proposed a model of delaying system execution as one means of active defense.
The Chameleon project aims to build a more powerful spectrum-behavior OS.
In this section, we show that common malware can be quite sensitive to relatively minor misbehavior by the operating system.
In this case study, many, these errors are often within the specification of the network or potential storage failure modes; a robust application would detect most issues with end-to-end checks [58] such as checksumming files, or, in other cases, checks designed to shield against a malicious OS, such as MAC checks on an encrypted socket.
A few cases, such as injected keystrokes, would be hard for any robust application to detect on current software stacks.Our preliminary study uses ptrace to interpose on system calls invoked by a keylogger and a botnet, introducing unpredictable behavior into their execution.
In these cases, the malware runs without crashing, but some I/O is corrupted.
We selected candidate system calls for spectrum behavior based on analysis of system call behavior of benign processes and malware.
We compared the system call patterns of 39 benign applications from Sourceforge [59] to 86 malware samples for Linux, including 17 backdoors, 20 general exploits, 24 Trojan horses and 25 viruses.
We found that malware invokes a system call set that is smaller than benign software; approximately 50 different system calls.
The most commonly invoked by malware include write(), wait(), clone(), close(), read(), open(), send() and fstat().
In selecting strategies for spectrum behavior, our aim is to perturb system calls that harm malware yet al-low benign code to run.
We found that the following system calls are critical to process start-up and execution, and cannot be easily varied: fstat(), getuid(), ioperm(), set thread area(), and mprotect().
In other cases, perturbing system call parameters leads to non-fatal deviations.
For instance, decreasing the length of a write() will cause a keylogger to lose keystrokes, silencing a send() might cause a process to fail sending an e-mail, and extending the time of a nanosleep() will just slow down a process.
We try to balance risks to benign processes with harm to malware through an experimentally-determined unpredictability threshold, which bounds the amount of unexpected variation in system call behavior.We studied these strategies for spectrum behavior: Strategy 1: Silence the system call: we immediately return a fabricated value upon system call invocation.
This strategy only succeeds when subsequent system calls are not highly dependent on the silenced action.
For example, this strategy worked for read() and write(), but not on open(), where a subsequent read() or write() would fail.Strategy 2: Change buffer bytes: we randomly change some bytes or shorten the length of a buffer passed to a system call, such as read(), write(), send() and recv().
This strategy corrupts execution of some scripts, and can frustrate attempts to read or exfiltrate sensitive data.Strategy 3: Add more wait time: the goal of this strategy is to slow down a questionable process, for example rate-limiting network attacks.
We randomly increase the time a nanosleep() call yields the CPU.Strategy 4: Change file offset: this approach simulates file corruption by randomly changing the offset in a file descriptor between read()s and write()s.We first applied unpredictability to the Linux Keylogger (LKL) [60], a user-space keylogger, using strategies 1, 2 and 4.
The keylogger not only lost valid keystrokes but also had some noise data added to the log file.Next we applied unpredictability to the BotNET [61] malware, which is mainly a communication library for the IRC protocol that was refined to add spam and SYNflood capabilities.
We used the IRC client platform irssi [62] to configure the botnet architecture with a bot herder, bots and victims.
The unpredictable strategies were applied to one of the bots.We first tested commands that successfully reached the bot, such as adduser, deluser, list, access, memo, sendMail and part.
The bot reads commands one byte at a time, and one lost byte will cause a command to fail.
Randomly silencing a subset of read() system calls in our unpredictable environment results in losing 40% of the commands from the bot herder.We measured the impact of the unpredictable environ- ment on the ability of the bot to send spam emails, shown in Figure 1.
In the normal environment, nine emails varying in length from 10 to 90 bytes were successfully sent.
In the unpredictable environment only partial random bytes were sent out by arbitrarily reducing the buffer size of send() in the bot process.
In the case of a spam bot, truncated emails will streamline the filtering process, not only for automatic filters, but also for the end users.
We also performed a SYN-flood attack to analyze the effectiveness of the unpredictable environment in mitigating DDoS attacks.
In a standard environment, one client can bring down a server in one minute with SYN packets.
When we set the unpredictability threshold to 70% and applied strategies 1 and 3, the rate of SYN packets arriving at the victim server decreased (Figure 2), requiring two additional bots to achieve the same outcome.Preliminary tests with Thunderbird, Firefox and Skype running in the unpredictable environment showed that these applications can run normally most of the time, occasionally showing warnings, and with some functionalities temporarily unavailable.
Chameleon combines inconsistent and consistent deception with software diversity for active defense of com- puter systems and herd protection.
It provides three distinct environments for process execution (Figure 3): (i) a diverse environment for whitelisted processes, (ii) an unpredictable environment for unknown or suspicious processes (inconsistent deception), and (iii) a consistently deceptive environment for malicious processes.
The Chameleon prototype is ongoing work.Known benign or whitelisted processes run in the diverse operating system environment, where the implementation of the program APIs are randomized to reduce instances with the same combinations of vulnerable code.
In some sense, the diverse environment combines ASLR and other known randomization techniques with N-version programming [1], except that Chameleon doesn't run the versions in parallel, but rather diversifies across processes.
Our insight is that a modular library OS design makes the effort of manual diversification more tractable.
Rather than require multiple complete OS implementations, the Chameleon design modularizes the Graphene library OS [11] and components are reimplemented at finer granularity and possibly in higher-productivity languages.
The power of this design is that mixing and matching pieces of N implementations multiplies the diversity by the granularity of the pieces.Unknown processes run in the unpredictable environment, where a subset of the system calls have their parameters modified or are silenced probabilistically.
Unpredictability is primarily implemented at the system call table, or library OS platform abstraction layer.
The execution of processes in this environment is unpredictable as they can lose some I/O data and functionality.
A malicious process in the unpredictable environment will have difficulty accomplishing its tasks, as some system call options used to exploit OS vulnerabilities might not be available, some sensitive data being collected from and transferred to the system might get lost, and network connectivity with remote malicious hosts is not guaranteed.Unpredictability raises the bar for large-scale attacks.
An attacker might notice the hostile environment, but its unpredictable nature will leave her with few options, one of them being system exit, which from the host perspective is a winning outcome.Processes identified as malicious run in a deceptive environment, where a subset of the system calls are modified to deceive an adversary with a consistent, but false appearance while forensic data is collected and forwarded to response teams such as CERT [63].
Shadow Honeypots [64] have been used similarly for testing the effects of anomalous network traffic and protecting against potentially unknown attacks.
In this environment, files the attacker intends to leak will be honeyfiles, and any system privileges she thinks she has will be limited to a sandbox.
Connections and activities with malicious remote hosts will be intercepted and logged.
Chameleon can adjust its behavior over the lifetime of a process.
Its design includes a dynamic, machine learning-based process categorization module that observes behavior of unknown processes, and compares to training sets of known good and malicious code.
Based on its behavior, a process can migrate to the diverse or deceptive environment.
We currently have the worst of both worlds: rather simple attacks work, and both research and industry are moving towards models of mutual distrust between applications and the operating system [5][6][7][8].
If applications code will trust the operating system less in the future, why not leverage this as a way to make malware and attacks harder to write?
Sacrificing predictability will introduce new, but tractable, research questions-especially around usability.
For example, a user who installs a new game with a potential Trojan horse will be tempted to simply whitelist the game if it isn't playable.
We believe unpredictability can be adjusted dynamically to avoid interfering with desirable behavior, potentially with user feedback.We envision Chameleon's architecture adopted in desktop computers.
Common, whitelisted applications, such as office software, run unperturbed with less risk of exploitation.
If successful, sacrificing predictable behavior can finally give systems an edge over one of the primary sources of computer compromises [65]: malware installed by unwitting users.
We thank the anonymous reviewers, Michalis Polychronakis, and Chia-Che Tsai for insightful comments on earlier drafts of this paper.
This research is supported in part by NSF grants CNS-1149730, SES-1450624, CNS- 1149229, CNS-1161541, CNS-1228839, CNS-1405641, CNS-1408695.
OCI-1246061, and DUE-1344369.
