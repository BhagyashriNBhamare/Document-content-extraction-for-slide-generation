The wide deployment of 4G/5G has enabled connected vehicles as a perfect edge computing platform for a plethora of new services which are impossible before, such as remote real-time diagnostics and advanced driver assistance.
In this work, we propose CLONE, a collaborative learning setting on the edges based on the real-world dataset collected from a large electric vehicle (EV) company.
Our approach is built on top of the federated learning algorithm and long short-term memory networks, and it demonstrates the effectiveness of driver personalization, privacy serving, latency reduction (asynchronous execution), and security protection.
We choose the failure of EV battery and associated accessories as our case study to show how the CLONE solution can accurately predict failures to ensure sustainable and reliable driving in a collaborative fashion.
The proliferation of edge computing technologies is strongly stimulating the adoption of machine learning methods on vehicles so that they can provide a variety of intelligent onboard services.
On the one hand, current vehicles, such as connected and autonomous vehicles (CAVs), can generate over 11 TB of privacy-sensitive data per day [43].
Such big data amount brings not only opportunities but also challenges to domain researchers -practices have proved that larger training data set can achieve more remarkable results [40]; however, training a big model often requires excessive computation and memory resources, which hinders the application of machine learning algorithms on the resource-constrained edge devices [47].
On the other hand, electric vehicles (EVs) have received significant attention as an important efficient and sustainable transportation system.
As a key component of EVs, the battery system largely determines the safety and durability of EVs [2,50].
Due to the aging process or abuse maneuvers, various faults may occur at each constituent cell or associated accessories.
It is essential to develop early failure detection techniques for EV battery and associated accessories to ensure availability and safety of EVs through anticipated replacements.Besides, although EIC (electric, instrumentation and computer control system) data (such as voltage and current) of EVs are able to show the symptoms of an imminent failure of EV battery and associated accessories, they are hard to tell the reason why the battery and associated accessories failed.
Driver behavior metrics such as speed, acceleration, and steering reflect the usage of an EV.
We believe such usage is one of the main root causes of failures.Hence, in this paper, we choose the failure of EV battery and associated accessories as our case study and seek to answer the following vital questions: "How to construct a personalized model by continuously tuning parameters on connected vehicles?"
, "What is the influence of the driver behavior metrics on the failure prediction of EVs?"
.
Our main contributions include:• To the best of our knowledge, this is the first work to predict an imminent failure of EV battery and associated accessories based on the real-world EV dataset which involves EIC attributes and driver behavior metrics.
• Our analysis reveals that adding driver behavior metrics can improve the prediction accuracy of EV failures.
• We train random forest (RF), gradient boosting decision tree (GBDT), and long short-term memory networks (LSTMs) to predict failures, and we find LSTMs outperform other methods based on our dataset.
• We propose CLONE, a collaborative learning setting on the edges for connected vehicles, and it can reduce training time significantly without sacrificing prediction accuracy.
This study presents the analysis of EV health characteristics based on the data measured at and collected from a large EV company.
We analyze three different models of EVs, and the corresponding data is reported and collected every 10 milliseconds during the whole 6-hour collection period.
In general, our data set is collected from the core control systems of EVs, which includes the vehicle control unit (VCU) [34], motor control unit (MCU) [30], and battery management system (BMS) [7].
BMS [7] is responsible for the battery maintenance and state estimation.
The VCU [34], as a key component of the whole EV, sends orders to other modules based on the driver manipulation (such as gear signal, accelerator pedal signal, and vehicle mode) via CAN communication network [41].
MCU [30] controls the wheel motor locally according to the commands from VCU.
Fig- ure 1 shows the structure diagram of the core control systems.
42 features listed in Table 1 and Table 2 were analyzed.
More specifically, we analyze EV data in the two aspects: (1) EIC attributes, and (2) driver behavior metrics.
Here, EIC refers to electric, instrumentation, and computer control system [29].
It includes battery features collected from BMS (most commonly used for EV battery durability analysis by other studies [2,16,39,44,51]) and the data reported from other control systems.
Most of the selected features can be understood intuitively; hence, we choose some vague features to give our explanations.
"MCUF" represent the MCU for the front wheels, and "MCUR" means the MCU for the rear wheels.
Positive Temperature Coefficient heater (PTC) is the heating unit of the battery in EVs.
Besides, state of charge (SOC) is the indicator of battery left capacity, and "Comp" is an acronym of the compressor.
The models learned in this paper are implemented in Python, using tensorflow 1.5.0 [1], keras 2.1.5 [21], and scikit-learn libraries [38] for model building.
We use 5-fold cross-validation method [27,42] to evaluate the proposed prediction approach.Note that there is a trade-off between long prediction horizon and the sampling frequency, with the constraints of computing resources.
After conducting a series of sensitivity study, we choose 15 seconds as our prediction horizon so that it can predict failures for the next 1,500 data points.
Before employing CLONE, we first combine the whole realworld dataset of three EVs to train different machine learning models on the Intel FogNode (the hardware information is shown in Table 5).
The goal is to find a suitable algorithm to predict failures, and answer the question of "What is the influence of the driver behavior metrics on EV failure prediction?
"We tackle the failure prediction problem using random forest (RF) [31], gradient boosted decision tree (GBDT) [17,52], and long short-term memory networks (LSTMs) [13,23] since they have become highly successful learning models for both classification and regression problems.To show the impact of driver behavior metrics on the failure prediction, we conduct experiments on two experimental groups.
Our first step is to combine all selected EIC attributes and driver behavior metrics to train models using RF, GBDT, and LSTMs methods, and we label this group as ED Group.
Then, we exclude all driver behavior metrics but keep EIC attributes, and we denote it as E Group.
Table 3 shows the input features for ED Group and E Group.
Table 4 presents the average evaluation scores of ED and E Group.
Based on our experimental results, we have the following observations:• Excluding driver behavior metrics results in around 8% reduction in the average F-measure.
• LSTMs outperform RF and GBDT in both two groups based on our dataset.
By observing experiment results of stand-alone learning, we can see that driver behavior metrics has non-negligible impacts on the failure prediction, and employing LSTMs can achieve better results.
Therefore, we aim to deploy LSTMsbased collaborative learning approaches on the edges based on EIC attributes and driver behavior metrics.
We term our approach CLONE, which is the solution of the problems -"How to construct a personalized model on connected vehicles?"
.
The learning tasks of CLONE is solved by a group of distributed participating vehicles (edge nodes) which are coordinated by a Parameter EdgeServer.
Each vehicle has its local training dataset which is never uploaded to the Parameter EdgeServer or transferred to the cloud.
Instead, each vehicle is responsible for continuously performing training and inference locally based on its private data.
When a vehicle finishes one epoch [19], which refers to the number of iteration related with the input dataset during training, it will push the value of current parameters to the Parameter EdgeServer, where the parameter values are aggregated by computing the weighted average value.
Then, each vehicle can immediately pull the updated parameter values from the Parameter EdgeServer, and set the updated parameters as their current parameters to start the next epoch.
The above steps will be repeated as necessary.
Figure 2 shows the basic framework of CLONE.
Note that when a new vehicle joins in, it will pull the current aggregated parameters from the Parameter EdgeServer first, and set them as the initial parameters for the first round of training, which speeds up the training process of unseen vehicles.
Besides, since it is asynchronous communication, for each vehicle, there is no need to stop and wait for other vehicles to complete an epoch, which greatly reduces the latency.
To illustrate the aggregation protocol of this work, we need to introduce the loss function first, which is defined as follows:Loss = ∑ i 񮽙ˆy 񮽙ˆ 񮽙ˆy (i) * log(y (i) ) + (1 − ˆ y (i) ) * log(1 − y (i) ) 񮽙Here, ˆ y i is the predicted output of the machine learning model, and the scalar y i is the desired output of the model for each data sample i.
We then define the formula to aggregate and update parameters as:񮽙 P(p) ← Loss(v) Loss(p)+Loss(v) P(p) + Loss(p) Loss(p)+Loss(v) P(v) Loss(p) ← Loss(v)Where P represents the value of a parameter, and Loss stands for the value of the loss function.
Besides, p refers to the Parameter EdgeServer, and v represents a specific vehicle.
For the more accurate vehicle (lower value of loss function), we assign a higher weight to its parameter.
To build heterogeneous hardware cluster representing different models of EVs, we adopt two different types of hardware -Intel FogNode and Jetson TX2, with different CPUs, operating systems and so on (shown in Table 5).
More specifically, we choose one Intel FogNode as the Parameter EdgeSever, and we treat the other two Intel FodeNodes and one Jetson TX2 as the edge nodes (vehicles) to continuously "learn" latent patterns.
In Section 3.1, we trained an accurate LSTMs model with 4 layers on the front and followed by a fully connected layer (dense layer).
Now, we aim to deploy a collaborative LSTMs with the same number of layers on the edges, i.e., with the same hyperparameters.
We first distribute our whole dataset to three edge nodes so that each edge node (vehicle) has its locally private dataset.
Table 6 shows the parameter distribution of the LSTMs model on the first two LSTMs layers (marked as lstm_1 and lstm_2) and the last fully connected layer (labelled as dense_1).
The "kernel" and "recurrent_kernel" are the parameter vectors, and the last column represents the shape (size) of the parameters for each vector.
For example, (16, 400) indicates that there are 16 × 400 of parameters.
Our whole network contains up to 297,700 parameters, including the weights and the biases.
Weight can reflect the strength of the connection between input and output.
Bias shows how far off the predictions are from the real values.
Figure 3 shows the I/O throughput per second at the Parameter EdgeServer when the three edge nodes are working at the same time.
It can be seen that the peak of the data throughput is relatively stable, and the peak appears intermittently.
Besides, the maximum I/O throughput for push and pull process is around 750 KB/s and 250 KB/s respectively, which indicates that there is no big pressure on the network throughput.
Figure 3 also proves that the push process is usually much slower than the pull process which was concluded by the work of [28].
This observation shows the importance to investigate methods that can reduce the communication latency of push process in the future work.
In this section, we present the experimental results of CLONE, and compare it with the algorithm performance of stand-alone learning in two aspects -(1) training time, and (2) evaluation scores including precision, recall, accuracy, and F-measure.
To have a clear comparison, we conduct experiments on three experimental groups.
The first group is the stand-alone learning, and we set the epoch of stand-alone learning equal to 210.
The second group is CLONE with the epoch of 70 for each edge node, and we label it as CLONE1.
Since there are three edge nodes in CLONE1, the equivalent number of iterations in total is also 210 (70 × 3).
As to the third group (CLONE2), the epoch is 100 for a single edge node, which results in 300 (100 × 3) of total iterations.
We first profile and compare how the training time is spent on the three experimental groups, which is shown in Table 7.
For the stand-alone learning, the used training time varies with different edge devices -it takes 1183s and 1573s on two Intel FogNodes (different working states) respectively, while taking 1497s to execute the training task on Jetson TX2.
As to CLONE1, the training time of each edge node is much lower than the training time of the single edge node of stand-alone learning.
Since there are three edge nodes in CLONE1, the training time of CLONE1 should be one-third of stand-alone learning theoretically.
However, due to the inevitable delay of the parameter transmissions, the training time of CLONE1 is greater than one-third of stand-alone learning.
We then increase the epoch value from 70×3 to 100×3 (CLONE2), we can see the required training time is longer than CLONE1 as it has a larger number of iteration related with the input dataset during training, but it still less than stand-alone learning which has a lower epoch value.
Note that with the participation of more edge nodes and larger size of the input dataset, the advantages of CLONE in training time reduction will be more obvious.
We then calculate the average evaluation scores for each group, which is shown in Figure 4.
Compared stand-alone learning and CLONE1, we can see that the overall evaluation scores of CLONE1 are lower than stand-alone learning.
This may be caused by the fact that -in stand-alone learning, the prediction accuracy will be improved with the increasing number of iterations passing the full dataset through the current model.
However, in CLONE1, due to the hardware difference, powerful edge nodes may train the model with high accuracy prior to other edge nodes.
When the parameters of the poor training results are uploaded to the Parameter EdgeServer, the global accuracy of CLONE1 will be influenced.
This may explain the performance gap between stand-alone learning and CLONE1 whose total epoch values are the same.However, when we further increase the value of epoch (CLONE2), it can achieve high evaluation scores as standalone learning.
Note that, by observing Table 7, the training time of CLONE2 is much lower than stand-alone learning, even though CLONE2 has higher epoch.
Compared with stand-alone learning, CLONE can reduce model training time without sacrificing algorithm performance.
With more edge nodes involved, the advantages of CLONE in training time reduction will be more obvious.
Besides, compared with the collaborative cloud-edge approach, the main advantages of CLONE is to speed up the analysis tasks and protect user privacy better as it does not need to transfer any portion of the big and sensitive dataset via the network.
There are some possible improvements for CLONE.
We list three of them for the discussion.
• Bandwidth demand -As the increasing number of edge nodes or the participation of larger neural networks, the communication of CLONE may be limited by bandwidth.In this context, we can use the Parameter EdgeServer group.
In the group, Parameter EdgeServers can communicate with each other.
Each Parameter EdgeServer is only responsible for a portion of parameters, and they work together to maintain globally shared parameters and their updates.
• Aggregation protocol -It is essential to find a suitable aggregation rule for the Parameter EdgeServer to aggregate parameters, which requires excessive experiments based on the specific experimental conditions.
• Push latency -Pushing parameters to the Parameter EdgeServer is usually much slower than pulling parameters.
Hence, it is essential to investigate methods to reduce uplink latency (possible solutions include structured updates and sketched updates [28]).
Besides, there are a variety of other meaningful use cases that CLONE can help, particularly for two types of scenarios:• Real-time applications which requires developing suitable machine learning algorithms on the resourceconstrained edges.
• Due to the privacy or/and the large network bandwidth constraints, the training dataset cannot be moved away from its source.
Although machine learning algorithms are widely deployed, it is difficult to deploy them on the resource-constrained devices.In this context, model compression technologies [9,11,12,22,33,45] and lightweight machine learning algorithms [8,24,25] have been proposed, but they can not guarantee to solve the problem completely when the training data and machine learning models are particularly large.
Another popular choice to address this limitations is employing distributed data flow systems such as MapReduce [10], Spark [54], Naiad [36], and XGBoost [6].
They are able to robustly scale with the increasing dataset size, but when training complex neural networks tasks, the data flow systems fail to scale as they are inefficient at executing iterative workloads [3,55].
This restriction sparked the development of distributed machine learning (DML) algorithms [5,14,15,32,53].
Later on, federated learning (FL), a novel DML, was proposed by Google researchers [4,18,28,35,49].
The main difference between conventional DML and FL is that, in FL, data is collected at the edges directly and stored persistently; thus, the data distribution at different edge nodes are usually not independent and identically distributed (non-i.i.d) [48].
Our work is inspired by FL, and the advantages of CLONE on the vehicles are shown in Table 8.
Each vehicle trains the neural network model locally based on its private data; local models will be updated according to the dynamic changes of the local dataset.
The training data can always be kept in its original location.
There is no need to stop and wait for other vehicles to perform an iteration; solve the inefficient communication problem of bulk synchronous execution.
Analyze vehicle data onboard; vehicles just need to push the parameter value to the Parameter EdgeServer rather than the whole data set.
Reduce security risks by limiting the attack surface to only the edges, instead of the edges and the cloud.Cloud-based method, Cloudedge method Different from the collaborative cloud-edge method that a few papers proposed [20,26,48], CLONE has three main strengths -(1) reduces power consumption by eliminating the use of central data centers, (2) speeds up the analysis and modeling tasks as it always analyze real-time data onboard and just need to communicate with the Parameter EdgeServer about the current parameters [37,46], and (3) reduces security risk by limiting the attack surface to only the edges.
In this paper, we conduct a field study of EVs based on a realworld dataset collected from a large EV company.
We discover that driver behavior metrics are potentially good indicators of the failures of EV battery and associated accessories.
Besides, we propose CLONE, collaborative learning setting on the edges for connected vehicles, which can reduce model training time significantly.
In the future, we plan to explore more advanced neural networks, enlarge the applying scope, and find a more suitable aggregation protocol for CLONE.
