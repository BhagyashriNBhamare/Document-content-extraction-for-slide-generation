Modern distributed stream processors predominantly rely on LSM-based key-value stores to manage the state of long-running computations.
We question the suitability of such general-purpose stores for streaming workloads and argue that they incur unnecessary overheads in exchange for state management capabilities.
Since streaming operators are instantiated once and are long-running, state types, sizes, and access patterns, can either be inferred at compile time or learned during execution.
This paper surfaces the limitations of established practices for streaming state management and advocates for configurable streaming backends, tailored to the state requirements of each operator.
Using workload-aware state management, we achieve an order of magnitude improvement in p99 latency and 2x higher throughput.
Any non-trivial streaming computation maintains and continuously updates state: rolling aggregations, synopses, window contents, triggers and timers.
To support larger-than-memory state, streaming dataflow systems rely on data partitioning and (embedded) persistent key-value stores.
The most prominent store is RocksDB [4], used by open-source systems such as Apache Spark Structured Streaming [9], Apache Flink [13], Apache Kafka [2], and Apache Samza [27], as well as Facebook's Stylus [17].
RocksDB has been widely adopted by many stream processors due to its solid performance on SSDs, incremental checkpointing capabilities, robustness, and active community.
However, it was not designed with streaming workloads in mind.We argue that the state management capabilities generalpurpose stores provide to streaming applications come at considerable cost.
The established monolithic approach to streaming state management is problematic: one type of state store (either RocksDB or in-memory) manages the state of all dataflow operators.
Some operators, such as joins, accumulate large state and benefit form efficient range scans, while others, such as rolling counters, store small state and need efficient inplace updates.
Yet, state requests are served by stores which are configured in a manner oblivious to each operator's state types, sizes, and access patterns.With this paper, we advocate for workload-aware state management.
The central idea is to exploit the fact that streaming dataflow operators are instantiated once and are long-running.
Thus, their access patterns and state size bounds are largely known in advance.
We believe that streaming systems would greatly benefit from novel configurable state stores, supporting different physical layouts and data types, and capable of leveraging knowledge about operator state characteristics.As a proof of concept in support of workload-aware state management, we build a testbed that allows defining custom state stores per operator and we use it to compare state management strategies.
As an alternative to RocksDB, we experiment with FASTER [15], a recent key-value store backed by a cache-optimized hash index with efficient in-place updates and support for custom data types.
Our testbed is implemented on top of Timely dataflow [19], a high-performance streaming dataflow engine written in Rust.To quantify the benefits of workload-aware state management, we compare RocksDB and FASTER on window operators and stateful queries from a streaming benchmark suite.
Our evaluation shows that not one store fits all when it comes to streaming workloads.
Although FASTER's in-place updates favor some queries, RocksDB's lazy evaluation is superior for others.
Using FASTER instances with explicit type information and tailored to the needs of individual operators, we achieve an order of magnitude improvement in p99 latency and 2x higher throughput.
We first revisit the fundamentals of dataflow stream processing and then describe established practices for streaming state management.
The dataflow model described below applies with minor variations to the majority of distributed stream processors supporting data-parallelism and local state, such as Apache Flink, Kafka, Samza, and Timely dataflow.
Stream processors with external state management are briefly discussed in Section 2.2.
A dataflow stream processor executes data-parallel computations on shared-nothing architectures.
A program is represented as a logical directed graph G = (V, E), where vertices in V denote operators and edges in E are data dependencies.
Upon deployment, the logical graph is translated to a physical execution plan, G = (V , E ), which maps dataflow operators to provisioned workers.
We call vertices in V tasks of an operator in V and edges in E physical data channels.
One or more tasks of the same or different operators can be assigned to the same worker, and the assignment strategy is systemspecific.
The assignment is computed at deployment time and remains static throughout job execution, unless a reconfiguration occurs.
Tasks are scheduled once and are long-running.
Figure 1 shows the logical graph and corresponding physical plan for Nexmark Q4 [3,31].
The query joins a stream of auctions and a stream of bids and outputs a rolling average of winning bid prices for each auction category.
The provisioned workers, w1, w2, execute parallel tasks as indicated by the enclosing rectangles, and each owns an embedded store to manage state.
The join and average operators are stateful and data-parallel, so that each worker operates on disjoint partitions of the input streams.
A streaming application can define and maintain different types of state throughout its lifetime, including partial results, sketches, buffered tuples in window buckets, triggers, timers, and notifications.
A state backend defines the physical location where streaming state resides.
The backend type determines how state is organized into data structures, its maximum supported size, and whether it is guarded against failures.
For instance, an in-memory backend keeps working state in memory and, thus, the maximum supported state size is limited.
A database backend keeps working state in an embedded key-value store and leverages the disk to support larger-than-memory state.Users interact with state through APIs provided by the stream processor.
Those allow defining state primitives of different types, such as lists, maps, and arbitrary values.
Most dataflow programming models assume a key-value schema for input records [9,13,14,20,25] and always associate state with a key.
The API also enforces state scopes, which determine the accessibility of state through the application.The assignment of tasks to worker threads shown in Fig- ure 1 has significant implications to state scoping.
The runtime guarantees that input records with the same key will be processed by the same task.
As a result, any state associated with a particular key is read and modified by a single worker at any point in time.
By employing data parallelism, the dataflow model provides single-writer isolation to state.
Thus, it is crucial that any key-value store used as a streaming backend provides excellent single-thread performance.External state management.
An alternative to managing state inside the stream processor is essentially making all operators stateless and externalizing all intermediate data.
While this design simplifies fault-tolerance and re-configuration, it induces higher latency and presumes access to a highly available and scalable external state store.
Some systems that follow this approach include Storm [30], which can spill state to HDFS or Cassandra [6], and MillWheel [8] that uses BigTable [16].
For further details, we refer interested readers to a recent survey on streaming state management [29].
Stream processors use embedded stores as out-of-the-box solutions for state management.
This straw man approach restricts applications to using one type of backend, which is configured globally for all dataflow operators, regardless of their state requirements 1 .
In this section, we summarize the drawbacks of this monolithic design.
An evident issue with storing multiple operator states in the same store is the requirement to manage keys and values of different types.
To support arbitrary types, RocksDB maintains all in-memory and disk-resident data as byte arrays.
This eager de/serialization approach means that every state access includes one or more data marshalling operations, all of which are on the critical path of the request.
A recent study [23] finds that Flink spends 20% of execution time de/serializing RocksDB data of non-primitive types (e.g., Vec<String>), while similar results are shown in other works as well [7].
Samza tries to mitigate this overhead by maintaining a cache [28].
While a well-designed cache policy can avoid some data marshalling, we believe this task must be the responsibility of the state store.
State types in streaming dataflow applications are known at compile time and do not change during execution.
If each primitive is backed by a dedicated store, in-memory data can be kept in their native format, incurring de/serialization only when fetching or flushing pages from/to disk, asynchronously.
The second major issue with monolithic state management is that all state stores in a streaming job use the same configuration, regardless of the access patterns and size requirements of the operators they serve.
This is problematic, as operators in a dataflow can have vastly different behavior in terms of state.
For example, in the query of Fig. 1, the join is write-heavy and can potentially accumulate large state, while the aggregation performs two read-modify-write operations per input event and its state can fit in memory.
A write-optimized store capable of spilling state to disk is suitable for managing the join state, while a hash-based in-memory store with in-place updates would fit the rolling aggregation better.
Streaming backends do not need all sophisticated features of general-purpose key-value stores.
In case we use one backend per operator task or state primitive, there is no need to support concurrent external requests within the backend, since dataflow systems already guarantee single-thread access to state.
Although concurrency control does not affect singlethread performance in practice, it introduces unnecessary complexity and maintenance overhead.
Similar simplifications are also possible for other features of existing key-value stores.
For instance, the performance of deletes can be significantly improved by leveraging the knowledge of streaming operators.
In the case of windows, the backend can simply purge the entire window state when it expires.The following operations are handled by dataflow systems already and need not be implemented in state backends:State partitioning is performed according to the input data partitioning functions used by shuffling operators.
State scoping is enforced by the dataflow state APIs, which abstract store operations and provide access to per-key local state only.
Per-key read/write isolation is guaranteed by the dataflow model.
Worker threads process disjoint data partitions, thus, each key will always be accessed by a single thread.Checkpointing is coordinated by the streaming runtime which uses locks and synchronization mechanisms to copy the state.
Stream processors implement global snapshot algorithms [13] and do not rely on key-value stores for fault-tolerance.
To explore the potentials of a more flexible state management approach, we have implemented a testbed on top of the Timely Dataflow stream processor [19].
Timely is a Rust implementation of Naiad [26] and allows for a fair performance evaluation, free from runtime and JNI overheads or other incompatibility issues present in JVM-based systems.
For example, Flink's JNI bridge to RocksDB imposes a maximum key-value pair size of 2.2GB 2 .
This limit may be too restrictive in some cases.
A window operator can represent its state as a key-value pair, where the key is the start or end timestamp and the value contains the window contents, whose size may exceed 2.2GB (cf. Section 5.1).
Further, custom merge comparators are not supported in Java, as RocksDB expects C++ code.
Custom merge comparators are used by RocksDB for lazy evaluation in the merge operator.
Merging is semantically equivalent to a Read-Modify-Write operation: each time a key-value pair is added to the database using merge, the pair is appended to the mutable mem-table and the actual value update is performed "lazily" during compaction (if any) or upon a get for the respective key, by calling the comparator function.
Our testbed does not have any of the above restrictions, it minimizes the FFI overhead when interacting with the key-value store, and allows defining custom RocksDB comparators directly in Rust.
Currently, the testbed supports three types of backends: inmemory, RocksDB, and FASTER.
We use a wrapper around FASTER's C++ library [1] to expose its interface for accessing the key-value store and performing internal operations.
For the integration with RocksDB, we use the Rust wrapper [5].
We also provide a versatile API so that users can define basic state primitives in their streaming dataflows, including ManagedCount (for counters), ManagedValue (for arbitrary values), and ManagedMap (for maps) [11].
Figure 2 illustrates how users can take advantage of flexible state management with our API and testbed.
Each operator can define one or more individually configured state stores, instantiated with specific types.
Store configuration is currently manual, but we are working on automating this process by leveraging knowledge about the computation.
We present a set of evaluation results with RocksDB and FASTER using our state management testbed 3 .
The evaluation does not aim to thoroughly compare RocksDB with FASTER but to showcase that not one store fits all streaming workloads.
We demonstrate the effect of a backend's data layout when evaluating window operators in § 5.1 and the effect of leveraging knowledge about types, state sizes, and state accesses in § 5.2.
The evaluation is focused on tail latency, as the major performance metric for real-time streaming applications, and on single-thread performance, as the dataflow model guarantees per-key read/write isolation ( § 3.3).
To provide a clear picture of tail latency, we use complementary CDFs (CCDFs): a (x, y) point indicates that y% of records have at least x ms end-to-end latency.
Thus, the lines at y = 10 −1 and y = 10 −2 correspond to p90 and p99 latency, respectively.
We briefly discuss throughput and multi-worker dataflows in § 5.2.
Benchmarks.
We use the Nexmark streaming benchmark [31,32].
Queries include an incremental join (Q3), window joins (Q4, Q6, Q8), and custom window aggregations (Q5, Q7).
Experimental setup.
We use Timely 0.9.0, compiled with Rust 1.37.0, the latest FASTER C++ version from [1], and the RocksDB Rust wrapper (0.12.4) [5].
We configure FASTER with a 128MB hash index and 8GB in-memory log, where 10% (resp.
90%) is given to the immutable (resp.
mutable) region, as in the FASTER paper [15].
RocksDB is 3 Available at https://github.com/jliagouris/wassm configured with 128KB block size, 2 mem-tables of 4GB each, a 256MB LRU cache, and a 100MB hash block index.
We use c5d.2xlarge and r5d.12xlarge instances on Amazon EC2, for single-thread and multi-thread experiments, respectively.
Any streaming state management backend needs to efficiently support windows, as they are perhaps the most common streaming operators.
Windows enable evaluation of blocking computations, such as aggregations and joins on streams, and provide continuous fresh results to applications.
For the purpose of this paper, we consider fixed windows which can be identified by their start and end timestamps.
Windowing splits unbounded inputs into a series of bounded sets of records and we say that a window triggers when the system's notion of time arrives at its end timestamp.
At this point, the window produces results and deletes its state.
Window evaluation functions can be applied eagerly, upon receiving a new record that belongs to the window, or lazily, on trigger.We evaluated various strategies and window functions and concluded that there is no clear winner between LSM-based and hash-based approaches.
Nevertheless, we identified the parameters which affect performance and can guide the design of a configurable, workload-aware store.
Our results indicate that FASTER performs better than RocksDB across configurations for eager aggregations, especially for large sliding windows.
As for holistic aggregations, RocksDB's merge performs best across configurations and, it is the only approach that can keep up with high input rates.
In the interest of space, we discuss only one representative experiment.We evaluate RANK, a holistic aggregation that fetches the entire window contents on trigger, and COUNT, which can be eagerly computed to maintain a single value per window.
We run each experiment for 10 minutes in an open loop and measure the end-to-end latency per record.
The input rate was set to 1K rec/s for RANK and 10K rec/s for COUNT.
In both operators, the state is organized as follows: the key is the window start timestamp and the value is the window content (a vector of integers in RANK and a single integer in COUNT).
Figure 3 plots the latency CCDF for a sliding and a tumbling window.
For every input record, the operator performs one read-modify-write operation for each active window state 4 .
While FASTER supports in-place updates, RocksDB issues either a pair of get + put operations or a merge, depending on the implementation.
On trigger, the operator retrieves the window contents with one get operation and then purges state.
FASTER performs best for COUNT (100× lower p99 latency than RocksDB MERGE) as it can leverage fast lookups with in-place updates.
For RANK, FASTER and RocksDB with PUT/GET continuously remove and reinsert growing vectors of integers, while lazy evaluation with RocksDB MERGE pays off (p99 latency is orders of magnitude lower than FASTER's).
To showcase the additional benefits of workload-aware store configuration, we use carefully configured implementations of Nexmark queries that leverage knowledge about (i) k-v types (to reduce de/serialization), and (ii) state size and access patterns.
FASTER was superior to RocksDB in all these queries, thus, our implementations use FASTER; in particular, one instance per primitive configured according to Table 2.
The monolithic baseline uses one FASTER instance per worker configured as described in the experimental setup.
We use 1M rec/s input rate for Q3, Q7 and Q8, and 10K rec/s for Q4-6.
The first set of experiments evaluates single-thread latency for monolithic and workload-aware configurations.
Then, we evaluate scaling with respect to the input rate (throughput) and the number of parallel workers.Single-thread latency.
Figure 4 shows the latency CCDFs for Q3, Q4, Q5, and Q7 run with a single worker thread (Q6 has almost identical behavior to that of Q4, and Q8 accumulates Query Configuration Q3 1GB (people), 7GB (auctions) Q4 6GB (bids), 1.5GB (auctions), 512MB (average) Q5 6GB (additions), 1GB (deletions) 512MB (accumulations), 512MB (hot items) Q6 6GB (bids), 1.5GB (auctions), 512MB (average) Q7 6GB (pre-reduce), 2GB (all-reduce) Q8 4GB (people), 4GB (auctions) negligible state).
Table 1 summarizes the p99 latency and speedup for all queries and state management approaches.
The workload-aware implementation achieves significant p99 latency speedups for many queries: 14× for Q5 and 6× for Q4, Q6.
Q3 has a 10× speedup as well, for higher percentiles.To understand the source of performance improvements, we describe how our workload-aware implementations compare to the monolithic ones.
Primitives that can grow arbitrarily and accumulate larger-than-memory state are backed by a dedicated FASTER store that is instantiated with the respective data types.
For example, each of the two join inputs in Q3 (people and auction streams) has its own FASTER instance.
The instances are typed after the stream they manage and their configuration is shown in Table 2.
In Q3, most of the available memory is allocated to the auctions state, as auctions accumulate faster than people.
As shown in Fig. 4a, this optimization pays off for the tail latency, where we see more than 10× speedup.
The benefit is more evident for Q4 (Fig. 4b) and Q7 (Fig. 4d).
In Q5, we split state further and use two more primitives with dedicated FASTER backends for auction counts and hot items (cf. Table 2).
Small state with bounded size, such as the computation progress in Q8, is kept in memory.Throughput vs latency.
Fig. 5a plots the median per-record latency for Q7 for increasing input rates.
We vary the input rate from 100K rec/s up to 2M rec/s, on a single thread.
The latency of the workload-aware implementation scales better with increasing throughput and remains below 1ms for up to 1M rec/s.
Further, with the workload-aware implementation, a single thread can comfortably sustain a 2M rec/s input rate with a median latency of 1.3ms. That is 2× higher throughput than the monolithic approaches, which cannot keep up with input rates over 1M rec/s.
Multiple worker threads.
To confirm that the benefits of workload-aware implementation persist in multi-worker dataflows, we run Q7 with an increasing number of worker threads (1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16), in an open-loop setting, with input rate fixed at 1M rec/s.
Note that the dataflow model still ensures singlethread isolation to state, however, the number of key-value store instances sharing the resources of the same machine increases significantly.
The monolithic approach uses one FASTER instance per worker, whereas the workload-aware uses two (cf. Table 2).
Fig. 5b plots the median per-record latency.
The workload-aware implementation scales similarly to the monolithic and remains consistently better as we increase the number of workers.
Workload-aware streaming state management can boost performance, beyond avoiding data marshalling costs.
A careful assignment of state backends to primitives not only reduces latency but also improves throughput and scalability.
Our results motivate the need for configurable streaming backends, in the spirit of the recent developments on designing data structures tailored around a particular workload [21,22].
One store that fits all or many?
An open question towards implementing workload-aware state management for a production-ready stream processor is whether it is worth designing a new stream-optimized key-value store from scratch.
One could argue that a selected set of existing key-value stores can be plugged-in via a flexible API like the one we discuss in Section 4.
It certainly appears that a store like RocksDB could serve range queries and lazy evaluation whereas a store like FASTER could be used for operators with frequent point lookups and in-place updates.
However, this approach introduces undesirable project dependencies for the streaming system and does not account for changes in state characteristics (e.g. due to increased input rates) that might make pre-configured backends unsuitable over time.Streaming state benchmarks.
Despite the fact that keyvalue stores are an integral component of streaming systems and crucial to their performance, their suitability for streaming state management has not been studied prior to this paper.
In fact, no benchmark exists that captures the workload characteristics and temporal locality of streaming applications.Existing key-value store evaluations [10,15,24] focus on multi-threaded performance and use request-driven benchmarks, such as YCSB [18], which is oblivious to key-space locality [12].
Such benchmarks and metrics cannot provide reliable results for data-parallel stream processors, which instead execute continuous queries, perform single-thread state access, and issue frequent deletions (e.g. for windows).
Fault-tolerance and re-configuration.
Even though stream processors that implement their own checkpointing mechanisms do not rely on key-value stores for providing exactlyonce guarantees upon failures, certain store features are still desirable.
Besides reads and writes in regular processing, a streaming engine must also support efficient state operations for fault-tolerance and re-configuration.
Those include quick copies, incremental checkpointing, state migration upon repartitioning, and bulk loads for quick recovery.
For example, although RocksDB supports incremental checkpoints (at the SST level) by leveraging key order, hash-based stores, such as FASTER, do not.
How to support all these operations efficiently within the same key-value store is an open question.Extracting and/or learning state characteristics.
The queries of Section 5 use built-in operators, however, most real streaming computations include operators with UDFs.
One option to infer state access patterns in UDFs is to use static code analysis.
This approach can be further combined with online learning techniques to infer data-dependent access patterns and changes in state characteristics.
Learning access patterns with temporal dependencies in a streaming setting is an interesting direction for future research.
