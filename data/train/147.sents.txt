This paper presents Venus, a service for securing user interaction with untrusted cloud storage.
Specifically, Venus guarantees integrity and consistency for applications access-ing a key-based object store service, without requiring trusted components or changes to the storage provider.
Venus completes all operations optimistically, guaranteeing data integrity.
It then verifies operation consistency and notifies the application.
Whenever either integrity or consistency is violated, Venus alerts the application.
We implemented Venus and evaluated it with Amazon S3 commodity storage service.
The evaluation shows that it adds no noticeable overhead to storage operations.
Our results demonstrate that data integrity and consistency for remote storage accessed by multiple clients can be obtained without significant overhead, no additional trusted components, and seamlessly integrated with the normal operations.
Specifically, Venus is the first practical decentralized algorithm that• verifies cryptographic integrity and consistency of remotely stored data accessed by multiple clients without introducing trusted components, • does not involve client-to-client coordination or introduce extra communication on the critical path of normal operations, • provides simple semantics to clients, lets operations execute optimistically, but guarantees that either all operations eventually become consistent, or that every client is informed about the service failure, and • is practically implemented on top of a commodity cloud storage service.Venus may secure a variety of applications that currently use cloud storage, such as online collaboration, Internet backup, and document archiving.
No less important is that Venus enables many applications that require verifiable guarantees and do not blindly trust a service provider to be deployed in the cloud.
The remainder of the paper is organized as follows: Section 2 discusses related work.
Section 3 presents the design of Venus, and Section 4 defines its semantics.
The protocol for clients and the verifier is given in Section 5.
Section 6 describes our implementation of Venus, and finally, Section 7 presents its evaluation.
Data integrity on untrusted storage accessed by a single client with small trusted memory can be protected by storing the root of a hash tree locally [2].
Systems applying this approach to outsourced file systems and to cloud storage have also been demonstrated [10,11].
In cryptographic storage systems with multiple clients, such "root hashes" are additionally signed; TDB [18], SiRiUS [9], and Plutus [15] are some representative examples implementing this method.
In order to ensure freshness, the root hashes must be propagated by components that are at least partially trusted, however.
Going beyond ensuring the integrity of data that is actually read from an untrusted service by a single client, recent work by Juels and Kaliski [14] and by Ateniese et al. [1] introduces protocols for assuring the client that it can retrieve its data in the future, with high probability.
Unlike Venus, this work does not guarantee consistency for multiple clients accessing the data concurrently.Several recent systems provide integrity using trusted components, which cannot be subverted by intrusions.
A prominent system of this kind is CATS [26], which provides storage accountability based on an immutable public bulletin board available to the service and to all clients.
Another proposal is A2M [8], which utilizes a trusted append-only memory (realized in hardware or software) to ensure atomic semantics.
Venus uses client signatures on the data, but no trusted components.A separate line of work provides so-called forking semantics [20], which are weaker than conventional atomic semantics, but these systems do not require any trusted components whatsoever.
SUNDR [17], Cachin et al. [7,4] and Majuntke et al. [19] propose storage systems of this kind that ensure forking consistency semantics.
They guarantee that after a single consistency violation by the service, the views seen by two inconsistent clients can never again converge.
The main drawbacks of these systems lie, first, in the difficulty of understanding forking semantics and exploiting them in applications, and, second, in their monolithic design, which integrates storage operations with the consistency mechanism.
Hence, it is difficult to use these approaches for securing practical cloud storage.Furthermore, the systems with forking semantics mentioned so far [17,7,4] may block reading clients when a readwrite conflict occurs [6,5].
In such situations, readers cannot progress until the writer completes its operation, which is problematic, especially if the writer crashes.
Majuntke et al. [19] and Williams et al. [24] provide fork-consistency and guarantee system-wide progress but their algorithms may abort some conflicting operations.
Going beyond forkconsistent protocols designed for untrusted storage, the system of Williams et al. [24] provides an untrusted database server and supports transactions.
In contrast, Venus never blocks a client operation when the storage service is correct, and every client may proceed independently of other clients and complete every operation.
Venus provides more intuitive system semantics, whereby operations complete optimistically before their consistency is verified.
In the absence of failures, every client operation is eventually marked as green, and Venus ensures that all clients observe a single sequence of green operations.FAUST [5] implements the notion of weak fork-linearizability, which allows client operations to complete optimistically, as in Venus.
It also provides notifications to clients, but they are different and less intuitive -FAUST issues stability notifications, where each notification includes a vector indicating the level of synchronization that a client has with every other client.
This stability notion is not transitive and requires every client to explicitly track the other clients in the system and to assess their relation to the data accessed by an operation.
FAUST is therefore not easily amenable to dynamic changes in the set of clients.
Furthermore, it is unclear how clients can rely on FAUST stability notifications in a useful manner; global consistency in FAUST (among all clients) is guaranteed only if no client ever crashes.
FAUST does not work with commodity storage; like other proposals, it integrates storage operations with the consistency mechanism, and it does not allow multiple clients to modify the same object, which is the usual semantics of commodity storage services.In contrast, indications in Venus simply specify the last operation of the client that has been verified to be globally consistent, which is easy to integrate with an application.
Venus eliminates the need for clients to track each other, and enables dynamic client changes.
Unlike the previous protocols [5,7,19], Venus allows all clients to modify the same shared object.
Most importantly, the design of Venus is modular, so that it can be deployed with a commodity storage service.Orthogonal to this work, many storage systems have been proposed that internally use replication across several nodes to tolerate a fraction of corrupted nodes (e.g., [12] and references therein).
For instance, HAIL [3] is a recent system that relies replicated storage servers internally, of which at least a majority must be correct at any time.
It combines data replication with a method that gives proofs of retrievability to the clients.
But a storage service employing replication within its cloud infrastructure does not solve the problem addressed by Venus -from the perspective of the client, the cloud service is still a single trust domain.
Figure 1 depicts our system model, which includes a storage service, a generic commodity online service for storing and retrieving objects of arbitrary size, a verifier, which implements our consistency and verification functions and multiple clients.
The storage service is used as is, without any modification.
Usually the storage service and the verifier are hosted in the same cloud and will be correct; but they may become faulty or corrupted by an adversary, and they may collude together against the clients.There are an arbitrary number of clients, which are subject to crash failures.
Clients may be connected intermittently and are frequently offline.
The core set of clients is a publicly known subset of the clients with a special role.
These clients help detect consistency and failures (Section 5.4) and manage client membership (Section 5.6); to this end, clients occasionally communicate directly with clients from the core set.
A quorum of the core set of clients must not crash (but may also be offline temporarily).
Note that clients of cloud services, and especially users of cloud storage, do not operate continuously.
Hence, clients should not depend on other clients for liveness of their operations.
Indeed, every operation executed by a client in Venus proceeds independently of other clients and promptly completes, even if all other clients are offline.Clients in Venus are honest and do not deviate from their specification (except for crashing).
Note that tolerating malicious clients does not make a lot of sense, because every client may write to the shared storage.
From the perspective of the correct clients, the worst potential damage by another client is to simply overwrite the storage with bogus information.
Venus, just like commodity cloud storage, cannot not perform application-specific validation of written data.Venus clients are admitted by a member of the core set, as determined by the same access-control policy as the one used at the commodity storage interface.
Clients are identified by a signature public key and an email address, bound together with a self-signed certificate.
Every client knows initially at least the public keys of all clients in the core set.Messages between clients and the verifier or the storage service are sent over reliable point-to-point channels.
Clientto-client communication is conducted using digitally signed email messages; this allows clients to go temporarily offline or to operate behind firewalls and NATs.
Clients rarely communicate directly with each other.The storage service is assumed to have an interface for writing and reading data objects.
The write operation takes the identifier obj of the object and some data x as parameters and returns an acknowledgment.
The read operation expects an object identifier obj and returns the data stored in the object.
After a new object is successfully stored, clients are able to read it within a bounded period of time, though perhaps not immediately.
We assume that this bound is known; in practice, it can be obtained dynamically 5 .
The assumption that such time threshold exists reflects clients' expectation from any usable storage service.
Inability to meet this expectation (e.g., due to an internal partition) can be perceived as a failure of the storage provider as far as clients are concerned.
Venus makes several attempts to read the object, until this time bound is exceeded, at which time a failure notification is issued to clients.
Venus overrides the write(obj, x) and read (obj) operations for accessing an object identified by obj in the interface of the storage service.
Venus does not allow partial updates of an object, the value x overwrites the value stored previously.
If the object does not exist yet, it is created.
For simplicity of presentation, we assume that each client executes operations sequentially.Venus extends the return values of write and read operations by a local timestamp t, which increasing monotonically with the sequence of operations executed by the client.
An operation o always completes optimistically, without waiting for other clients to complete their operations; at this point, we say that o is red, which means that the integrity of the operation has been checked, but its consistency is yet unverified.A weak form of consistency is nevertheless guaranteed for all operations that become red.
Namely, they ensure causal consistency [13], which means intuitively that all operations are consistent with respect to potential causality [16].
For example, a client never reads two causally related updates in the wrong order.
In addition, it guarantees that a read operation never returns an outdated value, if the reader was already influenced by a more recent update.
Causality has proven to be important in various applications, such as various collaborative tools and Web 2.0 applications [21,25].
Although usually necessary for applications, causality is often insufficient.
For example, it does not rule out replay attacks or prevent two clients from reading two different versions of an object.Venus provides an asynchronous callback interface to a client, which issues periodic consistency and failure notifications.
A consistency notification specifies a timestamp t that denotes the most recent green operation of the client, using the timestamp returned by operations.
All operations of the client up to this operations have been verified to to be consistent and are also green.
Intuitively, all clients observe the green operations in the same order.
More precisely, Venus ensures that there exists a global sequence of operations, including at least the green operations of all clients, in which the green operations appear according to their order of execution.
Moreover, this sequence is legal, in the sense that every read operation returns the value written by the last write that precedes the read in the sequence, or an empty value if no such write exists.
Note that the sequence might include some red operations, in addition to the green ones.
This may happen, for instance, when a client starts to write and crashes during the operation, and a green read operation returns the written value.Failure notifications indicate that the storage service or the verifier has violated its specification.
Venus guarantees that every complete operation eventually becomes green, unless the client executing it crashes, or a failure is detected.
Section 5.1 describes the interaction of Venus clients with the storage service.
Section 5.2 describes versions, used by Venus to check consistency of operations.
Section 5.3 presents the protocol between the clients and the verifier.
Section 5.4 describes how clients collect information from other clients (either through the verifier or using client-toclient communication), and use it for eventual consistency and failure detection.
Section 5.5 describes optimizations.For simplicity, we first describe the protocol for a fixed set of clients C1, . . . , Cn, and relax this assumption later in Section 5.6.
The algorithm uses several timeout parameters, which are introduced in Figure 2.
We have formally proven that Venus provides the properties defined in Section 4; these proofs are omitted due to space limitations.In what follows, we distinguish between objects provided by Venus and which can be read or written by applications, and objects which Venus creates on storage.
The former are simply called objects, while the latter are called low-level objects.
Every update made by the application to an object obj managed with Venus creates a new low-level object at the storage service with a unique identifier, denoted px in the description below, and the verifier maintains a pointer to the last such update for every object managed by Venus.
Clients periodically garbage-collect such low-level objects (see also Section 5.5).
Number of times an operation is retried on the storage service.
tdummy Frequency of dummy-read operations.tsend Time since last version observed from another client, before that client is contacted directly.
treceive Frequency of checking for new messages from other clients.
The protocol treats all objects in the same way; we therefore omit the object identifier in the sequel.The general flow of read and write operations is presented in Figure 3.
When a write(x) operation is invoked at a client Ci to update the object, the client calculates hx, a cryptographic hash of x, and writes x to the storage service, creating a new low-level object with a unique path px, chosen by the client-side library.
Using px as a handle, the written data can later be retrieved from storage.
Notice that px identifies the low-level object created for this update, and it is different from the object identifier exported by Venus, which is not sent to storage.
After the low-level write completes, Ci sends a submit message to the verifier including px and hx, informing it about the write operation.
Ci must wait before sending the submit message, since if Ci crashes before x is successfully stored, px would not be a valid handle and read operations receiving px from the verifier would fail when trying to retrieve x from the storage service.
The verifier orders all submit messages, creating a global sequence H of operations.
When a read operation is invoked, the client first sends a submit message to the verifier, in order to retrieve a handle corresponding to the latest update written to the object.
The verifier responds with a reply message including px and hx from the latest update.
The reader then contacts the storage service and reads the low-level object identified by px.
In most cases, the data will be returned by the storage service.
The reader then checks the integrity of the data by computing its hash and comparing it to hx; if they are equal, it returns the data to the application.
If the storage provider responds that no low-level object corresponds to px, the client re-executes the read.
If the correct data can still not be read after R repetitions, the client announces a failure.
Similarly, failure is announced if hashing the returned data does not result in hx.
Updates follow the same pattern: if the storage does not successfully complete the operation after R attempts, then the client considers it faulty.Since the verifier might be faulty, a client must verify the integrity of all information sent by the verifier in reply messages.
To this end, clients sign all information they send in submit messages.
A more challenging problem, which we address in the next section, is verifying that px and hx returned by the verifier correspond to the latest write operation, and in general, that the verifier orders the operations correctly.
In order to check that the verifier constructs a correct sequence H of operations, our protocol requires the verifier to supply the context of each operation in the reply.
The context of an operation o is the prefix of H up to o, as determined by the client that executes o.
This information can be compactly represented using versions as follows.Every operation executed by a client Ci has a local timestamp, returned to the application when the operation completes.
The timestamp of the first operation is 1 and it is incremented for each subsequent operation.
We denote the timestamp of an operation o by ts(o).
Before Ci completes o, it determines a vector-clock value vc(o) representing the context of o; the j-th entry in vc(o) contains the timestamp of the latest operation executed by client Cj in o's context.In order to verify that operations are consistent with respect to each other, more information about the context of each operation is needed.
Specifically, the context is compactly represented by a version, as in previous works [20,7,5].
For simplicity, we sometimes write vc(o) and vh(o) for version(o).
vc and version(o).
vh, respectively.
We define the following order (similarly to [20,7,5]), which determines whether o could have appeared before another operation o 񮽙 in the same legal sequence of operations:Order on versions: version(o) ≤ version(o 񮽙 ) whenever both of the following conditions hold:1.
vc(o) ≤ vc(o 񮽙 ), i.e., for every k, vc(o)[k] ≤ vc(o 񮽙 )[k].
2.
For every k such that vc(o)[k] = vc(o 񮽙 )[k], it holds that vh(o)[k] = vh(o 񮽙 )[k].
The first condition checks that the context of o 񮽙 includes at least all operations that appear in the context of o. Suppose that o k is the last operation of C k appearing both in the context of o and in that of o 񮽙 .
In this case, the second condition verifies that the prefix of H up to o k is the same in the contexts of o and o 񮽙 .
We say that two versions are comparable when one of them is smaller than or equal to the other.
The existence of incomparable versions indicates a fault of the verifier.
Each client maintains a version corresponding to its most recently completed operation oprev.
Moreover, if oprev is a read operation, the client keeps pprev and hprev retrieved by oprev from the verifier.
Note that client Ci does not know context and version of its current operation when it sends the submit message, as it only computes them after receiving the reply.
Therefore, it sends the version of oprev with the submit message its next operation to the verifier.When sending the submit message for a read operation o, Ci encloses a representation of o (including the timestamp ts(o)), the version oprev of its previous operation as well as a signature on vh(oprev) [i].
Such a signature is called a proof and authenticates the prefix of Ci's context of oprev.
If o is a write operation, the message also includes the tuple (px, hx, ts(o)), where px is the handle and hx is the hash of the data already written to the storage provider.
Otherwise, if o is a read operation, and oprev was also a read, the message includes (pprev, hprev, ts(oprev)).
All information in the submit message is signed (except for the proof, which is a signature by itself).
Recall that the verifier constructs the global sequence H of operations.
It maintains an array Ver, in which the j-th entry holds the last version received from client Cj.
Moreover, the verifier keeps the index of the client from which the maximal version was received in a variable c; in other words, Ver [c] is the maximal version in Ver.
We denote the operation with version Ver[c] by oc.
The verifier also maintains a list Pending, containing the operations that follow oc in H. Hence, operations appear in Pending according to the order in which the verifier received them from clients (in submit messages).
Furthermore, a variable Proofs contains an array of proofs from submit messages.
Using this array, clients will be able to verify their consistency with Cj up to Cj's previous operation, before they agree to include Cj's next operation in their context.Finally, the verifier stores an array Paths containing the tuple (px, hx, ts(o)) received most recently from every client.
Notice that if the last operation of a client Cj is a write, then this tuple is included in the submit message and the verifier updates Paths[j] when it receives the submit.
On the other hand, the submit message of a read operation does not contain the handle and the hash; the verifier updates Paths [j] only when it receives the next submit message from Cj.
The verifier processes every submit message atomically, updating all state variables together, before processing the next submit message.After processing a submit message, the verifier sends a reply message that includes c, version(oc), Pending, Proofs (only those entries in Proofs which correspond to clients executing operations in Pending), and for a read operation also a tuple (px, hx, tx) with a handle, hash, and timestamp as follows.
If there are write operations in Pending, then the verifier takes (px, hx, tx) from the entry in Paths corresponding to the client executing the last write in Pending.
Otherwise, if there are no writes in Pending, then it uses the tuple (px, hx, tx) stored in Paths[c].
When Ci receives the reply message for its operation o, it verifies the signatures on all information in the message, and then performs the following checks: is at least as big as the version corresponding to Ci's previous operation, version(oprev).
2.
The timestamp ts(oprev) of Ci's previous operation is equal to vc(oc) [i], as oprev should be the last operation that appears in the context of oc.
3.
If o is a read operation, then (px, hx, tx) indeed corresponds to the last write operation in Pending, or to oc if there are no write operations in Pending.
This can be checked by comparing tx to the timestamp of the appropriate operation in Pending or to ts(oc), respectively.Next, Ci computes version(o), by invoking the function shown in Figure 4, to represent o's context based on the prefix of the history up to oc (represented by version(oc)), and on the sequence of operators in Pending.
The following additional checks require traversing Pending, and are therefore performed during the computation of version(o), which iterates over all operations in Pending:4.
There is at most one operation of every client in Pending, and no operation of Ci, that is, the verifier does not include too many operations in Pending.
1: function compute-version-and-check-pending(o) 2:(vc, vh) ← version(oc) histHash ← vh [c] 4:for q = 1, . . . , |Pending| : // traverse pending ops let C j be the client executing Pending [q] 6:vc[j] ← vc[j] + 1 7: histHash ← hash(histHash񮽙Pending[q])8:vh[j] ← histHash 9:perform checks 4, 5, and 6 (see text below) version ( If one of the checks fails, the application is notified and a failure message is sent to the core set clients, as described in Section 5.4.
An application of Venus registers for two types of callback notifications: consistency notifications, which indicate that some operations have become green and are known to be consistent, and failure notifications, issued when a failure of the storage service or the verifier has been detected.
Below we describe the additional mechanisms employed by the clients for issuing such notifications, including client-toclient communication.Each client Ci maintains an array CVer.
For every client Cj in the core set, CVer [j] holds the biggest version of Cj known to Ci.
The entries in CVer might be outdated, for instance, when Ci has been offline for a while, and more importantly, CVer [j] might not correspond to an operation actually executed by Cj , as we explain next.
Together with each entry of CVer, the client keeps the local time of its last update to the entry.Every time a client Ci completes an operation o, it calculates version(o) and stores it in CVer [i].
To decide whether its own operations are globally consistent, Ci must also collect versions from other clients.
More precisely, it needs to obtain the versions from a majority quorum of clients in the core set.
Usually, these versions arrive via the verifier, but they can also be obtained using direct client-to-client communication.To obtain another client's version via the verifier, Ci piggybacks a version-request message with every submit message that it sends.
The version-request message includes the identifier k of some client in the core set.
In response, the verifier includes Version [k] with the reply message.
When Ci receives the reply message, it updates CVer [k] if the received version is bigger than the old one (of course, the signature on the received version must be verified first).
Whenever Ci executes an operation, it requests the version of another client from the core set in the versionrequest message, going in round-robin over all clients in the core set.
When no application-invoked operations are in progress, the client also periodically (every t dummy time units) issues a dummy-read operation, to which it also attaches version-request messages.
The dummy-read operations are identical to application-invoked reads, except that they do not access the storage service after processing the reply message.
A dummy-read operation invoked by Ci causes an update to Version [i] at the verifier, even though no operation is invoked by the application at Ci.
Thus, clients that repeatedly request the version of Ci from the verifier see an increasing sequence of versions of Ci.It is possible, however, that C k goes offline or crashes, in which case Ci will not see a new version from C k and will not update CVer [k].
Moreover, a faulty verifier could be hiding C k 's new versions from Ci.
To client Ci these two situations look the same.
In order to make progress faced with this dilemma, Ci contacts C k directly whenever CVer [k] does not change for a predefined time period t send .
More precisely, Ci sends the maximal version in CVer to C k , asking C k to respond with a similar message.
When C k is online, it checks for new messages from other clients every treceive time units, and thus, if C k has not permanently crashed, it will eventually receive this message and check that the version is comparable to the maximum version in its array CVer.
If no errors are found, C k responds to Ci with the maximal version from CVer, as demonstrated in Figure 5(a).
Notice that this maximal version does not necessarily correspond to an operation executed by Ci.
All client-to-client messages use email and are digitally signed to prevent attacks from the network.
When a client Ci receives a version directly from C k , it makes sure the received version is comparable with the maximal version in its array CVer.
If the received version is bigger than CVer [k], then Ci updates the entry.Whenever an entry in CVer is updated, Ci checks whether additional operations become green, which can be determined from CVer as explained next.
If this is the case, Venus notifies the application and outputs the timestamp of the latest green operation.
To check if an operation o becomes green, Ci invokes the function in Figure 6, which computes a consistency set C(o) of o.
If C(o) contains a majority quorum of the clients in the core set, the function returns green, indicating that o is now known to be consistent.1: function check-consistency(o) 2:C(o) ← ∅ for each client C k in the core set:4: if CVer[k].
vc[i] ≥ ts(o) then add k to C(o) if C(o) contains a quorum of the core set then return green 8: Ci starts with the latest application-invoked (non-dummy) red operation o, going over its red operations in reverse order of their execution, until the first application-invoked red operation o is encountered that becomes green.
If such an operation o is found, Ci notifies the application that all operations with timestamps smaller than or equal to ts(o) are now green.If at any point a check made by the client fails, it broadcasts a failure message to all core set clients; when receiving such message for the first time, a core set client forwards this message to all other core set clients.
When detecting a failure or receiving a failure message, a client notifies its application and ceases to execute application-invoked and dummy operations.
After becoming aware of a failure, a core set client responds with a failure message to any received version message, as demonstrated in Figure 5(b).
Access to the storage service consumes the bulk of execution time for every operation.
Since this time cannot be reduced by our application, we focus on overlapping as much of the computation as possible with the access to storage.For a read operation, as soon as a reply message is received, the client immediately starts reading from the storage service, and concurrently makes all checks required to complete its current operation.
In addition, the client prepares (and signs) the information about the current operation that will be submitted with its next operation (notice that this information does not depend on the data returned by the storage service).
A write operation is more difficult to parallelize, since a submit message cannot be sent to the verifier before the write to the storage service completes.
This is due to the possibility that a submit message reaches the verifier but the writer crashes before the data is successfully written to the storage service, creating a dangling pointer at the verifier.
If this happens, no later read operation will be able to complete successfully.We avoid this problem by proceeding with the write optimistically, without changing the state of the client or verifier.
Specifically, while the client awaits the completion of its write to the storage, it sends a dummy-submit message to the verifier, as shown in Figure 7.
Unlike a normal submit, this message is empty and thus cannot be misused by the verifier, e.g., by presenting it to a reader as in the scenario described above.
When receiving a dummy-submit message, the verifier responds with a reply message identical to the one it would send for a real submit message (notice that a reply message for a write operation does not depend on the contents of the submit message).
The writer then optimistically makes all necessary checks, calculations and signatures.
When storing the data is complete, the client sends a submit message to the verifier.
If the reply message has not changed, pre-computed information can be used, and otherwise, the client re-executes the checks and computations for the newly received information.Venus creates a new low-level object at the storage provider for every write operation of the application.
In fact, this is exactly how updates are implemented by most cloud storage providers, which do not distinguish between overwriting an existing object and creating a new one.
This creates the need for garbage collection.
We have observed, however, that with Amazon S3 the cost of storing multiple low-level objects for a long period of time is typically much smaller than the cost of actually uploading them (which is anyway necessary for updates), thus eager garbage collection will not significantly reduce storage costs.
In Venus, each client pe- riodically garbage-collects low-level objects on storage corresponding to its outdated updates.
We have described Venus for a static set of clients so far, but in fact, Venus supports dynamic client joins.
In order to allow for client joins, clients must have globally unique identifiers.
In our implementation these are their unique email addresses.
All arrays maintained by the clients and by the verifier, including the vector clock and the vector of hashes in versions, are now associative arrays, mapping a client identifier to the corresponding value.
Clients may also leave Venus silently but the system keeps their entries in versions.The verifier must not accept requests from clients for which it does not have a public key signed by some client in the core set.
As mentioned in Section 3, every client wishing to join the system knows the core set of clients and their public keys.
To join the system, a new client Ci sends a join message, including its public key, to some client in the core set; if the client does not get a response it periodically repeats the process until it gets a successful response.
When receiving a join request from Ci, a client Cj in the core set checks whether Ci can be permitted access to the service using the externally defined access policy, which permits a client to access Venus if and only if the client may also access the object at the storage service.
If access to Ci is granted, Cj still needs to verify that Ci controls the public key in the join message.
To this end, Cj asks the joining client to sign a nonce under the supplied public key, as shown in Figure 8.
acknowledged its receipt, Cj sends a final acknowledgment to Ci, and from this time on, Ci may invoke read and write operations in Venus.
The verifier informs a client Ci about clients that are yet unknown to Ci, by including their signed public keys in reply messages to Ci.
In order to conclude what information Ci is missing, the verifier inspects version(oprev) received from Ci in the submit message, where it can see which client identifiers correspond to values in the associative arrays.
A client receiving a reply message extracts all public keys from the message and verifies that the signature on each key was made by a client from the core set.
Then, it processes the reply message as usual.
If at any time some information is received from the verifier, but a public key needed to verify this information is missing, then Ci concludes that the verifier is faulty and notifies its application and the other clients accordingly.
We implemented Venus in Python 2.6.3, with Amazon S3 as the storage service.
Clients communicate with S3 using HTTP.
Communication with the verifier uses direct TCP connections or HTTP connections; the latter allow for simpler traversal of firewalls.Client-to-client communication is implemented by automated emails.
This allows our system to handle offline clients, as well as clients behind firewalls or NATs.
Clients communicate with their email provider using SMTP and IMAP for sending and receiving emails, respectively.
Clients are identified by their email addresses.For signatures we used GnuPG.
Specifically, we used 1024-bit DSA signatures.
Each client has a local key-ring where it stores the public keys corresponding to clients in our system.
Initially the key-ring stores only the keys of the clients in the core set, and additional keys are added as they are received from the verifier, signed by some client in the core set.
We use SHA-1 for hashing.Venus does not access the versioning support of Amazon S3, which was announced only recently, and relies on the basic key-value store functionality.To evaluate how Venus detects service violations of the storage service and the verifier, we simulated some attacks.
Here we demonstrate one such scenario, where we simulate a "split-brain" attack by the verifier, in a system with two clients.
Specifically, the verifier conceals operations of each client from the other one.
Figure 9 shows the logs of both clients as generated by the Venus client-side library.
We observe that one email exchange suffices to detect the inconsistency.
We report on measurements obtained with Venus for clients deployed at the Technion (Haifa, Israel), Amazon S3 with the US Standard Region as the storage service, and with the verifier deployed at MIT (Cambridge, USA) and locally at the Technion.The clients in our experiments run on two IBM 8677 Blade Center chassis, each with 14 JS20 PPC64 blades.
We dedicate 25 blades to the clients, each blade having 2 PPC970FX cores (2.2 GHz), 4GB of RAM and 2 BroadCom BCM5704S NICs.
When deployed locally, the verifier runs on a separate HS21 XM blade, Intel QuadCore Xeon E5420 with 2.5GHz, 16GB of RAM and two BroadCom NetXtreme II BCM5708S NICs.
In this setting the verifier is connected to the clients by a 1Gb Ethernet.When run remotely at MIT, the verifier is hosted on a shared Intel Xeon CPU 2.40GHz machine with 2GB RAM.
In this case, clients contact the verifier using HTTP, for tunneling through a firewall, and the requests reach the Venus verifier redirected by a CGI script on a web server.All machines run the Linux 2.6 operating system.
We examine the overhead Venus introduces for a client executing operations, compared to direct, unverified access to S3, which we denote here by "raw S3."
Figure 10 shows the average operation latency for a single client executing operations (since there is a single client in this experiment, operations become green immediately upon completing).
The latencies are shown for raw S3, with the verifier in the same LAN as the client at the Technion, and with the remote verifier at MIT.
Each measurement is an average of the latencies of 300 operations, with the 95% confidence intervals shown.
We measure the average latency for different sizes of the data being read or written, namely 1KB, 10KB, 100KB and 1000KB.
Figure 10 shows that the latency for accessing raw S3 is very high, in the orders of seconds.
Many users have previously reported similar measurements 6,7 .
The large confidence intervals for 1000KB stem from a high variance in the latency (also previously reported by S3 users) of accessing big objects on S3.
The variance did not decrease when an average of 1000 operations was taken.The graphs show that the overhead of using Venus compared to using Amazon S3 directly depends on the location of the verifier.
When the verifier is local, the overhead is negligible.
When it is located far from the clients, the overhead is constant (450-550 ms.) for all measured data sizes.
It stems from one two-way message exchange between the client and verifier, which takes two round-trip times in practice, one for establishing a TCP connection and another one for the message itself.
Although we designed the verifier and the clients to support persistent HTTP connections, we found that the connection remained open only between each client and a local proxy, and was closed and re-opened between intermediate nodes in the message route.
We suspect the redirecting web server does not support keeping HTTP connections open.We next measure the operation latency with multiple clients and a local verifier.
Specifically, we run 10 clients, 3 of which are the core set.
Half of the clients perform read operations, and half of them perform writes; each client executes 50 operations.
The size of the data in this experiment is 4KB.
Figure 11 shows the average time for an operation to complete, i.e., to become red, as well as the time until it becomes green, with t dummy set to 3 sec., or to 5 sec.
Client-to-client communication was disabled for these experiments.One can observe that as the time between user-invoked operations increases, the average latency of green notifications initially grows as well, because versions advance at a slower rate, until the dummy-read mechanism kicks in and from detecting a simulated "split-brain" attack, where the verifier hides each client's operations from the other clients.
System parameters were set to t dummy = 5sec.
, t send = 10sec.
, and t receive = 5sec.
There are two clients in the system, which also form the core set.
After 10 seconds, client #2 does not observe a new version corresponding to client #1 and contacts it directly.
Client #1 receives this email, and finds the version in the email to be incomparable to its own latest version, as its own version does not reflect any operations by client #2.
The client replies reporting of an error, both clients notify their applications and halt.
ensures steady progress.
Of course the time it takes for an operation to complete, i.e., to become red, is not affected by the frequency of invocations.
Knowing that the overhead of our algorithm at the clientside is small, we proceed to test the verifier's scalability and throughput.
Since our goal here is to test the verifier under high load, we perform this stress test with a synthetic multiclient program, which simulates many clients to the server.
The simulated clients only do as much as is needed to flood the verifier with plausible requests.Amazon S3 does not support pipelining HTTP operation requests, and thus, an operation of a client on S3 has to end before that client can invoke another operation.
Consequently, the throughput for clients accessing raw S3 can be expected to be the number of client threads divided by the average operation latency.
In order to avoid side effects caused by contention for processing and I/O resources, we do not run more 2 client threads per each of our 25 dualcore machines, and therefore measure throughput with up to 50 client threads.
As Venus clients access Amazon S3 for each application-invoked operation, our throughput cannot exceed that of raw S3, for a given number of clients.
Our measurements show that the throughput of Venus is almost identical to that of raw S3, as can be seen in Figure 12.
In this paper we presented Venus, a practical service that guarantees integrity and consistency to users of untrusted cloud storage.
Venus can be deployed transparently with commodity online storage and does not require any additional trusted components.
Unlike previous solutions, Venus offers simple semantics and never aborts or blocks client operations when the storage is correct.
We implemented Venus and evaluated it with Amazon S3.
The evaluation demonstrates that Venus has insignificant overhead and can therefore be used by applications that require cryptographic integrity and consistency guarantees while using online cloud storage.
We thank Maxim Gurevich and Jean-Philippe Martin for helpful discussions and the anonymous reviewers for valuable comments.Alexander Shraer was supported by an Eshkol Fellowship from the Israeli Ministry of Science.
This work has also been supported in part by the European Commission through the ICT programme under contract ICT-2007-216676 ECRYPT II.
