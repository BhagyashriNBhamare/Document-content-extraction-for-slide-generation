We present a simple two-person Bucket Game, based on throwing balls into buckets, and we discuss possible players' strategies.
We use these strategies to create an approximation algorithm for a generalization of the well known Set Cover problem, where we need to cover each element by at least k sets.
Furthermore, we apply these strategies to construct a randomized algorithm for Dynamic Page Migration problem achieving the optimal competitive ratio against an oblivious adversary.
In this paper we present a simple two-player Bucket Game.
In this game we have a set of n initially empty buckets, an infinite set of balls and a constant parameter 0 < c < 1.
In each turn, player A associates arbitrarily a non-negative weight w i with each bucket i.
We can easily extend the notion of weight to sets of buckets, i.e. the weight of a set X is the sum of weights of all buckets from X (the total weight is the sum of weights of all n buckets).
On the basis of weights {w i }, player B chooses a subset of buckets, whose weight is at least a fraction c of the total weight, and throws a ball into each bucket from this subset.
The goal of player A is to fill each bucket with at least T balls for a given threshold T in as few rounds as possible, whereas the goal of player B is to postpone it.One of the most straightforward questions that arises is: "Assuming the optimal strategy of player B, how fast can player A achieve the given threshold T ?"
. 񮽙
Extended abstract.
The full version of this paper is available under http://wwwhni.In this paper we address this issue, proving tight bounds in Sect. 2.
The trivial answers to this question which we present are O(T · log n) and Ω(T + log n).
We develop a simple Exponential Balancing technique, which constructively yields an algorithm for player A.
This algorithm is guaranteed to fill all the buckets to the given threshold T in O(T + log n) rounds.
All logs in the paper are to base 2, unless stated otherwise.Although the Bucket Game might be interesting itself, the main contribution of our paper is applying the Exponential Balancing scheme to improve approximation and competitive ratios of Set Multicover and Dynamic Page Migration problems, respectively.
We present the problems and summarize our results, separately, in the two following subsections.
The Set Cover problem is defined as follows.
Given a collection C of subsets of S, such that 񮽙 si∈C s i = S, find a collection C 񮽙 ⊆ C, such that 񮽙 si∈C 񮽙 s i = S and |C 񮽙 | is minimal.The Set Cover problem is NP-complete.
Moreover, it was proved by Raz and Safra [12] that the existence of an (c · log n)-approximation algorithm for Set Cover with c < 1 would imply that NP ⊆ DTIME(n log log n ).
On the other hand, Johnson [8] and Lovász [9] showed two different algorithms, both of which approximate Set Cover within a factor of H n = 񮽙 n i=1 1 i = Θ(log n).
A natural generalization of Set Cover is the Set Multicover problem, where each element x of S needs to be covered by at least l x sets, and each subset s i ∈ C may be used arbitrary number of times.
This problem was addressed by Rajagopalan and Vazirani in [11], where an H n -approximation algorithm was presented.
We will, however, concentrate on the particular case where each element must be covered by at least k subsets (i.e. l x = k for all x ∈ S).
We call this problem k-SetCover.
A similar problem -a variant where each set may be used at most once (known as Constrained Set Multicover) -was recently shown in [3] to have applications to reverse engineering of protein and gene networks.Our Contribution.
We propose an algorithm for the k-SetCover problem, that in polynomial time produces a k-cover with O((k + log n) · c * ) sets where c * is a number of sets in the optimal solution to the classical Set Cover problem.
In other words, c * is the cost of the optimal solution of the original input instance with k = 1.
Furthermore, our algorithm can be extended without loss of approximation guarantee to handle a weighted version of k-SetCover, i.e. the one with different costs charged for using particular sets in the cover.Our algorithm is based on a reduction from Set Cover to Uncapacitated Facility Location (UFL) problem.
The reduction is similar to the one used by Guha and Khuller [7] to prove a lower bound on the approximation factor for UFL.Although we do not improve the approximation ratio for k-SetCover achieved by the greedy algorithm of Vazirani, we believe the result is worth of interest.
It relates the cost of the computed solution to c * which is for some instances Ω(k) times smaller than the optimal solution to k-SetCover.
We give an example of instances for which our algorithm computes Ω(log n) times cheaper solutions.
The Dynamic Page Migration (DPM) problem [4,5] arises in a network of n processors (nodes) v 1 , v 2 , . . . , v n , which share one indivisible memory page (shared variable) of size D.
This variable is stored in the local memory of one of these processors, initially at v 1 .
The processors are placed in a metric space (X , d), i.e. the distances between the points from X are given by the metric d.We assume discrete time steps t = 1, 2, . . ..
At the beginning of the step the adversary may move each node by at most a constant distance ∆, i.e. for any node v i , its positions p t−1 (v i ) and p t (v i ) in two consecutive time steps cannot be too far apart,d(p t−1 (v i ), p t (v i )) ≤ ∆.
A tuple describing the positions of all the nodes in time t is called configuration at time step t and is denoted by C t .
After moving nodes, the adversary chooses one node, denoted by σ t , which wants to access (read or write) a single unit of data from a page.
If the page is stored in its local memory, then such a transaction is free.
Otherwise, the node has to send a request to the processor holding the page, say v * , and appropriate data is sent back.
This incurs a cost, which is defined to be c t (σ t , v * ) := d t (σ t , v * ) + 1.
The function d t (v a , v b ) denotes the distance in step t between any two nodes v a and v b , i.e. d t (v a , v b ) := d(p t (v a ), p t (v b )).
To avoid the problem of maintaining consistency among multiple copies of the page, the model allows only one copy of the page to be stored within the network.
After serving the request, the algorithm may decide to migrate the page to another processor.
The migration cost between two nodes v a andv b is equal to D · c t (v a , v b ).
The goal is to decide, online, when and where to move the page in order to exploit the locality of the request, and minimize the total cost of communication for all possible pairs of sequences of requests (σ t ) and network changes (C t ).
We consider only online algorithms, i.e. the ones which make decision in step t solely on the basis of the initial part of the input up to step t. To analyze the performance of an online algorithm ALG we use competitive analysis [13].
We say that a randomized algorithm ALG is c-competitive, if for all input sequences(C t , σ t ) it holds E[C ALG ((C t , σ t ))] ≤ c · C OPT ((C t , σ t )), where the expected value is taken over all random choices made by an algorithm.
C ALG ((C t , σ t )) and C OPT ((C t , σ t )) are the cost of ALG and the optimal offline solution, respectively, on the input sequence (C t , σ t ).
The factor c is called the competitive ratio of the algorithm.
In this paper we consider only oblivious adversaries [2], which have no access to the random bits used by the algorithm.Related Work.
The case in which network is static, i.e. C t = C t+1 for all time steps t and the constant overhead for communication is not present, called Page Migration, had been introduced in [6].
The best known algorithms (deterministic and randomized), achieving constant competitive ratios for general topologies, were presented in [1,14].
For the DPM problem Bienkowski, Dynia, and Korzeniowski gave in [4] a deterministic, O(min{ √ D · n, D, λ})-competitive algorithm Mark,where λ is the maximum distance between any pair of nodes occurring during runtime.
This result is up to a constant factor optimal due to the matching lower bound for any randomized algorithm playing against an adaptive-online adversary given in [5].
In [4] it was also shown that a direct randomization of Mark yields the algorithm R-Mark which is O(min{ √ D · log n, D, λ})-competitive against an oblivious adversary.
The best known lower bound for this case, given in [4], isΩ(min{ √ D · log n, D 2/3 , λ}).
Our Contribution.
In Sect. 4 we partially close the gap mentioned above.
We use Exponential Balancing technique to approximate the node holding page of the optimal algorithm by an accurate probability distribution over all nodes.
We prove that our algorithm is O( √ D · log n)-competitive.
Since it can be combined with trivial O(D) and O(λ) algorithms from [5], we get an algorithm which is O(min{ √ D · log n, D, λ})-competitive against an oblivious adversary.
In this section we formally define a two-player Bucket Game and discuss possible strategies for each player.
Assume we have a set of n buckets, which are initially empty, numbered from 1 to n and let [n] := {1, 2, . . . , n}.
We also have an infinite set of balls.
For any i ∈ [n], let c i be the current number of balls in bucket i. Let 0 < c < 1 be any fixed constant.
The Bucket Game is played in rounds by two players A and B. Each round of the game is defined as follows.-Player A defines a sequence of non-negative weights {w i } n i=1 and shows it to player B. -Player B chooses some subset X ⊆ [n] of buckets, s.t.񮽙 i∈X w i ≥ c· 񮽙 n i=1 w i , and throws exactly one ball into each bucket from X.The game ends when each of the buckets contains at least T balls (i.e. c i ≥ T for all i ∈ [n]).
The goal of player A is to minimize the number of rounds, while B wants to play as long as possible.Let us make the following simple observations.
First of all, to throw at least one ball into each bucket (i.e. for the case T = 1), O(log n) rounds are sufficient.
Player A simply defines w i = 1 for empty buckets and w i = 0 for non-empty ones.
Then in each round at least a fraction c of empty buckets gets a ball.
Hence, after at most log 1/(1−c) n rounds there is no empty bucket left.
Thus, to fill each bucket to the threshold T , player A can repeat this scheme T times, which yields an upper bound of O(T · log n).
Second, for any T , there exists a player B strategy which prevents finishing the game in less than Ω(T + log n) rounds.
Since each bucket may get at most one ball per round, the number of rounds cannot be smaller than T .
On the other hand, suppose that in every round B chooses the subset with the smallest number of empty buckets.
With this strategy, in the i-th round, at most 񮽙c · e i 񮽙 of empty buckets get a ball, where e i is the number of empty buckets at the beginning of the i-th round.
Thus Ω(log n) rounds are also necessary.Surprisingly, there is a simple player A strategy, which we call Exponential Balancing, and which asymptotically matches the above-mentioned lower bound.
Theorem 1.
For T = 񮽙log 2 n񮽙, there exists a player A strategy which guarantees finishing the game in O(log n) rounds.Proof.
In each round A defines w i = n 2 c i .
We call w i a value of a bucket, and define the total value of the game as W = 񮽙 i∈ [n] w i .
Initially, all buckets are empty, w i = n for all i. Hence, the initial value of the game is n 2 .
In any round each bucket "offers" to player B a half of its current value for putting a ball into this bucket.
According to the rules of the game, B must collect at least a fraction c of this offered value.
Thus, in every round, the game loses at least a fraction 1 2 · c of its value.
If W and W 񮽙 denote the game values in two consecutive rounds, then W 񮽙 ≤ (1 − c/2) · W.
If in some round W ≤ 1, then the threshold T is reached and the game ends.
Precisely,W = 񮽙 i n 2 c i ≤ 1 implies n 2 c i ≤ 1 for all i ∈ [n], and thus c i ≥ log n for all i ∈ [n].
It remains to observe that the value of the game can be reduced from n 2 to 1 in at most log 1/(1−c/2) n 2 = 2 1−log 2 (2−c) · log 2 n rounds.
If threshold T is larger than 񮽙log 2 n񮽙, then player A may act as if he was playing 񮽙 T 񮽙log 2 n񮽙 񮽙 times a game with threshold 񮽙log 2 n񮽙.
By Theorem 1, each of these sub-games lasts O(log n) rounds, and thus the whole game ends after at most O(T ) rounds.
Thus, we get the following.
Corollary 1.
For any T , there exists a player A strategy which guarantees finishing the game in O(T + log n) rounds.
In this section we present a new approximation algorithm for the k-SetCover problem, the modification of the Set Multicover defined in Sect. 1.1.
Our algorithm uses an approximation algorithm for the Uncapacitated Facility Location (UFL) problem as a subroutine.In the following, we say that A is a λ-approximation algorithm for a minimization problem P , if for any instance of the problem P it produces, in polynomial time, a solution with a cost at most λ times higher than the cost of an optimal solution.Uncapacitated Facility Location (UFL) Problem.
In the UFL problem we are given a set F of n f facilities and a set C of n c cities.
For every facility i ∈ F, a non-negative number f i denotes the opening cost of the facility.
Furthermore, for every city j ∈ C and facility i ∈ F, c ij is a connection cost between facility i and city j.
The goal is to open a subset of the facilities F 񮽙 ⊆ F, and connect each city to an open facility so that the total cost is minimized.The UFL problem is NP-complete, and MAX SNP-hard (see [7]).
A UFL instance is metric if its connection cost function satisfies the triangle inequality (i.e. c ij ≤ c ik + c kj for any i, j, k ∈ C ∪ F).
There are several approximation algorithms for the metric UFL problem, the currently best one achieving the approximation ratio of 1.52 [10].
Guha and Khuller [7] have proved by a reduction from Set Cover that there is no polynomial time λ-approximation algorithm for metric UFL with λ < 1.463, unless N P ⊆ DT IM E(n log log n ).
Another of their results was a (1.463 . . . + 񮽙)-approximation algorithm for the case when all the connection costs are either 1 or 3.
We use this algorithm in our construction.Computing Partial Set-Covers.
First, we address a problem of computing partial set-covers for a given instance of the Set Cover problem.
By a set-cover we mean a feasible solution to an instance of the Set Cover problem (i.e. a family C 񮽙 ⊂ C covering every element of S), whereas a partial set-cover is a family C 񮽙񮽙 ⊂ C that covers at least a certain fraction of elements of S.We put weights on elements to indicate that covering certain elements is more important than covering others.
We would like to know how much we can cover with at most k · c * sets, for a constant k ∈ R + and c * denoting the number of sets in an optimal set-cover.
Let λ 0 be the approximation factor of an algorithm for the metric UFL problem with connection costs 1 or 3 (By [7], we may choose λ 0 ≈ 1.463). 񮽙
x∈S w(x), outputs a partial set-cover C p with the number of sets c p ≤ k · c * , and the total weight of elements not covered by C p is at most k(λ0−1)2k−2λ0 · 񮽙 x∈S w(x).
To prove this lemma, we use a reduction of a Set Cover instance to a UFL instance with distances 1 and 3 and an approximation algorithm for the UFL problem.
The core of the reduction and the algorithm were proposed by Guha and Khuller [7].
We slightly extended the original reduction to encode the elements' weights into quantities of groups of cities representing particular elements.
The complete proof of Lemma 1 can be found in the full version of the paper.In Lemma 1 we bounded the fraction of uncovered weight of elements byk(λ0−1)2k−2λ0 , when k · c * subsets are used.
Suppose we want to cover certain fraction c of elements' weight and wonder how many sets do we need to use.
We may consider the uncovered weight fraction 1−c = k(λ0−1) 2k−2λ0 and conclude the following.
2 ≈ 0.768 there exists an algorithm that, in polynomial time, computes a partial set-cover that covers at least a fraction c of elements' weight, using at most 2λ0·(1−c) 3−λ0−2c · c * sets.
We call this algorithm Set-UFL c .
Now we combine Corollary 2 with Theorem 1 to present an algorithm for the k-SetCover problem.
First, we present an algorithm A 1 (see Fig. 1), solving the 1.
Sol ← ∅ /* empty multiset */ 2.
guess c * -the number of sets in an optimal set-cover a 3.
define weight function w : S → N+ 4.
compute a partial set-cover Cp ← Set-UFLc((S, C), c * , w) 5.
add this partial set-cover to the current solution (Sol ← Sol ∪ Cp) 6.
if Sol does not cover each element of S at least 񮽙log 2 n񮽙 times, go to step 3 7.
return Sol a There are only polynomially many possible values of c * .
problem for k = log 2 n, where n = |S|.
A1, given an instance of a (log 2 n)-SetCover problem (S, C), produces as a solution a multiset Sol.
Algorithm A1, such that the algorithm produces, in polynomial time, a solution to (log 2 n)-SetCover instance with at most O(log n · c * ) sets.Proof.
We use the Exponential Balancing technique to upper-bound the total number of sets in Sol.
Let c ∈ (0, 0.768) be a fixed parameter of the Algorithm A1.
From Corollary 2, in step 4 of Algorithm A1, we may cover a fraction c of weight with at most 2λ0(1−c) 3−λ0−2c · c * sets.
Defining a weight function w in step 3 is like playing a role of player A in the Bucket Game, except for the fact that now the weights are restricted to be positive integers.
Fortunately, the proof of Theorem 1 may be easily modified to use only integer, polynomially bounded weights.
Hence, by Theorem 1, we may bound the number of used partial covers by 2 log 2 n 1−log 2 (2−c) .
Concluding, |Sol| -the total number of used sets, may be bounded as|Sol| ≤ 4λ 0 (1 − c) (1 − log 2 (2 − c))(3 − λ 0 − 2c) · log 2 n · c * .
(1)When we set c = 0.553 and λ 0 ≈ 1.463, we obtain |Sol| < 12.006 · log 2 n · c * . 񮽙 񮽙
Theorem 2.
There exists an algorithm that for any instance of the k-SetCover problem, in polynomial time, produces a k-set-cover with O((k + log n) · c * ) sets, where c * is the number of sets in an optimal classical set-cover.
Proof.
Let r = 񮽙k/񮽙log 2 n񮽙񮽙, and let r * Sol denote a multi-set containing the same elements as the multi-set Sol, but the quantity of each element e in r * Sol is r times the quantity of e in Sol.
We use Algorithm A1 to compute a solution Sol, that covers each element at least 񮽙log 2 n񮽙 times, and multiply it r times to obtain a solution r * Sol.
By Lemma 2, Sol has O(log n · c * ) sets.
Thus, r * Sol has O((k + log n) · c * ) sets.
Note on Weighted Version of Set Multicover.
Like in the case of the classical Set Cover problem, one may generalize the Set Multicover problem by defining a weight function on the subsets representing the cost of using a particular set in the solution.
Same as in the unweighted version, we define the cost of using a set l times to be l times the cost of using this set only once.All the results concerning the Set Multicover problem (especially Theorem 2) presented in this paper may be easily generalized to the Weighted Set Multicover problem formulation.
The proof will be presented in the full version of the paper.
k-SetCover problem, in polynomial time, produces a k-set-cover with cost at most O(k + log n) times the cost of an optimal solution to the classical Weighted Set Cover problem on this instance.Motivating Example.
Let us consider the following instances of the (log n)-SetCover problem.
Let the family C consist of subsets P 1 , P 2 , . . . , P n+1 with weights c 1 , c 2 , . . . , c n+1 such that P j = {j} and c j = 1/j for j = 1, 2, . . . , n, whereas P n+1 = {1, 2, . . . , n} and c n+1 = 1 + 񮽙 for some 񮽙 ∈ R + .
Consider the greedy algorithm of Vazirani, i.e. the algorithm that consecutively chooses the most cost effective set.
If we use it to produce (log n)-set-cover for the instance above, it outputs a multi-set containing each of the sets P 1 , P 2 , . . . , P n log n times.
This solution has a cost equal to H n · log n. However, since the optimal solution to the classical Set Cover has cost c * = 1 + 񮽙, by Theorem 3, our algorithm produces (log n)-set-cover with cost O(log n).
In this section we design a randomized algorithm EBM (Exponential Balancing Marking) and prove that it achieves a competitive ratio of O( √ D · log n).
Our algorithm is based on the Mark algorithm [4].
First, we construct a marking scheme, which induces the partition of input sequence into epochs.
This partition is independent of the algorithm, and depends only on the input.
On the basis of the computed marking we construct the EBM algorithm.
We also use Bucket Game as an underlying concept, however the relation between EBM and the game is more obscure here.Marking Scheme.
We divide input sequence into chunks of length K := 2 · 񮽙 D/ log n time steps.
The partitioning of input sequence into epochs is performed as follows.
Each epoch consists of some non-empty sequence of chunks.
Let M i be the number of marks that v i has; initially all M i are set to 0.
Marks are DPM's equivalents of the balls from the Bucket Game.The first epoch starts with the beginning of the input.
Let E denote the current epoch; at the beginning of input sequence E = ∅.
By a subsequence we understand any time interval of the input sequence.
For any subsequence S and any v i ∈ V , let A i (S) denote the cost (of serving requests) of an algorithm which remains in v i for the whole S and does not move its page.
After each chunk I j we run the marking routine depicted in Fig. 2.
Marking of v i is computed entirely on the basis of A i (E).
If at the end of I j node v i becomes marked (with one orE := E E Ij for each vi ∈ V do Mi := 񮽙Ai(E ) · log n D 񮽙if Mi ≥ log n for all vi ∈ V then for each vi ∈ V do set Mi := 0.
E := ∅ /* beginning of a new epoch */ Fig. 2.
Marking routine after chunk Ij more marks), i.e. the corresponding value of M i increases, then I j is called a marking chunk for v i , and we say that v i is marked in I j .
Additionally, if S is any subsequence, then by M i (S) and M 񮽙 i (S) we denote the number of marks v i has before S and after S, respectively.
We also define ∆M i (S) = M 񮽙 i (S)−M i (S).
An epoch ends when all nodes are marked at least log n times.As an important fact, we get that C OPT (E) = Ω(D) for each epoch E. Actually, if OPT remains at one node v i , then v i is marked at least log n times during E, and thus OPT pays A i (E) ≥ log n · D log n = D. Otherwise, OPT pays at least D for moving the page between nodes.Jump Sets.
Before we construct our algorithm, we adapt the construction of Jump Sets from [4] for our needs.
We consider one single chunk I and we number time steps within I from 1 to K.
Then σ i denotes the node which issues a request in the i-th step of I, and d i (·), c i (·) are the distance and cost functions in the i-th step.
The following lemma is a simple reformulation of [4-Lemma 2].
Definition 2.
A gravity center for I is a vertex v, which minimizes the sum񮽙 K i=1 c K (v, σ i ).
If there is more that one such vertex, then we choose any of them.
We denote this node by G I .
Definition 3.
For any chunk I and any integer k ≥ 1, a k-JumpSet, which we denote by J k (I), is the set of all nodes whose distance to G I , measured in the last step of I is at most 9· k · K, i.e. J k (I) = {v ∈ V : d K (v, G I ) ≤ 9 · k · K}.
Lemma 3.
For any chunk I of K steps, any node v i ∈ V , and any k ≥ 1, if v i / ∈ J k (I) at the and of I, thenA i (I) ≥ k 4 · K 2 ≥ k · D log n .
This implies that if v i gets k marks in I, then v i ∈ J k+1 (I).
Algorithm EBM works in chunks, i.e. it remains at one node for a chunk, and then at the end of the chunk, it makes its decision (where to move the page) on the basis of computed gravity centers and marking.
If chunk I is the last chunk of the epoch, then EBM moves its page to G I .
Otherwise, if the node holding the algorithm's page is marked in I, then at the end of I, EBM chooses randomly a node v * , further called I-JumpCandidate, and moves its page to v * .
Any node v i is chosen with probability 2 −Mi /(񮽙 i∈[n] 2 −Mi ).
Theorem 4.
The algorithm EBM is O( √ D · log n)-competitive.Consider any epoch E, and let m be the number of its chunks, i.e. E = (I 1 , I 2 , . . . , I m ).
Movements of EBM's page partition E into a sequence of p phases, i.e. E = (P 1 , P 2 , . . . , P p ), each phase consisting of one or more chunks.
In one phase P i , EBM remains at one node, denoted by P EBM (P i ).
First, we prove that the expected number of phases in one epoch is bounded.Lemma 4.
The expected number of phases in one epoch is O(log n).
Proof.
We define a value of a node after any chunk I as n·2 −M 񮽙 i (I) .
The total value after I is defined as the sum of nodes' values, i.e.W I := 񮽙 i∈[n] n · 2 −M 񮽙 i (I).
We make two key observations.
First, W I is monotonically non-increasing within E. Second, W I ≤ n 2 for any chunk I ∈ E, and W Im−1 ≥ 1 (because after I m−1 at least one node has less marks than log n).
EBM starts E at some node v and remains there till v becomes marked.
This first phase lasts for at least one chunk, and after it the total value is at most n 2 .
After the first phase, for the analysis, we may safely assume that EBM chooses a jump candidate v * at the beginning of a phase, moves its page to v * and remains at v * till it becomes marked.
Thus, this choice of a node v i determines where the phase ends: either at the first marking chunk for v i , or at I m , if v i is not marked in the remaining part of E.
This chunk we call stopping for v i .
We show that, with probability at least 1/2, one phase reduces the total value by a constant factor or is the last phase in the epoch.
We call such phase successful.
If the total value is reduced below 1, then the corresponding phase ends with I m−1 or with I m , and thus at most one additional phase consisting of the last chunk I m is sufficient.
Thus, O(log n) successful phases are sufficient to finish the whole epoch, and therefore the expected number of phases in epoch is O(2 · log n).
Consider the beginning of any phase.
We sort the nodes in the order induced by their stopping chunks, obtaining a sorted sequence v i1 , . . . , v in .
Let p i1 , . . . , p in be the probabilities of choosing these nodes as jump candidates.
Let j be the smallest index for which 񮽙 j k=1 p i k ≥ 1/2, and I 񮽙 be the stopping chunk for v ij .
Since j is the smallest index with this property, it follows immediately that, with probability 񮽙 n k=j p i k ≥ 1/2, EBM chooses one of v ij , v ij+1 , . . . , v in as a jump candidate.
Any such choice guarantees that the phase lasts at least to the end of I 񮽙 .
If I 񮽙 = I m , then the process ends here and the lemma follows.
Otherwise, note that between the end of I and the end of I 񮽙 , v i1 , v i2 , . . . , v ij are marked at least once.
Since probabilities p i k are directly proportional to the corresponding values of nodes, and 񮽙 j k=1 p i k ≥ 1/2, these nodes' values constitute at least half of the total value W I .
By marking them once, one half of their values (and thus at least 1/4 of the total value) is removed.
Thus,W I 񮽙 ≤ 3 4 · W I .
Before we analyze the cost of EBM in a single phase, we introduce the notion of potential Φ.
If the distance between nodes holding the pages of OPT and EBM is equal to L, then Φ := 2 · D · L.
If S is any subsequence of steps, then by ∆Φ(S) we denote the difference between the potential right after and right before S. By an amortized cost of an action we understand the actual cost of this action plus the change in the potential this action induced.
In the following we bound the amortized cost of EBM in any phase P j .
Assume that P j consists of 񮽙 chunks numbered from 1 to 񮽙, i.e. P j = (I 1 , I 2 , . . . , I 񮽙 ), and consider the following thought experiment.
At the end of P j , instead of moving directly to I 񮽙 -JumpCandidate v * , EBM first moves its page to G I 񮽙 , and then to v * .
Note, that for the last phase in E we do not need the latter part of this movement.
Obviously, the (amortized) cost of this combined movement upper-bounds the (amortized) cost of the actual move.We denote the part of the amortized cost EBM pays for serving all the requests in P j and moving to G I 񮽙 by B A (P j ).
The remaining part of the cost depends on the random choice of v * .
Since we are interested only in the expected value of this variable, we can view this move as transporting parts of the page from G I 񮽙 to the nodes.
These transports are schematically presented in Fig. 3.
Precisely, to node v i we transport a part p i := 2 −M 񮽙 i (Pj ) /( 񮽙 k 2 −M 񮽙 k (Pj ) ) of the page, paying p i · D · c K (G I 񮽙 , v i ).
We divide the total amortized cost of this transport into two parts: a transport within the boundary of the 1-JumpSet (dashed lines in Fig. 3), denoted by B B (P j ), and a transport from this boundary to the appropriate nodes (dotted lines in Fig. 3), denoted by B C (P j ).
We note that B B (P j ) and B C (P j ) are random variables.
The following two lemmas are straightforward generalizations of [5- Lemma 3,4].
The complete proof of Lemma 7 can be found in the full version of the paper.
Here we mention only that the proof uses Lemma 3 to argue, that the B C part of cost in one phase can be high (i.e. the page is transported far away from the gravity center), only if these far nodes received a lot of marks in this phase.
This implies that an epoch E cannot contain many of such phases.Finally, we can combine the lemmas above to prove EBM's competitiveness.Proof (of Theorem 4).
Consider any epoch E and let E = (P 1 , P 2 , . . . P p ) be its division into phases.
Then, E[C EBM (E) + ∆Φ(E)] ≤ 񮽙 p j=1 [B A (P j ) + B B (P j )] + E[ 񮽙 p j=1 B C (P j )], and by Lemmas 5, 6, and 7, the amortized cost is bounded by O(D/K)·C OPT (E)+E[p]·O(D·K)+O(D·K ·log n).
Thus, using E[p] = O(log n) and C OPT (E) ≥ D, we finally get E[C EBM (E) + ∆Φ(E)] ≤ O( √ D · log n) · C OPT (E).
By summing this inequality over all the epochs, the proof follows. 񮽙 񮽙
