RDF graph has been increasingly used to store and represent information shared over the Web, including social graphs and knowledge bases.
With the increasing scale of RDF graphs and the concurrency level of SPARQL queries, current RDF systems are confronted with inefficient concurrent query processing on massive data paral-lelism, which usually leads to suboptimal response time (latency) as well as throughput.
In this paper, we present Wukong+G, the first graph-based distributed RDF query processing system that efficiently exploits the hybrid parallelism of CPU and GPU.
Wukong+G is made fast and concurrent with three key designs.
First, Wukong+G utilizes GPU to tame random memory accesses in graph exploration by efficiently mapping data between CPU and GPU for latency hiding, including a set of techniques like query-aware prefetch-ing, pattern-aware pipelining and fine-grained swapping.
Second, Wukong+G scales up by introducing a GPU-friendly RDF store to support RDF graphs exceeding GPU memory size, by using techniques like predicate-based grouping, pairwise caching and look-ahead replacing to narrow the gap between host and device memory scale.
Third, Wukong+G scales out through a communication layer that decouples the transferring process for query metadata and intermediate results, and leverages both native and GPUDirect RDMA to enable efficient communication on a CPU/GPU cluster.
We have implemented Wukong+G by extending a state-of-the-art distributed RDF store (i.e., Wukong) with distributed GPU support.
Evaluation on a 5-node CPU/GPU cluster (10 GPU cards) with RDMA-capable network shows that Wukong+G outperforms Wukong by 2.3X-9.0X in the single heavy query latency and improves latency and throughput by more than one order of magnitude when facing hybrid workloads.
Resource Description Framework (RDF) is a standard data model for the Semantic Web, recommended by W3C [5].
RDF describes linked data as a set of triples forming a highly connected graph, which powers information retrievable through the query language SPARQL.
RDF and SPARQL have been widely used in Google's knowledge graph [22] and many public knowledge bases, such as DBpedia [1], PubChemRDF [38], Wikidata [8], Probase [59], and Bio2RDF [10].
The drastically increasing scale of RDF graphs has posed a grand challenge to fast and concurrent queries over large RDF datasets [17].
Currently, there have been a number of systems built upon relational databases, including both centralized [40,12,58] and distributed [48,44,23] designs.
On the other hand, Trinity.RDF [62] uses graph exploration to reduce the costly join operations in intermediate steps but still requires a final join operation.
To further accelerate distributed query processing, Wukong [51] leverages RDMA-based graph exploration to support massively concurrency queries with low latency requirement and adopts full-history pruning to avoid the final join operation.Essentially, many RDF queries have embarrassing parallelism, especially for heavy queries, which usually touch a large portion of the RDF graph on an excessive amount of paths using graph exploration.
This poses a significant challenge even for multicore CPUs to handle them efficiently, which usually causes lengthy execution time.
For example, the latency differences among seven queries in LUBM [7] is more than 3,000X (0.13ms and 390ms for Q5 and Q7 accordingly).
This may cause one heavy query block all other queries, substantially extending the latency of other queries and dramatically impairing the throughput of processing concurrent queries [51].
This problem has also gained increased attention [45].
In this paper, we present Wukong+G 1 with a novel design that exploits a distributed heterogeneous CPU/GPU cluster to accelerate heterogeneous RDF queries based on distributed graph exploration.
Unlike CPUs pursuing the minimized execution time for single instructions, GPUs are designed to provide high computational throughput for massive simple control-flow operations with little or no control dependency.
Such features expose a design space to distribute hybrid workloads by offloading heavy queries to GPUs.
Nevertheless, different from many traditional GPU workloads, RDF graph queries are memory-intensive instead of computeintensive: there are limited arithmetic operations and most of the processing time is spent on random memory accesses.
This unique feature implies that the key of performance optimizations in Wukong+G is on smart Q L Q H Fig. 1: A sample of RDF data and two SPARQL queries (Q H and Q L ).
White circles indicate the normal vertices (subjects and objects); dark circles indicate the (type and predicate) index vertices.
Q H is a heavy query, and Q L is a light query.
memory usage rather than improving the computation algorithm.
Wukong+G is made fast and concurrent with the following key designs:GPU-based query execution ( §4.1).
To achieve the best performance for massive random accesses demanded by heavy queries, Wukong+G leverages the many-core feature and latency hiding ability of GPUs.
Besides making use of hardware advantages, Wukong+G surmounts the limitations of GPU memory size and PCIe (PCI Express) bandwidth by adopting query-aware prefetching to mitigate the constraints on graph size, pattern-aware pipelining to hide data movement cost, and fine-grained swapping to minimize data transfer size.GPU-friendly RDF store ( §4.2).
To support desired CPU/GPU co-execution pattern while still enjoying the fast graph exploration, Wukong+G follows a distributed in-memory key/value store and proposes a predicatebased grouping to aggregate keys and values with the same predicate individually.
Wukong+G further smartly manages GPU memory as a cache of RDF store by supporting pairwise caching and look-ahead replacing.Heterogeneous RDMA communication ( §4.3).
To preserve better communication efficiency in a heterogeneous environment, Wukong+G decouples the transferring process of query metadata and intermediate results for SPARQL queries.
Wukong+G uses native RDMA to send metadata like query plan and current step among CPUs, and uses GPUDirect RDMA to send current intermediate results (history table) directly among GPUs.
This preserves the performance boost brought by GPUs from potential expensive CPU/GPU data transfer cost.We have implemented Wukong+G by extending Wukong [51], a state-of-the-art distributed RDF query system to support heterogeneous CPU/GPU processing.
To confirm the performance benefit of Wukong+G, we have conducted a set of evaluations on a 5-node CPU/GPU cluster (10 GPU cards) with RDMA-capable network.
The experimental results using the LUBM [7] benchmark show that Wukong+G outperforms Wukong by 2.3X-9.0X in the single heavy query latency and improves latency and throughput by more than one order of magnitude when facing hybrid workloads.
An RDF dataset is composed by triples, in the form of sub ject, predicate, ob ject.
To construct a graph (aka RDF graph), each triple can be regarded as a directed edge (predicate) connecting two vertices (from subject to object).
In Fig. 1, a simplified sample RDF graph of LUBM dataset [7] includes two professors (Logan and Erik), three students (Marie, Bobby, and Kurt), and two courses (OS and DS).
2 There are also three predicates (teacherOf (to), advisor (ad) and takeCourse (tc)) to link them.
Two types of indexes, predicate and type, are added to accelerate query processing on RDF graph [51].
SPARQL, a W3C recommendation, is a standard query language developed for RDF graphs, which defines queries regarding graph patterns (GP).
The principal part of SPARQL queries is as follows:Q := SELECT RD WHERE GPwhere (RD) is the result description and GP consists of triple patterns (TP).
The triple pattern looks like a normal triple except that any constant can be replaced by a variable (e.g., ?
X) to match a subgraph.
The result description RD contains a subset of variables in the triple patterns (TP) to define the query results.
For example, the query Q H in Fig. 1 asks for professors (?
X), courses (?
Y) and students (?
Z) such that the professor advises (ad) the student who also takes a course (tc) taught by (to) the professor.
After exploring all three TPs in Q H on the sample graph in Fig. 1, the exact match of RD (?
X, ?
Y and ?
Z) is only a binding of Logan, OS, and Bobby.Query processing on CPU.
There are two representative approaches adopted in state-of-the-art RDF systems, (relational) triple join [40,58,12,23] and graph exploration [62,51].
A recent study [51] found that graph exploration with full-history pruning can provide low latency and high throughput for concurrent query processing.
Therefore, we illustrate this approach to demonstrating the query processing on CPU with the sample RDF graph and SPARQL query (Q H ) in Fig. 1.
As shown in Fig. 2, all triple patterns of the query (Q H ) will be iterated in sequence (➊) to generate the results (history table) by exploring the graph, which is stored in an in-memory key/value store.
According to the variable (?
Y) of the current triple pattern (TP-2), each row of a certain column in the history table (➋) will be combined with the constant (takesCourse) of the triple pattern as the key (➌) to retrieve the value (➍).
The value will be appended to a new column (?
Z) of the history table (➎).
Note that an extra triple pattern (TP-0) from an index vertex (teacherOf) will be used to collect all start vertices satisfying a variable (?
X) in TP-1.
Full-history pruning.
Since processing RDF query by graph exploration needs to traverse the RDF graph, it is crucial to prune infeasible paths for better performance.
There are basically two approaches: partial-history pruning [62], by inheriting partial history information (intermediate results) from previous steps of traversing to prune the following traversal paths; and full-history pruning [51], by passing the history information of all previous traversal steps for pruning.
Wukong has exploited full-history pruning to prune unnecessary intermediate results precisely and make all traversal paths completely independent.
Thanks to the fast RDMAcapable network as well as the relative cost-insensitivity of one-sided RDMA operations regarding payload size, full-history pruning is very effective and efficient to handle concurrent queries.Workload heterogeneity.
Prior work [62,23,51] has observed that there are two distinct types of SPARQL queries: light and heavy.
Light queries (e.g., Q L in Fig. 1) usually start from a (constant) normal vertex and only explore a few paths regardless of the dataset size.
In contrast, heavy queries (e.g., Q H in Fig. 1) usually start from an (type or predicate) index vertex and explore massive amounts of paths, which increases along with the growth of dataset size.
The top of Fig. 3 demonstrates the number of paths explored by two typical queries (Q5 and Q7) on LUBM-10240 (10 vs. 16,000,000).
The heterogeneity in queries can result in tremendous latency differences on state-of-the-art RDF stores [51], even reaching more than 3,000X (0.13ms and 390ms for Q5 and Q7 on LUBM-10240 accordingly).
3 Therefore, the multi-threading mechanism is widely used by prior work [23,62,51] to improve the performance of heavy queries.
However, such approach is intrinsically restricted by the limited computation resource of CPU.
Currently, the maximum number of cores in a commercial CPU processor is usually less than 16.
Moreover, 3 Detailed experimental setup and results can be found in §6.
the lengthy queries will significantly extend the latency of light queries and impair the throughput of processing concurrent queries.
Some CPU systems like Oracle PGX [4] try to address this issue by adopting priority mechanism.
However, with no variation of computing power, the sacrifice of user experience for one type of queries is unavoidable.
Hardware heterogeneity.
With the prevalence of computational workloads (e.g., machine learning and data mining applications), it is now not uncommon to see server-class machines equipped with GPUs in the modern datacenter.
As a landmark difference compared to CPU, the number of GPU cores (threads) can easily exceed two thousand, which far exceeds existing multicore CPU processors.
As shown in Fig. 3, in a typical heterogeneous (CPU/GPU) machine, CPU and GPU have their private memory (DRAM) connected by PCIe with limited bandwidth (10GB/s).
Compared to host memory (CPU DRAM), device memory (GPU DRAM) has much higher bandwidth (288GB/s vs. 68GB/s) but less capacity (12GB vs. 128GB).
Generally, GPU is optimized for performing massive, simple and independent operations with intensive accesses on a relatively small memory footprint.
GPUDirect is a family of technologies that is continuously developed by NVIDIA [3].
Currently, it can support various efficient communications, including internode, intra-node, and inter-GPU.
RDMA (Remote Direct Memory Access) is a networking feature to directly access the memory of a remote machine, which can bypass remote CPU and operating system, and avoid redundant memory copy.
Hence, it has unique features like high speed, low latency and low CPU overhead.
GPUDirect RDMA has been introduced in NVIDIA Kepler-class GPUs, like Tesla and Quadro series.
This technique enables direct data transfer between GPUs by InfiniBand NICs as the name suggests [2].
Though prior work (e.g., Wukong [51]) has successfully demonstrated the low latency and high throughput of running light queries solely by leveraging graph exploration with full-history pruning, it is still incompetent to handle heavy queries efficiently.
This leads to suboptimal performance when facing hybrid workloads comprising both light and heavy queries.This problem is not due to the design and implementation of existing state-of-the-art systems, which have been heavily optimized by several approaches including multithreading [62,23,51] and work-stealing scheme [51].
We attribute the performance issues mainly to the lim- itation of handling hybrid workloads (light and heavy queries) on the homogeneous hardware (CPU), which can provide neither sufficient computation resources (a few cores) nor efficient data accesses (low bandwidth).
GPU is a good candidate to host heavy queries.
First, the graph exploration strategy for query processing heavily relies on traversing massive paths on the graph store, which is a typical memory-intensive workload targeted by GPU's high memory bandwidth.
Second, the memory latency hiding capability of GPU is inherently suitable for the random traversal on RDF graph, which is notoriously slow due to poor data locality.
Third, every traversal path with the full-history pruning scheme is entirely independent, which can be fully parallelized on thousands of GPU cores.In summary, the recent trend of hardware heterogeneity (CPU/GPU) opens an opportunity for running different queries on different hardware; namely, running light queries on CPUs and heavy queries on GPUs.
System architecture.
An overview of Wukong+G's architecture is shown in Fig. 4.
Wukong+G assumes running on a modern cluster connected with RDMA-capable fast networking, where each machine is equipped with one or more GPU cards.
The GPU's device memory is treated as a cache for the large pool of the CPU's host memory.
Wukong+G targets various SPARQL queries over a large volume of RDF data; it scales by partitioning the RDF graph into a large number of shards across multiple servers.
Wukong+G may duplicate edges to make sure each server contains a self-contained subgraph (e.g., no dangling edges) of the input RDF graph for better locality.
Note that there are no replicas of vertices in Wukong+G as no vertex data needs to synchronize.
Moreover, Wukong+G also creates index vertices [51] for types and predicates to assist query processing.Similar to prior work [51], Wukong+G follows a decentralized, shared-nothing, main-memory model on the server side.
Each server consists of two separate layers: query engine and graph store.
The query engine layer employs a worker-thread model by running N worker threads atop N CPU cores and dedicates one CPU core to run an agent thread; the agent thread will assist the worker threads on GPU cores to run queries.
Each worker/agent thread on CPU has a task queue to continuously handle queries from clients or other servers, one at a time.
The graph store layer adopts an RDMA-friendly key/value store over a distributed hash table to support a partitioned global address space.
Each server stores a partition of the RDF graph, which is shared by all of worker/agent threads on the same server.Wukong+G uses a set of dedicated proxies to run the client-side library and collect queries from massive clients.
Each proxy parses queries into a set of stored procedures and generates optimal query plans using a cost-based approach.
The proxy will further use the cost to classify a query into one of two types (light or heavy), and deliver it to a worker or agent thread accordingly.
4Basic query processing on GPU.
In contrast to the query processing on CPU, which has to perform a triple pattern with massive paths in a verbose loop style (see Fig. 2), Wukong+G can fully parallelize the graph exploration with thousands of GPU cores.
The basic approach is to dedicate one CPU core to perform the control-flow of the query, and use massive GPU cores to parallelize the data-flow of the query.
As shown in Fig. 5, the agent thread on CPU core will first read the next triple pattern (➊) of the current query and prepare a cache of RDF datasets on GPU memory (➋).
After that, the agent thread will leverage all GPU cores to perform the triple pattern in parallel (➌).
Each worker thread on GPU core can independently fetch a row in the history table (➍) and combine it with the constant (takesCourse) of the triple pattern (TP-2) as the key (➎).
The value retrieved by the key (➏) will be appended to a new column (?
Z) of the history table (➐).
While the hybrid design seems intuitive, Wukong+G still faces three challenges to run SPARQL queries on GPUs, which will be addressed by the techniques in §4: C1: Small GPU memory.
It is well known that GPU can obtain optimal performance only when the device memory (GPU DRAM) gets everything ready.
Prior systems [62,23,51] can store an entire RDF graph in the host memory (CPU DRAM) since it is common that server-class machines equip with several hundred GBs of memory.
However, this assumption does not apply to GPU since its current memory size usually stays less than 16GB.
We should not allow device memory size to limit the upper bound of the supported working sets.TP-0 TP-1 TP-2 ...C2: Limited PCIe bandwidth.
The memory footprint of SPARQL queries may touch arbitrary triples of the RDF graph.
Therefore, the data transfer between CPU and GPU memory during query processing is unavoidable, especially for concurrent query processing.
However, GPUs are connected to CPUs by PCIe (PCI Express), which has insufficient memory bandwidth (10GB/s).
To avoid the bottleneck of data transfer, we should carefully design mechanisms to predict access patterns and minimize the number, volume and frequency of data swapping.C3: Cross-GPU communication.
With the increasing scale of RDF datasets and the growing number of concurrent queries, it is highly demanding that query processing systems can scale to multiple machines.
Prior work [57,51] has shown the effectiveness and efficiency of the partitioned RDF store and the workerthread model.
However, the intra-/inter-node communication between multiple GPUs has a long path: 1) device-to-host via PCIe; 2) host-to-host via networking; 3) host-to-device via PCIe.
We should customize the communication flow for various participants to reduce the latency of network traffic.
Facing the challenges like small GPU memory and limited PCIe bandwidth, we propose the following three key techniques to overcome them.Query-aware prefetching.
With the increase of RDF datasets, the limited GPU memory size (less than 16GB) is not enough to host the entire RDF graph.
Wukong+G thus treats the GPU memory as a cache of CPU memory, and only ensures the necessary data is retained in GPU memory before running a query.
However, it is nontrivial to decide the working set of a query accurately.
As shown in the second timeline of Fig. 6, Wukong+G proposes to just prefetch the triples with the predicates involved in a query, which can enormously reduce the memory footprint of a query from the entire RDF graph to the per-query scale.
This assumption is based on two observations: 1) each query only touches a part of RDF graph; 2) the predicate of a triple pattern is commonly known (i.e., ?
X, predicate, ?
Y ).
For example, the sample query (Q H ) only requires three predicates (teacherOf, takesCourse, and advisor), occupying about 3.7GB memory (0.3GB, 2.9GB, and 0.5GB respectively) for LUBM-2560.
Pattern-aware pipelining.
For a query with many triple patterns, the total memory footprint of a single query may still exceed the GPU memory size.
Fortunately, we further observe that the triple patterns of a query will be executed in sequence.
It implies that Wukong+G can further reduce the demand for memory to the per-pattern scale.
As shown in the third timeline of Fig. 6, Wukong+G can only prefetch the triples with a certain predicate that is used by the triple pattern will be immediately executed.
Thus, for the sample query (Q H ) on LUBM-2560, the demand for GPU memory will further reduce to 2.9GB, the size of the maximum predicate (takesCourse).
Moreover, since the data prefetching and query processing are split into multiple independent phases, Wukong+G can use a software pipeline to create parallelism between the execution of the current triple pattern and the prefetching of the next predicate, as shown in the fourth timeline of Fig. 6.
Note that it will also increase the memory footprint to the maximum size of two successive predicates (takesCourse and advisor).
Fine-grained swapping.
Although the pattern-aware pipelining can overlap the latency of data prefetching and query processing, it is hard to perfectly hide the I/O cost due to limited bandwidth between system and Table 1: A summary of optimizations for query processing on GPU.
"X|Y " indicates XGB memory footprint and Y GB data transfer.
( †) The numbers are evaluated on 6GB GPU memory.
Main Techniques Q7 (GB) on LUBM-2560 Entire graphBasic query processing 16.3 | 16.3 Per-queryQuery-aware prefetching 5.6 | 5.6 Per-pattern Pattern-aware pipelining 2.9 | 5.6 Per-block Fine-grained swapping 2.9 | 0.7 † device memory (e.g., 10GB/s).
For example, prefetching 2.9GB triples (takesCourse) requires about 300ms, which is even longer than the whole query latency (100ms).
Therefore, Wukong+G adopts a fine-grained swapping scheme to maintain the triples cached in GPU memory.
All triples with the same predicate will be further split into multiple fixed-size blocks, and the GPU memory will cache the triples in a best-effort way ( §4.2).
Consequently, the demand of memory will be further reduced to the per-block scale.
Moreover, the data transferring cost will also become the per-block scale, and all cached data on GPU memory can be reused by multiple triple patterns of the same query or even multiple queries.
As shown in the fifth timeline of Fig. 6, when most triples of the required predicates have been retained in GPU memory, the prefetching cost can be perfectly hidden by query processing.
Even for the first required predicate, Wukong+G still can hide the cost by overlapping it with the planning time of this query or the processing time of a previous query.
Table 1 summarizes the granularity of data prefetching on GPU memory, and shows the size of memory footprint and data transfer for a real case (Q7 on LUBM-2560).
Note that Q7 is similar to Q H but requires five predicates.
The memory footprint of Q7 with finegrained swapping is equal to the available GPU memory size (6GB) since Wukong+G only swaps out the triples of predicates on demand.
Prior work [62,51,64] uses a distributed in-memory key/value store to physically store the RDF graph, which is efficient to support random traversals in graphexploration scheme.
In contrast to the intuitive design [62] This design can prominently reduce the graph traversal cost for both local and remote accesses.
However, the triples (both key and value) with the same predicate are still sprinkled all over the store.
It implies that the cost of prefetching keys and values for a triple pattern is extremely high or even impossible.
Therefore, the key/value store on CPU memory should be carefully re-organized for heterogeneous CPU/GPU processing by aggregating all triples with the same predicate and direction into a segment.
Furthermore, the key and value segments should be maintained in a fine-grained way (block) and be cached in pairs.
Finally, the mapping between keys and values should be retained in the key/value cache on GPU memory, which uses a separate address space.
Wukong+G proposes the following three new techniques to construct a GPU-friendly key/value store, as shown in the right part of Fig. 7.Predicate-based grouping (CPU memory).
Based on the idea of predicate-based decomposition in Wukong, Wukong+G adopts predicate-based grouping to exploit the predicate locality of triples and retains the encoding of keys and values.
The basic idea is to partition the key space into multiple segments, which are identified by the combination of predicate and direction (i.e., [pid, d]).
To preserve the support of fast graph exploration, Wukong+G still uses the hash function within the segment but changes the parameter from the entire key (i.e., [vid, pid, d]) to the vertex ID (vid).
The number of keys and values in each segment are collected during loading the RDF graph and aligned to an integral multiple of the granularity of data swapping (block).
To ensure that all values belonged to the triples with the same predicate are stored contiguously, Wukong+G groups such triples and inserts them together.
Moreover, Wukong+G uses an indirect mapping to link keys and values, where the link is an offset within the value space instead of a direct pointer.
As shown in the right part of Fig. 7, the triples required by TP-2 (i.e., Kurt,tc, DS and Bobby,tc, OS) are aggregated together in both key and value spaces (the purple boxes).
Pairwise caching (GPU memory).
To support finegrained swapping, Wukong+G further splits each segment into multiple fixed-size blocks and stores them into discontinuous blocks of the cache on GPU memory, like [Logan,to, out] and [Erik,to, out].
Note that the block size for keys and values can be different.
Wukong+G follows the design on CPU memory to cache key and value blocks into separate regions on GPU memory, namely key cache and value cache.
Wukong+G uses a simple table to map key and value blocks, and the link from key to value becomes the offset within the value block.
Unlike the usual cache, the linked key and value blocks must be swapped in and out the (GPU) cache in pairs, like [OS, tc, in] and [Bobby] (the purple boxes).
Thus, Wukong+G maintains a bidirectional mapping between the pair of cached key and value blocks.
Moreover, a mapping table of block ID between RDF store (CPU) and cache (GPU) is used to re-translate the link between keys and values, when the pairwise blocks (key and value) are swapped in GPU memory.Look-ahead replacement policy.
The mapping table of block IDs between RDF store and cache records whether the block has been cached.
Before running a triple pattern of the query, all of key and value blocks should be prefetched to the GPU memory.
For example, the key [OS,tc, in] and value [Bobby] should be loaded into the cache before processing TP-2.
Wukong+G proposes a look-ahead LRU-based replacement policy to decide where to store prefetched key and value blocks.
Specifically, Wukong+G prefers to use free blocks first and then chooses the blocks that will not be used by the following triple patterns of this query (look-ahead), with the highest LRU score.
The worst choice is the blocks will be used by the following triple patterns, and then the block of the farthest triple pattern will be replaced.
Note that the replacement policies for keys and values are the same and there is at most a pair of key/value blocks will be swapped out due to the pairwise caching scheme.
For example, as shown in the right part of Fig. 7, before running the triple pattern TP-2, all key/value blocks of the predicate takeCourse (tc) should be swapped in the cache (the purple boxes).
The value block with [Bobby] can be loaded to a free block, while the key block with [OS,tc, in] will replace the cached block with [Pidx,to, in], since it was used by TP-0 with the highest LRU score.
Wukong+G splits the RDF graph into multiple disjoint partitions by a differentiated partitioning algorithm [51,19] 5 , and each machine hosts an RDF graph partition and launches many worker threads on CPUs and GPUs to handle concurrent light and heavy queries respectively.
The CPU worker threads on different machines will only communicate with each other for (light) query processing, and it is the same to GPU worker threads for (heavy) query processing.To handle light queries on CPU worker threads, Wukong+G simply follows the procedure (see Fig. 2) that has been successfully demonstrated by Wukong [51].
However, to handle heavy queries on GPU worker threads, the procedure (see Fig. 5) becomes complicated due to the assistance of (CPU) agent thread and the maintenance of (GPU) RDF cache.Execution mode: fork-join.
Prior work [51] proposes two execution modes, in-place and fork-join, for distributed graph exploration to migrate data and execution respectively.
The in-place execution mode synchronously leverages one-sided RDMA READ to directly fetch data from remote machines, while the fork-join mode asynchronously splits the following query computation into multiple sub-queries running on remote machines.
Wukong+G follows the design on CPU worker threads but only adopts the fork-join mode for query processing on GPU, because the in-place mode is usually inefficient for heavy queries [51] and migrating data from remote CPU memory to local GPU memory is still very costly even with RDMA operations.In the fork-join mode, the agent thread will split the running query (metadata) with intermediate results (his- tory table) into multiple sub-queries for the following query processing, and dispatch them to the task queue of agent threads on remote machines by leveraging onesided RDMA WRITE.
Therefore, multiple heavy queries can be executed on multiple GPUs concurrently in a time-sharing way.
However, the current history table is located in GPU memory (see Fig. 8), such that it would be inefficient to fetch and split the table by using a single agent thread on CPU (➊ and ➋ in Fig. 8(a)).
Therefore, Wukong+G leverages all GPU cores to partition the history table in fully parallel (➊ in Fig. 8(b)) using a dynamic task scheduling mechanism [47,18].
Communication flow.
To support fork-join execution, the sub-queries will be sent to target machines with their metadata (e.g., query plan and current step) and history table (intermediate results), and the history table will be sent back with final results at the end.
As shown in Fig. 8(a), the query metadata will be delivered by onesided RDMA operations between the CPU memory of two machines (➌ and ➏).
In contrast, the history table has to go through a long path from local GPU memory to the remote GPU memory, and finally goes back to the local CPU memory.
A detailed communication flow for history table (see Fig. 8(a)): 1) from local GPU memory to local CPU memory (➊, Device-to-Host); 2) from local CPU memory to remote CPU memory (➌, Host-to-Host); 3) from remote CPU memory to remote GPU memory (➍, Host-to-Device); 4) from remote GPU memory to remote CPU memory (➎, Device-to-Host); 5) from local CPU memory to remote CPU memory (➏, Host-to-Host).
GPUDirect [3] opens an opportunity for Wukong+G to directly write history table from local GPU memory to remote GPU and CPU memory.
Hence, Wukong+G decouples the transferring process of query metadata and history table (➋ and ➋ in Fig. 8(b)), and further shortens the communication flow for history table by leveraging GPUDirect RDMA.
It also avoids the contention on agent thread with the metadata transferring.
A detailed communication flow for history table (see Fig. 8(b)): 1) from local GPU memory to remote GPU memory (➋, Device-to-Device); 2) from remote GPU memory to local CPU memory (➌, Device-to-Host).
Moreover, to mitigate the pressure on GPU memory when handling multiple heavy queries, Wukong+G choose to send the history table of pending queries from local GPU memory to the buffer on remote CPU memory first via GPUDirect RDMA, and delay the prefetching of history table from CPU memory to GPU memory till handling the query on GPU.
Wukong+G prototype is implemented in 4,088 lines of C++/CUDA codes atop of the code base of Wukong.
This section describes some implementation details.Multi-GPUs support.
Currently, it is not uncommon to equip every CPU socket with a separate GPU card for low communication cost and good locality.
To support such multi-GPUs on a single machine, Wukong+G runs a separate server for each GPU card and several co-located CPU cores (usually a socket).
All servers comply with the same communication mechanism via GPUDirectcapable RDMA operations, regardless of whether two servers share the same physical machine or not.Too large intermediate results.
In rare cases, the intermediate results may overflow the history buffer on The remaining strips will stay in CPU memory and be swapped in GPU memory one-by-one while processing a single triple pattern.
Hardware configuration.
All evaluations are conducted on a rack-scale cluster with 10 servers on 5 machines.
We run two servers on a single machine.
Each server has one 12-core Intel Xeon E5-2650 v4 CPU with 128GB of DRAM, one NVIDIA Tesla K40m GPU with 12GB of DRAM, and one Mellanox ConnectX-3 56Gbps InfiniBand NIC via PCIe 3.0 x8 connected to a Mellanox IS5025 40Gbps IB Switch.
Wukong+G only provides a one-to-one mapping between the work and agent threads on different servers [51], which mitigates the scalability issue of RDMA networks with reliable transports [31] and simplifies the implementation of the task queue.
In all experiments, we reserve two cores on each CPU to generate requests for all servers to avoid the impact of networking between clients and servers as done in prior work [54,56,57,20,51].
Benchmarks.
Our benchmarks include one synthetic and two real-life datasets, as shown in Table 2.
The synthetic dataset is the Lehigh University Benchmark (LUBM) [7].
We generate 5 datasets with different sizes (up to LUBM-10240) and use the query set published in Atre et al. [13], which are widely used by many distributed RDF systems [36,62,23,51].
The real-life datasets include the DBpedia's SPARQL Benchmark (DBPSB) [1] and YAGO2 [9,30].
For DBPSB, we use the query set recommended by its official site.
For YAGO2, we collect our query set from both H 2 RDF+ [43] and RDF-3X [42] to make sure the test covers both light and heavy queries.Comparing targets.
We compare our system against two state-of-the-art distributed RDF query systems, TriAD [23] (RDF relational store) and Wukong [51] (RDF graph stores).
Note that TriAD does not sup- port concurrent query processing, so we only compare to it in the single query performance.
As done in prior work [62,23,51], the string server is enabled for all systems to save memory usage, reduce network bandwidth, and boost string matching.
We first study the performance of Wukong+G for a single query using the LUBM dataset.
Table 3 shows the optimal performance of different systems on a single server with LUBM-2560.
For Wukong+G, there is no data swapping during single query experiment since the current memory footprint of all queries on LUBM-2560 (the numbers in brackets) is smaller than the GPU memory (12GB).
The query-aware prefetching reduces the memory footprint to the per-query granularity (see Table 1).
Although Wukong and TriAD have enabled multithreading (10 worker threads), Wukong+G can still significantly outperform such pure CPU systems for heavy queries (Q1-Q3, Q7) by up to 8.3X and 21.9X (from 4.5X and 5.2X) due to wisely leveraging hardware advantages.
The improvement of average (geometric mean) latency reaches 5.9X and 8.5X. For the light queries (Q4-Q6), Wukong+G inherits the prominent performance of Wukong by leveraging graph exploration and outperforms TriAD by up to 32.7X.We further compare Wukong+G with Wukong and TriAD (multi-threading enabled) on 10 servers using LUBM-10240 in Table 4.
For heavy queries, Wukong+G still outperforms the average (geometric mean) latency of Wukong by 4.6X (ranging from 2.3X to 9.0X), thanks to the heterogeneous RDMA communication for preserving the good performance of GPU at scale.
Further, using up all CPU worker threads to accelerate a single query is not practical for concurrent query processing since it will result in throughput collapse.
For light queries, Wukong+G incurs about 12% performance overhead (geometric mean) compared to Wukong due to adjusting the layout of key/value store on CPU memory for predicate-based grouping.
Wukong+G is still one order of magnitude faster than TriAD due to the in-place execution with one-sided RDMA READ [51].
To study the impact of each technique and how they affect the query performance, we iteratively enable each optimization and collect the average latency by repeatedly running the same query on a single server with 3GB GPU memory for LUBM-2560.
As shown in Table 5, even using query-aware prefetching (per-query), the memory footprints of query Q1, Q3 and Q7 still exceed available GPU memory (see Table 3).
Hence, they can not run until enabling pattern-aware prefetching (per-pattern).
The effectiveness of fine-grained swapping (per-block) varies on different queries.
It is quite effective on Q2 and Q3 (8.8X and 5.0X) since all triples required by triple patterns can almost be stored in 3GB GPU memory.
Note that Q3 returns an empty history table (intermediate results) half-way and reduces the practical runtime memory footprint to 2.5GB.
For Q1 and Q7, although the relative large memory footprint (3.6GB and 5.6GB), incurs massive data swapping (1.5GB by 187 time and 5.1GB by 734 times), the cache sharing with fine-grained mechanism can still notably reduce the query latency by 2.4X and 1.4X. Moreover, pipeline does not work on Q2 and Q3 without data prefetching time.The improvement for Q1 and Q7 is still limited since the prefetching and execution time for each triple pattern are quite imbalanced.
For example, 88% of blocks are swapped at two triple patterns for Q7.
We evaluate the scalability of Wukong+G with the increase of servers.
Since the latency of light queries of Wukong+G mainly inherits from Wukong, We only report the experimental results of heavy queries handled by GPUs.
As shown in Fig. 9, the speedup of heavy queries ranges from 4.8X to 23.8X.
As the number of servers increases from 2 to 10, a good horizontal scalability is shown.
After a detailed analysis of the experimental results, we reveal that there are two different factors improving the performance at different stages.
In the first stage (from 2 to 4 servers), the increase of total GPU memory provides the main contribution to the performance gains, ranging from 3.2X to 8.5X, by reducing memory swapping cost.
In the second stage (from 4 to 10 servers), since Wukong+G stops launching expensive memory swapping operations when enough GPU memory is available, the main performance benefits come from using more GPUs, ranging from 1.5X to 2.8X.Discussion.
With the further increase of servers, the single query latency may not further decrease due to fewer workload per server and more communication cost.
It implies that it is not worth making all resources (GPUs) participate in a single query processing, especially for a large-scale cluster (e.g., 100 servers).
Therefore, Wukong+G will limit the participants of a single query and can still scale well in term of throughput by handling more concurrent queries simultaneously on different servers.
One principal aim of Wukong+G is to handle concurrent hybrid (light and heavy) queries in an efficient and scalable way.
Prior work [51] briefly studied the performance of Wukong with a mixed workload, which consists of 6 classes of light queries (Q4-Q6 and A1-A3 6 ).
The light query in each class has a similar behavior except that the start point is randomly selected from the same type of vertices (e.g., Univ0, Univ1, etc.).
The distribution of query classes follows the reciprocal of their average latency.
Therefore, we first extend original mixed workload by adding 4 classes of heavy queries (Q1-Q3, Q7), and then allow all clients to freely send light and heavy queries 7 according to the distribution of query classes.
We compare Wukong+G (WKG) with two different settings of Wukong: Default (WKD) and Isolation (WKI).
Wukong/Default (WKD) allows all worker threads to handle hybrid queries, while Wukong/Isolation (WKI) reserves half of the worker threads to handle heavy queries.
Each server runs two emulated clients on dedicated cores to send requests.
Wukong launches 10 worker threads, while Wukong+G launches 9 worker threads and an agent thread.
The multi-threading for heavy queries is configured to 5.
We run the hybrid workload over LUBM-10240 on 10 servers for 300 seconds (10s warmup) and report the throughput and median (50 th percentile) latency for light and heavy queries separately over that period in Fig. 10.
For heavy queries, Wukong+G improves throughput and latency by over one order of magnitude compared to Wukong (WKD and WKI).
The throughput of WKD is notably better (about 80%) than that of WKI, since it can use all worker threads to handle heavy queries.
For light queries, Wukong+G performs up to 345K queries per second with median latency of 0.6ms by 9 worker threads.
The latency can be halved with a small 10% impact in throughput.
As expected, WKI can provide about half of the throughput (199K queries/s) with a similar latency since only half of the worker threads (5) are used to handle light queries.
However, the throughput and latency of WKD for light queries are thoroughly impacted by the processing of heavy queries.
To study the influence of GPU cache for the performance of heavy queries on Wukong+G, we first evaluate the single query latency using LUBM-2560 on a single server with the GPU memory sizes varying from 3GB to 10GB.
We repeatedly send one kind of heavy queries until the cache on GPU memory is warmed up, and illustrate the average latency of heavy queries in Fig. 11.
Since the memory footprint of Q2 (2.4GB) is always smaller than the GPU memory, the latency is stable, and there is no data swapping.
For Q3, although the memory footprint of the query is about 3.6GB, the latency is still stable since the history table becomes empty after the first two triple patterns due to contradictory conditions, where the rest predicate segment (about 1.1GB) will never be loaded.
For Q1 and Q7, the latency decreases with the increase of GPU memory due to the decrease of data swapping size.
However, the break point of Q7 is later than that of Q1 since it has a relatively larger memory footprint (5.6GB vs. 3.6GB).
To show the effectiveness of sharing GPU cache by multiple heavy queries, We further evaluate the performance of a mixture of four heavy queries using LUBM-2560 on a single server with 10GB GPU memory.
As shown in Fig. 12, the geometric mean of 50 th (median) and 99 th percentile latency is just 84.5 and 93.8 milliseconds respectively, under the peak throughput.
Compared to the single query latency (see Table 3), the performance degradation is just 3% and 14%, thanks to our fine-grained swapping and look-ahead replacing.
During the experiment, the number and volume of blocks swapped in per second are about 96 and 750MB.
We further compare the performance of Wukong+G with Wukong on two real-life datasets, DBPSB [1] and YAGO2 [9].
As shown in Table 7, for light queries (D2, D3, Y1, and Y2), Wukong+G can provide a close performance to Wukong due to following the same execution mode and a similar in-memory store.
For heavy queries (D1, D4, D5, Y3, and Y4), Wukong+G can notably outperform Wukong by up to 4.3X (from 1.9X).
Wukong+G is inspired by and departs from prior RDF query processing systems [40,58,41,12,50,13,65,60,14,61,62,23,51,32,64], but differs from them in exploiting a distributed heterogeneous CPU/GPU cluster to accelerate heterogeneous RDF queries.Several prior systems [11,12] have leveraged columnoriented databases [53] and vertical partitioning for RDF dataset, which group all triples with the same predicate into a single two-column table.
The predicate-based grouping in Wukong+G is driven by a similar observation.
However, Wukong+G still randomly (hash-based) assign keys within the segment to preserve fast RDMAbased graph exploration, which plays a vital role for running light queries efficiently on CPU.Using prefetching and pipelining mechanisms are not new, which have been exploited in many graph-parallel systems [49,35] and GPU-accelerated systems [37] to hide the latency of memory accesses.
Wukong+G employs a SPARQL-specific prefetching scheme and enables such techniques on multiple concurrent jobs (heavy queries) that share a single cache on the GPU memory.
There has been a lot of work [25,24,39,26,29,27,55,46,28] focusing on exploiting the unique features of GPUs to accelerate database operations.
Mega-KV [63] is an in-memory key/value store that uses GPUs to accelerate index operations by only saving indexes on the GPU memory to ease device memory pressure.
CoGaDB [15,16] uses a column-oriented caching mechanism on GPU memory to accelerate OLAP workload.
SABER [34] is a hybrid high-performance relational stream processing engine for CPUs and GPUs.
Wukong+G is inspired by prior work, while the differences in workloads result in different design choices.
To our knowledge, none of the above systems exploit distributed heterogeneous (CPU/GPU) environment, let alone using RDMA as well as GPUDirect features.To reduce communication overhead between multiple GPUs, NVIDIA continuously puts forward GPUDirect technology [3], including GPUDirect RDMA and GPUDirect Async (under development [6]).
They enable direct cross-device data transfer on data plane and control plane, respectively.
Researchers have also investigated how to provide network [33,21] and file system abstractions [52] based on such hardware features.
Our design currently focuses on using GPUs to deal with heavy queries for RDF graphs.
The above efforts provide opportunities to build a more flexible and efficient RDF query system through better abstractions.
The trend of hardware heterogeneity (CPU/GPU) opens new opportunities to rethink the design of query processing systems facing hybrid workloads.
This paper describes Wukong+G, a graph-based distributed RDF query system that supports heterogeneous CPU/GPU processing for hybrid workloads with both light and heavy queries.
We have shown that Wukong+G achieves low query latency and high overall throughput in the single query performance and hybrid workloads.
We sincerely thank our shepherd Howie Huang and the anonymous reviewers for their insightful suggestions.
This work is supported in part by the National Key Research & Development Program (No. 2016YFB1000500), the National Natural Science Foundation of China (No. 61772335, 61572314, 61525204), the National Youth Top-notch Talent Support Program of China, and Singapore NRF (CREATE E2S2).
