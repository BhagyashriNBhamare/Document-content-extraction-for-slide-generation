Effective analysis of raw data from networked systems requires bridging the semantic gap between the data and the user's high-level understanding of the system.
The raw data represents facts about the system state and analysis involves identifying a set of semantically relevant behaviors, which represent "interesting" relationships between these facts.
Current analysis tools, such as wireshark and splunk, restrict analysis to the low-level of individual facts and provide limited constructs to aid users in bridging the semantic gap.
Our objective is to enable semantic analysis at a level closer to the user's understanding of the system or process.
The key to our approach is the introduction of a logic-based formulation of high-level behavior abstractions as a sequence or a group of related facts.
This allows treating behavior representations as fundamental analysis primitives, elevating analysis to a higher semantic-level of abstraction.
In this paper, we propose a behavior-based semantic analysis framework which provides: (a) a formal language for modeling high-level assertions over networked systems data as behavior models, (b) an analysis engine for extracting instances of user-specified behavior models from raw data.
Our approach emphasizes reuse, com-posibility and extensibility of abstractions.
We demonstrate the effectiveness of our approach by applying it to five analyses tasks; modeling a hypothesis on traffic traces, modeling experiment behavior, modeling a security threat, modeling dynamic change and composing higher-level models.
Finally, we discuss the performance of our framework in terms of behavior complexity and number of input records.
The ability to convert raw data into higher-level insights and understanding has become a key enabler in many fields.
We approach one particular aspect of this problem, namely the analysis of data within the domain of networked and distributed systems.
Such systems routinely generate a plethora of logs, trace and audit data during their operation.
Users, such as researchers and system administrators, use this raw data to understand system behavior, diagnose problems, discover new behaviors, or verify hypotheses.
Effective analysis of such raw data requires bridging the semantic gap between raw data and the user's high-level understanding of the analysis domain.
Our experience with analysis tools reveals that this problem is ill-addressed.
A typical approach to data analysis involves the user sifting through the data using simple search and correlation constructs like boolean queries to identify relationships and infer meaning from data.
For example, wireshark [19] can help identify complete or incomplete TCP flows from packet traces and splunk [16] can help identify spurious logins from a server log.
Our study of four popular tools, discussed in Section 2.1, reveals that current approaches require cumbersome multi-step analyses to infer semantic relationships from data.
For example, a user analyzing a network packet trace may first have to extract individual flows by specifying specific attribute values related to each flow, and then somehow manually infer relationships like concurrency between the flows.
This problem is further complicated if the user has to reason and analyze over multiple types of data.
This separation between the raw data and the meaning it carries constitutes the semantic gap.In this paper, we focus on the problem of expressing analyses tasks that are meaningful and useful to the user.
Specifically, given a finite, timestamped list of facts about the system under observation, our objective is to assist the user in expressing and modeling semantically relevant behaviors, which are "interesting" relationships between these facts or sequence of facts.
These relationships encompass notions of ordering, causality, dependence, or concurrency.Our insight is that higher-level understanding in networked and distributed systems can be expressed in the form of relationships between system states, simple behaviors, and complex behaviors.
For example, in most situations, a typical web-server operation is better understood as a concurrent relationship between multiple HTTP sessions to a server rather than the details of the protocols and specific values in the packet headers.
Thus, our data analysis approach introduces a behavior as a primitive analysis construct.
Behaviors can be extended or constrained to create a behavior model, which forms an assertion about the overall behavior of the system.
A behavior model can then be rapidly applied over data to validate the assertion.
We discuss complete details about specifying behavior models in Section 3, and Section 4 presents the analysis engine for extracting instances of user-specified behavior models from raw data.The behavior models are abstract entities to capture the semantic essence of a particular relationship without focusing on unnecessary details or particular parameters that may vary between individual facts or behaviors.
Incorporation of abstract behavior models as explicitly represented and manipulated constructs within our framework provides two key benefits.
First, this abstraction allows users of our framework to analyze and understand the raw data at a semantically relevant level.
In Section 3.4, we introduce an example of a behavior model to identify pairs of communication events where the destination IP of the second event is same as the source IP of the first.
Such models can be used to analyze many different datasets without any modification.
Additionally, since behavior models are primitive analysis constructs, the framework supports extensibility by composing new models from behavior models present in the knowledge base as demonstrated in Section 5.5.
Thus, representing analysis expertise explicitly as behavior models formalizes the semantics for data analysis in networked systems.The second key benefit of our work is the ability to foster sharing and reuse of knowledge embedded in explicitly represented behavior models.
Our first-hand experience with existing tools suggests that in most cases knowledge inferred from analysis resides either in a domain-specific tool or a single expert's brain.
This is due to a lack of an explicit representation for capturing, storing, sharing, and reusing such knowledge in a context-independent way.
Many current tools are either static in nature, handling only a fixed set of analyses and record types, or may offer limited extensibility, but through some mechanism that involves significant effort.For example, wireshark [19] is easily extensible using plugins, but writing a plugin requires understanding the wireshark API and C programming skills.
In contrast, a well defined shareable format for representing knowledge about networked systems data offers the prospect that many different tools can be driven by, and contribute to, a single shared knowledge base.Beyond the basic challenge, the task of semantic-level analysis is difficult for two disparate reasons.
First, the definition of "interesting" may vary widely in different situations, requiring a rich toolbox of techniques for effective analysis.
We address this problem by restricting the definition of "interesting relationships" to expressing a particular set of characteristics of networked systems as discussed in Section 3.1.
Second, in large scale systems, efficient and intelligent data analysis is extremely resource intensive due to the sheer volume of system events and traces.
While in Section 6 we report performance results, this paper primarily discusses the fundamental aspects of defining and employing explicit behavior models as a data analysis tool.
Real-time analysis of data for applications such as intrusion detection is a future goal as discussed in Section 7.
The fundamental contribution of this paper is the introduction of a behavior-based semantic analysis framework for confirmatory and exploratory analysis of multivariate, multi-type, timestamped data captured from networked systems.
The main elements of the semantic framework include (a) a specialized formal language for specifying behavior models and (b) an analysis engine for extracting instances of user-specified behavior models from data.
In confirmatory analysis, the user specifies a validation criteria, expected system behavior or hypothesis, by writing a specific model or through composing a high-level model from existing models contained within the knowledge base of the framework.
In exploratory analysis, a user applies existing models from the knowledge base to explore data for new or unanticipated behaviors.
In Section 5 we present five detailed examples of how the framework can be applied for these data analysis tasks.
In this section, we set the context for our work by first studying four popular analysis tools followed by a discussion on specification-based approaches for analysis of networked systems data.
In this section, we study four popular analysis methodologies: wireshark v1.2.7 [19], splunk v4.1 [16], Simple Event Correlator (SEC) v2.5.3 [18], Bro v1.5.2 [14], and compare them with our behavior-based semantic analysis framework (SAF mainly interactive analysis tools while Bro and SEC are real-time monitoring tools.
The behavior-based semantic analysis framework (SAF) falls in the category of interactive analysis tools.
The tools are compared along seven dimensions in Table 1; (a) high-level goals, (b) input data types, (c) analysis specification language (d) primitive analysis constructs, (e) semantic analysis constructs, (f) ability to compose specifications and (g) abstraction, that is, specifications in terms of relationships between data attributes.
Each paragraph below introduces an analysis framework and the reader is directed to Table 1 for details.
The corresponding features for our framework (SAF) are introduced in Table 1 and explored in future sections.
We have not considered SQL-based approaches on streaming data for comparison [6], since SAF representations are at a higher-level of abstraction than database query languages.
However, we further discuss how our framework could benefit by using the above SQL extensions to optimize event storage and retrieval in Section 7.
wireshark [19] is an open-source tool for interactive analysis of a large variety of network data from a packet capture file.
Wireshark's design can be separated into the analysis framework and plugins.
The analysis framework provides the ability to sift through large volumes of packets visually and provides a boolean query grammar for finding "interesting" relationships and statistical summaries over typical networking concepts, for example, rate, flows, bytes, and connections.
The plugin architecture, on the other hand, is responsible for normalizing and presenting different types of packet data and protocol behavior to the analysis framework in a uniform way.splunk [16] is a popular commercial framework for unified data analysis of a large variety of data.
Splunk's strength comes from its ability to index various types of data, allowing the user to sift through logs by combining search queries using boolean operations, pipes and powerful statistical and aggregation functions.
Splunk supports time-based, event-based, value-based correlations and also allows combining queries into higher-level queries.
Splunk is extensible using apps, which allow encoding knowledge as queries for sharing and wider dissemination.
However, it does not provide support for explicitly capturing domain expertise with semantic constructs.
It does provide the ability to invoke external commands, thus providing an indirect way to incorporate explicit domain expertise into the analyses.Simple Event Correlator(SEC) [18] is an opensource framework for rule-based event correlation.
SEC reads the analysis specifications from a configuration file containing a set of event matching rules and corresponding actions.
SEC processes data from log files, pipes and standard streams to trigger the configured actions on a match.
It supports both time-based and event-based correlations and also allows specifying abstract rules that bind their values at runtime.
SEC is more sophisticated than the previous two tools, it supports composing higher-level events by correlating low-level events, providing a framework for semantic understanding.
Its ruletypes pair and pairwithwindow capture some of the semantics of ordering and duration.
However, it lacks support for inferring interval-based temporal relationships like concurrency and overlap and the analysis specification in the configuration files are not intuitive to capture and share domain expertise in a generic way.Bro [14] is a high-speed intrusion detection system for checking security policy violations by passively monitoring network traffic in real-time.
Bro's security policies are written in the specialized Bro scripting language which is geared towards security analysis.
The language supports semantic constructs such as connections, IP addresses, ports, and various network protocols along with various operators and functions to express different forms of network analyses.
Bro has the ability to do timebased and event-based correlation.
However, Bro mainly processes network packet data and uses a programming language-based analysis approach.
Specification-based approaches are particularly appealing in various areas of networked and distributed systems due to their ability to be abstract, concise, precise, and verifiable.
In formal verification of distributed and concurrent systems, a system is specified in logic and then formal reasoning is applied on the specification to verify desired properties [3,9].
In declarative networking, a specification language, Network Datalog (NDLog) [10], allows defining high-level networking specifications for rapidly specifying, modeling, implementing, and experimenting with evolving designs for network architectures.
In testbed-based experimentation, a simple set of usersupplied expectations are used to validate expected behavior of an experiment [12].
The formal specification approaches have been well developed within the intrusion detection community and have been successfully applied to network and audit data for analysis.
In this section we first present a brief overview of four such approaches and then compare them to SAF.Roger et al. [15], leverage the idea that attack signatures are best expressed in simple temporal logic using temporal connectives to express ordering of events.
They pose the detection problem as a model-checking problem against event logs.
Naldurg et al. [13], propose another temporal-logic based approach for real-time monitoring and detection.
Their language EAGLE supports parameterized recursive equations and allows specifying signatures with complex temporal event patterns along with properties involving real-time, statistics and data values.
Kinder et al. [8], extend the logic CTL (Computation Tree Logic) and introduce CTPL (Computation Tree Predicate Logic) to describe malicious code as a highlevel specification.
Their approach allows writing specifications that capture malware variants.
Ellis et al. [4], introduce a behavioral detection approach to malware by focusing on detecting patterns at higher-level of abstractions.
They introduce three high-level behavioral signatures which have the ability to detect classes of worms without needing any apriori information of the worm behavior.The SAF abstract models are comparable to the approaches of [13,8,4] in their use of formal logic and temporal constructs for specifications.
But, in addition to providing an extended set of sophisticated intuitive operators and constructs, the behavior models presented in this paper can be generically applied to model various scenarios over a variety of data and are easily composed into semantically relevant higher-level models.
This allows creating a knowledge base to explicitly capture domain expertise required for analyzing a large variety of operations encountered in networked and distributed systems as shown in Section 5.
The higher-level behavioral signatures [4] based on the network-theoretic abstract communication network (ACN) are tightly bound to networking constructs like hosts, routers, sensors and links making them very restrictive in their ability to express general networked systems behaviors.The SAF is based on a logic-based specification approach rather than a programming language-based specification approach like the one followed in Bro.
Our goal is that the behavior models should be abstract but also concise and precise to support well-known knowledge representation and reasoning approaches.
Logic is declarative and type-free, imparting formal semantics, abstract specifications, and efficient processing by analysis engines.
The logic-based approach also enables building a knowledge base of behavior models to explicitly capture domain expertise that can be used to automatically reason and infer behavior models.
However, logic-based approaches are less expressive than programming languages.
The expressiveness of our approach is based on requirements derived from characteristics of networked systems as discussed in Section 3.1.
A particular execution of a networked system or process can be captured as a sequence of states, where a state is a collection of attributes and their values.
A behavior (b) is a sequence of one or more related states.
A system execution is thus defined as a combination of different behaviors, and each new execution may generate a unique set of behaviors.
A behavior model (φ) is a formula that makes an assertion about the overall behavior of the system.For example, consider a simplified IP flow in networking, where a flow is a communication between two hosts identified by their IP addresses.
For simplicity we assume an IP flow to be broken into two states: ip s2d denotes a packet from some source to destination host and ip d2s denotes a packet from a destination to source.
Then, a valid IP flow behavior, IPFLOW, is one where ip s2d and ip d2s are related by their source and destination attributes with the additional criteria that ip d2s always occurs after ip s2d.
The behavior model (φ ipf low ) is an assertion that IPFLOW is valid.
We discuss details of this example and extend it further in Section 3.4.
In this section, we first discuss the requirements and design choices for a language to specify behaviors followed by the formal syntax and semantics of the language.
As discussed in Section 1, the key objective of our framework is to enable semantic-level analysis over data.
A semantically expressive language for analysis over networked and distributed systems data must meet the following requirements: (a) enable analysis over multitype, multi-variate, timestamped data, (b) express a wide variety of "interesting" relationships, (c) enable analysis over higher-level abstractions, and (d) enable composing abstractions into higher-level abstractions.The language should express at-least the following "interesting" relationships to capture the core characteristics of networked and distributed systems: (a) causal relationships between behaviors, for example, a file being opened only if a user is authorized; (b) partial or total ordering, for example, in-order or out-of-order arrival of packets; (c) dynamic changes over time, for example, traffic between client and server drops after an attack on the server; (d) concurrency of operations, for example, simultaneous web client sessions; (e) multiple possible behaviors, for example, a polymorphic worm behavior may vary on each execution; (f) synchronous or asynchronous operations, for example, some operations need to complete within a specific time whereas others need not; (g) value dependencies between operations, for example, a TCP flow is valid only if the attribute-values contained in the individual packets are related to each other; (h) invariant operations, for example, some operations may always hold true and, (i) eventual operations, for example, some operations happen in the course of time.
In addition, we need traditional mechanisms, such as boolean operators and loops, for combining these relationships into complex behaviors and mechanisms for basic counting of events and reasoning over the counts.We do not claim completeness of the above requirements but we believe that being able to express the above classes of primitive relationships and combining them to form complex relationships would suffice for a wide range of situations, a few of which we demonstrate as case studies in Section 5.
The following four design decisions realize the requirements listed above.
First, our framework provides logicbased support to formulate behavior abstractions as a sequence or group of related events, where events are uniform representation of system facts as discussed later.
This formulation allows treating this behavior representation as fundamental analysis primitive, elevating analyses to a higher semantic-level of abstraction.Second, the language combines operators from Allen's interval-temporal logic [1], Lamport's Temporal Logic of Actions [9] and boolean logic.
Temporal logic allows expressing the ordering of events in time without explicitly introducing time.
Interval-temporal logic allows expressing relationships like concurrency, overlap and ordering between behaviors as relationships between their time-intervals.
Additionally, complex behaviors are easily composed from simpler ones using boolean operators.Third, the framework enables specifying dependency relationships between event attributes while leaving the values to be dynamically populated at runtime.
Late binding enables abstract specifications that enrich the knowledge base as they can be directly applied to a wide variety of data-sets.
This also enables parametrization of models during complex model composition as discussed in Section 5.5.
Lastly, the framework introduces the notion of a domain-independent event as a uniform representation of multi-type, multi-variate, timestamped data.
Specifically, an event (e) is a representation of system state and is given by a 4-tuple o, c, t, av where o is the event-origin (for example, the host IP), c is the eventtype (for example, PKT TCP or APP HTTPD), t is the event timestamp and av = { a i , v i | a i ∈ A , v i ∈ Strings , 1 ≤ i ≤ D c } are the attribute-value pairs contained in the event.
A is the set of attribute labels, for example, sip, dip, etype.
D c is the number of attributes in an event of type c.
This normalization of data to events ensures that the analysis algorithms are independent of the input domain.We believe these design decisions ensure developing abstract behavior models as first-order primitives for capturing, storing, and reusing domain expertise for the analysis of networked systems.
Next we discuss the syntax of such a language.
The language grammar for defining a behavior model φ as a formula, consists of five key elements as shown in Figure 1: state propositions S as atomic formulae; grouping operators '(' and ')' to define sub-formulae; logical operators and temporal operators for relating sub-formulae or atomic-formulae; the optional behavior constraints bcon and operator constraints opcon written within ' [' and ']'; and the relational operators relop.A state proposition, S, is an atomic formula for capturing events that satisfy specified relations between at- tributes and their values.
In essence, S captures states of a system or process and is the basic element of a behavior model.
The most trivial behavior model is one with a single state proposition.
Formally, S is represented as a finite collection of related attribute-value tuples as:φ ::= '(' S | φ ')' { bcon } | not φ (negation) | φ and φ (logical and) | φ or φ (logical or) | φ xor φ (logical xor) | φ (opcon) φ (leadsto) | (opcon) φ (always) | φ olap (opcon) φ (overlaps) | φ dur (opcon) φ (during) | φ sw (opcon) φ (startswith) | φ ew (opcon) φ (endswith) | φ eq (opcon) φ (equals) bcon ::= '[' {tc | cc} ']' tc ::= {at | duration | end} relop t{: t} cc ::= {icount | bcount | rate} relop c{: c} opcon ::= '[' relop t{: t} ']' relop ::= { > | < | = | ≥ | ≤ | = } t ::= [0 − 9] + {s|ms} c ::= [0 − 9]+S = {(a i , r i , v i ) | i ∈ N, a i ∈ A, v i ∈ V, r i ∈ (=, >, <, ≥, ≤, 񮽙 =)}A is a set of string labels, such as sip, dip, etype and V is a set of string constants, such as 10.1.1.2,/bin/sh, along with two special strings: (a) strings prefixed with '$', as in $$,$s2.dst (b) strings with the wild-card character '*', as in /etc/pas * .
Considering our previous example of IPFLOW, the state propositions ip s2d and ip d2s are written as:ip s2d = {etype=PKT IP, sip=$$,dip=$$} ip d2s = {etype=PKT IP,sip=$ip s2d.dip, dip=$ip s2d.sip}State proposition ip s2d contains three attributes etype, sip and dip.
etype has a constant value PKT IP, while sip and dip attributes use the '$' prefixed special variables which are dynamically bound at runtime.
State proposition ip d2s defines the values of its sip and dip attributes as being dependent on values of state ip s2d.
Dependent attributes along with dynamic binding of values allows leaving out details like the actual IP addresses from the specification.The temporal operators allow expressing temporal relationships like ordering and concurrency between oneor-more behaviors.
The linear-time temporal operator 񮽙 (leadsto), written as ∼>, is used to express causal relationships between behaviors.
The interval temporal logic operators express concurrent relationships between behaviors as either relationships: (a) between their starttimes using sw (startswith), (b) between their endtimes using ew (endswith) or (c) between their durations using olap (overlap), eq (equals) and dur (during).
The (always) operator, written as [ ], allows expressing invariant behaviors.
The logical operators not, and, or, xor are supported for logical operations over behaviors and for creating complex behaviors.Behavior constraints allow placing additional constraints on the matching behavior instances and are specified immediately following the behavior within square brackets.
Constraints and their values are related using the standard relational operators.
The six behavior constraints are divided as time constraints tc and count constraints cc.
Time constraints allow constraining behavior starttime using at, behavior endtime using end and behavior duration using duration.
The time value, t, for the constraint can be specified as a single positive value or as a range.
Additionally, the values can be suffixed with either 's' or 'ms' to indicate seconds or milliseconds respectively.
The count constraints allow constraining number of matching behavior instances using icount, the size of each behavior instance using bcount and rate of events within a behavior instance using rate.
Operator constraints allow specifying time bounds over the temporal operators thus allowing their semantics to be slightly modified.
The operator constraint values are specified as a single value or a range along with a relational operator.
Table 2 presents detailed semantics of operators along with behavior and operator constraints.Expressing a behavior in the language constitutes writing sub-formulae.
Behaviors are always enclosed within parenthesis '(' and ')'.
Simple behaviors are constructed by relating one-or-more state propositions using operators, while complex behaviors are constructed by relating one-or-more behaviors.
The grammar also allows expressing complex behaviors using recursion and we present an example in Section 5.3.
Recursive definitions allow expressing looping behavior for which the loop bounds can be optionally specified using the bcount behavior constraint.
The current grammar does not support existential and universal quantification since such a need is not clear.
We explore these language extensions as part of our future work.Writing behavior models in the framework involves additional syntax such as namespaces, headers and variables which are discussed along with the case-studies in Section 5.1 and Section 5.2.
Next section presents the formal semantics of the language.
We first define two concepts important for understanding the semantics.
A sequential log (L) is a finite sequence of timestamped events L = e 1 , e 2 , e 3 , . . . , e n such that e i .
t ≤ e j .
t , ∀ i < j.
A behavior instance B φ for a behavior model φ is sequence or groups of events satisfyingBehavior model ψ Meaning of ψ L satisfies ψ (L |= ψ) iff (φ) φ is a behavior.
∃B φ ⊆ L and |B φ | > 0 S S is a state proposition defined as S = {(a 1 , r 1 , v 1 ) . . ., (a d , r d , v d )}.
(a) |B S | > 0, (b) ∀ e ∈ B S , ∀ i ∈ {1, .
.
.
, d}, e.a i is defined and values e.v i and S.v i satisfy relation r i .
(neg φ) Negation of behavior is true.
L |= φ, that is, |B φ | = 0 (φ 1 and φ 2 ) Both φ 1 and φ 2 are true.
L |= φ 1 and L |= φ 2 (φ 1 or φ 2 ) φ 1 and φ 2 are not both false simultaneously.
L |= φ 1 or L |= φ 1 or satisfies both φ 1 and φ 2 (φ 1 xor φ 2 ) Either of φ 1 or φ 2 are true but not both.
L |= φ 1 or L |= φ 2 but not both(φ 1 φ 2 ) φ 1 leadsto φ 2 , that is, whenever φ 1 is satisfied φ 2 will eventually be satisfied.
(a) L |= φ 1 and L |= φ 2 , (b) B φ 1 [1] 񮽙 = B φ 2 [1], (c) B φ 2 .
starttime ≥ B φ 1 .
endtime (φ 1 [≤ t] φ 2 )Whenever φ 1 is satisfied φ 2 will be satisfied within t time units.
(a) L |= (φ 1 φ 2 ), (b) B φ 2 .
starttime ≤ (B φ 1 .
endtime + t) ( φ) φ is always satisfied, that is, satisfied by each event.
∀ e ∈ L, e |= φ ([= t] φ)φ is always satisfied within every consecutive interval(epoch) of t time units.t > 0 and for all consecutive intervals t, lt ⊆ L and lt |= φ(φ 1 sw φ 2 ) φ 1 starts with φ 2 .
(a) L |= φ 1 and L |= φ 2 , (b) B φ 1 [1] 񮽙 = B φ 2 [1], (c) B φ 1 .
starttime = B φ 2 .
starttime (φ 1 sw[≥ t] φ 2 ) φ 1 starts t time units after φ 2 .
(a) L |= (φ 1 sw φ 2 ), (b) B φ 1 .
starttime ≥ (B φ 2 .
starttime + t) (φ 1 ew φ 2 ) φ 1 ends with φ 2 .
(a) L |= φ 1 and L |= φ 2 , (b) B φ 1 [1] 񮽙 = B φ 2 [1], (c) B φ 1 .
endtime = B φ 2 .
endtime (φ 1 ew[= t] φ 2 ) φ 1 ends t time units after φ 2 .
(a) L |= (φ 1 ew φ 2 ), (b) B φ 1 .
endtime = (B φ 2 .
endtime + t) (φ 1 olap φ 2 ) φ 1 overlaps φ 2, that is, φ 1 starts after φ 2 starts but before φ 2 ends and ends after φ 2 ends.
(a) L |= φ 1 and L |= φ 2 , (b) B φ 1 [1] 񮽙 = B φ 2 [1], (c) (B φ 2 .
starttime < B φ 1 .
starttime < B φ 2 .
endtime) and (B φ 1 .
endtime > B φ 2 .
endtime) (φ 1 olap[> t] φ 2 )φ 1 overlaps φ 2 and the overlapping region is greater than t time units.
(a) L |= (φ 1 olap φ 2 ), (b) the overlap (B φ 2 .
endtime − B φ 1 .
starttime) > t (φ 1 eq φ 2 ) φ 1 equals φ 2 in duration.
(a) L |= φ 1 and L |= φ 2 , (b) B φ 1 [1] 񮽙 = B φ 2 [1], (c) B φ 1 .
duration = B φ 2 .
duration (φ 1 eq[= t] φ 2 ) φ 1 and φ 2 are both of duration t. (a) L |= (φ 1 eq φ 2 ), (b) B φ 1 .
duration = B φ 2 .
duration = t (φ 1 dur φ 2 ) φ 1 occurs during φ 2 , that2 .
(a) L |= (φ 1 dur φ 2 ), (b) (t 1 ≤ B φ 1 .
duration ≤ t 2 ) (φ)[icount = c] The number of behavior instances satisfying φ is c. (a) L |= φ, (b) there exist distinct B 1 φ . . . B c φ ⊆ L (φ)[bcount = c]Behavior instances satisfying φ are of size c.(a) L |= φ, (b) B φ .
bcount = c (φ)[rate > c]Behavior instances satisfying φ have a rate, defined as (behavior size / behavior duration) greater than c.(a) L |= φ, (b) (B φ .
bcount/B φ .
duration) > c and B φ .
duration > 0 (φ)[at < t]Starting time of behavior instances satisfying φ must be less than absolute time t.(a) L |= φ, (b) B φ .
starttime < t (φ)[end ≥ t]Behavior instances satisfying φ have endtime greater than absolute time t. where(a) L |= φ, (b) B φ .
endtime ≥ t (φ)[duration 񮽙 = t] Behavior instances satisfying φ are of duration 񮽙 = t. (a) L |= φ, (b) B φ .
duration 񮽙 = t(e i1 , . . . , e i k ) ⊆ L.Given a finite sequential log L and a user-defined behavior model φ, goal of the analysis is to find all behavior instances (B 1 φ , B 2 φ , . . .) from L that satisfy the behavior model, where satisfiability is defined as follows:L |= φ iff ∃B φ ⊆ L and |B φ | > 0That is, the log L satisfies (|=) the behavior model φ iff there exists a behavior instance B φ in L of finite length |B φ |.
Since φ is a composite formula created using many sub-formulas, the satisfiability of φ is determined as a function of satisfiability of its sub-formulae.
Table 2 defines the satisfiability criteria for sub-formulae formed using the operators and constraints.
We next explain the key language ideas by defining simple models and applying them to a fictitious data set.Assume a packet trace of seven IP packets representing an interaction between four nodes A, B, C and D as shown in Figure 2.
Let the sequential log of corresponding events be e 1 , e 2 , . . . , e 7 .
Using the states ip s2d and ip d2s defined earlier in Section 3.3, IP flow behavior is written as a causal relationship between the state propositions ip s2d and ip d2s as IPFLOW=(ip s2d ip d2s).
There are three IP flow instances in Figure 2 that satisfy IPFLOW, that is, icount = 3 with bcount = 2 for each instance:B 1 ipf low = (e1, e7) B 2 ipf low = (e2, e5) B 3 ipf low = (e3, e4)Extending the example, a complex behavior for pairs of overlapping IP flows can now be written as IPFLOW PAIRS=(IPFLOW olap IPFLOW).
There are in all three instances of overlapping IPFLOW pairs from Figure 2.
That is,B 1ipf low pairs = ((e1, e7), (e2, e5)) B 2 ipf low pairs = ((e1, e7), (e3, e4)) B 3 ipf low pairs = ((e2, e5), (e3, e4))Again, icount = 3 and for each instance bcount = 2, since bcount counts the number of IPFLOW occurrences and not individual events.We can additionally define a bad IP flow behavior BAD IPFLOW as one for which there was no matching response from the destination.That is, BAD IPFLOW=(ip s2d (not ip d2s)).
Event e 6 matches BAD IPFLOW model since it has no matching response.
That is, B 1 bad ipf low = (e 6 ), with bcount = 1.
The next section describes the architecture of the analysis framework.
Given our objective of semantic-level data analysis, we require the analysis framework to support (a) analysis of multi-type, multi-variate, timestamped data, (b) defining new models by composing existing models, and (c) storage, retrieval and extensibility of domain-specific behavior models.
The framework has five components as shown in Figure 3; the knowledge base, a data normalizer, an event storage system, an analysis engine and a presentation engine.
The decoupling of behavior model specification, the input processing and the analysis algorithms, allows the framework to be directly applied across several different domains.
Subsequent sections discuss the details of each component.
The knowledge base provides a namespace-based storage mechanism to store behavior models and is central in providing an extensible framework.
For example, our networking domain currently defines models for ipflow, tcpflow, icmpflow and udpflow.
These behavior models capture common domain information and allow a user to rapidly compose higher-level models by reusing existing behavior models.
Reusing a behavior model from the knowledge base constitutes importing it using its namespace and name.
For example, referring to the behavior model in Figure 4(a), line 5 imports the IPFLOW model from the NET.BASE PROTO domain.
The namespace allows categorization of models into domain-specific areas while allowing composition of models across domains.
We implement namespaces similar to Java namespaces, that is, each component in the namespace corresponds to a directory name on the filesystem.
This simple design ensures that the knowledge base is easily customizable and extensible.
The data normalizer maps a data record to the event format defined in Section 3.2.
Raw data accepted by the normalizer can be in the form or trace files, packet dumps, audit logs, security logs, syslogs, kernel logs or script output with the only requirement that each data record have a timestamp and a message field.
Specialized plugins in the normalizer convert each type of raw data into corresponding events.
The event storage component is responsible for storing the events from the data normalizer into a database.
Every event-type has a separate table, the columns of the tables correspond to the event attributes and each row describes an event.
The current implementation stores all events into a SQLite database for two reasons: (a) it provides a standard and ready-to-use interface for storing and fetching events and (b) its server-less operation and open-source nature ensures portability on commodity systems.
Our experience suggests that SQLite performs reasonably well for a large number of situations but presents challenges for complex analysis as the volume of events increases.
Our future work includes investigating the scale and efficiency challenges involved in storage and retrieval of events.
Given a finite sequential log L and a user-defined behavior model φ, goal of the analysis engine is to find all behavior instances (B 1 φ , B 2 φ , . . .) from L that satisfy the behavior model.
Let the events in L be stored internally in the event storage database E db .
We discuss only the key ideas behind the analysis process by describing extraction of behavior instances satisfying the IPFLOW model defined in Section 3.4 from the sample data in Figure 2.
The behavior model φ is first internally represented in a manner similar to a compiler expression-tree and is then evaluated left-to-right in a post-order fashion.
The satisfiability of the behavior model is determined as a function of satisfiability of each of the component behaviors according to the semantics defined in Table 2.
For the IPFLOW model, the state proposition ip s2d={etype=PKT IP,sip=$$,dip=$$} is evaluated first.
Since it does not have any dependent attributes, its expression is converted to the following query {etype=PKT IP,sip= * ,dip= * } and is used to fetch all events in E db matching the query.
All events (e 1 , e 2 , e 3 , e 4 , e 5 , e 6 , e 7 ) match the state ip s2d.Next, the proposition ip d2s={etype=PKT IP, sip=$ip s2d.dip, dip=$ip s2d.sip} is evaluated.
The attributes depend on the attributes of state ip s2d.
So, using each event that matched ip s2d, a corresponding query is generated by resolving the values of sip and dip using the values from the matched events.
From Fig- ure 2, e 1 matches e 7 , e 2 matches e 5 , e 3 matches e 4 .
e 5 and e 6 are also possible candidates but since e 5 already matched e 2 , it is not paired with e 6 .
Finally, the operator 񮽙 is evaluated, where the satisfiability criteria described in Table 2 is applied and any specified operator constraints are checked.
The three instances satisfying the criteria (e 1 ,e 7 ), (e 2 , e 5 ), and (e 3 ,e 4 ) are returned.The presentation engine is responsible for extracting the output from the analysis stage and presenting it in a summarized format.
We currently support printing the output in a tabular format as shown in Figure 3(c).
We next present a brief analysis of the algorithm.Algorithm Analysis As described in Section 3.3, state propositions could either contain constant attributevalues (cStates), such as 10.1.1.2; dependent values (dStates), such as $s1.dip; or dynamic values (iStates), such as $$.
A simple behavior consists of a combination of these states using one or more combinations of operators and constraints.
We assume a constant processing time for all operators and constraints.
Then, given an input of N events, processing a state proposition can involve two important operations which influence the runtime: (i) querying using the state expression and (ii) processing the results of the query if any.
In the case of cStates and iStates, there is exactly one query made, and it generates at most N responses.
Thus, the worst case for processing those N responses is O(N ).
In the case of a dstate, given N events, there are N queries to be made and in the worst case every query may return O(N ) results that have to be processed.
Thus, processing dependent states involves a worst case of O(N 2 ) operations.
We present our performance results in Section 6.
In this section, we evaluate the utility of our semantic framework by applying it to five different analysis scenarios: (a) confirming a hypothesis on collected network traces, (b) specifying expected system behavior during network experimentation, (c) modeling worm behavior as an example security threat, (d) modeling dynamic change, and (e) rapidly composing models to create higher-level behaviors.
We present detailed explanation of input, the behavior model and analysis output for the first two cases.
Due to space constraints, we briefly discuss the remaining three cases with their corresponding behavior models, demonstrating features of our semantic analysis framework.
Researchers frequently need to validate hypothesis or test results presented by other researchers.
We emulate one such scenario by validating the results presented by Hussain et al. [5] to demonstrate how behavior models can be rapidly created to reproduce results.
We also discuss the syntax involved in writing a complete behavior model.In the above referenced paper, a threshold-based heuristic was presented to identify DDoS attacks in traces captured at an ISP.
Attacks on a victim were identified by testing for two thresholds on anonymized traces: (a) the number of sources that connect to the same destination within one second exceeds 60, or (b) the traffic rate exceeds 40,000 packets/sec.
We demonstrate the advantages of behavior model-based analysis by defining a model to test for the two heuristics listed above using 10 seconds of the trace file containing the start of an attack.
We normalize the packet traces to 142,530 PKT IP events.
-------------------------------------------------------- timestamp | sip | dip | etype ---------------------------------------------------------------------------------------------------------------- timestamp | sip | dip | etype --------------------------------------------------------Referring to the model script shown in Figure 4(a), lines 2-5 define the model header.
Line 4 does not specify any qualifying conditions, that is, filters, for the events it can process.
Line 5 imports the IPFLOW model from the knowledge base.
Lines 7-8 define the necessary state propositions.
Line 7 defines sA, a simple state which just captures an IP packet from some source to destination.
Line 8 defines a state sB with a dependency that its dip has to be equal to the dip in sA.
State sA thus provides a context for sB.Line 10 expresses the first hypothesis that there should be more than 60 sources connecting to the same destination for an attack.
We apply the 񮽙 operator to denote that we expect sA to occur before sB.
The behavior constraint bcount (refer Section 3.4) applied to sA limits number of events returned to 1, whereas it is applied to sB so that atleast 59 events should occur since the event matching sA occurred.
Additionally, the operator constraint [<=1s] !"
#$%&!'
()$ *)+ ,(- .
/, '(0 ,(-0 0(a) DNS Kaminsky experiment setup.
(b) Set of possible experiment behaviors.
binds sA and sB to occur within a second in the order specified.Line 11 defines the second hypothesis that requires that the packet rate be ≥ 40,000 by using the rate constraint on state proposition sA.
Lastly, line 13 defines the behavior model DDOS HYP which asserts that either hyp 1 or hyp 2 or both are valid.
The four attributes timestamp,sip,dip,etype are reported in the final output.When the model is applied to the packet trace, it produces an output as shown Figure 4(b).
We see that there are two instances reported matching hypothesis hyp 1 both with 60 events within a 1 second interval.
The output also shows the corresponding state or behavior definitions matching the following events.
The two destination IPs that are under attack are 87.231.216.115 and 87.134.184.48.
This output is consistent with the findings reported in the original paper [5].
This example clearly demonstrates the ease with which simple hypotheses could be modeled and validated.
The original authors wrote about 2,000 lines of C code to identify attacks.
The same validation was expressed in about five lines as a behavior model.
Additionally, this model can now be shared and easily modified and extended.
Running experiments on a testbed, such as DETER [2], is challenging since it is hard to ascertain the validity of the experiment manually.
With our framework, a model can be used to capture the "definition of validity" which includes possible successful and failed behaviors for an experiment and then confirmatory analysis can verify if it was met.
Such a model can also be easily shared with other experimenters promoting sharing and reuse of experiments.We present an experiment emulating Dan Kaminsky's popular DNS attack [7] using the metasploit [11] framework.
Referring to Figure 5(a), the attackers objective is to poison the cache of the victimns so that any requests to eby.com are redirected to a fake nameserver (fakens) instead of the real nameserver (realns).
We refer the reader to [7] for a detailed understanding of the attack.
Since the attack exploits a race condition, our experiment setup has to permit successful occurrences as well as failed occurrences of the attack.
Figure 5(b) captures the experiment behavior as a tree of possibilities where the nodes are the experiment states and the paths connecting the states are possible experiment behaviors.
These states are not exhaustive but sufficient to capture most of the semantics of the experiment.
Specifically, we see that there are three possible behaviors that can lead to failures and one behavior that can lead to success.The behavior model script is shown in Figure 5(c).
Lines 2-4 define the model as DNSKAMINSKY over events of type PKT DNS.
Line 5 imports the DNSREQRES model that already defines states and behaviors relevant to the DNS protocol.Lines 7-17 define five different states that are relevant to the experiment.
Line 8 defines the first DNS query from attacker to victim and provides a context for further states.
Line 10 defines a query from the victim to real nameserver by requiring that the source IP address of this query be same as the destination IP address of the previous query and the DNS questions of both states be identical.
This makes sure that the forwarded query by the victim nameserver is the same as the one received.
Line 12 defines the response from the real nameserver to the victim nameserver.
The response is related to the request in line 10 by using the state identifier of the query state VtoR query.
To specifically distinguish this response from the attacker's response, we mention the value of the dnsauth attribute that is expected in the response.
There are two cases for specifying the attacker's response.
Line 14 defines the attacker's response same as the real nameserver response except that we mention the fake nameserver as value of the dnsauth attribute.
Line 16 defines the case where the attacker's response is incorrect due to a wrongly guessed DNS transaction id.
The bcount constraint specifies that any number of responses can be matched since the attacker can send multiple forged responses.
Attribute values not defined in the above states default to their definitions in DNSREQRES.Lines 19-22 specify four possible behaviors corresponding to the four different paths in Figure 5(b).
Line 20 uses the xor operator to merge two behavior paths.
The other behaviors use the 񮽙 operator to capture the causation between the states.
Finally, the behavior model is defined in the model section using FAILURE and SUCCESS behaviors.
Referring to Figure 5(b), we see that b 1 and b 2, where b 1 is a composite of two behaviors, lead to FAILURE and b 3 leads to SUCCESS.
By default, the framework composes the final model by or'ing the behaviors specified in the model section.After running the experiment and capturing DNS packets, we normalize the last 10,000 packets to PKT DNS events since they contain a successful attack along with failures representative of rest of the capture.
The framework outputs one SUCCESS instance and 622 FAILURE instances as shown in Figure 6.
of the next scan to be the same as the previously infected host.
The forward-dependent attribute src is initialized automatically the first time single spread is parsed by considering it to be a dynamic ($$) variable.
The next iteration over spread chain then uses the values as determined dynamically by single spread.
Dynamic changes are a fundamental characteristic of networked and distributed environments.
One example of a dynamic change is the change in rate of a stream of packets due to an anomalous condition such as a DoS attack.
Our objective in this case study is to model an expected reduction in the rate of legitimate HTTP traffic due to DoS attack on a server.
Our raw data consists of IDS DoS attack alerts and HTTP packets.The DYNAMIC CHANGE model, containing only the relevant aspects is described in Figure 7(b).
Line 2 defines a state capturing a HTTP packet between a source and destination.
Line 3 defines a state capturing a DoS attack alert, additionally requiring the destination to be same as the destination in the HTTP packet.
Lines 4 and 5 describe the HTTP packet stream rates before and after the attack respectively.
The change boundary is defined by the attack event that is triggered once the attack starts.
Since attack event represents a single event, it has the same starttime and endtime.
Line 6 use the ew (endswith) operator to define the attack start condition, which specifies that the http stream at100 behavior end within five seconds of the attack event.
The DYNAMIC CHANGE model is then an assertion that the HTTP stream rate reduces following the attack.
Our final case study demonstrates the ease of composing and extending existing models to define semantically relevant higher-level behavior.We combine our previously defined models DNSKAMINSKY and WORMSPREAD to create a COMBINED ATTACK scenario as shown in Figure 7(c).
Line 2 captures the behavior where a worm infects a host machine and scans and infects another host.
Line 3 describes the behavior where the worm launches a DNS Kaminsky attack on some DNS server from the last infected host.
We do not specify any server for the DNS Kaminsky attack due to the abstractness of the DNSKAMINSKY model which infers the destination dynamically.
Line 4 is the final behavior model combining both the attacks.
In line 3, we only constrain the sip and leave other attributes unspecified.
This demonstrates the ability to extend the imported models with only the desired attribute values while leaving the others as defined in the imported model.
A common approach for semantic-level analysis involves use of custom scripts or tools encoding context-specific semantics.
Since custom scripts and tools can be written using a variety of programming and optimization techniques, any evaluation of our generic framework against them would be very subjective and thus flawed.
Instead, we choose to report the raw runtime performance of our prototype implementation on five basic analyses tasks over event datasets of increasing size.The runtime performance of the framework depends on the language constructs, input data, analysis algorithm and implementation mechanisms used.
Since our primary focus in this paper is on enabling semantic functionality, we prototyped the framework in Python using a SQLite database as backend for storing events.
The input events used were PKT DNS events collected for the case study in Section 5.2.
The performance analysis was conducted on a laptop with an Intel Pentium-M processor running at 1.86 GHz and with a memory of 2 GB.We measure runtime as a function of two variables: (a) the number of events input to the algorithm, (b) the behavior complexity, defined as the processing complexity of state propositions in a behavior formula.
As discussed in Section 3.3, there are three types of state propositions based on attribute assignments; constant value attributes denoted as cState, dependent value attributes denoted as dState, and dynamic attribute values denoted as iState.
These states can be combined to form five basic behaviors, each representing a basic semantic analysis task: b1 = (cState), represents extracting events with known attributes and values; b2 = (iState), represents extracting events with particular attributes but unknown values; b3 = (iState 񮽙 iState), represents extracting causally correlated yet value-independent events; b4 = (iState 񮽙 dState), represents extracting causally correlated and value-dependent events; and b5 = (iState 񮽙 dState 񮽙 dState 񮽙 dState), represents extracting a long chain of causal events.
Although we limit our analysis to the 񮽙 operator, all operators incur uniform processing overhead in the algorithm, thus resulting in similar performance results.
The chosen event set along with the behaviors are representative of a worst-case input to the framework.
We measure the performance using above behaviors over event sets in increments of 10,000 events.
We stop at the event set when runtime exceeds 60 minutes.The results are averaged over three runs and are shown in Figure 8.
The plots for behaviors consisting of cStates and iStates b1, b2 and b3 tend to be linear as discussed in Section 4.4.
One would expect that behavior b5, containing three dStates would show significantly higher runtime than behavior b4 containing only one dState.
Both show quadratic performance, since, in a chain of dependent states, the states further in the chain process lesser events than states in front of the chain.
We thus see that runtime quickly becomes quadratic given a worst-case set of events and behaviors containing dependent state propositions.
The current Python and SQLite-based implementation also add penalty to the framework runtime.
We investigate these issues as part of our future work.
In this paper, we presented a behavior-based semantic analysis framework that allows the user to analyze data at a higher-level of abstraction.
Typically, system experts rely on their intuition and experience to manually analyze and categorize scenarios and then hand-craft rules and patterns for analysis.
Hence due to the manual and ad-hoc nature of this analysis process, there is limited extensibility and composibility of analysis strategies.
In this paper we show that our approach is more systematic, can retain expert knowledge, and supports composing behaviors from existing models.
We evaluated the utility of our framework against five analyses scenarios which demonstrated the ease with which a user's higherlevel understanding of system operation was expressed as behavior models over data.Our future work includes investigating the scale and efficiency issues that arise during processing large volumes of data in both offline and real-time settings like intrusion detection.
We will investigate stream-based SQL query extensions [6] to improve performance.
We will also investigate extending our logic with existential and universal quantifiers.
Currently, our framework requires a user to either manually specify behavior models or use existing models from the knowledge base to explore data.
To further exploratory analysis, we would need to alert users to interesting unanticipated behaviors.
We are exploring data mining algorithms to automatically discover and compose behavior models from data.The fundamental goal of the behavior-based semantic analysis framework is to introduce a semantic approach to data analysis in networked and distributed systems research and operations.
We hope that this paper serves as a catalyst for further research on semantic data analysis.
tIn this case study, we define a behavior model of a typical worm spread detected by IDS alerts collected from multiple hosts.
Assume a network with IDSes on each host reporting two types of timestamped alerts: a SCAN alert when a scan is detected by a host and an INFECT alert when the host is found infected.
Assume an event log created by normalizing the alerts to two types of events with their corresponding attributes.
Given the event log, our objective here is to define a behavior model to extract all possible infection chains of any length and report the hosts involved.We model the worm spread behavior as shown in Figure 7(a) in two stages; by first defining a single spread behavior using events from a single host and then defining the spread chain as a chain of related single spread occurrences.
The single spread behavior, concerning a vulnerable host A, is a sequence of two dependent and casual events: (a) a scan A event with its src attribute pointing to an earlier infected host, followed by (b) an infect A event with its host attribute the same as scan A.dst.
A worm spread chain (spread chain) is then simply defined by a recursive occurrence of related single spread behaviors.
Referring to the model, the forward-dependent attribute src in the definition of scan A connects successive single spread behaviors by requiring the src In this case study, we define a behavior model of a typical worm spread detected by IDS alerts collected from multiple hosts.
Assume a network with IDSes on each host reporting two types of timestamped alerts: a SCAN alert when a scan is detected by a host and an INFECT alert when the host is found infected.
Assume an event log created by normalizing the alerts to two types of events with their corresponding attributes.
Given the event log, our objective here is to define a behavior model to extract all possible infection chains of any length and report the hosts involved.We model the worm spread behavior as shown in Figure 7(a) in two stages; by first defining a single spread behavior using events from a single host and then defining the spread chain as a chain of related single spread occurrences.
The single spread behavior, concerning a vulnerable host A, is a sequence of two dependent and casual events: (a) a scan A event with its src attribute pointing to an earlier infected host, followed by (b) an infect A event with its host attribute the same as scan A.dst.
A worm spread chain (spread chain) is then simply defined by a recursive occurrence of related single spread behaviors.
Referring to the model, the forward-dependent attribute src in the definition of scan A connects successive single spread behaviors by requiring the src
