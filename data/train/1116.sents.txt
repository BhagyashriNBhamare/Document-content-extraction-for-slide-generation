We describe a method to extract content text from diverse Web pages by using the HTML document's Text-To-Tag Ratio (TTR) rather than specific HTML cues that are not constant across various Web pages.
We describe how to compute the TTR on a line-by-line basis and then cluster the results into content and non-content areas.
The resulting TTR-histogram is not easily clustered because of its one dimensionality; therefore we present a technique to better represent the histogram in two-dimensions.
Next, we compare clustering techniques such as EM, K-Means, and Farthest First-in density and distance modes-with a threshold partitioning technique on the resulting two-dimensional data.
These clustering techniques are also enhanced with the use of histogram smoothing techniques.
We then evaluate our approach using standard accuracy, precision and recall metrics.
The amount of information being gathered and stored on the Internet continues to increase.
The artifacts of this growing market provide interesting new research opportunities that explore social interactions, language, art, mathematics, etc.
Many of these new research opportunities require the content of the Internet to be gathered, processed and stored quickly and efficiently.
This effort is often hampered by the use of structure tags in HTML and XML.
These tags are meaningful only to the browser that renders the document, but bear little semantic meaning to the end user.
Tags and other non-content related HTML characters -images not included -comprise the majority of each page's size (Lu, et al. 2004), and yet, Internet researchers are forced to crawl, compute and store web content in their entirety.This work focuses on extracting content from Web pages that are otherwise laden with structural data, links and advertisements, commonly called Text Extraction (Soderland 1997).
This work is particularly challenging because of the difficulty in determining which part of a web page is meaningful and which part is not.In this paper, we extend our previous work on Web content extraction with the use of the Text-To-Tag Ratio (TTR).
The TTR approach to Web content extraction makes no assumptions about the particular structure of a given Web page, nor does it look for particular cues such as specific HTML tags, etc. as previous research does.
The only necessary pre-condition of a page's structure is that it has some structure.
With this in mind, we construct a TTR-array with the contention that for each line k in the array, the higher the TTR is for the element k relative to the mean TTR of the entire array the more likely that k represents a line of content-text within the HTML document.In this and in previous work (Weninger et al. 2008), we observe that the TTR-array closely resembles a histogram, in that each histogram bucket represents the TTR of a line in an HTML document.
By that observation this paper presents Web content extraction as a histogram clustering task.
Histogram clustering is a widely researched topic that is especially popular with image researchers.
This is especially true among researchers who wish to use the histogram footprints of images as a means for classification, segmentation, etc. (Puzicha et.
al. 1999) ( Sezgin et al. 2004).
However, this research is largely inapplicable because of the dimensionality of images is inherently 2D, whereas Figure 1 clearly shows that the TTR histogram can only originally be represented in a single dimension.As an example, consider the news article from The Hutchinson News 1 that appeared on Wednesday, March 19, 2008.
This Web page is similar to many pages on the Web.
The title banner, hyperlinks and advertisements take up most of the space on the webpage while the content of the page is confined to a relatively small space in the middle.
At the bottom of the page more advertisements and images are displayed along with links to copyright and other administrative information.
This paper will compare the performance of threshold partitioning to that of density and distance-based clustering techniques at the task of extracting content-text from a diverse set of Web pages.
For this particular domain our empirical goal is to maximize recall because we believe that the extraction of errant content is less detrimental than the exclusion of actual content.
In this section we describe the threshold partitioning technique.
For our purposes we consider threshold partitioning to be our baseline because of the ideal results gained from our previous research.
Before the threshold is applied a smoothing pass is made on the histogram.
This is done because without smoothing many important content lines might be lost.
In our Web content domain, these lost content lines may include the page title, a news article byline or dateline, short or one sentence paragraphs, etc. where the TTR would fall below that of the standard deviation.
As a pathological example, consider a 1 The Hutchinson News is available online at http://hutchnews.com.
The specific article is not permanently linked.
To resolve this problem we apply a Gaussian smoothing pass to h. Standard Gaussian smoothing algorithms (Weisstein 2008) are generally implemented for image processing and are continuous, and thus do not suit our purposes.
Therefore the algorithm used in this approach was re-implemented as a discrete function operating in a single dimension.
Equation 1 shows the construction of a Gaussian kernel (k) with a width of 2񮽙񮽙񮽙񮽙񮽙 񮽙 1.񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙
, 0 2񮽙񮽙񮽙񮽙񮽙.
(Eq.
1)The size of and values within k vary according to σ because as the variance of h increases, smoothing necessity also increases.
Next, Equation 2 shows that k is normalized to form kʹ.񮽙
ʹ 񮽙 񮽙 񮽙 񮽙 ∑ 񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 , 0 2񮽙񮽙񮽙񮽙񮽙.
(Eq.
2)Finally, Equation 3 shows that kʹ is convolved with h in order to form a smoothed histogram (hʹ).񮽙
ʹ 񮽙 ʹ 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 , 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙.
(Eq.
3)Compared to Figure 2, hʹ, shown in Figure 3, is better suited for clustering because of the increased cohesiveness within sections and strict differences between sections.
Furthermore, hʹ has a lower standard deviation (40.55 TTR) because outlying peaks and valleys are smoothed.
Similarly, outliers, such as advertisements, that may occupy a single high-TTR line among many low-TTR lines, are smoothed to below the threshold.Finally, let C be the set of content lines such thatd i 񮽙 C iff hʹ i ≥ σʹ.
Note d i 񮽙 hʹ i .
After elements of C are selected, each content-line is stripped of all remaining HTML tags -usually paragraph and anchor tags.
Then the cleaned lines are combined and output to a file for storage, indexing, etc.
This section presents a density and distance-based approach to clustering 1-dimensional histograms.
Specifically, in previous work we observed that when clustering algorithms are applied to 1D data, such as a Text-to-Tag Ratio (TTR) histogram (񮽙), results are consistently inaccurate.
We contend that by transforming the histogram data so that it may be represented in 2-dimensions we can obtain more accurate results.For this task, we define the two dimensions to be (1) a smoothed TTR histogram (񮽙ʹ), and (2) a derivative array of the computed from 񮽙ʹ (񮽙ʹ).
These definitions came about strictly through observations and trial-and-error experimentation.
To compute 񮽙ʹ, first smooth 񮽙 in the same manner as described in Eq.
1-3 to get 񮽙ʹ.
Next, find the derivatives for each element in the array; specifically, we subtract 񮽙ʹ 񮽙 from the mean of the next three elements in order to differentiate on the moving average (񮽙) instead of line-by-line as shown in Equation 4 The smoothed difference array (񮽙ʹ) shows two peaks: the first at the beginning of a content section and a second at the end of a content section with relatively higher values in between.
Of course, histograms can have non-continuous content sections, and in such cases an appropriate number of peaks are displayed.
Finally, we observed that when 񮽙ʹ and 񮽙ʹ are combined as conjoining dimensions ideal clustering properties emerge as shown on left in Figure 5.
As illustrated on right in Figure 5.
, when we manually identified each point to be either content (○) or non-content (×) we observed that the dense collection of points near the origin were non-content lines and the remaining points were content lines.
To test the effectiveness of both the threshold partitioning and clustering approaches documented in the above sections, 176 complete Web pages were downloaded by searching for the keyword "the" from Yahoo's search engine and harvesting the results.
The goal of our experiments was to determine the content data of the Web pages and filter out all extraneous advertisements and site links.
We determined the actual content of each Web page by opening each downloaded file in a browser and manually selecting To evaluate the results of our experiments we used standard accuracy, precision and recall metrics where true positives are content lines that were correctly identified, true negatives are non-content lines that were correct identified, false positives are noncontent lines that were incorrectly identified as content, and false negatives are content lines that were incorrectly identified as non-content.
Diff comparisons are made on a word-by-word basis between the automatically extracted content text and the manually extracted content text to determine the true positives, etc.
Contrary to previously used metrics in (Weninger et al. 2008) a single errant character is not prohibitively detrimental to the final result.Initially, we tested threshold partitioning by setting the threshold at 1 standard deviation as shown in Table 1.
Secondly, we generated an ROC curve by applying a coefficient ranging from 0 to 19 to the threshold as shown in Figure 6.
We tested density and distance clustering techniques by transforming the data into 2-dimensions as described in the previous section and then by running Farthest First (Hochbaum 1985), K-Means (MacQueen 1996) and EM (Dempster et al. 1977) algorithms in distance and density (Ester et al. 1996) modes with 3 clusters.
With 3-clusters the non-content label is given to the cluster with the centroid closest to the origin and the remaining two clusters are labeled content.
The results are presented in Table 2.
For our text extraction purposes we wish to maximize recall.
That is, it is far more detrimental to exclude content text than it is to include non-content text.
Even so, some recall can be sacrificed to make sufficiently large gains in precision.
Therefore we declare Density-Farthest First (DFF) to be the winner.
Furthermore, DFF is comparable to threshold partitioning at 1σ in terms of recall (98.99% -99.51%), but DFF is substantially better in terms of precision (72.10% -61.06%).
Thus, for our purposes, we declare DFF to be the overall winner.
In this paper, we proposed two approaches to clustering histogram data.
We showed that threshold partitioning, although simple, can be used to segment histogram data with a high degree of recall at all levels of sensitivity.
Also, we showed that by generating a second dimension to the histogram via smoothed derivatives we can use standard clustering techniques to achieve high recall and precision.
Finally, by comparing the results of our experiments we observed that the Farthest First clustering algorithm in density mode is best suited for extracting content areas from Web pages in this paradigm.
In future work we plan to experiment with edge detection algorithms on variations on the Text-to-Tag Ratio Histogram.
We also wish to empirically compare this approach with other methods of text extraction.
This research was supported in part by the Defense Intelligence Agency.
We thank Dr. Dan Andresen, Dr. David Gustafson and Dr. Doina Caragea for their insight and useful comments, and Daniel Jones, John Drouhard, Imran Hameed and Jack Hart for their assistance with this project.
