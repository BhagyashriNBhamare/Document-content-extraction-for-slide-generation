We instrumented the Android platform to collect data regarding how often and under what circumstances smart-phone applications access protected resources regulated by permissions.
We performed a 36-person field study to explore the notion of "contextual integrity," i.e., how often applications access protected resources when users are not expecting it.
Based on our collection of 27M data points and exit interviews with participants, we examine the situations in which users would like the ability to deny applications access to protected resources.
At least 80% of our participants would have preferred to prevent at least one permission request, and overall, they stated a desire to block over a third of all requests.
Our findings pave the way for future systems to automatically determine the situations in which users would want to be confronted with security decisions.
Mobile platform permission models regulate how applications access certain resources, such as users' personal information or sensor data (e.g., camera, GPS, etc.).
For instance, previous versions of Android prompt the user during application installation with a list of all the permissions that the application may use in the future; if the user is uncomfortable granting any of these requests, her only option is to discontinue installation [3].
On iOS and Android M, the user is prompted at runtime the first time an application requests any of a handful of data types, such as location, address book contacts, or photos [34].
Research has shown that few people read the Android install-time permission requests and even fewer comprehend them [16].
Another problem is habituation: on average, Android applications present the user with four permission requests during the installation process [13].
While iOS users are likely to see fewer permission requests than Android users, because there are fewer possible permissions and they are only displayed the first time the data is actually requested, it is not clear whether or not users are being prompted about access to data that they actually find concerning, or whether they would approve of subsequent requests [15].
Nissenbaum posited that the reason why most privacy models fail to predict violations is that they fail to consider contextual integrity [32].
That is, privacy violations occur when personal information is used in ways that defy users' expectations.
We believe that this notion of "privacy as contextual integrity" can be applied to smartphone permission systems to yield more effective permissions by only prompting users when an application's access to sensitive data is likely to defy expectations.
As a first step down this path, we examined how applications are currently accessing this data and then examined whether or not it complied with users' expectations.We modified Android to log whenever an application accessed a permission-protected resource and then gave these modified smartphones to 36 participants who used them as their primary phones for one week.
The purpose of this was to perform dynamic analysis to determine how often various applications are actually accessing protected resources under realistic circumstances.
Afterwards, subjects returned the phones to our laboratory and completed exit surveys.
We showed them various instances over the past week where applications had accessed certain types of data and asked whether those instances were expected, and whether they would have wanted to deny access.
Participants wanted to block a third of the requests.
Their decisions were governed primarily by two factors: whether they had privacy concerns surrounding the specific data type and whether they understood why the application needed it.We contribute the following:• To our knowledge, we performed the first field study to quantify the permission usage by third-party applications under realistic circumstances.
• We show that our participants wanted to block access to protected resources a third of the time.
This suggests that some requests should be granted by runtime consent dialogs, rather than Android's previous all-or-nothing install-time approval approach.
• We show that the visibility of the requesting application and the frequency at which requests occur are two important factors which need to be taken into account in designing a runtime consent platform.
While users are required to approve Android application permission requests during installation, most do not pay attention and fewer comprehend these requests [16,26].
In fact, even developers are not fully knowledgeable about permissions [40], and are given a lot of freedom when posting an application to the Google Play Store [7].
Applications often do not follow the principle of least privilege, intentionally or unintentionally [44].
Other work has suggested improving the Android permission model with better definitions and hierarchical breakdowns [8].
Some researchers have experimented with adding fine-grained access control to the Android model [11].
Providing users with more privacy information and personal examples has been shown to help users in choosing applications with fewer permissions [21,27].
Previous work has examined the overuse of permissions by applications [13,20], and attempted to identify malicious applications through their permission requests [36] or through natural language processing of application descriptions [35].
Researchers have also developed static analysis tools to analyze Android permission specifications [6,9,13].
Our work complements this static analysis by applying dynamic analysis to permission usage.
Other researchers have applied dynamic analysis to native (non-Java) APIs among third-party mobile markets [39]; we apply it to the Java APIs available to developers in the Google Play Store.Researchers examined user privacy expectations surrounding application permissions, and found that users were often surprised by the abilities of background applications to collect data [25,42].
Their level of concern varied from annoyance to seeking retribution when presented with possible risks associated with permissions [15].
Some studies employed crowdsourcing to create a privacy model based on user expectations [30].
Researchers have designed systems to track or reduce privacy violations by recommending applications based on users' security concerns [2,12,19,24,28,[46][47][48].
Other tools dynamically block runtime permission requests [37].
Enck et al. found that a considerable number of applications transmitted location or other user data to third parties without requiring user consent [12].
Hornyack et al.'s AppFence system gave users the ability to deny data to applications or substitute fake data [24].
However, this broke application functionality for onethird of the applications tested.Reducing the number of security decisions a user must make is likely to decrease habituation, and therefore, it is critical to identify which security decisions users should be asked to make.
Based on this theory, Felt et al. created a decision tree to aid platform designers in determining the most appropriate permission-granting mechanism for a given resource (e.g., access to benign resources should be granted automatically, whereas access to dangerous resources should require approval) [14].
They concluded that the majority of Android permissions can be automatically granted, but 16% (corresponding to the 12 permissions in Table 1) should be granted via runtime dialogs.Nissenbaum's theory of contextual integrity can help us to analyze "the appropriateness of a flow" in the context of permissions granted to Android applications [32].
There is ambiguity in defining when an application actually needs access to user data to run properly.
It is quite easy to see why a location-sharing application would need access to GPS data, whereas that same request coming from a game like Angry Birds is less obvious.
"Contextual integrity is preserved if information flows according to contextual norms" [32], however, the lack of thorough documentation on the Android permission model makes it easier for programmers to neglect these norms, whether intentionally or accidentally [38].
Deciding on whether an application is violating users' privacy can be quite complicated since "the scope of privacy is wideranging" [32].
To that end, we performed dynamic analysis to measure how often (and under what circumstances) applications were accessing protected resources, whether this complied with users' expectations, as well as how often they might be prompted if we adopt Felt et al.'s proposal to require runtime user confirmation before accessing a subset of these resources [14].
Finally, we show how it is possible to develop a classifier to automatically determine whether or not to prompt the user based on varying contextual factors.
Our long-term research goal is to minimize habituation by only confronting users with necessary security decisions and avoiding showing them permission requests that are either expected, reversible, or unconcerning.
Selecting which permissions to ask about requires understanding how often users would be confronted with each type of request (to assess the risk of habituation) and user reactions to these requests (to assess the benefit to users).
In this study, we explored the problem space in two parts:we instrumented Android so that we could collect actual usage data to understand how often access to various protected resources is requested by applications in practice, and then we surveyed our participants to understand the requests that they would not have granted, if given the option.
This field study involved 36 participants over the course of one week of normal smartphone usage.
In this section, we describe the log data that we collected, our recruitment procedure, and then our exit survey.
In Android, when applications attempt to access protected resources (e.g., personal information, sensor data, etc.) at runtime, the operating system checks to see whether or not the requesting application was previously granted access during installation.
We modified the Android platform to add a logging framework so that we could determine every time one of these resources was accessed by an application at runtime.
Because our target device was a Samsung Nexus S smartphone, we modified Android 4.1.1 (Jellybean), which was the newest version of Android supported by our hardware.
Our goal was to collect as much data as possible about each applications' access to protected resources, while minimizing our impact on system performance.
Our data collection framework consisted of two main components: a series of "producers" that hooked various Android API calls and a "consumer" embedded in the main Android framework service that wrote the data to a log file and uploaded it to our collection server.We logged three kinds of permission requests.
First, we logged function calls checked by checkPermission() in the Android Context implementation.
Instrumenting the Context implementation, instead of the ActivityManagerService or PackageManager, allowed us to also log the function name invoked by the user-space application.
Next, we logged access to the ContentProvider class, which verifies the read and write permissions of an application prior to it accessing structured data (e.g., contacts or calendars) [5].
Finally, we tracked permission checks during Intent transmission by instrumenting the ActivityManagerService and BroadcastQueue.
Intents allow an application to pass messages to another application when an activity is to be performed in that other application (e.g., opening a URL in the web browser) [4].
We created a component called Producer that fetches the data from the above instrumented points and sends it back to the Consumer, which is responsible for logging everything reported.
Producers are scattered across the Android Platform, since permission checks occur in multiple places.
The Producer that logged the most data was in system server and recorded direct function calls to Android's Java API.
For a majority of privileged function calls, when a user application invokes the function, it sends the request to system server via Binder.
Binder is the most prominent IPC mechanism implemented to communicate with the Android Platform (whereas Intents communicate between applications).
For requests that do not make IPC calls to the system server, a Producer is placed in the user application context (e.g., in the case of ContentProviders).
The Consumer class is responsible for logging data produced by each Producer.
Additionally, the Consumer also stores contextual information, which we describe in Section 3.1.2.
The Consumer syncs data with the filesystem periodically to minimize impact on system performance.
All log data is written to the internal storage of the device because the Android kernel is not allowed to write to external storage for security reasons.
Although this protects our data from curious or careless users, it also limits our storage capacity.
Thus, we compressed the log files once every two hours and upload them to our collection servers whenever the phone had an active Internet connection (the average uploaded and zipped log file was around 108KB and contained 9,000 events).
Due to the high volume of permission checks we encountered and our goal of keeping system performance at acceptable levels, we added rate-limiting logic to the Consumer.
Specifically, if it has logged permission checks for a particular application/permission combination more than 10,000 times, it examines whether it did so while exceeding an average rate of 1 permission check every 2 seconds.
If so, the Consumer will only record 10% of all future requests for this application/permission combination.
When this rate-limiting is enabled, the Consumer tracks these application/permission combinations and updates all the Producers so that they start dropping these log entries.
Finally, the Consumer makes a note of whenever this occurs so that we can extrapolate the true number of permission checks that occurred.
We hooked the permission-checking APIs so that every time the system checked whether an application had been granted a particular permission, we logged the name of the permission, the name of the application, and the API method that resulted in the check.
In addition to timestamps, we collected the following contextual data:• Visibility-We categorized whether the requesting application was visible to the user, using four categories: running (a) as a service with no user interaction; (b) as a service, but with user interaction via notifications or sounds; (c) as a foreground process, but in the background due to multitasking; or (d) as a foreground process with direct user interaction.
• Screen Status-Whether the screen was on/off.
• Connectivity-The phone's WiFi connection state.
• Location-The user's last known coordinates.
In order to preserve battery life, we collected cached location data, rather than directly querying the GPS.
• View-The UI elements in the requesting application that were exposed to the user at the time that a protected resource was accessed.
Specifically, since the UI is built from an XML file, we recorded the name of the screen as defined in the DOM.
• History-A list of applications with which the user interacted prior to the requesting application.
• Path-When access to a ContentProvider object was requested, the path to the specific content.Felt et al. proposed granting most Android permissions without a priori user approval and granting 12 permissions (Table 1) at runtime so that users have contextual information to infer why the data might be needed [14].
The idea is that, if the user is asked to grant a permission while using an application, she may have some understanding of why the application needs that permission based on what she was doing.
We initially wanted to perform experience sampling by probabilistically questioning participants whenever any of these 12 permissions were checked [29].
Our goal was to survey participants about whether access to these resources was expected and whether it should proceed, but we were concerned that this would prime them to the security focus of our experiment, biasing their subsequent behaviors.
Instead, we instrumented the phones to probabilistically take screenshots of what participants were doing when these 12 permissions were checked so that we could ask them about it during the exit survey.
We used reservoir sampling to minimize storage and performance impacts, while also ensuring that the screenshots covered a broad set of applications and permissions [43].
Figure 1 shows a screenshot captured during the study along with its corresponding log entry.
The user was playing the Solitaire game while Spotify requested a WiFi scan.
Since this permission was of interest (Table 1), our instrumentation took a screenshot.
Since Spotify was not the application the participant was interacting with, its visibility was set to false.
The history shows that prior to Spotify calling getScanResults(), the user had viewed Solitaire, the call screen, the launcher, and the list of MMS conversations.
[14].
We randomly took screenshots when these permissions were requested by applications, and we asked about them in our exit survey.
We placed an online recruitment advertisement on Craigslist in October of 2014, under the "et cetera jobs" section.
1 The title of the advertisement was "Research Study on Android Smartphones," and it stated that the study was about how people interact with their smartphones.
We made no mention of security or privacy.Those interested in participating were directed to an online consent form.
Upon agreeing to the consent form, potential participants were directed to a screening application in the Google Play store.
The screening application asked for information about each potential participant's age, gender, smartphone make and model.
It also collected data on their phones' internal memory size and the installed applications.
We screened out applicants who were under 18 years of age or used providers other than T-Mobile, since our experimental phones could not attain 3G speeds on other providers.
We collected data on participants' installed applications so that we could preinstall free applications prior to them visiting our laboratory.
(We copied paid applications from their phones, since we could not download those ahead of time.)
We contacted participants who met our screening requirements to schedule a time to do the initial setup.
Overall, 48 people showed up to our laboratory, and of those, 40 qualified (8 were rejected because our screening application did not distinguish some Metro PCS users from T-Mobile users).
In the email, we noted that due to the space constraints of our experimental phones, we might not be able to install all the applications on their existing phones, and therefore they needed to make a note of the ones that they planned to use that week.
The initial setup took roughly 30 minutes and involved transferring their SIM cards, helping them set up their Google and other accounts, and making sure they had all the applications they needed.
We compensated each participant with a $35 gift card for showing up at the setup session.
Out of 40 people who were given phones, 2 did not return them, and 2 did not regularly use them during the study period.
Of our 36 remaining participants who used the phones regularly, 19 were male and 17 were female; ages ranged from 20 to 63 years old (µ = 32, σ = 11).
After the initial setup session, participants used the experimental phones for one week in lieu of their normal phones.
They were allowed to install and uninstall applications, and we instructed them to use these phones as they would their normal phones.
Our logging framework kept track of every protected resource accessed by a userlevel application along with the previously-mentioned contextual data.
Due to storage constraints on the devices, our software uploaded log files to our server every two hours.
However, to preserve participants' privacy, screenshots remained on the phones during the course of the week.
At the end of the week, each participant returned to our laboratory, completed an exit survey, returned the phone, and then received an additional $100 gift card (i.e., slightly more than the value of the phone).
When participants returned to our laboratory, they completed an exit survey.
The exit survey software ran on a laptop in a private room so that it could ask questions about what they were doing on their phones during the course of the week without raising privacy concerns.
We did not view their screenshots until participants gave us permission.
The survey had three components:• Screenshots-Our software displayed a screenshot taken after one of the 12 resources in Table 1 was accessed.
Next to the screenshot (Figure 2a), we asked participants what they were doing on the phone when the screenshot was taken (open-ended).
We also asked them to indicate which of several actions they believed the application was performing, chosen from a multiple-choice list of permissions presented in plain language (e.g., "reading browser history," "sending a SMS," etc.).
After answering these questions, they proceeded to a second page of questions ( Figure 2b).
We informed participants at the top of this page of the resource that the application had accessed when the screenshot was taken, and asked them to indicate how much they expected this (5-point Likert scale).
Next, we asked, "if you were given the choice, would you have prevented the app from accessing this data," and to explain why or why not.
Finally, we asked for permission to view the screenshot.
This phase of the exit survey was repeated for 10-15 different screenshots per participant, based on the number of screenshots saved by our reservoir sampling algorithm.
• Locked Screens-The second part of our survey involved questions about the same protected resources, though accessed while device screens were off (i.e., participants were not using their phones).
Because there were no contextual cues (i.e., screenshots), we outright told participants which applications were accessing which resources and asked them multiple choice questions about whether they wanted to prevent this and the degree to which these (a) On the first screen, participants answered questions to establish awareness of the permission request based on the screenshot.
(b) On the second screen, they saw the resource accessed, stated whether it was expected, and whether it should have been blocked.
After participants completed the exit survey, we reentered the room, answered any remaining questions, and then assisted them in transferring their SIM cards back into their personal phones.
Finally, we compensated each participant with a $100 gift card.Three researchers independently coded 423 responses to the open-ended question in the screenshot portion of the survey.
The number of responses per participant varied, as they were randomly selected based on the number of screenshots taken: participants who used their phones more heavily had more screenshots, and thus answered more questions.
Prior to meeting to achieve consensus, the three coders disagreed on 42 responses, which resulted in an inter-rater agreement of 90%.
Taking into account the 9 possible codings for each response, Fleiss' kappa yielded 0.61, indicating substantial agreement.
In many cases, it is entirely expected that an application might make frequent requests to resources protected by permissions.
For instance, the INTERNET permission is used every time an application needs to open a socket, ACCESS FINE LOCATION is used every time the user's location is checked by a mapping application, and so on.
However, in these cases, one expects users to have certain contextual cues to help them understand that these applications are running and making these requests.
Based on our log data, most requests occurred while participants were not actually interacting with those applications, nor did they have any cues to indicate that the applications were even running.
When resources are accessed, applications can be in five different states, with regard to their visibility to users:1.
Visible foreground application (12.04%): the user is using the application requesting the resource.
2.
Invisible background application (0.70%): due to multitasking, the application is in the background.
3.
Visible background service (12.86%): the application is a background service, but the user may be aware of its presence due to other cues (e.g., it is playing music or is present in the notification bar).
4.
Invisible background service (14.40%): the application is a background service without visibility.
5.
Screen off (60.00%): the application is running, but the phone screen is off because it is not in use.
Combining the 3.3M (12.04% of 27M) requests that were granted when the user was actively using the application (Category 1) with the 3.5M (12.86% of 27M) requests that were granted when the user had other contextual cues to indicate that the application was running (Category 3), we can see that fewer than one quarter of all permission requests (24.90% of 27M) occurred when the user had clear indications that those applications were running.
This suggests that during the vast majority of the time, access to protected resources occurs opaquely to users.
We focus on these 20.3M "invisible" requests (75.10% of 27M) in the remainder of this subsection.Harbach et al. found that users' phone screens are off 94% of the time on average [22].
We observed that 60% of permission requests occurred while participants' phone screens were off, which suggests that permission requests occurred less frequently than when participants were using their phones.
At the same time, certain applications made more requests when participants were not using their phones: "Brave Frontier Service," "Microsoft Sky Drive," and "Tile game by UMoni."
Our study collected data on over 300 applications, and therefore it is possible that with a larger sample size, we would observe other applications engaging in this behavior.
All of the aforementioned applications primarily requested AC-CESS WIFI STATE and INTERNET.
While a definitive explanation for this behavior requires examining source code or the call stacks of these applications, we hypothesize that they were continuously updating local data from remote servers.
For instance, Sky Drive may have been updating documents, whereas the other two applications may have been checking the status of multiplayer games.
Table 2 shows the most frequently requested permissions from applications running invisibly to the user (i.e., Categories 2, 4, and 5); Contextual integrity means ensuring that information flows are appropriate, as determined by the user.
Thus, users need the ability to see information flows.
Current mobile platforms have done some work to let the user know about location tracking.
For instance, recent versions of Android allow users to see which applications have used location data recently.
While attribution is a positive step towards contextual integrity, attribution is most beneficial for actions that are reversible, whereas the disclosure of location information is not something that can be undone [14].
We observed that fewer than 1% of location requests were made when the applications were visible to the user or resulted in the displaying of a GPS notification icon.
Given that Thompson et al. showed that most users do not understand that applications running in the background may have the same abilities as applications running in the foreground [42], it is likely that in the vast majority of cases, users do not know when their locations are being disclosed.This low visibility rate is because Android only shows a notification icon when the GPS sensor is accessed, while offering alternative ways of inferring location.
In 66.1% of applications' location requests, they directly queried the TelephonyManager, which can be used to determine location via cellular tower information.
In 33.3% of the cases, applications requested the SSIDs of nearby WiFi networks.
In the remaining 0.6% of cases, applica-tions accessed location information using one of three built-in location providers: GPS, network, or passive.
Applications accessed the GPS location provider only 6% of the time (which displayed a GPS notification).
In the other 94% of the time, 13% queried the network provider (i.e., approximate location based on nearby cellular towers and WiFi SSIDs) and 81% queried the passive location provider.
The passive location provider caches prior requests made to either the GPS or network providers.
Thus, across all requests for location data, the GPS notification icon appeared 0.04% of the time.While the alternatives to querying the GPS are less accurate, users are still surprised by their accuracy [17].
This suggests a serious violation of contextual integrity, since users likely have no idea their locations are being requested in the vast majority of cases.
Thus, runtime notifications for location tracking need to be improved [18].
Apart from these invisible location requests, we also observed applications reading stored SMS messages (125 times per user/day), reading browser history (5 times per user/day), and accessing the camera (once per user/day).
Though the use of these permissions does not necessarily lead to privacy violations, users have no contextual cues to understand that these requests are occurring.
Some permission requests occurred so frequently that a few applications (i.e., Facebook, Facebook Messenger, Google Location Reporting, Google Maps, Farm Heroes Saga) had to be rate limited in our log files (see Section 3.1.1), so that the logs would not fill up users' remaining storage or incur performance overhead.
Table 4 shows the complete list of application/permission combinations that exceeded the threshold.
For instance, the most frequent requests came from Facebook requesting ACCESS NETWORK STATE with an average interval of 213.88 ms (i.e., almost 5 times per second).
With the exception of Google's applications, all ratelimited applications made excessive requests for the connectivity state.
We hypothesize that once these applications lose connectivity, they continuously poll the system until it is regained.
Their use of the getActiveNetworkInfo() method results in permission checks and returns NetworkInfo objects, which allow them to determine connection state (e.g., connected, disconnected, etc.) and type (e.g., WiFi, Bluetooth, cellular, etc.).
Thus, these requests do not appear to be leaking sensitive information per se, but their frequency may have adverse effects on performance and battery life.
It is possible that using the ConnectivityManager's NetworkCallback method may be able to fulfill this need with far fewer permission checks.
Table 4: The application/permission combinations that needed to be rate limited during the study.
The last two columns show the fastest interval recorded and the average of all the intervals recorded before rate-limiting.
Felt et al. posited that while most permissions can be granted automatically in order to not habituate users to relatively benign risks, certain requests should require runtime consent [14].
They advocated using runtime dialogs before the following actions should proceed:1.
Reading location information (e.g., using conventional location APIs, scanning WiFi SSIDs, etc.
Table 5: The sensitive permission requests (per user/day) when requesting applications were visible/invisible to users.
"Data exposed" reflects the subset of permission-protected requests that resulted in sensitive data being accessed.ers may decide to only show runtime warnings to users when protected data is read or modified.
Thus, we attempted to quantify the frequency with which permission checks actually result in access to sensitive resources for each of these four categories.
Table 5 shows the number of requests seen per user/day under each of these four categories, separating the instances in which sensitive data was exposed from the total permission requests observed.
Unlike Section 4.1, we include "visible" permission requests (i.e., those occurring while the user was actively using the application or had other contextual information to indicate it was running).
We didn't observe any uses of NFC, READ CALL LOG, ADD VOICEMAIL, accessing WRITE SYNC SETTINGS or INTERNET while roaming in our dataset.Of the location permission checks, a majority were due to requests for location provider information (e.g., getBestProvider() returns the best location provider based on application requirements), or checking WiFi state (e.g., getWifiState() only reveals whether WiFi is enabled).
Only a portion of the requests actually exposed participants' locations (e.g., getLastKnownLocation() or getScanResults() exposed SSIDs of nearby WiFi networks).
Although a majority of requests for the READ SMS permission exposed content in the SMS store (e.g., Query() reads the contents of the SMS store), a considerable portion simply read information about the SMS store (e.g., renewMmsConnectivity() resets an applications' connection to the MMS store).
An exception to this is the use of SEND SMS, which resulted in the transmission of an SMS message every time the permission was requested.Regarding browser history, both accessing visited URLs (getAllVisitedUrls()) and reorganizing bookmark folders (addFolderToCurrent()) result in the same permission being checked.
However, the latter does not expose specific URLs to the invoking application.Our analysis of the API calls indicated that on average, only half of all permission checks granted applications access to sensitive data.
For instance, across both visible and invisible requests, 5,111 of the 11,598 (44.3%) permission checks involving the 12 permissions in Table 1 resulted in the exposure of sensitive data (Table 5).
While limiting runtime permission requests to only the cases in which protected resources are exposed will greatly decrease the number of user interruptions, the frequency with which these requests occur is still too great.
Prompting the user on the first request is also not appropriate (e.g., ` a la iOS and Android M), because our data show that in the vast majority of cases, the user has no contextual cues to understand when protected resources are being accessed.
Thus, a user may grant a request the first time an application asks, because it is appropriate in that instance, but then she may be surprised to find that the application continues to access that resource in other contexts (e.g., when the application is not actively used).
As a result, a more intelligent method is needed to determine when a given permission request is likely to be deemed appropriate by the user.
To identify when users might want to be prompted about permission requests, our exit survey focused on participants' reactions to the 12 permissions in Table 1, limiting the number of requests shown to each participant based on our reservoir sampling algorithm, which was designed to ask participants about a diverse set of permission/application combinations.
We collected participants' reactions to 673 permission requests (≈19/participant).
Of these, 423 included screenshots because participants were actively using their phones when the requests were made, whereas 250 permission requests were performed while device screens were off.
2 Of the former, 243 screenshots were taken while the requesting application was visible (Category 1 and 3 from Section 4.1), whereas 180 were taken while the application was invisible (Category 2 and 4 from Section 4.1).
In this section, we describe the situations in which requests defied users' expectations.
We present explanations for why participants wanted to block certain requests, the factors influencing those decisions, and how expectations changed when devices were not in use.
When viewing screenshots of what they were doing when an application requested a permission, 30 participants (80% of 36) stated that they would have preferred to block at least one request, whereas 6 stated a willingness to allow all requests, regardless of resource type or application.
Across the entire study, participants wanted to block 35% of these 423 permission requests.
When we asked participants to explain their rationales for these decisions, two main themes emerged: the request did notin their minds-pertain to application functionality or it involved information they were uncomfortable sharing.
When prompted for the reason behind blocking a permission request, 19 (53% of 36) participants did not believe it was necessary for the application to perform its task.
Of the 149 (35% of 423) requests that participants would have preferred to block, 79 (53%) were perceived as being irrelevant to the functionality of the application:• "It wasn't doing anything that needed my current location."
(P1) • "I don't understand why this app would do anything with SMS."
(P10)Accordingly, functionality was the most common reason for wanting a permission request to proceed.
Out of the 274 permissible requests, 195 (71% of 274) were perceived as necessary for the core functionality of the application, as noted by thirty-one (86% of 36) participants:• "Because it's a weather app and it needs to know where you are to give you weather information."
(P13) • "I think it needs to read the SMS to keep track of the chat conversation."
(P12)Beyond being necessary for core functionality, participants wanted 10% (27 of 274) of requests to proceed because they offered convenience; 90% of these requests were for location data, and the majority of those applications were published under the Weather, Social, and Travel & Local categories in the Google Play store:• "It selects the closest stop to me so I don't have to scroll through the whole list."
(P0) • "This app should read my current location.
I'd like for it to, so I won't have to manually enter in my zip code / area."
(P4)Thus, requests were allowed when they were expected: when participants rated the extent to which each request was expected on a 5-point Likert scale, allowable requests averaged 3.2, whereas blocked requests averaged 2.3 (lower is less expected).
Participants also wanted to deny permission requests that involved data that they considered sensitive, regardless of whether they believed the application actually needed the data to function.
Nineteen (53% of 36) participants noted privacy as a concern while blocking a request, and of the 149 requests that participants wanted to block, 49 (32% of 149) requests were blocked for this reason:• "SMS messages are quite personal."
(P14)• "It is part of a personal conversation."
(P11)• "Pictures could be very private and I wouldn't like for anybody to have access."
(P16)Conversely, 24 participants (66% of 36) wanted requests to proceed simply because they did not believe that the data involved was particularly sensitive; this reasoning accounted for 21% of the 274 allowable requests:• "I'm ok with my location being recorded, no concerns."
(P3) • "No personal info being shared."
(P29) Based on participants' responses to the 423 permission requests involving screenshots (i.e., requests occurring while they were actively using their phones), we quantitatively examined how various factors influenced their desire to block some of these requests.Effects of Identifying Permissions on Blocking: In the exit survey, we asked participants to guess the permission an application was requesting, based on the screenshot of what they were doing at the time.
The real answer was among four other incorrect answers.
Of the 149 cases where participants wanted to block permission requests, they were only able to correctly state what permission was being requested 24% of the time; whereas when wanting a request to proceed, they correctly identified the requested permission 44% (120 of 274) of the time.
However, Pearson's product-moment test on the average number of blocked requests per user and the average number of correct answers per user 3 did not yield a statistically significant correlation (r=−0.171, p<0.317).
Effects of Visibility on Expectations: We were particularly interested in exploring if permission requests originating from foreground applications (i.e., visible to the user) were more expected than ones from background applications.
Of the 243 visible permission requests that we asked about in our exit survey, participants correctly identified the requested permission 44% of the time, and their average rating on our expectation scale was 3.4.
On the other hand, participants correctly identified the resources accessed by background applications only 29% of the time (52 of 180), and their average rating on our expectation scale was 3.0.
A Wilcoxon Signed-Rank test with continuity correction revealed a statistically significant difference in participants' expectations between these two groups (V=441.5, p<0.001).
Effects of Visibility on Blocking: Participants wanted to block 71 (29% of 243) permission requests originating from applications running in the foreground, whereas this increased by almost 50% when the applications were in the background invisible to them (43% of 180).
We calculated the percentage of denials for each participant, for both visible and invisible requests.
A Wilcoxon Signed-Rank test with continuity correction revealed a statistically significant difference (V=58, p<0.001).
Effects of Privacy Preferences on Blocking: Participants completed the Privacy Concerns Scale (PCS) [10] and the Internet Users' Information Privacy Concerns (IUIPC) scale [31].
A Spearman's rank test yielded no statistically significant correlation between their privacy preferences and their desire to block permission requests (ρ = 0.156, p<0.364).
Effects of Expectations on Blocking: We examined whether participants' expectations surrounding requests correlated with their desire to block them.
For each participant, we calculated their average Likert scores for their expectations and the percentage of requests that they wanted to block.
Pearson's product-moment test showed a statistically significant correlation (r=−0.39, p<0.018).
The negative correlation shows that participants were more likely to deny unexpected requests.
In the second part of the exit survey, participants answered questions about 10 resource requests that occurred when the screen was off (not in use).
Overall, they were more likely to expect resource requests to occur when using their devices (µ = 3.26 versus µ = 2.66).
They also stated a willingness to block almost half of the permission requests (49.6% of 250) when not in use, compared to a third of the requests that occurred when using their phones (35.2% of 423).
However, neither of these differences was statistically significant.
Felt et al. posited that certain sensitive permissions (Table 1) should require runtime consent [14], but in Section 4.3 we showed that the frequencies with which applications are requesting these permissions make it impractical to prompt the user each time a request occurs.
Instead, the major mobile platforms have shifted towards a model of prompting the user the first time an application requests access to certain resources: iOS does this for a selected set of resources, such as location and contacts, and Android M does this for "dangerous" permissions.How many prompts would users see, if we added runtime prompts for the first use of these 12 permissions?
We analyzed a scheme where a runtime prompt is displayed at most once for each unique triplet of (application, permission, application visibility), assuming the screen is on.
With a na¨ıvena¨ıve scheme, our study data indicates our participants would have seen an average of 34 runtime prompts (ranging from 13 to 77, σ =11).
As a refinement, we propose that the user should be prompted only if sensitive data will be exposed (Section 4.3), reducing the average number of prompts to 29.
Of these 29 prompts, 21 (72%) are related to location.
Apple iOS already prompts users when an application accesses location for the first time, with no evidence of user habituation or annoyance.
Focusing on the remaining prompts, we see that our policy would introduce an average of 8 new prompts per user: about 5 for reading SMS, 1 for sending SMS, and 2 for reading browser history.
Our data covers only the first week of use, but as we only prompt on first use of a permission, we expect that the number of prompts would decline greatly in subsequent weeks, suggesting that this policy would likely not introduce significant risk of habituation or annoyance.
Thus, our results suggest adding runtime prompts for reading SMS, sending SMS, and reading browser history would be useful given their sensitivity and low frequency.Our data suggests that taking visibility into account is important.
If we ignore visibility and prompted only once for each pair of (application, permission), users would have no way to select a different policy for when the application is visible or not visible.
In contrast, "askon-first-use" for the triple (application, permission, visibility) gives users the option to vary their decision based on the visibility of the requesting application.
We evaluated these two policies by analyzing the exit survey data (limited to situations where the screen was on) for cases where the same user was asked twice in the survey about situations with the same (application, permission) pair or the same (application, permission, visibility) triplet, to see whether the user's first decision to block or not matched their subsequent decisions.
For the former pol-icy, we saw only 51.3% agreement; for the latter, agreement increased to 83.5%.
This suggests that the (application, permission, visibility) triplet captures many of the contextual factors that users care about, and thus it is reasonable to prompt only once per unique triplet.A complicating factor is that applications can also run even when the user is not actively using the phone.
In addition to the 29 prompts mentioned above, our data indicates applications would have triggered an average of 7 more prompts while the user was not actively using the phone: 6 for location and one for reading SMS.
It is not clear how to handle prompts when the user is not available to respond to the prompt: attribution might be helpful, but further research is needed.
We constructed several statistical models to examine whether users' desire to block certain permission requests could be predicted using the contextual data that we collected.
If such a relationship exists, a classifier could determine when to deny potentially unexpected permission requests without user intervention.
Conversely, the classifier could be used to only prompt the user about questionable data requests.
Thus, the response variable in our models is the user's choice of whether to block the given permission request.
Our predictive variables consisted of the information that might be available at runtime: permission type (with the restriction that the invoked function exposes data), requesting application, and visibility of that application.
We constructed several mixed effects binary logistic regression models to account for both inter-subject and intra-subject effects.
In our mixed effects models, permission types and the visibility of the requesting application were fixed effects, because all possible values for each variable existed in our data set.
Visibility had two values: visible (the user is interacting with the application or has other contextual cues to know that it is running) and invisible.
Permission types were categorized based on Table 5.
The application name and the participant ID were included as random effects, because our survey data did not have an exhaustive list of all possible applications a user could run, and the participant has a non-systematic effect on the data.
Table 6 shows two goodness-of-fit metrics: the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC).
Lower values for AIC and BIC represent better fit.
Table 6 shows the different parameters included in each model.
We found no evidence of interaction effects and therefore did not include them.
Visual inspection of residual plots of each model did not reveal obvious deviations from homoscedasticity or normality.
We initially included the phone's screen state as another variable.
However, we found that creating two separate models based on the screen state resulted in better fit than using a single model that accounted for screen state as a fixed effect.
When the screen was on, the best fit was a model including application visibility and application name, while controlling for subject effects.
Here, fit improved once permission type was removed from the model, which shows that the decision to block a permission request was based on contextual factors: users do not categorically deny permission requests based solely on the type of resource being accessed (i.e., they also account for their trust in the application, as well as whether they happened to be actively using it).
When the screen was off, however, the effect of permission type was relatively stronger.
The strong subject effect in both models indicates that these decisions vary from one user to the next.
As a result, any classifier developed to automatically decide whether to block a permission at runtime (or prompt the user) will need to be tailored to that particular user's needs.
Using these two models, we built two classifiers to make decisions about whether to block any of the sensitive permission requests listed in Table 5.
We used our exit survey data as ground truth, and used 5-fold cross-validation to evaluate model accuracy.We calculated the receiver operating characteristic (ROC) to capture the tradeoff between true-positive and false-positive rate.
The quality of the classifier can be quantified with a single value by calculating the area under its ROC curve (AUC) [23].
The closer the AUC gets to 1.0, the better the classifier is.
When screens were on, the AUC was 0.7, which is 40% better than the random baseline (0.5).
When screens were off, the AUC was 0.8, which is 60% better than a random baseline.
During the study, 80% of our participants deemed at least one permission request as inappropriate.
This violates Nissenbaum's notion of "privacy as contextual integrity" because applications were performing actions that defied users' expectations [33].
Felt et al. posited that users may be able to better understand why permission requests are needed if some of these requests are granted via runtime consent dialogs, rather than Android's previous installtime notification approach [14].
By granting permissions at runtime, users will have additional contextual information; based on what they were doing at the time that resources are requested, they may have a better idea of why those resources are being requested.We make two primary contributions that system designers can use to make more usable permissions systems.
We show that the visibility of the requesting application and the frequency at which requests occur are two important factors in designing a runtime consent platform.
Also, we show that "prompt-on-first-use" per triplet could be implemented for some sensitive permissions without risking user habituation or annoyance.Based on the frequency with which runtime permissions are requested (Section 4), it is infeasible to prompt users every time.
Doing so would overwhelm them and lead to habituation.
At the same time, drawing user attention to the situations in which users are likely to be concerned will lead to greater control and awareness.
Thus, the challenge is in acquiring their preferences by confronting them minimally and then automatically inferring when users are likely to find a permission request unexpected, and only prompting them in these cases.
Our data suggests that participants' desires to block particular permissions were heavily influenced by two main factors: their understanding of the relevance of a permission request to the functionality of the requesting application and their individual privacy concerns.Our models in Section 6.1 showed that individual characteristics greatly explain the variance between what different users deem appropriate, in terms of access to protected resources.
While responses to privacy scales failed to explain these differences, this was not a surprise, as the disconnect between stated privacy preferences and behaviors is well-documented (e.g., [1]).
This means that in order to accurately model user preferences, the system will need to learn what a specific user deems inappropriate over time.
Thus, a feedback loop is likely needed: when devices are "new," users will be required to provide more input surrounding permission requests, and then based on their responses, they will see fewer requests in the future.
Our data suggests that prompting once for each unique (application, permission, application visibility) triplet can serve as a practical mechanism in acquiring users' privacy preferences.Beyond individual subject characteristics (i.e., personal preferences), participants based their decisions to block certain permission requests on the specific applications making the requests and whether they had contextual cues to indicate that the applications were running (and therefore needed the data to function).
Future systems could take these factors into account when deciding whether or not to draw user attention to a particular request.
For example, when an application that a user is not actively using requests access to a protected resource, she should be shown a runtime prompt.
Our data indicates that, if the user decides to grant a request in this situation, then with probability 0.84 the same decision will hold in future situations where she is actively using that same application, and therefore a subsequent prompt may not be needed.
At a minimum, platforms need to treat permission requests from background applications differently than those originating from foreground applications.
Similarly, applications running in the background should use passive indicators to communicate when they are accessing particular resources.
Platforms can also be designed to make decisions about whether or not access to resources should be granted based on whether contextual cues are present, or at its most basic, whether the device screen is even on.Finally, we built our models and analyzed our data within the framework of what resources our participants believed were necessary for applications to correctly function.
Obviously, their perceptions may have been incorrect: if they better understood why a particular resource was necessary, they may have been more permissive.
Thus, it is incumbent on developers to adequately communicate why particular resources are needed, as this impacts user notions of contextual integrity.
Yet, no mechanisms in Android exist for developers to do this as part of the permission-granting process.
For example, one could imagine requiring metadata to be provided that explains how each requested resource will be used, and then automatically integrating this information into permission requests.
Tan et al. examined a similar feature on iOS that allows developers to include free-form text in runtime permission dialogs and observed that users were more likely to grant requests that included this text [41].
Thus, we believe that including succinct explanations in these requests would help preserve contextual integrity by promoting greater transparency.In conclusion, we believe this study was instructive in showing the circumstances in which Android permission requests are made under real-world usage.
While prior work has already identified some limitations of deployed mobile permissions systems, we believe our study can benefit system designers by demonstrating several ways in which contextual integrity can be improved, thereby empowering users to make better security decisions.
This work was supported by NSF grant CNS-1318680, by Intel through the ISTC for Secure Computing, and by the AFOSR under MURI award FA9550-12-1-0040.
sThe following graph shows the distribution of requests throughout a given day averaged across the data set.
The following graph shows the distribution of requests throughout a given day averaged across the data set.
