The intelligent traffic light control is critical for an efficient transportation system.
While existing traffic lights are mostly operated by hand-crafted rules, an intelligent traffic light control system should be dynamically adjusted to real-time traffic.
There is an emerging trend of using deep reinforcement learning technique for traffic light control and recent studies have shown promising results.
However, existing studies have not yet tested the methods on the real-world traffic data and they only focus on studying the rewards without interpreting the policies.
In this paper, we propose a more effective deep reinforcement learning model for traffic light control.
We test our method on a large-scale real traffic dataset obtained from surveillance cameras.
We also show some interesting case studies of policies learned from the real data.
In this section, we firstly introduce conventional methods for traffic light control, then introduce methods using reinforcement learning.
Early traffic light control methods can be roughly classified into two groups.
The first is pre-timed signal control [6,18,23], where a fixed time is determined for all green phases according to historical traffic demand, without considering possible fluctuations in traffic demand.
The second is vehicle-actuated control methods [5,20] where the real-time traffic information is used.
Vehicle-actuated methods are suitable for the situations with relatively high traffic randomness.
However, this method largely depends on the handcraft rules for current traffic condition, without taking into account future situation.
Therefore, it cannot reach the global optimal.
Recently, due to the incapability of dealing with dynamic multidirection traffic in previous methods, more works try to use reinforcement learning algorithms to solve the traffic light control problem [13,17,24].
Typically, these algorithms take the traffic on the road as state, and the operation on light as action.
These methods usually show better performance compared with fixed-time and traffic-responsive control methods.
Methods in [1,2,4,8,24] designed the state as discrete values like the location of vehicles or number of waited cars.
However, the discrete state-action pair value matrix requires huge storage space, which keeps these methods from being used in large state space problems.To solve the in-managablely large state space of previous methods, recent studies [15,22] propose to apply Deep Q-learning methods using continuous state representations.
These studies learn a Q-function (e.g. a deep neural network) to map state and action to reward.
These works vary in the state representation including hand craft features (e.g., queue length [15,17], average delay [10,22]) and image features [9,16,22]) They are also different in reward design, including average delay [3,22],the average travel time [16,22], and queue length [15].
However, all these methods assume relatively static traffic environments, and hence far from the real case.
Further, they only focus on rewards and overlook the adaptability of the algorithms to the real traffic.
Therefore, they cannot interpret why the learned light signal changes corresponding to the traffic.
In this paper, we try to test the algorithms in a more realistic traffic setting, and add more interpretation other than reward.
In our problem, we have the environment E as an intersection of two roads (and the traffic on this intersection).
There is an intelligent traffic light agent G. To make the notation simpler, we use "N", "S", "W", "E" to represent north, south, west, and east respectively, and use "Red" and "Green" to represent red light and green light correspondingly.
A setting of the traffic light is defined as a phase (e.g., green light on the west-east direction which can be simplified as Green-WE).
When a light changes from green to red, there is a 3 second yellow light, while the other directions still keep red.
So one green light and the subsequent yellow light can be represented together by "Green".
To simplify the problem, we assume there are only two phases of the traffic light, i.e., 1) Green-WE, and 2) Red-WE.
Due to the limitation of real-world setting, the traffic light can only change in a specific order (i.e., 1 -> 2 -> 1 -> 2 -> ...).
Given the state s (describing the positions and speed of the traffic near this intersection), the goal of the agent G is to give the optimal action a (i.e., whether to change the light to the next phase), so that the reward r (i.e., the smoothness of traffic) can be maximized.
Delay of lane i.M ∈ R N ×N Image representation of vehicles' position P c Current phase P n Next phase C ∈ {0, 1}Light switches (1) or not (0) N Number of vehicles passed the intersection after the action.
T Travel time in system of all vehicles that passed the intersection during ∆t. Traffic light control has attracted a lot of attention in recent years due to its essential role in adjusting traffic.
Current methods generally have two categories, conventional methods, and deep reinforcement learning based methods.
Conventional methods usually rely on previous knowledge to set fixed time for each light phase or set changing rules.
These rules are prone to dynamically changing traffic.
Reinforcement learning methods usually take the traffic condition (e.g., queue length of waiting cars and updated waiting time) as state, and try to make actions that can improve the traffic condition based on the current state.
However, the current methods do not consider the complex situations in real case, and hence may lead to stuck in one single kind of action.
This will lead to inferior traffic adjusting performance under complex traffic situation.In this section, we propose a deep reinforcement traffic light agent to solve this problem.
We will first introduce the model framework in Section 4.1.
Then, we show the design of agent in Section 4.2.
We further describe the network structure in Section 4.3.
In addition, we describe the memory palace in Section 4.4.
Note that, although our model is designed for a four way intersection with two phases, it is not difficult to extend it to other types of intersections or to multiple phases scenarios.
Online Our model is composed of offline part and online part (as shown in Figure 4).
We extract five kinds of features describing the traffic conditions as state (detailed in Section 4.2), and use reward to describe how much the action has improved the traffic (detailed in Section 4.2).
In offline stage, we set a fixed timetable for the lights, and let traffic go through the system to collect data samples.
After training with the samples logged in this stage, the model will be put into the online part.
In online stage, at every time interval ∆t, the traffic light agent will observe the state s from the environment and take action a (i.e., whether to change light signal to the next phase) according to ϵ-greedy strategy combining exploration (i.e., random action with probability ϵ) and exploitation (i.e., taking the action with maximum estimated reward).
After that, the agent G will observe the environment and get the reward r from it.
Then, the tuple (s, a, r) will be stored into memory.
After several timestamps (e.g., t 2 in Figure 4), agent G will update the network according to the logs in the memory.
First, we introduce the state, action and reward representation.
• State.
Our state is defined for one intersection.
D i = 1 − lane speed speed limit(1)(3) Sum of updated waiting time W over all approaching lanes.
This equals to the sum of W over all vehicles on approaching lanes.
The updated waiting time W for vehicle j at time t is defined in Equation 2.
Note that the updated waiting time of a vehicle is reset to 0 every time it moves.
For example, if a vehicle's speed is 0.01m/s from 0s to 15s, 5m/s from 15s to 30s, and 0.01m/s from 30s to 60s, W j is 15 seconds, 0 seconds and 30 seconds when t =15s, 30s and 60s relatively.W j (t) = W j (t − 1) + 1 vehicle speed < 0.1 0 vehicle speed ≥ 0.1(2)(4) Indicator of light switches C, where C = 0 for keeping the current phase, and C = 1 for changing the current phase.
(5) Total number of vehicles N that passed the intersection during time interval ∆t after the last action a. (6) Total travel time of vehicles T that passed the intersection during time interval ∆t after the last action a, defined as the total time (in minutes) that vehicles spent on approaching lanes.
Hence, given the current state s of the traffic condition, the mission of the agent G is to find the action a (change or keep current phase) that may lead to the maximum reward r in the long run, following the Bellman Equation (Equation 4) [21].
In this situation, the action value function q for time t is the summation of the reward of the next timestamp t + 1 and the maximum potential future reward.
Through this conjecture of future, the agent can select action that is more suitable for long-run reward.Reward = w 1 * i∈I L i + w 2 * i∈I D i + w 3 * i∈I W i + w 4 * C + w 5 * N + w 6 * T.(3)q(s t , a, t) = r a,t +1 + γ max q(s a,t +1 , a ′ , t + 1) In order to estimate the reward based on the state, and action, the agent needs to learn a Deep Q-Network Q(s, a).
In the real-world scenario, traffic is very complex and contain many different cases need to be considered separately.
We will illustrate this in Example 4.1.
Example 4.1.
We still assume a simple intersection with twophase light transition here: 1) Green-WE, and 2) Red-WE.
The decision process of whether to change the traffic light consists of two steps.
The first step is the mapping from traffic condition (e.g., how many cars are waiting, how long has each car been waiting) to a partial reward.
An example of this mapping could be r = −0.5 × L − 0.7 × W.
This is shared by different phases, no matter which lane the green light is on.
Then, to determine the action, the agent should watch on the traffic in different lanes during different phases.
For instance, as is shown in Figure 3 (a), when the red light is on the NS direction, more waiting traffic (i.e., lower reward in the first step) on the NS direction will make the light tend to change (because by changing the light on this lane from red to green, more cars on this lane can pass through this intersection), while more waiting traffic (i.e., lower reward in the first step) on the WE direction will make the light tend to keep.
When the red light is on the In previous studies, due to the simplified design of the model for approximating Q-function under complex traffic condition, agents are having difficulties in distinguishing the decision process for different phases.
Therefore, we hereby propose a network structure that can explicitly consider the different cases explicitly.
We call this special sub-structure "Phase Gate".
Our whole network structure can be shown as in Figure 5.
The image features are extracted from the observations of the traffic condition and fed into two convolutional layers.
The output of these layers are concatenated with the four explicitly mined features, queue length L, updated waiting time W, phase P and number of total vehicles V.
The concatenated features are then fed into fully-connected layers to learn the mapping from traffic conditions to potential rewards.
Then, for each phase, we design a separate learning process of mapping from rewards to the value of making decisions Q(s, a).
These separate processes are selected through a gate controlled by the phase.
As shown in Figure 5, when phase P = 0, the left branch will be activated, while when phase P = 1, the right branch will be activated.
This will distinguish the decision process for different phases, prevent the decision from favoring certain action, and enhance the fitting ability of the network.
Periodically, the agent will take samples from the memory and use them to update the network.
This memory is maintained by adding the new data samples in and removing the old samples occasionally.
This technique is noted as experience replay [19] and has been widely used in reinforcement learning models.However, in the real traffic setting, traffic on different lanes can be really imbalanced.
As previous methods [9,10,15,22] store all the state-action-reward training samples in one memory, this memory will be dominated by the phases and actions that appear most frequently in imbalanced settings.
Then, the agent will be learned to estimate the reward for these frequent phase-action combinations well, but ignore other less frequent phase-action combinations.
This will cause the learned agent to make bad decisions on the infrequent phase-action combinations.
Therefore, when traffic on different lanes are dramatically different, these imbalanced samples will lead to inferior performance on less frequent situation.Inspired by Memory Palace theory [11,14] in cognitive psychology, we can solve this imbalance by using different memory palaces for different phase-action combinations.
As shown in Figure 6, training samples for different phase-action combinations are stored into different memory palaces.
Then same number of samples will be selected from different palaces.
These balanced samples will prevent different phase-action combinations from interfering each other's training process, and hence, improve the fitting capability of the network to predict the reward accurately.
In this section, we conduct experiments using both synthetic and real-world traffic data.
We show a comprehensive quantitative evaluation by comparing with other methods and also show some interesting case studies 1 .
The experiments are conducted on a simulation platform SUMO (Simulation of Urban MObility) 2 .
SUMO provides flexible APIs for road network design, traffic volume simulation and traffic light control.
Specifically, SUMO can control the traffic moving according to the given policy of traffic light (obtained by the traffic light agent).
The environment for the experiments on synthetic data is a fourway intersection as Figure 2.
The intersection is connected with four road segments of 150-meters long, where each road have three incoming and three outgoing lanes.
The traffic light in this part of experiment contains two phases: (1) Green-WE (green light on WE with red light on SN ), (2) Red-WE (red light on WE with green light on SN ).
Note that when a green light is on one direction, there is a red light on the other direction.
Also, a green light is followed by a 3-second yellow light before it turns to red light.
Although this is a simplification of the real world scenario, the research of more types of intersections (e.g., three-way intersection), and more complex light phasing (e.g., with left-turn phasing) can be further conducted in similar way.
The parameter setting and reward coefficients for our methods are shown in Table 2 and Table 3 respectively.
We found out that the action time interval ∆t has minimal influence on performance of our model as long as ∆t is between 5 seconds and 25 seconds.
We evaluate the performance of different methods using the following measures: In summary, a higher reward indicates a better performance of the method, and a smaller queue length, delay and duration indicates the traffic is less jammed.
To evaluate the effectiveness of our model, we compare our model with the following baseline methods, and tune the parameter for all methods.
We then report their best performance.
• Fixed-time Control (FT ).
Fixed-time control method use a pre-determined cycle and phase time plan [18] and is widely used when the traffic flow is steady.
• Self-Organizing Traffic Light Control (SOTL) [5].
This method controls the traffic light according to the current traffic state, including the eclipsed time and the number of vehicles waiting at the red light.
Specifically, the traffic light will change when the number of waiting cars is above a hand-tuned threshold.
• Deep Reinforcement Learning for Traffic Light Control (DRL).
Proposed in [22], this method applies DQN framework to select optimal light configurations for traffic intersections.
Specifically, it solely relies on the original traffic information as an image.
In addition to the baseline methods, we also consider several variations of our model.
• IntelliLight (Base).
Using the same network structure and reward function defined as in Section 4.2 and 4.3.
This method is without Memory Palace and Phase Gate.
• IntelliLight (Base+MP).
By adding Memory Palace in psychology to IntelliLight-Base, we store the samples from different phase and time in separate memories.
• IntelliLight (Base+MP+PG).
This is the model adding two techniques (Memory Palace and Phase Gate).
5.5.1 Synthetic data.
In the first part of our experiment, synthetic data is used with four traffic flow settings: simple changing traffic (configuration 1), equally steady traffic (configuration 2), unequally steady traffic (configuration 3) and complex traffic (configuration 4) which is a combination of previous three configurations.
As is shown in Table 4, the arriving of vehicles are generated by Poisson distribution with certain arrival rates.
5.5.2 Real-world data.
The real-world dataset is collected by 1,704 surveillance cameras in Jinan, China over the time period from 08/01/2016 to 08/31/2016.
The locations of these cameras are shown in Figure 7.
Gathered every second by the cameras facing towards vehicles near intersections, each record in the dataset consists of time, camera ID and the information about vehicles.
By analyzing these records with camera locations, the trajectories of vehicles are recorded when they pass through road intersections.
The dataset covers 935 locations, where 43 of them are four-way intersections.
We use the number of vehicles passing through 24 intersections as traffic volume for experiments since only these intersections have consecutive data.
Then we feed this real-world traffic setting into SUMO as online experiments.
It can be seen from Table 5 that traffic flow on different roads are dynamically changing in the real world.
Table 6, 7, 8 and 9 we can see that our method performs better than all other baseline methods in configurations 1, 2, 3 and 4.
Although some baselines perform well on certain setting, they perform badly in other configurations (e.g., SOTL achieves good rewards under configuration 1, almost the same as our method in 3 digit floats.
This is because our method has learned to keep the light until 36000 s and switch the light after that, and SOTL is also designed to behave similarly.
Hence, these two methods perform very similar).
On the contrary, our method IntelliLight shows better performance under different configurations.
Comparison with variants of our proposed method.
Table 6, 7, 8 and 9 show the performance of variants of our proposed method.
First, we can see that adding Memory Palace helps achieve higher reward under configuration 3 and 4, although it does not boost the reward under configuration 1 and 2.
This is because for the simple case (configuration 1 and 2), the phase is relatively steady for a long time (because the traffic only comes from one direction or keeps not changing in a long time).
Therefore, the memory palace does not help in building a better model for predicting the reward.
Further adding Phase Gate also reduces the queue length in most cases and achieves highest reward, demonstrating the effectiveness of these two techniques.
To understand what our method have learned w.r.t. dynamic traffic conditions, we show the percentage of duration for phase Green-WE (i.e., green light on WE direction with red light on SN direction), along with the ratio of traffic flow on WE over total traffic flow from all directions.
With the changing of traffic, an ideal traffic light control method would be able to adjust its phase duration to traffic flows and get high reward.
For example, as traffic changes from direction WE to SN , the traffic light agent is expected to adjust its phase duration Table 10.
Our method IntelliLight achieves the best reward, queue length, delay and duration over all the compared methods, with a relative improvement of 32%, 38%, 19% and 22% correspondingly over the best baseline method.
In addition, our method has a relatively steady performance over multiple intersections (small standard deviation).
Observations with respect to real traffic.
In this section, we make observations on the policies we learned from the real data.
We analyze the learned traffic light policy for the intersection of Jingliu Road (WE direction) and Erhuanxi Auxiliary Road (SN direction) under different scenarios: peak hours vs. non-peak hours, weekdays vs. weekends, and major arterial vs. minor arterial.1.
Peak hour vs. Non-peak hour.
Figure 9 (a) shows the average traffic flow from both directions (WE and SN ) on a Monday.
On this day, there is more traffic on WE direction than SN for most of the time, during which an ideal traffic light control method is expected to give longer time for WE direction.
It can be seen from Figure 9 (c) that, the ratio of the time duration for phase Green-WE (i.e., green light on WE, while red light on SN ) is usually larger than 0.5, which means for most of the time, our method gives longer time for WE.
And during peak hours (around 7:00, 9:30 and 18:00), the policies learned from our method also give longer time for green light on WE than non-peak hours.
In early morning, the vehicle arrival rates on SN are larger than the rates on WE, and our method automatically gives longer time to SN .
This shows our method can intelligently adjust to different traffic conditions.2.
Weekday vs. Weekend.
Unlike weekdays, weekend shows different patterns about traffic condition and traffic light control policies.
Our policy gives less green light on WE (more green light on SN ) during weekend daytime than it gives on weekday.
This is because there is more traffic on SN than on WE during weekend daytime in Figure 9 (b), while during weekday traffic on SN is less than on WE.
Besides, by comparing Figure 9 (a) with Figure 9 (b), we can see that the traffic of WE and SN during late night time on Monday is similar, making the ratio of duration Green-We close to 0.5.
3.
Major arterial vs. Minor arterial.
Major arterials are roads that have higher traffic volume within a period, and are expected to have a longer green light time.
Without prior knowledge about major arterial, learned traffic light control policy using our method prefer giving the major arterial green light (including keeping the green light already on major arterial, and tend to switching red light to green light for major arterial).
Specifically, we look into three periods of time (3:00, 12:00 and 23:30) of August 1st.
From Figure 9 (a), we can tell that the road on WE direction is the main road, since traffic on WE is usually heavier than traffic on SN .
As is shown in Figure 10, the dotted lines indicates the number of arriving cars for every second on two different directions.
Along with the arrival rate, we also plot the change of phases (dashed area).
It can be seen from Figure 10 (a) that: 1) the overall time period of phase Red-WE is longer than Green-WE, which is compatible with traffic volume at this time.
2) although the traffic volume of SN is larger than WE, the traffic light change from Green-WE to Red-WE is usually not triggered by waiting cars on SN direction.
On the contrary, in Figure 10 (b) and Figure 10 (c), the change from Green-WE to Red-WE is usually triggered by waiting cars on SN direction.
This is mainly because the road on WE is the main road during these time periods, and the traffic light tends to favor phase Green-WE.
In this paper, we address the traffic light control problem using a well-designed reinforcement learning approach.
We conduct extensive experiments using both synthetic and real world experiments and demonstrate the superior performance of our proposed method over state-of-the-art methods.
In addition, we show in-depth case studies and observations to understand how the agent adjust to the changing traffic, as a complement to quantitative measure on rewards.
These in-depth case studies can help generate traffic rules for real world application.We also acknowledge the limitations of our current approach and would like to point out several important future directions to make the method more applicable to real world.
First, we can extend the two-phase traffic light to multi-phase traffic light, which will involve more complicated but more realistic state transition.
Second, our paper addresses a simplified one intersection case, whereas the real world road network is much more complicated than this.
Although some studies have tried to solve the multi-intersection problem by using multiple reinforcement learning agents, they do not explicitly consider the interactions between different intersections (i.e., how can the phase of one intersection affect the state of nearby intersections) and they are still limited to small number of intersections.
Lastly, our approach is still tested on a simulation framework and thus the feedback is simulated.
Ultimately, a field study should be conducted to learn the real-world feedback and to validate the proposed reinforcement learning approach.
The work was supported in part by NSF awards #1544455, #1652525, #1618448, and #1639150.
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.
