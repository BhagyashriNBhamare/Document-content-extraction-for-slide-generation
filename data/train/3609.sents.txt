Approximate nearest neighbor (ANN) algorithms are the foundation for many applications on mobile devices.
Real-time incremental learning with ANN on mobile devices is emerging.
However, incremental learning with current ANN algorithms on mobile devices is hard, because data is dynamically and incrementally generated and as a result, it is difficult to reach high timing and recall requirements on indexing and search.
Meeting the high timing requirements is critical on mobile devices because of the requirement of short user response time and because battery lifetime is limited.
We introduce an indexing and search system for graph-based ANN on mobile devices called RIANN.
By constructing ANN with dynamic ANN construction properties, RI-ANN enables high flexibility for ANN construction to meet the strict timing and recall requirements in incremental learning.
To select an optimal ANN construction property, RIANN incorporates a statistical prediction model.
RIANN further offers a novel analytical performance model to avoid runtime overhead and interaction with the device.
In our experiments, RIANN significantly outperforms the state-of-the-art ANN (2.42× speedup) on Samsung S9 mobile phone without compromising search time or recall.
Also, for incrementally indexing 100 batches of data, the state-of-the-art ANN satisfies 55.33% batches on average while RIANN can satisfy 96.67% with minimum impact on recall.
Approximate nearest neighbor (ANN) is an essential algorithm for many applications, e.g., recommendation systems, data mining and information retrieval [2,4,7,10,[15][16][17] on mobile devices.
For example, applications on mobile devices often provide recommendation functionalities to help users quickly identify interesting content (e.g., videos from YouTube [6], images from Flickr [14], or content from Taobao [9]).
To meet user requirements, it is important to incrementally construct ANN on mobile devices in real-time.
For example, it is essential to recommend to users new content that is of interest while the content is still fresh, as there is a clear tendency for users to prefer newer content.
This necessitates real-time incremental learning for ANN on mobile devices.However, real-time incremental learning for ANN on mobile devices imposes two challenges.
First, current ANN models cannot meet the real-time requirement of high recall for incremental learning due to static graph construction.
Specifically, with different size of batches in incremental learning, current ANN algorithms either index batches of data with high recall without meeting the real-time requirement or index data in real-time with low recall.Second, current ANN algorithms perform end-to-end indexing hence indexing time, query time and recall are unknown to users prior to or during ANN indexing, while these results are required to reach high recall in real-time incremental learning.To address the above challenges, we propose RIANN, a system to enabling real-time ANN incremental learning on mobile devices.
To achieve our goals, we propose a dynamic ANN indexing structure based on HNSW [13].
With the dynamic ANN graph construction, we can target different indexing times, query times and recall on the fly.
Next, we propose a statistical performance model to guide dynamic ANN construction over millions of possible properties.
We further propose an analytical performance model to avoid interaction with mobile devices and runtime overhead.
Dynamic ANN graph construction.
Currently, most graphbased ANN algorithms work in the following manner: during graph construction, the algorithms build a graph G = (P, E) based on the geometric properties of points in dataset P with connecting edges E between the points.
At search time, for a query point p ∈ P, ANN search employs a natural greedy traversal on G. Starting at some designated point d ∈ P, the algorithms traverse the graph to get progressively closer to p.
The state-of-the-art ANN algorithm HNSW exploits the above procedure with building a multi-layer structure consisting of hierarchical set of proximity graphs (layers) for nested subsets of the stored elements.However, since current graph-based ANN algorithms including HNSW are designed using static graph construction properties, there is little flexibility in controlling graph construction properties for real-time ANN incremental learning.To address this problem, we propose RIANN to construct graphs in a dynamic manner.
The dynamic graph construction depends on user requirements and a batch size of data Optimal Properties Figure 1: Overview of RIANN.points.
To achieve this goal, we build dynamic construction graph properties (e.g., out-degree edges of each point and candidates to build those edges).
The advantage of dynamic group construction properties is that we can meet the real-time requirement while maintaining high recall.
Domain-specific statistical prediction model.
The traditional approach to obtain the optimal indexing properties is to examine different indexing properties [3,9,11,13].
However, to obtain the optimal indexing properties, this approach 1) requires an excessive amount of time (days and even weeks) [3] to obtain the optimum and 2) requests the exact indexing data size which is impractical in ANN incremental learning.
We propose a statistical prediction model to solve the problem.
Figure 1 presents the overview of our design.
The statistical prediction model is to estimate the recall and indexing or query time of each construction property for indexing a batch of data.
The model is based on gradient boosted trees [8](GBTs) with simulated annealing [12].
We use XGBoost [5] as the GBTs model for training and implement a light-weighted XGBoost inference engine for mobile devices.
Analytical performance model.
Though the prediction model is promising to predict ANN recall, the model has two issues to predict indexing/query time: 1) it interacts with mobile devices frequently to collect training data and 2) it incurs nonnegligible runtime overhead.To address those problems, we propose an analytical performance model to predict indexing/query time at runtime.
Equation 1 is used to estimate the time of querying data point p ∈ P in one layer, where P refers to the set of points in one batch.
The metric is defined as:T lyr (cand, deg, N) = T d * h(cand, D avg (deg, N)) * D avg (deg, N) (1)Where cand represents candidate points, deg is the outdegree, N is the set of all points in one layer and T d represents the time to calculate the distance.
h (cand, D avg (deg, N)) is a function to obtain the average number of hops from the entry point to the target point.
The function can be formulated with small sample profiling offline.D avg (deg, N) = 1 |N|+1 (∑ |N| i=1 N i + deg)calculates the average out-degree after inserting p.With the number of candidates and out-degree of p, the query time T qry and indexing time T idx are defined as follows: T qry (cand q , deg)= 񮽙 max ∑ 񮽙=2 T lyr (1, deg, N 񮽙 ) + T lyr (cand q , deg * 2, N 1 ) (2)T idx (cand i , deg)= 񮽙 max ∑ 񮽙=񮽙 p T lyr (1, deg, N 񮽙 ) + 񮽙 p ∑ 񮽙=2 (T lyr (cand i , deg, N 񮽙 ) + f (deg)) + T lyr (cand i , dre * 2, N 1 ) + f (dre) (3)Where 񮽙 max is the maximum number of layers, 񮽙 p represents the indexing layer for p and cand q and cand i represent the number of candidates for query and indexing.
The time of querying p from 񮽙 max to 񮽙 can be calculated by ∑ 񮽙 max 񮽙 T lyr (cand, deg, N 񮽙 ).
The time of updating out-degree of p is calculated by f (deg) that depends on the implementation and linear to the number of out-degree.
Comparison of RIANN and HNSW.
We compare RIANN with HNSW which is the state-of-the-art ANN algorithm [3].
We employ SIFT [1] dataset on a Samsung S9 with Android 9.
We use the graph construction properties listed in the paper [13] denoted as default HNSW.
The optimal HNSW represents hypothetical results in which 1) users know the data size of ANN indexing which is impractical; 2) users spend a large amount of time (days and even weeks) to obtain the optimal HNSW.
We experiment different batch size (10, 100 and 1000) in batch increments of 10 for incremental learning.In Figure 2, we observe that RIANN shows significant performance improvement (2.42 times speedup on average) than the default HNSW and 8.67% performance less than the optimal HNSW while RIANN maintains the same recall and query time compared to the optimal and default HNSW.
The runtime overhead of RIANN is included in Figure 2.
Evaluation with User Requirement.
We incrementally index 100 batches and the batch size starts from 10 to 1000.
In Figure 2, we observe that 55.33% batches are satisfied using default HNSW while RIANN can satisfy 96.67% with only 2.43% loss in recall.
We present RIANN, a real-time graph-based ANN indexing and search system.
It constructs graphs in a dynamic manner with a statistical prediction model and an analytical performance model to incrementally index and search data in realtime on mobile devices.
RIANN significantly outperforms the state-of-the-art ANN (2.42× speedup) without compromising query time or recall in incremental learning.
Also, for incrementally indexing 100 batches of data, the state-of-the-art ANN satisfies 55.33% batches on average while RIANN can satisfy 96.67% with compromising 2.43% recall.
