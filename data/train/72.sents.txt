We consider the Correlation Clustering problem introduced in [2].
Given a graph G = (V, E) where each edge is labeled either "+" (similar) or "−" (different), we want to cluster the nodes so that the + edges lie within the clusters and the − edges lie between clusters.
Specifically, we want to maximize agreements-the number of + edges within clusters and − edges between clusters.
This problem is NP-Hard [2].
We give a 0.7666-approximation algorithm for maximizing agreements on any graph even when the edges have non-negative weights (along with labels) and we want to maximize the weight of agreements.
These were posed as open problems in [2].
Previously the only results known were a trivial 0.5-approximation for arbitrary edge weighted graphs, and a PTAS with unit edge weights when |E| = Ω(|V | 2).
Somewhat surprisingly, our algorithm always produces a clustering with at most 6 clusters.
As a corollary we get a 0.7666-approximation algorithm for the k-clustering variant of the problem where we may create at most k clusters.
A major component of this algorithm is a simple, easy-to-analyze algorithm that by itself achieves an approximation ratio of 0.75, opening at most 4 clusters.
We consider a somewhat more general version of correlation clustering to maximize agreements.
Each edge e has two weights w in (e), w out (e) ≥ 0.
An edge e contributes w in (e) to the total agreement weight if it lies within a cluster and w out (e) otherwise.
The problem is to find a clustering that maximizes 񮽙 e within cluster w in (e) + 񮽙 e not in cluster w out (e).
The weights w in (e), w out (e) can be viewed as confidence estimates of whether e should be labeled + or − respectively, thus giving a soft labeling.
The correlation clustering problem considered in [2] is a special case obtained by setting w in (e) = 1 if e is labeled + and 0 otherwise, w out (e) = 1 − w in (e).
Let e i ∈ R n be the vector with 1 in the i th coordinate and 0s everywhere else.We can formulate the problem as the following mathematical program: maximize 񮽙񮽙e=(u,v) 񮽙 w in (e)(x u · x v ) + w out (e)(1 − x u · x v ) 񮽙 : x v ∈ {e 1 , . . . , e n } for every v ∈ V 񮽙.
Vector e i represents a possible cluster i. For any clustering, if we set x v = e i for every vertex v assigned to cluster i, i = 1, . . . , k, the objective function value becomes the weight of agreements in the clustering.
We relax the constraints x v ∈ {e 1 , . . . , e n } to get a semidefinite program.max 񮽙 e=(u,v) 񮽙 w in (e)(x u ·x v )+w out (e)(1 − x u ·x v ) 񮽙 (SP) s.t. x v · x v = 1 for all v x u · x v ≥ 0 for all u, v, u 񮽙 = v (1.1)Our formulation resembles the MAX k-CUT relaxation in [6] but they relax a mathematical program involvingk vectors {a i } s.t. a i · a i = 1, a i · a j = −1 k−1 for i 񮽙 = j.
We solve (SP) and round the optimal solution.
We consider two rounding procedures.
Due to space limitations we only describe one of these which by itself gives an approximation ratio of 0.75, and sketch the improvements.We extend the Goemans-Williamson rounding for MAX CUT by choosing multiple hyperplanes.
Let {x v ∈ R n } be the optimal solution to (SP).
While rounding we need to ensure that both, the probability that edge e lies inside a cluster, and the probability that e lies between clusters, are comparable to the coefficients of w in (e) and w out (e) respectively in the objective function.
Choosing too many random hyperplanes rapidly decreases the probability of the former, and with too few hyperplanes, e.g., 1, the probability of the latter decreases to 0.5 times the coefficient of w out (e).
We choose 2 hyperplanes passing through the origin independently at random with normals distributed uniformly in the unit sphere.
Let q 1 , q 2 be the normals to the hyperplanes.
These partition the vertices into 4 sets, some possibly empty, based onx v · q i .
Let R s1,s2 = {v : (−1) si x v · q i ≥ 0, i = 1, 2}where s i ∈ {0, 1}.
Each such non-empty set defines a cluster.Analysis.
Let p in (θ), p out (θ) = 1 − p in (θ) denote the probabilities that nodes u and v with x u · x v = cos θ lie in the same cluster or different clusters respectively.Lemma 2.1.
p in (θ) = (1 − θ/π) 2 .
Lemma 2.2.
For any θ ∈ 񮽙 0, π 2 ], p in (θ) ≥ 0.75 cos θ and p out (θ) ≥ 0.75(1 − cos θ).
Proof.
Let f (θ) = pin(θ) cos θ and g(θ) = pout(θ) (1−cos θ) .
f (θ) is minimized at the unique point ϑ ∈ 񮽙 0, π 2 񮽙 s.t. tan θ = 2 π−θ .
So ϑ < 0.68288 and f (θ) ≥ (1−ϑ/π) 2 cos ϑ = 2(π−ϑ) π 2 sin ϑ > 0.7895 for θ ∈ 񮽙 0, π 2 񮽙 .
dg dθ = g(θ) 񮽙 1 θ − 1 2π−θ − cot(θ/2) 񮽙 ≤ 0 for θ ∈ 񮽙 0, π 2 񮽙 since cos θ ≥ 1 − θ 2 2 , sin θ ≤ θ for θ ∈ 񮽙 0, π 2 񮽙 .
So, g(θ) ≥ g( π 2 ) = 0.75 for θ ∈ 񮽙 0, π 2 .
Theorem 2.1.
The above rounding procedure delivers a solution of expected value at least 0.75 · OPT .
Proof.
Let C be the clustering obtained by rounding.
Let X e be the contribution of edge e = (u, v) to C and θ = arccos(x u · x v ).
E 񮽙 X e 񮽙 = w in (e)p in (θ) + w out (e)p out (θ) ≥ 0.75 񮽙 w in (e)(x u ·x v )+w out (e)(1−x u ·x v ) 񮽙 by Lemma 2.2, so E 񮽙 C 񮽙 = 񮽙 e E 񮽙 X e 񮽙 ≥ 0.75 · OPT .2.1 Improvements.
For the other rounding procedure we adapt a rounding scheme in [6].
We choose 6 random vectors r 1 , . . . , r 6 ∈ R n whose coordinates have the standard normal distribution.
Each r i defines a (possibly empty) cluster C i = {v : v · r i = max j v · r j } in our clustering.
The analysis of this algorithm is however significantly more involved.
Randomly choosing this scheme or the 2-hyperplane rounding algorithm gives a 0.7666-approximation algorithm that produces at most 6 clusters.
So this also works for the k-clustering variant when k ≥ 6.
, and round this by choosing either 1 or 2 hyperplanes.
This achieves a ratio of 0.77.
Theorem 2.2.
There is a 0.7666-approximation algorithm for maximizing agreements.
This also gives a 0.7666-approx.
algorithm for the k-clustering variant.
