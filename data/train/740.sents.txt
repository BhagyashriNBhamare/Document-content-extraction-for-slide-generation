Regular expressions have served as the dominant workhorse of practical information extraction for several years.
However, there has been little work on reducing the manual effort involved in building high-quality, complex regular expressions for information extraction tasks.
In this paper, we propose Re-LIE, a novel transformation-based algorithm for learning such complex regular expressions.
We evaluate the performance of our algorithm on multiple datasets and compare it against the CRF algorithm.
We show that ReLIE, in addition to being an order of magnitude faster, outperforms CRF under conditions of limited training data and cross-domain data.
Finally, we show how the accuracy of CRF can be improved by using features extracted by ReLIE.
A large class of entity extraction tasks can be accomplished by the use of carefully constructed regular expressions (regexes).
Examples of entities amenable to such extractions include email addresses and software names (web collections), credit card numbers and social security numbers (email compliance), and gene and protein names (bioinformatics), etc.
These entities share the characteristic that their key representative patterns (features) are expressible in standard constructs of regular expressions.
At first glance, it may seem that constructing * Supported in part by NSF 0438909 and NIH 1-U54-DA021519.
a regex to extract such entities is fairly straightforward.
In reality, robust extraction requires the use of rather complex expressions, as illustrated by the following example.Example 1 (Phone number extraction).
An obvious pattern for identifying phone numbers is "blocks of digits separated by hyphens" represented as R 1 = (\d+\-)+\d+.
1 While R 1 matches valid phone numbers like 800-865-1125 and 725-1234, it suffers from both "precision" and "recall" problems.
Not only does R 1 produce incorrect matches (e.g., social security numbers like 123-45-6789), it also fails to identify valid phone numbers such as 800.
865.1125, and (800)865-CARE.
An improved regex that addresses these problems is R 2 = (\d{3}[-.
\ ()]){1,2}[\dA-Z]{4}.
While multiple machine learning approaches have been proposed for information extraction in recent years ( McCallum et al., 2000;Cohen and McCal- lum, 2003;Klein et al., 2003;Krishnan and Man- ning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998;Fukuda et al., 1998;Cunningham, 1999;Tanabe and Wilbur, 2002;Li et al., 2006;DeRose et al., 2007;Zhu et al., 2007).
Yet, with a few notable exceptions, which we discuss later in Section 1.1, there has been very little work in reducing this human effort through the use of automatic learning techniques.
In this paper, we propose a novel formulation of the problem of learn-ing regexes for information extraction tasks.
We demonstrate that high quality regex extractors can be learned with significantly reduced manual effort.
To motivate our approach, we first discuss prior work in the area of learning regexes and describe some of the limitations of these techniques.
The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extraction (Alquezar and Sanfeliu, 1994;Dupont, 1996;Firoiu et al., 1998; Garofalakis et al., 2000;Denis, 2001;Denis et al., 2004;Fernau, 2005;Galassi and Giordana, 2005;Bex et al., 2006).
Much of this work assumes that the target regex is small and compact thereby allowing the learning algorithm to exploit this information.
Consider, for example, the learning of patterns motivated by DNA sequencing applications ( Galassi and Gior- dana, 2005).
Here the input sequence is viewed as multiple atomic events separated by gaps.
Since each atomic event is easily described by a small and compact regex, the problem reduces to one of learning simple regexes.
Similarly, in XML DTD inference ( Garofalakis et al., 2000;Bex et al., 2006), it is possible to exploit the fact that the XML documents of interest are often described using simple DTDs.
E.g., in an online books store, each book has a title, one or more authors and price.
This information can be described in a DTD as book ← titleauthor + price.
However, as shown in Example 1, regexes for information extraction rely on more complex constructs.In the context of information extraction, prior work has concentrated primarily on learning regexes over relatively small alphabet sizes.
A common theme in (Soderland, 1999;Ciravegna, 2001;Wu and Pottenger, 2005;Feldman et al., 2006) is the problem of learning regexes over tagged tokens produced by other text-processing steps such as POS tagging, morphological analysis, and gazetteer matching.
Thus, the alphabet is defined by the space of possible tags output by these analysis steps.
A similar approach has been proposed in (Brill, 2000) for POS disambiguation.
In contrast, our paper addresses extraction tasks that require "fine-grained" control to accurately capture the structural features of the entity of interest.
Consequently, the domain of interest consists of all characters thereby dramatically increasing the size of the alphabet.
To enable this scale-up, the techniques presented in this paper exploit advanced syntactic constructs (such as character classes and quantifiers) supported by modern regex languages.Finally, we note that almost all of the above described work define the learning problem over a restricted class of regexes.
Typically, the restrictions involve either disallowing or limiting the use of Kleene disclosure and disjunction operations.
However, our work imposes no such restrictions.
In a key departure from prior formulations, the learning algorithm presented in this work takes as input not just labeled examples but also an initial regular expression.
The use of an initial regex has two major advantages.
First, this expression provides a natural mechanism for a domain expert to provide domain knowledge about the structure of the entity being extracted.
Second, as we show in Section 2, the space of output regular expressions under consideration can be meaningfully restricted by appropriately defining their relationship to the input expression.
Such a principled approach to restrict the search space permits the learning algorithm to consider complex regexes in a tractable manner.
In contrast, prior work defined a tractable search space by placing restrictions on the target class of regular expressions.
Our specific contributions are:• A novel regex learning problem consisting of learning an "improved" regex given an initial regex and labeled examples Consider the task of identifying instances of some entity E. Let R 0 denote the input regex provided by the user and let M(R 0 , D) denote the set of matches obtained by evaluating R 0 over a document col-lection D. Let M p (R 0 , D) = {x ∈ M(R 0 , D) : x instance of E} and M n (R 0 , D) = {x ∈ M(R 0 , D) :x not an instance of E} denote the set of positive and negative matches for R 0 .
Note that a match is positive if it corresponds to an instance of the entity of interest and is negative otherwise.
The goal of our learning task is to produce a regex that is "better" than R 0 at identifying instances of E. Given a candidate regex R, we need a mechanism to judge whether R is indeed a better extractor for E than R 0 .
To make this judgment even for just the original document collection D, we must be able to label each instance matched by R (i.e., each element of M(R, D)) as positive or negative.
Clearly, this can be accomplished if the set of matches produced by R are contained within the set of available labeled examples, i.e., if M(R, D) ⊆ M(R 0 , D).
Based on this observation, we make the following assumption: Assumption 1.
Given an input regex R 0 over some alphabet Σ, any other regex R over Σ is a candidate for our learning algorithm only if L(R) ⊆ L(R 0 ).
(L(R) denotes the language accepted by R).
Even with this assumption, we are left with a potentially infinite set of candidate regexes from which our learning algorithm must choose one.
To explore this set in a principled fashion, we need a mechanism to move from one element in this space to another, i.e., from one candidate regex to another.
In addition, we need an objective function to judge the extraction quality of each candidate regex.
We address these two issues below.Regex Transformations To systematically explore the search space, we introduce the concept of regex transformations.
Definition 1 (Regex Transformation).
Let R Σ denote the set of all regular expressions over some alphabet Σ.
A regex transformation is a function T :R Σ → 2 RΣ such that ∀R ∈ T (R), L(R ) ⊆ L(R).
For example, by replacing different occurrences of the quantifier + in R 1 from Example 1 with specific ranges (such as {1,2} or {3}), we obtain expressions such as R 3 = (\d+\-){1,2}\d+ and R 4 = (\d{3}\-)+\d+.
The operation of replacing quantifiers with restricted ranges is an example of a particular class of transformations that we describe further in Section 3.
For the present, it is sufficient to view a transformation as a function applied to a regex R that produces, as output, a set of regexes that accept sublanguages of L(R).
We now define the search space of our learning algorithm as follows: Definition 2 (Search Space).
Given an input regex R 0 and a set of transformations T , the search space of our learning algorithm is T (R 0 ), the set of all regexes obtained by (repeatedly) applying the transformations in T to R 0 .
For instance, if the operation of restricting quantifiers that we described above is part of the transformation set, then R 3 and R 4 are in the search space of our algorithm, given R 1 as input.Objective Function We now define an objective function, based on the well known F-measure, to compare the extraction quality of different candidate regexes in our search space.Using M p (R, D) (resp.
M n (R, D))to denote the set of positive (resp.
negative) matches of a regex R, we defineprecision(R, D) = M p (R, D) M p (R, D) + M n (R, D) recall(R, D) = M p (R, D) M p (R 0 , D) F(R, D) = 2 · precision(R, D) · recall(R, D) precision(R, D) + recall(R, D)The regex learning task addressed in this paper can now be formally stated as the following optimization problem: Definition 3 (Regex Learning Problem).
Given an input regex R 0 , a document collection D, labeled sets of positive and negative examples M p (R 0 ,D) and M n (R 0 ,D), and a set of transformations T , compute theoutput regex R f = argmax R∈T (R0 ) F(R, D).
When applied to a collection of University web pages, we discovered that R 5 identified correct instances such as Netscape 2.0, Windows 2000 and Installation Designer v1.1.
However, R 5 also extracted incorrect instances such as course numbers (e.g. ENGLISH 317), room numbers (e.g. Room 330), and section headings (e.g. Chapter 2.2).
To eliminate spurious matches such as ENGLISH 317, let us enforce the condition that "each word is a single upper-case letter followed by one or more lower-case letters".
To accomplish this, we focus on the sub-expression of R 5 that identifies capitalized words, R 5 1 = ([A-Z]\w * \s+)+, and replace it withR 5 1a = ([A-Z][a-z] * \s+)+.
The regex resulting from R 5 by replacing R 5 1 with R 5 1a will avoid matches such as ENGLISH 317.
An alternate way to improve R 5 is by explicitly disallowing matches against strings like ENGLISH, Room and Chapter.
To accomplish this, we can exploit the negative lookahead operator supported in modern regex engines.
Lookaheads are special constructs that allow a sequence of characters to be checked for matches against a regex without the characters themselves being part of the match.
As an example, (?!
Ra)R b ("?!"
being the negative lookahead operator) returns matches of regex R b but only if they do not match R a .
Thus, by replacing R 5 1 in our original regex with R 5 1b =(?!
ENGLISH|Room|Chapter)[A-Z]\w * \s+, we produce an improved regex for software names.The above examples illustrate the general principle of our transformation technique.
In essence, we isolate a sub-expression of a given regex R and modify it such that the resulting regex accepts a sublanguage of R.
We consider two kinds of modifications -drop-disjunct and include-intersect.
In dropdisjunct, we operate on a sub-expression that corresponds to a disjunct and drop one or more operands of that disjunct.
In include-intersect, we restrict the chosen sub-expression by intersecting it with some other regex.
Formally, Definition 4 (Drop-disjunct Transformation).
Let R ∈ R Σ be a regex of the form R = R a ρ(X)R b , where ρ(X) denotes the disjunction R 1 |R 2 | . . . |R n of any non-empty set of regexes X = {R 1 , R 2 , . . . , R n }.
The drop-disjunct transformation DD(R, X, Y ) for some Y ⊂ X, Y = ∅ results in the new regex R a ρ(Y )R b .
Definition 5 (Include-Intersect Transformation).
Let .
\W \s \w [a-zA-Z] \d|[0-9] _ [a-z] [A-Z]Figure 1: Sample Character Classes in Regex R ∈ R Σ be a regex of the form R = R a XR b for someX ∈ R Σ , X = ∅.
The include-intersect transformation II(R, X, Y ) for some Y ∈ R Σ , Y = ∅ results in the new regex R a (X ∩ Y )R b .
We state the following proposition (proof omitted in the interest of space) that guarantees that both drop-disjunct and include-intersect restrict the language of the resulting regex, and therefore are valid transformations according to Definition 1.
Proposition 1.
Given regexes R, X 1 , Y 1 , X 2 and Y 2 from R Σ such that DD(R, X 1 , Y 1 ) and II(R, X 2 , Y 2 ) are applicable, L(DD(R, X 1 , Y 1 )) ⊆ L(R) and L(II(R, X 2 , Y 2 )) ⊆ L(R).
We now proceed to describe how we use different syntactic constructs to apply drop-disjunct and include-intersect transformations.
Character Class Restrictions Character classes are short-hand notations for denoting the disjunction of a set of characters (\d is equivalent to (0|1...|9); \w is equivalent to (a|. . .|z|A|. . .|Z|0|1. . .|9| ); etc.).
2 Figure 1 illustrates a character class hierarchy in which each node is a stricter class than its parent (e.g., \d is stricter than \w).
A replacement of any of these character classes by one of its descendants is an instance of the drop-disjunct transformation.
Notice that in Example 2, when replacing R 5 1 with R 5 1a , we were in effect applying a character class restriction.
Quantifier Restrictions Quantifiers are used to define the range of valid counts of a repetitive sequence.
For instance, a{m,n} looks for a sequence of a's of length at least m and at most n.
Since quantifiers are also disjuncts (e.g., a{1,3} is equivalent to a|aa|aaa), the replacement of an expression R{m, n} with an expression R{m 1 , n 1 } (m ≤ m 1 ≤ n 1 ≤ n) is an instance of the drop-disjunct transformation.
For example, given a subexpression of the form a{1,3}, we can replace it with one of a{1,1}, a{1,2}, a{2,2}, a{2,3}, or a{3,3}.
Note that, before applying this transformation, wildcard expressions such as a+ and a * are replaced by a{0,maxCount} and a{1,maxCount} respectively, where maxCount is a user configured maximum length for the entity being extracted.
Negative Dictionaries Observe that the includeintersect transformation (Definition 5) is applicable for every possible sub-expression of a given regex R. Note that a valid sub-expression in R is any portion of R where a capturing group can be introduced.
3 Consider a regex R = R a XR b with a subexpression X; the application of include-intersect requires another regex Y to yield R a (X ∩Y )R b .
We would like to construct Y such that R a (X ∩ Y )R b is "better" than R for the task at hand.
Therefore, we construct Y as ¬Y where Y is a regex constructed from negative matches of R. Specifically, we look at each negative match of R and identify the substring of the match that corresponds to X.
We then apply a greedy heuristic (see below) to these substrings to yield a negative dictionary Y .
Finally, the transformed regex R a (X ∩¬Y )R b is implemented using the negative lookahead expression Ra(?!
Y')XR b .
Greedy Heuristic for Negative Dictionaries Implementation of the above procedure requires certain judicious choices in the construction of the negative dictionary to ensure tractability of this transformation.
Let S(X) denote the distinct strings that correspond to the sub-expression X in the negative matches of R. 4 Since any subset of S(X) is a candidate negative dictionary, we are left with an exponential number of possible transformations.
In our implementation, we used a greedy heuristic to pick a single negative dictionary consisting of all those elements of S(X) that individually improve the F-measure.
For instance, in Example 2, if the independent substitution of R 5 1 with (?!
ENGLISH) [ • Applying every transformation on the current regex R new to obtain a set of candidate regexes • From the candidates, choosing the regex R whose F-measure over the training dataset is maximumF (R, Mtr) 7.
if (F (R , Mtr) <= F (Rnew, Mtr)) return Rnew 8.
if (F (R , M val ) < F (Rnew, M val )) return Rnew 9.
Rnew = R 10. }
while(true) endTo avoid overfitting, ReLIE terminates when either of the following conditions is true: (i) there is no improvement in F-measure over the training set; (ii) there is a drop in F-measure when applying R on the validation set.
The following proposition provides an upper bound for the running time of the ReLIE algorithm.
Proposition 2.
Given any valid set of inputs M tr , M val , R 0 , and T , the ReLIE algorithm terminates in at most | M n (R 0 , M tr )| iterations.
The running time of the algorithm T T otal (R 0 , M tr , M val ) ≤ | M n (R 0 , M tr )| * t 0 , where t 0 is the time taken for the first iteration of the algorithm.Proof.
With reference to Figure 2, in each iteration, the F-measure of the "best" regex R is strictly better than R new .
Since L(R ) ⊆ L(R new ), R eliminates at least one additional negative match compared to R new .
Hence, the maximum number of iterations is | M n (R 0 , M tr )|.
For a regular expression R, let n cc (R) and n q (R) denote, respectively, the number of character classes and quantifiers in R.
The maximum number of possible subexpressions in R is |R| 2 , where |R| is the length of R. Let MaxQ(R) denote the maximum number of ways in which a single quantifier appearing in R can be restricted to a smaller range.
Let F cc denote the maximum fanout 5 of the character class hierarchy.
Let T ReEval (D) denote the average time taken to evaluate a regex over dataset D.Let R i denote the regex at the beginning of iteration i.
The number of candidate regexes obtained by applying the three transformations isNumRE(Ri, Mtr) ≤ ncc(Ri) * Fcc+nq(Ri) * MaxQ(Ri)+|Ri| 2The time taken to enumerate the character class and quantifier restriction transformations is proportional to the resulting number of candidate regexes.
The time taken for the negative dictionaries transformation is given by the running time of the greedy heuristic (Section 3).
The total time taken to enumerate all candidate regexes is given by (for some constant c)TEnum(Ri, Mtr) ≤ c * (ncc(Ri) * Fcc + nq(Ri) * MaxQ(Ri) + |Ri| 2 * Mn(Ri, Mtr) * T ReEval (Mtr))Choosing the best transformation involves evaluating each candidate regex over the training and validation corpus and the time taken for this step isT P ickBest (Ri, Mtr, M val ) = NumRE(Ri, Mtr) * (T ReEval (Mtr) + T ReEval (M val ))The total time taken for an iteration can be written asTI (Ri, Mtr, M val ) =TEnum(Ri, Mtr) + T P ickBest (Ri, Mtr, M val )It can be shown that the time taken in each iteration decreases monotonically (details omitted in the interest of space).
Therefore, the total running time of the algorithm is given byT T otal (R0 , Mtr , M val ) = TI (Ri, Mtr, M val ) ≤ | Mn(R0 , Mtr )| * t0 .
where t 0 = T I (R 0 , M tr , M val ) is the running time of the first iteration of the algorithm.
In this section, we present an empirical study of the ReLIE algorithm using four extraction tasks over three real-life data sets.
The goal of this study is to evaluate the effectiveness of ReLIE in learning complex regexes and to investigate how it compares with standard machine learning algorithms.
Data Set The datasets used in our experiments are:• EWeb: A collection of 50,000 web pages crawled from a corporate intranet.
5 Fanout is the number of ways in which a character class may be restricted as defined by the hierarchy (e.g. Figure 1).
• AWeb: A set of 50,000 web pages obtained from the publicly available University of Michigan Web page collection ( Li et al., 2006), including a subcollection of 10,000 pages (AWeb-S).
• Email: A collection of 10,000 emails obtained from the publicly available Enron email collection ( Minkov et al., 2005).
Extraction Tasks SoftwareNameTask, CourseNumberTask and PhoneNumberTask were evaluated on EWeb, AWeb and Email, respectively.
Since web pages have large number of URLs, to keep the labeling task manageable, URLTask was evaluated onAWeb-S.
Gold Standard For each task, the gold standard was created by manually labeling all matches for the initial regex.
Note that only exact matches with the gold standard are considered correct in our evaluations.
6Comparison Study To evaluate ReLIE for entity extraction vis-a-vis existing algorithms, we used the popular conditional random field (CRF).
Specifically, we used the MinorThird (Cohen, 2004) implementation of CRF to train models for all four extraction tasks.
For training the CRF we provided it with the set of positive and negative matches from the initial regex with a context of 200 characters on either side of each match 7 .
Since it is unlikely that useful features are located far away from the entity, we believe that 200 characters on either side is sufficient context.
The CRF used the base features described in (Cohen et al., 2005).
To ensure fair comparison with ReLIE, we also included the matches corresponding to the input regex as a feature to the CRF.
In practice, more complex features (e.g., dictionaries, simple regexes) derived by domain experts are often provided to CRFs.
However, such features can also be used to refine the initial regex given to ReLIE.
Hence, with a view to investigating the "raw" learning capability of the two approaches, we chose to run all our experiments without any additional manually derived features.
In fact, the patterns learned by ReLIE through transformations are often similar to the features that domain experts may provide to CRF.
We will revisit this issue in Section 5.4.
Evaluation We used the standard F-measure to evaluate the effectiveness of ReLIE and CRF.
We divided each dataset into 10 equal parts and used X% of the dataset for training (X=10, 40 and 80), 10% for validation, and remaining (90-X)% for testing.
All results are reported on the test set.
Four extraction tasks were chosen to reflect the entities commonly present in the three datasets.
• The cross-validated results across all four tasks are presented in Figure 3.
• With 10% training data, ReLIE outperforms CRF on three out of four tasks with a difference in Fmeasure ranging from 0.1 to 0.2.
• As training data increases, both algorithms perform better with the gap between the two reducing for all the four tasks.
For CourseNumberTask and URLTask, CRF does slightly better than ReLIE for larger training dataset.
For the other two tasks, ReLIE retains its advantage over CRF.
9The above results indicate that ReLIE performs comparably with CRF with a slight edge in conditions of limited training data.
Indeed, the capability to learn high-quality extractors using a small training set is important because labeled data is often expensive to obtain.
For precisely this same reason, we would ideally like to learn the extractors once and then apply them to other datasets as needed.
Since these other datasets may be from a different domain, we next performed a cross-domain test (i.e., training and testing on different domains).
Cross-domain Evaluation Table 1 summarizes the results of training the algorithms on one data set and testing on another.
The scenarios chosen are: (i) SoftwareNameTask trained on EWeb and tested on AWeb, (ii) URLTask trained on AWeb and tested on Email, and (iii) PhoneNumberTask trained on Email and tested on AWeb.
10 We can see that ReLIE significantly outperforms CRF for all three tasks, even when provided with a large training dataset.
Compared to testing on the same dataset, there is a reduction in F-measure (less than 0.1 in many cases) when the regex learned by ReLIE is applied to a different dataset, while the drop for CRF is much more significant (over 0.5 in many cases).
11 Training Time Another issue of practical consideration is the efficiency of the learning algorithm.
Table 2 reports the average training and testing time for both algorithms on the four tasks.
On average Re-LIE is an order of magnitude faster than CRF in both building the model and applying the learnt model.
Robustness to Variations in Input Regexes The transformations done by ReLIE are based on the structure of the input regex.
Therefore given different input regexes, the final regexes learned by ReLIE will be different.
To evaluate the impact of the structure of the input regex on the quality of the regex learned by ReLIE, we started with different regexes 12 for the same task.
We found that ReLIE is robust to variations in input regexes.
For instance, on SoftwareNameTask, the standard deviation in F-measure 10 We do not report results for CourseNumberTask as course numbers are specific to academic webpages and do not appear in the other two domains 11 Similar cross-domain performance deterioration for a machine learning approach has been observed by (Guo et al., 2006).
12 Recall that the search space of ReLIE is limited by L(R0) (Assumption 1).
Thus to ensure meaningful comparison, for the same task any two given input regexes R0 and R 0 are chosen in such a way that although their structures are different,Mp(R0, D) = Mp(R 0 , D) and Mn(R0, D) = Mn(R 0 , D).
of the final regexes generated from six different input regexes was less than 0.05.
Further details of this experiment are omitted in the interest of space.
The results of our comparison study (Figure 3) indicates that for raw extraction quality ReLIE has a slight edge over CRF for small training data.
However, in cross-domain performance ( CourseNumberTaskR0 \b([A-Z][a-zA-Z]+)\s+\d{3,3}\bRfinal \b(((?!
(At|Between| · · · Contact|Some|Suite|Volume)) [ After examining the features learnt by CRF, it was clear that while CRF could learn features such as the negative dictionary it is unable to learn characterlevel features.
This should not be surprising since our CRF was trained with primarily tokens as features (cf. Section 5.1).
While this limitation was less of a factor in experiments involving data from the same domain (some effects were seen with smaller training data), it does explain the significant difference between the two algorithms in cross-domain tasks where the vocabulary can be significantly different.
Indeed, in practical usage of CRF, the main challenge is to come up with additional complex features (often in the form of dictionary and regex patterns) that need to be given to the CRF ( Minkov et al., 2005).
Such complex features are largely handcrafted and thus expensive to obtain.
Since the Re-LIE transformations are operations over characters, a natural question to ask is: "Can the regex learned by ReLIE be used to provide features to CRF?"
We answer this question below.
To understand the effect of incorporating ReLIEidentified features into CRF, we chose the two tasks (CourseNumberTask and PhoneNumberTask) with the least F-measure in our experiments to determine raw extraction quality.
We examined the final regex produced by ReLIE and manually extracted portions to serve as features.
For example, the negative dictionary learned by ReLIE for the CourseNumberTask (At|Between| · · · |Volume) was incorporated as a feature into CRF.
To help isolate the effects, for each task, we only incorporated features corresponding to a single transformation: negative dictionaries for CourseNumberTask and quantifier restrictions for PhoneNumberTask.
The results of these experiments are shown in Table 3.
The first point worthy of note is that performance has improved in all but one case.
Second, despite the F-measure on CourseNumberTask being lower than PhoneNumberTask (presumably more potential for improvement), the improvements on PhoneNumberTask are significantly higher.
This observation is consistent with our conjecture in Section 5.1 that CRF learns token-level features; therefore incorporating negative dictionaries as extra feature provides only limited improvement.
Admittedly more experiments are needed to understand the full impact of incorporating ReLIE-identified features into CRF.
However, we do believe that this is an exciting direction of future research.
We proposed a novel formulation of the problem of learning complex character-level regexes for entity extraction tasks.
We introduced the concept of regex transformations and described how these could be realized using the syntactic constructs of modern regex languages.
We presented ReLIE, a powerful regex learning algorithm that exploits these ideas.
Our experiments demonstrate that ReLIE is very effective for certain classes of entity extraction, particularly under conditions of cross-domain and limited training data.
Our preliminary results also indicate the possibility of using ReLIE as a powerful feature extractor for CRF and other machine learning algorithms.
Further investigation of this aspect of ReLIE presents an interesting avenue of future work.
We thank the anonymous reviewers for their insightful and constructive comments and suggestions.
We are also grateful for comments from David Gondek and Sebastian Blohm.
