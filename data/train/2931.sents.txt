Distributed storage systems have been increasingly deploying erasure codes (such as Reed-Solomon codes) for fault tolerance.
Though Reed-Solomon codes require much less storage space than replication, a significant amount of network transfer and disk I/O will be imposed when fixing unavailable data by reconstruction.
Traditionally , it is expected that unavailable data are fixed separately.
However, since it is observed that failures in the data center are correlated, fixing unavailable data of multiple failures is both unavoidable and even common.
In this paper, we show that reconstructing data of multiple failures in batches can cost significantly less network transfer and disk I/O than fixing them separately.
We present Beehive, a new design of erasure codes, that can fix unavailable data of multiple failures in batches while consuming the optimal network transfer with nearly optimal storage overhead.
Evaluation results show that Beehive codes can save network transfer by up to 69.4% and disk I/O by 75% during reconstruction.
Large-scale distributed storage systems, especially ones in data centers, store a massive amount of data that are also increasing rapidly.
Running upon commodity hardware, these storage systems are expected to keep data available, against software and hardware failures that make data unavailable on a daily basis [12].
Traditionally, replicated data are employed by distributed storage systems to keep data available.
For example, three replicas are stored by default in the Hadoop Distributed File System (HDFS) [2].
However, storing multiple copies of the original data brings expensive overhead to the storage system.
For example, three copies mean that only 33% of the total storage space can be effectively used.
Therefore, distributed storage systems (e.g., [6]) have been replacing replication with erasure codes, especially for cold or archival storage.
By migrating from replication to erasure codes, distributed storage systems can enjoy a better capability to tolerate unavailable data and meanwhile save storage overhead.
Among erasure codes, Reed-Solomon (RS) codes become the most popular choice as RS codes make the optimal usage of storage space while providing the same level of fault tolerance.To achieve fault tolerance with RS codes, we need to assume that data are stored in blocks with a fixed size, a common practice for most distributed storage systems.
With k data blocks, RS codes compute r parity blocks, such that any k of the total k + r blocks can recover all data blocks.
The data blocks and their corresponding parity blocks belong to the same stripe.
Therefore, such RS codes can tolerate at most r failures within the same stripe.
For example, RS codes with k = 4 and r = 2 can tolerate any two missing blocks with 1.5x storage overhead, while three-way replication, achieving the same level of fault tolerance, requires 3x storage overhead.Once one block becomes unavailable, the missing data should be fixed through a reconstruction operation and saved at some other server.
Under RS codes, the reconstruction operation requires to download k existing blocks and then decode the missing block, imposing k times of the network transfer under replication.
It has been reported from a Facebook's cluster that reconstructing unavailable data under RS codes can increase more than 100 TB of data transfer in just one day [10].
Besides, the same amount of disk I/O will also be imposed on the servers that store the downloaded blocks.To save network transfer during reconstruction, there have been considerable interests in the construction of a class of erasure codes called minimum-storage regenerating (MSR) codes (e.g., [11]).
The same as RS codes, MSR codes also make the optimal usage of storage space.
However, MSR codes can significantly save network transfer during reconstruction.
As shown in Fig. 1a let k = 3 and r = 3 for both RS codes and MSR codes here.
While RS codes need to download three blocks to reconstruct one missing block, MSR codes only need to download a fraction of each block.
In this example, though MSR codes require to download data from one more block, the total network transfer is still saved by 33.3% as only a half of network transfer is imposed on each block.
Nevertheless, even though only a fraction of a block is required for the reconstruction, in general it has to be encoded from the whole block.
1 Therefore, MSR codes do not ease but further increase the overhead of disk I/O, as the reconstruction requires to download data from even more blocks than RS codes.Traditionally, it is assumed that when there are unavailable blocks, distributed storage systems will reconstruct them separately.
However, inside data centers, data unavailability events can be correlated.
For example, many disks fail at similar ages [8].
When one disk fails, it suggests a high probability that some other disks fail roughly at the same time.
Not just limited to disk failures, correlated data failures can also happen due to various reasons [5] such as switch failures, power outages, maintenance operations, or software glitches.
Taking advantages of the correlated failures, we investigate the construction of erasure codes that allows us to reconstruct multiple missing blocks in batches, to save both network transfer and disk I/O during reconstruction.In this paper, we propose a new family of erasure codes, called Beehive, that reconstruct multiple blocks at the same time.
An instant benefit this brings is that each block will only be read once to reconstruct multiple blocks.
As illustrated in Fig. 1b, the total amount of disk read is saved by 50% when we reconstruct two blocks together at the same time, while we can even further save network transfer as well.
In fact, Beehive codes achieve the optimal network transfer per block in the reconstruction operation.
The construction of Beehive codes is built on top of MSR codes.
We implement Beehive codes in C++ and evaluate the performance of Beehive on Ama-1 There exist some constructions [9,14] of erasure codes that support to obtain a fraction of block without any encoding operations.
However, typically MSR codes do not optimize for disk I/O.
. . . Suppose that given k data blocks, we will have r parity blocks under some erasure code.
If any k of all the k + r blocks can decode the original data, such erasure codes make the optimal usage of storage space, e.g., RS codes.
A block contains a certain number of symbols.
Typically, a symbol is simply a single byte.
The encoding, decoding and reconstruction operations of the erasure code are performed on such symbols with the arithmetic of the socalled finite field.
In this paper, however, we do not rely on any direct knowledge of the finite field and readers can simply consider its arithmetic as usual arithmetic.
Given the original data, we can divide them into generations such that each generation contains k blocks ( f i , i = 1, . . . , k) with the same size.
For simplicity, we only consider one generation in this paper, as all generations will be encoded in the same way.
Depending on the erasure codes, a block may consist of one or multiple segments, where each segment is a row vector of w symbols.
For simplicity, we assume that one symbol is a byte in this paper.
In other words, each segment contains w bytes.
Assuming that each block contains α segments, each block has αw bytes, and we regard f i as an α × w matrix.
Let n = k + r. Given the k original blocks, erasure codes use an nα × kα generating matrix G to compute all blocks in a stripe, i.e., G · f T 1 · · · f T k T , wheref T i denotes the transpose of f i .
We can divide G into n submatrices of size α × kα such that G = g T 1 · · · g T n T .
Therefore, the n blocks generated by G can be represented as g i F (or block i, for simplicity), i = 1, . . . , n, whereF = f T 1 · · · f T k T .
The n blocks computed from the same generation belong to the same stripe.
We illustrate in brief the notations described above in Fig. 2.
If the first kα rows of G form an identity matrix, g i F is identical to f i , i = 1, . . . , k.
In this way, we can term g 1 F, . . . , g k F as data blocks and the rest as parity blocks.
Erasure codes described in this paper are systematic codes.
Typically, there is only one segment in each block under RS codes, i.e., α = 1.
We can construct RS codes by letting the rest of G be a Cauchy matrix.
Given any k blocks in the same stripe, we can have their corresponding submatrix of the generating matrix and then decode the original data by multiplying the inverse of this submatrix with these k blocks.Under MSR codes, on the other hand, a block con-tains d − k + 1 segments (i.e., α = d − k + 1),where d is the number of available blocks required to reconstruct a missing one, d > k.
During reconstruction, we call these d blocks as helpers and the one being reconstructed as a newcomer.
To reconstruct any newcomer, we do not need to decode it like under RS codes, but reconstruct it with fractions of d helpers in the same stripe.
For example, to reconstruct g i F, we will compute one segment v T i g j F, from block g j F where v i is a column vector of size α symbols.
With the d segments obtained from helpers, we can reconstruct g i F by multiplying an α × d matrix with these d segments.There have been several constructions of MSR codes (e.g., [11]).
In this paper, we will construct our Beehive codes based on one particular product-matrix construction proposed by Rashmi et al. [11], because 1) the MSR codes constructed are systematic; and 2) unlike other constructions that impose constraints on specific values of d or k, the construction proposed in [11] is much more general by only requiring d ≥ 2k − 2.
2 We construct Beehive on top of the product-matrix MSR codes, and we exploit an important property of this construction in Beehive.
In the construction of MSR codes with given k, r, d, we can explicitly obtain a constant vector (λ 1 . . . λ n ), and an n × (k − 1) matrix A in which every k − 1 rows are linearly independent.
We refer to the i-th row of A as A i = (a i,1 . . . a i,k−1 ).
Besides, in A the first k − 1 rows, i.e., A i , . . . , A k−1 , are standard bases.
When i, j ≥ k, to reconstruct a newcomer g j F, there exists a column vectorˆvvectorˆ vectorˆv i, j of size α such that the segment v T j g i F satisfies the following equation:(λ i − λ j ) −1 v T j g i F = k−1 ∑ l=1 a i,l · (λ l − λ j ) −1 v T j g l F + ˆ v T i, j g j F,(1)wherê v i, j is a vector of size α symbols that is linearly independent with any other α − 1 of such vectors with different values of i and the same value of j. 3 Under Beehive, we assume that t newcomers are reconstructed simultaneously, t > 1.
Unlike traditional erasure codes like RS or MSR codes, we divide one generation of the original data into two parts that contains k and k − 1 blocks, respectively.
In the first part, each block contains d − k + 1 segments, and t − 1 segments in the second part.
We denote the k blocks in the first part asF = [ f T 1 · · · f T k ]T and the k − 1 ones in the second part asC = [c T 1 · · · c T k−1 ] T .
We illustrate this process in Fig. 3.
. . . F C g i F the original data block i k񮽙1 X l=1 a i,l c l c 1 c 2 ck񮽙1 . . . f 1 f 2 f k Figure 3:The construction of Beehive codes.As described in Sec. 2, given k, r, d where d ≥ 2k − 2, we can construct a generating matrix G of the productmatrix MSR code.
Then we encode the first part F with such G and get g i F, i = 1, . . . , n. Along with the productmatrix MSR code, we can also obtain the corresponding matrix A, and then we encode data in the second part as ∑ k−1 l=1 a i,l c l , i = 1, . . . , n.From the original data, Beehive computes n blocks.
Each block contains the d − k + 1 segments from g i F and the t − 1 segments from ∑ k−1 l=1 a i,l c l .
Therefore, in the first k − 1 blocks we can find C directly, since the first k − 1 rows of A are standard bases.
Since the MSR code we use in the first part is systematic, we can also find F directly from the first k blocks.
Therefore, Beehive codes are systematic.On the other hand, there are d − k +t segments in each block under Beehive codes.
If each segment contains w symbols, the original data should be grouped into generations of [k(d −k +t)−(t −1)]w symbols.
With the same block size, this will lead to a reduction of (t − 1)w symbols in each generation under Beehive codes, compared to the theoretical optimum [15] (we omit the proof due to the page limit).
Considering the total amount of data in a generation, this loss of storage efficiency is insignificant.
To decode the original data from any k blocks in a stripe, both F and C must be decoded.
Apparently F can be decoded from any k blocks by MSR codes.
On the other hand, given k blocks, we have k linear combinations of c i , i = 1, . . . , k − 1.
Since any k − 1 rows in A are linearly independent, c i can be recovered from any k − 1 blocks.
Now we consider the reconstruction operation.
Without loss of generality, we only show the case of t = 2.
Let N be the set of newcomers and H be the set of helpers, where |N| = t, |H| = d, and N ∩ H = / 0.
For any i ∈ H, the helper i computes(λ i − λ j ) −1 v T j g i F + u T j ∑ k−1 l=1 a i,l c l , i = 1, .
.
.
, n, and sends it to newcomer j , where u j is a vector of size t − 1 symbols.
Any t − 1 vectors in {u j | j ∈ {1, . . . , n}} must be linearly independent.At the side of newcomers, we divide their operation into two stages.
Taking newcomer j for example, where j ≥ k (we also omit the proof for j < k due to the page limit), in the first stage, it will receive d segments from helpers in H. By (1), each segment can be written as(λ i − λ j ) −1 v T j g i F + u T j k−1 ∑ l=1 a i,l c l = k−1 ∑ l=1 a i,l ((λ l − λ j ) −1 v T j g l F + u T j c l ) + ˆ v T i, j g j F.In this way,a i,l ((λ l − λ j ) −1 v T j g l F + u T j c l ) is of rank 1, l = 1, . . . , k − 1, andˆvandˆ andˆv T i, j g j F is of rank d − k + 1, i ∈ H. Thus we can use d segments received from the d helpers to solve g j F, ∀ j ∈ N. Meanwhile, (λ l − λ j ) −1 v T j g l F + u T j c l can be solved as well, l = 1, . . . , k − 1.
In the second stage, the newcomer j will sendk−1 ∑ l=1 a j ,l ((λ l − λ j ) −1 v T j g l F + u T j c l ) + ˆ v T j , j g j F =(λ j − λ j ) −1 v T j g j F + u T j k−1 ∑ l=1 a j ,l c l (from (1))to another newcomer j , and it will receive t −1 segments from other newcomers as well.
Then the newcomer j can cancel out (λ j − λ j ) −1 v T j g j F as it has already solved g j F.
Then we can get u T j ∑ k−1 l=1 a j ,l c l , j ∈ N \{ j }, and solve ∑ k−1 l=1 a j ,l c l as well, from the t − 1 segments received from other newcomers.
Therefore, it is recommended that the matrix composed by {u j | j ∈ {1, . . . , n}} contains an identity submatrix, such that this operation can be simplified.
In this way, we can reconstruct both g j F and ∑ k−1 l=1 a j ,l c l .
During reconstruction, each newcomer will receive d + t − 1 segments, achieving the optimal network transfer [15] to reconstruct t blocks.
We implement Beehive in C++, using the Intel storage acceleration library (ISA-L) [1] for the finite field arithmetic.
We also implement RS codes and MSR codes with ISA-L, for comparison purposes.We let k = 6 and r = 6 and set the block size to be 60 MB.
We first compare the speed of different operations of RS, MSR (d = 10), and Beehive codes (d = 10,t = 2) in Fig. 4, running on Amazon EC2 instances of type c4.2xlarge.
We observe that the encoding and decoding operations of MSR codes and Beehive codes are quite close to each other.
The encoding operation of Beehive codes is a bit slower than MSR codes as Beehive codes are built on top of MSR codes.
Nevertheless, we do not observe that the speed of decoding or reconstruction operation is significantly different between MSR codes and Beehive codes.
The first stage of the reconstruction at the side of newcomers under MSCR codes has lower throughput.
However, since only one segment is produced in this stage, the real processing time is much faster than the second stage.
This is more important to distributed storage systems as data will be encoded only once but decoded or reconstructed many times.
RS codes, on the other hand, can enjoy a higher throughput as there is only one segment in each block under RS codes.With each block of size 60 MB, a generation under RS or MSR codes is 360 MB, but a generation under Beehive codes is 10 MB less.
In fact, with the same storage overhead (i.e., the same k and r), Beehive codes store less actual data.
In other words, with the same amount of original data, we may have more generations under Beehive codes.
However, this additional storage overhead is marginal.
It is just 2.8% in this case.We compare the disk read and network transfer during reconstruction for RS, MSR, and Beehive codes in Fig. 5.
In particular, when t = 1, Beehive codes will be the same as MSR codes.
When t > 1, RS codes and MSR codes reconstruct t blocks separately.
In other words, there will be data read during each reconstruction.
Hence, as shown in Fig. 5a, Beehive codes achieve much less disk read during reconstruction, which does not change with t, but only depends on the number of helpers.
Compared with MSR codes, Beehive codes can save up to 75% of disk read.
With regards to network transfer, we observe in Fig. 5b codes, however, can further save network transfer by up to 42.9% against MSR codes and by 69.4% against RS codes.
The reduction of network transfer becomes even more significant with an increasing of t.
In order to optimize network transfer during reconstruction without loss of fault tolerance, Dimakis et al. [4] explored the theoretical lower bound of network transfer of the single-block reconstruction, and there are a lot of literatures that present constructions of such erasure codes (called regenerating codes) [3].
Among these codes, the minimum-storage regenerating (MSR) codes achieve the optimal storage overhead [11].
Network transfer can be further saved when there are multiple blocks to reconstruct at the same time [15].
However, there has been no construction of erasure codes that can achieve this lower bound with general values of parameters and the optimal storage overhead.
Shum [15] and Li et al. [7] have proposed a construction of such erasure codes that achieve the optimal network transfer with d = k and t = 2, respectively.
In this paper, we propose a construction that achieves the optimal network transfer with near-optimal storage overhead and a wide range of parameters.
In this paper, we propose Beehive, a new family of erasure codes that reconstruct multiple blocks simultaneously and achieve the optimal network transfer with near optimal storage overhead.
Through experiments on Amazon EC2, we show that compared to existing erasure codes like RS and MSR codes, Beehive codes can both save network transfer and disk I/O significantly.
This paper is supported by the SAVI project and the NSERC Discovery Grant.
