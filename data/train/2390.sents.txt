Existing storage stacks are top-heavy and expect little from block storage.
As a result, new high-level storage abstractions-and new designs for existing abstractions-are difficult to realize, requiring developers to implement from scratch complex functionality such as failure atomicity and fine-grained concurrency control.
In this paper, we argue that pushing transactional isolation into the block store (in addition to atomicity and durability) is both viable and broadly useful, resulting in simpler high-level storage systems that provide strong semantics without sacrificing performance.
We present Isotope, a new block store that supports ACID transactions over block reads and writes.
Internally, Isotope uses a new multi-version concurrency control protocol that exploits fine-grained, sub-block parallelism in workloads and offers both strict serializability and snapshot isolation guarantees.
We implemented several high-level storage systems over Isotope, including two key-value stores that implement the LevelDB API over a hashtable and B-tree, respectively , and a POSIX filesystem.
We show that Iso-tope's block-level transactions enable systems that are simple (100s of lines of code), robust (i.e., providing ACID guarantees), and fast (e.g., 415 MB/s for random file writes).
We also show that these systems can be composed using Isotope, providing applications with transactions across different high-level constructs such as files, directories and key-value pairs.
With the advent of multi-core machines, storage systems such as filesystems, key-value stores, graph stores and databases are increasingly parallelized over dozens of cores.
Such systems run directly over raw block storage but assume very little about its interface and semantics; usually, the only expectations from the block store are durability and single-operation, single-block linearizability.
As a result, each system implements complex code to layer high-level semantics such as atomicity and isolation over the simple block address space.
While multiple systems have implemented transactional atomicity within the block store [18,24,46,6,19], concurrency control has traditionally been delegated to the storage system above the block store.In this paper, we propose the abstraction of a transactional block store that provides isolation in addition to atomicity and durability.
A number of factors make isolation a prime candidate for demotion down the stack.1) Isolation is general; since practically every storage system has to ensure safety in the face of concurrent accesses, an isolation mechanism implemented within the block layer is broadly useful.
2) Isolation is hard, especially for storage systems that need to integrate fine-grained concurrency control with coarse-grained durability and atomicity mechanisms (e.g., see ARIES [40]); accordingly, it is better provided via a single, high-quality implementation within the block layer.
3) Block-level transactions allow storage systems to effortlessly provide end-user applications with transactions over high-level constructs such as files or key-value pairs.
4) Block-level transactions are oblivious to software boundaries at higher levels of the stack, and can seamlessly span multiple layers, libraries, threads, processes, and interfaces.
For example, a single transaction can encapsulate an end application's accesses to an in-process key-value store, an in-kernel filesystem, and an out-of-process graph store.
5) Finally, multiversion concurrency control (MVCC) [17] provides superior performance and liveness in many cases but is particularly hard to implement for storage systems since it requires them to maintain multiversioned state; in contrast, many block stores (e.g., log-structured designs) are already internally multiversioned.
Block-level isolation is enabled and necessitated by recent trends in storage.
Block stores have evolved over time.
They are increasingly implemented via a combination of host-side software and device firmware [9,3]; they incorporate multiple, heterogeneous physical devices under a single address space [59,56]; they leverage new NVRAM technologies to store indirection metadata; and they provide sophisticated functionality such as virtualization [9,61], tiering [9], deduplication and wear-leveling.
Unfortunately, storage systems such as filesystems continue to assume minimum functionality from the block store, resulting in redundant, complex, and inefficient stacks where layers constantly tussle with each other [61].
A second trend that argues for pushing functionality from the filesystem to a lower layer is the increasing importance of alternative abstractions that can be implemented directly over block storage, such as graphs, key-value pairs [8], tables, caches [53], tracts [42], byte-addressable [14] and write-once [15] address spaces, etc.To illustrate the viability and benefits of block-level isolation, we built Isotope, a transactional block store that provides isolation (with a choice of strict serializability or snapshot isolation) in addition to atomicity and durability.
Isotope is implemented as an in-kernel software module running over commodity hardware, exposing a conventional block read/write interface augmented with beginTX/endTX IOCTLs to demarcate transactions.
Transactions execute speculatively and are validated by Isotope on endTX by checking for conflicts.
To minimize the possibility of conflict-related aborts, applications can provide information to Isotope about which sub-parts of each 4KB block are read or written, allowing Isotope to perform conflict detection at sub-block granularity.Internally, Isotope uses an in-memory multiversion index over a persistent log to provide each transaction with a consistent, point-in-time snapshot of a block address space.
Reads within a transaction execute against this snapshot, while writes are buffered in RAM by Isotope.
When endTX is called, Isotope uses a new MVCC commit protocol to determine if the transaction commits or aborts.
The commit/abort decision is a function of the timestamp-ordered stream of recently proposed transactions, as opposed to the multiversion index; as a result, the protocol supports arbitrarily fine-grained conflict detection without requiring a corresponding increase in the size of the index.
When transactions commit, their buffered writes are flushed to the log, which is implemented on an array of physical drives [56], and reflected in the multiversion index.
Importantly, aborted transactions do not result in any write I/O to persistent storage.Storage systems built over Isotope are simple, stateless, shim layers that focus on mapping some variablesized abstraction -such as files, tables, graphs, and keyvalue pairs -to a fixed-size block API.
We describe several such systems in this paper, including a key-value store based on a hashtable index, one based on a B-tree, and a POSIX user-space filesystem.
These systems do not have to implement their own fine-grained locking for concurrency control and logging for failure atomicity.
They can expose transactions to end applications without requiring any extra code.
Storage systems that reside on different partitions of an Isotope volume can be composed with transactions into larger end applications.Block-level isolation does have its limitations.
Storage systems built over Isotope cannot share arbitrary, inmemory soft state such as read caches across transaction boundaries, since it is difficult to update such state atomically based on the outcome of a transaction.
Instead, they rely on block-level caching in Isotope by providing hints about which blocks to cache.
We found this approach well-suited for both the filesystem application (which cached inode blocks, indirection blocks and allocation maps) and the key-value stores (which cached their index data structures).
In addition, information is invariably lost when functionality is implemented at a lower level of the stack: Isotope cannot leverage properties such as commutativity and idempotence while detecting conflicts.This paper makes the following contributions:• We revisit the end-to-end argument for storage stacks with respect to transactional isolation, in the context of modern hardware and applications.
• We propose the abstraction of a fully transactional block store that provides isolation, atomicity and durability.
While others have explored block-level transactional atomicity [18,24,46,19], this is the first proposal for block-level transactional isolation.
• We realize this abstraction in a system called Isotope via a new MVCC protocol.
We show that Isotope exploits sub-block concurrency in workloads to provide a high commit rate for transactions and high I/O throughput.
• We describe storage systems built using Isotope transactions -two key-value stores and a filesystem -and show that they are simple, fast, and robust, as well as composable via Isotope transactions into larger end applications.
Block-level isolation is an idea whose time has come.
In the 90s, the authors of Rio Vista (a system that provided atomic transactions over a persistent memory abstraction) wrote in [36]: "We believe features such as serializability are better handled by higher levels of software... adopting any concurrency control scheme would penalize the majority of applications, which are singlethreaded and do not need locking."
Today, applications run on dozens of cores and are multi-threaded by default; isolation is a universal need, not a niche feature.
Isolation is simply the latest addition to a long list of features provided by modern block stores: caching, tiering, mapping, virtualization, deduplication, and atomicity.
This explosion of features has been triggered partly by the emergence of software-based block layers, ranging from flash FTLs [3] to virtualized volume managers [9].
In addition, the block-level indirection necessary for many of these features has been made practical and inexpensive by hardware advances in the last decade.
In the past, smart block devices such as HP AutoRAID [65] were restricted to enterprise settings due to their reliance on battery-backed RAM; today, SSDs routinely implement indirection in FTLs, using supercapacitors to flush metadata and data on a power failure.
Software block stores in turn can store metadata on these SSDs, on raw flash, or on derivatives such as flashbacked RAM [34] and Auto-Commit Memory [7].
What about the end-to-end argument?
We argue that block-level isolation passes the litmus test imposed by the end-to-end principle [49] for pushing functionality down the stack: it is broadly useful, efficiently implementable at a lower layer of the stack with negligible performance overhead, and leverages machinery that already exists at that lower layer.
The argument regarding utility is obvious: pushing functionality down the stack is particularly useful when it is general enough to be used by the majority of applications, which is undeniably the case for isolation or concurrency control.
However, the other motivations for a transactional block store require some justification:Isolation is hard.
Storage systems typically implement pessimistic concurrency control via locks, opening the door to a wide range of aberrant behavior such as deadlocks and livelocks.
This problem is exacerbated when developers attempt to extract more parallelism via fine-grained locks, and when these locks interact with coarse-grained failure atomicity and durability mechanisms [40].
Transactions can provide a simpler programming model that supplies isolation, atomicity and durability via a single abstraction.
Additionally, transactions decouple the policy of isolation -as expressed through beginTX/endTX calls -from the concurrency control mechanism used to implement it under the hood.Isolation is harder when exposed to end applications.
Storage systems often provide concurrency control APIs over their high-level storage abstractions; for example, NTFS offers transactions over files, while Linux provides file-level locking.
Unfortunately, these high-level concurrency control primitives often have complex, weakened, and idiosyncratic semantics [44]; for instance, NTFS provides transactional isolation for accesses to the same file, but not for directory modifications, while a Linux fcntl lock on a file is released when any file descriptor for that file is closed by a process [1].
The complex semantics are typically a reflection of a complex implementation, which has to operate over high-level constructs such as files and directories.
In addition, composability is challenging if each storage system implements isolation independently: for example, it is impossible to do a transaction over an NTFS file and a Berkeley DB key-value pair.Isolation is even harder when multiversion concurrency control is required.
In many cases, pessimistic concurrency control is slow and prone to liveness bugs; for example, when locks are exposed to end applications directly or via a transactional interface, the application / *** Transaction API *** / int beginTX(); int endTX(); int abortTX(); //POSIX read/write commands / *** Optional API *** / //release ongoing transaction and return handle int releaseTX(); //take over a released transaction int takeoverTX(int tx_handle); //mark byte range accessed by last read/write int mark_accessed(off_t blknum, int start, int size); //request caching for blocks int please_cache(off_t blknum); Figure 1: The Isotope API.could hang while holding a lock.
Optimistic concurrency control [35] works well in this case, ensuring that other transactions can proceed without waiting for the hung process.
Multiversion concurrency control works even better, providing transactions with stable, consistent snapshots (a key property for arbitrary applications that can crash if exposed to inconsistent snapshots [31]); allowing read-only transactions to always commit [17]; and enabling weaker but more performant isolation levels such as snapshot isolation [16].
However, switching to multiversion concurrency control can be difficult for storage systems due to its inherent need for multiversion state.
High-level storage systems are not always intrinsically multiversioned (with notable exceptions such as WAFL [33] and other copy-on-write filesystems), making it difficult for developers to switch from pessimistic locking to a multiversion concurrency control scheme.
Multiversioning can be particularly difficult to implement for complex data structures used by storage systems such as B-trees, requiring mechanisms such as tombstones [26,48].
In contrast, multiversioning is relatively easy to implement over the static address space provided by a block store (for example, no tombstones are required since addresses can never be 'deleted').
Additionally, many block stores are already multiversioned in order to obtain write sequentiality: examples include log-structured disk stores, shingled drives [11] and SSDs.
The basic Isotope API is shown in Figure 1: applications can use standard POSIX calls to issue reads and writes to 4KB blocks, bookended by beginTX/endTX calls.
The beginTX call establishes a snapshot; all reads within the transaction are served from that snapshot.
Writes within the transaction are speculative.
Each transaction can view its own writes, but the writes are not made visible to other concurrent transactions until the transaction commits.
The endTX call returns true if the transaction commits, and false otherwise.
The abortTX allows the application to explicitly abort the transaction.
The application can choose one of two isolation levels on startup: strict serializability or snapshot isolation.The Isotope API implicitly associates transaction IDs with user-space threads, instead of augmenting each call signature in the API with an explicit transaction ID that the application supplies.
We took this route to allow applications to use the existing, highly optimized POSIX calls to read and write data to the block store.
The control API for starting, committing and aborting transactions is implemented via IOCTLs.
To allow transactions to execute across different threads or processes, Isotope provides additional APIs via IOCTLs: releaseTX disconnects the association between the current thread and the transaction, and returns a temporary transaction handle.
A different thread can call takeoverTX with this handle to associate itself with the transaction.Isotope exposes two other optional calls via IOCTLs.
After reading or writing a 4KB block within a transaction, applications can call mark accessed to explicitly specify the accessed byte range within the block.
This information is key for fine-grained conflict detection; for example, a filesystem might mark a single inode within an inode block, or a single byte within a data allocation bitmap.
Note that this information cannot be inferred implicitly by comparing the old and new values of the 4KB block; the application might have overwritten parts of the block without changing any bits.
The second optional call is please cache, which lets the application request Isotope to cache specific blocks in RAM; we discuss this call in detail later in the paper.
Figure 2 shows a snippet of application code that uses the Isotope API (the setattr function from a filesystem).
If a read or write is issued outside a transaction, it is treated as a singleton transaction.
In effect, Isotope behaves like a conventional block device if the reads and writes issued to it are all non-transactional.
In addition, Isotope can preemptively abort transactions to avoid buggy or malicious applications from hoarding resources within the storage subsystem.
When a transaction is preemptively aborted, any reads, writes, or control calls issued within it will return error codes, except for endTX, which will return false, and abortTX.Transactions can be nested -i.e., a beginTX/endTX pair can have other pairs nested within it -with the simple semantics that the internal transactions are ignored.
A nested beginTX does not establish a new snapshot, and a nested endTX always succeeds without changing the persistent state of the system.
A nested abortTX causes any further activity in the transaction to return error codes until all the enclosing abortTX/endTX have been called.
This behavior is important for allowing storage systems to expose transactions to end-user applications.
In the example of the filesystem, if an end-user application invokes beginTX (either directly on Isotope or through a filesystem-provided API) before calling the setattr function in Figure 2 multiple times, the internal transactions within each setattr call are ignored and the entire ensemble of operations will commit or abort.
As stated earlier, a primary benefit of a transactional block store is its obliviousness to the structure of the software stack running above it, which can range from a single-threaded application to a composition of multithreaded application code, library storage systems, outof-process daemons and kernel modules.
The Isotope API is designed to allow block-level transactions to span arbitrary compositions of different types of software modules.
We describe some of these composition patterns in the context of a simple photo storage application called ImgStore, which stores photos and their associated metadata in a key-value store.In the simplest case, ImgStore can store images and various kinds of metadata as key-value pairs in IsoHT, which in turn is built over a Isotope volume using transactions.
Here, a single transaction-oblivious application (ImgStore) runs over a single transaction-aware librarybased storage system (IsoHT).
Cross-Layer: ImgStore may want to atomically update multiple key-value pairs in IsoHT; for example, when a user is tagged in a photo, ImgStore may want to update a photo-to-user mapping as well as a user-to-photo mapping, stored under two different keys.
To do so, ImgStore can encapsulate calls to IsoHT within Isotope beginTX/endTX calls, leveraging nested transactions.
Cross-Thread: In the simplest case, ImgStore executes each transaction within a single thread.
However, if ImgStore is built using an event-driven library that requires transactions to execute across different threads, it can use the releaseTX/takeoverTX calls.
Cross-Library: ImgStore may find that IsoHT works well for certain kinds of accesses (e.g., retrieving a specific image), but not for others such as range queries (e.g., finding photos taken between March 4 and May 10, 2015).
Accordingly, it may want to spread its state across two different library key-value stores, one based on a hashtable (IsoHT) and another on a B-tree (IsoBT) for efficient range queries.
When a photo is added to the system, ImgStore can transactionally call put operations on both stores.
This requires the key-value stores to run over different partitions on the same Isotope volume.
Cross-Process: For various reasons, ImgStore may want to run IsoHT in a separate process and access it via an IPC mechanism; for example, to share it with other applications on the same machine, or to isolate failures in different codebases.
To do so, ImgStore has to call releaseTX and pass the returned transaction handle via IPC to IsoHT, which then calls takeoverTX.
This requires IsoHT to expose a transaction-aware IPC interface for calls that occur within a transactional context.
Figure 3 shows the major components of the Isotope design.
Isotope internally implements an in-memory multiversion index (B in the figure) over a persistent log (E).
Versioning is provided by a timestamp counter (A) which determines the snapshot seen by a transaction as well as its commit timestamp.
This commit timestamp is used by a decision algorithm (D) to determine if the transaction commits or not.
Writes issued within a transaction are buffered (C) during its execution, and flushed to the log if the transaction commits.
We now describe the interaction of these components.
When the application calls beginTX, Isotope creates an in-memory intention record for the speculative transaction: a simple data structure with a start timestamp and a read/write-set.
Each entry in the read/write-set consists of a block address, a bitmap that tracks the accessed status of smaller fixed-size chunks or fragments within the block (by default, the fragment size is 16 bytes, resulting in a 256-bit bitmap for each 4KB block), and an additional 4KB payload only in the write-set.
These bitmaps are never written persistently and are only maintained inmemory for currently executing transactions.
After creating the intention record, the beginTX call sets its start timestamp to the current value of the timestamp counter (A in Figure 3) without incrementing it.
Until endTX is called, the transaction executes speculatively against the (potentially stale) snapshot, without any effect on the shared or persistent state of the system.
Writes update the write-set and are buffered inmemory (C in Figure 3) without issuing any I/O.
A transaction can read its own buffered writes, but all other reads within the transaction are served from the snapshot corresponding to the start timestamp using the multiversion index (B in Figure 3).
The mark accessed call modifies the bitmap for a previously read or written block to indicate which bits the application actually touched.
Multiple mark accessed calls have a cumulative effect on the bitmap.
At any point, the transaction can be preemptively aborted by Isotope simply by discarding its intention record and buffered writes.
Subsequent reads, writes, and endTX calls will be unable to find the record and return an error code to the application.All the action happens on the endTX call, which consists of two distinct phases: deciding the commit/abort status of the transaction, and applying the transaction (if it commits) to the state of the logical address space.
Regardless of how it performs these two phases, the first action taken by endTX is to assign the transaction a commit timestamp by reading and incrementing the global counter.
The commit timestamp of the transaction is used to make the commit decision, and is also used as the version number for all the writes within the transaction if it commits.
We use the terms timestamp and version number interchangeably in the following text.
To determine whether the transaction commits or aborts, endTX must detect the existence of conflicting transactions.
The isolation guarantee provided -strict serializability or snapshot isolation -depends on what constitutes a conflicting transaction.
We first consider a simple strawman scheme that provides strict serializability and implements conflict detection as a function of the multiversion index.
Here, transactions are processed in commit timestamp order, and for each transaction the multiversion index is consulted to check if any of the logical blocks in its read-set has a version number greater than the current transaction's start timestamp.
In other words, we check whether any of the blocks read by the transaction has been updated since it was read.This scheme is simple, but suffers from a major drawback: the granularity of the multiversion index has to match the granularity of conflict detection.
For example, if we want to check for conflicts at 16-byte grain, the index has to track version numbers at 16-byte grain as well; this blows up the size of the in-memory index by     Commit  Abort   T1 T2 T3 T4 T5Conflict with T3!
Figure 4: Conflict detection under snapshot isolation: a transaction commits if no other committed transaction in its conflict window has an overlapping write-set.256X compared to a conventional block-granular index.As a result, this scheme is not well-suited for fine-grained conflict detection.To perform fine-grained conflict detection while avoiding this blow-up in the size of the index, Isotope instead implements conflict detection as a function over the temporal stream of prior transactions (see Figure 4).
Concretely, each transaction has a conflict window of prior transactions between its start timestamp and its commit timestamp.
• For strict serializability, the transaction T aborts if any committed transaction in its conflict window modified an address that T read; else, T commits.
• For snapshot isolation, the transaction T aborts if any committed transaction in its conflict window modified an address that T wrote; else, T commits.
In either case, the commit/abort status of a transaction is a function of a window of transactions immediately preceding it in commit timestamp order.When endTX is called on T , a pointer to its intention record is inserted into the slot corresponding to its commit timestamp in an in-memory array.
Since the counter assigns contiguous timestamps, this array has no holes; each slot is eventually occupied by a transaction.
At this point, we do not yet know the commit/abort status of T and have not issued any write I/O, but we have a start timestamp and a commit timestamp for it.
Each slot is guarded by its own lock.To decide if T commits or aborts, we simply look at its conflict window of transactions in the in-memory array (i.e., the transactions between its start and commit timestamps).
We can decide T 's status once all these transactions have decided.
T commits if each transaction in the window either aborts or has no overlap between its read/write-set and T 's read/write-set (depending on the transactional semantics).
Since each read/write-set stores fine-grained information about which fragments of the block are accessed, this scheme provides fine-grained conflict detection without increasing the size of the multiversion index.Defining the commit/abort decision for a transaction as a function of other transactions is a strategy as old as optimistic concurrency control itself [35], but choosing an appropriate implementation is non-trivial.
Like us, Bernstein et al. [48] formulate the commit/abort decision for distributed transactions in the Hyder system as a function of a conflict window over a totally ordered stream of transaction intentions.
Unlike us, they explicitly make a choice to use the spatial state of the system (i.e., the index) to decide transactions.
A number of factors drive our choice in the opposite direction: we need to support writes at arbitrary granularity (e.g., an inode) without increasing index size; our intention log is a local in-memory array and not distributed or shared across the network, drastically reducing the size of the conflict window; and checking for conflicts using read/write-sets is easy since our index is a simple address space.
If the outcome of the decision phase is commit, endTX proceeds to apply the transaction to the logical address space.
The first step in this process is to append the writes within the transaction to the persistent log.
This step can be executed in parallel for multiple transactions, as soon as each one's decision is known, since the existence and order of writes on the log signifies nothing: the multiversion index still points to older entries in the log.
The second step involves changing the multiversion index to point to the new entries.
Once the index has been changed, the transaction can be acknowledged and its effects are visible.One complication is that this protocol introduces a lost update anomaly.
Consider a transaction that reads a block (say an allocation bitmap in a filesystem), examines and changes the first bit, and writes it back.
A second transaction reads the same block concurrently, examines and changes the last bit, and writes it back.
Our conflict detection scheme will correctly allow both transactions to commit.
However, each transaction will write its own version of the 4KB bitmap, omitting the other's modification; as a result, the transaction with the higher timestamp will destroy the earlier transaction's modification.
To avoid such lost updates, the endTX call performs an additional step for each transaction before appending its buffered writes to the log.
Once it knows that the current transaction can commit, it scans the conflict window and merges updates made by prior committed transactions to the blocks in its write-set.
Isotope is implemented as an in-kernel software module in Linux 2.6.38; specifically, as a device mapper that exposes multiple physical block devices as a single virtual disk, at the same level of the stack as software RAID.
Below, we discuss the details of this implementation.
Log implementation: Isotope implements the log (i.e., E in Figure 3) over a conventional address space with a counter marking the tail (and additional bookkeeping information for garbage collection, which we discuss shortly).
From a correctness and functionality standpoint, Isotope is agnostic to how this address space is realized.
For good performance, it requires an implementation that works well for a logging workload where writes are concentrated at the tail, while reads and garbage collection can occur at random locations in the body.
A naive solution is to use a single physical disk (or a RAID-0 or RAID-10 array of disks), but garbage collection activity can hurt performance significantly by randomizing the disk arm.
Replacing the disks with SSDs increases the cost-to-capacity ratio of the array without entirely eliminating the performance problem [58].
As a result, we use a design where a log is chained across multiple disks or SSDs (similar to Gecko [56]).
Chaining the log across drives ensures that garbage collection -which occurs in the body of the log/chain -is separated from the first-class writes arriving at the tail drive of the log/chain.
In addition, a commodity SSD is used as a read cache with an affinity for the tail drive of the chain, preventing application reads from disrupting write sequentiality at the tail drive.
In essence, this design 'collars' the throughput of the log, pegging write throughput to the speed of a single drive, but simultaneously eliminating the throughput troughs caused by concurrent garbage collection and read activity.
Garbage collection (GC): Compared to conventional log-structured stores, GC is slightly complicated in Isotope by the need to maintain older versions of blocks.
Isotope tracks the oldest start timestamp across all ongoing transactions and makes a best-effort attempt to not garbage collect versions newer than this timestamp.
In the worst case, any non-current versions can be discarded without compromising safety, by first preemptively aborting any transactions reading from them.
The application can simply retry its transactions, obtaining a new, current snapshot.
This behavior is particularly useful for dealing with the effects of rogue transactions that are never terminated by the application.
The alternative, which we did not implement, is to set a flag that preserves a running transaction's snapshot by blocking new writes if the log runs out of space; this may be required if it's more important for a long-running transaction to finish (e.g., if it's a critical backup) than for the system to be online for writes.
Caching: The please cache call in Isotope allows the application to mark the blocks it wants cached in RAM.
To implement caching, Isotope annotates the multiversion index with pointers to cached copies of block versions.This call is merely a hint and provides no guarantees to the application.
In practice, our implementation uses a simple LRU scheme to cache a subset of the blocks if the application requests caching indiscriminately.
Index persistence: Thus far, we have described the multiversion index as an in-memory data structure pointing to entries on the log.
Changes to the index have to be made persistent so that the state of the system can be reconstructed on failures.
To obtain persistence and failure atomicity for these changes, we use a metadata log.
The size of this log can be limited via periodic checkpoints.A simple option is to store the metadata log on batterybacked RAM, or on newer technologies such as PCM or flash-backed RAM (e.g., Fusion-io's AutoCommit Memory [7]).
In the absence of special hardware on our experimental testbed, we instead used a commodity SSD.
Each transaction's description in the metadata log is quite compact (i.e., the logical block address and the physical log position of each write in it, and its commit timestamp).
To avoid the slowdown and flash wear-out induced by logging each transaction separately as a synchronous page write, we batch multiple committed transactions together [25], delaying the final step of modifying the multiversion index and acknowledging the transaction to the application.
We do not turn off the write cache on the SSD, relying on its ability to flush data on power failures using supercapacitors.
Memory overhead: A primary source of memory overhead in Isotope is the multiversion index.
A singleversion index that maps a 2TB logical address space to an 4TB physical address space can be implemented as a simple array that requires 2GB of RAM (i.e., half a billion 4-byte entries), which can be easily maintained in RAM on modern machines.
Associating each address with a version (without supporting access to prior versions) doubles the space requirement to 4GB (assuming 4-byte timestamps), which is still feasible.
However, multiversioned indices that allow access to past versions are more expensive, due to the fact that multiple versions need to be stored, and because a more complex data structure is required instead of an array with fixedsize values.
These concerns are mitigated by the fact that Isotope is not designed to be a fully-fledged multiversion store; it only stores versions from the recent past, corresponding to the snapshots seen by executing transactions.Accordingly, Isotope maintains a pair of indices: a single-version index in the form of a simple array and a multiversion index implemented as a hashtable.
Each entry in the single-version index either contains a valid physical address if the block has only one valid, non-GC'ed version, a null value if the block has never been written, or a constant indicating the existence of multiple versions.
If a transaction issues a read and encounters this constant, the multiversion index is consulted.
An ad-dress is moved from the single-version index to the multiversion index when it goes from having one version to two; it is moved back to the single-version index when its older version(s) are garbage collected (as described earlier in this section).
The multiversion index consists of a hashtable that maps each logical address to a linked list of its existing versions, in timestamp order.
Each entry contains forward and backward pointers, the logical address, the physical address, and the timestamp.
A transaction walks this linked list to find the entry with the highest timestamp less than its snapshot timestamp.
In addition, the entry also has a pointer to the in-memory cached copy, as described earlier.
If an address is cached, the first single-version index is marked as having multiple versions even if it does not, forcing the transaction to look at the hashtable index and encounter the cached copy.
In the future, we plan on applying recent work on compact, concurrent maps [28] to further reduce overhead.
Rogue Transactions: Another source of memory overhead in Isotope is the buffering of writes issued by inprogress transactions.
Each write adds an entry to the write-set of the transaction containing the 4KB payload and a 4K C bit bitmap, where C is the granularity of conflict detection (e.g., with 16-byte detection, the bitmap is 256 bits).
Rogue transactions that issue a large number of writes are a concern, especially since transactions can be exposed to end-user applications.
To handle this, Isotope provides a configuration parameter to set the maximum number of writes that can be issued by a transaction (set to 256 by default); beyond this, writes return an error code.
Another parameter sets the maximum number of outstanding transactions a single process can have inflight (also set to 256).
Accordingly, the maximum memory a rogue process can use within Isotope for buffered writes is roughly 256MB.
When a process is killed, its outstanding transactions are preemptively aborted.Despite these safeguards, it is still possible for Isotope to run out of memory if many processes are launched concurrently and each spams the system with rogue, never-ending transactions.
In the worst case, Isotope can always relieve memory pressure by preemptively aborting transactions.
Another option which we considered is to flush writes to disk before they are committed; since the metadata index does not point to them, they won't be visible to other transactions.
Given that the system is only expected to run out of memory in pathological cases where issuing I/O might worsen the situation, we didn't implement this scheme.Note that the in-memory array that Isotope uses for conflict detection is not a major source of memory overhead; pointers to transaction intention records are inserted into this array in timestamp order only after the application calls endTX, at which point it has relinquished control to Isotope and cannot prolong the transaction.
As a result, the lifetime of an entry in this array is short and limited to the duration of the endTX call.
To illustrate the usability and performance of Isotope, we built four applications using Isotope transactions: IsoHT, a key-value store built over a persistent hashtable; IsoBT, a key-value store built over a persistent B-tree; IsoFS, a user-space POSIX filesystem; and ImgStore, an image storage service that stores images in IsoHT, and a secondary index in IsoBT.
These applications implement each call in their respective public APIs by following a simple template that wraps the entire function in a single transaction, with a retry loop in case the transaction aborts due to a conflict (see Figure 2).
Library-based or 'embedded' key-value stores (such as LevelDB or Berkeley DB) are typically built over persistent, on-disk data structures.
We built two key-value stores called IsoHT and IsoBT, implemented over an on-disk hashtable and B-tree data structure, respectively.Both key-value stores support basic put/get operations on key-value pairs, while IsoBT additionally supports range queries.
Each API call is implemented via a single transaction of block reads and writes to an Isotope volume.
We implemented IsoHT and IsoBT in three stages.
First, we wrote code without Isotope transactions, using a global lock to guard the entire hashtable or B-tree.
The resulting key-value stores are functional but slow, since all accesses are serialized by the single lock.
Further, they do not provide failure atomicity: a crash in the middle of an operation can catastrophically violate data structure integrity.In the second stage, we simply replaced the acquisitions/releases on the global lock with Isotope beginTX/endTX/ abortTX calls, without changing the overall number of lines of code.
With this change, the key-value stores provide both fine-grained concurrency control (at block granularity) and failure atomicity.
Finally, we added optional mark accessed calls to obtain sub-block concurrency control, and please cache calls to cache the data structures (e.g., the nodes of the B-tree, but not the values pointed to by them).
Table 1 reports on the lines of code (LOC) counts at each stage for the two key-value stores.
IsoFS is a simple user-level filesystem built over Isotope accessible via FUSE [2], comprising 1K lines of C code.
Its on-disk layout consists of distinct regions for storing inodes, data, and an allocation bitmap for each.
Each inode has an indirect pointer and a double indirect pointer, both of which point to pages allocated from the data region.
Each filesystem call (e.g., setattr, lookup, or unlink) uses a single transaction to access and modify multiple blocks.
The only functionality implemented by IsoFS is the mapping and allocation of files and directories to blocks; atomicity, isolation, and durability are handled by Isotope.IsoFS is stateless, caching neither data nor metadata across filesystem calls (i.e., across different transactions).
Instead, IsoFS tells Isotope which blocks to cache in RAM.
This idiom turned out to be surprisingly easy to use in the context of a filesystem; we ask Isotope to cache all bitmap blocks on startup, each inode block when an inode within it is allocated, and each data block that's allocated as an indirect or double indirect block.
Like the key-value stores, IsoFS was implemented in three stages and required few extra lines of code to go from a global lock to using the Isotope API (see Table 1).
IsoFS trivially exposes transactions to end applications over files and directories.
For example, a user might create a directory, move a file into it, edit the file, and rename the directory, only to abort the entire transactions and revert the filesystem to its earlier state.
One implementation-related caveat is that we were unable to expose transactions to end applications of IsoFS via the FUSE interface, since FUSE decouples application threading from filesystem threading and does not provide any facility for explicitly transferring a transaction handle on each call.
Accordingly, we can only expose transactions to the end application if IsoFS is used directly as a library within the application's process.
Composability: As we stated earlier, Isotope-based storage systems are trivially composable: a single transaction can encapsulate calls to IsoFS, IsoHT and IsoBT.
To demonstrate the power of such composability, we built ImgStore, the image storage application described in Section 3.
ImgStore stores images in IsoHT, using 64-bit IDs as keys.
It then stores a secondary index in IsoBT, mapping dates to IDs.
The implementation of ImgStore is trivially simple: to add an image, it creates a transaction to put the image in IsoHT, and then updates the secondary index in IsoBT.
The result is a storage system that -in just 148 LOC -provides hashtable-like performance for gets while supporting range queries.
Isolation Levels: Isotope provides both strict serializability and snapshot isolation; our expectation was that developers would find it difficult to deal with the semantics of the latter.
However, our experience with IsoFS, IsoHT and IsoBT showed otherwise.
Snapshot isolation provides better performance than strict serializability, but introduces the write skew anomaly [16]: if two concurrent transactions read two blocks and each updates one of the blocks (but not the same one), they will both commit despite not being serializable in any order.
The write skew anomaly is problematic for applications if a transaction is expected to maintain an integrity constraint that includes some block it does not write to (e.g., if the two blocks in the example have to sum to less than some constant).
In the case of the storage systems we built, we did not encounter these kinds of constraints; for instance, no particular constraint holds between different bits on an allocation map.
As a result, we found it relatively easy to reason about and rule out the write skew anomaly.
Randomization: Our initial implementations exhibited a high abort rate due to deterministic behavior across different transactions.
For example, a simple algorithm for allocating a free page involved getting the first free bit from the allocation bitmap; as a result, multiple concurrent transactions interfered with each other by trying to allocate the same page.
To reduce the abort rate, it was sufficient to remove the determinism in simple ways; for example, we assigned each thread a random start offset into the allocation bitmap.
We evaluate Isotope on a machine with an Intel Xeon CPU with 24 hyper-threaded cores, 24GB RAM, three 10K RPM disks of 600GB each, an 128GB SSD for the OS and two other 240GB SSDs with SATA interfaces.
In the following experiments, we used two primary configurations for Isotope's persistent log: a three-disk chained logging instance with a 32GB SSD read cache in front, and a 2-SSD chained logging instance.
In some of the experiments, we compare against conventional systems running over RAID-0 configurations of 3 disks and 2 SSDs, respectively.
In the chained logging configurations, all writes are logged to the single tail drive, while reads are mostly served by the other drives (and the SSD read cache for the disk-based configuration).
The performance of this logging design under various workloads and during GC activity has been documented in [56].
In all our experiments, GC is running in the background and issuing I/Os to the drives in the body of the chain to compact segments, without disrupting the tail drive.Our evaluation consists of two parts.
First, we focus on the performance and overhead of Isotope, showing that it exploits fine-grained concurrency in workloads and provides high, stable throughput.
Second, we show that Isotope applications -in addition to being simple and robust -are fast, efficient, and composable into larger applications.
Figure 5: Without fine-grained conflict detection, Isotope performs well under low contention workloads.
To understand how Isotope performs depending on the concurrency present in the workload, we implemented a synthetic benchmark.
The benchmark executes a simple type of transaction that reads three randomly chosen blocks, modifies a random 16-byte segment within each block (aligned on a 16-byte boundary), and writes them back.
This benchmark performs identically with strict serializability and snapshot isolation, since the read-set exactly matches the write-set.
In the following experiments, we executed 64 instances of the micro benchmark concurrently, varying the size of the address space accessed by the instances to vary contention.
The blocks are chosen from a specific prefix of the address space, which is a parameter to the benchmark; the longer this prefix, the bigger the fraction of the address space accessed by the benchmark, and the less skewed the workload.
The two key metrics of interest are transaction goodput (measured as the number of successfully committed transactions per second, as well as the total number of bytes read or written per second by these transactions) and overall transaction throughput; their ratio is the commit rate of the system.
Each data point in the following graphs is averaged across three runs; in all cases, the minimum and the maximum run were within 10% of the average.
Figure 5 shows the performance of this benchmark against Isotope without fine-grained conflict detection; i.e., the benchmark does not issue mark accessed calls for the 16-byte segments it modifies.
On the x-axis, we increase the fraction of the address space accessed by the benchmark.
On the left y axis, we plot the rate at which data is read and written by transactions; on the right y-axis, we plot the number of transactions/sec.
On both disk and SSD, transactional contention cripples performance on the left part of the graph: even though the benchmark attempts to commit thousands of transactions/sec, all of them access a small number of blocks, leading to low goodput.
Note that overall transaction throughput is very high when the commit rate is low: aborts are cheap and do not result in storage I/O.
Conversely, disk contention hurts performance on the right side of Figure 5-Left: since the blocks read by each transaction are distributed widely across the address space, the 32GB SSD read cache is ineffective in serving reads and the disk arm is randomized and seeking constantly.
As a result, the system provides very few transactions per second (though with a high commit rate).
In the middle of the graph is a sweet spot where Isotope saturates the disk at roughly 120 MB/s of writes, where the blocks accessed are concentrated enough for reads to be cacheable in the SSD (which supplies 120 MB/s of reads, or 30K 4KB IOPS), while distributed enough for writes to not trigger frequent conflicts.We can improve performance on the left side of the graphs in Figure 5 via fine-grained conflict detection.
In Figure 6, the benchmark issues mark accessed calls to tell Isotope which 16-byte fragment it is modifying.
The result is high, stable goodput even when all transactions are accessing a small number of blocks, since there is enough fragment-level concurrency in the system to ensure a high commit rate.
Using the same experiment but with smaller and larger data access and conflict detection granularities than 16 bytes showed similar trends.
Isotope's conflict detection was not CPU-intensive: we observed an average CPU utilization of 5.96% without fine-grained conflict detection, and 6.17% with it.
As described earlier, we implemented two key-value stores over Isotope: IsoHT using a hashtable index and IsoBT using a B-tree index, respectively.
IsoBT exposes a fully functional LevelDB API to end applications; IsoHT does the same minus range queries.
To evaluate these systems, we used the LevelDB benchmark [5] as well as the YCSB [21] benchmark.
We ran the fillrandom, read-random, and delete-random workloads of the LevelDB benchmark and YCSB workload-A traces (50% reads and 50% updates following a zipf distribution on keys).
All these experiments are on the 2-SSD configuration of Isotope.
For comparison, we ran LevelDB on a RAID-0 array of the two SSDs, in both synchronous mode ('LvlDB-s') and asynchronous mode ('LvlDB').
LevelDB was set to use no compression and the default write cache size of 8MB.
For all the workloads, we used a value size of 8KB and varied the number of threads issuing requests from 4 to 128.
Results with different value sizes (from 4KB to 32KB) showed similar trends.For operations involving writes (Figure 7-(a), (c), and (d)), IsoHT and IsoBT goodput increases with the num- ber of threads, but dips slightly beyond 64 threads due to an increased transaction conflict rate.
For the read workload ( Figure 7-(b)), throughput increases until the underlying SSDs are saturated.
Overall, IsoHT has higher goodput than IsoBT, since it touches fewer metadata blocks per operation.
We ran these experiments with Isotope providing snapshot isolation, since it performed better for certain workloads and gave sufficiently strong semantics for building the key-value stores.
With strict serializability, for instance, the fill workload showed nearly identical performance, whereas the delete workload ran up to 25% slower.
LevelDB's performance is low for fill operations due to sorting and multi-level merging (Figure 7-(a)), and its read performance degrades as the number of concurrent threads increases because of the CPU contention in the skip list, cache thrashing, and internal merging operations (Figure 7-(b)).
Still, LevelDB's delete is very efficient because it only involves appending a small delete intention record to a log, whereas IsoBT/IsoHT has to update a full 4KB block per delete (Figure 7-(c)).
The point of this experiment is not to show Iso-HT/IsoBT is better than LevelDB, which has a different internal design and is optimized for specific workloads such as sequential reads and bulk writes.
Rather, it shows that systems built over Isotope with little effort can provide equivalent or better performance than an existing system that implements its own concurrency control and failure atomicity logic.
To evaluate the composability of Isotope-based storage systems, we ran the YCSB workload on ImgStore, our image storage application built over IsoHT and IsoBT.
In our experiments, ImgStore transactionally stored a 16KB payload (corresponding to an image) in IsoHT and a small date-to-ID mapping in IsoBT.
To capture the var- ious ways in which Isotope storage systems can be composed (see Section 3), we implemented several versions of ImgStore: cross-library, where ImgStore accesses the two key-value stores as in-process libraries, with each transaction executing within a single user-space thread; cross-thread, where ImgStore accesses each key-value store using a separate thread, and requires transactions to execute across them; and cross-process, where each key-value store executes within its own process and is accessed by ImgStore via socket-based IPC.
Figure 8 shows the resulting performance for all three versions.
It shows that the cost of the extra takeoverTX/releaseTX calls required for cross-thread transactions is negligible.
As one might expect, cross-process transactions are slower due to the extra IPC overhead.
Additionally, ImgStore exhibits less concurrency than IsoHT or IsoBT (peaking at 32 threads), since each composite transaction conflicts if either of its constituent transactions conflict.
Next, we compare the end-to-end performance of IsoFS running over Isotope using the IOZone [4] write/rewrite benchmark with 8 threads.
Each thread writes to its own file using a 16KB record size until the file size reaches 256MB; it then rewrites the entire file sequentially; and then rewrites it randomly.
We ran this workload against IsoFS running over Isotope, which converted each 16KB write into a transaction involving four 4KB Isotope writes, along with metadata writes.
We also ran ext2 and ext3 over Isotope; these issued solitary, nontransactional reads and writes, which were interpreted by Isotope as singleton transactions (in effect, Isotope operated as a conventional log-structured block store, so that ext2 and ext3 are not penalized for random I/Os).
We ran ext3 in 'ordered' mode, where metadata is journaled but file contents are not.
Figure 9 plots the throughput observed by IOZone: on disk, IsoFS matches or slightly outperforms ext2 and ext3, saturating the tail disk on the chain.
On SSD, IsoFS is faster than ext2 and ext3 for initial writes, but is bottlenecked by FUSE on rewrites.
When we ran IsoFS directly using a user-space benchmark that mimics IOZone ('IsoFS-lib'), throughput improved to over 415MB/s.
A : IOZone over IsoFS and ext2/ext3.
secondary point made by this graph is that Isotope does not slow down applications that do not use its transactional features (the high performance is mainly due to the underlying logging scheme, but ext2 and ext3 still saturate disk and SSD for rewrites), satisfying a key condition for pushing functionality down the stack [49].
The idea of transactional atomicity for multi-block writes was first proposed in Mime [18], a log-structured storage system that provided atomic multi-sector writes.
Over the years, multiple other projects have proposed blocklevel or page-level atomicity: the Logical Disk [24] in 1993, Stasis [54] in 2006, TxFlash [46] in 2008, and MARS [19] in 2013.
RVM [51] and Rio Vista [36] proposed atomicity over a persistent memory abstraction.
All these systems explicitly stopped short of providing full transactional semantics, relying on higher layers to implement isolation.
To the best of our knowledge, no existing single-machine system has implemented transactional isolation at the block level, or indeed any concurrency control guarantee beyond linearizability.On the other hand, distributed filesystems have often relied on the underlying storage layer to provide concurrency control.
Boxwood [37], Sinfonia [12], and CalvinFS [62] presented simple NFS designs that leveraged transactions over distributed implementations of high-level data structures and a shared address space.
Transaction isolation has been proposed for shared block storage accessed over a network [13] and for key-value stores [60].
Isotope can be viewed as an extension of similar ideas to single-machine, multi-core systems that does not require consensus or distributed rollback protocols.
Our single-machine IsoFS implementation has much in common with the Boxwood, Sinfonia, and CalvinFS NFS implementations which ran against clusters of storage servers.Isotope also fits into a larger body of work on smart single-machine block devices, starting with Loge [27] and including HP AutoRAID [65].
Some of this work has focused on making block devices smarter without changing the interface [57], while other work has looked at augmenting the block interface [18,64,30], modifying it [67], and even replacing it with an object-based interface [38].
In a distributed context, Parallax [39] and Strata [23] provide virtual disks on storage clusters.
A number of filesystems are multiversion, starting with WAFL [33], and including many others [50,41,22].
Underlying these systems is research on multiversion data structures [26].
Less common are multiversion block stores such as Clotho [29] and Venti [47].
A number of filesystems have been built over a full-fledged database.
Inversion [43] is a conventional filesystem built over the POSTGRES database, while Amino [66] is a transactional filesystem (i.e., exposing transactions to users) built over Berkeley DB.
WinFS [10] was built over a relational engine derived from SQL Server.
This route requires storage system developers to adopt a complex interface -one that does not match or expose the underlying grain of the hardwarein order to obtain benefits such as isolation and atomicity.
In contrast, Isotope retains the simple block storage interface while providing isolation and atomicity.TxOS [45] is a transactional operating system that provides ACID semantics over syscalls, include file accesses.
In contrast, Isotope is largely OS-agnostic and can be ported easily to commodity operating systems, or even implemented under the OS as a hardware device.
In addition, Isotope supports the easy creation of new systems such as key-value stores and filesystems that run directly over block storage.Isotope is also related to the large body of work on software transactional memory (STM) [55,32] systems, which typically provide isolation but not durability or atomicity.
Recent work has leveraged new NVRAM technologies to add durability to the STM abstraction: Mnemosyne [63] and NV-Heaps [20] with PCM and Hathi [52] with commodity SSDs.
In contrast, Isotope aims for transactional secondary storage, rather than transactional main-memory.
We described Isotope, a transactional block store that provides isolation in addition to atomicity and durability.
We showed that isolation can be implemented efficiently within the block layer, leveraging the inherent multi-versioning of log-structured block stores and application-provided hints for fine-grained conflict detection.
Isotope-based systems are simple and fast, while obtaining database-strength guarantees on failure atomicity, durability, and consistency.
They are also composable, allowing application-initiated transactions to span multiple storage systems and different abstractions such as files and key-value pairs.
This work is partially funded and supported by a SLOAN Research Fellowship received by Hakim Weatherspoon, DARPA MRC and CSSG (D11AP00266) and NSF (1422544,1053757,0424422,1151268,1047540).
We would like to thank our shepherd, Sage Weil, and the anonymous reviewers for their comments.
