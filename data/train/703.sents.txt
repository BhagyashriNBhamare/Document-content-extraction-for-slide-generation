Searching and mining large graphs today is critical to a variety of application domains, ranging from community detection in social networks, to de novo genome sequence assembly.
Scalable processing of large graphs requires careful partitioning and distribution of graphs across clusters.
In this paper, we investigate the problem of managing large-scale graphs in clusters and study access characteristics of local graph queries such as breadth-first search, random walk, and SPARQL queries, which are popular in real applications.
These queries exhibit strong access locality, and therefore require specific data partitioning strategies.
In this work, we propose a Self Evolving Distributed Graph Management Environment (Sedge), to minimize inter-machine communication during graph query processing in multiple machines.
In order to improve query response time and throughput, Sedge introduces a two-level partition management architecture with complimentary primary partitions and dynamic secondary partitions.
These two kinds of partitions are able to adapt in real time to changes in query workload.
Sedge also includes a set of workload analyzing algorithms whose time complexity is linear or sublinear to graph size.
Empirical results show that it significantly improves distributed graph processing on today's commodity clusters.
Large scale, highly interconnected networks pervade both our society and the information world around us [24,30].
Online social networks capture complex relationships among millions of users.
HTTP links connect billions of documents on the Web.
Synthesized graphs are available from genome sequence alignment and program traces.
The graphs of interest are often massive with millions, even billions of vertices, making common graph operations computationally intensive.
In the presence of data objects associated with vertices, it is clear that graph data can easily scale up to terabytes in size.
Moreover, with the advance of the Semantic Web, efficient management of massive RDF data is becoming increasingly important as Semantic Web technology is applied to real-world applications [1,3].
The recent Linked Open Data project has published more then 20 billion RDF triples [15].
Although the RDF data is generally represented in triples, the data inherently presents graph structure and is therefore interlinked.
Not surprisingly, the scale and the flexibility rise to the major challenges to the RDF graph management.The massive scale of graph data easily overwhelms memory and computation resources on commodity servers.
Yet online services must answer user queries on these graphs in near real time.
In these cases, achieving fast query response time and high throughput requires partitioning/distributing and parallel processing of graph data across large clusters of servers.
An appealing solution is to divide a graph into smaller partitions that have minimum connections between them, as adopted by Pregel [28] and SPAR [33].
As long as the graph is clustered to similar-size partitions, the workload of machines holding these partitions will be quite balanced.
However, the assumption becomes invalid for local graph queries when they are concentrated on a subset of vertices (hotspots), e.g., find/aggregate the attributes of h-hop neighbors around a vertex, calculate personalized PageRank [19], perform a random walk starting at a vertex, and calculate hitting time.
When these queries are not uniformly distributed or hitting partition boundaries, we will either have an imbalance of workload or intensive inter-machine communications.
A good graph partition management policy should consider these situations and adapt dynamically to changing workload.There could be three kinds of query workload in graphs.
For random access or complete traversal of an entire graph shown in Figure 1(a), a static balanced partition scheme might be the best solution.
For queries whose access is bounded by partition boundaries, as shown in Figure 1 they shall be served efficiently by the balanced partition scheme too.
However, if there are many graph queries crossing the partition boundaries shown in Figure 1(c), the static partition scheme might fail due to inter-machine communications.
One partition scheme cannot fit all.
Instead, one shall generate multiple partitions with complementary boundaries or new partitions on-the-fly so that these queries can be answered efficiently.
Graph partitioning is a hard and old problem, which has been extensively studied in various communities since 1970s [23,17,32,21].
Graph partitioning is also widely used in parallel computing (e.g., [16]).
The best approaches often depend on the properties of the graphs and the structure of the access patterns.
Much of the previous work has focused on graphs arising from scientific applications (meshes [13], etc) that have a different structure than social networks and RDFs focused in this study, where well-defined partitions often do not exist [26].
In this study, our focus is not to design new graph partition algorithms, but to adjust partitions to serve queries efficiently.
We design a Self Evolving Distributed Graph Management Environment (Sedge).
While Sedge adopts the same computation model and programming APIs of Pregel [28], it emphasizes graph partition management, which is the key to query performance.
It adds important functions to support overlapping partitions, with the goal of minimizing inter-machine communication and increasing parallelism by dynamically adapting graph partitions to query workload change.Our Contributions.
A major contribution of this study is an examination of an increasingly important data management problem in large-scale graphs and the proposal of a graph partition management strategy that supports overlapping partitions and replicates for fast graph query processing.
Dynamic graph partitioning and overlap graph partitioning were widely investigated before (e.g., [37]).
However, few methods study how to adapt partitions to satisfy dynamic query workload in social and information networks.
We addressed this issue and proposed Sedge, a workload driven method to manage partitions in large graphs.
We eliminate a constraint in Pregel [28] that does not allow duplicate vertices in partitions.
This constraint makes it difficult to handle skewed query workload.
It is able to replicate some regions of a graph and distribute them in multiple machines to serve queries in parallel.
For this goal, we develop three techniques in Sedge: (1) Complementary Partitioning; (2) Partition Replication; and (3) Dynamic Partitioning.
Complementary Partitioning is to find multiple partition schemes such that their partition boundaries are different from one another.
Partition replication is to replicate the same partitions in multiple machines to share the workload on these partitions.
Dynamic Partitioning is to construct new partitions to serve cross-partition queries locally.
In order to perform dynamic partitioning efficiently, we propose an innovative technique to profile graph queries.
As manifested later, it is too expensive to log all of the vertices accessed by each query.
We introduced the concept of color-blocks and coverage envelope to bound the portion of a graph that has been accessed by a query.
An efficient algorithm to merging these envelopes to formulate new partitions is thus developed.
The partition replication and dynamic partitioning are together termed on-demand partitioning since the two techniques are primarily employed during the runtime of the system to adapt evolving queries.
Additionally, a two-level partition architecture is developed to connect newly generated partitions with primary partitions.We implement Sedge based on Pregel.
However, the concepts proposed and verified in this work are also valid to other systems.
The performance of Sedge is validated with several large graph datasets as well as a public SPARQL performance benchmark.
The experimental results show that the proposed partitioning approaches significantly outperform the existing approach and demonstrate superior scaling properties.
Graph partitioning is an important problem with extensive applications in many areas, including circuit placement, parallel computing and scientific simulation.
Large-scale graph partitioning tools are available, e.g. METIS [21], Chaco [17], and SCOTCH [32], just to name a few.
This study is not to propose a new graph partitioning algorithm.
Instead, it is focused on a workload driven method to manage partitions in large graphs.Distributed memory systems in super-computing is able to process large-scale linked data, e.g., [22,29].
These systems could map shared data into the address space of multiple processors.
They are usually very general, supporting random memory access that has less locality than the graph queries introduced in this work, thus could not benefit from query locality.
Malewicz et al. [28] introduced Pregel, which could run graph algorithms in a distributed and faulttolerant manner.
Logothetis et al. [27] introduced a generalized architecture for continuous bulk processing (CBP) that is good for building incremental applications in large datasets including graphs.
Najork proposed the scalable hyperlink store, SHS [29].
SHS studied several key issues in large graph processing: real-time response, graph compression, fault tolerance, etc.
Our study touches another aspect on managing partitions to fit workload changes.
Kang et al.[20] developed a peta-scale graph mining system, PEGA-SUS, built on the top of the Hadoop platform.
PEGASUS proposed and optimized iterative matrix-vector multiplication operators.
The difference between Pregel and MapReduce can be referred to [28].
In this work, we implement and leverage the computing environment provided by Pregel, but focus on graph partition management, not optimization techniques for specific algorithms.
COSI [6] is a framework that is able to partition very large social networks according to query history.
Such work is optimized for static query workload and hence cannot be readily applied to dynamic query workload.
Pujol et al. [33] developed a social partitioning and replication middle-ware, SPAR, to achieve data locality while minimizing replication.
SPAR aims to opti- Figure 2: Sedge: System Architecture mize performance based on social network structures, e.g., communities, while our system develops partitioning techniques that adapt to query workload change.
As discussed before, network structures might not reflect actual query workload.
In addition to in-memory solutions, Nodine et al.[31] considered the problem of using disk blocks efficiently in searching graphs that are too large to fit in memory.
The idea of using redundant blocks is related to complementary partitioning proposed in Sedge.Distributed query processing has also been studied on semistructured data [36,8], relational data [11] and RDF [18].
The key technique is minimizing data movement by partial evaluation, hybrid shipping, two-phase optimization and replication (see [25] for a survey).
Additionally, as the emerging of Semantic Web, more and more data sources on the Web are organized in the RDF model and linked together.
With the observation of the heterogeneity and scalability challenges existing in the management of RDF data, innovative data schemas have been proposed.
One of the widely used techniques has been termed the property table [7,38].
The technique is to cluster subjects sharing similar properties/predicates.
Another technique, vertical table [1], is to vertically partition the schemas on property value.
Efficient RDF data management is still an open problem and has not been addressed thoroughly.
Many applications [33,11] employ graph partitioning methods for distributed processing.
Unfortunately, real life networks such as social networks might not have well-defined clusters [26], indicating that many cross-partition edges could exist for any kind of balanced partitions.
For queries that visit these edges, the inter-machine communication latency will affect query response time significantly.
To alleviate this problem, we propose Sedge, which is based on multi partition sets (Figure 2).
Sedge is designed to eliminate the inter-machine communication as much as possible.
As shown in Figure 2, the offline part first partitions the input graph in a distributed manner and distributes them to multiple workers.
It creates multiple partition sets so that each set runs independently.
Pregel [28] is a scalable distributed graph processing framework that works in a bulk synchronous mode.
Pregel is used as a computing platform that is able to execute local graph queries.
There are various kinds of local graph queries including breadth-first search, random walk, and SPARQL queries.
Unlike many graph algorithms, a local query usually starts at one vertex and only involves a limited number of vertices (termed active vertice).
In each iteration, a Pregel instance only accesses active vertices, thus eliminating many synchronous steps.
Section 6 will discuss synchronization for the queries with writes and updates.
The online part collects statistical information from workers and actively generates and removes partitions to accommodate the changing workload.
Therefore the set of online techniques built in Sedge must be very efficient to minimize overhead.
Our study is focused on partition management.
For fault-tolerance and live partition migration with ACID properties, detailed explorations of these issues are given in [28,12] and similar techniques can be applied here.
In the following discussion, we overview major components including complementary partitioning, on-demand partitioning, the mechanism to connect primary and secondary partitions, the meta-data to facilitate query routing and performance optimizer.
Definition 1 (Graph Partitioning).
Given a graph G = (V, E), graph partitioning, C, is to divide V into partitions {P1, P2, . . . , Pn} such that ∪iPi = V , and Pi ∩ Pj = ∅ for any i = j.
The edge cut set Ec is the set of edges whose vertices belong to different partitions.Graph partitioning needs to achieve dual goals.
On the one hand, in order to achieve the minimum response time, the best partitioning strategy is to split the graph using the minimum cut.
On the other hand, taking the system throughput into consideration, the partitions should be as balanced as possible.
This is exactly what the normalized cut algorithm can do [21].
Techniques derived from graph compression, e.g., [?]
can also be applied here.
However, partitioning a graph using a random hash function might not work very well.Complementary Partitioning is to repartition a graph such that the original cross-partition edges become internal ones.
Figure 3(b) shows an example of complementary graph partitions of Figure 3(a).
In the new partition set, the queries (shaded area R) on original cross-partition edge, e, will be served within the same partition.
Therefore, the new partition set can handle graph queries that have trouble in the original partition set.
If there is room to hold both S1 and S2 in clusters, for a query Q visiting the shaded area R in S1, the system shall route it to S2 to eliminate communication cost.
Meanwhile, the new partition set can also share the workload with original partition set.
This complementary partitioning idea can be applied multiple times to generate a series of partition sets.
We call each partition set a "primary partition set."
Each primary partition set is self complete, where a Pregel instance can run independently.
Primary partition set can serve queries that are uniformly distributed in the graph.
However, they are not good at dealing with unbalanced query workload: queries that are concentrated in one part of the graph.
It will be necessary to either create a replicated partition (Figure 4(a)) or generate a new overlapping partition (Figure 4(b)) in an idle machine so that the workload can be shared appropriately.
This strategy, called On-demand Partitioning, will generate new partitions online.
These add-on partitions, called "secondary partitions", could last until their corresponding workload diminishes.
Given many primary/secondary partitions, it is natural to inquire how to manage these partitions.
Here we propose the concept of Two-Level Partition Management.
Fig- ure 4 depicts one example, where there are intensive workloads on two shaded areas.
Based on a primary partition set, {A, B, C, D}, two secondary partitions, B and E, are created to share the unbalanced workload on primary partitions.
Since the vertices in secondary partitions are the duplicates of vertices in primary partitions, some of the vertices might connect to the vertices in primary partitions.
Therefore it is necessary to maintain the linkage between vertices in secondary partitions and those in primary partitions.
In our design, the linkage is only recorded in secondary partitions.
It is not necessary to maintain such links in primary partitions.
For example, for partition B , it has to maintain the linkage to A and C.
While for A and C, they only maintain links to B, but not to B .
During the runtime, each primary partition set and the corresponding secondary partitions are maintained by a Pregel instance that is running on a set of worker machines as indicated in Figure 2.
Multiple isolated independent Pregel instances are coordinated by meta-data management.
Meta-data is maintained by both the master and the Pregel instances.
As in Figure 2, the meta-data manager in the master node maintains the information about each live Pregel instance and a fine-grained table mapping vertices to the Pregel instances.
An index mapping vertices to partitions is also maintained by each live Pregel instance.
This two-level indexing strategy is used to facilitate fast query routing.
Specifically, when a query is issued to the system, the routing component first checks the vertex table maintained by the master.
The index entry maps the vertex id to the Pregel instance which can most efficiently execute the query.
After the query is routed to a particular Pregel instance, it is the duty of the vertex index maintained by the Pregel instance to decide to which partition the query should be forwarded.
The detailed techniques of indexing vertices and routing queries will be discussed in Section 6.
In order to facilitate different kinds of queries, in addition to vertex index, it is desirable to design indices for the attributes of vertices and edges.
Efficient decentralized/distributed indexing techniques, such as [35], have come to the fore in recent years.
However, this topic is beyond the scope of this work.
The Performance Optimizer continuously collects runtime information from all the Pregel instances via daemon processes and characterizes the execution of the query workload, such as vertex access times of each partition, and the number of cross-machine messages/queries.
The optimizer can update the meta-data maintained by the master and evoke on-demand partitioning routine as the workload varies.
It is notable that although we depict the on-demand partitioning as a component on the master side in Figure 2, the routine is actually executed by the Pregel instance on the worker side in a distributed manner.
Therefore the overhead of on-demand partitioning will be isolated and not affect the performance of other Pregel instances.
Complementary partitioning is to find multiple partition sets such that their partition boundaries do not overlap.
Formally, we define the problem as:Given a partition set {P1, P2, ..., P k } on G and the cut edges Ec = {e1, e2, ..., ei}.
The problem is to partition G into a new partition set {P 1 , P 2 , ..., P k } satisfying the same partitioning criteria (e.g., minimum cut) such that the new cut edges do not overlap with Ec.If we want to exclude more edges, Ec could be expanded to include edges near the original cut edges.
Without loss of generality, we assume G is an undirected graph with unit edge weight.
X is an n × k matrix, defined as follows,xij = 1 vi ∈ V (Pj), 0 otherwise.X gives a k-partition set of G. Furthermore, we define the following constraints on X: (1) full coverage and disjoint: X1 = 1, where 1 is a all-ones vector with appropriate size; (2) balance:X T 1 ≤ m, where mi = (1 + σ) n k. mi is a rough bound of partition size; σ controls the size balance.
(3) edge constraint: tr X T WX = 0, where W = (wij) is defined as an edge restrictive n × n Laplacian matrix.
Given the edge set Ec, if eij ∈ Ec, wij = −1, otherwise wij = 0.
Additionally, wii = − j =i wij.
The complementary partitioning problem can be described below:minimize 1 2 tr X T LX (1) s.t. X is binary X1 = 1, X T 1 ≤ m tr X T WX = 0where L = (lij) is a n × n Laplacian matrix.
By definition, if eij ∈ E(G), lij = −1, otherwise lij = 0 and lii = − j =i lij.
The objective function gives the overall cost of the cut edges with respect to a particular assignment of X.The above problem is a nonconvex quadratically constrained quadratic integer program (QCQIP ).
We rewrite the problem formulation so that we can reuse the existing balanced partitioning algorithms:minimize tr X T (L + λW)X (2) s.t.X is binaryX1 = 1, X T 1 ≤ mThis new definition drops edge constraint in (1) and incorporate it into the objective function using a weighting factor λ on the cut edges.
By changing the value of λ, we are able to control the overlap of the existing edge cut and the new edge cut generated by the complementary partition set.
It also provides a scalable solution: Given the cut edges of the existing partition sets, we increase their weight by λ and then run balanced partitioning algorithms such as METIS [21] to perform graph partitioning.
The value of λ plays a critical role.
Let the edge cut of the complementary partition set be E c .
If its value is small, the partitioning algorithm can not distinct the cut edges with the others.
On the other hand, if the value is too large, the algorithm might have to cut significantly more edges in order to completely avoid the existing edge cut.
That is, E c might be much larger than Ec, which is not good too.
In our implementation, we set λ = 2 k and experiment different k with a set of simulated graph queries.
For each k, we check the ratio β =|E c |−|Ec| |Ec|.
It was observed that when k = 4 and β ≤ 0.1, the obtained partition set can achieve good performance.Another possible technique for complementary partitioning is to delete all the edges in Ec first and then run classic partitioning algorithm.
We argue that this approach doesn't work since (1) edge deletion destroys the structure of the graph, and thus the new result may probably not reflect the real connections among the graph partitions; (2) in order to preserve a good partition schema, i.e., minimum cut, in complementary partitioning, some of the edges should be included in the edge cut repeatedly.The heuristic algorithm can be applied multiple times to generate a series of complementary partition sets, each of which try to partition the graph such that the boundary edges in one partition set will be internal edges in another partition set.
With multiple partition sets, for each vertex u, there could be several partitions P1, P2, . . . , P l to handle queries submitted to u. Queries should be routed to a partition where u is far away from partition boundaries.
We define such a partition as a safe partition for vertex u.
As soon as a new complementary partition set is generated, we can obtain the safe partitions for the vertices, especially those on the boundary of the original partitions.Remark.
There are some extreme cases, e.g., complete graph, where no complementary partition schema exists.
However, for large graphs with small dense substructures, we can continuously perform complementary partitioning.
In reality, due to space limitation, we can only afford a few sets of complementary partitions, and resort to on-demand partitioning algorithms to handle skewed query workloads that target some hotspots.
In the processing of many graph queries, primary partitions could have hotspots that are frequently visited.
The queries heading to these partitions will suffer longer response time.
There are two kinds of query hotspots: (1) internal hotspots that are located in one partition; (2) cross-partition hotspots that are on the boundary of multiple partitions.
We developed two partitioning techniques, partition replication and dynamic partitioning, to generate secondary partitions on demand to handle hotspots.
Definition 2 (Partition Workload).
Given a graph G, a partition P ⊆ G, and a query set Q = {q1, q2, . . . , qm}, the query set of P , written W (P ), is the queries that have accessed at least one vertex in P .
The internal query set of P , written Wint(P ), is the set of queries that only accessed vertices in P .
The external (cross-partition) query set of P , written Wext(P ), is equal to W (P ) − Wint(P ).
Given a partition P , when its internal workload (Wint(P )) becomes intensive, it will saturate the CPU cycles of the machine that holds P .
One natural solution is to replicate P to P .
If there is an idle machine with free memory space, Sedge will send P to that machine.
Otherwise, it will find a slack partition and replace it with P .
A slack partition is a secondary partition with low query workload on it.
By routing queries to P , the workload on P could be reduced.
When cross-partition hotspots exist, primary partitions have to communicate with each other frequently to answer cross-partition queries.
Instead of replicating multiple partitions, it is better to generate new partitions that only cover cross-partition hotspots.
The new partitions will not only share heavy workload, but also reduce communication overhead, thus improving query response time.Hotspot Analysis.
Before assembling a new partition, we need to find cross-partition hotspots first.
Given a partition, we calculate a ratio r = |W ext (P )| |W int (P )|+|W ext (P )| and resort to a hypothesis testing method to detect abnormal crosspartition query workload.If a query is uniformly and randomly distributed over a partition P , we can calculate the probability of observing a cross-partition query in P by either doing a simulation or approximating it using the following external edge ratio,p = |E ext (P )| |E int (P )|+|E ext (P )|, where |Eext(P )| is the number of cross-partition edges between P and other partitions, and |Eint(P )| is the number of internal edges.
If r is significantly higher than p, it could be reasonably assumed that there are cross-partition hotspots in P .
Let n = |Wint(P )|+|Wext(P )| and k = |Wext(P )|.
The chance to have ≥ k cross-partition queries isP r(x ≥ k) = n i=k n i p i (1 − p) n−i .
When P r(x ≥ k) is very small (e.g., 0.01), it means there is an abnormal large number of cross-partition queries in P .
Besides detecting cross-partition hotspots, we need a method to track the trail of cross-partition queries and pack them to form a new partition.
It is intuitive to record each query in the form of its exact search path.
However, it is not only space and time consuming for profiling, but also difficult to generalize.
Instead we mark the search path of a crosspartition query with coarse-granularity units, color-blocks.
A color-block is a set of vertices Vi ⊂ V where they are assigned with a unique color ci.
For any vertex v ∈ V , it has one and only one color.
Using color-blocks, we are able to coarsen a graph with a much smaller number of units.
To form color-blocks, we experimented on several algorithms, i.e., nearest-k neighbors, neighbors within k-hops, etc, and found that neighbors within 1-hop outperforms the others.
Disjointed 1-hop color-blocks could be generated as follows:(1) randomly select one vertex, find its 1-hop neighbors, and form a color-block; (2) delete the vertices of this color-block; (3) repeat (1) and (2) until no vertex is left.
[Query Profiling] Given a set C = {c1, c2, ..., cn} of color-blocks, we track the trail of a query with a subset of color-blocks, Lj = {cj 1 , cj 2 , ..., cj l }.
Since these color-blocks will be grouped together later, it is not necessary to record the visiting order of color-blocks.
Lj is termed an envelope of the query.
By tracking cross-partition queries using color-blocks, each query can be profiled as an envelope.
Figure 5 shows the relation among partitions, color-blocks and envelopes.
Given a set of candidate envelopes, a partition cannot assemble all of them due to its space constraint.
Herein we formulate the problem as an envelopes collection problem.
[Envelopes Collection] Given a partition with the storage capacity M , there are a set L = {L1, ..., Ln} of envelopes and a set n j=1 Lj of m colors, each envelope Lj encapsulates a set Lj = {ci 1 , ci 2 , ..., ci l } of colors and the size of color c k is w k .
If D ⊆ L and R = L j ∈D Lj, the objective is to find such a set D that maximizes |D| with the constraint c k ∈R w k ≤ M , where M is the default partition size.
Envelopes collection is reminiscent of the Set-Union Knapsack Problem, which is a classic NP-complete problem.
We propose a greedy algorithm based on the intuition that combining similar envelopes consumes less space than combining non-similar ones.
Given two envelopes Li and Lj, the overlap of their color-block sets is measured as the Jaccard coefficient Sim(Li, Lj) =|L i ∩L j | |L i ∪L j |.
Given n envelopes, performing pair-wise similarity comparison is a procedure running in O(n 2 ).
To cope with this challenge, we employ a hashbased algorithm, called Locality Sensitive Hashing (LSH) [14] to perform similarity search in a provably sublinear time.LSH is a probabilistic method that hashes items so that similar items can be mapped to the same buckets with high Algorithm 1 Similarity-Based Greedy Clustering Algorithm Input: Envelope set L = {Li} Output: New partition P 1: Initialize hash functions 2: for each Li ∈ L do 3: hash value = h(Li) 4:add Li to C hash value 5: end for 6: C = {C hash value } for each C hash value = ∅ 7: for each cluster Ci in C do 8:ρ[i] = |W (Ci)|/|Ci| 9: end for 10: Sort clusters on ρ in descending order 11: cluster set P = ∅ 12: Add clusters to P as many as possible, s.t., size(P ) ≤ M probability [14].
In our case, we adopt a LSH scheme called Min-Hash [10].
The basic idea of Min-Hash is to randomly permute the involved set of color-blocks and for each envelope Li we compute its hash value h(Li) as the index of the first color-block under the permutation that belongs to Li.
It has been shown in [10] that if we randomly choose a permutation that is uniformly distributed, the probability that two envelopes will be mapped to the same cluster is exactly equal to their similarity.
We use Min-Hash as a probabilistic clustering method that assigns a pair of envelopes to the same bucket with a probability proportional to the similarity between them.
Each bucket is considered as a cluster and the envelopes within the same bucket are combined together.
[Partition Generation] After obtaining a set of independent clusters, each cluster is assigned with a benefit score, ρ = |W (C)| , to measure the quality of the cluster.Here |W (C)| is the number of cross-partition queries denoted by all the envelopes in the cluster C (more accurately, the times of the color-blocks in C are accessed) and |C| is the size of the cluster.
We create an empty partition and iteratively assemble the cluster with the highest ρ at each step as long as the total size is no greater than the default partition size M .
Scalability issues.
The greedy algorithm is outlined in Algorithm 1.
For n envelopes, the complexity of MinHash clustering is O(n) (lines 1-5) and the sorting runs in O(mlog(m)) (line 9) where m is the number of the clusters generated (line 6).
In the worst case, combining the clusters needs O(nm) (line 12).
In total, the complexity of this greedy algorithm is O(nm).
There is still a concern that if n and m are large, this algorithm would lead to poor scalability.
To cope with this challenge, we limit the growth of n and m in the following way.
On one hand, we use a sampling method to constrain the size of n. For example, when the dynamic partitioning procedure is triggered, among a set of cross-partition queries we randomly select a number of queries as a sample to generate the new partition.
On the other hand, we could coarsen the size of color-blocks by increasing the number of vertices included in these blocks.
This will result in a color set much smaller than the vertex set.
In the experiment, we show that these two methods collectively guarantee that the dynamic partitioning method works in an efficient way.
Discussion: Duplicate Sensitive Graph Query.
As a design principle, primary partitions are disjointed: each vertex only has one copy in the partitions.
However, when secondary partitions exist, it is often the case that there are two copies v and v for the same vertex.
It might cause a potential issue, as illustrated in Figure 6.
Figure 6(a) shows the original graph.
In Figure 6(b), secondary partition P2 is added and v is a duplicate vertex v. Suppose we run the following algorithm to calculate the number of v's 2-hop friends :[Method 1] Starting at v , we send a message to its 1-hop friends and these friends send another message to their 1-hop friends.
Each partition reports the number of vertices who received messages.
Sum up the numbers.The above algorithm works correctly in primary partitions.
However, for Figure 6(b), it will produce a wrong answer.
Due to this complication, it is not straightforward to run queries correctly in secondary partitions.
Fortunately, for many local graph queries, there are implementations that are not sensitive to overlapping partitions.
If we change Method 1 slightly, it will work correctly.
[ Other graph queries such as random walk, personalized PageRank, hitting time and neighborhood intersection have implementations that are not sensitive to duplications.
We call queries that can be correctly answered on overlapping partitions Duplicate Insensitive Graph Queries.
If a duplicate sensitive graph query running on a secondary partition exceeds the boundary of the partition, the query will be terminated and restarted in a primary partition.
In Sedge, the query routing component (described in the next section) maintains a vertex-partition fitness list for the start vertex of a query.
It helps route the query to a partition that can serve it locally with high probability.
An incoming query arrives with at least one initial vertex.
The master node dispatches the query to a Pregel instance according to the initiated vertex.
As shown in Figure 3, if possible, a query shall be routed to a Pregel instance (P I for short) where its initiated vertex is in the safe region.
Here, we devise a data structure in the master node to coordinate query routing:• Instance Workload Table (IWT ): I → W (I), where I is the ID of a P I and W (I) is the workload of the P I.• Vertex-Instance Fitness List (VFL): v → Lv{I}, where Lv{I} is an id list of the P Is.Given a vertex v, the P Is where v is in safe region are ranked higher in VFL.
Since some vertices, such as those with very high degree, might not be in any safe region, we assign a random order of P Is to their VFLs.
During the runtime, the IWT is updated by the monitoring routine.
Given a query, the algorithm routes the query to the first P I in its VFL that is not busy with respect to the IWT.
Once the query is finished, if the query cannot be served locally in its assigned P I, the query fitness list will shift the P I to the end of the list.
Since the number of Pregel instances is small, VFL is implemented using bitset.
Bitset is an array optimized for space allocation: each element occupies only one bit.
For example, it uses only 3 bits to represent up to 8 P Is.
Our experimental results show that the simple greedy routing strategy can outperform random query routing significantly.Vertex-Partition Mapping.
In order to process queries, each Pregel instance needs to maintain the following tables to map vertices to partitions.
All partitions are mapped onto unique IDs.
• Partition Workload Table (PWT ): P → W (P ), where P is the ID of a partition and W (P ) is the workload.
• Vertex-Primary Partition Table (VPT ): v → P , where P is a primary partition.
Each vertex is mapped to one and only one primary partition.
• Partition-Replicates Table (PRT ): P → {SR}, where {SR} are the identical replicates of P .
For ∀v ∈ P , it may associate with several SR.• Vertex-Dynamic Partitions Table (VDT ): v → {SD|v ∈ SD}, where {SD} are the new partitions generated by the dynamic partitioning method.Space complexity.
Due to the limited number of partitions in practice, the size of the PWT and the PRT is negligible.
VPT is O(n), where n is the number of vertices in G.
It only takes several gigabytes to store a VPT table for billions of vertices.
The size of VDT depends on the number of vertices covered by the secondary partitions.
Usually, the size is far smaller than O(n).
In particular, each secondary partition is associated with one primary partition set from which it is created.
When a secondary partition is generated or deleted, an entry in PRT or VDT needs to be updated accordingly.
For K Pregel instances, we maintain their tables separately.
That is, we will have K sets of PWT, VPT, PRT and VDT.
These tables are stored in main memory.
The workload monitoring component in Sedge is built in the optimizer module (ref.
to Figure 2).
Report messages from all Pregel instances are sent to the master at the end of each period.
Typically a report message from a Pregel instance I includes the number of the queries served in I (i.e., Wint(I) and Wext(I)), the total access times of the vertices ( q∈W (I) |V (q)|), and the CPU run time of the machines holding I.
These messages encode the workload information of Pregel instances.
The master updates the IWT accordingly.
Analogously, each Pregel instance collects runtime information of their partitions and calculates the ratio between the total access times of the vertices and the size of the partition and sorts the partitions based on the ratio.
Then with respect to the threshold ratio, a partition can be marked as a hot or a slack one.
The information is maintained in the PWT.
As discussed in Section 5, secondary partitions are generated to deal with query hotspots.
In practice, the space that can be used to accommodate additional partitions is often limited.
Therefore, it is unlikely to create as many secondary partitions as possible.
At the same time, in real world applications, query hotspots may become "slack " ones after a period.
This practical issue motivates a partition replacement scheme that replaces a slack secondary partition with a newly generated one.
In Sedge, when a replacement is needed, we simply select the slackest secondary partition and replace it with the one newly generated.
Real world graphs usually change over time in terms of insertion and deletion of nodes and edges.
Sedge can adapt to these dynamic changes.
Here we take the update on one Pregel instance as an example.
Since the information of a vertex can be obtained by referring to the vertex-partition map, edge insertion and deletion can be accomplished directly.
For the insertion/deletion of edge (u, v), find the primary and secondary partitions of u and v, insert or delete the edge.
To delete vertex v, one can retrieve all of its edges and delete them, and then retrieve all of partitions containing v and delete v. For insertion of vertex v and its edges, one can first locate a primary partition P where the majority of v's neighbors are located, and then add v to that partition.
Meanwhile, update all of the replicates of P and then submit edge insertion requests.
For vertex insertion and deletion, we also need to update the vertex-partition map, i.e., VFL, VPT and VDT.
Note that the update should be applied to all the Pregel instances.
When the insertion of vertices and the following edge insertions make a primary partition too big, we need to redo the partitioning from scratch.
Additionally, when a query changes vertex values during its execution, the cost of keeping the vertex values in sync is usually quite high especially when there are many duplicates.
In Sedge, we adopt a simple strategy: when a query changes a vertex value, a new update query is issued to all the corresponding partitions.
An experiment in Section 7.2 demonstrates the efficiency of dynamic update in Sedge.
The system is programmed in Java.
We use a distributed version of METIS [21] to generate primary partitions.
To evaluate Sedge on a diversified set of graphs and queries, we test datasets in two categories: RDF benchmarks and real graph datasets using different sets of graph queries.
Our experiments are going to demonstrate that (1) Sedge is efficient and scalable, in comparison with the situation without partition management, and (2) the design of each component including complementary partitioning and on-demand partitioning is effective for performance improvement.The experiments are conducted on a cluster with 31 computing nodes: each has 4 GB RAM, two quad-core 2.60GHz Xeon Processors and a 160 GB hard drive.
Among these nodes, one serves as the master and the rest as workers.
The cluster is connected by a gigabit ethernet.
In each ex- Figure 7: Number of cross-partition queries.
The missing bars for the CP4 and CP5 of Q2 , the CP5 of Q4 and the CP5 of Q6 correspond to the value of 0, i.e., the cross-partition query vanishes.periment, we perform three cold runs over the same experimental setting and report the average performance.
For each graph in the following experiments, we generate 5 complementary partition sets beforehand.
We use CP1 to denote the performance when only using the first primary partition set while CP2, CP3, CP4 and CP5 to denote the performance when using 2, 3, 4 and 5 partition sets, respectively.
Each primary partition set consists of 12 primary partitions, which fill in 6 workers.
We first evaluate the system performance of Sedge on a SPARQL benchmark graph.
SPARQL is an emerging standard for RDF.
Efficient storage techniques for large scale RDF data and evaluation strategies for SPARQL are currently under exploration in the database community [34,3].
In this experiment, we will illustrate that our partitioning techniques can improve SPARQL query execution significantly.The SP 2 Bench Benchmark [34] chooses the DBLP library as its simulation basis.
It can generate arbitrary large RDF test data which mirrors vital real-world distributions found in the original DBLP data.
Using the generator provided by [34], we create an RDF graph with 100M edges (11.24GB).
It is a heterogenous graph with the subjects/objects as the vertices and the predicates as the links.
SP 2 Bench provides 12 query templates, Q1, Q2, . . . , Q12 that are delicately designed to capture all key features of the SPARQL query language.
In this work, we select five categories in which the existing SPARQL engines have difficulties.
These queries are listed in the Appendix.
From the view of query operation, Q6 and Q7 encode the operations of OPTIONAL (akin to left outer joins) with FILTER and BOUND; from the view of access pattern, Q2 and Q4 contain two distinctive graph patterns, "long path chains" and "bushy patterns" [34]; Q8, extracting the Erdös Number of the authors, showcases the queries that concentrate on a "hotspot".
We map the queries against specific vertices as the query starts and thereafter match the variables to the nodes or edges during the query execution.In order to validate the complementary partitioning approach, we generate a workload with 10, 000 queries, which are the equal mixture of the 5 query types with randomly selected starts.
The queries are routed automatically to the corresponding partitions with the assistance of the query routing module.
We compare the performance by varying the number of the used primary partition sets.
Figure 7 shows the effect of the approach.
Note that the Y-axis is plotted in logarithmic scale to accommodate the significant differences in the number of queries that access at least two partitions.
It is observed that by adding more complementary partition sets, the number of cross-partition queries can be dramatically reduced.
It vanishes for Queries Q2, Q4 and Q6 when 4 or more complementary partition sets are used.A close look at the difference in the performance between the variants of query types reveals that Q2, Q4 and Q6 exhibit high locality.
In contrast, Q7 and Q8 exhibit more complex access pattern.
Figure 7 shows for the queries of Q7 and Q8, CP5 outperforms CP1 by up to almost one order of magnitude.
The result suggests that our complementary partitioning is an effective way in response to cross-partition queries of various types.
Figure 7 also shows, with respect to different queries, how the percentage of vertices in safe partitions changes when the number of complementary partition sets increases.
For example, for Q7, the percentage of vertices in safe partitions increases from 50.9% (1 partition set) to 94.7% (5 complementary partition sets); and for Q8, it increases from 81.4% to 97.6%.
To demonstrate how Sedge responds to skewed workloads, we generate a synthetic evolving workload which contains 10 timesteps.
In each timestep, the workload consists of 10, 000 queries which are the mixture of the 5 query types with equal number.
To control the evolution of the workload, each query is assigned with a lifetime value.
If the query is internal (finished within a partition), it has lifetime, lif etimeI ; otherwise, it has lifetime, lif etimeC .
When a query expires, it will restart in the next timestep with a new lifetime and a randomly selected start.
Since random internal queries do not contribute to a skewed workload, we set lif etimeI = 1 for simplicity and vary the value of lif etimeC in the following experiments.
Note that when lif etimeC > lif etimeI , the number of cross-partition queries will increase gradually because more internal queries will become cross-partition queries than the reverse along the time.We compare the approaches from two perspectives: complementary partitioning and on-demand partitioning.
CP1× 5 uses 5 static replicates of the first partition set (i.e., run five Pregel's independently, each with 1/5 workload), and CP5 uses all the 5 complementary partition sets.
Both of the two approaches use up 30 worker space.
Note that we run these two settings only using Pregel instances where no query profiling (on-demand partitioning) is applied.
CP4 + DP uses 4 complementary partition sets and employs the rest worker space for on-demand partitioning.
To maintain a fair comparison, the number of secondary partitions can not exceed 12, the size of one partition set in our experiments.
Figure 8 reports the accumulated time cost of the query workload at each timestep with respect to the three approaches.
The overhead of on-demand partitioning is also included in the workload cost.
Figure 8(a) shows the performance of these approaches when lif etimeC = 2.
The curve of CP5 illustrates that the complementary partitioning technique significantly outperforms the static replication (CP1 × 5).
The advantage becomes more obvious along with the accumulation of the cross-partition queries.
It can also be seen that due to the generation of new secondary partitions, CP4 + DP outperforms CP5 after timestep 3.
When lif etimeC = 5, Figure 8(b) shows a similar result of the comparison between CP1 × 5 and CP5 as in Figure 8(a).
However, in Figure 8(a), CP4 + DP outperforms CP5 noticeably after timestep 3 and the time cost almost remains steady.
This is because when lif etimeC = 2, due to the dynamics of the queries, the system invokes on-demand partitioning more frequently (6 times) than that when lif etimeC = 5 (3 times).
Next, we evaluate the design of Sedge by testing the effectiveness of each component.
We use another set of graphs and queries to show the broad usage of Sedge.
Nevertheless, the same test can be conducted on SP 2 Bench and similar results will be observed.Web graph.
It is a uk-2007-05 web graph data from http://webgraph.dsi.unimi.it [5,4], which is a collection of UK websites.
After preprocessing, the graph contains 30M vertices and 956M edges.Twitter graph.
The Twitter graph is crawled from Twitter, consisting of 40.1M users.
There are 1.4B edges (including multi-edges) in this dataset.
For simplicity, we aggregated the multi-edges and the associated attributes as one edge which represents several messages sent from one user to another at different time.Bio graph.
The Bio graph is a de Bruijn Graph built from a sample of mRNA.
In this graph, vertices represent sub-sequences of DNA symbols with length of twenty one (a.k.a. k-mer length) and edges represent the adjacent relationships between vertices: the two vertices differ by a single symbol [39].
We collect 50M vertices and construct 68M edges.
The resulting de Bruijn graph is like a tree.Synthetic scale-free graph.
The graph is generated based on R-MAT [9].
It consists of 0.2 billion vertices and 1.0 billion edges.
The graph matches "pow-law " behaviors and naturally exhibits "community" structure.
Table 1 summarizes the size of the graphs, the time cost of building one primary (complementary) partition set, the size of the vertex-instance fitness list (VFL), and the size of the vertex-partition table (VPT ).
It can be seen that the auxiliary meta-data is much smaller than the graph it serves, only 0.5% − 5% of its size.
We use three classic local graph queries to experiment the performance: (1) h-hop Neighbor Search (h-NS): the query starts from a vertex v and does a breath-first search for all the vertices within h hops of v; (2) h-step Random Walk (h-RW): the query starts at a vertex and at each following step jumps to one of its neighbors with equal probability.
The query consists of h steps; (3) h-step Random Walk with Restart (h-RWR): it is a h-step random walk query; but at each step it may return to its start vertex with p probability.
We set p = 10% by default.
For global graph algorithms like single-source shortest distance, Sedge could also support them.
However, they are not the focus of this work.We test the effectiveness of our proposed algorithms: complementary partitioning, partition replication and dynamic partitioning.
Due to the space limitation, we first show the experiments on the Web graph with different test settings.
For the other datasets, we get quite similar results.
We will then give an evaluation of the system on the scalability, using all of the four graphs.
Figure 9 shows the effect of complementary partitioning in reducing the communication cost.
In this experiment, we use CP1 as the baseline (the result will not change if we replicate CP1 five times) and test 10, 000 h-RWR queries using different number of complementary partition sets.
By varying the step of the h-RWR, it can be seen that the complementary partitioning method can reduce the inter-machine messages.
As to queries with longer random walk, the performance of Sedge degrades.
However, with more complementary partitions, e.g., CP4 and CP5, Sedge can still achieve good performance in message reduction.
To evaluate the performance of partition replication on unbalanced workload, we randomly generate a workload with mixed queries, i.e., 3-NS, 5-RW, 5-RWR, on a specific graph partition (denoted as P1) and continuously increase the number of queries from 10, 000 to 50, 000.
We run this changing workload under 3 different settings: (1) CP1 (the baseline); (2) CP1 and 1 replicate of P1 (ref.
as CP1 + PS); (3) CP1 and 2 replicates of P1 (ref.
as CP1 +PS ×2).
Figure 10 shows the number of queries can be served per second (throughput) for each setting.
It is observed that the throughput by using partition replication significantly outperforms that of no replication one.
This is because the query workload on P 1 is distributed and processed in parallel among the primary partition and its replicates.
To test the performance of dynamic partitioning, we fo- cus on queries that access multiple partitions.
We randomly generate mixed cross-partition queries (3-NS, 5-RW and 5-RWR) and test the system performance by varying the number of queries from 10, 000 to 50, 000.
We run Sedge with only one primary partition set (CP1) as well as with one primary partition set and on-demand generated secondary partitions (CP1 + DP ), respectively.
Figure 11 shows the runtime cost of dynamic partitioning.
It measures the run time of each stage to finish a dynamic partitioning process: query profiling, envelopes collection and new partition generation.
The figure shows the cost per query by varying the number of cross-partition queries.
For all the three stages, it is observed that the cost remains almost constant.
Therefore the dynamic partitioning method is scalable with respect to the number of cross-partition queries.
We next use the same query workload to test the effect of dynamic partitioning.
Figure 12 shows the average response time by varying the number of cross-partition queries.
Note that the response time here only indicates the query answering time.
From the figure, we can observe the query response time is significantly improved compared to the static partitioning method.
This also explains that our algorithms are effective for serving cross-partition queries.
In the above experiments, Sedge uses slightly larger space with secondary partitions.
Additionally, we test the capability of Sedge to handle intensive cross-partition queries.
We generate five sets of query workload, each of which contains 100, 000 random queries and set the percentage of the cross-partition queries as 0%, 25%, 50%, 75% and 100%, respectively.
For this experiment, we use CP1 as the baseline and demonstrate the performance of CP1 + DP , where DP denotes secondary partitions generated by dynamic partitioning on demand.
We employ 6 machines to hold CP1 and assign additional machines gradually to accommodate the new partitions.
Figure 13: Cross-partition queries vs. Improvement ratio in avg.
response time Figure 13 shows the improvement ratio in average response time.
In this figure, we plot the lift of the average response time by using on-demand partitioning compared with the baseline.
The response time includes both the query answering time and the overhead of on-demand partitioning.
As we increase the percentage of cross-partition queries, it can be seen that for all the four datasets, there is a significant improvement in average response time.
In detail, however, we observe different improvement performance with respect to the changing workload.
For the Twitter graph and Synthetic graph, the ratio increases constantly.
This can be explained as follows.
In these two graphs, there are many tightly connected substructures (communities).
If these substructures are divided among multiple partitions, the cross-partition queries on them will visit these partitions frequently and as a result produce much inter-machine communication.
In this case, by collecting the hot substructures together, our system can dramatically improve the efficiency.As for the Bio graph, it is a tree-like structure.
Hence, the cross-partition query does not visit many partitions and the improvement in query response time is not remarkable when compared with the baseline method.
The characteristics of the Web graph are between these two types.
To test the performance of dynamic update/synchronization, we experiment on vertex addition and deletion on the large Synthetic graph.
To assure updates are indeed executed globally, 5 primary (complementary) partition sets are initially loaded and runs in parallel.
In the experiment of vertex addition, we generate new vertices with respect to the degree distribution of the graph, which is a "power-law" distribution with γ = 2.43 (a.k.a scaling parameter, [2]).
New edges are constructed according to preferential attachment.
As to the experiment of vertex deletion, we randomly select vertices in the graph to delete.
Figure 14 shows the average run time for each vertex addition/deletion operation by varying the number of vertices.
It is observed that the addition and deletion operation per vertex can be accomplished in about 0.2ms and 0.4ms respectively and the time is almost constant with respect to the number of updated vertices.
We introduced an emerging data management problem in large-scale social and information networks.
In order to process graph queries in parallel, these networks need to be partitioned and distributed across clusters.
How to generate and manage partitions becomes an important issue.
We illustrated that, for graph queries which have strong locality and skewed workload, static partition scheme does not work well.
Thus, we proposed two partitioning techniques, complementary partitioning and on-demand partitioning.
Based on these techniques, we introduced an architecture with a two-level partition structure, primary and secondary partitions, to handle graph queries with changing workload.
The experiments demonstrated the developed system can effectively minimize inter-machine communication during distributed graph query processing.
For future work, it is interesting to explore efficient RDF storage mechanisms and distributed metadata indexing solutions.
sThe queries used in the evaluation on the SP 2 Bench Benchmark [34] are listed as follows.Q2 Given an inproceeding, extract all the properties of the inproceeding, e.g., the title, the pages, the authors, the proceeding and the reference list.Q4 Given a journal, select all distinct pairs of article author names for authors that have published in the journal.Q6 Given a proceeding and a specific year, return the set of the inproceedings authored by persons that have not published in years before.Q7 Given a reference list, return the titles of the papers in the list that have been cited at least once, but not by any paper that has not been cited itself.Q8 Given an author, return the "collaborative distance" between the author and mathematician Paul Erdös (The distance is also known as Erdös Number ).
The queries used in the evaluation on the SP 2 Bench Benchmark [34] are listed as follows.Q2 Given an inproceeding, extract all the properties of the inproceeding, e.g., the title, the pages, the authors, the proceeding and the reference list.Q4 Given a journal, select all distinct pairs of article author names for authors that have published in the journal.Q6 Given a proceeding and a specific year, return the set of the inproceedings authored by persons that have not published in years before.Q7 Given a reference list, return the titles of the papers in the list that have been cited at least once, but not by any paper that has not been cited itself.Q8 Given an author, return the "collaborative distance" between the author and mathematician Paul Erdös (The distance is also known as Erdös Number ).
