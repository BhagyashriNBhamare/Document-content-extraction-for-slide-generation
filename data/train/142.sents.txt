This paper deals with the selection of the best of a finite set of systems, where best is defined with respect to the maximum mean simulated performance.
We extend the ideas of the knowledge gradient, which accounts for the expected value of one stage of simulation, by accounting for the future value of the option to simulate over multiple stages.
We extend recent work on the economics of simulation, which studied discounted rewards, by balancing undiscounted simulation costs and the expected value of information from simulation runs.
This contribution results in a diffusion model for comparing a single simulated system with a standard that has a known expected reward, and new stopping rules for fully sequential procedures when there are multiple systems.
These stopping rules are more closely aligned with the expected opportunity cost allocations that are effective in numerical tests.
We demonstrate an improvement in performance over previous methods.
This paper focuses on the use of sampling to select the best of a finite set of simulated systems, where 'best' is defined in terms of the maximum expected value of the simulation output.
Quite a bit has been written on this topic from both the frequentist (e.g., see Kim and Nelson 2006 for a review) and Bayesian perspectives (e.g., see Chick 2006 for a review).
This paper takes a Bayesian perspective, as this perspective has shown a high degree of effectiveness in sampling to quickly identify the best system (Branke, Chick, and Schmidt 2007).
Selection procedures use allocation rules to identify how many and from which system simulation samples should be drawn next, stopping rules to indicate whether sampling should continue or should stop, and selection rules that identify which system to pick when sampling has stopped.
Early work on Bayesian selection procedures focuses on allocation rules to improve sampling efficiency by seeking optimal allocations for two-stage procedures, where possible, or on finding useful heuristics or asymptotic properties (Gupta and Miescke 1996, Chen 1996, Chick and Inoue 2001.
Later work explores a greater variety of stopping rules, and finds that the choice of stopping rule can strongly influence the performance of an allocation ( Branke et al. 2007).
Stopping rules include sampling to exhaust a finite budget and adaptive stopping rules that continue to sample, as a function of the data, until a statistical threshold for the posterior expected loss is achieved.Frazier, Powell, and Dayanik (2008) employ a dynamic programming framework to analyze a greedy allocation rule introduced by Gupta and Miescke (1996), referring to it as the knowledge gradient (KG) to emphasize the way in which it compares the difference in expected value between two different quantities of information or knowledge, and also to emphasize its membership in a family of one-step lookahead expected value of information rules.
introduce a new stopping rule that, when used with the KG allocation rule, improves its selection efficiency in several numerical tests, as compared with the best combinations of allocation rules and stopping rules in Branke et al. (2007).
Both Branke, Chick, and Schmidt (2007) and observe that procedures tend to perform best when the allocation rule and the stopping rule are derived with a similar criterion, such as an expected opportunity cost (EOC) criterion or a probability of good selection criterion.
The stopping rules from those papers compare the expected reward from a single stage of additional sampling with either statistical thresholds or with sampling cost information.This paper seeks to further improve the performance of such procedures.
We develop new stopping rules that can be used to compare the value of the option to continue simulating (i.e., the expected reward from not just one stage of simulating, but from the option to simulate, observe the data, and still have the option to continue simulating) with the costs of simulating.
The stopping rules below therefore do a better job, in theory, of balancing the cost of simulating with the value of the option to continue simulating before deciding.
This balance is sought with the goal of better aligning these stopping rules with the EOC-based allocations that will be used.
Chick and Gans (2009) also assesses the value of the option to continue sampling before selecting.
That paper examines the special case of one system that can be simulated, then either accepted or rejected as being better or worse than a system with a known expected reward.
It does so by solving for the optimal stopping time for a particular diffusion process.
That paper extends the idea to multiple simulated systems by deriving upper and lower bounds on the expected value of the option to simulate.
Those bounds are shown to be close to each other, and so a diffusion approximation is fortunately not needed when there are multiple systems.
Those bounds result in stopping rules that effectively balance the expected value of information from the option to further simulate (not just the information from one stage), and the costs of simulation.
The costs of simulation in Chick and Gans (2009) include a cost per sample c ≥ 0 and a positive discount rate δ > 0, both of which incur costs when more samples are taken.
That paper does not show, however, the solution when there are positive sampling costs (c > 0) but no discounting (δ = 0).
The main contribution of this paper is to extend the expected value of information approach by focusing on the expected value of the option to continue sampling, not on just the expected value of a single stage of sampling.
We handle the case of positive sampling costs and no discounting costs.
We use dynamic programming ideas and treat simulation selection as an option to gain information.
We also develop easily-implemented stopping rules for determining when to select a system rather than continuing to simulate.
Those rules are based on the costs of simulation runs, rather than statistical thresholds.
An analyst seeks to implement one of k systems, labeled i = 1, 2, . . . , k, whose expected reward is not known with certainty, or to implement an alternative course of action, labeled i = 0, whose expected reward is the known value m. For example, if the analyst has the option to implement one of k simulated systems or to reject them all and 'do nothing', then m = 0.
Prior to selecting a system (i = 0, 1, 2, . . . , k), the analyst can choose to sequentially simulate one or more of the k systems until a system is chosen for implementation.
The analyst would like to maximize the expected reward from this process, including the cost of simulation replications and the expected reward from implementing a system.
Section 2.1 formalizes the general problem, then Section 2.2 specializes the analysis to the case of normally distributed simulation output with unknown means (but known sample variances).
Let X i be the random variable representing the reward of system i, where X 0 ≡ m.
If the analysis is risk neutral and the distributions of all X i 's are known to him, then he will select the system with the largest expected reward,i * = arg max i {E[X i ]}.
Here, we assume that the distributions of the X i 's are not known with certainty by the analyst, for i = 1, 2, . . . , k. Rather, he believes that a given X i comes from one of a family of probability distributions, P X i |θ i , indexed by a parameter θ i from a parameter space Ω Θ i .
We model his belief with a probability distribution on θ i , which we call P Θ i .
The expected reward of system i > 0 is then E[X i ] = X i dP X i |θ i dP Θ i .
We assume integrability of X i under dP X i |θ i dP Θ i .
When P Θ i can be uniquely represented by a finite set of sufficient statistics Θ i , such as for distributions that are in a regular exponential family of distributions, then P Θ i can be referred to by Θ i without ambiguity.
For example, the analyst may believe that X i is normally distributed with a known variance, σ 2 i , but unknown mean.
Then P Θ i is a probability distribution for the unknown mean.
If P Θ i itself is a normal distribution with mean µ 0i and variance σ 2 0i , then P Θ i can be identified withΘ i = (µ 0i , σ 2 0i).
When sufficient statistics are available for each system, we set Θ = (Θ 1 , Θ 2 , . . . , Θ k ).
If the distributions of the X i 's are not known, then the analyst has the option to use simulation as a tool to reduce uncertainty about the distribution before selecting which system to develop outside the simulator.
He may decide to simulate the outcome of system i a number of times, and he views the result of each replication as a sample X i from P X i |θ i , where θ i is the (unknown) true parameter value for system i.
We also assume that simulation results, and hence the evolution of the analyst's beliefs, are independent from one system to the next.We model simulations as occurring at a sequence of discrete stages t = 0, 1, 2, . . ..
Let X t be the set of all outputs seen through stage t.
We represent Bayesian updating of prior beliefs and sample outcomes through time, {( Θ t , X t ) |t = 0, 1, . . .}, as follows.
If system i > 0 is simulated at stage t with sample outcome x i,t , then X i,t = x i,t and Bayes' rule determines the posterior distribution P Θ i,t+1 .
The density of that posterior distribution is described as a function dP θ i |Θ i,t+1 of the parameter Chick and Frazierθ i ∈ Ω Θ i : dP θ i |Θ i,t+1 = dP X i | θ i dP θ i |Θ i,t θ i dP X i | θ i dP θ i |Θ i,t ∀ θ i ∈ Ω Θ i ,while Θ j,t+1 = Θ j,t , and X j,t need not be defined for all j = i. Thus, the evolution of the analyst's beliefs regarding the distribution of outcomes of each system, Θ i,t , is Markovian.
We assume that the incremental cost of each run of system i is c i .
Unlike Chick and Gans (2009), we do not consider discounting costs associated with delays from sampling.
In the absence of sampling costs or a discount rate, one could imagine pathological scenarios such as simulating forever to resolve all uncertainty about the mean performance of each system before selecting one.
Sampling costs avoid such pathological scenarios.The analyst must choose a sequence of simulation runs and ultimately select a system, so that the stream of costs and terminal expected reward, together maximize the expected reward.
We define indices to track the analyst's choices.
Let T ∈ {t = 0, 1, . . .} be the stage at which the analyst selects a system to implement.
For t < T , let i(t) ∈ {1, 2, . . . , k} be the index of the system simulated at time t. Let I(T ) ∈ {0, 1, . . . , k} be the index of the system that is ultimately chosen.Then a selection policy π = (i(·), T, I(T ))) is the choice of a sequence of simulation runs, a stopping time, and a final system.
We define Π to be the set of all non-anticipating selection policies.
For such policies, the function i(t), the indicator function of the event {T = t} and the function I(T ) (given the event {T = t}) each depend only on the history up to t:{ Θ 0 , X 0 , Θ 1 , X 1 , . . . , Θ t−1 , X t−1 , Θ t }, for t = 0, 1, . . ..
Given prior distributions Θ = (Θ 1 , Θ 2 , . . . , Θ k ) and a policy π ∈ Π, the expected value of the future stream of rewards isV π ( Θ) = E π T −1 ∑ t=0 −c i(t) + X I(T ),T | Θ 0 = Θ .
(1)Formally, we define the analyst's (undiscounted) simulation selection problem to be the choice of a selection policy π * ∈ Π that maximizes this (undiscounted) expected reward:V π * ( Θ) = sup π∈Π V π ( Θ), when the supremum is attained.
Note that, conditional on stopping to select, the system I(T ) that is selected gives an expected rewardmax{m, E[X 1,T | Θ T ], E[X 2,T | Θ T ], . . . , E[X k,T | Θ T ]}.
This section specializes (1) to the case of normally distributed realizations that have unknown means and known variances.
Let X i j be a random variable whose realization x i j is the output of the jth sample from system i, for i = 1, . . . , k and j = 1, 2, . . ..
Let U i be the unknown mean and let σ 2 i be the known variance of system i. Given a realization of the unknown means, we denote their order statistics by u [1] ≤ u [2] ≤ . . . ≤ u [k] , with ties broken randomly.
We assume that, conditioned on the unknown means U i ,X i j : j = 1, 2, . . . |U i iid ∼ Normal u i , σ 2 i for i = 1, 2, . . . , k. Let U = (U 1 ,U 2 , . . . ,U k ), u = (u 1 , u 2 , . . . , u k ) and σ 2 = (σ 2 1 , σ 2 2 , . . . , σ 2 k ).
To describe uncertainty about the mean expected rewards, we assume the following conjugate prior distributions,U i iid ∼ Normal µ 0i , σ 2 i /t 0i for i = 1, 2, . . . , k.The value t 0i can be interpreted as the effective number of replications of data that is embodied by the prior distribution for the unknown mean of system i.
When specializing (1) to the case of normally distributed output with an unknown mean whose distribution is normally distributed itself, P Θ i is the Normal µ 0i , σ 2 i /t 0i distribution, Θ i can be represented by the vector (µ 0i , σ 2 i /t 0i ) or an equivalent set of information, such as (µ 0i ,t 0i ) when σ 2 i is known, and the realization of the parameter θ i is U i = u i .
This section examines the special case of k = 1 simulated system and deciding whether to implement it or not after simulations have been run.
We drop i(t) and I(T ) in subscripts for notational convenience in this section since there is only one simulated system.
Then the optimization of (1) becomes the following optimal stopping problem,V π * ( Θ) = sup π E π −cT + max{m, E[X T | Θ T ]}| Θ 0 = Θ .
(2)We describe a diffusion approximation to estimate V π * ( Θ) and numerically solve it to find the optimal stopping boundary.
We approach the optimal stopping problem in (2) by using a diffusion approximation (analogous to innovations by Chernoff 1961 and used later by Chick and Gans 2009).
We note that the process {(µ 0 t 0 + ∑ n i=t X t ,t 0 + n) : n = 0, 1, 2, . . .} is a random walk with Gaussian and independent increments.
This discrete time process has the same distribution as the continuous time process {(Y t ,t) : t ≥ t 0 } at points t for which t −t 0 is an integer, where, conditioned on W , {Y t : t ≥ t 0 } is a Brownian motion with Y t 0 = µ 0 t 0 , drift W , and volatility σ 2 .
From this construction, we haveY t | U ∼ Normal µ 0 t 0 +U(t − t 0 ), σ 2 (t − t 0 ) and U ∼ Normal µ 0 , σ 2 /t 0 .
Define t = t 0 + n and y t 0 = µ 0 t 0 , so that (y t 0 ,t 0 ) is sufficient to describe the prior distribution for U.
If we set y t = y t 0 + ∑ n i=1 x i (the continuous time and the discrete time processes have the same statistics, so we adopt this abuse of notation here), thenU | (Y s , s) for s ∈ [t 0 ,t) ∼ Normal y t /t, σ 2 /t .
That is, the posterior distribution of the unknown expected reward from implementing a system, U, conditional on x 1 , x 2 , . . . , x n , is easily described by t and y t .
We can therefore refer to the posterior distribution Θ t by (y t ,t) and we will do so below.
We now convert the above diffusion (Y t ,t) to a standardized stopping problem for a diffusion that can be used to approximate the optimal stopping boundary and the expected reward for the simulation selection problem with k = 1.
We do this by adapting techniques in Chick and Gans (2009) to the context here.
This is done in two steps.
First, we rescale the variables in the problem with the goal of being able to solve for any combination of m, c and σ 2 with the solution to a single optimal stopping problem.
We introduce constants α and γ, which are arbitrary for now but will be defined later in terms of c and σ 2 .
Let r = γt be a scaled time and Z r = αY t be a scaled effective sum of observations.
Set β = α/γ so that Z r /r = βY t /t.Let the continuous time stopping time for the diffusion, T , take the role of the discrete time stopping time, t 0 + T , shifted by the effective number of observations in the prior distribution.
Then the continuous time analog of (2) isV π * (y t 0 ,t 0 ) = sup π E π −cT + max{m, E[X T | Θ T ]}| Θ 0 = Θ ≈ sup T ≥t 0 E π −c( T − t 0 ) + max{m,Y T / T } | (y t 0 ,t 0 ) (diffusion approximation)(3)Note that the diffusion approximation in (3) can be replaced with a ≤ because the diffusion approximation allows for all real-valued stopping times including integers whereas the original problem only allows for stopping at integer-valued times.
Second, we change to a reverse-time coordinate system so that the statistics of the Gaussian process are simplified.
To this end, let s = 1/r = 1/(γt), W s = αY t /(γt) = βY t /t and m = β m. Set s 0 = 1/(γt 0 ) and w s 0 = αy t 0 /(γt 0 ).
If we set α 2 σ 2 = γ (to control the variance of W s ), then (W s , s) is a Brownian motion without drift in the −s scale from reverse time s 0 back to 0 (Chernoff 1961).
Then S = 1/(γ T ) is a stopping time in the −s scale.
With a bit of algebra that is not shown, one can seek to standardize the optimization problem in (3) as follows.B(m, y t 0 ,t 0 ) ∆ = sup T ≥t 0 E π −c( T − t 0 ) + max{m,Y T / T } |Y t 0 = y t 0 (4) = m + sup S∈[0,s 0 ] β −1 E − cα γ 2 1 S − 1 s 0 + max{0,W S } |W s 0 = w s 0 − m .
(5)If, in addition to the constraints β = α/γ and α 2 σ 2 = γ that result in W s being a Brownian motion in the −s scale, we add the constraint cα = γ 2 , then (5) simplifies even further.
In particular, these three constraints imply thatα = c 1/3 σ −4/3 , β = c −1/3 σ −2/3 and γ = c 2/3 σ −2/3 ,(6)and with these parameter values, the supremum of the expectation in (5) isB(w s 0 − m, s 0 ) ∆ = sup S∈[0,s 0 ] E −( 1 S − 1 s 0 ) + max{0,W S } |W s 0 = w s 0 − m .
(7)This development leads to the following lemma.
It implies that only the standardized problem (c = 1, σ = 1, m = 0) in (7) need be solved in order to get the diffusion approximation V π * (y t 0 ,t 0 ) ≈ B(m, y t 0 ,t 0 ) for any m, c > 0 and σ > 0.
Proposition 1.
If B is as in (4), B is as in (7) and α, β , and γ are as in (6), then B(m, y t 0 ,t 0 ) = m + β −1 B(β (y t 0 t 0 − m),t 0 ).
The key to the optimal solution when k = 1, then, is to solve for the optimal stopping problem in (7).
Standard results ( Chernoff 1961, Bather 1970 can be used to show that the function B in (7) satisfies a free-boundary problem for a heat equation.
Such a problem is a partial differential equation whose boundary is not fixed.
The boundary is determined by a condition that expresses an indifference between the options of stopping and of continuing to sample.
The solution to such a problem determines a boundary that defines a continuation region, C , within which it is optimal to continue sampling.
Outside of C , it is optimal to stop sampling and to choose the better of taking a known m or implementing the simulated system.
See Chick and Frazier (2009) for details on how to derive and numerically solve that free boundary problem.
We implemented a finite differences (FD) scheme with Matlab in order to solve the free boundary problem that corresponds to (7).
This section describes the results from that FD scheme and applies it to some numerical examples.
Figure 1 displays the optimal stopping boundary for the standardized problem with hatched lines for two ranges of s. Chick and Frazier (2009) gives a 'Quick Approximation' for the optimal stopping boundary, ˜ b(s), for the range s ∈ (0, 6300).
Such an approximation obviates the need for other users to implement and solve the PDE for cases with k = 1.
The figure also displays the contours of the expected benefit for continuing to sample when it is optimal to do so, B(w, s) − max(0, w).
The continuation set in Figure 1 has both an upper and a lower stopping boundary.
This differs from the solution to the optimal continuation set for the selection problem when the discount rate is positive.
If the discount rate is positive but the cost of sampling is 0, then there is only an upper stopping boundary and one would only simulate for a finite time if the default reward m exceeds the NPV of simulating forever (Chick and Gans 2009).
Here, the penalty for simulating forever is infinite (unlike the case of a positive discount rate) and the positive sampling cost implies that if a system is sufficiently worse than m, one would prefer to stop sampling altogether and accept a known expected reward of m.For small values of s (left panel of Figure 1) the upper boundary of the stopping region appears to be convex in s and is narrow.
Small values of s indicates that a large number of samples have been observed, so the value of the mean is likely to be close to the scaled posterior mean, w. Only a narrow range of possible values of w merit additional sampling.For larger values of s (right panel of Figure 1) the upper boundary of the stopping region appears to be concave in s and includes a wider range of values of w in the continuation region.
This is because there is still a great deal of uncertainty about w when s is large, and sampling has more value.The functional form for the upper and lower boundaries in (y t /t,t) coordinates is m ± β −1 b(1/(γt).
That is, one continues sampling as long as the posterior mean, y t /t, is in the interval Figure 1: The optimal continuation set of the standardized free boundary problem that corresponds to (7) is between the dashed lines.
The solid lines give level sets of the expected reward of continuing when it is optimal to do so, B(w, s) − max(0, w).
Figure 2 shows contours that demonstrate the expected benefit of continuing to sample, given that it is optimal to do so, relative to stopping prematurely.
It does so in coordinates that are more natural for a simulation study (y t ,t rather than w s , s).
Mathematically, that benefit is B(m, y t ,t) − max{m, y t /t}.
Above the upper dashed line and below the lower dashed line it is optimal to stop immediately.
In those stopping regions, the expected benefit of following the optimal policy is 0.
The optimal stopping boundary in this section is useful for two reasons.
First, the PDE need not be implemented to re-solve the problem optimally when k = 1.
Instead, the optimal stopping boundary for the standard problem is readily approximated with an easy to compute function (Chick and Frazier 2009), and this standard stopping boundary is easily transformed to the stopping boundary appropriate to the problem that one wishes to solve.
Second, it allows assessing the effectiveness and calculating the suboptimality of the stopping rules that are developed below in Section 4.2.
Section 5 will pursue this with numerical experiments.
[m − c 1/3 σ 2/3 b(σ 2/3 /(c 2/3 t)), m + c 1/3 σ 2/3 b(σ 2/3 /(c 2/3 t))].
The approach in Section 3 for k = 1 simulated system, that of deriving a diffusion model and numerically solving a corresponding free boundary problem to approximate the optimal solution, would be unwieldly for large numbers of systems.
We therefore seek an alternative approach to developing a sequential selection procedure for k > 1.
The key elements of Chick and Frazier a sequential procedure are (i) a stopping rule that determines when to stop sampling, (ii) an allocation rule that identifies which system to sample when continuing, and (iii) a selection rule that chooses a system to implement when sampling has stopped.This section presents background useful for deriving several different stopping and allocation rules.
It then presents the rules themselves.
Although the question of which stopping and allocation rules are best is open, the optimal selection rule is well understood and easy to compute.
It is known that, conditional on being forced to select, the selection rule that minimizes the expected regret of a potentially incorrect selection is the decision that picks the system with the largest posterior mean reward (Chick and Inoue 2001).
In the context of the present work, this means it is optimal to pick the system with the maximal expected reward when stopping.
We use this selection rule uniformly.
The optimal expected reward (OER) V π * ( Θ) of the simulation selection problem is at least as large as the expected reward of any given policy.
That includes so-called one-stage allocation policies.
We use the idea of one-stage allocation policies to develop bounds for the OER and stopping rules when k > 1.
Formally, a one-stage allocation policy maps each sampling budget ß ≥ 1 to an allocation of those samples across the k systems, with a total of τ i = τ i (ß) ≥ 0 replications to be run for system i, so that ∑ k i=1 c i τ i = ß.
We set τ = (τ 1 , τ 2 , . . . , τ k ) and allow ß and the τ i to be real-valued for mathematical convenience.
In implementations, the values of τ i will be rounded to integer values.
We require a one-stage allocation to have the property that all of the τ i are all nondecreasing as ß increases.
For example, the equal allocation sets τ i = ß/k (rounding to retain the integer budget constraint if needed, while retaining the monotonicity of the τ i in ß).
After observing those samples, a one-stage allocation policy selects the system with the largest posterior expected reward, if that reward exceedsµ 00 ∆ = E[X 0 ] = m(8)and otherwise selects the default system, which has reward m. Suppose further that samples from system i are normally distributed with known variance σ 2 i , but unknown mean.
The prior distribution of this unknown mean is Normal µ 0i , σ 2 i /t 0i , with µ 0i = y 0i /t 0i .
Then the posterior mean of our belief about this sampling mean that will result after the future sampling is performed is the random variable (de Groot 1970)Z i ∼ Normal µ 0i , σ 2 i τ i t 0i (t 0i + τ i ) .
(9)If we consider the allocation to be a function of ß and vary ß over all feasible sampling budgets, we obtain the following lower bound.
The bound refers to order statistics (i) for i = 0, 1, . . . , k, which are defined so that µ 0(0) ≤ µ 0(1) ≤ . . . ≤ µ 0(k−1) ≤ µ 0(k) .
These statistics are updated as samples are observed.Proposition 2.
Let V π * Θ maximize (1) and τ = τ(ß) be a one-stage allocation.
Let Z i be the (random) posterior mean given that τ i replications for system i will be run.
ThenV π * ( Θ) ≥ OER( Θ) ∆ = sup ß≥1 E [max{µ 00 , Z 1 , Z 2 , . . . , Z k }] − ß.
(10)Complementing OER( Θ), we also define OER( Θ)∆ = E [max{µ 00 ,U 1 ,U 2 , . . . ,U k }]as the the expected reward of having perfect information about each system at no cost and selecting the best system.The expectation on the right hand size of (10) has some easy-to-compute bounds.Proposition 3.
Let τ be a one-stage allocation, assume that output is jointly independent and normally distributed with a known variance, and normal prior distribution so that (9) µ 0(k) + max i:i =(k) σ Z,i,(k),τ Ψ ∆ 0i σ Z,i,(k),τ ≤ E [max{µ 00 , Z 1 , Z 2 , . . . , Z k }] ≤ µ 0(k) + ∑ i:i =(k) σ Z,i,(k),τ Ψ ∆ 0i σ Z,i,(k),τ .
(11)Proposition 4 notes that the expectation on the right hand size of (10) is available analytically when k = 1 or when all samples are allocated to a single system.
The lemma and the development below refer to the notation in Proposition 3.
Proposition 4.
If the allocation τ allocates its entire budget to system i, that is, if τ i = ß/c i and τ j = 0 for all j = i, thenE [max{µ 00 , Z 1 , Z 2 , . . . , Z k }] = µ 0(k) + σ Z,i,τ Ψ ∆ 0i σ Z,i,τ .
When k = 1, the optimal stopping rule is known.
When k ≥ 1, the bounds and approximations in Section 4.1 provide additional mechanisms to assess whether the expected reward of continuing to sample exceeds the expected cost of continuing to sample.
The following stopping rules codify some of these tradeoffs.
KG ß stopping rule.
Continue to sample if and only if there is a system i such that using the ß exclusively for that system produces a net expected benefit, as computed by Proposition 4.
Thus, the KG ß stopping rule calls for sampling to continue if only if max i∈{0,1,.
.
.
,k} σ Z,i,τ Ψ [∆ 0i /σ Z,i,τ ] > ß,ν KG,ß i = 1 ß σ Z,i,τ Ψ ∆ 0i σ Z,i,τ .
This extends an earlier KG stopping rule that required ß = 1 and c i = 1 (Frazier and Powell 2008).
Instead of considering a fixed ß, we can consider the expected benefit obtained and cost incurred from allocating any budget ß entirely to a single system.
This idea gives rise to the following stopping rule.KG * stopping rule.
Continue to sample if and only if there exists a budget ß ≥ 1 such that the KG ß stopping rule calls for continuing.
That happens when ν KG, * i > 1 for some i, whereν KG, * i = sup ß≥1 1 ß σ Z,i,τ Ψ ∆ 0i σ Z,i,τ .
(12)The KG ß and KG * stopping rules test if allocating samples to a single system can justify continuing.
The next stopping rule allows for a specified one-stage allocation to sample from multiple systems when checking whether to continue sampling or not.
Further, it further increases sampling by using an upper bound on the expected reward of sampling.EOC c,k stopping rule.
Recall (11) and (10).
Continue sampling if and only if there is a budget ß ≥ 1 such thatµ 0(k) + ∑ i∈{0,1,...,k}\{(k)} σ Z,i,(k),τ Ψ ∆ 0i σ Z,i,(k),τ − ß > µ 0(k) .
(13)4.3 Allocations for k > 1The stopping rule indicates whether or not to continue sampling.
If sampling is to continue, we must also decide how to allocate the next sample to one of the k systems.
Previous work and the stopping rules in Section 4.2 suggest several allocations.
KG 1 /LL 1 Allocation.
The KG 1 /LL 1 allocation greedily allocates one sample at a time (e.g., if ß = c i for all i) to a single system in order to maximize the expected value of information (EVI), were the procedure to collect only one more sample.
That EVI, when the sample is allocated to system i,is EVI i = σ Z,i,τ Ψ [∆ 0i /σ Z,i,τ ].
The KG policy of Chick and Frazier for known variances and the LL 1 allocation of for unknown variances work on the same principle.
This allocation is also the (R1, R1, . . . , R1) rule of Gupta and Miescke (1996).
KG ß Allocation.
One sample is allocated to the system that maximizes ν KG,ß i .
Although a measurement is valued according to the average value obtained over a budget ß worth of samples, only one sample is allocated at a time.KG * Allocation.
While the KG ß allocation values a measurement according to the average benefit obtained by measuring it ß times, KG * values it according to the best average benefit obtainable, which is ν KG, * i .
The KG * allocation allocates one sample to the system that maximizes ν KG, * i .
The one-stage LL allocation asymptotically maximizes the right hand side of (11), subject to the constraint that the total number of samples is ß, for large values of ß (see Chick and Gans 2009 for the variation with known sample variances).
The sequential LL allocation takes advantage of this and is therefore aligned with the EOC c,k stopping rule.
It works as follows.Sequential LL Allocation.
In (13), there is an optimal budget ß * .
With that budget, the one-stage LL allocation allocates τ 1 (ß * ), τ 2 (ß * ), . . . , τ k (ß * ) samples to the k systems.
Sequential LL uses this batch allocation to choose a single sample to allocate next, allocating one sample to system arg max{c i τ i (ß * )}.
Equal Allocation.
The equal allocation allocates one replication at a time in round robin fashion to the k systems, so that each system has roughly the same number of samples at any time until sampling stops.
The above stopping and allocation rules allow for some flexibility in their specification and some approximations or tricks can speed up the computation of the quantities they use.
There are also numerical stability issues to resolve when implementing the conceptual models above.
For example, the optimal budget for the KG * stopping rule can be solved approximately with a readily-computed˜ßcomputed˜ computed˜ß * rather than by a brute force search for the optimal ß * .
Chick and Frazier (2009) provide more details.
Simulation results are presented for the case of k = 1 and then for the case k > 1.
With k = 1 we can compare the known optimal result for the diffusion with the approximations in the above stopping rules, in order to assess their degree of suboptimality.
With k > 1, an optimal diffusion result is not as tractable, and so we assess the performances of the various procedures by comparing them with bounds on their performance.
In each Monte Carlo (MC) experiment, 10 6 random problem instances were generated from the prior distribution Θ 0 for the unknown means of each system, and the average performance was assessed by applying each simulation selection procedure to each problem instance.
Figure 3 shows the stopping boundaries that delimit the continuation sets for several stopping rules.
The figure presumes that c = 1, µ 0 = 0, σ 2 = 10 5 , t 0 = 1 and m = 0.
The optimal stopping boundary somewhat exceeds the KG * stopping boundary and the relative error percentage increases somewhat as t increases.
The KG * continuation set coincides, theoretically, with the EOC c,k continuation set for the special case of k = 1.
The KG * continuation sets in turn contain the continuation sets of the KG ß stopping rules.
Those continuation sets are not nested for ß = 1, 10 and 100.
The figure also shows that the KG * boundary, as computed with a brute force search for the optimal ß * , is closely approximated by stopping boundary when ß * is approximated by˜ßby˜ by˜ß * .
Table 1 shows MC simulation results for these stopping rules when c = 0, µ 0 = 0, σ = 10 5 and m = 0 for both t 0 = 1 and t 0 = 100.
The table reports estimates (± one standard error) of the expected number of samples, E [T ], the expected opportunity cost, E[U [k] − U (k) | Θ T ], and the expected reward, E[max{m, E[X T | Θ T ]} − cT | Θ 0 ], where T depends on the specified stopping rule.
The PDE solution, as calculated via a first-order bias corrected FD scheme, is slightly higher than the MC estimates based on discrete time sampling and the estimated optimal stopping boundary, as expected.
The relative level of sub-optimality for each stopping rule is calculated in a comparison with the MC value of the reward with the estimated optimal stopping boundary.For both t 0 = 1 and t 0 = 100 we observe that there is an ordering of the stopping rules, which is decreasing in the expected number of samples, increasing in the expected opportunity cost, and decreasing in the expected reward.
The optimal stopping rule (as estimated with the FD approximation) performs best, with EOC c,k second best, followed by KG * and KG in this order.
This ordering in reward, opportunity cost, and number of samples is explained by the nesting of the The poor performance of KG 1 when t 0 = 100 can be explained by recalling Figure 3.
When t is in the range from 80 to 3000, the KG 1 stopping boundary is quite a bit lower than the optimal stopping boundary.
But that is the range of t that is most likely going to lead to stopping when t 0 = 100, given the value of E[T ].
When t < 80 or t > 3000, then the KG 1 stopping boundary is a relatively better estimate of the optimal stopping boundary, and so the KG 1 stopping rule performs relatively better, as compared to the optimal, when t 0 = 1.
A similar argument explains why the KG * and EOC c,k stopping rules perform less well when t 0 = 100 as compared with when t 0 = 1 (although these rules are both better than KG 1 ).
While the EOC c,k and KG * stopping rules are equivalent, if implemented without any approximations, in our implementation the KG * stopping rule uses an approximatioñ ß * to the optimal budget for KG, as noted in Section 4.4.
The implementation of EOC c,k checks the budget˜ßbudget˜ budget˜ß * to see if it merits continuing as well as other values of the budget.
Thus its implemented continuation region is slightly larger.
Table 2 summarizes the performance of various allocation and stopping rules for k > 1.
As we observed in the k = 1 case, the EOC c,k stopping rule performed the best, with other stopping rules stopping earlier, incurring larger opportunity costs, and lower rewards.
KG * performed second best, taking more samples than KG 1 and similarly achieving lower opportunity costs and higher rewards.
The KG 1 allocation with the KG 1 stopping rule performs the poorest of these allocations.
It is even worse than the optimal one-stage allocation (whose relevant values are in the rows labeled minOER) for the expected total reward and expected opportunity cost of an incorrect selection (but the one-stage allocation requires, by far, more sampling).
Thus, the KG 1 stopping rule is myopic and leads to suboptimal performance due to early stopping, even though the KG 1 (aka LL 1 ) allocation has been shown to have attractive properties and performance in other settings.
The use of the EOC c,k stopping rule is less myopic, and leads to an improved performance of the KG 1 allocation with that stopping rule.
The fully sequential KG * allocation with the KG * stopping rule performs somewhat better than the optimal two-stage allocation, whose reward is given by OER.In the table we observe a real improvement with the EOC c,k stopping rule.
When using the EOC c,k stopping rule, it does not seem to affect the expected net reward whether we use the sequential LL or KG * allocation rule.
The difference between those rules for this experiment is that LL samples slightly more and achieves a slightly smaller expected opportunity cost.The differences in performance between the policies grows with k.
As k grows, it seems likely that the approximations made by KG * and KG 1 (to consider allocations to only a single system) become more restrictive and less justified, causing Table 1: Performance of stopping rules for k = 1, c = 1 and σ = 10 5 calculated using Monte Carlo simulation with 10 6 samples.
In the upper table, t 0 = 1, and in the lower table, t 0 = 100.
Note that the KG * and EOC c,k stopping rules are identical for k = 1, except for the more thorough way in which EOC c,k searches the set of budgets.
Stopping Rule (µ 0 ,t 0 ) = (0, 1) E 10.53 ± 0.01 2504.5 ± 4.6 1474.4 ± 4.6 56.69% Note: PDE estimate is V π * (0,t 0 ) ≈ B(0, 0,t 0 ) = 3407, bounds are OER = 3989 and OER = 3170.
the difference in value between the best single system allocation and the best allocation overall to become larger.
The EOC c,k stopping rule accounts for the benefits of allocations that split observations over multiple systems.
This would explain the increased tendency to early stopping (not absolutely, but relative to EOC c,k ) observed in these rules with increased k. Ranking and selection in simulation has typically focused on frequentist or Bayesian rules that have statistical criteria for sampling and stopping.
This paper used the cost of sampling to determine when to stop sampling.
It extended previous KG/LL 1 work by examining more closely the expected value of the option to continue sampling, and not just the expected value of information of a single stage of sampling.
It extended previous work on the economics of simulation by indicating how to handle sampling costs in the absence of discounting costs.
r Table 2: Performance of sampling and stopping rules for k > 1, c = 1, σ = 10 5 , m = 0 and t 0 = 1, calculated using Monte Carlo simulation with 10 6 samples.
Table 2: Performance of sampling and stopping rules for k > 1, c = 1, σ = 10 5 , m = 0 and t 0 = 1, calculated using Monte Carlo simulation with 10 6 samples.
