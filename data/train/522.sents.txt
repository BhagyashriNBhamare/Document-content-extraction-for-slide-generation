We consider a generalization of the PDB homomorphism abstractions to what is called "structural patterns".
The basic idea is in abstracting the problem in hand into provably tractable fragments of optimal planning, alleviating by that the constraint of PDBs to use projections of only low di-mensionality.
We introduce a general framework for additive structural patterns based on decomposing the problem along its causal graph, suggest a concrete non-parametric instance of this framework called fork-decomposition, and formally show that the admissible heuristics induced by the latter abstractions provide state-of-the-art worst-case informativeness guarantees on several standard domains.
The difference between various algorithms for optimal planning as heuristic search is mainly in the admissible heuristic functions they define and use.
Very often an admissible heuristic function for domain-independent planning is defined as the optimal cost of achieving the goals in an over-approximating abstraction of the planning problem in hand (Pearl 1984;Bonet & Geffner 2001).
Such an abstraction is obtained by relaxing certain constraints present in the specification of the problem, and the desire is to obtain a tractable (that is, solvable in polynomial time), yet informative abstract problem.
The main question here is thus: What constraints should we relax to obtain such an effective over-approximating abstraction?The abstract state space induced by a homomorphism abstraction is obtained by a systematic contracting groups of original problem states into abstract states.
Most typically, such state-gluing is obtained by projecting the original problem onto a subset of its parameters, eliminating the constraints that fall outside the projection.
Homomorphisms have been successfully explored in the scope of domainindependent pattern database (PDB) heuristics (Edelkamp 2001;Haslum et al. 2007), inspired by the (similarly named) problem-specific heuristics for search problems such as (k 2 − 1)-puzzles, Rubik's Cube, etc. (Culberson & Scha- effer 1998).
A core property of the PDB heuristics is that the problem is projected onto a space of a small (up to logarithmic) dimensionality so that reachability analysis in that space could be done by exhaustive search.
This constraint implies a scalability limitation of the PDB heuristics-as the problems of interest grow, limiting patterns to logarithmic dimensionality might make them less and less informative with respect to the original problems.In this paper we consider a generalization of the PDB abstractions to what is called structural patterns.
As it was recently suggested by Katz and Domshlak (2007), the basic idea behind structural patterns is simple and intuitive, and it corresponds to abstracting the original problem to provably tractable fragments of optimal planning.
The key point is that, at least theoretically, moving to structural patterns alleviates the requirement for the abstractions to be of a low dimensionality.
The contribution of this paper is precisely in showing that structural patterns are far from being of theoretical interest only.
Specifically, we • Specify causal-graph structural patterns (CGSPs), a general framework for additive structural patterns that is based on decomposing the problem in hand along its causal graph.
• Introduce a concrete family of CGSPs, called fork decompositions, that is based on two novel fragments of tractable cost-optimal planning.
• Following the type of analysis suggested by Helmert and Mattmüller (2008), study the asymptotic performance ratio of the fork-decomposition admissible heuristics, and show that their worst-case informativeness on selected domains more than favorably competes with this of (even parametric) state-of-the-art admissible heuristics.
Problems of classical planning correspond to reachability analysis in state models with deterministic actions and complete information; here we consider state models captured by the SAS + formalism (Bäckström & Nebel 1995).
Definition 1 A SAS + problem instance is given by a quadruple Π = V, A, I, G, where:• V = {v 1 , . . . , v n } is a set of state variables, each associated with a finite domain dom(v i ).
The initial state I is aA C D B E F G t c 2 c 1 c 3 p1 p2Figure 1: Logistics-style example adapted from Helmert (2006a).
Deliver p 1 from C to G, and p 2 from F to E using the cars c 1 , c 2 , c 3 and truck t, and making sure that c 3 ends up at F .
The cars may only use city roads (thin edges), the truck may only use the highway (thick edge).
complete assignment, and the goal G is a partial assignment to V .
• A is a finite set of actions, where each action a is a pair pre(a), eff(a) of partial assignments to V called preconditions and effects, respectively.
Each action a ∈ A is associated with a non-negative real-valued cost C(a).
An action a is applicable in a state s ∈ dom(V ) iff s[v] = pre(a) [v] whenever pre(a) [v] is specified.
Applying a changes the value of v to eff(a) [v] if eff(a) [v] is specified.
In this work we focus on cost-optimal (also known as sequentially optimal) planning in which the task is to find a plan ρ ∈ A * for Π minimizing a∈ρ C(a).
Across the paper we use a slight variation of a Logisticsstyle example of Helmert (2006a).
This example is depicted in Figure 1, and in SAS + it hasV = {p1, p2, c1, c2, c3, t} dom(p1) = dom(p2) = {A, B, C, D, E, F, G, c1, c2, c3, t} dom(c1) = dom(c2) = {A, B, C, D} dom(c3) = {E, F, G} dom(t) = {D, E} I = {p1 = C, p2 = F, t = E, c1 = A, c2 = B, c3 = G} G = {p1 = G, p2 = E, c3 = F },and actions corresponding to all possible loads and unloads, as well as single-segment movements of the vehicles.
For instance, if action a captures loading p 1 into c 1 at C, then pre(a) = {p 1 = C, c 1 = C}, and eff(a) = {p 1 = c 1 }.
Given a problem Π = V, A, I, G, for any partial assignment x to V , and any V ⊆ V , by x [V ] we refer to the projection of x onto V .
Considering homomorphism abstractions of Π, each variable subset V ⊆ V defines an over-approximating pattern abstraction Π [V ] = V , A [V ] , I [V ] , G [V ] that is obtained by projecting the initial state, the goal, and all the actions' preconditions and effects onto V (Edelkamp 2001).
The idea behind the PDB heuristics is elegantly simple.
First, we select a (relatively small) set of subsets V 1 , . . . , V m of V such that, for 1 ≤ i ≤ m, the size of V i is sufficiently small to perform exhaustive-search reachability analysis in Π [Vi] .
Let h [Vi] (s) be the optimal cost of achieving the abstract goal G [Vi] from the abstract state s [Vi] .
Since each Π [Vi] is an overapproximating abstraction of Π, each h [Vi] (·) is an admissible estimate of the true cost h * (·).
Given that, if the set of abstract problems Π [V1] , . . . , Π [Vm] satisfy certain requirements of additivity (Felner, Korf, & Hanan 2004;Edelkamp 2001), then PDB heuristic can be set to m i=1 h [Vi] (s), and otherwise only to max m i=1 h [Vi] (s).
We now provide a syntactically slight, yet quite powerful generalization of the standard mechanism for constructing additive decompositions of planning problems along subsets of their state variables (Felner, Korf, & Hanan 2004;Edelkamp 2001).
Definition 2 Let Π = V, A, I, G be a SAS + problem, and let V = {V 1 , . . . , V m } be a set of some subsets of V .
An Vi] , and [Vi] , eff(a) [Vi] , thenadditive decomposition of Π over V is a set of SAS + prob- lems Π = {Π 1 , . . . , Π m }, such that (1) For each Π i = V i , A i , I i , G i , we have (a) I i = I [Vi] , G i = G [(b) if a [Vi] def = pre(a)A i = {a [Vi] | a ∈ A ∧ eff(a) [Vi] = ∅}.
(2) For each a ∈ A holds C(a) ≥ m i=1 C i (a [Vi] ).
(1)Definition 2 generalizes the idea of "all-or-nothing" action-cost partition from the literature on additive PDBs to arbitrary action-cost partitions-the original cost of each action is partitioned this or another way among the "representatives" of that action in the abstract problems, with Eq.
1 being the only constraint posed on this action-cost partition.Proposition 1 For any SAS + problem Π over variables V , any set of V 's subsets V = {V 1 , . . . , V m }, and any additive decomposition of Π over V, we have h * (s) ≥ m i=1 h * i (s [Vi] ) for all states s of Π.
1Structural Patterns: Basic Idea PDB heuristics and their enhancements are successfully exploited these days in the planning research (Edelkamp 2001;Haslum, Bonet, & Geffner 2005;Haslum et al. 2007).
However, the well-known Achilles heel of the PDB heuristics is that each pattern (that is, each selected variable subset V i ) is required to be small so that reachability analysis in Π [Vi] could be done by exhaustive search.
In short, computing h [Vi] (s) in polynomial time requires satisfying|V i | = O(log |V |) if |dom(v)| = O(1) for each v ∈ V i ,and satisfying |V i | = O(1), otherwise.
In both cases, this constraint implies an inherent scalability limitation of the PDB heuristics.
As the problems of interest grow, limiting patterns to logarithmic dimensionality will unavoidably make them less and less informative with respect to the original problems, and this unless the domain forces its problem instances to consist of small, loosely-coupled subproblems that can be captured well by individual patterns (Helmert & Mattmüller 2008).
However, as recently observed by Katz and Domshlak (2007), pattern databases are not necessarily the only way to proceed with homomorphism abstractions.
In principle, given a SAS + problem Π = V, A, I, G, one can select a set of subsets V 1 , . . . , V m of V such that, for 1 ≤ i ≤ m, the reachability analysis in Π [Vi] is tractable (not necessarily due to the size of but) due to the specific structure of Π [Vi] .
What is important here is that this requirement can be satisfied even if the size of each selected pattern V i is Θ(|V |).
In any event, having specified the abstract problems Π [V1] , . . . , Π [Vm] as above, the heuristic estimate is then formulated similarly to the PDB heuristics: If the set of abstract problemsc₁ c₂ c₃ t p₁ p₂ A C D B E F G D E at A at B at C at D at E at F at G in c₂ in c₁ in t in c₃ (a) (b) (c)Π [V1] , . . . , Π [Vm] satisfy additiv- ity, then we set h(s) = m i=1 h [Vi] (s), otherwise we set h(s) = max m i=1 h [Vi] (s).
A priori, this generalization of the PDB idea to structural patterns is appealing as it allows using patterns of unlimited dimensionality.
The pitfall, however, is that such structural patterns correspond to tractable fragments of cost-optimal planning, and the palette of such known fragments is extremely limited (Bäckström & Nebel 1995;Bylander 1994;Jonsson & Bäckström 1998;Jonsson 2007;Katz & Domshlak 2007).
Next, however, we show that this palette can still be extended, and this in the direction allowing us to materialize the idea of structural patterns heuristics.
The key role in what follows plays the causal graph structure that has already been exploited in complexity analysis of classical planning.
The causal graph CG(Π) = (V, E) of a SAS + problem Π = V, A, I, G is a digraph over the nodes V .
An arc (v, v ) belongs to CG(Π) iff v = v and there exists an action a ∈ A such that eff(a) [v ], and either pre(a) [v] or eff(a) [v] are specified.
In what follows, for each v ∈ V , by pred(v) and succ(v) we refer to the sets of all immediate predecessors and successors of v in CG(Π).
Though less heavily, we also use here the structure of the problem's domain transition graphs (Bäckström & Nebel 1995).
The domain transition graph DTG(v, Π) of v in Π is an arc-labeled digraph over the nodes dom(v) such that an arc (ϑ, ϑ ) belongs to DTG(v, Π) iff there is an action a ∈ A with eff(a) [v] = ϑ , and either pre(a) [v] is unspecified or pre(a) [v] = ϑ.
In that case, the arc (ϑ, ϑ ) is labeled with pre(a) [V \{v}] and C(a).
Figure 2 depicts the causal and (labels omitted) domain transition graphs for our running-example problem Π.
We now proceed with exploiting this structure of the problems in devising structural pattern heuristics.Though additive decomposition over subsets of variables is rather powerful, it is too general for our purposes because it does not account for any structural requirements one may have for the abstract problems.
For instance, focusing on the causal graph, when we project the problem onto subsets of its variables, we leave all the causal-graph connections between the variables in each abstract problem untouched.
In contrast, targeting tractable fragments of cost-optimal planning, here we aim at receiving abstract problems with causal graphs of specific structure.
This leads us to introducing what we call causal graph structural patterns; the basic idea here is to abstract the given problem Π along a subgraph of its causal graph, obtaining an over-approximating abstraction that preserves the effects of the Π's actions to the largest extent possible.Definition 3 Let Π = V, A, I, G be a SAS + problem, and G = (V G , E G ) be a subgraph of the causal graph CG(Π).
A causal-graph structural pattern (CGSP) Π G = V G , A G , I G , G G is a SAS + problem defined as follows.
[v ], and either [v ] is specified and either eff(a) [v] or pre(a) [v] is specified, and each1.
I G = I [V G ] , G G = G [V G ] , 2.
A G = a∈A A G (a), where each A G (a) = {a 1 , . . . , a l(a) }, l(a) ≤ |eff(a)|, is a set of actions over V G such that (a) for each a i ∈ A G (a), if eff(a i )eff(a i )[v] or pre(a i )[v] are specified, then (v, v ) ∈ E G .
(b) for each (v, v ) ∈ E G , s.t. eff(a)a i ∈ A G (a), if eff(a i )[v ] is specified, then either eff(a i )[v] or pre(a i )[v] is specified as well.
(c) for each s ∈ dom(V G ), if pre(a) [V G ] ⊆ s, then the action sequence ρ = a 1 · a 2 · . . . · a l(a) is applicable in s, and if applying ρ in s results in s ∈ dom(V G ), then s \ s = eff(a) [V G ] .
(d) C(a) ≥ l(a) i=1 C G (a i ).
For any SAS + problem Π = V, A, I, G, and any subgraph G = (V G , E G ) of the causal graph CG(Π), a CGSP Π G can always be (efficiently) constructed from Π, and it is ensured that CG(Π G ) = G.
The latter is directly enforced by the construction constraints in Definition 3.
To see the former, one possible construction of the action setsA G (a) is as follows-if {v 1 , . . . , v k } is the subset of V G affected by a, then A G (a) = {a 1 , . . . , a k } with eff(a i )[v] = ( eff(a)[v], v = vi unspecified, otherwise pre(a i )[v] = 8 > > > > > < > > > > > : eff(a)[v], v = vj ∧ j < i ∧ (vj, vi) ∈ EG pre(a)[v], v = vj ∧ j > i ∧ (vj, vi) ∈ EG pre(a)[v], v = vi pre(a)[v], v ∈ {v1, . . . , v k } ∧ (v, vi) ∈ EG unspecified, otherwiseNow, given a SAS + problem Π and a subgraph G of CG(Π), if the structural pattern Π G can be solved costoptimally in polynomial time, we can use its solution as an admissible heuristic for Π.
Moreover, given a set G = {G 1 , . . . , G m } of subgraphs of the causal graph CG(Π), these heuristic estimates induced by the structural patterns {Π G1 , . . . , Π Gm } are additive if holds a certain property given by Definition 4.
Definition 4 Let Π = V, A, I, G be a SAS + problem, and G = {G 1 , . . . , G m } be a set of subgraphs of the causal graph CG(Π).
An additive CGSP decomposition of Π over G is a set of CGSPs Π = {Π G1 , . . . , Π Gm } such that, for each action a ∈ A, holdsC(a) ≥ m i=1 a ∈A G i (a) C Gi (a ).
(2)Proposition 2 (Admissibility) For any SAS + problem Π, any set of CG(Π)'s subgraphs G = {G 1 , . . . , G m }, and any additive CGSP decomposition of Π over G, we haveh * (s) ≥ m i=1 h * i (s [V G i ] ) for all states s of Π.Relying on Proposition 2, we can now decompose any given problem Π into a set of tractable CGSPs Π = {Π G1 , . . . , Π Gm }, solve all these CGSPs in polynomial time, and derive an admissible heuristic for Π.
Note that (similarly to Definition 2) Definition 4 leaves the decision about the actual partition of the action costs rather open.
In what follows we adopt the most straightforward, uniform action-cost partitioning in which the cost of each action a is equally split among all the non-redundant abstractions of a in Π.
The choice of the action-cost partitioning, however, can sometimes be improved or even optimized; for further details see (Katz & Domshlak 2008).
We now introduce certain concrete additive decompositions of SAS + planning problems along their causal graphs.
In itself, these decompositions do not immediately lead to structural patterns abstractions, yet they provide an important building block on our way towards them.Definition 5 Let Π = V, A, I, G be a SAS + problem.
c₁ p₁ p₂ c₁ c₂ c₃ t p₁ CG(Π f c1 ) CG(Π if p1 )Figure 3: Causal graphs of a fork and an inverted fork structural patterns of the running example.-F-decomposition Π F = {Π f v } v∈V , -I-decomposition Π I = {Π i v } v∈V , and -FI-decomposition Π FI = {Π f v , Π i v } v∈V are additive CGSP decompositions of Π over sets of sub- graphs G F = {G f v } v∈V , G I = {G i v } v∈V , and G FI = G F ∪ G I , respectively, where, for v ∈ V , V G f v = {v} ∪ succ(v), E G f v = u∈succ(v) {(v, u)} V G i v = {v} ∪ pred(v), E G i v = u∈pred(v) {(u, v)}Note that all three fork-decompositions in Definition 5 are entirely non-parametric in the sense of flexibility left by the general definition of CGSPs.
Illustrating Definition 5, let us consider the (uniform) FI-decomposition of the problem Π from our running example, assuming all the actions in Π have the same unit cost.
After eliminating from G FI all the singletons 2 , we getG FI = {G f c1 , G f c2 , G f c3 , G f t , G i p1 , G i p2 }.
Considering the action sets of the problems in Π FI , each original driving action is present in some three problems in Π FI , while each load/unload action is present in some five such problems.
For instance, the action "drive-c 1 -from-Ato-D" is present in {Π f c1 , Π i p1 , Π i p2 }, and the action "load-p 1 - into-c 1 -at-A" is present in {Π f c1 , Π f c2 , Π f c3 , Π f t , Π i p1 }.
Since we assume a uniform partitioning of the action costs, the cost of each driving and load/unload action in each corresponding abstract problem is set to 1/3 and 1/5, respectively.From Proposition 2 we have the sum of costs of solving the problems Π FI ,h FI = v∈V h * Π f v + h * Π i v ,(3)being an admissible estimate of h * .
The question now is how good this estimate is.
The optimal cost of solving our problem is 19.
Taking as a basis for comparison the seminal (non-parametric) h max and h 2 heuristics (Bonet & Geffner 2001;Haslum & Geffner 2000), we have h max = 8 and h 2 = 13.
At the same time, we have h FI = 15, and hence it appears that using h FI is at least promising.
Unfortunately, despite the seeming simplicity of the problems in Π, turns out that fork-decompositions as they are do not fit the requirements of the structural patterns framework.
On the one hand, the causal graphs of {Π f c1 , Π f c2 , Π f c3 , Π f t } and {Π i p1 , Π i p2 } form directed forks and inverted forks, respectively (see Figure 3), and, in general, the number of variables in each such problem is Θ(n).
On the other hand, Domshlak and Dinitz (2001) show that even nonoptimal planning for SAS + problems with fork and inverted fork causal graphs is NP-complete.
Moreover, even if the domain-transition graphs of all the state variables are strongly connected, optimal planning for forks and inverted forks remain NP-hard (see Helmert (2003) and (2004) for the respective results).
In the next section, however, we show that this is not the end of the story for forkdecompositions.
While hardness of optimal planning for problems with fork and inverted fork causal graphs put a shadow on relevance of fork-decompositions, closer look at the proofs of the corresponding hardness results of Domshlak and Dinitz (2001) and Helmert (2003;2004) reveals that these proofs in particular rely on root variables having large domains.
It turns out that this is not incidental, and Propositions 3 and 4 below characterize some substantial islands of tractability within these structural fragments of SAS + .
Proposition 3 (Tractable Forks) Given a SAS + problem Π = V, A, I, G with a fork causal graph rooted at r, if (i) |dom(r)| = 2, or (ii) for all v ∈ V , |dom(v)| = O(1), then cost-optimal planning for Π is poly-time.
For the next proposition we use the notion of k-dependent actions-an action a is called k-dependent if it is preconditioned by ≤ k variables that are not affected by a (Katz & Domshlak 2007).
Proposition 4 (Tractable Inverted Forks) Given a SAS + problem Π = V, A, I, G with an inverted fork causal graph rooted at r ∈ V , if |dom(r)| = O(1) and all the actions A are 1-dependent, then cost-optimal planning for Π is poly-time.
Propositions 3 and 4 allow us to meet between the forkdecompositions and structural patterns.
The basic idea is to further abstract each CGSP in fork-decomposition of Π by abstracting domains of its variables to meet the requirements of the tractable fragments.
Such domain abstractions have been suggested for domain-independent planning by Hoff- mann et al. (2006), and recently successfully exploited in planning as heuristic search by Helmert et al. (2007).
Definition 6 Let Π = V, A, I, G be a SAS + problem, v ∈ V be a variable of Π, and Φ = {φ 1 , . . . , φ k } be a set of mappings from dom(v) to some setsΓ 1 , . . . , Γ k .
An ad- ditive domain decomposition of Π over Φ is a set of SAS + problems Π Φ = {Π 1 , . . . , Π k } such that (1) For each Π i = V i , A i , I i , G i , we have 3 (a) I i = φ i (I), G i = φ i (G), and (b) if φ i (a) def = φ i (pre(a)), φ i (eff(a)), then A i = {φ i (a) | a ∈ A ∧ φ i (eff(a)) ⊆ φ i (pre(a))}.
(2) For each a ∈ A holdsC(a) ≥ k i=1 C i (φ i (a)).
(4)Proposition 5 For any SAS + problem Π over variables V , any variable v ∈ V , any domain abstractionsΦ = {φ i } k i=1of v, and any additive domain decomposition of Π over Φ,we have h * (s) ≥ k i=1 h * i (φ i (s)) for all states s of Π.
Targeting tractability of the causal graph structural patterns, we now connect between fork-decompositions and domain decompositions as in Definition 6.
Given a FI-decomposition Π FI = {Π f v , Π i v } v∈V of Π, we • For each Π f v ∈ Π, associate the root r of CG(Π f v ) with mappings Φ v = {φ v,1 , . . . , φ v,kv }, k v = O(poly(|Π|)), and all φ v,i : dom(r) → {0, 1}, and then additively de-compose Π f v into Π f v = {Π f v,i } kv i=1 over Φ v .
• For each Π i v ∈ Π, first, reformulate it in terms of 1-dependent actions only; such a reformulation can easily be done in time/space O(poly(|Π i v |)).
Then, associate the root r of CG(Π i v ) with mappingsΦ v = {φ v,1 , . . . , φ v,k v }, k v = O(poly(|Π|)), and all φ v,i : (1), and then ad-dom(r) → {0, 1, . . . , b v,i }, b v,i = Oditively decompose Π i v into Π i v = {Π i v,i } k v i=1 over Φ v .
From Propositions 2 and 5 we then haveh FI = v∈V   kv i=1 h * Π f v,i + k v i=1 h * Π i v,i   ,(5)being an admissible estimate of h * for Π, and, from Propositions 3 and 4, h FI is also poly-time computable.
The question is, however, how further abstracting our fork decompositions using domain abstractions as above affects the informativeness of the heuristic estimate.
Below we show that the answer to this question can be somewhat surprising.To illustrate a mixture of structural and domain abstractions as above, here as well we use our running Logisticsstyle example.
To begin with an extreme setting of domain abstractions, first, let the domain abstractions for roots of both forks and inverted forks be to binary domains.
Among multiple options for choosing the mapping sets {Φ v } and {Φ v }, here we use a simple choice of distinguishing between different values of each variable v on the basis of their distance from I [v] in DTG(v, Π).
Specifically, for each v ∈ V , we set Φ v = Φ v , and, for each value ϑ ∈ dom(v),φ v,i (ϑ) = φ v,i (ϑ) = 0, d(I[v], ϑ) < i 1, otherwise(6)For example, the problem Π f c1 is decomposed (see Fig- ure 2b) into two problems, Π f c1,1 and Π f c1,2 , with the binary abstract domains of c 1 corresponding to the partitions {A}/{B, C, D} and {A, D}/{B, C} of dom(c 1 ), respectively.
Now, given the decomposition of Π over forks and {Φ v , Φ v } v∈V as above, consider the problem Π i p1,1 , obtained from abstracting Π along the inverted fork of p 1 and then abstracting dom(p 1 ) usingφp 1 ,1(ϑ) = ( 0, ϑ ∈ {C} 1, ϑ ∈ {A, B, D, E, F, G, c1, c2, c3, t}It is not hard to verify that, from the original actions affecting p 1 , we are left in Π i p1,1 only with actions conditioned by c 1 and c 2 .
If so, then no information is lost 4 if we 1.
remove from Π i p1,1 both variables c 3 and t, and the actions changing (only) these variables, and 2.
redistribute the (fractioned) cost of the removed actions between all other representatives of their originals in Π.
The latter revision of the action cost partitioning can be obtained directly by replacing the cost-partitioning steps corresponding to Eqs.
2 and 4 by a single, joint action cost partitioning applied over the final abstractionsv∈V (Π f v ∪ Π i v ) and satisfying C(a) ≥ X v∈V 0 B @ kv X i=1 X a ∈A G f v (a) C f v,i (φv,i(a )) + k v X i=1 X a ∈A G i v (a) C i v,i (φ v,i (a )) 1 C A(7)Overall, computing h FI as in Eq.
5 under "all binaryrange domain abstractions" provides us with h FI = 12 7 15 , and knowing that the original costs are all integers we can safely adjust it to h FI = 13.
Hence, even under the most severe domain abstractions as above, h FI does not fall from h 2 in our example problem.Let us now slightly relax our domain abstractions for the roots of the inverted forks to be to the ternary range {0, 1, 2}.
While mappings {Φ v } stay as before, {Φ v } are set to∀ϑ ∈ dom(v) : φ v,i = 8 > < > : 0, d(I[v], ϑ) < 2i − 1 1, d(I[v], ϑ) = 2i − 1 2, d(I[v], ϑ) > 2i − 1(8)For example, the problem Π i p1 is now decomposed into Π i p1,1 , . . . , Π i p1,3 along the abstractions of dom(p 1 ).
Applying now the same computation of h FI as in Eq.
5 over the new set of domain abstractions gives h FI = 15 1 2 , which, again, can be safely adjusted to h FI = 16.
Note that this value is higher than h FI = 15 obtained using the (generally intractable) "pure" fork-decomposition as in Eq.
3.
At first view, this outcome may seem counterintuitive as the domain abstractions are applied over the fork-decomposition, and one would expect a stronger abstraction to provide less precise estimates.
This, however, is not necessarily the case.
For instance, domain abstraction for the root of an inverted Table 1: Performance ratios of multiple heuristics in selected planning domains; ratios for h + , h k , h PDB , h PDB add are by Helmert and Mattmüller (2008).
Domain h + h k h PDB h PDB add h F h I h FI GRIPPER 2/3 0 0 2/3 2/3 0 1/3 LOGISTICS 3/4 0 0 1/2 1/2 1/2 1/2 BLOCKSWORLD 1/4 0 0 0 0 0 0 MICONIC 6/7 0 0 1/2 5/6 1/2 1/2 SATELLITE 1/2 0 0 1/6 1/6 1/6 1/6LOGISTICS, BLOCKSWORLD, MICONIC-STRIPS, SATELLITE, MICONIC-SIMPLE-ADL, and SCHEDULE.
(Familiarity with the domains is assumed; for an overview consult Helmert, 2006b.)
The h + estimate corresponds to the optimal cost of solving the well-known "ignore-deletes" abstraction of the original problem, and it is generally NP-hard to compute (Bylander 1994).
The h k , k ∈ N + , family of heuristics is based on a relaxation where the cost of reaching a set of n satisfied atoms is approximated by the highest cost of reaching a subset of k satisfied atoms (Bonet & Geffner 2001); computing h k is exponential only in k.
The h PDB and h PDB add heuristics are regular (maximized over) and additive (summed-up) pattern database heuristics where the size of each pattern is assumed to be O(log(n)), and (importantly) the choice of the patterns is assumed to be optimal.The results of Helmert and Mattmüller provide us with a baseline for evaluating our admissible heuristics h F , h I , and h FI corresponding to F-, I-, and FI-decompositions, respectively.
At this point, the reader may rightfully wonder whether some of these three heuristics are not dominated by the others, and thus are redundant for our analysis.
Proposition 6, however, shows that this is not the case-each of these three heuristics can be strictly more informative than the other two, depending on the problem instance and/or the state being evaluated.Proposition 6 (Undominance) None of the heuristic functions h F , h I , and h FI (with and/or without domain abstractions) dominate each other.
Table 1 presents now the asymptotic performance ratios of h F , h I , and h FI in GRIPPER, LOGISTICS, BLOCKSWORLD, MICONIC-STRIPS, and SATELLITE, 5 putting them in line with the corresponding results of Helmert and Mattmüller for h + , h k , h PDB , and h PDB add .
We have also studied the ratios of max{h F , h I , h FI }, and in these five domains they appear to be identical to these of h F .
6 Taking a conservative position, the performance ratios for the fork-decomposition heuristics in Table 1 are "worst-case" in the sense that (i) here we neither optimize the action-cost partitioning (setting it to "uniform"), nor eliminate clearly redundant patterns, and 5 We have not accomplished yet the analysis of MICONIC-SIMPLE-ADL and SCHEDULE; the action sets in these domains are much more heterogeneous, and thus more involved for analytic analysis of fork-decompositions.
6 Note that "ratio of max" should not necessarily be identical to "max of ratios".
(ii) use domain abstractions to (up to) ternary-valued abstract domains only.
Specifically, the domains of the inverted-fork roots are all abstracted using the "distance-from-initial-value" ternaryvalued domain decompositions as in Eq.
8, while the domains of the fork roots are all abstracted using the "leaveone-out" binary-valued domain decompositions as in Eq.
9.
∀ϑ i ∈ dom(v) : φ v,i (ϑ) = 0, ϑ = ϑ i 1, otherwise(9)In a sketch, the results in Table 1 are obtained as follows.
GRIPPER Assuming n > 0 balls should be moved from one room to another, all the three heuristics h F , h I , h FI account for all the required pickup and drop actions, and only for O(1)-portion of move actions.
On the other hand, the former actions are responsible for 2/3 of the optimal-plan length (= cost).
Now, with the basic uniform action-cost partition, h F , h I , and h FI account for whole, O(1/n), and 1/2 of the total pickup/drop actions' cost, respectively, providing the ratios as in Table 1.
7 Optimal plan contains at least as much loads/unloads as move actions, and all the three heuristics h F , h I , h FI fully account for the former, providing a lower bound of 1/2.
An instance on which all three heuristics achieve exactly 1/2 consists of two trucks t 1 , t 2 , no airplanes, one city, and n packages such that the initial and goal locations of all the packages and trucks are all pair-wise different.
Mattmüller (2008) for h PDB add .
MICONIC-STRIPS All the three heuristics fully account for all the loads/unloads.
In addition, h F accounts for the full cost of all the moves to the passengers' initial locations, and for half of the cost of all other moves.
This provides us with the lower bounds of 1/2 and 5/6, respectively.
Tightness of 1/2 for h I and h FI is, e.g., on the instance consisting of n passengers, 2n+1 floors, and all the initial and goal locations being pair-wise different.
Tightness of 5/6 for h F is, e.g., on the instance consisting of n passengers, n + 1 floors, the elevator and all the passengers are initially at floor n + 1, and each passenger i wishes to get to floor i.
The length of an optimal plan for a problem with n images to be taken and k satellites to be moved to some end-positions is ≤ 6n + k. All the three heuristics fully account for all the image-taking actions, and one satellite-moving action per satellite as above, providing a lower bound of 1 6 .
Tightness of 1/6 for all three heuristics is on the following instance: Two satellites with instruments {i} l i=1 and {i} 2l i=l+1 , respectively, where l = n − √ n. Each pair of instruments {i, l + i} can take images in modes {m 0 , m i }.
There is a set of directions {d j } n j=0 and a set of image objectives {o i } n i=1 such that, for 1 ≤ i ≤ l, o i = (d 0 , m i ), and, for l < i ≤ n, o i = (d i , m 0 ).
Finally, the calibration direction for each pair of instruments {i, l + i} is d i .
Overall, the results for fork-decomposition heuristics in Table 1 are very gratifying.
First, note that the performance ratios for h k and h PDB are all 0.
This is because every kelementary (for h k ) and log(n)-elementary (for h PDB ) subgoal set can be reached in the number of steps that only depends on k (respectively, log(n)), and not n, while h * (s n ) grows linearly in n in all the five domains.
This leaves us with h PDB add being the only state-of-the-art (tractable and) admissible heuristic to compare with.
Table 1 shows that the asymptotic performance ratio of max{h F , h I , h FI } is at least as good as this of h PDB add in all five domains, and it is superior to h PDB add in MICONIC-STRIPS, getting here quite close to h + .
Comparing between h PDB add and fork-decomposition heuristics, it is crucial to recall that the ratios devised by Helmert and Mattmüller for h PDB add are with respect to optimal, manually-selected set of patterns.
In contrast, forkdecomposition heuristics are completely non-parametric, and thus require no tuning of the pattern-selection process.
We presented a generalization of the pattern-database projections, called structural patterns, that is based on abstracting the problem in hand to provably tractable fragments of optimal planning.
The key motivation behind this generalization of PDBs is to alleviate the requirement for the patterns to be of a low dimensionality.
We defined the notion of (additive) causal graph structural patterns (CGSPs), and studied their potential on a concrete CGSP framework based on decomposing the problem into a set of fork and inverted fork components of its causal graph, combined with abstracting the domains of certain variables within these individual components.
We showed that the asymptotic performance ratios of the resulting heuristics on selected planning domains are at least as good, and sometimes transcends, these of the state-of-the-art admissible heuristics.The basic principles of the structural patterns framework motivate further research in numerous directions, and in particular, in (1) discovering new islands of tractability of optimal planning, and (2) translating and/or abstracting the general planning problems into such islands.
Likewise, currently we explore combining structural patterns (and, in particular, CGSPs) with PDB-style projection patterns, as well as with more flexible "merge-and-shrink" abstractions suggested in (Helmert, Haslum, & Hoffmann 2007).
Very roughly, the idea here is to "glue" and/or "duplicate" certain variables prior to projecting the problem in hand onto its tractable structural components.
We believe that a successful such combination of techniques has a potential to improve the effectiveness of the heuristics, and in particular their domain-dependent asymptotic performance ratios.
sGoing beyond our running example, obviously we would like to assess the effectiveness of fork-decomposition heuristics on a wider set of domains and problem instances.
A standard method for that is to implement admissible heuristics within some optimality-preserving search algorithm, run it against a number of benchmark problems, and count the number of node expansions performed by the search algorithm.
The fewer nodes the algorithm expands, the better.
While such experiments are certainly useful and important, as noted by Helmert and Mattmüller (2008), their results almost never lead to absolute statements of the type "Heuristic h is well-suited for solving problems from benchmark suite X", but only to relative statements of the type "Heuristic h expands fewer nodes than heuristic h on a benchmark suite X".
Moreover, one would probably like to get a formal certificate for the effectiveness of her heuristic before proceeding with its implementation.
In what follows, we formally analyze the effectiveness of the fork-decomposition heuristics similarly to the way Helmert and Mattmüller (2008) study some state-of-theart admissible heuristics.
Given domain D and heuristic h, Helmert and Mattmüller consider the asymptotic performance ratio of h in D.
The goal is to find a value, and (ii) there is a family of problems {Π n } n∈N ⊆ D and solvable, non-goal states {s n } n∈N such that s n ∈ Π n , lim n→∞ h * (s n ) = ∞, and.
In other words, h is never worse than α(h, D) · h * (plus a sublinear term), and it can become as bad as α(h, D) · h * (plus a sublinear term) for arbitrary large inputs; note that the existence and uniqueness of α(h, D) are guaranteed for any h and D.Helmert and Mattmüller (2008) study the asymptotic performance ratio of the admissible heuristics h + , h k , h PDB , and h PDB add on some benchmark domains from the first four International Planning Competitions, namely GRIPPER, Going beyond our running example, obviously we would like to assess the effectiveness of fork-decomposition heuristics on a wider set of domains and problem instances.
A standard method for that is to implement admissible heuristics within some optimality-preserving search algorithm, run it against a number of benchmark problems, and count the number of node expansions performed by the search algorithm.
The fewer nodes the algorithm expands, the better.
While such experiments are certainly useful and important, as noted by Helmert and Mattmüller (2008), their results almost never lead to absolute statements of the type "Heuristic h is well-suited for solving problems from benchmark suite X", but only to relative statements of the type "Heuristic h expands fewer nodes than heuristic h on a benchmark suite X".
Moreover, one would probably like to get a formal certificate for the effectiveness of her heuristic before proceeding with its implementation.
In what follows, we formally analyze the effectiveness of the fork-decomposition heuristics similarly to the way Helmert and Mattmüller (2008) study some state-of-theart admissible heuristics.
Given domain D and heuristic h, Helmert and Mattmüller consider the asymptotic performance ratio of h in D.
The goal is to find a value, and (ii) there is a family of problems {Π n } n∈N ⊆ D and solvable, non-goal states {s n } n∈N such that s n ∈ Π n , lim n→∞ h * (s n ) = ∞, and.
In other words, h is never worse than α(h, D) · h * (plus a sublinear term), and it can become as bad as α(h, D) · h * (plus a sublinear term) for arbitrary large inputs; note that the existence and uniqueness of α(h, D) are guaranteed for any h and D.Helmert and Mattmüller (2008) study the asymptotic performance ratio of the admissible heuristics h + , h k , h PDB , and h PDB add on some benchmark domains from the first four International Planning Competitions, namely GRIPPER,
