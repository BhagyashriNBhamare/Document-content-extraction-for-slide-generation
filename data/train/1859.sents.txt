Device drivers may encounter errors when communicating with OS kernel and hardware.
However, error handling code often gets insufficient attention in driver development and testing, because these errors rarely occur in real execution.
For this reason, many bugs are hidden in error handling code.
Previous approaches for testing error handling code often neglect the characteristics of device drivers, so their efficiency and accuracy are limited.
In this paper, we first study the source code of Linux drivers to find useful characteristics of error handling code.
Then we use these characteristics in fault injection testing, and propose a novel approach named EH-Test, which can efficiently test error handling code in drivers.
To improve the representativeness of injected faults, we design a pattern-based extraction strategy to automatically and accurately extract target functions which can actually fail and trigger error handling code.
During execution, we use a monitor to record runtime information and pair checkers to check resource usages.
We have evaluated EH-Test on 15 real Linux device drivers and found 50 new bugs in Linux 3.17.2.
The code coverage is also effectively increased.
Comparison experiments to previous related approaches also show the effectiveness of EH-Test.
As important components of the operating system, device drivers control hardware and provide fundamental supports for high-level programs.
During driver execution, different kinds of occasional errors may occur, such as kernel exceptions and hardware malfunctions [31].
Therefore, device drivers need error handling code to assure reliability.
But in some drivers, error handling code is incorrect or even missed.
In these drivers, serious problems like system crashes and hangs may occur when occasional errors are triggered.
According to our study on Linux driver patches, more than 40% of accepted patches add or update corresponding error handling code.
It shows that error handling code in device drivers is not reliable enough, so testing error handling code and detecting bugs inside are very necessary.A challenge of testing error handling code is that occasional errors are infrequent to happen in real execution [34].
For example, "bad address" (EFAULT) is a common error should be handled, but it happens only when the memory or I/O address is invalid.
Another example is hardware error, which happens only when the hardware malfunctions.
Triggering these errors in real environment is very hard and uncontrollable.To simulate software and hardware errors at runtime, software fault injection (SFI) is often used in driver testing.
This technique mutates the code to inject specific errors into the program, and enforces error handling code to be executed at runtime.
Linux Fault Injection Capabilities Infrastructure (LFICI) [43] is a well-known project integrated in Linux kernel.
It can simulate common errors, such as memory-allocation failures and bad data requests.
Inspired by LFICI, other fault injection approaches [7,13,23,32] have been proposed in recent years, and they have shown promising results in driver testing and bug detection.
However, these approaches still have some limitations in practical use.
The representativeness of injected faults is often neglected, and most injected faults are random or manually selected.
Random faults can not reflect real errors well.
Manually selected faults often omit representative injected faults.
Numerous redundant test cases are generated.
In fact, many generated test cases may cover the same error handling code, but they all need to be actually tested at runtime.
For this reason, they often spend much time in runtime testing.
Only several kinds of faults can be injected, such as memory-allocation failures.
But these faults can not cover most error handling code in drivers.
Much manual effort is needed.
The kinds and places of injected faults are often manually decided.
In fact, previous fault injection approaches aim to support general software, but they neglect the characteristics of target programs.
To relieve their limitations, we should consider the key driver characteristics in SFI.
For example, because drivers are often written in C, so built-in error handling mechanisms (such as "try-catch") are not supported.
For this reason, the developers often use an if check to decide whether the error handling code should be triggered in device drivers.
This characteristic can help to decide which functions can actually fail and should be fault-injected.
In this paper, we first study Linux driver code, and find three useful characteristics in error handling code: function return value trigger, few branches and check decision.
Then based on these characteristics and SFI, we propose a practical approach named EH-Test 1 to efficiently test error handling code and detect bugs inside.
Firstly, EH-Test uses a pattern-based extraction strategy to extract target functions which can fail from the captured runtime traces of normal execution.
This strategy can automatically and accurately extract real target functions to improve the representativeness of inject faults.
Then, we generate test cases by corrupting the return values of target functions.
Next, we run each test case on the real hardware, and use a monitor to record runtime information and pair checkers to check resource-usage violations.
These pair checkers contain the basic information of resource-acquiring and resourcereleasing functions, which can be obtained from specification mining techniques [18,20,37,38] and user configuration.
During driver execution, system crashes and hangs can be easily identified through kernel crash logs or user observation.
After driver execution, EH-Test can report resource-release omissions.
We have implemented EH-Test using LLVM, and evaluated it on 15 Linux drivers of three classes.
The results show that EH-Test can accurately find real bugs in error handling code and improve code coverage in runtime testing.
Comparison experiments to previous approaches also show its effectiveness.Compared to previous SFI approaches for testing drivers, our approach have four advantages: 1) Representative injected faults.
We design a pattern-based extraction strategy to automatically and accurately extract real target functions as representative injected faults.
It uses code patterns to decide whether a function can actually fail in driver execution.
This strategy can largely improve the effectiveness of SFI.2) Efficient test cases.
According to our study, many drivers have few branches in error handling code, so injecting a single fault in each test case is enough to cover most error handling code.
Moreover, our patternbased extraction strategy can filter many unrepresentative injected faults.
Therefore, the test cases generated by EH-Test are efficient, and the time usage of runtime testing can be largely shortened.3) Accurate bug detection.
By injecting representative faults, EH-Test can realistically simulate different kinds of occasional errors to cover error handling code.
Moreover, EH-Test runs on the real hardware and uses exact execution information to perform analysis.
These points assure the accuracy of bug detection.
show that EH-Test can efficiently perform driver testing and accurately find real bugs.
The rest of this paper is organized as follows.
Section 2 introduces the motivation.
Section 3 presents the three characteristics of device drivers found by our study on Linux driver code.
Section 4 presents our pattern-based extraction strategy.
Section 5 introduces EH-Test in detail.
Section 6 shows our evaluation on 15 Linux device drivers and comparison experiments to previous approaches.
Section 7 introduces the related work and Section 8 concludes this paper.
To ensure the reliability of device drivers, error handling code should be correctly implemented to handle different kinds of occasional errors.
But in fact, error handling code is incorrect or even missed in some drivers, so hard-to-find bugs may occur during execution.
In this section, we first reveal this problem using a concrete example and our study on Linux driver patches, and then we sketch the software fault injection technique used in this paper.
We first motivate our work using a real Linux driver bnx2.
This driver manages Broadcom NetXtreme II Ethernet Controller.
Figure 1 In this example, we have three findings.
Firstly, error handling code in drivers is often used to release allocated resources and undo recent operations [30].
It is because that many drivers are based on the fail-stop model [33], namely a simple error can force the driver to exit.
Due to this feature, many bugs in error handling are related to resource-usage violations, such as resource leaks and deadlocks.
Secondly, error handling code is often written in a separate segment in drivers (line 8274-8253 in Figure 1 is an example), and different "goto" target labels handle different errors.
This gotobased strategy is recommended by the Linux kernel documentation [44], because this strategy can simplify error handling logic and reduce repeated code.
Thirdly, bugs in error handling code are hard-to-find.
It is because that error handling code is rarely executed, and maintainers pay insufficient attention to it.
In the example, from Linux 3.1.1 (released in November 2011) to 3.17.2 (released in October 2014), the memory leak in Figure 1 had not been fixed.
Thus, it is very necessary to reveal and detect bugs in error handling code.
To clearly illustrate the reliability of current error handling code in device drivers, we make a study on Linux driver patches.
We manually read patches in the Patchwork project 2 and select accepted patches from them in July 2015.
These patches are from 5 driver classes, namely I2C bus drivers, PCI bus drivers, PowerPC drivers, real-time clock (RTC) drivers and network drivers.
Among them, we identify those which add or update corresponding error handling code.
The result is listed in Table 1.
The first column presents the driver class name; the second column shows the number of accepted patches; the third column shows the number and percentage of accepted patches add or update corresponding error handling code.From Table 1, we find that 40% accepted patches add or update corresponding error handling code.
In these accepted patches, many are used to fix common bugs, such as memory leaks and null pointer dereferences.
One reason for this phenomenon is that complex control flows and different kinds of occasional errors make it difficult to implement correct error handling code.
Another reason is that error handling code is often triggered by specific and infrequent conditions (such as insufficient memory and hardware errors), so developers hardly test it well at runtime.In brief, current error handling code in device drivers is not reliable enough as we expected, and many bugs are hidden in it.
Once these bugs are triggered, serious system problems may occur, such as crashes and resource leaks.
Therefore, it is important and necessary to test error handling code in device drivers and detect bugs inside.
Software fault injection (SFI) is a widely used technique of testing error handling code.
It intentionally introduces faults or occasional errors into the program, and then tests whether the program can correctly handle the injected faults or errors at runtime.
In this paper, we use SFI to test drivers and detect bugs.
To help better understand this paper, we explain several terms about SFI.Fault Injection.
We inject faults or errors to make error handling code executed at runtime.
In this paper, fault injection and error injection [17] can be identical, and injected faults can also be called injected errors.
As shown in Figure 1, we can inject a fault or error to make the function ioremap_nocache (line 7937) fail, and let its error handling code (line 8247-8253) executed.
Fault Representativeness.
It reflects whether an injected fault can represent a real fault or error to trigger error handling code.
If the injected fault is representative, it means that this fault or error can occur in real execution, so the bugs detected in this situation can be regarded as real bugs.
Otherwise, the detected bugs are very probably false.
Fault representativeness is a key factor, and it decides the effectiveness of SFI [24].
Target Function.
A target function is a called function which can fail and trigger error handling code, so it should be fault-injected in SFI.
A target function can be a kernel interface or defined in the driver code.
If a target function is real, its failure can be a representative injected fault, because its failure can cause a real error and actually trigger error handling code.
Namely, the realness of target functions largely decides the fault representativeness of SFI.
For example in Figure 1, the function ioremap_nocache can actually fail and return a null pointer to trigger error handling code, so it is a real target function.
False Positive.
There are two kinds of false positives in this paper.
One is the false positive of fault representativeness, which is the injected fault that can not actually trigger error handling code.
The other is the false positive of bug detection, which is the false detected bug.
Previous SFI approaches often have limitations in practical use, such as reporting many false bugs and needing much manual effort.
One reason is that they are often used for general software, but neglect key characteristics of device drivers.
To improve SFI in testing drivers, we first study the source code of Linux device drivers to find key characteristics of error handling code.
Occasional errors in drivers are often triggered with the function failures, which are reflected as bad return values (null pointers or negative integers of error codes).
As shown in Figure 1, when an error occurs in memory mapping, ioremap_nocache returns a null pointer.
In the example, we find that error handling code is triggered by a bad function return value.
To know about the proportion of this specific form, we write a program to automatically analyze the source code of 848 Linux (version 3.17.2) device drivers from 7 driver classes.
These driver classes are all commonly used, so the study result on them can be applicative to most drivers.In the study, we search for "goto" statements in the code, because they are often the entries of error handling code according to the goto-based strategy [30].
The result is shown in Table 2.
The first column shows the driver class name; the second column shows the number of drivers in each class; the third column shows the number of "goto" statements; the fourth column shows the number and proportion of "goto" statements in the "if" branches of bad function return values.From Table 2, we find that about 75% of "goto" statements are in the "if" branches of bad function return values.
It indicates that most error handling code in device drivers is triggered by bad function return values.
There are two common data types of function return values in device drivers, namely pointer and integer.
According to the Linux kernel documentation [44], a null pointer or non-zero integer indicates the operation failure.
Moreover, different non-zero integers represent different failure types.
For example, -EIO indicates an input/output error and -ENODEV indicates no such device.
As for the remaining 25% "goto" statements, they are triggered by data failures in the code, such as erroneous data read from registers and bad device states.
In user-mode applications, error handling code often contains many if branches [39].
The main reason is that most user-mode applications are based on fail-recovery model.
During recovery, error handling code should handle other errors.
Therefore, multiple faults need to be injected in user-mode applications to cover most error handling code in runtime testing.
1: func_set := ø; cand_set := ø; fault_set := ø; 2: func_set := called functions in normal execution traces;3: foreach func in func_set do if GetRetType(func) == integer or pointer then 5:AddSet(cand_set, func); end if 7 end foreach 8: foreach func in cand_set do if func's RetVal is checked by "if" in the driver then 10:AddSet(fault_set, func); else if func's RetVal is checked in other drivers then 12:AddSet(fault_set, func); else if func's RetVal is specified to be checked then 14:AddSet(fault_set, func); Different from user-mode applications, many device drivers are based on the fail-stop model [33].
Namely, when an error occurs, the driver only handles it and prepares to exit, but other errors are never handled at that time.
Thus, there are few if branches in error handling code of device drivers.
To validate this characteristic, we also write a program to automatically analyze the source code of these 848 Linux drivers.
In the study, we first filter out all annotations and blank lines, and then count source code lines with and without if branches in error handling code.
The result is shown in Table 3.
The first column shows the driver class name; the second column shows the number of drivers in each class; the third column presents the number of source code lines in error handling code; the fourth column presents the number and proportion of source code lines without if branches in error handling code.From Table 3, we can see that nearly 78% of error handling code is not in if branches in these drivers.
It indicates that injecting a single fault in each test case is enough to cover most error handling code.
This characteristic can help to simplify the complexity of injected faults and improve the efficiency of SFI.
The remaining 22% error handling code is in if branches because different resource-usage states or device states need to be separately handled in the same error handling code.This characteristic commonly exists in fail-stop drivers.
However, some drivers like SATA are based on the fail-recovery model, namely they will restart when an error occurs.
Thus, many branches are needed to handle the recovery procedure.
For these drivers, injecting a single fault is not enough to cover most error handling code.
In this paper, we mainly focus on fail-stop drivers, because they occupy a large part of existing drivers [34].
Linux drivers are often implemented in C, so built-in error handling mechanisms (such as "try-catch") are not supported.
To check whether an occasional error occur, an if check is often used in the source code.
The if statement checks whether the key data is erroneous and decides whether error handling code should be executed.
This key data can be a common variable or a function return value.
Thus, the characteristic in Section 3.1 can be regarded as an aspect of it.
For example in Table 2, all "goto" statements triggered by bad function return values are in if checks.
Particularly, most if checks for function return values only check whether the value is a null pointer or non-zero integer (line 7938 in Figure 1 is an example).
Namely, different bad function return values are often handled by the same error handling code.This if check decision characteristic is also recommended by the Linux kernel documentation [44].
It can help us inject more representative and efficient faults for SFI.
Specifically, we can inject faults in the data checked by these if checks, to simulate more realistic errors in device drivers.
The representativeness of injected faults is a key factor of SFI [24].
This property largely determines the accuracy of bug detection and the efficiency of runtime testing.
Injecting representative faults can simulate realistic errors to trigger real error handling, so detected bugs are very probably real.
Meanwhile, useless test cases are less generated when the injected faults are representative, so the time usage can be largely reduced.A common strategy is to inject random faults, which has been used in many previous SFI approaches [11,14,21,22].
But some studies [15,17,24] have proved this strategy can not well represent real errors, and they also introduces many false positives in bug detection.
Because most error handling code in drivers is triggered by bad function return values (in Section 3.1), it is feasible to inject faults in some manually selected target functions which can fail at runtime.
This strategy has been used in some previous approaches [7,32,43] to test drivers, but it has three problems.
Firstly, new target functions should be manually selected when testing a new driver.
Secondly, it is hard to assure the selected target functions can actually trigger realistic errors at runtime.
Thirdly, many real target functions may be omitted in manual selection.Based on the characteristics mentioned in Section 3.1 and 3.3, we propose a pattern-based extraction strategy to automatically and accurately extract real target functions from the source code.
Figure 2 shows the main procedure of this strategy, which consists of two phases.
The return values of some kernel interface functions are clearly specified to be checked in their declarations or annotations, because they can trigger errors.
For example in the Linux kernel code, a specific macro "__must_check" is defined.
If this macro is noted in the declaration of a function, its return value must be checked.
The function pci_request_regions in Figure 1 uses this macro.
Besides, some key phrases in the function annotation also indicate the function return value should be checked.
Therefore, the declaration and annotation of candidate functions should be checked as well.
This strategy has three advantages.
Firstly, when the driver source code and hardware are available, this strategy can automatically extract target functions without manual effort.
Secondly, by using exact runtime information and common code patterns, many unreal target functions are filtered out.
Thirdly, no real target functions in the captured runtime traces are omitted.
By using this strategy, we can automatically and accurately extract real target functions as representative injected faults to improve the effectiveness of SFI.
To efficiently test error handling code in device drivers, we propose EH-Test based on driver characteristics, code instrumentation and dynamic analysis.
Figure 3 shows the overall architecture of EH-Test, which consists of five modules: Fault extractor.
This module uses the patternbased extraction strategy to automatically extract target functions.
It needs the source code of the target driver, other drivers and kernel interface functions as input, which can be obtained from the OS source code.
Based on the architecture, two phases are performed when EH-Test works, namely test case generation and runtime testing.
The manual work only includes writing pair checkers, checking extracted target functions and rebooting the system when crash bugs are detected.
In this phase, we have two tasks, namely extracting target functions from the code and generating test cases of the driver by injecting faults on target functions.
The detailed steps are as follows.Firstly, we input the driver code and OS source code to the fault extractor.
It uses the pattern-based extraction strategy to extract target functions.
After extraction, the user also is allowed to check and modify target functions as needed.Secondly, we inject faults into target functions.
A key question is that how many faults should be injected in each test case.
Many previous approaches [7,22,35,39] inject multiple faults in each test case, because they aim to cover as much error handling code as possible.
But fault scenario explosion may occur in this situation, Figure 4: Compilation procedure of the tested driver.which can largely reduce testing efficiency.
To relieve this problem and speed up testing, these approaches have to use some expedients, such as limiting the number of injected faults (or searching paths) [7,39] and resorting to user guidance [22].
For many Linux drivers, a key characteristic is that there are few if branches in error handling code (in Section 3.2).
Namely, the error handling code in many device drivers only handles a single error at a time.
Thus, to cover most error handling code with less testing time, we only corrupt the return value of one target function in each test case.
The target function call is replaced by an error function in the code.
What this error function does is only returning a bad value.
If the return value of the target function is a pointer, the error function will return a null pointer; if the return value of the target function is an integer, the error function will return a random negative number.
Thirdly, we instrument probes to collect runtime information and count code coverage during execution.
The runtime information is used to detect bugs in the next phase.
The code coverage is used to quantify the effectiveness of runtime testing.
Finally, driver test cases are generated.
Each test case is a kernel object file, namely a loadable driver.In the second and third steps, code instrumentation is used.
We implement it at compile time using the Clang [40] compiler.
Figure 4 shows the compilation procedure of the tested driver.
Firstly, we use the Clang compiler to compile the C source code of the driver into the LLVM bytecode.
Secondly, we utilize the fault injector and probe inserter to instrument our handled code in the bytecode.
Thirdly, we use the Clang compiler to compile the bytecode into the assembly code, and then build the object file using GCC.
Finally, we link the object file and the runtime monitor's program together, and generate a kernel object file as a test case.
In this phase, we run each test case on the real hardware and detect bugs during execution.
Three kinds of bugs are detected in current implementation, namely crashes, hangs and resource-release omissions.When driver crashes occur, the OS outputs the dump information into the kernel crash log.
Therefore, we can check the kernel crash log to detect and locate crash bugs like null pointer dereferences.
For driver hangs, we can detect them by observing whether the system freezes.
These two kinds of bugs are easy to observe in real execution.
Table 4: Selected paired functions in device drivers.
As for resource-release omissions, they are hard-tofind in real execution, because they rarely lead to obvious exceptions.
However, they often cause resourceusage problems, such as resource leaks and memory leaks.
Moreover, resource-release omissions often occur in device drivers, especially in error handling code [31].
For these reasons, EH-Test should detect resourcerelease omissions in device drivers.
A resource-release omission occurs when a resource-acquiring function is successfully called but its resource-releasing function is not called.
For example in Figure 1, kzalloc is a resource-acquiring function and it is used to allocate kernel memory, but the resource-releasing function kfree is not called, which leads to a resource-release omission.
A resource-acquiring function and its resource-releasing function should be called in pairs, so they can be called paired functions [20].
Besides, they should operate the same mapped data (parameter or return value) as the handled resource.
In EH-Test, we implement some pair checkers to detect resource-release omissions.
Each pair checker contains the basic information of a pair of paired functions, including function names and mapped data.
Some previous approaches for specification mining [18,20,37,38] can be used to extract paired functions from the code.
In this paper, we use the mining result of PF-Miner [20], which is a static approach for mining paired functions in Linux drivers, to build the pair checkers.
Table 4 shows some selected paired functions in the checkers.
The first column shows function names; the second column shows the description; the third column shows the mapped data.During driver execution, the runtime monitor uses the inserted probes to record the runtime information of function calls and maintains a resource-usage list.
For each function call, the monitor checks whether it is in the pair checkers.
When a resource-acquiring function is called, the monitor checks its return value to judge whether the resource is successfully allocated.
If it is true, the monitor will create a node containing the function name and mapped data, and add it into the resource-usage list.
When a resource-releasing function is called, the monitor scans the list to match the node with the function information.
If it is matched, the node will be deleted to indicate the resource is released.
When the driver is removed, the monitor checks the nodes in the list.
If the list is not empty, it indicates resource-release omissions occur, so the monitor will report them.
To validate the effectiveness of EH-Test, we evaluate it on real device drivers.
The tested drivers should satisfy three criteria.
Firstly, they should be commonly used in practice.
Secondly, they should be within the driver classes in Section 3, because they can satisfy the characteristics found by the study.
Thirdly, they should run as kernel modules, because the test cases of them can be directly installed and removed without rebooting the operating system.
According to these criteria, 15 Linux device drivers are selected, including wireless, USB and Ethernet drivers.
The experiment runs on a Lenovo PC with two Intel i5-3470@3.20G processors and 2GB physical memory.
GCC 4.8 and Clang 3.2 are used for compilation.
We write 75 pair checkers based on the result of PF-Miner.
For each test case of the drivers, we install it in the system, run it on the workload, and finally remove it.
The workload consists of three kinds.
For wireless drivers, we turn on WiFi, ping another computer and turn off WiFi; for Ethernet drivers, we ping another computer; For USB drivers, we copy a 4MB file to the USB disk.
The representativeness of injected faults is a key factor of SFI.
In this paper, injected faults are bad return values of target functions, and all target functions are automatically extracted by our pattern-based extraction strategy.
Thus, the fault representativeness of SFI largely depends on the effectiveness of our pattern-based extraction strategy.
There are three important research questions about its effectiveness:RQ1: How many unrepresentative candidate functions are automatically filtered out?
RQ2: How much is the false positive rate of the strategy?
RQ3: How many real target functions are omitted?To answer these questions, we first evaluate EH-Test on the 15 drivers to extract candidate functions and target functions.
Then we manually check the extracted target functions to judge their realness.
Table 6 shows the result in Linux 3.17.2.
The first column presents the driver name; the second column shows the number of candidate functions; the third column shows the number of extracted target functions; the fourth column shows the number of real target functions.From Table 6, we can find that 523 target functions are extracted from 2183 candidate functions.
It indicates that 76% candidate functions are automatically filtered out because they are unrepresentative, which can answer RQ1.
By manually checking the documents and implementations of the extracted target functions, we find that 470 target functions are real, which means they can actually fail and trigger error handling code.
It indicates that the false positive rate of our pattern-based extraction strategy is only 10%, which can answer RQ2.
Many false target functions return integers which are also checked by if statements, but they reflect different driver configurations or states but never trigger occasional errors.
Answering RQ3 is difficult, because target functions are extracted from normal execution traces, but different execution paths may have different runtime traces.
Thus, the real target functions within the unexecuted paths will be omitted.
However, we find that all target functions in the captured runtime traces are extracted by our strategy.Reviewing the result, we also find an interesting phenomenon.
Most target functions are in the initialization procedure.
The data in the parenthesis of the fourth column show the numbers of these functions.
They occupy 86% of all target functions.
Namely, most kinds of occasional errors in drivers occur in the initialization procedure.
In fact, it has been noted in the Linux driver manual [9], and our results can successfully verify it.
The explanation for this phenomenon is that different kinds of configurations need to be made in the initialization, and each configuration can cause a kind of occasional error.
After the driver is initialized, only several kinds of errors can occur in the running procedure.
With the extracted target functions, we perform runtime testing to detect bugs in error handling code.
Each test case is generated by making one target function fail.
To validate whether EH-Test can find the known bugs having been fixed, we first use EH-Test to test the 15 drivers in an older Linux version 3.1.1 (released in November 2011).
Then we test these drivers in a newer Linux version 3.17.2 (released in October 2014) to validate whether EH-Test can find new bugs.
Table 7 shows the result.
The first column shows the driver name; the second and seventh columns ("Test case") show the number of generated test cases; the third and eighth columns ("Time usage") present the time usage of the runtime testing; the fourth and ninth columns ("Crash / Hang") show the number of detected crashes and hangs; the fifth and tenth columns ("Resource") present the number of detected resource-release omissions; the sixth and eleventh columns ("Bugs") show the number of detected bugs.
Specifically, the number of memory leaks is shown in the parenthesis of the fifth and tenth columns ("Resource"), because the memory leak is an important kind of resource-release omission.From Table 7, we make the following observations: Firstly, EH-Test finds 32 bugs in the 15 drivers in Linux 3.1.1, including 10 crashes and 22 resourcerelease omissions.
Among these resource-release omissions, 10 are memory leaks.
Reviewing the driver code, 8 resource-release omissions (sky2 driver) and 1 crash (rt2800 driver) have been fixed in Linux 3.17.2.
It indicates that EH-Test can find the known bugs.Secondly, EH-Test finds 50 bugs in the 15 drivers in Linux 3.17.2, including 13 crashes, 1 hang and 36 resource-release omissions.
Among the resource-release omissions, 29 are memory leaks.
Moreover, 23 bugs are reserved from the legacy code in 3.1.1, and 27 bugs are introduced due to new implementations.
We send all the bugs to the driver developers, and all of them have been confirmed.
We also send 17 patches 3 to fix them, and 15 have been applied by the maintainers.
It indicates that EH-Test can accurately find new bugs in drivers.Actually, a threat to validity is that false extracted target functions may introduce false bugs.
In our evaluation, no false bugs are detected for this reason.Thirdly, the time usage of EH-Test is short.
About 2 hours are spent in totally testing the 15 drivers, and only several minutes are spent for most drivers.
This time usage is shorter than many previous SFI approaches [7,13,23] for testing drivers.
One reason is that EH-Test uses the pattern-based extraction strategy to filter out many unrepresentative candidate functions, so no redundant test cases are generated.
Thus, the test cases are efficient, and the testing time is largely shortened.
Figure 6: Code coverage of the tested drivers.Fourthly, resource-release omissions occupy a large part of detected bugs.
The main reason is that resourcerelease omissions often get little attention by developers.
The complex execution paths make it difficult to correctly manage resources in error handling code.
Meanwhile, resource-release omissions rarely lead to obvious exceptions, so they are hard-to-find in runtime testing.
Figure 5 shows a crash detected by EH-Test in the e100 driver.
The function pci_pool_create (line 2967) is called to create a pool of consistent memory blocks for the PCI device, and this function returns a pointer (nic->cbs_pool) to this memory area.
But the function pci_pool_create may fail when the memory is insufficient, and it will return a null pointer in this situation.
Thus, a null pointer dereference will occur, when the function pci_pool_alloc (line 1910) uses this pointer to allocate a memory block.
This crash is detected when we inject a fault in the function pci_pool_create.
This function is extracted as a target function in our patternbased extraction strategy, because many other drivers check its return value in the code (pattern 2 in Section 4).
To fix this bug, we add an if check after the function pci_pool_create (line 2967) to check its return value and implement the corresponding error handling code.
Code coverage is a key criterion in runtime testing.
To calculate code coverage, we use the inserted probes to count executed instructions at runtime.
Because most target functions are in the initialization procedure, we focus on measuring the code coverage in this procedure.
Figure 6 shows the results in Linux 3.17.2.
The average code coverage of EH-Test is increased by 8.82% compared to the normal execution.
It indicates that hundreds of more instructions are executed in runtime testing.In fact, not all error handling code can be covered by EH-Test.
Firstly, EH-Test only injects faults in target functions, but some error handling code is triggered by erroneous data read from hardware registers.
Secondly, our approach injects a single fault in each test case, but some error handling code is triggered by multiple errors.
Thirdly, target functions in unexecuted paths are not extracted in our pattern-based strategy, so their error handling code can not be covered.
These points cause that the bugs in the uncovered code will be missed.
Software fault injection and symbolic execution are two runtime techniques which are often used to test drivers.
Software Fault Injection.
We compare EH-Test to ADFI [7], a state-of-the-art SFI approach for testing drivers.
It uses a bounded trace-based iterative strategy to relieve fault scenario explosion and a permutationbased replay mechanism to assure the fidelity of fault injection.
Similar to EH-Test, it injects faults in some target functions and generates test cases to detect bugs.
But there are two main differences between ADFI and EH-Test.
Firstly, the target functions in ADFI are all manually selected.
Only memory, DMA and PCI related interfaces are considered.
Thus, much manual work is needed, and many real target functions may be omitted.
EH-Test can automatically and accurately extract all target functions in the captured runtime traces without omissions, and the only optional manual work is checking the extracted target functions.
Secondly, ADFI injects multiple faults in each test case.
The advantage is that much more configuration and error handling code can be covered to detect more bugs.
But numerous test cases are generated, so it spends much more time (often several hours) than EH-Test when testing a driver.ADFI program and its detailed bug reports are not available, thus we compare the number of its detected bugs from the paper.
ADFI and EH-Test both test three drivers with the same workload.
For the e100 and r8169 drivers, they both find the same number of bugs.
For the ehci_hcd driver, EH-Test finds 10 bugs, but ADFI does not find any bug in this driver.
Reviewing the code, we find that these bugs are triggered by the target functions which are not memory, DMA or PCI related functions, so ADFI omits them.
Symbolic Execution.
We select SymDrive [28], a famous symbolic execution approach to make the comparison.
This approach uses a symbolic device and some checkers to detect bugs, including memory leaks and null pointer dereferences.SymDrive program is open-source, and we successfully run it to test the e100 driver.
It runs for nearly 80 minutes and searches 4838 paths, finally exits due to insufficient disk space.
In the experiment, SymDrive does not find the bug shown in Figure 5.
The reason is that the return value of the function pci_pool_create is not marked as a symbolic value in SymDrive, so the corresponding error handling path is not searched.
Besides, SymDrive can only test the drivers whose devices are supported by QEMU [4].
But many devices are not supported by the QEMU used in SymDrive, so the drivers for these devices can not be directly tested.
The sky2 and iwl4965 drivers are the typical examples.
In software testing, software fault injection is a technique for testing rarely executed code by deliberately injecting faults.
In particular, it is often used to test error handling code in software systems.Many approaches [11,14,21,22] use random fault injection in software testing.
They replace the program data with random faulty data or inject faults into random places, and then run test cases to validate whether the software can properly handle the faults.
But random fault injection often leads to poor code coverage and low bug-detection accuracy.
To relieve this problem, some approaches [3,12,39] use program information to guide fault injection and generate efficient test cases.
Moreover, to improve the representativeness of injected faults, much research [10,17,24,25] gives useful solutions through empirical studies.
In fact, these approaches are often used for general software, especially usermode applications.
They often neglect the characteristics of device drivers, so their effectiveness may be largely limited when directly testing device drivers.Besides user-mode applications, SFI is also carefully designed to test drivers [7,13,23,29,32,35].
Mendonca et al. [23] perform robustness testing for Windows drivers based on random fault injection.
Frequently used kernel interfaces in drivers are called with random parameters.
ADFI [7] uses a bounded trace-based iterative strategy to relieve fault scenario explosion and a permutation-based replay mechanism to assure the fidelity of fault injection.
But these approaches still have limitations when testing device drivers.
A typical limitation is that they often neglect the representativeness of injected faults, and their injected faults are often random or manually selected.
To relieve this limitation, EH-Test uses a pattern-based extraction strategy to automatically and accurately extract real target functions as representative injected faults.
Some approaches [5,8,16,28] introduce symbolic execution in driver testing without the real hardware.
DDT [16] is a tool for testing binary drivers against undesired behaviors, such as resource leaks and race conditions.
It combines virtualization with selective symbolic execution to test drivers using some modular dynamic checkers.
SymDrive [28] provides a symbolic device based on QEMU [4] to simulate real hardware behaviors.
It utilizes a favor-success path-selection algorithm in execution, which can increase the exploration priority of executing path at every successful function return within drivers and kernel interfaces.
Some checkers are provided to detect common bugs like memory leaks.Symbolic execution is very time consuming, and it needs much programmer guidance to avoid path explosion.
Moreover, many devices can not be simulated well in virtual machines, so their drivers can not be directly tested using symbolic execution.
Static analysis is often used to detect bugs in device drivers.
It only analyzes the driver source code or binary code without actually running target drivers.
Some approaches [1,2,6,26,27,36,41,42] are based on program verification.
For example, SDV [2] is a famous static tool to verify Windows drivers.
It abstracts the C source code to a simpler form encoded as a state machine, and checks violations of kernel API usage rules.
Some approaches [18,19,20,31] mine implicit specifications from the driver code and detect related bugs.
For example, PR-Miner [19] exploits data mining techniques to automatically extract implicit programming rules from software code and detect violations against these extracted rules.Compared to runtime testing, static analysis lacks precise context information of real execution, so some false positives may be introduced in bug detection.
In this paper, we first study the source code of Linux drivers, and find three useful characteristics of error handling code.
Then based on these characteristics, we propose a practical approach named EH-Test to efficiently test error handling code and detect bugs inside.
It uses a pattern-based extraction strategy to automatically and accurately extract real target functions as representative injected faults.
It has been evaluated on 15 Linux drivers and found 50 new real bugs.
Our work shows that by introducing the characteristics of target programs, software testing can be more effective.
We would like to express our gratitude to our shepherd Jon Howell and other reviewers for their helpful comments and suggestions.
We also thank the Linux driver developers for their useful feedbacks.
This work is supported by Research Grant of Beijing Higher Institution Engineering Research Center and Tsinghua University Initiative Scientific Research Program (2014z09102).
Shi-Min Hu is the corresponding author of this paper.
