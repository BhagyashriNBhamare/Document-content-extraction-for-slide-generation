The I/O forwarding architecture is widely adopted on modern supercomputers, with a layer of intermediate nodes sitting between the many compute nodes and backend storage nodes.
This allows compute nodes to run more efficiently and stably with a leaner OS, offloads I/O coordination and communication with backend from the compute nodes, maintains less concurrent connections to storage systems , and provides additional resources for effective caching, prefetching, write buffering, and I/O aggregation.
However, with many existing machines, these forwarding nodes are assigned to serve a fixed set of compute nodes.
We explore an automatic mechanism, DFRA, for application-adaptive dynamic forwarding resource allocation.
We use I/O monitoring data that proves affordable to acquire in real time and maintain for long-term history analysis.
Upon each job's dispatch, DFRA conducts a history-based study to determine whether the job should be granted more forwarding resources or given dedicated forwarding nodes.
Such customized I/O forwarding lets the small fraction of I/O-intensive applications achieve higher I/O performance and scalability, meanwhile effectively isolating dis-ruptive I/O activities.
We implemented, evaluated, and deployed DFRA on Sunway TaihuLight, the current No.3 su-percomputer in the world.
It improves applications' I/O performance by up to 18.9×, eliminates most of the inter-application I/O interference, and has saved over 200 million of core-hours during its test deployment on TaihuLight for 11 months.
Finally, our proposed DFRA design is not platform-dependent, making it applicable to the management of existing and future I/O forwarding or burst buffer resources.
Supercomputers today typically organize the many components of their storage infrastructure into a parallel and global controlled file system (PFS).
Performance optimization by manipulating the many concurrent devices featuring different performance characteristics is a complicated yet criti- * Most work conducted during appointment at Qatar Computing Research Institute.
† Wei Xue is the corresponding author.
Email: xuewei@tsinghua.edu.cn cal task to administrators, application developers, and users.
Moreover, it gets more challenging due to I/O contention and performance interference caused by concurrent jobs sharing the same PFS, bringing significant I/O performance fluctuation [28,38,40,44,61].
Meanwhile, different applications have vastly different I/O demands and behaviors, making it impossible for center administrators to decide one-size-forall I/O configurations.
The task is even more difficult when it comes to the design and procurement of future systems.
It is hard for machine owners to gauge the I/O demand from future users and design a "balanced" system with coordinated computation, network, and I/O resources.
In particular, design and procurement typically happen years before any application could test run, while even decades-old programs usually see very different performance and scalability due to newer architecture/hardware/software on the more powerful incoming machine.
To give an example, consider the design of an I/O forwarding infrastructure [19], a widely adopted I/O subsystem organization that adds an extra forwarding layer between the compute nodes and storage nodes, as illustrated in Figure 1.
This layer decouples file I/O from the compute nodes (CN i in Fig 1), shipping those functions to the forwarding nodes instead, which are additional I/O nodes responsible for transferring I/O requests.
It also enables compute nodes (1) to adopt a lightweight OS [48,53,64] that forwards file system calls to forwarding nodes, for higher and more consistent application performance [19], (2) to maintain fewer concurrent connections to the storage subsystem than having clients directly access file system servers, for better operational reliability, and (3) to facilitate the connection between two different network domains, typically set up with different topol-ogy and configurations, for computation and storage respectively.
Finally, it provides an additional layer of prefetching/caching (or, more recently, burst buffer operations [51]), significantly improving user-perceived I/O performance and reducing backend data traffic.
Machine Due to these advantages, I/O forwarding is quite popular, adopted by 9 out of the current TOP20 supercomputers (by the latest TOP500 list [16]).
Table 1 summarizes their current TOP500 rankings and system configurations, including the number of compute and forwarding nodes.
Note that recent Cray installations such as Cori and Trintiy use forwarding nodes with SSD-based burst buffers [3].
Forwarding architecture is also targeted in an Exascale storage design [45].
Despite the I/O forwarding layer's nature in decoupling compute nodes from backend storage nodes and enabling flexible I/O resource allocation, to provision a future system with forwarding resources (or to manage them for a current one) is challenging, as reasoned earlier.
As a result, existing systems mostly adopt a fixed forwarding-node mapping (FFM) strategy between compute nodes and forwarding nodes, as illustrated in Figure 1.
Though compute nodes are connected to all forwarding nodes, each forwarding node is assigned a fixed subset of k compute nodes to serve [48,49,63].
E.g., the compute-to-forwarding mapping is fixed at 512-1 at the No.3 supercomputer TaihuLight [30], and 380-1 at the No.5 Piz Daint [55].
This paper proposes a new method of forwarding resource provisioning.Rather than making fixed mapping decisions based on rough estimates, supercomputer owners could enable dynamic forwarding resource allocation (DFRA), with flexible, application-aware compute-toforwarding node mappings.
We argue that DFRA not only alleviates center management's difficult hardware provisioning burdens, but significantly improves forwarding resource utilization and inter-application performance isolation.DFRA is motivated by results of our whole-system I/O monitoring at a leading supercomputing center and extensive experiments.
Specifically, we found the common practice of FFM problematic: (1) while the default allocation suffices on average in serving applications' I/O demands, the forwarding layer could easily become a performance bottleneck, leading to poor application I/O performance and scalability as well as low backend resource utilization; meanwhile the majority of forwarding nodes tend to stay under-utilized.
(2) Forwarding nodes shared among relatively small jobs or partitions of large jobs become a contention point, where applications with conflicting I/O demands could inflict severe performance interference to each other.
Section 2 provides a more detailed discussion of these issues.Targeting these two major limitations of FFM, we devised a practical forwarding-node scaling method, which estimates the number of forwarding nodes needed by a certain job based on its I/O history records.
We also performed an in-depth inter-application interference study, based on which we developed an interference detection mechanism to prevent contention-prone applications from sharing common forwarding nodes.
Both approaches leverage automatic and online I/O subsystem monitoring and performance data analysis that require no user effort.We implemented, evaluated, and deployed our proposed approach in the production environment of Sunway TaihuLight, currently the world's No.3 supercomputer.
Deployment on such a large production system requires us to adopt practical and robust decision making and reduce software complexity when possible.
In particular, we positioned DFRA as a "remapping" service, performed only when projected I/O time savings significantly offset the node-relinking overhead.Since its deployment in Feb 2018, DFRA has been applied to ultra-scale I/O intensive applications on TaihuLight and has brought savings of bringing around 30 million corehours per month, benefiting major users (who together consume over 97% of total core-hours).
Our results show that our remapping can achieve up to 18.9× improvement to real, large-scale applications' I/O performance.
Finally, though our development and evaluation are based on the TaihuLight supercomputer, the proposed dynamic forwarding resource allocation is not platform-specific and can be applied to other machines adopting I/O forwarding.
Modern I/O forwarding architectures in HPC machines typically deploy a static mapping strategy [18] (referred to as FFM for the rest of the paper), with I/O requests from a compute node mapped to a fixed forwarding node.
Here we demonstrate the problems associated with this approach, using the world's No.3 supercomputer TaihuLight as a sample platform.
Specifically, we discuss resource misallocation, inter-application interference, and forwarding node anomalies, proceeded by introduction to the platform and the realworld applications to be discussed.
Platform Sunway TaihuLight is currently the world's No.3 supercomputer [30], with over 10M cores and 125-Petaflop peak performance.
Its main storage system is a 10PB Lustre parallel file system [24], delivering 240GB/s and 220GB/s aggregating bandwidths for reads and writes respectively, using 288 storage nodes and 144 Sugon DS800 disk arrays.
Between its compute nodes and the Lustre backend is a globally-shared layer of 240 I/O forwarding nodes.
Each forwarding node provides a bandwidth of 2.5GB/s and plays a dual role, both as a Lightweight File System (LWFS) [6] server to the compute nodes and a client to the Lustre backend.
Before our DFRA deployment, 80 forwarding nodes were used for daily service, the other 160 reserved as backup or for large production runs with whole-system reservations.In addition, TaihuLight has an online, end-to-end I/O monitoring system, Beacon [21].
It provides rich profiling information such as average application I/O bandwidth, I/O time and I/O access mode, as well as real-time system load and performance measurements across different layers of TaihuLight's storage system.
Applications Our test programs include 11 real-world applications and one parallel I/O benchmark.
Six of them are 2017 and 2018 ACM Gordon Bell Prize contenders: CESM [37] (Community Earth System Model) is an earth simulation software system which consists of many climate models; CAM [59] is a standalone global atmospheric model deriving from the CESM project for climate simulation/projection; AWP [25] is a widely-used earthquake simulator [26,54]; Shentu [35] is an extreme-scale graph engine; LAMMPS [68] (Large-scale Atomic/Molecular Massively Parallel Simulator) is a popular molecular dynamics software; Macdrp [23] is a new earthquake simulation tool, specializing in accurate replay of earthquake scenarios with complex surface topography.
CAM and AWP were among the three 2017 Gordon Bell Prize finalists (AWP being the final winner), while Shentu is in the 2018 finalist.Note that although all 6 applications above can scale to the full TaihuLight system's 40,000+ compute nodes, full-scale production runs are conducted mostly with pre-arranged system-wide reservation.
In most cases, we do not have such reservation or the largest-scale input datasets to evaluate their maximum-scale executions.
However, throughout the year, their developers and users conducted many mid-size runs, each using hundreds or thousands of compute nodes.
Most of our experiments evaluate at such scale, where I/O performance improvement can save shared I/O resources and reduce application execution time.
Meanwhile, our findings here remain applicable to larger-scale runs.The remaining large-scale applications in our testbed are: DNDC [32] (biogeochemistry application for agroecosystems simulation), WRF [1] (regional numerical weather prediction system), APT [67] (particle dynamics simulation code), XCFD (computational fluid dynamics simulator), and swDNN [29] (deep neural network engine).
For the ease of controlling I/O behaviors and execution parameters, we also use MPI-IO [7], a widely-used MPI-IO benchmark by LANL.These programs represent diverse data access behaviors regarding request characteristics, I/O volume, I/O library, and file sharing mode.
Table 2: Summary of test programs' I/O characteristics.
"N-N" mode means N processes operate N separate files.
"N-1" means N processes operate on one shared file.
"1-1" means only one process among all processes operates on one file.files.
Here we roughly label each application as "high" or "low" in three dimensions: I/O throughput, IOPS, and metadata operation intensity, using empirical thresholds.
1 As shown above, applications have drastically different I/O demands, some requiring a much lower compute-toforwarding nodes ratio than others.
Traditional FFM does not account for varying I/O behaviors across applications, leading to significant resource misallocation.
Below we discuss concrete sample scenarios.
Forwarding node under-provisioning The default I/O forwarding node allocation of one per 512 compute nodes in TaihuLight is adequate for the majority of applications we have profiled, but severely low for the most I/O intensive applications, where the forwarding nodes become an I/O performance bottleneck.
Due to the transparent nature of the forwarding layer, such bottleneck is often obscure and hard to detect by application developers or users.
Figure 2 demonstrates the impact of allocating more forwarding nodes to two representative real-world applications: XCFD and WRF 1 .
We plot the I/O performance speedup (normalized to that under the default allocation of one forwarding node), as a function of the number of exclusive forwarding nodes assigned to the application.We find that XCFD benefits significantly from increased forwarding nodes.
XCFD adopts an N-N parallel I/O mode, where each MPI process accesses its own files.
Thus many backend storage nodes and OSTs (Object Storage Targets, Lustre term for a single exported backend object storage volume) are involved in each I/O phase, especially when N is large.
In general, applications with such I/O behavior suffer under FFM, due to the limited processing bandwidth in the assigned forwarding nodes.
Our I/O profiling on TaihuLight indicates that among jobs using at least 32 compute nodes, around 9% use the N-N I/O mode, potentially seeing significant performance improvement given more forwarding nodes.
Such underprovisioning was observed on other supercomputers, e.g., recent Cray systems where I/O requests issued by a single compute node can saturate a forwarding node [27].
Applications like WRF 1 , meanwhile, adopt the 1-1 I/O mode, where they aggregate reads/writes to a single file in each I/O phase.
Intuitively, such applications do not benefit from higher forwarding node allocation.
In addition, on TaihuLight applications with the 1-1 mode typically do not generate large I/O volumes in a single I/O phase, though they tend to run longer.
Combining these two factors, 1-1 applications are mostly insensitive to additional forwarding layer resources beyond the default allocation.Forwarding node load imbalance Application-oblivious forwarding resource allocation can lead to severe load imbalance across forwarding nodes.
To verify this, we examined historical I/O traces collected on TaihuLight's forwarding nodes to check how they are occupied over time.For every forwarding node, TaihuLight's profiling system records its per-second pass-through bandwidth.
Analysis of such results first indicates that during the majority of profiled time intervals, the forwarding nodes are severely underutilized, echoing other studies' findings on overall low supercomputer I/O resource utilization [43,47].
Meanwhile we found high variability of loads across forwarding nodes and high day-to-day variances on forwarding node occupancy.We illustrate this with the forwarding nodes' daily occupancy, calculated as the fraction of 1-second windows in a day where a node's average bandwidth reaches 80% of the peak forwarding bandwidth of 2.5 GB/s.
Figure 3 plots the minimum, average, and maximum daily occupancy across the 80 TaihuLight forwarding nodes, between July 15th and August 31st, 2017.
We see both high variability in overall load (irregular average and maximum curves) and high load imbalance (large difference between the two).
2 With recent and emerging systems adopting a burst buffer (BB) layer, such under-utilization and imbalance could bring wasted NVM spaces, buffer overflow, unnecessary data swapping, or imbalanced device wear.
I/O interference is a serious problem known to modern supercomputer users [28,38,40,44,61].
The common FFM practice not only neglects individual applications' I/O demands, but also creates an additional contention point by sharing forwarding nodes among concurrent jobs with conflicting I/O patterns.
Figure 4 illustrates this using three real applications: AWP Shentu, and LAMMPS.All used the default 512-1 compute-to-forwarding mapping.
We tested two execution modes, with each application allocated dedicated forwarding nodes vs. applications using shared ones.
In both modes all three applications ran simultaneously.
Note that for Shentu in the shared mode, it was allocated one dedicated forwarding node and two more nodes to share with other applications: one with AWP and one with LAMMPS, which were each running on 256 compute nodes (and thus allocated half of a forwarding node each).
As expected, all three experienced faster I/O with dedicated forwarding nodes.
However, some suffered much higher performance interference.
While AWP and LAMMPS saw mild slowdowns (4% and 23% increase in total I/O time), Shentu had a 3× increase.
This is due to the highly disruptive behavior of AWP's N-1 I/O mode (discussed in more details later), causing severe slowdown of Shentu processes accessing the same forwarding node.
Given the synchronous nature of many parallel programs, their barrierstyle parallel I/O operations wait for all processes involved to finish.
Thus slowdown from the "problem forwarding node" shared with AWP is propagated to the entire application, despite that it had one dedicated forwarding node and shared the final one with a much more friendly LAMMPS.In Section 5, we present an in-depth inter-application interference study, based on which we perform applicationaware interference estimation to avoid sharing forwarding nodes among applications prone to interference.
Finally, when certain forwarding nodes show abnormal behavior due to software or hardware faults, applications assigned to work through these slow nodes under FFM would suffer.
We found TaihuLight forwarding nodes prone to correctable failures in memory or network, confirming the "failslow" phenomenon observed at data centers [33].
No.8 Figure 5 shows sample benchmarking results measuring read/write bandwidth across 96 currently active forwarding nodes, conducted during system maintenance.
While most forwarding nodes do report consistent bandwidth levels (with expected variability due to network contention, disk status, etc.), a small number of them clearly exhibit performance anomalies.
In particular, forwarding node No.8 (highlighted with arrow) is an obvious outlier, with average read and write bandwidth at 7% and 12% of peak, respectively.Fortunately, the I/O monitoring system in TaihuLight performs routine, automatic node anomaly detection across all layers of the I/O infrastructure.
As shown in Section 3, our proposed dynamic forwarding system leverages such anomaly detection to skip nodes experiencing anomalous behavior in its dynamic allocation.
Given the above multi-faceted problems caused by FFM, we propose a practical-use and efficient dynamic forwarding resource allocation mechanism, DFRA.
DFRA works by remapping a group of compute nodes (scheduled to soon start executing an application) to other than their default forwarding node assignments, whenever the remapping is expected to produce significant application I/O time savings.
It serves three specific purposes: (1) to perform applicationaware forwarding node allocation to avoid resource underprovisioning for I/O-intensive jobs, (2) to mitigate interapplication performance interference at the forwarding layer, and (3) to (temporarily) exclude forwarding nodes identified as having performance anomalies.To remap at the job granularity does not pose much technical difficulty by itself.
The challenge lies in developing an automatic workflow that examines both the application's I/O demands and real-time system status, and performs effective inter-application I/O interference estimation, while remaining as transparent as possible to users and relieving administrators from labor-intensive manual optimizations.
To this end, we leverage the Beacon I/O monitoring system on TaihuLight to perform continuous application I/O profiling and learn the I/O characteristics of applications from history.
3 Assisted with all-layer profiling data, from the compute nodes to the backend OSTs, plus per-job scheduling history that reveals the mapping of a job's processes to compute nodes, we obtain a detailed understanding of each past job's I/O behavior, including peak bandwidth per compute node, request type/size distribution, periodic I/O frequency, I/O mode (N-N, N-1, 1-1, etc.), and metadata access intensity.
Given that HPC platforms typically see applications run repeatedly, with very similar I/O patterns [62], there is high likelihood that the past reflects the future.We have designed, implemented, and deployed a dynamic forwarding resource allocation mechanism on TaihuLight, as depicted in Figure 6 It determines whether a target job A, scheduled to begin execution on a certain set of compute nodes, needs to have forwarding nodes remapped and if so, to which nodes.
Implementation-wise, such proposed dynamic forwarding resource allocation component resides on a single dedicated server (DFRA server).
It interacts with the Beacon I/O monitoring system and the job scheduler.
Beacon provides an I/O performance database to query using A's job information (e.g., application name, project name, user name, and execution scale) and estimates its I/O characteristics based on historical records.
The job's expected I/O features, such as I/O mode and the number of compute nodes performing I/O, are then fed to the DFRA server.
First it checks whether this application needs to scale out to use more forwarding nodes.
If not (the more likely case), then we incorporate real-time scheduling information to know about the "neighbor applications" A n (the set of applications currently running on D, the forwarding nodes to be assigned under the default allocation).
This allows the DFRA server to check whether the default mapping will produce significant performance interference with neighbors already running there.
If significant interference is expected, we keep the default allocation size D, but would remap the compute nodes to dedicated forwarding nodes.
4 If scaling is required, we first calculate S, the number of forwarding nodes needed.
We then check I, the number of idle forwarding nodes currently available, excluding those undergoing performance anomaly, and allocate the fewer between S and I. Though more sophisticated "partial-node" allocation is possible, we choose the more simple scheme considering the overall forwarding node under-utilization.
In summary, there are two types of "upgrades": to grant more forwarding nodes (for capacity) or grant unused forwarding nodes (for isolation).
In both cases, as we only allocate dedicated nodes from the idle pool, no interference check is further needed.
In the specific case of TaihuLight, at the beginning of this research, beside the 80 forwarding nodes using the default 512-1 mapping, more than 100 are reserved for backup or manual allocation.
For systems without such over-provisioning, we recommend the default allocation be lowered to serve the majority of jobs, who are not I/O-intensive, and have a set of "spare" forwarding nodes for ad-hoc remapping.Note that this is a best-effort system transparent to users.
Additionally, for the majority of applications, who are not I/O-intensive enough to warrant higher allocation and not significantly interference-prone with expected neighbors, the decision is to remain with default mapping.The actual remapping process is conducted upon the jobs' dispatch and involves making RPCs from the DFRA server to the compute nodes concerned, instructing them to drop off the original connection and connect to newly assigned forwarding nodes, allocated from the current available forwarding node pool.
Considering that a job tends to have consistent I/O behavior, this remapping is done once per job execution, rather than per request.
If remapped, when A completes, its compute nodes will be reset to default mapping, making DFRA maintenance simple and robust.
To decide on the "upgrade eligibility" of a job, we estimate its multiple I/O behavior metrics based on the query results of I/O monitoring database.
When historical information is not sufficient, e.g., as in the case of new applications, our system does not change the default mapping.
I/O monitoring data collected from these runs will help forwarding resource allocation in future executions.Our scaling decision-making adopts a per-job forwarding node allocation algorithm.
It considers both the applicationspecific I/O workload characteristics and historical performance data of forwarding node load levels while serving this application.
Most of the threshold values are set empirically according to our extensive benchmarking of the system, and can be further adjusted based on continuous I/O performance monitoring.
More specifically, the target job A needs to meet the following criteria to be eligible for a higher forwarding resource allocation than the default setting:1.
its total I/O volume is over V min during its previous execution; 2.
it has at least N min compute nodes performing I/O; and 3.
it is not considered metadata-operation-bound, i.e., its past average number of metadata operations waiting at a forwarding node's queues is under W metadata .
The rationale is based on the primary reason for a job to have an upgraded allocation: it possesses enough I/O parallelism to benefit from more forwarding resources.
For such benefit to offset the forwarding node remapping overhead, first the application needs to generate a minimum amount of I/O traffic.
Applications diagnosed as metadata-operation-heavy, regardless of their total I/O volume or I/O parallelism, are found to not benefit from more forwarding nodes as their bottleneck is the metadata server (MDS).
If A passes this test, with past history showing that it is expected to use N A of its compute nodes to perform I/O, the number of its forwarding node allocation S is calculated as N A /F.
Here F is a scaling factor that reflects typically how many I/O-intensive compute nodes can be handled by a forwarding node without reaching its performance cap.
In our implementation, F is set as B f /B c , where B f and B c are the peak I/O bandwidths of a single forwarding and compute node, respectively.
If not enough idle forwarding nodes are available, we allocate all the available nodes.
We expect this case to be extremely rare, as given the typical system load, there are enough idle forwarding nodes to satisfy all allocation upgrades.In our deployment on TaihuLight, we empirically set V min at 20 GB, N min at the F value (32 based on the formula above), and W metadata at twice the per-forwarding-node thread pool size, also 32.
These can be easily adjusted based on machine specifications and desired aggressiveness.We do not downgrade allocations for the metadata-heavy or 1-1 I/O mode jobs, considering their baseline per-process I/O activities (such as executable loading, logging, and standard output).
Also considering TaihuLight's sufficient backup forwarding nodes, we opt not to pay the remapping overhead for downgrading allocations in this deployment, though downgrading is easy to implement when needed.
Figure 7 shows how application I/O performance, in aggregate I/O bandwidth measured from the application side, changes with different compute-to-forwarding node ratios.
As these tests used dedicated forwarding nodes, we started from the 256-1 allocation, rather than the default 512-1.
Here several applications, namely APT, DNDC, WRF 1 , and CAM, due to insufficient I/O parallelism or being metadataheavy, do not pass the eligibility test.
Their I/O performance results confirm that they would have received very little performance improvement with more forwarding nodes been allocated.
The other applications, however, see substantial I/O bandwidth enhancement with increased forwarding node allocations, by up to a factor of 10.9×.
Judging from results across all such applications, our current F setting of 32 deliver best aggregate I/O bandwidth in most cases.
Our DFRA system attempts to mitigate this performance interference by assigning jobs that are expected to interfere to different forwarding nodes.
Note that prior work on interference detection and optimization focused mostly on deriving offline, analytical interference models (e.g., [28,31]).
In contrast, our work focuses on designing practical online interference estimation techniques that DFRA can use effectively.
We first conduct a rather controlled study, to observe I/O interference behavior between pairs of representative I/O applications.
From the applications described in Table 2, we select eight that cover different I/O resource-consumption patterns.
Next, we perform pairwise co-execution among these selected applications.
For this, we use 256 compute nodes (1024 MPI processes) each, so that the paired workloads have equal execution scale.
Under the default allocation, the 512 compute nodes running the two programs hence share one forwarding node.
To gauge interference, we measure each application's I/O slowdown by calculating the application's relative slowdown factor in overall I/O performance (the time spent in the I/O interference interval) from that of its solo run.
Table 3 shows the pairwise results, with high-interference pairs (with either slowdown factor >3) marked in bold, and mediuminterference ones (those among the rest with either slowdown factor >2) marked with "*".
The majority of our applications in this study are intensive in at least one dimension of I/O resource usage and are expected to see I/O performance slowdown when they share the same I/O path.
Results in Table 3 confirm this.
An application exhibits an I/O slowdown of around 2× when co-running with itself (another instance of the same application), due to the expected resource contention.
The remaining pairwise slowdown results reveal several interesting interference behaviors.First, we find that applications with low demands in all three dimensions (throughput, IOPS, and metadata operation rate) do not introduce or suffer significant I/O slowdown when co-running with other applications, with the exception of applications using the N-1 I/O mode (recall Table 2).
To understand the reasons behind, we conducted followup investigations.
The three applications that fall into the "Low/Low/Low" category are WRF 1 , CAM, and AWP.
Among them, AWP turns out to be a highly disruptive workload, causing high degrees of I/O slowdown to whoever runs with it.
We performed additional experiments, including MPI-IO tests emulating its behavior with different I/O parameters, and identified the problem being its N-1 file sharing mode.
While N-1 writes have been notoriously slow (such as with Lustre [20], also verified by our own benchmarking), our study reveals that it brings high disturbance (average of 38.4× to other applications tested).
Further examination of profiling results identified the forwarding layer as the source of interference.
Each forwarding node maintains a fixed thread pool, processing client requests from the compute nodes it is in charge of.
While designed to allow parallel handling of concurrent client requests, applications using the N-1 file sharing mode generate a large number of requests and flood the thread pool.
Their occupation of the forwarding layer thread resources is further prolonged by the slow Lustre backend processing of such I/O requests (often involving synchronization via locks).
The result is that other concurrent applications, whose I/O requests might be far fewer and more efficient, are blocked waiting for thread resources, while the I/O system remains under-utilized.
show the queue lengths of pending requests at the forwarding layer.
While the queue length increases proportionally to the number of compute processes, as expected, the "co-run" queue length of MPI-IO N does not grow significantly from its solo run.
The much greater increase in MPI-IO N latency (red curves using the right y axis), meanwhile, comes from the slowdown of each MPI-IO 1 request.Secondly, we observe from Table 3 that DNDC introduced significant slowdown to all other workloads (by a factor from 2.4× to 33.3×).
A closer look finds that DNDC is the only application in our testbed with significant metadata access intensity.
DNDC's production runs are not particularly large (only using 2048 processes), which simultaneously read 64,000 small files (up to several KBs each No.
of processes processing than open, with only slight increase in processing time when DNDC joins a read-heavy job, indicating bottleneck-free Lustre handling.
The wait time, however, sees almost 4× increase for read and around 2× for open operations.
Besides that the forwarding node thread pool being the point of contention, the asymmetric delay prompted us to examine its scheduling policy.
We found that metadata requests were given higher priority over normal file I/O, favoring interactive file system user experience.
This, combined with their longer processing time, makes metadataheavy applications like DNDC unsuspected disruptive workloads.
While our ongoing work targets more adaptive policies, for DFRA we specifically check jobs' metadata operation intensity for interference estimate.
Finally, we find that even applications with seemingly orthogonal resource usage patterns may not get along well, with asymmetric performance impact on each other.
In particular, we find that high-bandwidth, low-IOPS applications impact the performance of low-bandwidth, high-IOPS ones (but not vice versa).
This can be seen from the APT-MPI-IO N results in Table 3, with the high-IOPS APT suffering an almost 10× slowdown while the high-bandwidth MPI-IO N is hardly impacted.
A closer look reveals that APT reaches IOPS of over 80,000, with requests sized under 1KB.
The reason behind the asymmetric slowdown is then intuitive: high-bandwidth applications likely perform sequential I/O with large request sizes, which force the many small requests from the high-IOPS applications to wait long.WRF 1 - - - (1.0, 1.0) (1.0, 1.0) (1.0, 1.0) (1.0, 1.0) (50.0, 1.0) WRF N - - - - *(2.1, 2.1) *(2.0, 2.3) (1.0, 1.0) (12.5, 1.3) Shentu - - - - - *(2.0, 2.0) (1.0, 1.0) (12.5, 1.1) CAM - - - - - - (1.0, 1.0) (100.0, 1.0) AWP - - - - - - - *(2.0, 2.0)In summary, we discover that I/O interference not only comes from bandwidth-intensive applications and problematic access patterns (as assumed by previous studies [31,44]), but also from applications issuing inefficient I/O requests, while simultaneously incurring high contention and low utilization, such as in the metadata-heavy and high-IOPS cases.
We now discuss DFRA's inter-application interference check, introduced in Section 5.1.
Recall that it is needed only when we decide that the target job A, which is to be scheduled, does not need more forwarding nodes than granted by the default mapping.
The interference check is then performed pairwise, between A and each member of its neighbor application set A n .
As actual I/O interferences incurred during co-executions of applications depend on other factors such as their I/O phases' frequency and interleaving, we use our interference analysis results to make conservative, qualitative decisions.
More specifically, for A and each of its neighbor in A n , we consider interference is likely if either A or the neighbor is:1.
using the N-1 I/O mode, or 2.
considered "metadata operation heavy" (average number of metadata operations waiting at a forwarding nodes queue > W metadata ), or 3.
considered "high-bandwidth" or "high-IOPS" (using criteria described in Section 2.1).
For A, the above check has to be based on our monitoring system's per-application I/O performance history data.
For jobs in A n , however, our history-based I/O behavior inference can and should be complemented with real-time I/O behavior analysis.
In particular, as the inferred I/O behavior includes pattern information such as I/O phase frequency, I/O volume per process performing I/O, and I/O mode, such estimates can be verified by actual data collected during the neighbors' current execution.
E.g., if a forwarding node is receiving unexpectedly low I/O load from an application running, DFRA considers the application turns off I/O for this run, overriding its positive interference estimate.
Similarly, if an application is issuing I/O at intensity not indicated by its past history, we play safe and use the peak load level measured during its execution so far on the forwarding node(s) involved, to determine whether interference is likely.
First, DFRA's working relies on applications' overall consistency in I/O behavior.
We verified this with the 18-month TaihuLight I/O profiling results, confirming observations by existing studies [31,44].
Specifically, if we simply forecast a new job's I/O mode and volume as those in its latest run using the same number of compute nodes, we can successfully predict these parameters with under 20% deviation for 96,621 jobs (90.3%) out of 107,001 in total.
We then give statistics about DFRA's decisions and its potential beneficiaries, by running these 18-month job I/O profiles through DFRA's scaling decision making.
For jobs that were refused allocation upgrades, we categorize them by the first test failed during the DFRA allocation scaling eligibility check (Section 4).
Table 4 lists the results.
First, 13.7% jobs (minority in count yet accounting for 79.0% of core-hours) are granted upgrades and expected to benefit from DFRA.
This demonstrates that though the I/O system is overall underutilized, there are substantial amount of I/O-intensive jobs as potential beneficiaries.
Among the rest, most fail to meet the total I/O volume threshold V min , followed by the number of I/O nodes involved.
No job fails at the metadata-intensity check, as such applications in this particular job history do not pass the I/O volume test.Also, throughout this history "replay" using DFRA, the average forwarding node consumption is 171.2, suggesting that DFRA can get much better I/O performance while working well under the total 240-node forwarding capacity.
Next we examine the impact of DFRA's deployment on real applications' I/O performance in the TaihuLight production environment.
We run the 11 applications (introduced in Section 2.1) each for 10 times at randomly selected times during a 1-month period, each time under DFRA and FFM within the same job execution, with remapping done in between.
To control total resource usage, Shentu, LAMMPS, and Macdrp run with 1,024 compute nodes, with the other applications run at their typical mid-size run scale (swDNN using 512 nodes while the rest using 256).
They are further divided into two groups: scaling, with more forwarding nodes granted by DFRA, and non-scaling, with dedicated forwarding node allocation if deemed interference-prone by DFRA (which may depend on their neighbor jobs under the default mapping, though APT and DNDC are always isolated).
DFRA brings an average I/O speedup of 3.5× across all 11 applications, from 1.03×(CAM) to 18.9×(Shentu).
As expected, applications in the scaling group receive higher speedup (average at 4.8× and up to 18.9×), while nonscaling applications benefit more from reduced performance variability (and potential slowdown incurred on their neighbors).
However, the scaling group also obtains dramatic improvement in I/O performance consistency, with average reduction of 91.1% in range of I/O times.The reason lies in the "mis-alignment" of compute nodes to forwarding nodes using FFM.
Our job history finds over 99% of large-scale jobs (using 512 compute nodes or more) assigned to share forwarding nodes with other jobs, though their job scales are often perfect multiples of the default factor of 512.
Intuitively, such fragmentation often also leads to dramatic load imbalance across forwarding nodes (partially) serving the same I/O-intensive application.
Comp Table 5 describes the impact of DFRA on resourceintensive applications' overall performance.
All applications but one (swDNN) have clear repeated phases alternating between computation and I/O, while the number of such computation-I/O cycles may vary across runs according to users' needs.
Therefore we illustrate the relative impact by listing the more stable per-cycle average computation time, average I/O time (with FFM and DFRA respectively), and the percentage of total time saving by DFRA.
The last column does not change when a particular production run adjusts the number of computation-I/O cycles.
swDNN, unlike timestep numerical simulations, is a parallel deep learning model training application that has fine-grained, interleaving computation and I/O, therefore we treat its execution as a single I/O phase.As most applications conform to their "total I/O budget" by adjusting their I/O frequency, by taking one snapshot every k computation timesteps, DFRA is not expected to yield significant overall runtime reduction, especially with the non-scaling ones.
However, it does bring impressive total time savings for I/O-bound applications Shentu (45%) and swDNN (76%), as well as over 8% savings for APT and LAMMPS.
Meanwhile, making I/O faster also implies that the applications could afford to output more frequently under the same I/O budget.
Figure 10 illustrates this scenario using a Shentu test run using 1024 compute nodes, which are not allocated contiguously.
Under DFRA, its 32 dedicated forwarding nodes serve equal partitions of compute nodes, as each compute node can be individually remapped to any forwarding node, allowing almost all compute nodes finishing I/O simultaneously.
Under FFM, instead, these dispersed 1024 compute nodes are (a) Before DFRA (b) After DFRA Figure 11: Impact of DFRA's interference avoidance on pairwise application co-run slowdown.
Darkness of each block reflects the slowdown factor of the application at row header by the application at the column header.
Blocks with slowdown factor values give co-running pairs with interference anticipated by DFRA and hence allocated separate forwarding resources.
In all experiments, the compute-to-forwarding mapping uses the default setting (512-1), with DFRA allocating dedicated forwarding nodes to application pairs it considers interference-prone.
To evaluate our proposed interference avoidance, we rerun the pairwise experiments (see Section 5) with DFRA on TaihuLight.
Results are in Figure 11.
We found DFRA can detect potential interference with pairs having slowdown factors over 1.1 at either side.
In this test, we only separate these applications, without scaling up forwarding nodes, to isolate the benefits brought by interference avoidance.Compared with the left plot, where just by sharing a forwarding node, certain applications could perceive a 2× to 100× I/O slowdown, the right plot reduces such slowdown to uniformly under 1.1×.
With many jobs on TaihuLight sharing forwarding nodes, DFRA removes the infrequent (yet highly damaging) inter-application interference cases.Finally, we evaluate an alternative approach, RR, which maps compute nodes to forwarding nodes in a round-robin manner.
We test RR 32-1, where each group of contiguous 32 compute nodes are assigned to one forwarding node.
Fig- ure 12 gives the speedup (again over the 512-1 fixed allocation) of running one of the 5 applications given at the x-axis simultaneously with either DNDC or AWP.
Each application runs on 256 compute nodes, with two co-running applications sharing 8 forwarding nodes using RR.
For fair comparison, DFRA uses 64-1 allocation here, so that all co-run experiments enlist 8 forwarding nodes in total.
RR spreads the load of each application to all 8 forwarding nodes, but does not offer the performance isolation brought by DFRA, when two applications running on disjoint compute nodes get mapped to common forwarding nodes.
DFRA gives the two applications each a 64-1 dedicated allocation, delivering much higher I/O speedup in most cases, plus performance isolation from co-executing applications.
We now validate DFRA's forwarding node scaling decisions.
Figure 13 shows, in log scale, performance of MPI-IO benchmarks with parameters uniformly sampled from a range, to adopt different I/O modes (N-1, N-N and N-M), I/O performing nodes, I/O request sizes, and metadata operation ratios.
All tests are again divided into the scaling and non-scaling groups, referring to cases where the DFRA automatic scaling decision making processes chose to upgrade a job's forwarding node allocation, or retain the default one.
The final results for both cases are consistent with the estimations projected by DFRA.
Scaling cases can achieve on average 2.6× speedup (min at 1.1× and max at 7.1×), while the non-scaling ones' performance receives only trivial performance improvement (up to 1.05×).
Next we further examine the effectiveness of DFRA scaling, by measuring the queue length and I/O bandwidth of real-world applications on TaihuLight.
Figure 14 shows results, again in log scale, with representative applications covering all I/O categories mentioned in Table 2.
Among them, DNDC and APT are "non-scaling": DNDC is metadata-intensive and APT issues a large number of small-size I/O requests.
We find their performance bottleneck not at the forwarding layer, explaining their little improvement in queue length and bandwidth when given more forwarding nodes.
AWP adopts an N-1 I/O mode, generating high request pressure for forward- DFRA 64-1, dedicated RR 32-1, corun w. DNDC RR 32-1, corun w. AWP Figure 12: Speedup over 512-1 fixed allocation baseline, with two applications co-running, each using 256 compute nodes.
Note that with its dedicated allocation, DFRA's performance is not impacted by co-running applications.
ing nodes, thus receiving significant queue length improvement.
Both Shentu and LAMMPS are bandwidth-hungry, benefiting significantly from the bandwidth side.
In particular, Shentu gets a higher speedup as scaled-up allocation soothes its forwarding-side cache trashing.
DFRA could screen out the abnormal forwarding nodes automatically.
During our investigation, anomaly on forwarding nodes occurs for 6 times from Apr 2017 to Aug 2018.
Jobs using such abnormal forwarding nodes typically experience substantial performance degradation.
Figure 15 shows the performance impact when jobs get allocated an abnormal forwarding node.
The I/O performance could see a 20× slowdown, due to the explicit barriers common with parallel I/O, forcing all processes to wait for the slow progress of the impaired forwarding node.
Here we assess DFRA's overhead in performing the actual node remapping, while the allocation decision itself takes under 0.1s in all tests on TaihuLight.
Figure 16 shows the average remapping time cost for different job sizes, plus the corresponding job dispatch time (without remapping) for reference.
Though the remapping overhead increases linearly when more compute nodes are involved, it composes a minor addition to the baseline job dispatch overhead (the latter mainly due to compute nodes' slow wake-up from their power saving mode).
Note that this overhead is offset by our conservative screening based on jobs' past I/O profile.
Even with 16,384 compute nodes, such minor delay in job dispatch is negligible compared with the total time saved in I/O phases, especially for long-running jobs.
Since its deployment in Feb 2018, DFRA has brought an average execution time saving of over 6 minutes (up to several hours) to I/O-intensive jobs eligible for its remapping, estimated by comparing the I/O bandwidth benchmarked with the same application at the same job scale, before and after DFRA.
Going over the actual TaihuLight job history, we thus estimate DFRA's overall resource saving at over 200 million of core-hours.
Finally, we briefly report our recent effort to apply DFRA techniques to dynamic allocation of burst buffer (BB) resources.
We setup a testbed following the BB construction adopted by a previous study [41], containing 8 forwarding nodes, each with one 1.2TB Memblaze SSD to compose remote shared burst buffers.
Figure 17 shows the performance impact of scaling up BB node allocations.
All runs use 256 compute nodes.
Not surprisingly, the more I/O-intensive applications (using N-N or N-1) benefit significantly from more BB nodes, while the 1-1 mode WRF 1 sees little improvement.
The similarity between such result and that with forwarding resource scaling suggests that DFRA is promising for BB layer management as well.
To this end, the next generation Sunway supercomputer will adopt DFRA, including for its planned BB layer.
I/O forwarding design and optimization Cplant [12] first introduces the I/O forwarding layer, but without support for data caching or request aggregation.
I/O forwarding then became popular at extreme scale in IBM Blue Gene (BG) platforms [14,36,48].
IOFSL [13] is an open-source, portable, high performance I/O forwarding solution that provides a POSIX-like view of a forwarded file system to an application.
The Cray XC series uses Data Virtualization Service (DVS) [4] for I/O forwarding.
Our proposed DFRA methodology is compatible with recent trends in I/O forwarding adoption at large supercomputers, such as the Cray series.For better I/O forwarding performance, Ohta et al.[50] present two optimization methods to reduce I/O bottlenecks: I/O pipelining and request scheduling.
Vishwanath et al. [63] boost I/O forwarding through a work-queue model to schedule I/O and asynchronous data staging.
PLFS adds an interposition software layer that transparently partitions files to improve N-1 performance [20].
DFRA is orthogonal to these optimizations and focuses on application-aware forwarding allocation and performance isolation.
Resource-aware scheduling This work echoes efforts in resource-aware scheduling, such as approaches improving utilization of datacenters/cloud resources, including CPU, cache, memory, and storage [22,34,58].
Our focus, however, is on HPC systems.
To this end, AID [44] identifies applications' I/O patterns and reschedules heavy I/O applications to avoid congestion.
CALCioM [28] coordinates applications' I/O activities dynamically via inter-application communication.
Gainaru et al. propose a global scheduler [31], which based on system condition and applications' historical behavior prioritizes I/O requests across applications to reduce I/O interference.
The libPIO [65] library monitors resource usage at the I/O routers and based on the loads allocates OSTs to specific I/O clients.Regarding application-level I/O aware scheduling, AS-CAR [40] is a storage traffic management framework that improves bandwidth utilization by I/O pattern classification.
Lofstead et al. [46] propose an adaptive approach that groups processes and directs their output to particular storage targets, with inter-group coordination.
IOrchestrator [71] builds a monitoring program to retrieve spatial locality information and schedules future I/O requests.Our proposed scheme takes a different path that does not require any application or I/O middleware modification.
It observes application and system I/O performance, and based on both real-time monitoring results and past monitoring history, automatically adjusts its default allocation to grant more or dedicated forwarding resources.
I/O interference analysis On detecting and mitigating interference, Yildiz et al. [70] examine sources of I/O interference in HPC storage systems and identify the bad flow control across the I/O path as a main cause.
CALCiom [28] and Gainaaru's study [31] show that concurrent file system accesses lead to I/O bursts, and propose scheduling strategy enhancements.
On relieving burst buffer congestion, Kougkas et al. [39] leverage burst buffer coordination to stage application I/O.
TRIO [66] orchestrates application's write requests in the burst buffer and Thapaliya et al. [60] manage interference in the shared burst buffer through I/O request scheduling.
The ADIOS I/O middleware manages interference by dynamically shifting workload from heavily used OSTs to those less loaded [42].
Qian et al. [52] present a token bucket filter in Lustre to guarantee QoS under interference.This work is complementary to the above studies and uses interference analysis as a tool, achieving performance isolation using interference avoidance.
In this work, we explore adaptive storage resource provisioning for the widely used I/O forwarding architecture.
Our experience of deploying it on the No.3 supercomputer and evaluating with ultra-scale applications finds dynamic, perapplication forwarding resource allocation highly profitable.
Judiciously applied to a minor fraction of jobs expected to be sensitive to forwarding node mapping, our remapping scheme both generates significant I/O performance improvement and mitigates inter-application I/O interference.
We also report multiple prior findings by other researchers as confirmed or contradicted by our experiments.
Finally, though this study has focused on the allocation of forwarding nodes, the same approach can apply to other resource types, such as burst buffer capacity/bandwidth allocation.
We thank Prof. Zheng Weimin for his valuable guidance and advice.
We appreciate the thorough and constructive comments/suggestions from all reviewers.
We thank our shepherd, Rob Johnson, for his guidance during the revision process.
We would like to thank the National Supercomputing Center in Wuxi for great support to this work, as well as the Sunway TaihuLight users for providing test applications.
This work is partially supported by the National Key R&D Program of China (Grant No. 2017YFA0604500 and 2016YFA0602100), and National Natural Science Foundation of China (Grant No. 61722208, 41776010, and U1806205).
