This paper presents the results of the shared tasks from the 2nd workshop on Asian translation (WAT2015) including J↔E, J↔C scientific paper translation subtasks and C→J, K→J patent translation subtasks.
For the WAT2015, 12 institutions participated in the shared tasks.
About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated.
The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages.
Following the success of the previous workshop WAT2014( Nakazawa et al., 2014), WAT2015 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation.
We are working toward the practical use of machine translation among all Asian countries.For the 2nd WAT, we adopt new translation subtasks "Chinese-to-Japanese and Koreanto-Japanese patent translation" in addition to the subtasks that were conducted in WAT2014.WAT is unique for the following reasons:• Open innovation platformThe test data is fixed and open, so evaluations can be repeated on the same data set to confirm changes in translation accuracy over time.
WAT has no deadline for automatic translation quality evaluation (continuous evaluation), so translation results can be submitted at any time.
• Domain and language pairs WAT is the world's first workshop that uses scientific papers as the domain, and Chinese↔Japanese and Korean→Japanese as language pairs.
In the future, we will add more Asian languages, such as Vietnamese, Indonesian, Thai, Burmese and so on.
• Evaluation method Evaluation is done both automatically and manually.
For human evaluation, WAT uses crowdsourcing, which is low cost and allows multiple evaluations, as the first-stage evaluation.
Also, JPO adequacy evaluation is conducted for the selected submissions according to the crowdsourcing evaluation results.
WAT uses the Asian Scientific Paper Excerpt Corpus (ASPEC) 1 and JPO Patent Corpus (JPC) 2 as the dataset.
ASPEC is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT).
It consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for J↔E subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for J↔C subtasks.The statistics for each corpus are described in Table1.
JPC was constructed by the Japan Patent Office (JPO).
It consists of a Chinese-Japanese patent description corpus (JPC-CJ) and KoreanJapanese patent description corpus (JPC-KJ) with four sections, which are Chemistry, Electricity, Mechanical engineering, and Physics, based on International Patent Classification (IPC).
Each corpus is separated into training, development, development-test and test data, which are sentence pairs.
This corpus was used for patent subtasks C→J and K→J.
The statistics for each corpus are described in Table2.
The Sentence pairs in each data were randomly extracted from a description part of comparable patent documents under the condition that a similarity score between sentences is greater than or equal to the threshold value 0.05.
The similarity score was calculated by the method from ( Utiyama and Isahara, 2007) as with ASPEC.
Document pairs which were used to extract sentence pairs for each data were not used for the other data.
Furthermore, the sentence pairs was extracted to be same number among the four sections.
The maximize number of sentence pairs which are extracted from one document pair was limited to 60 for training data and 20 for the developent, development-test and test data.
The training data for JPC-CJ was made with sentence pairs of Chinese-Japanese patent documents published in 2012.
For JPC-KJ, the training data was extracted from sentence pairs of Korean-Japanese patent documents published in 2011 and 2012.
The development, development-test and test data for JPC-CJ and JPC-KJ were respectively made with 100 patent documents published in 2013.
Human evaluations were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each participant's system.
That is, the specific baseline system was the standard for human evaluation.
A phrase-based statistical machine translation (SMT) system was adopted as the specific baseline system at WAT 2015, which is the same system as that at WAT 2014.
In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems.
The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page 4 .
We used Moses ( Koehn et al., 2007;Hoang et al., 2009) as the implementation of the baseline SMT systems.
The Berkeley parser ( Petrov et al., 2006) was used to obtain syntactic annotations.
The baseline systems are shown in Table 3.
The commercial RBMT systems and the online translation systems were operated by the organizers.
We note that these RBMT companies and online translation companies did not submit themselves.
Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper.
We used the following data for training the SMT baseline systems.
• Training data for the language model: All of the target language sentences in the parallel corpus.
• Training data for the translation model: Sentences that were 40 words or less in length.
(For Japanese-English training data, we only used train-1.txt, which consists of one million parallel sentence pairs with high similarity scores.)
• Development data for tuning: All of the development data.
We used the following tools for tokenization.
4 http://lotus.kuee.kyoto-u.ac.jp/WAT/ • Juman version 7.0 5 for Japanese segmentation.
• Stanford Word Segmenter version 2014-01-04 6 (Chinese Penn Treebank (CTB) model) for Chinese segmentation.
• The Moses toolkit for English tokenization.
• Mecab-ko 7 for Korean segmentation.To obtain word alignments, GIZA++ and growdiag-final-and heuristics were used.
We used 5-gram language models with modified Kneser-Ney smoothing, which were built using a tool in the Moses toolkit (Heafield et al., 2013).
We used the following Moses configuration for the phrase-based SMT system.
• distortion-limit = 20 except for KJ and distortion-limit = 0 for KJ • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuringThe default values were used for the other system parameters.
We used the following Moses configuration for the hierarchical phrase-based SMT system.
• max-chart-span = 1000 • Phrase score option: GoodTuringThe default values were used for the other system parameters.
We used the Berkeley parser to obtain target language syntax.
We used the following Moses configuration for the string-to-tree syntax-based SMT system.
• max-chart-span = 1000 • Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, and NonTermConsecSource.The default values were used for the other system parameters.
We used the Berkeley parser to obtain source language syntax.
We used the following Moses configuration for the baseline tree-to-string syntaxbased SMT system.
• max-chart-span = 1000• Phrase score option: GoodTuring • Phrase extraction options: MaxSpan = 1000, MinHoleSource = 1, MinWords = 0, NonTermConsecSource, and AllowOnlyUnalignedWords.The default values were used for the other system parameters.
We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU ( Papineni et al., 2002) and RIBES ( Isozaki et al., 2010).
BLEU scores were calculated using multi-bleu.
perl distributed with the Moses toolkit ( Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 .
All scores for each task were calculated using one reference.
Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language.For Japanese segmentation, we used three different tools: Juman version 7.0 ( Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 (Tseng, 2005).
For Korean segmentation we used mecab-ko 12 .
For English segmentation we used tokenizer.perl 13 in the Moses toolkit.Detailed procedures for the automatic evaluation are shown on the WAT2015 evaluation web page 14 .
The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results.
Fig- ure 1 shows the submission interface for participants.
The system requires participants to provide the following information when they upload translation results:• Subtask:-Scientific papers subtask (J ↔ E, J ↔ C); -Patents subtask (C → J, K → J);• Method (SMT, RBMT, SMT and RBMT, EBMT, Other);• Use of other resources in addition to ASPEC or JPC;• Permission to publish the automatic evaluation scores on the WAT2015 web page.The server for the system stores all submitted information, including translation results and scores, although participants can confirm only the information that they uploaded.
Information about translation results that participants permit to be published is disclosed on the web page.
In addition to submitting translation results for automatic evaluation, participants submit the results for human evaluation using the same web interface.
This automatic evaluation system will remain available even after WAT2015.
Anybody can register to use the system on the registration web page 15 .
In WAT2015, we conducted 2 kinds of human evaluations: pairwise crowdsourcing evaluation and JPO adequacy evaluation.
The pairwise crowdsourcing evaluation is the same as the last year.
We used Lancers as the crowdsourcing platform.
There are two reasons of choosing Lancers.
One is that we can set the category of the crowdsourcing task ('Translation' in this case).
We can reach the appropriate workers by choosing the appropriate categories.
The other reason is that we can assign the task to identityverified workers.
This function guaranteed the quality of the workers.
These two advantages ensure a high evaluation quality.We used the same sentences as the last year for the pairwise crowdsourcing evaluation.
We randomly chose documents from the Test set from the ASPEC data, for a total of 400 sentence pairs for JE and JC.
We excluded documents containing sentences longer than 100 Japanese characters.
Each submission is compared with the baseline translation (Phrase-based SMT, described in Section 3) and given a Crowd score 16 .
We conducted pairwise evaluation of each of the 400 test sentences.
The input sentence and two translations (the baseline and a submission) are shown to the workers, and the workers are asked to judge which of the translation is better, or if they are of the same quality.
The order of the two translations are at random.
Figure 2 illustrates the evaluation.
The crowdsourcing workers are not specialists, and thus the quality of the judgments are not necessarily precise.
To guarantee the quality of the evaluations, each sentence is evaluated by 5 different workers and the final decision is made depending on the 5 judgements 17 .
We define each judgement j i (i = 1, · · · , 5) as:j i =    1if better than the baseline −1 if worse than the baseline 0 if the quality is the sameThe final decision D is defined as follows usingS = ∑ j i : D =    win (S ≥ 2) loss (S ≤ −2) tie (otherwise) Suppose that W is the number of wins compared to the baseline, L is the number of losses and T is the number of ties.
The Crowd score can be calculated by the following formula:Crowd = 100 × W − L W + L + TFrom the definition, the Crowd score ranges between -100 and 100.
There are several ways to estimate a confidence interval.
We chose to use bootstrap resampling (Koehn, 2004) to estimate the 95% confidence interval.
The procedure is as follows:1.
randomly select 300 sentences from the 400 human evaluation sentences, and calculate the Crowd score of the selected sentences 2.
iterate the previous step 1000 times and get 1000 Crowd scores 3.
sort the 1000 scores and estimate the 95% confidence interval by discarding the top 25 scores and the bottom 25 scores 17 We used 3 judgements in WAT2014.
A major benefit of using crowdsourcing is that it reduces the cost of evaluations.
In WAT2015, one judgment costs 5 JPY.
The evaluation of a submission requires 5 (judgments) × 400 (sentence pairs) = 2,000 judgments and costs 5 × 2,000 = 10,000 JPY.
The time for the evaluation differs depending on the translation direction.
On average, one evaluation takes a couple of days.
The participants' systems, which achieved the top 3 highest scores among the pairwise crowdsourcing evaluation results of each subtask, were also evaluated with the JPO adequacy evaluation.
The JPO adequacy evaluation was carried out by translation experts with a quality evaluation criterion for translated patent documents which the Japanese Patent Office (JPO) decided.
In addition to the top 3 systems, the Sense 1 system of the JPC-KJ subtask, which was the lower score on the pairwise crowdsourcing evaluation despite the high score on the automatic evaluation, was evaluated exceptionally.
For each system, two annotators evaluate the test sentences to guarantee the quality.
The number of test sentences for the JPO adequacy evaluation is 200.
The 200 test sentences were randomly selected from the 400 test sentences of the pairwise evaluation.
The test sentence include the input sentence, the submitted system's translation and the reference translation.
Table 4 shows the JPO adequacy criterion from 5 to 1.
The evaluation is performed subjectively.
"Important information" represents the technical factors and their relationships.
The degree of importance of each element is also considered to evaluate.
The percentages in each grade are rough indications for the transmission degree of the source sentence meanings.
The detailed criterion can be found on the JPO document (in Japanese) 18 .6 Participants List Table 5 shows the list of participants for WAT2015.
This includes not only Japanese organizations, but also some organizations from outside Japan.
12 teams submitted one or more translation results to the both automatic evaluation server and human evaluation.
Waseda University ✓ naver ( Lee et al., 2015) NAVER Corporation ✓ ✓ EHR (Ehara, 2015) Ehara NLP Research Laboratory ✓ ✓ ✓ ✓ ntt ( Sudoh and Nagata, 2015) NTT Communication Science Laboratories ✓ Table 5: List of participants who submitted translation results to WAT2015 and their participation in each subtasks.
In this section, the evaluation results for WAT2015 are reported from several perspectives.
Some of the results for both automatic and human evaluations are also accessible at the WAT2015 website 19 .
Crowd Score Figure 5 shows the official crowdsourcing evaluation results.
The error bars in the figures show the 95% confidence interval (see Section 5.1.4).
Note that overlapping error bars between two submissions do not necessarily mean that there is no significant difference.
If an error bar crosses the x-axis (Crowd score = 0), it means that there is no significant difference between the submission and the baseline (SMT Phrase).
Tables 6, 7, 8, 9, 10 and 11 show the results of statistical significance testing for the JE, EJ, JC and CJ translations respectively where all the pairs of submissions are tested.
≫, ≫ and > mean that the system in the row is better than the system in 19 To assess the reliability of agreement between the crowdsourcing workers, we calculated the Fleiss' κ (Fleiss and others, 1971) values.
The results are shown in Table 12.
We can see that the κ values are larger for X → J translations than for J → X translations.
This may be because we used a Japanese crowdsourcing service for the evaluation and so the majority of the crowdsourcing workers are Japanese, and the evaluation of one's mother tongue is much easier than for other languages in general.
Also, K → J evaluations seem to be more consistent than the other directions.
This may be because the quality of K → J translations are much better than the other directions.
Figure 6 and 7 show the correlations between the automatic evaluation measures (BLEU/RIBES) and the Crowd score.
Table 7: Statistical significance testing of the ASPEC-EJ Crowd scores.
Figure 8 shows the chronological evaluation results of ASPEC.
The x-axis indicates the BLEU score and the y-axis indicates the Crowd score.
Note that the first 3 annotations among 5 by the crowdsourcing were used, and the decision for each sentence is made by the same criteria in WAT2014 for calculating the Crowd score of WAT2015 submissions.
Figure 10 shows the summary of automatic and human evaluations for the selected submissions.
We can see that all of Crowd, BLEU and RIBES scores partially correlate to the JPO adequacy score, but none of them perfectly correlates.
Especially, for JPC-KJ, both BLEU and RIBES failed to correctly evaluate the quality of Sense team.
From the evaluation results, the following can be observed (see Figure 10):8 Kyoto-U 2 TOSHIBA 2 TOSHIBA 1 RBMT D Kyoto-U 1 NICT 1 NAIST 2 SMT S2T NICT 2 Online D Sense 1 Sense 2 TMU NAIST 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ TOSHIBA 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ TOSHIBA 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ RBMT D - - ≫ ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ NICT 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ NAIST 2 ≫ ≫ ≫ ≫ ≫ ≫ SMT S2T - ≫ ≫ ≫ ≫ NICT 2 ≫ ≫ ≫ ≫ Online D ≫ ≫ ≫ Sense 1 ≫ ≫ Sense 2 ≫NAIST 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ WEBLIO MT 1 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ naver 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ NAIST 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ naver 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ WEBLIO MT 2 > > ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 1 - ≫ ≫ ≫ ≫ ≫ ≫ TOSHIBA ≫ ≫ ≫ ≫ ≫ ≫ Online A - ≫ ≫ ≫ ≫ EHR - ≫ ≫ ≫ SMT T2S ≫ ≫ ≫ RBMT B ≫ ≫ Sense 2 ≫ Kyoto-U 1 Kyoto-U 2 SMT S2T NAIST 1 NAIST 2 TOSHIBA 1 RBMT B Online D TOSHIBA 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 1 > ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 2 ≫ ≫ ≫ ≫ ≫ ≫ SMT S2T - ≫ ≫ ≫ ≫ NAIST 1 ≫ ≫ ≫ ≫ NAIST 2 - ≫ ≫ TOSHIBA 1 ≫ ≫ RBMT B >NAIST 1 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ NAIST 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ EHR ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ Kyoto-U 2 - - - ≫ ≫ ≫ ≫ TOSHIBA 1 - - ≫ ≫ ≫ ≫ SMT T2S - ≫ ≫ ≫ ≫ Kyoto-U 1 ≫ ≫ ≫ ≫ BJTUNLP ≫ ≫ ≫ TOSHIBA 2 ≫ ≫ Online A ≫• Neural Network based re-ranking is effective (NAIST, Kyoto-U, naver).
• The top SMT outperformed RBMT for CJ and KJ patent translation.
• K→J patent translation achieved high scores for automatic and human evaluations.
• A new problem of automatic evaluation was found in the KJ evaluation.
The number of published automatic evaluation results for the twelve teams exceeded 400 before the start of WAT2015, and 56 translation results for pairwise crowsdourcing evaluation were submitted by twelve teams.
Furthermore, we selected 3 translation results from each subtask and evaluated them for JPO adequacy evaluation.
We will organize the all of the submitted data for human evaluation and make this public.
This paper summarizes the WAT2015 machine translation evaluation campaign.
We had 12 participants worldwide, and collected a large number of useful submissions for improving the current machine translation systems by analyzing the submissions and identifying the issues.For the next WAT workshop, we plan to conduct context-aware MT evaluations.
The test data for WAT are prepared using the paragraphs as the unit, while almost all other evaluation campaigns use the sentences as the unit.
Therefore, it is suitable to investigate the importance of context in translation.We would also be very happy to include other languages if the resources are available.
Tables 14, 15, 16, 17, 18, 19, summarize all the submissions listed in the automatic evaluation server at the time of the WAT2015 workshop (16th, October, 2015).
The OTHER RE-SOURCES column shows the use of resources such as parallel corpora, monolingual corpora and 10 TOSHIBA 2 EHR 1 SMT T2S ntt 1 TOSHIBA 1 Kyoto-U 1 EHR 2 ntt 2 Online A WASUIPS RBMT A Kyoto-U 2 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ TOSHIBA 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ EHR 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ SMT T2S ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ntt 1 - - ≫ ≫ ≫ ≫ ≫ TOSHIBA 1 - ≫ ≫ ≫ ≫ ≫ Kyoto-U 1 ≫ ≫ ≫ ≫ ≫ EHR 2 - ≫ ≫ ≫ ntt 2 ≫ ≫ ≫ Online A ≫ ≫ WASUIPS ≫NICT 2 EHR 1 NICT 1 naver 1 EHR 2 TOSHIBA 2 Sense 2 TOSHIBA 1 SMT Hiero RBMT A Sense 1Online .
-6)!
78!
5)92:12;#<5-1==2"4!
A ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ naver 2 ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ NICT 2 - > ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ EHR 1 - ≫ ≫ ≫ ≫ ≫ ≫ ≫ ≫ NICT 1 - - ≫ ≫ ≫ ≫ ≫ ≫ naver 1 - > ≫ ≫ ≫ ≫ ≫ EHR 2 - > ≫ > ≫ ≫ TOSHIBA 2 - - - ≫ ≫ Sense 2 - - ≫ ≫ TOSHIBA 1 - ≫ ≫ SMT Hiero ≫ ≫ RBMT A ≫'& !
& *& ,& %& !"
#!
$% !!
% &'#($% $"#$% ('#($% ()#!
$% *(#$% (*#!
$% )% )#($% &#!
$% ))% '#$% +#!
$% (% ',%.
;<=>?
@3% AB4C;% :DE7% F/:E0% !"
##$$ !"
%&$$ !"'
($$ !
("%($$ )#"(*$$ '("%($$ &)"%($$ !
#"(!
$$ !
%"+*$$ #("()$$ #&"*%$$ #'",*$$ *$$ )*$$ '*$$ !
*$$ &*$$ (*$$ ,*$$ %*$$ #*$$ +*$$ *$$ )$$ '$$ !
$$ &$$ ($$ -
