Cooow scheduling improves data-intensive application performance by improving their networking performance.
State-of-the-art online cooow schedulers in essence approximate the classic Shortest-Job-First (SJF) scheduling by learning the cooow size online.
In particular, they use multiple priority queues to simultaneously accomplish two goals: to sieve long cooows from short cooows, and to schedule short cooows with high priorities.
Such a mechanism pays high overhead in learning the cooow size: moving a large cooow across the queues delays small and other large cooows, and moving similar-sized cooows across the queues results in inadvertent round-robin scheduling.
We propose PPPPPP, a new online cooow scheduler that exploits the spatial dimension of cooows, i.e., a cooow has many ows, to drastically reduce the overhead of cooow size learning.
PPPPPP pre-schedules sampled ows of each cooow and uses their sizes to estimate the average ow size of the cooow.
It then resorts to Shortest Cooow First, where the notion of shortest is determined using the learned cooow sizes and cooow contention.
We show that the sampling-based learning is robust to ow size skew and has the added beneet of much improved scalability from reduced coordinator-local agent interactions.
Our evaluation using an Azure testbed, a publicly available production cluster trace from Facebook shows that compared to the prior art Aalo, PPPPPP reduces the cooow completion time (CCT) in average (P90) cases by 1.50× (8.00×) on a 150-node testbed and 2.72× (9.78×) on a 900-node testbed.
Evaluation using additional traces further demonstrates PPPPPP's robustness to ow size skew.
In big data analytics jobs, speeding up the communication stage where the data is transferred between compute nodes is important to speed up the jobs.
However, improving network level metrics such as ow completion time may not translate into improvements at the application level metrics such as job completion time.
The cooow abstraction [18] was proposed to bridge such a gap.
The abstraction captures the collective network requirements of applications, as reduced cooow completion time (CCT) can directly lead to faster job completion time [20,24].
There have been a number of eeorts on network designs for cooows [7,21,27] that assume complete prior knowledge of cooow sizes (The cooow size is deened as the total size of its constituent ows.)
.
However, in many practical settings, cooow characteristics are not known a priori.
For example, multi-stage jobs pipeline data from one stage to the next as soon as the data is generated, which makes it diicult to know the size of each ow [22,40].
A recent study [40] shows various other reasons why it is not very plausible to learn ow sizes from applications, for example, learning ow sizes from applications requires changing either the network stack or the applications.Scheduling cooows in such non-clairvoyant settings, however, is challenging.
The major challenge in developing an eeective non-clairvoyant cooow scheduling scheme has centered around how to learn the cooow sizes online quickly and accurately, as once the cooow sizes (bytes to be transferred) can be estimated, one can apply variations of the classic Shortest-Job-First (SJF) algorithm such as Shortest Cooow First [21] or apply an LP solver (e.g., [7]).
State-of-the-art online non-clairvoyant schedulers such as Saath [30], Gravtion [29] and Aalo [19] in essence learn cooow sizes and approximate SJF using discrete priority queues, where all newly arriving cooows start from the highest priority queue, and move to lower priority queue as they send more data (without nishing), i.e., cross the per-queue thresholds.
In this way, the smaller cooows nish in high priority queues, while the larger cooows gradually move to the lower priority queues where they nish after smaller cooows.To realize the above idea in scheduling cooows which have ows at many network ports, i.e., in a distributed setting, Aalo uses a global coordinator to assign cooows to logical priority queues, and uses the total bytes sent by all ows of a cooow as its logical "length" in moving cooows across the queues.
The logical priority queues are mapped to local priority queues at each port, and the individual local ports then schedule the ows in its local priority queues, e.g., by enumerating ows from the highest to lowest priority queues and using FIFO to order the ows within each queue.In essence, Aalo learns cooow sizes by actually scheduling the cooow, a "try and miss" approach to approximate SJF.
As cooow sizes are not known, in each queue, Aalo schedules each cooow for a xed amount of data (try).
If the cooow does not nish (miss), it is demoted to a lower priority queue.Afterwards, such a cooow will no longer block cooows in higher priority queues.Using multiple priority queues to learn the relative cooow sizes of cooows this way, however, negatively aaects the average CCT and the scalability of the coordinator:(1) Intrinsic queue-transit overhead: Every cooow that Aalo transits through the queues before reaching its nal queue worsens the average CCT because during transitions, such a cooow eeectively blocks other shorter cooows in the earlier queues it went through, which would have been scheduled before this cooow starts in a perfect SJF.
(2) Overhead due to inadvertent round-robin: Although Aalo attempts to approximate SJF, it inadvertently ends up doing round-robin for cooows of similar sizes as it moves them across queues.
Aalo assigns a xed threshold of data transfer for each cooow in each queue.
Assume there are "N" cooows in a queue that do not nish in that queue.
Aalo schedules one cooow (chosen using FIFO) and demotes it to a lower priority queue when the cooow reaches the data threshold.
At that point, the next cooow from the same queue is scheduled, which joins the previous cooow at a lower priority queue after exhausting its quantum, and this cycle continues as cooows of similar sizes move through the queues.
EEectively, these cooows experience the roundrobin scheduling which is known to have the worst average CCT [39], when jobs are of similar sizes.
(3) Limited scalability from frequent updates from local ports: To support the try-and-error style learning, the coordinator requires frequent updates from all local ports of the bytes sent for each cooow in order to move cooows across multiple queues timely.
This results in high load on the central coordinator from receiving frequent updates and calculating and sending new rate allocations, which limits the scalability of the overall approach.Empirical measurement We quantify the cooow size learning overhead of Aalo, deened as the portion of the bytes of a cooow that has been transferred (or the fraction of its CCT spent in doing so) before reaching its correct queue, using a trace from Facebook clusters [4] (see detailed methodology in §8).
Figure 1 shows that 40% of the cooows that moved beyond the initial queue reached the correct priority queue after spending more than 20% of their CCT moving across early queues.
We propose PPPPPP, a new non-clairvoyant cooow scheduler with a dramatically diierent approach to learning cooow sizes to enable online SJF.
To leverage optimal scheduling SJF in cooow scheduling, it is vital to learn the cooow sizes quickly and accurately.
PPPPPP achieves this objective by exploiting the spatial dimension of cooows, i.e., a cooow typically consists of many ows, via sampling, a highly eeective technique used in large-scale surveys [34].
In particular, PPPP pre-schedules sampled ows, called pilot ows, of each cooow and uses their measured size to estimate the cooow size.
It then resorts to SCF using the estimated job size.
Intuitively, such a sampling scheme avoids all three sources of overhead in Aalo -Once the cooow sizes are learned, the cooows are assigned to the correct queues, which avoids the intrinsic queue-transit and round-robin eeects.
Further, a sampling-based design has an important beneetit ooers much higher scalability than priority-queue-based learning in Aalo.
This is because unlike Aalo, after estimating the cooow size, PPPPPP clients do not need to send periodic updates of bytes sent-so-far to the centralized coordinator.Developing a complete non-clairvoyant cooow scheduler based on the simple sampling idea raises three questions: (1) Why is sampling more eecient than the priority-queuebased cooow size learning?
(2) Will sampling be eeective in the presence of skew of ow sizes?
(3) How to design the complete scheduler architecture?
We systematically address these questions with design rational, theoretical analysis, system design, prototyping, and extensive evaluation.In summary, this paper makes the following contributions:(1) Using a production datacenter trace from Facebook, we show that the prior art scheduler Aalo spends substantial amount of time and network bandwidth in learning cooow sizes, which negatively aaects the CCT of cooows.
(2) We propose the novel idea of applying sampling in the spatial dimension of cooow to signiicantly reduce the overhead of online learning cooow sizes.
(3) We present theoretical underpinning explaining why sampling remains eeective in the presence of ow size skew.
(4) We present the design and implementation of PPPPPP.
(5) We extensively evaluate PPPPPP via simulations and testbed experiments, and show that compared to the prior art, the new design reduces the average CCT by 1.51× for the Facebook cooow trace and by 1.36× for a trace with properties similar to a Microsoft production cluster.
(6) The CCT improvement mainly stems from reduced cooow size learning overhead.
PPPPPP reduces the median latency and data sent in nding the right queue for cooows in Aalo by 19.0× and 20.0×, respectively ( §8.2).
We start with a brief review of the cooow abstraction and the need for non-clairvoyant cooow scheduling.
We then state the network model and problem formulation.Cooow abstraction In data-parallel applications such as Hadoop [1] and Spark [2], the job completion time heavily depends on the completion time of the communication stage [12,20].
The cooow abstraction [18] was proposed to speed up the communication stage to improve application performance.
A cooow is deened as a set of ows between several nodes that accomplish a common task.
For example, in map-reduce jobs, the set of all ows from all map to all reduce tasks in a single job forms a typical cooow.
The cooow completion time (CCT) is deened as the time duration between when the rst ow arrives and the last ow completes.
In such applications, improving CCT is more important than improving individual ows' completion time (FCT) for improving the application performance [19,21,24,29,30].
Non-clairvoyant cooows Data-parallel directed acyclic graphs (DAGs) typically have multiple stages which are represented as multiple cooows with dependencies between them.
Recent systems (e.g., [3,22,28,36]) employ optimizations that pipeline the consecutive computation stages which removes the barrier at the end of each cooow, making knowing ow sizes of each cooow beforehand diicult.
Thus in this paper, we focus on non-clairvoyant cooow scheduling which do not assume knowledge about cooow characteristics such as ow sizes upon cooow arrival.Non-blocking network fabric We assume the same nonblocking network fabric model in recent network designs for cooows [7,19,21,29,30], where the datacenter network fabric is abstracted as a single non-blocking switch that interconnects all the servers, and each server (computing node) is abstracted as a network port that sends and receives ows.
In such a model, the ports, i.e., server uplinks and downlinks, are the only source of contention as the network core is assumed to be able to sustain all traac injected into the network.
We note that the abstraction is to simplify our description and analysis, and is not required or enforced in our evaluation.Problem statement Our goal is to develop an eecient nonclairvoyant cooow scheduler that optimizes the communication performance, in particular the average CCT, of data-intensive applications without prior knowledge, while guaranteeing starvation freedom and work conservation and being resilient to the network dynamics.
The problem of non-clairvoyant cooow scheduling is NP-hard because cooow scheduling even assuming all cooows arrive at time 0 and their size are known in advance is already NP-hard [21].
Thus practical non-clairvoyant cooow schedulers are approximation algorithms.
Our approach is to dynamically prioritize cooows by eeciently learning their ow sizes online.
Our new non-clairvoyant cooow scheduler design, PPPPPP, is based on a key observation about cooows that a cooow has a spatial dimension, i.e., it typically consists of many ows.
We thus propose to explicitly learn cooow sizes online by using sampling, a highly eeective technique used in large-scale surveys [34].
In particular, PPPPPP preschedules sampled ows, called pilot ows, of each cooow and uses their measured sizes to estimate the cooow size.
It then resorts to SJF or variations using the estimated cooow sizes.
Developing a complete non-clairvoyant cooow scheduler based on the simple sampling idea raises three questions:(1) Why is sampling more eecient than the priority-queuebased cooow size learning?
Would scheduling the remaining ows after sampled pilot ows are completed adversely aaect the cooow completion time?
(2) Will sampling be eeective in the presence of skew of ow sizes?
(3) How to design the complete scheduler architecture?We answer the rst two questions below, and present the complete architecture design in §4.
Scheduling pilot ows rst before the rest of the ows can potentially incur two sources of overhead.
First, scheduling pilot ows of a newly arriving cooow consumes port bandwidth which can delay other cooows (with already estimated sizes).
However, compared to the multi-queue based approach, the overhead is much smaller for two reasons: (1) PPPPPP schedules only a small subset of the ows (e.g., fewer than 1% for cooows with many ows).
(2) Since the CCT of a cooow depends on the completion of its last ow, some of its earlier nishing ows could be delayed without aaecting the CCT.
PPPPPP exploits this observation and schedules pilot ows on the least-busy ports to increase the odds that it only aaects earlier nishing ows of other cooows.Second, scheduling pilot ows rst may elongate the CCT of the newly arriving cooow itself whose other ows cannot start until the pilot ows nish.
This is again typically insigniicant for two reasons: (1) A cooow (e.g., from a MapReduce job) typically consists of ows from all sending ports to all receiving ports.
Conceptually, pre-scheduling one out of multiple ows from each sender may not delay the cooow progress at that port, because all ows at that port have to be sent anyway.
(2) Cooow scheduling is of high relevance in a busy cluster (when there is a backlog of cooows in the network), in which case the CCT of cooow is expected to be much higher than if it were the only cooow in the network, and hence the piloting overhead is further dwarfed by a cooow's actual CCT.3.2 Why is sampling eeective in the presence of skew?The ow sizes within a cooow may vary (skew).
Intuitively, if the skew across ow sizes is small, sampling even a small number of pilot ows will be suucient to yield an accurate estimate.
Interestingly, even if the skew across ow sizes is large, our experiment indicates that sampling is still highly eeective.
In the following, we give both the intuition and theoretical underpinning for why sampling is eeective.
Consider, for example, two cooows and the simple setting where both cooows share the same set of ports.
In order to improve the average CCT, we wish to schedule the shorter cooow ahead of the longer cooow.
If the total sizes of the two cooows are very diierent, then even a moderate amount of estimation error of the cooow sizes will not alter their ordering.
On the other hand, if the total sizes of the two cooows are close to each other, then indeed the estimation errors will likely alter their ordering.
However, in this case since their sizes are not very diierent anyway, switching the order of these two cooows will not signiicantly aaect the average CCT.Analytic results.
To illustrate the above eeect, we show that the gap between the CCT based on sampling and assuming perfect knowledge is small, even under general ow size distributions.
Speciically, cooows C 1 and C 2 have cn 1 and cn 2 ows, respectively.
Here, we assume that n 1 and n 2 are xed constants.
Thus, by taking c to be larger, we will be able to consider wider cooows.
Assume that each ow ofC 1 (correspondingly, C 2 ) has a size that is distributed within a bounded interval [a 1 , b 1 ] ([a 2 , b 2 ]) with mean µ 1 (µ 2 ), i.i.d.across ows.
However, the exact distributions can be arbitrary.
Let T c be the total completion time when the exact ow sizes are known in advance.
Let˜TLet˜ Let˜T c be the average CCT by sampling m 1 and m 2 ows from C 1 and C 2 , respectively.
Without loss of generality, we assume that n 2 µ 2 ≥ n 1 µ 1 .
Then, using Hoeeding's Inequality, we can show that,lim c→∞˜T c→∞˜ c→∞˜T c − T c T c ≤ 4 exp   − 2(n 2 µ 2 − n 1 µ 1 ) 2 n 2 (b 2 −a 2 ) √ m 2 + n 1 (b 1 −a 1 ) √ m 1 2    n 2 µ 2 − n 1 µ 1 n 2 µ 2 + 2n 1 µ 1(1) (Note that here we have used the fact that, since both cooows share the same set of ports and c is large, the CCT is asymptotically proportional to the cooow size.)
Equation (1) can be interpreted as follows.
First, due to the rst exponential term, the relative gap betweeñ T c and T c decreases as b 1 − a 1 and b 2 − a 2 decrease.
In other words, as the skew of each cooow decreases, sampling becomes more eeective.
Second, when b 1 − a 1 and b 2 − a 2 are xed, if n 2 µ 2 − n 1 µ 1 is large (i.e., the two cooow sizes are very diierent), the value of the exponential function will be small.
On the other hand, if n 2 µ 2 − n 1 µ 1 is close to zero (i.e., the two cooow sizes are close to each other), the numerator on the second term on the right hand side will be small.
In both cases, the relative gap betweeñ T c and T c will also be small, which is consistent with the intuition explained earlier.
The largest gap occurs when n 2 µ 2 − n 1 µ 1 is on the same order asn 2 (b 2 −a 2 ) √ m 2 + n 1 (b 1 −a 1 ) √ m 1.
Finally, although these analytical results assume that both cooows share the same set of ports, similar conclusions on the impact of estimation errors due to sampling also apply under more general settings.The above analytical results suggest that, when c is large, the relative performance gap for CCT is a function of the number of pilot ows sampled for each cooow, but is independent of the total number of ows in each cooow.
In practice, large cooows will dominate the total CCT in the system.
Thus, these results partly explain that, while in our experiments the number of pilot ows is never larger than 1% of the total number of ows, the performance of our proposed approach is already very good.Finally, the above analytical results do not directly tell us how to choose the number of pilot ows, which likely depends on the probability distribution of the ow size.
In practice, we do not know such distribution ahead of time.
Further, while choosing a larger number of pilot ows reduces the estimation errors, it also incurs higher overhead and delay.
Therefore, our design ( §4) needs to have practical solutions that carefully address these issues.
In this section, we present the detailed design of PPPPPP, which addresses three design challenges: (1) Cooow size estimation: How to choose and schedule the pilot ows for each newly arriving cooow?
(2) Starvation avoidance: How to schedule cooows after size estimation using variations of SJF that avoid starvation?
(3) Cooow scheduling: How to schedule among all the cooows with estimated sizes?
(2) local agents that run on individual ports.
A computing framework such as Spark [42] rst registers (removes) a cooow when a job arrives ((nishes).
Upon a new cooow arrival, old cooow completion, or pilot ow completion, the coordinator calculates a new cooow schedule, which includes (1) cooows that are to be scheduled in the next time slot, and (2) ow rates for the individual ows of a cooow, and pushes this information to the local agents which use this information to allocate their bandwidth.
The local agents will follow the current schedule until they receive a new schedule.
As discussed in §3, PPPPPP estimates the size of a cooow online by actually scheduling a subset of its ows (pilot ows) at their ports.
We do not schedule the ows of a cooow other than the pilot ows until the completion of the pilot ows in order to avoid unnecessary extra blocking of other potentially shorter cooows.
How many pilot ows?
When a new cooow arrives, PPPP rst needs to determine the number of pilot ows.
As discussed at the end of §3, the number of pilot ows aaects the trade-oo between the cooow size estimation accuracy and scheduling overhead.
For cooows with skewed ow sizes, accurately estimating the total cooow size potentially requires sampling the sizes of many pilot ows.
However, scheduling pilot ows has associated overhead, i.e., if the cooow turns out to be a large cooow and should have been scheduled to run later under SJF.We explore several design options for choosing the number of pilot ow.
Two natural design choices are using a constant number of pilot ows or a xed fraction of the total number of ows of a cooow.
In addition, we observe that typical cooows consist of ows between a set of senders (e.g., mappers) and a set of receivers (e.g., reducers) [23].
We thus include a third design choice of a xed fraction of sending ports.
This design also spreads the pilot ows to avoid having multiple pilot ows contending for the same sending ports.
We empirically found that ( §8.2) limiting the pilot ows to 5% to 10% of the number of its sending ports (e.g., mappers in a MapReduce cooow) strikes a good balance between estimation accuracy and overhead.
We note the total number of ows sampled in this case is still under 1%.
Finally, we estimate the total cooow size as S = f i · N, where N is the number of ows in a cooow, and f i is the average size of the sampled pilot ows.
Which ows to probe?
Second, PPPPPP needs to decide which ports to schedule the chosen number of probe ows for a cooow.
For this, we use a simple heuristic where, upon the arrival of a new cooow, we select the ports for its pilot ows that are least busy, i.e., having pilot ows from the least number of other cooows.
PPPPPP starts with the least busy sending port and iterates over receiving ports starting with the least busy receiving port and assigns the ow if it exists.
It then updates the statistics for the number of pilot ows scheduled at each port and repeats the above process.
Such a choice will likely delay fewer cooows when the pilot ows are scheduled and hence reduce the elongation on their CCT.
We note that such an online heuristic may not be optimal; more sophisticated algorithms can be derived by picking ports for multiple cooows together.
However, we make this design choice for its simplicity and low time complexity to ensure that the coordinator makes fast decisions.
How to schedule pilot ows?
In PPPPPP, we prioritize the pilot ows of a new cooow over existing ows to accelerate learning the size of the new cooow.
In particular, at each port, pilot ows have high priority over non-pilot ows.
If there are multiple outstanding pilot ows (of diierent cooows) at a port, PPPPPP schedules them in the FIFO order.
Once the sizes of cooows are learned, we can apply variations of the SJF policy to schedule them.
However, it is well known that such policies can lead to starvation.
There are many ways to mitigate the starvation issue.
However, a subtlety arises where even slight diierence in how starvation is addressed can result in diierent performance.
For example, the multiple priority queues in Aalo has the beneet of ensuring progress of all cooows, but assigning diierent time-quanta to diierent priority queues can result in diierent average CCT for the same workload.
To ensure the fairness of performance comparison with Aalo, we need to ensure that both PPPPPP and Aalo provide the same level of starvation freedom (or progress measure).
For this reason, in this paper, we inherit the multiple priority queue structure from Aalo for cooow scheduling.
As in Aalo, PPPPPP sorts the cooows among multiple priority queues.
In particular, PPPPPP uses N queues, Q 0 to Q N−1 , with each queue having lower queue threshold Q lo q and higher threshold Q hi q , where Q lo 0 = 0, Q hi N−1 = ∞, Q lo q+1 = Q hi q , and the queue thresholds grow exponentially, i.e., Q hi q+1 = E · Q hi q .
The overall cooow scheduling in PPPPPP works as follows.
After the cooow size is estimated using pilot ows, PPPPPP assigns the cooow to the priority queue using inter-cooow policies discussed in §4.4.
Within a queue, we use FIFO to schedule cooows.
Lastly, we use weighted sharing of network bandwidth among the queues, where a priority queue receives a network bandwidth based on its priority.
As in Aalo, the weights decrease exponentially with decrease in the priority of the queues.Using FIFO within the priority queue and weighted fair sharing among the queues together ensure the same starva-tion freedom and thus meaningful performance comparison between PPPPPP and Aalo [19].
In PPPPPP, we explore four diierent scheduling policies based on diierent combinations of cooow size and contention, two size-based policies (A, B) as in Aalo, a contention-based, similar to the intra-queue policy used in Saath [30] We use the following parameters of a cooow to deene the metrics in scheduling algorithms: (1) average ow length (l) from piloting, (2) number of ows (n), (3) number of sender and receiver ports (s, r), (4) total amount of data sent so far (d), (5) contention (c), deened as the number of other cooows sharing any ports with the given cooow, and (6) port-wise contention (c p ), deened as the number of other cooows blocked at a given port p.PPPPPP uses Policy D by default, as it results in the least average CCT ( §8).
For all policies, we continue to use the priority-queue based scheduling, and the algorithms only diier in what metric they use in assigning cooows to the priority queues.
In contrast, Aalo does not handle inter-cooow contention, and uses the total bytes sent so far (d) to move cooows across multiple priority queues.
Once the scheduling order of the cooows is determined, we need to determine the rates for the individual ows at each port.
First, since we want to quickly nish the pilot ow, at any port that has pilot ows, PPPPPP assigns the entire port bandwidth to the pilot ows.
For the remaining ports, as discussed in §4.3, across multiple queues, PPPPPP assigns weighted shares of the port bandwidth, by assigning them varying numbers of scheduling intervals according to the weights assigned to each priority queues.Second, at each scheduling interval, PPPPPP assigns rates for the ows of the cooow in the head of the FIFO queue as follows.
It assigns equal rates at all the ports containing its ows as there is no beneet in speeding-up its ows at certain ports when its CCT depends on the slowest ow.
At each port, we could use max-min fairness to schedule the individual ows of the cooow (to diierent receivers), and then assign the rate of the slowest ow to all the ows in the cooow.
Afterwards, the port-allocated bandwidths are incremented accordingly at the coordinator, which then allocates rates for the next cooow in the same FIFO queue, and so on.Though the above max-min approach has the advantage of minimizing bandwidth wastage, it slows down the coordinator which has to iterate over many ows.
In our experiments, we used a simple scheme where we assign the entire bandwidth at the sender and receiver ports to one ow of the cooow at the head of the FIFO queue at a time.
We found that this simple scheme has very marginal eeect on CCTs but makes the rate assignment process considerably faster.
Thin cooow bypass Recall that, in PPPPPP, when a new cooow arrives, PPPPPP only schedules its pilot ows.
All other ows of that cooow are delayed until the pilot ows nish and cooow size is known.
However, such a design choice can inadvertently lead to higher CCTs for cooows, particularly for thin cooows, e.g., a two--ow cooow would end up serializing scheduling its two ows, one for the piloting purpose.To avoid CCT degradations for thin cooows, we schedule all ows of a cooow if its width is under a threshold (set to 7 in PPPPPP; §8.6 provides sensitivity analysis for thresholds).
Failure tolerance and recovery Cluster dynamics such as stragglers or node failure can delay some of the ows of a cooow or start new ows, increasing their CCT.
The PPPPPP design automatically self-adjusts to speed up cooows that are aaected by cluster dynamics using the following mechanisms: (1) It adjusts the cooow size as the amount of data left by the cooow, which is essentially the diierence between the size calculated using pilot ows and amount of data already sent.
(2) It calculates contention only on the ports that have unnnished ows.Work Conservation By default, PPPPPP schedules non-pilot ows of a cooow only after all its pilot ows are over.
This can lead to some ports being idle where the non-pilot ows are waiting for the pilot ows to nish.
In such cases, PPPPPP schedules non-pilot ows of cooows which are still in the sampling phase at those ports.
In work conservation, the cooows are scheduled in the FIFO order of arrival of cooows.
Compared to learning cooow sizes using priority queues (PQbased) [19,30], learning cooow sizes by sampling PPPPPP not only reduces the learning overhead as discussed in §3.1 and shown in §8.2, but also signiicantly reduces the amount of interactions between the coordinator and local agents and thus makes the coordinator highly scalable, as summarized in Table 1.
First, PQ-based learning requires much more frequent update from local agents.
PQ-based learning estimates cooow sizes by incrementally moving cooows across priority queues according to the data sent by them so far.
As such, the scheduler needs frequent updates (every δ ms) of data sent per cooow from the local agents.
In contrast, PPPPPP directly estimates a cooow's size upon the completion of all its pilot ows.
The only updates PPPPPP needs from the local agents are about the ow completion which is needed for updating contentions and removing ows from active consideration.
.
Second, PQ-based learning results in much more frequent rate allocation.
In sampling-based approach, since cooow sizes are estimated only once, cooows are re-ordered only upon cooow completion or arrival events or in the case of contention based policies only when contention changes, which is triggered by completion of all the ows of a cooow at a port.
In contrast, in PQ-based learning, at every δ interval, cooow data sent are updated and cooow priority may get updated, which will trigger new rate assignment.Our scalability experiments in §9.3 connrms that PPPPPP achieves much higher scalability than Aalo.
We implemented both PPPPPP and Aalo scheduling policies in the same framework consisting of the global coordinator and local agents (Fig. 2), in 5.2 KLoC in C++.
Coordinator: The coordinator schedules the cooows based on the operations received from the registering framework.
The key implementation challenge for the coordinator is that it needs to be fast in computing and updating the schedules.
The PPPPPP coordinator is optimized for speed using a variety of techniques including pipelining, process aanity, and concurrency whenever possible.Local agents: The local agents update the global coordinator only upon completion of a ow, along with its length if it is a pilot ow.
Local agents schedule the cooows based on the last schedule received from the coordinator.
They comply to the last schedule until a new schedule is received.
To intercept the packets from the ows, local agents require the compute framework to replace datasend(), datarecv() APIs with the corresponding PPPPPP APIs, which incurs very small overhead.Cooow operations: The global coordinator runs independently from, and is not coupled to, any compute framework, which makes it general enough to be used with any framework.
It provides RESTful APIs to the frameworks for cooow operations: (a) register() for registering a new cooow when it enters, (b) deregister() for removing a cooow when it exits, and (c) update() for updating cooow status whenever there is a change in the cooow structure, e.g., during task migration and restarts after node failures.
We evaluated PPPPPP using a 150-node and a 900-node testbed cluster in Azure and using large scale simulations by utilizing a publicly available Hive/MapReduce trace collected from a 3000-machine, 150-rack Facebook production cluster [4] and multiple derived traces with varying degrees of ow size skew to measure PPPPPP's robustness to skew.
• Facebook (FB) trace: The trace contains 150 ports and 526 (> 7 × 10 5 ows) cooows, that are extracted from Hive/MapReduce jobs from a Facebook production cluster.
Each cooow consists of pair-wise ows between a set of senders and a set of receivers.Due to the lack of other publicly available cooow trace 1 , we derived three additional traces using the original Facebook trace in order to more thoroughly evaluate PPPPPP under varying cooow size skew:• Low-skew--ltered: Starting with the FB trace, we ltered out cooows that have skew (max ow length/min ow length) less than a constant k.
We generated ve traces in this class with k = 1, 2, 3, 4, 5.
The ltered traces have 142, 100, 65, 51 and 43 cooows, respectively.
• Mantri-like: Starting with the FB trace, we adjusted the sizes of the ows sent by the mappers, keeping the total reducer data the same as given in the original trace, to match the skew of a large Microsoft production cluster trace as described in Mantri [12].
In particular, the sizes are adjusted so that the coeecients of variation across mapper data are about 0.34 in the 50 th percentile case and 3.1 in the 90 th percentile case.
This trace has the same numbers of cooows and ports as the FB trace.
• Wide-cooows-only: We ltered out all the cooows in the FB trace with the total number of ows ≤ 7, the default thin cooow bypass threshold (thinLimit) in PPPP .
The ltered trace has 269 cooows spreading over 150 ports.The primary performance metrics used in the evaluation are CCT or CCT speedup, deened as the ratio of a CCT under other baseline algorithms and under PPPPPP, piloting overhead, and cooow size estimation accuracy.The highlights of our evaluation results are:(1) PPPPPP signiicantly improves the CCTs.
In simulation using the FB trace, the average CCT is improved by 1.51× over the prior art, Aalo.
Individual CCT speedups are 1.78× in the median case (P90 = 9.58×).
For the Mantri-like trace, the average CCT is improved by 1.36× and individual CCT speedups are 1.75× in the median case (P90 = 12.0×).
(2) The CCT improvement mainly stems from the reduction in the learning overhead (in terms of latency and amount of data sent) in determining the right queue for the cooows.
Compared to Aalo, median reduction in the absolute latency in nding the right queue for cooows in PPPPPP is 19.0×, and in absolute amount of data sent is 20.0× ( §8.2).
(3) PPPPPP improvements are consistent when varying the skew among the ow sizes in a cooow ( §8.5).
(4) PPPPPP improvements are consistent when varying its parameters ( §8.6).
(5) The PPPPPP coordinator is much more scalable than that of Aalo ( §9.3).
We present detailed simulation results in this section, and the testbed evaluation of our prototype in §9.
Experimental setup: Our simulated cluster uses the same number of nodes (sending and receiving network ports) as in the trace.
As in [19], we assume full bisection bandwidth is available, and congestion can happen only at network ports.The default parameters for Aalo and PPPPPP in the experiments are: starting queue threshold (Q hi 0 ) is 10MB, exponential threshold growth factor (E) is 10, number of queues (K) is set to 10, the weights assigned to individual priority queues decrease exponentially by a factor of 10, and the new schedule calculation interval δ is set to 8ms for the 150-node cluster 2 , the default suggested in its publicly available simulator [19].
In PPPPPP, a new schedule is calculated on demand, upon arrival of a new cooow, completion of a cooow, or completion of all pilot ows of a cooow.
Finally, in PPPPPP the threshold for thinLimit (T) is set to 7, the number of pilot ows assigned to wide cooows are max(1, 0.05 · S), where S is the number of senders, and the default inter-cooow scheduling policy in PPPPPP is Least length-weighted total-port contention.
We start by evaluating the impact of diierent policies in choosing the pilot ows for a cooow in PPPPPP.
Aalo and average error in size estimation normalized to the actual cooow size, when varying the pilot ow selection policy while keeping other parameters as the default in PPPPPP, using the FB trace.Unsurprisingly, the estimation accuracy increases when increasing the number of pilot ows across the three selection schemes: constant, fraction of senders, and fraction of total ows.
However, as the number of pilot ows increases (over the range of parameter choices), the CCT speedup (P50 and P90 of individual cooow CCT speedups) decreases.
This is because the beneet from size estimation accuracy improvement from using additional pilot ows does not ooset the added overhead from completing the additional pilot ows and the delay they incur to other cooows.We nd sampling 5% of the number of senders per cooow strikes a good trade-oo between piloting overhead and size estimation accuracy leading to the best CCT reduction.
We thus set it (0.05 · S) as the default pilot ow selection policy.
Next, using the default pilot selection policy, we evaluate PPPPPP's eeectiveness in estimating cooow sizes by sampling pilot ows.
Fig. 3 shows a scatter plot of the actual cooow size vs. estimated size from running PPPPPP under the default settings.
We observe that PPPPPP cooow's size estimation is highly accurate except for a few outliers.
Overall, the average and standard deviation of relative estimation error are 0.06 and 0.15, respectively, and for the top 99% and 95% cooows (in terms of estimation accuracy), the average (standard deviation) of relative error are only 0.05 (0.12) and 0.03 (0.07) respectively.
Interestingly, a few cooows experience large estimation errors, and we found they all have very high skew in their ow lengths; the mean standard deviation in ow lengths, normalized by the average length, of the bottom 1% (in terms of accuracy) ranges between 4.6 and 6.8.
Fig. 1 shows the cost of estimating the correct queue for each cooow in PPPPPP and Aalo, measured as the time in learning the cooow size as a fraction of the cooow's CCT in PPPPPP and Aalo.
We see that under PPPPPP, about 63% of the cooows spent less than 1% of their CCT in the learning phase, while under Aalo, 63% cooows reached the correct priority queue after spending up to 22% of their CCT moving across other queues.
Compared to Aalo, PPPPPP in the median case sends 20× less data in determining the right queue and reduces the latency in determining the right queue by 19×.
PPPPPP diiers from Aalo in two ways: online size estimation and inter--ow scheduling policy.
Here, we evaluate the eeectiveness of the four inter-cooow scheduling policies of PPPPPP discussed in §4.4, keeping the remaining parameters as the default.
Such evaluation allows us to decouple the contribution of sampling-based learning from the eeect of scheduling policy diierence.
Table 3 shows the CCT improvement of PPPPPP under the four inter--ow scheduling policies over Aalo.
We make the following observations.
First, PPPPPP with the purely sized-based policy, Smallest job rst (A), which uses the same inter-queue and intraqueue scheduling policy as Aalo and only diiers from Aalo in cooow size estimation, reduces the average CCT (P50) of Aalo by 1.40x (1.48x).
In contrast, the default PPPPPP uses Least lengthweighted total-port contention (D), which uses the sum of size-weighted port contention to assign cooows to priority queues, and slightly outperforms the size-based policy A; it reduces the average CCT (P50) of Aalo by 1.51x (1.78x).
This is because it captures the diversity of contention at diierent ports, which happens often in real distributed settings, and at the same time accounts for the cooow size by using lengthweighted sum of the port-wise contention.
The above results for policy A and policy D indicate that the primary improvement in PPPPPP comes from its sampling-based cooow size estimation scheme.Shortest remaining time rst (B) performs similarly as smallest job rst.
This is because the preemptive nature of SRTF will kick in only on arrival of new cooows.
Also, although SRTF is advantageous for small cooows, since PPPPPP already schedules thin cooows at high priority, many thin and thus small cooows are anyways being scheduled at high priority under both policies A and B, and as a result they perform similarly.Finally, Least contention rst (C) performs poorly.
This is because contention for a cooow is deened as the unique number of other cooows that share ports, and as a result such a policy completely ignores the size (length) of the cooows.
We now compare the CCT speedups of PPPPPP against 5 wellknown cooow scheduling policies: (1) Aalo, (2) Aalo-Oracle, which is an oracle version of Aalo where the scheduler knows the nal queue of a cooow upon its arrival time and directly starts the cooow from that queue, (3) SEBF in Varys [21] which assumes the knowledge of cooow sizes apriori and uses the Shortest EEective Bottleneck First policy, where the cooow whose slowest ow will nish rst is scheduled rst.
(4) FIFO, which is a single queue FIFO based cooow scheduler, and (5) FAIR, which uses per--ow fair sharing.
We do not include Saath [30] in the comparison as it does not provide the same liveliness guarantees as PPPPPP which as discussed in §4.3 can obscure the comparison result.
All experiments use the default parameters discussed in the setup, including K, E, S, unless otherwise stated.
The results are shown in Fig. 4(a).
We make the following observations.
First, we compare CCT under PPPPPP against under AaloOracle, where Aalo-Oracle starts all cooows at the correct priority queues (i.e., no learning overhead).
PPPPPP improves the average CCT by 1.18× and P50 CCT by 1.40×, respectively.
Since Aalo-Oracle pays no overhead for cooow size estimation, its worse performance suggests that using length-weighted total-port contention in assigning cooows to the priority queues in PPPPPP outperforms Aalo's sizebased, contention-oblivious policy in assigning cooows to the queues.Second, PPPPPP improves the average CCT over Aalo by 1.51× (median) and P50 by 1.78.
The signiicant additional improvement on top of the gain over Aalo-Oracle comes from fast and accurate estimation of the right queues for the cooows (Fig. 1).
Third, PPPPPP, which requires no cooow size knowledge a priori, achieves comparable performance as SEBF [21]; it reduces the average CCT by 1.16×.
Again this is because its total-port contention policy outperforms the contentionoblivious SEBF.Finally, PPPPPP signiicantly outperforms the single-queue FIFO-based cooow scheduler, with a median (P90) CCT speedup of 3.00 (77.96)× and average CCT speedup of 3.16×, and the un-coordinated ow-level fair-share scheduler, with a median (P90) CCT speedup of 70.82× (1947×) and average CCT speedup of 5.66×.
To gain insight into how diierent cooows are aaected by PPPPPP over Aalo, we group the cooows in the trace into Figure 4: CCT speedup using PPPPPP compared to using other cooow schedulers on diierent traces.
In Fig. 4(c), the x-axis denotes the minimum skew in the 5 Low-skew--ltered traces.Bin-1 Bin-2 Bin-3 Bin-4 Table 4.
four bins deened in Table 4, and show in Fig. 5 the CCT speedups for each bin.
We see that PPPPPP improves CCT for all cooows in bin 1 and 3 and for large fraction in bin-4.
Most of the underperforming cooows fall in bin-2.
Cooows in bin-2 have width > 7 and size < 100MB, i.e., the ows are short but wide.
Because the width exceeds the thinLimit, PPPPPP schedules the pilot ows to estimate the cooow size rst ( §4).
Thus, although the remaining ows are short, they get delayed until the completion of the pilot ows, which results in CCT increase.
Finally, since thin cooows beneet from PPPPPP's scheme of bypassing probing for thin cooows, we also compare PPPPPP with other schemes using the Wide-cooows-only trace which consists of all cooows wider than the default thinLimit (7) in PPPPPP.
Fig. 4(b) shows that PPPPPP continues to perform well, reducing the average CCT by 1.54×, 1.15×, and 1.12× over Aalo, Aalo-Oracle, and SEBF, respectively.
Next, we evaluate PPPPPP's robustness to ow size skew by comparing it against Aalo using traces with varying degrees of skew.
First, we evaluate PPPPPP using the Mantri-like trace.
Fig. 4(d) shows that PPPPPP consistently outperforms prior-art cooow schedulers.
In particular, PPPPPP reduces the average CCT by 1.36x compared to Aalo.
Second, we evaluate PPPPPP using the Low-skew--ltered traces which have low skew cooows ltered out.
Fig. 4(c) shows that PPPPPP performs better than Aalo even with highly skewed traces and reduces the average CCT by 1.45×, 1.44×, 1.44×, 1.40× and 1.38× for the ve Low-skew--ltered traces containing cooows with skew of at least 1, 2, 3, 4 and 5, respectively.
Compared to Aalo, PPPPPP has only two additional paramaters: thinLimit and ow sampling rate.
We already discussed the choice of sampling rate in §8.1.
Below, we evaluate the sensitivity of PPPPPP to thinLimit and other design parameters common to Aalo by varying one parameter at a time while keeping the rest as the default.Thin cooow bypassing limit (T ) In this experiment, we vary thinLimit (T) in PPPPPP for bypassing cooows from the probing phase.
The result in Fig. 7(a) shows that the average CCT remains almost the same as T increases.
This is because the average CCT is dominated by wide and large cooows, which are not aaected by thinLimit.
However, the P50 speedup increases till T = 7 and tapers oo after T = 7.
The reason for the CCT improvement until T = 7 is that all ows of thin cooows (with width ≤ 7) are scheduled immediately upon arrival which improves their CCT, and the number of thin cooows is signiicant.Start queue threshold (Q hi 0 ) We next vary the threshold for the rst priority queue from 2 MB to 64 MB.
Fig. 7(b) shows the average CCT of PPPPPP over Aalo.
Overall, PPPPPP is not very sensitive to the threshold of rst priority queue and the CCT speedup over Aalo is within 8% of the default PPPPPP (10 MB).
The speedup appears to oscillate with a periodicity of 5x to 10x.
For example, the speedups for 2 MB and 64 MB are close to that of the default (10 MB), while for 4 MB and 32 MB are lower.
This can be explained by the impact of the rst queue threshold on job segregation; with the default queue threshold growth factor of 10, every time the rst queue threshold changes by close to 10x, the distribution of jobs across the queues become similar.Multiplication factor (E) In this experiment, we vary the queue threshold growth factor from 2 to 64.
Recall that the queue thresholds are computed as Q hi q = Q hi q−1 · E. Thus, as E grows, the number of queues decreases.
As shown in Fig. 7(c), smaller queue threshold multiplication factor which leads to more queues performs better because of ne-grained priority segregation.
Next, we deployed PPPPPP in a 150-machine Azure cluster and a 900-machine cluster to evaluate its performance and scalability.Testbed setup: We rerun the FB trace on a Spark-like framework on a 150-node cluster in Microsoft Azure [5].
The coordinator runs on a Standard DS15 v2 server with 20-core 2.4 GHz Intel Xeon E5-2673 v3 (Haswell) processor and 140GB memory.
The local agents run on D2v2 with the same processor as the coordinator with 2-core and 7GB memory.
The machines on which local agents run have 1 Gbps network bandwidth.
Similarly as in simulations, our testbed evaluation keeps the same ow lengths and ow ports in trace replay.
All the experiments use default parameters K, E, S and the default pilot ow selection policy.
In this experiment, we measure CCT improvements of PPPP compared to Aalo.
Fig. 6 shows the CDF of the CCT speedup of individual cooows under PPPPPP compared to under Aalo.
The average CCT improvement is 1.50× which is similar to the results in the simulation experiments.
We also observe 1.63× P50 speedup and 8.00× P90 speedup.We also evaluated PPPPPP using the Wide-cooow-only trace.
Table 5 shows that PPPPPP achieves 1.52× improvement in average CCT over Aalo, similar to that using the full FB trace.
This is because the improvement in average CCT is dominated by large cooows, PPPPPP is speeding up large cooows, and the Wide-cooow-only trace consists of mostly large cooows.
Next, we evaluate how the improvement in CCT aaects the job completion time (JCT).
In data clusters, diierent jobs spend diierent fractions of their total job time in data shuue.
In this experiment, we used 526 jobs, each corresponding to one cooow in the FB trace.
The fraction of time that the jobs spent in the shuue phase follows the same distribution used in Aalo [19], i.e., 61% jobs spent less than 25% of their total time in shuue, 13% jobs spent 25-49%, another 14% jobs spent 50-74%, and the remaining spent over 75% of their total time in shuue.
Fig. 6 shows the CDF of individual speedups in JCT.
Across all jobs, PPPPPP reduces the job completion time by 1.16× in the median case and 7.87× in the 90 th percentile.
This shows that improved CCT translates into better job completion time.
As expected, the improvement in job completion time is smaller than the improvement in CCT because job completion time depends on the time spent in both compute and shuue (communication) stages, and PPPPPP improves only the communication stage.
Finally, we evaluate the scalability of PPPPPP by comparing its performance with Aalo on a 900-node cluster.
To drive the evaluation, we derive a 900-port trace by replicating the FB trace 6 times across ports, i.e., we replicated each job 6 times, keeping the arrival time for each copy the same but assigning sending and receiving ports in increments of 150 (the cluster size for the original trace).
We also increased the scheduling interval δ by 6 times to δ = 6×δ.
PPPPPP achieved 2.72× (9.78×) speedup in average (P90) CCT over Aalo.
The higher speedup compared to the 150-node runs (1.50×) comes from higher scalability of PPPPPP.
In 900-node runs, Aalo was not able to nish receiving updates, calculating new rates and updating local agents of new rates within δ in 37% of the intervals, whereas PPPPPP only missed the deadline in 10% of the intervals.
For 150-node runs these values are 16% for Aalo and 1% for PPPPPP.
The 21% increase in missed scheduling intervals in 900-node runs in Aalo resulted in local agents executing more frequently with outdated rates.
As a result, PPPPPP achieved even higher speedup in 900-node runs.As discussed in §5, Aalo's poorer coordinator scalability comes from more frequent updates from local agents and more frequent rate allocation, which result in longer coordinator CPU time in each scheduling interval.
Table 6 shows the average coordinator CPU usage per interval and its breakdown.
We see that (1) on average PPPPPP spends much less time than Aalo in receiving updates from local agents, because PPPPPP does not need updates from local agents at every interval -on average in every scheduling interval PPPPPP receives updates from 49 local agents whereas Aalo receives from 429 local agents, and (2) on average PPPPPP spends much less time calculating new rates and send new rates.
This is because rate calculation in PPPPPP is triggered by events and PPPPPP did not have to ush rates in 66% of the intervals.
Cooow scheduling: In this paper, we have shown PPPPPP outperforms prior-art non-clairvoyant cooow scheduler Aalo from more eecient learning of cooow sizes online.
Saath [30] and Graviton [29] also learn cooow sizes online using priority queues and hence suuers the same ineeciency as Aalo.
Graviton [29] uses the number of ports a cooow is present at, as an additional indicator of its size.
In [19], Aalo was shown to outperform previous non-clairvoyant cooow schedulers Baraat [24] by using global coordination, and Orchestra [20] by avoiding head-of-line blocking.Clairvoyant cooow schedulers such as Varys [21] and Sincronia [7] assume prior knowledge of cooows upon arrival.
Varys runs a shortest-eeective-bottleneck--rst heuristic for inter-cooow scheduling and performs per--ow rate allocation at the coordinator.
Sincronia improves the scalability of the centralized coordinator of Varys by only calculating the cooow ordering at the coordinator (by solving an LP) and oooading ow rate allocation to individual local agents.
Sincronia is orthogonal to PPPPPP; once cooow sizes are learned through sampling, ideas from Sincronia can be adopted in PPPPPP to order cooows and oooad rate allocation to local ports.
CODA [44] tackles an orthogonal problem of identifying ows of individual cooows online.However, recent studies [19,40] have shown various reasons why it is not very plausible to learn ow sizes from applications beforehand.
For example, many applications stream data as soon as data are generated and thus the application does not know the ow sizes until ow completion, and learning ow sizes from applications requires changing either the network stack or the applications.Flow scheduling: There exist a rich body of prior work on ow scheduling.
EEorts to minimize ow completion time (FCT), both with prior information (e.g., PDQ [26], pFabric [9]) and without prior information (e.g., Fastpass [35], PIAS [13], [14]), fall short in minimizing CCTs which depend on the completion of the last ow [21].
Similarly, Hedera [8] and MicroTE [15] schedule the ows with the goal of reducing the overall FCT, which again is diierent from reducing the overall CCT of cooows.Speculative scheduling Recent works [16,33] use the idea of online requirement estimation for scheduling in datacenter.
In [31], recurring big data analytics jobs are scheduled using their history.Job scheduling: There have been much work on scheduling in analytic systems and storage at scale by improving speculative tasks [11,12,43], improving locality [10,41], and end-point exibility [17,38].
The cooow abstraction is complimentary to these work, and can beneet from them.
Combining cooow with these approaches remains a future work.Scheduling in parallel processors: Cooow scheduling by exploiting the spatial dimension bears similarity to scheduling processes on parallel processors and multi-cores, where many variations of FIFO [37], FIFO with backklling [32] and gang scheduling [25] have been proposed.
State-of-the-art online cooow schedulers approximate the classic SJF by implicitly learning cooow sizes and pay a high penalty for large cooows.
We propose the novel idea of sampling in the spatial dimension of cooows to explicitly and eeciently learn cooow sizes online to enable eecient online SJF scheduling.
Our extensive simulation and testbed experiments show the new design ooers signiicant performance improvement over prior art.
Further, the sampling-inspatial-dimension technique can be generalized to other distributed scheduling problems such as cluster job scheduling.
We have made our simulator publicly available at https: //github.com/coflowPhilae/simulator [6].
