Advertising (ad) revenue plays a vital role in supporting free websites.
When the revenue dips or increases sharply, ad system operators must find and fix the root-cause if actionable, for example, by optimizing infrastructure performance.
Such revenue debugging is analogous to diagnosis and root-cause analysis in the systems literature but is more general.
Failure of infrastructure elements is only one potential cause; a host of other dimensions (e.g., advertiser, device type) can be sources of potential causes.
Further, the problem is complicated by derived measures such as costs-per-click that are also tracked along with revenue.
Our paper takes the first systematic look at revenue debugging.
Using the concepts of explanatory power, succinctness, and surprise, we propose a new multi-dimensional root-cause algorithm for fundamental and derived measures of ad systems to identify the dimension mostly likely to blame.
Further, we implement the attribution algorithm and a visualization interface in a tool called the Adtributor to help troubleshooters quickly identify potential causes.
Based on several case studies on a very large ad system and extensive evaluation, we show that the Adtributor has an accuracy of over 95% and helps cut down troubleshooting time by an order of magnitude.
Many free websites are supported today by revenue generated through advertisements (ads).
Website ads can be of two types, namely, search and display.
In the case of a search ad, an end user goes to a publisher website such as bing.com and enters a query phrase.
The response to the query is a search results page that may contain one or more ads.
If the user clicks on one of these ads, the publisher earns revenue.
In the case of a display ad, an end user may visit a publisher website, such as cnn.com, where she might see ads at the top or sides of the page.
The display of these ads earns revenue for the publisher.Ad systems facilitate generation and accounting of millions of such search and display ads every day.
Apart from users and publishers noted above, there are two other key constituents who interact with the ad system.
The ads shown to the user are the result of an ad auction between various advertisers who bid to compete to have their ad displayed to the user.
Also in the midst are various fraud operators [8] that try to usurp a fraction of the advertising revenue.Ad systems manage the interaction between users, publishers, advertisers and fraud operators.
Ad systems implement various ad-related algorithms that run the real-time ad auctions between the advertisers, return the winning ads to the publisher, monitor the user clicks, detect and remove potential fraudulent activity, compute the revenue from each displayed or clicked ad, charge the advertiser the appropriate bid amount, and pay the publishers.
At the core of the ad system is a large-scale distributed system consisting of thousands of servers distributed across several data centers that execute the ad algorithms and manage the serving and accounting of ads.The focus of this paper is on debugging ad systems.
Typically, an ad system monitor issues an alert whenever a measure of interest is identified as anomalous (e.g., revenue or number of searches is down sharply).
1 Our goal is to automatically identify the potential root cause of this anomaly.
We term our approach revenue debugging, even though it is applicable to several measures of interest to ad system operators, to acknowledge the prominence of the revenue metric.
In this paper, we describe a new revenue debugging algorithm that analyses the large amount of data logged by the ad system and narrows down the scope of potential root-cause of an anomaly to a sub-component of the ad system for further investigation by a human troubleshooter.Root-cause identification and diagnostics is an ageold problem in systems.
Various performance rootcausing tools have been proposed in the past [1,2,3,10,14,15].
But all these solutions have focused on performance/failure debugging.
Here, we address a similar yet more general problem: diagnostics in ad systems.
While performance/failure of infrastructure systems components can be one possible root-cause for an anomalous measure, there may be various other rootcauses that depend on other components that interact with the ad system.
Consider the following examples.1.
Papal Election: We noticed that the papal election caused a revenue drop because many searches were made for non-monetizable query terms such as pope or papal election, that advertisers typically do not bid for.
The total number of ads shown dropped which resulted in an anomalous revenue drop.
While identifying the rootcause as the papal election is not actionable, root-cause identification is still important as it eliminates an actionable root-cause such as the example below.
2.
Browser Ad Failure: We found a revenue drop was caused by a manual error in updating a configuration file that had the side-effect of not showing ads on certain browser versions.
In this case, quick identification helped rectify the configuration error, thereby restoring advertising revenue.
A more extensive set of examples is depicted in Table 1 and discussed in Section 2.1.
The first challenge in ad systems debugging is sheer scale.
There are hundreds of millions of searches and clicks every day; performing diagnostics at the level of a search or a click is not scalable (imagine running Magpie [3] or tracking a string of system calls through hundreds of system components for every click).
Thus, for scalability reasons, ad system debugging operates over aggregates of various measures.
These measures are typically counters aggregated over certain time intervals (e.g., revenue generated over the last 1 hour).
Root-cause identification can only be triggered by anomalous behavior of these aggregate counters.A second distinguishing characteristic of ad systems as compared to typical systems trouble shooting is the existence of multiple dimensions, and the need to first isolate the dimension that explains the anomaly.
Measures such as revenue can be broken down or projected along different dimensions such as advertiser, browser, or data center.
For instance, in Example 2, if revenue were projected along the browser dimension, one could observe that some browser versions were not generating their "typical" share of revenue.
However, if the same revenue were sliced by the advertiser dimension, perhaps the distribution of revenue would not have changed significantly.Typical systems root-causing algorithms such as SCORE [11] use succinctness (Occam's razor) and explanatory power (does the root-cause explain the change?)
as their main parameters for optimization and do not have to account for multiple dimensions.
To isolate anomalous dimensions, we introduce the notion of surprise, captured by quantifying the change in distribution of measure values across each dimension.
For instance, in Example 2, change in distribution of revenue along the browser dimension is more surprising than the change in distribution of revenue along the advertiser dimension.
Thus, our first contribution in the paper is the root-causing algorithm described in Section 3 that uses surprise in addition to succinctness and explanatory power to identify root-causes in ad systems.A third unique characteristic of ad systems is the prevalence of derived measures.
Consider two fundamental measures: revenue per hour and number of clicks per hour.
From these two measures, one can define a derived measure called cost-per-click that is simply revenue divided by number of clicks.
Ad system operators monitor and track many such derived measures that are functions of various fundamental measures (see Figure 1).
For example, the change in number of clicks and change in revenue may be small by themselves and not anomalous (e.g., less than 10%).
However, correlated changes (e.g., revenue drops and simultaneously clicks increase, each by say 10%), are anomalous and is captured by the derived cost-per-click measure (20% change).
As we discuss in Section 4, attributing a root-cause to derived measures is challenging.
To address this, we propose a novel partial-derivative inspired attribution solution for derived measures, our second contribution of the paper.The outcome of our root-cause identification algorithm is a set of candidates that potentially explain an anomaly.
However, this is only the first step in the diagnosis process where a troubleshooter may, if appropriate, take actions to fix the issue.
To help the troubleshooter quickly identify potential root-cause candidates, we have implemented our root-cause identification algorithm and a graphical visualizer in a tool called the Adtributor, our third contribution of the paper.
Through experiences from a pilot deployment in a production system, we have refined the visualization interface and data representation techniques in Adtributor to further reduce turnaround time for troubleshooters.Finally, we perform extensive evaluation of our rootcausing algorithm.
First, we tabulate and discuss a representative set of case studies that highlight the value of our root-causing tool.
Second, we evaluate our algorithm on 128 anomaly alerts over 2 weeks of real ad system data and find that our algorithm achieves an accuracy of over 95%.
In fact, Adtributor even found root-causes for a few anomalies that were missed by the manual troubleshooters.
Further, the tool also speeds up the troubleshooting process by an order of magnitude.
In this section, after providing a system overview, we show examples of real problems and their root-causes.
Next, we state the problem more precisely and motivate our solution.
Figure 1 shows a simplified representation of an ad system, and the entities such as users, fraud operators, publishers and advertisers that directly interact with the ad system.
The ad system itself has various subcomponents, some of which we show.
While the logging infrastructure does track each Figure 1: A simplified representation of an ad system, and the measures it monitors.search request or ad-click, the sheer scale makes it hard to track down a problem at the individual request level.
Instead, the system monitors a set of aggregate measures, as shown in Figure 1.
From the raw logs, it first calculates, for each time interval, total searches received, total ads shown, total ad-clicks received, and total revenue from these clicks.
These measures are all additive, and can be sliced along different dimensions.
For instance, the total revenue is the sum of the revenue made from each advertiser using the system.
The total revenue is also the sum of revenue received from different geographical regions where the ad system is active.
We term such additive measures fundamental measures.Additionally, the system also monitors a set of non-additive derived measures, which are functions of fundamental measures, such as ads-per-search (ads/searches), clicks-per-ad (clicks/ads), cost-perclick (revenue/clicks), and revenue-per-search (revenue/searches).
An anomalous rise or drop in any of these measures is an indication of a problem.
Therefore, a diagnostic engine needs to first detect an anomaly, and then perform root-cause analysis.
In this paper, we focus on the latter aspect of root-causing, while relying on well-known ARMA model-based methods [4] for anomaly detection.
The anomaly detector generates a model-based prediction of measure values based on 8 weeks of historical data, taking into account normal time-of-day and day-ofweek fluctuations.
It then compares the actual value with the forecasted value -when the actual value of a measure is significantly different from the forecasted value, it generates an anomaly alert.
The threshold difference above which we generate an alert is measured in terms of a percentage deviation from the expected value.
In the current system, troubleshooters manually set this value based on experience.
For each alert, our objective is to attribute the anomaly in a measure to a dimension and its corresponding elements.
We define these terms next.
Dimension: A dimension is an axis along which a measure can be projected.
For instance, we can project revenue along the axis of advertisers, and determine how much revenue comes in from each advertiser.
The dimension in this case is "Advertiser".
Derived measures can be similarly projected across dimensions.
Some other dimensions are "Publisher", "Data Center", and "User Location".
Typically, an ad system deals with dozens of such dimensions.
Note that a dollar of revenue could be added to Advertiser 1 in one dimension, and Publisher 3 in a second dimension.
Element: Every dimension has a domain of values called elements.
For instance, the "Advertiser" domain can have the following elements: {Geico, Microsoft, Toyota, Frito-Lay, ...}.
The Publisher dimension may have elements: {Bing, Amazon, NetFlix, ...}.
Table 1 provides a number of problem examples we encountered, both actionable and not actionable, that need to be detected and root-caused to the appropriate dimensions and elements.
Column 1 shows that problems can happen at various levels.
Column 3 shows the anomalous measure.
Column 4 shows the output of the root-cause analysis, the focus of this paper.Note that Column 4 is only the first step towards rootcausing, but it is essential as it gives the troubleshooter the best indication of where the problem actually lies.
Other post-processing techniques (correlation engines, NLP techniques, manual investigation) use the output of the multi-dimensional analysis to perform a deeper dive into the issue to arrive at the final root-cause, shown in Column 5, but this aspect of root-causing is outside the scope of this paper.
For instance, in row 9, while the multi-dimensional analysis did narrow down the problem to a few query strings, an administrator had to semantically interpret the strings to determine that the papal election was the cause.
The multi-dimensional analysis problem of revenue debugging is to find the dimension and its elements that best explain an anomalous rise or fall in a measure.
In this context, we need to define what constitutes the "best explanation" for an anomaly.Consider the following example.
The revenue of an ad system was forecasted to be $100 at a given time.
In real- Publisher: P1 One publisher launched a new UI with more ads shown on the top of the page than on the side.
Users tend to click more on ads at the top of the page, and so this publisher reported more ad-clicks.
See Section 6.
8.
Revenue dropped Publisher: P2, P3 Publishers P2 and P3 started blocking ads returned by the ad system to make for a cleaner UI.
Their revenue dropped.
Ads-per-search dropped Query string: "pope", "papal election"During the papal election, users searched for "Pope", "Papal election", etc. which are non-monetizable searches.
These searches showed no ads, consequently the derived measure ads-per-search dropped.
10.
Revenue dropped User Location: New Orleans A hurricane in New Orleans caused fewer searches from the affected geographical areas.
Fraud 11.
Searches increased User-agent String A large number of searches used an identical user-agent string.
This was traced to a bot that was spoofing search requests and blindly replicating the user-agent string.
See Section 6.
Table 4: Revenue by Device Type ity, the actual revenue was only $50.
An alert is triggered on the revenue measure, which brings a troubleshooters attention to the problem.To find the root-cause when such problems occur, the ad system continuously tracks the revenue generated across a host of dimensions.
For this scenario, consider three such dimensions: Data center (DC), Advertiser (AD), and Device type (DT).
Tables 2, 3, 4 show the projection of revenue values along these dimensions, and the values attributed to the individual elements.We now explain the semantics of these attributions.
When the ad system receives a search query, it routes the query to a data center that in turn serves a number of ads in response.
The revenue attributed to a data center is the total revenue received from clicks on ads that this data center serves.
Each ad has an associated advertiser.
When a user clicks an ad, the system charges the advertiser a pre-determined sum of money.
The revenue attributed to the advertiser is the total cost of all such clicks made on the advertiser's ads.
Users make search queries using a host of devices, which could be phones, tablets, or PCs.
The revenue attributed to a device type is the sum total of all revenue that the ad system obtains from ad-clicks from that specific device-type.
The question that we seek to answer is: how do we pinpoint the revenue drop to the right dimensions and their elements?
We restate the problem as follows:"Find a Boolean expression, in terms of dimensions and their elements, such that the revenue drop attributed to the expression best explains the total drop in revenue.
"While we examine how to determine "best" shortly, consider the following expressions that could explain the $50 revenue drop:Revenue Drop(DC == X) = $47 (1) Revenue Drop(AD == A1 ∨ AD == A3 ∨ AD == A4) = $51 (2) Revenue Drop(DT == M obile ∨ DT == T ablet) = $49(3)For example, equation 2 states that the sum of the differences between the forecasted and actual revenues for rows 1, 3, and 4 of the advertiser table is $51, which is very close to the total revenue drop of $50.
In general, such expressions could include multiple dimensions such as Revenue Drop (DT == P C ∧ DC == X) which refers to a revenue drop across PC users served ads from data center X. Based on about one year of monitoring alerts in ad systems we have observed, through manual study as well as through using an attribution algorithm that blames anomalies on multiple dimensions, that such cases where multiple dimensions contribute together to a root-cause are very rare.
Therefore, for simplicity of exposition, in this paper, we limit our discussions to finding a Boolean expression that involves a single dimension and a set of its elements that explains the anomalous change.
To understand what constitutes the "best" dimension and a set of its elements, we studied several criteria.
Consider the following strawman approach that motivates our final problem statement.
Strawman: Find the dimension and a set of its elements whose revenue drop is at least a threshold fraction, T EP , of the total revenue drop, and is most succinct.We quantify the explanatory power (EP) of a set of elements as the fraction of the measure change that it explains.
We quantify succinctness (P) of a set of elements as the total number of elements in the expression.
Therefore, the strawman will find the expression that has explanatory power of at least T EP , and uses the smallest number of elements.Occam's razor suggests that the most succinct set, as long as it explains the drop within a certain margin of error (T EP ), is the best explanation.
By this argument, if T EP is set to 0.9, the best dimension and set of elements among the three equations is in Equation 1, since the data center X alone can explain 94% of the total drop.
This approach, however, has deficiencies for rootcausing in the presence of multiple dimensions.
Though data center X's revenue drop is a high 94% of the total revenue drop, notice that both the forecasted and actual revenue are equally spread between the two data centers X and Y. Data center X provided 94% of the forecasted revenue ($94 out of $100), and actual revenue ($47 out of $50).
Data center Y contributed 6% across both values.
By comparison, in the device type dimension, device type PC contributed 50% of forecasted revenue ($50 out of $100), but 98% of actual revenue ($49 out of $50).
The contributions of Mobile and Tablet device types also varies widely from 25% of forecasted revenue to 0% of actual revenue.
The contributions vary along the advertiser dimensions as well, but not as much as they do along the device type dimension.This large change in the contributions between forecasted and actual revenue from the different elements of the device type dimension is, in general, surprising and unexpected.
Consequently, we propose that surprise is a better indication of a problem than if we only used succinctness and explanatory power of an expression.
Say the root-cause of this revenue drop was due to a configuration file error which caused no ads to be shown on mobiles and tablets.
While data center X would still show a huge drop in revenue because it provides 94% of all ads shown across devices, the actual root-cause is better explained by the device type dimension, and the elements Mobile and Tablet.
In other words, the expression in Equation 3 is the best one, even though it is not the most succinct.To capture this observation, our approach includes a notion of "surprise" (S) associated with an expression (Section 3 has the precise definition).
Therefore, generalizing to any measure, our final revenue debugging problem statement can be captured in three steps:• For a dimension, find all sets of elements that explain at least a threshold fraction, T EP , of the change in the measure (have high explanatory power).
• Among all such sets for each dimension, find the sets that are most succinct in that dimension.
• Across all such sets for all dimensions, find the one set that is the most surprising in terms of changes in contribution.Again, for the mock example, with T EP = 0.9, the first step will narrow down the sets to {X} for Data Center, {A1, A3, A4} for Advertiser, and {Mobile, Tablet} and {PC, Mobile, Tablet} for Device Type.
Step 2 will narrow down the sets for each dimension to {X}, {A1, A3, A4}, and {Mobile, Tablet}.
Step 3 will then use the surprise metric to pick the Device Type dimension and its set {Mobile, Tablet} as the best explanation of the drop.Our algorithm use a per-element threshold of the change in the measure, T EEP , to add to the idea of succinctness.
Not only do we want the smallest set of elements, we also want only those elements that contribute at least a fraction of T EEP to the anomaly.We show in Section 3.4 that solving this problem can take exponential time (in number of elements) in the worst case.
Therefore, we use a greedy approach that solves this problem approximately.
We start with some notation and use it to formally define explanatory power and surprise.
We then describe the root-cause identification algorithm.
While the algorithm remains the same for fundamental and derived measures, the way explanatory power and surprise are computed for derived measures is more complex and is discussed separately in Section 4.
The list of important terms used in this section and their notation are summarized in For each of the measures m ∈ M of interest (including the fundamental and derived measures) and for each of the elements E ij , we have access to the predicted or forecasted values, F ij , as well as the actual observed values, A ij .
Note that, as discussed earlier, these values are aggregates corresponding to some time interval of interest (e.g., $100 revenue forecast, $90 revenue actual for element Flower123, dimension advertiser).
For fundamental measures such as revenue or number of searches, both the overall forecasted value for the measure, F (m), as well as the overall actual value, A(m), remain identical across all the dimensions (e.g., $100 forecasted and $50 actual revenue in the example in the previous section).
For fundamental measures, the overall measure is simply the summation of value of the measures of the elements of the respective dimensions, but the same is not true for derived measures as they are not additive (Section 4).
Thus, given F (m) and A(m), the algorithm needs to output a potential root cause to explain the difference between the two.
For this, it uses explanatory power and surprise, defined next.
Explanatory power of an element can be defined as the percentage of change in the overall value of the measure that is explained by change in the given element's value.
For fundamental measures, the explanatory power of an element j in dimension i is simplyEP ij = (A ij (m) − F ij )/(A(m) − F (m)) (4)For example, the total number of searches at a given hour deviates from a forecasted value of 1 million to 0.8 million, and the number of searches at the same hour at a particular data center, DC1, differs from its forecasted value of 0.5 million to 0.4 million, the explanatory power for element DC1 is (0.4-0.5)/(0.8-1) = 50%.
Note that, explanatory power for an element can be more than 100% or even negative, if the change in element is in opposite direction to overall change.
However, the sum of explanatory powers of all elements of any dimension should sum up to 100%.
Thus, explanatory power fully explains the change in the overall measure.
As discussed in the example in Section 2, a dimension that has large change in its distribution (e.g., Device Type) is more likely to be a root-cause than the dimension that does not exhibit such a change (e.g., Data Center).
We now formally define a measure of surprise to capture this notion.For each element E ij , let p ij (m) be the forecasted or prior probability value given byp ij (m) = F ij (m)/F (m), ∀E ij(5)Given a new anomalous observation, let q ij (m) be the actual or posterior probability valueq ij (m) = A ij (m)/A(m), ∀E ij(6)Intuitively, the new observations for a given dimension are surprising if the posterior probability distribution is significantly different from the prior probability distribution.
This difference between two probability distributions P and Q can be captured by the relative entropy or Kullback-Leibler (KL) divergence [12].
However, the use of KL divergence in our context has two issues.
First, KL divergence is not symmetric.
Second, KL divergence is only defined if, for all i, q i = 0 only if p i = 0, which does not hold in our setting (e.g., advertiser pauses his campaign).
Thus, instead of KL Divergence, we use a related measure called the Jensen-Shannon (JS) divergence [12] for computing surprise, defined asD JS (P, Q) = 0.5(Σ i p i log 2p i p i + q i + Σ i q i log 2q i p i + q i )Observe that D JS (P, Q) is symmetric and is finite even when q i = 0 and/or p i = 0.
Further, 0 ≤ D JS (P, Q) ≤ 1, where 0 denotes no change in distribution between P and Q, with higher values denoting greater differences.Thus, to compute surprise S ij for element E ij , we use p = p ij (m) and q = q ij (m) to computeS ij (m) = 0.5 (p log( 2p p + q ) + q log( 2q p + q ))(7)3.4 Algorithm The root-cause identification algorithm seeks to solve the optimization problem specified in Section 2 using the above definitions of explanatory power and surprise.1 Foreach m ∈ M // Compute surprise for all measures 2 Foreach E ij // all elements, all dimensions 3 p = F ij (m)/F (m) // Equation 5 4 q = A ij (m)/A(m) // Equation 6 5 S ij (m) = D JS (p, q) // Equation 7 6 ExplanatorySet = {} 7 Foreach i ∈ D 8 SortedE = E i .
SortDescend(S ij (m)) //Surprise 9 Candidate = {}, Explains = 0, Surprise = 0 10 Foreach E ij ∈ SortedE 11 EP = (A ij (m) − F ij (m))/(A(m) − F (m)) 12 if (EP > T EEP ) //Note that, obtaining the optimal solution to the problem in the worst case will take exponential time.
This can be shown through a simple example: consider a set of size n where each element has an identical explanatory power and we require n/2 elements of the set to explain T EP .
In this case, every possible subset of cardinality n/2 is of minimum size possible (succinct) and has explanatory power of T EP .
Thus, we have to compare the surprise values of all these subsets (whose count is exponential in n) in order to find the subset that has the maximum surprise, the optimal solution.Instead of enumerating various minimum cardinality subsets that have explanatory power of at least T EP , our algorithm ( Figure 2) uses the following greedy heuristic.
In each dimension, after computing the surprise for all elements (lines 1-5), it first sorts the elements in descending order of surprise (line 8).
It then adds each element to a candidate set as long as the element explains at least T EEP of the total anomalous change by itself (lines 12-15).
The parameter T EEP helps control the cardinality of the set (Occam's razor).
For example, if T EEP is 10% and T EP is 67%, we can have at most 7 elements that explain anomalous change.
Further, by examining elements in descending order of surprise, we greedily seek to maximize the surprise of the candidate set.
The algorithm adds at most one candidate set per dimension (lines [16][17][18][19], as long as the set is able to explain a majority (T EP ) of the anomalous change (explanatory power).
Finally, the algorithm sorts the various candidate sets by their surprise value and returns the top three most surprising candidate sets as potential root-cause candidates (lines 21-22).
Derived measures are functions of fundamental measures that are tracked by troubleshooters since they reveal more information than if one simply tracked the fundamental measures.
In this section, we discuss how we compute explanatory power and surprise for derived measures.
While attributing contribution of an individual element to the overall value of a derived measure is important for root-cause identification, this is not as straightforward as computing the same for fundamental measures.
In this section, we first start with a illustrative example that helps define explanatory power for derived measures and then present our solution to the derived measure attribution problem.
Example.
Consider the hypothetical example in Tables 6 and 7 that shows revenue and number of clicks, respectively, for four different advertisers during an anomalous period.
For these two fundamental measures, attribution of the overall change to each of the advertisers is simple using the explanatory power (equation 4) and is shown in the column labelled EP.
Thus, for the revenue drop, one can attribute it to advertiser A1 (400%) while for the increase in clicks, one can attribute it to advertiser A2 (200%).
Let us assume that an anomaly is thrown on a measure if it differs from its expected value by at least 20%.
Note that the overall revenue has gone down by 10% while the number of clicks is up 16%, neither of which exceeds the anomalous threshold.
The corresponding cost-per-click values are shown in Table 8 and using the same 20% threshold, the overall cost-per-click (22.5% decrease) can be labelled anomalous Thus, one can see that derived measures can be useful in surfacing anomalies that are not surfaced by just examining fundamental measures.
We confirm this quantitatively in Section 6.
The derived measure attribution problem is the following: how does one attribute the drop in overall cost-perclick from 0.2 (expected) to 0.155 (actual) to each of the advertisers?
If one examines the individual cost-perclicks of the advertisers in Table 8 at first glance, it appears that none of the advertisers can be blamed for the overall drop but surely one or more of them must be responsible!
Given this situation, how do we go about assigning explanatory power values for the change in cost-per-click to these advertisers?
Examining the fundamental measures does help shed more light.
For example, even though cost-per-click of A1 is unchanged, A1 had a 5X drop compared to its forecasted values for both revenue and clicks.
Given A1's cost-per-click (0.5) was higher than the overall value (0.2), the 5X reduction implies that A1 was indeed pulling down the overall cost-per-click.
The fact that A1 explains some of the decrease in the overall derived measure can be further validated by observing that if we used A1's actual values but assume that the rest of the advertisers delivered their respective forecasted values, then the overall cost-per-click goes down to 60/420 = 0.143 for an impact of -29%.
Similarly, while A2 had 0 revenue as forecasted, A2 had a large increase in clicks, which ends up decreasing the overall cost-per-click.
Again, if we used A2's actual values but keep the rest of the advertisers' measures to their forecasted value, the overall cost-per-click goes down to 100/660 = 0.152 for an impact of -24%.
The above exercise of changing one advertiser's value at a time also suggests that A1 was more responsible for pulling down overall cost-per-click than A2 (since use of A1's actual values resulted in lower overall value than for A2).
Now consider A3.
A3 had a higher revenue than forecasted without change in clicks, so A3 was clearly not contributing to the overall drop.
Using A3's actual values in the above exercise would in fact increase the overall cost-per-click to 0.26, for an impact of +30%.
Finally, A4 had no change in either revenue or clicks.
Therefore, A4 had no impact in overall cost-per-click.
Normalizing the individual impact values so that all the elements in total explain 100% of the overall change, the above exercise would give A1's explanatory power as 125%, A2's as 106%, A3's as -131% and A4's as 0%.
Summarizing the observations in the above example, one can see that an element's explanatory power for derived measures can be determined by computing a new derived measure value, where the actual value of the given element and forecasted values of all other elements are used, and comparing this derived measure value to the expected value of the derived measure.
Now, the question is how do we formalize this intuition in order to determine the explanatory power for arbitrary derived measures?
We describe this next.
Derived measure attribution.
Our solution to the derived measure attribution problem is adapted from partial derivatives and finite-difference calculus.
Recall that a partial derivative is a measure of how a function of several variables changes when one of its variable changes.
However, since we operate in the discrete domain, we use partial derivative equivalents from finite-difference calculus [13].
We formally define explanatory power of an element i for a derived measure, which is function h(m 1 ,...,m k ) of fundamental measures m 1 , ..., m k , as the partial derivative with respect to i in finite-differences of h(.)
, normalized so that the value across all elements of the dimension sum up to 100%.
While the above definition is general and applicable to derived measures that are arbitrary functions of fundamental measures (as long as they are differentiable in finite-differences), we now illustrate it through the specific example of derived functions of the form A(m 1 )/A(m 2 ), which make up many of the derived measures in ad systems (Figure 1).
For example, for the cost-per-click derived measure, we have m 1 = revenue and m 2 = clicks.The partial derivative in finite-differences of f(.)
/g(.)
is of the form (∆f * g − ∆g * f )/(g * (g + ∆g)), and is similar to continuous domain partial derivative, except for the extra ∆g in the denominator.Thus, explanatory power of element j for dimension i for derived measures of the form m 1 /m 2 is given byEP ij = ((A ij (m 1 ) − F ij (m 1 )) * F (m 2 ) −(A ij (m 2 ) − F ij (m 2 )) * F (m 1 )) /(F (m 2 ) * (F (m 2 ) + A ij (m 2 ) − F ij (m 2 ))) (8)We compute EP ij for each of the elements using the above equation and normalize it so that they add up to 100%.
Table 8 shows the explanatory power computed using the above formula for each of the advertisers.
We can see that the rank ordering of A1, A2, A4, and A3 and their respective explanatory power values for the attribution to the overall change agrees with the intuitive observations made earlier.
Recall that we defined surprise for fundamental measures in Section 3.3 based on the relative entropy (specifically, JS divergence) between the prior and posterior mass functions of values for measure m.
In this section, we seek to extend the notion of surprise to derived functions of multiple measures.Consider the cost-per-click example in the previous section.
A simple approach for computing surprise for derived measure is as follows.
Just as for fundamental measures, one could compute prior and posterior probability values for cost-per-click for each element E ij , say p ij (cost-per-click) and q ij (cost-per-click) and compute the surprise just as in Section 3.3.
However, such an approach will not work.
Consider the example of advertiser A2 in Table 8.
A2's cost-perclick was forecasted to be zero and the actual value was also 0.
Thus, if one used the above approach to compute surprise for element A2, it would have a value of 0 (no surprise).
However, we found that A2 had a high explanatory power of 106% for the overall change in costper-click due to changes in A2's number of clicks.Examining the problem from the perspective of relative-entropy, given several measures, we first need to compute the joint probability distribution of the measures and then compute relative entropy of the joint probability distribution function.
If the measures are independent, then the relative entropy (JS divergence as well) of the joint probability distribution is simply the sum of the relative entropy of the individual measure's probability distributions.
In ad systems, the measures are not always strictly independent since some of them can be correlated (e.g., as the number of searches increase, revenue can be expected to increase).
However, as an approximation, we assume that measures are independent, and compute the surprise for derived measures as the summation of the surprise of the individual measures that are part of the derived function.
In this section, we describe our implementation of the above algorithms in the Adtributor tool and outline our experience with a pilot deployment in a production ad system.
In our implementation, a database records, in real-time, counters for all measures, dimensions, and elements and exposes them as an OLAP service that supports multidimensional analytical queries [19].
When the system triggers an anomalous event, the Adtributor toolchain first gathers data relevant to the anomaly such as time of anomaly, measure, data for various measures, dimensions and elements.
After the data has been queried from the database, Adtributor employs the root-cause algorithm to discover potential root-causes for the given anomaly.Recall that measures are not necessarily independent of each other.
An anomaly on a certain measure could be correlated with changes in value of another.
Therefore, we build a dependency graph of measures, and for a given anomalous measure, run the root-causing algorithm for every measure that correlates with it.Adtributor filters the candidate set of root-causes (as described in Section 3) to produce the final list of rootcauses.
We use a T EP value of 67% and a T EEP value of 10%.
These threshold values are driven by what the troubleshooters already use in the manual process.
Also, our current implementation singles out a list of the top three dimensions.
The troubleshooting experts recommended this number based on their own requirements and also on ease of visualization.
With a smaller number they could miss useful information, while a larger number would lead to too much information for them to sift through.The final output is a self-contained HTML5 application.
Figure 3 shows an example of the output produced by the Adtributor toolchain.
The visualization of the root-causes contains the following information:• Dependency Graph: A graphical representation of dependencies between the different measures in the system (left half).
• Measure Historical Graph: A graph depicting the historical behavior of a measure (top right graph).
• Element Root-Causes and Historical Graph: For a given measure and dimension, the top elements that are root-causes.
The element root-causes are grouped by dimensions with their historical graphs (under top right graph).
We arrived at the visualization requirements through iterative discussions with the troubleshooting experts.
The dependency graph allows them to observe causality between the values of different measures, and the histori-cal graphs per-dimension help them in making a more informed choice on what exactly was the root-cause.
The entire Adtributor toolchain is implemented using .
NET Framework 4 using 12,500 lines of code and executed automatically for each anomaly.
We conducted a pilot deployment of Adtributor between May 1, 2013 andMay 10, 2013 with the troubleshooters who work with the production system on root-causing anomalies to understand the usefulness of Adtributor.
This deployment was partially successful in helping the troubleshooters with their current processes.
The findings of this pilot resulted in a set of improvements to our algorithm and visualization which led to significantly better performance as we show in our evaluation in Section 6.
Volatile dimensions: Various dimensions can be extremely volatile, and unexpected changes can occur in measures along these axes even though they are not necessarily the root-cause of the problem.
Consider the example of an advertiser who frequently changes the budget allotment to their ads.
When there is a revenue anomaly, this can sometimes cause the root-causing algorithm to pick the advertiser as a culprit even though the change coincidentally occurred just a little before the anomaly event.
This drove us to improve our prediction algorithm for measures associated with elements of volatile dimensions by increasing the weightage given to large changes in the near-past in our prediction model, thereby fixing this problem to a large extent.
Visualization enhancements: The dependency graph of related measures was found to be very useful by the troubleshooters.
However, the current view in the tool is limited to a small set of measures.
There are hundreds of other measures being monitored within the ad system for which the dependencies are not known.
We have therefore used a Bayesian structure learning algorithm [5] to infer a subset of these dependencies and plan to enhance the visualization of the dependency graph with these additional measures.
In the Section, we first describe four case studies in which multi-dimensional analysis is key to arriving at the final root-cause.
Next, we provide a quantified evaluation of the accuracy of Adtributor, and the time savings we achieve with the tool.
Case 1: This was triggered by an anomalous drop in revenue.
On performing the multi-dimensional analysis, we found that the dimension Browser was responsible.
Fig- ure 4 helps explain how Adtributor arrived at this result.It shows the percentage contribution to revenue along three dimensions -Browser, Data Center, and Bucket -for predicted revenue and actual revenue (see Table 1, example 3 for the definition of a bucket.)
.
Notice that Browser 3's revenue contribution was predicted to be 12%, but its actual revenue was 0%!
Similarly, Browser 1's contribution was predicted to be 60%, but was actually much higher at 74%.
Neither the Data Center dimension nor the Bucket dimension show such surprising changes in contribution.
This problem was actionable, since a further investigation revealed that a configuration error had caused no ads to be shown to users on Browser 3.
Correcting the error fixed the problem and further loss in revenue.Case 2: We noted an anomalous revenue increase at a particular time, which Adtributor attributed to a certain set of six advertisers.
Two of these advertisers were airline ticket vendors, two were car rental agencies, and the remaining two were hotels.
In aggregate, they fully explained the change in revenue.
Delving into the issue, we noticed that these advertisers had deliberately increased their budgets for a certain period of time.
The ads were appearing in a geographic region which had a long-weekend holiday approaching.
Thus, we inferred that the advertisers were trying to capitalize and capture the attention of users as they performed vacation-related searches.
Clearly, in this case, the sudden rise in revenue was attributed to advertiser behavior and not due to an actionable bug in the system.
Several such anomalies also occur when advertisers deliberately drop their budgets as well.Case 3: The total number of searches went anomalously high, and an analysis showed that most of the increase was attributed along the User-agent string dimension.
From post-processing on this result, it was inferred that a majority of the searches with the repeated useragent string were coming from a small range of IP addresses, and therefore, suspiciously characteristic of bottraffic.
In particular, the goal of this bot was to perform queries and collect information for search-engine optimization (SEO).
This was an actionable issue which was fixed promptly by filtering the contribution of this traffic to the various metrics.Case 4: We notice that sometimes, publishers change the placement of advertisements on their page, which make ads more (or less) conspicuous.
This in turn causes a corresponding increase or decrease in revenue.
These show up as revenue changes along the dimension of Ad Position on the page.
For instance, if the publisher moves an ad meant to be shown on the side of the page to the top of the page, this presents itself as a surprising increase in revenue attributed to ads shown on the top.
This is because users tend to click more on ads shown on the top of a page than they do on ads shown on the side.
Our quantitative evaluation is based on using Adtributor to root-cause problems in a widely deployed ad system.
We evaluate all anomalies generated on a total of 12 measures, both fundamental and derived, across 33 dimensions.
The results we present here use a subset of 128 valid anomalies generated by over a billion searches between September 1, 2013 and September 15, 2013 across 8 populations: PC and Mobile ad systems for USA, UK, France and Germany.
For the purpose of this study, we do not consider false-positives in the anomaly generation process as they are weeded out by troubleshooters before applying the root-causing process 2 .
50% of the tested anomalies were generated solely on derived measures, with no related anomalies being generated on the respective fundamental measures that constitute the derived measure.
This shows that using derived measures in aggregate root-cause analysis is extremely important.We compare the output of Adtributor's multidimensional analysis with the output of the troubleshooting team that performs an in-depth and detailed analysis of these anomalies through manual means with the assistance of other tools (not Adtributor).
Manually analyzing the cause of the anomalies has a number of advantages.
The troubleshooters are aware of a large amount of information and domain-knowledge, and they frequently use this knowledge in the troubleshooting process.
An automated tool such as Adtributor cannot possibly have an understanding of all of this.
Further, Adtributor only narrows the scope of the root-cause (Column 4 of Table 1) -a manual process may still be necessary in many cases to identify the final root-cause (Column 4 of Table 1) since some of the data necessary to do this next step may not be available for the automated process (e.g., verifying whether the publisher indeed changed the position of ads).
However, the advantage of using Adtributor is that it aids the manual troubleshooting process by 1) using the multi-dimensional root-cause analysis to exhaustively check all possible dimensions (as we show, in a few cases, the manual process may overlook a dimension, leading to erroneous conclusions) and 2) Significantly faster processing to bubble up the top suspect candidates.
For example, there are dozens of dimensions and some dimensions can have thousands of elements.As described in Section 5, Adtributor displays the top three dimensions and their elements as potential suspects.
We say that Adtributor matches the output of the manual root-causing process if it shows the same dimension and exactly the same elements as the manual process at any one of these three positions.
Table 9: Results summary from our comparison of Adtributor with manual scrutiny (and Strawman).
Table 9 shows the results of the comparison between the output of Adtributor and the manual investigation.
Of the 128 anomalies, Adtributor matched the results of the manual analysis in 118 cases.
Of these, 81 (69%) matched in position 1, 27 (23%) matched only in position 2 and not in position 1, and 10 (8%) matched in position 3, and not in position 1 or 2.
Of the 10 anomalies for which we did not match the manual output, we performed a deeper dive with the troubleshooting expert.
On careful scrutiny, we found that out of the 10, 4 of the manual root-causes were erroneous, and Adtributor's output in position 1 was, in fact, the correct root-cause.
This shows the utility of using a systematic algorithm, as in Adtributor, that exhaustively searching all dimensions to perform multi-dimensional root-cause analysis.Out of the remaining 6 anomalies, the manual output was correct in 5 of them while the output of Adtributor was erroneous.
In all of these cases, Adtributor suffered from a lack of domain knowledge, or the lack of knowledge of events external to the system which the troubleshooters were explicitly aware of.
In one case (labelled ambiguous), however, the troubleshooter felt the dimension and elements blamed by Adtributor was as likely to be the true root-cause as the one obtained through manual analysis.
In this case, he felt a further drill-down would be required to determine the correct root-cause.
Taking the manual errors into account, Adtributor's overall accuracy was (118+4)/128, or 95%.
We also compare the potential time that could be saved using Adtributor compared to the first step in the manual troubleshooting process that identifies the dimension and elements that may be potential root-causes.
Adtributor uses a multi-threaded implementation and caching to speed up the process of studying every dimension and every measure.
It has a turnaround of approximately 3-5 minutes for each anomaly.
The manual process of troubleshooting took between 13 minutes for the fastest anomaly to up to 231 minutes, with an average turnaround time of 73 minutes.
Therefore, we conclude that Adtributor speeds up the initial root-causing process by an order of magnitude.Finally, we show the value of using surprise by comparing our algorithm to the Strawman discussed in Section 2 that only uses succinctness and explanatory power.
Compared to Adtributor's accuracy of 95%, we found that Strawman had an accuracy of only 20%.
This clearly demonstrates the value of using surprise to identify the right dimension and elements as the root-cause.
We believe that the techniques introduced in this paper are general enough to be useful in other settings.
For example, Multi-dimensional analysis: Consider a web-server with a global audience that suddenly sees the number of hits drop sharply.
Many of the dimensions considered in this paper such as data centers or CDNs, browsers, user locations, fraud operators/bots, etc. may all be potential root-causes that a multi-dimensional analysis can help disambiguate.
Derived measure attribution: Consider the following problem.
The Mean-opinion-score (MOS) for VoIP calls has dropped and the investigators would like to understand which of the links in the route of the call is most responsible for this drop.
Each link may have different amounts of delay, jitter, and loss percentages, and the MOS is a complex function of measures such as delay, loss, and jitter [6].
The use of the derived attribution technique can help compute the explanatory power of the drop in MOS for each of links.
System and Network Root-Cause Analysis: Previous research has extensively studied root-causing performance and failure problems in systems and networks [14,2,10,1,21,15,3,20,11,18].
Some of these use traces across individual requests through systems [3,14] to diagnose problems, while others use aggregate counters of system performance or configuration values [15,10,20] to diagnose problems.Distalyzer [14] is an example of the former category.
It uses individual event logs and learns anomalous patterns between events that indicate a performance problem in a system component.
Ganesha [15] is an example of the latter.
It uses clustering approaches across aggregate measures, such as CPU usage, to build distinct profiles of MapReduce nodes.
While our approach too uses aggregate measures, we intend to find more than performance problems or diagnose failures.Q-Score [18] uses machine-learning to arrive at rootcauses.
We tried similar approaches and decided against them because selecting the right set of features to input to a stock machine-learning algorithm turned out to be a non-trivial task.
Instead, we found that building a customized algorithm was simpler and better suited to analysis and feedback by our domain experts.SCORE [11] localizes IP faults to underlying components using succinctness of explanation.
Given a set of link failures as observation, it determines the smallest set of risk groups that explain failures.
However, as we show, this approach is not enough to perform attribution across dimensions and a notion of surprise is essential to complete our solution.
Data Mining for Summarization: Previous work in data mining [17,16,7] has concentrated on summarizing multi-dimensional data in OLAP products.
The objective is to provide an easily interpretable summary of the differences in data values across multiple dimensions.
Such summarization techniques have been applied to network traffic summarization as well [9].
While data summarization across multiple dimensions is related to our work, it does not match our objective of finding surprising changes to perform root-cause analysis.
In fact, our approach to root-cause analysis is complementary to these approaches and can be applied on the summaries that they generate.
We have described an algorithm, implementation, and evaluation of an approach that uses multi-dimensional analysis for root-causing problems in large-scale ad systems.
We found that our approach has high accuracy (95%), helped identify more accurate root causes than the manual investigation in a few cases, and was able to reduce troubleshooting time significantly.
We would like to thank our shepherd VYAS SEKAR for his valuable comments and suggestions.
We would also like to thank MURALI KRISHNA for helping us validate the output of Adtributor and determine its accuracy.
