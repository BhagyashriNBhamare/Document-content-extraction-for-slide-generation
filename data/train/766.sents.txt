In the context of large space MDPs with linear value function approximation, we introduce a new approximate version of λ-Policy Iteration (Bertsekas & Ioffe, 1996), a method that generalizes Value Iteration and Policy Iteration with a parameter λ ∈ (0, 1).
Our approach, called Least-Squares λ Policy Iteration , generalizes LSPI (Lagoudakis & Parr, 2003) which makes efficient use of training samples compared to classical temporal-differences methods.
The motivation of our work is to exploit the λ parameter within the least-squares context, and without having to generate new samples at each iteration or to know a model of the MDP.
We provide a performance bound that shows the soundness of the algorithm.
We show empirically on a simple chain problem and on the Tetris game that this λ parameter acts as a bias-variance trade-off that may improve the convergence and the performance of the policy obtained.
We consider the question of solving Markov Decision Processes (MDPs) approximately, in the case where the value function is estimated by a linear architecture and using sampling, typically when the state space is too large for an exact solution.
TD(λ) with linear approximation (Sutton & Barto, 1998) is a fundamental algorithm of the reinforcement learning community.
It estimates the value function by using temporal differences based on a sample trajectory and the λ parameter controls the length of the backups made to update the value function.Least-squares (LS) methods such as LSTD(0) (Bradtke & Barto, 1996), LSTD(λ) (Boyan, 2002) and LSPE(λ) (Nedi´cNedi´c & Bertsekas, 2003;Yu & Bertsekas, 2009) are alternative approaches for estimating the value function.
They build explicitly a low-dimensional linear system that characterizes the solution TD(λ) would converge to, but they use simulation data more efficiently, and they usually don't need a decreasing stepsize parameter which is often hard to tune.
Moreover, they converge faster in practice, though each iteration has a complexity of p 2 instead of p, where p is the dimension of the linear architecture.
It was shown empirically (see Boyan (2002); Yu & Bertsekas (2009); Lagoudakis et al. (2002)) that LS methods are more stable and can succeed much better than TD(λ).
LSTD(λ) and LSPE(λ) focus on the problem of evaluating a policy.
We are interested in this article in learning a policy.
The λ-Policy Iteration algorithm (λPI), proposed by Bertsekas & Ioffe (1996), iterates over policies and uses a λ paramater that introduces a bias-variance trade-off similar to the one of TD(λ) when sampling is used to evaluate the current policy.
λPI generalizes the classical Dynamic Programming (DP) algorithms Value Iteration and Policy Iteration to learn a policy: λ represents the size of the step made toward the value function of the policy (λ = 0 corresponds to Value Iteration and λ = 1 corresponds to Policy Iteration).
One the one hand, when λ is close to 1, the variance of the value function estimation may deteriorate the eventual performance of the obtained policy.
On the other hand, reducing λ introduces some kind of optimism, since we do not try to compute completely the value of the current policy before we switch to a new policy, and such a bias can degrade the convergence speed.
Thus, this parameter λ creates a bias-variance trade-off similar to that of TD(λ), and in practice, an intermediate value between 0 and 1 can inria-00520841, version 1 -24 Sep 2010 Author manuscript, published in "International Conference on Machine Learning (2010) (Sutton & Barto, 1998) × LSTD(0) (Bradtke & Barto, 1996) × LSTD(λ) (Boyan, 2002) × × LSPE(λ) (Yu & Bertsekas, 2009) × × λPI (Bertsekas & Ioffe, 1996) × × × LSPI (Lagoudakis & Parr, 2003) × × LSλPI × × × × Figure 1.
Main characteristics of related works: a trade-off parameter λ, the LS context, the optimistic evaluation of the value function and the off-policy evaluation.give the best results (Bertsekas & Ioffe, 1996).
When they use a λ trade-off parameter, the approaches mentioned above make an on-policy estimation of the value function, that is, they need samples generated by the policy under evaluation.
The offpolicy evaluation can be used in a policy iteration context without having to generate new samples when the policy changes.
This idea is developed in LSPI (Lagoudakis & Parr, 2003).
The difference with our approach is the fact that LSPI does not make an optimistic evaluation of the policy using a λ parameter.In this article, we introduce Least-Squares λ Policy Iteration (LSλPI for short), a generalization of LSPI that adds the λ parameter of λPI.
LSλPI is to our knowledge the first algorithm that includes all interesting characteristics discussed above: the bias-variance trade-off, the LS framework, the optimistic evaluation of the value function and the off-policy evaluation.
Figure 1 lists the related works and summarizes their characteristics to show the main contributions of LSλPI.
Overall, LSλPI can be seen as an optimistic generalization of LSPI with a λ parameter, an offpolicy, LS implementation of λPI, or as an off-policy, optimistic policy iteration variant of LSPE(λ).
The paper is organized as follows.
Section 2 presents the notations and the λPI algorithm in the exact case.In section 3, we consider the approximate case and detail LSλPI.
Section 4 finally gives some experimental results showing the interest of this parameter λ.
We introduce here the notations we will use and present an overview of λPI in the exact case.
The reader can refer to Bertsekas & Ioffe (1996) for a more detailed description of the λPI method.We define an MDP as a tuple (S, A, P, R), where S is the state space, A is the action space, P is the transition function (P (s, a, s ) is the probability of getting to state s after taking action a from state s) and R is the reward function: R(s, a, s ) ∈ R is the reward received in state s after taking action a from state s.
We will use the simplified notation R(s, a) to denote the expected reward of a state-action pair:R(s, a) = s ∈S P (s, a, s )R(s, a, s ).
A policy is a mapping from S to A.
The value function of a policy π is the function Q π : S × A → R that associates to each state-action pair (s, a) the expected value of the discounted, cumulative reward one can get from state s, taking action a and following policy π then:Q π (s, a) = E π t≥0 γ t r t s 0 = s, a 0 = awhere E π denotes the expected value with respect to the policy π, r t is the reward received at time t and γ ∈ (0, 1) is a discount factor 1 .
For any vector Q, let B π be the Bellman operator defined by B π Q = R + γP π Q where R is the reward vector and P π is the |S||A| × |S||A| transition matrix induced by the choice of an action and the policy π then.
It is well known (see for instance Puterman, 1994) that this operator is a contraction mapping of modulus γ and that its only fixed point is Q π .
Thus, the value function Q π is the solution of the Bellman equation Q = B π Q. Let us also denote by Q * the optimal value function, that is, Q * (s, a) = max π Q π (s, a).
If the optimal value function is known, it is straightforward to deduce an optimal policy by taking a greedy policy with respect toQ * : π * (s) ∈ arg max a Q * (s, a).
We now present λPI, a method introduced by Bertsekas & Ioffe (1996) that generalizes the standard DP algorithms Value Iteration (VI) and Policy Iteration (PI).
A parameter λ ∈ (0, 1) specifies whether the algorithm is closer to PI (λ = 1) or VI (λ = 0).
At each iteration, λPI (see Algorithm 1) computes a value function Q k+1 from the greedy policy (π k+1 ) with respect to the previous value function Q k .
The value function update corresponds to a λ-geometric average of the terms (B π k+1 ) i Q k .
The bigger λ, the more the Figure 2.
Visualizing λPI on the greedy partition sketch: Following Bertsekas & Tsitsiklis (1996, page 226), one can decompose the value space as a collection of polyhedra, such that each polyhedron corresponds to a region where the greedy policy remains the same.
PI generates a one-step move toward Q π k+1 whereas VI makes small steps toward Q π k+1 .
λPI is intermediate between PI and VI: it makes a λ-adjustable step toward Q π k+1 .
value function gets close to Q π k+1 .
An intuitive view of the value function updates made by VI, PI and λPI is given in the greedy partition represented on Figure 2.
Algorithm 1 λ-Policy Iteration loop π k+1 ← greedy(Q k ) Q k+1 ← (1 − λ) i≥1 λ i−1 (B π k+1 ) i Q k end loopBertsekas & Ioffe (1996) introduced an operator denoted M k that gives an alternative formulation of the algorithm.
At each iteration k, this operator is defined as follows:M k Q = (1 − λ)B π k+1 Q k + λB π k+1 Q.(1)Intuitively, M k can be seen as a damped application of B π k+1 .
They showed that M k is a contraction mapping of modulus γλ and that its only fixed point is the value Q k+1 computed by λPI.
Then, to calculate Q k+1 in practice, one can make successive applications of this M k operator until the fixed point is obtained.Bertsekas & Ioffe (1996) also showed that the value function update can be seen as an incremental updateQ k+1 ← Q k + ∆ k , where ∆ k (s 0 , a 0 ) = E π k+1 ∞ n=0 (λγ) n δ k (s n , a n , s n+1 ) (2) with δ k (s, a, s ) = R(s, a, s ) + γQ(s , π k+1 (s )) − Q(s, a).
Equation (2) gives some insight on the variance reduction property of small values of λ.
Indeed, one can see that the policy evaluation phase may be easier when λ < 1 because the horizon of the sum estimated is reduced.
However, λPI is known to converge with an asymptotic speed that depends on λ ( Bertsekas & Ioffe, 1996): small values of λ deteriorate the asymptotic convergence speed because they introduce a bias in the evaluation (the algorithm does not estimate the true value function of the policy anymore).
This drawback is less important when using approximation and simulation since the asymptotic convergence speed is not a crucial factor.
We have presented λPI in the exact case.
However, λPI was designed for a context of approximation and sampling.
The algorithm we introduce now, called LSλPI, is an approximate version of λPI which reduces to LSPI (Lagoudakis & Parr, 2003) when λ = 1.
LSλPI is an iterative, sampling-based algorithm that computes at each iteration an estimation of the value function of the current policy, and then uses this approximation to improve the policy.
As it uses value functions defined over the state-action space, knowing a model of the MDP is not required.
As LSPI, LSλPI is an off-policy algorithm: the policy under evaluation may be different from the one used to generate samples.
Thus, a unique sample set can be reused for all iterations even if the policy changes.
LSλPI relies on a usual linear representation of the value function.
At each iteration k, we consider a policy π k and an approximate value function Q k that estimates the Q k that the exact version of λPI would compute.
The new policy π k+1 is the greedy policy with respect to Q k , then we approximate Q k+1 by a linear combination of basis functions:Q k+1 (s, a) = p i=1 φ i (s, a)w i .
The φ i terms are p arbitrary basis functions and the w i are the parameters of this architecture.
As in general p |S||A| when the state space is large, such a value function requires much less resources than an exact (tabular) representation.
If we denote by φ(s, a) the column vector of size p whose elements are the basis functions applied to (s, a), and Φ the |S||A| × p matrix composed of the transpose of all these vectors, we can write Q k+1 = Φw k+1 , where w k+1 is the parameter vector representing the approximate value function Q k+1 .
To compute w k+1 , LSλPI can use two standard methods.
Those methods are described for instance in Lagoudakis & Parr (2003) and we generalize them to LSλPI.
They both try to compute an approximate value function Q k+1 which approaches the vector Q k+1 .
In other words, they look for a Q k+1 such that M k Q k+1 Q k+1 .
When λ = 1, we have M k = B π k+1 and the resulting algorithm, which approximates the Bellman equation's solution, matches LSPI.
As the fixed-point equationQ k+1 = M k Q k+1 has no solution in general since M k Q k+1is not necessarily in the space induced by the basis functions, the fixedpoint method (FP) applies an orthogonal projection to it.
This means that we seek an approximate value function Q k+1 that verifiesQ k+1 = Φ(Φ T D µ Φ) −1 Φ T D µ M k Q k+1 .
(3)D µ represents the diagonal matrix of size |S||A| whose terms are the projection weights, denoted by µ(s, a).
µ is a probability distribution over S × A. By developping Equation (3), it can be verified that w k+1 is the solution of the linear system Aw = b of size p × p withA = Φ T D µ (Φ − λγP π k+1 Φ) and b = Φ T D µ (R + (1 − λ)γP π k+1 Φw k ).
When the number of states is large, A and b cannot be computed directly even if a model of the MDP is available.
We can estimate them by using a set of L samples of the form (s, a, r , s ) where (s, a) ∼ µ, s ∼ P (s, a, ·) and r = R(s, a, s ).
It can be seen thatA = E φ(s, a)(φ(s, a) − λγφ(s , π k+1 (s ))) T and b = E φ(s, a)(r + (1 − λ)γφ(s , π k+1 (s )) T )w k .
Let us denote by A and b the sampling-based estimations of A and b. For each sample (s, a, r , s ) considered, we can update A and b as followsA ← A + 1 L φ(s, a) (φ(s, a) − λγφ(s , π k+1 (s ))) T , b ← b + 1 L φ(s, a) 񮽙 r + (1 − λ)γφ(s , π k+1 (s )) T w k 񮽙 .
To simplify these update rules, as we intend to solve the linear system Aw = b, we can remove the 1 L factor in the A and b updates without changing the solution.
Once we have estimated A and b from a sample source, we solve the linear p × p system Aw = b to obtain the new parameter vector w k+1 .
As in Lagoudakis & Parr (2003), it is possible to update A −1 incrementally by using the Sherman-Morisson formula (starting with an initial estimate βI for a small β), instead of solving the system after each sample:A −1 ← A −1 + A −1 uv T A −1 1 + v T A −1 u(4)where u = φ(s, a) and v = φ(s, a) − λγφ(s , π k+1 (s )).
The weight vector w k+1 is then computed as A −1 b, which reduces the complexity to p 2 instead of p 3 .
To calculate the approximate value function Q k+1 , an alternative to FP is the Bellman residual minimization method (BR).
Let us consider the (generalized) Bellman equation Q k+1 = M k Q k+1 and the corresponding residual defined by Q k+1 − M k Q k+1 .
We seek a value function Q k+1 that minimizes the quadratic, weighted norm of this residualQ k+1 −M k Q k+1 µ,2where · µ,2 denotes the quadratic norm weighted by the distribution µ: Q µ,2 = 񮽙 s,a µ(s, a)Q(s, a) 2 .
It can be verified that the norm to minimize equals(Φ − λγP π k+1 Φ)w k+1 − R − (1 − λ)γP π k+1 Φw k µ,2 .
Therefore, by a standard least-squares analysis, the parameter vector w k+1 that minimizes the Bellman residual is the solution of the p×p linear system Aw = b withA = (Φ − λγP π k+1 Φ) T D µ (Φ − λγP π k+1 Φ) and b = (Φ − λγP π k+1 Φ) T D µ (R + (1 − λ)γP π k+1 Φw k ).
As in the case of FP, A and b can be estimated from a set of samples whose distribution corresponds to µ.
However, BR requires for each sample to generate two possible next independant states instead of only one.
This requirement is known for λ = 1 (Sutton & Barto, 1998) and also applies for λ > 0.
Let us denote by (s, a, r , s , s ) a sample, where s and s are the results of two independent realizations of the action a from the state s, and r = R(s, a, s ) (the reward of s is not needed).
Again, if (s, a) is drawn from µ, s and s from P (s, a, ·), and r = R(s, a, ·), we haveA = E (φ(s, a) − λγφ(s , π k+1 (s ))) (φ(s, a) − λγφ(s , π k+1 (s ))) T and b = E (φ(s, a) − λγφ(s , π k+1 (s ))) (r + (1 − λ)γφ(s , π k+1 (s )) T w k ) inria-00520841, Least-Squares λ Policy IterationAs in FP, it is straightforward from these equations to build estimations A and b from the samples, by using similar incremental update rules.
We can solve the p × p linear system Aw = b to get the new value function after each sample, or incrementally update A −1 using the Sherman-Morisson formula: this update is identical to that of FP (Equation 4) except that in this case u = φ(s, a) − λγφ(s , π k+1 (s )).
For both methods, if a model of the MDP is available, we can integrate it in the update rule: the samples are then reduced to state-action pairs (s, a) and we replace the successor terms φ(s , π k+1 (s )) and r by their respective expected values s ∈S P (s, a, s )φ(s , π k+1 (s )) and R(s, a).
Note that for BR, when there is a model of the MDP, the double sampling issue discussed above disappears.
FP and BR find different solutions in general, though when λ = 0, they are equivalent (this can be seen in the formulas above) and they reduce to Fitted Value Iteration (Szepesvári & Munos, 2005).
When λ = 1, according to the literature (Lagoudakis & Parr, 2003), FP seems to give better results.
One can find examples where BR does not compute the right solution while FP does ( Sutton et al., 2009).
However, Schoknecht (2002) showed that FP can suffer from numerical instability (the matrix A for FP is not always invertible).
LSλPI iterates over policies.
State-of-the-art LS policy evaluation approaches such as LSTD(λ) (Boyan, 2002) and LSPE(λ) (Yu & Bertsekas, 2009) could also be used in a policy iteration context to address control problems.
The major difference is that LSλPI is an optimistic algorithm: our algorithm switches to the next policy before the current policy is completely evaluated.
In LSTD(λ), the value is evaluated completely and λ controls the depth of the backups made from the sampled trajectories.
LSPE(λ) is an approximation of λPI, applied to the evaluation of a fixed policy.
It makes multiple λ-steps towards the value of the policy using an LS method until the policy is evaluated completely.
LSλPI makes only one λ-step and changes the policy then.
Another difference between LSPE(λ) and LSλPI is the fact that they estimate different terms to make the approximate λ step: LSPE(λ) uses one or several trajectories generated with the current policy and relies on the equation Q k+1 = Q k + ∆ k (see Equation (2)) whereas LSλPI considers the equation Q k+1 = M k Q k+1 and can use off-policy samples.
Bertsekas & Tsitsiklis (1996) provided a performance bound for the policy sequence generated by approximate policy/value iteration algorithms (the cases λ = 0 and λ = 1).
It turns out that a similar bound applies to a large class of approximate optimistic policy iteration algorithm, of which LSλPI is a specific case.
For all these algorithms, if the approximation error is bounded at each iteration, then the asymptotic distance to the optimal policy is bounded:Theorem 1 (Performance bound for opt.
PI) Let (λ n ) n≥1 be a sequence of positive weights such that n≥1 λ n = 1.
Let Q 0 be an arbitrary initialization.
We consider an iterative algorithm that generates the sequence(π k , Q k ) k≥1 with π k+1 ← greedy(Q k ), Q k+1 ← n≥1 λ n (B π k+1 ) n Q k + k+1 .
k+1 is the approximation error made when estimating the next value function.
Let be a uniform majoration of that error, i.e. for all k, k ∞ ≤ .
Thenlim sup k→∞ Q * − Q π k ∞ ≤ 2γ (1 − γ) 2 .
The proof, which is omitted for lack of space, can be found in (Thiery & Scherrer, 2010).
Approximate versions of λPI such as LSλPI correspond to the case where λ n = (1 − λ)λ n−1 for all n, but the result remains valid for any choice of coefficients λ n that sum to 1.
Thus, the bound also applies to Modified Policy Iteration (Puterman, 1994), which consists in applying m times the Bellman operator with m fixed (i.e. we take λ m = 1 and λ n = 0 for all n 񮽙 = m).
We now present an experimental validation of LSλPI on two problems addressed by Lagoudakis et al. (2002; for LSPI: a simple chain MDP where the exact optimal value function is known, and the more challenging Tetris problem.
We consider the exact same chain problem as in Lagoudakis & Parr (2003), that is, a chain of 20 states with two possibles actions: left (L) or right (R).
Each action goes in the wanted direction with probability 0.9, and in the wrong direction with probability 0.1.
When the agent gets to the left or right end of the chain, a reward of 1 is obtained.
In all other cases, the reward is 0.
The optimal value can be computed exactly.
In our experiments, we compare at each iteration the current value Q k+1 to the optimal one, and the (exact) value of the current policy to the optimal value.
We do not use the knowledge of the model (transitions and rewards) of the MDP and we represent the state space with the set of polynomial or Gaussian basis functions proposed by Lagoudakis & Parr (2003).
We naturally observe that the value function convergence is harder when the number of samples is low, or when γ is high (i.e. the horizon is bigger).
When this is the case, λ has an influence.
Figure 3 represents the distance between the approximate value function and the optimal value function for several values of λ, γ = 0.95, averaged over 10 executions with different samples.
The left graph corresponds to FP and the right graph to BR.
As expected, we observe that for λ < 1, the approximation is better because the sampling variance is reduced, and more iterations are needed to obtain this better approximation.
For λ = 1, after only a few iterations, the value function stops improving and oscillates between values that are relatively far from the optimal ones, compared to lower values of λ.
Thus, intermediate values of λ seem to offer the best trade-off between approximation capability and convergence speed.
We actually observe, especially for BR, that it might be interesting to use a decreasing value of λ: indeed, in the first iterations, values of λ close to 1 come faster near the optimal value function, and then, smaller values of λ lead to better asymptotic convergence.
One can notice that these curves are similar to the ones of Kearns & Singh (2000), which provide a formal analysis of the biasvariance trade-off of TD(λ).
On most of the experiments we ran, FP and BR give similar performance, with a slight advantage for FP.
We observe on this typical experiment that for FP, there is a bigger range of λ values that make the value function converge to a good approximation.
This confirms the discussion of section 3.3: in practice, FP seems to give better results than BR even though in theory, numeric stability problems can occur.Another observation is that when the value function does not converge, the policy oscillates with a frequency that increases with λ.
This happens when there is a cycle in the sequence of policies.
It can be seen on Figure 5, which draws Q π k − Q * ∞ for the FP experiment of Figure 3.
For small values of λ, the policy oscillates slowly because LSλPI makes small steps.
When λ increases, the oscillations are faster as the steps are bigger (recall the intuitive view of Fig- ure 2).
For big values of λ, the policy does not converge anymore and oscillates with a higher frequency.
For λ = 0.7, we observed that the policy converged: this is all the more interesting that in such a convergent case, the performance guarantee of Theorem 1 can slightly be refined (Thiery & Scherrer, 2010).
Q k − Q * ∞.
Right: evolution of Q π k − Q * ∞.
Average curves of 10 runs, each run using a different set of episodes of 200 samples.
We observe that when the policy is the same for different values of λ, the value seems to converge to a limit that does not depend on λ.
We verified this property, which does not apply to BR.
Figure 4 plots an experiment with FP where convergence is easier than in the previous example as we set γ = 0.9 (other parameters are unchanged).
The right graph shows the quality of the policy: it seems to converge except when λ = 1.
The left graph represents the distance between the current approximate value and the optimal value.
After about 40 iterations, the policy becomes the same for all converging values of λ.
Then, it seems that the value function converges to the same vector for all these values of λ.
We can verify that for FP, if the policy and the value fonction both converge, then the limit of the value function does not depend on λ (this is in general not the case for BR).
If we denote by Π the projection Φ(Φ T D µ Φ) −1 Φ T D µ , by using the definition of M k (Equation (1)) and the facts thatQ k+1 = Q k and π k+1 = π k , we have Q k+1 = ΠM k Q k+1 = Π((1 − λ)B π k+1 Q k+1 + λB π k+1 Q k+1 ) = ΠB π k+1 Q k+1 .
Tetris is a popular video game that consists in moving and rotating different shapes to fill as many rows as possible in a 10 × 20 grid.
We reproduced the experimental settings of Lagoudakis et al. (2002).
Though our initial policy is the same as theirs (personal communication), the scores cannot be compared easily.
The initial policy scores about 250 rows per game on our implementation, whereas they report an initial mean score of 600 rows.
This may be due to Tetrisspecific implementation differences (see the review of Thiery & Scherrer, 2009).
We first ran LSλPI on a set of 10000 samples, as Lagoudakis et al. (2002) did for λ = 1.
We observed that diminishing λ did not improve the performance (it just made the convergence slower).
We think that the sample set was too large to make λ useful.
We then tried a smaller set of training data (1000 samples instead of 10000) in order to make the convergence more difficult.
Figure 6 represents the performance of the learned policies for various values of λ.
When λ = 1, the algorithm diverges: this is because there is a small number of samples.
The score switches between 0 and about 600 for FP, and falls to 0 for BR.Better scores are achieved for other values of λ.
As in the chain walk experiment, λ has more influence on the performances in BR.
After convergence, the best performance is reached by BR with λ = 0.9 and the corresponding policy scores about 800 rows per game.
We introduced LSλPI, an implementation of λPI (Bertsekas & Ioffe, 1996) within the LS context.
Our algorithm generalizes LSPI (Lagoudakis & Parr, 2003) by adding the optimistic evaluation of λPI.LSλPI is to our knowledge the first method that has the following four characteristics: it uses a λ parameter to make a bias-variance trade-off in the value estimation phase, makes an optimistic evaluation of the policy, fits within the LS framework to makes an efficient use of training data, and iterates over policies in an off-policy way while most LS works make on-policy evaluation.
It shares the virtues of LSPI: a model of the MDP is not needed (though it can be integrated when available), and as the value function is estimated off-policy, generating new trajectories or samples with each policy is not required.We provide the analytical claim that optimistic policy iteration algorithms such as LSλPI are sound algorithms and we ran experiments that emphasize the influence of λ in two control problems: a simple chain walk problem and the Tetris game.
Our empirical results show that intermediate values of λ can give the best results in practice when the number of samples is low.
This may be of particular interest in online, off-policy learning applications.
