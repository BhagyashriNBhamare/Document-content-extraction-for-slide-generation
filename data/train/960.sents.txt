A Single Nucleotide Polymorphism (SNP) is a position in the genome at which two or more of the possible four nucleotides occur in a large percentage of the population.
SNPs account for most of the genetic variability between individuals, and mapping SNPs in the human population has become the next high-priority in genomics after the completion of the Human Genome project.
In diploid organisms such as humans, there are two non-identical copies of each autosomal chromosome.
A description of the SNPs in a chromosome is called a haplotype.
At present, it is prohibitively expensive to directly determine the haplotypes of an individual, but it is possible to obtain rather easily the conflated SNP information in the so called genotype.
Computational methods for genotype phasing, i.e., inferring haplotypes from genotype data, have received much attention in recent years as haplotype information leads to increased statistical power of disease association tests.
However, existing algorithms have impractical running time for phasing large genotype datasets such as those generated by the international HapMap project.
In this paper we propose a highly scalable algorithm based on entropy minimization.
Our algorithm is capable of phasing genotype data coming from either unrelated individuals or families consisting of a child and one or both parents.
Experimental results show that our algorithm achieves a phasing accuracy close to that of best existing methods while being several orders of magnitude faster.
After the completion of the Human Genome Project has provided us with a blueprint of the DNA present in each human cell, genomics research is now focusing on the study of DNA variations that occur between individuals and understanding how these variations confer susceptibility to common diseases such as diabetes or cancer.
The most common form of genomic variation are the so called single nucleotide polymorphisms (SNPs), i.e., the presence of different DNA nucleotides, or alleles, at certain chromosomal locations.
Over 9 million common SNPs have already been catalogued in the dbSNP database maintained by NCBI.In diploid organisms such as humans, there are two non-identical copies of each autosomal chromosome, one inherited from the mother and one inherited from the father.
The combinations of SNP alleles in the maternal and paternal chromosomes are referred to as the individual's haplotypes.
Although it is possible to directly determine the haplotypes of an individual by experimental techniques, such methods are prohibitively expensive and time consuming.
In contrast, there are many cost-effective high-throughput techniques for This work was supported in part by NSF CAREER award IIS-0546457 and NSF award DBI-0543365.
Authors address: University of Connecticut, Computer Science & Engineering Department, 371 Fairfield Rd., Storrs, CT 06269-2155, {bogdan,ion}@engr.uconn.edu determining the conflated SNP information called genotype, which specifies the identities of the two alleles at each SNP position, but does not assign the alleles to specific chromosomes for heterozygous SNP positions i.e., SNP positions at which the individual has two different alleles.Since haplotypes determine the exact sequence (and hence function) of proteins encoded by the genes, finding the haplotypes in human populations is an important step in determining the genetic basis of complex diseases.
For this reason, computational inference of haplotypes from genotype date, known as the genotype phasing problem, has received an increasing amount of attention in the literature in the past few years, see, e.g., [1], [2], [3], [4] for recent surveys.
While many of the existing methods achieve high haplotype reconstruction accuracy, their runtime does not scale well with the number of SNPs and the number of genotypes in the sample.
In particular, existing methods are vastly inadequate for handling datasets of the size envisioned to be produced by next generation of genome-wide association studies.
These studies are expected to result in thousands of individual genotypes with 500,000 or more SNPs [5] by leveraging recent advances in genotyping technologies such as the Affymetrix Mapping 500K Array Set [6].
In this paper we propose a highly scalable algorithm based on the entropy minimization principle that has previously been proposed in the context of genotype phasing and haplotype missing data recovery by Halperin and Karp [7].
Unlike the simple greedy algorithm employed in [7], we use a local optimization algorithm, which in practice results in genotype phasings with lower entropy.
After formalizing the problem in Section II, in Section III we describe a simple yet very efficient implementation of this algorithm, and a novel overlapping window approach for handling genotypes with large numbers of SNPs.
We also describe the extension of our algorithm to the case when the input genotypes come from a mixture of unrelated individuals and families consisting of a child and one or both parents.
Phasing related genotypes is likely to gain in importance in future genotyping studies since relationships between genotypes can be exploited to reliably infer haplotype phase for a substantial fraction of the SNPs based on the no-recombination assumption [5].
Finally, in Section IV we present experimental results on large real datasets extracted from the HapMap repository [8] showing that our algorithm achieves a phasing accuracy close to that of best existing methods while being several orders of magnitude faster.
Following the standard practice, in this paper we restrict our attention to bi-allelic SNPs, which form the vast majority of known SNPs.
In this case a haplotype can be represented as a 0/1 vector -typically by representing the most frequent SNP allele as a 0 and the alternate allele as a 1.
A genotype can be viewed as a 0/1/2 vector, where 0 (1) means that both chromosomes contain the 0 (1) allele while 2 means that the two chromosomes contain different alleles.We say that haplotype h is compatible with genotype g if g(i) = h(i) whenever g(i) ∈ {0, 1}.
A pair of haplotypes(h 1 , h 2 ) explains genotype g if h 1 (i) = h 2 (i) = g(i)whenever g(i) ∈ {0, 1}, and h 1 (i) 񮽙 = h 2 (i) whenever g(i) = 2.
For a given pair (h 1 , h 2 ) that explains g we say that h 1 and h 2 are complements with respect to g.A phasing of a set of genotypes G, each of length k, is a function φ : G → {0, 1} k × {0, 1} k , such that, for every g ∈ G, φ(g) is a pair of haplotypes that explain g. For a haplotype h and a phasing φ, the coverage of h under φ, denoted by cov(h, φ), is the number of genotypes g ∈ G such that φ(g) = (h, h 񮽙 ) or φ(g) = (h 񮽙 , h) plus twice the number of of genotypes g ∈ G such that φ(g) = (h, h).
As in [7], we define the entropy of a phasing φ asH(φ) = 񮽙 h:cov(h,φ)񮽙 =0 − cov(h, φ) 2|G| log cov(h, φ) 2|G| (1)The Minimum Entropy Genotype Phasing Problem can then be defined as follows: Given a set of genotypes, find a phasing with minimum entropy.
Halperin and Karp [7] proposed a greedy algorithm for the related minimum-entropy set cover problem, and showed that a variant of this algorithm can be applied to genotype phasing.
However, this algorithm cannot be applied directly to phasing long genotypes, i.e., genotypes with large numbers of SNPs.
Indeed, in this case each haplotype is likely to be compatible with a single genotype, and thus all phasings are likely to have the same entropy of − log 1 2|G| .
Furthermore, even for short genotypes, the greedy algorithm in [7] is producing phasings whose entropy can be further decreased.
In this paper we use the entropy minimization objective of [7] within a local improvement framework.
In Section III-A we describe the local improvement algorithm for phasing short genotypes of unrelated individuals.
Then, in Sections III-B and III-D we describe extensions of the local improvement algorithm to the problem of phasing long genotypes of unrelated, respectively related individuals.
We have implemented a simple local improvement algorithm for entropy minimization.
Our algorithm which we refer to as ENT, starts from a random phasing, then, at each step, finds the genotype whose re-explanation yields the largest decrease in phasing entropy (see Figure 1).
The use of random initial phasings is justified by observing Input: Set G of genotypes Output: Phasing φ of the genotypes in G Generate a random phasing φ for genotypes in G 2.
Repeat forever2.1 Find the pair (g, (h 񮽙 1 , h 񮽙 2 )) such that H(φ 񮽙 ) is minimized, where φ 񮽙 is obtained from φ by re-explaining g with (h 񮽙 1 , h 񮽙 2 ) 2.2 If H(φ 񮽙 ) < H(φ), then φ ← φ 񮽙Else exit the repeat loop 3.
Output φ Fig. 1.
ENT phasing of short genotypes.that a random phasing of a genotype with i heterozygous positions matches the real phasing with probability 2 −i .
E.g., for the Daly children dataset (see Section IV), random phasing results in an average of 46% correct haplotypes over windows of 5 consecutive SNPs.
We have also experimented with a version of the algorithm in which the initial phasing is obtained by running the greedy algorithm of [7].
However, the use of random initial phasings was found to yield convergence to final phasings with lower entropy.If there exists more than one pair (g, (h 1 , h 2 )) with minimum H(φ 񮽙 ) in step 2.1 of the algorithm, then we pick the pair (g, (h 1 , h 2 )) maximizing P rob(h 1 ) × P rob(h 2 ), where P rob(h) is defined as A common approach to phasing long genotypes is to phase small non-overlapping windows of the input genotypes and then stitch together the resulting haplotypes using various statistical approaches.
Recently, Eskin, Sharan, and Halperin [9] proposed a dynamic programming algorithm for selecting a set of tiling windows maximizing a natural maximum likelihood function.
Our algorithm also uses a window-based approach to phasing long genotypes, however, unlike previous approaches, it employs a set of overlapping windows.
Each window consists of a set of l "locked" SNPs, which have been previously phased, and a set of f "free" SNPs, which are currently being phased.
For each window, the phasing algorithm proceeds as described in the previous section, except that only re-explanations consistent with the already determined haplotypes of the locked SNPs are considered in the local improvement step (see Figure 2).
The basic implementation of the ENT algorithm takes l and f as input parameters.
We have also implemented variants of the algorithm that dynamically compute the number of locked, respectively free SNPs based on the input data.
These variants pick l and f as large as possible subject to the constraint that the numbers of ambiguous (heterozygous or missing) SNP genotypes in the locked, respectively free region of the current window do not exceed twice the number of genotypes.
The number of free SNPs f is further constrained to disallow having more than 7 ambiguous SNPs in the free region of any genotype.
Input: Set G of genotypes Output: Phasing φ of the genotypes in G Divide the genotypes in groups of f consecutive SNPs from left to right 2.
For each group, add the preceding l SNPs to create a window of size l + f SNPs (leftmost window has no locked SNPs and is of size f ) 3.
Run the phasing algorithm in Figure 1 for each window, in left to right order, where the haplotypes over the locked l SNPs are not allowed to change 4.
Output the resulting phasing φ When phasing n unrelated genotypes over k SNPs, the algorithm in Figure 1 is run on 񮽙k/f 񮽙 windows.
For each window, the algorithm evaluates at most n × 2 f candidate pairs of haplotypes for finding the best pair in Step 2.1.
Computing the entropy gain for each candidate pair takes constant time.
Indeed, H(φ 񮽙 ) differs from H(φ) in at most four terms corresponding to the haplotypes that can change their coverages, namely the haplotypes explaining g in φ and φ 񮽙 .
Empirically, the number of iterations required in Step 2 of the algorithm in Figure 1 is linear in the number n of genotypes, resulting in an overall runtime of O(n 2 2 f k/f ).
The number of iterations can be reduced to nearly constant by re-expaining multiple genotypes per iteration.
This speedup technique -which results in a runtime that depends nearly linearly on the number of genotypes -will be included in the next implementation of our algorithm, but was not used for obtaining the experimental results in Section IV.
A trio is a nuclear family composed of the two parents plus a child.
In the no-recombination assumption each parent passes one of its chromosomes to the child.
That is, the child shares one haplotype with the mother and the other one with the father.
The no-recombination assumption provides very useful information about phasing all members of a trio.
The only situation when there is phasing ambiguity for a given SNP is when all three genotypes are heterozygous at that SNP.
For example, in the CEU and YRI trio populations of HapMap [8], the phase of only around 15% of the SNPs is ambiguous, while the phase of the remaining 85% of the SNPs can be inferred based on the no-recombination assumption.The ENT algorithm described above can be easily adapted to phase families of related genotypes under the norecombination assumption.
In order to enforce the norecombination assumption, at each local improvement step, we re-explain a whole family, rather than an individual genotype.
The entropy can still be recomputed in constant time after each update by a straightforward extension of the method described in Section III-A.
Our current implementation handles trio genotype data as well as mixtures of independent genotypes, full trios, and partial trios consisting of one parent and one child.
In a first set of experiments we assessed the effect of the windowing strategy (number of free and locked SNPs) on phasing accuracy of the ENT algorithm.
We conducted these experiments on a well-known dataset from Daly et al. [10].
This dataset contains 129 trios from a European population.
Each individual was genotyped at 103 SNP positions in the 5q31 region of chromosome 5.
The trio genotypes were used to infer as much as possible out of the "true" haplotypes of the children under the no-recombination assumption.
We use the following three measures [5] to assess phasing accuracy on the unrelated genotypes of the children in the Daly dataset:Switching error.
Given inferred haplotypes (h, h 񮽙 ) of a genotype g with true haplotypes (t, t 񮽙 ), the number of switches is defined as the number of times one has to switch between h and h 񮽙 to obtain t.
The number of ambiguous SNPs in a genotype g is the number of 2's (heterozygous positions) plus the number of missing SNP genotypes.
The switching error rate (given in percents) for a set G of n genotypes is defined as the ratio between the total number of switches and the total number of ambiguous SNPs minus n, since the maximum number of switches in a genotype is one less than the number of ambiguous SNPs.Haplotype accuracy.
The percentage of haplotypes correctly recovered.SNP accuracy.
The number of correctly phased SNPs as percentage of the total number of SNPs.
Table I reports the switching error obtained by our ENT algorithm with various settings for the number of free and locked SNPs on the Daly dataset.
We varied the number of locked and free SNPs from 1 to 9, and also included in comparison the ENT variants which dynamically choose either one or both l and f as described in Section III-B.
The version that chooses both l and f dynamically yields the smallest switching error, with next best results being obtained by using fixed window sizes with 5 locked and 5 free SNPs.
Table II gives all three accuracy measures for the two best performing variants of ENT, the widely used PHASE [11], and the more recent GERBIL [12] and 2SNP [13] phasing algorithms.
The two ENT variants have slightly worse, yet very close accuracy compared to the other methods.Unfortunately, the methods in [11], [12], [13] do not directly handle trio data.
An extension of PHASE to trio data has been described in [5], however, its runtime does not scale well to very large trio datasets such as those generated in the HapMap project [8].
To test the scalability of the ENT algorithm, in a second set of experiments we used two datasets from HapMap Phase I release 16a, each one consisting of 30 trios.
The first dataset was collected from a population of Utah residents with ancestry from northern and western Europe (CEU), and the other one from a population of Yoruba people of Ibadan, Nigeria (YRI).
As reported on the HapMap website, phasing these datasets using the trio version of the PHASE algorithm [5] requires extensive computational resources (months of CPU time on two clusters with a combined total of 238 nodes) and for this reason the haplotypes can be recomputed only for major releases of the datasets.
In contrast, only a few hours on a 2.8GHz Pentium Xeon computer were required by the ENT variant which dynamically picks the number of locked and free SNPs.
In Table III we report the accuracy of ENT phasing with respect to the results obtained by PHASE.
The accuracy is computed as the number of trio ambiguous SNPs -i.e., positions in which all three members of the trio have ambiguous SNPs -that are differently phased by ENT and PHASE, as percentage of the total number of trio ambiguous SNPs.
Of the approximately 15% of the SNPs that are trio ambiguous, only 7-12% are inferred differently by ENT and PHASE, depending on the population and the chromosome.
Thus, our method results in a SNP genotyping difference of 1-2% with respect to PHASE, while being many orders of magnitude faster.In the above experiments, the ENT algorithm was run on the genotypes inferred from the PHASE haplotypes since the corresponding genotypes (which most likely have missing data) are not available at [8].
In order to test the capacity of our method to recover missing alleles we randomly deleted 1%, 2%, 5%, and 10% of the genotype SNPs and used the genotypes with missing data as input to ENT.
In percentage of deleted data, but is on the average over 97.5% for the CEU population and over 95.8% percent for the YRI population.
In this paper we have presented a highly scalable algorithm for genotype phasing based on entropy minimization.
Experimental results on large datasets extracted from the HapMap repository show that our algorithm is several orders of magnitude faster than existing phasing methods while achieving a phasing accuracy close to that of best existing methods.
The source code of our implementation is available from the authors upon request.
