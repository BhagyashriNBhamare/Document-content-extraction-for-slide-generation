An important, if not very well known, problem that afflicts many web servers is duplicate client browser requests due to server-side problems.
A legitimate request is followed by a redundant request, thus increasing the load on the server and corrupting state at the server end (such as, the hit count for the page) and at the client end (such as, state maintained through a cookie).
This problem has been reported in many developer blogs and has been found to afflict even popular web sites, such as CNN and YouTube.
However, to date, there has not been a scientific, technical solution to this problem that is browser vendor neutral.
In this paper, we provide such a solution which we call GRIFFIN.
We identify that the two root causes of the problem are missing resource at the server end or duplicated Javascripts embedded in the page.
We have the insight that dynamic tracing of the function call sequence creates a signature that can be used to differentiate between legitimate and duplicate requests.
We apply our technique to find unre-ported problems in a large production scientific collaboration web service called HUBzero, which are fixed upon reporting the problems.
Our experiments show an average overhead of 1.29X for tracing the PHP-runtime on HUBzero across 60 unique HTTP transactions.
GRIFFIN has zero false-positives (when run across HTTP transaction of size one and two) and an average detection accuracy of 78% across 60 HTTP transactions.
The affliction of duplicated web requests: A duplicate web request occurs when the client web browser sends two requests for the same web page, the second being a redundant duplicate request.
This affliction does not affect poorly run web sites alone.
It afflicts two of the top 10 most visited sites -CNN and YouTube [15].
Our tests (with Chrome) show that at least 22 out of top 98 (on April 4, 2014) globally ranked Alexa [1] web sites give a duplicate request on accessing their homepages.
On the academic side, we found that it affects HUBzero, a widely used open source software platform (originating from Purdue) for building powerful Web sites that support scientific discovery, learning, and collaboration [14].
Why do duplicate web requests happen?
There are two root causes for the problem of duplicate web requests, which have been separately pointed out in many developer forums and blog posts [3,4,17].
The first cause is the incorrect way in which browsers handle missing component names, or empty tags, such as, <img src="">, <script src="">, and <link href="">.
Equivalently, this could be caused by JavaScript which dynamically sets the src property on either a newly created image or an existing one: The most readable and comprehensive treatment of this first cause can be found in [3].
We will refer to this first root cause as missing resource cause.
The second cause is the same Javascript being included in the page twice, or more number of times [15].
This is the root cause behind the duplicate web requests in CNN and YouTube.
Two main factors increase the odds of a script being duplicated in a single web page: team size and number of scripts.
It takes a significant amount of resources to develop a web site, especially if it is a top destination.
In addition to the core team building the site, other teams contribute to the HTML in the page for things such as advertising, branding, and data feeds.
With so many people from different teams adding HTML to the page, it is easy to imagine how the same script could be added twice, e.g., CNN and YouTube's main pages have 11 and 7 scripts respectively.
We will refer to this second root cause as duplicate script cause.
How to fix the problem?
The "missing resource cause" happens because the HTML specification, version 4 [5] 1 is silent on this seemingly esoteric aspect.
Even though the specification indicates that the src attribute should contain a Uniform Resource Identifier (URI), it fails to define the behavior when src does not contain a URI.
Consequently, different browsers behave in different ways.
For example, Internet Explorer (IE) sends the duplicate request to the directory of the page rather than the page itself, while Firefox and Chrome send the duplicate request to the page itself.
Further, the behavior of different browsers for handling different missing resources is different, e.g., IE does not initiate a duplicate request with missing script while Firefox and Chrome do.
The overall approach to handling this could be to write server-side code that will catch a similar request arising close in time to the original request and correlated with finding a missing URI in a tag.
However, due to the differences in browser behaviors and for different tags, this would lead to ungainly code, with case statements for a large number of different cases.
An indirect evidence comes from the fact that though this problem has been known for a while (since at least 2009), this solution is seldom deployed.
The "duplicate script cause" of course has no easy solution available currently.
The solution is mainly process-based -enabling better communication and coordination between developers writing or using scripts to create web pages.
Our solution approach: In this paper, we present a general-purpose solution to the above problem, in a system called GRIFFIN 2 .
By "general-purpose", we mean that the solution applies unmodified to all kinds of resources and browsers.
The solution has at its heart the observation that the duplicate web requests cause a repeated signal, for some definition of "signal".
The signal should be defined such that it can be easily traced in a production web server, without impacting computation or storage resources and without needing specialized code insertion.
We find that the function call depth is the signal that satisfies these conditions, while preserving enough fidelity that the repeated sequence can be easily and automatically discerned.
To automatically discern the repeated pattern, we use the simple-to-calculate autocorrelation function for the signal and at a lag, equal to the size of the web request (in terms of number of HTTP commands), GRIFFIN sees a spike in autocorrelation which it uses to flag the detection.When tested over a wide range of buggy and nonbuggy behavior, we find that GRIFFIN performs well with respect to both the detection and the false positive.
We find that GRIFFIN has no false positive and an 80% detection accuracy.
To make GRIFFIN feasible in real production settings, we adopt a mix of synchronous and asynchronous approaches, both without modifying the application's source code, or even needing access to the Figure 1: Duplicate bug-manifestation (with missing images) before and after the fix source code.
Synchronously we capture the call stack depth, using a built-in functionality, in the tracing tool called SYSTEMTAP.
Then, asynchronously, GRIFFIN calculates the autocorrelation function for various lags, filters the values, and flags a detection when the value exceeds a threshold.
In addition to detection, GRIFFIN also provides some diagnostic insight, i.e., gives an idea of the module where the root cause lies.
<span style:background=SPACE,EMPTY> resulted in a duplicate request for both browsers.
For Firefox, <img src=SPACE>, <script src=SPACE,EMPTY>, and <link href=SPACE> created duplicate requests.
These injections provide evidence that browsers do behave differently and erroneously under unexpected special characters for URIs.
Here we detail the design of GRIFFIN to detect duplicate web requests.
At a high level, it comprises three steps: model application behavior at the web server (in terms of the function calls and returns), create a signal of the function call depths, and compute the auto-correlation of the signal to trigger detection.
Figure 2 shows these steps in GRIFFIN.
We leverage SYSTEMTAP [12], a tracing/probing framework that can provide synchronous tracing data on Linux hosts.
To enable tracing, SYSTEMTAP allows to write probe-point scripts.
Probe-point scripts tell SYSTEM-TAP two things.
(1).
What event do you want to trace?
(2).
What do you want to print at the traced event-location?
.
GRIFFIN logs both function-entry and function-return events and prints timestamp, thread-id, function call depth, funcation name, file name, line number, and class name, if available.
Further tracing implementation details are available in [7].
For modeling purposes, we define a numeric metric called function call-depth that represents the runtime function call-depth.
At every function-call, the calldepth is incremented and at every return, it is decremented.
Our foundational intuition for modeling application behavior is that the flow of an application can be roughly represented by how function call depth changes.
The function call depth sequence for a given high-level web operation can be considered as a fingerprint of the high-level operation.
For further exploration of this intuition, let us first define some terms: web-request, webclick, http-transaction.
Starting from the lowest level, a web request is the HTTP request sent by the web browser, such as, GET and POST.
A web click is a human user clicking in the browser to send web requests.
A single web click can generate multiple web requests.
A set of web clicks done in a particular sequence, as permitted by the workflow in the website, is called an http transaction.
An http transaction can consist of one or more web clicks; in typical usage this will be more than one web click.
An example of an http-transaction of size two is going to the homepage followed by going to the login page (HomePageâ†’Login).
Now coming back to our intuition for detecting duplicate web requests, consider that a duplicate web request will create a duplicated signal of the function call depths.
It is easy to concoct a synthetic example where this intuition is violated.
For example, consider two legitimate consecutive web clicks and the corresponding web requests:(a (b (c c') b') a') (d (e (f f') e') d')giving a call-depth sequences of (1 2 3 3 2 1) (1 2 3 3 2 1).
This would give the appearance to GRIFFIN of duplicated web requests.
However, we find that for real web pages, the length of web clicks in terms of the number of function calls and returns tends to be much larger.
This kind of accidental matching of the function call depth signal happens only very rarely for these real situations.To get the call-depth at runtime, we add a function called thread indent depth(long) to SYSTEMTAP 's native scripts.
This function returns a number corresponding to the depth of nesting.
We call this function thread indent depth(1) in the probe-point SYS-TEMTAP script.
Here, the argument one means that at every function-call, increment the depth by one.
We submitted this function to the SYSTEMTAP repository and it has been merged into SYSTEMTAP 's master-branch and is available out-of-the-box after SYSTEMTAP is installed [6].
With the function call-depth sequence captured, the next goal is to detect whether the sequence has a repetitive pattern and to do this efficiently with respect to time.To do this, we use a common signal analysis technique to detect repeating patterns, auto-correlation [19] of the function call-depth signal.
Auto-correlation of a signal x is defined by R xx (Equation 1) as a function of lag-value t, where t varies from zero (perfect signal match with R xx =1) to n, the sequence length in terms of the number of function calls and exits.
Ideally for GRIFFIN to detect duplicate web requests resulting from a single user web click, it would be possible to segment the web requests for each web click.
But that is not always possible in practice, as we discuss in [7].
Auto-correlation can be viewed as a sequences of shift, multiply, sum operations for all lag values on function call-depth signal.
Intuitively, we are using auto-correlation to estimate the similarity between the signal and its time shifted versions for various values of the time shift.
If the function-depth signal is exactly repeated twice, we expect to see a peak of 0.5 around the lag value of n/2.
R xx [t] = C t C 0where t=0,. . . ,nC t = 1 n min(nâˆ’t,n) âˆ‘ s=max(1,âˆ’t) [X s+t âˆ’ Â¯ X][X s âˆ’ Â¯ X](1)After auto-correlation computation for all lag-values, we find the index at which the auto-correlation first becomes negative, call this t 0 .
For values of autocorrelation beyond t 0 , we find if there is any value greater than a threshold value Ï„.
If yes, we flag a duplicatedetection.
For the duplication of a set of web requests once, we expect ideally an auto correlation peak of 0.5.
But to tolerate the normal variation in function call-depth signal, we set the threshold Ï„ to be a little lower than 0.5.
We report on our sensitivity empirical study in Section 5.3.
The reason for starting the search beyond t 0 is that then we eliminate the high values of autocorrelation that we will see due to the original signal being correlated with itself with small time lags.
The pseudocode for GRIFFIN's detection algorithm is available in [7].
We envision GRIFFIN to work in two scenarios, preproduction testing and in-production.
In testing, developer's have control of the environment and trace segmentation is not an issue.
Here, a possible concern by developers could be GRIFFIN's detection latency, which is in order of seconds.
For in-production mode, operators' main concern could be the overhead of configuring and tuning GRIFFIN and the application tracing overhead, which is incurred in the critical path of all web requests and responses.
GRIFFIN's configuration is minimal with only one threshold parameter for which we provide a recommendation (threshold=0.4) with our sensitivity analysis.
To further minimize the tracing overhead, an operator can run GRIFFIN in time intervals of low load on the web server .
NEEShub infrastructure is running Apache/2.2.16 (Debian) web server in Prefork MPM (Multi-Processing Module) [2] mode, i.e., with multiple processes and one thread per process, on a VM with Intel(R) Xeon(R) CPU E5-2643 0 @ 3.30GHz with 6GB RAM.
The PHPruntime (libphp5.so) version is 5.3.3 and is compiled with --enable-dtrace option in order for SYSTEM-TAP (ver 2.4) to be able to intercept PHP-function calls and returns with its probes.
We evaluate GRIFFIN's detection performance with traditional definitions of accuracy and precision.
Accuracy is defined as the percentage of true positives and true negatives.
Precision is defined as the percentage of true positives out of all detections.
We establish the ground truth through manual verification, at client-end, by checking duplicate requests for each web-click using browser debugging tools, Firebug and Chrome-dev-tools.
We measure the overhead of GRIFFIN in two areas, tracing overhead and detection overhead.
Tracing-overhead is the fraction of total time, taken by SYSTEMTAP 's probes while processing a given web-click.
Detection overhead or detection latency is measured in the standard way as the time elapsed for all the detection steps.
GRIFFIN's testing was conducted on a replica of the production site (www.nees.org), technically referred to as a "staging machine" where developers merge their code after doing the unit testing on their own development box.
We made no modifications or synthetic error injections.
Therefore, we expected to find few, if any, problems with the website.
We tested GRIFFIN's duplicate-detection performance by sending a total of 60 HTTP transactions of varying sizes.
The size of a transaction is measured by the number of web clicks incorporated within the transaction.
Thus, the transaction HomePageâ†’Login has a size of two.
Also, for the analysis (autocorrelation computation), the signal is considered the entire transaction.
We used 20 transactions for each of the sizes 1,2,3.
These 60 HTTP transactions were executed following different possible user workflows as enabled by the web portal.
We tried to cover all the workflows that a typical user would follow while visiting the website.Ideally, the analysis in GRIFFIN will consider the traces corresponding to a single web click from a single user.
Within a single user, we expect that different web clicks are handled by threads of different IDs.
We empirically validated that this is always the case for all our transactions.
Out of the 7 duplicate request problems (among the 60 HTTP transactions), GRIFFIN was able to correctly find 4 duplicated requests i.e., HomePage, Topics-page, SimulationWiki-page and Wiki-page.
SimulationWiki page was due to a Javascript-based duplication, while the other three were due to missing-resources.
GRIF-FIN missed 3 cases of duplicated requests, warehouse, simulation and education pages.GRIFFIN's accuracy and precision with different HTTP transaction sizes is presented in Table 1.
GRIF-FIN provides an average accuracy of 80% across HTTP transactions of size one and two with no false positives.
With three web clicks, GRIFFIN's performance degrades-here 0% precision is misleading in the sense that out of the 20 HTTP transactions of size three, only one (HOMEPAGEâ†’LOGINâ†’LOGGINGIN (Fig- ure 3)) had a duplicate request which GRIFFIN did not detect.
GRIFFIN falsely flagged 4 out of 20 transactions giving a false positive rate of 20% for HTTP transactions of size three.
The reason why GRIFFIN did not detect HOMEPAGE web-click within HOMEPAGEâ†’LOGINâ†’LOGGINGIN transaction is due to the significant difference of LOGGGINGIN function call-depth signal from the signals of HOMEPAGE With HTTP transaction of size 3, GRIF-FIN is performing its analysis after combining these three signals into one.
Thus, the divergence in the single combined signal means that the autocorrelation values, even with one duplication, tend to be low, and stay below the threshold.
In practice, the HTTP transactions of size 3 will be very rare because of the discrimination that GRIF-FIN will be able to do using the thread ID [7].
Table 1: Summary of Performance results With the ideal (and practically common) case of analysis over HTTP transaction of size 1, GRIFFIN shows 90% accuracy and 100% precision.
As an example, the function call-depth and autocorrelation for HOME webtransaction is presented in Figure 2.
We see that the autocorrelation has a clear peak value of 0.4998 near a lagvalue of 40,000 which is detected by GRIFFIN (with a threshold set at 0.4).
Manual checking, both at user-end and at server-end revealed that HOME web-request ("/") is being sent twice by the user's browser.
Further inspection on the server revealed that a field called hits in the back-end database is incremented on every HOME webtransaction.
We reported this hitherto unknown problem to the web developer at NEES, and it was subsequently fixed and not pushed into the production environment.
Testing GRIFFIN with HTTP transactions of size 2, we observe a drop in accuracy (to 70%).
This happens due to the significant variability in the basic signal due to the very different nature of the function call invocations in the two web clicks.
Expectedly, autocorrelating a divergent signal gives low autocorrlation values, which sometime fall below the GRIFFIN threshold (0.4).
Figure 4.
We set GRIFFIN threshold to 0.4 as the default value for GRIF-FIN to provide us zero false positives, i.e., 100% precision.
The user can decrease the threshold for fine tuning her system, but we suggest to not go below 0.35 (based on Figure 4b) as that can result in possible false positives.The detection latency as a function of the sequence length (i.e., the number of trace events due to SYSTEM-TAP probes) shows the expected behavior of greater latency with increasing sequence length [7].
This is due to a larger number of autocorrelation computations for a longer trace length.
However, the upper range of the sequence length is typically about 100K and with that we have a detection latency of about half a minute, which should be fast enough to be useful for the subsequent manual process of fixing the problem.
The average tracing overhead across the 60 tested HTTP transactions is 28.6% with a standard devitation of 10.0%.
The overhead for HTTP transactions for each size is presented in Table 2.
The tracing overhead is independent of the length of the sequence and the differences seen are due to statistical variations.
When GRIFFIN detects duplicate web-requests, a diagnostic-context about the detection would help the developers as a starting point for debugging.
At detectiontime, in addition to the autocorrelation value, we also have the lag when this autocorrelation value exceeded the threshold, call this t max .
We use t max alongwith the information provided by an additional SYSTEMTAP probe that records the HTTP-request going from apache-core to PHP-runtime, to provide the diagnostic-context.
With the t max , we get the nearest next fired apache-core to PHP event.
We then extract a high-level component (module name) from the file name.
For the duplicate bug of Figure 1, this simple scheme is able to correctly flag mod fpss module in Joomla, the Content Management System, on which HUBzero is built.
Most of the existing approaches to handle duplicate requests are not at the application-level.
TCP [9] is the classic example that uses sequence numbers along with a windowing-based mechanism to do duplicate detection of IP packets.
Stateless protocols like HTTP have to deal with the request-response nature and maintain state at the application-level.
Application-level works include similarity detection [16] deployed at web-proxy caches to eliminate redundant network traffic, duplicate-content detection [18] with clustering and similarity metrics [11].
These are directed at generic payloads and are therefore less accurate than GRIFFIN in general.Finding relevant system events to detect and diagnose failures is often equated to the problem of finding a needle in a haystack.
Over the last decade, several researchers have proposed solutions to this challenging problem [10,20,8,13].
The high-level objective here is to mine vast amounts of system data to find relevant signatures for failures.
Our work falls within this broad umbrella.
We automate the process of detecting duplicated web requests by looking at a compressed signal from system events, specifically function calls and returns.
In this paper, we have presented a systematic method and an automated tool called GRIFFIN for detecting an important problem that afflicts many web servers, namely, duplicate client browser requests.
This causes an artificially high load on servers and corrupts server and client state.
Culling together many blog posts and developer forum reports, we identify the two fundamental root causes of the problem and come up with a solution that handles both, without needing special case logic for the two root causes or for different browsers.
We use GRIF-FIN for detecting the problem in a production web portal for an NSF center at Purdue and identify that the problem is more widespread than previously identified.
Our evaluation on the production site revealed no false positive.
The dynamic system tracing using SYSTEMTAP is lightweight and the detection latency small enough (less than half a minute) as to be useful in practice.
Our contributions were considered significant enough that the problem was fixed in the web portal and our addition to the dynamic tracing facility was accepted in its official release.
