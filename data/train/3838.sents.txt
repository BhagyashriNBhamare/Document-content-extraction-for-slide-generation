Function identification is a fundamental challenge in reverse engineering and binary program analysis.
For instance , binary rewriting and control flow integrity rely on accurate function detection and identification in binaries.
Although many binary program analyses assume functions can be identified a priori, identifying functions in stripped binaries remains a challenge.
In this paper, we propose BYTEWEIGHT, a new automatic function identification algorithm.
Our approach automatically learns key features for recognizing functions and can therefore easily be adapted to different platforms, new compilers, and new optimizations.
We evaluated our tool against three well-known tools that feature function identification: IDA, BAP, and Dyninst.
Our data set consists of 2,200 binaries created with three different compilers, with four different optimization levels , and across two different operating systems.
In our experiments with 2,200 binaries, we found that BYTE-WEIGHT missed 44,621 functions in comparison with the 266,672 functions missed by the industry-leading tool IDA.
Furthermore, while IDA misidentified 459,247 functions , BYTEWEIGHT misidentified only 43,992 functions.
Binary analysis is an essential security capability with extensive applications, including protecting binaries with control flow integrity (CFI) [1], extracting binary code sequences from malware [9], and hot patching vulnerabilities [25].
Research interest in binary analysis shows no sign of waning.
In 2013 alone, several papers such as CFI for COTS [34] (referred to as COTS-CFI in this paper), the Rendezvous search engine for binaries [21], and the Phoenix decompiler [28] focus on developing new binary analysis techniques.Function identification is a preliminary and necessary step in many binary analysis techniques and applications.
For example, one property of CFI is to constrain interfunction control flow to valid paths.
In order to reason about such paths, however, binary-only CFI infrastructures need to be able to identify functions accurately.
In particular, COTS-CFI [34], CCFIR [33], MoCFI [12], Abadi et al. [1], and extensions like XFI [15] all depend on accurate function identification to be effective.CFI is not the only consumer of binary-level function identification techniques.
For example, Rendezvous [21] is a search engine that operates at the granularity of function binaries; incorrect function identification can therefore result in incomplete or even incorrect search results.
Decompilers such as Phoenix [28], Boomerang [32], and Hex-Rays [18] recover high-level source code from binary code.
Naturally, decompilation occurs on only those functions that have been identified in the input binary.Given the foundational impact of accurate function identification in so many security applications, is this problem easy and can thus be regarded as "solved"?
Interestingly, recent security research papers seem to have conflicting opinions on this issue.
On one side, Kruegel et al. argued in 2004 that function start identification can be solved "very well" [23, §4.1] in regular binaries and even some obfuscated ones.
On the other side, Perkins et al. described static function start identification as "a complex task in a stripped x86 executable" [25, §2.2.3] and therefore applied a dynamic approach in their ClearView system.
A similar opinion is also shared by Zhang et al., who stated that "it is difficult to identify all function boundaries" [34, §3.2] and used a set of heuristics for this task.So how good are the current tools at identifying functions from stripped, non-malicious binaries?
To find out, we collected a dataset of 2,200 Linux and Windows binaries generated by GNU gcc, Intel icc, and Microsoft Visual Studio (VS) with multiple optimization levels.
We then use our dataset to evaluate the most recent release of three popular off-the-shelf solutions for function identification: (i) IDA (v6.5 at submission), used in CodeSurfer/x86 [2], Choi et al.'s work on statically determining binary similarity [11], BinDiff [4], and BinNavi [5]; (ii) the CMU Binary Analysis Platform (BAP, v0.7), used in the Phoenix decompiler [28] and the vulnerability analysis tool Mayhem [10]; and (iii) the unstrip utility in Dyninst (dated 2012-11-30), used in BinSlayer [7], Sharif et al.'s work on dynamic malware analysis [29], and Sidiroglou et al.'s work on software recovery navigation [30].
Our finding is that while IDA performs better than BAP and Dyninst on our dataset, its result can still be quite alarming-in our experiment, IDA returned 521,648 true positives (41.81%), 266,672 false negatives (21.38%), and 459,247 false positives (36.81%).
While there is no doubt that such failures can have a negative impact on downstream security analyses, a real issue is in setting the right expectation on the subject within the security research community.
If there is a publicly-available function identification solution where both its mechanism and limitations are well-understood by researchers, then researchers may be come up with creative strategies to cope with the limitations in their own projects.
The goal of this paper is to explain our process of developing such a solution and to establish its quality through evaluating it against the aforementioned solutions.We draw inspirations from how BAP and Dyninst perform function identification since their source code is available.
Both solutions rely on fixed, manually-curated signatures.
Dyninst, at the version we tested, uses the byte signature 0x55 (push %ebp in assembly) to recognize function starts in ELF x86 binaries [14].
BAP v0.7 uses a more complex signature, but it is also manually generated.
Unfortunately, the process of manually generating such signatures do not scale well.
For example, each new compiler release may introduce new idioms that require new signatures to capture.
The myriad of different optimization settings, such as omit frame pointers, may also demand even more signatures.
Clearly, we cannot expect to manually catch up.One approach to recognizing functions is to automatically learn key features and patterns.
For example, seminal work by Rosenblum et al. proposed binary function start identification as a supervised machine learning classification problem [27].
They model function start identification as a Conditional Random Field (CRF) in which binary offsets and a number of selected idioms (patterns) appear in the CRF.
Since standard inference methods for CRF on large, highly-connected graphs are expensive, Rosenblum et al. adopted feature selection and approximate inference to speed up their model.
However, using hardware available in 2008, they needed 150 computedays just for the feature selection phase on 1,171 binaries.In this paper, we propose a new automated analysis for inferring functions and implemented it in our BYTE-WEIGHT system.
A key aspect of BYTEWEIGHT is the ability to learn signatures for new compilers and optimizations at least one order of magnitude faster than as reported by Rosenblum et al. [27], even after generously accounting for CPU speed increase since 2008.
In particular, we avoid using CRFs and feature selection, and instead opt for a simpler model based on learning prefix trees.
Our simpler model is scalable using current computing hardware: we finish training 2,064 binaries in under 587 compute-hours.
BYTEWEIGHT also does not require compiler information of testing binaries, which makes the tool more powerful in practice.
In the interest of open science, we also make our tools and datasets available to seed future improvements.At a high level, we learn signatures for function starts using a weighted prefix tree, and recognize function starts by matching binary fragments with the signatures.
Each node in the tree corresponds to either a byte or an instruction, with the path from the root node to any given node representing a possible sequence of bytes or instructions.
The weights, which can be learned with a single linear pass over the data set, express the confidence that a sequence of bytes or instructions corresponds to a function start.
After function start identification, we then use value set analysis (VSA) [2] with an incremental control flow recovery algorithm to find function bodies with instructions, and extract function boundaries.To evaluate our techniques, we perform a large-scale experiment and provide empirical numbers on how well these tools work in practice.
Based on 2,200 binaries across operating systems, compilers and optimization options, our results show that BYTEWEIGHT has a precision and recall of 97.30% and 97.44% respectively for function start identification.
BYTEWEIGHT also has a precision and recall of 92.84% and 92.96% for function boundary identification.
Our tool is adaptive for varying compilers and therefore more general than current pattern matching methods.Contributions.
This paper makes the following contributions:• We enumerate the challenges we faced and implement a new function start identification algorithm based on prefix trees.
Our approach is automatic and does not require a priori compiler information (see §4).
Our approach models the function start identification problem in a novel way that makes it amenable to much faster learning algorithms.
• We evaluate our method on a large test suite across operating systems, compilers, and compiling optimizations.
Our model achieves better accuracy than previously available tools.
• We make our test infrastructure, data set, implementation, and results public in an effort to promote open science (see §5).
Figure 1: Example C Code.
IDA fails to identify functions sum, sub, and assign in the compiled binary.
We start with a simple example written in C, shown in Figure 1.
In this program, three functions are stored as function pointers in the array funcs.
When the program is run, input from the user dictates which function gets called, as well as the function arguments.
We compiled this example code on Linux Debian 7.2 x86-64 using gcc with -O3, and stripped the binary using the command strip.
We then used IDA to disassemble the binary and perform function identification.
Many security tools use IDA in this way as a first step before performing additional analysis [9,20,24].
Unfortunately, for our example program IDA failed to identify the functions sum, sub, and assign.
IDA's failure to identify these three critical functions has significant implications for security analyses that rely on accurate function boundary identification.
Recall that the CFI security policy dictates that runtime execution must follow a path of the static control flow graph (CFG).
In this case, when the CFG is recovered by first identifying functions using IDA, any call to sum, sub, or assign would be incorrectly disallowed, breaking legitimate program behavior.
Indeed, any indirect jump to an unidentified or mis-identified function will be blocked by CFI.
The greater the number of functions missed, the more legitimate software functionality incorrectly lost.
Secondly, suppose we are checking code for potential security-critical bugs.
In our sample program, the function assign is vulnerable to a buffer overflow attack, but is not identified by IDA as a function.
For tools like ClearView [25] that operate on binaries at the function level, missing functions can mean missing vulnerabilities.In our analysis of 1,171 binaries, we observed that that IDA failed to identify 266,672 functions.
BYTE-WEIGHT improves on this number, missing only 44,621.
BYTEWEIGHT also makes fewer mistakes, incorrectly identifying functions 43,992 times compared to 459,247 with IDA.
While these results are not perfect, they demonstrate that our automated machine learning approach can outperform years of manual hand-tuning that has gone into IDA.
The goal of function identification is to faithfully determine the set of functions that exist in binary code.
Determining what functions exist and which bytes belong to which functions is trivial if debug information is present.
For example, "unstripped" Linux binaries contain a symbol table that maps function names to locations in a binary, and Microsoft program database (PDB) information contains similar information for Windows binaries.
We start with notation to make our problem definition precise and then formally define three function identification problems.
We then describe several challenges to any approach or algorithm that addresses the function identification problems.
In subsequent sections we provide our approach.
A binary program is divided into a number of sections.
Each section is given a type, such as code, data, read-only data, and so on.
In this paper we only consider executable code, which we treat as a binary string.Let B denote a binary string.
For concreteness, think of this as a binary string from the .
text section in a Linux executable.
Let B[i] denote the i th byte of a binary string, and B[i : i + j] refer to the list of contiguous bytesB[i], B[i + 1],...,B[i + j − 1].
Thus, B[i : i + j] is j-bytes long (with j ≥ 0).
Each byte in an executable is associated with an address.
The address of byte i is calculated with respect to a fixed section offset, i.e., if the section offset is ω, the address of byte i is i + ω.
For convenience, we omit the offset, and refer to i as the i th address.
Since the real address can always be calculated by adding the fixed offset, this can be done without loss of generality.A function F i in a binary B is a list of addresses corresponding to statements in either a function from the original compiled language or a function introduced directly by the compiler, denoted asF = {B[i], B[ j],...,B[k]}Note that function bytes need not be a set of contiguous addresses.
We elaborate in §3.3 on real optimizations that result in high-level functions being compiled to a set of non-contiguous intervals of instructions.Towards our goal of determining which bytes of a binary belong to which functions, we define the set of functions in a binaryFUNCS(B) = {F 1 , F 2 ,...,F k }.
Note that functions may share bytes, i.e., it may be that We do not restrict ourselves to a specific oracle implementation, as realizable oracles may vary across operating system and compiler.
For example, the boundary oracle can be implemented by retaining debug information for Windows or Linux binaries.
The function oracle can be implemented by instrumenting a compiler to output a list of instruction addresses included in each compiled function.F 1 ∩ F 2 � = / 0 With the above definitions, we are now ready to state our problem definitions.
We start with the least powerful identification (function start) and build up to the most difficult one (entire function).
Definition 3.1.
The Function Start Identification (FSI) problem is to output the complete list of function starts {s 1 , s 2 ,..., s k } given a binary B compiled from a source with k functions.Suppose there is an algorithm A FSI (B) for the FSI problem which outputs S = {s 1 , s 2 ,...,s k }.
Then:• The set of true positives, TP, is S ∩ O start (B).
• The set of false positives, FP, is S − O start (B).
• The set of false negatives, FN, isO start (B) − S.We also define precision and recall.
Roughly speaking, precision reflects the number of times an identified function start is really a function start.
A high precision means that most identified functions are indeed functions, whereas a low precision means that some sequences are incorrectly identified as functions.
Recall is the measurement describing how many functions were identified within a binary.
A high recall means an algorithm detected most functions, whereas a low recall means most functions were missed.
Mathematically, they can be expressed as A more difficult problem is to identify both the start and end addresses for a function:Definition 3.2.
The Function Boundary Identification (FBI) problem is to identify the start and end bytes (s i , e i ) for each function i in a binary, i.e., S = {(s 1 , e 1 ), (s 2 , e 2 ),...,(s k , e k )}, given a binary B compiled from a source with k identified functions.Suppose there is an algorithm A FBI (B) for the FBI problem which outputs S = {(s 1 , e 1 ), (s 2 , e 2 ),...,(s k , e k )}.
We then define true positives, false positives, and false negatives similarly to above with the additional requirement that both the start and end addresses must match the output of the boundary oracle, i.e., for oracle output (s gt , e gt ) and algorithm output (s A , e A ), a positive match requires s gt = s A and e gt = e A .
A false negative occurs if either the start or end address is wrong.
Precision and recall are defined analogously to the FSI problem.Finally, we define the general function identification problem:Definition 3.3.
The Function Identification (FI) problem is to output a set {F 1 , F 2 ,...,F k } where each F i is a listof bytes corresponding to high-level function i given a binary B with k identified functions.We define true positives, false positives, false negatives, precision, and recall for the FI problem in the same ways as FSI and FBI but add the requirement that all bytes of a function must be matched between agorithm and oracle.The above problem definitions form a natural hierarchy, where function start identification is the easiest and full function identification is the most difficult.
For example, an algorithm A FBI for function boundaries can solve the function start problem by returning the start element of each tuple.
Similarly, an algorithm for the function identification problem needs only return the maximum and minimum element to solve the function boundary identification problem.
Identifying functions in binary code is made difficult by optimizing compilers, which can manipulate functions in unexpected ways.
In this section we highlight several challenges posed by the behavior of optimizing compilers.
Functions may be non-contiguous.
Functions may have gaps.
The gaps can be jump tables, data, or even instructions for completely different functions [26].
As noted by Harris and Miller [19], function sharing code can also lead to non-contiguous functions.
Functions may not be reachable.
A function may be dead code and never called, but nonetheless appear in the binary.
Recognizing such functions is still important in many security scenarios.
For example, suppose two malware samples both contain a unique, identifying, yet uncalled function.
Then the two malware samples are likely related even though the function is never called.One consequence of this is that techniques based solely on recursive disassembling from program start are not well-suited to solve the function identification problem.
A recursive disassembler only disassembles bytes that occur along some control flow path, and thus by definition will miss functions that are not called.Unreachability may occur for several reasons, including compiler optimizations.
For example, Figure 4 shows a function for computing factorials called fac.
When compiled by gcc -O3, the result of the call to fac is precomputed and inlined.
Although the code of fac appears, it is never called in the binary code.Security policies such as CFI and XFI must be aware of all low-level functions, not just those in the original code.
The compiled binary has both symbol table entries.
Unlike shared code for two functions that were originally separate, the compiler here has introduced shared code via multiple entries as an optimization.
Identifying both functions is necessary in many security scenarios, e.g., CFI needs to identify each function entry point for safety, and realize that both are possible targets.
More generally, any binary rewriting for protection (e.g., memory safety, control safety, etc.) would need to reason about both entry points.Functions may be removed.
Functions can be removed by function inlining, especially small functions.
Compilers perform function-inlining to reduce function call overhead and expose more optimization opportunities.
For example, the function utimens_symlink is inlined into the function copy_internal when compiled by gcc with -O2.
The source code and assembly code are shown in Figure 6.
Note that function inlining does not have to be explicitly declared with inline annotation in source code.
Many compilers inline functions by default unless explicitly disabled with options such as -fno-deault-inline [17].
This indicates that for those binary analysis techniques which need function information, even though source code is accessible, a robust function identification technique should still operate on the program binary.
If using source code, function identification may be less precise due to functions that are inlined during compilation.Each compilation is different.
Binary code is not only heavily influenced by the compiler but also the compiler version and specific optimizations employed.
For example, icc does not pre-compute the result of fac in Fig- ure 4, but gcc does.
Even different versions of a compiler may change code.
For example, traditionally gcc (e.g., version 3) would only omit the use of the frame pointer register %ebp when given the -fomit-frame-pointer option.
Recent versions of gcc (such as version 4.2), however, opportunistically omit the frame pointer when compiled with -O1 and -O2.
As a result several tools that identified functions by scanning for push %ebp break.
For example, Dyninst, used for instrumentation in several security projects, relies on this heuristic to identify functions and breaks on recent versions of gcc.
In this section, we detail the design and algorithms used by BYTEWEIGHT to solve the function identification problems.
We first start with the FSI problem, and then move to the more general function identification problem.We cast FSI as a machine learning classification problem where the goal is to label each byte of a binary as either a function start or not.
We use machine learning to automatically generate literal patterns so that BYTE-WEIGHT can handle new compilers and new optimizations without relying on manually generated patterns or heuristics.
Our algorithm works with both byte sequences and disassembled instruction sequences.Our overall system is shown in Figure 7.
Like any classification problem, we have a training phase followed by a classification phase.
During training, we first compile a reference corpus of source code to produce binaries where the start addresses are known.
At a high level, our algorithm creates a weighted prefix tree of known function start byte or instruction sequences.
We weight vertices in the prefix tree by computing the ratio of true positives to the sum of true and false positives for each sequence in the reference data set.
We have designed and implemented two variations of BYTEWEIGHT: one working with raw bytes and one with normalized disassembled instructions.
Both use the same overall algorithm and data structures.
We show in our evaluation that the normalization approach provides higher precision and recall, and costs less time (experiment 5.2).
In the classification phase, we use the weighted prefix tree to determine whether a given sequence of bytes or1 int fac(int x) 2 { 3if (x == 1) return 1; 4else return x * fac(x -1);5 } 6 7void main(int argc, char **argv) 8 { 9printf("%d", fac (10) instructions corresponds to a function start.
We say that a sequence corresponds to a function start if the corresponding terminal node in the prefix tree has a weight value larger than the threshold t.
In the case where the sequence exactly matches a path in the prefix tree, the terminal node is the final node in this path.
If the sequence does not exactly match a path in the tree, the terminal node is the last matched node in the sequence.Once we identify function starts, we infer the remaining bytes (and instructions) that belong to a function using a CFG recovery algorithm.
The algorithm incrementally determines the CFG using a variant of VSA [2].
If an indirect jump depends on the value of a register, then we over-approximate a solution to the function identification problem by adding edges that correspond to locations approximated using VSA.
The input to the learning phase is a corpus of training binaries T, and a maximum sequence length � > 0. �
serves as a bound on the maximum tree height.In BYTEWEIGHT, we first generate the oracle O bound by compiling known source using a variety of optimization levels while retaining debug information.
The debug information gives us the required (s i , e i ) pair for each function i in the binary.In this paper, we consider two possibilities: learning over raw bytes and learning over normalized instructions.
We refer to both raw bytes and instructions as a sequence of elements.
The sequence length � determines how many raw sequential bytes or instructions we consider for training.Step 1: Extract first � elements for each function (Ex- traction Step 2: Generate a prefix tree (Tree Generation).
In step 2, we generate a prefix tree from the extracted sequences to represent all possible function start sequences up to � elements.A prefix tree, also called a trie, is a data structure enabling efficient information retrieval.
In the tree, each non-root node has an associated byte or instruction.
The sequence for a node n is represented by the elements that appear on the path from the root to n. Note that the tree represents all strings up to � elements, not just exactly � elements.
Figure 8a shows an example tree on instructions, where node callq 0x43a28 represents the instruction sequence: better.
We perform two types of normalization: immediate number normalization and call & jump instruction normalization.
As shown in Table 1, normalization takes an instruction as input and generalizes it so that it can match against very similar, but not identical instructions.
These two types of normalization help us improve recall at the cost of a little precision (Table 2).
In our running example, only the function assign is recognized as a function start when matched against the unnormalized prefix tree (Figure 8a), while functions assign, sub, and sum can all be recognized when matched against the normalized prefix tree (Figure 8b).
Step 3: Calculate tree weights (Weight Calculation).
The prefix tree represents possible function start sequences up to 񮽙 elements.
For each node, we assign a weight that represents the likelihood that the sequence corresponding to the path from the root node to this node is a function start in the training set.
For example, according to Figure 8, the weight of node push %ebp is 0.1445, which means that during training, 14.45% of all sequences with prefix of push %ebp were truly function starts, while 85.55% were not.To calculate the weight, we first count the number of occurrences T + in which each prefix in the tree matches a true function start with respect to the ground truth O start for the entire training set T.Second, we lower the weight of a prefix if it occurs in a binary, but is not a function start.
We do this by performing an exhaustive disassembly starting from every address that is not a function start [23].
We match each exhaustive disassembly sequence of 񮽙 elements against the tree.
We call these false matches.
The number of false matches T − is the number of times a prefix represented in the tree is not a function start in the training set T.
The weight for each node n is then the ratio of true positives to overall matchesW n = T + T + + T − .
(1)Since the prefix tree can end up being quite large, it is beneficial to prune the tree of unnecessary nodes.
For each node in the tree, we remove all its child nodes if the value of T − for this node is 0.
For any child node, the value of T − is never negative and never larger than the value of T − for the parent node.
Hence, if T − is 0 for a parent node, then the value must be 0 for all of the child nodes as well.
The intuition here is that if a child node matches a sequence that is not a function start, then so must the parent node.
Thus, if the parent node does not have any false matches, then neither can a child node.
Based on Equation 1, if T − = 0 and T + > 0, then the weight of the node is 1.
Since the child nodes of such a node also have a T − value of 0 and are not included in the tree if T + = 0, they must also have a weight of 1.
As discussed more in Section 4.2, child nodes with identical weights are redundant and can safely be removed without affecting classification.This pruning optimization helps us greatly reduce the space needed by the tree.
For example, pruning reduced the number of nodes in the prefix tree from 2,483 to 1,447 for our Windows x86 dataset.
Moreover, pruning increases the speed of matching, since we can determine the weight of test sequences after traversing fewer nodes in the tree.
The output of the learning phase is a weighted prefix tree (e.g., Figure 8).
The input to the classification step is a binary B, the weighted prefix tree, and a weight threshold t.To classify instructions, we perform exhaustive disassembly of the input binary B and match against the tree.
Matching is done by tokenizing the disassembled stream, performing normalization as done during learning, and walking the tree.
To classify bytes rather than instructions, we again start at every offset but instead match the raw bytes instead of normalized instructions.The weight of a sequence is determined by last matching node (the terminal node) during the walk.
For example, given the tree in Figure 8a, and our running example with sequences ?
<!
-)0x[1-9a-f][0-9a-f]*,%eax movzwl -0x6c(%ebp),%eax movzl -0x[1-9a-f][0-9a-f]*\(%ebp\),%eax Call & Jump call 0x804cf32 call[q]* +0x[0-9a-f]*For immediate normalization, we generalize immediate operands.
There are five kinds of generalization: all, zero, positive, negative, and npz.
For jump and call instruction normalization, we generalize callee and jump addresses.
we would have weight 0.1445.
We say the sequence is the beginning of a function if the output weight w is not less than the threshold t.
At a high level, we address the function identification problem by first determining the start addresses for functions, and then performing static analysis to recover the CFG of instructions that are reachable from the start.
Direct control transfers (e.g., direct jumps and calls) are followed using recursive disassembly.
Indirect control transfers, e.g., from indirect calls or jump tables, are enumerated using VSA [2].
The final CFG then represents all instructions (and corresponding bytes) that are owned by the function starting at the given address.
CFG recovery starts at a given address and recursively finds new nodes that are connected to found nodes.
The process ends when no more vertices are added into graph.
Starting at the addresses classified for FSI, CFG recovery recursively adds instructions that are reachable from these starts.
A first-in-first-out vertex array is maintained during CFG recovery.At the beginning, there is only one element -the start address in the array.
In each round, we process the first element by exploring new reachable instructions.
If the new instruction is not in the array, it will be appended to the end.
Elements in the array are handled accordingly until all elements have been processed and no more instructions are added.If the instruction being processed is a branch mnemonic, the reachable instruction is the branch reference.
If it is a call mnemonic, the reachable instructions include both the call reference and the instruction directly following the call instruction.
If it is an exit instruction, there will be no new instruction.
For the rest of mnemonics, the new instruction is the next one by address.
We handle indirect control transfer instruction by VSA: we infer a set that over-approximates the destination of the indirect jump and thus over-approximate the function identification problem.Note that functions can exit by calling a no-return function such as exit.
This means that some call instructions in fact never return.
To detect these instances, we check the call reference to see if it represents a known no-return function such as abort or exit.
Pattern matching can miss functions; for example, a function that is written directly in assembly may not obey calling conventions.
To catch these kinds of missed functions, we continue to supplement the function start list during CFG recovery.
If a call instruction has its callee in the .
text section, we consider the callee to be a function start.
We then do CFG recovery again, starting at the new function start until there are no more functions added into the function start list.
We will refer to this strategy as recursive function call resolution (RFCR).
In §5.3, we discuss the effectiveness of this technique in function start identification.
In this section, we describe how BYTEWEIGHT addresses the challenges raised in §3.3.
First, BYTEWEIGHT recovers functions that are unreachable via calls because it does not depend on calls to identify functions.
In particular, BYTEWEIGHT recovers any function start that matches the learned weighted prefix tree as described above.
Similarly, our approach will also learn functions that have multiple entries, provided a similar specialization occurs in the training set.
This seems realistic in many scenarios since the number of compiler optimizations that create multiple entry functions are relatively few and can be enumerated during training.BYTEWEIGHT also deals with overlapping byte or instruction sequences provided that there is a unique start address.
Consider two functions that start at different addresses, but contain the same bytes.
During CFG recovery, BYTEWEIGHT will discover that both functions use the same bytes, and attribute the bytes to both functions.
BYTEWEIGHT can successfully avoid false identification for inlined functions when inlined function does not behave like an empirical function start (does not weighted over threshold in training).
Finally, note that BYTEWEIGHT does not need to attribute every byte or instruction to a function.
In particular, only bytes (or instructions) that are reachable from the recovered function entries will be owned by a function in the final output.
In this section, we discuss our experiments and performance.
BYTEWEIGHT is a cross-platform tool which can be run on both Linux and Windows.
We used BAP [3] to construct CFGs.
The rest of the implementation consists of 1988 lines of OCaml code and 222 lines of shell code.
We set up BYTEWEIGHT on one desktop machine with a quad-core 3.5GHz i7-3770K CPU and 16GB RAM.
Our experiments aimed to address three questions:1.
Does BYTEWEIGHT's pattern matching model perform better than known models for function start identification?
( §5.2) 2.
Does BYTEWEIGHT perform function start identification better than existing binary analysis tools?
( §5.3)3.
Does BYTEWEIGHT perform function boundary identification better than existing binary analysis tools?
( §5.4) In this section, we first describe our data set and ground truth (the oracle), then describe the results of our experiments.
We performed three experiments answering the above three questions.
In each experiment, we compared BYTEWEIGHT against existing tools in terms of both accuracy and speed.Because BYTEWEIGHT needs training, we divided the data into training and testing sets.
We used standard 10-fold validation, dividing the element set into 10 sub-sets, applying 1 of the 10 on testing, and using the remaining 9 for training.
The overall precision and recall represent the average of each test.
Our data set consisted of 2,200 different binaries compiled with four variables: Operating System.
Our evaluation used both Linux and Windows binaries.
Instruction Set Architecture (ISA).
Our binaries consisted of both x86 and x86-64 binaries.
One reason for varying the ISA is that the calling convention is different, e.g., parameters are passed by default on the stack in Linux on x86, but in registers on x86-64.
Compiler.
We used GNU gcc, Intel icc, and Microsoft VS. Optimization Level.
We experimented with the four optimization levels from no optimization to full optimization.
On Linux, our data set consisted of 2,064 binaries in total.
The data set contained programs from coreutils, binutils, and findutils compiled with both gcc 4.7.2 and icc 14.0.1.
On Windows, we used VS 2010, VS 2012, and VS 2013 (depending on the requirements of the program) to compile 68 binaries for x86 and x86-64 each.
These binaries came from popular open-source projects: putty, 7zip, vim, libsodium, libetpan, HID API, and pbc (a library for protocol buffers).
Note that because Microsoft Symbol Server releases only public symbols which do not contain information of private functions, we were unable to use Microsoft Symbol Server for ground truth and include Windows system applications in our experiment.We obtained ground truth for function boundaries from the symbol table and PDB file for Linux and Windows binaries, respectively.
We used objdump to parse symbol tables, and Dia2dump [13] to parse PDB files.
Additionally, we extracted "thunk" addresses from PDB files.
While most tools do not take thunks into account, IDA considers thunks in Windows binaries to be special functions.
To get a fair result, we filtered out thunks from IDA's output using the list of thunks extracted from PDB files.
Our first experiment evaluated the signature matching model for function start identification.
We compared BYTEWEIGHT and Rosenblum et al.'s implementation in terms of both accuracy and speed.
In order to equally evaluate the signature matching models, recursive function call resolution was not used in this experiment.The implementation of Rosenblum et al. is available as a matching tool with 12 hard-coded signatures for gcc and 41 hard-coded signatures for icc.
Their learning code was not available, nor was their dataset.
Although they evaluated VS in their paper, the version of the implementation that we had did not support VS and was limited to x86.
Each signature has a weight, which is also hard-coded.
After calculating the probability for each sequence match, it uses a threshold of 0.5 to filter out function starts.
Taking a binary and a compiler name (gcc or icc), it generates a list of function start addresses.
To adapt to their requirements, we divide Linux x86 binaries into two groups by compiler, where each group consists of 516 binaries.
We did 10-fold cross validation for BYTEWEIGHT, and use the same threshold as Rosenblum et al.'s implementation.We also evaluated another two varieties of our model: one without normalization, and one with a maximum tree height of 3, which is same as the model used by Rosenblum et al. and BYTEWEIGHT (3), respectively.
Table 2 shows precision, recall, and runtime for each compiler and each function start identification model.
From the table we can see that Rosenblum et al.'s implementation had an accuracy below 70%, while both BYTE-WEIGHT-series models achieved an accuracy of more than 85%.
Note that BYTEWEIGHT with 10-length and normalized signatures (the last row in table) performed particularly well, with an accuracy of approximately 97%, a more than 35% improvement over Rosenblum et al.'s implementation.
Table 2 also details the accuracy and performance differences among BYTEWEIGHT with different configurations.
Comparing against the full configuration model (BYTEWEIGHT), the model with a smaller maximum signature length (BYTEWEIGHT (3)) performs slightly faster (3% improvement), but sacrifices 7% in accuracy.
The model without signature normalization (BYTEWEIGHT (no-norm)) has only 1% higher precision but 6.68% lower recall, and the testing time is ten times longer than that of the normalized model.
The second experiment evaluated our full function start identification against existing static analysis tools.
We compared BYTEWEIGHT (no-RFCR)-a version without recursive function call resolution, BYTEWEIGHT, and the following tools:IDA.
We used IDA 6.5, build 140116 along with the default FLIRT signatures.
All function identification options were enabled.
BAP.
We used BAP 0.7, which provides a get_function utility that can be invoked directly.
Dyninst.
Dyninst offers the tool unstrip [31] to identify functions in binaries without debug information.
Naive Method.
This matched simple 0x55 (push %ebp or push %rbp) and 0xc3 (ret or retq) signatures only.
We divided our data set into four categories: ELF x86, ELF x86-64, PE x86, and PE x86-64.
Unlike the previous experiment, binaries from various compilers but the same target were grouped together.
Overall, we had 1032 ELF x86 and ELF x86-64 binaries, and 68 PE x86 and PE x86-64 binaries.
We evaluated these categories separately, and again applied 10-fold validation.
During testing, we used the same score threshold t = 0.5 as in the first experiment.Note that not every tool in our experiment supports all binary targets.
For example, Dyninst does not support ELF x86-64, PE x86, or PE x86-64 binaries.
We use "-" to indicate when the target is not supported by the tool.
Also, we omitted 3 failures in BYTEWEIGHT, and 10 failures in Dyninst during this experiment.
Due to a bug in BAP, BYTEWEIGHT failed in 3 icc compiled ELF x86-64 binaries: ranlib with -O3, ld_new with -O2, and ld_new with -O3.
Dyninst failed in 8 icc compiled ELF x86-64 binaries and 2 gcc compiled ELF x86-64 binaries.
The results of our experiment are shown in Table 3.
As evident in Table 3, BYTEWEIGHT achieved a higher precision and recall than BYTEWEIGHT without recursive function call resolution.
BYTEWEIGHT performed above 96% in Linux, while all other tools all performed below 90%.
In Windows, we have comparable performance to IDA in terms of precision, but improved results in terms of recall.Interestingly, we found that the naive method was not able to identify any functions in PE x86-64.
This is mainly because VS does not use push %rbp to begin a function; instead, it uses move instructions.
The third experiment evaluated our function boundary identification against existing static analysis tools.
As in the last experiment, we compared BYTEWEIGHT, BYTE-WEIGHT (no-RFCR), IDA, BAP, and Dyninst, classified binaries by their target, and applied 10-fold validation on each of the classes.
The results of our experiment are shown in Table 4.
Our tool performed the best in Linux, and was comparable to IDA in Windows.
In particular, for Linux binaries, BYTEWEIGHT and BYTEWEIGHT (no-RFCR) have both precision and recall above 90%, while IDA is below 73%.
For Windows binaries, IDA achieves better results than BYTEWEIGHT with x86-64 binaries, but is slightly worse for x86 binaries.
Training.
We compare BYTEWEIGHT against Rosenblum et al.'s work in terms of time required for training.Since we do not have access to either their training code or their training data, we instead compare the results based on the performance reported in paper.
There are two main steps in Rosenblum et al.'s work.
First, they conduct feature selection to determine the most informative idioms -patterns that either immediately precede a function start, or immediately follow a function start.
Second, they train parameters of these idioms using a logistic regression model.
While they did not provide the time for parameter learning, they did describe that feature selection required 150 compute days for 1,171 binaries.
Our tool, however, spent only 586.44 compute hours to train on 2,064 binaries, including overhead required to setup cross-validation.
Testing.
We list the performance of BYTEWEIGHT, IDA, BAP, and Dyninst for testing.
As described in section 4, BYTEWEIGHT has three steps in testing: function start identification by pattern matching, function boundary identification by CFG and VSA, and recursive function call resolution (RFCR).
We report our time performance separately, as shown in Table 5.
IDA is clearly the fastest tool for PE files.
For ELF binaries, it takes a similar amount of time to use IDA and BYTEWEIGHT to identify function starts, however our measured times for IDA also include the time required to run other automatic analyses.
BAP and Dyninst have better performance on ELF x86 binaries, mainly because they match fewer patterns than BYTEWEIGHT and do not normalize instructions.
This table also shows that function boundary identification and recursive function call resolution are expensive to compute.
This is mainly because we use VSA to resolve indirect calls during CFG recovery, which costs more than typical CFG recovery by recursive disassembly.
Thus while BYTEWEIGHT with RFCR enabled has improved recall, it is also considerably slower.
Recall that our tool considers a sequence of bytes or instructions to be a function start if the weight of the corresponding terminal node in the learned prefix tree is greater than 0.5.
The choice to use 0.5 as the threshold was largely dictated by Rosenblum et al., who also used 0.5 as a threshold in their implementation.
While this appears to be a good choice for achieving high precision and recall in our system, it is not necessarily the optimal value.
In the future, we plan to experiment with different thresholds to better understand how this affects the accuracy of BYTEWEIGHT.While there are similarities betwen Rosenblum et al.'s approach and ours, there are also several key differences that are worth highlighting:• In the future, we plan to optimize the length to strike a balance between training speed and recognition accuracy.
• Rosenblum et al.'s CRF model considers both positive and negative features.
For example, their algorithm is designed to avoid identifying two function starts where the second function begins within the first instruction of the first function (the so-called "overlapping disassembly").
Although we consider both positive and negative features as well, in constrast the above outcome is feasible with our algorithm.
While our technique is not compiler-specific, it is based on supervised learning.
As such, obtaining representative training data is key to achieving good results with BYTE-WEIGHT.
Since compilers and optimizations do change over time, BYTEWEIGHT may need to be retrained in order to accurately identify functions in this new environment.
Of course, the need for retraining is a common requirement for every system based on supervised learning.
This is applicable to both BYTEWEIGHT and Rosenblum et al.'s work, and underscores the importance of having a computationally efficient training phase.Despite our tool's success, there is still room for improvement.
As shown in Section 5, over 80% of BYTE-WEIGHT failures are due to the misclassification of the end instruction for a function, among which more than half are functions that do not return and functions that call such no-return functions.
To mitigate this, we could backward propagate information about functions that do not return to the functions that call them.
For example, if function f always calls function g, and g is identified as a no-return function, then f should also be considered a no-return function.
We could also use other abstract domains along with the strided intervals of VSA to increase the precision of our indirect jump analysis [2], which can in turn help us identify more functions more accurately.One other scenario where BYTEWEIGHT currently struggles is with Windows binaries compiled with hot patching enabled.
With such binaries, functions will start with an extra mov %edi,%edi instruction, which is effectively a 2-byte nop.
A training set that includes binaries with hot patching can reduce the accuracy of BYTE-WEIGHT.
Because the extra instruction mov %edi,%edi is treated as the function start in binaries with hot patching, any subsequent instructions are treated as false matches.
Thus, any sequence of instructions that would normally constitute a function start but now follows a mov %edi,%edi is considered to be a false match.
Consider a hypothetical dataset where all functions start with push %ebp; mov %esp,%ebp, but half of the binaries are compiled with hot patching and thus start functions with an extra mov %edi,%edi.
Half of the time, the sequence push %ebp; mov %esp,%ebp will be treated as a function start, but in the other half it will not be treated as such, thus leaving the sequence with a weight of 0.5 in our prefix tree.
In order to deal with this compiler peculiarity, we would need give special consideration to mov %edi,%edi, treating both this instruction and the instruction following it as a function start for the sake of training.Although training BYTEWEIGHT for function start identification is relatively fast, training for function boundary identification is still quite slow.
Profiling reveals that most of the time is spent building CFGs, and in particular resolving indirect jumps using VSA.
In future work, we plan to explore alternative approaches that avoid VSA altogether.Finally, obfuscated or malicious binaries which intentionally obscure function start information are out of scope of this paper.
In addition to the already discussed Rosenblum et al. [27], there are a variety of existing binary analysis platforms tackle the binary identification problem.
BitBlaze [6] assumes debug information.
If no debug information is present, it treats the entire section as one function.
BitBlaze also provides an interface for incorporating Hex Rays function identification information.Dyninst [19] also offers tools, such as unstrip [31], to identify functions in binaries without debug information.
Within the Dyninst framework, potential functions in the .
text section are identified using the hex pattern 0x55 representing push %ebp.
First, Dyninst will start at the entry address and traverse inter-and intra-procedural control flow.
The algorithm will scan the gaps between functions and check if push %ebp is present.
This does not preform well across different optimizations and operating systems.IDA using proprietary heuristics and FLIRT [16] technique attempts to help security researchers recover procedural abstractions.
However, updating the signature database requires an amount of manual effort that does not scale.
In addition, because FLIRT uses a pattern matching algorithm to search for signatures, small variations in libraries such as different compiler optimizations or the use of different compiler versions, prevent FLIRT from recognizing important functions in a disassembled program.
The Binary Analysis Platform (BAP) also attempts to provide a reliable identification of functions using custom-written signatures [8].
Kruegel et al. perform exhaustive disassembly, then use unigram and bigram instruction models, along with patterns, to identify functions [23].
Jakstab uses two predefined patterns to identify functions for x86 code [22, §6.2].
In this paper, we introduce BYTEWEIGHT, a system for automatically learning to identify functions in stripped binaries.
In our evaluation, we show on a test suite of 2,200 binaries that BYTEWEIGHT outperforms previous work across two operating systems, two compilers, and four different optimizations.
In particular, BYTEWEIGHT misses only 44,621 functions in comparison with the 266,672 functions missed by the industry-leading tool IDA.
Furthermore, while IDA misidentifies 459,247 functions, BYTEWEIGHT misidentifies only 43,992 functions.
To seed future improvements to the function identification problem, we are making our tools and dataset available in support of open science.
This material is based upon work supported by DARPA under Contract No.
HR00111220009.
Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA.
