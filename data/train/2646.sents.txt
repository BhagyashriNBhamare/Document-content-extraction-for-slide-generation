The "pay-as-you-go" cloud computing model has strong potential for efficiently supporting big data analysis jobs expressed via data-flow languages such as Pig Latin.
Due to security concerns-in particular leakage of data-government and enterprise institutions are however reluctant to moving data and corresponding computations to public clouds.
We present Cryp-sis, a system that allows execution of MapReduce-style data analysis jobs directly on encrypted data.
Crypsis transforms data analysis scripts written in Pig Latin so that they can be executed on encrypted data.
Crypsis to that end employs existing practical partially homomor-phic encryption schemes, and adopts a global perspective in that it can perform partial computations on the client side when PHE alone would fail.
We outline the original program transformations underlying Crypsis for reducing the cost of data analysis computations in this larger perspective.
We show the practicality of our approach by evaluating Crypsis on standard benchmarks.
Cloud computing offers an attractive paradigm for industry and government wanting to perform cost-efficient data analysis.
However, in order to adopt the paradigm, sensitive data has to be moved to the cloud, and trust must be placed on the infrastructure provider to ensure data confidentiality.
Even with a trusted provider, malicious users and programs and software defects can lead to data leaks.The ability to maintain sensitive data only in an encrypted form in the cloud and still perform meaningful data analysis is of great value.
Even within an enterprise, maintaining data in an encrypted format helps prevent insider attacks and accidental leaks.
As a consequence, homomorphic encryption has become the holy grail of cryptography.
Despite advances occurring regularly (e.g. [13]), generic, fully homomorphic encryption (FHE) still exhibits prohibitive overheads.
However, FHE may not be necessary for many real-life computations.
Several existing cryptographic systems ("cryptosystems") are partially homomorphic, i.e., homomor- * Financially supported by DARPA grant # N11AP20014, Northrop Grumman Information Systems, Purdue Research Foundation grant # 204533, and Google Award "Geo-Distributed Big Data Processing".
phic with respect to certain operations.
More precisely, if E(x) and D(x) denote the encryption and decryption functions for input data x respectively, a cryptosystem is said to be homomorphic with respect to operation χ iff ∃ψ | D(E(x 1 )ψE(x 2 )) = x 1 χx 2 .
For example, when χ is ×, the cryptosystem is said to be multiplicative homomorphic (e.g., unpadded RSA [19], ElGamal [12]).
Leveraging such existing homomorphic schemes however goes through addressing several challenges, especially in the context of big data analysis.
Many of these jobs are typically expressed in domainspecific security-agnostic high-level data flow languages, like Pig Latin [16] or FlumeJava [7], which are compiled to sequences of several MapReduce tasks [9].
This makes the application of established techniques hard.
Recent approaches to leveraging partially homomorphic encryption (PHE) for secure cloud-based computing target different application scenarios or work loads.
For intance, CryptDB [18] is based on the MySQL database server and does not enable parallelization through a MapReduce substrate; MrCrypt [24] only supports single MapReduce tasks.
This paper presents Crypsis, a runtime system for Pig Latin which allows corresponding scripts to be executed efficiently by exploiting cloud resources but without exposing input data in the clear.
More specifically, Crypsis extends the scope of encryption-enabled big data analysis based on the following insights:Extended program perspective: By analyzing entire data flow programs, Crypsis can identify many opportunities for operating in encrypted mode.
For example, Crypsis can identify operations in Pig Latin scripts that are inter-dependent with respect to encryption, or inversely, independent of each other.
More precisely, when applying two (or more) operations to a same data item, many times the second operation does not use any side-effect of the former, but operates on the original field value.
Thus, multiple encryptions of a same field can support different operations by carefully handling relationships between such encryptions.Extended system perspective: By considering the possibility of performing subcomputations on the client side, Crypsis can still exploit cloud resources rather than giving up and forcing users to run entire data flow programs in their limited local infrastructure, or defaulting to FHE (and then aborting [24]) when PHE does not suffice.
For example, several programs in the PigMix (I+II) benchmarks [3] end up averaging over the values of a given attribute for several records after performing some initial filtering and computations.
While the summation underlying averaging can be performed in the cloud via an additive homomorphic encryption (AHE) scheme, the subsequent division can be performed on the client side.Considering that the amount of data continuously decreases as computation advances in most analysis jobs, it makes sense to compute as much as possible in the cloud.The contributions of this paper are as follows.
After presenting background information (Section 2), we 1.
propose an architecture for executing Pig Latin scripts in the cloud without sacrificing confidentiality of data (Section 3).2.
outline a novel field-sensitive program analysis and transformation for Pig Latin scripts that distinguishes between operations with side-effects (e.g., whose results are used to create new intermediate data) and without (e.g., filters).
The results are semantically equivalent programs executable by Crypsis that maximize the amount of computations done on encrypted data in the cloud (Section 4).
Section 6 contrasts with related work.
Section 7 concludes with final remarks.
Apache Pig [2] is a data analysis platform which includes the Pig runtime system for the high-level data flow language Pig Latin [16].
Pig Latin expresses data analysis jobs as sequences of data transformations, and is compiled by Pig to MapReduce tasks executed by Hadoop [14].
Pig allows data analysts to query big data without the complexity of writing MapReduce programs.
Also, Pig does not require a fixed schema to operate, allowing seamless interoperability with other applications in the enterprise ecosystem.
These desirable properties of Pig Latin as well as its wide adoption 1 , prompted us to select it as the data flow language for Crypsis.
We give a short overview of Pig Latin below and refer the reader to [16] for more details.
1.
3.
6.
Source Script Functions.
Pig Latin includes built-in functions (e.g., ABS, COS AVG) and allows users to define their own user defined functions (UDFs) if needed.
For the purpose of this work, we assume that Crypsis is aware of the encryption type required for the correct operation of all functions that are part of a Pig Latin script.
We enforce this by pre-registering a set of functions with Crypsis.
We designed Crypsis having in mind an adversary capable of fully manipulating the cloud infrastructure.
The adversary can see encrypted data and Pig Latin scripts that operate on the data.
The adversary can control the computation software and control the cloud infrastructure.
Crypsis ensures confidentiality in the presence of such an adversary.
However, Crypsis does not address integrity and availability issues.
(Corresponding solutions are described in our previous work [23] which inversely, however, does not address confidentiality.)
Figure 1 illustrates the architecture of Crypsis prototype.
Script execution proceeds by the following steps: 1.
Program transformation.
The user submits a source Pig Latin script that operates on unencrypted data.
Crypsis analyzes it to identify the required encryption schemes under which the input data should be encrypted.
Operators in source script are replaced with calls to Crypsis UDFs that perform the corresponding operations on encrypted data and constants are replaced with their encrypted values to generate a target script that executes entirely on encrypted data.
Details of this transformation are presented in Section 4.
The encryption service keeps an input data encryption schema which tracks what parts of the input data are already encrypted and stored in the cloud.
This is necessary since some parts of the input data might be encrypted under multiple encryption schemes (to support multiple operations) and other parts might not be available in the cloud at all.
Based on the input data encryption schema and the required encryption schemes inferred in the previous step, the encryption service identifies the encryption schemes missing from the cloud.3.
Encrypt and send data to the cloud.
In case some required encryption schemes are not available in the cloud, the encryption service draws the unencrypted data from the local storage, encrypts it using the appropriate cryptosystem and sends it to the cloud storage.
Crypsis makes use of several encryption schemes each implemented using different cryptosystems.
The first scheme is the randomized (RAN) encryption which does not support any operators, and is intuitively, the most secure encryption scheme.
We implement RAN using Blowfish [22] to encrypt integer values, taking advantage of its smaller 64-bit block size, and use AES [8] which has a 128-bit block size to encrypt everything else.
We use CBC mode in both of these cryptosystems with a random initialization vector.
The next scheme is the deterministic (DET) encryption which allows equality comparisons over encrypted data.
We construct DET using Blowfish and AES pseudo-random permutation block ciphers for values of 64 bits and 128 bits respectively, and pad smaller values appropriately to match the expected block size.
For values longer than 128 bits we follow the approach used in CryptDB [18] and use a variant of CMC mode [15] with a zero initialization vector.
We implement the order-preserving encryption (OPE) scheme which allows order comparisons using the order-preserving symmetric encryption [5] implementation from CryptDB.
Lastly, we use the Paillier [17] cryptosystem to implement additive homomorphic encryption (AHE) which allows additions over encrypted data and ElGamal [12] cryptosystem to implement multiplicative homomorphic encryption (MHE) which allows us to perform multiplications over encrypted data.
6.
Re-encryption.
During the target script execution, it is possible that intermediate data are generated after some operations are performed.
The encryption scheme of that data depends on the last operation performed on that data.
For example, after an addition operation, the resulting sum will be encrypted under AHE.
If that intermediate data is subsequently involved in an operation that requires an encryption scheme other than the one it is encrypted under (for example multiplying sum with another value requires MHE), the operation cannot be performed.
Crypsis handles this situation by re-encrypting the intermediate data.
Specifically, the intermediate data is sent to the client where it can be securely decrypted and then encrypted under the required encryption scheme (for example sum is re-encrypted under MHE), before sent back to the cloud.
Once the re-encryption completed, execution of target script can proceed.7.
Results.
Once the job is complete, the encrypted results are sent to the client where they can be decrypted.
We use the Pig Latin script shown in Listing 1 as a running example to explain the analysis and subsequent transformation process for Pig Latin scripts in Crypsis.
This script loads two input files: input1 with two fields and input2 with a single field.
The script then filters out all rows from input1 which are less than or equal to 10 (Line 3).
Lines 4 and 5 group input1 by the first field and find the sum of the second field for each group.
Line 6 joins the sum per group with the second input file input2 to produce the final result which is stored into an output file (Line 7).
This script is representative of the most commonly used relational operations in Pig Latin and allow us to explain key features about Crypsis.
Input script analysis.
First, Crypsis checks the usersubmitted source (Pig Latin) script for syntax errors and generates a directed, acyclic data flow (DAG) representation of it.
The data flow representation uses relations as vertices and the data flow between relations as the edges.
We also generate two additional data structures: (a) a map of expression trees (MET), and (b) a set of annotated fields (SAF).
MET consists of all expressions that are part of the source script.
Each vertex in the data flow graph is given keys to all expressions that are part of the relation represented by that vertex.
SAF contains one entry for each field in the schema of each relation.
In other words every relation, f ield pair maps to one entry in the SAF.
We refer to an individual entry simply as an AF (annotated field).
The intution behind using AFs instead of actual field names is that, in the transformed program, one field in the source script may be loaded as multiple fields, each under a different encryption scheme.
Each AF also tracks its lineage using a parent field.
The parent of an annotated field could be another annotated field or an expression tree.
In cases where an AF represents a field which is newly generated as part of a Pig Latin relational operation (GROUP.
.
BY, FOREACH etc.), the parent will be the expression tree used to generate it.
Otherwise, the parent of an AF will be the corresponding AF in the vertex in the DAG that comes before it.Encryption analysis.
The program transformation component then identifies the encryption scheme required for each field.
The required encryption for each field is identified by observing all occurrences of the field in the MET.
All operators and UDFs to be used in the script are pre-registered with the encryption scheme required for operands and the encryption scheme of output generated.
Some Pig Latin relational operators also require fields to be in specific encryption schemes.
For example, fields or expressions that become the grouped field or joined field, of GROUP BY or JOIN respectively, require the field or expression to be in DET. This information, along with all the expression trees in MET, is sufficient to identify the encryption scheme required for each AF.1 A = LOAD 'enc_input1' AS (a0_ope, a0_ah, a1_det); 2 B = LOAD 'enc_input2' AS (x0_det); 3 C = FILTER A BY OPE_GREATER(a0_ope , '0xD0004D3D841327F2CCE7133ABE1EFC14'); 4 D = GROUP C BY a1_det ; 5 E = FOREACH D GENERATE group AS b0, SUM(B.a0_ah) AS b1; 6 F = JOIN E BY b0, B BY x0_det; 7 STORE F into 'out'; Script transformation.
Once we know the encryption scheme required for each field, we use the lineage information available through the parent field of AFs and metadata about the encrypted input file to identify which fields of the encrypted input file should be loaded.
In cases where a valid encryption scheme is not present, we wrap the field in a reencrypt operation.
The reencrypt operation contacts the encryption service on the trusted client to convert the field to the required encryption scheme.
Constants in MET are also transformed into encrypted form as required for the operation in which they are used.
The transformation generates the list of fields to be loaded from the encrypted version of the input file, and modifies the MET to use the newly loaded files.
Listing 2 shows the transformed Pig Latin script for our example.
Note that field a0 is loaded twice, under two encryption schemes, and that the constant 10 is encrypted using the OPE scheme to perform the comparison.
In this section we demonstrate the practicality of Crypsis.
Because of space constraints we summarize our results here and provide details at [4].
Microbenchmarks.
We first constructed a microbenchmark that compares the size of the unencrypted data with the size of the encrypted data.
We also compare the time taken to perform simple operations on unencrypted data with the time taken to perform the same operations on encrypted data and show the results in Table 1.
The evaluation of this microbenchmark was performed on a single machine with two 32 bit CPUs and 3 GB of RAM.PigMix.
We ran the Apache PigMix2 [3] benchmark to evaluate the perfomance of Crypsis.
PigMix2 is a set of 17 Pig Latin scripts that tests the latency and scalability of the Pig runtime.
Our evaluation was carried out using a cluster of 11 c3.large nodes from Amazon EC2 [1].
The nodes have two 64 bit virtual CPUs and 3.75 GB of RAM.
We discuss the results here and some lessons learned.We used the data generator script that is part of PigMix to generate an input data set with 3300000 rows (5GB).
These input data were encrypted at the client side using Pig Latin queries running entirely on the client cluster.While running benchmark, one problem we faced was in Pig Latin scripts (e.g. L1) that projects the value of map fields using chararray constants as keys (map#'key').
Current Pig implementation does not allow variables, functions or data types other than chararrays to act as keys for such projections.
This causes an issue because we consider encrypted text always as bytearrays.
Further, even if we cast a bytearray to chararray in order use it as a key in the map, we will have to deal with representing special characters in the bytearray in script file.
To avoid this, during encryption of keys for maps, we generate the hex representation of the byte array and store it as a string of characters.
This allows us to transform a projection like page_links#'b' into page_links#'4E87D339A9550DCDB6137AAD'.
Also, Pig Latin supports int, long, float and double literals for representing numbers within the Pig Latin script.
Unfortunately, the range of numbers that may appear in the cipher text space may exceed the max limits in each of these supported data types.
We overcome this limitation by representing the numeric constants as strings and performing the actual comparison using BigInteger or BigDecimal classes within UDFs.
Figure 3 shows the results of the PigMix benchmark.
On average we observe 3× overhead in terms of latency, which is extremely low compared to FHE.
Differential privacy and functional encryption solve different problems associated with confidentiality.
Differential privacy aims to improve the accuracy of statistical queries, without revealing information about individual records; data and computation reside within the trusted server.
Functional encryption focuses on ensuring that an untrusted process learns only the ouput of a function f (x) about data.
In comparison, Crypsis assumes data and computation to reside in an untrusted environment and prevents the untrusted process from learning anything about data.
sTile [6] keeps data confidential by distributing computations onto large numbers of nodes in the cloud, requiring the adversary to control more than half of the nodes in order to reconstruct input data.
Airavat [20] combines mandatory access control and differential privacy to enable MapReduce computations over sensitive data.
Both sTile and Airavat require the cloud provider and cloud infrastructure to be trustworthy.
CryptDB [18] is a seminal system leveraging PHE for cloud-based data management.
As suggested by its name, CryptDB is a database system extending MySQL, focusing on SQL queries.
Monomi [25] improves performance of encrypted queries similar to those used in CryptDB and introduces a designer to automatically choose an efficient design suitable for each workload.
Both CryptDB and Monomi lacks MapReduce-style parallelization.
MrCrypt [24] consists in a program analysis for MapReduce jobs that tracks operations and their requirements in terms of PHE.
When sequences of operations are applied to a same field, the analysis defaults to FHE, noting that the system does not currently execute such jobs at all due to lack of available FHE cryptosystems.
In this paper, we have outlined the necessity for computing on encrypted big data and the potential of partially homomorphic encryption towards this goal.
We also presented the high-level design of Crypsis, a system that achieves this goal.We are currently investigating a number of optimizations, and further, heuristics for selecting from different possible execution paths in the transformed Pig Latin script.
In particular we're investigating paths which perform re-encryption of data at clients (in contrast to costly re-encryption in the cloud as with FHE) and minimizing these re-rencryptions themselves.
To this end we're also employing sampling to determine amounts of data at different points in analysis jobs.
