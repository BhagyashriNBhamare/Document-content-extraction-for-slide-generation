Microarray datasets typically contain large number of columns but small number of rows.
Association rules have been proved to be useful in analyzing such datasets.
However, most existing association rule mining algorithms are unable to efficiently handle datasets with large number of columns.
Moreover, the number of association rules generated from such datasets is enormous due to the large number of possible column combinations.
In this paper, we describe a new algorithm called FARMER that is specially designed to discover association rules from microarray datasets.
Instead of finding individual association rules, FARMER finds interesting rule groups which are essentially a set of rules that are generated from the same set of rows.
Unlike conventional rule mining algorithms, FARMER searches for interesting rules in the row enumera-tion space and exploits all user-specified constraints including minimum support, confidence and chi-square to support efficient pruning.
Several experiments on real bioinformatics datasets show that FARMER is orders of magnitude faster than previous association rule mining algorithms.
With recent advances in DNA chip-based technologies, we can now measure the expression levels of thousands of genes in cell simultaneously resulting in a large amount of highdimension data.
These microarray datasets typically have a large number of columns but a small number of rows.
For example, many gene expression datasets may contain up to 10,000-100,000 columns but only 100-1000 rows.Recent studies have shown that association rules are very useful in the analysis of microarray data.
Due to their relative simplicity, they are more easily interpreted by biologists.
Association rules can be applied in the following two scenarios: (1) it is shown in [9,13] that classifiers built from association rules are rather accurate in identifying cancerous cell; (2) it is suggested in [7] that association rules can be used to build gene networks since they can capture the associations among genes.In this paper, we focus on a special type of association rule which takes the form of LHS → C, where LHS is a set of items and C is a class label.
We use the term "support of A" to refer to the number of rows containing A in the database and denote this number as sup(A).
The probability of the rule being true is referred to as "the confidence of the rule" and is computed as sup(LHS ∪ C)/sup(LHS).
The number of rows in the database that match the rule is defined as "the support of the rule".
User-specified constraints such as minimum support (a statement of generality) and minimum confidence (a statement of predictive ability) are often imposed on mining such association rules.Microarray datasets pose a great challenge for existing rule mining algorithms in both runtime and the number of discovered rules.
While there are a large number of algorithms that have been developed for association rule mining [1,11,18,23], their basic approaches are all column enumeration in which combinations of columns are tested systematically to search for association rules.
Such an approach is unsuitable for microarray datasets.
This is because if i is the maximum length of a row in a dataset, the search space based on column enumeration could be as large as 2 i .
Previous column enumeration methods work well for datasets with small average row length (usually i < 100).
However, for micorarray datasets, i can be in the range of tens of thousands.
These high-dimension bioinformatics datasets with thousands of columns render most of the existing algorithms impractical.On the other hand, the number of rows in such datasets is typically in the order of hundreds to a thousand.
If m is the number of rows, the size of the row enumeration space will be 2 m .
In our application domain (e.g., microarray datasets), the size of the row enumeration space is much less than the size of the column enumeration space.
Therefore, it seems reasonable to devise the algorithm that does not perform column enumeration but row enumeration.
To the best of our knowledge, none of existing studies investigate the possibilities of discovering rules by row enumeration.A large number of long frequent itemsets may be discovered from microarray datasets due to their large number of columns.
As a result, large number of association rules may be generated due to the combinatorial explosion of frequent itemsets [3].
For example, given a dataset with one row, five columns and one class label: {a, b, c, d, e, Cancer}, we could have 31 rules of the form LHS → Cancer since any combination of a, b, c, d and e could be the LHS for the rule.
These 31 rules all cover the same row and have the same confidence (100%).
Such a large set of rules contains a lot of redundancy and is difficult to interpret.
Instead of generating all 31 rules, we propose to discover these rules as a rule group whose consequent is Cancer, and which can be identified by a unique upper bound plus a set of lower bounds.
The upper bound of a rule group is the rule with the most specific LHS among the rules.
In our example, the upper bound rule is abcde → Cancer.
The lower bounds of the rule group are the rules with the most general LHS in the rule group.
For our example, the rule group has 5 lower bounds (a → Cancer, b → Cancer, c → Cancer, d → Cancer, and e → Cancer).
Given the upper bound and the lower bounds of the rule group, other rules within the group can be easily derived.We further reduce the number of rules by finding interesting rule groups only.
Consider two rules abcd → Cancer with confidence 90% and ab → Cancer with confidence 95%, it is obvious that ab is a better indicator of Cancer since ab → Cancer has a higher confidence and all rows covered by abcd → Cancer must be covered by ab → Cancer.
With ab → Cancer, rule abcd → Cancer is not interesting 1 .
In this paper, we describe a novel algorithm FARMER 2 , that is specially designed to mine interesting rule groups from microarray datasets.
FARMER discovers upper bounds of interesting rule groups by performing depth-first rowwise enumeration instead of the usual column-wise approach taken by existing rule mining algorithms.
This basic idea is combined with efficient search pruning strategies based on user-specified thresholds (minimum support, minimum confidence and minimum chi-square value), yielding a highly optimized algorithm.
We also describe an efficient algorithm for computing the lower bounds.
Our experiments show that FARMER substantially outperforms other rule mining algorithms described in [2], [23](CHARM) and [21](CLOSET+) To further illustrate the usefulness of the discovered interesting rule groups in biology, we build a simple classifier based on these interesting rule groups, which outperforms the wellknown CBA [14] and SVM [12] on 5 real-life datasets.The rest of this paper is organized as follows: In the next section, we will introduce some preliminaries and give our problem definitions.
The FARMER algorithm will be explained in Section 3.
Experimental results will be given in Section 4 on real-life microarray datasets.
Section 5 introduces some of the related work for this paper.
We will conclude our discussion in Section 6.
In this section, we introduce some basic notations and concepts that are useful for further discussion.
Dataset: the dataset (or table) D consists of a set of rows, R={r1, ..., rn}.
Let I={i1, i2, ..., im} be the complete set of items of D, and C = {C 1 , C 2 , ..., C k } be the complete set of class labels of D, then each row r i ∈ R consists of one or more items from I and a class label from C.As an example, Figure 1(a) shows a dataset where items are represented with alphabets from 'a' to 't'.
There are altogether 5 rows, r 1 ,...,r 5 , in the dataset, the first three of which are labeled C while the other two are labeled ¬C. To simplify the notation, we use the row id set to represent a set of rows and the item id set to represent a set of items.
For instance, "234" denotes the row set {r 2 , r 3 , r 4 }, and "acf " denotes the itemset {a, c, f }.
Given a set of items I ⊆ I, we define the row support set, denoted R(I ) ⊆ R, as the largest set of rows that contain I .
Likewise, given a set of rows R ⊆ R, we define item support set, denoted I(R ) ⊆ I, as the largest set of items that are common among the rows in R .
Example 1.
R(I ) and I(R ) Consider again the table in Figure 1(a).
Let I be the itemset {a, e, h}, then R(I ) = {r 2 , r 3 , r 4 }.
Let R be the row set {r 2 , r 3 }, then I(R )={a, e, h} since this is the largest itemset that occurs in both r2 and r3.
2Association Rule: an association rule γ, or just rule for short, from dataset D takes the form of A → C, where A ⊆ I is the antecedent and C is the consequent (here, it is a class label).
The support of γ is defined as the |R(A ∪ C)|, and its confidence is |R(A ∪ C)|/|R(A)|.
We denote the antecedent of γ as γ.A, the consequent as γ.C, the support as γ.sup, the confidence as γ.conf and the chi-square value is γ.chi.As discussed in the introduction, in real biological applications, people are often interested in rules with a specified consequent C that meet specified thresholds, like minimum support and minimum confidence.
The interesting rule group is a concept which helps to reduce the number of rules discovered by identifying rules that come from the same set of rows and clustering them conceptually into one entity.
Let D be a dataset with itemset I and C be a specified class label.
G = {A i → C|A i ⊆ I} is a rule group with antecedent support set R and consequent C, iff (1) Figure 1: Running Example∀A i → C ∈ G, R(Ai) = R, and (2) ∀R(Ai) = R, Ai → C ∈ G. Rule γu ∈ G (γu: Au → C) is an upper bound of G iff there exists no γ ∈ G (γ :A → C) such that A ⊃ A u .
Rule γ l ∈ G (γ l : A l → C) is a lower bound of G iff there exists no γ ∈ G (γ : A → C) such that A ⊂ A l .
2Lemmai j R(i j ) C ¬C a 1,2,3 4 e 2,3 4 h 2,3 4 Figure 2: T T | {2,3}Example 2.
Rule Group A running example is shown in Figure 2 in which R({e}) = R({h}) = R({ae}) = R({ah}) = R({eh}) = R({aeh}) = {r 2 , r 3 , r 4 }.
They make up a rule group {e → C, h → C, ..., aeh → C} of consequent C, with the upper bound aeh → C and the lower bounds e → C and h → C. 2It is obvious that all rules in the same rule group have the same support, confidence and chi-square value since they are essentially derived from the same subset of rows.
Based on the upper bound and all the lower bounds of a rule group, we can identify its remaining members according to the lemma below.Lemma 2.2.
Suppose rule group G with the consequent C and antecedent support set R has an upper bound A u → C and a lower bound A l → C. Rule γ(A → C), where A ⊂ A u and A ⊃ A l , must be a member of G.
Our algorithm FARMER is designed to find IRGs that satisfy user-specified constraints including minimum support, minimum confidence and minimum chi-square value 3 .
FARMER finds the upper bounds of all IRGs first, and then gathers their lower bounds.
This makes it possible for users to recognize all the rule group members as and when they want to.Proof: Since A ⊂ Au, R(A) ⊇ R(Au).
Likewise, R(A) ⊆ R(A l ).
Since R(A l ) = R(A u ) = R, R(A) = R.
So γ(A → C) belongs to G.To illustrate our algorithm, we first give a running example (Figure 1).
Table T T (Figure 1(b)) is a transposed version of the example table (Figure 1(a)).
In T T , the items become the row ids while the row ids become the items.
A row id r m in the original table will appear in tuple i n of T T if and only if the item i n occurs in the row r m of the original table.
For instance, since item d occurs in row r2 and r5 of the original table, row ids "2" and "5" occur in tuple d of T T .
To avoid confusion, we hereafter refer to the rows in the transposed table as tuples while referring to those in the original table as rows.We provide a conceptual explanation of FARMER algorithm to discover upper bounds of interesting rule groups in Section 3.1, the pruning strategies in Section 3.2, and the implementation details in Section 3.3.
In Section 3.4, we describe subroutine MineLB of FARMER to discover the lower bounds of interesting rule groups.
Unlike existing column-wise rule mining algorithms which perform their search by enumeration of columns [18], FARMER performs search by enumeration of row sets to find interesting rule groups with consequent C. Figure 3 illustrates the enumeration tree which represents the search of FARMER conceptually for the interesting rule groups in the absence of any pruning strategies.
Each node X of the enumeration tree corresponds to a combination of rows R and is labeled with I(R ) that is the antecedent of the upper bound of a rule group identified at this node.
For example, node "12" corresponds to the row combination {r1, r2} and "al" indicates that I({r 1 , r 2 }) = {a, l}.
An upper bound al → C can be discovered at node "12".
This is correct because of the following lemma.Lemma 3.1.
Let X be a subset of rows from the original table, then I(X) → C must be the upper bound of the rule group G whose antecedent support set is R(I(X)) and consequent is C. Proof: First, according to Definition 2.1, I(X) → C belongs to rule group G with antecedent support set R(I(X)) and consequent C. Second, assume that I(X) → C is not the upper bound of G, then there must exist an item i such that i / ∈ I(X), and I(X) ∪ {i} → C belongs to G.
So we get R(I(X)) = R(I(X) ∪ {i}).
Since rows in X contain all items of I(X), we get X ⊆ R(I(X)), and then X ⊆ R(I(X) ∪ {i}).
This means that i is also found in every row of X, which contradicts the definition that I(X) is the largest set of items that are found in every row of X.
So I(X) → C is the upper bound of the rule group with antecedent support set R(I(X)).2 FARMER performs a depth-first search on the enumeration tree by moving along the edges of the tree.
By imposing an order ORD, in which the rows with consequent C are ordered BEFORE the rows without consequent C(this is done to support efficient pruning which will be explained later), we are able to perform a systematic search by enumerating the combinations of rows based on the order ORD.
For example, let "1 2 3 4 5" according to ORD, the order of search in Figure 3 will be {"1", "12", "123", "1234", "12345", "1235",...,"45", "5"} in absence of any optimization and pruning strategies.
Note that the order also serves for confidence pruning purpose (explained in section 3.2.3).
Next, we prove that the complete rule groups can be discovered by a complete row enumeration.Lemma 3.2.
By enumerating all possible row combinations on the row enumeration tree, we can obtain the complete set of upper bounds and the corresponding complete set of rule groups in the dataset.
Hence the proof.
2It is obvious that a complete traversal of the row enumeration tree is not efficient.
Various pruning techniques will be introduced to prune off unnecessary searches in the next section.
We will next introduce the framework of our algorithm for discovering the upper bounds of rule groups.
We first introduce two concepts.Definition 3.1.
Conditional Transposed Table (T T | X )Given the transposed table T T (used at the root of the enumeration tree), a X-conditional transposed table (T T |X ) at node X (X is the row combination at this node) is a subset of tuples from T T such that for each tuple t of T T that t ⊇ X, there exists a tuple t = t in T T |X .
2Example 3.
Let T T be the transposed table in Figure 1(b) and let X = {2, 3}.
The X-conditional transposed table, T T |X is shown in Figure 2.
2Definition 3.2.
Enumeration Candidate List (T T | X .
E)Let T T |X be the X-conditional transposed table and rmin ∈ X be the row id with the lowest ORD order in row combination X. Let E P = {r|r ORD r min ∧ r ∈ R(C)} (all rows of consequent C ordered after r min ), and E N = {r|r ORD rmin ∧ r ∈ R(¬C)} (all rows with class C ordered after r min ).
The enumeration candidate list forT T | X , denoted as T T | X .
E, is defined to be E P ∪ E N .
2 Notation Description T T | X .
E enumeration candidates; T T | X .
E P enumeration candidates with label C; T T | X .
E N enumeration candidates without label C; T T | X .
Yenumeration candidates that occur in each tuple of T T | X .
In the rest of this paper, we will use the notations in Figure 3.1 to describe various operations on the conditional transposed table, T T |X .
Our formal algorithm is shown in Figure 5.
FARMER involves recursive computations of conditional transposed tables by performing a depth-first traversal of the row enumeration tree.
Each computed conditional table represents a node in the enumeration tree of Figure 3.
For example, the {2, 3}-conditional table is computed at node "23".
After initialization, FARMER calls the subroutine M ineIRGs to recursively generate X-conditional tables.The subroutine M ineIRGs takes in four parameters at node X: T T | X , sup p , sup n and IRG.
T T | X is the Xconditional transposed table at node X with enumeration candidates T T |X .
EP and T T |X .
EN .
supp is the number of identified rows that contain I(X) ∪ C while sup n is the number of identified rows that contain I(X) ∪ ¬C before scanning T T |X .
IRG stores the upper bounds of interesting rule groups discovered so far.Steps 1, 2, 4 and 5 in the subroutine M ineIRGs perform the pruning.
They are extremely important for the efficiency of FARMER Algorithm and will be explained in the next subsection.Step 3 scans the table T T |X .
Step 6 moves on into the next level enumerations in the search tree.
Step 7 checks whether I(X) → C is the upper bound of an IRG that satisfies the user-specified constraints before inserting it into IRG.
Note that step 7 must be performed after step 6 (the reason will be clear later).
We first prove the correctness of the two steps by two lemmas as follows:Lemma 3.3.
T T | X | r i = T T | X+r i , r i ∈ T T | X .
E. 2Lemma 3.3 is useful for explaining Step 6.
It simply states that a X + r i conditional transposed table can be computed from a X conditional transposed table T T | X in the next level search after node X.Lemma 3.1 ensures that at Step 7 only upper bounds of rule groups are possibly inserted into IRG.
To determine whether an upper bound γ discovered at node X represents an interesting rule group satisfying user-specified constraints, we need to compare γ.conf with all γ .
conf , where γ .
A ⊂ γ.A and γ satisfies user specified constraints.
FARMER ensures that all such γ have already been discovered and kept in IRG at Step 7 by lemma 3.4 below.Lemma 3.4.
Let γ : I(X) → C be the upper bound rule discovered at node X.
The rule group with upper bound γ : A → C such that A ⊂ I(X) can always be discovered at the descendent nodes of node X or in an earlier enumeration.
Proof: Since A ⊂ I(X), and γ and γ are the upper bounds of two different rule groups, we see R(A ) ⊃ R(I(X)) ⊇ X. Let RS = {r|r ∈ R(A ) ∧ r / ∈ X} and r min ∈ X be the row with the lowest ORD rank in row set X.
If ∃r ∈ RS such that r rmin, then node R(A ) is traversed before node X; otherwise node R(A ) is traversed at a descendent node of node X. Subroutine: MineIRGs(T T | X , supp, supn, IRG).
Parameters:T T | X : a X-conditional transposed table;sup p and sup n : support parameters;IRG: the set of discovered interesting rule groups;Method:1.
Apply Pruning 2: If I(X) → C is already identified, then return.2.
Apply Pruning 3: If prunable with the loose upper bounds of support or confidence, then return.3.
Scan T T | X and count the frequency of occurrences for each enumeration candidate,r i ∈ T T | X .
E, Let U p ⊆ T T | X .
E P be the set of rows from T T | X .
E P which occur in at least one tuple of T T | X ; Let U n ⊆ T T | X .
E N be the set of rows from T T | X .
E N which occur in at least one tuple T T | X ; Let Y p ⊂ T T | X .
E P be the set of rows from T T | X .
E P found in every tuple of T T | X ; Let Y n ⊂ T T | X .
E N be the set of rows from T T | X .
E N found in every tuple of T T | X ; sup p = sup p + |Y P | (|R(I(X) ∪ C)|); supn = supn + |Y N | (|R(I(X) ∪ ¬C)|);4.
Apply Pruning 3: If prunable with one of the three tight upper bounds, then return.5.
Apply Pruning 1: Update enumeration candidate list,T T | X .
E P = U P − Y P , T T | X .
E N = U N − Y N .
6.
for each r i ∈ T T | X .
E do if r i ∈ R(C) then T T | X | r i .
E P = {r j |r j ∈ T T | X .
E P ∧ r j ORD r i }; T T | X |r i .
E N = T T | X .
E N ; a = sup p + 1; b = sup n ; else T T | X | r i .
E P = ∅; T T | X | r i .
E N = {r j |r j ∈ T T | X .
E N ∧ r j ORD r i }; a = sup p ; b = sup n + 1; M ineIRGs(T T | X |r i , a, b, IRG); 7.
Let conf = (supp)/(supp + supn); If (sup p ≥ minsup) ∧ (conf ≥ minconf )∧ (chi(sup p , supp+ supn) ≥ minchi) then if ∀γ, (γ ∈ IRG) ∧ (γ.A ⊂ I(X)) ⇒ (conf > γ.conf ) then add upper bound rule I(X) → C into IRG.
Step 7 is done after Step 6 to ensure that all descendant nodes of X are explored before determining whether the upper bound rule γ at X is an IRG.
Together with Lemma 3.2, we know that the complete and correct set of interesting rule groups will be in IRG.Note that Step 6 implicitly does some pruning since it is possible that the enumeration candidate list is empty, i.e. T T | X .
E = ∅.
It can be observed from the enumeration tree that there exist some combinations of rows, X, such that I(X) = ∅ (an example is node "134").
This implies that there is no item existing in all the rows in X.
When this happens, T T |X .
E is empty and no further enumeration will be performed.
We next look at the pruning techniques that are used in FARMER, which are essential for the efficiency.
Our emphasis here is to show that our pruning steps do not prune off any interesting rule groups while preventing unnecessary traversals of the enumeration tree.
Combining this with our earlier explanations on how all interesting rule groups are enumerated in FARMER without the pruning steps, the correctness of our algorithm will be obvious.
Pruning strategy 1 is implemented at Step 5 of MineIRGs by pruning T T | X .
Y , the set of enumeration candidate rows that occur in all tuples of the T T | X .
We partition T T | X .
Y to two subsets: Y p with consequent C and Y n without.
The intuitive reason for the pruning is that we obtain the same set of upper bound rules along the branch X WITHOUT such rows.
The correctness of such a pruning strategy is due to the following lemma.Lemma 3.5.
Let T T |X be a X-conditional transposed ta- ble.
Given any subset R , R ⊂ T T |X .
E, we have I(X ∪ R ) = I(X ∪ T T | X .
Y ∪ R ).
Proof: By definition, I(X ∪ R ) contains a set of items which occur in every row of (X ∪ R ).
Suppose candidate y ∈ T T |X .
Y (y occurs in every tuple of T T |X ), then ei- ther y ∈ X ∪ R (if y ∈ R ) or y occurs in every tuple of the T T | X∪R (if y / ∈ R ).
In either case, I(X ∪ R ) = I(X ∪ R ∪{y}).
Thus, I(X ∪ R ) = I(X ∪ T T |X .
Y ∪ R ).
2With Lemma 3.5, we can safely delete the rows in T T |X .
Y from the enumeration candidate list T T | X .
E.Example 4.
Consider T T | {2,3} , the conditional transposed table in Figure 2.
Since enumeration candidate row 4 occurs in every tuples of T T | {2,3} , we can conclude that I({2, 3}) = I({2, 3, 4}) = {a, e, h}.
Thus, we need not traverse node "234" and create T T | {2,3,4} .
Row 4 can be safely deleted from T T | {2,3} .
E. 2Since I({2, 3, 4}) = I({2, 3}), the upper bound rule is identified at node "23" and node "234" is redundant.
We say that node "234" is compressed to node "23".
We argue here that Lemma 3.4 still holds after applying pruning strategy 1.
Without applying pruning strategy 1, for each node X, A → C, where A ⊂ I(X), is identified at a node X , which is traversed before node X or is a descendent node of node X. With pruning strategy 1, X might be compressed to a node X (X ⊂ X and I(X ) = I(X ) = A ), and we can see node X is either traversed before the subtree rooted at node X, or inside this subtree.
This pruning strategy is implemented at Step 1 of MineIRGs.
It will stop searching the subtree rooted at node X if the upper bound rule I(X) → C was already discovered previously in the enumeration tree because this implies that any rules to be discovered at the descendants of node X would have been discovered too.
Lemma 3.6.
Suppose pruning strategy 1 is utilized in the search.
Let T T | X be the conditional transposed table of the current node X. All upper bounds to be discovered in the subtree rooted at node X must have already been discovered if there exists such a row r that satisfies the following conditions: (1)r / ∈ X; (2)r / ∈ T T | X .
E; (3)for any ancestor node Xi of node X, r / ∈ T T |X i .
Y (pruned by strategy 1); and (4)r occurs in each tuple of T T | X .
Proof: Let X = {r 1 , r 2 , ..., r m }, where r 1 ORD r 2 ORD ... ORD r m .
Suppose that there is a node X (X = X ∪ {r }), we can have the following properties: (1) I(X) = I(X ); (2) r ORD r m , since r / ∈ T T | X .
E and r / ∈ X;(3) T T | X .
E = T T | X .
E.X is either enumerated or compressed to a node XC , where I(XC ) = I(X ) and T T | X .
E ⊆ T T |X C .
E.
We can prove that either node X or node X C is traversed before node X by considering the following two cases: (1) If r ORD r1, node X or node XC falls in the subtree rooted at node {r }, which is traversed before node X. (2) If row ids in X follow the order r 1 ORD r 2 ORD ... ORD r t ORD r ORD r t+1 ORD ... ORD r m , node X or node XC falls in the subtree rooted at node X = {r1, ..., rt, r }, which is also traversed before node X. Because T T | X .
E = T T | X .
E and T T | X .
E ⊆ T T | X C .
E, we can conclude that all upper bounds to be discovered in the subtree rooted at node X must have already been discovered earlier in the subtree rooted at node X or node X C .
2In the implementation of pruning strategy 2, the existence of such a r can be efficiently detected by a process called back counting without scanning the whole T T | X .
Details are explained in section 3.3.
Example 5.
Consider node "23" in Figure 3 where the upper bound rule {a, e, h} → C is identified for the first time.
When it comes to node "34", we notice that row "2" occurs in every tuple of T T | {3,4} , "2" / ∈ T T | {3,4} .
E, and "2" / ∈ T T | {3} .
Y .
So we conclude that all upper bounds to be discovered down node "34" have already been discovered before (I({3, 4}) = I({2, 3}) = {a, e, h}.
I({3, 4, 5}) = ∅).
We can prune the search down node "34".
2 Pruning strategy 3 performs pruning by utilizing the userspecified thresholds, minsup, minconf and minchi.
We estimate the upper bounds of the measures for the subtree rooted at the current node X.
If the estimated upper bound at X is below the user-specified threshold, we stop searching down node X.
A important thing to note here is that our pruning strategy is highly dependent on the order ORD which rank all rows with consequent C before rows with consequent ¬C.Pruning strategy 3 consists of 3 parts: pruning using confidence upper bound, pruning using support upper bound and pruning using chi-square upper bound.
This strategy is executed separately at Step 2 and Step 4 ( Figure 5).
At Step 2, we will perform pruning using the two loose upper bounds of support and confidence that can be calculated BEFORE scanning T T | X .
At Step 4 we calculate the three tight upper bounds of support, confidence and chi-square value AFTER scanning T T |X .
For clarity, we will use the notations in Figure 3.2.3 to explain our pruning strategy here.
X the current enumeration node; γ the upper bound rule I(X) → C at node X; X the immediate parent node of X; γ the upper bound rule I(X ) → C at node X ; r m a row id such that T T | X = T T | X | rm ; Figure 6: Notations for Search Pruning We have two support upper bounds for the rule groups identified at the subtree rooted at node X: the tight support upper bound U s1 (after scanning T T | X ) and the loose support upper bound Us2 (before scanning T T | X ).
If the estimated upper bound is less than the minimum support minsup, the subtree can be pruned.If r m has consequent C:Us1 = γ .
sup + 1 + M AX(|T T |X .
EP ∩ t|), t ∈ T T |X ; Us2 = γ .
sup + 1 + |T T |X .
EP |;If r m has consequent ¬C then U s1 = U s2 = γ .
sup; Note that we need to scan T T | X to get U s1 while U s2 can be obtained directly from the parameters supp and X passed by the parent node.
Pruning Using Confidence Upper Bound Similarly, we estimate two confidence upper bounds for the subtree rooted at node X, the tight confidence upper bound Uc1 and the loose confidence upper bound Uc2.
If the estimated upper bound is less than minimum confidence minconf , the subtree rooted at node X can be pruned.Given Us1 and Us2, the two confidence upper bounds of subtree rooted at node X, U c1 (tight) and U c2 (loose), are:U c1 = U s1 /(U s1 + |R(γ.A ∪ ¬C)|); U c2 = U s2 /(U s2 + |R(γ .
A ∪ ¬C)|) (r m has consequent C); Uc2 = Us2/(Us2 + |R(γ .
A ∪ ¬C)| + 1) (rm has consequent ¬C).
Lemma 3.8.
Uc1 and Uc2 are the confidence upper bounds for the rules discovered in the subtree rooted at node X. Proof: For a rule γ discovered in subtree rooted at node X, its confidence is computed as |R(γ .
A ∪ C)|/(|R(γ .
A ∪ C)| + |R(γ .
A ∪ ¬C)|).
This expression can be simplified as x/(x+y), where x = |R(γ .
A∪C)| and y = |R(γ .
A∪¬C)|.
This value is maximized by choosing the maximum value for x (U s1 and U s2 ) and minimum value for y. Suppose rule γ is discovered at node X. For any rule γ discovered under the enumeration tree under node X, γ .
A ⊂ γ.A because of pruning strategy 1, so we can see |R(γ .
A∪¬C)| ≥ |R(γ.A∪ Example 6.
Suppose minimum confidence minconf = 95%.
At node "134", the discovered upper bound rule is "a → C" with confidence 0.75 < 0.95.
Since row 4 has no consequent C, any descendent enumeration will only reduce the confidence.
Thus we can stop next level searching.
The chi-square value of an association rule is the normalized deviation of the observed values from the expected values.
Let γ be a rule in the form of A → C of dataset D, n be the number of rows in D, and m be the number of instances with consequent C in D.
The four observed values for chi-square value computation are listed in the following table.
For example, O A¬C represents the number of rows that contain A but do not contain C. Let x = OA and y = OAC .
Since m and n are constants, the chi-square value is determined by x and y only and we get chi-square function chi(x, y).
C ¬C Total A O AC = y O A¬C O A = x ¬A O ¬AC O ¬A¬C O ¬A = n − x Total O C = m O ¬C = n − m nThe following lemma gives an estimation of upper bound of chi square value for rules down the node X. 1)x ≤ x ≤ n (|R(A)| ≤ |R(A )|) 2) y ≤ y ≤ m (|R(A ∪ C)| ≤ |R(A ∪ C)|) 3) y ≤ x (|R(A ∪ C)| ≤ |R(A )|) 4) n − m ≥ x − y ≥ x − y (|R(A ∪ ¬C)| ≥ |R(A ∪ ¬C)|)The value pair (x (γ ), y (γ )) falls in the gray parallelogram (x(γ), y(γ)), (x(γ)−y(γ)+m, m), (n, m), (y(γ)+n−m, y(γ) (Figure 7).
Since the chi-square function chi(x, y) is a convex function [15], which is maximized at one of its vertexes, and chi(n, m) = 0 (please refer to [15]), we only need to consider the remaining three vertexes.
2 In the implementation of FARMER, we use memory pointers [4] to point at the relevant tuples in the in-memory trans- posed table to simulate the conditional transposed table.
Our implementation assumes that despite the high dimensionality, the microarray datasets that we are trying to handle are still sufficiently small to be loaded completely into the main memory.
This is true for many gene expression datasets which have small number of rows.Following is the running example.
Suppose the current node is node "1" (Figure 8(a)), and minsup = 1.
The inmemory transposed table is shown on the right hand side of the figure.
Memory pointers are organized into conditional pointer lists.In Figure 8(a), the "1"-conditional pointer list (at the top left corner of the figure) has 6 entries in the form of < f i , P os > which indicates the tuple (f i ) that contains r 1 and the position of r1 within the tuple (P os).
For example, the entry < a, 1 > indicates that row r 1 is contained in the tuple 'a' at position 1.
We can extend the "1"-conditional transposed table T T | {1} by following the P os.
During one full scan of the transposed table, FARMER also generates the conditional pointer lists for other rows (i.e. r 2 , r 3 , r 4 and r 5 ).
However, the generated "2"-conditional pointer list is slightly different in that it contains an entry for each tuple that contains r2 BUT NOT r1.
For example, although the tuple 'a' contains r 2 , it does not appear in the "2"-conditional pointer list.
It will be inserted subsequently as we will see later.A further scan through the "1"-conditional pointer list will allow us to generate the "12", "13", "14" and "15" conditional pointer lists.
Figure 8(b) shows the state of memory pointers when we are processing node {1, 2}.
Finally, we show the state of conditional pointer lists after node {1} and all its descendants have been processed (Figure 8(c)).
Since all enumerations involving row r 1 have been either processed or pruned off, the entries in the "1"-conditional pointer list are moved into the remaining conditional pointer lists.
The entries in the "2"-conditional pointer list will be moved to the other conditional pointer lists after node {2} and its descendants are processed, and so on.
Throughout all the enumerations described above, we need to implement our three pruning strategies.
The implementation of strategies 1 and 3 is straightforward.
For pruning strategy 2, we do a back scan through the conditional list to see whether there exists some row that satisfies the condition of Lemma 3.6.
For example at node "2" in Figure 8(c), we scan from the position of each pointer to the head of each tuple, instead of scanning the transposed table from the position of each pointer to the end of each tuple.
In this example, there is no row that satisfies the pruning condition of Lemma 3.6.
Such an implementation is proven to be efficient for our purpose as shown in our experiments.
In this section, we describe the algorithm, MineLB, which is designed to find the lower bounds of a rule group.
Since a rule group has a unique upper bound and the consequent of a rule group is fixed, the problem can be regarded as generating the lower bounds for the antecedent of the upper bound rule.
This antecedent could be regarded as a closed set (Definition 3.3) and the problem can be solved as long R(A) = R(A ).
A l , A l ⊆ A, is a lower bound of closed set A, iff R(A l ) = R(A) and there is no A ⊂ A l such that R(A ) = R(A).
2MineLB is an incremental algorithm that is initialized with one closed set A, the antecedent of an upper bound rule, A → C for a rule group.
It then updates the lower bounds of A incrementally whenever a new closed set A is added, where A ⊂ A and A is the antecedent of the newly added upper bound A → C.
In this way, MineLB keeps track of the latest lower bounds of A. MineLB is based on the following lemma.Lemma 3.10.
Let A be the closed set whose lower bounds will be updated recursively and be the set of closed sets that are already added.
Let A.Γ be the current collection of lower bounds for A.
When a new closed set A ⊂ A is added, A.Γ is divided into two groups, A.Γ1 and A.Γ2, where A.Γ1 = {l i |l i ∈ A.Γ ∧ l i ⊆ A }, A.Γ2 = A.Γ − A.Γ1.
Then the newly generated lower bounds of A must be in the form of l 1 ∪ {i}, where l 1 ∈ A.Γ1, i ∈ A − A .
Proof: Suppose l is a newly generated lower bound of A.(1) we prove l ⊃ l1.
Since R(l) = R(A) (Definition 2.1) before A is added, there must exist a l i ∈ A.Γ such that l i ⊂ l ⊂ A.
If l i ∈ A.Γ2, l can not be a new lower bound, since li ∈ A.Γ2 is still a lower bound of A after A is added.
So l ⊃ l1, l1 ∈ A.Γ1.
(2) Obviously, the newly generated lower bound must contain at least one item from the set (A − A ).
(3) l = l1 ∪ {i} is a bound for A after adding A , where l1 ∈ A.Γ1, i ∈ A and i / ∈ A .
Before A is added, l = l1 ∪{i} is a bound, so for any X ∈ , l X.
After A is added, l A because i / ∈ A .
So, l = l 1 ∪ {i} is a new bound for A after adding A .
Based on (1), (2) and (3), we come to the conclusion that the newly generated lower bound for A after inserting A takes the form of l 1 ∪ {i}, where l 1 ∈ A.Γ1 and i ∈ (A − A ).
2Itemset l 1 ∪ {i} described in Lemma 3.10 is a candidate lower bound of A after A is added.
If l 1 ∪ {i} does not cover any l 2 ∈ A.Γ2 and other candidates, l 1 ∪ {i} is a new lower bound of A after A is added.
MineLB adopts bit vector for the above computation.
Thus A.Γ can be updated efficiently.
The detailed algorithm is illustrated in Figure 9.
We can ensure that the closed sets (those that cover all the longest closed set A ⊂ A) obtained at Step 2 are sufficient for the correctness of MineLB because of Lemma 3.11Lemma 3.11.
If a closed set A1 ⊂ A is already added and the collection of A's lower bounds, A.Γ, is already updated, A.Γ will not change after adding closed set A2, A2 ⊂ A1.
Proof:After A1 ⊂ A is added, A.Γ is updated so that no l i ∈ A.Γ can satisfy l i ⊆ A1.
So no l i ∈ A.Γ can satisfy l i ⊆ A2, A2 ⊂ A1.Since A2 will not cover any li ∈ A.Γ, A.Γ will not change, according to Lemma 3.10.
2Example 7.
Finding Lower Bound Given an upper bound rule with antecedent A = abcde and two rows, r1 : abcf and r2 : cdeg, the lower bounds A.Γ of A can be determined as follows: 1)Initialize the set of lower bounds A.Γ = {a, b, c, d, e}; 2)add "abc" (= I(r1) ∩ A): We get A.Γ1 = {a, b, c} and A.Γ2 = {d, e}.
Since all the candidate lower bounds, "ad", "ae", "bd", "be", "cd", "ce" cover a lower bound from A.Γ2, no new lower bounds are generated.
So A.Γ = {d, e};3)add "cde" (= I(r2) ∩ A): We get A.Γ1 = {d, e} and A.Γ2 = ∅.
The candidate lower bounds are "ad", "bd", "ae" and "be".
Because none of them is covered by another candidate and A.Γ2 = ∅, A.Γ = {ad, bd, ae, be}.
2 In this section, we will look at both the efficiency of FARMER and the usefulness of the discovered IRGs.
All our experiments were performed on a PC with a Pentium IV 2.4 Ghz CPU, 1GB RAM and a 80GB hard disk.
Algorithms were coded in Standard C. Datasets: The 5 datasets are the clinical data on lung cancer (LC) 4 , breast cancer (BC) 5 , prostate cancer (PC) 6 , Subroutine: MineLB (Table:D, upper bound rule: γ).1.
A = γ.A; A.Γ = {i|i ∈ A}; Σ = ∅; 2.
for each row r id of D that r id / ∈ R(A): if (I(r id ) ∩ A) ⊂ A then add (I(r id ) ∩ A) to Σ;3.
for each closed set A ∈ Σ:A.Γ1 = A.Γ2 = ∅; for each lower bound l i ∈ A.Γ: if l i ⊆ A then add l i to A.Γ1; else add l i to A.Γ2; CandiSet = ∅; for each l i ∈ A.Γ1 and each i ∈ A && i / ∈ A : add candidate l i ∪ {i} to CandiSet; A.Γ = A.Γ2; for each candidate c i ∈ CandiSet if c i does not cover any l i ∈ A.Γ2 and c i does not cover any other c j ∈ CandiSet then add c i to A.Γ 4.
Output A.Γ.
Table 1 shows the characteristics of these 5 datasets: the number of rows (# row), the number of columns (# col), the two class labels (class 1 and class 0), and the number of rows for class 1 (# class 1).
All experiments presented here use the class 1 as the consequent; we have found that using the other consequent consistently yields qualitatively similar results.To discretize the datasets, we use two methods.
One is the entropy-minimized partition (for CBA and IRG classifier) 9 and the other is the equal-depth partition with 10 buckets.
Ideally, we would like to use only the entropy discretized datasets for all experiments since we want to look at the classification performance of IRGs.
Unfortunately, the two rule mining algorithms that we want to compare against are unable to run to completion within reasonable time (we ran them for several days without results) on the entropy discretized datasets, although FARMER is still efficient.
As a result, we will report our efficiency results based on the equal-depth partitioned data while our classifier is built using the entropy-discretized datasets.
The efficiency of FARMER will first be evaluated.
We compare FARMER with the interesting rule mining algorithm in [2].
The algorithm in [2] is the one most related to FARMER in terms of interesting rule definition (but not the same, see related work).
To our best knowledge, it is also the most efficient algorithm that exists with the purpose of mining interesting rules of our kind.
We denote this algorithm as ColumnE since it also adopts column enumeration like most existing rule mining algorithms.
We also compare FARMER with the closed set discovery algorithms CHARM [23] and CLOSET+ [21], which are shown to be more efficient than other association rule mining algorithms in many cases.
We found that CHARM is always orders of magnitude faster than CLOSET+ on the microarray datasets and thus we do not report the CLOSET+ results here.
Note that the runtime of FARMER includes the time for computing both the upper bound and lower bounds of each interesting rule group.
Compared with CHARM, FARMER does extra work in: 1)computing the lower bounds of IRGs and 2) identifying the IRGs from all rule groups.
Unlike FARMER that discovers both upper bound and lower bounds for each IRG, ColumnE only gets one rule for each IRG.
The first set of experiments ( Figure 10) shows the effect of varying minimum support threshold minsup.
The graphs plot runtime for the three algorithms at various settings of minimum support.
Note that the y-axes in Figure 10 are in logarithmic scale.
We set both minconf and minchi as ZERO, which disables the pruning with confidence upper bound and the pruning with the chi-square upper bound of FARMER.For CHARM, minsup represents the least number of rows that the closed sets must match.
The runtime of CHARM is not shown in Figures 10(a) and 10(b) because CHARM runs out of memory even at the highest support in Figure 10 on datasets BC and LC.
Figure 10 shows that FARMER is usually 2 to 3 orders of magnitude faster than ColumnE and CHARM (if it can be run).
Especially at low minimum support, FARMER outperforms ColumnE and CHARM greatly.
This is because the candidate search space for ColumnE and CHARM, dependent on the possible number of column combinations after removing the infrequent items, is orders of magnitude greater than the search space of FARMER, dependent on the possible number of row combinations, on microarray datasets.As shown in Figure 10(f), the number of interesting rule groups discovered at a low minsup is much larger than that at a high minsup.
Besides the size of row enumeration space, the number of IRGs also affects the efficiency of FARMER due to two reasons.
First, since FARMER discovers IRGs by comparison (see algorithm section, step 7), more time will be spend when the number of IRGs to be compared against increase.
Second, the time complexity of computing lower bounds in FARMER is O(n), where n is the number of IRGs.
We observe that at high minsup, the time used to compute lower bounds takes 5% to 10% of the runtime of FARMER while the time taken at low minsup can be up to 20%.
ColumnE also does the comparison to get interesting rules while all the runtime of CHARM is used to discover closed sets.We choose our minimum support such that the runtime of FARMER is around 10 seconds.
Although ColumnE and CHARM could perform better that FARMER for higher minsup, the absolute time difference however will be less than 10 seconds and thus is not interesting for comparison.
This is negligible compared to the difference in runtime at low minsup.
The next set of experiments ( Figure 11) shows the effect of varying minconf when minsup is fixed.
The minchi pruning is still disabled by setting it to ZERO.
For all the parameter settings in Figure 11, CHARM is unable to finish because of insufficient memory after several hours while ColumnE always has a runtime of more than 1 day.
This is because we adopt a relative low minsup to study the effectiveness of confidence pruning in the experiment.
To show the effect of various minconf clearly, we do not give the runtime of ColumnE.
We will first ignore the lines marked with "minchi=10" here.
We set minsup = 1, which means that minimum support pruning is almost disabled.
Figure 11 shows that the runtime of FARMER decreases when increasing minconf on all the 5 datasets (Figure 11(f) lists the number of IRGs).
This shows that it is effective to exploit the confidence constraint for pruning.
There is only a slight decrease in runtime of FARMER when the minconf increases from 85% to 99% since there are few upper bound rules with confidence between 85% and 99%.
We observe that nearly all IRGs discovered at confidence 85% on these 5 datasets have a 100% confidence.
As a result, FARMER does no additional pruning when minconf increases from 85% to 99%.
The result that many discovered IRGs have a 100% confidence is interesting and promising.
It means that the IRGs are decisive and have good predictability.
The last set of experiments was performed to study the effectiveness of the chi-square pruning.
Minimum chi-square constraint is usually treated as a supplementary constraint of minimum support and minimum confidence.
We set minchi = 10 and plot the runtime vs various minconf in Figure 11 due to the space limitation, where minconf is set the same as in section 4.1.2.
The pruning exploited by constraint minchi = 10 is shown to be very effective on datasets BC, PC, CT and ALL.
In some cases, the saving can be more than an order of magnitude.
The pruning effect is not so obvious on dataset LC.
By checking the identified IRGs, we found that discovered IRGs from LC usually have higher chi-square value.
If we impose a tighter chi-square constraint by increasing minchi, the minchi pruning will be more obvious.
Due to space constraint, we do not discuss this further.As can be seen, in all the experiments we conducted, FARMER outperforms ColumnE and CHARM.
Moreover, the prunings based on minsup, minconf and minchi are effective.
In general, the runtime of FARMER correlates strongly with the number of interesting rule groups that satisfy all of the specified constraints.
Our experimental results demonstrate that FARMER is extremely efficient in finding IRGs on datasets with small number of rows and large number of columns.In additional to these experiments, we also look at how the performance of FARMER varies as the number of rows increase.
This is done by replicating each dataset a number of times to generate a new dataset.
It is observed that the performance of FARMER still outperform other algorithms even when the datasets are replicated for 5-10 times.
Due to lack of space, we refer readers to [6] for these additional experiments.
In order to show the usefulness of the discovered IRGs, we build a simple classifier called IRG classifier based on those IRGs that we discovered.
Note that our emphasis here is not to build a new classifier but to provide some evidence that the discovery of IRGs is at least useful for such purpose.We will compare our IRG classifier with two well-known classifiers CBA [14] and SVM [12], both available through the Internet.
The open-source CBA algorithm (and all competitors we look at in the earlier section) failed to finish running in one week.
To go around this problem, we build the CBA classifier by obtaining the frequent rules based on the upper bounds and lower bounds generated by FARMER.
Our IRG classifier is similar to CBA but we use IRGs to build classifiers instead of all rules.
Due to space limitation, we do not explain the details of the IRG classifier.For CBA, we set the minimum support threshold as 0.7 * number of training data of class C for each class C and set the minimum confidence threshold as 0.8 (According to our experiments, if we further lower the minimum confidence threshold, the final CBA classifier is the same); For IRG classifier, we use the same parameters as CBA; For SVM, we always use the default setting of SV M light [12].
Table 2 illustrates the percentages of correctly predicted test data for the IRG classifier, CBA and SVM on the 5 microarray datasets.
We can see that the IRG classifier has the highest average accuracy.
Although SVM performs very well on LC and ALL, it fails on BC.
No classifier outperforms the others on all datasets.
Our IRG classifier is both efficient and easily understandable and thus could be a good reference tool for biological research.
Association rule mining has attracted considerable interest since a rule provides a concise and intuitive description of knowledge.
It has already been applied on biological data, such as [7,8,19].
Association rule can relate gene expressions to their cellular environments or categories, thus available for building accurate classifiers on microarray datasets as in [9,13].
Moreover, it can discover the relationship between different genes, so that we can infer the function of an individual gene based on its relationship with others [7] and build the gene network.
Association rules might reveal more patterns than clustering [5], considering that a gene may belong to many rules while it is usually grouped to one cluster (or a hierarchy of clusters).
There are many proposals about rule mining in the data mining literatures.
They can be roughly divided into three classes.
The first two classes are related to association rule mining.
All existing association rule mining algorithms adopt the column enumeration in the mining process, therefore they are very time-consuming on microarray datasets.
The first class of rule mining algorithms identifies the interesting (or optimal) rules with some interestingness measures [2].
The interesting rule discussed in [2] is quite similar to our interesting rule group.
However, [2] randomly picks a rule for each rule group while FARMER discovers the upper bound and lower bounds for each interesting rule group.The second class of rule mining algorithms aims to find all association rules satisfying user-specified constraints by identifying all frequent itemsets at the key step, such as [1,11].
Recently the concept of closed itemset [18] is proposed to reduce redundant itemsets and rules [22].
Several efficient mining algorithms [18,23,21] are proposed to mine frequent closed itemsets.
On the other hand, there is some work [16,20] that investigates incorporating item constraints to reduce the number of frequent itemsets.
Other work [3,15] leverages the item constraint as rule consequent and utilizes minimum thresholds of confidence, support and other statistic constraints.
FARMER differs from these approaches in term of its enumeration method and pruning strategies.The third class of algorithms aims at mining predictive rules.
One example is the decision tree induction algorithm [10].
Alternatively, some work [9,14] has been done to build classifiers from association rules and has obtained better classification results than decision trees in many cases.
It is obvious that these methods are also applicable based on the concept of interesting rule groups.In a short paper [17], the idea of using row enumeration for mining closed patterns in biological datasets is introduced.
The idea is however restricted to finding frequent closed patterns that satisfy a certain support threshold.
FARMER on the contrary finds IRGs that satisfy interestingness constraints like minconf and minchi.
The effectiveness of pruning with such constraints is evident in our experiments.
In this paper, we proposed an algorithm called FARMER for finding the interesting rule groups in microarray datasets.
FARMER makes use of the special characteristic of microarray datasets to enhance its efficiency.
It adopts the novel approach of performing row enumeration instead of the conventional column enumeration so as to overcome the extremely high dimensionality of microarray datasets.
Experiments show that FARMER outperforms existing algorithms like CHARM and ColumnE by a large order of magnitude on microarray datasets.Our IRG classifier built on interesting rule groups demonstrates the usefulness of discovered IRGs.
Our experiments showed that it has the highest average accuracy compared with CBA and SVM.
• For our case, we assume the dataset can be fitted into the main memory and used pointerbased algorithm similar to BUC r SampleN ~Cancer 100-500 rows• Find closed patterns which occur frequently among genes.
• Find rules which associate certain combination of the columns that affect the class of the rows -Gene1,Gene10,Gene1001 -> Cancer Challenge Ilower bounds• Large number of patterns/rules -number of possible column combinations is extremely high• Solution: Concept of a closed pattern -Patterns are found in exactly the same set of rows are grouped together and represented by their upper bound• Example: the following patterns are found in row 2,3 and 4 i ri Class 1 a ,b,c,l,o,s C 2 a ,d, e , h ,p,l,r C 3 a ,c, e , h ,o,q,t C 4 a , e ,f, h ,p,r ~C 5 b,d,f,g,l,q,s,t ~C i ri Class 1 a,b,c,l,o,s C 2 a,d,e,h,p,l,r C 3 a,c,e,h,o,q,t C 4 a,e,f,h,p,r ~C 5 b,d,f,g,l,q,s,t ~C C ~C a 1,2,3 4 e 2,3 4 h 2,3 4TT| {2,3} Example Table Transposed Table, r4 has 100% support in the conditional table of "r2r3", therefore branch "r2 r3r4" will be pruned.Pruning method 2• if a rule is discovered before, we can prune enumeration below this node -Because all rules below this node has been discovered before -For example, at node 34, if we found that {aeh} has been found, we can prune off all branches below it TT| {3,4} Pruning Method 3: Minimum Support C ~C a 1,2,3 4 b 1 5 c 1,3 l 1,2 5 o 1,3 s 1 5ij R ( ij ) TT| {1}• Example: From TT| {1} , we can see that the support of all possible pattern below node {1} will be at most 5 rows.From CARPENTER to FARMER a ,b,c,l,o,s C 2 a ,d, e , h ,p,l,r C 3 a ,c, e , h ,o,q,t C 4 a , e ,f, h ,p,r ~C 5 b,d,f,g,l,q,s,t ~C1 5 c 1,3 d 2 5 e 2,3 4 f 4,5 g 5 h 2,3 4 l 1,2 5 o 1,3 p 2 4 q 3 5 rExperimental studies • Efficiency of FARMER -On five real-life Switching Condition• Naïve idea of switching based on row number and feature number does not work well • to estimate the required computation of an enumeration sub-tree, i.e., row enumeration sub-tree or feature enumeration sub-tree.
-Estimate the maximal level of enumeration for each children subtree• Example of estimating the maximal level of enumeration:-Suppose r=10, S(f1)=0.8, S(f2)=0.5, S(f3)=0.5, S(f4)=0.3 and minsup=2 -S(f1)*S(f2)*S(f3)*r =2 ≥ minsup -S(f1)*S(f2)*S(f3)*S(f4)*r =0.6 < minsup -Then the estimated deepest node under f1 is f1f2f3Switching Condition Switching ConditionTo estimate for a node:To estimate for a path:To sum up estimation of all paths as the final estimation .
.
SampleN-1 SampleNExtension of our work by other groups (with or without citation) III From TT| {1} , we can see that the support of all possible pattern below node {1} will be at most 5 rows.
C ~C a 1,2,3 4 b 1 5 c 1,3 l 1,2 5 o 1,3 s 1 5ij R ( ij ) TT| {1}Extension of our work by other groups (with or without citation) VII TT| {3,4}Extension of our work (Conclusion)• The sample/enumeration framework had been successfully adopted by other groups in mining microarray datasets • We are proud of our contribution as the group the produce the first row/sample enumeration algorithm CARPENTER and is happy that other groups also find the method useful • However, citations from these groups would have been nice.
After all academic integrity is the most important things for a researcher.
• Find closed patterns which occur frequently among genes.
• Find rules which associate certain combination of the columns that affect the class of the rows -Gene1,Gene10,Gene1001 -> Cancer Challenge Ilower bounds• Large number of patterns/rules -number of possible column combinations is extremely high• Solution: Concept of a closed pattern -Patterns are found in exactly the same set of rows are grouped together and represented by their upper bound• Example: the following patterns are found in row 2,3 and 4 TT| {2,3} Example Table Transposed Table, r4 has 100% support in the conditional table of "r2r3", therefore branch "r2 r3r4" will be pruned.Pruning method 2• if a rule is discovered before, we can prune enumeration below this node -Because all rules below this node has been discovered before -For example, at node 34, if we found that {aeh} has been found, we can prune off all branches below it ij R ( ij ) • Example: From TT| {1} , we can see that the support of all possible pattern below node {1} will be at most 5 rows.From CARPENTER to FARMER 1 5 c 1,3 d 2 5 e 2,3 4 f 4,5 g 5 h 2,3 4 l 1,2 5 o 1,3 p 2 4 q 3 5 rExperimental studies • Efficiency of FARMER -On five real-life • Naïve idea of switching based on row number and feature number does not work well • to estimate the required computation of an enumeration sub-tree, i.e., row enumeration sub-tree or feature enumeration sub-tree.
-Estimate the maximal level of enumeration for each children subtree• Example of estimating the maximal level of enumeration:-Suppose r=10, S(f1)=0.8, S(f2)=0.5, S(f3)=0.5, S(f4)=0.3 and minsup=2 -S(f1)*S(f2)*S(f3)*r =2 ≥ minsup -S(f1)*S(f2)*S(f3)*S(f4)*r =0.6 < minsup -Then the estimated deepest node under f1 is f1f2f3Switching Condition To estimate for a node:To estimate for a path:To sum up estimation of all paths as the final estimation .
.
Extension of our work by other groups (with or without citation) III From TT| {1} , we can see that the support of all possible pattern below node {1} will be at most 5 rows.
ij R ( ij ) Extension of our work by other groups (with or without citation) VII Extension of our work (Conclusion)• The sample/enumeration framework had been successfully adopted by other groups in mining microarray datasets • We are proud of our contribution as the group the produce the first row/sample enumeration algorithm CARPENTER and is happy that other groups also find the method useful • However, citations from these groups would have been nice.
After all academic integrity is the most important things for a researcher.
