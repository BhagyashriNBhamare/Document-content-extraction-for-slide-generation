Traffic anomalies such as failures and attacks are commonplace in today's network, and identifying them rapidly and accurately is critical for large network operators.
The detection typically treats the traffic as a collection of flows that need to be examined for significant changes in traffic pattern (e.g., volume, number of connections).
However, as link speeds and the number of flows increase , keeping per-flow state is either too expensive or too slow.
We propose building compact summaries of the traffic data using the notion of sketches.
We have designed a variant of the sketch data structure, k-ary sketch, which uses a constant, small amount of memory, and has constant per-record update and reconstruction cost.
Its linearity property enables us to summarize traffic at various levels.
We then implement a variety of time series forecast models (ARIMA, Holt-Winters, etc.) on top of such summaries and detect significant changes by looking for flows with large forecast errors.
We also present heuristics for automatically configuring the model parameters.
Using a large amount of real Internet traffic data from an operational tier-1 ISP, we demonstrate that our sketch-based change detection method is highly accurate, and can be implemented at low computation and memory costs.
Our preliminary results are promising and hint at the possibility of using our method as a building block for network anomaly detection and traffic measurement.
Traffic anomalies are an integral part of daily life for today's network operators.
Some traffic anomalies are expected or unanticipated but tolerable.
Others are often indications of performance bottlenecks due to flash crowds [25], network element failures, or malicious activities such as denial of service attacks (DoS) [23] and worms [28].
Suitable motivation exists to process massive data streams (available from diverse sources) quickly to examine them for anomalous behavior.Two basic approaches to network anomaly detection are common.
The first approach is the signature-based approach.
It detects traffic anomalies by looking for patterns that match signatures of known anomalies.
For example, Moore et al. [29] infer DoS activities based on address uniformity, a property shared by several popular DoS toolkits.
Signature-based methods have been extensively explored in the literature and many software systems and toolkits such as Bro [31] and Snort [32] have been developed and are being used.
One limitation of this approach is the requirement that the anomaly signatures be known in advance; thus it cannot be applied to identify new anomalies.
Also, a malicious attacker can evade signature-based detection systems by garbling the signatures.
One can see a parallel in the failure of filter-based spam fighting systems where spammers introduce random hashes in the spam messages.The second approach is the statistics-based approach, which does not require prior knowledge about the nature and properties of anomalies and therefore can be effective even for new anomalies or variants of existing anomalies.
A very important component of statisticsbased approach is change detection.
It detects traffic anomalies by deriving a model of normal behavior based on the past traffic history and looking for significant changes in short-term behavior (on the order of minutes to hours) that are inconsistent with the model.
Our goal in this work is to come up with an efficient, accurate, and scalable change detection mechanism for detecting significant changes in massive data streams with a large number of flows.
Change detection has been extensively studied in the context of time series forecasting and outlier analysis [35,36,12,13].
The standard techniques include different smoothing techniques (such as exponential smoothing or sliding window averaging), the BoxJenkins ARIMA modeling [6,7,2], and finally the more recent wavelet-based techniques [4,3].
Prior works have applied these techniques to network fault detection and intrusion detection.
Examples in fault detection include [22,26,38].
Feather et al.identify faults based on statistical deviations from normal traffic behavior [18]; a method of identifying aberrant behavior by applying thresholds in time series models of network traffic is described in [9].
Methods for intrusion detection include neural networks [20], Markov models [40], and clustering [34].
Barford et al.recently provide a characterization of different types of anomalies [4] and propose wavelet-based methods for change detection [3].
Unfortunately, existing change detection techniques can typically only handle a relatively small number of time series.
While this may suffice for detecting changes in highly aggregated network traffic data (e.g., SNMP link counts with 5 minute sample interval), they cannot scale up to the needs at the network infrastructure (e.g., ISP) level.
At an ISP level, traffic anomalies may be buried inside the aggregated traffic, mandating examination of the traffic at a much lower level of aggregation (e.g., IP address level) in order to expose them.
Given today's traffic volume and link speeds, the detection method has to be able to handle potentially several millions or more of concurrent network time series.
Directly applying existing techniques on a per-flow basis cannot scale up to the needs of such massive data streams.
Recent research efforts have been directed towards developing scalable heavy-hitter detection techniques for accounting and anomaly detection purposes [17].
Note that heavy-hitters do not necessarily correspond to flows experiencing significant changes and thus it is not clear how their techniques can be adapted to support change detection.
In the database research community, however, computation over massive data streams has been an active research area over the past several years.
The emerging field of data stream computation deals with various aspects of computation that can be performed in a space-and time-efficient fashion when each tuple in a data stream can be touched only once (or a small number of times).
A good survey of the algorithms and applications in data stream computation can be found in [30].
One particularly powerful technique is sketch [24,21,15], a probabilistic summary technique proposed for analyzing large streaming datasets.
Sketches avoid keeping perflow state by dimensionality reduction, using projections along random vectors.
Sketches have some interesting properties that have proven very useful in data stream computation: they are space efficient, provide provable probabilistic reconstruction accuracy guarantees, and are linear (i.e., sketches can be combined in an arithmetical sense).
In this work, we incorporate data stream computation techniques into change detection.
Our solution, labeled sketch-based change detection, is the first one, to the best of our knowledge, that is capable of detecting significant changes in massive data streams with a large number of network time series.
With sketch-based change detection, we first build compact summaries of the traffic data using sketches.
We have designed a variant of the sketch data structure, k-ary sketch, which uses a constant, small amount of memory, and has constant per-record update and reconstruction cost.
We then implement a variety of time series forecast models (ARIMA, Holt-Winters, etc.) on top of such summaries and detect significant changes by looking for flows with large forecast errors.
Being able to compute significant differences in the list of top flows quickly can point towards potential anomalies.
Depending on the length of the time period for which we compute forecasts and the duration of significant changes, we can accurately identify the presence of an anomaly.
Note that an anomaly can be a benign surge in traffic (like a flash crowd) or an attack.
We also present heuristics for configuring the model parameters.We demonstrate using a large amount of real Internet traffic data that our sketch-based change detection method is highly accurate when compared with per-flow analysis, and can be implemented at low computation and memory costs.
Our evaluation shows that we can reconstruct lists of the top flows in a time period efficiently and accurately; we are also able to achieve similar forecast errors when compared with per-flow techniques.
While our techniqueshave not yet been directly applied to anomaly detection, our preliminary results in change detection are promising and we believe that our method can serve as a building block for network anomaly detection.
The rest of the paper is organized as follows: Section 2 gives an overview of the framework of our sketch-based change detection, followed by detailed discussions of different modules in Section 3.
Section 4 describes the experimental setup.
Section 5 presents key portions of the results of our extensive testing of sketch-based change detection on different large and real datasets.
We summarize our ongoing research in Section 6 and conclude the paper in Section 7.
Over the past several years, various models have been proposed to describe data streams, including Time Series Model, Cache Register Model, and Turnstile Model [30].
We use the the most general one-the Turnstile Model.Specifically, let I = α1, α2, · · · , be an input stream that arrives sequentially, item by item.
The above model is very general and one can instantiate it in many ways with specific definitions of the key and updates.
In the context of network anomaly detection, the key can be defined using one or more fields in packet headers such as source and destination IP addresses, source and destination port numbers, protocol number etc.
It is also possible to define keys with entities like network prefixes or AS numbers to achieve higher levels of aggregation.
The update can be the size of a packet, the total bytes or packets in a flow (when flow-level data is available).
To keep the parameter space within a manageable size, however, we only use destination IP address and bytes in the experiments discussed in this paper.
Alternative choice of keys and values may affect the running time, but we expect the accuracy results to be quite similar.
In an ideal environment with infinite resources, we can perform time series forecasting and change detection on a per-flow basis.
Specifically, we break time into discrete intervals I1, I2, · · · .
For each time interval It, and each signal A[a] that appears before or during interval It, we first compute the observed value-total update to A[a] during interval It: oa(t) = ¡ i∈Aa(t) ui, where the set of indices Aa(t) def = {i | ai = a ∧ (ai, ui) arrives during It}.
We also compute the forecast value fa(t) by applying a forecasting model to observed values in the past intervals.
We then compute the forecast error ea(t) = oa(t) − fa(t) and raise an alarm whenever ea(t) is significant according to certain detection criteria.In the real world, however, per-flow analysis can be prohibitive because the number of signals present in the input stream can be very large.
For instance, if we use source and destination IPv4 addresses as the key, the key space [u] can be as large as 2 64 , and the number of signals can easily reach tens of millions given today's traffic volume and link speeds.
Hence it can be too slow or too expensive to perform change detection on a per-flow basis.Our solution is to create sketches to summarize the input stream and implement various forecasting models on top of the sketches.
Specifically, our sketch-based change detection consists of the following three basic modules:1.
Sketch module 2.
Forecasting module 3.
Change detection moduleThe first module-sketch module-creates a (space-and timeefficient) sketch to summarize all the observed values oa(t) (total update to signal A[a]) during each time interval It-the observed sketch So(t).
The forecasting module produces a forecast sketch S f (t) using some forecasting models based on observed sketches in the past intervals.
It then computes the forecast error sketch Se(t) as the delta between So(t) and S f (t), i.e., Se(t) = So(t) − S f (t).
The linearity of the sketch data structure allows us to implement various forecasting models and compute the deltas directly at the sketch level.
The change detection module uses the error sketch Se(t) to determine significant changes.
We next describe these modules in details.
Let (a1, u1), (a2, u2), · · · be an input stream (for example, the substream of I that is observed during a given time interval).
For each key a ∈ [u], let va = ¡ i∈Aa ui, where the set of indices Aa def = {i | ai = a}.
For each interval, the second moment (F2) is defined as the sum of squares of the values associated with all the keys, i.e., F2def = ¡ a v 2a .
We refer to the square root of the second moment ( √ F2) as the L2 norm.The sketch module uses the sketch data structure to summarize all the va in each time interval.
Sketch is a probabilistic summary data structure based on random projections (See [30] for a good overview of sketches and the general field of data stream computation).
We have designed a variant of the sketch data structure, which we call the k-ary sketch.
The k-ary sketch is similar to the count sketch data structure recently proposed by Charikar et al. [11].
However, the most common operations on k-ary sketch use simpler operations and are more efficient than the corresponding operations defined on count sketches [33].
Just like the count sketch, a k-ary sketch S consists of a H × K table of registers:TS[i][j] (i ∈ [H], j ∈ [K]).
Each row TS[i][·] (i ∈ [H])is associated with a hash function from [u] to [K]: hi.
We can view the data structure as an array of hash tables.
The hash functions are required to be 4-universal [10,39] to provide probabilistic guarantees of reconstruction accuracy.
We construct them using the fast tabulation-based method developed in [33].
Different hi are constructed using independent seeds, and are therefore independent.There are four basic operations defined for k-ary sketches: UP-DATE to update a sketch, ESTIMATE to reconstruct va for a given key a, ESTIMATEF2 to estimate the second moment F2, and COM-BINE to compute the linear combination of multiple sketches.
They are used in various modules of our change detection scheme: UP-DATE in the sketch module to update the observed sketch So(t); COMBINE in the forecasting module to implement various forecasting models and to compute the forecast sketch S f (t) and forecast error sketch Se(t); ESTIMATE in the change detection module to reconstruct forecast errors from Se(t); and ESTIMATEF2 in the change detection module to choose the threshold for judging whether forecast errors are significant.The formal specification of these operations is as follows.1.
UPDATE(S, a, u):For ∀i ∈ [H], TS[i][hi(a)]+ = u. 2.
ESTIMATE(S, a): Let sum(S) = ¡ j∈[K] TS[0][j]be the sum of all values in the sketch, which only needs to be computed once before any ESTIMATE(S, a) is called.
Return an estimate of vav est a = median i∈[H] {v h i a }wherev h i a = T [i][hi(a)] − sum(S)/K 1 − 1/K As shown in Appendix A, each v h i a (i ∈ [H])is an unbiased estimator of va with variance inversely proportional to (K − 1).
v est a further improves accuracy by avoiding the extreme estimates.3.
ESTIMATEF2(S): Return an estimate of the second momentF est 2 = median i∈[H] {F h i 2 } whereF h i 2 = K K − 1 j∈[K] (TS[i][j]) 2 − 1 K − 1 (sum(S)) 2As shown in Appendix B, each F h i 2 forms an unbiased estimator of F2 with variance inversely proportional to (K − 1).
F est 2 further improves accuracy by avoiding the extreme estimates.4.
COMBINE(c1, S1, · · · , c 񮽙 , S 񮽙 ):The linearity of the sketch data structure allows us to linearly combine multiple sketches S = ¡ 񮽙 k=1 c k · S k by combining every entry in the table:TS[i][j] = 񮽙 k=1 c k · TS k [i][j] The forecasting module uses the observed sketches in the past intervals So(t 񮽙 ) (t 񮽙 < t) to compute the forecast sketch S f (t) and along with it the error between the observed and forecast sketches as Se(t).
In this work, we explore six models commonly used in univariate time series forecasting and change detection.
The first four models are simple smoothing models; the other two belong to the family of ARIMA models.
All six models can be implemented on top of sketches by exploiting the linearity property of sketches.
The first four models are simple smoothing models and are popular due to their simplicity.
They are moving average (MA), exponentially weighted moving average (EWMA), S-shaped moving average (SMA), and non-seasonal Holt-Winters (NSHW).
Moving Average (MA) This forecasting model assigns equal weights to all past samples, and has a single integer parameter W ≥ 1 which specifies the number of past time intervals used for computing the forecast for time t.S f (t) = ¡ W i=1 S f (t − i) W , W ≥ 1S-shaped Moving Average (SMA) This is a class of weighted moving average models that give higher weights to more recent samples.S f (t) = ¡ W i=1 wi · S f (t − i) ¡ W i=1 wi , W ≥ 1We use a subclass that gives equal weights to the most recent half of the window, and linearly decayed weights for the earlier half (see [19] for discussion).
Exponentially Weighted Moving Average (EWMA) With exponentially weighted moving average, the forecast for time t is the weighted average of the previous forecast and the newly observed sample at time t − 1.
S f (t) = α · So(t − 1) + (1 − α) · S f (t − 1), t > 2 So(1), t = 2The parameter α ∈ [0, 1] is called the smoothing constant.
It indicates how much weight is given to new samples vs. the history.
Non-Seasonal Holt-Winters (NSHW) The Holt-Winters model [8] is another commonly used smoothing model and it has been applied in [9] to detect aberrant behavior.
In the non-seasonal HoltWinters model, there is a separate smoothing component Ss(t) and a trend component St(t).
There are two parameters α ∈ [0, 1] and β ∈ [0,1].
Ss(t) = α · So(t − 1) + (1 − α) · S f (t − 1), t > 2 So(1), t = 2 St(t) = β · (Ss(t) − Ss(t − 1)) + (1 − β) · St(t − 1), t > 2 So(2) − So(1), t = 2The forecast is then S f (t) = Ss(t) + St(t).
Box-Jenkins methodology, or AutoRegressive Integrated Moving Average (ARIMA) modeling technique [6,7], is a class of linear time series forecasting techniques that capture the linear dependency of the future values on the past values.
They are able to model a wide spectrum of time series behavior.
As a result, they have been extensively studied and widely used for univariate time series forecasting and change detection.An ARIMA model includes three types of parameters: the autoregressive parameter (p), the number of differencing passes (d), and the moving average parameter (q).
In the notation introduced by Box and Jenkins, models are summarized as ARIMA (p, d, q).
A model described as (0, 1, 2) means that it contains p = 0 (zero) autoregressive parameters and q = 2 moving average parameters which were computed for the time series after it was differenced once (d = 1).
Note that we use only integral values for p, d, and q. Although there has been recent work on models with fractional d (the ARFIMA model) in the context of action-reaction models [27], we have not yet examined their application in the networking context.A general ARIMA model of order (p, d, q) can be expressed as:Zt − q i=1 M Ai · Zt−i = C + et − p j=1 ARj · et−iwhere Zt is obtained by differencing the original time series d times, et is the forecast error at time t, M Ai (i = 1, ..., q) and ARj (j = 1, ..., p) are MA and AR coefficients.
In practice, p and q very rarely need to be greater than 2.
The number of differences (d) is typically either 0 or 1.
Therefore, when we extend ARIMA models to the sketch context, we only consider the following two types of ARIMA models (the names are based on the number of differences):• ARIMA0: ARIMA models of order (p ≤ 2, d = 0, q ≤ 2) • ARIMA1: ARIMA models of order (p ≤ 2, d = 1, q ≤ 2)In ARIMA models, the choice of MA and AR coefficients M Ai (i = 1, ..., q) and ARj (j = 1, ..., p) must ensure that the resulting models are invertible and stationary.
As a necessary but insufficient condition, M Ai and ARj must belong to the range [−2, 2] when p, q ≤ 2.
After constructing the forecast error sketch Se(t), the change detection module chooses an alarm threshold TA based on the estimated second moment of Se(t):TA def = T ·[ ESTIMATEF2(Se(t)) ] 1 2 ,where T is a parameter to be determined by the application.
Now for any key a, the change detection module can reconstruct its forecast error in Se(t) using ESTIMATE(Se(t), a) and raise an alarm whenever the estimated forecast error is above the alarm threshold TA.The remaining question is how to obtain the stream of keys for the change detection module.
Sketches only support reconstruction of the forecast error associated with a given key.
It does not contain information about what keys have appeared in the input stream.There are several possible solutions to this problem.
With the brute-force solution, one can record all the keys that appeared in recent intervals (e.g., the same interval t over which Se(t) is defined) and replay them after Se(t) has been constructed.
This still requires maintaining per-flow information.
Its scalability is limited by the maximum number of keys that appear in the window for key collection.
We can avoid keeping per-flow state by using a two-pass algorithm-construct Se(t) in the first pass and detect changes on the second pass.
Since the input stream itself will provide the keys, there is no need for keeping per-flow state.
This requires access to the same input stream twice and thus useful only in the offline context.
A third alternative is to use the keys that appear after Se(t) has been constructed.
This works in both online and offline context.
The risk is that we will miss those keys that do not appear again after they experience significant change.
This is often acceptable for many applications like DoS attack detection, where the damage can be very limited if a key never appears again.
Note that we do not need to do this for every newly arrived item.
If we can tolerate the risk of missing some very infrequent keys, we can sample the (future) input streams and only work on a substream of keys.
Another possibility is to incorporate combinatorial group testing into sketches [14].
This allows one to directly infer keys from the (modified) sketch data structure without requiring a separate stream of keys.
However, this scheme also increases the update and estimation costs and additional research is required to make it more efficient.
In this paper, we use the offline two-pass algorithm in all experiments.
Our change detection framework includes sketch-related parameters as well as control parameters for various forecasting models.
Below we provide guidelines and heuristics for properly configuring these parameters-an important step for making our framework practical.
There are two sketch-related parameters: the number of hash functions (H), and the size of hash tables (K).
Depending on the choice of H and K, k-ary sketches can provide probabilistic guarantees on the estimation accuracy of the forecast errors and their total energy (see Appendix A and B for details).
We can use such analytical results to determine the choice of H and K that are sufficient to achieve targeted accuracy.
As the analytical results apply in a data-independent way, the resulting H and K may be too conservative for the actual dataset.
Hence, we use analytical results to derive data-independent choice of H and K and treat them as upper bounds.
We then use training data to find the best (data-dependent) H and K values.
Criteria for good parameters In the context of univariate time series forecasting, a commonly used simple heuristic for configuring model parameters is choosing parameters that minimize the total residual energy, i.e., the sum of squares (of forecast errors) over time.We can extend the above heuristic to the sketch context and look for parameters that minimize the total energy in the resulting forecast error sketches over time ¡ t F2(Se(t)), where F2(Se(t)) is the second moment for all the forecast errors summarized by sketch Se(t).
We will not know the true F2(Se(t)) unless we do per-flow analysis for each parameter setting, which can be prohibitive.
Instead we use the estimated second moment F est 2 (Se(t)), as long as F est 2 (Se(t)) closely approximates F2(Se(t)).
In other words, we try to find parameters that minimize the estimated total energy of forecast errors ¡ t F est 2 (Se(t)).
Multi-pass grid search For parameters that are continuous, we use a multi-pass grid search algorithm to find a good choice.
Consider for example the EWMA model.
The first pass finds a parameter α ∈ {0.1, 0.2, ..., 1.0} that minimizes the estimated total energy for the forecast errors.
Let a0 be the best α.
The second pass equally subdivides range [a0−0.1, a0+0.1] into N = 10 parts and repeats the process.
We obtain high precision via multiple passes.
For models with integral parameters, such as the moving average model, we can simply vary the parameter to find the best one.
Note that grid search is only a heuristic.
It does not guarantee that we will find the optimal parameter combination that minimizes the estimated total energy for forecast errors.
However, we only need to have good enough parameters such that the resulting model captures the overall time series behavior.
We will show later that grid search indeed achieves this.
We use large amounts of real Internet traffic data to evaluate and validate our approach.
Below we describe our datasets and the experimental parameter settings.
Input data is chosen to be four hours worth of netflow dumps from ten different routers in the backbone of a tier-1 ISP.
Nearly 190 million records are processed with the smallest router having 861K records and the busiest one having over 60 million records in a contiguous four hour stretch.
In this section we present the various values of parameters that we used in our experiments and justify their choices.
We also present ways in which these values should be tailored in using our approach based on the local data available.
Note that some of the parameters would have different values when the sketch technique is used for different applications.The cost of estimation and updating is dominated by the number of hash tables, so we choose small values for H. Meanwhile, H improves accuracy by making the probability of hitting extreme estimates exponentially small (see Theorem 2, 3, and 5 in Appendix), suggesting again that it is enough to use small H.
We vary H to see the impact on the accuracy of estimation with respect to the cost.
Our choices of H (1, 5, 9, and 25) are driven by the fact that we can use optimized median networks to find the medians quickly without making any assumptions on the nature of the input [16,37].
The analytic upper bound needed to provide a specific degree of error threshold by using k-ary sketches is used as the upper reach of K.
We can tighten the lower bound of zero by empirically examining values between 0 and the upper bound in log(upper-bound) steps.In our experiments we used an upper bound of 64K and using our data we quickly zoomed in on a lower bound of K = 1024.
Another important parameter is the interval size: a long interval would result in delays since our scheme reports anomalies at the end of each interval and we will miss more events that occur within a single interval.
A short interval requires us to update the sketch-based forecasting data structures more frequently.
We choose 5 minutes as a reasonable tradeoff between the responsiveness and the computational overhead.
Such an interval is used in other SNMP based network anomaly detection systems [3].
We also use 1 minute intervals to examine the impact of shorter intervals.Each of the six models requires different choices of parameters.
For the moving average models (MA and SMA) we pick a single time interval to be the minimum window size and 10 (12) to be the maximum window size for interval size of 5 (1) minutes.
The window size yielding the minimum total energy of forecast errors across each of the interval values is selected as the parameter.
For the remaining models we apply a 2-pass grid search algorithm to choose different parameters.
For the EWMA and NSHW models, during each pass we partition the current ranges into 10 equal intervals.
For ARIMA models, however, the number of parameters is much larger and the search space becomes too large if we partition each parameter range into 10 parts.
To limit the search space, we partition the current search range into 7 parts instead.
During grid search, H is fixed at 1 and K at 8K.
As we will see later, with H = 1 and K = 8K, the estimated total energy of forecast errors closely approximates the true energy obtained using per-flow analysis.
In this section, we present the results of our evaluation of the feasibility of using sketches for change detection.
The setup for the various experiments is described in Section 4.2 and we present results in detail for three models (EWMA and ARIMA with d = 0 and 1) and occasional results for NSHW.
We should note that in most cases the results from the various models are largely similar and we exclude them in the interest of brevity.The evaluation is divided into three parts: We first report on the validity of the parameters generated by our grid search.
Next, we report on evaluation of sketches at the flow-level-focusing on what sketch reports as (i) the top-N flows with the maximum absolute forecast errors, and (ii) the flows whose absolute forecast error exceeds a threshold, and comparing the sketch report with what per-flow scheme reports.
We then report on the implementation complexity and the running time.
The experiments in this section are concerned with determining appropriate parameter settings for the forecast models, values for H and K, and in evaluating the usefulness of grid search.
• We use the estimated total energy (instead of the true total energy) as the metric for selection of the forecast model parameter setting(see Section 3.4.2).
For this approach to yield good performance, we need to ensure that the estimated value closely tracks the true value.
This is the focus of the first part of the study in this section.
• We also explore the space of (H,K) values and various parameter settings to zoom in on suitable choices of H and K that result in good performance.
• Note that we use grid search to select the parameter setting that results in the minimum total energy (see Section 3.4.2).
In this section we evaluate the "goodness" of the parameter selected by grid search, compared to a random selection of parameters.
We first perform a set of experiments (called random) over a collection of 10 router files (consisting of over 189 million flow records).
For each forecast model, we randomly select a number of points in the model parameter space, and for each chosen point and (H,K) value combination, run both sketch and per-flow based detection on each router trace.
The goal here is to examine differences between the different forecast models, and to evaluate parameter value choices for H and K (the hash table and range sizes).
The experiment also allows us to explore how sketches and per-flow compare when the forecast parameters are not selected carefully.
The comparison metric is the Relative Difference, which is defined as: the difference between the total energy (square root of the sum of second moments for each time interval) computed from the sketchbased technique and the total energy obtained using per-flow detection, expressed as a percentage of the total energy obtained using per-flow detection.
For a particular forecast model and (H,K) combination, for each router file, we obtain multiple Relative Difference values, one for each selected point in the parameter space for that model.
In Figures 1-3, each curve corresponds to a particular forecast model and (H,K) combination, and represents the empirical CDF of the Relative Difference values aggregated from across all the routers.
Figure 1 shows that even for small H (1) and K (1024), across all the models, most of the mass is concentrated in the neighborhood of the 0% point on the x-axis, indicating that even for randomly chosen model parameters, the total energy from the sketch-based approach is very close to that for per-flow.
Only for the NSHW model a small percentage of points have sketch values that differ by more than 1.5% from the corresponding per-flow values.
The worst case difference is 3.5%.
Next, we examine the impact of varying the H parameter.
Fig- ure 2 shows, for the EWMA and ARIMA0 models, that there is no need to increase H beyond 5 to achieve low relative difference.The last set of results for the random parameter technique is shown in Figure 3, and demonstrates that once K = 8192 (8K) the relative difference becomes insignificant, obviating the need to increase K further.The grid search technique for identifying parameters uses six models for both 60s and 300s intervals, a representative sample of router files (one large, one medium, and one small sized file), and (H=1, K=8192) combination.
For each (model,router,H,K) combination, grid search outputs the parameter value(s) for that model that minimize the total energy in the resulting forecast errors.
Using this parameter setting output by grid search, we run per-flow analysis and obtain the corresponding total energy.
The per-flow estimate is then compared against the per-flow estimates of the random parameters generated in the previous technique, for the same router file and model.
The goal of this experiment is twofold: first, we ensure that grid search results are never worse than any of the per-flow values of the random parameters.
Second, we show that grid search results can be significantly better than the results in the random case.
Our experimental results (no graphs shown) show that in all cases (all models, three router files, both intervals) grid search is never worse than the random parameters.
Secondly, in at least 20% of the cases the results with the random parameters are at least twice (and in many cases much more) as bad as the errors in the grid search case.
This justifies the use of grid search to generate the parameters for the remainder of the experiments.
After validating the set of parameters from the grid search scheme, we need to demonstrate that the sketch scheme's results are accurate as compared to per-flow estimates.
We estimate accuracy in our experiments via two techniques: (i) Top-N, and (ii) Thresholding.The values of H and K are key to the accuracy of our forecasts as well as efficient operation.
Care must be taken to choose the proper range of values on a per-application basis.
Our experimental results based on large and diverse data sets show that the values we have chosen (H = 1.
.25), (K = 1K .
.
64K) are indeed the right ones for the change detection class of applications.
Here we are interested in exploring, for a given N, how many of the top-N flows (ranked in order of decreasing magnitude of forecasting errors) detected by the per-flow scheme are also detected as top-ranking by our sketch-based scheme.We choose three values of H (5, 9, 25) and K (8K, 32K, 64K), two values of intervals (60s and 300s), and selected three router data files representing high volume (over 60 Million), medium (12.7 Million), and low (5.3 Million) records, to carry out sketch accuracy evaluation across all models.
For the model parameters, we use the parameter values selected by the grid search process.
For each time interval, we generate the top-N flows with the maximum absolute forecast errors (recall that a higher absolute forecast error indicates that a flow's volume has a higher deviation from that predicted by the underlying model) for both sketches and per-flow techniques.
For four values of N (50, 100, 500, 1000), we see how many of the top-N flows are in common between the two resulting sets and compute a similarity metric NAB/N , where NAB is the number of common elements in the two sets.While some of the top-N ranked elements from the per-flow technique may not belong to exactly the top-N elements output by the sketch technique, the hope is that these elements will still be high in the sketch-based ranking.
Thus, it is possible to increase the accuracy by comparing the top-N per-flow list with additional elements in the sketch-based ranked list.
To evaluate this possibility, a second set of comparisons involves comparing the top-N per-flow results against the top-X*N (X = 1, 1.2, 1.5, 2) results from the sketch-based approach.
Results in this section show how well sketches perform when compared with per-flow by comparing their top-N (N=50, 100, 500, 1000) flows.
The metric is essentially a similarity one: the number of common elements in the two sets normalized by N.
We start by showing how this metric is remarkably consistent across the time intervals, for moderate H and K.
We set aside the first hour of the four hour data sets for model warmup purposes leaving 180 and 37 intervals respectively in the 60s and 300s time interval cases.
Fig- ures 4 (a) and (b) show that even for large N (1000), the similarity is around 0.95 for both the 60s and 300s intervals, for H=5 and K=32K.
In the rest of the figures in this section, we show the mean similarity value across the 180 and 37 intervals.
, where H is fixed at 5 and K varies between 8K and 64K, for both 300s and 60s time intervals.
As can be seen, for K=32K, the similarity is over 0.95 even for large N. For a smaller N (say 50 or 100), the overlap is nearly 100%.
Larger values of K are of limited additional benefit.
We note that similarity improves (for large N) with the smaller interval size of 60.
This increased accuracy can be attributed to the fact that for a smaller interval, there are potentially fewer flows that have to be summarized in each interval.
We next explore the potential of improving the accuracy by performing the top-N vs. top-X*N (X = 1, 1.2, 1.5, 2) comparison.
As can be seen in Figures 6 (a) and (b), the similarity increases for K=8K, even for large N. With X=1.5, the similarity has risen significantly even for large N. For the settings we examined, we get very high accuracy with X ≤ 1.5, and higher values of X result in marginal additional accuracy gains.
This is desirable, because a larger X while increasing accuracy, also results in more false positives.We next consider the effect of varying H on the accuracy.
Fig- ure 7 (a) shows that with a small K=8K, H needs to be at least 9 to get high similarity values, especially for large N.
A large H is undesirable as an increase in H directly corresponds to increased computation overhead (the number of update operations per key is proportional to the value of H) and memory (for sketches) overhead.
But, as Figure 7 (b) shows, even for very large N, increasing K to 32K instantly increases similarity to nearly 1, for a small H=5.
A larger K (for sketches) implies a large space overhead.
This sug- (top-N vs. topN) gests a space-computation overhead tradeoff.
In many applications where the computation overhead is more critical, with K = 32K or more, we can get good accuracy results with small H.We next show the results for a different router file (all files have similar output).
Figure 8 (a) and (b) show the similarity metric for EWMA model for the medium sized router file.Likewise, we show the effect of another model (all models had similar results)-this time we use ARIMA0, i.e., ARIMA with d = 0.
Figure 9 (a) and (b) show similarity for large and medium sized router files respectively for an interval of 300s.
Instead of comparing just the top-N values, as in the previous accuracy tests, we now limit the flows to ones whose absolute forecast error is greater than or equal to a certain fraction of the L2 norm (square root of the sum of squares of the forecast errors of all flows in a time interval).
We vary this threshold level across 0.01, 0.02, 0.05, 0.07, and 0.1.
We show results for each of the two time intervals (60s, 300s) for three models (EWMA, NSHW, and ARIMA with d = 0).
For each of sketch and per-flow based change detection, we rank the flows in decreasing order of absolute value of forecast error.The metrics of interest here are the false negative ratio, false positive ratio, and the number of alarms.
For a given value of threshold τ , let N pf (τ ) and N sk (τ ) refer to the number of flows that meet the threshold in per-flow and sketch based detection, respectively.
The number of alarms for per-flow and sketches are then N pf (τ ) and N sk (τ ) respectively.
Let NAB(τ ) be the count of flows that are common to both the sketch and per-flow lists.
The false negative ratio is computed asN pf (τ )−N AB (τ ) N pf (τ ).
The false positive ratio is:N sk (τ )−N AB (τ ) N sk (τ ).
At this point, for each metric we have a time series, with one value per time interval.
In our study below, we consider the mean value over the entire time series.
In this part of the results, we demonstrate the similarity of sketch and per-flow results when flows are selected by thresholding.
The overall summary here is that with K set to be at least 32K, we can provide excellent guarantees for low false negatives and false positives.
We show two sets of figures.The first set is for the large sized router data file and uses the nonseasonal Holt-Winters model for the 60s (Figure 10) and 300s (Fig- ure 11) time intervals.
Figure 10(a) shows that for a very low value of H (=1), the number of alarms are very high.
Simply increasing H to 5 suffices to dramatically reduce the number of alarms.
The figure also demonstrates the significant reduction in number of alarms that can be realized by increasing the threshold value.
Finally, it shows that there is virtually no difference between the per-flow results and the sketch results when H ≥ 5 and K ≥ 8K.
Figure 10(b) shows that for K=32K and beyond, the false negative ratio drops rapidly to be less than 2% even for very low threshold values and is below 1% for threshold of 0.05.
The false positive ratio, as Figure 10(c) shows, for K=32K and even a low threshold of 0.02, is quite low (below 1%).
The overall results are similar for the 300s interval.The second set of figures uses the medium sized router data file, for a single interval size (300s) and varies across four mod- (b) Model=ARIMA1 Figure 12 (a) shows the false negative ratio for the EWMA model to be well below 1% for thresholds larger than 0.01.
Likewise, Figure 12 (b) shows the false negative ratio for the non-seasonal Holt-Winters model to be slightly better than the EWMA model.
Figure 13 (a) and (b) show for the two different ARIMA models (d = 0 and 1, respectively), that false negatives are low but differ a bit as compared to EWMA and NSHW models for a low threshold of 0.01.
Figure 14 (a) and (b) show the false positive ratios for the EWMA and NSHW models respectively, to be well below 1% for thresholds larger than 0.01 for K=32K or higher.
Likewise, Figure 15 (a) and (b) show low false positive ratios for both ARIMA models.
There are three key components in our sketch-based change detection implementation: 4-universal hash functions, sketches, and forecasting.
The implementation of 4-universal hash functions is about 200 lines, sketches around 250 lines, while forecasting code varies with the forecasting models and all of the models together are less than 800 lines (all in C).
Hash computation and sketch UPDATE need to be done on every item in the input stream.
Sketch ESTIMATE, by default, also needs to be done on a per-item basis.
However, if we are willing to miss some keys that appear too infrequently (which arguably can only cause limited damage) we can sample the stream of incoming keys and only do ESTIMATE on the substream.
Operations like ESTIMATEF2 only need to be done infrequently-once every interval-and their amortized costs are insignificant.
(b) Model=ARIMA1 16 .
As we can see, the overhead of these operations are not very high.
We note that the current implementation has not been fully optimized allowing room for further speedup.
Our preliminary exploration indicates that our sketch-based change detection method is highly accurate when compared with per-flow time series analysis.
It offers promise to be a building block for network anomaly detection and traffic measurement.
We outline some avenues that we are exploring• Online change detection: We have currently evaluated our methods in an offline setting.
These evaluations suggest that the technique may be capable of near real-time change detection.
One change required is modifying our technique to obtain the forecast model parameters online.
One possible way is periodically recomputing the forecast model parameters using history data to keep up with changes in overall traffic behavior.
• Avoiding boundary effects due to fixed interval sizes.
Possible solutions include (i) simultaneously run multiple models using different interval sizes, and different starting points, (ii) randomize the interval size (e.g., using exponentially distributed interval size) and detect changes of total values normalized by interval size.
The linearity of sketches makes this possible.
• Reducing false positives: We have focused on accurate detection of significant deviation from normal behavior.
However, some anomalies are benign.
The problem of reducing false alarms is a major challenge for all change-detection based network anomaly detection systems.
Our change detection framework has tunable parameters which can be adjusted to limit the false positives.
For instance, the technique can be asked to only report the top N major changes, or the changes that are above a threshold.
The particular application needs will guide the actual setting of these tunable parameters.
• Combining with sampling: Given the massive volumes of data generated in large networks, sampling is increasingly being used in ISP network measurement infrastructures for reducing the volume of data that has to be collected.
Our current approach combines time series analysis with sketches for scalable change detection in massive data sets.
We plan to explore combining sampling techniques with our approach for increased scalability.
• Better guidelines for choosing parameters: Given the wide range of parameters we have, it would be useful to have reasonable guidance for selecting proper and justifiable values for them.
The full factorial method [5] in the statistical experimental design domain can help in narrowing the number of levels (or "versions") for the various variables.
We are exploring such techniques to see which parameters are independent of each other and move towards identifying reasonable values overall based on the similarity.
For example, H has overall impact independent of other parameters.
The tedium related to having multiple runs can also be reduced for example by using Yates algorithm [5].
In this paper, we presented a sketch-based change detection technique.
Our work is motivated by anomaly detection and other applications that can benefit from having a quick and efficient change detection mechanism.
The scheme is capable of detecting significant changes in massive data streams with a large number of network time series.
As part of the technique, we designed a variant of the sketch data structure, called k-ary sketch, which uses a constant, small amount of memory, and has constant per-record update and reconstruction cost.
We implemented a variety of time series forecast models (ARIMA, Holt-Winters, etc.) on top of such summaries and detect significant changes by looking for flows with large forecast errors.
We also presented heuristics for automatically configuring the forecast model parameters.We demonstrate using a large amount of real Internet traffic data that our sketch-based change detection method is highly accurate when compared with per-flow analysis, and can be implemented at low computation and memory costs.
Our preliminary results are promising and point to the potential of using our technique as a building block for network anomaly detection and traffic measurement in large networks.THEOREM 1.
E v h i a ¡ = va (1) Var v h i a ¡ ≤ F2 K − 1(2)PROOF.
For any h ∈ {h0, ..., hH−1}, we have v h a = ¡ b∼a v b − (1/K) · ¡ b v b 1 − 1/K = b∼a v b − 1 K − 1 b񮽙 ∼a v b = va + b∼a∧b񮽙 =a v b − 1 K − 1 b񮽙 ∼a v b(3)Since h is 4-universal, for any distinct a, b ∈ [u], we haveE [X a,b ] = 0 (5) E ¢ X 2 a,b£ = 1 K − 1(6)In addition, for any distinct a, b, c ∈ [u], we haveE [X a,b Xa,c] = 0(7)Now we are ready to prove the theorem.
PROOF.
The proof is almost identical and is omitted here in the interest of brevity.As an example, let K = 2 16 , α = 2, β = 0.5, T = 1/32, and H = 20.
If we raise an alarm whenever v est a ≥ √ F2/32, then according to Theorem 2, the probability that we will miss a va > √ F2/16 is less than 9.0 × 10 −13 ; according to Theorem 3, the probability that we will falsely raise an alarm for a va < √ F2/64 is less than 9.5 × 10 −7 .
Accuracy of F h i 2The following theorem is proved in [33], which shows that each F h i 2 forms an unbiased estimator of F2 with variance inversely proportional to (K − 1).
[33] also shows that in order to achieve the same variance using count sketch, one has to either live with lower speed or double the memory requirement.
is the median of H copies of F h i 2 , by the Chernoff inequality, we immediately obtain (12).
As an example, let K = 2 16 , λ = 0.05, and H = 20, Theorem 5 states that the probability that our estimate F est 2 is 5% off its real value F2 is below 7.7 × 10 −14 .
