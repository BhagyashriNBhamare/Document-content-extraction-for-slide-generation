This paper studies the question of how to overcome inefficiencies due to hidden actions in a rational milieu , such as a grid computing system with open clientele.
We consider the so-called principal-agent model known from economic theory, where the members (or agents) of a distributed system collaborate in complex ways.
We adopt the perspective of the principal and investigate auditing mechanisms that incentivize participants to contribute more to a common project.
As conducting audits might be costly, the principal must balance the tradeoff between low auditing costs and the level of incentives offered to the participants to exert high effort.
We present optimal solutions for this optimization problem in scenarios, where the project success either depends on all, on any or on the majority of the participants succeeding in their subtask.
In the first case, we additionally find that with an increasing principal valuation, there is exactly one transition point where the optimal choices for achieving the maximal principal utility switch.
Compared to a combinatorial agency without the leverage of audits, this transition occurs earlier.
The Internet heralded a new computing paradigm that allows machines all over the planet to collaborate.
It soon became the platform for many success stories such as the World Wide Web or the emailing system which had a deep impact on our society in general.
Distributed computer networks today are decentralized.
Different parts are operated by different stakeholders and the participating machines are no longer under a single administrative domain but under the control of individual users.
This has led researchers to use game-theoretic models in order to gain deeper insights into the functioning of these systems.Distributed computing solutions such as grid computing or peer-to-peer technology rely on resource contributions by the participants.
However, in the absence of a corresponding incentive mechanism, participants may not be willing to share some of their bandwidth or CPU cycles, but prefer to free-ride.
Unfortunately, it is often hard to uncover and penalize selfish participants, because their actions are hidden.
For instance, in a computational grid network organized by a central server that has an open clientele, the client machines may compute different subtasks individually and return their results to contribute to the overall project.
Some of these computations may be redundant, others not.
The server seeks to provide incentives to the participants as to have them perform their calculations properly.
However the server typically only evaluates the overall outcome rather than the quality of the individual computations.
This situation constitutes a so-called principal-agent problem where the principal seeks to influence the effort made by the agents indirectly by promising certain benefits or payments if the project as a whole succeeds.
This problem is well-studied in economic theory and has many applications, e.g., in organization theory.In this paper, we seek to complement the existing literature by focusing on the combinatorial structure of dependencies between the agents' actions.
In particular, we study the question of whether and how auditing one or more agents can improve the efficiency of an agency.
Of course, audits can be costly.
In grid computing, for instance, in order to verify the result submitted by a client, the server may have to recompute the subtask.
Or in the context of a software company, the Java classes programmed by an employee may be checked during a code review by the entire team.
When auditing all agents is too expensive, it can make sense to audit only a subset of agents at random.
This paper shows that such a sampling approach can indeed reduce the principal's cost to implement a desired outcome.
Since the influential talk by Papadimitriou at STOC 2001 [25], many economic aspects (e.g., of the Internet) have been investigated from a computational (or algorithmic) point of view.
In addition, measurement studies (e.g., [19]) have confirmed the presence of selfish participants in today's distributed systems, for example in peer-to-peer networks.
Most often, selfish behavior has a negative impact on a system's performance.
Researchers in the field of algorithmic mechanism design [23] seek to come up with implementations that improve the efficiency or social welfare in selfish milieus.
The presence of selfish participants or cheaters in grid computing [15] is also well-known [7], [29].
Solutions based on so-called ringers [16] or Merkle trees [7] have been investigated.
The work in [21] exploits the asymmetry between the time-consuming computation of subtasks from scratch and the more efficient result verification.
In [6], the Lottery Tree mechanism encourages contributions (and the solicitation of new participants) in asymmetric network-effect systems such as BOINC [1].
One difficulty in providing cooperation incentives to the agents or players is information asymmetry.
While algorithmic mechanism design studies how to extract private information from the participants, the principal-agent theory [9], [17], [18] deals with hidden actions that influence the final outcome.
It has its origins in the theory of the firms [4], [5], [20], [24].
Researchers have identified several practical means to improve an agency, such as investments into corporate culture, information systems (e.g., definition of milestones), reputation and trust solutions, incentive systems or bureaucratic control (cf also [13], [14]).
Our work studies the possibilities of probabilistic auditing.This paper builds upon the seminal work by Babaioff, Feldman and Nisan [2] who coined the notion of combinatorial agency.
The authors initiated the study of handling combinations of agents rather than single agents, and put an emphasis on dealing with complex dependencies between the agents' actions.
Babaioff et al. introduced the binary agency model as a simplification and derived very interesting results for different classes of action dependencies-which they call technologies.
Their paper has opened a wide range of exciting questions.
In [3], the same authors study combinatorial agency where the agents mix strategies, and it is shown that the principal can benefit compared to situations where merely pure Nash equilibria are considered.
In [10], Emek and Feldman provide a complexity analysis of the OR and series-parallel technologies, and present a FPTAS for the OR technology.
In contrast to this body of work, we investigate the effect of audits.
We allow the principal to audit a number of agents and verify their work.
While audits come at a certain cost, we show that the cooperation among the agents can be improved.There are interesting results on the effect of control in social psychology [11].
It has been estimated that in the United States, more than 20 million workers were subject to electronic monitoring in 1993, and that companies spent more than USD 1 billion on monitoring software in 1996.
For example, Longhaul trucking firms used the Global Positioning System (GPS) to track the truck driver's speed, fuel use, and route location [28].
Pitman et al. [26] find that a physically present surveillant can reduce the motivation of individuals; [27] comes to a similar conclusion in scenarios with video cameras.
Falk et al. [12] evaluate the cost of distrust experimentally and find that the principal earns 31 percent less if agents are control-averse.
In contrast to this work, we assume that the agents are rational in the strict sense that they aim at maximizing their individual revenue.
As we will see, in this context, the possibility of conducting audits does not reduce the principal's utility.
Moreover, we will shed light on the question of how much auditing is optimal for the principal.
Our model is presented in three stages.
We first explain the general principal-agent setting.
Subsequently, we introduce the binary principal-agent problem as a simplification.
Finally, we describe how a principal audits agents.
Throughout this paper, we will refer to the principal as a she and to an agent as a he.The Principal-Agent Setting: We consider a set of n (riskneutral) agents N = {1, . . . , n} which can be contracted and audited by a principal.
Each agent i has a set of possible actions A i , where each action a i ∈ A i entails a certain cost c i (a i ) ≥ 0 to the agent.
The combination of the actions chosen by the agents yields a certain probabilistic outcome o ∈ O according to a success function t :A 1 × . . . × A n → ∆(O),where ∆ is the set of probability distributions on outcome O.
This outcome is considered a monetary value (or some other form of benefit) which accrues at the principal.
In the following, let a −i ∈ A −i denote the (n − 1)-dimensional vector of the agents' actions excluding agent i, i.e., a −i = (a 1 , ..., a i−1 , a i+1 , ..., a n ).
A crucial assumption underlying the principal-agent model is that the principal cannot see the individual agents' actions.
The principal motivates the agents by offering each agent i a contract that specifies a payment p i for each possible outcome of the whole project.
Note that p i ≥ 0, i.e., agents cannot be fined (limited liability constraint).
Given these contracts, the agents will optimize their personal utilities u i , maximizing the expected payments p i they receive minus the cost of the action c i (a i ).
Since the agents' actions depend on each other, we can model the situation as a game and assume that the agents reach a Nash equilibrium.In this paper, we adopt the point of view of the principal.
The principal seeks to design an optimal set of contracts that maximizes her expected utility u(o), i.e., the expected benefits minus the expected total payments i p i .
The principal thus creates a game for the agents, where the desired behavior is a Nash equilibrium.
Note that although there might exist other Nash equilibria in the induced game, we assume that the agents reach the equilibrium desired by the principal.
This is legitimate since any Nash equilibrium can be implemented for free, e.g., by providing insurances to the agents just in case the other agents would deviate from the specific Nash equilibrium (cf [8], [22]).
The Binary Agency: In this paper, we will investigate a simplified principal-agent model where each agent i only has two possible actions: exert high effort (a i = 1) and exert low effort (or shirk) (a i = 0).
An agent's cost are 0 for a i = 0 and c i for a i = 1.
Moreover, the whole project can only either succeed or fail, that is, the set of possible outcomes O has cardinality two, and O = {success, f ailure}.
As has been pointed out by Babaioff et al. [2], this model already contains the main interesting ingredients.
How the agents are organized and how the project success builds on the combination of their contributions depends on the nature of the project.
For instance, in an assembly line setting, each agent may be perfectly specialized and contribute exactly one unique part to the project.
In this scenario, there is no redundancy and the project succeeds if and only if all agents successfully provide their part.
We will refer to such a project as an AND technology.
In contrast, in the OR technology, the project succeeds if and only if at least one of the agents successfully contributes its (sub-)task.A technology is characterized by its success function t(a).
In this paper, we will focus on anonymous technologies: a technology is called anonymous if the success function t(a 1 , ..., a n ) depends only on the number-and not on a particular subset-of agents who exert high effort.
We then write the success function as t m , where m is the number of agents exerting high effort.
Furthermore, we demand that also the costs c i of all agents are identical, i.e., c = c i .
Let x i denote the probabilistc event of agent i succeeding in his subtask.
In particular, we will study the following technology functions:1) AND technology: The project is successful if all agents succeed in their subtask, i.e., f (x) = i x i .
2) OR technology: The project is successful if at least one agent succeeds in his subtask, i.e., f (x) = i x i .
3) MAJORITY technology: The project is successful if a majority of the agents succeeds in their subtasks, i.e., f (x) is 1 if at least n/2 agents succeed and 0 otherwise.
Audits: We are now ready to introduce the final component to our model.
We assume that in addition to observing the overall outcome, the principal can audit a specific agent's action.
Auditing agent i will reveal a i , but comes at a cost κ.With the possibility of audits, the principal may offer slightly different contracts to the agents.
She still offers agent i a payment p i based on the outcome.
If agent i is not audited, he is paid p i if the project as a whole succeeds and 0 otherwise, independent of his action.
However, if he is audited and it turns out that he has not exerted high effort, i.e., a i = 0, the principal does not have to pay him, independent of the project outcome.
We say that the principal contracts with agent i if p i > 0.
The principal informs all agents about all payments p i .
We assume that the principal then chooses k out of the l ≤ n contracted agents uniformly at random to audit their contribution to the project.
This yields a probability π = k/l that agent i is audited.
This audit probability is communicated to the agents.
While such a uniform audit strategy is reasonable with anonymous technologies, it might be better with non-anonymous technologies to inflict a more sophisticated probability distribution with individual audit probabilities π i .
1 Let µ i be 1 if agent i is audited and 0 otherwise.
For each agent, the principal stipulates a payment depending on the project outcome, the agent's action and µ i .
The payment is hence a function p i : O × A i × {0, 1} → R + .
Agent i's utility depends on the expected payment p i he receives 2 , which in turn is a function of the success probability under the given technology t(a), his action a i and the probability that he is audited, and the cost c i (a i ) of the action: u i (a) = p i (t(a), a i , µ i )−c i (a i ).
We assume that the success function t(a) is strictly monotone in the sense that ∀i ∈ N, ∀a −i ∈ A −i : t(1, a −i ) > t(0, a −i ).
The principal's valuation of a successful project is given by a scalar v > 0.
The valuation 1 If, for instance, it is vital for the project that agent i succeeds whereas agent j's contribution is less important for the project success, the principal might be best off by auditing agent i (π i = 1) for sure and agent j with a low probability (π j << 1).
However, we do not explore these issues in this work.
2 Note that in the following, we will simply writep i instead of E[p i ].
of a project failure is 0.
The principal's utility can hence be expressed byu(a) = v(t(a)) − i p i (t(a), a i , µ i ) − k · κ.The principal's aim is to maximize u under the assumption that the agents are rational in the sense that they act so as to maximize their expected utility in turn.
Finally, let us introduce some notation.
In this paper, we will analyze two shirker models, the shirking without contribution model M 0 and the shirking with contribution model M 1 .
We will indicate the studied technology as a subscript (AN D for AND technologies, OR for OR technologies, and M AJ for MAJORITY technologies).
For example, M 0 M AJ refers to the no contribution model under a MAJORITY technology.
We start our analysis with some general observations.
First of all, since the principal can always choose to audit no agents at all, we have the following trivial fact.Fact 2.1: The possibility of auditing can never be detrimental to the principal's utility.We now want to study the impact of audits on the prices that need to be paid to the agents.Theorem 2.2: If the principal decides to contract agent i, she will offer him a paymentp i = c i t(1, a −i ) − t(0, a −i )(1 − π) .
The principal's utility isu(a, v) = t(a) · (v − i p i ) − k · κ.
Agent i's utility is u i (1, a −1 ) = c i t(1, a −i ) t(1, a −i ) − t(0, a −i )(1 − π i ) − 1if he is contracted and 0 otherwise.
Proof: Given the other agents' actions a −i , agent i will exert high effort if and only if the contracted payment p i implies thatu i (1, a −i ) ≥ u i (0, a −i ), i.e., p i (t(1, a −i ), µ i , 1) − c i (1) ≥ p i (t(0, a −i ), µ i , 0) − c i (0).
We have p i · t(1, a −i ) − c i ≥ p i · t(0, a −i )(1 − π i ) ⇔ p i ≥ c i /(t(1, a −i ) − t(0, a −i )(1−π)).
Thus, the principal will only offer contracts withp i = c i /(t(1, a −i ) − t(0, a −i )(1 − π i )).
Offering a smaller or larger amount of money is wasteful as it has no influence on the agent's decision.
The principal and agent utilities then follow from our definitions.Note that in anonymous technologies, an optimal payment is p i = c/(t l −t l (1−π)) where l is the number of contracted agents.
This yields a principal utility u(v) = t l · (v − lp i ) − kκ and an agent utility u i (a i = 1) = t l c/(t l − t l−1 (1 − π)) − c if contracted and 0 otherwise.
Since the payments are identical for all contracted agents, we usually omit the index i, i.e., p = p i .
Additionally, Theorem 2.2 reveals the fact that auditing only affects the optimal payment offered to an agent whom the principal wants to contract with.
Thus, for the principal, auditing a non-contracted agent would not decrease the payments needed to have this agent exert high effort.
On the contrary, it would even increase the costs for her, because the extra audit would cost her κ.Fact 2.3: It makes no sense to audit non-contracted agents.
This justifies the design decision already incorporated in our model that the principal only conducts audits among the contracted agents.
The principal may increase an agent's chance π of being audited by carrying out more random audits.
The higher π is, the less she has to pay agent i in order to contract with him.
However, increasing π comes at a cost: the principal must pay the extra audits.
Thus, it is often not optimal for the principal to simply audit all contracted agents, but rather a fraction or even no agent at all.
This depends on the magnitude of the audit cost κ.
This section studies the case where a shirker-an agent who does not exert high effort-does not contribute anything at all to the project.
Hence, we presume that the probability of an agent's subtask being completed successfully is zero if he shirks.
If he exerts high effort, his subtask succeeds with a certain probability δ i < 1.
We call this case as the no contribution case and refer to it as M 0 .
Recall that under optimal contracts, a rational agent exerts high effort (a i = 1) if he is contracted and he shirks (a i = 0) otherwise.
In the following, we further assume that all agents have the same success probability, i.e., δ i = δ ∀i ∈ N .
We proceed by examining the no contribution case under different technologies.
A first observation is that in the AND technology, the probability of project success is always zero if less than all n agents exert high effort.
Hence, it is only worthwhile for the principal to either not contract with any agent and therewith not to make any payments to the agents, or to contract with all n agents.
Applying Theorem 2.2, we get that the payment which needs to be offered to an agent i if the principal wants to contract with all agents is p ≥ c/δ n , since t(0, ·) = 0.
Observe that this payment is independent of the number of audits, k, and the audit costs κ.
As conducting audits would only cost the principal and in no way increase her utility, auditing does not yield any benefits in the AND technology.
Moreover, note that the payments grow exponentially in the number of agents, as the project's success relies on each agent contributing.
We can state the following.Theorem 3.1: The optimal utility of the principal in M 0 AN D is given by u(v) = max{δ n · v − n · c, 0}.
If v > n · c/δ n , it is worthwhile to contract all agents, otherwise no agent is contracted.
It is always best to carry out no audits (k = 0).
Proof: Since auditing can only decrease the payments needed to contract with an agent, and since p ≥ c/δ n is independent of k, it is never worthwhile to invest any money in audits.
Thus, k = 0 and hence also the total auditing cost κ · k = 0.
We only need to consider two cases: either all agents are contracted or none.
This yields a principal utility u(v) of δ n · v − n · c if all agents are contracted and 0 if no agent is contracted.
The principal always chooses the better of the two options, and hence, the claim follows.
In the OR technology, the project already succeeds with a probability δ if only one agent exerts effort.
Each additional contracted agent diminishes the failure probability by a factor of (1−δ).
It turns out that the analysis of the OR technology is more difficult.Theorem 3.2: In M 0 OR , if the principal contracts l agents and conducts k ≤ l audits, the lowest payment for which a contracted agent exerts effort is p = c/((1 − (1 − δ) l ) − (1 − (1 − δ) l−1 )(1 − k/l)).
The resulting principal utility is u(v) = (1 − (1 − δ) l )(v − l · p) − k · κ.Proof: The project's success probability in the OR technology is given by t l = (1 − (1 − δ) l ) where l is the number of contracted agents.
Applying this success function t l to Theorem 2.2 yields p and u.Theorem 3.2 allows the principal to compute the optimal number of contracted agents l and the optimal number of audits k ≤ l.
If done without any optimizations, this requires O(n 2 ) function evaluations.
It is interesting to study how l and k relate to each other.
Assume for a moment that l is fixed.
Auditing an additional agent (k := k + 1) will reduce the payment p needed to contract an agent, but at the same time, it costs κ.
For a given l, the optimal choice of k follows from the equality of this additional gain and the additional loss, i.e., whenl·c (1−(1−δ) l )−(1−(1−δ) l−1 )(1−(k+1)/l) − l·c (1−(1−δ) l )−(1−(1−δ) l−1 )(1−k/l)= κ.
We omit an analytical characterization of the optimal k but present some numerical results.
Figure 1 (left) plots the effect of different audit costs κ on the principal's utility u.
The figure illustrates that, as expected, for larger κ, the principal's utility declines.
Moreover, for larger κ, in order to maximize the utility, fewer audits are conducted.
Figure 1 (right) studies the effect of different agent costs c. For larger c, the utility is smaller, and it is worthwhile to audit more agents.
Furthermore, we see that for many configurations of κ and c, the optimal number of audits is not simply 0 or l, but a value in between.
To conclude our analysis of the no contribution case, we consider the MAJORITY technology where at least half of the agents must be successful.Theorem 3.3: In M 0 M AJ , if the principal contracts l agents and conducts k ≤ l audits, the lowest payment for which a contracted agent exerts effort is wherep =        c Υ l n/2 −Υ l−1 n/2 ·(1−k/l) if l > n/2 c Υ l n/2 if l = n/2 ∞ otherwise ,0 10 20Υ β α = β j=α j n/2 · δ j (1 − δ) β−j .
This yields a principal utility ofu(v) = Υ l n/2 · (v − l · p) − k · κ,where the payments p and Υ β α are as given above.
Proof: The probability that the project succeeds with exactly j ∈ {n/2, . . . , l} successful agents out of the l contracted agents is binomially distributed.
The MAJORITY technology's success function t l can be calculated by summing over all the possibilities of success.
Hence t l is Υ l n/2 if l ≥ n/2 and 0 otherwise.
Payment p and utility u then follow from Theorem 2.2.
In the second part of this paper, we consider a setting where an agent exerting low effort still contributes to the project.
We call this setting shirking with contribution case and denote it by M 1 .
Agents playing a i = 1 succeed with a certain probability δ and an agent playing a i = 0 succeeds with a probability γ < δ.
In the following, we assume that the principal willing to implement the project makes a small inalienable payment p to each agent in order to make him participate in the project.
This payment is independent of the agent's decision whether to exert high or low effort.
An agent's utility is given by u i (a) = p i (t(a), a i , µ i ) − c i (a i ) + p and the principal utility is u(a) = v(t(a))− i p i (t(a), a i , µ i )−kκ−np .
The minimal payment offered by the principal to make an agent exert high effort can be computed similarly to Theorem 2.2.
We still assume that the agents seek to maximize their expected utility.
However, in contrast to the no contribution case, the inalienable payments p are subtracted from the principal's utility and added to the agents' utilities.Corollary 4.1: If the principal contracts with agent i, she offers him an optimal payment of p i , which adds to p :p i = c i t(1, a −i ) − t(0, a −i )(1 − π) .
The principal utility u and the agent utility u i areu(a, v) = t(a) · (v − i p i ) − kκ − np and u i (1, a −i ) = t(1, a −i ) t(1, a −i ) − t(0, a −i )(1 − π i ) − 1 c i + pif contracted and 0 otherwise.
Note that, as agents exerting low effort can also succeed in their subtask, the success function t(a) is different in the shirking with contribution case.
For instance, an AND technology project may be completed successfully even if no agent exerts high effort.
Moreover, observe that also in case of inalienable payments p , it is never worthwhile to audit non-contracted agents.
This is due to the fact that not contracted agents receive only the minimal payment p -independently of their action.
Being audited does not decrease these payments and therefore the additional audits only entail costs.
In order to initiate the study of the shirking with contribution case, we consider a very small project with two agents only.
This setting already raises interesting questions.
We first study the AND technology.
If no agent exerts effort, the project success probability is t 0 = γ 2 .
It is t 1 = γ · δ if one agent exerts effort and t 2 = δ 2 if both agents exert effort.
The principal has three possibilities.
She can either contract none (l = 0), one (l = 1), or both (l = 2) agents.
The optimal mechanism can be found by considering these cases in turn.Case l = 0: The principal does not contract any agent, and hence all agents exert low effort.
The principal's utility becomes u k,0 (v) = t 0 v − kκ − 2p .
(Henceforth, we will use the notation u k,l (v) to denote the principal utility when k agents are audited and l agents contracted.)
Since noncontracted agents are never subject to auditing, k = 0 and thusu 0,0 (v) = t 0 · v − 2p .
Case l = 1: The principal contracts one agent.
Both agents are paid p and from Corollary 4.1, we know that one agent is paid an additional p = c/(t 1 −(1−π)t 0 ) in case of success.
The principal's utility is u k,1 (v) = t 1 (v − p) − kκ − 2p .
The principal can either audit none (k = 0) or one (k = 1) agent.
This yields an auditing probability for the contracted agent of π = 1 or π = 0 respectively, and hence,u 0,1 (v) = t 1 · (v − c t 1 − t 0 ) − 2p u 1,1 (v) = t 1 · v − c − κ − 2p .
Case l = 2: The principal contracts two agents.
Both agents are paid an additional p = c/(t 2 −(1−π)t 1 ) on success.
The principal's utility is u k,2 (v) = t 2 (v − 2p) − kκ − 2p .
Either, we can audit none (k = 0, π = 0), one (k = 1, π = 1/2), or both agents (k = 2, π = 1).
We haveu 0,2 (v) = t 2 · (v − 2 c t 2 − t 1 ) − 2p u 1,2 (v) = t 2 · (v − 2 c t 2 − (1/2)t 1 ) − κ − 2p u 2,2 (v) = t 2 · v − 2c − 2κ − 2p .
Observe that the principal's utility is linear in v. Furthermore, the gradient is t l and hence does not depend on c, k or κ.
For given costs c and κ, there is an optimal choice of k independent of the principal's valuation v. For an optimal k and given l, we can compute the relationship between κ and c. For instance, when contracting one agent, there is one transition point at κ = (t 1 /(t 1 − t o ) − 1) · c, i.e., auditing the contracted agent yields a higher utility than without any audits when the costs κ do not exceed (δ/(δ − γ) − 1) · c. Otherwise the principal is better off by not auditing.
When contracting two agents, there are two transition points at κ = (t 2 /(t 2 − t 1 ) − t 2 /(t 2 − (1/2)t 1 )) · 2c and κ = (t 2 /(t 2 − (1/2)t 1 ) − 1) · 2c, where for audit costs smaller than (δ/(δ − γ/2) − 1) · 2c, auditing two agents, for costs over (δ/(δ − γ) − δ/(δ − γ/2)) · 2c, no agent, and in-between, one agent yields the highest principal utility.As an example, let c = 1, γ = 1/4 , δ = 3/4, and p = 0.
Then we have a success probability of t 0 = 1/16 if no agent exerts effort, t 1 = 3/16 if one agent exerts effort and t 2 = 9/16 if both agents exert effort.
Refer to Figure 2 for an overview of the three different cases that occur in a twoplayer AND technology.
It illustrates the fact that the optimal number of audits can be either one, two or none at all.Let us briefly consider the two-player example in an OR technology.
All formulas given for the utility functions u k,l also hold with an OR technology.
The only difference concerns the success function t l .
If the principal contracts with no agent, the project succeeds with probability t 0 = 1− (1−γ) 2 .
For l = 1 and l = 2, we have t 1 = 1−(1−γ)·(1−δ) andt 2 = 1 − (1 − δ) 2 .
If κ is slightly smaller than the cost c, there are two transition points v * 1 and v * 2 , where the principal is best off by contracting with no agent for a valuation v ≤ v * 1 , by contracting with one agent and conducting one audit forv * 1 ≤ v ≤ v * 2and by contracting with both agents and auditing both agents for v ≥ v * 2 .
If κ is twice as large as c, there are also two transition points v * 1 and v * 2 .
The principal is best off by contracting with no agent for a valuation v ≤ v * 1 , by contracting with one agent and auditing none for v * 1 ≤ v ≤ v * 2 and by contracting with both agents and conducting one audit for v ≥ v * 2 .
If κ is about a factor twenty larger than c, there are again two transition points v * 1 and v * 2 .
This time, the principal is best off by contracting with no agent for a valuation v ≤ v * 1 , by contracting with one agent and monitoring none for v * 1 ≤ v ≤ v * 2 and by contracting with both agents and auditing none for v ≥ v * 2 .
This suggests that in an OR technology, there are always n transition points v * 1 < . . . < v * n where it is optimal for a v ∈ (v * l , v * l+1 ) to contract with l agents and audit a certain fraction k of the l contracted agents (cf Conjecture 4.10).
The two-player case already discloses some insights about how audits can be useful to a principal.
It shows that audits facilitate a higher principal benefit in many cases.
In certain configurations, it even enables the principal to carry out a project that otherwise would not prospect profit.
We say a project is profitable if the principal's expected utility is positive.Theorem 4.2: In the AND and OR technology, there are situations where a project is only profitable if the principal employs audits.Proof: In a M 1 AN D technology with two players, let p = 4, γ = 1/4, δ = 3/4, c = 1, p = 0.1 and κ = 1.
Let the principal have a valuation v = 35.
Then the optimal number of contracts without audits is l = 2.
However, this yields a negative utility of u 0,2 (35) = −0.3125, whereas auditing both agents yields a positive utility u 2,2 (35) = 1.6875.
Creating a respective situation for OR technologies is straightforward.
We will now generalize our results to anonymous technologies with n agents and provide the corresponding results for two anonymous technologies, namely for the AND technology as well as for the OR technology.Recall that in anonymous technologies, the success function only depends on the number of agents exerting high effort and not on the particular set of agents exerting high effort.
For the AND technology for instance, we have t(a) = t m = δ m γ n−m where m is the number of agents exerting high effort.
Let l be the number of agents contracted by the principal.
If the agents are rational decision makers, all contracted agents will exert high effort and thus m = l.
Our aim is to give advice to the principal on how to choose the number l of contracts and the number k of audits given her valuation v and the parameters c, κ and n.In an anonymous technology where the principal contracts with l out of n agents and conducts k audits among the l contracted agents uniformly at random, the minimal payment needed to convince a rational agent not to cheat is If the principal's valuation of the project is lower than 20, her optimal choice is k = 0, l = 0, if it is higher, k = 2, l = 2 is best.
In this case, auditing both contracted agents brings a benefit of 2 for a valuation of 15 or larger.
As the absolute value of the audit benefit is constant for a certain k and l, the gain from auditing decreases in terms of percentage with v. Notice also that for a low valuation v = 1 the principal's utility is negative.
This is because the basic payments p made to all agents exceed the benefits from the project and hence the principal will not carry out the project.
and the principal's utility isp = c t l − t l−1 · (1 − k l )(1u k,l (v) = t l · (v − lp) − kκ − np .
(2)A first thing to notice is that u k,l is a linear function of v. Secondly, the gradient of u k,l (v) is independent of k.
It depends only on n and the number of contracted agents l. Theorem 4.3: In any anonymous technology under M 1 , if the principal contracts with l agents, it is optimal for her to audit k * agents, wherek * (l) =        l for κ ≤ t l−1 The authors would like to thank Markus Kern from the Max-Planck-Institute in Heidelberg and Alban Fischer from the Chair of Strategic Management and Innovation at ETH Zurich for interesting discussions.
yIn the AND technology, from Equation (1) it follows that the principal pays p i = c/(δ l γ n−l − δ l−1 γ n−l+1 · (1 − k/l)) in order to contract with an agent i.
The resulting expected principal utility isWe observe that the gradient of u k,l is independent of k and, since γ < δ, the gradient is maximal for l = n, namely δ n , and minimal for l = 0, γ n .
Hence, we know that for a valuation v large enough, the principal will contract with all agents.
We also see that u 0,0 (v) = γ n v − np is the best choice for a v small enough, as the costs of np are inevitable for any choice of l and k. Of course, if γ n v < np , the principal's utility is negative.
In that case, she would not carry out the entire project at all.
Interestingly, it never pays off to contract with only a fraction of the agents.
The principal utility is maximized if she either contracts with nobody and obviously conducts no audits or if she contracts with all agents and chooses an optimal number of audits.Theorem 4.5: In M 1 AN D , there exists a value v * such that for any value v ≤ v * it is optimal for the principal to contract with no agent (and thus audit no agent) and for any value v ≥ v * it is optimal to contract with all n agents and audit k * agents at random, where k * is given by.
In order to prove Theorem 4.5, we will first apply the insight gained in Theorem 4.3 to the AND technology, namely that for any number of contracted agents l, there is an optimal audit number k * (l) in Lemma 4.6.
Second, we show that at the intersection point of u 0,0 and u k * (n),n , no other choice of l yields a higher principal utility even with the corresponding optimal k * (l) in Lemma 4.7.
Finally, we show that this is sufficient.
The following lemma is an immediate implication of Theorem 4.3.
Lemma 4.6: If the principal contracts with l agents in M 1 AN D , it is optimal for her to conduct k * audits given byAN D , no other choice of l and k yields a higher principal utility, i.e., ifProof:.
As shown in Lemma 4.6, for each choice of l, there is an optimal choice of k, namely k * (l).
Thus it suffices to show thatIn order to see that inequality (3) holds, let us look at the difference functionObserve that f has two roots at l = 0 and l = n.
The second derivative of f isSince δ > γ and 0 ≤ l ≤ n, the second derivative of f is strictly negative for n > 0.
Thus, the first derivative is strictly descending and consequently, the gradient of f is strictly greater at v = 0 than at v = n.
As f (0) = 0 and f (n) = 0, it must hold that f (l) > 0 for any l in (0, n), and the claim follows.
Equipped with Lemma 4.7, we are finally ready to conclude the proof of Theorem 4.5.
Proof: Note that the transition point indicated in Theorem 4.5, v * , is exactly the intersection point of u 0,0 (v) and u k * (n),n (v).
We conclude by showing that no choice of k and l yields a higher utility than u 0,0 (v) for a v < v * andRecall that u k,l (v) is linear and that for a sufficiently small v, choosing l = k = 0 is optimal, i.e., u 0,0 (v) = max l,k u k,l (v), and for a sufficiently large v, choosing l = n and k = k * (n) is optimal, i.e., u k * (n),n (v) = max l,k u k,l (v).
This is because the gradient of u k,l (v) is maximal for l = n and minimal for l = 0.
We now show that no choice of k and l yields a higher utility than u 0,0 (v) for a v < v * and u k * (n),n (v) for a v > v * .
Assume for the sake of contradiction that there is a k , a l > 0 and a v < v * such that u k ,l (v ) ≥ u 0,0 (v ).
As u k ,l (v) is linear and its gradient is larger than the gradient of u 0,0 (v), it holds that.
This is a contradiction to Lemma 4.7.
Assume for the sake of contradiction that there is a k , a l < n and a v > v * such thatKnowing the number of agents to contract and the number of audits, we are finally interested in the question of how much the principal can actually benefit from employing audits.Theorem 4.8: With an AND technology, the benefit of auditing is u k * (n),n (v) − max{u 0,0 (v), u 0,n (v)} for v > v * and 0 otherwise where v * is the intersection point of u k * (n),n (v) and u 0,0 (v).
Proof: From Theorem 4.5, we know that for a v ≤ v * , the optimal k and l is k = l = 0.
In this case, the principal does not conduct any audits and hence does not profit from the possibility of employing audits.
Babaioff et al. have shown in [2] that without audits, it is optimal for the principal to either contract with no agent or with all agents (in an AND technology).
With audits, choosing k = k * (n) and l = n is optimal for a v > v * (Theorem 4.5).
By subtracting the optimal gain without audits, max{u 0,0 (v), u 0,n (v)}, from the utility achieved with this optimal choice, we get the auditing surplus.
Refer to Figure 3 for a plot of the benefit from auditing compared to the absolute revenue in a technology with two agents.
Once v grows larger than the transition point, the benefit increases and reaches a maximum level at the transition point without audits.
Obviously, the benefit surplus compared to the absolute revenue in % then decreases with growing valuation.
D. OR TechnologyThe success function of the anonymous OR technology is t m = 1 − (1 − γ) n−m (1 − δ) m , where m is the number of agents exerting high effort.
Applying Theorem 4.3 to the OR technology gives the following result.Lemma 4.9: In M 1 OR with n agents, it holds that if the principal contracts with l agents, it is optimal for her to audit k * agents given by Proof: Since the OR technology is anonymous, the formula for k * follows directly from Theorem 4.3 by plugging in the OR technology success function t l = 1−(1−γ) n−l (1− δ) l .
We presume that the OR technology has n transition points on the value of v, where it is optimal for the principal to increase the number of contracts by one with increasing v and to audit k * agents.
Unfortunately, we did not succeed in proving this property completely.
However, we can provide a proof under the assumption that the following conjecture holds.Conjecture 4.10: Let v * l be the transition point where choosing k = k * (l − 1), l = l − 1 yields the same principal utility as k = k * (l), l = l. With OR technologies, it holds for all l ∈ {1, . . . , n − 1} that v * l is smaller than v * l+1 .
A proof may be similar to the one of Lemma 4.7: Computerespectively.
Then define the difference function f (l) = v * l+1 − v * l and show that it is always positive for all l ∈ {1, . . . , n − 1}.
We did not succeed in the last step of the proof analytically, because the maths are not as advantageous as in the AND technology.
The reason for this is mainly that in the AND technology, the term t l /t l−1 dissolves to δ/γ and thus it is independent of l and n.
In the OR technology, however, this term still depends on l and n.
We conducted simulations of the difference function f (l) for a variety of configurations and could not find a counter-example to our conjecture.
Another fact that supports our conjecture is that an anonymous OR technology has n transitions in the purelyhidden-actions case as well as in the observable-actions case [2].
Since the case with audits could be interpreted as a combination of these two cases, it seems likely that there are also n transitions in this case.Lemma 4.11: Let k * (l) be the function defined in Lemma 4.9.
If Conjecture 4.10 holds then for any OR technology with n agents, there exist n transition points, it is optimal for the principal to contract with l agents and audit k * (l) agents out of the l contracted agents.
For a valuationProof: If Conjecture 4.10 holds, the intersection point of a utility function u k * (l),l with u k * (l−1),l−1 is reached with a smaller valuation v than the intersection point withSince the utility functions are linear in v and the gradient of any u k,l+1 (v) is larger than the gradient of any u k,l (v), the utility.
Applying this argument recursively for all l ∈ {l + 2, l + 3 . . . , n} and all l ∈ {1, . . . , l − 2}, we get that the largest possible utility lies in the interval.
Furthermore, for a sufficiently small valuation, choosing k = 0, l = 0 yields the highest utility since the principal pays only the minimal payments p and still has a chance of success.
(If the minimal payments p exceed the principal's valuation, her utility is negative, but still as large as possible.)
Thus, if the principal's valuation is in the interval (0, v * 1 ), choosing k = l = 0 yields the best utility possible, in any intervalWe have provided guidelines for the principal on how to maximize her revenue in an OR technology.
For a particular project, where the parameters n, δ, γ, κ, c and p are known, she could e.g., compute l * = arg max l u k * (l),l and then contract with l * agents and audit k * (l * ) out of the contracted agents.
Additionally, we would like to know how much she can actually benefit from employing audits.Corollary 4.12: Let v * l denote the intersection point of u k * (l),l (v) and u k * (l−1),l−1 (v).
If Conjecture 4.10 holds, the benefit of audits in an OR technology, is Figure 3 shows the benefit from auditing compared to the absolute revenue with two agents.
One can nicely observe the two transition points where with growing valuation, the benefit first increases and then decreases again until the next transition point is reached.
E. MAJORITY TechnologyIn the contribution case, the MAJORITY technology is quite complex as there is also a chance of success if less than half of the agents exert high effort.
To complete our analysis on the effects of allowing the principal to employ audits, we provide the principal with the optimal payments and her resulting utility given that she contracts with l agents.Theorem 4.13: In M 1 M AJ , if the principal contracts l agents and conducts k ≤ l audits, the optimal payment to contract with an agent isThis yields a principal utility ofwhere the payments p and Ψ r,s are as given above.
Proof: Ψ r,s represents the probability that exactly r contracted agents and exactly s non-contracted agents succeed.
The MAJORITY technology's success function t l can be calculated by summing over all the possibilities of success, i.e., the probabilities of the project succeeding with exactly j ∈ {n/2, . . . , l} successful agents, which in turn are composed of all feasible combinations of r and s such that r + s = j. Payment p and utility u then follow from Theorem 2.2.
V. CONCLUSIONMany open distributed systems have to deal with the problem of hidden actions, i.e., actions or contributions of (often anonymous) participants that are not observed directly.
If one can, however, reveal certain participants' actions by conducting audits, the coordinator may improve the cooperation and therewith the efficiency of such systems.
The cost of auditing and the efficiency gain constitute an optimization problem.
In both a model where shirkers contribute and a model where they do not contribute, there are general solutions on how to optimally choose the number of audits, the number of contracts and the payments to the participants in a combinatorial principal-agent setting with AND, OR, or MAJORITY technologies.
It turns out that with increasing principal valuation, a system with the additional leverage of audits often exhibits similar transitions as in settings without audits in terms of the optimal choices for achieving the maximal principal utility.
However, with audits, these transitions occur earlier than without audits.
Sometimes, employing audits makes a project profitable which could otherwise not be carried out.It would be interesting to test our results "in the wild"-our paper has studied anonymous technologies in theory only and using a simplified model.
Another natural extension would be to study audits with non-anonymous forms of interaction.
In such technologies, the question of which set of agents to audit arises.
Furthermore, we ask: Can we devise general algorithms to solve the principal's optimization problem for arbitrary technologies?
And what is the computational complexity?
In the AND technology, from Equation (1) it follows that the principal pays p i = c/(δ l γ n−l − δ l−1 γ n−l+1 · (1 − k/l)) in order to contract with an agent i.
The resulting expected principal utility isWe observe that the gradient of u k,l is independent of k and, since γ < δ, the gradient is maximal for l = n, namely δ n , and minimal for l = 0, γ n .
Hence, we know that for a valuation v large enough, the principal will contract with all agents.
We also see that u 0,0 (v) = γ n v − np is the best choice for a v small enough, as the costs of np are inevitable for any choice of l and k. Of course, if γ n v < np , the principal's utility is negative.
In that case, she would not carry out the entire project at all.
Interestingly, it never pays off to contract with only a fraction of the agents.
The principal utility is maximized if she either contracts with nobody and obviously conducts no audits or if she contracts with all agents and chooses an optimal number of audits.Theorem 4.5: In M 1 AN D , there exists a value v * such that for any value v ≤ v * it is optimal for the principal to contract with no agent (and thus audit no agent) and for any value v ≥ v * it is optimal to contract with all n agents and audit k * agents at random, where k * is given by.
In order to prove Theorem 4.5, we will first apply the insight gained in Theorem 4.3 to the AND technology, namely that for any number of contracted agents l, there is an optimal audit number k * (l) in Lemma 4.6.
Second, we show that at the intersection point of u 0,0 and u k * (n),n , no other choice of l yields a higher principal utility even with the corresponding optimal k * (l) in Lemma 4.7.
Finally, we show that this is sufficient.
The following lemma is an immediate implication of Theorem 4.3.
Lemma 4.6: If the principal contracts with l agents in M 1 AN D , it is optimal for her to conduct k * audits given byAN D , no other choice of l and k yields a higher principal utility, i.e., ifProof:.
As shown in Lemma 4.6, for each choice of l, there is an optimal choice of k, namely k * (l).
Thus it suffices to show thatIn order to see that inequality (3) holds, let us look at the difference functionObserve that f has two roots at l = 0 and l = n.
The second derivative of f isSince δ > γ and 0 ≤ l ≤ n, the second derivative of f is strictly negative for n > 0.
Thus, the first derivative is strictly descending and consequently, the gradient of f is strictly greater at v = 0 than at v = n.
As f (0) = 0 and f (n) = 0, it must hold that f (l) > 0 for any l in (0, n), and the claim follows.
Equipped with Lemma 4.7, we are finally ready to conclude the proof of Theorem 4.5.
Proof: Note that the transition point indicated in Theorem 4.5, v * , is exactly the intersection point of u 0,0 (v) and u k * (n),n (v).
We conclude by showing that no choice of k and l yields a higher utility than u 0,0 (v) for a v < v * andRecall that u k,l (v) is linear and that for a sufficiently small v, choosing l = k = 0 is optimal, i.e., u 0,0 (v) = max l,k u k,l (v), and for a sufficiently large v, choosing l = n and k = k * (n) is optimal, i.e., u k * (n),n (v) = max l,k u k,l (v).
This is because the gradient of u k,l (v) is maximal for l = n and minimal for l = 0.
We now show that no choice of k and l yields a higher utility than u 0,0 (v) for a v < v * and u k * (n),n (v) for a v > v * .
Assume for the sake of contradiction that there is a k , a l > 0 and a v < v * such that u k ,l (v ) ≥ u 0,0 (v ).
As u k ,l (v) is linear and its gradient is larger than the gradient of u 0,0 (v), it holds that.
This is a contradiction to Lemma 4.7.
Assume for the sake of contradiction that there is a k , a l < n and a v > v * such thatKnowing the number of agents to contract and the number of audits, we are finally interested in the question of how much the principal can actually benefit from employing audits.Theorem 4.8: With an AND technology, the benefit of auditing is u k * (n),n (v) − max{u 0,0 (v), u 0,n (v)} for v > v * and 0 otherwise where v * is the intersection point of u k * (n),n (v) and u 0,0 (v).
Proof: From Theorem 4.5, we know that for a v ≤ v * , the optimal k and l is k = l = 0.
In this case, the principal does not conduct any audits and hence does not profit from the possibility of employing audits.
Babaioff et al. have shown in [2] that without audits, it is optimal for the principal to either contract with no agent or with all agents (in an AND technology).
With audits, choosing k = k * (n) and l = n is optimal for a v > v * (Theorem 4.5).
By subtracting the optimal gain without audits, max{u 0,0 (v), u 0,n (v)}, from the utility achieved with this optimal choice, we get the auditing surplus.
Refer to Figure 3 for a plot of the benefit from auditing compared to the absolute revenue in a technology with two agents.
Once v grows larger than the transition point, the benefit increases and reaches a maximum level at the transition point without audits.
Obviously, the benefit surplus compared to the absolute revenue in % then decreases with growing valuation.
The success function of the anonymous OR technology is t m = 1 − (1 − γ) n−m (1 − δ) m , where m is the number of agents exerting high effort.
Applying Theorem 4.3 to the OR technology gives the following result.Lemma 4.9: In M 1 OR with n agents, it holds that if the principal contracts with l agents, it is optimal for her to audit k * agents given by Proof: Since the OR technology is anonymous, the formula for k * follows directly from Theorem 4.3 by plugging in the OR technology success function t l = 1−(1−γ) n−l (1− δ) l .
We presume that the OR technology has n transition points on the value of v, where it is optimal for the principal to increase the number of contracts by one with increasing v and to audit k * agents.
Unfortunately, we did not succeed in proving this property completely.
However, we can provide a proof under the assumption that the following conjecture holds.Conjecture 4.10: Let v * l be the transition point where choosing k = k * (l − 1), l = l − 1 yields the same principal utility as k = k * (l), l = l. With OR technologies, it holds for all l ∈ {1, . . . , n − 1} that v * l is smaller than v * l+1 .
A proof may be similar to the one of Lemma 4.7: Computerespectively.
Then define the difference function f (l) = v * l+1 − v * l and show that it is always positive for all l ∈ {1, . . . , n − 1}.
We did not succeed in the last step of the proof analytically, because the maths are not as advantageous as in the AND technology.
The reason for this is mainly that in the AND technology, the term t l /t l−1 dissolves to δ/γ and thus it is independent of l and n.
In the OR technology, however, this term still depends on l and n.
We conducted simulations of the difference function f (l) for a variety of configurations and could not find a counter-example to our conjecture.
Another fact that supports our conjecture is that an anonymous OR technology has n transitions in the purelyhidden-actions case as well as in the observable-actions case [2].
Since the case with audits could be interpreted as a combination of these two cases, it seems likely that there are also n transitions in this case.Lemma 4.11: Let k * (l) be the function defined in Lemma 4.9.
If Conjecture 4.10 holds then for any OR technology with n agents, there exist n transition points, it is optimal for the principal to contract with l agents and audit k * (l) agents out of the l contracted agents.
For a valuationProof: If Conjecture 4.10 holds, the intersection point of a utility function u k * (l),l with u k * (l−1),l−1 is reached with a smaller valuation v than the intersection point withSince the utility functions are linear in v and the gradient of any u k,l+1 (v) is larger than the gradient of any u k,l (v), the utility.
Applying this argument recursively for all l ∈ {l + 2, l + 3 . . . , n} and all l ∈ {1, . . . , l − 2}, we get that the largest possible utility lies in the interval.
Furthermore, for a sufficiently small valuation, choosing k = 0, l = 0 yields the highest utility since the principal pays only the minimal payments p and still has a chance of success.
(If the minimal payments p exceed the principal's valuation, her utility is negative, but still as large as possible.)
Thus, if the principal's valuation is in the interval (0, v * 1 ), choosing k = l = 0 yields the best utility possible, in any intervalWe have provided guidelines for the principal on how to maximize her revenue in an OR technology.
For a particular project, where the parameters n, δ, γ, κ, c and p are known, she could e.g., compute l * = arg max l u k * (l),l and then contract with l * agents and audit k * (l * ) out of the contracted agents.
Additionally, we would like to know how much she can actually benefit from employing audits.Corollary 4.12: Let v * l denote the intersection point of u k * (l),l (v) and u k * (l−1),l−1 (v).
If Conjecture 4.10 holds, the benefit of audits in an OR technology, is Figure 3 shows the benefit from auditing compared to the absolute revenue with two agents.
One can nicely observe the two transition points where with growing valuation, the benefit first increases and then decreases again until the next transition point is reached.
In the contribution case, the MAJORITY technology is quite complex as there is also a chance of success if less than half of the agents exert high effort.
To complete our analysis on the effects of allowing the principal to employ audits, we provide the principal with the optimal payments and her resulting utility given that she contracts with l agents.Theorem 4.13: In M 1 M AJ , if the principal contracts l agents and conducts k ≤ l audits, the optimal payment to contract with an agent isThis yields a principal utility ofwhere the payments p and Ψ r,s are as given above.
Proof: Ψ r,s represents the probability that exactly r contracted agents and exactly s non-contracted agents succeed.
The MAJORITY technology's success function t l can be calculated by summing over all the possibilities of success, i.e., the probabilities of the project succeeding with exactly j ∈ {n/2, . . . , l} successful agents, which in turn are composed of all feasible combinations of r and s such that r + s = j. Payment p and utility u then follow from Theorem 2.2.
Many open distributed systems have to deal with the problem of hidden actions, i.e., actions or contributions of (often anonymous) participants that are not observed directly.
If one can, however, reveal certain participants' actions by conducting audits, the coordinator may improve the cooperation and therewith the efficiency of such systems.
The cost of auditing and the efficiency gain constitute an optimization problem.
In both a model where shirkers contribute and a model where they do not contribute, there are general solutions on how to optimally choose the number of audits, the number of contracts and the payments to the participants in a combinatorial principal-agent setting with AND, OR, or MAJORITY technologies.
It turns out that with increasing principal valuation, a system with the additional leverage of audits often exhibits similar transitions as in settings without audits in terms of the optimal choices for achieving the maximal principal utility.
However, with audits, these transitions occur earlier than without audits.
Sometimes, employing audits makes a project profitable which could otherwise not be carried out.It would be interesting to test our results "in the wild"-our paper has studied anonymous technologies in theory only and using a simplified model.
Another natural extension would be to study audits with non-anonymous forms of interaction.
In such technologies, the question of which set of agents to audit arises.
Furthermore, we ask: Can we devise general algorithms to solve the principal's optimization problem for arbitrary technologies?
And what is the computational complexity?
