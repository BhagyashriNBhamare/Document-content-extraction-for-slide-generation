Erasure coding becomes a practical redundancy technique for distributed storage systems to achieve fault tolerance with low storage overhead.
Given its popularity, research studies have proposed theoretically proven erasure codes or efficient repair algorithms to make erasure coding more viable.
However, integrating new erasure coding solutions into existing distributed storage systems is a challenging task and requires non-trivial re-engineering of the underlying storage workflows.
We present OpenEC, a unified and configurable framework for readily deploying a variety of erasure coding solutions into existing distributed storage systems.
OpenEC decouples erasure coding management from the storage work-flows of distributed storage systems, and provides erasure coding designers with configurable controls of erasure coding operations through a directed-acyclic-graph-based programming abstraction.
We prototype OpenEC on two versions of HDFS with limited code modifications.
Experiments on a local cluster and Amazon EC2 show that OpenEC preserves both the operational performance and the properties of erasure coding solutions; OpenEC can also automatically optimize erasure coding operations to improve repair performance.
Erasure coding provides a low-cost redundancy mechanism for fault-tolerant storage, and is now widely deployed in today's distributed storage systems (DSSs).
Examples include enterprise-level DSSs [15,21,30] and many open-source DSSs [1,3,7,31,54,55].
Unlike replication that simply creates identical data copies for redundancy protection, erasure coding introduces much less storage overhead through the coding operations of data copies, while preserving the same degree of fault tolerance [53].
Modern DSSs mostly realize erasure coding based on the classical Reed-Solomon (RS) codes [43], yet RS codes have high performance penalty, especially in repairing lost data when failures happen.
Thus, research studies have proposed new erasure coding solutions with improved performance, such as erasure codes with theoretical guarantees and efficient repair algorithms that are applicable to general erasure-coding-based storage ( §6).
However, deploying new erasure coding solutions in DSSs is a daunting task.
Existing studies often integrate new erasure coding solutions into specific DSSs by re-engineering the DSS workflows (e.g., the read/write paths).
The tight coupling between erasure coding management and the DSS workflows makes new erasure coding solutions hard to be generalized for other DSSs and further enhanced.
Some DSSs with built-in erasure coding features (e.g., HDFS with erasure coding [1,5], Ceph [54], and Swift [7]) provide certain configuration capabilities, such as interfaces for implementing various erasure codes and controlling erasure-coded data placement, yet the interfaces are rather limited and it is nontrivial to extend the DSSs with more advanced erasure codes and repair algorithms ( §2.2).
How to fully realize the power of erasure coding in DSSs remains a challenging issue.We present OpenEC, a unified and configurable framework for erasure coding management in DSSs, with the primary goal of bridging the gap between designing new erasure coding solutions and enabling the feasible deployment of such new solutions in DSSs.
Inspired by software-defined storage [16,48,51], which aims for configurable storage management without being constrained by the underlying storage architecture, we apply this concept into erasure coding management.
Our main idea is to decouple erasure coding management from the DSS workflows.
Specifically, OpenEC runs as a middleware system between upper-layer applications and the underlying DSS, and is responsible for performing all erasure coding operations on behalf of the DSS.
Such a design relaxes the stringent dependence on the erasure coding support of DSSs.
More importantly, OpenEC takes the full responsibility of erasure coding management, and hence provides flexibility for erasure coding designers to (i) incorporate a variety of erasure coding solutions, (ii) configure the workflows of erasure coding operations, and (iii) decide the placement of both erasure-coded data and erasure coding operations across storage nodes.
Our contributions are summarized as follows:• We propose a new programming model for erasure coding implementation and deployment.
Our model builds on an abstraction called an ECDAG, a directed acyclic graph that defines the workflows of erasure coding operations.
We show how we feasibly realize a general erasure coding solution through the ECDAG abstraction.
• We design OpenEC, which translates an ECDAG into erasure coding operations atop a DSS.
OpenEC supports encoding operations on or off the write path as well as various state-of-the-art repair operations.
In particular, it can automatically optimize an ECDAG for hierarchical topologies to improve repair performance.
• We implement a prototype of OpenEC on HDFS-RAID [5] and Hadoop 3.0 HDFS (HDFS-3) [1].
Its integrations into HDFS-RAID and HDFS-3 only require limited code changes (with no more than 450 LoC).
• We evaluate OpenEC on a local cluster and Amazon EC2.OpenEC incurs negligible performance overhead in DSS operations, supports various state-of-the-art erasure codes and repair algorithms, and increases the repair throughput by at least 82% through automatically customizing an ECDAG for a hierarchical topology.The source code of our OpenEC prototype is available at: http://adslab.cse.cuhk.edu.hk/software/openec.
Consider a DSS that comprises multiple storage nodes and organizes data in units of blocks.
We construct erasure coding as an (n, k) code with two configurable parameters n and k, where k < n. For every k fixed-size original blocks (called data blocks), an (n, k) code encodes them into n − k redundant blocks of the same size (called parity blocks), such that any k out of the n erasure-coded blocks (including both data and parity blocks) can decode the k data blocks; that is, any n − k block failures can be tolerated.
We call the collection of n erasure-coded blocks a coding group.
A DSS encodes different sets of k data blocks independently, and distributes the n erasure-coded blocks of each coding group across n storage nodes to protect against any n − k storage node failures.
In this paper, our discussion focuses on the coding operations (i.e., encoding or decoding) of a single coding group.For performance reasons, a DSS implements coding operations in small-size units called packets, while the read/write units are in blocks; for example, our experiments set the default packet and block sizes as 128 KiB and 64 MiB, respectively).
It divides a block into multiple packets, and encodes the packets at the same block offsets in a coding group together.
Thus, instead of first reading the whole blocks to start coding operations, a DSS can perform packet-level coding operations, while reading the whole blocks, in a pipelined manner.
To simplify our discussion, we use blocks as the units of coding operations, and only differentiate packets and blocks in our implementation ( §4.5).
Given the prevalence of failures, repairs are frequent operations in DSSs [40].
We consider two types of repairs: (i) degraded reads, which decode the unavailable data blocks that are being requested, and (ii) full-node recovery, which decodes all lost blocks of a failed storage node.
Since repairs trigger substantial traffic [40], achieving high repair performance is important in erasure coding deployment.
RS codes [43] are the most popular erasure codes that are widely used in production [5,7,15,31,54,55], but they incur high repair costs.
Thus, many repair-friendly erasure codes have been proposed.
Since single-failure repairs (i.e., repairing a single lost block of a coding group in degraded reads or a single failed node in full-node recovery) are the most common repair scenarios [21,40], existing repair-friendly erasure codes aim to minimize the repair bandwidth or I/O in singlefailure repairs.
Examples are regenerating codes [14], including minimum-storage regenerating (MSR) and minimumbandwidth regenerating (MBR) codes, as well as locally repairable codes (LRCs) [21,23,44,49].
Our work focuses on practical erasure codes.
In particular, we target linear codes, which include RS codes, MSR and MBR codes, as well as LRCs.
Linear codes perform linear coding operations based on the Galois field arithmetic [17].
Mathematically, for an (n, k) code, let d 0 , · · · , d k−1 be the k data blocks, and p 0 , · · · , p n−k−1 be the n − k parity blocks.
Each parity block p j (0 ≤ j ≤ n − k − 1) can be expressed as p j = ∑ k−1 i=0 γ ji d i , where γ ji is some coding coefficient for computing p j .
Note that the linear operations are additive associative (i.e., independent of how additions are grouped).
Also, our work addresses sub-packetization, which is used in various designs of MSR and MBR codes [14,18,32,39,42,45,50,52].
Sub-packetization divides each block into smallersize sub-blocks, so that repairs can be done by retrieving sub-blocks rather than whole blocks.Most DSSs assume that all erasure-coded blocks are immutable and do not support in-place updates.
Thus, we focus on four basic operations: writes, normal reads, degraded reads, and full-node recovery ( §4.2), while we address inplace updates in future work.
Modern DSSs now support erasure coding, yet existing erasure coding management in such DSSs remains stringent and still faces practical limitations.
To motivate our study, we review six state-of-the-art DSSs that currently realize erasurecoded storage: HDFS-RAID [5], HDFS-3 [1], QFS [31], Tahoe-LAFS [55], Ceph [54], and Swift [7].
HDFS-RAID is the erasure coding extension of HDFS [46] in the earlier version of Hadoop.
Here, we focus on Facebook's HDFS-RAID implementation [3], which builds on Hadoop version 0.20.
HDFS-3 builds on the newer Hadoop version 3.0, which includes erasure coding by design.
QFS resembles HDFS and includes erasure coding by design.
All HDFS-RAID, HDFS-3, and QFS organize data in fixed-size blocks.
In contrast, Tahoe-LAFS, Ceph, and Swift organize data in variable-size objects and partition each object into equal-size data blocks for erasure coding.
(L1) Limited support for adding advanced erasure codes: Existing DSSs provide encoding/decoding interfaces for implementing new erasure codes.
However, most DSSs do not provide interfaces for adding erasure codes with subpacketization (e.g., MSR and MBR codes [14,18,32,39,42,45,50,52]) and handling erasure-coded blocks at the granularity of sub-blocks, while only recently Ceph includes the sub-packetization feature in its master codebase [52].
Also, recent erasure codes [19,38] address the hierarchical nature of DSSs to reduce cross-rack [19] (or cross-cluster [38]) repair traffic, yet realizing such hierarchy-aware erasure codes needs modifications to the DSS workflows.
(L2) Limited configurability for workflows of coding operations: Enabling configurable workflows of coding operations allows better resource usage within a DSS.
Take repairs (degraded reads or full-node recovery) as an example.
DSSs execute repairs at different entities upon the detection of failures.
For a degraded read, it is executed at the client (in HDFS-RAID, HDFS-3, QFS, and Tahoe-LAFS), the proxy (in Swift), or a storage node (in Ceph); for full-node recovery, it is executed at either storage nodes (in HDFS-RAID, HDFS-3, QFS, Ceph, and Swift) or the client (in Tahoe-LAFS).
Both degraded reads and full-node recovery operate in a fetch-andcompute manner, in which the entity that executes the repair will retrieve available blocks from other non-failed storage nodes and reconstruct the lost blocks.
On the other hand, besides the fetch-and-compute approach, we cannot configure a DSS to adopt different repair workflows or distribute the repair loads across storage nodes.
For example, recent repair algorithms [25,29] decompose a single-block repair operation into partial sub-block repair operations that are parallelized across storage nodes for better bandwidth usage, but existing DSSs do not support this feature by design.
(L3) Limited configurability for placement of coding operations: All DSSs we consider ensure that the n erasurecoded blocks of each coding group are stored in n distinct storage nodes, and most of them additionally allow configurable block placement.
For example, both HDFS-RAID and HDFS-3 provide a base class for configuring block placement policies; QFS provides an in-rack placement option to store multiple blocks in a rack; Ceph uses placement groups, while Swift uses object rings, to control how erasure-coded blocks are placed in different storage nodes.However, existing DSSs focus on how erasure-coded blocks are placed after encoding, but do not specify where to perform the coding operations.
For example, in encoding operations, we may want to co-locate the computations of parity blocks at one storage node (rather than distribute the computations across different storage nodes) to limit the I/Os of retrieving data blocks.
Also, the repair algorithms in [25,29] require some storage nodes that store available data blocks to first compute partially decoded blocks and send the results to other storage nodes for further decoding.
In this case, we need to place the partial decoding operations at specific storage nodes.
Such fine-grained placement of coding operations is currently not supported in existing DSSs.
The root cause of the limitations in §2.2 is that the current erasure coding management is tightly coupled with the DSS workflows.
Realizing erasure coding in DSSs needs to address how coding operations are performed (i.e., the control flow) and how erasure-coded blocks are stored and accessed (i.e., the data flow).
The current practice is that erasure coding designers only define an erasure code and its coding operations (e.g., the coding coefficients used in coding operations), while DSS developers require dedicated engineering efforts to integrate the coding operations into the read/write paths of DSSs without compromising the correctness of upper-layer applications.
Such tight coupling makes the extensions of erasure coding features inflexible.OpenEC decouples erasure coding management from the underlying DSS by providing a unified and configurable framework for erasure coding management, such that erasure coding designers can leverage OpenEC to realize new erasure coding solutions and configure the workflows of coding operations, without worrying how they are integrated into the DSS workflows.
Specifically, OpenEC addresses the limitations in §2.2 with the following goals: (i) extensibility of new erasure codes; (ii) configurable workflows of coding operations; and (iii) configurable placement of both erasurecoded blocks and coding operations.
To achieve these goals, OpenEC builds on a programming model for erasure coding management, as elaborated in the following sections.
We propose a programming model that allows erasure coding designers to not only define an erasure code structure and its coding operations, but also configure how coding operations are performed in a DSS.
We present a new erasure coding abstraction called an ECDAG ( §3.1), followed by three primitives for constructing an ECDAG ( §3.2).
We then propose a programming interface for realizing an erasure code based on the ECDAG abstraction ( §3.3).
At a high level, an ECDAG is a directed acyclic graph that describes the workflows of coding operations of a coding group of an erasure code.
Each vertex represents a block in the coding group, and the connections among vertices describe how vertices are related by linear combinations.
To address the limitations in §2.2, we design ECDAGs to work for general linear codes (L1 addressed).
Also, we can construct different ECDAGs to configure how and where coding operations are performed (L2 and L3 addressed, respectively).
Consider a coding group of an (n, k) code with n erasurecoded blocks; to simplify our discussion, we do not consider sub-packetization first.
We index the blocks from 0 to n − 1, and let b i denote the block with index i. Without loss of generality, we refer to b 0 , · · · , b k−1 as k data blocks, and b k , · · · , b n−1 as n − k parity blocks.
In some cases (see below), the coding operations may generate some intermediately computed blocks that will not be finally stored (as opposed to blocks b 0 , b 1 , · · · , b n−1 ).
We call such blocks virtual blocks, and denote a virtual block by b i for some i ≥ n.In an ECDAG, let v i (i ≥ 0) be a vertex that maps to block b i ; we call a vertex v i (i ≥ n) that maps to a virtual block b i a virtual vertex.
Let e i, j (i, j ≥ 0) be a directed edge from v ito v j indicating that b i is an input to the linear combination for computing b j .
Each edge is associated with a coding coefficient for the linear combination.
If there exists an edge e i, j , we say that v j is the parent of v i , while v i is a child of v j .
A vertex can have any number of parents and children.
Both encoding and decoding operations are each associated with an ECDAG.
The ECDAG for encoding is constructed at the beginning of the encoding operation to describe how data blocks are linearly combined to form each parity block.
In contrast, the ECDAG for decoding is constructed on demand depending on what blocks are currently available.For example, consider a (5, 4) code (i.e., (4+1)-RAID-5).
We can parallelize partial decoding operations as in PPR [29] by constructing another ECDAG for decoding b 0 (see Figure 1(c)), in which we first compute in parallel the partially decoded blocks b 5 and b 6 (both of which are virtual blocks) from b 1 and b 2 and from b 3 and b 4 , respectively, followed by computing b 0 from b 5 and b 6 .
This shows that we can flexibly configure coding operations by constructing different ECDAGs.
Note that PPR needs to compute b 5 and b 6 at the storage nodes where data blocks (e.g., b 2 and b 4 , respectively) are stored (see [29] for details).
We address this issue in §3.2.
We can also construct an ECDAG for erasure codes with sub-packetization.
Let w be the number of sub-blocks per block (w = 1 means no sub-packetization).
We index the sub-blocks of block b 0 from 0 to w − 1, those of b 1 from w to 2w − 1, and so on.
Each vertex v i (i ≥ 0) now corresponds to the sub-block with index i, while any vertex v i for i ≥ nw is a virtual vertex.
For example, consider the (4, 2) MISER code [45] (an MSR code based on interference alignment), where w = 2.
[45], in which we first compute an encoded sub-block from each of other available blocks b 1 , b 2 , and b 3 (represented by the virtual vertices v 8 , v 9 , and v 10 , respectively), followed by using the encoded sub-blocks to decode the lost sub-blocks of b 0 .
An ECDAG can be constructed from three primitives: Join, BindX, and BindY.
Join is used for constructing an ECDAG, while BindX and BindY control the placement of coding operations.
Listing 1 shows their definitions in C++ format.Join: It specifies how a parent vertex (with index pidx) is formed by the linear combinations of a list of child vertices (with indices in cidxs) and the corresponding coding coefficients (in coefs).
For example, we deploy the (6, 4) RS code and encode four data blocks b 0 , b 1 , b 2 , and b 3 into two new parity blocks b 4 and b 5 .
We can construct an ECDAG with Join as follows (see Figure 3(a)):ECDAG* ecdag = new ECDAG(); ecdag->Join(4, {0,1,2,3}, {1,1,1,1}); ecdag->Join(5, {0,1,2,3}, {1,2,4,8});BindX: It co-locates the coding operations of multiple vertices (with indices in idxs) that reside at the same level of an ECDAG (i.e., in the x-direction), so as to reduce I/O in coding operations.
For example, in Figure 3(a), suppose that the data blocks being encoded are stored in different storage nodes.
Without BindX, we need to compute b 4 and b 5 separately and retrieve each data block twice.
Instead, we can call BindX on vertices v 4 and v 5 to create a new virtual vertex v 6 as follows (see Figure 3(b)):int vidx = ecdag->BindX({4,5});This indicates that blocks b 4 and b 5 are first computed together at the same storage node before being distributed to different storage nodes.
Now we only need to retrieve each data block once.
Note that the index of v 6 (i.e., 6) is generated randomly and returned as vidx by BindX.
Listing 2: Erasure coding programming interface.Thus, we compute parity blocks b 4 and b 5 at the same storage node that stores b 0 , thereby saving the I/Os of retrieving b 0 .
Note that BindY enables us to implement the repair algorithms (e.g., PPR [29] and repair pipelining [25]) that need to compute partially decoded blocks at the storage nodes that store the data blocks.
For example, referring to Figure 1(c) for PPR, we can call BindY on v 2 and v 5 , and on v 4 and v 6 , to co-locate the computations of the partially decoded blocks b 4 and b 5 at the storage nodes that store b 2 and b 4 , respectively.Remarks: We provide flexibility for erasure coding designers to construct any ECDAG using the above three primitives, yet this also puts burdens on erasure coding designers to configure coding operations.
Nevertheless, OpenEC can also automatically call BindX and BindY on some specific subgraph structures of an ECDAG ( §4.4).
We provide a programming interface for realizing an erasure code.
Unlike the traditional approach that takes data blocks as input and generates parity blocks, we program an erasure code through the construction of ECDAGs.
OpenEC then parses the ECDAGs to perform the actual coding operations and store the erasure-coded blocks.Listing 2 shows the erasure coding programming interface as a base class ECBase.
To realize an erasure code, we (as erasure coding designers) inherit ECBase and first define all necessary member variables (e.g., n, k, w, and encoding coefficients) in the constructor method as in traditional erasure code programming.
Note that we can store encoding coefficients in a generator matrix [35] and compute decoding coefficients later based on the available blocks.
We then implement three functions, namely Encode, Decode, and Place.Encode: It constructs an ECDAG that describes the encoding operation.
For example, to encode the (6, 4) RS code based on Figure 3( blocks (with indices in to).
For example, we can implement Decode for a single lost block as in Listing 4, in which the decoding coefficients are computed based on the available blocks in from.
In general, Decode constructs an ECDAG for one of the two scenarios: (i) decoding one lost block, in which we can choose an efficient single-failure repair approach (e.g., see Figure 2(c) for the (4, 2) MISER code); or (ii) decoding multiple lost blocks, in which we can choose any k available blocks (e.g., the first k blocks in from) to compute the decoding coefficients and decode all lost blocks.Place: It configures how erasure-coded blocks are placed with hierarchy awareness.
In addition to storing erasurecoded blocks in different storage nodes, we can configure how the blocks are grouped (e.g., in the same rack in rack-based DSSs).
This supports fine-grained block placement configurations as in existing DSSs ( §2.2), and allows the realization of hierarchy-aware erasure codes [19,38].
For example, we can divide n erasure-coded blocks into two groups via Place as in Listing 5.
Note that BindX and BindY in ECDAG construction ( §3.2) address the placement of coding operations, while Place addresses the placement of erasure-coded blocks.
We design OpenEC to provide erasure coding management for a DSS.
We show its architecture ( §4.1) and supported basic operations ( §4.2).
We then describe how it parses ECDAGs to realize coding operations ( §4.3).
We further show how it automatically optimizes coding operations ( §4.4).
We conclude this section with the implementation details ( §4.5).
OpenEC runs as a middleware system atop a DSS.
We assume that the controller is reliable (i.e., no singlepoint-of-failure).
Our measurements show that the controller can serve a request of parsing an ECDAG for coding operations in less than 0.3 ms in our local cluster ( §5), and hence it incurs limited overhead to basic operations.Agent: Each agent performs coding operations as instructed by the controller.
It accesses the erasure-coded blocks in HDFS through the HDFS client interface.
Note that agents can communicate among themselves to perform coding operations and exchange erasure-coded blocks.
We currently deploy each agent at a DataNode, so that the agent can access the local storage of the DataNode without network transfers.OECClient: Each OECClient is associated with an agent, and serves as an interface between an upper-layer application and the agent.
It connects to the agent via Redis-based communication ( §4.5).
An application now accesses HDFS through an OECClient instead of an HDFS client.
1 We also implement OpenEC atop QFS [31].
See [27] for details.
OpenEC supports four basic operations: (i) writes; (ii) normal reads; (iii) degraded reads; and (iv) full-node recovery.
Writes: Note that HDFS-3 supports online encoding (i.e., clients perform encoding on the write path), while HDFS-RAID supports offline encoding (i.e., clients first write the data blocks in uncoded form, and the data blocks are later encoded in the background).
OpenEC is currently designed to support both online and offline encoding.
An OECClient specifies which encoding mode to use in a write request.
For online encoding, OpenEC encodes data on a per-file basis.
When an OECClient writes a file, its agent encodes every k data blocks into n − k parity blocks and writes the n erasurecoded blocks to n DataNodes through the HDFS client.
For offline encoding, an OECClient first writes file data via its agent to HDFS.
When OpenEC receives an encoding request, the controller parses the specified ECDAG ( §4.3) and instructs all agents to perform encoding, such that every k blocks are encoded into n erasure-coded blocks as a coding group.
Normal reads: An OECClient issues normal reads (under no failures) via its agent, which connects to the DataNodes that store the uncoded data blocks and retrieves the data blocks from the DataNodes.
Degraded reads: An OECClient issues degraded reads (under failures) via its agent, which connects to non-failed DataNodes and retrieves the available blocks for decoding the lost blocks based on the ECDAG specification.
Full-node recovery: The controller coordinates the full-node recovery operation.
When it receives a report of lost blocks from the NameNode, it informs the agents to repair the lost blocks based on the ECDAG specification.
OpenEC parses ECDAGs to perform coding operations in writes (online or offline encoding), degraded reads, and fullnode recovery.
Given an ECDAG, OpenEC decomposes a coding operation into multiple tasks, each of which is executed by an agent.
Each task operates in blocks (or sub-blocks in sub-packetization).
There are four types of tasks:• Load: It loads a block into memory from the agent's input stream, which could be either the OECClient if the block is from upper-layer applications, or the HDFS client if the block is from HDFS.
• Fetch: It retrieves blocks from other agents.
• Compute: It computes a block based on the linear combination of blocks and coding coefficients.
• Persist: It either writes a block to HDFS via the HDFS client, or returns the block to an OECClient.Parsing procedure: OpenEC performs topological sorting of an ECDAG (based on depth-first search) to identify the vertex sequence of coding operations.
It then assigns tasks to each vertex based on the ECDAG structure.
Depending on the Vertices Nodes types of basic operations, OpenEC may perform coding operations on the client side (for online encoding and degraded reads) or distribute the coding operations across storage nodes (for offline encoding and full-node recovery).
Tasks v 0 C Load b 0 v 1 C Load b 1 v 2 C Load b 2 v 3 C Load b 3 v 6 C Compute b 4 from {b 0 , b 1 , b 2 , b 3 } with coding coefficients {1,1,1,1}; Compute b 5 from {b 0 , b 1 , b 2 , b 3 } with coding coefficients {1,2,4,8} v 4 C - v 5 C - - C Persist b 0 ; Persist b 1 ; Persist b 2 ; Persist b 3 ; Persist b 4 ; Persist b 5 (a) Online encoding Vertices Nodes Tasks v 0 N 0 Load b 0 v 1 N 1 Load b 1 v 2 N 2 Load b 2 v 3 N 3 Load b 3 v 6 N 0 Fetch b 1 from N 1 ; FetchOpenEC associates tasks with different types of vertices.
At a high level, the Load task is associated with a vertex without any child; the Fetch task is associated with a parent vertex that has a child vertex; the Compute task is associated with a vertex with more than one child for the linear combination; the Persist task is associated with a vertex without any parent, while it is also associated with a vertex without any child in the case of online encoding (see the example below).
Example: We show the parsing procedure via an example.
Suppose that we encode four data blocks (i.e., b 0 , b 1 , b 2 , and b 3 ) to generate two parity blocks (i.e., b 4 and b 5 ) using the (6, 4) RS code, based on the ECDAG in Figure 3(c) and the Encode function in Listing 3.
Table 1 shows the vertex sequence of tasks for both online and offline encoding.For online encoding (see Table 1(a)), the client-side agent (denoted by C) performs all coding operations.
It finally persists all data blocks and parity blocks into HDFS.For offline encoding (see Table 1 Since v 4 and v 5 have no parent and are the last vertices in the topological order, they persist the blocks to HDFS.
Note that OpenEC can parallelize the coding operations on the vertices that have no dependencies on others.
For example, OpenEC can simultaneously execute the tasks for v 0 , v 1 , v 2 , and v 3 , and similarly the tasks for v 4 and v 5 .
In addition to letting erasure coding designers construct ECDAGs, OpenEC can automatically customize ECDAGs for performance optimizations to save manual configuration efforts.
We address this in two aspects.
Automated BindX and BindY: OpenEC can automatically call BindX and BindY for some specific subgraph structures of an ECDAG.
For BindX, OpenEC examines all parent vertices that have more than one child vertex in an ECDAG.
If multiple parent vertices have the same set of child vertices, OpenEC calls BindX on those parent vertices (e.g., v 4 and v 5 in Figure 3(b)).
For BindY, for any parent vertex (with one or more child vertices), OpenEC calls BindY on the parent vertex and any one of the child vertices (e.g., the parent vertex v 6 and the child vertex v 0 in Figure 3(c)).
Hierarchy awareness: OpenEC can further enhance the repair performance based on the physical DSS topology.
One scenario is that a DSS hierarchically organizes storage nodes in racks [19] (or clusters [38]), such that the cross-rack bandwidth is much more constrained than the inner-rack bandwidth.
OpenEC can transform an ECDAG into a pipelined ECDAG, so as to mitigate the cross-rack traffic.
Our idea is based on repair pipelining [25], which pipelines partial coding operations across multiple storage nodes.
We additionally perform all partial coding operations within a rack before sending the partial coding results to another rack.
To illustrate, suppose that we deploy an (n, k) RS code with k = 6.
We want to repair a lost block b 0 from six other available blocks b 1 , b 2 , b 3 , b 4 , b 5 , and b 6 , such that blocks b 1 , b 3 , and b 5 are in one rack, while blocks b 2 , b 4 , and b 6 are in another rack.
We also want to store the reconstructed block b 0 at the same rack as b 2 , b 4 , and b 6 .
The conventional repair approach is to retrieve all six available blocks and construct an ECDAG as in Figure 5(a).
Then we need to transfer three blocks (i.e., b 1 , b 3 , and b 5 ) across racks.
Instead, OpenEC can automatically construct another ECDAG as in Figure 5(b), in which it first We implement an OpenEC prototype in C++ with around 7K LoC.
We use Intel's Intelligent Storage Acceleration Library (ISA-L) [6] to implement erasure coding functionalities.
Here, we highlight several implementation details of OpenEC.From blocks to packets: OpenEC performs coding operations in units of packets to improve performance, while the read/write operations are still in units of blocks ( §2.1).
By default, the packet size is 128 KiB.
For encoding (both online and offline), OpenEC writes n erasure-coded packets to n DataNodes; in the case of sub-packetization, each packet is divided into sub-packets.
If a DataNode receives an amount of packet data equal to the HDFS block size (64 MiB by default), it seals the block and stores additional packets in a different block.
The n sealed erasure-coded blocks then form a coding group.
Note that while OpenEC is sending packets to DataNodes, it can start encoding for the next group of packets.
Thus, both the sending and encoding operations can be done in parallel.
Similarly, OpenEC performs decoding (for degraded reads and full-node recovery) at the packet level.
As OpenEC performs packet-level coding operations, the block layouts differ in online and offline encoding.
For online encoding, OpenEC adopts a striped layout as in HDFS-3 [4], as it stripes file data across blocks at the granularities of packets.
For offline encoding, OpenEC adopts a contiguous layout, as the file data is first stored in a block before encoding.
Figure 6 depicts both block layouts.
Internal communication: OpenEC uses Redis [8] for internal communications among the controller, agents, and OECClient.
Each agent maintains a local in-memory key-value Redis store.
The controller sends the task instructions of coding operations to an agent via the Redis client, and the task instructions are buffered at the agent for subsequent processing.
Agent-to-agent communications are pull-based via the Fetch tasks ( §4.3), such that the sender agent buffers the blocks to be sent in its local Redis store, and the receiver fetches the buffer via the Redis client.
Each OECClient also communicates with its associated agent via Redis.
We conduct testbed experiments on OpenEC.
We summarize our major findings on OpenEC: (i) it preserves the performance of HDFS-RAID and HDFS-3 in erasure coding deployment ( §5.2); (ii) it supports various state-of-the-art erasure coding solutions and preserves their properties, especially in network-bound environments ( §5.3); (iii) it can automatically optimize the repair performance for a hierarchical topology ( §5.4); and (iv) it achieves scalable performance in real cloud environments ( §5.5).
Testbeds: We evaluate OpenEC on both a local cluster ( §5.2- §5.4) and Amazon EC2 ( §5.5).
Our local cluster testbed comprises 16 machines, each of which has a quad-core 3.
as the default DSS for OpenEC, except when we compare OpenEC with HDFS-RAID.
Regarding the automated optimization features ( §4.4), our experiments enable automated BindX and BindY, except when we evaluate the original performance of erasure codes without OpenEC optimization in §5.3 and when we evaluate BindX and BindY in §5.4.
We also disable hierarchy-aware repairs until we evaluate this feature in §5.4.
We assign a dedicated machine to serve both the OpenEC controller and the HDFS NameNode, while each remaining machine serves an OECClient, an OpenEC agent, an HDFS client, and an HDFS DataNode.
We plot the average results over 10 runs, including the error bars showing the maximum and minimum of the 10 runs.
We compare OpenEC with HDFS-RAID and HDFS-3 in terms of basic operations using our local cluster.
As OpenEC adds another software layer between upper-layer applications and the underlying DSS, it may incur extra overhead.
We show that such overhead (if any) is limited; in some cases, OpenEC even significantly improves performance.
We also compare OpenEC with native coding performance and evaluate its performance for different block and packet sizes.Single-client performance in online encoding: We first compare the single-client performance between HDFS-3 and OpenEC, both of which are configured with online encoding to generate erasure-coded data.
Here, we use the (9, 6) RS code (as in QFS [31]).
We first write a file of size 384 MiB (i.e., six times the block size), and issue a normal read to the file without failures.
We also issue a degraded read to the file with one data block deleted.
Figure 7(a) shows the throughput results of writes, normal reads, and degraded reads.
Both OpenEC and HDFS-3 have similar performance: OpenEC's throughput is slightly less than HDFS-3's by 2.36% in writes, and is slightly higher than HDFS-3's by 2.83% and 4.41% in normal reads and degraded reads, respectively.Multi-client performance in online encoding: We compare the multi-client performance between HDFS-3 and OpenEC.We run a total of five clients, each of which writes a file of size 384 MiB under the (9, 6) RS code.
Figure 7(b) shows the aggregate throughput of all five clients in writes, normal reads, and degraded reads.
OpenEC has lower aggregate throughput than HDFS-3 in writes by 1.95%, but higher aggregate throughput in normal reads and degraded reads by 12.9% and 8.97%, respectively.
Nevertheless, considering the error bars in the figure, we do not see significant performance differences between OpenEC and HDFS-3.
Offline encoding: We compare the performance between HDFS-RAID and OpenEC in offline encoding.
We now deploy OpenEC on HDFS-RAID for fair comparisons.
We write 180 blocks, and use offline encoding to generate erasurecoded blocks using the (9, 6) RS code (i.e., a total of 30 coding groups).
We then delete the blocks of one storage node and trigger full-node recovery.
Here, we measure the offline encoding throughput (i.e., the amount of input data being encoded per unit time) and the full-node recovery throughput (i.e., the amount of lost data being recovered per unit time).
Note that HDFS-RAID performs offline encoding and full-node recovery via MapReduce.
To exclude the MapReduce startup overhead in our evaluation, we start an empty MapReduce job to measure its latency, and subtract this latency (which is around 20 s) in our evaluation of HDFS-RAID.
Note that OpenEC does not use MapReduce in offline encoding and full-node recovery.
Figure 7(c) shows the results.
Interestingly, OpenEC increases the offline encoding throughput of HDFS-RAID by 137%.
We study the HDFS-RAID source code and find that the performance difference is mainly due to the extra step of HDFS-RAID in reading and re-writing all parity blocks into a single HDFS file after parity regeneration.
For fullnode recovery, OpenEC has slightly higher throughput than HDFS-RAID by 7.9%, yet the two systems have limited differences considering the error bars.Online vs. offline encoding: We further compare online and offline encoding in OpenEC versus the file size, and study the performance difference between the striped layout (in online encoding) and the contiguous layout (in offline encoding).
We deploy OpenEC atop HDFS-3, and show that it allows both online and offline encoding atop HDFS-3 (which currently supports online encoding only).
We consider the single-client performance, in which a client uses the (12, 8) RS code and writes a file of size ranging from 1 MiB to 64 MiB (assuming that the file size is divisible by eight).
For online encoding, OpenEC stripes the file in packets across eight blocks and seals the blocks after the file write is completed (note that each block is less than the default block size 64 MiB); for offline encoding, OpenEC stores the file in a block and later encodes it with seven other blocks ( §4.2).
We compare their performance in a normal read (without failures) and a degraded read (with one data block deleted) to the file; in offline encoding, we delete the data block that stores the file in our degraded read evaluation.
Figure 7(d) shows the results.
The throughput increases with the file size, since the data transfer performance becomes more dominant as the blocks become larger.
We also see the performance differences in online and offline encoding.
In online encoding, both normal reads and degraded reads show similar performance, in which the client issues reads to eight blocks in parallel.
In offline encoding, its normal read throughput is much higher than that in online encoding (by 44-718%), as any slowdown in one of the parallel reads to online-encoded data can degrade the overall performance.
However, the degraded read throughput in offline encoding is much less than that in online encoding especially for larger file sizes, as it needs to retrieve eight blocks (i.e., seven additional blocks over the original file) to recover the file.
To validate our results, we conduct similar experiments using the original erasure coding implementations in HDFS-3 and HDFS-RAID (which realize online and offline encoding, respectively) and they show similar performance differences as in OpenEC (we omit the results here in the interest of space).
Comparisons with native coding operations: We compare the computational performance of the ECDAG-based coding operations with that of the native coding operations using ISA-L in HDFS-3.
Figure 8(a) shows the encoding throughput for k 64-MiB blocks under (n, k) RS codes.
ECDAG-based encoding has 29-38% lower throughput than native encoding, mainly because there is additional overhead for creating multiple compute tasks for computing the n − k parity blocks.
Figure 8(b) shows the decoding throughput for decoding one block, in which ECDAG-based decoding has only slightly less throughput (by 0.6-3.2%) than native decoding, as there is only one compute task for decoding a single block.
Nevertheless, compared to the overall read/write operations (Figure 7), the computations of ECDAG-based coding are much faster and incur limited overhead.
The performance degrades if the packet size is too small since there are many function calls for retrieving individual packets, or if the packet size is too large since there is less parallelism.
To achieve high performance, our default setup chooses the block size as 64 MiB and the packet size as 128 KiB.
We realize several state-of-the-art repair-friendly erasure coding solutions based on the ECDAG abstraction.
Recall from §2.1 that existing repair-friendly codes are designed to minimize the repair bandwidth or I/O in single-failure repairs.
Thus, we focus on evaluating their performance of repairing one lost block in a coding group under OpenEC.
We configure two bandwidth settings in our local cluster: 1 Gb/s and 10 Gb/s.
For the 1 Gb/s case, network transfer becomes the bottleneck (compared to coding computations and disk I/O), and we expect that the empirical performance conforms to the theoretical gains.
We use the conventional repair approach of RS codes as our baseline, in which it retrieves k blocks from k non-failed DataNodes to decode the lost block in a fetch-and-compute manner ( §2.2).
We compare the conventional repair approach with the following solutions:• LRC (Figure 10(a)): We compare RS codes with Azure's LRC [21].
For RS codes, we set (n, k) = (9, 6); for LRC, we set (n, k) = (10, 6), in which there are two local parity blocks, each of which is encoded from a local group of three data blocks, and two global parity blocks that are encoded from all six data blocks.
• MSR codes (Figure 10(b) MSR codes: MISER codes [45] (which require n ≥ 2k) and Butterfly codes [32] (which require n = k + 2).
We consider the (6, 4) RS code, the (6, 4) Butterfly code, and the (8, 4) MISER code.
• Repair algorithms (Figure 10(c)): We study how the repair algorithms, namely PPR [29] and repair pipelining [25], improve the repair performance of RS codes by parallelizing partial repair operations.
We compare them with the conventional repair under the (9, 6) RS code.
• Double Regenerating Codes (DRC) (Figure 10(d)): We compare RS codes with DRC [19] in a hierarchical network setting.
We divide our local cluster into three logical racks.
We use the Linux tc command to limit the bandwidth between any two storage nodes at different logical racks as 1 Gb/s [44], while the bandwidth between any two storage nodes within the same logical rack remains 10 Gb/s.
We compare RS codes and DRC under (n, k) = (6, 4) and (n, k) = (9, 6).
In both cases, we distribute the erasurecoded blocks of each coding group evenly across different nodes in three racks (with n/3 erasure-coded blocks each).
Figure 10 shows the results; for our comparisons, Table 2 also shows the theoretical throughput gains of the erasure coding solutions over the conventional repair approach for RS codes.
For the 1 Gb/s network, we observe that the empirical throughput gains of the erasure coding solutions are consistent (with only slight degradations) with the theoretical throughput gains.
For the 10 Gb/s network, the empirical gains decrease since the coding computation and disk I/O overheads become more significant.
For example, MISER codes have less throughput than Butterfly codes in the 10 Gb/s network; the throughput gain of MISER codes drops to 1.25×, while that of Butterfly codes drops to 1.35× (Figure 10(b)).
The reason is that both MSR codes retrieve data from n − 1 non-failed storage nodes for repairs, and MISER codes connect to more storage nodes than Butterfly codes (seven versus five) and incur higher disk I/O overhead.
Overall, OpenEC preserves the properties of the erasure coding solutions.
We now evaluate how OpenEC achieves performance gains via automated optimizations ( §4.4) for a hierarchical topology.
We again configure a three-rack logical topology in our local cluster as in our DRC experiments in §5.3.
We first compare the offline encoding performance for three configurations: (i) automated optimization is disabled, (ii) only automated BindX is enabled, and (iii) both automated BindX and BindY are enabled (our default setting).
We consider the (8,6), (10,8), and (12, 10) RS codes.
We measure the throughput of offline encoding by writing 30 coding groups of blocks into HDFS-3 via OpenEC, which evenly distributes the blocks across three racks.
Figure 11(a) shows that enabling only BindX increases the throughput by 37-42%, while enabling both BindX and BindY increases the throughput by 38-44%.
We also evaluate how OpenEC automatically improves the repair performance via the construction of a pipelined ECDAG.
We delete all blocks of one storage node and trigger full-node recovery on the same node.
Figure 11(b) shows that the repair optimization increases the repair throughput of OpenEC by 82-128%.
We finally evaluate OpenEC in Amazon EC2.
We configure three settings with N instances, where N = 10, 20, and 30 (see §5.1 for the instance type).
One instance hosts the OpenEC controller and the HDFS NameNode, and each of the remaining N − 1 instances hosts an OECClient, an OpenEC agent, an HDFS client, and an HDFS DataNode.
We consider the (9, 6) RS code, and all N − 1 clients issue different basic operations as in §5.2.
Figure 12 shows the results when OpenEC realizes online and offline encoding atop HDFS-3.
We observe consistent throughput patterns as in our local cluster experiments in §5.2 (e.g., both normal reads and degraded reads have similar throughput).
Also, the performance of OpenEC scales well with the number of instances.
(a) Online encoding (b) Offline encoding Figure 12: Performance in Amazon EC2.
New erasure coding solutions: RS codes [43] are widely deployed today (e.g., [5,7,15,31,54,55]), mainly for two reasons.
First, RS codes are maximum distance separable (MDS), meaning that under the coding parameters (n, k), the fault tolerance against n − k block failures is achieved with the minimum storage redundancy (i.e., n/k times the original data).
Second, RS codes support general coding parameters n and k (provided that k < n).
However, RS codes have high repair costs, and hence many new erasure coding solutions have been proposed to reduce the repair bandwidth or I/O.
One direction of research is to design new erasure codes.
Minimum-storage regenerating (MSR) codes [14] minimize the repair bandwidth and preserve the MDS property.
Followup studies design new MSR codes [18,32,39,42,45,50,52], some of which are evaluated in open-source DSSs (e.g., PM-RBT codes [39] are evaluated in HDFS, while Butterfly [32] and Clay [52] codes are evaluated in Ceph).
Aside MSR codes, some MDS codes incur slightly more repair bandwidth than the minimum point but can be easily constructed with any (n, k) (e.g., [24,41]), while some non-MDS erasure codes trade more storage redundancy than MDS codes for less repair I/O (e.g., [21-23, 33, 44, 49]).
DRC [19] minimizes the crossrack repair bandwidth in hierarchical topologies.Another direction of research is to design efficient repair algorithms that apply to general erasure codes.
Lazy repair [11,47] reduces repair executions by deferring a repair until a threshold number of failures occurs.
PPR [29] and repair pipelining [25] parallelize a single-failure repair across storage nodes.
Proactive degraded reads [20] mitigate tail latencies via the load balancing of read requests.Unlike the above studies, OpenEC targets a different perspective and focuses on unified and configurable erasure coding management.
It supports different new erasure codes and repair algorithms in a unified framework.
Erasure coding programming: Several open-source libraries are available for erasure coding programming.
Zfec [10] implements RS codes and is used by Tahoe-LAFS [55].
Jerasure [36] is a C library that supports various erasure codes.
It is later extended with GF-Complete [34] to enable fast Galois Field arithmetic.
ISA-L [6] is another C library that supports various erasure codes, and it optimizes Galois Field arithmetic for Intel hardware.
Both Jerasure and ISA-L libraries are widely used in production (e.g., Ceph and Hadoop 3.0).
PyEClib [9] is a Python library used by OpenStack Swift.
It builds on liberasurecode [2], which unifies different erasure coding libraries including both Jerasure and ISA-L.
OpenEC emphasizes the deployment of erasure codes in DSSs, and it can leverage the above libraries to implement erasure codes via the ECDAG abstraction.
Configurable storage: There is an increasing demand of providing flexibility for storage system management and configuring different storage policies based on application requirements.
Existing approaches rely on either client-side customization [12,13,28,37] or the coordination by a centralized controller under the software-defined storage (SDS) framework [16,48,51].
OpenEC borrows the same principle from SDS, but specifically focuses on configurable erasure coding management in distributed environments.
This paper presents OpenEC, a new framework that provides unified and configurable erasure coding management for distributed storage.
It leverages the ECDAG abstraction to define erasure codes and configure the workflows of coding operations.
Our OpenEC prototype achieves effective performance atop HDFS in both local cluster and Amazon EC2 environments, while supporting a variety of state-of-the-art erasure codes and repair algorithms.
Our work sheds light on how to facilitate erasure coding designers to deploy erasure coding solutions in a simple and flexible manner.
This paper currently focuses on HDFS, which organizes data in fixed-size blocks.
Our technical report [27] also describes how we integrate OpenEC into QFS [31].
In future work, we study how OpenEC can be deployed in other DSSs, especially object-storage-based DSSs (e.g., Ceph and Swift) that organize data in variable-size objects.
