Exploiting lexical and semantic relationships in large unstructured text collections can significantly enhance managing, integrating, and querying information locked in unstructured text.
Most notably, named entities and relations between entities are crucial for effective question answering and other information retrieval and knowledge management tasks.
Unfortunately, the success in extracting these relationships can vary for different domains, languages, and document collections.
Predicting extraction performance is an important step towards scalable and intelligent knowledge management, information retrieval and information integration.
We present a general language modeling method for quantifying the difficulty of information extraction tasks.
We demonstrate the viability of our approach by predicting performance of real world information extraction tasks, Named Entity recognition and Relation Extraction.
The vast amount of information that exists in unstructured text collections is still primarily accessible via keyword querying at the document level.
Unfortunately, this method of information access largely ignores the underlying lexical and semantic relationships between terms and entities in the text.
These relationships can be extremely valuable for answering questions, browsing the documents, and managing information associated with the entities of interest.
Additionally, document retrieval relevance could be improved if we detect meaningful terms (e.g., named entities such as dates, persons, organizations, and locations); and related entities (e.g., pairs of entities such as "person's birth date" and "person who invented a device") could be used directly to answer questions.
Furthermore, indexing entities and relationships would support more intelligent document browsing and navigation, and would allow for richer interactions with the document collections Hence, being able to reliably extract such relationships from text may be of vital importance to knowledge management and information retrieval.
However, real collections can exhibit properties that make them difficult for information extraction.
At the same time, tuning an information extraction system for a given collection, or porting an information extraction system to a new language, can require significant human and computational effort.
Hence, predicting if an extraction task will be successful (i.e., the required information can be extracted with high accuracy) is extremely important for adapting, deploying, and maintaining information extraction systems, and, ultimately, for managing and retrieving information in large text collections.We observe that document collection properties, such as typical text contexts surrounding the entities or relation tuples, can affect difficulty of an extraction task.
In this paper, we present a first general approach to use context language models for predicting whether an extraction task will succeed for a given document collection.More specifically, we consider two crucial information extraction tasks: Named Entity Recognition, and Relation Extraction.
• Named Entity Recognition (NER) is a task of identifying entities such as "Person", "Organization", and "Location" in text.
The ability to identify such entities has been established as an important pre-processing task in several areas including information extraction, machine translation, information retrieval, and question answering.
NER often serves as an important step in the Relation Extraction task described next.
• Relation Extraction (RE), is a task of identifying semantic relationships between entities in the text, such as "person's birth date", which relates a person name in the text to a date that is the person's birth date.
Once the tuples for this relation (e.g., <"Albert Einstein", "14 March 1879">) are identified, they can be used to directly answer questions such as "When was Albert Einstein born?
"Most state of the art NER and RE systems rely on local context to identify entities or determine the relationship between target entities.
In NER, contextual patterns such as "Mr." or "mayor of" are often used for hypothesizing occurrences of entities and classifying such identified entities, especially when they are polysemous or of a foreign origin.
The local context is also important for the RE task.
Intuitively, if the context surrounding the entities of interest for a given relation looks similar to the general text of the documents (i.e., there are no consistent and obvious "clues" that the entities or relationships of interest are present), then the RE task for that relation will be hard.
While NER systems can resort to dictionary lookups in some cases (e.g., for the "Location" entities, dictionaries can be particularly helpful), for others (e.g., people's names or organizations) high accuracy may not be possible.
In contrast, if the text context around entities in the collection tends to contain telltale clues, such as "Mr." preceding a person name, the extraction task is expected to be easier, and higher accuracy achievable.Our approach formalizes and exploits this observation by building two language models for the collection -a task-specific context language model for the extraction task, and the overall background model for the collection.
We can then compare the two language models and compute the divergence of the taskspecific language model from that of the overall collection model.
If the divergence is high (i.e., the task-specific language model is different from the overall model), the extraction task is expected to be easier than if the divergence is low (i.e., the taskspecific language model is very similar to the document language model).
Interestingly, our approach can be potentially helpful for other applications, including better term weighting for information retrieval, and supporting active learning for interactive information extraction.
For example, we could derive improved term weighs for domain-specific retrieval tasks such as birthday finding by incorporating context model weights.
We will discuss other promising future directions of this work in Section 5.
The rest of this paper is organized as follows.
In the next section we review related work.
In Section 3 we present our formal model and the algorithms for building the language models.
In Section 4 we present experimental results for the NER and RE tasks over large document collections.
In Section 5 we present our conclusions, and discuss potential future research directions.
Our work explores language modeling for information extraction and thus touches on areas of information retrieval, information extraction, and language modeling.Our approach is partly inspired by the work of CronenTownsend, Zhou, and Croft [9] on predicting query performance by measuring the similarity of a language model LMQ derived from the retrieved documents for a query and a language model for the whole target collection of documents LMColl.
Using simple unigram language models, they showed that the relative entropy between the query and collection language models correlates with the average precision in several TREC test collections.
In this paper, we apply similar language modeling techniques to the task of predicting information extraction performance.Language modeling, typically expressed as the problem of predicting the occurrence of a word in text or speech, has been an active area of research in speech recognition, optical character recognition, context-sensitive spelling, and machine translation.
An in-depth analysis of this problem in natural language processing is presented in [20], Chapter 6.
Language modeling has also been used to improve term weighting in information retrieval (e.g., [22,30] and others).
However, in previous work LM was used as a tool for improving the specific system performance, whereas in our work we attempt to predict performance for general extraction tasks.An important distinction of our work is that we consider taskspecific contexts.
As our results indicate, using the locality in the overall document collection may not be sufficient, as local context models can become similar to the background model for overall document collection.
Our approach is similar in spirit to the use of entity language models described in [23] for classifying and retrieving named entities.
Our work is complementary as we present a general approach for modeling the performance of extraction tasks including both named entity recognition and relation extraction.For the named entity recognition task, numerous ways of exploiting local context were proposed, from relatively simple character-based models such as [11] and [19] to complex models making use of various lexical, syntactic, morphological, orthographical information, such as [6] and [10].
In this work, we show that we can predict the difficulty of identifying several types of named entities by using relatively simple context language models.
This study can be viewed as complementary to Collins' work [8] on the difficulty of identifying named entity boundaries, regardless of entity type.Relation extraction systems rely on variety of features (e.g., syntactic, semantic, lexical, co-occurrence), but all depend heavily on context.
Once the entities are identified, it is the textual context that expresses the relationship between the entities.
Partially supervised relation extraction systems (e.g., [1], [12], [18], [24], and others) rely on the text contexts of example facts to derive extraction patterns.For relation extraction, the task difficulty was previously analyzed by considering the complexity of the target extraction templates ( [2] and [17]).
Another promising approach described in [13] modeled the task domain variability by considering the different paraphrases used to express the same information in the text.
In contrast, our work quantifies the difference between the contexts around the entities and unrelated text contexts.
If the contexts of the example facts are similar to the background text, an extraction system is expected to have more difficulty deriving extraction patterns and recognizing the relevant entities.
In this section we describe the general approach we take for modeling the difficulty of an extraction task, and hence the expected performance of an extraction system on the task (Section 3.1).
Then, in Section 3.2, we describe the algorithms for computing the language models to make our predictions.
As we discussed, the textual context (i.e., the local properties of the text surrounding the entities and relations of interest) can be of crucial importance to extraction accuracy.
Intuitively, if the contexts in which the entities occur are similar to the general text then extraction is expected to be difficult.
Otherwise, if there are strong contextual clues, the extraction should be easier and we should expect higher extraction accuracy.To quantify the notion of context, we use a basic unigram language model, which is essentially a probability distribution over the words in the text's vocabulary.
In this study, we derive this probability distribution from the histogram of words occurring in the local context of target entities by using maximum likelihood estimation.
Our purpose is to compare the language model associated with an entity type or relationship LMC with a background language model for the whole target text, denoted by LMBG.
Therefore, no smoothing of these models is necessary.
Intuitively, if the background language model for the collection is very similar to the language model constructed from the context of the valid entities then the task is expected to be hard.
Otherwise (if LMC is very different from LMBG), the task is expected to be easier.A common way to measure the difference between two probability distributions is relative entropy, also known as the Kullback-Leibler divergence:񮽙 ∈ ⋅ = V w BG C i C BG C w LM w LM w LM LM LM ) ( ) ( log ) ( ) || ( KLIn Information Theory, KL-divergence represents the average number of bits wasted by encoding messages drawn from the distribution LMC using as model the distribution LMBG.Alternatively, we can measure how different two models are by using cosine similarity, which represents the cosine of the angle between the two language models seen as vectors in a multidimensional space in which each dimension corresponds to one word in the vocabulary:|| || || || ) , ( Cosine C BG BG C BG C LM LM LM LM LM LM ⋅ > ⋅ < =The closer the cosine is to 1, the smaller the angle and thus, the more similar the two models.
Hence, to measure the difference of the two models LMC and LMBG we define CDist as:CDist(LMC || LMBG) = 1 -Cosine(LMC, LMBG)to maintain symmetry with the KL metric, with larger values indicating larger difference between models.
We now describe how to construct a language model for a given extraction task.
For clarity, we describe a unigram language model, but our methodology can be extended to higher-order features.
For syntax-based extraction systems, we could parse the text and incorporate that information into the model as in [6].
However, as we will show experimentally, a simple unigram model is sufficient to make useful predictions.To construct the task-specific context language model LMC we search the collection for occurrences of valid entities (or relation tuples).
While for the NER and RE tasks LMC is constructed slightly differently (as described below), the overall approach is to consider the text context to be the K words surrounding the entities in question.More specifically, the language model for NER is constructed as outlined in Figure 3.1.
We scan the document collection D, searching for occurrences of each known entity Ei.
When an entity is detected, we add to LMC up to K terms to the right and to the left of the entity.The algorithm for constructing a task-specific language model for RE is outlined in Figure 3.2.
The procedure is similar to the NER algorithm above.
We scan the document collection D, searching for occurrences of each known example tuple Ti for the target relation.
For this, we search for all attributes of Ti in the text.
If all entities are present, and occur within K words of each other, we increment the LMC counts of all the words between the leftmost and the rightmost entities.
If the entities in a relation tuple are close together (i.e., there are fewer than K words separating the entities in the text), we include all the terms separating the entities.
Unfortunately, we don't have all the valid entities available (i.e., when predicting whether a task will succeed without going through the complete extraction process).
Hence, our model is build based on sampling the collection using a small (20-40) sample of the known entities or tuples by providing only these example entities as input to the NER and RE language model construction algorithms above.
For a large corpus, the samplebased model is expected to be a reasonable approximation of the complete task specific language model.
The background language model, LMBG is derived through maximum likelihood estimation using the word frequencies in each document collection.
When we discard stopwords from LMC we also discard them from LMBG.
For each term w in d [start +1],…, d [end -1] Increment count of w in LMC Normalize LMC return LMC ConstructNERLanguageModel (Entities E, Documents D, K ) For each document d in D For each entity Ei in E if Ei is present in d For each instance of Ei spanning from start to end For each term w in d [start -K], …, d [start-1] Increment count of w in LMC For each term w in d [end + 1], …, d [end + K] Increment count of w in LMC Normalize LMC return LMCIn order to interpret the divergence of a task specific language model LMC from the background language model, we build a reference context language model LMR (also denoted as RANDOM).
We construct LMR, by taking random samples of words in the vocabulary (excluding stopwords) of the same size as the entity samples.
We then use these words input to Algorithm 3.1.
Using LMR we can then compute the "reference" divergence of a context language model from the background model for a given sample size.
For large sample sizes, LMR is expected to approximate the background model.
Indeed, Figure 3.1 reports that for larger random word sample sizes, LMR becomes more similar to the background model, and the divergence steadily decreases.
We also use LMR to normalize our divergence measures to be robust to different entity sample sizes and collection sizes.
For this, we compute the normalized divergence as the ratio of the KL-divergence value of LMC, and the KL-divergence value of LMR, as compared to the background distribution LMBG.
We can similarly compute normalized cosine distance as the ratio of the CDIST values of LMC and LMR compared to LMBG.Constructing the context models LMC and LMR can be done efficiently by using any off-the-shelf search engine and considering only the documents retrieved by search for the example entities or tuples, and run Algorithms 3.1 and 3.2 only over these reduced document sets.
Having described constructing the language models for extraction tasks, we now turn to experimental evaluation.
We evaluated our prediction for two real-world tasks: Named Entity Recognition (NER) and Relation Extraction (RE).
We first describe the experimental setup (Section 4.1), including the datasets, entity and relation types, and parameter settings we considered.
Then we describe our experiments for predicting NER difficulty (Section 4.2), followed by our experiments on predicting RE difficulty (Section 4.3).
In order to design a realistic evaluation we focused on two extraction tasks, NER and RE, over large document collections.The overall goal of the experiments is to determine if the language models, constructed from a realistically small sample of the extractions of interest, can make useful predictions about the observed accuracy of the extraction task for that collection.
To validate our extraction performance predictions for the NER task, we used as reference the top performing systems in the CoNLL shared task competition, which were evaluated over a manually annotated subset of news articles from the same RCV1 corpus as described above.
Moreover, we built the samples of named entities by randomly sampling the set of named entities present in the training set provided by the CoNLL competition organizers (described in [26] and [27]).
To validate our performance predictions on the RE task, we used a bootstrapping-based extraction system similar to Snowball [1], which is heavily dependent on the example entities and the text context in which they appear to derive extraction patterns.
For comparison, we also report RANDOM, the divergence of the random keyword sample-based language model, LMR.In our experiments we explored the following parameters:• Context size: number of words to the left and to the right of entity to include as context.
• Maximum distance separating the entities (for RE task) • Divergence metric, CDist or KL: The language model divergence metrics defined in Section 3.1.
We found that CDIST is strongly correlated with KL, and does not provide additional information.
Therefore, for clarity and brevity, we report only the KL values for our experimental evaluation.
• Example set size S: number of randomly drawn entities (or relation tuples).
• Random sample size R: number of randomly drawn terms to estimate the background model.
For all experiments, R was equal to the value of S above for each task.
• Stopwords: we analyze two cases, when stopwords (common English words such as prepositions, conjunctions, numerals, etc.) are included in the language model, and when they are excluded.
In both cases, we discard punctuation.
• N-gram size N: We considered word unigrams, bigrams and trigrams as features of the language models.
In order to explore the parameter space and evaluate the accuracy of our predictions, we use as reference the reported performance of the top five systems in the CoNLL 2003 shared task competition [27], which is summarized in Table 4.3.
According to the reported numbers, the Person (PER) and Location (LOC) entities are the "easiest" to extract, whereas the Miscellaneous (MISC) and Organization (ORG) are relatively difficult.
1/10, discarding stopwords).
Tables 4.4a and 4.4b report the prediction results using stopwords in the language model (Table 4.4a) and discarding the stopwords (Table 4.4b).
As expected, the context language models are more similar to the background model when stopwords are included, but in both cases the conclusions are the same.
This is encouraging, as it shows that our approach may work even for languages where no lexical information (such as stopwords) is known a priori.A drawback of our current approach is that we do not consider how easy it is to identify entities of a given type based on sources of information other than context, such as morphology, internal capitalization, or gazetteer lists.
Consequently, our system may not be able to predict accurately the extraction performance of fully-featured systems for entities with various intrinsic properties that make them easier or harder to identify independent of context (e.g. the real performance for MISC is somewhat lower than expected based on our prediction due to the fact that capitalization and length varies to a much larger degree for MISC entities than for the other entity types).
We now consider the sensitivity of our results to sample size (Figure 4.1) and N-gram size (Figure 4.2).
Figure 4.1 reports normalized KL divergence for RCV 1/100 for seed sample sizes of 10, 20, 30, 40, and 50.
As we can see, for sample sizes greater than 20 our predictions do not change.
Hence, sample size of 20 seed entities will be used for our subsequent experiments.
We now show that our prediction technique also applies to languages other than English, by evaluating it for the named entity task on the Spanish and Dutch collections used in the CoNLL 2002 shared task evaluation [24].
As we discussed, porting information extraction systems to new domains and new languages can require significant effort.
For languages other than English, annotated data and other language specific resources are less readily available.
Hence, developing systems for these languages is typically more difficult, and this is also shown by the lower performance of state-of-the-art NER systems on Spanish and Dutch (Tables 4.6 This is confirmed by the actual results for Spanish.
For Dutch, the best performing systems in CoNLL 2002 also performed much better for PER than for ORG and MISC, LOC being, similarly to English, an outlier.
Our system predicts that LOC is a difficult entity type to extract based on context for all languages mainly because many of the relevant corresponding contexts in these languages are stopwords, which occur frequently throughout the text.
However, real systems were able to identify the LOC entities because the percentage of actual entities seen both in the training and in the test is typically greater than for the other entity types.
This aspect of the problem is not modeled by our approach, and can be addressed in future work.
We now turn to predicting performance of relation extraction tasks (RE).
The goal is to predict which relations are "difficult" to extract, and which ones are "easy".
Table 4.10 reports the actual extraction accuracy on the RE task using a simple bootstrapping-based information extraction system similar to Snowball [1] and KnowItAll [12].
We report the precision on each task estimated by sampling 100 facts from the extracted relation instances.
As we can see, the BORN and DIED relations are "easy" for the extraction system (exhibiting precision of as high as 97%), whereas INVENT and WROTE are relatively "hard" (exhibiting precision as low as 50%).
To further investigate the required effort needed for robust prediction on the RE task, in Figure 4.3 we report the predictions for varying the seed sample size from 10 to 40 relation tuples.
As we can see, our predictions remain relatively stable for sample sizes of at least 20 seed tuples.
Interestingly, adding additional seed tuples beyond 20 does not improve the overall prediction accuracy as our approach: while our method distinguishes between the "easy" and "hard" relations correctly, it is not able to further distinguish between the two "hard" relations, namely that the WROTE relation is more difficult to extract than the INVENT relation.
We presented a general, domain and language independent approach for predicting extraction performance.
We have shown that our language modeling approach is effective for predicting extraction accuracy for tasks such as named entity recognition and relation extraction, both tasks crucial for high accuracy and domain-specific information retrieval and information management.As our experiments indicate, starting with even a small sample of available entities can be sufficient for making a reasonable prediction about extraction accuracy.
Our results are particularly encouraging as we consider a relatively simple model that does not require extra information to that typically available to modern NER and RE systems.Extending our method to use more sophisticated language models can further improve our predictions.
For languages where reliable NLP tools are available, one promising direction would be to incorporate syntactic features, and to apply techniques such as co-reference resolution to build richer and more accurate context language models.
Additionally, incorporating gazetteer lists similar to those typically used by the NER systems can further improve prediction accuracy.
Another interesting direction for future work is to correlate our predictions with the actual accuracy values for more fine-grained predictions.Furthermore, our results could be applied for building interactive information extraction systems that could guide the user by requesting more examples for the extraction tasks predicted to be "difficult".
Such an interactive system could more effectively focus valuable human effort on the "difficult" extraction tasks, where it is most sorely needed.As we have shown, our approach is general and languageindependent.
With amounts of new information available in text increasing daily, our techniques could be extremely valuable for developing, maintaining, and deploying information extraction technology for better information access.
We thank Luis Gravano for ideas and discussions that inspired this work.
We also thank Eric Brill and the anonymous referees for their insightful comments.
