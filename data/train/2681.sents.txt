Scale-out applications have emerged as the dominant In-ternet services today.
A request in a scale-out work-load generally involves task partitioning and merging with barrier synchronization, making it difficult to predict the request tail latency to meet stringent tail Service Level Objectives (SLOs).
In this paper, we find that the request tail latency can be faithfully predicted, in the high load region, by a prediction model using only the mean and variance of the task response time as input.
The prediction errors for the 99th percentile request latency are found to be consistently within 10% at the load of 90% for both model and measurement-based testing cases.
Consequently, the work in this paper establishes an important link between the request tail SLOs and the low order task statistics in a high load region, where the resource provisioning is desired.
Finally, we discuss how the prediction model may facilitate highly scalable, tail-constrained resource provisioning for scale-out workloads.
Scale-out, online-data-intensive (OLDI) workloads, such as web searching and social networking, provide userfacing services that involve a large number of servers for parallel processing, while requiring sub-second request responsiveness under high incoming request rates.
The system running these workloads usually operates under stringent SLOs, such as imposing a tight tail constraint on high percentile request response time, e.g., 99th or 99.9th-percentile, to satisfy as many user requests as possible [7,17].
However, imposing tight tail SLO for OLDI workloads makes resource provisioning in datacenter a challenging task.
Due to the lack of good understanding of request tail behaviors, the current practice is to overprovision datacenter resources to meet SLO, at the cost of low resource utilization, e.g., less than 50% CPU and memory utilizations [5,8,22].
Although resource provisioning proposals with tail SLOs in mind exist, they generally do not incorporate tail SLOs explicitly as design constraints and rely on empirical data to verify whether the design meets tail SLOs or not.
For example, the resource provisioning problem is formulated as the minimization of the variance of data flow path latency, as a way to indirectly curtail the tail latency [12]; the target tail latency SLOs are tracked using online dynamic feedback-loop-control-based schedulers [9,25]; and employing job priority and a rate limiting technique based on the network calculus theory [27].
The root cause of the status quo is due to the lack of a link between systemlevel request tail SLOs and the subsystem-level task performance requirements.
The key difficulty lies in the fact that a scale-out workload may involve task partitioning and merging as well as task queuing.
Each request in the request flow with average request rate λ involves tasks to be queued at and processed by up to several thousands of task subsystems in parallel and then all the task results are merged and returned, as depicted in Fig. 1a.
Here a task subsystem may involve multiple replicated servers for task-level fault tolerance and load balancing, e.g., Fig. 1b, where λ r = λ /3 in the case of load balancing.
Notable examples are Web search engines [4] and social networking [20].
In this case, the request response time is determined by the slowest task [7].
As the system scales out, the probability that the request response time may hit the tail task latency quickly increases [7].
To date, no general results are available that can predict the tail request response time at scale.
This lack of understanding of request tail behaviors is further exacerbated by the various task scheduling and tail-cutting techniques being used in task processing.
In particular, as an effective tail-cutting technique, replicated servers in each subsystem are being used to allow redundant task issues to more than one replicated server to be processed, with the earliest result returned and rest removed [7,24].
Although some analytic results are available on redundant task issues [10,21,26], they either address only a single replicated server subsystem with exponential task service time distribution only [10] or parallel request load balancing without task partitioning [21,26].
The task partitioning-and-merging part of a scale-out workload generally lies in the critical path for request processing and constitutes a major part of request processing time and hardware cost, e.g., more than two-third of the total processing time and 90% hardware cost for a Web search engine [13].
Hence, it is of paramount importance to establish a link between the system-level request tail SLOs and the subsystemlevel task performance requirements to facilitate explicitly tail-constrained resource provisioning at scale.
This paper aims at tackling the above challenge.
It makes the following two major contributions.
First, by treating each subsystem as a black box, we find that the tail behavior of a task mapped to a subsystem can be captured by a generalized exponential distribution function in the high load region, which uses the mean and variance of the task response time as input.
This black-box solution allows the request distribution function and thus any given request tail SLOs to be explicitly expressed as a function of the means and variances of the individual task response times as the system scales out.
Hence, in the case of homogeneous subsystems for parallel task processing, the request tail SLO is only dependent on the mean and variance of the task response time for one task mapped to any given subsystem.
Second, we discuss how the proposed request tail prediction mechanism may be used to facilitate highly scalable, explicitly tail-constrained resource provisioning using homogeneous virtual machines (VMs) in a cloud environment.The remainder of the paper is organized as follows.
Section 2 presents the prediction model, simulation results, and analyses.
Section 3 discusses how the proposed model may facilitate tail-constraint resource provisioning.
Finally, Section 4 concludes the paper.
A system serving scale-out, OLDI workloads commonly involves a large number of task subsystems for parallel processing.
The diversity in the actual implementation of subsystems makes it extremely difficult to predict the task performance, let alone the request performance, in general.
However, since the ultimate goal of this research is to be able to design request scheduling algorithms that can meet stringent tail SLOs by proof of design, while achieving high resource utilization, we are interested in the peak-load resource provisioning in a high load region, e.g., 90% or higher.
In this region, it is possible to predict the task performance for a task mapped to a wide range of subsystems using a simple prediction model, as we now explain.
There is a large body of research results in the context of queuing performance in high load regions (e.g., see [23] and the references therein).
In particular, a classic result, known as the central limit theorem for heavy traffic queuing systems [14,15], states that for a G/G/m (here m is the number of servers) queue under heavy traffic load, the waiting time distribution could be approximated by an exponential distribution.
Clearly, this theorem applies to the response time distribution as well, since the response time distribution converges to the waiting time distribution as the traffic load increases.
The intuition behind this approximation is that in the high load region, the long queuing effect helps effectively smooth out service time fluctuations (i.e., the law of large numbers), which causes the waiting time or response time to converge to a distribution closely surrounding its mean value, i.e., the short-tailed exponential distribution, regardless of the actual arrival process and service time distribution.
Inspired by this result, in this paper, we treat any task subsystem, e.g., the one in Fig. 1b, as a black box, given in Fig. 2.
We further postulate that for a task mapped to a black box subsystem and in the high load region, the task response time distribution F(x) for any arrival process can be approximated as a generalized exponential distribution function [11], as follows,E[X], V[X] λ λF ge (x) = (1 − e −µx ) α x > 0, 0 otherwise,(1)where µ and α are the scale and shape parameter, respectively.
The mean and variance of the task response time are given by [11] E[X] = 1 µ [ψ(α + 1) − ψ(1)],(2)V [X] = 1 µ 2 [ψ ′ (1) − ψ ′ (α + 1)],(3)where ψ(.)
and its derivatives are the digamma and polygamma functions.
From Eqs.
(2) and (3), it is clear that the distribution in Eq.
(1) is completely determined by the mean and variance of the task response time.
The rationale behind the use of this distribution, instead of the exponential distribution, is that it can capture both heavy-tailed and shorttailed task behaviors depending on the parameter settings and meanwhile, it degenerates to the exponential distribution at α = 1 and E[X] = 1/µ.
As we shall see in the following subsection, this distribution significantly outperforms the exponential distribution in terms of tail latency predictive power for all the cases studied.The implication of the above black box approximation is significant.
It allows not only the task performance of a task mapped to a diverse range of subsystems to be captured by a unified distribution function, but also the request response time distribution and hence the tail SLO for the entire task-partitioning-merging system to be derived.
To see why this is the case, one notes that with all the task subsystems in Fig. 1a being viewed as black boxes, one effectively transforms the task-partitioning-merging problem into a split-andmerge model [16] whose distribution function can be expressed as follows, assuming the task response times for tasks mapped to different subsystems are independent random variables,F (N) (x) = ∏ N i=1 (1 − e −µ i x ) α i x > 0, 0 otherwise,(4)Now assume that the parallel subsystems are homogeneous, the distribution function can be further simplified as,F (N) (x) = (1 − e −µx ) Nα x > 0, 0 otherwise,(5)With Eq.
(5), it can be easily shown that the p-th percentile request response time x p can be written as,x p = − 1 µ log 1 − p 100 1 Nα(6)Since x p is a function of µ and α, which in turn, are functions of E[X] and V [X] of the task response time (according to Eqs.
(2) and (3)), a link between any given tail SLO in terms of x p and p, and E[X] and V [X] is established.
The implication of this result is significant.
On one hand, with any given tail SLO, the resulting E [X] and V [X] can serve as the task response time budgets for highly scalable, distributed task-level resource provisioning.
On the other hand, with given measured task response time statistics in terms of E[X] and V [X], whether the system meets the target tail SLO or not can be accurately predicted.
In the following two subsections, we test the performance of this prediction model at the subsystem and system levels, separately.
In this section, we test the accuracy of the proposed prediction model against a wide range of subsystems including pure model-based subsystems, hybrid measurement-and-model-based subsystems, as well as a pure measurement-based subsystem.For pure model-based and hybrid subsystems, we consider a typical subsystem setup given in Fig. 1b.
It includes a dispatcher and three replicated servers.
A task arriving at the subsystem is distributed to server replicas by a dispatcher based on a predetermined policy.
Each server replica is modeled as an M/G/1 queuing system.
Namely, for all the cases studied, the task arrival process is modeled as a Poisson process, which is considered a good model for scale-out workloads [19].
Both modelbased and measurement-based service time distribution functions are considered, including the following, -Empirical distribution measured from a Google search test leaf node provided in [18], which has a mean service time of 4.22ms, a coefficient of variance (CV) of 1.12, and the largest tail value of 276.6ms;-A heavy-tailed truncated Pareto distribution [2] with the same mean service time, i.e., 4.22ms, and a CV of 1.2, resulting in the corresponding parameters: the shape α = 2.0119, the lower bound L = 2.14ms, and the upper bound H = 276.6ms, which is set at the same maximum value of the empirical distribution above.-Weibull distribution [6] also with the same mean service time and a CV of 1.5, resulting in the corresponding parameters: the shape parameter α = 0.6848 < 1, i.e., a heavy-tailed distribution [6], and the scale parameter β = 3.2630.
We consider two task dispatching policies.
The first policy is a popular one, known as the Round-Robin (RR) policy.
In this policy, the dispatcher will send tasks to different server replicas in an RR fashion.
The second policy is still RR, but it also allows redundant-task issue, a well-known tail-cutting technique [7,24].
This policy allows one or more replications of a task to be sent to different server replicas in the subsystem.
The replications may be sent in predetermined intervals to avoid overloading the server replicas.
In our experiments, at most one task replication can be issued, provided that the original one does not finish within 10ms, which is around the 95th-percentile of the empirical distribution above.
For model-based and hybrid subsystems, the simulated tail task response time is compared against the tail response time predicted by the proposed prediction model, i.e., Eq.
(1), which uses the simulated mean and variance of the task response time as input.For the pure measurement-based subsystem, we implemented a Solr search engine [1] subsystem using a cluster of three Amazon EC2 m3.medium instances, each responsible for the same sample shard of the Wikipedia index.
The dispatching policy is, again, RR.
In this experiment, multiple client threads issue tasks to the servers in a stop-and-wait fashion, i.e., a client sends a task and then waits for the response before sending the next one.
The random combination of the task flows from all the clients mimics a random arrival process.
We focus on the cases when the number of clients is large enough to put a heavy load on the servers.
Without knowing the inner-working of the VMs, we simply treat them as black boxes and the testing is solely based on the measured task response time statistics.For the experiments on both pure model-based and hybrid subsystems, Fig. 3 presents the prediction errors for both the exponential and generalized exponential distributions at the load of 90%.
First, we note that the generalized exponential distribution significantly outperforms the exponential distribution for all the cases studied.
Second, the prediction errors for the generalized exponential distribution are consistently within 10% across the entire 95-99.9th percentile response time range, even for the RR case without tail cutting.
These results confirm our postulation that the generalized exponential distribution function could accurately predict the task tail performance in the high load region.
Now we further test the performance of the generalized exponential distribution for the aforementioned measurement-based subsystem.
The relative errors of the predicted task tail latencies against the measured ones are given in Table 1.
In this experiment, as the number of clients increases, the aggregate task throughput increases and then levels off as the number of clients reaches 40, indicating that the subsystem is under heavy load condition.
As one can see, the prediction errors reduce to less than 10% for all cases as the number of clients reaches 40, consistent with the performance data for the modelbased and hybrid cases.
In this section, we evaluate the accuracy of our generalized exponential distribution model as the system scales out.
We consider the task-partitioning-merging system in Fig. 1a with N = 10, 100, 500, and 1000 nodes for all the previously studied model-based and hybrid subsystems.
Fig. 4 presents the prediction errors at different load levels for the 99th percentile request response times.
Again, for all the cases studied, the errors are within 10% at the load of 90%.
Even at the load of 80%, the prediction errors are with 10% and 20% for the cases with and without tail cutting, respectively.
As a work in progress, the testing of the proposed prediction model for a complete Solr-based search engine on Amazon EC2 is underway.
The above testing results at both subsystem and system levels give us the confidence to expect that the results from this testing case will also be fairly accurate.
In this section, we discuss how the above prediction model may be used to facilitate highly scalable, explicitly tail-constrained resource provisioning.
For ease of discussion, we use the following example scenario as a guide throughout the discussion.
Assume that a content service provider wants to outsource its OLDI scale-out services to a cloud service provider.
With a given size of parallel searchable database D (e.g., an entire index as in a Web search engine) and monetary budget C, the content service provider wants to know whether or not the service to be deployed may sustain R requests per second, while meeting the tail SLO, i.e., the p-th-percentile request response time of L ms.An ad hoc approach is to immediately deploy the service to a certain scale and at runtime, scale out/up or down the system dynamically in a pay-as-you-go manner to meet the performance targets or monetary budget.
Without an initial estimation, however, such approaches run the risk of either over budgeting or failing to meet SLO and/or targeted request throughput performance.
Moreover, using the pay-as-you-go service for dynamic resource provisioning is generally much more Figure 4: The prediction errors for the system composed of both model-based and hybrid subsystems with the Round-Robin (upper three plots) and redundant-task-issue (lower three plots) policies.
                                                     d e f d g h i j d k t  l m n o o p  l m n o  q r s q t s u p v  l m n o o p v  l m n o  q r s q t s u p~ x  y  y                       Empirical, 3-replica, Round-Robin } y      } y y      x y y      } y y y                                   ¡  ¢  £ ¤ ¥ ¦ § ¨ © ¤ ¤ ª ª « ª ¬ ­ ® ¯ ° ± ² ³ ´ µ Truncated Pareto, 3-replica, Round-Robin   ¶ · ¸ ¹ º    ¶ · ¸ ¹ º    ¶ · ¸ ¹ º     ¶ · ¸ ¹ º » ¼ ½ » ¾ ½ » ¿ ½ » À ½ » Á ½ ½ Á ½ À ½ ¿ ½ ¾ ½ ¼ ½ ¼ ½ Â ¼ Ã ½ Ä ½ Å AE Ç È É Ê Ë AE AE Ì Ì Í Ì Î Ï Ð Ñ Ò Ó Ô Õ Ö × Weibull, 3-replica, Round-Robin Á ½ Ø Ù Ú Û Ü Á ½ ½ Ø Ù Ú Û Ü ¼ ½ ½ Ø Ù Ú Û Ü Á ½ ½ ½ Ø Ù Ú Û Ü Ý Þ ß Ý à ß Ý á ß Ý â ß Ý ã ß ß ã ß â ß á ß à ß Þ ß Þ ß ä Þ å ß ae ß ç è é ê ë ì í è è î î ï î ð ñ ò ó ô õ ö ÷ ø ù Empirical, 3-replica, Redundant ã ß ú û ü ý þ ã ß ß ú û ü ý þ Þ ß ß ú û ü ý þ ã ß ß ß ú û ü ý þ ÿ ¢ £ ÿ ¤ £ ÿ ¥ £ ÿ ¦ £ ÿ § £ £ § £ ¦ £ ¥ £ ¤ £ ¢ £ ¢ £ ¨ ¢ © £ £ ¡ ! "
# $ % & ' Truncated Pareto, 3-replica, Redundant § £ ( ) 0 1 2 § £ £ ( ) 0 1 2 ¢ £ £ ( ) 0 1 2 § £ £ £ ( )0 1expensive than static resource reservation for resource planning [3].
Given the sheer size of the system to be deployed, it is of paramount importance to develop an offline, highly scalable resource provisioning approach that can provide a quick initial assessment of whether the performance targets and monetary budget can be met or not.
The idea of our approach is sketched by the following tail-constrained resource provisioning procedure, in the context of the above example scenario: -For a desired type of VMs with, e.g., given CPU speed, memory size, and pricing model, build a replicated server cluster subsystem in the cloud using m (two to three) VMs by replicating a portion of the total database, i.e., D/N, to all the VM replicas, where N, an integer value, may be selected in such a way that D/N can fit comfortably in the memory in each VM;-Measure the mean and variance of task response time in the cluster running a task scheduling policy, at desired task rate λ = R;-Find the parameters of the generalized exponential distribution in Eq.
(1) by plugging in the measured mean and variance task latency into Eqs.
(2) and (3), respectively;-Estimate the p-th-percentile request response time x p based on Eq.
(6); -Finally, x p is compared against L and the total cost for running N VM clusters with m each is compared against the associated budget C, to see if both the tail SLO and monetary budget are met.
If both are met, a feasible tail-constrained resource provisioning is found.
Otherwise, the performance targets and/or budget are revised and then rerun the procedure.
Note that if x p is found well below L, one may consider reducing N and/or m and see if it is still below L.
This iterative testing can help minimize the cost.
This paper proposed a simple prediction model to predict the tail SLOs for scale-out applications involving taskpartitioning-merging.
The required inputs to the prediction model are only the mean and variance of task response time for a task mapped to a subsystem.
The experimental results showed that the prediction model yields accurate prediction with errors consistently within 10% at the server loads of 90% or higher, providing a much needed prediction tool to facilitate tail-constrained resource provisioning for scale-out applications.This is a work in progress.
A full-fledged testing of the proposed prediction model in Amazon EC2 cloud is currently underway.
We would like to thank anonymous reviewers for their helpful comments and suggestions on improving the presentation of the paper.
