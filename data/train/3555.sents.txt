Data-sharing systems are often used to store sensitive data.
Both academia and industry have proposed numerous solutions to protect the user privacy and data integrity from a compromised server.
Practical state-of-the-art solutions, however , use weak threat models based on centralized trust-they assume that part of the server will remain uncompromised, or that the adversary will not perform active attacks.
We propose Ghostor, a data-sharing system that, using only decentralized trust, (1) hides user identities from the server, and (2) allows users to detect server-side integrity violations.
To achieve (1), Ghostor avoids keeping any per-user state at the server, requiring us to redesign the system to avoid common paradigms like per-user authentication and user-specific mailboxes.
To achieve (2), Ghostor develops a technique called verifiable anonymous history.
Ghostor leverages a blockchain rarely, publishing only a single hash to the blockchain for the entire system once every epoch.
We measured that Ghostor incurs a 4-5x throughput overhead compared to an insecure baseline.
Although significant, Ghostor's overhead may be worth it for security-and privacy-sensitive applications.
Systems for remote data storage and sharing have seen widespread adoption over the past decade.
Every major cloud provider offers it as a service (e.g., Amazon S3, Azure Blobs), and it is estimated that 39% of corporate data uploaded to the cloud is related to file sharing [51].
Given the relentless attacks on servers storing data [45], a long-standing problem in academia [14,31,35,41,49,55,60,64,75,87] and industry [27,46,52,77,98] has been to provide useful security guarantees even when the storage server, and some users, are compromised by an adversary.To address this, early systems [35,48] have users encrypt and sign files.
However, a sophisticated adversary can still:• observe metadata about users' identities [24,38,47,102].
Even if the files are encrypted, the adversary sees which users are sharing a file, which user is accessing a file at a given time, and the list of users in the system.
Fig. 1 shows an example where the attacker can conclude that Alice has cancer from such metadata.
Further, this allows the attacker to learn the graph of user social relations [81,89].
• perform active attacks.
Despite the signatures, an adversary can revert a file to an earlier state as in a rollback attack, or hide users' updates from each other as in a fork attack, without being detected.
These are dangerous if, for example, *Sam Kumar and Yuncong Hu contributed equally to this work.
They are listed in alphabetical order by last name.
Ghostor's Anonymous E2EE Unknown (could be same as above) writes to F at 3pm Google search says BobMD is an oncologist.
Each of these tells me that Alice might suffer from cancer.Figure 1: An example of what a server attacker sees in a typical E2EE system versus Ghostor's Anonymous E2EE the shared file is Alice's medical profile, and she does not learn that her doctor changed her treatment.
Research over the past 15 years has striven to mitigate these attacks by providing anonymity-hiding users' identities from the storage server-or verifiable consistency-enabling users to detect rollback and fork attacks.
In achieving these stronger security guarantees, however, state-of-the-art systems employ weaker threat models that rely on centralized trust: a trust assumption on a few specific machines.
For example, they rely on a trusted party [66,90], split the server into two components assuming one is honest [49,54,74], or assume the adversary is honest-but-curious (not malicious) [7,16,65,104] meaning the attacker does not change the server's data or execution.Attackers have notoriously performed highly targeted attacks, spreading malware with the ability to modify software, files, or source code [62,106,107].
In such attacks, a determined attacker can compromise any few central servers.
Ideally, we would avoid any trust in the server or other clients, but unfortunately, that is impossible: Mazières and Shasha [69] proved that, if one cannot assume that clients are reliably online [55], clients cannot detect fork attacks without placing some trust in the server.
Hence, this paper asks the question: Can we achieve strong privacy and integrity guarantees in a data-sharing system without relying on centralized trust?To answer this question, we design and build Ghostor, an object store based on decentralized trust that achieves anonymity and verifiable linearizability (abbreviated VerLinear).
At a high level, anonymity 1 means that the protocol does not reveal directly to the server any user identity with any request, as previously defined in the secure storage literature [54,65,74,104].
As shown in Fig. 1, the server does not see which user owns which objects, which users have read or Hides the timing of accesses Figure 2: Information leakage in a data-sharing system and associated privacy properties write permissions to a given object, or even who are the users of the system.
The server essentially sees ghosts accessing the storage, hence the name "Ghostor."
VerLinear means clients can verify that each write is reflected in later reads, except for benign reordering of concurrent operations as formalized by linearizability [42].
To achieve these properties, we build Ghostor's integrity on top of a consistent storage primitive based on decentralized trust, like a blockchain [17,73,105] or verifiable ledger [30,44], while using it only rarely.
Achieving anonymity in practical data-sharing systems like Ghostor is difficult because common system design paradigms, like user login, per-user mailboxes on the server, and client-side caching, let the server track users.
We rearchitect the system to avoid these paradigms ( §4), using data-centric key distribution and encrypted key lists instead of server-side ACLs.
Like prior systems [4,33,57], Ghostor uses cryptographic keys as capabilities, allowing the server and other users to verify each access is performed by an authorized user.
Ghostor also leverages this technique to achieve anonymity by having all users authorized in a particular way share the same capability, and by distributing these capabilities to users without revealing ACLs to the server.
We find this technique, anonymously distributed shared capabilities, interesting because anonymity is not typically a goal of public-key access control [4,33] or capability-based systems [63,72,84].
An additional challenge is to guard against resource abuse while preserving anonymity.
This is typically done by enforcing per-user resource quotas (e.g., Google Drive requires users to pay for additional space), but this is incompatible with Ghostor's anonymity.
One solution is for users to pay for each operation via an anonymous cryptocurrency (e.g., Zcash [105]), but this puts an expensive blockchain operation in the critical path.
To avoid this, Ghostor leverages blind signatures [18,22,23] to allow a user to pay the Ghostor server for service in bulk and in advance, while removing the linkage between payments and operations.
Relationship to obliviousness.
Fig. 2 positions Ghostor's anonymity with respect to other privacy properties.
Global obliviousness [7,66], which hides which object is accessed across all uncompromised objects and users in the system, is orthogonal to Ghostor's anonymity, which hides which user performs each access.
Obliviousness and anonymity are also complementary: (1) In some cases, without obliviousness, users may be identified based on access patterns.
(2) Without anonymity, knowing which user issued a request may reveal information about what data that request may access.
We view Ghostor's techniques for anonymity as a transformation:• If applied to an E2EE system, we obtain Ghostor, an anonymous E2EE system.
• If applied to a globally oblivious scheme, we obtainGhostor-MH, a data-sharing scheme that hides all metadata (except when initializing a group of objects or redeeming payments, as explained in Appendix D).
Hiding metadata from a malicious adversary, as in Ghostor-MH, is a very strong guarantee-existing globally oblivious schemes inherently reveal user identities [66] or assume the adversary is honest-but-curious [7,65].
However, globally oblivious data-sharing schemes, like Ghostor-MH, are theoretical schemes that are far from practical.
Thus, Ghostor-MH is only a proof of concept demonstrating the power of Ghostor's techniques to lift a globally oblivious scheme all the way to virtually zero leakage for a malicious adversary.
To provide VerLinear, prior work has clients sign hashes [55] so the clients can verify that they see the same hash, or store hashes on a separate hash server [49], trusted not to collude with the storage server.
Neither technique can be used in Ghostor: client signatures are at odds with anonymity, and the hash server is a trusted party, which Ghostor aims to avoid.One way to adapt the prior designs to Ghostor's decentralized trust is to store hashes on a blockchain, which can be accomplished by running the hash server in a smart contract.
Unfortunately, this design is too slow to be practical.
The client posts a hash on the blockchain for every object write, which is expensive: blockchains incur high latency per transaction, have low transaction throughput, and require cryptocurrency payment for each transaction [17,73,105].
To sidestep the limitations of a blockchain, we design Ghostor to only interact with the blockchain rarely and outside of the critical path.
Ghostor divides time into intervals called epochs.
At the end of each epoch, the Ghostor server publishes to the blockchain a small checkpoint, which summarizes the operations performed during that epoch for all objects and users in the system.
Each user can then verify that the results of their accesses during the epoch are consistent with the checkpoint.
The consistency properties of a blockchain ensure all clients see the same checkpoint, so the server is committed to a single history of operations and cannot perform a fork attack.
Commit chains [53] and monitoring schemes [15,93] are based on similar checkpoints, but Ghostor applies them to object storage while maintaining users' anonymity.A significant obstacle is that a hash-chain-based history is not amenable to concurrent appends.
Each entry in the history contains the hash of the previous entry, causing one [64], reveal per-user version numbers that would undermine Ghostor's anonymity.
Our insight in Ghostor is to have the server, not the client, populate the hash of the previous entry when appending a new entry.
To make this safe despite a malicious adversary, we carefully design a conflict resolution strategy, involving multiple linked entries in the history for each write, that prevents attackers from manipulating data via replay or time-stretch attacks.
We call the resulting design a verifiable anonymous history.
Our goals and techniques are summarized in Table 1.
Overall, this paper's contributions are:• We design an object store providing anonymity and verifiable linearizability based only on decentralized trust.
• We develop techniques to (1) share capabilities for anonymity and distribute them anonymously, (2) create and checkpoint a verifiable anonymous history, and (3) support concurrent operations on a single object with a hash-chain-based history.
• We combine these with existing building blocks to instantiate Ghostor, an object store with anonymity and VerLinear.
• We also apply these to a globally oblivious scheme to instantiate Ghostor-MH, which hides nearly all metadata.
We also implemented Ghostor and evaluated it on Amazon EC2.
Overall, Ghostor brings a 4-5x throughput overhead on top of a simplistic and completely insecure baseline.
There are two types of latency overhead.
Completing an individual operation takes several seconds.
Afterward, it may take several minutes for a checkpoint to be incorporated into the blockchain, to confirm that no active attack has occurred for a batch of operations.
We explain how these latencies play out in the context of a particular application, EHR Sharing ( §7.1).
Ghostor is an object store, which stores unstructured data items ("objects") and allows shared access to them by multiple users.
We instantiate Ghostor as an object store (as in Amazon S3 or Azure Blobs) because it is a basic primitive on top of which more complex systems can be built.
Fig. 3 Figure 3: System overview of Ghostor.
Shaded areas indicate components introduced by Ghostor.have shared access to objects on the Ghostor server.
Server.
The Ghostor storage server processes requests from clients.
At the end of each epoch, the server generates a single small checkpoint and publishes it to the blockchain.
Client.
The client software consists of a Ghostor library, linked into applications, and a verification daemon, which runs as a separate process.
The Ghostor library receives requests from the application and interacts with the server to satisfy each request.
Upon accessing an object, the library forwards a digest summarizing the operation to the verification daemon.
At the end of each epoch, the daemon (1) fetches object histories from the server, (2) verifies that they are consistent with the server's checkpoint on the blockchain, and (3) checks that the digests collected during the epoch are consistent with the object histories, as explained in §5.
The daemon stores the user's keypair.
If a user loses her secret key, she loses access to all objects that she created or was granted access to.
Similarly, an attacker who steals a user's secret key can impersonate that user.
To securely back up her key on multiple devices, a user can use standard techniques like secret sharing [82,83,99].
A user who accesses Ghostor from multiple devices uses the same key on all devices.Application developers interact with Ghostor using the API below.
Developers can work with usernames, ACLs, and object IDs, but Ghostor clients will not expose them to the Ghostor server.
Below is a high-level description of each API call; a step-by-step technical description is in Appendix A. ♦ create_user(): Creates a Ghostor user by generating keys for a new user.
This operation runs entirely in the Ghostor client-the server does not know this operation was invoked.
♦ user.pay(sum): Users pay the server through an anonymous cryptocurrency such as Zcash [105], and obtain tokens from the server proportional to the amount paid.
These tokens can later be anonymously redeemed and used as proof of payment when invoking the below API functions.
♦ user.create_object(id): Creates an object with ID id, owned by user who invokes this.
The client expends one token obtained from a previous call to pay.
The id can be a meaningful name (e.g., a file path).
It lives only within the client-the server receives some cryptographic identifier-so different clients can assign different ids to the same object.
♦ user.set_acl(id, acl): The user who invokes this must be the owner of the object with ID id.
This function sets a new ACL for that object.
For simplicity, only the owner of an object can set its ACL, but Ghostor can be extended to permit other users as well.
The client encodes acl into an object header that hides user identities, as in §4.
If new users are given access, they are notified via an out-of-band channel.
Existing data-sharing systems also have this requirement; for example, Dropbox and Box send an email with an access URL to the user.
In Ghostor, all keys are transferred in-band; the out-of-band channel is used only to inform the user that she has been given access.
Ghostor does not require a specific out-of-band channel; for example, one could use Tor [29] or secure messaging [95,97].
♦ user.get_object(id), user.put_object(id, content): The user can GET or PUT an object if permitted by its ACL.
Against a malicious attacker who has compromised the server, Ghostor provides:• verifiable linearizability, as described in §3.2, and• a notion of user anonymity, described in §3.3: briefly, it does not reveal user identities, but reveals object access patterns.
Ghostor-MH additionally hides access patterns.
Ghostor does not protect against attacks to availability.
Nevertheless, its anonymity makes it more difficult for the server to selectively deny service to (or fork views of) certain users.
Users, and the Ghostor client instances running on their behalf, can be malicious and can collude with the server.Formal definitions and proofs for these properties require a large amount of space, so we relegate them to Appendix E and Appendix F. Below, we include only informal definitions.
Ghostor is designed to derive its security from decentralized trust.
Thus, our threat model assumes an adversary who can compromise any few machines, as described below.
Blockchain.
Ghostor makes the standard assumption that the blockchain is immutable and consistent (all users see the same transaction history).
This is based on the assumption that, in order to attack a blockchain, the adversary cannot simply compromise a few machines, but rather a significant fraction of the world's computing power.
Ghostor's design is not tied to a specific blockchain.
Our implementation uses Zcash [105] because it supports both public and private transactions; we use Zcash's private transactions for Ghostor's anonymous payments.
The privacy guarantees of Zcash can be implemented on top of other blockchains as well [11].
Network.
We assume clients communicate with the server in a way that does not reveal their network information.
This can be done using mixnets [21] or secure messaging [95,97] based on decentralized trust.
Our implementation uses Tor [29].
If an attack is immediately detectable to a user-for example, if the server fails to honor payment or provides a malformed response (e.g., bad signature)-we consider it an attack on availability, which Ghostor does not prevent.Clients should be able to detect active attacks, including fork and rollback attacks.
Some reordering of concurrent operations, however, is benign.
We use linearizability [42] to define when reordering at the server is considered benign or malicious.
Informally, linearizability requires that after a PUT completes, all later GETs return the value of either (1) that PUT, (2) a PUT that was concurrent with it, or (3) a PUT that comes after it.
We provide a more formal definition in Appendix F. Ghostor provides verifiable linearizability (abbreviated VerLinear).
This means that if the server deviates from linearizability, clients can detect it at the end of the epoch.
We discuss how to choose the epoch length in §9.
Ghostor does not provide consistency guarantees for malicious user, or for objects for which a malicious user has write access.Guarantee 1 (Verifiable linearizability).
For any object F and any list E of consecutive epochs, suppose that, for each epoch in E, the set of honest users who ran the verification procedure includes all writers of F in that epoch (or is nonempty if F was not written).
If the server did not linearizably execute the operations that verifying clients performed in the epochs that they verified, then at least one of the verifying clients will encounter an error in the verification procedure and can generate a proof that the server misbehaved.
As explained in §1.1, Ghostor's anonymity means that the server sees no user identities associated with any action.
In particular, an adversary controlling the server cannot tell which user accesses each object, which users are authorized to access each object, or which users are part of the system.Ghostor.
We informally define Ghostor's privacy via a leakage function: what the server learns when a user makes each API call ( §2).
For create_object -put_object, the server learns the object identifier and the type of the operation.
The server also sees the time of the operation, and the size of the encrypted ACL and encrypted object, which can be hidden via padding at an extra cost.
create_user leaks no information to the server, and pay reveals only the sum paid and when.
The server learns no user identities, no object contents, and no ACLs.
If the attacker has compromised some users, he learns the contents of objects those users can access, including prior versions encrypted under the same key.
Collectively, the verification daemons leak the number of clients performing verification for each object.
If all clients in an object's ACL are honest and running, this equals the ACL size.
If the ACL is padded to a maximum size, the owner should run verification more times to hide the ACL size.
Ghostor does not hide access patterns or timing (Fig. 2).
An adversary who uses (PVK, PSK) Signing keypair used to set ACL (RVK, RSK) Signing keypair used to get object (WVK, WSK) Signing keypair used to put object (OSK)Symmetric key for object contents Table 2: Per-object keys in Ghostor.
The server uses the global signing keypair (SVK, SSK) to sign digests for objects.this information cannot see the contents of files and ACLs because they are encrypted.
But such an adversary could try to deduce correlations between which users issue different operations based on access patterns and timing, and in some cases, identify the user based on that information.
This can be partially mitigated by carefully designing the application using Ghostor ( §4.5).
In contrast, Ghostor-MH does hide access patterns.
In Appendix E, we formally define Ghostor's privacy guarantee in the simulation paradigm of Secure MPC.Ghostor-MH.
We informally define Ghostor-MH's privacy via a leakage function, as above.
create_object reveals that a group of objects was created.
set_acl, get_object, and put_object reveal nothing if the object's ACL contains only honest users; otherwise, they reveal which object was accessed.
create_user and pay have the same leakage as described for Ghostor above.
The leakage function also includes the total number of honest users in the system.
System design paradigms used in typical data-sharing systems are incompatible with anonymity.
We identify the incompatible system design patterns and show how Ghostor replaces them.
Ultimately, we arrive at anonymously distributed shared capabilities, which allow Ghostor to enforce access control for anonymous users without server-visible ACLs.
Data-sharing systems typically have some storage space on the server, called an account file, dedicated to a user's account.
For example, Keybase [52] has a user account and Mylar [75] has a user mailbox where the user receives a key to a new file.
Accesses to the account file, however, can be used to link user operations.
As an example, suppose that when a user accesses an object, her client first retrieves the decryption key from a user-specific mailbox.
This violates anonymity because the server can tell whether or not two accesses were made by the same user, based on whether the same mailbox was accessed first.
Instead, Ghostor's anonymity requires that any sequence of API calls ( §2) with the same inputs, when performed by any honest user, results in the same server-side accesses.Ghostor does not have any user-specific storage as in existing systems.
To allow in-band key exchange, Ghostor associates a header with each object.
The object header functions like an object-specific mailbox, in that it is used to distribute the object's keys among users who have access to the object.
Unlike a user-specific mailbox, it preserves anonymity because, for a given object, each user reads the same header Enc(Object Content) OSK • (RVK, WVK)• Signature Header • Enc(RSK, WSK, OSK) User1 • Enc(RSK, OSK) User2• ......
An honest server must be able to prevent unauthorized users from modifying objects, and users must be able to verify that objects returned by the server were produced by authorized writers.
This is typically accomplished by having writers sign objects, and having the server check that the user who signed the object is on the object's ACL.
However, this requires the ACL to be visible to the server, which violates anonymity.We observe that by switching to a design based on shared capabilities, we can allow the server and other users to verify that writes are indeed made by authorized users, without requiring the server or other users to know the ACL of the object, or which users are authorized.
Every Ghostor object has three associated signing keypairs ( Table 2).
All users of the object (and the server) know the verifying keys PVK, RVK, and WVK because PVK is the name of the object, and RVK and WVK are in the object header; the associated signing keys PSK, RSK, and WSK are capabilities that grant access to set the ACL, get the object, and put the object, respectively.
To distribute these capabilities to users in the object's ACL, the owner places a key list in the object header.
The key list contains, for each user, a list of capabilities encrypted under that user's public key.
If a user has read/write access to an object, her entry in the key list contains WSK, RSK, and OSK; a user with only read access is given a dummy key instead of WSK.
Crucially, different users with the same permission share the same capability, so the server cannot distinguish between users on the basis of which capability they use.
When accessing an object, a user downloads the header and decrypts her entry in the key list to obtain OSK (used to decrypt the object contents) and her capabilities for the object.Users sign updates to the object with WSK, allowing the server and other users to verify that each update is made by a user with write access.
PSK is stored locally by the owner and is used to sign the header.
The owner can set the object's ACL by (1) freshly sampling (RVK, RSK), (WVK, WSK), and OSK, (2) re-encrypting the object with OSK and signing it with WSK, (3) creating a new object header with an updated key list, (4) signing the new header with PSK, and (5) uploading it to the server.
(RVK, RSK) will be relevant in §5.
Ghostor's object layout is summarized in Fig. 4.
Prior systems [64] reveal the user's public key to the server when the client interacts with it.
For example, SUNDR requires users to provide a signature along with each operation.
First, the signature itself could leak the user's public key.
Second, to check the legitimacy of writes, the server needs to know the user's public key to verify the signature.
The server can use the public key as a pseudonym to track users.The key list in §4.2, however, potentially leaks users' public keys: each entry in the key list is a set of capabilities encrypted under a user's public key, but public-key encryption is only guaranteed to hide the message being encrypted, not the public key used to encrypt it.
For example, an RSA ciphertext leaks which public key was used for encryption.
Therefore, Ghostor uses key-private encryption [10], which is guaranteed to hide both the message and the public key.In summary, Ghostor has users share capabilities for anonymity, and then distributes the capabilities anonymously, without revealing ACLs to the server.
We call the resulting technique anonymously distributed shared capabilities.
Assuming that an object's ACL changes rarely, it may seem natural for clients to locally cache an object's keypairs (RVK, RSK) and (WVK, WSK), to avoid downloading the header on future accesses to that object.
Unfortunately, the mere fact that a client did not download the header before performing an operation tells the server that the same user recently accessed that object.
As a result, Ghostor's anonymity prohibits user-specific caching.
That said, server-side caching of commonly accessed objects is allowed.
Ghostor does not hide access patterns or timing information from the server.
A sophisticated adversary could, for example, deny or delay accesses to a particular object and see how access patterns shift, to try and deduce which user made which accesses.
Therefore, one should carefully design the application using Ghostor to avoid leaking user identities in its access patterns.
For example, just as Ghostor has no clientside caching or user-specific mailboxes, an application using Ghostor should avoid caching data locally to avoid requests to the server or using an object as a user-specific mailbox.
Note that Ghostor-MH hides these access patterns.
Ghostor's verifiable anonymous history achieves the "verifiable equivalent" of a blockchain for critical-path operations, while using the underlying blockchain rarely.
It consists of:(1) a hash chain of digests, (2) periodic checkpoints on a real blockchain, and (3) a verification procedure that does not require knowledge of user identities.
We now achieve fork consistency for a single object in Ghostor using techniques inspired from SUNDR [64] Table 3: A digest for an operation in Ghostor because SUNDR is not anonymous.
Each access to an object, whether a GET or a PUT, is summarized by a digest shown in Table 3.
The object's history is stored as a chain of digests.To access the object, a client first produces a digest summarizing that operation as in Table 3.
This requires fetching the object header from the server, so that the client can obtain the secret key (RSK, WSK, or PSK) for the desired operation.
Then the client fetches the latest digest for the object and computes Hash prev in the new digest.
To GET the object, the client copies Hash data from the latest digest; to PUT it, the client hashes the new contents to obtain Hash data .
If the client is changing permissions, then Hash keylist is calculated from the new header; otherwise, it is copied from the latest digest.Then the client signs the digest with the appropriate key and provides the signed digest to the server.
The server signs the digest using SSK, appends it to a log, and returns the signed digest and the result of the operation.
At the end of the epoch, the client downloads the digest chain for that object and epoch, and verifies that (1) it is a valid history for the object, and that (2) it contains the operations performed by that client.
We specify protocol details in Appendix A.Ghostor's digests differ from SUNDR in two main ways.
First, for anonymity, a client does not sign digests using the user's secret key, but instead uses RSK, WSK, or PSK, which can be verified without knowing the user's public key.
When inspecting the digest, the server no longer learns which user performed the operation, only that the user has the required permission.
Second, each digest is signed by the server.
Thus, if the server violates linearizability, the client can assemble the offending digests into a proof of misbehavior.
The construction so far is susceptible to fork attacks [64], in which the server presents two users with different views over the same object.
To detect fork attacks, Ghostor requires the server to produce a checkpoint at the end of each epoch, consisting of the hash of the object's latest digest and the epoch number, and publish the checkpoint to the blockchain.
The verification procedure run by a client consists of fetching the checkpoint from the blockchain, checking it corresponds to the hash for the last digest in the list of digests obtained from the server, and running the verification in §5.1.
The blockchain guarantees that all users see the same checkpoint.
This prevents the server from forking two users' views, as the latest digests for two different views cannot both match the published checkpoint.
In this way, we bootstrap the blockchain's consistency guarantees to achieve verifiable consistency over an entire epoch of operations.
So far, the server puts one checkpoint in the blockchain per object, which is undesirable when there are many objects.
We address this as follows.
The server computes the hash of the final digest of each object, builds a Merkle tree over those hashes, and publishes the root hash in the blockchain as a single checkpoint for all objects.
To verify integrity at the end of an epoch, a Ghostor client fetches the digest chain from the server for objects that are either (1) accessed by the client during the epoch or (2) owned by the client's user.
It verifies that all operations that it performed on those objects are included in the objects' digest chains.
Then, it requests Merkle proofs from the server to check that the hash of the latest digest is included in the Merkle tree at the correct position based on the object's PVK.
Finally, it verifies that the Merkle root hash matches the published checkpoint.Although we maintain a separate digest chain for each object, the collective history of operations, across all objects, is also linearizable.
This follows from the classical result that linearizability is a local property [42].
Thus, Ghostor provides verifiable linearizability across all objects, while supporting full concurrency for operations on different objects.
As explained in §5.1, the client must fetch the latest digest from the server to construct a digest for a new GET or PUT.
If two clients attempt to GET or PUT an object concurrently, they may retrieve the same latest digest for that object, and therefore construct new digests that both have the same Hash prev .
An honest server can only accept one of them; the other operation must be aborted.
A naïve fix is for clients to acquire locks (or leases) on objects during network round trips, but this limits single-object throughput according to client round-trip times.
How can we allow concurrent operations on a single object without holding server-side locks during round trips?
We explain our techniques at a high level below; Appendix A contains a full description of our protocol.
GETs.
We optimize GETs so that clients need not fetch the latest digest, obviating the need to lock for a round trip.
When a client submits a GET request to the server, the client need not include Hash prev , Hash data , or Hash keylist in the digest presented to the server.
The client includes the remaining fields and a signature over only those fields.
Then, the server chooses the hashes for the client and returns the resulting digest, signed by the server.
Although the server can replay operations, this is harmless because GETs do not affect data.
When the verification daemon verifies a GET, it checks the client signature without including Hash prev , Hash data , or Hash keylist .
PUTs.
The above technique does not apply to PUTs, because the server can roll back objects by replaying PUTs.
Simply using a client-provided nonce to detect replayed PUTs is not sufficient, because the server can delay incorporating a PUT (which we call a time-stretch attack) to manipulate the final object contents.
For PUTs, Ghostor uses a two-phase protocol.
In the PREPARE phase, the client operates in the same way as GET, but signs the digest with WSK; the server fills in the hashes, signs the resulting digest, appends it to the object's digest chain, and returns it to the client.
In the COMMIT phase, the client creates the final digest for the operation-omitting Hash prev and appending an additional field Hash prep , which is the hash of the server-signed digest obtained in the PRE-PARE phase-and uploads it to the server with the new object contents.
The server fills in Hash prev based on the object's digest chain (which could have changed since the PREPARE phase), signs the resulting digest, appends it to the object's digest chain, and returns it to the client.
The server can replay PREPARE requests, but it does not affect object contents.
The server cannot generate a COMMIT digest for a replayed PREPARE request, because the client signed the COMMIT digest including the hash of the server-signed PREPARE digest, which includes Hash prev .
The server can replay a COMMIT request for a particular PREPARE request, but this is harmless because of our conflict resolution strategy described below.
Resolving Conflicts.
If two accesses are concurrent (i.e., neither commits before the other prepares), then linearizability does not require any particular ordering of those operations, only that all clients perceive the same ordering.
If a GET is concurrent with a PUT (GET digest between the PREPARE and COMMIT digests for a PUT), Ghostor linearizes the GET as happening before the PUT.
This allows the result of the GET to be served immediately, without waiting for the PUT to finish.
For concurrent PUTs, it is unsafe for the linearization order to depend on the COMMIT digest, because the server could perform a time-stretch or replay attack on a COMMIT digest, to manipulate which PUT wins.
Therefore, Ghostor chooses as the winning PUT the one whose PREPARE digest is latest.
The server can still delay PREPARE digests, but the client can choose not to COMMIT if the delay is unacceptably large.
To simplify the implementation of this conflict resolution procedure, we require that the PREPARE and COMMIT phases happen over the same session with the client, during which the server can keep in-memory state for the relevant object.
This allows the server to match PREPARE and COMMIT digests without additional accesses to secondary storage.
Verification Complexity.
To verify PUTs, the verification daemon must check that Hash data only changes on COMMIT digests for winning writes.
Thus, it must keep track of all PREPARE digests since the latest PREPARE digest whose corresponding COMMIT has been seen.
We can bound this state by requiring that PUT requests do not cross an epoch boundary.
ACL Updates.
We envision that updates to the ACL will be rare, so our implementation does not allow set_acl operations to proceed concurrently with GETs or PUTs.
It may be possible to apply a two-phase technique, similar to our concurrent PUT protocol, to allow set_acl operations to proceed concurrently with other operations.
We leave exploring this to future work.
To prevent resource abuse, commercial data-sharing systems, like Google Drive and Dropbox, enforce per-user resource quotas.
Ghostor cannot do this, because Ghostor's anonymity prevents it from tracking users.
Instead, Ghostor uses two techniques to prevent resource abuse without tracking users: anonymous payments and proof of work.
A strawman approach is for users to use an anonymous cryptocurrency (e.g., Zcash [105]) to pay for each expensive operation (e.g., operations that consume storage).
Unfortunately, this requires a separate blockchain transaction for each operation, limiting the system's overall throughput.Instead, Ghostor lets users pay for expensive operations in bulk via the pay API call ( §2).
The server responds with a set of tokens proportional to the amount paid via Zcash, which can later be redeemed without using the blockchain to perform operations.
Done naïvely, this violates Ghostor's anonymity; the server can track users by their tokens (tokens issued for a single pay call belong to the same user).
To circumvent this issue, Ghostor uses blind signatures [18,22,23].
A Ghostor client generates a random token and blinds it.
After verifying that the client has made a cryptocurrency payment, the server signs the blinded token.
The blind signature protocol allows the client to unblind it while preserving the signature.
To redeem the token, the client gives the unblinded signed token to the server, who can verify the server's signature to be sure it is valid.
The server cannot link tokens at the time of use to tokens at the time of issue because the tokens were blinded when the server originally signed them.
Another way to mitigate resource abuse is proof of work (PoW) [6].
Before each request from the client, the server sends a random challenge to the client, and the client must find a proof such that Hash(challenge, proof, request) < diff.
diff controls the difficulty, which is chosen to offset the amplification factor in the server's work.
Because of the guarantees of the hash function, the client must iterate through different proofs until it finds one that works.
In contrast, the server efficiently checks the proof by computing one hash.
Ghostor uses anonymous payments and PoW together to mitigate resource abuse.
Our implementation requires anonymous payment only for create_object, which requires the server to commit additional storage space for the new object.
This is analogous to systems like Google Drive or Dropbox, which require payment to increase a user's storage limit but do not charge based on the count or frequency of object accesses.
Implicit in this model are hard limits on object size and perobject access frequency, which Ghostor can enforce.
Although our implementation requires payment only for create_object, an alternate implementation may choose to require payment for every operation except pay.
Ghostor requires PoW for all API calls.
This includes pay and create_object, to offset the cost of Zcash payments and verifying blind signatures.
In this section, we discuss two applications of Ghostor that we implemented: EHR Sharing and Ghostor-MH.
Our goal in this section is to show how a real application may interface with Ghostor's semantics (e.g., ownership, key management, error handling) and how Ghostor's security guarantees might benefit a real application.
To make the discussion concrete, we explore a particular use case: multi-institutional sharing of electronic health records (EHRs).
It has been of increasing interest to put patients in control of their data as they move between different healthcare providers [37,43,85].
As it is paramount to protect medical data in the face of attackers [28], various proposals for multi-institutional EHR sharing use a blockchain for access control and integrity [5,70].
Below, we explore how to design such a system using Ghostor to store EHRs in a central object store, using only decentralized trust.
We also implemented the system for Open mHealth [3].
Each patient owns one or more objects in the central Ghostor system representing their EHRs.
Each patient's Ghostor client (on her laptop or phone) is reponsible for storing the PSKs for these objects.
The PSKs could be stored in a wristband, as in [70], in case of emergency situations for at-risk patients.
When the patient seeks treatment from a healthcare provider, she can grant the healthcare provider access to the objects containing the relevant information in Ghostor.
Each healthcare provider's Ghostor client maintains a local metadata database, mapping patient identities (object IDs, §2) to PVKs.
This mapping could be created when a patient checks in to the office for the first time (e.g., by sharing a QR code).
Benefits.
Existing proposals leverage a blockchain to achieve integrity guarantees [5,70] but use the blockchain more heavily than Ghostor: for example, they require a blockchain transaction to grant access to a healthcare provider, which results in poor performance and scalability.
Additionally, Ghostor provides anonymity for sharing records.
Epoch Time.
An important aspect of Ghostor's semantics is that one has to wait until the next epoch before one can verify that no fork has occurred.
It is reasonable to fetch a patient's record at the time that they check in to a healthcare facility, but before they are called in for treatment.
This allows the time to wait until the end of an epoch to overlap with the patient's waiting time.
In the case of scheduled appointments, the record can be fetched in advance so that integrity can be verified by the time of the appointment.
An epoch time of 15-30 minutes would probably be sufficient.
Error Handling.
If a healthcare provider detects a fork when verifying an epoch, it informs other healthcare providers of the integrity violation out-of-band of the Ghostor system.
Ghostor does not constrain what happens next.
One approach, used in Certificate Transparency (CT), is to abandon the Ghostor server for which the integrity violation was detected.
We envision that there would be a few Ghostor servers in the system, similar to logs in CT, so this would require affected users to migrate their data to a new server.
Another approach is to handle the error in the same way that blockchain-based systems [5,70] handle cases where the hash on the blockchain does not match the hash of the data-treat it as an availability error.
While neither solution is ideal, it is better than the status quo, in which a malicious adversary is free to perform fork or rollback attacks undetected, causing patients to receive incorrect treatments based on old or incorrect data, potentially resulting in serious physical injury.
Ghostor's anonymity techniques can be combined with a globally oblivious scheme, AnonRAM [7], to obtain a metadatahiding object-sharing scheme, Ghostor-MH.
Ghostor-MH is not a practical system, but only a theoretical scheme; our goal is to show that Ghostor's techniques are complementary to and compatible with those in globally oblivious schemes.
Below we summarize how we apply Ghostor's techniques in Ghostor-MH; we discuss Ghostor-MH in more detail in Appendix D. First, we apply Ghostor's principle of switching from a user-centric to a data-centric design.
Whereas each ORAM instance in AnonRAM corresponds to a user, each ORAM instance in Ghostor-MH corresponds to an object group, a fixed-sized set of objects with a shared ACL.
Second, we apply the design of Ghostor's object header in Ghostor-MH.
This is accomplished by storing the ORAM secret state, encrypted, on the server.
Finally, we use similar techniques to mitigate resource abuse in Ghostor-MH as we do in Ghostor.
We implemented a prototype of Ghostor in Go.
It consists of three parts, as in Fig. 3, server (≈ 2100 LOC), client library (≈ 1000 LOC), and verification daemon (≈ 1000 LOC), which all depend on a set of core Ghostor libraries (≈ 1400 LOC).
Our implementation uses Ceph RADOS [101] for consistent, distributed object storage.
We use SHA-256 for the cryptographic hash and the NaCl secretbox library (which uses XSalsa20 and Poly1305) for authenticated symmetric-key encryption.
For key-private asymmetric encryption (to encrypt signing keys in the object header), we implemented the El Gamal cryptosystem, which is key-private [10], on top of the Curve25519 elliptic curve.
We use an existing blind signature implementation [1] based on RSA with 2048-bit keys and 1536-bit hashes.
We use Ed25519 for digital signatures.As discussed in §3, Ghostor uses external systems for anonymous communication and payment.
In our implementation, clients use Tor [29] to communicate with the server and Zcash 1.0.15 for anonymous payments.
We build a Zcash test network, separate from the Zcash main network.
Ghostor, A 50% R, 50% W B 95% R, 5% W C 100% R D 95% R, 5% Insert E 95% R, 5% Range F 50% R, 50% R-Modify-W Figure 6: YCSB workloads (R: read, W: write) 10000 however, could also be deployed on the Zcash main chain.
Zcash is also used as the blockchain to post checkpoints.
Our implementation runs as a single Ghostor server that stores its data in a scalable, fault-tolerant, distributed storage cluster.
We discuss how to scale to multiple servers in Appendix B.We implemented a proof of concept of our theoretical scheme Ghostor-MH ( §7.2), in ≈ 2100 additional LOC.
As it is a theoretical scheme, our focus in evaluating Ghostor-MH is simply to understand the latency of operations.
Ghostor-MH includes AnonRAM's functionality, which, to our knowledge, has not been previously implemented.
We omit zeroknowledge proofs in our implementation, as they are similar to AnonRAM and are not Ghostor-MH's innovation.
We run our experiments on Amazon EC2.
Ghostor's storage cluster consists of three i3en.xlarge servers.
We configure Ceph to replicate each object (key-value pair) on two SSDs on different machines, for fault-tolerance.
Basic Crypto Primitives.
We measured the latency of crypto operations used in Ghostor's critical path.
En/decryption of object contents varies linearly with the object size, and takes ≈ 2 ms for 1 MiB.
Key-private en/decryption for object headers and signing/verification of digests takes less than 150 us.
Blind Signatures.
We also measure the blind signature scheme used for object creation, which consists of four steps.
Fig. 7, we measure the overhead of verification for digests in a single epoch.
For client verification time, we perform an end-to-end test, measuring the total time to fetch digests and to verify them.
The client has 1,000 signed digests for operations the client performed during the epoch that the client needs to check were included in the history of digests.
We vary the total number of digests in the object's history for that epoch.
The reported values in Fig.
7a are the total time to verify the object, divided by the total number of operations on the object, indicating the verification time per digest.
The trend indicates a constant overhead when the total number of operations on the object is small, that is amortized when the number of operations is large.
Fig. 7b shows the server's overhead to compute the Merkle root.
We inserted objects using YCSB ( §9.2.2) during an epoch, and measured the time to compute the Merkle root at the end of that epoch.
For 10,000 objects, this takes about 2.5 seconds; for 1,000,000 objects, it takes about 280 seconds.
Reading the latest digest for each object (leaves of the Merkle tree) dominates the time to compute the Merkle root (2 seconds for 10,000 objects, 272 seconds for 1,000,000 objects).
The reason is that our on-disk data structures are optimized for single-object operations, which are in the critical path.
In particular, each object's digest chain is stored as a separate batched linked list, so reading the latest digests requires a separate read for each object.
This section measures to what extent anonymity and VerLinear affect Ghostor's performance.
To ensure that the bottleneck was on the server, we set proof of work to minimum difficulty and do not use anonymous communication ( §3), but we return to evaluating these in §9.3.
We measure the end-to-end performance of operations in Ghostor, both as a whole and for instantiations of Ghostor having only anonymity or VerLinear.
We compare these to an insecure baseline as well as to competitive solutions for privacy and verifiable consistency, as we now describe.
1.
Insecure system ("Insec").
This system uses the traditional ACL-based approach for serving objects.
Each object access is preceded by a read to the object's ACL to verify that the user has permission to access the object.
Similarly, creating an object requires a read to a per-user account file.
It provides no security against a compromised server.
2.
End-to-End Encrypted system ("E2EE").
This system encrypts objects placed on the server using end-to-end encryption similarly to SiRiUS [35].
Such systems have an encrypted KeyList similar to Ghostor's, but clients can cache their keys locally on most accesses unlike Ghostor.
3.
Ghostor's anonymity system ("Anon").
This is Ghostor with VerLinear disabled.
This fits a scenario where one wants to hide information from a passive server attacker.
Unlike the E2EE system above, this system cannot cache keys locallyevery operation incurs an additional round trip to fetch the KeyList from the server.
In addition, every operation incurs yet another round trip at the beginning for the client to perform a proof of work.
On the positive side, the server does not maintain any per-user ACL.4.
Fork Consistent system ("ForkC").
This system maintains Ghostor's digest chain ( §5.1), but does not post checkpoints.
Each operation appends to a per-object log of digests, using the techniques in §5.4.
This system also performs an ACL check when creating an object.
5.
Ghostor's VerLinear system ("VLinear").
This system corresponds to the VerLinear mechanism in §5 (including §5.2).
This matches a use case where one wants integrity, but does not care about privacy.
We do not include the verification procedure, already evaluated in §9.1.
6.
Ghostor.
This system achieves both anonymity and VerLinear, and therefore incurs the costs of both guarantees.
In each setup, we measured the latency for create, GET, and PUT operations (Fig. 8a), throughput for GETs/PUTs to a single object (Fig. 9a), and the throughput for creating objects and for GETs/PUTs to multiple objects (Fig. 9b).
Fork consistency adds substantial overhead, because additional accesses to persistent storage are required for each operation, to maintain each object's log of digests.
Ghostor, which both maintains a per-object log of digests and provides anonymity, incurs additional overhead because clients do not cache keys, requiring the server to fetch the header for each operation.
In contrast, for Anon, the additional cost of reading the header is offset by the lack of ACL check.
For 1 MiB objects, en/decryption adds a visible overhead to latency.End-to-end encryption adds little overhead to throughput; this is because we are measuring throughput at the server, whereas encryption and decryption are performed by clients.
The only factor affecting server performance is that the ciphertexts are 40 bytes larger than plaintexts.Single-object throughput is lower for ForkC, VLinear, and Ghostor, because maintaining a digest chain requires requests to be serialized across multiple accesses to persistent storage.
In contrast, Insec, E2EE, and Anon serve requests in parallel, relying on Ceph's internal concurrency control.In the multi-object experiments, in which no two concurrent requests operate on the same object, this bottleneck disappears.
For small objects, throughput drops in approximately an inverse pattern to the latency, as expected.
For large objects, however, all systems perform commensurately.
This is likely because reading/writing the object itself dominated the throughput usage for these experiments, without any concurrency overhead at the object level to differentiate the setups.
In this section, we evaluate our system using the Yahoo! Cloud Serving Benchmark (YCSB).
YCSB provides different workloads representative of various use cases, summarized in Table 6.
We do not use Workload E because it involves range queries, which Ghostor does not support.
As shown in Fig.
9c, anonymity incurs up to a 25% overhead for benchmarks containing insertions, owing to the additional accesses to storage required to store used object creation tokens.
However, it shows essentially no overhead for GETs and PUTs.
Fork VerLinear adds essentially no overhead on top of fork consistency; this is to be expected, because the overhead of VerLinear is outside of the critical path (except for insertions, where the overhead is easily amortized).
Ghostor, which provides both anonymity and VerLinear, must forgo client-side caching, and therefore incurs additional overhead, with a 4-5x throughput reduction overall compared to the Insec baseline.
We now analyze the performance of Ghostor from the client's perspective, including the cost of proof of work and anonymous communication ( §3).
The latency experienced by a Ghostor client is the latency measured in Fig. 8, plus the additional overhead due to the proof of work mechanism and anonymous communication.The difficulty of the proof of work problem is adjustable.
For the purpose of evaluation, we set it to a realistic value to prevent denial of service.
Fig. 8b indicates that it takes ≈ 32 ms for a Ghostor operation; therefore, we set the proof of work difficulty such that it takes the client, on average, 100 times longer to solve (≈ 3.2 s).
Fig. 10 shows the distribution of latency for the client to solve the proof of work problem.
As expected, the distribution appears to be memoryless.
In our implementation, a client connects to a Ghostor server by establishing a circuit through the Tor [29] network.
The performance of the connection, in terms of both latency and throughput, varies according to the circuit used.
Fig. 10 shows the distribution of (1) circuit establishment time, (2) roundtrip time, and (3) network bandwidth.
We used a fresh Tor circuit for each measurement.
Based on our measurements, a Tor circuit usually provides a round-trip time less than 1 second and bandwidth of at least 2 Mb/s.
We now measure the end-to-end latency of each operation in Ghostor's client API ( §2), including all overheads experienced by the client.
As explained in §9.3.1, the overhead due to proof of work and Tor is quite variable; therefore, we repeat each experiment 1000 times, using a separate Tor circuit each time, and report the distribution of latencies for each operation in Fig. 12.
Comparing Fig. 12 to Fig. 8, the client-side latency is dominated by the cost of PoW and Tor; Ghostor's core techniques in Fig. 8 have relatively small latency overhead.
For the pay operation, we measure only the time to redeem a Zcash payment for a single token, not the time for proof of work or making the Zcash payment (see §9.4 for a discussion of this overhead).
GET and PUT for large objects are the slowest, because Tor network bandwidth becomes a bottleneck.
The create_user operation (not shown in Fig. 12) is only 132 microseconds, because it generates an El Gamal keypair locally without any interaction with the server.
In our implementation, we build our own Zcash test network to avoid the expense from Zcash's main network.
Since our system leverages Zcash in a minimal way, the overhead of Zcash is not on the critical path of our protocol.
According to the Zcash website [105] and block explorer [2], the block size limit is about 2 MiB, and block interval is about 2.5 minutes.
In the past six months, the maximum block size has been less than 150 KiB and the average transaction fee has been much less than 0.001 ZEC (0.05 USD at the time of writing).
Hence, even with shorter epochs (less time for misbehavior detection), the price of Ghostor's checkpoints is modest since there is a single checkpoint per epoch for the whole system.
For completeness, we evaluate the theoretical Ghostor-MH scheme presented in §7.2, focusing only on the latency of accessing an object.
We do not use Tor and we set the PoW difficulty to minimum.
Latency is dominated by en/decryption on the client, because object contents and ORAM state are encrypted with El Gamal encryption, which is much slower than symmetric-key encryption.
Fig. 11a shows the object access latency for an object group, as we vary its size.
It scales logarithmically, as expected from Path ORAM.
An additional overhead of ≈ 2 s comes from re-encrypting ORAM client state (32 KiB, after padding and encryption) on each access.
Fig. 11b shows the object access latency as we vary the number of object groups (each object group is 31 KiB).
It scales linearly, because the client makes fake accesses to all other object groups to hide which one it truly accessed.
Latency could be improved by using multiple client CPU cores.
Systems Providing Consistency.
We have already compared extensively with SUNDR [64].
Venus [87] achieves eventual consistency; however, Venus requires some clients to be frequently online and is vulnerable to malicious clients.
Caelus [55] has a similar requirement and does not resist collusion of malicious clients and the server.
Verena [49] trusts one of two servers.
SPORC [31], which combines fork consistency with operational transformation, allows clients to recover from a fork attack, but does not resist faulty clients.
Depot [67] Figure 12: End-to-end latencies of client-side operations notion of consistency than VerLinear.
Furthermore, its consistency techniques are at odds with anonymity.
Ghostor and these systems use hash chains [39,68] as a key building block.
Systems Providing E2EE.
Many systems provide end-toend encryption (E2EE), but leak significant user information as discussed in §3.3: academic systems such as Persona [8] [98].
Systems Using Trusted Hardware.
Some systems, such as Haven [9] and A-SKY [25], protect against a malicious server by using trusted hardware.
Existing trusted hardware, like Intel SGX, however, suffer from side-channel attacks [96].
Oblivious Systems.
A complementary line of work to Ghostor aims to hide access patterns: which object was accessed.
Standard Oblivious RAM (ORAM) [36,86,100] works in the single-client setting.
Multi-client ORAM [7,40,50,65,66,80,90] extends ORAM to support multiple clients.
These works either rely on central trust [80,90] (either a fully trusted proxy or fully trusted clients) or provide limited functionality (not providing global object sharing [7], or revealing user identities [66]).
GORAM [65] assumes the adversary controlling the server does not collude with clients.
Furthermore, it only provides obliviousness within a single data owner's objects, not global obliviousness across all data owners.AnonRAM [7] and PANDA [40] provide global obliviousness and hide user identity, but are slow.
They do not provide for sharing objects or mitigate resource abuse.
One can realize these features by applying Ghostor's techniques to these schemes, as we did in §7.2 to build Ghostor-MH.
Unlike these schemes, Ghostor-MH is a metadata-hiding object-sharing scheme providing both global obliviousness and anonymity without trusted parties or non-collusion assumptions.
Decentralized Storage.
Peer-to-peer storage systems, like OceanStore [56], Pastry [79], CAN [78], and IPFS [12], allow users to store objects on globally distributed, untrusted storage without any coordinating central trusted party.
These systems are vulnerable to rollback/fork attacks on mutable data by malicious storage nodes (unlike Ghostor's VerLinear).
While some of them encrypt objects for privacy, they do not provide a mechanism to distribute secret keys while preserving anonymity, as Ghostor does.
Recent blockchain-based decentralized storage systems, like Storj [92], Swarm [94], Filecoin [32], and Sia [88], have similar shortcomings.
Decentralized Trust.
As discussed in §1, blockchain systems [17,20,73,103] and verifiable ledgers [61,71] can serve as the source of decentralized trust in Ghostor.Another line of work aims to provide efficient auditing mechanisms.
EthIKS [15] leverages smart contracts [17] to monitor key transparency systems [71].
Catena [93] builds log systems based on Bitcoin transactions, which enables efficient auditing by low-power clients.
It may be possible to apply techniques from those works to optimize our verification procedure in §5.2.
However, none of them aim to build secure data-sharing systems like Ghostor.
Secure Messaging.
Secure messaging systems [26,95,97] hide network traffic patterns, but they do not support object storage/sharing as in our setting.
Ghostor can complementarily use them for its anonymous communication network.
Ghostor is a data-sharing system that provides anonymity and verifiable linearizability in a strong threat model that assumes only decentralized trust.
[16] F. Buccafurri, G. Lax, S. Nicolazzo, and A. Nocera.
Accountability-preserving anonymous delivery of cloud services.
In TrustBus, 2015.
[17] V. Buterin et al.
Ethereum white paper.
GitHub repository, 2013.
[18] J. L. Camenisch, J. Piveteau, and M. A. Stadler.
Blind signatures based on the discrete logarithm problem.
In EUROCRYPT, 1994.
[19] R. Canetti.
Universally composable security: A new paradigm for cryptographic protocols.
In FOCS, 2001.
[20] M. Castro and B. Liskov.
Practical Byzantine fault tolerance.
In OSDI, 1999.
[21] D. Chaum.
Untraceable electronic mail, return addresses, and digital pseudonyms.
CACM, 1981.
[22] D. Chaum.
Blind signatures for untraceable payments.In EUROCRYPT, 1983.
[23] D. Chaum.
Blind signature system.
In EUROCRYPT, 1984.
[24] S. Chen, R. Wang, X. Wang, and K. Zhang.
Sidechannel leaks in web applications: A reality today, a challenge tomorrow.
In S&P, 2010.
[25] S. Contiu, S. Vaucher, R. Pires, M. Pasin, P. Felber, and L. Réveillère.
Anonymous and confidential file sharing over untrusted clouds.
SRDS, 2019.
[ Below, we describe the client-server protocol used by Ghostor.
1.
Server sends a PoW challenge to the client ( §6).
2.
Client sends the server the PoW solution, PVK of the object that the user wishes to access, and the server returns the object header and current epoch.
3.
The client assembles a digest for the GET operation, including the epoch number, PVK, RVK, WVK, and a random nonce, and signs it with RSK (obtained from the header).
It sends the signed digest to the server.
4.
Server reads the latest digest and checks that the client's candidate digest is consistent with it.
If not (for example, if the header was changed in-between round trips), the server gives the client the object header, and the protocol returns to Step 3.
5.
Server adds Hash prev , Hash header , and Hash data to the digest (according to the order in which it commits operations on the object).
Then it signs it and adds it to the log of digests for that object.
6.
Server returns the object contents and the digest, including the server's signature, to the client.
7.
Client checks that the signed digest matches the object contents and digest that the client provided.
If so, it returns the object contents to the user and sends the signed digest to the verification daemon.
1.
Server sends a PoW challenge to the client ( §6).
2.
Client sends the server the PoW solution and PVK of the object to PUT, and the server returns the object header, current epoch, and latest server-signed digest for that object.
3.
The client assembles a PREPARE digest for the write operation, including the epoch number, PVK, RVK, WVK, and signs it with WSK (obtained from the header).
It sends the signed digest to the server.
4.
Server reads the latest digest and checks that the client's candidate digest is consistent with it.
If not, then the server gives the client the object header, and the protocol returns to Step 3.
5.
Server adds Hash prev , Hash header , and Hash data to the digest (according to the order in which it commits operations on the object).
Then it signs it and adds it to the log of digests for that object.
6.
Server returns the signed digest to the client.
7.
Client assembles a COMMIT digest for the write operation, including the same fields as the PREPARE digest, and also Hash prep and Hash data according to the new data.
Then it signs it and uploads it to the server, including the new object contents.
8.
Server decides if this PUT "wins."
It wins as long as no other PUT whose PREPARE digest is after this PUT's PRE-PARE digest has already committed.
If this PUT wins, then the server performs the write, signs the digest, and adds it to the log of digests for that object.
If not, it still signs the digest and adds it to the log, but it replaces Hash data with the current hash of the data, including the value provided by the client as an "addendum" so that the verification daemon can still verify the client's signature.
The server may also reject the COMMIT digest if the key list changed meanwhile due to a set_acl operation.
9.
Server returns the digest, including the server's signature, to the client.
10.
Client checks that the signed digest matches the object contents and digest that the client provided.
If so, it sends the signed digest to the verification daemon.
1.
Server sends a PoW challenge to the client ( §6).2.
Client sends the server the PoW solution and PVK of the object to write, and the server returns the object header, current epoch, and latest server-signed digest for that object.
3.
The client assembles a digest for the write operation, including all fields, and signs it with PSK.
It sends the signed digest to the server.
Client also signs PVK with PSK and includes that signature in the request.
Client also includes the new header.
4.
Server acquires a lock (lease) on the object for this client (unless it is already held for this client), reads the latest digest, and checks that the client's candidate digest is consistent with it.
If not, then the server gives the client the object header, and the protocol returns to Step 3.
When returning to Step 3, the server checks if the client's signature over PVK is correct.
If so, the server holds the lock on the object during the round trip.
If not, the server releases it.
5.
Server updates the header, signs the digest, adds it to the log of digests for that object, and releases the lock.
6.
Server returns the digest, including the server's signature, to the client.
7.
Client checks that the signed digest matches the object contents and digest that the client provided.
If so, it returns the object contents to the user and sends the signed digest to the verification daemon.
The owner of the object generates new keys and encrypts the object under the new key.
If a user is being granted access, the owner may still generate new keys to prevent the server from learning whether or not a user was revoked.
The owner shuffles the key list upon any change to it.
The owner may also add padding to hide the number of users in the key list.
1.
Server sends a PoW challenge to the client ( §6) 2.
Client sends the server the PoW, PVK of the object that the user wishes to create, a token signed by the server for proof of payment ( §2), the header for the new object, and the object's first digest (for which Hash prev is empty).
This involves generating all the keys in Fig. 4) for the new object.3.
Server verifies the signature on the token, and checks that it has not been used before.
4.
Server "remembers" the hash of the token by storing it in permanent storage.
5.
Server writes the object header.
It signs the digest and creates a log for this object containing only that digest.
6.
Server returns the digest, including the server's signature, to the client.
7.
Client checks that the signed digest matches the object contents and digest that the client provided.
If so, it returns the object contents to the user and sends the signed digest to the verification daemon.
At the end of each epoch, the verification daemon downloads the digest chain and checkpoints to verify operations performed in the epoch.1.
Server sends a PoW challenge to the daemon ( §6 For example, if this operation is read, Sig client must be signed using RSK.
First, the user pays the server using an anonymous cryptocurrency such as Zcash [105], and obtains a proof of payment from Zcash.
Then, the client obtains tokens from the server, as follows:1.
Server sends a PoW challenge to the client ( §6).2.
Client sends the server the PoW, proof of payment, and t blinded tokens, where t corresponds to the amount paid.
3.
Server checks that the proof of payment is valid and has not been used before.
4.
Server "remembers" the proof of payment by storing it in persistent storage.
5.
Server signs the blinded tokens, ensuring that t indeed corresponds to the amount paid, and sends the signed blinded tokens to the client.
6.
Client unblinds the signed tokens and saves them for later use.
Our implementation of Ghostor that we evaluated in §9 consists of a single Ghostor server, which stores data in a storage cluster that is internally replicated and fault-tolerant (Ceph RADOS).
In this appendix, we discuss techniques to scale this setup by replicating the Ghostor server as well.Given that we consider a malicious adversary, it may seem natural to use PBFT [20].
PBFT, however, is neither necessary nor sufficient in Ghostor's setting.
It is not necessary because we already post checkpoints to a ledger based on decentralized trust ( §5.2) to achieve verifiable integrity.
It is not sufficient because we assume an adversary who can compromise any few machines across which we replicate Ghostor, which is incompatible with Byzantine Fault Tolerance.The primary challenge to replicating the Ghostor server is synchronization: if multiple operations on the same object may be handled by different servers, the servers may concurrently mutate the on-disk data structure for that object.
A simple solution is to use object-level locks provided by Ceph RADOS.
This is probably sufficient for most uses.
But, if server-side caching of objects in memory is implemented, caches in the Ghostor servers would have to be kept coherent.Alternatively, one could partition the object space among the servers, so each object has a single server responsible for processing operations on it.
A set of load balancer servers run Paxos [58,59] to arrive at a consensus on which servers are up and running, so that requests meant for one server can be re-routed to another if it goes down.
Note that Paxos is outside of the critical path; it only reacts to failures, not to individual operations.
Based on the consensus, the load balancers determine which server is responsible for each object.
Because all objects are stored in the same storage pool, the objects themselves do not need to be moved when Ghostor servers are added or removed, only when storage servers are added or removed (which is handled by Ceph).
Object-level locks in Ceph RADOS would still be useful to enforce that at most one server is operating on a Ghostor object at a time.
Our design of Ghostor can be extended to support a hierarchy of directories and files.
Each directory or file corresponds to a PVK and associated Ghostor object; the PVK has a similar role to an inode number in a traditional file system.
The Ghostor object corresponding to a directory contains a mapping from name to PVK as a list of directory entries.
Given the PVK of a root directory and a filepath, a client iteratively finds the PVK of each directory from left to right; in the end, it will have the PVK of the file, allowing it to access the Ghostor object corresponding to a file.
The procedure is analogous to resolving a filepath to an inode number in a traditional file system.
The Ghostor object corresponding to a file may either contain the file contents directly, or it may contain the PVKs of other objects containing the file data, like an inode in a traditional file system.The "no user-side caching" principle §4 applies here, in the sense that clients may not cache the PVK of a file after resolving it once.
A client must re-resolve a file's PVK on each access; caching the PVK and accessing the file without first accessing all parent directories would reveal that the same user has accessed the file before.D Additional Description of Ghostor-MH §7.2 explains Ghostor-MH at a high level.
§8 and §9 describe our implementation and evaluation of Ghostor-MH.
Appendix D.2 below provides a more in-depth explanation of Ghostor-MH.
We first provide more details about AnonRAM in Appendix D.1.
This is necessary because, as explained in §7.2, we construct Ghostor-MH by applying Ghostor's techniques to AnonRAM [7].
ORAM [36] is a technique to access objects on a remote server without revealing which objects are accessed.
Many ORAM schemes, such as Path ORAM [91], allow a single user to access data.
Path ORAM [91] works by having the client shuffle a small amount of server-side data with each access, such that the server cannot link requests to the same object.
Clients store mutable secret state, including a stash and position map, used to find objects after shuffling.AnonRAM extends single-user ORAM to support multiple users.
Each AnonRAM user essentially has her own ORAM on the server.
When a user accesses an object, she (1) performs the access as normal in her own ORAM, and (2) performs a fake access to all of the other users' ORAMs.
To the server, the fake accesses are indistinguishable from genuine accesses, so the server does not learn to which ORAM the user's object belongs.
This, together with each individual ORAM hiding which of its objects was accessed, results in global obliviousness across all objects in all ORAMs.To support fake accesses, re-randomizable public-key encryption (e.g., El Gamal) is used to encrypt objects in each ORAM.
To guard against malicious clients, the server requires a zero-knowledge proof with each real or fake access, to prove that either (1) the client knows the secret key for the ORAM, or (2) the new ciphertexts encrypt the same data as existing ciphertexts (i.e., they were re-randomized correctly).
A limitation of AnonRAM is that there is no object sharing among users; each user can access only the objects she owns.
Furthermore, AnonRAM and similar schemes ( §10) are theoretical-they consider oblivious storage from a cryptographic standpoint, but do not consider challenges like payment, user accounts, and resource abuse.
Recall from §7.2 that we apply to AnonRAM Ghostor's principle of switching from a user-centric to a data-centric design.
Each ORAM now corresponds to an object group, which is a fixed-size set of objects with a shared ACL.
Each object group has one object header and one digest chain.Ghostor-MH uses Path ORAM, which organizes serverside storage as a binary tree.
To guard against a malicious adversary controlling the server, we build a Merkle tree over the binary tree, and compute Hash data in each digest as the hash of the Merkle root and ORAM secret state.
This allows each client to efficiently compute the new Hash data after each ORAM access, without downloading the entire ORAM tree.
The ORAM secret state is stored on the server, encrypted with OSK, so multiple clients can access an object group.
This is analogous to Ghostor's object header, which stores an object's keys encrypted on the server.To access an object, a client (1) identifies the object group containing it, (2) downloads the object header and encrypted ORAM secret state, (3) obtains OSK from the object header, (4) decrypts the ORAM secret state, (5) uses it to perform the ORAM access, (6) encrypts and uploads the new ORAM secret state, (7) computes a new digest for the operation, (8) has the server sign it, and (9) sends it to the verification daemon.
For all other object groups, the client performs a fake access that fetches data from the server and generates a digest, but only re-randomizes ciphertexts instead of performing a real access.
This hides which object group contains the object.
When writing an object, the client pads it to a maximum size (the ORAM block size) to hide the length of the object.Below, we explain some more details about Ghostor-MH: Fake accesses.
OSK is replaced with an El Gamal keypair.
This allows ciphertexts in the ORAM tree and the ORAM secret state to be re-randomized.
We no longer attach a client signature to each digest, but instead modify the zero-knowledge proof in AnonRAM to prove that either the client can produce a signature over the digest with WSK, or the ciphertexts were properly re-randomized.
Hiding timing.
Similar to secure messaging systems [97], Ghostor-MH operates in rounds (shorter than epochs) to hide timing.
In each round, each client either accesses an object as described above, or performs a fake access on all ORAMs if there is no pending object access.
Each client chooses a random time during the round to make its request to the server.
Using tokens.
In a globally oblivious system like Ghostor-MH, it is impossible to enforce the per-object quotas discussed in §6.3.
Thus, it is advisable to require users to expend tokens for all operations (except pay), not just create_object.
Our PoW mechanism applies to Ghostor-MH unchanged.
Object group creation.
The server can distinguish payment (to obtain tokens) and object group creation from GET/PUT operations.
The most secure solution is to have a setup phase to create all object groups and perform all payment in advance.
Barring this, we propose adding a special round at the start of each epoch, used only for creation and payment; all object accesses during an epoch happen after this special round.
List of object groups.
To make fake accesses, each client must know the full list of object groups.
To ensure this, we can add an additional digest chain to keep track of all created object groups, checkpointed every epoch with the rest of the system.
Changing permissions.
In our solution so far, the server can distinguish a set_acl operation from object accesses.
To fix this, we require the owner of each object group to perform exactly one set_acl for that object group during each epoch; if he does not wish to change it, he sets it to the same value.
Concurrency.
When a client iterates over all ORAMs to make accesses (fake or real), the client locks each ORAM individually and releases it after the access.
No "global lock" is held while a client makes fake accesses to all ORAMs.
In this appendix, we use the simulation paradigm of Secure Multi-Party Computation (SMPC) [19] to define Ghostor's privacy guarantee.
We begin in Appendix E.1 by providing an overview of our definition and proof sketch, along with an explanation of how our simulation-based definition matches the one in §3.3.
We formally define Ghostor's anonymity by specifying an ideal world.
We provided a definition in §3.3, but we consider it to be informal because it does not clearly state what the adversary learns if some users are compromised/malicious.
The ideal world is specified such that it is easy to reason about what information the adversary learns; what the adversary learns in the ideal world is our definition of what an anonymous object sharing system leaks to an adversary (i.e., what anonymity does not hide).
In the ideal world, clients interact with an uncorruptible trusted party F called an ideal functionality.
On each API call issued by a client, F services the request and provides to the adversary (denoted S in the ideal world) a well-defined subset of information in the API call.
The subset of information that F gives to S defines what information Ghostor leaks to the adversary, and provides a clear definition of what anonymity means in our setting.
To allow for a malicious adversary, S chooses what response is returned to the client.
S may violate integrity in a way that the client will only detect at the end of the epoch (e.g., fork attack), but cannot deny service by returning a message that the client would immediately detect as fake (e.g., a message with a bad or missing signature).
To prove that Ghostor achieves that definition of anonymity, we additionally define a real world.
The purpose of the real world is to model the Ghostor system in the abstract environment we used for the ideal world.
In the real world, clients interact directly with the adversary (denoted A in the real world), which services the requests and learns some information.
The protocol that clients use to interact with A is the same as that used in the actual Ghostor system.In both worlds, there is another party Z called the environment.
The environment can communicate freely with the adversary and decides what operations the clients issue.
To prove that Ghostor achieves our definition of anonymity as specified in the ideal world, we demonstrate that for every real-world adversary A in the real world, there exists an ideal-world adversary S in the ideal world such that the environment Z cannot distinguish whether it is interacting with the real world or the ideal world.
Intuitively, this means that any "attack" that the real-world adversary A can perform in the real world, can also be performed by the ideal-world S in the ideal world.
Because the ideal-world setup is, by definition, anonymous, this shows that any attacks that A can perform are those allowed by anonymity, which implies that the real-world setup achieves anonymity.Given a real-world adversary A, we construct the corresponding ideal-world adversary S via a simulation.
This means that S uses A as a black box by carefully simulating a "real world" that runs in tandem with the ideal world.
In §3.3, we explained Ghostor's privacy guarantee in terms of a leakage function.
Anonymity, as defined by our ideal world below, maps to the leakage function given in §3.3 as follows.
The leakage function in §3.3 is largely the same as the information that F gives to S on each API call (Appendix E.2.2).
There are a few minor differences, which we now explain.
Timing information is not included in Appendix E.2.2 because the model we use in our cryptographic formalization does not have a notion of time.
That said, the order in which the requests are processed is given to S; it is implicit in the order in which F sends messages to it.
Finally, although not explicit in Appendix E.2.2, S can infer how many round trips are performed between the client and server in processing each operation: as long as there is no client-side caching of data ( §4.4), the adversary can infer how many round trips are required from the client-server protocol (Appendix A), because we do not model concurrently executing operations.
We consider the protocol to be public, so this does not reveal any meaningful information.Our definition of anonymity matches the everyday use of the word "anonymity" because S does not receive any userspecific information for operations issued by honest users on objects that no compromised user is authorized to access.
Furthermore, S does not see the membership of the system (public keys of users) or even know how many users exist in the system, apart from corrupt/maliicous users.
Although our cryptographic formalization is useful to prove Ghostor's anonymity, there are some aspects of Ghostor that it does not model.
First, we do not directly model the anonymous payment (e.g., Zcash) aspect of Ghostor.
Instead, we assume the existence of an ideal functionality for Zcash, F Zcash , that can be queried to validate payment (i.e., learn how much was paid and when).
Second, we do not directly model network information (e.g., IP addresses) leaked to the server when clients connect, because this is hidden by the use of an anonymity network like Tor ( §8).
Third, whereas the Ghostor system allows operations to be processed concurrently (i.e., round trips of different operations may be interleaved), our formalization assumes that the Ghostor server processes each operation one at a time.
Fourth, we do not fully model Ghostor's integrity mechanisms, such as the return value of obtai n_digests.Users may also be malicious (i.e., controlled by the adversary).
In our formalization, the adversary may compromise users, but we restrict the adversary to doing so statically.
This means that the adversary compromises users at the time of their creation.
The environment Z may choose to give the adversary control over certain users and clients to try and distinguish the ideal world from the real world.
We define an ideal functionality for an anonymous object sharing system in the simulation paradigm, which captures Ghostor's privacy guarantee.
Our notation and setup are as follows.
The environment Z interacts with the party P representing a Ghostor client, which simply relay messages to the ideal functionality F.
The ideal-world adversary S interacts with F. Control begins with the environment Z.
The environment may request P to initiate an operation provided by Ghostor's Client API: GET, PUT, set_acl, create_user, obtain_token, or obtain_digests.
This is done via Initiate and New_User messages.
In the ideal world, the P is a dummy party, which forwards these Initiate and New_User messages to F.We model create_object as a special case of set_acl.
We find this convenient because both create_object and set_acl set the object's header.
Furthermore, our implementation ( §8) uses the same RPC call to handle both.To perform certain operations (e.g., GET, PUT, set_acl, etc.), a user keypair is necessary.
This user keypair can be used for asymmetric encryption/decryption with a key-private encryption scheme, and is used in order to obtain the object's signing key from the object header.
To formalize this, we draw a distinction between users and clients.
Users have keypairs and are represented in the ideal world with IDs; in contrast, the client is P. Each Initiate message contains the user_ID of the user on whose behalf the operation will be performed.
That there is only client that will actually perform the operation informally captures the guarantee given by the anonymity network, that the server cannot tell apart different Ghostor clients on the basis of network information.In summary, each Initiate message contains:• user_ID specifying which user's keypair to use for this request • opcode, which can be one of GET, PUT, set_acl, create_user, obtain_token, or obtain_digests • new_contents if opcode = PUT or opcode = set_acl • new_header if opcode = set_acl • payment_ID (forwarded to F Zcash ) if opcode = obtain_token • object_ID specifying the object on which this request operates • Payment token to fund the operation (if applicable) No information related to proof of work is included because S will be able to simulate it without any external information.
Upon receiving an Initiate message, F reveals some information to S, described in Appendix E.2.2.
As mentioned earlier, we allow users to be corrupted, but require corruption to be static: users are corrupted at the time they are created.
This is handled by the New_User message, which contains:• inform, a bit indicating if the adversary is aware of this user • compromise, a bit indicating if this user is corrupted or not Upon receiving a New_User message, F generates a random user_ID, and keeps track of whether the user is compromised.
If the inform bit is set, then the user_ID is given to the adversary S so that malicious users may add this user to ACLs.
If the user is compromised, then F uses this information to give more information to S when processing requests (see Appendix E.2.2).
In each PUT operation, F generates a fresh ID, denoted content_ID, to represent the contents being written to that object.
We refer to this mapping from PUT operation to content_ID as the content table.
E.2.2 Information that F gives to S Each Initiate message that the dummy party P sends to F represents an API call ( §2) to the server.
Given each API call, F processes the request and reveals some information to S. First, F checks if the user issuing the request is malicious or not.
If the user is malicious, then F reveals to S all information about the request, including which user makes the request and all arguments to the request.
If the user issuing the request is honest, then F reveals to S the opcode and the following information:• For create_user, the user_ID is given to S if either the inform or compromise bits are set.
Otherwise, nothing is given to S. • For GET, F gives S only the object_ID of the object being accessed.
S gives back to F the content_ID of the content to be returned, or ⊥ if the operation fails or is aborted by S.• For PUT, F gives S only the object_ID of the object being accessed, and the content_ID and length of the object contents being written.
However, if a malicious user has ever been on the ACL of the object, the object contents are given to S in cleartext.
• For set_acl, F scans the ACL being set, identifying which users are malicious.
For each honest user in the ACL, F replaces the corresponding rows of the ACL with NULL.
As object is being re-encrypted, F either gives S a content_ID and length, or the cleartext contents, depending on whether a malicious user has ever been on the ACL of the object.
• For obtain_token, F reveals to S the payment_ID.
S responds with tokens that can be redeemed with future operations.
F returns integers back to the party, which can be used as payment tokens in future Initiate messages to pay for operations.
F keeps track of which of these tokens are spent, based on feedback from S indicating for which operations the payment was accepted.
• For obtain_digests, F reveals to S the epoch number and object_ID for which digests are to be obtained.
Additionally, F checks that the payment token provided in the Initiate message is valid, and reveals to S a single bit indicating whether a valid token was provided.We have not yet specified what F returns to P.
In order to allow the adversary to make arbitrary integrity violations during an epoch, the return value must originate from S. For GET, S returns the content_ID for the returned content; F translates it back into actual content and gives it to the party P who requested it.
For obtain_token, F forwards the response from S back to P. For operations involving token payment, S gives F a bit indicating whether the payment was accepted, which is forwarded to the original party P. For operations performed by a malicious user, P gives Z the result of the operation.At any time, S can send F Zcash a payment_ID.
If it does so, it will receive from F Zcash a response message indicating if the payment to the server is valid, and if so, and how much was paid and when.
The real world models Ghostor's execution.
We will prove that our model of Ghostor in the real world reveals essentially the same information to the adversary as is revealed to the adversary in the ideal world.The real world has the following key differences from the ideal world, in order to properly model Ghostor's execution:• The party P handles Initiate messages from Z, instead of simply forwarding them to F. • The party P sends Request messages to A and receiveResponse messages from A (instead of F).
• The party P encrypts object headers and object contents, and A receives the ciphertexts, according to the Ghostor protocol.Upon receiving an Initiate message from Z, the P performs the operation specified in the Initiate message by interacting with A according to the Ghostor protocol (Appendix A).
We do not specify the protocol in additional detail here because it is already specified in Appendix A. Upon receiving a New_User message, P creates a keypair (pk, sk) and generates a user_ID for the new user and stores them locally.
If the compromise bit is set, it shares the secret key with A, and if either the inform or compromise bits are set, then it informs A of the user_ID and public key.
As in the ideal world, malicious users' results are given to Z.For obtain_token, recall that we model Zcash as an ideal functionality F Zcash , which allows the adversary to validate a payment transaction via Zcash and learn how much was paid and when.
Although A may follow the protocol in Appendix A at times, it is not obligated to; it may violate the protocol in ways that are not immediately detectable to the clients.
Z can also create users via New_User messages, which are handled locally by P.
They generate the corresponding keypair and locally store which user_ID maps to that keypair.
If the New_User message has the inform bit set, then the user_ID and pk for that user are given to A; if the compromise bit is set, then A is also given sk for that user.
We now describe a simulator S that, given any real-world adversary A, performs the same attack in the ideal world as A does in the real world, by invoking A as a black box.
Note that S, by the design of F, is not given any user identities, yet needs to interact with A as some user.
The key idea is that S simply creates a single "dummy" user keypair, and performs all interaction with A on behalf of honest users as that one user.
The design of Ghostor is such that the server cannot distinguish this from a separate keypair being consistently used for each honest user.S works by simulating a real world in which A exists as a black box.
Recall that the real world consists of the parties P, Z, and A; for clarity, we use Q to refer to P in this simulated real world, to distinguish it from P in the ideal world.
E.4.1 State Maintained by S S maintains a pool of tokens to use.
Successful calls to obtai n_token contribute to this token pool, S stores tokens in this pool.
For operations that require payment, F does not tell S which particular tokens to use, so S chooses tokens randomly from the pool.S also maintains a ciphertext table.
In the messages received, certain encryptable pieces of data (e.g., content_IDs) correspond to encrypted data in the actual Ghostor.
To account for this, the ciphertext table maps each encryptable datum received by S to a fake ciphertext.
• The fake ciphertext corresponding to object contents is an encryption of a "zero string" of the same length as the object contents.
The key used to encrypt the zero string is the same as the key normally used to encrypt object contents.
2 • The fake ciphertext corresponding to a NULL entry in the object header is an encryption of a "zero string" of the same length as the plaintext object header entry, using the dummy user keypair.
Now, we explain how S interacts with A upon receiving information from F.
When F asks S to start an operation, it interacts with A over multiple round trips according to the Ghostor protocol via the simulated party, making sure to blind the request messages appropriately by replacing ciphertexts with fake ciphertexts.
All object header entries corresponding to non-corrupt users are blinded; entries are created for them in the ciphertext table.
The decision of whether to blind the object contents depends on whether a corrupt user has permission to read the object.
Note that F has already determined this by the time it has sent the message to S, and has NULLed object header entries for non-corrupt users and replaced data for each object not shared with corrupt users with an ID from its contents table.
Therefore, S simply needs to create fake ciphertexts for object data that correspond to IDs in F's content table and for NULLed object header entries.
Any object contents or object header entries that are not blinded are encrypted exactly as in the normal Ghostor system; S then forwards the ciphertexts to A. Now, we describe the simulator more precisely.
For operations that require payment, S verifies that the message it received from F indicates that a valid token were paid.
Then it chooses a token randomly from its store, unblinds it, and uses it when interacting with A.
If the operation is successful, it marks the token as "used" so it is not chosen for a later operation.
create_user.
Suppose S receives a message from F with a create_user opcode.
If the compromise bit is set, then S generates a keypair (pk, sk) for this user and stores the mapping from the provided user_ID to this keypair.
If the inform bit or compromise bit is set, then A is informed of this user_ID, as if Q received a New_User message.
set_acl.
Suppose S receives a message from F with a set_acl opcode.
S has the party Q perform a set_acl operation.
• If this operation creates the object, then S generates the keypairs for the object, and creates the encrypted key list for the object.
S constructs each entry of the key list correctly in plaintext, and then encrypts each one as follows.
If the entry corresponds to a malicious user, then it encrypts the entry using that user's public key.
If the entry corresponds to an honest user, then it creates a fake ciphertext (encryption of zero string of the same length) using the honest keypair shared by all honest users and adds the mapping in the ciphertext table.
Then it completes the operation using the resulting encrypted keylist.
• If this operation operates on an existing object, then S performs the operation using PSK (with a check if the owner is malicious).
If the message from F includes a content_ID and length, then S has the same operation include a fake ciphertext for the re-encrypted object contents; otherwise if F includes the contents, then S encrypts it to produce the new data ciphertext.
In both cases, the key to encrypt the object data is updated with a fresh one.
PUT.
Suppose S receives a message from F with a PUT opcode.
There are three cases:• Suppose the PUT was performed by an honest user, and no malicious users have ever been on the ACL.
S receives the ID of the object and the length of the contents being written.
In the simulation, S has Q perform a PUT operation, using WSK.
S uses a fake ciphertext (encrypted string of zeros of the correct length) and adds a mapping from the provided content_ID to the fake ciphertext in the ciphertext table.
• Suppose the PUT was performed by an honest user, but malicious users have been on the ACL of the object.
S receives the ID of the object and the object contents.
Then S encrypts the object contents and uses the resulting ciphertext instead of using a fake ciphertext, and has Q interact with A to write the fake ciphertext to the specified object.
• Suppose the PUT was performed by a malicious user.
Then S has Q perform the operation using the information in the Initiate message, without using any fake ciphertexts.
GET.
Suppose that S receives a message from F with a GET opcode.
There are two cases:• Suppose the GET was performed by an honest user.
In this case, S gets the object_ID of the object being accessed.
Then S has Q perform the GET operation using RSK.
The ciphertext returned by A is translated back to a content_ID based on the ciphertext table (or decrypted if it is not a fake ciphertext), and given back to F. • Suppose the GET was performed by a malicious user.
In this case, S gets the entire Initiate message used to initiate this operation.
Then S has Q perform the GET operation using the keypair for that malicious user.
The ciphertext returned by A is translated back to a content_ID based on the ciphertext table (or decrypted if it is not a fake ciphertext), and given back to F. obtain_token.
Suppose that S receives a message from F with an obtain_token opcode.
The message contains the payment_ID, which is forwarded to A.
The tokens produced by A are then collected by S. S keeps the tokens from A in its global pool of tokens.
Then S forwards identifiers for the tokens back to F as the return value.
If A attempts to send a message to F Zcash (as part of obtain_token or at any other time), then S sends the message to F Zcash in the ideal world, and gives the response to A in simulation.
obtain_digests.
Suppose that S receives a message from F with an obtain_digests opcode.
The message is forwarded to A.Notably, this model does not include the payment phase in which the client initiates a Zcash transaction to transfer funds.
Instead, we model Zcash as a trusted party, which the adversary cannot control.
This ensures that the server learns nothing during the payment phase in the actual protocol.
Formally, we define an ideal Zcash functionality F Zcash , which the adversary can use to check if a Zcash transaction ID is valid.
F Zcash reveals only the time of the transaction and the amount paid.
Modeling Zcash (i.e., providing a real-world setup that realizes F Zcash ) is out of scope for this work.
We are now ready to define Ghostor's anonymity.
We denote the security parameter as κ throughout this paper.Theorem 1 (Privacy in Ghostor).
Suppose that in Ghostor, the data encryption scheme is CCA2-secure, the ACL encryption scheme is CPA-secure, the ACL encryption scheme is key-private, payment tokens are blind, and F Zcash is an ideal functionality for Zcash.
For every non-uniform probabilistic polynomial-time real-world adversary A, there exists a nonuniform probabilistic polynomial-time ideal-world adversary S such that for every non-uniform probabilistic polynomialtime environment Z, Z cannot distinguish the real world with adversary A from the ideal world with adversary S.Proof.
We shall demonstrate that for every real-world adversary A, there exists an ideal-world adversary (simulator) S such that there exists no environment Z probabilistic polynomial-time in κ that can distinguish between interacting with the real world and interacting with the ideal world.
Specifically, for an arbitrary real-world adversary A, we construct an ideal-world adversary S that uses A as a black box to perform the same attack in the ideal world as A performs in the real world.
S simulates an environment that is computationally indistinguishable from the real world, meaning that A will behave the same way in simulation with at most a negligible difference in probability.
We take S as the simulator described in Appendix E.4.
There are two things to prove: 1.
From A's perspective, the simulated world provided by S is computationally indistinguishable from the real world.
2.
From Z's perspective, the real world with adversary A is computationally indistinguishable from the ideal world with adversary S. To show that these statements are true, we consider a sequence of seven hybrid setups.
Although the two statements above are in principle separate, we use the same sequence of hybrids to prove both of them.
Note that H 0 is equivalent to the realworld setup, and H 6 is equivalent to the simulated setup.
In a true hybrid argument, only one operation can be modified at a time; our hybrids in the proof sketch below should be interpreted as key stages.Hybrid H 0 .
This is exactly the real-world setup in Appendix E.3.
Hybrid H 1 .
This is the same as H 0 , except that we replace A with S. S, in this hybrid, maintains a simulated party Q corresponding to P, and internal to S, these simulated parties interact with A. P interacts with S; when S receives a message from P, it forwards it to A via Q, and when A sends a message to one of S's simulated parties Q, it forwards it to P. Similarly, when A sends a message to F Zcash , S forwards the message to F Zcash , obtains the response, and forwards it to A, as if A communicated with F Zcash directly.S acts simply as a relay, shuttling data back and forth between P and Q and between A and F Zcash .
In particular, the messages observed by A and Z are exactly the same as before.
Therefore, neither A nor Z can distinguish H 0 from H 1 .
Hybrid H 2 .
This is the same as H 1 , except that we now introduce the ideal functionality F. F, in this hybrid, just relays messages back and forth between the real-world party P and the simulator S.Here, the newly introduced F acts as another intermediate relay.
Again, the messages observed by A and Z are distributed exactly the same as before.
Therefore, neither A nor Z can distinguish H 1 from H 2 .
Hybrid H 3 .
We change P to a dummy party as in the ideal world.
Instead, S handles participating in the protocol as the honest clients, including PoW.
The requests for operations are forwarded by the party P to F.Although S now uses its dummy user keypair to interact with the server, the encryption is key-private; the server cannot distinguish an ACL entry encrypted under a user's key from the same ACL entry encrypted with S's dummy user key.
Therefore, neither Z nor A can distinguish H 2 from H 3 .
Hybrid H 4 .
This is the same as H 3 , except that F replaces ACL entries of honest users with NULL; S replaces NULL entries with encryptions of zero under the dummy key, for the ACLs of the real-world protocol.The semantic security of the encryption scheme used for ACLs guarantees that, to the adversary, an encryption of zero is indistinguishable from the actual encrypted ACL entry.
Therefore, neither Z nor A can distinguish H 3 from H 4 .
Hybrid H 5 .
This is the same as H 4 , except that F also replaces object contents with IDs in its content table, and S in turn replaces these IDs with fake ciphertexts in its ciphertext table.
In particular, if all users in an object's ACL are honest, then F and S, together, replace the contents of the object with an encryption of the zero message of the same length, using the same key normally used to encrypt the object contents.The semantic security of the encryption scheme used to encrypt object contents guarantees that A cannot distinguish between the fake ciphertext and the actual ciphertext.
Furthermore, because the plaintext is returned as the result of the operation, we need to be sure that A cannot create a new valid ciphertext with a different plaintext distribution.
Fortunately, the fact that we use CCA2-secure authenticated encryption guarantees this; the adversary cannot create a new ciphertext based on the fake one.
Therefore, neither A nor Z can distinguish H 4 from H 5 .
Hybrid H 6 .
This is the same as H 5 , except that S keeps track of a pool of tokens, S gives F identifiers for the tokens, and F gives S a bit indicating if a valid token was used instead of specifying which token was used.The blindness property of the blind signature scheme means that, to the server, different payment tokens, after being unblinded, are indistinguishable from each other.
To the environment Z, the interface is exactly the same and tokens are expended exactly as before.
Therefore, neither A nor Z can distinguish H 5 from H 6 .
In this appendix, we state the integrity guarantee provided by Ghostor.
Before we formalize Ghostor's VerLinear guarantee, we define linearizability as a consistency property.
Linearizability is well-studied in the systems literature [34,42], and providing a comprehensive survey of this literature and a fully general definition is out of scope for this paper.
Here, we aim to define linearizability in the context of Ghostor, to help frame our contributions.Definition 1 (Linearizability).
Let F be a set of objects stored on a Ghostor server, and let U be a set of users who issue read and write operations on those objects.
The server's execution of those operations is linearizable if there exists a linear ordering L of those operations on F, such that the following two conditions hold.
1.
The result of each operation must be the same as if all operations were executed one after the other according to the linear ordering L. 2.
For every two operations A and B where B was dispatched after A returned, it must hold that B comes after A in the linear ordering L.In Ghostor, an object's digest chain implies a linear ordering L of GET and PUT operations, as follows.
Linear ordering L implied by a digest chain.
The linear ordering L to which the server commits is based on the digest chain as follows.
First, we assign a sequence number to write operations according to the order of their PREPARE digests in the digest chain.
Next, we bind each operation to a digest in the digest chain as follows:• Each read is bound to the digest representing that read.
• A write with sequence number i is bound to the first COM-MIT digest whose sequence number is at least i.
This is either the COMMIT digest for this write, or the COM-MIT digest for a concurrent write that wins over this one based on the conflict resolution policy in §5.4.
Assuming the digest chain is well-formed (all cases except Case 1 below), each write will be bound to a COMMIT digest that is after its PREPARE digest and before or at its COMMIT digest.
Finally, we generate the linear ordering as follows:• If two operations are bound to different digests, then they appear in L in the same order as the digests appear in the digest chain.
• If two writes are bound to the same digest, then they are ordered in L according to their sequence numbers.
For example, suppose the digest chain contains (R 1 , P 1 , R 2 , P 2 , R 3 ,C 2 , R 4 , P 3 , R 5 ,C 1 , R 6 ,C 3 , R 7 , P 4 , R 8 ,C 4 , R 9 ), where R denotes a read digest, P denotes a PRE-PARE digest, and C denotes a COMMIT digest.
The corresponding linear ordering of operations is L = (R 1 , R 2 , R 3 ,W 1 ,W 2 , R 4 , R 5 , R 6 ,W 3 , R 7 , R 8 ,W 4 , R 9 ), where R denotes a read operation and W denotes a write operation.
We begin by stating and proving Theorem 2 below, which specifies the achieved guarantees when some users perform the verification procedure for an epoch.
Then, we present the VerLinear property of Ghostor as Corollary 1, a special case of Theorem 2.
We use this approach because Theorem 2, despite being a more general statement, has fewer edge cases than Corollary 1, and we feel its proof is easier to understand in isolation.
The statement of Corollary 1 maps directly to our informal definition of verifiable linearizability in §3; the key differences are only that Corollary 1 is explicit that security depends on collision resistance of Ghostor's hash function and existential unforgeability of Ghostor's signature scheme, introduces variables that are useful in the proof, and states the security guarantee as the contrapositive of Guarantee 1.
Theorem 2 (Epoch Verification Theorem).
Suppose the hash function H used by Ghostor is a collision-resistant hash function with security parameter κ.
Let B be a non-uniform adversary that is probabilistic polynomial-time in κ performing an active attack on the server.
Let E be a list of consecutive epochs.
For each epoch e ∈ E, let U e be a set of users for whom the verification procedure for a particular object F detected no problems during epoch e, and let O e be the set of operations performed by those users on F.
If U e = ∅ (i.e., U e is nonempty) for all e ∈ E, then there exists, with probability at least 1 − µ(κ), where µ denotes a negligible function, a linear ordering L of operations in O = e∈E O e and possibly some other operations, such that for the users in U and their operations O, the following two statements hold.
1.
The result of each successful operation is the same as if all operations were executed one after the other according to L. 2.
For every two operations A and B where B was dispatched after A returned, B comes after A in L.Proof.
We will perform a reduction to show that if there exists an adversary B that can cause one of the two conditions to be violated, then there exists an adversary A that can violate the collision-resistance of H with non-negligible probability.
For concreteness, suppose that B performs such an attack with non-negligible probability δ(κ) (so that the condition in the theorem holds with probability 1−δ(κ)).
We will explain how A can succeed in finding a hash collision with non-negligible probability.By the nature of the attack, B is able to violate the property in the theorem statement, while remaining undetected by users in U. Observe that B's attack must fall into at one of four cases.
1.
There exists at least one object such that B does not commit to a valid digest chain for an epoch, for some honest user.
2.
There exists at least one object such that B commits to a different digest chain for different honest users.
3.
There exists an operation on an object f ∈ F whose result is different from the result that would be obtained by applying the operations one after the other in the linear ordering implied by f 's digest chain.
4.
There exist operations a and b on the same object, where a was issued after b completed, but a precedes b in the linear ordering implied by the digest chain.
In particular, if B's attack does not fall into one of these cases, then the locality property proved in §3 of [42] guarantees that B's behavior is consistent with the theorem statement (linearizability of operations in L).
We will show that no matter which of the above four cases describes B's attack, A can find a hash collision.
Case 1.
In this case, B returns an invalid linear ordering to a user when the user performs an obtain_digests operation.
The ordering could be invalid because the digest is not signed properly, or the digests do not form a well-formed chain.
This also includes the case where a user's operation is missing from the digest chain.
Because we require that U e = ∅ for all e ∈ E, this will be detected with probability 1.
Therefore, we do not consider this case.
3 An important note is that if each L e is valid, then L is valid.
Case 2.
In this case, the adversary returns different histories to different users.
Because the histories differ, they cannot be the same in all epochs; we consider an epoch e in which they differ.
This allows us to confine our argument to a single epoch.
In particular, there exist two obtain_digests operations on the same object during epoch e, for which B returns different histories in a way that is not detectable.
4 We define two subcases.In the first subcase, the leaf of the Merkle tree, containing the hash of the final digest for the object in the epoch, is differ-f during epoch e, and the signature scheme is existentially unforgeable, we know that all operations in L that are not in O must be reads.
Let denote the subset of L consisting only of operations in O. Now, observe that Properties #1 and #2 in the statement of Theorem 2 also hold for the operations in O with respect to .
This is because (1) L is the same as with some additional read operations, so the result of each operation, when operations are executed one after the other, is the same for both orderings, and (2) the relative ordering of operations in O is the same in both L and .
Because contains only the operations in O and it satisfies Properties #1 and #2, it fulfills Definition 1.
Therefore, the execution of operations in O is linearizable.
We thank the anonymous reviewers and our shepherd, Carmela Troncoso, for their invaluable feedback.
We would also like to thank students from the RISELab Security Group and BETS Research Group for giving us feedback on early drafts, and David Culler for advice and discussion.This work has been supported by NSF CISE Expeditions Award CCF-1730628, as well as gifts from the Sloan Foundation, Bakar, Alibaba, Amazon Web Services, Ant Financial, Capital One, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk, and VMware.
This research is also supported in part by the National Science Foundation Graduate Research Fellowship Program under Grant No.
DGE-1752814.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
