This paper develops analytical models to predict the throughput and the response time of a replicated database using measurements of the workload on a standalone database.
These models allow workload scalability to be estimated before the replicated system is deployed, making the technique useful for capacity planning and dynamic service provisioning.
The models capture the scalability limits stemming from update propagation and aborts for both multi-master and single-master replicated databases that support snapshot isolation.
We validate the models by comparing their throughput and response time predictions against experimental measurements on two prototype replicated database systems running the TPC-W and RUBiS workloads.
We show that the model predictions match the experimental results for both the multi-master and single-master designs and for the various workload mixes of TPC-W and RUBiS.
Predicting the performance of replicated databases is important for their wide adoption.
Performance models are employed for capacity planning [Lazowska 1984] and for dynamic service provisioning [Urgaonkar 2005b] as in data centers that host several e-commerce applications and receive external loads that vary with the diurnal cycles and seasonal effects.
To the best of our knowledge, it is not possible yet to know how an application is going to scale on a replicated database without actually building the replicated system and running the application with a scaled workload.In this paper, we develop analytical models that predict application workload scalability on a replicated database system.
Performance of such replicated systems depends on the workload parameters.
We demonstrate that measurements of the workload running on a standalone system capture sufficient information for our models to predict the performance as more replicas are added.
The models are designed for middleware-based replicated systems in a LAN environment employing snapshot isolation and running transactional workloads from e-commerce.
Our models borrow from prior work in calculating abort rates by Gray et al. [ Gray 1996] and from modeling update propagation by Jiménez-Peris et al. [ Jiménez-Peris 2001], but go beyond these works by predicting throughput and response time estimates that combine update propagation overhead and conflicts, rather than calculating upper bounds on system scalability.We model both multi-master systems (in which each replica handles both read-only and update transactions) and single-master systems (in which the master replica executes update transactions and slave replicas execute read-only transactions).
We validate the models by comparing their predictions against the measured performance of prototypes for both the multi-master and single-master systems.
While we are aware of the many complexity and availability tradeoffs between single-and multi-master replication, in this paper we focus only on performance prediction.The contributions of this work are the following: (1) We derive analytical models that predict the performance of multi-master and single-master replicated databases running snapshot isolation.
(2) We show how to use the analyt-ical models to predict the performance of the two designs from workload measurements of a standalone system.
(3) We validate the models by comparing their predictions against experimental measurements of prototype implementations.The remainder of the paper is structured as follows: Section 2 presents the necessary background to follow the analytical models, which are derived in Section 3.
Section 4 shows how to estimate model parameters.
We describe the implementation of the prototype systems in Section 5 and experimentally validate the models against the prototype systems in Section 6.
We discuss related work in Section 7, and present the conclusions in Section 8.
Snapshot isolation (SI).
Snapshot isolation (SI) [Berenson 1995] is an optimistic multi-version database concurrency control model for centralized databases.
SI achieves high concurrency with low conflicts at the cost of using more space (employing multiple versions of data items) and aborting (rather than blocking and reordering) few update transactions.
When a transaction begins, it receives a logical copy, called snapshot, of the database for the duration of the transaction.
This snapshot is the most recent version of the committed state of the database.
Once assigned, the snapshot is unaffected by (i.e., isolated from) concurrently running transactions.
A read-only transaction can always commit after reading from its snapshot.
An update transaction commits if it does not have a write-write conflict with any committed update transaction that ran concurrently.
When an update transaction commits, it produces a new version of the database.
The granularity of conflict detection is typically a row in a database table (i.e., a tuple in a relation).
SI has attractive performance properties.
Most notably, read-only transactions do not get blocked or aborted, and they do not cause update transactions to block or abort, making SI particularly suitable for read-dominated workloads with short updates as in e-commerce environments.Many database vendors use SI, e.g., PostgreSQL, Microsoft SQL Server and Oracle.
SI is weaker than serializability, but in practice many applications run serializably under SI [Elnikety 2005, Fekete 2005b, Fekete 1999, including the widely used database benchmarks TPC-C, TPC-W and RUBiS.
Generalized snapshot isolation (GSI).
Generalized snapshot isolation (GSI) [Elnikety 2005] extends SI to replicated databases such that the performance properties of SI in a centralized setting are maintained in a replicated setting.
In addition, workloads that are serializable under SI are also serializable under GSI.
When a transaction starts, it receives the most recent snapshot at the replica where it executes.
Although this local snapshot might be slightly older than the globally latest snapshot, it is available immediately without the need for additional communication.
A read-only transaction executes entirely locally at the receiving replica.
An update transaction executes first locally at the receiving replica.
Then at commit, the writeset of the transaction is extracted and a certification service is invoked.
The certification service detects system-wide writewrite conflicts.
If no conflict is detected the transaction commits, otherwise it aborts.
The writeset of an update transaction captures the transaction effects and is used both in certification and in update propagation.
Multi-master replication.
In a multi-master (MM) system [Elnikety 2006, Elnikety 2007, Lin 2005, each replica executes both readonly and update transactions.
The replication middleware resolves conflicts by aborting conflicting update transactions.
The MM system consists of a load balancer, several database replicas and a certifier that certifies update transactions to prevent write-write conflicts.
Single-master replication.
In a single-master (SM) system [Daudjee 2006, Gray 1996, Plattner 2004 (also called master-slave), the master database executes all update transactions and several slave replicas execute read-only transactions.
Restricting the execution of update transactions to the master makes SM systems less flexible but simpler to build compared to MM systems.
A SM system is simpler to build because it does not need a certifier.
The master handles all write operations, and therefore its concurrency control subsystem can abort transactions that introduce write-write conflicts.
Conflict window.
The conflict window [Elnikety 2005] captures the time interval during which an update transaction is vulnerable to write-write conflicts, which result in aborting the update transaction.
For a standalone database, the conflict window of an update transaction is its execution time on the database.
For a single-master system, the conflict window is the execution time on the master.
For a multi-master system, the conflict window has three components: (1) the age (staleness) of the snapshot the transaction receives, (2) transaction local execution time on the database replica, and (3) time for the certification service to certify the transaction.
The analytical models aim to predict the performance of ecommerce workloads on replicated snapshot-isolated databases.
Our aim is to capture the essential system features, including update propagation and aborts, while keeping the models sufficiently simple to be analytically tractable.
The transaction workload for a standalone database consists of R read-only transactions per second and W update trans-actions per second generated by a fixed number of clients.
Each client submits a transaction, waits for the database response, examines the response during the think time, and then submits the next transaction, following a closed-loop model [Schroeder 2006].
The fraction of read-only transaction is Pr and the fraction of update transactions is Pw, such that Pr + Pw =1.
We scale the workload with the number of replicas in the replicated database such that a replicated database system that has N replicas receives requests from N times the number of clients in a standalone database.
In this section we construct queuing models that compute the throughput and response time of each replica using transaction service demands that we derive in Section 3.3.
Figure 1 depicts a separable closed queuing network that captures the components of the multi-master system.
We model the CPU and disk on the database replica as regular service centers with queues.
We model the delays introduced by the load balancer and local area network as a single delay center called load balancer delay.
The certification time is almost constant, insensitive to the number of concurrent certification requests.
We, therefore, approximate the certifier as a delay center rather than a service center to make the model tractable.
We provide more details in Section 6.3 to justify these assumptions.
Model inputs.
The model inputs are the following: service demands at each service center, D MM for CPU and disk; delay time at each delay center for think time, load balancer, and certifier delays; number of replicas (N); and number of clients per replica (C).
Model outputs.
The model computes average throughput and response time.
The model produces additional data for each resource such as utilization and residence time.
Solving the model.
We use the mean value analysis (MVA) algorithm [Lazowska 1984] which is a standard algorithm to solve closed-loop queuing models.
All replicas are identical and they contribute the same throughput assuming perfect load balancing.
MVA iterates over the number of clients, adding N clients (one client per replica) in each iteration.
MVA computes the residence time at each service center, system throughput and queue lengths.
Figure 2 depicts a separable closed queuing network for the single-master system.
The single-master model has similar inputs and outputs as the multi-master model.
Solving the model, however, requires balancing the load among the master and the slaves.Intuitively at steady state when the system is balanced, the ratio of the slaves throughput : master throughput should be Pr : Pw.
Here the queuing model is not symmetric because updates are handled by the master only, raising two unbalanced cases: First, the master becomes underutilized if the transaction workload is dominated by read-only transactions which make the slaves the bottleneck.
Since the master has excess capacity, it should process extra readonly transactions as well as all update transactions.Second, when the master is the bottleneck, the total system throughput becomes limited by the master, forcing clients to queue at the master and reducing the load on the slaves.
Figure 3 presents a load balancing algorithm that balances the load and accounts for these two cases.
If the system is balanced, the algorithm terminates immediately and reports balanced read and update throughputs.
Single-master queuing model (SM system).
If the system is not balanced, the algorithm uses two properties to rebalance the solution: (1) the constant ratio of read-only to update transactions Pr : Pw provided as an input property of the workload, and (2) the fixed number of clients in system, who are distributed among centers proportional to residence times.
We use MVA as a building block.
The system has 1 master and N-1 slaves, and the total number of clients is N·C.
We distribute clients among the master and slaves, and solve the queuing model.
We compare the ratio of the resulting throughputs (slaves throughput : master throughput = Pr : Pw).
If the master throughput is too high, indicating there is excess capacity at the master, we move read-only transactions from the slaves to the master until the ratios are balanced.
If the slaves' throughput is too high, we reduce the number of clients on the slaves because more clients queue on the master.
Both cases terminate quickly because they iterate over a finite number of clients.
The system response time is computed using Little's law [Kleinrock 1975].
We estimate the CPU and disk service demands at the replicas for executing transactions.
We first model resources in a standalone database system and then extend the discussion to replicated databases.
Table 1 lists the symbols used in the model.
The average service demand of a read-only transaction is rc (e.g., CPU time or disk time), and the average service demand of an update transaction is wc.
Some update transactions are aborted due to write-write conflicts under snapshot isolation.
Those transactions are retried.
Let A 1 be the probability that an update transaction aborts in a standalone database.
To successfully commit W update transactions, W/(1 -A 1 ) update transactions are submitted, of which W commit and W· A 1 /(1 -A 1 ) abort.
Therefore, the resources required for R committed read-only transactions and W committed update transactions are:1 (1) (1 ) W Load R rc wc A = ⋅ + ⋅ −The average the service demand for one transaction is then:1 (1) (1 ) Pw D Pr rc wc A = ⋅ + ⋅ −To quantify A 1 , we run the workload and measure A 1 directly on the standalone database system.
However, here we derive the abort probability for a standalone database because we later use a similar derivation to relate the abort rate of a replicated database to that of the standalone database.Assume the database has DbUpdateSize objects that can be modified by update transactions.
Each update transaction modifies U objects.
The probability that an update operation conflicts with another update operation is p = 1/DbUpdateSize.
Conversely, an update transaction succeeds if it conflicts with none of the concurrent updates.
The probability of success is (1 -p) to the power of the total number of update operations by the concurrent transactions.
The execution time of the update transaction is L(1), which is its conflict window.
The number of concurrent updates during the conflict window of the update transaction is L(1) · W · U.
Since each of the U update operations of an update transaction must succeed for the transaction to commit, the probability of success is:2 (1) (1) (1 ) L W U Success p ⋅ ⋅ = −The probability of abort is A 1 =1 -Success(1):2 (1) 1 1 (1 ) L W U A p ⋅ ⋅ = − − Sub-routine:Center.MVA() Inputs: readClients, writeClients Outputs: readThoughput , writeThoughputBalancing Algorithm:masterClients = Pw·C·N slaveClients = Pr·C·N/(N-1) ( -,writeThput ) = Master.MVA( 0, masterClients ) ( readThput, -) = Slave.MVA( slaveClients, 0 )//is the system balanced?
if( readThput : writeThput ≈ Pr : Pw ) return (readThput , writeThput ) //the system is not balanced.
//either master has exceess capacity or it is the bottleneck.
if( readThput : writeThput < Pr : Pw ) { //master has excess capacity: add reads to master j = 0; loop { j++; ( extraReadThput , writeThput ) = Master.MVA( j·(N-1) , masterClients ) ( readThput , -) = Slave.MVA( slaveClients -j , 0 ) } until ( (extraReadThput +readThput) : writeThput ≈ Pr : Pw ) return (extraReadThput +readThput , writeThput ) } else { //master is the bottleneck, more clients queue at master j = 0; loop { j++; ( -, writeThput ) = Master.MVA( 0, masterClients + j·(N-1) ) ( readThput, -) = Slave.MVA( slaveClients -j , 0 ) } until ( readThput : writeThput ≈ Pr : Pw ) return ( readTh , writeTh ) } Figure 3.
Balancing throughput of master and slaves.
Each replica in a multi-master (MM) system processes its local input transactions plus the writesets from remote update transactions.
Depending on the workload, the cost of a propagated writeset can be less than the cost of fully processing the original update transaction.
We, therefore, assign it a cost, ws, the average service demand required to process a propagated writeset.
In an N-replica system, each replica commits R read-only transactions, W update transactions and (N-1)·W propagated writesets.The abort probability is A N .
Aborts affect only local update transactions, and do not affect propagated writesets, which partially cause higher A N .
The resources needed to process the transaction workload at the replica are:( 1) (1 ) ( ) N MM W R rc wc W N ws A Load N = ⋅ + ⋅ + ⋅ − ⋅ −The average service demand for one transaction is:( )(1 )1)N MM Pw N Pr rc wc Pw ws A D N = ⋅ + ⋅ ⋅( ⋅ − + −Next, we relate A N to A 1 .
The same formula used to derive Success(1) in Section 3.3.1 can be used to derive the success probability Success MM (N) for the MM system.
Approximately, the N-replica multi-master system has N times the throughput and a different conflict window, CW(N).
The total concurrent update operations is CW(N)·N·W·U, and the success probability is:2 ( ) ( ) (1 ) N CW N W U MM Success N p ⋅ ⋅ ⋅ = −which can also be written as: 2 ( ) (1) (1) ( ) (1 ) CW N N L WU L MM Success N p ⋅ ⋅ ⋅ = − ⎡ ⎤ ⎣ ⎦ ( ) (1) ( )(1)1 (1 ) (1 ) CW N L N N A A ⋅ − = −(1)The final service demand equation is the following: ( )(1) 1 ( ) (1 ) 1)D master (N)Average service demand to execute one transaction on a master in a single-master system having 1 master and N-1slaves ( § 3.3.3)D MM (N)Average service demand to execute one transaction in a multi-master system having N replicas ( § 3.3.2)D slave (N)Average service demand to execute one transaction on a slave in a single-master system having 1 master and N-1slaves ( § 3.
An N-replica SM system has two components, 1 master and N-1 slaves.
We derive the service demand on each component, assuming a total system load equivalent to an Nreplica MM system.
The SM system commits N·W update and N·R read-only transactions.Master Service Demand.
The master processes all update transactions in the system.
Aborts increase the number of submitted update transactions.
N·W/(1 -A' N ) update transactions are submitted to commit N·W update transactions under the master abort rate of A' N .
The master's resource consumption is:(1 ' ) ( ) N master W N w c A Load N = ⋅ ⋅ −The average service demand per update transaction is:( ) (1 ' ) N master wc N A D = −The difference between the MM system abort probability A N and the master-slave abort probability A' N is that the master resolves all conflicts locally, like a standalone system, but at a higher rate of update transactions than the standalone system.
Slave service demand.
The slaves process N·R read-only transactions and N·W propagated writesets from the master.
Thus, each of the (N-1) slaves must process N / (N-1) readonly transactions and all remote writesets.
The slaves process only committed writesets; there are no aborts at the slaves.
The resource consumption at a slave is: master N E NW N r c N W E N W E A wc D ⋅ = ⋅ + ⋅ ⋅ + ⋅ + − ( ) slave N N W N rc ws N R E D ⋅ ( −1) ⋅ = + ⋅ ⋅ −(1 ) We summarize the main assumptions of the analytical model.1.
The workload is based on e-commerce applications; i.e., high volume of relatively short lived transactions that can be effectively distributed across replicas.
E-commerce workloads are typically read dominated.2.
The concurrency protocol is based on snapshot isolation; therefore, only write-write conflicts occur.3.
Since (generalized) snapshot isolation is a multiversion concurrency control algorithm, the bottleneck is much more likely to be a physical resource rather than a logical resource.
GSI trades space (as it maintains multiple versions) to achieve fewer conflicts; readers never block writers and writers never have to wait for readers.
The model assumes that the bottleneck is a physical resource rather than a logical resource.
The model, therefore, does not directly capture logical resources such as semaphores and lock contention.
However, their effects are partially reflected on the physical resources.4.
The abort probability of update transactions in the standalone database as well as in the replicated database is small.
Updatable data items are updated uniformly, i.e., the database does not have a hotspot.5.
The database server is scalable; resource consumption is linear with the server throughput.
Modern server operating systems are unlikely to thrash because they employ mechanisms that prevent over-subscription of physical resources, such as the O(1) thread scheduler and admission control policies.
The model does not apply in overload regions that are not linear if they exist.6.
The model uses perfect load balancing among identical machines and inherits the assumptions employed by the MVA algorithm [Lazowska 1984], such as having exponential distributions of the service demands.7.
The database is replicated in a LAN environment rather than a WAN or geographically distributed environment.When these assumptions are not valid, the model in general predicts an upper bound on performance.
This is the case for example, when the abort probabilities are high.
We investigate the sensitivity of the model to some of these assumptions in Section 6.3.
We use standard workload characterization techniques [Menasce 1998] to estimate model parameters.
These techniques gather information using measurement on an offline system.
Online methods can, however, be employed if the underlying live database system provides the required resource utilization for each transaction.
We take a backup of the database and capture the transaction workload from the standalone database system using the database log file.
The log must contain the full SQL statements, a client or session identifier and a start timestamp at which the statement was executed.
Our experience is that these values can be generated by most database logging facilities.
For example, in PostgreSQL 7, this information is generated by turning on log_statement, log_pid, log_connection and log_timestamp.
In PostgreSQL 8, a log line prefix string such as '%r %p %m %c %x' captures the necessary information.
Additionally, we need to generate the writesets corresponding to the update transactions.
This is done by defining triggers on all tables to extract and record the transaction writeset.The service demand equation D MM (N) in Section 3.
as well as rc, wc, ws for the CPU and disk.
Pr, Pw, and A 1 .
We count the number of read-only and update transactions in the captured log to determine the fractions Pr and Pw.
We count the number of aborted update transactions to calculate the abort probability A 1 .
rc, wc, ws and L(1).
We instrument a standalone database with triggers and play the log to capture the writesets.
We play read-only transactions from the log against the database and collect CPU and disk utilization to compute the service demands rc CPU and rc disk using the Utilization Law [Lazowska 1984].
The average service demand at a resource is the resource utilization divided by the throughput.
Next we play update transactions against the database to compute wc CPU and wc disk .
We also play the writesets to compute ws CPU and ws disk in a separate run.
We finally replay both read-only and update transactions to measure L(1), the average response time for update transactions in a standalone system.
A N , CW(N).
We can derive A N from A 1 using the formula below, but this requires CW(N), the conflict window, which is not available from standalone measurements.
[Elnikety 2005].
However, to compute the latter three terms we still need CW(N).
Since the MVA algorithm iterates over the number of clients, we approximate CW(N) at iteration i+1 by the sum of CPU, disk residence time and certification time at iteration i.
This slightly underestimates the abort probability.
We investigate higher abort rates in Section 6.3.3.
Finally, the model requires the think time (Z) and the number of clients (N).
In the experimental validation section these values are inputs.
However in a practical deployment, well-known approaches [Jain 1991 can be used to estimate the think time and predict the number of clients.
The certifier delay time is 12 ms as discussed in Section 6.3.2.
We estimate the model parameters for SM in the same way as for MM, except for A' N .
A' N (Master-Slave Abort Rate).
Because all updates are processed at the master, the abort rate for any level of replication can be measured directly by loading a database with a scaled update transaction workload.
The MM system is based on Tashkent [Elnikety 2006].
Each replica has two main components: a database system and a proxy which intercepts all incoming requests to the database, as shown in Figure 4.
Transaction processing.
When the load balancer receives a transaction T, it forwards T to the proxy of the least loaded replica.
The proxy executes T on the database.
All T's read and write operations, e.g., the SELECT, UPDATE, INSERT and DELETE SQL commands, are executed locally on the replica.
The proxy intercepts the SQL commit command at which point the proxy examines the writeset of T.
If the writeset is empty (T is read-only), the proxy commits T immediately.
Otherwise, the proxy invokes the certification service, sending the writeset of T and the version of its snapshot to the certifier.
The certifier decides whether to commit or abort the update transaction and performs the update propagation functionality.
When the proxy receives the certifier response, it either commits or aborts T, and forwards the outcome to the client.
Replica database.
We use a database engine on each replica that processes client read-only and update transactions.
Replica proxy.
The proxy performs two main functions.
First, it applies incoming writesets to the database.
Second, it intercepts all requests to the database to prevent interference between local update transactions and propagated writesets.
The proxy eagerly extracts the writeset of each transaction to perform early certification on partial write-sets, obviating the hidden deadlock problem [Lin 2005].
It aborts local update transactions whenever they conflict with a propagated writeset [Elnikety 2006].
Certifier.
Certification is a lightweight stateful service that maintains committed writesets and their versions.
The request to certify a transaction contains its writeset and version.
The certifier detects write-write conflicts by comparing the writeset of the transaction to be certified to the writesets of the transactions that committed after the version supplied in the request.
An update transaction is committed when its writeset is made persistent by the certifier.
Certification is deterministic and the certifier is replicated using Paxos [Lamport 1998] for fault-tolerance.
Figure 5 depicts the architecture of a single-master system.
The master database executes both read-only and update transactions.
The slaves execute read-only transactions and the propagated updates from the master.
Load balancer.
The load balancer dispatches all update transactions to the master.
When the load balancer receives a read-only transaction, it selects the least loaded replica (among the master and slaves) and forwards the transaction to that replica.
When the load balancer receives a writeset from the master, it relays the writeset to the slaves.
Transaction processing.
The master executes update transactions and either commits or aborts them.
On a commit, the master proxy extracts the transaction's writeset from the master database.
This information is forwarded to the load balancer, which forwards the commit to the client.
Read-only transactions can execute on master or slaves.
Master database.
The master database processes all update transactions.
We define triggers on all replicated tables to capture the transaction writeset in main memory.Master proxy.
The master proxy intercepts incoming requests to the master database.
When the proxy intercepts the SQL COMMIT, it invokes a trigger to retrieve the writeset of the transaction and then forwards the commit command to the database.
Slave database.
The slave database is effectively a cache against which read-only transactions are executed.
Slave proxy.
The slave proxy applies incoming writesets to the database.
It is the only source of updates to the database.
Hardware and Software Environment.
Each machine in the database cluster runs the 2.6.11 Linux kernel on a single Intel Xeon 2.4GHz CPU with 1GB ECC SDRAM, and a 120GB 7200pm disk drive.
The machines are interconnected using gigabit Ethernet.
We monitor the system load with a modified version of the Mercury server management system [Heath 2006].
For the certifier, we use a leader and two backups for fault tolerance.
We use the PostgreSQL 8.0.3 database configured to run transactions at the snapshot isolation level (which is the strictest isolation level in PostgreSQL and called the "serializable transaction isolation level").
Both TPC-W and RUBiS are serializable under GSI [Elnikety 2005].
To drive the replicated database system, we use many client machines.
Each client machine runs Tomcat 5.5 and a remote terminal emulator (RTE) for TPC-W or RUBiS.
The RTE is a multithreaded Java program in which each thread represents one client and the application server (Tomcat) executes the requested Java Servlets which access the database using JDBC.
If an update transaction is aborted, the Java Servlet retries the transaction.The client machines are lightly loaded and the processing delay is less than 100 ms per transaction.
Client think time follows an exponential distribution with an average of 900 ms. For the analytical models we use 1000 ms as the effective think time to account for processing times on the client machines, think time, load balancer delay and networking delay.
Each point in the graphs below represents the result of one experiment.
We report sustained average throughput and response time during 15 minutes after a warm-up period of 10 minutes.
These intervals are selected such that the measurements are performed while the system performance is in steady state.
To evaluate the accuracy of the models, we compare the measured performance of the TPC-W benchmark to the predicted performance across the three workload mixes.TPC-W parameters are summarized in Table 2.
The parameters needed for modeling CPU and disk service demands per transaction are listed in Table 3.
The abort rate of TPC-W in the standalone database, A 1 , is very small for all mixes below 0.023%.
We address the topic of prediction under higher abort rates in Section 6.3.3.
Figure 6 plots the throughput in transactions per second (tps) on the y-axis for TPC-W browsing, shopping and ordering mixes as a function of the number of replicas on the x-axis for the MM system using solid lines.
The corresponding throughput curves predicted by the model are shown using dotted lines.
The browsing mix scales almost linearly: Its throughput curve starts at 22 tps at one replica and increases to 347 tps at 16 replicas, which is a speedup of 15.7 times.
The browsing mix has excellent scalability because it is dominated by read-only transactions.In contrast, the ordering mix increases from 45 tps at one replica up to 304 tps at 16 replicas yielding a speedup of 6.7 times due to the high ratio of update transactions in the workload.
Notice that read-only transactions are in general more expensive than update transaction.
For this reason, the browsing mix starts at 22 tps on one replica, whereas the ordering mix starts at 45 tps.
As more replicas are added the cost of processing writesets (during update propagation) limits the scalability in the ordering mix.The predicted throughput curves from the model match the measured throughputs.
We find that the model captures the overhead of processing update transactions in the replicated system.
Figure 7 depicts the average response times for the three TPC-W mixes.
The x-axis is the number of replicas and yaxis is the average response time in millisecond (ms).
The response time curve for the browsing mix remains almost flat because there are a few update transactions.
We see an increase in the response time curve for the ordering mix.
The model predicts response times well for the three mixes.In summary, the multi-master performance estimates match well with the measured results for all workload mixes with an error margin below 15%.
Next, we analyze performance measurements and predictions for the singlemaster system.
Figure 8 plots the throughput of TPC-W browsing, shopping and ordering mixes as a function of the number of replicas for the single-master (SM) replicated database system.
In contrast to MM, update transactions are executed on the master only and the system saturates as soon as the master becomes the bottleneck.
Both the real system and the model scale linearly with the browsing mix since it is dominated by read-only transactions and the extra capac We present analytical models to predict the performance of two middleware-based database replication designs, multimaster and single-master.
The analytical models capture the characteristics of these systems in terms of update propagation overheads and abort rates.
We describe how to measure the system performance metrics on a standalone database and use these measurements as inputs to the analytical models.
For experimental validation, we use prototypes of both systems and show that the models match well with the measured system performance.
Performance predictions are within 15%.
We thank our shepherd, Fernando Pedone (University of Lugano), for his feedback and suggestions.
We also thank Tim Harris, Alexandre Proutiere, Bozidar Radunovic, Eno Thereska, and Milan Vojnovic (Microsoft Research in Cambridge), and the anonymous reviewers for their constructive comments.This research was partially supported by the Swiss National Science Foundation grant number 200021-121931 and by the Hasler Foundation grant number 2316.
