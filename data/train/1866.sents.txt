NUMA multicore machines are pervasive and many multithreaded applications are suffering from lock contention.
To mitigate this issue, application and library developers can choose from the plethora of optimized mu-tex lock algorithms that have been designed over the past 25 years.
Unfortunately, there is currently no broad study of the behavior of these optimized lock algorithms on realistic applications.
In this paper, we attempt to fill this gap.
We perform a performance study of 27 state-of-the-art mutex lock algorithms on 35 applications.
Our study shows that the case is not yet closed regarding locking on multicore machines.
Indeed, our conclusions include the following findings: (i) at its optimized contention level, no single lock is the best for more than 52% of the studied workloads; (ii) every lock is harmful for several applications , even if the application parallelism is properly tuned; (iii) for several applications, the best lock changes when varying the number of threads.
These findings call for further research on optimized lock algorithms and dynamic adaptation of contention management.
Today, multicore machines are pervasive and many multithreaded applications are suffering from bottlenecks related to critical sections and their corresponding locks.
To mitigate these issues, application and library developers can choose from the plethora of optimized mutex lock algorithms that have been designed over the past 25 years but there is currently no clear study to guide this puzzling choice for realistic applications.
In particular, the most recent and comprehensive empirical performance evaluation on multicore synchronization [9], due to its breadth (from hardware protocols to high-level data structures), only provides a partial coverage of locking algorithms.
Indeed, the aforementioned study only considers 9 algorithms, does not consider hybrid spinning/blocking waiting policies, omits emerging approaches (e.g., loadcontrol algorithms described in §2) and provides a modest coverage of hierarchical locks [14,5,6], a recent and efficient approach.
Furthermore, most of the observations are based on microbenchmarks.
Besides, in the case of papers that present a new lock algorithm, the empirical observations are often focused on the specific workload characteristics for which the lock was designed [21,26], or mostly based on microbenchmarks [14,12].
The present paper provides a broad performance study on Linux/x86 of 27 state-of-the-art mutex lock algorithms on a set of 35 realistic and diverse applications (the PARSEC, Phoenix, SPLASH2 suites, MySQL and an SSL proxy).
We make a number of observations, several of which have not been previously mentioned: (i) about 60% of the studied applications are significantly impacted by lock performance; (ii) no single lock is systematically the best, even for a fixed number of contending cores; (iii) worse, at their optimized contention level (individually tuned for each application), the best locks never dominate for more than 52% of the lock-sensitive applications; (iv) any of the locks is harmful (i.e., significantly inefficient compared to the best one) for at least several workloads; (v) across all the lock-sensitive applications, there is no clear performance hierarchy among the locks, even at a fixed number of contending cores; (vi) for a given application, the best lock varies according to both the number of contending cores and the machine; (vii) unlike previous recommendations [9] advocating that standard Pthread mutex locks should be avoided for workloads using no more than one thread per core, we find that, with our studied workloads, the current Linux implementation of these locks actually yields good performance for many applications with this pattern.
Moreover, we show that all these results hold even when each configuration, i.e., each (application, lock) pair, is tuned to its optimal degree of parallelism.
From our performance study, we draw two main conclusions.
First, specific lock algorithms should not be hardwired into the code of applications.
Second, the observed trends call for further research both regarding lock algorithms and runtime support for parallel performance and contention management.To conduct our study, manually modifying all the applications in order to retrofit the studied lock algorithms would have been a daunting task.
Moreover, using a meta-library that allows plugging different lock algorithms under a common API (such as liblock [26] or libslock [9]) would not have solved the problem, as this would still have required a substantial re-engineering effort for each application.
In addition, such meta-libraries provide no or limited support for important features like Pthread condition variables, used within many applications.
Therefore, we implemented LiTL 1 , a lowoverhead library that allows transparent interposition of Pthread mutex lock operations and support for mainstream features like condition variables, without any restriction on the application-level locking discipline.The remainder of the paper is organized as follows: §2 presents a taxonomy of existing lock designs and the list of algorithms covered by our study.
§3 describes our experimental setup and the studied applications.
§4 describes the LiTL library.
§5 exposes the main results from our empirical observations.
§6 discusses related works and §7 concludes the paper.
The body of existing works on optimized lock algorithms for multicore architectures is rich and diverse and can be split into the following five categories: 1) Flat approaches correspond to simple algorithms (typically based on one or a few shared variables accessed by atomic instructions) such as: simple spinlock [33], backoff spinlock [2,30], test and test-and-set (TTAS) lock [2], ticket lock [30], partitioned ticket lock [11], and standard Pthread mutex lock.
2) Queue-based approaches correspond to locks based on a waiting queue in order to improve fairness as well as the memory traffic, such as: MCS [30,33] and CLH [7,29,33].3) Hierarchical approaches are specifically aimed at providing scalable performance on large-scale NUMA machines, by attempting to reduce the rate of lock migrations among NUMA nodes.
This category includes HBO [32], HCLH [28], FC-MCS [13], HMCS [5], AHMCS [6] and the algorithms that stem from the lock cohorting framework [14].
A cohort lock is based on a combination 1 LiTL: Library for Transparent Lock interposition.
of two lock algorithms (similar or different): one used for the global lock and one used for the local locks (there is one local lock per NUMA node); in the usual C-L A -L B notation, L A and L B respectively correspond to the global and the node-level lock algorithms.
The list includes C-BO-MCS, C-PTL-TKT and C-TKT-TKT (also known as Hticket [9]).
The BO, PTL and TKT acronyms respectively correspond to backoff lock, partitioned ticket lock, and standard ticket lock.
4) Load-control approaches correspond to algorithms that aim at limiting the number of threads that concurrently attempt to acquire a lock, in order to prevent a performance collapse.
These algorithms are derived from queue-based locks.
This category includes MCSTimePub 2 [19] and so-called Malthusian algorithms like Malth Spin and Malth STP 3 [12].
5) Delegation-based approaches correspond to algorithms in which it is (sometimes or always) necessary for a thread to delegate the execution of a critical section to another thread.
The typical benefits expected from such approaches are improved cache locality and better resilience under high lock contention.
This category includes Oyama [31], Hendler [20], RCL [26], CC-Synch [15] and DSM-Synch [15].
Another important design dimension is the waiting policy used when a thread cannot immediately obtain a requested lock [12].
There are three main approaches: (i) spinning on a memory address, (ii) immediate parking (i.e., blocking the thread) either for a fixed amount of time or until the thread gets a chance to obtain the lock, and (iii) spinning-then-parking (STP), a hybrid strategy using a fixed or adaptive threshold [22].
The choice of the waiting policy is mostly orthogonal to the lock design but, in practice, policies other than pure spinning are only considered for certain types of locks: the queue-based (from categories 2-4 above) and the standard Pthread mutex locks.
Besides, note that the GNU C library for Linux provides two versions of Pthread mutex locks: the default one uses parking (via the futex syscall) and the second one uses an adaptive spin-thenpark strategy.
The latter version can be enabled with the PTHREAD MUTEX ADAPTIVE NP option [23].
Our choice of studied locks is guided by the decision to focus on portable lock algorithms.
We therefore exclude the following locks that require strong assumptions on 2 MCS-TimePub is mostly known as MCS-TP but we use MCTimePub to avoid confusion with MCS STP.3 Malth Spin and Malth STP correspond to MCSCR-S and MCSCR-STP, respectively, but we do not use the latter names to avoid confusion with other MCS locks.
Table 1: Hardware characteristics of the testbed platforms.the application/OS behavior, code modifications, or fragile performance tuning: HCLH, HBO, FC-MCS, and all the delegation-based locks (see Dice et al. [14] for detailed arguments).
Our study considers 27 mutex lock algorithms that are representative of both well-established and state-ofthe-art approaches.
We use the Spin and STP suffixes to differentiate variants of the same algorithm that only differ in their waiting policy.
The -LS tag corresponds to optimized algorithms borrowed from libslock [9].
Our set includes ten flat locks (Backoff, Partitioned ticket, Phtread, Pthread adaptive, Spinlock, Spinlock-LS, Ticket, Ticket-LS, TTAS, TTAS-LS), seven queue-based locks (Alock-LS, CLH-LS, CLH Spin, CLH STP, MCS-LS, MCS Spin, MCS STP), seven hierarchical locks (C-BO-MCS Spin, C-BO-MCS STP, C-PTL-TKT, C-TKT-TKT, Hticket-LS, HMCS, AHMCS), and three loadcontrol locks (Malth Spin, Malth STP, MCS-TimePub).3 Experimental setup and methodology Our experimental testbed consists of three Linux-based servers whose main characteristics are summarized in Table 1.
All the machines run the Ubuntu 12.04 OS with a 3.17.6 Linux kernel (CFS scheduler), glibc 2.15 and gcc 4.6.3.
For our comparative study of lock performance, we consider (i) the applications from the PARSEC benchmark suite (emerging workloads), (ii) the applications from the Phoenix 2 MapReduce benchmark suite, (iii) the applications from the SPLASH2 high-performance computing benchmark suite 4 , (iv) the MySQL database running the Cloudstone workload, and (v) SSL proxy, an event-driven SSL endpoint that processes small messages.
In order to evaluate the impact of workload changes on locking performance, we also consider so called "long-lived" variants of four of the above workloads denoted with a " ll" suffix.
Note that six of 4 We excluded the Cholesky application because of extremely short completion times.
the applications cannot be evaluated on the two 48-core machines because, by design, they only accept a number of threads that correspond to a power of two: facesim, fluidanimate (from PARSEC), fft, ocean cp, ocean ncp, radix (from SPLASH2).
Most of these applications use a number of threads equal to the number of cores, except the three following ones: dedup (3× threads), ferret (4× threads) and MySQL (hundreds of threads).
Two thirds of the applications use Pthread condition variables.
For the lock algorithms that rely on static thresholds, we use the recommended values from the original papers and implementations.
The algorithms based on a spinthen-park waiting policy (e.g., Malth STP [12]) rely on a fixed threshold for the spinning time that corresponds to the duration of a round-trip context switch [22] -in this case, we calibrate the duration using a microbenchmark on the testbed platform.All the applications are run with memory interleaving (via the numactl utility) in order to avoid NUMA memory bottlenecks.
Generally, in the experiments presented in this paper, we study the performance impact of a lock for a given contention level, i.e., the number of threads of the application.
We vary the contention level at the granularity of a NUMA node (i.e., 8 cores for the A-64 machine, 6 cores for the A-48 machine, and 12 cores for the I-48 machine).
For most of the experiments detailed in the paper, the application threads are not pinned to specific cores.
The impact of pinning is nonetheless discussed in §5.3.
Finally, each experiment is run at least five times and we compute the average value.
Overall, we observe little variability for most configurations.
For all experiments, the considered application-level performance metric is the throughput (operations per time unit).
In order to carry out the lock comparison study, we have developed LiTL, an interposition library for Linux/x86 allowing transparently replacing the lock algorithm used for Pthread mutexes.
We describe its design, implementation, and assess its performance.
The design of LiTL does not impose any restriction on the level of nested locking and is compatible with arbitrary locking disciplines (e.g., hand-over-hand locking [33]).
The pseudo-code of the main wrapper functions of the LiTL library is depicted in Figure 1.
General principles The primary role of LiTL is to maintain a mapping structure between an instance of the standard Pthread lock (pthread mutex t) and an instance of the chosen optimized lock type (e.g., MCS Spin).
This implies that LiTL must keep track of the lifecycle of all the application's locks through interposition of the calls to pthread mutex init() and pthread mutex destroy(), and that each interposed call to pthread mutex lock() must trigger a lookup for the instance of the optimized lock.
In addition, lock instances that are statically initialized can only be discovered and tracked upon the first invocation of pthread mutex lock() on them (i.e., a failed lookup leads to the creation of a new mapping).
The lock/unlock API of several lock algorithms requires an additional parameter (called "struct" hereafter) in addition to the lock pointer.
For example, in the case of an MCS lock, this parameter corresponds to the record to be inserted in (or removed from) the lock's waiting queue.
In the general case, a struct cannot be reused nor freed before the corresponding lock has been released.
For instance, an application may rely on nested critical sections (i.e., a thread T must acquire a lock L 2 while holding another lock L 1 ).
In this case, T must use a distinct struct for L 2 in order to preserve the integrity of L 1 's struct.
In order to gracefully support the most general cases, LiTL systematically allocates exactly one struct per lock instance and per thread.Supporting condition variables Dealing with condition variables inside each optimized lock algorithm would be complex and tedious as most locks have not been designed with condition variables in mind.
We therefore use the following strategy: our wrapper for pthread cond wait() internally calls the true pthread cond wait() function.
To issue this call, we need to hold a real Pthread mutex lock (of type pthread mutex t).
This strategy (depicted in the pseudocode of Figure 1) does not introduce high contention on the internal Pthread lock.
Indeed, for workloads that do not use condition variables, the Pthread lock is only requested by the holder of the optimized lock associated with the critical section.
Furthermore, workloads that use condition variables are unlikely to have more than two threads competing for the Pthread lock: the holder of the optimized lock and a notified thread.
Note that the latter claim also holds for workloads that rely on pthread cond broadcast() because the Linux implementation of this call only wakes up a single thread from the wait queue of the condition variable and directly transfers the remaining threads to the wait queue of the Pthread lock.Support for specific lock semantics The design of LiTL is compatible with specific lock semantics when the underlying lock algorithms offer the corresponding properties.
For example, LiTL supports non-blocking lock requests (pthread mutex trylock()) for all the currently implemented locks except CLH-based locks and Hticket-LS, which are not compatible with such semantics.Although not yet implemented, LiTL could easily support blocking requests with timeouts for the socalled "abortable" locks (e.g., MCS-Try [34] and MCSTimePub [19]).
Moreover, support for optional Pthread Figure 2: Performance comparison (throughput) of manually implemented locks (black bars) vs. transparently interposed locks using LiTL (white bars).
The throughput is normalized with respect to the best performing configuration for a given application (A-64 machine).
mutex behavior like reentrance and error checks 5 could be easily integrated in the generic wrapper code by managing fields for the current owner and the lock acquisition counter.
The library relies on a scalable concurrent hash table (CLHT [10]) in order to store, for each Pthread mutex instance used in the application, the corresponding optimized lock instance, and the associated perthread structs.
For well-established locking algorithms like MCS, the code of LiTL borrows from other libraries [9,1,26].
Other algorithms are implemented from scratch based on the description of the original papers.
For algorithms that are based on a parking or on a spinning-then-parking waiting policy, our implementation directly relies on the futex Linux system call.
Finally, the source code of LiTL relies on preprocessor macros rather than function pointers.
Indeed, we have observed that the use of function pointers in the critical path introduced a surprisingly high overhead.
Moreover, all data structures are cache-aligned in order to mitigate the impact of false sharing.
In this section, we assess the performance of LiTL using the A-64 machine.
To that end, we compare the performance (throughput) of each lock on a set of applications running in two distinct configurations: manually modified applications and unmodified applications using interposition with LiTL.
Clearly, one cannot expect to ob-5 Using respectively the PTHREAD MUTEX RECURSIVE and PTHREAD MUTEX ERRORCHECK attributes.
tain exactly the same results in both configurations, as the setups differ in several ways, e.g., with respect to the exercised code paths, the process memory layout and the allocation of the locks (e.g., stack-vs.
heap-based).
However, we show that between both configurations: (i) the achieved performance is close and (ii) the general trends for the different locks remain stable.We selected three applications: pca ll, radiosity ll and s raytrace ll.
These three applications are particularly lock-intensive and the last two use Pthread condition variables.
Therefore, all three represent an unfavorable case for LiTL.
Moreover, we focus the discussion on the results under the highest contention level (i.e., when the application uses all the cores of the target machine), as this again represents an unfavorable case for LiTL.
Figure 2 shows the normalized performance (throughput) of both configurations (manual/interposed) for each (application, lock) pair: black bars correspond to manually implemented locks, whereas white bars correspond to transparently interposed locks using LiTL.
In addition, Table 2 summarizes the performance differences for each application: number of locks for which each version performs better and, in each case, the average gain and the relative standard deviation.We observe that, for all of the three applications, the results achieved by the two versions of the same lock are very close: the average performance difference is below 5%.
Besides, Figure 2 highlights that the general trends observed with the manual versions are preserved with the interposed versions.
We thus conclude that using LiTL to study the behavior of lock algorithms in an application yields only very modest differences with respect to the performance behavior of a manually modified version.
Table 2: Detailed statistics for the performance comparison of manually implemented locks vs. transparently interposed locks using LiTL (A-64 machine).
In this section, we use LiTL to compare the behavior of the different lock algorithms on different workloads and at different levels of contention.
In the interest of space, we do not systematically report the observed standard deviations.
However, in order to mitigate the impact of variability, when comparing the performance of two locks, we consider a margin of 5%: lock A is considered better than lock B if B's achieved performance is below 95% of A's.
Besides, in order to make fair comparisons, the results presented for the Pthread locks are obtained using the same library interposition mechanism as with the other locks.
Note that some configurations are not tested because of specific restrictions.
First, streamcluster, streamcluster ll, and vips cannot use CLH-based locks or Hticket-LS as they do not support trylocks semantics.
Second, we omit the results for most locks with MySQL: given the extremely large ratio of threads to cores, most locks yield performance close to zero.
Third, some applications, e.g., dedup and fluidanimate, run out of memory for some configurations.Finally, for the sake of space, we do not report all the results for the three studied machines.
We rather focus on the A-64 machine and provide summaries of the results for the A-48 and I-48 machines.
Nevertheless, the entire set of results can be found in a companion technical report [18].
The section is structured as follows.
§5.1 provides preliminary observations that drive the study.
§5.2 answers the main questions of the study regarding the observed lock behavior.
§5.3 discusses additional observations.
Before proceeding with the detailed study, we highlight some important characteristics of the applications.
Table 3 shows two metrics for each application and for different numbers of nodes on the A-64 machine: the performance gain of the best lock over the worst one, as well as the relative standard deviation for the performance of the different locks.
For the moment, we only focus on the relative standard deviations at the maximum number of nodes (max nodes-highest contention) given in the 5th column (the detailed results from this table are discussed in §5.2.1).
We consider that an application is lock-sensitive if the relative standard deviation for the performance of the different locks at max nodes is higher than 10% (highlighted in bold font).
We observe that about 60% of the applications are impacted by locks.
We observe similar trends on the three studied machines (see Table 4).
In the remainder of this study, we focus on locksensitive applications.
90 95 95 87 92 92 84 79 94 90 90 88 89 85 109 84 89 125 88 107 87 105 102 107 97 114 81 70 103 124 121 89 92 96 73 87 75 111 114 82 45 103 72 73 234 49 136 60 106 173 ocean ncp 93 99 90 73 69 90 93 79 76 90 81 73 84 85 73 92 95 61 98 97 85 206 56 89 57 93 97 62 99 72 82 123 50 62 52 59 69 128 79 86 109 82 83 131 162 222 114 74 70 108 154 water nsquared water spatial 97 104 linear regression 44 227 12 21 132 67 45 34 7 49 44 15 25 8 51 47 24 50 10 8 38 8 21 27 matrix multiply 259 92 287 66 62 7 64 65 55 mysqld -- - - - - - - - - - - - - 25 - - - - - - - ocean cp In multicore applications, optimal performance is not always achieved at the maximum number of available nodes (abbreviated as max nodes) due to various kinds of scalability bottlenecks.
Therefore, for each (application, lock) pair, we empirically determine the optimized configuration (abbreviated as opt nodes), i.e., the number of nodes that yields the best performance.
For the A-64 and A-48 machines, we consider 1, 2, 4, 6, and 8 nodes.
For the I-48 machines, we consider 1, 2, 3, and 4 nodes.
Note that 6 nodes on A-64 and A-48 correspond to 3 nodes on I-48, i.e., 75% of the available cores.The results for the A-64 machine are displayed in Table 5.
For each (application, lock) pair, the corresponding cell indicates the performance gain of the optimized configuration with respect to the max-node configuration.
The background color of a cell indicates the number of nodes for the optimized configuration.
In addition, Table 6 provides a breakdown of the (application, lock) pairs according to their optimized number of nodes for all machines.We observe that, for many applications, the optimized number of nodes is lower than the max number of nodes.
Moreover, we observe ( Table 5) that the performance gain of the optimized configuration is often extremely large.
This confirms that tuning the degree of parallelism has frequently a very strong impact on performance.
We also notice that, for some applications, the optimized number of nodes varies according to the chosen lock.
Table 6: Breakdown of the (application, lock) pairs according to their optimized number of nodes (all machines).
In light of the above observations, the main questions investigated in the study ( §5.2) will be considered from two complementary angles: (i) comparing locks at a fixed number of nodes, and (ii) comparing locks at their optimized configurations (i.e., with possibly a different number of nodes for each).
The first angle offers insight for situations in which the degree of parallelism cannot be adjusted, while the second is useful for scenarios in which more advanced application tuning is possible.
5.2.1 How much do locks impact applications?
Table 3 shows, for each application, the performance gain of the best lock over the worst one at 1 node, max nodes, and opt nodes for the A-64 machine.
The table also shows the relative standard deviation for the performance of the different locks.We observe that the impact of locks on the performance of applications depends on the number of nodes.
At 1 node, the impact of locks on lock-sensitive applications is moderate.
More precisely, most applications exhibit a gain of the best lock over the worst one that is lower than 30%.
In contrast, at max nodes, the impact of locks is very high for all lock-sensitive applications.
More precisely, the gain brought by the best lock over the worst lock ranges from 42% to 3343%.
Finally, at the optimized number of nodes, the impact of locks is high, but noticeably lower than at max nodes.
We explain this difference by the fact that, at max nodes, some of the locks trigger a performance collapse for certain applications (as shown in Table 5), which considerably increases the observed performance gaps between locks.
We observe the same trends on the A-48 and I-48 machines (see the companion technical report [18]).
Table 7 shows the coverage of each lock, i.e., how often it stands as the best one (or is within 5% of the best) over all the studied applications for the A-64 machine.
The results are shown for three configurations: 1 node, max nodes, and opt nodes.
Besides, Table 8 Table 7: For each lock, fraction of the lock-sensitive applications for which the lock yields the best performance for three configurations: 1 node, max nodes, and opt nodes (A-64 machine).
Table 8: Statistics on the coverage of locks for three configurations: 1 node, max nodes, and opt nodes (all machines).
We make the following observations (Table 8).
No lock is among the best for more than 89% of the applications at 1 node and for more than 52% of the applications both at max nodes and at the optimal number of nodes.
We also observe that the average coverage is much higher at 1 node than at max nodes, and slightly higher at the optimized number of nodes than at max nodes.
This is directly explained by the observations made in §5.2.1.
First, at 1 node, locks have a much lower impact on applications than in other configurations and thus yield closer results, which increases their likelihood to be among the best ones.
Second, at max nodes, all of the different locks cause, in turn, a performance collapse, which reduces their likelihood to be among the best locks.
This latter phenomenon is not observed at the optimized number of nodes.
We observe the same trends on the A-48 and I-48 machines (see the companion technical report [18]).
Table 9 shows pairwise comparisons for all locks, at max nodes on the A-64 machine.
In each table, cell (rowA, colB) contains the score of lock A vs. lock B, i.e., the percentage of applications for which lock A is at least 5% better than lock B. For example, Table 9 shows that for 38% of the applications, AHMCS performs at least 5% better than Backoff at the optimized number of nodes.
Similarly, the table shows that Backoff is at least 5% better than AHMCS for 29% of the applications.
From these two values, we can conclude that the two above mentioned locks perform very closely for 33% of the applications.
At the end of each line (resp.
column), the table also shows the mean of the fraction of applications for which a lock is better (resp.
worse) than others.
Besides, the latter two metrics are summarized for the three machines in Table 10.
We observe that there is no clear global performance hierarchy between locks.
More precisely, for most pairs of locks (A, B), there are some applications for which A is better than B, and vice-versa ( Table 10: For each lock, at the optimized number of nodes, mean of the fraction of applications for which the lock is better (resp.
worse) than other locks (all machines).
never yields better performance than B.
The results at max nodes (not shown due to lack of space) exhibit similar trends as the ones at opt nodes.
Besides, we make the same observations (both at opt nodes and max nodes) on the A-48 and I-48 machines (see the companion technical report [18]).
Our goal is to determine, for each lock, if there are applications for which it yields substantially lower performance than other locks and to quantify the magnitude of such performance gaps.
Table 11 displays, for the A-64 machine, the performance gain brought by the best lock with respect to each of the other locks for each application at max nodes (top part) and at the optimized number of nodes for each lock (bottom part).
For example, the top part of the table shows that for the dedup application, the best lock (0%, here Spinlock-LS) is 598% better than the Alock-LS lock.
The gray cells highlight values greater than 15%.
Thus, for each lock in a column, the number of grey cells corresponds to the number of applications for which the lock is beaten by a gap of 15% or more by the best lock(s) for this application.
In addition, Table 12 displays, for each machine, the fraction of applications that are significantly hurt by a given lock.On the three machines, we observe that, both at max 228 24 20 108 57 31 62 0 52 28 11 17 0 49 46 56 3 39 15 0 83 15 32 9 31 18 37 22 16 27 38 38 24 29 29 15 23 27 27 43 32 0 24 11 19 129 5 55 5 38 81 ocean ncp 27 28 29 30 9 25 27 28 12 28 16 10 20 22 14 36 37 11 29 31 27 118 0 25 2 29 93 pca 65 69 155 46 357 61 48 220 40 38 59 39 38 0 43 58 214 23 45 110 39 252 75 110 23 157 112 pca ll 47 38 251 24 664 25 51 511 30 24 41 0 18 36 17 50 526 15 27 206 68 584 128 128 17 241 47 801 9 2k 50 16 2k 35 45 3 28 59 63 62 12 2k 44 76 567 267 2k 396 614 82 1k 18 3k 96 87 3k 68 169 0 164 84 291 99 69 3k 111 157 639 335 2k 428 813 332 1k 1k ssl proxy 0 18 532 1 1k 47 16 879 9 41 379 20 16 35 43 47 900 29 36 293 153 47 6 29 0 37 53 0 89 106 82 92 93 0 56 46 35 15 3 26 38 21 19 30 0 33 32 16 14 32 Table 11: For each application, at max nodes (top part) and at the optimized number of nodes (bottom part), performance gain (in %) obtained by the best lock(s) with respect to each of the other locks.
The grey background highlights cells for which the performance gains are greater than 15%.
A line with many gray cells corresponds to an application whose performance is hurt by many locks.
A column with many gray cells corresponds to a lock that is outperformed by many other locks.
Dashes correspond to untested cases.
(A-64 machine).19 49 matrix multiply 9 559 5 26 7 18 9 3 24 136 608 642 5 3 639 27 2 0 33 3 3 5 637 3 633 5 630 mysqld -- - -30 - - - - - - - - 0 - - 7 173 -97 102 - - - - - - ocean cpnodes and at the optimal number of nodes, all locks are potentially harmful, yielding sub-optimal performance for a significant number of applications (Table 12).
We also notice that locks are significantly less harmful at the optimized number of nodes than at max nodes.
This is explained by the fact that several of the locks create performance collapses at max nodes, which does not occur at the optimized number of nodes.
Moreover, we observe that, for each lock, the performance gap to the best lock can be significant (Table 11).
Impact of the number of nodes.
Table 13 shows, for each application on the A-64 machine, the number of pairwise changes in the lock performance hierarchy when the number of nodes is modified.
For example, in the case of the facesim application, there are 18% of the pairwise performance comparisons between locks that change when moving from a 1-node configuration to a 2-node configuration.
Similarly, there are 95% of pairwise comparisons that change at least once when considering A-64 A-48 I-48 Lock Max Opt Max Opt Max Opt ahmcs 62% 24% 56% 39% 39% 33% alock-ls 87% 39% 61% 39% 58% 58% backoff 61% 35% 68% 53% 58% 53% c-bo-mcs spin 61% 35% 53% 58% 47% 32% c-bo-mcs stp 71% 38% 80% 65% 55% 45% clh-ls 84% 37% 73% 40% 69% 62% clh Table 12: For each lock, at max nodes and at the optimized number of nodes, fraction of the applications for which the lock is harmful (all machines).
the 1-node, 2-node, 4-node and 8-node configurations.
We observe that, for all applications, the lock performance hierarchy changes significantly according to the chosen number of nodes.
Moreover, we observe the same trends on the A-48 and I-48 machines (see the companion technical report [18] Table 13: For each application, percentage of pairwise changes in the lock performance hierarchy when changing the number of nodes (A-64 machine).
Impact of the machine.
Table 14 shows the number of pairwise lock inversions observed between the machines (both at max nodes and at the optimized number of nodes).
More precisely, for a given application at a given node configuration, we check whether two locks are in the same order or not on the target machines.
We observe that the lock performance hierarchy changes significantly according to the chosen machine.
Interestingly, we observe that there is approximately the same number of inversions between each pair of machines.
A note on Phtread locks.
The various results presented in this paper show that the current Linux Pthread locks perform well (i.e., are among the best locks) for a significant share of the studied applications, thus providing a different insight than recent results, which were mostly based on synthetic workloads [9].
Beyond the changes of workloads, these differences may also be explained by the continuous refinement of the Linux Pthread implementation.
It is nevertheless important to note that on each machine, some locks stand out as the best ones for a higher fraction of the applications than Pthread locks.
Finally, we note that Pthread adaptive locks perform slightly better than standard Pthread locks.Impact of thread pinning.
As explained in §3.2, all the above-described experiments were run without any restriction on the placement of threads, leaving the corresponding decisions to the Linux scheduler.
However, in order to better control CPU allocation and improve locality, some developers and system administrators use pinning to explicitly restrict the placement of each thread to one or several core(s).
The impact of thread pinning may vary greatly according to workloads and can yield both positive and negative effects [9,27].
In order to assess the generality of our observations, we also performed the complete set of experiments with an alternative configuration in which each thread is pinned to a given node, leaving the scheduler free to place the thread among the cores of the node.
Note that for an experiment with a N-node configuration, the complete application runs on exactly first N nodes of the machine.
We chose thread-tonode pinning rather than thread-to-core pinning because we observed that the former generally provided better performance for our studied applications, especially the ones using more threads than cores.
The detailed results of our experiments with thread-to-node pinning are available in the companion technical report [18].
Overall, we observe that all the conclusions presented in the paper still hold with per-node thread pinning.
The design and implementation of the LiTL lock library borrows code and ideas from previous open-source toolkits that provide application developers with a set of optimized implementations for some of the mostestablished lock algorithms: Concurrency Kit [1], liblock [25,24,26], and libslock [9].
All of these toolkits require potentially tedious source code modifications in the target applications, even in the case of algorithms that have been specifically designed to lower this burden [3,33,36].
Moreover, among the above works, none of them provides a simple and generic solution for supporting Pthread condition variables.
The authors of liblock [26] have proposed an approach but we discovered that it suffers from liveness hazards due to a race condition.
Indeed, when a thread T calls pthread cond wait(), it is not guaranteed that the two steps (releasing the lock and blocking the thread) are always executed atomically.
Thus, a wake-up notification issued by another thread may get interleaved between the two steps and T may remain indefinitely blocked.
Several research works have leveraged library interposition to compare different locking algorithms on legacy applications (e.g., Johnson et al. [21] and Dice et al. [14]) but, to the best of our knowledge, they have not publicly documented the design challenges to support arbitrary application patterns, nor disclosed the corresponding source code and the overhead of their interposition library has not been discussed.Several studies have compared the performance of different multicore lock algorithms, either from a theoretical angle or based on experimental results [4,33,9,24,14].
In comparison, our study encompasses significantly more lock algorithms and waiting policies.
Moreover, the bulk of these studies is mainly focused on characterization microbenchmarks while we focus instead on workloads designed to mimic real applications.
Two noticeable exceptions are the work from Boyd-Wickizer et al. [4] and Lozi et al. [26] but they do not consider the same context as our study.
The former is focused on kernel-level locking bottlenecks, and the latter is focused on applications in which only one or a few heavily contended critical sections have been optimized (after a profiling phase).
For all these reasons, we make observations that are significantly different from the ones based on all the above-mentioned studies.
Other synchronization-related studies like the one from Gramoli [16] have a different scope and focus on concurrent data structures, possibly based on other facilities than locks.Finally, some tools have been proposed to facilitate the identification of locking bottlenecks in applications [35,8,26].
These publications are orthogonal to our work.
We note that, among them, the profilers based on library interposition can be stacked on top of LiTL.
Optimized lock algorithms for multicore machines are abundant.
However, there are currently no clear guidelines and methodologies helping developers to select the right lock for their workloads.
In this paper, we have presented a broad study of 27 locks algorithms with 35 applications on Linux/x86.
To perform that study, we have implemented LiTL, an interposition library allowing the transparent replacement of lock algorithms used for Pthread mutex locks.
From our study, we draw several conclusions, including the following ones: at its optimized contention level, no single lock dominates for more than 52% of the lock-sensitive applications; any of the locks is harmful for at least several applications; for a given application, the best lock varies according to both the number of contending cores and the machine that executes the application.
These observations call for further research on optimized lock algorithms, as well as tools and dynamic approaches to better understand and control their behavior.The source code of LiTL and the data sets of our experimental results are available online [17].
We thank the anonymous reviewers and our shepherd, Tim Harris, for their insightful comments on ealier drafts of this paper.
Dave Dice provided detailed answers for our questions on Malthusian locks.
Baptiste Lepers provided valuable insights for some of the case studies.
Pierre Neyron provided his help to set up experiments on the I-48 machine.
Finally, this work has been partially supported by: LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01), EmSoc Replicanos and AGIR CAEC projects of Université Grenoble-Alpes and GrenobleINP, and the INRIA/LIG Digitalis project.
