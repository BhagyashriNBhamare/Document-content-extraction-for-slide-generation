Edge computing is an emerging computing paradigm where data is generated and processed in the field using distributed computing devices.
Many applications such as real-time video processing, augmented/virtual reality gaming, environment sensing, benefit from such decentralized, close-to-user deployments where low-latency, real-time results are expected.
As with any distributed application, one of the key challenges in the development of collaborative applications is how to efficiently share data and state among multiple edge clients.
The dynamic and heterogeneous environment together with diverse application's requirements make data sharing at the edge a challenging problem.
Although there have been prior efforts, a systematic understanding of the area is missing.
In this paper, we conduct a methodological study of different edge applications, their data sharing needs, and designs of state-of-the-art systems.
In the process, we identify design options, under-explored opportunities, and associated challenges.
We then present Griffin, our edge data sharing service, and seek feedback on its design.
In recent years, we have witnessed a computing shift outwards from a data center to the network "edge", incubating a new paradigm called edge computing.
In edge computing, a large amount of data is generated and consumed in the field (i.e., data locality) outside the data center by various edge applications.
While cloud computing infrastructure services within a data center enabled flexible and economic pay-asyou-go solutions with automatic resource management, edge computing is growing due to its high potential to have immediate real-world deployments and impact in everyday situations [94], such as driving, farming, manufacturing, logistics, and monitoring.
As a result, many frameworks are developed that propose new edge-friendly abstractions and APIs for applications [4,43,76,81,82,95,96].
One of the key challenges in the development of applications at the edge is how to efficiently share data 1 among clients.
This data sharing is necessary to build applications such as collaborative machine learning (ML) [87,100,101], AR/VR gaming [23,64,117,119], autonomous driving [18,55,59,67,118], video analytics [44,51,52,70], and distributed environment sensing [9,38,105,113].
A hallmark of these applications is that multiple clients work on a shared dataset and coorporate to achieve a common objective [62].
Data sharing can be managed within individual application frameworks or through an external storage service.
The latter is a more popular choice at the edge because it allows extending the familiar RESTful/serverless abstraction from the cloud to the edge [48,75,90].
The use of an external, shared data store cleanly decouples compute from state management, leading to a stateless, fault-tolerant computation framework [14,104].
The separation of state and compute is not a new idea.
Within data centers there has already been a push to maintain ephemeral data and application-specific state in first-class storage services [57,89,99].
There exists more than half a dozen key-value storage, file systems, and database services that cater to various shared state and data storage needs [3,5,41,42,77].
In comparison, platforms for edge storage services are still in their infancy.
Furthermore, edge infrastructure presents new challenges ( §1.1) for efficient data sharing.In this paper, we present a first-of-its-kind systematic study of data sharing requirements at the edge ( §2) and how proposed systems (fail to) fulfill them ( §3).
We then identify under-addressed concerns (e.g., heterogeneity, mobility), analyze trade-offs, present the design of Griffin, our edge data sharing service ( §4), and conclude with discussion points ( §5).
We adopt a definition of "edge" similar to the one from ETSI [36], where an edge platform consists of edge sites which can be gateway node(s) [24,28] and/or local server machines (as done in Farmbeats [105]) that connect to the Internet, or edge station(s) such as a cell tower, a base station, or a roadside units (RSUs).
An edge site can contain multiple edge nodes.
End-devices, such as mobile devices, wearable sensors, IoT nodes, and smart cars, will connect to the edge nodes for storage/compute services using 4/5G, WAN, and WiFi (including V2X and IEEE 802.11p standards) technologies [88] as shown in Figure 1.
We expect one or more edge nodes will be involved between an end-device and the nearest data center.
We assume fixed edge sites, but with limited resources that decrease as we move closer to end-devices: small few-core CPUs, limited DRAM, and little permanent storage [88].
In this environment, a data center state or storage service (such as geo-distributed storage services) faces multiple challenges because of the following properties [25]:• Distributed.
Edge sites are dispersed geographically and do not have a centralized, equi-distant node to run the server service (such as the namenode in HDFS).
For more decentralized approaches, such as Cassandra [61], a previous study has shown that the lack of a centralized administrative node makes consistent metadata management (membership, keyspace) in the presence of client mobility a challenging task [25].
Furthermore, typical multi-RTT application-level protocols (e.g., MySQL protocols) can diminish many edgerelated latency gains for applications [86].
• Heterogeneous.
Edge sites/nodes can be heterogeneous in terms of their storage and network capabilities.
For example, an embedded edge node (e.g., colocated at a WiFi access point) with a Jetson board is typically equipped with a 16/32 GB eMMC, while an edge site can host 10-100s of servers with few TB storage capacities each.
Edge network links might be bandwidth constrained and not symmetric, thus presenting challenges for cross-site traffic for achieving consistency.
A large quantity of metadata to track resource heterogeneity is required, which needs to be handled efficiently and in a scalable manner.
• Dynamic.
The edge environment can be unpredictable (interferences) and dynamic (changing conditions due to mo- Edge ML/DL [87,100,101] model parameters, training data Gaming (AR/VR) [23,64,117,119] user profiles, game world and state Autonomous cars [18,55,59,67,118] LiDAR data, maps, ML models Edge analytics [6,10,31,34] aggregation states, ephemeral datasets Video analytics [44,51,52,70] frames, execution state, objects Sensing [9,38,105,113] environment, health, sensor data Table 1: Overview of shared data types in edge applications.bility, diurnal patterns) with multiple administrative domains.
Thus, keeping up-to-date information regarding the infrastructure, its usage, and monitoring can be challenging.
Edge applications are diverse, and so are their data sharing needs.
In this section we focus on distributed and collaborative edge applications and synthesize their shared data types, sizes, mobility patterns, consistency and performance requirements.
Table 1 provides an overview.
Edge machine learning.
Machine learning represents one of the most popular applications between the cloud and edge [87,100,101].
Here, the model parameters represent the distributed shared state that is read and updated by multiple participants (typically) using the distributed parameter server approach [19,60,63].
Further research has also demonstrated the additional value of doing collaborative, federated learning and transfer of knowledge between multiple nodes [69,100,110].
Typically, a model size would be 10-100 MBs and would require 1-100 ms (depending upon the network technology) of access and update latencies [50,54,87,100].
There is also a large body of work in model compression, splitting, and network-efficient communication [17], which is orthogonal to this work.
The expected state mobility is low to medium based on the client mobility.Real-time massively multiplayer online game (MMOG).
Collaborative, real-time MMOGs have multiple geo-dispersed players that share and interact within a common game world, potentially AR/VR augmented, e.g., Pokémon Go.
An edgeconscious deployment can help deliver a smooth gameplay by meeting the strict latency and bandwidth requirements for shared world accesses from players [23,32,64,117,119].
In a streaming based setup, the interaction events can be a few kilobytes, with recommended streaming bandwidth in 1-10 Mbps and access delay tolerance of tens of millisecond [66].
Furthermore, different shared state types in a game environment can tolerate different consistency levels (strong, causal, read-your-writes, or eventual) to deliver the best gaming experience [29,30,109].
For example, a player-local view data needs to be refreshed very quickly with a strong consistency (within 20 ms [12]) in response to events, whereas shared game/world state updates can tolerate higher latencies (~100 ms) [117].
Based on the nature of the game (client mobility, co-locating, load balancing) medium-to-high dynamic Abstraction/API Locality Heterogeneity Mobility Failover Scalability App.
Semantics PathStore [80,81] relational/CQL [7] session/eventual FogStore [46] key-value context-aware DataFog [47] key-value eventual RedWedding [75] CRDT [97] conflict-free DPaxos [84] transactions quorum-based EdgeCons [49] events quorum-based Timeseries DBs [2,115] timeseries range, aggregateApp-specific edge storage Cachier [34] objects, content N/A Vision-specific [91] key-frames, feature vectors N/A : full support : partial support : no support : unknown Table 2: Requirements analysis of existing edge storage systems.state migration is needed.
Mobile to cloud offloading of games already exists [53].
Within the edge, the EC+ gaming architecture models client mobility using a Markov decision process and proactively moves the game state [117].
Cooperative autonomous driving.
Autonomous driving applications can be deployed in a collaborative manner where federated learning between multiple cars can be possible to mark hazards, predict traffic in real-time, share models, and transfer learning about a driver's profile [18,55,59,118].
Highprecision, environment condition maps and LiDAR data are other datasets that can be maintained in a cooperative manner between multiple vehicles [22,114].
Depending upon the volatility in the environment conditions, the dataset will be frequently updated, shared, and needs strong consistency.
As autonomous vehicles have a strict time budget to make decisions (100 ms@10 Hz [65]), it is important that datasets are accessed in a predictable, low-latency manner.
Depending on the type of the dataset, we expect none (e.g., neighborhood condition map) to high (e.g., a driver's profile) mobility.Others.
Beyond these key domains, there is a large body of work in analytics [6], stream processing [31], video analytics [44,51,52,70], caching/CDN [33,34,108], etc.
Stream applications are developed to prevent DDoS attacks that utilize malfunctioning/hacked IoT devices by sharing the monitoring state [10].
Here, the edge represents the first line of defense, but not a single edge site can observe all the traffic, and deduce that there is a DDoS attack underway.
Hence, it is important that edge sites collectively maintain a shared network patterns state that can be used to identify an attack.
For analytics, serverless has emerged as a popular choice (e.g., AWS IoT Greengrass, and others [48,75,90]).
In the stateless serverless functions, data is shared though an external data store [57], which has unique properties [56].
Such a model is also being proposed for stream processing systems [71,73].
Summary.
Edge applications are diverse, and have a wide set of requirements for their state management.
Instead of letting each application have to reinvent the wheel, there is an opportunity to build a common, unified shared data management service for the edge.
We synthesize eight storage requirements for the aforementioned edge applications and conduct a comprehensive analysis (see Table 2) of the existing storage systems by focusing on how well they fulfill these requirements.
RQ1: Abstractions and APIs.
Edge applications deal with different data types, e.g., text, images, and videos, with associated access and consistency semantics.
Apart from the traditional RDBMS with SQL interfaces [8,27,98], the simple key-value interface (get/put) has been the most popular and versatile interface used by many data center stores [11,46,61].
Some edge stores also use the key-value interface for its simplicity [46,47].
There are others that adopt heavy transactional semantics [49,84].
However, works in other domains such as ML have shown that having an expressive storage API is beneficial to run common operations, like data reduce, close to the state instead of always pulling/pushing raw values [13,68].
Such domain-specific data abstractions and APIs could make edge applications more efficient by relieving both the computation and the network traffic burden.
Time series data is also popular in edge applications like environment monitoring/sensing.
Cloud timeseries databases (TSDBs) such as OpenTSDB [2] need support from backend storage services such as HDFS.
For edge-ready TSDBs [111,115] the focus has been on efficient index building, stream merging, and range queries computations.
RQ2: Data locality.
The proximity to data in edge computing helps reduce network latency, save network bandwidth via data filtering, and protect privacy.
Many existing edge stores [75,84] borrow ideas from geo-distributed storage systems such as Cassandra [61] and Spanner [27] that typically achieve data locality by careful replica placement and request dispatching across the distributed infrastructure [11,112].
PathStore [81] adopts a hierarchical architecture that achieves data locality by serving requests from the nearest edge node by partially replicating data sets there.
FogStore [46] and DataFog [47] perform location-aware replica placement by leveraging spatio-temporal encoding (e.g., ST-Hash [45]) to partition the data.
In essence, data partitioning and replica placement on edge nodes/sites should be carefully chosen and continuously adapted following client mobility.
RQ3: Heterogeneity.
Edge nodes can have very diverse hardware specifications, as do edge sites hosting different numbers of edge nodes.
Unfortunately, existing edge stores mostly run under the infinite-homogeneous-resource assumption and only rarely draw attention to the implications of heterogeneity [46].
One exception is DataFog [47], which explicitly addresses resource scarcity at the edge through TTLbased data eviction and data aggregation and compression.
The design of PathStore inherently handles heterogeneity by caching only partial replicas on edge nodes at the expense of high fetching cost in case of cache misses [81].
However, none of these systems explicitly consider the load, capacity, and processing power available at the edge in their replica placement decision.
Such constraints can be very dynamic due to frequent capacity reclamation or client movements.
RQ4: Mobility.
Similar to heterogeneity, mobility is a unique feature of the edge environment, which mainly comes from the movement of end-devices such as smartphones and autonomous vehicles.
Existing edge storage systems are mostly mobility-agnostic [46,47,49,75,81,84].
DPaxos treats client mobility as a non-imminent issue and relies on reactive data migration [84].
PathStore partially supports client mobility by tracking state changes (logging CQL queries) and reactively updating the replica when a client reestablishes the network connection to a new replica with session consistency guarantees [80].
There are generally two ways of addressing mobility: (1) Reactive methods constantly monitor the client's movement through the wireless network connection by tracking the real-time signal strength (e.g., RSSI) [83,120].
(2) Proactive methods try to build a model for the client mobility pattern and then take precautionary actions according to model-generated predictions [16,117].
RQ5: Failure, partitions, and limited connectivity.
We expect limited and high cost of communication between edge nodes, hence building a conventional peer-to-peer system would not be useful as (1) all edge peers do not have equal storage, network and processing capabilities and (2) peer pairs may have limited connectivity.
Such a setup also necessitates a careful placement of centralized components of data management protocols (e.g., Paxos leaders, primary backup) where one node has more responsibilities than others.
Existing edge stores mostly rely on the default data replication feature for fault tolerance and generally follow the CAP theorem.
PathStore handles temporary node failures by retrying to propagate timestamped writes to node ancestors for merging [80,81].
Under network partitions, PathStore keeps serving write requests with local replicas and also read requests if the data is replicated in the local cache.
FogStore handles geographically correlated failures by placing the replicas out of the context of interest (where end-devices are) far away from the others [46].
DPaxos handles client-specified failure rates at both the edge site level and the zone (a collection of edge sites) level [84].
Konwar et al. explore the use of erasure codes between edge nodes and cloud for a two-layered distributed edge store [58].
RedWedding leverages CRDT data types to allow concurrent accesses in a partitioned state, and then eventually reaches a common final state [97].
RQ6: Scalability and load balancing.
A single edge store can be deployed across hundreds of edge sites (e.g., cell towers within a large city) with small capacities, several thousands of clients, with millions of objects.
So far, scalability has been considered as a first-class citizen in most edge store designs.
FogStore [46] and DataFog [47] adopt the spatio-temporal hashing which balances requests across edge nodes.
In addition, DataFog uses neighbor edge nodes for offloading when hotspots are found in the system.
PathStore [80,81] scales well with the hierarchical architecture, but it does not address request load balancing issues.
RedWedding store proposes to create a storage instance along with every serverless function invocation to achieve elastic concurrency [75].
An ideal edge storage system should be able not only to scale to enormous edge nodes/sites (needs efficient metadata management), but also to handle the read/write requests imbalance.
RQ7: Application-specific semantics.
As we saw before, different edge applications have different consistency semantics.
These requirements might be contextual (game vs. player data in online gaming), location-based (distance from a data source), or state dependent (reading a configuration vs. training data).
Most existing edge stores adopt a single semantic, i.e., a single consistency model such as strong [49,84] or eventual consistency [47], for all target applications.
PathStore provides session consistency on top of eventual consistency [80].
FogStore allows applications to specify contexts of interest and apply differential consistency based on the context [46].
Ideally, an edge store would allow applications to easily attach multiple consistency semantics on stored data sets, thus dynamically trading performance with consistency to match the dynamic nature of edge deployments.
RQ8: Monitoring infrastructure.
Considering an edge store is expected to operate in a highly dynamic environment, we expect the store to either leverage or provide good monitoring capabilities to identify environment changes, node mobility, or failures.
So far, monitoring an edge store has been overlooked in the literature.
CloudPath provides a monitoring module to collect statistics about CPU and memory metrics from the edge system [81], but it is not clear how the system leverages this information for data store optimization.
Having discussed requirements and various state-of-the-art stores proposed in the literature, we now present our initial design sketch for Griffin, a shared storage service for the edge, and seek feedback on further work in §5.
Overview.
Griffin is a multi-layered hierarchical distributed storage service.
It uses a client-server model where every edge site and node runs a data storage daemon.
Data creation and placement decisions are taken in the cloud (centralized resource provisioning), and then read/write accesses are done by the client.
A client bootstraps by first contacting a well-known service address in the cloud.
Similar to DataFog [47], Griffin ensures data locality (RQ2) and scalability (RQ6) by including space-time labels to data items and employing spatiotemporal encoding (e.g., the Hilbert's curve [93]) for data partitioning and load balancing.
Griffin relies on data replicas and careful replica placement for node and geographically correlated failures, respectively (RQ5).
In addition, Griffin's design aims to answer the following questions:What are the right data storage abstraction (RQ1) and consistency model (RQ7)?
Griffin is a multi-consistency storage system.
However, defining the consistency and latency trade-off can be a tedious job.
We take inspiration from the Pileus storage service [102] which supports multiple consistency models with a declarative policy interface.
Instead of having to choose one consistency model at the development time, a developer can specify their expected SLA latencies, desired consistency models (multiple are possible, e.g., eventual consistency can also be satisfied with a monotonic read), and their utilities in a declarative manner.
The system dynamically optimizes to deliver the best utility for every data access.
Building such a system at the edge presents challenges associated with: (1) How to reliably predict loads and latencies in the edge environment?
Such information is necessary for a client to determine which consistency model can be satisfied under the current operational environment.
There has been a large body of work in data center sensing, monitoring, and modeling congestion and RTTs [40,78].
Our on-going work is looking into validating and extending the work to the edge.
(2) Can we generate accurate timestamps for providing ordering guarantees between concurrent accesses?
Previous research has shown that it is possible to build a bounded clock abstraction in data centers [27,39].
Edge stores like PathStore expect such abstractions to be available at the edge.
D'souza et al. have proposed Time-as-a-Service (TaaS) middleware for sub-millisecond clock synchronization (using a mix of NTP, PTP, and Huygens [39] protocols) at the geo-scale infrastructure [35].
However, it expects certain underlying infrastrucutre support (e.g., hardware timestamping, Zookeeper, and pub/sub services).
Mani et al. have shown that clock drifts, stability, and errors at the IoT/Edge environment exhibit very a different behavior than the server clock, thus needing a new clock synchronization mechanism [72].
Their algorithm, SPoT, can synchronize the clock of many heterogenous IoT devices with a cloud server within an accuracy of ~15 ms. We are investigating the use of SPoT in Griffin to generate timestamps.
(3) How to reduce high (de)serialization costs associated with the key-value abstraction?
Here we propose to use lightweight in-memory data formats as the storage format (standardized Apache Arrow [1], or high-performance formats like Albis [103]).
This helps to reduce the cost of conversion between multiple languages and frameworks [79].
How to handle heterogeneity at the edge (RQ3)?
Griffin incorporates a graph-based optimization engine for data replication and placement, taking into account the capability constraints of edge nodes/sites.
Similar approaches have been used for managing edge compute resources [20,85,106,113].
Griffin assumes that edge sites are fixed and builds an annotated graph of the edge infrastructure to capture the system status.
A vertex in the graph represents an edge site consisting of a set of edge nodes, annotated with properties including resource specification and utilization.
A link in the graph represents a connection hop between edge sites, annotated with properties such as per-hop network latency and bandwidth.
Note that some of the properties on the graph, such as the resource (CPU, DRAM, and storage capacity) specification, are static, while others including real-time load, utilization, and the number of connected clients are dynamic.
Applications will also be modeled through annotated graphs of data flow with performance constraints (e.g., required storage capacity, SLA latency) specified on both vertices and links.
Essentially, the task of data replication and placement can be transformed into mapping the data flow graph of applications to the infrastructure graph.
This problem is similar to the conventional virtual network embedding problem [37] and we will leverage its recent advancements [92] to solve this problem.
Through the graph-based models and optimization, the heterogeneity of edge sites/nodes will be addressed inherently.
How to support system dynamics imposed by client mobility (RQ4, RQ8)?
Griffin includes an in-band monitoring system which continuously collects and reports system statistics for the optimization engine to build the graphs and then adapts system configurations in response to system dynamics.
For the graph-based optimization engine to work properly, at least the following system status information should be gathered at runtime: (1) the current compute/storage utilization on edge nodes, (2) the network bandwidth and latency between edge nodes/sites, (3) the quality of the (wireless) connection to the end-devices of users.
The first two are to be used directly in the graph-based optimization engine and need to be measured constantly.
However, achieving real-time monitoring, especially when the system dynamic is very high in application scenarios such as autonomous driving, is nontrivial and our monitoring system will have to make sure that both the time and traffic overhead are negligible [15].
The last is used for proactively taking data management actions for upcoming client movements and for delivering SLA guarantees for data accesses.
Existing approaches, like those based on directly measuring the wireless channel RSSI, are typically end-device centric.
However, end-devices are out of the reach of the storage system and thus, such approaches will not be applicable.
Griffin proposes to leverage indirect statistics information, such as the packet retransmission rate and queue sizes, that are available at the cellular base stations for example to derive the desired connection quality information [21].
Expected feedback and discussion points.
• Is it realistic to assume that developers can identify, often manually, consistency and performance requirements of different application datasets?
We hope to hear opinions from developers about what kind of abstractions are needed to facilitate an easy (or automatic) state externalization.
• How should we benchmark a data sharing service at the edge?
Are there standard, open-source storage benchmarks and data sets (similar to Cloud YCSB [26], DeFog [74], CAVBench [107]) we can use to evaluate edge stores?
• We expect that a storage service will not be the only service to be deployed on the shared edge infrastructure.How should resources at the edge be provisioned and (de)allocated to the storage service?
• More broadly, we seek to instill a discussion regarding what it would take to completely automate the edge data management with the help of runtime, compiler, new storage abstraction, and the deployment framework.
Controversial topics and open challenges.
• Is another data store needed at the edge right now?
• Modeling and monitoring any distributed system is a nontrivial pursuit, especially at the edge.
Is it realistic to model the edge environment with all its complexities?
There are some precedents inside a data center 2 .
Can we learn and extend those ideas in an edge environment?
• We have not investigated edge client-side concerns related to processing power, memory, and energy requirements to access data.
For example, we expect client-heavy protocols where heavy states (network, storage, consistency) are maintained at the client side to face challenges.
Ideally, a stateless thin-client protocol would be preferred where most of the logic resides on the server-side.
A systematic exploration of client's needs would be an interesting study.
When does the idea fall apart?
• Contrary to data centers, no single operator runs the whole edge infrastructure.
An edge operator/company may not be willing to disclose topology, resources, and monitoring information.
So there has to be a clear value-addition to their business cases for providers to share information and build such a service.
• Applications may not externalize state due to security and privacy concerns, e.g., health data stored on smart watches.
Can we integrate "end-devices" in the edge storage service as well?
Such a design will drastically change the current proposal where we must then deal with unreliable connections, high churn rates, without having to deal with data sharing for specific data sets (all accesses must be local).
We can envision storage solutions integrated with a lightweight web-assembly function shipping to end-nodes to support secure computation [116].
Lin Wang was funded partially by the German Research Foundation (DFG) under the joint Sino-German grant No. 392046569 (NSFC: 61761136014) and by the DFG Collaborative Research Center (CRC) 1053 -MAKI.
