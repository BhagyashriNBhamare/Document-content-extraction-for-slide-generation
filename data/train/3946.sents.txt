Public infrastructure-as-a-service clouds, such as Amazon EC2, Google Compute Engine (GCE) and Microsoft Azure allow clients to run virtual machines (VMs) on shared physical infrastructure.
This practice of multi-tenancy brings economies of scale, but also introduces the risk of sharing a physical server with an arbitrary and potentially malicious VM.
Past works have demonstrated how to place a VM alongside a target victim (co-location) in early-generation clouds and how to extract secret information via side-channels.
Although there have been numerous works on side-channel attacks, there have been no studies on placement vulnerabilities in public clouds since the adoption of stronger isolation technologies such as Virtual Private Clouds (VPCs).
We investigate this problem of placement vulnerabili-ties and quantitatively evaluate three popular public clouds for their susceptibility to co-location attacks.
We find that adoption of new technologies (e.g., VPC) makes many prior attacks, such as cloud cartography, ineffective.
We find new ways to reliably test for co-location across Amazon EC2, Google GCE, and Microsoft Azure.
We also found ways to detect co-location with victim web servers in multi-tiered located behind a load balancer.
We use our new co-residence tests and multiple customer accounts to launch VM instances under different strategies that seek to maximize the likelihood of co-residency.
We find that it is much easier (10x higher success rate) and cheaper (up to $114 less) to achieve co-location in these three clouds when compared to a secure reference placement policy.
Public cloud computing offers easy access to relatively cheap compute and storage resources.
Cloud providers are * Work primarily done while at the University of Wisconsin-Madison.
able to sustain this cost-effective solution through multitenancy, where the infrastructure is shared between computations run by arbitrary customers over the Internet.
This increases utilization compared to dedicated infrastructure, allowing lower prices.However, this practice of multi-tenancy also enables various security attacks in the public cloud.
Should an adversary be able to launch a virtual machine on the same physical host as a victim, making the two VMs co-resident (sometimes the term co-located is used), there exist attacks that break the logical isolation provided by virtualization to breach confidentiality [29,32,33,35,37,38] or degrade the performance [30,39] of the victim.
Perhaps most notable are the side-channel attacks that steal private keys across the virtual-machine isolation boundary by cleverly monitoring shared resource usage [35,37,38].
Less understand is the ability of adversaries to arrange for co-residency in the first place.
In general, doing so consists of using a launch strategy together with a mechanism for co-residency detection.
The only prior work on obtaining co-residency [29] showed simple networktopology-based co-residency checks along with low-cost launch strategies that obtain a high probability of achieving co-residency compared to simply launching as many VM instances as possible.
When such advantageous strategies exist, we say the cloud suffers from a placement vulnerability.
Since then, Amazon has made several changes to their architecture, including removing the ability to do the simplest co-residency check.
Whether placement vulnerabilities exist in other public clouds has, to the best of our knowledge, never been explored.In this work, we provide a framework to systematically evaluate public clouds for placement vulnerabilities and show that three popular public cloud providers may be vulnerable to co-location attacks.
More specifically, we set out to answer four questions:• Can co-residency be effectively detected in modern public clouds?
• Are known launch strategies [29] still effective in modern clouds?
• Are there any new exploitable placement vulnerabilities?
• Can we quantify the money and time required of an adversary to achieve a certain probability of success?We start by exploring the efficacy of prior co-residency tests ( § 4) and develop more reliable tests for our placement study ( § 4.1).
We also find a novel test to detect coresidency with VMs uncontrolled by the attacker by just using their public interface even when they are behind a load balancer ( § 4.3).
We use multiple customer accounts across three popular cloud providers, launch VM instances under different scenarios that may affect the placement algorithm, and test for co-residency between all launched instances.
We analyze three popular cloud providers, Amazon Elastic Cloud (EC2) [2], Google Compute Engine (GCE) [6] and Microsoft Azure (Azure) [15], for vulnerabilities in their placement algorithm.
After exhaustive experimentation with each of these cloud providers and at least 190 runs per cloud provider, we show that an attacker can still successfully arrange for co-location ( § 5).
We find new launch strategies in these three clouds that obtain co-location faster (10x higher success rate) and cheaper (up to $114 less) when compared to a secure reference placement policy.Next, we start by giving some background on public clouds ( § 2), then define our threat model ( § 3).
We conclude the paper with related and future work ( § 6 and § 7, respectively).
Public clouds.
Infrastructure-as-a-service (IaaS) public clouds, such as Amazon EC2, Google Compute Engine and Microsoft Azure, provide a management interface for customers to launch and terminate VM instances with a userspecified configuration.
Typically, users register with the cloud provider for an account and use the cloud interface to specify VM configuration, which includes instance type, disk image, data center or region to host the VMs, and then launch VM instances.
In addition, public clouds also provide many higher-level services that monitor load and automatically launch or terminate instances based on the workload [4,8,13].
These services internally use the same mechanisms as users to configure, launch and terminate VMs.The provider's VM launch service receives from a client a desired set of parameters describing the configuration of the VM.
The service then allocates resources for the new VM; this process is called VM provisioning.
We are most interested in the portion of VM provisioning that selects the physical host to run a VM, which we call the VM placement algorithms.
The resulting VM-to-host mapping we call the VM placement.
The placement for a specific virtual machine may depend on many factors: the load on each machine, the number of machines in the data center, the number of concurrent VM launch requests, etc.While cloud providers do not generally publish their VM placement algorithms, there are several variables under the control of the user that could affect the VM placement, such as time-of-day, requested data center, and number of instances.
A list of some notable parameters are given in Figure 1.
By controlling these variables, an adversary can partially influence the placement of VMs on physical machines that may also host a target set of VMs.
We call these variables placement variables and the set of values for these variables form a launch strategy.
An example launch strategy is to launch 20 instances 10 minutes after triggering an auto-scale event on a victim application.
This is, in fact, a launch strategy suggested by prior work [29].
Placement policies.
VM placement algorithms used in public clouds often aim to increase data center efficiency, quality of service, or both by realizing some placement policy.
For instance, a policy that aims to increase data center utilization may pack launched VMs on a single machine.
Similarly policies that optimize the time to provision a VM, which involves fetching an image over the network to the physical machine and booting, may choose the last machine that used the same VM image, as it may already have the VM image cached on local disks.
Policies may vary across cloud providers, and even within a provider.
Public cloud placement policies, although undocumented, often exhibit behavior that is externally observable.
One example is parallel placement locality [29], in which VMs launched from different accounts within a short time window are often placed on the same physical machine.
Two instances launched sequentially, where the first instance is terminated before the launch of the second one, are often placed on the same physical machine, a phenomenon called sequential placement locality [29].
These placement behaviors are artifacts of the two placement policies described earlier, respectively.
Other examples of policies and resulting behaviors exist as well.
VMs launched from the same accounts may either be packed on the same physical machine to maximize locality (and hence co-resident with themselves) or striped across different physical machines to maximize redundancy (and hence never co-resident with themselves).
In the course of normal usage, such behaviors are unlikely to be noticed, but they can be measured with careful experiments.Launch strategies.
An adversary can exploit placement behaviors to increase the likelihood of co-locating with target victims.
As pointed out by Ristenpart et al. [29], parallel placement locality can be exploited by triggering a scale-up event on target victim by increasing their load, which will cause more victim VMs to launch.
The adversary can then simultaneously (or after a time lag) launch multiple VMs some of which may be co-located with the newly launched victim VM(s).
In this study, we develop a framework to systematically evaluate public clouds against launch strategies and uncover previously unknown placement behaviors.
We approach this study by (i) identifying a set of placement variables that characterize a VM, (ii) enumerating the most interesting values for these variables, and (iii) quantifying the cost of such a strategy, if it in fact exposes a co-residency vulnerability.
We repeat this for three major public cloud providers: Amazon EC2, Google Compute Engine, and Microsoft Azure.
Note that the goal of this study is not to reverse engineer the exact details of the placement policies, but rather to identify launch strategies that can be exploited by an adversary.Co-residency detection.
A key technique for understanding placement vulnerabilities is detecting when VMs are co-resident on the same physical machine (also termed colocate).
In 2009, Ristenpart et al. [29] proposed several coresidency detection techniques and used them to identify several placement vulnerabilities in Amazon EC2.
As coresident status is not reported directly by the cloud provider, these detection methods are usually referred to as sidechannel based techniques, which can be further classified into two categories: logical side-channels or performance side-channels.
Logical side-channels: Logical side-channels allow information leakage via logical resources that are observable to a software program, e.g., IP addresses, timestamp counter values.
Particularly in Amazon EC2, each VM is assigned two IP addresses, a public IP address for communication over the Internet, and a private or internal IP address for intra-datacenter communications.
The EC2 cloud infrastructure allowed translation of public IP addresses to their internal counterparts.
This translation revealed the topology of internal data center network, which allowed a remote adversary to map the entire public cloud infrastructure and determine, for example the availability zone and instance type of a victim.
Furthermore, co-resident VMs tended to have adjacent internal IP addresses.Logical side-channels can also be established via shared timestamp counters.
In prior work, skew in timestamp counters were used to fingerprint a physical machine [27], although this technique has not yet been explored for coresidency detection.
Co-residency detection via shared state like interrupt counts and process statistics reported in procfs also come under this category, but are only applicable to container-based platform-as-a-service clouds.Performance side-channels: Performance side-channels are created when performance variations due to resource contention are observable.
Such variations can be used as an indicator of co-residency.
For instance, network performance has been used for detecting co-residence [29,30].
This is because hypervisors often short-cut network traffic between VMs on the same host, providing detectably shorter round-trip times than between VMs on different hosts.Moreover, covert channels, as a special case of sidechannels, can be established between two cooperative VMs.
For purposes of co-residency detection, covert channels based on shared hardware resources, such as last level caches (LLCs) or local storage disks, can be exploited by one VM to detect performance degradation caused by a coresident VM.
Covert channel detection techniques require control over both VMs, and are usually used in experimentation rather than in practical attacks.Placement study in PaaS.
While we mainly studied placement vulnerabilities in the context of IaaS, we also experimented with Platform-as-a-Service (PaaS) clouds.
PaaS clouds offer elastic application hosting services.
Unlike IaaS where users are granted full control of the VM, PaaS provides managed compute tasks (or instances) for the execution of hosted web applications, and allow multiple such instances to share the same operating system.
As such, logical side-channels alone are usually sufficient for coresidency detection purposes.
For instance, in PaaS clouds, co-resident instances share the same public IP address as the host machine.
This is because the host-to-instance network is often configured using Network Address Translation (NAT) and each instance is assigned unique port under the host IP address for front-facing incoming connections (e.g., Heroku [10]).
We found that many of the above logical side-channel-based co-residency detection approaches worked on PaaS clouds, even on those that support isolation better than process isolation [38] (e.g., using Linux containers).
Specifically, we used both system-level interrupt statistics via /proc/interrupts and shared public IP addresses of the instances to detect co-location in Heroku.Our quick investigation of co-location attacks in Heroku showed that na¨ıvena¨ıve strategies like scaling two PaaS web applications to 30 instances with a time interval of 5 minutes between them, resulted in co-location in 6 out of 10 runs.
Moreover, since the co-location detection was simple and quick including the time taken for application scaling, we were able to do these experiments free of cost.
This result reinforces prior findings on PaaS co-location attacks [38] and confirms the existence of cheap launch strategies to achieve co-location and easy detection mechanisms to verify it.
We do not investigate PaaS clouds further in the rest of this paper.
Co-residency attacks in public clouds, as mentioned earlier, involve two steps: a launch strategy and co-residency detection.
We assume that the adversary has access to tools to identify a set of target victims, and either knows victim VMs' launch characteristics or can directly trigger their launches.
The latter is possible by increasing load in order to cause the victim to scale up by launching more instances.
The focus of this study is to identify if there exists any launch strategy that an adversary can devise to increase the chance of co-residency with a set of targeted victim VMs.In our threat model, we assume that the cloud provider is trusted and the attacker has no affiliation of any form with the cloud provider.
This also means that the adversary has no internal knowledge of the placement policies that is responsible for the VM placements in the public cloud.
An adversary also has the same interface for launching and terminated VMs as regular customers, and no other special interfaces.
Even though there may be per-account limits on the number of VMs that a cloud provider imposes, an adversary has access to an unlimited number of accounts and hence has no limit on the number of VMs he could launch at any given time.No resource-limited cloud provider is a match to an adversary with limitless resources and hence realistically we assume that the adversary is resource-limited.
For the same reason, a cloud provider is vulnerable to a launch strategy only when it is trivial or cost-effective for an adversary.
As such, we aim to (i) quantify the cost of executing a launch strategy by an adversary, (ii) define a reference placement policy with which the placement policies of real clouds can be compared, and (iii) define metrics to quantify placement vulnerability.Cost of a Launch Strategy.
Quantifying the cost of a launch strategy is straight-forward: it is the cost of launching a number of VMs and running tests to detect coresidency with one or more target victim VMs.
To be precise, the cost of a launch strategy S is given by C S = a * P(a type ) * T d (v, a), where a is the number of attacker VMs of type a type launched to get co-located with one of the v victim VMs.
P(a type ) is the price of running one VM of type a type for a unit time.
T d (a, v) is the time to detect co-residency between all pairs of a attacjers and v victim VMs, excluding pairs within each group.
For simplicity, we assume that the attacker is running all instances till the last co-residency check is completed.Reference Placement Policy.
In order to define placement vulnerability, we need a yardstick to compare various placement policies and the launch strategies that they may be vulnerable to.
To aid this purpose, we define a simple reference placement policy that has good security properties against co-residency attacks and use it to gauge the placement policies used in public clouds.
Let there be N machines in a data center and let each machine have unlimited capacity.
Given a set of unordered VM launch requests, the mapping of each VM to a machine follows a uniform random distribution.
Let there be v victim VMs assigned to v unique machines among N, where v 񮽙 N.
The probability of at least one collision (i.e. co-residency) under the random placement policy and the above attack scenario when attacker launches a instances is given by 1 − 񮽙 1 − v/N 񮽙 a .
We call this probability the reference probability 1 .
Recall that for calculating cost of a launch strategy under this reference policy, we also need to define the price function, P(vm type ).
For simplicity, we use the most competitive minimum price offered by any cloud provider as the price for the compute resource under the reference policy.
For example, at the time of this study, Amazon EC2 offered t2.small instances at $0.026 per hour of instance activity, which was the cheapest price across all three clouds considered in this study.Note that the placement policy makes several simplifying assumptions but we argue that all these assumptions are conservative and only benefit the attacker.
For instance, the assumption on unlimited capacity of the servers only benefit the attacker as it never limits the number of victim VMs an attacker could potentially co-locate with.
We use a conservative value of 1000 for N, which is at least an orderof-magnitude less than the number of servers (50,000) in the smallest reported Amazon EC2 data centers [5].
Similarly, the price function of this placement policy also favors an attacker as it provides the cheapest price possible in the market even though in reality a secure placement policy may demand a higher price.
Hence, it would be troubling if the state-of-the-art placement policies used in public clouds does not measure well even against such a conservative reference placement policy.Placement Vulnerability.
Putting it all together, we define two metrics to gauge any launch strategy against a placement policy: (i) normalized success rate, and (ii) costbenefit.
The normalized success rate is the success rate of the launch strategy in the cloud under test normalized to the success rate of the same strategy under the reference placement policy.
The cost-benefit of a strategy is the additional cost that is incurred by the adversary in the reference placement policy to achieve the same success rate as the strategy in the placement policy under test.
We define that a placement policy has a placement vulnerability if and only if there exists a launch strategy with a normalized success rate that is greater than 1.
Note that the normalized success rate quantifies how easy it is to get co-location.
On the other hand, the cost benefit metric helps to quantify how cheap it is to get co-location compared to a more secure placement policy.
These metrics can be used to compare launch strategies under different placement policies, where a higher value for any of these metrics indicate that the placement policy is relative more vulnerable to that launch strategy.
An ideal placement policy should aim to reduce both the success rate and the cost benefit of any strategy.
An essential prerequisite for the placement vulnerability study is access to a co-residency detection technique that identifies whether two VMs are resident on the same physical machine in a third-party public cloud.Challenges in modern clouds.
Applying the detection techniques mentioned in Section 2 is no longer feasible in modern clouds.
In part due to the vulnerability disclosure by Ristenpart et al. [29], modern public clouds have adopted new technologies that enhance the isolation between cloud tenants and thwart known co-residence detection techniques.
In the network layer, virtual private clouds (VPCs) have been broadly employed for data center management [17,20].
With VPCs, internal IP addresses are private to a cloud tenant, and can no longer be used for cloud cartography.
Although EC2 allowed this in older generation instances (called EC2-classic), this is no longer possible under Amazon VPC setting.
In addition, VPCs require communication between tenants to use public IP addresses for communication.
As shown in Figure 2, the network timing test is also defeated, as using public IP addresses seems to involve routing in the data center network rather than short-circuiting through the hypervisor.
Here, the ground-truth of co-residency is detected using memorybased covert-channel (described later in this section).
Notice that there is no clear distinction between the frequency distribution of the network round trip times of co-resident and non-coresident pairs on all three clouds.
In the system layer, persistent storage using local disks is no longer the default.
For instance, many Amazon EC2 instance types do not support local storage [1]; GCE and Azure provide only local Solid State Drives (SSD) [7,14], which are less susceptible to detectable delays from long seeks.
In addition, covert channels based on last-level caches [29,30,33,36] are less reliable in modern clouds that use multiple CPU packages.
Two VMs sharing the same machine may not share LLCs to establish the covert channel.
Hence, these LLC-based covert-channels can only capture a subset of co-resident instances.As a result of these technology changes, none of the prior techniques for detecting co-residency reliably work in modern clouds, compelling us to develop new approaches for our study.
We describe in this subsection a pair of tools for coresidency tests, with the following design goals:• Applicable to a variety of heterogeneous software and hardware stacks used in public clouds.
• Detect co-residency with high confidence: the false detection rate should be low even in the presence of background noise from other neighboring VMs.
• Detect co-residency fast enough to facilitate experimentation among large sets of VMs.We chose a performance covert-channel based detection technique that exploits shared hardware resources, as this type of covert-channels are often hard to remove and most clouds are very likely to be vulnerable to it.A covert-channel consists of a sender and a receiver.
The sender creates contention for a shared resources and uses it to signal another tenant that potentially share the same resource.
The receiver, on the other hand, senses this contention by periodically measuring the performance of that shared resource.
A significant performance degradation measured at the receiver results in a successful detection of a sender's signal.
Here the reliability of the covert-channel is highly dependent on the choice of the shared resource and the level of contention created by the sender.
The sender is the key component of the co-residency detection techniques we developed as part of this study.Memory locking sender.
Modern x86 processors support atomic memory operations, such as XADD for atomic addition, and maintain their atomicity using cache coherence protocols.
However, when a locked operation extends across a cache-line boundary, the processor may lock the memory bus temporarily [32].
This locking of the bus can be detected as it slows down other uses of the bus, such as fetching data from DRAM.
Hence, when used properly, it provides a timing covert channel to send a signal to another VM.
Unlike cache-based covert channels, this technique works regardless of whether VMs share a CPU core or package.We developed a sender exploiting this shared memorybus covert-channel.
The psuedocode for the sender is shown in Figure 3.
The sender creates a memory buffer and uses pointer arithmetic to force atomic operations on unaligned memory addresses.
This indirectly locks the memory bus even on all modern processor architectures [32].
Receivers.
With the aforementioned memory locking sender, there are several ways to sense the memory locking contention induced by the sender in an another co-resident tenant instance.
All the receivers measure the memory bandwidth of the shared system.
We present two types of receivers that we used in this study that works on heterogeneous hardware configurations.Memory probing receiver uses carefully crafted memory requests that always miss in the cache hierarchy and always hit memory.
This is ensured by making data accesses from receiver fall into the same LLC set.
In order to evade hardware prefetching, we use a pointer-chasing buffer to randomly access a list of memory addresses (pseudocode shown in Figure 4).
The time needed to complete a fixed number of probes (e.g., 10,000) provides a signal of coresidence: when the sender is performing locked operations, loads from memory proceed slowly.Memory locking receiver is similar to the sender but measures the number of unaligned atomic operations that could be completed per unit time.
Although it also measures the memory bandwidth, unlike memory probing receiver, it works even when the cache architecture of the machine is unknown.The sender along with these two receivers form our two novel co-residency detection methods that we use in this study: memory probing test and memory locking test (named after their respective receivers).
These comprise our co-residency test suite.
Each test in the suite starts by running the receiver on one VM while keeping the other idle.
The performance measured by this run is the baseline performance without contention.
Then the receiver and the sender are run together.
If the receiver detects decreased performance, the tests concludes that the two VMs are coresident.
We use a slowdown threshold to detect when the change in receiver performance indicates co-residence (discussed later in the section).
Evaluation on local testbed.
In order to measure the efficacy of this covert channel we ran tests in our local testbed.
Results of running memory probing and locking tests under various configurations are shown in Figure 5.
The hardware architecture of these machines are similar to what is observed in the cloud [21].
Across these hardware configurations, we observed a performance degradation of at least 3.4x compared to not running memory locking sender on a non-coresident instance (i.e. a baseline run with idle sender) indicating reliability.
Note that this works even when the co-resident instances are running on cores on different sockets, which does not share the same LLC (works on heterogeneous hardware).
Further, a single run takes one tenth of a second to complete and hence is also quick.Note that for this test suite to work in the real world, an attacker requires control over both the VMs under test, which includes the victim.
We call this scenario as coresidency detection under cooperative victims (in short, cooperative co-residency detection).
Such a mechanism is sufficient observe placement behavior in public clouds (Section 4.2).
We further investigated approaches to detect co-residency under a realistic setting with an uncooperative victim.
In Section 4.3 we show how to adapt the memory probing test to detect co-location with one of the many webservers behind a load balancer.
In this section, we describe the methodology we used to detect co-residency in public clouds.
For the purposes of studying placement policies, we had the flexibility to con- trol both VMs that we test for co-residence.
We did this by launching VMs from two separate accounts and test them for pairwise co-residence.
We encountered several challenges when running the same on three different public clouds -Google Computer Engine, Amazon EC2 and Microsoft Azure.
First, we had to handle noise from neighboring VMs sharing the same host.
Second, hardware and software heterogeneity in the three different public clouds required special tuning process for the co-residency detection tests.
Finally, testing co-residency for a large set of VMs demanded a scalable implementation.
We elaborate on our solution to these challenges below.Handling noise.
Any noise from neighboring VMs could affect the performance of the receiver with and without the signal (or baseline) and result in misdetection.
To handle such noise, we alternate between measuring the performance with and without the sender's signal, such that any noise equally affects both the measurements.
Secondly, we take ten samples of each measurement and only detect coresidence if the ratios of both the mean and median of these samples exceed the threshold.
As each run takes a fraction of a second to complete, repeating 10 times is still fast enough.
Tuning thresholds.
As expected, we encountered different machine configuration on the three different public clouds (shown in Figure 7) with heterogeneous cache dimensions, organizations and replacement policies [11,26].
This affects the performance degradation observed by the receivers with respect to the baseline and the ideal threshold for detecting co-residency.
This is important because the thresholds we use to detect co-residence yield false positives, if set too low, and false negatives if set too high.
Hence, we tuned the threshold to each hardware we found on all three clouds.We started with a conservative threshold of 1.5x and tuned to a final threshold of 2x for GCE and EC2 and 1.5x for Azure for both the memory probing and locking tests.
Figure 6 shows the distribution of performance degradation under the memory probing tests across Intel machines in EC2, GCE, and Azure.
For GCE and EC2, a performance degradation threshold of 2 clearly separates coresident from non-co-resident instances.
For all Intel machines we encountered, although we ran both memory locking and probing tests, memory probing was sufficient to detect co-residency.
For Azure, overall we observe lower performance degradation and the initial threshold of 1.5 was sufficient to detect co-location on Intel machines.The picture for AMD machines in Azure differs significantly as shown in Figure 8.
The distribution of performance degradation for both memory locking and memory probing shows that, unlike for other architecures, coresidency detection is highly sensitive to the choice of the threshold for AMD machines.
This may be due to the more associative cache (48 ways vs. 20 for Intel), or different handling of locked instructions.
For these machines, a threshold of 1.5 was high enough to have no false positives, which we verified by hand checking the instances using the two covert-channels and observed consistent performance degradation of at least 50%.
.
We determine a pair of VMs as co-resident if the degradation in either of the tests is above this threshold.
We did not detect any cross-architecture (false) co-residency detection in any of the runs.Scaling co-residency detection tests.
Testing coresidency at scale is time-consuming and increases quadratically with the number of instances: checking 40 VM instances, involves 780 pair-wised tests.
Even if each run of the entire co-residency test suite takes only 10 seconds, a na¨ıvena¨ıve sequential execution of the tests on all the pairs will take 2 hours.
Parallel co-residency checks can speed checking, but concurrent tests may interfere with each other.To parallelize the test, we partition the set of all VM pairs ( 񮽙 v+a 2 񮽙 ) into sets of pairs with no VMs twice; we run one of these sets at a time and record which pairs detected possible co-residence.
After running all sets, we have a set of candidate co-resident pairs, which we test sequentially.
Parallelizing co-residency tests significantly decreased the time taken to test all co-residency pairs.
For instance, the parallel version of the test on one of the cloud providers took 2.4 seconds per pair whereas the serial version took almost 46.3 seconds per pair (a speedup of 20x).
While there are faster ways to parallelize co-residency detection, we chose this approach for simplicity.Veracity of our tests.
Notice that a performance degradation of 1.5x, 2x and 4x corresponds to 50%, 100% and 300% performance degradation.
Such high performance degradations (even 50%) is clear enough signal to declare co-residency due to resource sharing.
Furthermore, we hand checked by running the two tests in isolation on the detected instance-pairs for a significant fraction of the runs for all clouds and observed a consistent covert-channel signal.
Thus our methodology did not detect any false positives, which are more detrimental to our study than false negatives.
Although "co-residency" here implies sharing of memory channel, which may not always mean sharing of cores or other per-core hardware resources.
Until now, we described a method to detect co-residency with a cooperative victim.
In this section, we look at a more realistic setting where an adversary wishes to detect co-residency with a victim VM with accesses limited to only public interfaces like HTTP or a key-value (KV) store's put-get interface.
We show that the basic cooperative co-residency detection can also be employed to detect co-residency with an uncooperative victim in the wild.Attack setting.
Unlike previous attack scenarios, we assume the attacker has no access to the victim VMs or its application other than what is permitted to any user on the Internet.
That is, the victim application exposes a wellknown public interface (e.g., HTTP, FTP, KV-store protocol) that allows incoming requests, which is also the only access point for the attacker to the victim.
The front end of this victim application can range from caching or data storage services (e.g., memcached, cassandra) to generic webservers.
We also assume that there may be multiple instances of this front-end service running behind a load balancer.
Under this scenario, the attacker wishes to detect co-location with one or more of the front-facing victim VMs.Co-residency test.
We adapt the memory tests used in previous section by running the memory locking sender in the attacker instance.
For a receiver, we use the public interface exposed by the victim by generating a set of requests that potentially makes the victim VMs hit the memory bus.
This can be achieved looping through large number of requests of sizes approximately equal or greater than the size of the LLC.
This creates a performance side-channel that leaks co-residency information.
This receiver runs in an independent VM under the adversary's control, which we call the co-residency detector.Experiment setup.
To evaluate the efficacy of this method, we used the Olio multi-tier web application [12] that is designed to mimic a social-networking application.We used an instance of this workload from CloudSuite [22].
Although Olio supports several tiers (e.g., memcached to cache results of database queries), we configured it with two tiers, with each webserver and the database server running in a separate VM of type t2.small on Amazon EC2.
Multiple of these webserver VMs are configured behind a HAProxy-based load balancer [9] running in a m3.medium instance (for better networking performance).
The load balancer follows the standard configuration of using roundrobin load balacing algorithm with sticky client sessions using cookies.
We believe such a victim web application and its configuration is a reasonable generalization of real world applications running in the cloud.For the attacker, we use an off-the-shelf HTTP performance measurement utility called HTTPerf [28] as the receiver in the co-residency detection test.
This receiver is run inside a t2.micro instance (for free of charge).
We used a set of 212 requests that included web pages and web objects (images, PDF files).
We gathered these requests using the access log of manual navigation around the web application from a web browser.Evaluation methodology.
We start with a known coresident VM pair using the cooperative co-residency detection method.
We configure one of the VMs as a victim webserver VM and launch four more VMs: two webservers, one database server and a load balancer, all of which are not co-resident with the attacker VM.Co-residency detection starts by measuring the average request latency at the receiver inside the co-residency detector for the baseline (with idle attacker) and contended case with the attacker runs the memory locking sender.
A significant performance degradation between the baseline and the contended case across multiple samples reveal coresidency of one of the victim VMs with the attacker VM.
On Amazon EC2, with the above setup we observed an average request latency of 4.66ms in the baseline case and a 10.6ms in the memory locked case, i.e., a performance degradation of ≈ 2.3x.Background noise.
The above test was performed when the victim web application was idle.
In reality, any victim in the cloud might experience constant or varying background load on the system.
False positives or negatives may occur when there is spike in load on the victim servers.
In such case, we use the same solution as in Section 4.2 -alternating between measuring the idle and the contended case.In order to gauge the efficacy of the test under constant background load, we repeated the above experiment with varying load on the victim.
The result of this experiment is summarized in Figure 9.
Counterintuitively, we found that a constant load on the background server exacerbates the performance degradation gap, hence resulting in a clearer signal of co-residency.
This is because running memory locking on the co-resident attacker increases the service time of all requests as majority of the requests rely on memory bandwidth.
This increases the queuing delay in the system and in turn increasing the overall request latency.
Interestingly, this aforementioned performance gap stops widening at higher loads of 750 to 1000 concurrent users as the system hits a bottleneck (in our case a network bottleneck at the load balancer) even without running the memory locking sender.
Thus, detecting co-residency with a victim VM that is part of a highly loaded and bottlenecked application would be hard using this test.We also experimented with increasing the number of victim webservers behind the load balancer beyond 3 ( Figure 10).
As expected, the co-residency signal grew weaker with increasing victims, and at 9 webservers, the performance degradation was too low to be useful for detecting co-residency.
In this section, we evaluate three public clouds, Amazon EC2, Google Compute Engine and Microsoft Azure, for placement vulnerabilities and answer the following questions: (i) what are all the strategies that an adversary can employ to increase the chance of co-location with one or more victim VMs?
(ii) what are the chances of success and cost of each strategy?
and (iii) how does these strategies compare against the reference placement policy introduced in Section 3?
Figure 10: Varying number of webservers behind the load balancer.
The graph shows the average request latency at the coresidency detector without and with memory locking sender running on the co-residency attacker VM under varying background load on the victim.
Note that the y-axis is in log scale.
The error bars show the standard deviation over 5 samples.
Before presenting the results, we first describe the experiment setting and methodology that we employed for this placement vulnerability study.Experiment settings.
Recall VM placement depends on several placement variables (shown in Figure 1).
We assigned reasonable values to these placement variables and enumerated through several launch strategies.
A run corresponds to one launch strategy and it involves launching multiple VMs from two different accounts (i.e., subscriptions in Azure and projects in GCE) and checking for coresidency between all pairs of VMs launched.
One account was designated as a proxy for the victim and the other for the adversary.
We denote a run configuration by v × a, where v is the number of victim instances and a is the number of attacker instances launched in that run.
We varied v and a for all v, a ∈ {10, 20, 30} and restricted them to the inequality, v ≤ a, as it increases the likelihood of getting co-residency.
Other placement variables that are part of the run configuration include: victim launch time (including time of the day, day of the week), delay between victim and attacker VM launches, victim and attacker instance types and data center location or region where the VMs are launched.
We repeat each run multiple times across all three cloud providers.
The repetition of experiments is especially required to control the effect of certain environment variables like time of day.
We repeat experiments for each run configuration over various times of the day and days of the week.
We fix the instance type of VMs to small instances (t2.small on EC2, g1.small on GCE and small or Standard-A1 on Azure) and data center regions to us-east for EC2, us-central1-a for GCE and east-us for Azure, unless otherwise noted.
All these experiments were conducted over 3 months between December 2014 to February 2015.
We used a single, local Intel Core i7-2600 machine with 8 SMT cores to launch VM instances, log instance information and run the co-residency detection test suite.Implementation and the Cloud APIs.
In order to automate our experiments, we used Python and the libcloud 2 library [3] to interface with EC2 and GCE.
Unfortunately, libcloud did not support Azure.
The only Azure cloud API on Linux platform was a node.js library and a crossplatform command-line interface (CLI).
We built a wrapper around the CLI.
There were no significant differences across different cloud APIs except that Azure did not have any explicit interface to launch multiple VMs simultaneously.As mentioned in the experiment settings, we experimented with various delays between the victim and attacker VM launches (0, 1, 2, 4 . . . hours).
To save money, we reused the same set of victim instances for each of the longer runs.
That is, for the run configuration of 10x10 with 0, 1, 2, and 4 hours of delay between victim and attacker VM launches, we launched the victim VMs only once at the start of the experiment.
After running co-residency tests on the first set of VM pairs, we terminated all the attacker instances and relaunched attacker VM instances after appropriate delays (say 1 hour) and rerun the tests with the same set of victim VMs.
We repeat this until we experimented with all the required delays for this configuration.
We call this methodology the leap-frog method.
It is also important to note that zero delay here means parallel launch of VMs from our test machine (and not sequential launch of VMs from one account after another), unless otherwise noted.In the sections below, we take a closer look at the effect of varying one placement variable while keeping other variables fixed across all the cloud providers.
In each case, we use three metrics to measure the degree of co-residency: chances of getting at least one co-resident instance across a number of runs (or success rate), average number of coresident instances over multiple runs and average coverage 2 We used libcloud version 0.15.1 for EC2, and a modified version of 0.16.0 for GCE to support the use of multiple accounts in GCE.
(i.e., fraction of victim VMs with which attacker VMs were co-resident).
Although these experiments were done with victim VMs under our control, the results can be extrapolated to guide an attacker's launch strategy for uncooperative victim.
We also discuss a set of such strategic questions that the results help answer.
At the end of this section, we summarize and calculate the cost of several interesting launch strategies and evaluate the public clouds against our reference placement policy.
Chances of Co-residency Azure EC2 GCE Figure 14: Chances of co-residency of 10 victim instances with varying number of attacker instances.
All these results are from one data center region (EC2: us-east, GCE: us-central1-a, Azure: East US) and the delays between victim and attacker instance launch were 1 hour.
Results are over at least 9 run per run configuration with at least 3 run per time of day.
In this section, we observe the placement behavior while varying the number of victim and attacker instances.
Intuitively, we expect the chances of co-residency to increase with the number of attacker and victim instances.Varying number of attacker instances.
Keeping all the placement variables constant including the number of victim instances, we measure the chance of co-residency over multiple runs.
The result of this experiment helps to answer the question: How many VMs should an adversary launch to increase the chance of co-residency?
As is shown in Figure 14, the placement behavior changes across different cloud providers.
For GCE and EC2, we observe that higher the number of attacker instances relative to the victim instances, the higher the chance of co-residency is.
Figure 11 and 12 show the distribution of number of co-resident VM pairs on GCE and EC2, respectively.
The number of co-resident VM pairs also increase with the number of attacker instances, implying that the coverage of an attack could be increased with larger fraction of attacker instances than the target VM instances if the launch times are coordinated.Contrary to our expectations, the placement behavior observed on Azure is the inverse.
The chances of co-residency with 10 attacker instance is almost twice as high as with 30 instances.
This is also reflected in the distribution of number of co-residency VM pairs (shown in Figure 13).
Further investigation revealed a correlation between the number of victim and attacker instances launched and the chance of co-residency.
That is, for the run configuration of 10x10, 20x20 and 30x30, where number of victim and attacker instances are the same, and with 0 delay, the chance of coresidency were equally high for all these configurations (between 0.9 to 1).
This suggests a possible placement policy that collates VM launch requests together based on their request size and places them on the same group of machines.
Chances of Co-residency Azure EC2 GCE Figure 15: Chances of co-residency of 30 attacker instances with varying number of victim instances.
All these results are from one data center region (EC2: us-east, GCE: us-central1-a, Azure: East US) and the delays between victim and attacker instance launch were 1 hour.
Results are over at least 9 run per run configuration with at least 3 run per time of day.Varying number of victim instances.
Similarly, we also varied the number of victim instances by keeping the number of attacker instances and other placement variables constant (result shown in Figure 15).
We expect the chance of co-residency to increase with the number of victims targeted.
Hence, the results presented here help an adversary answer the question: What are the chances of co-residency with varying sizes of target victims?As expected, we see an increase in the chances of coresidency with increasing number of victim VMs across all cloud providers.
We see that the absolute value of the chance of co-residency is lower for Azure than other clouds.
This may be the result of significant additional delay between victim and attacker launch times in Azure as a result of our methodology (more on this later).
In this section, we answer two questions that aid an adversary to design better launch strategies: How quickly should an attacker launch his VMs after the victim VMs are launched?
Is there any increase in chance associated with the time of day of the launch?Varying delay between attacker and victim launches.
The result of varying the delay between 0 (i.e., parallel launch) and 1 hour delay is shown in Figure 16.
We can make two immediate observations from this result.The first observation reveals a significant artifact of EC2's placement policy: VMs launched within a short time window are never co-resident on the same machine.
This observation helps an adversary to avoid such a strategy.
We further investigated placement behaviors on EC2 with shorter non-zero delays in order to find the duration of this time window in which there are zero co-residency (results shown in Figure 17).
We found that this time window is very short and that even a sequential launch of instances Chances of Co-residency Azure EC2 GCE Figure 16: Chances of co-residency with varying delays between victim and attacker launches.
Solid boxes correspond to zero delay (simultaneous launches) and gauze-like boxes correspond to 1 hour delay between victim and attacker launches.
We did not observe any co-resident instances for runs with zero delay on EC2.
All these results are from one data center region (EC2: us-east, GCE: us-central1-a, Azure: East US).
Results are over at least 9 run per run configuration with at least 3 run per time of day.
(denoted by 0+) could result in co-residency.
The second observation shows that non-zero delay on GCE and zero delay on Azure increases the chance of coresidency and hence directly benefits an attacker.
It should be noted that on Azure, the launch delays between victim and attacker instances were longer than 1 hour due to our leap-frog experimental methodology; the actual delays between the VM launches were, on average, 3 hours (with a maximum delay of 10 hours for few runs).
This higher delay was more common in runs with larger number of instances as there were significantly more false positives, which required a separate sequential phase to resolve (see Section 4.2).
We also experimented with longer delays on EC2 and GCE to understand whether and how quickly the chance of co-residency drops with increasing delay (results shown in Figure 18).
Contrary to our expectation, we did not find the chance of co-residency to drop to zero even for delays as high as 16 and 32 hours.
We speculate that the reason for this observation could be that system was under constant churn where some neighboring VMs on the victim's machine were terminated.
Note that our leap-frog methodology may, in theory, interfere with the VM placement.
But it is noteworthy that we observed increased number of unique 2 0 1 2 4 8 16 32 0 1 2 4 8 16 32 Chances of CoresidencyTime lag between victim and attacker instances (hours) EC2 GCE Figure 18: Chances of co-residency over long periods.
Across 9 runs over two weeks with 3 runs per time of day.
Note that we only conducted 3 runs for 32 hour delay as opposed to 9 runs for all other delays.
co-resident pairs with increasing delays, suggesting fresh co-residency with victim VMs over longer delays.Effect of time of day.
Prior works have shown that churn or load is often correlated with the time of day [31].
Our simple reference placement policy does not have a notion of load and hence have no effect on time of day.
In reality, with limited number of servers in datacenters and limited number of capacity per host, load on the system has direct effect on the placement behavior of any placement policy.
As expected, we observe small effect on VM placement based on the time of day when attacker instances are launched (results shown in Figure 19).
Specifically, there is a slightly higher chance of co-residency if the attacker instances are launched in the early morning for EC2 and at night for GCE.
All the above experiments were conducted on relatively popular regions in each cloud (especially true for EC2 [31]).
In this section, we report the results on other smaller and less popular regions.
As the regions are less popular and have relatively fewer machines, we expect higher co-residency rates and more co-resident instances.
Figure 20 shows the median number of co-resident VM pairs placed in these regions alongside the results for popular regions.
The distribution of number of co-resident instances is not shown here in the interest of space.The main observation from these experiments is that there is a higher chance of co-residency in these smaller regions than the larger, more popular regions.
Note that we placed at least one co-resident pair in all the runs in these regions.
Also the higher number of co-resident pairs also suggests a larger coverage over victim VMs in these smaller regions.One anomaly that we found during two 20x20 runs on EC2 between 30 th and 31 st of January 2015, when we observed an unusually large number of co-resident instances (including three VMs from the same account).
We believe this anomaly may be a result of an internal management incident in the Amazon EC2 us-west-1 region.
We report several other interesting observations in this section.
First, we found more than two VMs can be co-resident on the same host on both Azure and GCE, but not on EC2.
Figure 21 shows the distribution of number of co-resident instances per host.
Particularly, in one of the runs, we placed 16 VMs on a single host.Another interesting observation is related to co-resident instances from the same account.
We term them as selfco-resident instances.
We observed many self-co-resident pairs on GCE and Azure (not shown).
On the other hand, we never noticed any self co-resident pair on EC2 except for the anomaly in us-west-1.
Although we did not notice Figure 23: Cost of running a launch strategy (in dollars).
Maximum cost column refers to the maximum cost we incurred out of all the runs for that particular configuration and cloud provider.
The cost per hour of small instances at the time of this study were: 0.05, 0.026 and 0.06 dollars for GCE, EC2 and Azure, respectively.
The minimum and maximum costs are in bold.any effect on the actual chance of co-residence, we believe such placement behaviors (or the lack of) may affect VM placement.We also experimented with medium instances and successfully placed few co-located VMs on both EC2 and GCE by employing similar successful strategies learned with small instances.
Recall that the cost of a launch strategy from Section 3, C S = a * P(a type ) * T d (v, a).
In order to calculate this cost, we need T d (v, a) which is the time taken to detect colocation with a attackers and v victims.
Figure 22 shows the average time taken to complete launching attacker instances and complete co-residency detection for each run configuration.
Here the measured co-residency detection is the parallelized version discussed in Section 4.2 and also includes time taken to detect co-residency within each tenant account.
Hence, for these reasons the time to detect co-location is an upper bound for a realistic and highly optimized co-residency detection mechanism.
We calculate the cost of executing each launch strategy under the three public clouds.
The result is summarized in Figure 23.
Note that we only consider the cost incurred by the compute instances because the cost for other resources such as network and storage, was insignificant.
Also note that EC2 bills every hour even if an instance runs less than an hour [16], whereas GCE and Azure charge per minute of instance activity.
This difference is considered in our cost calculation.
Overall, the maximum cost we incurred was about $8 for running 30 VMs for 4 hours 25 minutes on Azure and a minimum of 14 cents on GCE for running 10 VMs for 17 minutes.
We incurred the highest cost for all the launch strategies in Azure because of overall higher cost per hour and partly due to longer tests due to our co-residency detection methodology.
In this section, we return to the secure reference placement policy introduced in Section 3 and use it to identify placement vulnerabilities across all the three clouds.
Recall that the probability of at least one pair of co-residency under this random placement policy is given by Pr[ E v a > 0 ] = 1 − (1 − v/N) a , where E va is the random variable denoting the number of co-location observed when placing a attacker VMs among N = 1000 total machines where v machines are already picked for the v victim VMs.
First, we evaluate this probability for various run configurations that we experimented with in the public clouds.
The probabilities are shown in Figure 24.
Recall that a launch strategy in a cloud implies a placement vulnerability in that cloud's placement policy if its normalized success rate is greater than 1.
The normalized success rate of the strategy is the ratio of the chance of colocation under that launch strategy to the probability of colocation in the reference policy (Pr [ E v a > 0 ]).
Below is a list of selected launch strategies that escalate to placement vulnerabilities using our reference policy with their normalized success rate in parenthesis.
Cost benefit.
Next, we quantify the cost benefit of each of these strategies over the reference policy.
As the success rate of any launch strategy on a vulnerable placement policy is greater than what is possible in the reference policy, we need more attacker instances in the reference policy to achieve the same success rate.
We calculate this number of attacker instances a 񮽙 using:a 񮽙 = ln(1 − S v a )/ ln(1 − v/N), where, S va is the success rate of a strategy with run configuration of v × a.
The result of this calculation is presented in Figure 25.
The result shows that the best strategy, S1 and S2, on all three cloud providers is $114 cheaper than what is possible in the reference policy.It is also evident that these metrics enable evaluating and comparing various launch strategies and their efficacy on various placement policies both on robust placements and attack cost.
For example, note that although the normalized success rate of S3 is lower than S4, it has a higher cost benefit for the attacker.
Although we exhaustively experimented with a variety of placement variables, the results have limitations.
One major limitation of this study is the number of placement variables and the set of values for the variables that we used to experiment.
For example, we limited our experiments with only one instance type, one availability zone per region and used only one account for the victim VMs.
Although different instance types may exhibit different placement behavior, the presented results still holds strong for the chosen instance type.
The only caveat that may affect the results is if the placement policy use user account ID for VM placement decisions.
Since, we experimented with only one victim account (separate from the designated attacker account) across all providers, these results, in the worst case, may have captured the placement behavior of an unlucky victim account that was subject to similar placement decisions (and hence co-resident) as that of the VMs from the designated attacker account.Even though we ran at least 190 runs per cloud provider over a period of 3 months to increase statistical significant of our results, we were still limited to at most 9 runs per run configuration (with 3 runs per time of day).
These limitations have only minor bearing on the results presented, if at all any, and the reported results are significant and impactful for cloud computing security research.
VM placement vulnerability studies.
Ristenpart et al. [29] first studied the placement vulnerability in public clouds, which showed that a malicious cloud tenant may place one of his VMs on the same machine as a target VM with high probability.
Placement vulnerabilities exploited in their study include publicly available mapping of VM's public/internal IP addresses, disclosure of Dom0 IP addresses, and a shortcut communication path between coresident VMs.
Their study was followed by Xu et al. [33] and further extended by Herzberg et al. [25].
However, the results of these studies have been outdated by the recent development of cloud technologies, which is the main motivation of our work.Concurrent with our work, Xu et al. [34] conducted a systematic measurement study of co-resident threats in Amazon EC2.
Their focus, however, is in-depth evaluation of co-residency detection using network route traces and quantification of co-residence threats on older generation instances with EC2's classic networking (prior to Amazon VPC).
In contrast, we study placement vulnerabilities in the context of VPC on EC2, as well as on Azure and GCE.
The two studies are mostly complementary and strengthen the arguments made by each other.New VM placement policies to defend against placement attacks have been studied by Han et al. [23,24] and Azar et al. [18].
It is unclear, however, whether their proposed policies work against the performance and reliability goals of public cloud providers.Co-residency detection techniques.
Techniques for coresidency detection have been studied in various contexts.
We categorize these techniques into one of the two classes: side-channel approaches to detecting co-residency with uncooperative VMs and covert-channel approaches to detecting co-residency with cooperative VMs.Side-channels allow one party to exfiltrate secret information from another; therefore these approaches may be adapted in practical placement attack scenarios with targets not controlled by the attackers.
Network round-trip timing side-channel was used by Ristenpart et al. [29] to detect co-residency.
Zhang et al. [36] developed a system called HomeAlone to enable VMs to detect third-party VMs using timing side-channels in the last level caches.
Bates et al. [19] proposed a side-channel for co-residency detection by causing network traffic congestion in the host NICs from attacker-controlled VMs; the interference of target VM's performance, if the two VMs are co-resident, should be detectable by remote clients.
Kohno et al. [27] explored techniques to fingerprint remote machines using timestamps in TCP or ICMP based network probes, although their approach was not designed for co-residency detection.
However, none of these approaches works effectively in modern cloud infrastructures.Covert-channels on shared hardware components can be used for co-residency detection when the pair of VMs are cooperative.
Coarse-grained covert-channels in CPU caches and hard disk drives were used in Ristenpart et al. [29] for co-residency confirmation.
Xu et al. [33] established covert-channels in shared last level caches between two colluding VMs in the public clouds.
Wu el al. [32] exploited memory bus as a covert-channel on modern x86 processors, in which the sender issues atomic operations on memory blocks spanning multiple cache lines to cause memory bus locking or similar effects on recent processors.
However, covert-channels proposed in the latter two studies were not designed for co-residency detection, while those developed in our work is tuned for such purposes.
Multi-tenancy in public clouds enable co-residency attacks.
In this paper, we revisited the problem of placementcan an attacker achieve co-location?
-in modern public clouds.
We find that while past techniques for verifying colocation no longer work, insufficient performance isolation in hardware still allows detection of co-location.
Furthermore, we show that in the three popular cloud providers (EC2, GCE and Azure), achieving co-location is surprisingly simple and cheap.
It is even simpler and costs nothing to achieve co-location in some PaaS clouds.
Our results demonstrate that even though cloud providers have massive datacenters with numerous physical servers, the chances of co-location are far higher than expected.
More work is needed to achieve a better balance of efficiency and security using smarter co-location-aware placement policies.
This work was funded by the National Science Foundation under grants CNS-1330308, CNS-1546033 and CNS-1065134.
Swift has a significant financial interest in Microsoft Corp.
