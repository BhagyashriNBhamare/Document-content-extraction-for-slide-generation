Traditionally, data similarity and correlation analysis is done in the Data Analytics Layer (DAL).
This implies slower velocity and increase in I/O and communication costs due to data movement.
On the contrary, the storage layer provides a centralized and proximate place for data.
Furthermore, this analysis would benefit the storage layer in terms of space optimization.
In today's connected world, we have several smart city applications that use various sensors.
Consider traffic, air quality, weather, and energy consumption sensors that report their periodic readings in a big data storage system.
Healthy sensor data has high similarity and relatively fixed correlation patterns.
For example, with the same temporal and spatial context, air quality readings negatively correlate with traffic congestion, and energy consumption readings correlate with temperature (positively in hot countries and negatively in cold ones).
A System with these characteristics would benefit from the proposed self-cleaning service by deduplicating similar data and detecting anomalous data.
While this is a use case, most datasets are self-similar and would benefit from the proposed service, as they tend to have embedded correlation patterns.
Leverage lightweight correlation and similarity analysis at the storage level to clean redundant, anomalous data and accelerate extraction of insights in the DAL.
Reduce the "Garbage in garbage out" problem and facilitate big data cleaning which is the most time and effortconsuming task in big data analysis [2].
The proposed self-cleaning service imposes minimal changes to the application layer and provides a unified way of data cleaning at the storage level.
Ultimately, this allows faster and richer insights at the DAL to arise, for example forecasting, recommending, and what-if analysis built on top of clean data and correlation information.
1) Overhead: this relies on the complexity of correlation and similarity algorithms and selecting convenient time to execute them so that storage performance is minimally affected.
Recent correlation and anomaly detection algorithms have shown lightweight ones that do not need complex machine learning [3].
2) Hardware support: Carrying out correlation at line-rate on large datasets is spatially and computationally expensive process.
ASIC hardware support is necessary to support the software design; however, the ASIC needs to be generally programmable and adhere to power and silicon area budgets.
This requires work at the hardware level.
3) Similarity/correlation definition: similarity can be defined as exact duplicates or similar semantics depending on data domain; correlation and similarity can be expressed either using rules as in Intelliclean [4] or modeled via machine learning algorithms as in Tamr and twitter long term anomaly detection [5,6].
Rules might be tedious to define and generalize yet have less overhead, and while ML automates similarity/correlation extraction, it prerequisites feature engineering which metadata could assist in.
5) Data Lineage: has high capture overhead but can accelerate data cleaning tasks.
Analytics tasks were significantly accelerated using lineage in SEeSAW [7].
