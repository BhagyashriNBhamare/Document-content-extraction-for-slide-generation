We study in this paper the problem of incremental crawling of web forums, which is a very fundamental yet challenging step in many web applications.
Traditional approaches mainly focus on scheduling the revisiting strategy of each individual page.
However, simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites.
Instead of treating each individual page independently, we propose a list-wise strategy by taking into account the site-level knowledge.
Such site-level knowledge is mined through reconstructing the linking structure, called sitemap, for a given forum site.
With the sitemap, posts from the same thread but distributed on various pages can be concatenated according to their times-tamps.
After that, for each thread, we employ a regression model to predict the time when the next post arrives.
Based on this model, we develop an efficient crawler which is 260% faster than some state-of-the-art methods in terms of fetching new generated content; and meanwhile our crawler also ensure a high coverage ratio.
Experimental results show promising performance of Coverage, Bandwidth utilization, and Timeliness of our crawler on 18 various forums.
To the best of our knowledge, little existing work in literatures has systematically investigated the problem of forum incremental crawling.
However, there are still some previous work that should be reviewed, as our approaches were motivated by them.Some early work in [2,3] first investigated the dynamic web and treated the information as a depreciating commodity.
They first introduced the concepts of lifetime and age of a page which is important for measuring the performance of an incremental crawler.
However, they treated every page equally and ignored the importance and change frequency which are also important to an incremental crawler.Later work improved the above work and optimized the crawling process by prioritizing the pages.
Whether to minimize the age or to maximize the freshness leads to a variety of analytical models by investigating different features.
We classify them into three categories:(I) How often the content of a page is updated by its owner.
Coffman et.
al. [7] analyzed the crawling problem theoretically.
Cho et.
al. [6] proposed several methods based on the page update frequency.
However, most of these methods are based on the assumption that most web pages change as a Poisson or memoryless process.
But the experimental result in [3] showed that most web pages are modified during the span of US working hours (between 8 AM and 8 PM, Eastern time, Monday to Friday).
This phenomenon is even more evident in forum websites because the content are all generated by forum users.
Edwards et.
al. [10] tried to use an adaptive approach to maintaining data on actual change rates for the optimization without the above assumption.
However, it still treated each page independently and may waste bandwidth.
Furthermore, we argue that some other factors, such as time intervals of the latest posts, are more important in web forums.
We will show their importance in the experiment part.
(II) The importance of each web page.
Baeza-Yates et.
al. [1] tried to determine the weight of each page based on some strategies similar to PageRank.
Wolf et.
al. [16] assigned the weight of each page based on the embarrassment metric of users' search results.
In the user-centric crawling strategy [13], the targets are mined from user queries to guide the refreshing schedule of a generic search engine; First of all, some pages may have equal importance weight but different update frequency, thus only measuring the importance of each web page is insufficient.
Second, both static rank and content importance are useless in web forums.
Most pages in web forums are dynamic pages which are generated using some pre-defined templates.
It is very hard to compute their PageRank-like static rank since there are medial links among these pages.
Furthermore, the content importance measurement is also useless.
Once a post is generated, this post always exists unless the author deletes it manually.
Before we get these post information, it is very hard to predict their importance.
However, once we have their content importance information we usually do not need to revisit it anymore.
Some work named focused crawling attempts to only retrieve web pages that are relevant to some pre-defined topics [5,9,11] or some labeled examples [14] by assigning pages similar to the target page a higher weight.
The target descriptions in focused crawling are quite different in various applications.
(III) The information longevity of each web page.
Olston et.
al. [12] introduced the longevity to determine revisiting frequency of each web page.
However, the information longevity in forums is useless since once a post never disappears unless being deleted.
This is one of the major differences between general web pages and forum web pages.
Moreover, its three generative models are still based on the poisson distribution and some modified forms.Realizing the importance of forum data and the challenges in forum crawling, Cai et al. [4,15] studied how to reconstruct the sitemap of a target web forum, and how to choose an optimal traversal path to crawl informative pages only.
However, this work only addressed the problem of fetching as much as possible valuable data, but left the problem of refreshing previously downloaded pages [12] untouched.
All the existing methods ignore the tradeoff between discovering new threads and refreshing existing threads.
Intuitively, index-of-thread pages should get higher update frequency than post-of-thread pages because any users' update activities will change index-of-thread pages.
However, because the number of post-of-thread pages is much larger than index-of-thread pages, index-of-thread pages may be overwhelmed by a mass of post-of-thread pages even if only a small percentage of post-of-thread pages are very active.
These post-of-thread pages will occupy most bandwidth, and as a result new discussion threads cannot be downloaded efficiently.
Unfortunately, few of existing methods has taken this issue into account.
We will show its importance in the experiment part.
In this section, we first describe the system overview of the proposed solution, and then introduce the details of each component.
The proposed solution consists of two parts, offline mining and online crawling, as shown in Fig. 2.
The offline mining is to learn the sitemap of a given forum site with a few pre-sampled pages from the target site.
Through analyzing these sampled pages, we can find out how many kinds of pages are there in that site, and how these pages are linked with each others [4].
This site-level knowledge is then employed in the online crawling process.The online crawling consists of three steps: (a) identifying index lists and post lists; (b) predicting the update frequency of a list using a regression model; and (c) balancing bandwidth by adjusting the numbers of various lists in the crawling queue, as shown in the left part of Fig. 2.
Given a new crawled page, we first identify if it is an index-of-thread page or a post-of-thread page according to its layout information.
Since an index list or post list consists of multiple pages, to reconstruct a list, we concatenate corresponding pages following the detected pagination links [15].
Then, for each reconstructed list, we extract the timestamps of the records (either posts or thread entities) containing in that list.
Several statistics are then proposed based on the timestamps to characterize the growth of a list; and a regression model is trained to predict the arrival time of the next record of that list.
Finally, given a fixed bandwidth, we balance the numbers of index /post lists, to satisfy the requirements of both completeness and timeliness.
The sitemap knowledge is an organization graph of a give forum site, which is the fundamental component of the proposed crawling method in this paper.
The details of its generation algorithm can be found in [4,15], here we just briefly describe it as follows.
To estimate the sitemap, we first sample a few pages from the target site.
Because the sampling quality is crucial to the whole mining process, to diversity the sampled pages in terms of page layout and to retrieve pages at deep levels, we adopt a combined strategy of breadth-first and depth-first using a double-ended queue.
In the implementation, we try to push as many as possible unseen URLs from each crawled page to the queue, and then randomly pop a URL from the front or the end of the queue for a new sampling.
In practice, it was found that sampling a few thousands pages is sufficient to reconstruct the sitemap for most forum sites.
After that, pages with similar layout structures are further clustered into groups using the single linkage algorithm, as marked with blue nodes in Fig. 3.
In our approach, we utilize the repetitive regions to characterize the layout of each page.
Repetitive regions are very popular in forum pages to present data records stored in a database.
Considering that two similar pages usually have different numbers of advertisements, images, and even some complex sub-structure embedded in user posts, the repetitive region-based representation is more robust than the whole DOM tree [17,18].
Finally, all possible links among various page groups are established, if in the source group there is a page having an out-link pointing to another page in the target group.After that, a classifier is introduced to identify the node of index-of-thread pages or post-of-thread pages in a sitemap.
The classifier is built by some simple yet robust features.
For example, the node of post-of-thread pages always contains the largest number of pages in sitemap, and the index-ofthread pages should be the parents of post-of-thread pages, as marked with the red rectangle or green rectangle in Fig.3.
The classifier is site-independent and can be used in different forum sites.
We then map each individual page into one of the nodes by evaluate its layout similarity such as sim(p new , N index ) and sim(p new , N post ); and δ is the threshold for decision.
We concatenate all the individual pages belonging to the same index list or post list together.
The details of this process is described in Algorithm 1.
The above process is fully automatic.
With the concatenated index lists or post lists, our crawling method becomes a list-wise strat- for all page p i in N index , and OutLinks i is the pagination out-links list of p i do 5:If (1) ∃linkj ∈ OutLinksi and pnew = linkj, or (2) ∃link k ∈ OutLinks new and p i = link k .
6:Concatenate pnew into the index list of pi.
end for 8: else if sim(p new , N post ) < δ then 9:for all page pi in Npost, and OutLinksi is the pagination out-links list of p i do 10:If (1) ∃linkj ∈ OutLinksi and pnew = linkj, or (2) ∃link k ∈ OutLinks new and p i = link k .
11:Concatenate pnew into the post list of pi.
end for 13: end if egy rather than page-level strategies used by most existing general crawlers.
To predict the update frequency of a list more accurately, we need to first extract the time information of each record (thread creation time in index lists or post time in post lists).
It is difficult to extract the correct time information due to the existence of noisy time records in forum pages, such as users' registration time, last login time and so on.
In Fig.4, the time contents in orange rectangle are all noisy content while only the purple one is the correct time information.There are three steps to extract the time information.
We first get the timestamp candidates whose content is a short one and contains digit string such as mm-dd-yyyy or dd/mm/yyyy, or some specific words such as Monday, Tuesday, January, February, etc.
Second, we align the html elements containing time information into several groups based for all page p j in L do 4:Get all cadidates tsList j in p j whose DOM path is pathi and sort them in their appearance order.
for all time candidate tc k in tsList j , 1 ≤ k ≤ M and M is the length of tsj do 6:if tc k is earlier than tc k+1 then 7:SeqOrd + + 8:else if tc k is later than tc k+1 then 9:RevOrd + + 10: end if 11: end for 12: end for 13: Set Order The latest replies in each thread can be easily found by revisiting post lists.
Similarly, the new discussion threads can be discovered by revisiting index lists.
The problem of incremental crawling of web forums is converted to how to estimate the update frequency and predict when the next new record of a list (post list or index list) arrives.
Based on the timestamp of each record, we propose several features to help predict the update frequency and describe them in Table.1.
Furthermore, for index lists, we analyze the average thread activity in forum sites.
We process all the threads and calculate their active time by checking the time of the first post and the last post in each thread.
The result is shown in Fig. 5.
The figure represents the percentage of threads with different active time.
From the figure we can see the percetage of active thread drops significantly when the active time becomes longer.
More than 40% threads keep active no longer than 24 hours and 70% threads are no longer than 3 days.
This is one of the major reasons why forum incremental crawling strategy is different from tra-¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¦ § ¨ © § ¡ ¡ ¢ ¢ £ ¤ ¥ § !
¦ "# $ % Figure 5: The thread activity analysis.ditional incremental crawling strategies.
Once a thread is created, it usually becomes static after a few days when there is no discussion activity.
Thus we introduce a state indicator to avoid bandwidth waste.
Suppose there is no discussion activity for ∆t na time since the last post.
We compute the standard deviation of time interval ∆t sd by∆t sd = 1/(N − 1) · N −2 i=1 (∆ti − ∆tavg) 2 ,where N is the number of post records.
If ∆t na − ∆t avg > α · ∆t sd , we may set ds = 1, otherwise, ds = 0.
This factor is for index lists only.To combine these factors together, we leverage a linear regression model which is a lightweight computation model and is efficient for online processing.F (x) = w 0 + N i=1 w i · x i(1)For each forum site, we train two models F list (x) and F post (x) for index lists and post lists separately.
The two models are kept updated during the crawling process for the new crawled pages.
In practice, we update the two models every two months.
By setting x0 = 1, we can get the corresponding W by Equation 2.
W = (X T · X) −1 · X T · Y(2)In the crawling process, we predict the new coming records of each list by ∆I = (CT − LT )/F (x), where CT is the current time and LT is the last revisit time (by crawler).
We use the predicted ∆I to schedule the crawling process.
Though a list may contain multiple pages, we do not need to revisit all of them.
We sort the pages in each list based on the timestamps of the records in each page.
For an index list, if there are N um list records in each index-of-thread page, and we only need to revisit the top ∆I list /N um list pages, where ∆I list = (CT − LT )/F list (x).
For a post list, we need to revisit the last page or the new discovered pages.
As we have discussed in previous sections, we need to make a tradeoff between discovering new pages and refreshing existing pages.
The number of post-of-thread pages is much larger than index-of-thread.
Even if only a small percentage of post-of-thread pages are very active, index-of-thread pages may be overwhelmed by a mass of post-of-thread pages, which will occupy most bandwidth.
In this case, we need to allocate a dedicated bandwidth for post-of-thread pages since we can only get new discussion threads from them.
We introduce a hyper bandwidth control strategy to balance the bandwidth between two kinds of list.
The ratio between index lists and post lists can be defined as: ratio ∝ N index N P ost (3) Different from previous work which only considered revisiting existing pages, in this paper, the scenario we have considered is a real case.
The crawler is required to crawl a target forum site starting from its portal page.
To have a thorough evaluation, a crawling task needs to last for one year or even longer so that we can measure the crawling performance using different metrics at different periods, for example, the warming up stage and the stable stage.
This creates the need to build a simulation platform to compare different algorithms because the real-time crawling cannot be repeated for multiple crawling approaches.
Before we describe the experimental details, we first introduce the experimental setup and the evaluation metrics.
To evaluate the performance of our system on various situations, 18 forums were selected in diverse categories (including bicycle, photography, travel, computer technique, and some general forums) in the experiments, as listed in Table 2.
The average length of service of 18 sites is about 4.08 years.As we wish to evaluate different methods in a long time period (about one year) under several different bandwidth situations while we still want the evaluation to be repeated under the same environment to fairly compare different approaches.
Because it is impractical to repeat a long lasting crawling process for many times on real sites, we built a simulation platform to facilitate our evaluation.
Typically, a forum hosting server organizes forum data using a backend database, and dynamically generates forum pages using page templates.
To build the simulation platform for a fo- More precisely, we first mirrored the 18 forum sites using a customized commercial search engine crawler.
The crawler was configured to be domain-limited and depth-unlimited.
For each site, the crawler started from its portal page and followed any links within that domain, and a unique URL address was followed only once.
Consequently, the crawler mirrored all the pages up to June 2008 from 18 forum sites.
The mirrored dataset contains 990,476 pages and 5,407,854 individual posts, from March 1999 to June 2008.
Using manually wrote data extraction wrappers, we parsed all the pages and stored all the data records in a database.The basic behavior of this simulation platform is to response for a URL request associated with a timestamp.
We wrote 18 page generators for 18 forum sites to simulate the responses to requests.
For any requested URL, since all the corresponding records can be accessed from the database, we can generate a HTML page with the same contents, layout, and related links as the one in the real site.
Here we make an assumption that a post will never be deleted after it is generated.
Based on this simple yet reasonable assumption, we can easily figure out at any given time whether a record exists based on its post time and how records are organized based on the forum's layout, and therefore be able to restore a snapshot of a forum site to the given time.This simulation platform was used in all the following crawling experiments, and provided a fair experimental environment to each crawling approach.
Assuming a fixed bandwidth, each crawler was required to crawl the given forum site starting its portal page and from the dummy time period 2006-01-01 to 2007-01-01.
To differentiate the advantage of list-wise strategy and the benefit of bandwidth control, we implemented two variants of our method: (1) list-wise strategy (LWS); (2) list-wise strategy + bandwidth control (LWS+BC).
Since the Curve-Fitting policy and Bound-Based policy in [12] are the state-of-the-art approaches and are more relevant to our work, we also included them in the experiments.
The original Bound-Based policy only crawl existing pages.
We have tried our best to adapt the structure-driven-based approach to forum crawling by: 1) giving a new discovered URL the highest weight; and 2) relaxing the interval condition for adjusting refresh period and reference time to accommodate the high frequent update situation in forum sites.We also introduced an oracle strategy for comparison.
In the oracle strategy, every update activity of each page in the target site is supposed to be known exactly.
Given the fixed bandwidth, the oracle policy can choose the pages with more new valuable information to visit.
The oracle strategy is an ideal policy and an upper bound for other crawlers.
Following pervious work, we assume that the costs of visiting different pages are equal, and we measure the bandwidth cost as the total number of pages which are required to crawl in a given time period [12].
To evaluate the overall crawling performance for each approach, we measure from the following three aspects:• Bandwidth Utilization.
If the bandwidth is fixed, bandwidth utilization is an important measure of crawler's capability.
This measurement is used to analyze if the crawler can make the best use of a limited bandwidth:B = Inew IB (4)where IB is the bandwidth cost defined as the total number of pages crawled in a time unit and I new is the number of pages containing new information compared to the existing indexed repository.
• Coverage.
A crawler needs to balance between fetching new pages and refreshing existing pages.
If a crawler wastes too much bandwidth to refresh previously downloaded pages, it may not be able to crawl all new requested pages, and vice versa.
To measure the issue, we define coverage as follows:Cov = I crawl I all(5)where I all is the measurement of valuable information existing in the target site and I crawl is the measurement of valuable information having been downloaded by crawler.
• Timeliness.
To measure if we can fetch each post "timely", we introduce timeliness.
Suppose there are N elements of valuable information which we have downloaded.
The timeliness is defined as:T = 1 N N i=1 ∆ti (6)where ∆t i represents the time period from its updating time to its downloading time.
If the element was updated three day ago and we downloaded it one day ago, ∆ti is two day.
This value is smaller the better.In the forum crawling task, different from general web sites, once a post is submitted, the post will always exist until it is manually deleted by the creator or administrator.In this paper, we use the number of unique posts to measure the valuable information in forum sites.
In the definition of coverage, I all should be the number of unique posts existing in the target forum site and I crawl should be the number of unique posts having been downloaded.
In the definition of timeliness, ∆ti is the time period from a post's creating time to its downloading time.
If we download a post one day after the post was created, ∆ti is one day.
To make a fair evaluation for all crawlers, we require all the crawlers to start from the portal page of each site with a fixed bandwidth 3000 pages per day.
The crawlers begin to crawl pages from the dummy time 2007-01-01 and last about one year.
To illustrate the performance changes of all crawlers in different time periods, we calculate the average performance in everyday in terms of the aforementioned three measurements and present the results in Fig. 6.
From the figure we can see the performance changes apparently in the first 100 days and become stable after about 120 days.
This is due to the so called warming up stage during which a crawler needs to first mirror the existing pages and after that it can download new pages.
Suppose there are P old posts existing before the crawler starts, ∆P new posts will be generated every day and the bandwidth allows the crawler to crawl B posts per day.
At the d th day, there are about P old + ∆P · d posts existing in the target site.
At the beginning, since P old ∆P ·d, the crawler is required to download almost all posts belonging to P old .
These are all new valuable information compared to the indexed repository.
This is why in the first 100 days the bandwidth utilization was approximate to 1, the Coverage increases quickly and the Bandwidth Utilization decreases quickly.
We call this stage the warming up stage for the crawler.
After about P old /(B − ∆P ) days, the crawler may have finished downloading all old posts and begins to only focus on the posts belonging to ∆P every day and the performance becomes stable.Whatever refresh strategy a crawler chooses, if it only assigns new valuable information with the highest weight, In general, the oracle method always performs the best in all measurements and acts as an ideal method.
Beside the oracle method, the LWS+BC performs significantly better than other methods in terms of timeliness.
The average coverage per day for all methods becomes to 100% after 100 days.
This is because the bandwidth is enough and these methods can archive most historical records.
But the performance of these methods on fetching daily new records are different as shown in timeliness results.
For the given bandwidth, the timeliness of LWS+BC will decrease after 100 days.
This is because LWS+BC can save enough buffer to catch up the update progress for new posts after it finishes crawling all existing posts.
The LWS can just keep the timeliness stable because it does not have additional bandwidth to catch up the update progress.
The bound-based and curve-fitting policies get very similar performance.
The timeliness of them all increases (note that the smaller the better for the timeliness measure) since they cannot fetch new posts timely and thus delay the downloading of new posts.
We will analyze them in next section.
¡ ¢ £ ¢ ¤ ¥ ¦ § ¨ © § § ! "
# $ % & ' " # $ ¢ ( ¢ ¡ ) ¥ ¥ ) ( ( ) 0 0 ) 1 ¦ 2 ¦ 3 4 5 6 & 7 8 9 @ A & B !
@ ' 8 C !
A D E F F E 9 G (a) Bandwidth Utilization ¡ ¢ £ ¢ ¤ ¥ ¦ § ¨ © ¨ ! "
# ¢ $ ¢ ¡ % ¥ ¥ % $ $ % & & % ' ¦ ( ) 0 1 " 2 3 4 5 6 " 7 5 # 3 8 6 9 @ A A @ 4 B (b) Coverage ¡ ¡ ¢ ¡ ¡ £ ¤ ¥ ¦ § ¤ ¨ ¨ © !
!
¡ " ¡ ¡ ¡ # ¡ $ ¡ ¡ $ # ¡ % ¡ ¡ % # ¡ " ¡ ¡ " # ¡ &¦ £ ' ( ) 0¡ ¢ ¡ ¢ £ ¤ ¥ ¦ § ¨ ¦ ¤ © ¤ ¦ ¤ ! "
# $ % & & % ' ( ) 0 ! '
1 # ) 2 1 3 4 5 ¡ £ ¢ 6 7 8 £ 9 @ ¥ A ¤ B C D E F G E H I P Q C R S T U Q S V E C W X 3 4 5 3 4 5 Y ) (a) Bandwidth Utilization ¡ ¢ ¢ ¡ ¢ ¢ £ ¤ ¥ ¦ § ¨ © § ! "
# # " $ % & ' $ ( & ) ( ¡ ¢ 0 ¡ ¢ 0 £ ¤ 1 2 3 £ 4 5 ¥ 6 7 8 9 @ A 9 B C D E 7 F G H I E G P 9 7 Q R S T U S T U V & (b) Coverage ¡ ¡ ¢ ¡ ¡ £ ¤ ¥ ¦ § ¤ ¨ ¨ © !
! "
# $ % " & $ ' & ( ) 0 ¡ 1 ¡ ¡ 2 ¡ ¡ ¡ 1 ¡ ¡ ¡ 3 ¡ ¡ ¡ ¡ ¡ ¡ 4 ¡ ¡ ¡ ¢ ¡ ¡ ¡ 5¦ 6 7 8 9 @ A 9 B C D E 7 F G H I E G P 9 7 Q R ( ) 0 ( ) 0 S $ (c) Timeliness A crawler for a commercial search engine is usually not allowed to frequently restart.
The performance after its warming-up stage is thus more meaningful.
We evaluated all methods under various bandwidth conditions and all crawlers were required to start from the portal page of the given site.
The crawlers start from the dummy time 2006-01-01 and last about one year.
We only calculate the average performance of different methods for the last 90 days from 2006-10-01 to 2006-12-31 and present the results in Fig. 7.
The curve-fitting policy and bound-based policy perform similarly on timeliness and bandwidth utilization while curvefitting policy performs slightly better on coverage.
This is consistent with their original results in [12].
LWS is better than curve-fitting policy and bound-based policy on all measurements, because we can estimate the update frequency more accurately with list-wise information.
Furthermore, we can also avoid visiting all the pages in a list and thus save considerable bandwidth.LWS+BC is the best policy which further improves the performance apparently compared to LWS.
Though the average coverage of different methods seems very close (from 0.98 to 1.0), the actual gap is relatively large when it multiplies 1 million pages or 5 million posts.
And the gap may become even larger when we continue to crawl more sites or the bandwidth becomes more limited, as shown in the Fig. 7(b).
Although LWS can estimate the update frequency for index lists and post lists relatively more accurately, it is still very hard to balance these two kinds of pages.
In contrast, the bandwidth control policy is more effective to balance between fetching valuable data and refreshing downloaded pages.
Such a policy only slightly affects postof-thread pages but benefits the index-of-thread pages a lot.
When the bandwidth is set to 3000 pages per day, the average timeliness for LWS+BC is about 65 minutes while the average timeliness for bound-based policy or curve-fitting policy is about 170 minutes and 165 minutes respectively.
This shows that LWS+BC is 260% faster than the boundbased policy or curve-fitting policy and thus is capable of downloading new posts timely.
Meanwhile, it also achieve a high coverage ratio compared to other methods.
To get more insights to this problem, we evaluate these two kinds of pages separately in the next section.
Given a fixed bandwidth of crawling 3000 pages per day, we evaluated the performance for index lists and post lists separately and showed the results in Table 3.
From the table, we can see that LWS improves the performance for both two kinds of pages.
The timeliness of both index-of-thread pages and post-of-thread pages decreases significantly in Table 3 compared to the curve-fitting policy and bound-based policy.
This is because LWS leverages more information and can estimate the update frequency for both two kinds of pages more accurately.LWS+BC further improves the performance for index-ofthread pages compared to LWS.
The timeliness of indexof-thread pages decreases significantly in Table 3 while the timeliness of post-of-thread pages remains the same.
It confirms our previous assumption that bandwidth control can assign the right ratio according to the real update numbers for different kinds of pages.
Realizing the importance of forum data and the challenges in forum crawling, in this paper, we proposed a list-wise strategy for incremental crawling of web forums.
Instead of treating each individual page independently, as did in most existing methods, we have made two improvements.
First, we analyze each index list or post list as a whole by concatenating multiple pages belong to this list together.
And then we take into account user behavior-related statistics, for example, the number of records and the timestamp of each record.
Such information is of great help for developing an efficient recrawl strategy.
Second, we balance discovering new pages and refreshing existing pages by introducing a bandwidth control policy for index lists and post lists.
To evaluate the proposed crawling strategy, we conducted extensive experiments on 18 forums, compared it with several state-of-the-art methods, and evaluated it under various situations, including different stages through one year's crawling simulation, different bandwidths, and different kinds of pages.
Experimental results show that our method outperforms state-of-the-art methods in terms of bandwidth utilization, coverage, and timeliness in all situations.
The new strategy is 260% faster than existing methods and meanwhile it also achieves a high coverage ratio.
