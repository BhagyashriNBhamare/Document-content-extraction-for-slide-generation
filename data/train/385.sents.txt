Principal component analysis (PCA) is a widely used technique for data analysis and dimensionality reduction.
Eigenvalue decomposition is the standard algorithm for solving PCA, but a number of other algorithms have been proposed.
For instance, the EM algorithm is much more efficient in case of high dimensionality and a small number of principal components.
We study a case where the data are high-dimensional and a majority of the values are missing.
In this case, both of these algorithms prove inadequate.
We propose using a gradient descent algorithm inspired by Oja's rule, and speeding it up by an approximate Newton's method.
The computational complexity of the proposed method is linear to the number of observed values in the data and to the number of principal components.
The experiments with Netflix data confirm that the proposed algorithm is about ten times faster than any of the four comparison methods.
Principal component analysis (PCA) [1][2][3][4][5][6] is a classic technique in data analysis.
It can be used for compressing higher dimensional data sets to lower dimensional ones for data analysis, visualization, feature extraction, or data compression.
PCA can be derived from a number of starting points and optimization criteria [2][3][4].
The most important of these are minimization of the mean-square error in data compression, finding mutually orthogonal directions in the data having maximal variances, and decorrelation of the data using orthogonal transformations [5].
While standard PCA is a very well-established linear statistical technique based on second-order statistics (covariances), it has recently been extended into various directions and considered from novel viewpoints.
For example, various adaptive algorithms for PCA have been considered and reviewed in [4,6].
Fairly recently, PCA was shown to emerge as a maximum likelihood solution from a probabilistic latent variable model independently by several authors; see [3] for a discussion and references.In this paper, we study PCA in the case where most of the data values are missing (or unknown).
Common algorithms for solving PCA prove to be inadequate in this case, and we thus propose a new algorithm.
The problem of overfitting is also studied and solutions based on regularization and variational Bayesian learning are given.
Principal subspace and components Assume that we have n d-dimensional data vectors x 1 , x 2 , . . . , x n , which form the d×n data matrix X = [x 1 , x 2 , . . . , x n ].
The matrix X is decomposed intoX ≈ AS,(1)where A is a d × c matrix, S is a c × n matrix and c ≤ d ≤ n. Principal subspace methods [6,4] find A and S such that the reconstruction errorC = X − AS 2 F = d i=1 n j=1 (x ij − c k=1 a ik s kj ) 2 ,(2)is minimized.
There F denotes the Frobenius norm, and x ij , a ik , and s kj elements of the matrices X, A, and S, respectively.
Typically the row-wise mean is removed from X as a preprocessing step.
Without any further constraints, there exist infinitely many ways to perform such a decomposition.
However, the subspace spanned by the column vectors of the matrix A, called the principal subspace, is unique.
In PCA, these vectors are mutually orthogonal and have unit length.
Further, for each k = 1, . . . , c, the first k vectors form the k dimensional principal subspace.
This makes the solution practically unique, see [4,2,5] for details.There are many ways to determine the principal subspace and components [6,4,2].
We will discuss three common methods that can be adapted for the case of missing values.Singular Value Decomposition PCA can be determined by using the singular value decomposition (SVD) [5] X = UΣV T ,(3)where U is a d × d orthogonal matrix, V is an n × n orthogonal matrix and Σ is a d × n pseudodiagonal matrix (diagonal if d = n) with the singular values on the main diagonal [5].
The PCA solution is obtained by selecting the c largest singular values from Σ, by forming A from the corresponding c columns of U, and S from the corresponding c rows of ΣV T .
Note that PCA can equivalently be defined using the eigendecomposition of the d × d covariance matrix C of the column vectors of the data matrix X:C = 1 n XX T = UDU T ,(4)Here, the diagonal matrix D contains the eigenvalues of C, and the columns of the matrix U contain the unit-length eigenvectors of C in the same order [6,4,2,5].
Again, the columns of U corresponding to the largest eigenvalues are taken as A, and S is computed as A T X.
This approach can be more efficient for cases where d ≪ n, since it avoids the n × n matrix.
The EM algorithm for solving PCA [7] iterates updating A and S alternately.
When either of these matrices is fixed, the other one can be obtained from an ordinary least-squares problem.
The algorithm alternates between the updatesS ← (A T A) −1 A T X , A ← XS T (SS T ) −1 .
(5)This iteration is especially efficient when only a few principal components are needed, that is c ≪ d [7].
Subspace Learning Algorithm It is also possible to minimize the reconstruction error (2) by any optimization algorithm.
Applying the gradient descent algorithm yields rules for simultaneous updatesA ← A + γ(X − AS)S T , S ← S + γA T (X − AS) .
(6)where γ > 0 is called the learning rate.
Oja-Karhunen learning algorithm [8,9,6,4] is an online learning method that uses the EM formula for computing S and the gradient for updating A, a single data vector at a time.A possible speed-up to the subspace learning algorithm is to use the natural gradient [10] for the space of matrices.
This yields the update rulesA ← A + γ(X − AS)S T A T A , S ← S + γSS T A T (X − AS) .
(7)If needed, the end result of subspace analysis can be transformed into the PCA solution, for instance, by computing the eigenvalue decomposition SS T = U S D S U T S and the singular value decompositionAU S D 1/2 S = U A Σ A V T A .
The transformed A is formed from the first c columns of U A and the transformed S from the first c rows ofΣ A V T A D −1/2 S U T S S.Note that the required decompositions are computationally lighter than the ones done to the data matrix directly.
Let us consider the same problem when the data matrix has missing entries 1 .
In the following there are N = 9 observed values and 6 missing values marked with a question mark (?)
:X =   −1 +1 0 0 ?
−1 +1 ?
?
0 ?
?
−1 +1 ?  
.
(8)We would like to find A and S such that X ≈ AS for the observed data samples.
The rest of the product AS represents the reconstruction of missing values.Adapting SVD: Imputation Algorithm One can use the SVD approach (4) in order to find an approximate solution to the PCA problem.
However, estimating the covariance matrix C becomes very difficult when there are lots of missing values.
If we estimate C leaving out terms with missing values from the average, we get for the estimate of the covariance matrixC = 1 n XX T =   0.5 1 0 1 0.667 ?
0 ?
1   .
(9)There are at least two problems.
First, the estimated covariance 1 between the first and second components is larger than their estimated variances 0.5 and 0.667.
This is clearly wrong, and leads to the situation where the covariance matrix is not positive (semi)definite and some of its eigenvalues are negative.
Secondly, the covariance between the second and the third component could not be estimated at all 2 .
Another option is to complete the data matrix by iteratively imputing the missing values (see, e.g., [11]).
Initially, the missing values can be replaced by zeroes.
The covariance matrix of the complete data can be estimated without the problems mentioned above.
Now, the product AS can be used as a better estimate for the missing values, and this process can be iterated until convergence.
This approach requires the use of the complete data matrix, and therefore it is computationally very expensive if a large part of the data matrix is missing.
The time complexity of computing the sample covariance matrix explicitly is O(nd 2 ).
We will further refer to this approach as the imputation algorithm.Note that after convergence, the missing values do not contribute to the reconstruction error (2).
This means that the imputation algorithm leads to the solution which minimizes the reconstruction error of observed values only.Adapting the EM Algorithm Grung and Manne [12] studied the EM algorithm in the case of missing values.
Experiments showed a faster convergence compared to the iterative imputation algorithm.
The computational complexity is O(N c 2 + nc 3 ) per iteration, where N is the number of observed values, assuming na¨ıvena¨ıve matrix multiplications and inversions but exploiting sparsity.
This is quite a bit heavier than EM with complete data, whose complexity is O(ndc) [7] per iteration.Adapting the Subspace Learning Algorithm The subspace learning algorithm works in a straightforward manner also in the presence of missing values.
We just take the sum over only those indices i and j for which the data entry x ij (the ijth element of X) is observed, in short (i, j) ∈ O.
The cost function isC = (i,j)∈O e 2 ij , with e ij = x ij − c k=1 a ik s kj .
(10)and its partial derivatives are∂C ∂a il = −2 j|(i,j)∈O e ij s lj , ∂C ∂s lj = −2 i|(i,j)∈O e ij a il .
(11)The update rules for gradient descent areA ← A + γ ∂C ∂A , S ← S + γ ∂C ∂S(12)and the update rules for natural gradient descent areA ← A + γ ∂C ∂A A T A , S ← S + γSS T ∂C ∂S .
(13)We propose a novel speed-up to the original simple gradient descent algorithm.
In Newton's method for optimization, the gradient is multiplied by the inverse of the Hessian matrix.
Newton's method is known to converge fast especially in the vicinity of the optimum, but using the full Hessian is computationally too demanding in truly high-dimensional problems.
Here we use only the diagonal part of the Hessian matrix.
We also include a control parameter α that allows the learning algorithm to interpolate between the standard gradient descent (α = 0) and the diagonal Newton's method (α = 1), much like the well known Levenberg-Marquardt algorithm.
The learning rules then take the forma il ← a il − γ ′ ∂ 2 C ∂a 2 il −α ∂C ∂a il = a il + γ j|(i,j)∈O e ij s lj j|(i,j)∈O s 2 lj α ,(14)s lj ← s lj − γ ′ ∂ 2 C ∂s 2 lj −α ∂C ∂s lj = s lj + γ i|(i,j)∈O e ij a il i|(i,j)∈O a 2 il α .
(15)The computational complexity is O(N c + nc) per iteration.
A trained PCA model can be used for reconstructing missing values:ˆ x ij = c k=1 a ik s kj , (i, j) / ∈ O .
(16)Although PCA performs a linear transformation of data, overfitting is a serious problem for large-scale problems with lots of missing values.
This happens when the value of the cost function C in Eq.
(10) is small for training data, but the quality of prediction (16) is poor for new data.
For further details, see [13].
Regularization A popular way to regularize ill-posed problems is penalizing the use of large parameter values by adding a proper penalty term into the cost function; see for example [3].
In our case, one can modify the cost function in Eq.
(2) as follows:C λ = (i,j)∈O e 2 ij + λ(A 2 F + S 2 F ) .
(17)This has the effect that the parameters that do not have significant evidence will decay towards zero.
A more general penalization would use different regularization parameters λ for different parts of A and S. For example, one can use a λ k parameter of its own for each of the column vectors a k of A and the row vectors s k of S. Note that since the columns of A can be scaled arbitrarily by rescaling the rows of S accordingly, one can fix the regularization term for a k , for instance, to unity.It is well known that an equivalent optimization problem can be obtained using a probabilistic formulation with (independent) Gaussian priors and a Gaussian noise model:p(x ij | A, S) = N x ij ; c k=1 a ik s kj , v x ,(18)p(a ik ) = N (a ik ; 0, 1) , p(s kj ) = N (s kj ; 0, v sk ) ,where N (x; m, v) denotes the random variable x having a Gaussian distribution with the mean m and variance v.
The regularization parameter λ k = v sk /v x is the ratio of the prior variances v sk and v x .
Then, the cost function (ignoring constants) is minus logarithm of the posterior for A and S:C BR = (i,j)∈O e 2 ij /v x + ln v x + d i=1 c k=1 a 2 ik + c k=1 n j=1 s 2 kj /v sk + ln v sk(20)An attractive property of the Bayesian formulation is that it provides a natural way to choose the regularization constants.
This can be done using the evidence framework (see, e.g., [3]) or simply by minimizing C BR by setting v x , v sk to the means of e 2 ij and s 2 kj respectively.
We will use the latter approach and refer to it as regularized PCA.Note that in case of joint optimization of C BR w.r.t. a ik , s kj , v sk , and v x , the cost function (20) has a trivial minimum with s kj = 0, v sk → 0.
We try to avoid this minimum by using an orthogonalized solution provided by unregularized PCA from the learning rules (14) and (15) for initialization.
Note also that setting v sk to small values for some components k is equivalent to removal of irrelevant components from the model.
This allows for automatic determination of the proper dimensionality c instead of discrete model comparison (see, e.g., [14]).
This justifies using separate v sk in the model in (19).
Variational Bayesian Learning Variational Bayesian (VB) learning provides even stronger tools against overfitting.
VB version of PCA by [14] approximates the joint posterior of the unknown quantities using a simple multivariate distribution.
Each model parameter is described a posteriori using independent Gaussian distributions.
The means can then be used as point estimates of the parameters, while the variances give at least a crude estimate of the reliability of these point estimates.
The method in [14] does not extend to missing values easily, but the subspace learning algorithm (Section 3) can be extended to VB.
The derivation is somewhat lengthy, and it is omitted here together with the variational Bayesian learning rules because of space limitations; see [13] for details.
The computational complexity of this method is still O(N c + nc) per iteration, but the VB version is in practice about 2-3 times slower than the original subspace learning algorithm.
Collaborative filtering is the task of predicting preferences (or producing personal recommendations) by using other people's preferences.
The Netflix problem [15] is such a task.
It consists of movie ratings given by n = 480189 customers to d = 17770 movies.
There are N = 100480507 ratings from 1 to 5 given, from which 1408395 ratings are reserved for validation (or probing).
Note that 98.8% of the values are thus missing.
We tried to find c = 15 principal components from the data using a number of methods.
3 We subtracted the mean rating for each movie, assuming 22 extra ratings of 3 for each movie as a Dirichlet prior.Computational Performance In the first set of experiments we compared the computational performance of different algorithms on PCA with missing values.
The root mean square (rms) error is measured on the training data,E O = 1 |O| (i,j)∈O e 2 ij .
All experiments were run on a dual cpu AMD Opteron SE 2220 using Matlab.First, we tested the imputation algorithm.
The first iteration where the missing values are replaced with zeros, was completed in 17 minutes and led to E O = 0.8527.
This iteration was still tolerably fast because the complete data matrix was sparse.
After that, it takes about 30 hours per iteration (we computed 500 rows (and columns) of the covariance matrix at a time because the size of the complete data matrix is huge: d × n > 8 · 10 9 ).
After three iterations, E O was still 0.8513.
Also, estimating the covariance matrix based on only the observed part of the data led to problems as explained in Section 3.
The covariance matrix had both missing values and values out of range.
Using the EM algorithm by [12], the E-step (updating S) takes 7 hours and the M-step (updating A) takes 18 hours.
(There is some room for optimization since we used a straightforward Matlab implementation.)
Each iteration gives a much larger improvement compared to the imputation algorithm, but starting from a random initialization, EM could not reach a good solution in reasonable time.We also tested the subspace learning algorithm described in Section 3 with and without the proposed speed-up.
Each run of the algorithm with different values of the speed-up parameter α was initialized in the same starting point (generated randomly from a normal distribution).
The learning rate γ was adapted such that if an update decreased the cost function, γ was multiplied by 1.1.
Each time an update would increase the cost, the update was canceled and γ was divided by 2.
Figure 1 (left) shows the learning curves for basic gradient descent, natural gradient descent, and the proposed speed-up with the best found parameter value α = 0.625.
The proposed speed-up gave about a tenfold speed-up compared to the gradient descent algorithm even if each iteration took longer.
Natural gradient was slower than the basic gradient.
Table 1 gives a summary of the computational complexities.Overfitting We compared PCA (Section 3), regularized PCA (Section 4) and VB-PCA (Section 4) by computing the rms reconstruction error for the validation set V , that is, testing how the models generalize to new data:E V = 1 |V | (i,j)∈V e 2 ij .
We tested VB-PCA by firstly fixing some of the parameter values (this run is marked as VB1 in Fig. 1 Table 1.
Summary of the computational performance of different methods on the Netflix problem.
Computational complexities (per iteration) assume na¨ıvena¨ıve computation of products and inverses of matrices and ignores the computation of SVD in the imputation algorithm.
While the proposed speed-up makes each iteration slower than the basic gradient update, the time to reach the error level 0.85 is greatly diminished.adapting them (marked as VB2).
We initialized regularized PCA and VB1 using normal PCA learned with α = 0.625 and orthogonalized A, and VB2 using VB1.
The parameter α was set to 2/3.
Fig. 1 (right) shows the results.
The performance of basic PCA starts to degrade during learning, especially using the proposed speed-up.
Natural gradient diminishes this phenomenon known as overlearning, but it is even more effective to use regularization.
The best results were obtained using VB2: The final validation error E V was 0.9180 and the training rms error E O was 0.7826 which is naturally larger than the unregularized E O = 0.7657.
We studied a number of different methods for PCA with sparse data and it turned out that a simple gradient descent approach worked best due to its minimal computational complexity per iteration.
We also gave it a more than tenfold speed-up by using an approximated Newton's method.
We found out empirically that setting the parameter α = 2/3 seems to work well for our problem.
It is left for future work to find out whether this generalizes to other problem settings.
There are also many other ways to speed-up the gradient descent algorithm.
The natural gradient did not help here, but we expect that the conjugate gradient method would.
The modification to the gradient proposed in this paper, could be used together with the conjugate gradient speed-up.
This will be another future research topic.There are also other benefits in solving the PCA problem by gradient descent.
Algorithms that minimize an explicit cost function are rather easy to extend.
The case of variational Bayesian learning applied to PCA was considered in Section 4, but there are many other extensions of PCA, such as using non-Gaussianity, non-linearity, mixture models, and dynamics.The developed algorithms can prove useful in many applications such as bioinformatics, speech processing, and meteorology, in which large-scale datasets with missing values are very common.
The required computational burden is linearly proportional to the number of measured values.
Note also that the proposed techniques provide an analogue of confidence regions showing the reliability of estimated quantities.
