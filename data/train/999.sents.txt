Ranking algorithms, whose goal is to appropriately order a set of objects/documents, are an important component of information retrieval systems.
Previous work on ranking algorithms has focused on cases where only labeled data is available for training (i.e. supervised learning).
In this paper, we consider the question whether unlabeled (test) data can be exploited to improve ranking performance.
We present a framework for transductive learning of ranking functions and show that the answer is affirmative.
Our framework is based on generating better features from the test data (via Ker-nelPCA) and incorporating such features via Boosting, thus learning different ranking functions adapted to the individual test queries.
We evaluate this method on the LETOR (TREC, OHSUMED) dataset and demonstrate significant improvements.
Ranking algorithms, whose goal is to appropriately order a set of objects/documents, are an important component of information retrieval (IR) systems.
In applications such as web search, accurately presenting the most relevant documents to satisfy an information need is of utmost importance: a suboptimal ranking of search results may frustrate the entire user experience.proposed transductive framework.
Section 3 presents the main experimental results.
Then, Section 4 raises several questions and delves into more detailed analyses of results.
Finally, Section 5 draws connections to various related work and Section 6 ends with a discussion on future research.
When designing a machine learning solution to a problem, there are several commonly chosen strategies:(a) Engineering better features to characterize the data.
(b) Designing an objective function that more closely approximates the application-specific evaluation metric.
(c) Developing a more effective algorithm for optimizing the objective.In this work, we explore option (a).
The key idea is to automatically derive better features using the unlabeled test data.
In particular, an unsupervised learning method (i.e. a learning algorithm that does not require labels, e.g. clustering, principal components analysis) is applied to discover salient patterns (Pu) in each list of retrieved test documents du.
The training data is projected on the directions of these patterns and the resulting numerical values are added as new features.
The main assumption is that this new training set better characterizes the test data, and thus should outperform the original training set when learning rank functions.
Input: Train set S = {(q l , d l , y l } l=1.
.
L Input: Test set E = {(qu, du)}u=1.
.
U Input: DISCOVER(), unsupervised algorithm for discovering salient patterns Input: LEARN(), a supervised ranking algorithm Output: Predicted rankings for test: {yu}u=1.
.
U 1: for u = 1 to U do 2: Pu = DISCOVER(du) # find patterns on test data 3: ˆ du = Pu(du) # project test data along Pu 4:for l = 1 to L do 5: ˆ d l = Pu(d l ) # project train data along Pu 6: end for 7:Fu(· ) = LEARN({(q l , ˆ d l , y l )} l=1.
.
L ) 8: yu = Fu( ˆ du) # predict test ranking 9: end for Algorithm 1 shows the pseudocode for this meta-algorithm.
DISCOVER() is the generic unsupervised method.
It is important to note that it is applied to each test document list du separately (line 2).
This is because queries are formulated independently and that different du exhibit different salient patterns.
2 LEARN() is the generic supervised method for learning rank functions.
Since the feature-based representations of the training documents ({d l } l=1.
.
L ) are enriched with additional test-specific features (line 5), we learn a different ranking function Fu(· ) for each test query (line 7).
The usefulness of test-specific features and test-specific ranking functions is illustrated in Figures 1(a) and 1(b).
2 One can imagine extensions where successive queries are dependent.
See Section 6.
Figure 1: Plots of documents for 2 different queries in TREC'04 (y-axis = BM25, x-axis = HITS score).
Relevant documents are dots, irrelevant ones are crosses.
Note that (a) varies on the y-axis whereas (b) varies on the x-axis, implying that query-specific rankers would be beneficial.These are plots of documents from two TREC'04 queries.
The x-axis shows the (normalized) HITS Hub score of a document, while the y-axis shows the (normalized) BM25 score of the extracted title (both are important features for learning).
Irrelevant documents are plotted as small crosses whereas relevant documents are large dots.
For the first query ( Fig. 1(a)), we see that the data varies mostly along the y-axis (BM25); for the second query (Fig 1(b)), the variation is on the x-axis (HITS).
These 2 document lists would be better ranked by 2 different rankers, e.g. one which ranks documents with BM25 > 2.5 as relevant, and the second which ranks documents with HITS > 1.25 as relevant.
A single ranker would find it difficult to simultaneosly rank both lists with high accuracy.In this paper, we use kernel principal components analysis (Kernel PCA) [24] as the unsupervised method and RankBoost [8] as the supervised ranker.
In theory, any algorithm can be plugged in for DISCOVER() and LEARN().
In practice, it is important to consider the interaction between feature and learning, and to ensure that DISCOVER() generates features that LEARN() is able to exploit.
We will argue here that Kernel PCA and RankBoost is one such good pair, but there may well be others.
Principal components analysis (PCA) is a classical technique for extracting patterns and performing dimensionality reduction from unlabeled data.
It computes a linear combination of features, which forms the direction that captures the most variance in the data set.
This direction is called the principal axis, and projection of a data point on it is called the principal component.
The magnitude of the principal component values indicates how close a data point is to the main directions of variation.Kernel PCA [24] is a powerful extension to PCA that computes arbitrary non-linear combinations of features.
As such, it is able to discover patterns arising from higher-order correlations between features.
We can imagine Kernel PCA as a procedure that first maps each data point into a (possibly) non-linear and higher-dimensional space, then performs PCA in that space.
More precisely, let d be a list of m documents and d j be the original feature vector of document j. 3 Then Kernel PCA can be seen as the following procedure: In practice, Kernel PCA uses the dual formulation to avoid solving the above eigen-problem in high dimensional space (this is known as the kernel trick).
See [24] for the derivation; here we only present the steps needed for this paper:1.
Map each doc d j to a new space d j → Φ(d j ),1.
Define a kernel function k(· , · ) : (d j , d j) → R which maps two document vectors to a real number indicating the similarity between the two documents.
2.
There exist kernels of the formk(d j , d j ) = Φ(d j ), Φ(d j ), (i.e. dot product of the document mappings in high-dimensional space) such that the mapping does not need to be computed explicitly to get the kernel value.
3.
Let the m × m matrix K be the kernel values of all pairs of documents in the list.
i.e.K jj = k(d j , d j ) ∀j, j ∈ {1, 2, . . . , m}.4.
Kernel PCA reduces to solving the eigen-problem mλα = Kα.
We pick only the α with the largest eigenvalues.
5.
For a new document d n , its principal component is computed asP m j=1 αjk(d j , d n ).
The kernel function defines the type of non-linear patterns to be extracted.
In this work, we use the following kernels:• Polynomial: Computes dot product of all monomials of order p,k(d j , d j ) = d j , d j p .
• Radial basis function:k(d j , d j ) = exp (− ||d j −d j || 2σ).
This is an isotropic kernel, with bandwidth σ adjusting for smoothness.
• Diffusion kernel [15]: This is suitable for graph data.We generate a k-nearest neighbor graph with documents as nodes and edges defined by the inverse Eu-clidean distance 1/||d j − d j ||.
k(d j , d j )is defined by running a lazy random walk from d j to d j .
A time-constant parameter τ adjusts how long to run the random walk (e.g. larger τ leads to a more uniform distribution).
Performing Kernel PCA with diffusion kernels is equivalent to running PCA on a non-linear manifold.
•Linear: k(d j , d j ) = d j , d j .
Equivalent to PCA.Kernel PCA scales as O(m 3 ), due to solving the eigenproblem on the m × m kernel matrix K. Nevertheless, extremely fast versions have been proposed; for instance, Sparse kernel feature analysis [26] is based on sparsity constraints and can extract patterns in O(m).
RankBoost [8] is an extension of the boosting philosophy [23] for ranking.
In each iteration, RankBoost searches for a weak learner that maximizes the (weighted) pairwise ranking accuracy (defined as the number of document pairs that receive the correct ranking).
A weight distribution is maintained for all pairs of documents.
If a document pair receives an incorrect ranking, its weight is increased, so that next iteration's weak learner will focus on correcting the mistake.It is common to define the weak learner as a non-linear threshold function on the features (decision stump).
For example, a weak learner h(· ) may be h(d j ) = 1 if "BM25 score > 1" and h(d j ) = 0 otherwise.
The final ranking function of RankBoost is a weighted combination of T weak learners:F (d j ) = T X t=1 θtht(d j ),where T is the total number of iterations.
θt is computed during the RankBoost algorithm and its magnitude indicates the relative importance of a given weak learner (feature).
Finally, a ranking over a document list d is obtained by calculating y j = F (d j ) for each document and sorting the list by the value of y j .
There are several advantages to using RankBoost with Kernel PCA in our transductive framework: .
This assumes that good ranking is directly correlated to the feature values (e.g. large BM25 implies more relevance).
Kernel PCA, however, may generate features that have a non-linear relationship to ranking (e.g. large positive and negative deviation from the principal axes implies less relevance).
Non-linear rankers can handle this possibility more robustly.
3.
"Anytime" training: Boosting can be seen as gradient descent in function space [19] and each iteration improves on the training accuracy.
If training time is a concern (e.g. in practical deployment of the transductive framework), then RankBoost can be stopped before reaching T iterations.
The resulting ranker may be less optimized, but it should still give reasonable predictions.Finally, for clarity, we present a concrete walk-through of a search scenario using our transductive framework: 0.
A training set is prepared and stored in memory/disk.
1.
User submits a query qu 2.
Initial IR engine returns possibly relevant documents du.
3.
Run Algorithm 1, lines 2 to 8.
Output yu.
4.
Rank the documents du by yu.
Return results to user.In contrast to the supervised approach, the practical challenges here are: (a) scalable storage of the training set, (b) fast computation of Algorithm 1 (during user wait).
We leave these computational issues as future work.
We perform experiments on the LETOR dataset [18], which contains three sets of document retrieval data: TREC'03, TREC'04, and OHSUMED.
This is a re-ranking (subset ranking) problem, where an initial set of documents have been retrieved and the goal is sort the set in an order most relevant to the query.
The TREC data is a Web Track Topic Distillation task.
The goal is to find webpages that are good entry points to the query topic in the .
gov domain.
The OHSUMED data consists of medical publications and the queries represent medical search needs.
For TREC, documents are labeled {relevant,irrelevant}; an additional label {partially relevant} is provided for OHSUMED.The LETOR dataset conveniently extracts many stateof-the-art features from documents, including BM25 [22], HITS [14], and Language Model [34].
It is a good datasest for our experiments since we will be discovering patterns from features that have already been proven to work.
Table 1 summarizes the data (e.g. in TREC'03, the ranker needs to sort on average 983 documents per query, with only 1 document in the set being relevant); see [18] for details.Our experimental setup compares three rankers.
The baseline is a supervised RankBoost, trained on the original training data.
This is compared with transductive, which is Algorithm 1 with LEARN() being RankBoost and DIS-COVER() being Kernel PCA run 5 times with different kernels:-Polynomial kernel (poly), order=2 -Radial basis function kernel (rbf), bandwidth=1 -Diffusion kernel (diff), time constant=1, 10-nn graph -Diffusion kernel (diff), time constant=10, 10-nn graph -Linear kernel (linear) (corresponds to linear PCA)We create 25 additional features representing the first 5 principal components from each of the above 5 kernels.
The third ranker (combined) performs a simple system combination of baseline and transductive.
For each test query, it first normalizes the outputs yu of baseline and transductive, then returns the averages of these normalized outputs.
While more sophisticated combination methods have been proposed (c.f. [16]), here we investigate whether a simple unweighted average is sufficient to give improvements.Following LETOR convention, each dataset is divided into 5 folds with a 3:1:1 ratio for training, validation, and test set.
We use the validation set to decide which kernels to use in the transductive system.
The transductive system employs all 5 kernels (25 features) in TREC'04 and OHSUMED, but only poly and rbf kernels (10 features) in TREC'03.
We decided not to use the validation set to pick features in a more fine-grained fashion (e.g. how many principal components, what kernel parameters) since we expect the optimal settings to vary for each test query, and relying on RankBoost's inherent feature selection ability is a more efficient and effective solution.
For both transductive and baseline, RankBoost is run for a total of 150 iterations, determined from initial experiments.
We conjecture that transductive may be improved by automatically tuning for the best number of boosting iterations for each test query, but we leave that to future work.The evaluation metrics are mean averaged precision (MAP) and normalized discount cumulative gain (NDCG@n) [12].
We report results from the average of 5-fold cross-validation and judge statistical significance using the dependent t-test.
Figure 2 compares the 3 systems on the 3 datasets.
Table 2 shows the same results in numbers (boldfaced MAP/NDCG numbers indicate a statistically significant improvement (p<0.05) over baseline.)
4 The main observations are: 1.
Both transductive and combined outperform baseline on all datasets and all metrics.
2.
For TREC'03, all improvements are statistically significant.
For TREC'04 and OHSUMED, only combined gives a statistically significant improvement.
3.
In TREC'04 and OHSUMED, combined improves on both transductive and baseline, implying that transductive and baseline make different kinds of errors and thus benefit from system combination.
In TREC'03, combined is worse than transductive-a simple averaging is less likely to work if one system (baseline) is significantly worse in performance than the other.We believe these are very promising results which demonstrate that transductive learning (and the framework we propose in Algorithm 1) has the potential to improve ranking.
We ask several questions in order to examine the properties of the transductive framework in more detail.
These questions seek to answer what is beneficial and not beneficial in the proposed framework.
Are we seeing gains in transductive simply because Kernel PCA extracts good features per se, or particulary because the features are extracted on the test set (i.e. the transductive aspect)?
To answer this, we built a new system, KPCA on train, which runs Kernel PCA on each training list (as opposed to projecting the training lists to principal directions of the test lists).
We then train RankBoost on this data, which is the same training data as baseline except for the additional Kernel PCA features.
This new RankBoost is then evaluated on the test set.
The results (Table 3) show that KPCA on train is worse than transductive (e.g. .2534 vs. .3226 MAP for TREC'03), implying that transductive aspect of adapting to each test query is essential.
Kernel PCA features are in general difficult to interpret because they involve non-linear combinations and the α generated from the eigen-problem represents weights on samples, not features.
We can only get a rough answer by computing the correlation between the values of a Kernel PCA feature and an original feature.
What weak learners h(· ) in transductive's multiple ranking functions (Fu(· ) = P T t=1 θtht(· )) achieve large |θt|?
For instance, how often are Kernel PCA features chosen compared to the original features?
To analyze this, we look at the 25 transductive ranking functions in TREC'04 that improve more than 20% over the baseline.
For each ranking function, we look at the top 5 features and note their type: {original, polynomial, rbf, diffusion, linear}.
24 of 25 functions have both original and Kernel PCA features in (1) orig+diff (7) orig+poly (4) orig+linear (3) orig+poly+diff (4) orig+diff +linear (1) orig+poly +linear (4) orig+rbf+diff (1) Diversity of boosted rankers the top 5, indicating that Kernel PCA features are quite useful.
It is even more interesting to note the distribution of top 5 feature combinations (Figure 3): no single combination is more prevalent than others.
This again supports the intuition that test-specific rankers are better than a single general ranker.
.
Thus, non-linearity is important in most cases, but one should not expect non-linear kernels to always outperform linear ones.
The best strategy is to employ multiple kernels.
In Section 3, we present overall results averaged over all test queries.
A more detailed analysis would include perquery MAP and NDCG.
Figure 4 reports a histogram of queries that are improved vs. degraded by transductive.
For each plot, the bars on the right side indicates the number of queries that improved more than 1%, 20%, and 50% over baseline.
Bars on the left side indicate the number of queries that become more than 1%, 20%, and 50% worse than baseline.One observes that our transductive approach does not give improvements across all queries.
We are seeing gains in Section 3 because the proportion of improved queries is greater than that of degraded queries (especially for TREC'03).
It would be helpful to understand exactly the conditions where the transductive approach is beneficial vs. harmful.
On TREC'03, there is slight evidence showing that transductive seems to benefit queries with poorer baselines (See Figure 5, scatterplot of baseline and transductive MAP scores).
One hypothesis is that the original features of more difficult queries are not sufficiently discriminative, so Kernel PCA has more opportunity to show improvements.
However, this trend is not observed in TREC'04.
We also attempted to see if differences at the query level correlates with e.g. (a) number of documents, (b) number of relevant documents, (c) pairwise ranking accuracy in training, but no single factor reliably predicts whether a query will be benefited by the transductive ranker.
Ideally, both DISCOVER() and LEARN() need to operate at real-time since they are called for each test query.
In our experiments on a Intel x86-32 (3GHz CPU), KernelPCA (implemented in Matlab/C-MEX) took on average 23sec/query for TREC and 4.3sec/query for OHSUMED; Rankboost (implemented in C++) took 1.4sec/iteration for TREC and 0.7sec/iteration for OHSUMED.
The total compute time per query (assuming 150 iterations) is around 233sec/query for TREC and 109sec/query for OHSUMED.
It remains to be seen whether real-time computation can be achieved by better code optimization or novel distributed algorithms.
There are generally two types of problems in "learning to rank with partially-labeled data."
In the problem we consider here, the document lists in our dataset are either fully labeled {d l } l=1.
.
L , or not labeled whatsoever {du}u=1.
.
U .
The second type of problem is concerned with the case when a document list d is only partially-labeled, i.e. some docu- one uses a implicit feedback mechanism [13] to generate labels and some documents simply cannot acquire labels with high confidence.
So far the research community does not yet have precise terminology to differentiate the two problems.
Here we will call Problem One "Semi-supervised Rank Learning" and Problem Two "Learning to Rank with Missing Labels".
Several methods have been proposed for the Missing Labels problem, e.g. [36,30,10,29]: the main idea there is to build a manifold/graph over documents and propagate the rank labels to unlabeled documents.
One can use the propagated labels as the final values for ranking [36] (transductive), or one can train a ranking function using these values as true labels [10,29] (inductive).
One important point about these label propagation methods is that they do not explicitly model the relationship that document d j is ranked above, say, d k .
Instead it simply assumes that the label value for d j is higher than that of d k , and that this information will be preserved during propagation.An alternative approach that explicitly includes pairwise ranking accuracy in the objective is proposed in [1].
It also builds a graph over the unlabeled documents, which acts as a regularizer to ensure that the predicted values are similar for closely-connected documents.
[6] also proposes a graph-based regularization term, but in contrast to [1], it defines the graph nodes not as documents, but as document pairs.
Just as the pairwise formulation allows one to extend Boosting to RankBoost, this formulation allows one to adopt any graph-based semi-supervised classification technique to ranking.
However, generating all possible pairs of documents in a large unlabeled dataset quickly leads to intractable graphs.Most prior work consist of graph-based approaches for the Missing Labels problem.
However, they may be extended to address the Semi-supervised Rank Learning problem if one defines the graph across both d l and du.
For instance, [29] investigates label propagation across queries, but concluded that it is computationally prohibitive.
Beyond the computational issue, however, how to construct a graph across different queries (whose features may be at different scales and not directly comparable) is an open research question.To the best of our knowledge, [27] 5 is the only work that tractably addresses the Semi-supervised Rank Learning problem.
First, it uses a supervised ranker to label the documents in an unlabeled document list; next, it takes the most confident labels as seeds for label propagaton.
A new supervised ranker is then trained to maximize accuracy on the labeled set while minimizing ranking difference to label propagation results.
Thus this is a bootstrapping approach that relies on the initial ranker producing relatively accurate seeds.Our feature-based approach differs from the above graphbased and bootstrapping approaches, and is more similar to work in feature extraction for domain adaptation [3,21] or multi-task learning [2].
In fact, one can consider transductive learning as an extreme form of domain adaptation, where one adapts only to the given test set.Finally, we wish to note that various approaches incorporating similar semi-supervised ideas have been explored in the IR community.
For instance, query expansion by pseudo-relevance feedback [31] can be thought as a queryspecific transductive techniques that uses the bootstrapping assumption (i.e. top retrieved documents are relevant and can be exploited).
We present a flexible transductive framework for learning ranking functions.
The main intuition is based on extracting features from test documents and learning query-specific rankers.
Our experiments using Kernel PCA with RankBoost demonstrate significant improvements on TREC and OHSUMED, and point to a promising area of further research.
Possible extensions and future work include:• Investigating different algorthim pairs for DISCOVER() and LEARN().
• Implementing kernels that directly capture semantic relationships between documents, as opposed to relying on a vectorial bag of features representation of documents.
• Automatic tuning of parameters (e.g. kernel parameters, number of features) on a per-query basis.
One possibility is to employ stability analysis [25] and calculate residual errors for tuning Kernel PCA.
• Explicitly incorporating a feature selection component (e.g. [9]).
• Understanding the exact conditions in which test-specific ranking functions work well.
• Computational speed-ups for practical deployment: for instance, can we re-use previously-trained ranking functions?
• Modeling dependent queries in an interactive search scenario: (1) user issues inital query, (2) transductive system returns ranked documents, (3) user reformulates query, (4) transductive system now learns from both previous and new document list.
