A common task in many text mining applications is to generate a multi-faceted overview of a topic in a text collection.
Such an overview not only directly serves as an informative summary of the topic, but also provides a detailed view of navigation to different facets of the topic.
Existing work has cast this problem as a categorization problem and requires training examples for each facet.
This has three limitations: (1) All facets are predefined, which may not fit the need of a particular user.
(2) Training examples for each facet are often unavailable.
(3) Such an approach only works for a predefined type of topics.
In this paper, we break these limitations and study a more realistic new setup of the problem, in which we would allow a user to flexibly describe each facet with keywords for an arbitrary topic and attempt to mine a multi-faceted overview in an unsupervised way.
We attempt a probabilistic approach to solve this problem.
Empirical experiments on different genres of text data show that our approach can effectively generate a multi-faceted overview for arbitrary topics; the generated overviews are comparable with those generated by supervised methods with training examples.
They are also more informative than unstruc-tured flat summaries.
The method is quite general, thus can be applied to multiple text mining tasks in different application domains.
Following the definitions in [17], we formally define the key concepts of the problem of Multi-Faceted Overview Mining as follows:Definition 1 (Document): We define a document d in a text collection C as a sequence of words d = {w1, w2, . . . , w |d| }.
Following most existing work on topic modeling, we do not model the sequential order of words, thus the document is simply treated as a bag of words.
We use c(w, d) to denote the occurrences of word w in d.Definition 2 (Facet Model): A facet model θ in a text collection C is a multinomial distribution of words {p(w|θ)}w∈V , which represents a semantic facet.
The assumption of this model is that the words in text are sampled following word distributions corresponding to each facet, i.e., p(w|θ).
Therefore, the words with the highest probability in such a distribution would suggest the semantic topic represented by that facet.
For example, a facet about car performance may assign high probability to words like "engine," "horsepower," and "speed."
Definition 3 (Multi-faceted Overview): A multifaceted overview of a topic is a semi-structured summary of all information about the queried topic.
Such an overview is structured in the way that sentences are grouped into the most relevant facets.
We assume that a user would specify a facet she likes by a few keywords.
Within each facet, sentences are ranked based on the relevance of their content to the corresponding facet.
For example, a user searching for information about "Honda Accord" may want an overview with multiple facets such as "engine,""design," and "safety.
"Based on the above concepts, we define the task of MultiFaceted Overview Mining (MuFOM) from a text collection as follows: given an ad hoc topic and a few user specified keywords about the facets they are interested in, the goal is to generate a semi-structured overview of the query topic and present it with the user-specified facets.The problem as defined above is challenging is many ways.
First, we cannot rely on training examples to mine multifaceted overview of arbitrary topics.
The reasons are twofold: 1) it is impossible to create training examples for all ad hoc topics and facets; 2) it is usually too much burden for a user to label training examples for their interested facets.
Therefore, a reasonable solution should not rely on any well-studied supervised-learning method.
Whether a multi-faceted overview of arbitrary topics can be effectively generated without any training examples has not been proved in existing literature.
Second, we alternatively assume that a user could specify facets by providing a few keywords.
How to model and discriminate different facets using this limited information is not straightforward.
In the next section, we will present a unified two-stage framework and probabilistic approaches to meet these challenges.
We propose a novel system to generate multi-faceted overviews of arbitrary topics from text.
It consists of two major components: a facet initialization module that initializes the representation of the user specified facets, and a facet modeling module that extracts and models the facets by statistical topic modeling.
In general, given an arbitrary topic, we expect that a user would guide the system by providing some information about the facets she wishes to use to organize the overview.
To minimize the burden of the user, instead of asking the user to label training examples, we allow a user to specify the facets with keywords (e.g., "price" and "performance" can be used to describe two interesting facets for the topic of buying vehicles).
We initialize the facets with such keyword guidance from the user.A facet can often be described in many different ways.
For instance, in biomedical literature, the facet about genetical interaction can be described using different verbs such as "regulate," "inhibit," "promote" and "enhance."
When writing a review about the interior design features of a car, the reviewer might describe it with different terms such as "air condition,""seat" etc.
In contrast to this richness of information, the guidance provided by a user is sparse: usually a couple of words.
If we simply represent the facet of genetical interaction using the words "genetic" and "interaction", and represent the car's internal design features using the words "interior", "design", and "feature", we will end up with losing the information which is relevant to this facet but not described in such keywords.
We solve this problem by borrowing the idea of query expansion commonly used in information retrieval.
Specifically, we propose to iteratively expand/enrich the initial representation of a facet (i.e., a few keywords) with additional related terms mined from the text collection, and generate an enriched initialization of facets.
Such an representation would be used as a better "seed" to guide facet modeling.Formally, we construct an undirected graph of terms, G = (V, E), where each node in V is a term, and an edge e = 񮽙vi, vj ∈ E indicates a similarity relationship between two terms.
We weight e using M I(i, j) (i 񮽙 = j), where M I(i, j) = logp(v i ,v j ) p(v i )p(v j ).
M I(i, j) is known as the pointwise mutual information [3] of two terms, which measures the association of two terms based on their co-occurrences.
The current representation of a facet i is a set of keywords, or more generally a facet initial language model ¯ θi.
Let us use Ai to denote the words with non-zero probability in ¯ θi.
We then iteratively add new terms into Ai, and adjust ¯ θi byp(w| ¯ θ (t+1) i ) = (1−λ)p(w| ¯ θ (0) i )+λ 񮽙 w 񮽙 ∈N (w) M I(w, w 񮽙 ) Deg(w 񮽙 ) p(w| ¯ θ (t) i ),(1) where Deg(w) = 񮽙 w 񮽙 ∈V M I(w, w 񮽙 ) and N (w) is node w's maximum weighted neighbors.The facet language models ¯ θ (t) i after t iterations are passed to the next step as the initialization of the facets.
There are two parameters associated with this step: n nearest neighbors in the graph G computed with Eqn.
1, and t iterations of adjusting ¯ θi.
Statistical topic modeling [1,8,27,18] is quite effective for mining topics in a text collection.
In this kind of approaches, a document is often assumed to be generated from a mixture of k topic models.
Probabilistic Latent Semantic Analysis (PLSA) [8] is a commonly used topic model.
In this model, the log likelihood of a collection C is defined as:L(C) ∝ 񮽙 d∈C 񮽙 w∈V c(w, d) log k 񮽙 j=1 p(θj|d)p(w|θj)(2)According to this mixture model, an article is "written" by following a stochastic process: first, the author would decide what topic to write about according to p(θj|d), which is the topic distribution of the article; then a word is sampled from the selected topic according to the word distribution of this topic p(w|θj).
In this model, the parameters to be estimated are the word distributions of each topic and the topic distributions of each document Θ = {{p(θj|d)} j=1.
.
k , {p(w|θj)} j=1.
.
k }.
Generally, an ExpectationMaximization (EM) algorithm is applied to estimate the topic models.
We introduce hidden variables z(w, d, j), to represent the probability that a term w in document d is generated from topic j. To maximize the observed data likelihood, EM iteratively performs the following expectation (E) step and maximization (M) step.
E-step:z(w, d, j) = p(w, d) ∈ θj|Θn = pn(θj|d)pn(w|θj) 񮽙 k j 񮽙 =1 pn(θ j 񮽙 |d)pn(w|θ j 񮽙 )(3)M-step:pn+1(w|θj) = 񮽙 d c(w, d)z(w, d, j) 񮽙 d 񮽙 w 񮽙 c(w 񮽙 , d)z(w 񮽙 , d, j) (4) pn+1(θj|d) = 񮽙 w c(w, d)z(w, d, j) 񮽙 w 񮽙 j 񮽙 c(w, d)z(w, d, j 񮽙 )(5)Applying this model to our problem, facets are now the topics in the above mixture model.
Without any constraints, estimating the facet models by maximizing the log likelihood of data alone may overfit the data and result in facet clusters which do not necessarily represent user's interested facets.
That's essentially the problem with unsupervised clustering.
However, in our task, we have user specified facet keywords, which serve as potentially good guidance.
We want the resulted facet models to represent the user defined facets by constraining the estimated models to be close to the initial facet definitions.
Therefore, rather than directly estimating the facet models by maximizing log likelihood of the data, we propose two alternative strategies to guide the facet model estimation: a generative model by applying prior distribution and a discriminative way of applying regularization.
In the following, we discuss both approaches in detail and derive corresponding EM updating formulas.
In this paper, we incorporate the prior knowledge of the facets by applying a prior distribution of the facet model.
Ideally, such prior knowledge would be obtained from training data.
However, producing training data for every facet of arbitrary topics is not realistic.
Here we use previously initialized facet models to define a prior on the facets and estimate the models using the maximum a posterior (MAP) estimator.Specifically, let ¯ θj be the previously initialized facet models, we define the conjugate Dirichlet prior for the model θj as Dir({1 + µjp(w| ¯ θj)}w∈V ), where the parameter µj indicates how much confidence is put on this prior.
The model prior is now defined asp(Θ) ∝ 񮽙 k j=1 񮽙 w∈V p(w|θj) µ j p(w| ¯ θ j ), and the impact of adding the prior is equivalent to adding µjp(w| ¯ θj) pseudo counts for word w in estimating p(w|θj).
This way would allow us to adopt previously initialized facet models into our parameter estimation.
For example, when the user defines one facet using "genetic interaction," we would "guide" the model estimation by constraining the resulted model to be as close as possible to the prior model which has high probability of generating these two words.
Here, we can use a uniform µ for all µj if our confidences on different facets are roughly the same.With this defined prior, we may now use the MAP estimator: ˆ Θ = argmax Θ p(C|Θ)p(Θ) by applying the EM algorithm.
Following [15,17,14], corresponding change in the M steps (Eqn.
4) is to incorporate the pseudo counts given by the prior:pn+1(w|θj) = µjp(w| ¯ θj) + 񮽙 d c(w, d)z(w, d, j) µj + 񮽙 d 񮽙 w 񮽙 c(w 񮽙 , d)z(w 񮽙 , d, j).
The estimated p(w|θj) can then be assumed to be our facet models.
Note that a user does not have to give keywords for every facet; indeed, if a user has not given keywords for the j-th facet, we could set µj = 0, and the j-th facet would be discovered as an additional facet to those specified by the user.
An alternative solution of utilizing the user defined facets to guide the model estimation is to apply a discriminative approach.
Here, we propose a regularized two-stage estimation framework.
First, from previously initialized facets, we retrieve top ranked documents for each facet.
We estimate the facet distribution of these documents by fixing the word distribution of each facet to the initialized models and estimating the facet distribution of the top retrieved documents using the PLSA EM updating formulas in Section 3.2.1.
Again, as in the case of applying priors, a user does not have to give keywords for every facet.
For the additional facets to those specified by the user, the corresponding facet distributions are estimated by initializing their word distribution from a random distribution and iteratively applying PLSA EM updating formulas.
The goal here is to obtain a "training" document set d ∈ CT , whose facet distribution ¯ p(θj|d) will be used to "constrain" the final estimation of facet models in a regularized framework.
The second phase adopts the idea of applying network regularization to topic modeling in [16], which is to estimate facet models by maximizing the regularized data likelihood:O(C, CT ) = (1 − α − β)L(C) − αR(C) − βRT (CT )(7)In the objective function, L(C) is the log likelihood of the collection C defined by Eqn.2, R(C) is a regularizer defined on the collection C according to document similarity, and RT (CT ) is the regularizer defined on the "training" document sets CT based on the facet distribution.
We define the regularizer similar to the graph harmonic function in [28], in which the graph nodes are documents and the edge weights represent document similarities.
The underlying assumption is that two document similar in content should have similar facet distributions, and the final estimated facet distribution of the "training" documents should be close to the "constraint" distribution ¯ p(θj|d).
R(C) = 񮽙 񮽙u,v񮽙∈E w(u, v) k 񮽙 j=1 (p(θj|u) − p(θj|v)) 2(8)RT (CT ) = 񮽙 u∈C T ( 񮽙 v∈C w(u, v)) k 񮽙 j=1 (p(θj|u) − ¯ p(θj|u)) 2 (9)The basic idea of this framework is to "supervise" statistical topic modeling using the discriminative regularization.
Intuitively, L(C) in Eqn.7 measures how likely the data is generated from the facet models, and RT (CT ) constrains the estimated facet models to be close to the initial facet models.
Utilizing R(C) essentially propagates this constraint through the entire collection according to document similarities.
The parameter α and β can then be set between 0 and 1, which indicates how much we want to follow the "training" facet distributions.
When α + β = 0, the objective function boils down to the log likelihood of PLSA.
Thus maximizing O(C, CT ) = L(C) will lead to the facets which best fit the contents of the collection.
When α + β = 1, it boils down to the general graph-based semi-supervised learning.
Formally, the objective function is:O(C, CT ) = −(1 − α − β) 񮽙 d∈C 񮽙 w∈V c(w, d) log k 񮽙 j=1 p(θj|d)p(w|θj) + α 񮽙 񮽙d,d 񮽙 񮽙∈E w(d, d 񮽙 ) k 񮽙 j=1 (p(θj|d) − p(θj|d 񮽙 )) 2(10)+ β 񮽙 d∈C T ( 񮽙 d 񮽙 ∈C w(d, d 񮽙 )) k 񮽙 j=1 (p(θj|d) − ¯ p(θj|d)) 2Note, this model can handle arbitrary number of facets, and applicable to the more general situation in which the number of facets can be larger than the number of facets specified by user's keywords, so that the users can discover new facets other than what they have in mind.
Now, the remaining task is to estimate model parameters by maximizing the objective function in Eqn.10 with the constraints that 񮽙 j p(θj|d) = 1 and 񮽙 w p(w|θj) = 1.
Please note that Eqn.10 is closely related to the regularized PLSA model used in [16].
The difference is that with the additional regularizer RT (CT ), this model now becomes a semi-supervised method.
Following [16], we can use a generalized EM algorithm (GEM) [19] with the similar EM updating procedure.
We outline every iteration of the GEM algorithm as follows:1.
In E step (Iteration n), computing the hidden variables using Equation 3.
This is exactly the same with PLSA.2.
In M step (Iteration n):2.1 Compute the updated p n+1 (w|θ j ) using Equation 4.
n+1 (θ j |d) using Equation 5.2.3 Iteratively compute: p(t+1) n+1 (θ j |d) = (1 − γ)p (t) n+1 (θ j |d) + γ · α α + β ¯ p(θ j |d) + γ · β α + β 񮽙 d 񮽙 w(d, d 񮽙 )p (t) n+1 (θ j |d) 񮽙 d 񮽙 w(d, d 񮽙 )(11)until O n+1 (C, C T ) cannot be improved.3.
Stop EM iteration when On(C, C T ) converges.Applying this algorithm, the estimated p(w|θj) can then be assumed to be our facet models.
Our strategy for generating multi-faceted overviews essentially resembles the strategy in [12], i.e., using the estimated facet models to categorize the sentences about the query topic into appropriate semantic facets.
The process is as follows.
First, we retrieve the documents relevant to the query topic, and segment each document into sentences.
Secondly, we compute the relevance score S between each sentence-facet pair.
To ensure reliable association between sentences and facets, for each sentence, we rank all facets based on S and keep only the facets with highest scores.
Essentially, the sentences assigned to a certain facet compose a summary of the facet.
The underlying rationale is to empirically only consider the most dominant facets in one sentence.
Then, for each facet, we present it in a ranked list of sentences according to S.
Such multi-faceted overview is similar to the "attribute data" report in FlyBase, and the online reviews of a product, e.g., the editor review of cars at edmunds.com 2 .
Given facet models, we can use the negative KL-divergence [10] function to measure the similarity between the sentence s and the estimated facet θj:S = −D(θj||θs) = 񮽙 w p(w|θj) log p(w|θs) p(w|θ j ), where θs, θj represents the language model of the sentence and the facet respectively.
The sentence model is computed using relative frequency of words in sentence after Dirichlet smoothing: p(w|θs) = c(w,s)+µp(w|C) |s|+µ , where c(w, s) is the count of word w in sentence s and p(w|C) = c(w, C)/|V | is the collection background model.
The framework we proposed is quite general, and can be applied to many domains.
In this section, we evaluate our system in two completely different domains and show that the strategy of facet expansion is quite effective, and the regularized topic model approach performs the best among almost all compared methods.The experiments are conducted on a relatively "clean" set of documents retrieved for the given topic.
We made this assumption because the focus here is not to study the retrieval performance.
For example, the Gene Summarizer will first tag all abstracts with gene names utilizing the gene name entity recognizer [9] developed for related project.
Since our method can discover additional facets that a user has not specified, it can still work if the retrieval results are not very accurate as the non-relevant documents (e.g., those matching a distracting sense of an ambiguous query word) likely will be separated as forming additional facets.
In such a case, our overview enables a user to quickly zoom into relevant facets.
In the following experiments, in order to evaluate the results, we set the number of facets to be identical to that specified by a user, even though in a real application we may use a larger number of facets to accommodate any additional facets not specified by the user.
Automated generating semi-structured gene summaries from biomedical literature is a very important task in modern biomedical research.
A supervised solution has been proposed to extract relevant information about a gene from literature and present it in six predefined generic facets [11].
We experimented our methods in this problem, and adopted the same strategy to generate the gene summaries proposed by [11].
Basically, for each retrieved sentence of the query gene, we rank all the facets based on the sentence-facet relevance score, and assign it to two most relevant facets.
Then each facet is summarized by a ranked list of assigned sentences based on that score.
In this experiment, we retrieved 22590 PubMed abstracts about fruit fly as our document collection by matching the keyword "Drosophila melanogaster" in the MESH 3 field.
We used Lemur Toolkit 4 to implement the system.
Adopting the six facets defined in [11] (see Table 1), our system started from the facet keywords and estimated facet models based on the entire collection.
Different methods are evaluated based on their final generated gene summaries.The experiment was done on 19 randomly selected fruit fly genes.
We retrieved 463 sentences relevant to these testing genes from our fruit fly document collection, and asked an insect biologist to annotate these sentences with the predefined six facets in Table 1 to construct a gold standard.
A sentence is assigned a facet label if and only if it contains information on this facet, regardless of whether it contains any extra information.
To study how different methods affect the final generated summary, we evaluated them based on the precision of best five sentences for each facet separately.
The results are shown in Table 2 and 4.
We first evaluated the facet expansion module in Table 2.
In this experiment, we fixed the facet model estimation step to the regularized approach in Section 3.2.3 with parameters α = 0.1, β = 0.5 and top-5 "training" document per facet, and measured precision@5 using the human annotated gold standard for results with 5 different levels of facet expansion.
The facet models is constructed by (1) Orig: relative frequency of the original facet keywords; (2) n10u1: one iteration adjusting the facet models through 10 neighbors in the MI graph; (3) n10u10: 10 iterations through 10 neighbors; (4) n50u1: one iteration through 50 neighbors; (5) n50u10: 10 iterations through 50 neighbors.
Among these runs, run 1 did not expand the original representation of the facets, and run 5 went through the most extensive expansion.
Note, column UpBd indicates the upper bound precision@5 scores as some testing genes with relatively few references do not have 5 sentences per facet in our gold standard annotations.
As can be seen from Table.2, along with more expansion on the facet representation, the generated summary achieves better score.
The summarization performance varies across facets.
In general, sequence information (SI) and genetic interaction (GI) get best scores, and Wildtype Function & Phenotypic Information (WFPI) have the lowest precision.
This may be because these two best facets are the most specific thus easier to be discriminated from others.
While WFPI is too broad and may be described Table 4 when comparing different facet modeling methods.
The effectiveness of facet expansion on this task also varies across facets.
As we should expect, when the original keywords from the user work well (i.e., SI, GI and GP), expansion is not so effective as in the case where the original keywords do not work well (i.e., EL, MP and WFPI).
In Table.3, we show the top 10 words of each facet model after ten iterations of expansion with 50 MI neighbors.
(Due to space limit, the probabilities of these terms are not shown.)
In facet GP, we see terms like "encode" now ranked very high, which is actually a very informative term indicating gene product information.
In facet MP, terms like "allele," "defect," "lethal" indicating important information of mutant phenotype are now expanded into the original facet.
By adjusting facet model with these informative terms, the model is more accurate and closer to actual word usage of the facets.Secondly, we evaluated the effectiveness of different facet modeling approaches in Table.4.
In this experiment, we compared 5 runs: Pri and Reg are our prior-based and regularizer-based approaches (see Section 3.2.2 and 3.2.3) with most extensive facet expansion; Sup represents the result of the system by [11] using the supervised approach; MQR casts this task as a multi-query retrieval problem, where it treats the original query and keywords of each facet as independent queries and generate final summary following our strategy in Section 4; MQR+FB is a variation of MQR with pseudo feedback.
We made three observations from Table.
4.
First, Sup performs worse than other methods in facet SI, EL and MP.
In the beginning, this seems unexpected, since Sup applies the supervised method which exploits training examples and presumably should work better.
However, after looking at the procedure how the training examples are [11], it is not surprising at all.
Training examples for facets SI, EL and MP are extracted from the "Summary" paragraph in FlyBase, which are actually generated from a common template by filling in the key information term in their underline database.
For example, sentences about sequence information is always described using the template: "It has been sequence and its amino acid sequence contains a ...".
While in real abstracts, the same information would be described in variety of ways using different terms.
Apparently, using these "faked" training examples would hurt the summarization.
The training examples for other three facets were real sentences heuristically extracted from abstracts, thus provides good supervision to the summarization.
We can see that our proposed methods (Pri and Reg) perform comparable with Sup in facets GI and GP, and only worse than Sup in WFPI which is a difficult facet to model.
Secondly, both of our proposed methods (Pri and Reg) perform better than the multi-query retrieval methods with or without pseudo feedback (MQR and MQR+FB).
The regularized approach performs the best for 5 out of 6 facets and achieves best average score over all six facets.
Compared with it, the prior-based approach is slightly worse.
One interesting thing is that the prior-based approach has lowest score for the facet WFPI.
The reason might be again related to the difficulty of this facet.
As the prior-based approach constrains the final model to be close to the prior expanded from original keywords, when the facet expansion could not improve much (comparing column u0 and n50u10 in Table 2) for this difficult facet, trusting too much in this prior will certainly hurt.
However, the regularizer approach can overcome this problem since it constrains the final model on the facet distribution instead of the facet model itself.Third, for the multi-query retrieval methods, using pseudo feedback improved the four relatively difficult facets like GP, EL, MP, and WFPI, while performed worse than that without pseudo feedback in the two easier facets SI and GI.
This is not surprising, as pseudo feedback plays a similar role in retrieval as in facet expansion and probabilistic mixture models in our system.
For easy facets, the original keywords already captured majority of the information, while pseudo feedback may introducing other noisy terms thus shift the facet model away from what it should be.
After averaging over all facets, using pseudo feedback or not achieved similar scores.
In this section, we test our system in another domain, i.e., online car reviews.
We crawled the consumers' reviews from edmunds.com on 15 car models like "chevrolet, malibu, 2006," "honda, accord, 2006," as our document collection (1156 reviews in total).
Among these queries, 12 have comprehensive editor reviews, which are used as our gold standard overviews for evaluation.
We applied different methods on generating overviews of the consumer reviews, and evaluated the final performance using ROUGE 5 .
The generated overviews consist of ten best sentences per facet.
We evaluated different methods with all the metrics provided by ROUGE, and report the ROUGE-1 Average R score averaging over all 12 test queries in Table 7 (performance on other metrics are all consistent and not presented here).
In this evaluation, the five facets used in editor reviews are picked as our test facet set (see Table 5).
In this experiment, the effectiveness of facet expansion on this task is demonstrated by the word distribution of facet models after initialization using the MI graph.
The top words of each facet model expanded by one iteration of adjustment through ten MI neighbors are displayed in Table 6.
In the facet Powertrains & Performance (PP), we see terms like "economy" is ranked very high, which is actually a very informative term indicating fuel economy of the car.
In the facet Driving Impressions (DI), terms like "drive," "seat" indicating driving experience are now expanded into the original model.
In Table.7, Pri and Reg represent the prior-based and regularizer-based model estimation methods based on the expansion above.
MQR and MQR+FB represent the multiquery retrieval method without or with pseudo feedback.
The regularizer-based method performed best for all five facets.
Consistent with the above experiments on the gene summarization task, both of our proposed methods (Pri and Reg) performed better than the multi-query retrieval methods.
We also presented one example of our generated overview (with top-2 sentences per facet) for the query "honda, accord, 2006" and its corresponding editor's review in Table 8.
Especially, our extracted sentence for the facet interior design matches excellently well to the editor's review.The above analysis is based on the facets defined by Edmunds's editor reviews.
As our system is able to generate overviews for any user defined facets, one important feature that our system provides is allowing a user to customize the overview using her own interested facets.
To illustrate this, we experimented our system on another set of facets with the same parameter setting as above, and generated the overview in Table.9 for the same query "honda, accord, 2006".
This facet definition represents a different user information need.
In this case, the user do not want to discriminate between interior and exterior design features, but is interested in another facet about price.
Now, the returned sentences about exterior and interior all come to the facet "Design."
In the "Finance" facet, the sentences about price are extracted.
This example shows that our system is effective for generating multi-faceted overviews according to users' ad hoc information interests without requiring any training examples.
As a primary strategy for presenting search results, generating an overview by organizing search results has attracted a lot of attention in both web industry and academia research communities.
Current research is mostly conducted in either unsupervised (e.g., data-driven clustering), or supervised manner by exploiting extra resources.
In one line of work [7,20,25,26], clustering algorithms are used to cluster the top documents returned from a traditional information retrieval system based on the assumption that relevant documents tend to form clusters [23].
However these works generate overviews solely based on unsupervised clustering of the search results, thus the obtained clusters do not necessarily correspond to users' real interests.
Supervised methods by exploiting external resources such as Web directory, search logs, and WordNet are also studied [2, 4, I am very pleased with fuel economy.
I was amazed at the performance of the 4cyl engine, great pick-up and great fuel economy....
you'll get a combined 253 hp and 232 lb-ft of torque and a 25 city/34 highway EPA rating (best in the lineup).
Four-cylinder engines are available with ... Safety I enjoy honda cars because they are reliable, and have a good resale value, safety features, and they make a quality product.
Take it from me...my original Honda Accord Coupe did not give me any problems at all....
In IIHS testing, the Honda Accord earned a Good rating (the best possible) for frontal-offset crash safety; in side-impact tests, it received a Good rating when equipped with side airbags ... The interior is beautiful -I got all of the features and the navigation is extremely easy to use.
Accord's interior is top notch, nice design, clear gauges, comfy seats, lots of storage space.Honda tailored the Accord's interior to meet the needs of the American family.
The seating arrangements are top-notch, and the interior design and materials quality continue the high-caliber standards ... The car's backseat is among the roomiest in the segment... Driving Impressions There are sportier-handling rides, but it is still very responsive, yet comfortable on long trips.
Drives very nicely & is comfortable....
The Accord's steering has a slick, precise feel and the suspension provides a comfortable ride as well as decent levels of road grip ... Brake feel is reassuring... I was amazed at the performance of the 4cyl engine, great pick-up and great fuel economy.
Fuel economy is better than the sticker,(35+ on a recent Boston trip) as was the CRV.
Finance finance, price When I bought it I was amazed at the trim level for the price.
It is extremely fun to drive, fit and finish is fantastic, the oversteer could easily be corrected, at the price, it has no peer and is 10k less then a comparable BMW Safety safety I enjoy honda cars because they are reliable, and have a good resale value, safety features, and they make a quality product.
For the price of the car, there are many safety features with airbags front, side and rear side bags.
Driving comfort, fun There are sportier-handling rides, but it is still very responsive, yet comfortable on long trips.
Drives very nicely & is comfortable.
24,22].
However their generality is often limited due to the labor required to build the external resources (e.g., training data); more importantly, they cannot accommodate a customized structure of organizing search results for a user.
In bioinformatics, people have studied how to automatically generate gene summaries to facilitate biologists in finding gene-centered information from biomedical literatures [11].
Existing solutions are summarizing all gene information from MEDLINE abstracts into six predefined generic facets.
This approach adopted a training data set by utilizing the annotated data from the model organism databases (e.g., FlyBase).
Another related problem is mining and summarizing opinions in Weblogs [13,6,5], where the goal is mainly to mine user opinions by identifying and extracting positive and negative opinions or analyzing and extracting topical contents of blog articles.
None of this body of work would allow a user to impose an ad hoc structure of the summary.
The simultaneous topic-sentiment analysis work in [17] is more related to our work as the idea of applying prior to mining topics from blog articles is similar to one of the approaches for modeling facets discussed in Section 3.2.2.
However, the effectiveness of such a method is not quantitatively evaluated.
And the sentiment model is still launched in a completely supervised manner which requires training examples.
Our regularized topic model is based on the general regularization framework proposed in [16], and the MAP estimator of PLSA has also previously been used in [14] for opinion integration.
In this paper, we studied a novel problem in text mining: automated generation of multi-faceted overviews for arbitrary topics from a text collection.
We developed a system which combines both generative and discriminative topic mining techniques to automatically summarize information about a query topic into multiple facets.
Empirical experiments demonstrated that our proposed system, especially the regularized PLSA model, is quite effective in mining multi-faceted overviews for applications in two completely different domains: the gene summarization task in biomedical literature and the car review mining task for online customer reviews.
Given our general setup of the problem (i.e., no need for training examples and a user can flexibly specify facets with keywords), our proposed methods can be applied to many domains.The general problem of mining multi-faceted overviews for arbitrary topics represents a new research direction in text mining.
Our work can be extended in several directions: (1) We have not considered the removal of redundant information in generated overviews.
One future improvement would be to integrate other text features for redundancy removal.
(2) The user specified facets might lie in different granularities, which is not tackled in this work, but will certainly be an interesting future topic to explore.
(3) In our model of estimating facets, incorporating training examples and user feedbacks is a very natural extension.
Extensive experiments and studies on utilizing interactive user feedback would bring important insight into how to leverage additional information in user activities.
(4) There are many types of online resources that can be utilized to improve the facet modeling, research on how to integrate them into our framework would be another interesting future direction.
We thank all the anonymous reviewers for their valuable comments.
We also thank Moushumi Sen Sarma for her great effort in annotating the gene summary sentences.
This work is in part supported by the National Science Foundation under award numbers 0425852, 0713571, 0428472, as well as a Yahoo! PhD Fellowship.
