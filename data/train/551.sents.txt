In recent years, peer-to-peer (P2P) and peer-assisted streaming have emerged as promising models for low-cost multime-dia distribution to large scale user communities.
In this paper , we study streaming of scalable video streams over these systems.
Scalable video streams are composed of multiple layers and can easily be adapted according to the characteristics and needs of receivers.
Thus, they can efficiently support a wide spectrum of heterogeneous peers participating in a P2P streaming system.
We present an analytical model for forecasting the long-term behavior of a P2P streaming system with scalable video streams.
Our analysis takes as inputs the characteristics of a dynamic P2P streaming system and the video streams.
It then analytically computes the expected throughput of the streaming system and the expected video quality delivered to peers.
The analysis also provides an upper bound on the maximum number of peers that can be admitted to the system at once (i.e., in flash crowd scenarios), while ensuring a certain video quality.
We present a general analysis framework that can be customized to various practical P2P streaming systems with different characteristics.
Then, we show the detailed analysis of a typical P2P streaming system and we explain how other systems can be analyzed using our model.
We validate our analysis by comparing its results to those obtained from simulations, which confirm the accuracy of our analysis.
Our analysis and simulations enable administrators of P2P streaming systems to predict the throughput and the video quality that can be delivered to users.
The demand for multimedia services has seen a rapid growth in the past few years, which is even expected to accelerate in near future as confirmed by many market research reports [4,16].
To meet portions of this demand, peer-assisted and peer-to-peer streaming systems have been successfully designed and deployed for large-scale user communities [1][2][3]5,19].
In these systems, peers contribute bandwidth and storage to serve other peers.
However, the contributions from the peers are often less than the capacity needed to serve high quality video streams.
To make up for the shortage in capacity, a number of dedicated servers are usually used in these streaming systems, which we refer to as seed servers.
Because of the existence of seed servers, these streaming systems are sometimes called peer-assisted systems.
We use the terms peer-assisted and P2P streaming systems interchangeably in this paper.In current P2P streaming systems, a video is encoded at a certain bitrate, typically ranging from 300 kbps to 1 Mbps [6].
To support a wider range of receivers, a lowerbitrate video is preferred, but it provides a low quality for everyone.
This problem may be solved by encoding and distributing multiple versions of the video, which is called simulcasting.
However, a video has to be encoded many times for different combinations of decoding capabilities, connection bandwidths, and viewing resolutions.
Moreover, switching among versions is not easy, because: (i) for every switching, a client has to wait for the next Intra-coded frame (I-frame) of the new version, and (ii) streams of different versions could be asynchronous [9].
In addition, P2P streaming with multiple video versions divides users to separate networks which may result in reduced connectivity and less efficient utilization of peers' resources.
As an alternative, Multiple Description Coding (MDC) can encode a video into multiple descriptions, where the quality of the video will be proportional to the number of descriptions received.
However, MDC techniques have considerable bitrate overhead and are computationally complex [9].
In contrast, a scalable video stream can be encoded once and a wide range of heterogenous clients can benefit from the video.
In addition, heterogeneous clients receiving different layers can still share common layers and participate in the same overlay network, leading to a larger pool of re-sources.
Moreover, scalable coding has a lower overhead and is simpler than MDC [9].
Recent scalable video coding techniques, e.g., H.264/SVC [14], have further improved this coding efficiency and significantly outperformed previous scalable videos [18].
Accordingly, they have received an increasing adoption in practice [7,13].
In this paper, we study streaming of scalable videos over P2P networks.
We develop an analytical model to forecast the behavior of a given P2P system for streaming scalable videos.
Our analysis takes as input the characteristics of the network, including the distribution of upload and download bandwidths of peers, peer arrival and failure rates, and video bitrates.
In addition, the analysis takes into account the serving capacity of auxiliary seed servers in the system, which need to be deployed to complement the limited upload bandwidth of peers.
For example, a home user with DSL or cable connection typically has an upload bandwidth of a few hundred kilobits per second, whereas an averageto-good quality video stream requires 1-2 Mbps.
Taking the characteristics of the P2P network and the capacity of auxiliary seed servers as inputs, our goal is to answer the following questions:Q1.
What is the expected throughput of a given P2Pstreaming system in the long term?
The throughput directly impacts the video quality delivered to peers.Q2.
To increase the throughput to a desired level, how much capacity we need to provide in terms of auxiliary seed servers?Q3.
How many peers a system can support in case of a flash crowd arrival of peers?We present an analytical model to answer the above important questions.
Our model is general and can be applied to P2P streaming systems with different characteristics and network conditions.
We present numerical analysis of example systems and derive various insights on their performance.
In addition, we validate our analytical results through extensive simulations.
Our simulation results confirm the accuracy and usefulness of our analytical model.
This paper is organized as follows.
Related works are summarized in Section 2.
In Section 3, the considered P2P streaming model is described.
Our analytical model is presented in Section 4.
In Section 5, we conduct an analysis using this model for a sample network.
Then, we validate our analysis using simulations, and we use the analysis to study the performance of a P2P streaming system in Section 6.
Finally, we conclude the paper in Section 7.
While the problem of peer-to-peer or peer-assisted video streaming has been studied by numerous previous works from various angles, such as overlay construction, distribution of data availability information, and piece scheduling algorithms, the problem of theoretically analyzing the behavior of P2P streaming systems has received less attention.
Current analytical works addressing this problem assume that the video streams are encoded in nonscalable manner.
Nonscalable video streams have fixed bitrates and cannot be adapted easily.
For example, Tu et al. [17] present an analytical framework for studying the performance of P2P streaming systems.
This framework mathematically analyzes the pattern of capacity growth of a given dynamic P2P system over time.
It then estimates the time at which the capacity of the peer population suffices for continuing the operation of the network, which is actually the time at which the servers can be turned off.
A similar analysis is done in [8] based on a stochastic fluid model, which finds the maximum streaming bitrate that can be supported in a P2P system.
However, the works in [8,17] only consider nonscalable streams with a constant bitrate.
They also divide peers into a number of classes based on their bandwidth contributions.
In addition, in [8] only live streaming scenarios are considered.
The work in [17] can analyze on-demand streaming scenarios as well, but the time granularity of this analysis, i.e., the length of the smallest time unit, is the length of a complete streaming session.
In our analytical model, we consider ondemand streaming of scalable video streams with variable bitrates, and we do not make any restrictive assumptions on the bandwidth of peers.
Moreover, we analyze the capacity of the system over time with a fine granularity, which is the length of a video segment of a few minutes at most.
This enables us to capture the dynamics of the network more accurately.
For example, the time a peer stays in the network for seeding is often a fraction of the video length, which cannot be captured when we analyze the system based on a time unit equal to a complete video length.
Liu et al. [10] analyze peer-assisted streaming systems by constructing an appropriate tree structure for the network and provisioning resources for it.
In contrast, we do not assume any particular structure imposed on the network formed by peers.
That is, each peer can decide its neighbor list, based on the information it receives from trackers and/or other peers.
Thus, our approach can be used to analyze mesh-based and receiver-driven systems, which have been repeatedly reported to be more efficient and robust than tree-based approaches.
Since mesh-based systems are employed by most of today's P2P streaming systems [1-3, 6, 19], our analytical model is more practical and useful than previous ones.
Another performance analysis on P2P streaming systems is conducted by Small et al. in [15].
However, similar to [10], the authors consider constructing a specific P2P topology for the overlay network, unlike our analysis which does not make any particular assumption on the network topology formed by peers.Zhou et al. [20] and Parvez et al. [12] analyze the performance of P2P streaming systems with nonscalable videos when two different chunk selection strategies are employed by peers.
These analyses are motivated by the observation that a poor streaming performance, particularly in terms of startup delay, is experienced when we adopt BitTorrent-like P2P downloads for streaming.
In BitTorrent, a rarest-first strategy is often employed by peers to select which data chunks to download, which diversifies the chunks available at peers.
This practice improves the overall download throughput by avoiding situations where many peers cannot find or they compete for a rare chunk.
However, it is not appropriate for streaming, since streaming requires a peer to have pieces sequentially from the beginning of the file.
Sequential download of pieces, on the other hand, is appropriate for streaming, but is not efficient for BitTorrent-like downloads.
The authors of [20] analyze the performance of P2P streaming systems when these two strategies, rarestfirst and sequential, are employed by peers.
They then propose a hybrid strategy to be employed by peers for P2P live streaming, which reduces the significant startup delay-the reduced delay can still be considerable when we are to ensure a high probability for continuous playback under this download model.
The authors of [12] present a similar analysis, which is more general than [20] and consider on-demand streaming as well.
However, the work in [12] also assumes that all peers have equal bandwidths and all peer-to-peer connections have the same throughput, i.e., no peer heterogeneity.
Nevertheless, the argument that sequential download reduces the system throughput relies on the assumption that peers select their partners randomly.
However, this assumption is no longer correct if peers request and serve data to each other more carefully to balance the load between the supply and the demand of peers.
For example, each peer requests data from other peers that are closer to it in terms of playback time, and a sending peer always gives priority to requests coming from closer peers.
This example is consistent with the P2P streaming models assumed in [12,20], but it makes the rarest-first and the sequential strategy equal in terms of the overall download throughput.
In our analysis, we assume a sequential download strategy for peers.
The considered P2P streaming architecture consists of trackers, seed servers, and peers.
Peers join the system by contacting one of the trackers.
The tracker receives periodic update reports from peers, informing it about their available data and capacity.
A number of seed servers exist to inject the content into the network and to serve requests when there is not enough capacity in the peer population.
Peers download the video in a streaming form, meaning that the video is downloaded sequentially at a bitrate equal to the bitrate of the video.In the considered P2P system, peers are expected to use their upload bandwidth for serving lower layers of the videos first, in order to avoid the situation that some peers are starving (for not receiving lower layers) while other peers are receiving the highest quality.
Moreover, peers serve as many layers as they can upload.
For example, if all layers have a rate of 100 kbps and a peer has 250 kbps upload bandwidth, it will upload the two lowest layers at rate 100 kbps and the third one at 50 kbps.We divide the video file into S small segments.
Let Ps denote the set of peers that are currently watching the video at segment s where 1 ≤ s ≤ S.
A peer can only serve data to peers that are at earlier segments.
In other words, a peer in Ps can be served by any peer in P s ′ , s < s ′ ≤ S, given that the requested layer is available at the sending peer.
Peers in Ps cannot be served by any peer in P s ′ where 1 ≤ s ′ ≤ s.In addition to receiving from other peers, a peer may request to receive the stream from seed servers.
Seed servers are often loaded with a volume of requests larger than their serving capacity.
Thus, it is important to efficiently manage this capacity and carefully choose which requests to serve.
In our previous work [11], we analyzed two methods for serving peers by seed servers and matching them to other peers: random and contribution-based.
In the former case, peers are matched together and/or served by seed servers randomly.
In the latter case, this task is done based on the contribution of peers and the bitrates/qualities of video substreams.
We have shown how to use the contribution-based method to gain an overall benefit approaching the optimal benefit that can be gained in the system.
In this paper, we The time each peer stays in the network for seeding (after finished watching).
e[l]The capacity served for layer l to peers at a given video segment.
f (e, l, r l , n)The capacity that can be served for layer l (at rate r l ) by n peers demanding this layer, when an input capacity e is served to them.o[l]The capacity served for layer l by peers at a given video segment.q l (t)Video quality (e.g., Y-PSNR) added by the l-th layer of the video at segment t. r l (t)Bitrate of the l-thlayer (1 ≤ l ≤ L) of the video at segment t. γ(t)The probability that a peer that is at segment t fails/leaves.
In this section, we present the proposed model for analyzing P2P streaming systems.
We first present a high level overview of the analysis, followed by step-by-step details.
We derive general formulas which can yield the throughput of the P2P system as a function of the characteristics of the network.
These general formulas can be readily customized for a given set of network characteristics, and allow us to capture a wide range of network specifications.
This mainly consists of replacing the general variables used in this section, e.g., random variables representing peer bandwidth distributions, by values specific to the P2P system being analyzed.
In the next section we present an example of this customization.
Our analysis takes the following variables as inputs, and it outputs the throughput of the network in various forms, such as the average video quality delivered to peers.
• The joint distribution of upload and download bandwidth of peers.
We define random variables D and U as the download and upload bandwidth, which are not independent; they should be considered jointly when analyzing the capacity of peers for uploading video layers.Let P r(x1 ≤ D ≤ x2, y1 ≤ U ≤ y2)where x1, x2, y1, y2 ∈ R denote the probability that a peer's download and upload bandwidth values are in the range [x1, x2] and [y1, y2], respectively.
• The expected peer seeding duration, T seed .
• The seed servers capacity C, number of layers L, and bitrate of each video layer r l (t) at each segment t.• The distribution of peer arrivals.
N is the random variable representing the number of arrivals per segment and P r(N = n) denotes the probability of having n arrivals in Tseg seconds.
• The peer failure rate, γ(t), denoting the probability that a peer at segment t leaves the network; γ(t) is a function of t to capture realistic peer behaviors, e.g., peers that have stayed longer are less likely to leave.At a high level, the analysis proceeds as follows.
Recall that a peer in Ps can stream to peers in P s ′ (s ′ < s) only.
Accordingly, starting from the last segment of the video to the first (s = S, S − 1, . . . , 1), in each step of the analysis we consider the set of peers that are watching the same video segment s, i.e., peers in Ps.
We estimate the output capacity of these peers as a function of the capacity served to them.
Using this function, we can compute the capacity that is going to be served to these peers and the capacity that they will serve to peers in P1, . . . , Ps−1.
We then proceed to segment s − 1 and so on.
In our analysis, we treat capacity values as arrays rather than single numbers, where To analyze the set of peers at a given segment, we define the function o = f (e, l, r l , n) as the output capacity of peers for layer l when this layer is served to them, i.e., n peers demanding the layer at the segment, with an input capacity e.
The bitrate of the video layer at that segment is r l .
e[1], . . .In the remainder of this section, we will first obtain the function f (·) in Section 4.2.
This function, as just described, captures the distribution of only one video segment to a given number of peers.
We employ this function in Section 4.3 to analyze the distribution of the complete video in a dynamic network.
We define function o = f (e, l, r l , n) as the output capacity for layer l when serving this layer with a capacity e to n peers demanding it at a segment.
r l is the bitrate of the layer at the segment.
Clearly, if e is equal to or larger than the demand of n peers, i.e., e ≥ r × n, all the n peers can be served using e. On the other hand, if e < r × n, we can serve only a subset of peers.
The estimated output capacity o depends on how these subsets are chosen.
As discussed in Section 3, we consider two cases for this purpose: (i) when the serving is done by giving preference to peers with higher contribution, and (ii) when the serving is done randomly, as in many current P2P streaming systems.
We distinguish three different f (e, l, r, n) functions: fopt(·) for case (i), f rnd (·) for case (ii), and f all (·) for the case where e ≥ r l × n, in which it does not matter how to serve peers since the demand is smaller than the input capacity.Among these three forms, we first obtain o =fopt(e, l, r l , n).
For this case, peers are sorted based on the bitrate that they can stream layer l to other peers, and are selected one by one for being served with layer l.
The bitrate u p,l at which a peer p streams layer l to other peers is estimated according the system model in Section 3.
This model describes how peers are expected to share their upload bandwidth for serving different layers, i.e., lower layers first, and as many layers as possible.
Thus, u p,l can be calculated as follows:u p,l = min 񮽙 up − l−1 񮽙 i=1 up,i, r l 񮽙 1 ≤ l ≤ Lp,(1)where Lp is the number of layers that peer p can download, and up is its total upload bandwidth.
If the upload bandwidth of p is higher than the total bitrate of the demanded layers, the bandwidth remained from Eq.
(1) is equally divided among layers:u p,l = min 񮽙 up − l−1 񮽙 i=1 up,i, r l 񮽙 + up − 񮽙 Lp i=1 ri Lp(2)Using Eqs.
(1) and (2) and the distribution of upload bandwidths (U), we can calculate the distribution of U l : the random variable representing the upload rate of peers for layer l.Peers with higher u p,l values are selected for being served with layer l.
The number of peers that can be served using capacity e is k = ⌊e/r⌋.
Since for calculating fopt(·), we know that k < n, the value to be calculated, o, consists of "the total capacity that can be streamed by the k peers that have the highest U l -capacity values among the total n peers".
We estimate this capacity by a probabilistic analysis as follows.
Assume that in a list of peers sorted according to the serving capacity for layer l, the k + 1-st peer has a u p,l value equal to x. Thus, the first k peers all have a u p,l value of u p,l ≥ x. Using this, we can obtain the expected value for the total capacity of the first k peers for layer l as:E[ k × U l | U l > x, D ≥ R l ] = k × 񮽙 ∞ x P r(U l = y, D ≥ R l ) y dy P r(U l ≥ x, D ≥ R l ) ,(3)where R l is the cumulative bitrate of the first l layers.
The two conditions considered in this expected value capture the fact that the involved peers in this calculation already have an upload bandwidth higher than the k + 1-st peer, and a download bandwidth that can afford the first l layers; clearly, peers that are not capable of downloading the first l layers cannot be an uploader for that layer.To obtain the expected capacity of the first k peers, we need to integrate Eq.
(3) with respect to x, and consider all possible combinations of peers in an arbitrary sorted list of peers.
For the candidate k + 1-st peer that we considered in Eq.
(3), there are 񮽙 n 1 񮽙 choices.
Then, there are 񮽙 n−1 k 񮽙 choices for selecting the first k peers among the remaining ones.
Hence, the expected value of fopt(e, l, r l , n) is calculated as:fopt(e, l, r l , n) = 񮽙 ∞ 0 񮽙 n 1 n−1 k 񮽙 P r(U l = x | D ≥ R l ) 񮽙 񮽙񮽙 񮽙 I × P r(U l > x | D ≥ R l ) k 񮽙 񮽙񮽙 񮽙 II × P r(U l < x | D ≥ R l ) n−k−1 񮽙 񮽙񮽙 񮽙 III × E[ k × U l | U l > x, D ≥ R l ] 񮽙 񮽙񮽙 񮽙 Eq.
(3) dx,(4)where term I is the probability that the candidate k + 1-st peer has a capacity of x for serving layer l, term II is the probability that there are k peers with higher capacity than x, and term III is the probability that there are n − k − 1 peers with lower such capacity.
The product of these terms indicates the probability that the peer we assumed as the k + 1-st peer is actually the k + 1-st among a total of n. Note that, rather than the expected value of o, one might argue that we should target the complete distribution of o since it could lead to more detailed results.
For that distribution, however, we need to calculate the sum of k distributions of U l .
In other words, Eq.
(4) changes to:P r(o ≥ x0) = 񮽙 ∞ 0 񮽙 n 1 n−1 k 񮽙 P r(U l = x | D ≥ R l )× P r(U l > x | D ≥ R l ) k × P r(U l < x | D ≥ R l ) n−k−1 × P r(k × U l ≥ x0 | U l > x, D ≥ R l ) 񮽙 񮽙񮽙 񮽙 I dx,(5)where term I denotes the sum of k independent values that follow the distribution of U l .
Calculation of Eq.
(5) requires convolving the distribution of U l with itself k times, which is not feasible even for uniform distributions, the simplest ones.
Specifically, in Eq.
(4) we took advantage of the interesting property of additivity of expected values, i.e.,E[X + Y] = E[X] + E[Y], and in particular,E[k × X] = k × E[X], which cannot be benefited from in Eq.
(5).
We now consider the case where peers are served randomly.
In this case we have:f rnd (e, l, r l , n) = E[k × U l | D ≥ R l ] = k × E[U l | D ≥ R l ] = k × 񮽙 ∞ 0 P r(U l = x, D ≥ R l ) x dx P r(D ≥ R l ) ,(6)Finally, we note that if the number of requesting peers is less than or equal to the number of peers that can be served, i.e., k ≥ n, we have:f all (e, l, r l , n) = n × E[ U l | D ≥ R l ],(7)since we can simply serve all peers that are at the segment.
We now use the formulas obtained for fopt(·), f rnd (·), and f all (·) to calculate the capacity of the whole network.
We first extend these functions to capture the dynamics of the network including arrivals and failures at one segment.
We then employ these functions to generalize our analysis from the distribution of one segment to that of the whole video.Peers dynamics.
To accommodate peer arrivals and departures (or failures) that may happen at different time instances, we define o = ˆ f (e, l, r l , N, ǫ) as the output capacity for serving a segment where the number of peers at the segment is represented by the random variable N and the failure probability for peers at the segment is ǫ.
ˆ f (e, l, r l , N, ǫ) is calculated as follows.
Let N l be a random variable representing the number of peers at a segment demanding layer l.
The probability distribution of N l , given that the number of arrivals is N, is given as:P r(N l = n | N) = ∞ 񮽙 i=n P r(there are i peers) × P r(n of the i demand l) = ∞ 񮽙 i=n P r(N = i) 񮽙 i n 񮽙 P r(D ≥ R l ) n P r(D < R l ) i−n .
(8)Accordingly, we calculatê f (e, l, r l , N, ǫ) as:ˆ f (e, l, r l , N, ǫ) = k 񮽙 n=0 P r(N l = n) × f all (e, l, r l , n) × (1 − ǫ)+ ∞ 񮽙 n=k+1 P r(N l = n) × f opt|rnd (e, l, r l , n) × (1 − ǫ).
(9)Distribution of video segments over time.
Consider the steady state of the network, where some peers are watching the video at segments s = 1, . . . , S and some peers have finished watching and are seeding for up to T seed time units.
We assume that the throughput of the network in the steady state is smaller or equal to the demand of the peer population.
Otherwise, we reduce the bandwidth or even turn off the seed servers; note that the peer population is usually not capable of supporting its demand, since peers upload capacities are typically far less than their demand.
We estimate the steady-state throughput of the network for both cases of random and contribution-based peer serving.If serving randomly, the expected capacity of k arbitrary peers at segment t for serving layerl is k × E[ U l | D ≥ 񮽙 l i=1 ri(t) ], as addressed in Eq.
(6).
If this value is less than k×r l (t), which is the bandwidth required to serve layer l to k peers, then layer l of segment t will eventually vanish in the network since a random peer receiving it cannot serve it completely, e.g., if two peers receive the complete layer at a given time instance then only one peer can receive it at the next time instance-partial layers are of no use as they cannot be decoded.
In other words, the number of peers that can be served with layer l of segment t constantly decreases and the steady-state capacity for serving that layer converges to zero.
On the other hand, if the aforementioned expected value is greater or equal to k × r l (t), eventually all peers at t that demand layer l can receive it, even if we initially seed this layer to a few random peers only.
Accordingly, using Eq.
(6) we can determine the video layers that survive and continue being streamed between peers, and those that do not.
Using this, the video quality that is expected to be served in the network can be readily obtained, as we do in our sample analysis in Section 5.
On the other hand, if peers are served according to their upload capacity, the fate of a particular layer is not necessarily either vanishing or being served to everybody, as it is with random peer serving.
As an extreme example, if at every segment we only have 1 peer capable of serving layer l, and all other peers are incapable of that, we can still have 1 peer at each segment receiving layer l while the others receive only up to the l − 1-st layer.
We can have more peers receiving layer l by using seed servers.
For instance, we can have more than 2 peers receiving the layer if we seed only one layer bitrate, since the peer we seed to will itself serve to others a portion of the layer.
We now determine the number of peers that can receive layer l in the steady state as a function of the bitrate seeded.Recall that the number of peer arrivals is represented by the random variable Λ, and the failure probability at a segment t is γ(t).
Considering peers that may have left/failed at earlier segments than t, the number of peers at segment t can be represented by the random variable Λt as:Λt = Λ × ( 1 − t−1 񮽙 i=0 γ(i) ).
(10)Once the distribution of layer l of segment t is started, at each time instance i we have:x i t,l = ˆ f 񮽙 x i−1 t,l × T + T seed T + C t,l 񮽙 񮽙񮽙 񮽙 I , l , r l (t) , Λt , γ(t) 񮽙 ,(11)where x i t,l (i ≥ 1) is the layer-l throughput of peers at segment t at time instance i of the system; x 0 t,l is defined as zero.
Term I in Eq.
(11) represents the capacity served for layer l to peers at segment t, which consists of C t,l , the capacity served by seed servers for layer l of segment t, and x i−1 t,l , the capacity served by other peers.
The coefficientT +T seed Tis to take into account the expected capacity shared by seeding peers-this coefficient has a few subtle details capturing the dynamics of the network and the probability that a peer at segment t stays for seeding, which we do not include here as they do not have a considerable effect.
Note that we are given a total seed server capacity C, not C t,l values.
The way we divide C among C t,l values depends on the method used for allocation of seed servers.
We first divide C over all video segments according to the Λt value of each segment t and the bitrate/quality of the video at that segment (r(t), q(t)).
Then, we divide the capacity given to each segment t over different layers, based on the server allocation method, the demand for the layers (according to D), and the bitrate/quality of the layers.
Since conducting our analysis for a given network consists of several iterations over Eq.
(11) for different segments and layers, we perform the above step in each iteration.We first note that x i t,l values converge over time, i.e., for i = 1, 2, . . . , in either case of random or contribution-based serving of peers.
This is intuitively true, and is specifically because x i t,l values are monotonic with i, i.e., either constantly non-increasing or constantly non-decreasing.
Otherwise, for instance, we could have the case that 5 peers, either randomly selected or just the top 5 high-capacity peers, can serve 8 peers, but 8 peers (selected the same way) can serve only 7 peers, which is not possible.
As x i t,l is monotonic and it is finite, it converges to a specific value.
We illustrate a sample of this convergence in the next section.Denoting the converged value of x i t,l simply by x t,l , we can now determine the steady-state number of peers that are expected to receive layer l of segment t.
This number, denoted by m t,l , can be obtained as:m t,l = x t,l r l (t) .
(12)Using the m t,l values we can estimate the steady-state throughput of the system along a variety of metrics.
For example, the total video bitrate served in the network can be calculated as:Bitrate served = T 񮽙 t=1 L 񮽙 l=1 m t,l × r l (t).
(13)Similarly, the average video quality served to peers is as:Average quality = 1 񮽙 T t=1 Λt T 񮽙 t=1 񮽙 Λt L 񮽙 l=1 m t,l ×q l (t) 񮽙 ,(14)in which the use of Λt captures the fact that the expected number of peers at different video segments is not the same.
Using our estimate of the throughput of the system, we can forecast the capacity of the system for supporting sudden extensive loads such as a flash crowd.
In a flash crowd event, a large number of peers arrive at the system at about the same time.
Taking advantage of the flexibility of scalable streams, a system may sustain in these circumstances and support the new peers by temporarily lowering the video quality delivered in the network until the system recovers from the shock and returns to a stable state.
Our analysis can determine the robustness of a P2P system against flash crowds, which we define as the maximum number of peers that can be admitted to the system at once.
This maximum can be achieved when we serve only a minimum video quality to all peers, i.e., the base layer.
Thus, having estimated the total throughput of the network, we can calculate the number of minimal substreams that can be served using this throughput, i.e., an upper bound on the maximum number of peers that can be served when a flash crowd arrives.
We assume that a peer keeps the video file in its buffer until it finishes watching the video; this is realistic since the peer is going to seed afterwards.
Thus, all peers can serve the base layer of the first few segments of the video.
Hence, the maximum flash crowd size N FC that can be supported is calculated as:N FC = 1 r1(1) 񮽙 C + T 񮽙 t=1 L 񮽙 l=1 m t,l × r l (t) − T 񮽙 t=1 Λt × r1(t) 񮽙 񮽙񮽙 񮽙 I 񮽙 ,(15)where term I represents the bitrate served for delivering the base layer to peers that were already in the network.
Eq.
(15) calculates the number of base layers that can be served to newly arrived peers-the bitrate of the base layer of the first few segments of the video is assumed to be nearly equal to that of the first segment, r1(1).
Note that it is the serving of the first few segments that is most important for sustaining a flash crowd.
After the first few segments, peers arrived in the flash crowd also contribute to uploading, and the throughput of the system increases accordingly.
In this section, we have presented our general analytical model for studying the behavior of P2P streaming systems.
We divide a video file into S segments, and analyze the bandwidth consumed and the bandwidth contributed by peers that are watching each video segment s for s = S, S − 1, . . . , 1.
This part of the analysis is done using Eq.
(9).
The preliminary functions needed for calculation of Eq.
(9) are derived in its preceding equations.
Then, we plug the single-segment analysis function obtained in Eq.
(9) into Eq.
(11), which analyzes the distribution of each video segment/layer over time.
Using Eqs.
(11) and (12), we can obtain the number of peers that are expected to receive each video segment/layer of the video stream in the steady state.
These numbers are then used to predict the total bitrate that is going to be served in the system in the long term (Eq.
(13)), the average video quality delivered to peers (Eq.
(14)), the capability of the system for supporting flash crowds of peers (Eq.
(15)), or others user-defined metrics for capturing the performance of the system.
In this section, we show the details of employing the proposed analytical model for analyzing a sample P2P streaming system.
This is to demonstrate how our general analysis in Section 4 can be used in practice.
P2P streaming systems with different characteristics than considered in this section can be analyzed in a similar manner.
Analysis Setup.The P2P streaming network we analyze is specified as follows.
Suppose we are distributing a video file consisting of L layers, each at a fixed rate of r kbps.
The video length is S segments.
We consider both random and contribution-based serving of peers.
The download bandwidth of peers is uniformly distributed in [0, M ] kbps, and the upload bandwidth of each peer a constant fraction α of its download bandwidth, which makes it uniformly distributed in [0, αM ] kbps.
Peers arrive according to a Poisson distribution with an average arrival rate of λ, and stay after they finish watching the video for seeding for up to T seed time units.
A fraction of peers leave the system before watching the complete video and seeding.
These departures happen with probabilityˆγtprobabilityˆ probabilityˆγt for peers at segment t (1 ≤ t ≤ S).
Note that the general analysis framework presented in the previous section is not limited to any of the above assumptions, e.g., the bitrate of video layers can be different, and the bandwidth of peers can follow an arbitrary distribution.We customize the model and equations derived in the previous section for the network described above.
First, we need to calculate the functions f all (·), f rnd (·), and fopt(·), which take as input the bandwidth e kbps for serving the r-kbps layer l to n peers.
For this purpose, we need to obtain the closed form expression for E[ U l | D ≥ R l ], the expected value of the upload capacity for layer l by a peer that can download layer l, since this value is repeatedly used in the following equations.
This is done using the distribution of bandwidths as:E[ U l | D ≥ R l ] = E[ U l | U ≥ αrl ],where E[ U l | U ≥ z ] for any arbitrary z is calculated according to the way a peer shares its upload bandwidth among layers (Eqs.
(1) and (2)) and the distribution of the upload bandwidth of peers, which is uniform in our case.
More specifically, denoting by U l (u) the layer-l upload bandwidth of a peer whose total upload bandwidth is u, according to Eqs.
(1) and (2) we have:U l (u) =              u L rL ≤ u ≤ αM r rl ≤ u < rL u − r(l − 1) r(l − 1) ≤ u < rl 0 0 ≤ u < r(l − 1),which leads to calculation of the desired expected valueE[ U l | U ≥ z ] as: E[ U l | U ≥ z ] = 񮽙 ∞ z P r(U = u)U l (u) du P r(U ≥ z) = 񮽙 αM z U l (u) du αM − z =                                      1 αM −z 񮽙 αM z u L du rL ≤ z ≤ αM 1 αM −z 񮽙 rL z r du + E[ U l | U ≥ rL ] rl ≤ z < rL 1 αM −z 񮽙 rl z (u − r(l − 1)) du + E[ U l | U ≥ rl ] r(l − 1) ≤ z < rl 1 αM −z 񮽙 r(l−1) z 0 du + E[ U l | U ≥ r(l − 1) ] 0 ≤ z < r(l − 1).
(16)The first row in Eq.
(16) corresponds to the case where the upload bandwidth of a peer is higher than the total bitrate of video layers, in which a bandwidth u is evenly divided among the L layers.
The second row adds to this case the possibility that the upload bandwidth is less than the bitrate of all layers, but higher than rl, the bitrate of the first l layers.
In this case, the bitrate at which layer l is streamed to other peers is just equal to r.
The third row adds the case that the peer's upload bandwidth is less than the bitrate of the first l layers, but higher than the first l − 1 of them.
In this case, it can upload layer l at a rate less than r. Finally, the last row considers the case where the peer cannot upload layer l at all.
Now that we obtained E[ U l | U ≥ z ] for any arbitrary z, we can calculate functions f all (·) and f rnd (·) using Eqs.
(6) and (7):f all (e, l, r, n) = n × E[ U l | U ≥ αrl ] f rnd (e, l, r, n) = ⌊ e r ⌋ × E[ U l | U ≥ αrl ]Next, we calculate fopt(·) using Eq.
(4):fopt(e, l, r l , n) = 񮽙 αM 0 P r(U = u | U ≥ αrl) × P r(U ≥ u | U ≥ αrl) k × P r(U < u | U ≥ αrl) n−k−1 × E[U l | U ≥ αrl, U ≥ u] du,(17)in which all probabilities can be easily derived according to the uniform distribution of U in [0, αM ].
The next function we need to calculate is P r(N l = n | ǫ), the probability distribution of the number of peers at a segment which are demanding layer l, when the failure probability for peers at that segment is ǫ.
This is done using Eq.
(8) as follows:P r(N l = n | ǫ) = ∞ 񮽙 i=n fPoisson(⌈ i 1 − ǫ ⌉; λ) 񮽙 i n 񮽙 ( αM − rl αM ) n ( rl αM ) i−n (18)We can now analyze the distribution of each video segment through Eqs.
(9) and (11), since we have already obtained all its preliminaries in the above equations.
Thus, we have:x i t,l = ˆ f ( x i−1 t,l × T + T seed T +C t,l , l, r, λ(1 − t−1 񮽙 i=1ˆγi i=1ˆ i=1ˆγi) 񮽙 񮽙񮽙 񮽙 I , ˆ γt ),(19)where term I denotes the expected number of peers (with Poisson distribution) at segment t, i.e., peers whose expected number was λ at the beginning of their joining, and left the (a) The streaming capacity when we only seed at the first time step.
The curves from bottom to top correspond to initial seed values from 1 layer rate to 10 layer rates.
(b) The streaming capacity when we seed for longer periods.
The bottom curve is for when we seed with 1 layer bitrate at the first time step only, and the other 10 curves correspond to longer seeding periods with capacities 1 to 10 layer bitrates.
system with probabilityˆγiprobabilityˆ probabilityˆγi at each time instance i. Denoting the value converged over time by x t,l , the steady-state number of peers receiving layer l of segment t, i.e., m t,l , is calculated using Eq.
(12).
Since each video segment is analyzed individually, let us first focus on the distribution of one segment.
We would like to see how many peers at a given segment can be served with a particular layer l in the long term.
We then use these results as a basis for analyzing the distribution of the complete video.
To carry out a numerical analysis, we fix the value of some parameters introduced in the previous subsection as follows.
We suppose the distribution of a 1-hour video which we divide into 60 1-minute segments.
The video has L = 10 layers, each at rate r = 200 kbps.
The maximum possible download bandwidth of a peer is M = 10 Mbps and the upload bandwidth is α = 20% of its download bandwidth.
Peers arrive at a rate of λ = 10 peers per minute with a Poisson distribution, and after watching the video, they stay for seeding for up to T seed = 30 minutes.
25% of peers leave the system before they finish watching and seeding, which happens with an exponential probability distribution with most of the departures taking place in the first few minutes of watching.Step 1: Distribution of one video segment.
We first note that the number of peers served with layer l of a given segment t converges over time, as discussed in Section 4.3.
This can be observed in Figure 1(a), which shows a sample result.
In this figure, considering a window of peers at one segment over time, the number of peers that are served with layer 5 in each time step is plotted.
Each curve, from bottom to top, corresponds to an initial seed capacity, starting from 1 layer bitrate and increasing to 10.
The failure probability for peers at the segment is assumed 20%.
In Figure 1(a), independent of the initial seed, the number of peers served with layer 5 of the segment converges to 5.6 on average, though it takes more time to boost the capacity when the initial seed is small while larger initial seeds lead to faster convergence.
If we do not stop seeding after the first time step and keep seeding over time, indeed a higher bitrate is served to the peers.
This increase in the throughput for streaming layer 5 of the segment is illustrated in Figure 1(b).
In this figure, the increase in the number of served peers when the seeding capacity is increased is more significant for smaller capacity values, which is particularly observable as the gap between the three lowest curves.
This is because in those cases, the entire seeded bitrate is consumed by peers, and since peers will also contribute to serving, the increase in the throughput is more than the seeded bitrate.
On the other hand, if the seed is more than the demand of the peers, only part of it is consumed, and the number of peers served will converge to the expected number of peers demanding the layer.
This is observed in Figure 1(b) for seeding capacities of more than 3 layer bitrate.
In particular, 7 out of the 11 curves gather on top of the figure.
Though Figure 1(b) corresponds to the serving of layer 5 only, serving of other layers also demonstrate similar behaviors.
For lower layers (1 to 4), the most bottom curve converges to higher values than 5.6, because the total upload rate of lower layers by the peers is higher.
Moreover, because of the same reason, more curves gather on top of the figure.
For higher layers (6 to 10), the reverse of these behaviors is observed: the most bottom curve converges to a smaller value than 5.6, and less curves gather on top of the figure.In summary, using our analysis and given the seeding capacity, we can determine for each layer the value to which the number of served peers at a segment converges, i.e., the m t,l value introduced in Section 4.3, which is the steadystate number of peers served with layer l of segment t.
We use these values for analyzing the distribution of the complete video in the next step.Step 2: Deriving the throughput for the complete video.
Using the previous step, we obtain m t,l for all valid t, l values.
Consequently, we can calculate the throughput of the network in terms of various metrics.
For example, we es- timate the average video quality delivered to peers using Eq.
(14) for different values of seeding capacity C.
This is plotted in Figure 2 for both cases of contribution-based and random serving.
Figure 2 answers questions Q1 and Q2 discussed in Section 1: it estimates the throughput of the system in terms of video quality, and it computes the capacity needed for providing a desired level of video quality.
For example, if we provide 25 Mbps seeding capacity, the average video quality delivered to peers will be 35.9 dB.
If we increase this capacity to 125 Mbps, the average quality will increase to 37.3 dB.
This increase continues as we provider higher capacities, and it eventually approaches a value of approximately 38.5 dB, which is the maximum quality demand of peers-note that although the video stream can provide a quality of up to 40 dB, not all peers are demanding this quality level.
In this section, we first validate our analytical model by comparing its results to those obtained from simulation.
Then, we show how we can use the analysis for studying different performance aspects of P2P streaming systems.
We validate our analytical model by comparing its results to those of running the considered P2P streaming network in a simulator for P2P on-demand streaming of scalable videos.Simulation setup.
We simulate distribution of a 1-hour video which consists of L = 10 layers, each at rate r = 200 kbps.
The video segment length is Tseg = 1 minute.
The download bandwidth of peers is uniformly distributed in [0,10] Mbps and the upload bandwidth of a peer is α = 20% of its download bandwidth.
Peers arrive according to a Poisson distribution with an average arrival rate of λ = 10 peers per minute, and may stay for seeding for up to T seed = 30 minutes after they finished watching the video.
25% of peers leave the system before they finish watching and seeding, where most of the departures take place in the first few minutes of watching.
According to the arrival and failure/departure rates, the video length, and the peer seeding time, the number of peers in the network varies with time, but on average there are 500-600 at any time.
Each simulation runs for 5 hours of simulation time.
The simulation follows an event-driven procedure that gathers events and once every 10 seconds applies all of them.
As a sender, each peer disconnects its connection to a receiver according to a Bernoulli probability distribution with an expected value of 2 minute.
This is done so as to make enough dynamics in the network and to simulate peer failures in addition to the considered arrivals and departures.
Moreover, we make peers disobey our assumption about the way they share their upload bandwidth among different layers in Eqs.
(1) and (2), by having each u p,l value deviate by up to 50% from its supposed value.
This is used to capture peers and network dynamics in real systems.
Furthermore, a peer may receive different number of layers from different segments, which, if varies non-smoothly, results in a poor quality.
Thus, to make the simulation more realistic, we avoid quality fluctuations at receiver side by having each peer take a simple heuristic that does not allow more than 1 layer change in the number of layers in two consecutive video segments, i.e., the heuristic at each peer drops the enhancement layers that violate this criterion.
The video segment length and the simulation time step are 10 seconds.
We run the simulation for different values of seed server capacity, and compare the results with those of our analysis.
This comparison is illustrated in Figure 3, which shows the average video quality in the steady state obtained by the analytical and the simulation study for different values of seeding capacity.
Figure 3 shows that estimations of the analysis are reasonably close to the results obtained by simulations, except for small seeding capacities.
The deviation observed for these values in the case of random serving is because the analytical model assumes a certain value as the upload bandwidth of a peer for a particular layer in Eqs.
(1) and (2).
This assumption becomes far from reality when the seeding capacity is small and it is not carefully allocated, because in this case only a few video layers are exchanged between peers and the upload bandwidths of peers for higher layers become significantly smaller than expected; and those for lower layers are larger than expected.
Except for this particular case, the simulation explained in this section with dynamic P2P streaming systems confirms the accuracy of our analysis.
Using our analytical model, we compare the two methods of random and contribution-based serving of peers introduced in Section 3.
We first compare these two approaches in terms of the video quality that they can deliver to peers.
We compute the average delivered quality in these two cases using our analytical model, and plot the results in Figure 3.
The figure demonstrates the impact of the seed server and peer matching policy.
The contribution-based policy clearly outperforms the random policy.
In some cases, up to 50 Mbps higher seeding capacity is needed for the random policy to provide the same quality level as the contributionbased policy.
This difference is specially significant for limited seed server capacities, which is the typical case in practice.
The difference diminishes for very large seeding capacities, as the capacity approaches a value that suffices for fully serving all peers with their demanded substreams.Next, we compare the total video bitrate served in the network.
This value is the sum of the bitrates of all substreams that the peers receive in the steady state.
Figure 4 illustrates this value for the two cases of contribution-based and random serving.
Similar to the average video quality, the gap between the bitrate served in these two cases is more significant (up to 300 Mbps) for smaller seeding capacity values.
That is, careful serving with limited seeding capacity leads to a much stronger boost in the capacity of the network.
Most notably, when there is no seeding, e.g., seed servers are turned off in the steady state, the total bitrate that peers can serve to each other in case of contributionbased serving is nearly twice its value with random serving.
As another example, the random approach can boost a 50 Mbps seeding capacity to a throughput of 540 Mbps in the P2P system, whereas this value is 660 Mbps (22% higher) with careful serving.
Therefore, our analysis can be used to analyze the impact of different methods for allocating seed servers and/or matching peers on the throughput of P2P systems.
We presented two sample methods, serving randomly and serving towards maximizing the overall video quality delivered.
Nevertheless, other methods and heuristics for prioritizing peers and their requests can as well be accommodated in the analytical model.
For example, in Eq.
(4) one might take into account not only the upload capacity of the requesting peer, but also the demanded substreams in order to minimize the gap between peers' received layers and their demanded layers.
One of the important concerns for administrators of multimedia streaming systems is the flash crowd event, in which a large volume of peers arrive in the system at approximately the same time.
This imposes an extensive load on the system and may significantly impact its performance.
In such events, the system may not be able to serve all the newly arrived peers, and may have to reject many peers.
In these scenarios, an important question is how many peers the system can admit in case of a flash crowd of peers.Using the flash crowd analysis in Section 4.4, we obtain the maximum number of peers (arriving together) that can be served by the system for different values of seeding capacity.
The results of this test are plotted in Figure 5, which answer question Q3 discussed in Section 1.
In this figure, curves labeled "Analysis" show the upper bound on the flash crowd size that can be supported by the system, using the total serving capacity of peers and seed servers.
Curves labeled "Simulation" depict the number of peers supported in a flash crowd in a simulated P2P system.
To obtain these numbers, we make a set of 10,000 peers arrive in the system at the same time.
Shortly after the arrivals, we measure the number of peers out of these 10,000 that could receive a video stream (the base layer).
Note that the analytical results for the flash crowd test only provide an upper bound on the number of peers that can be served.
Accordingly, there is a gap between simulation results and analysis results for this test.
Moreover, although with a smaller domain, the results of simulation also do not exhibit a smooth behavior.
This can be attributed to the high degree of dynamics and instability introduced to the simulated system by the large crowd of peers.
Nevertheless, Figure 5 shows that for smaller seed server capacities, the two serving methods make a significant difference in terms of the flash crowd size that can be supported.
For example, to support 3,500 peers we need approximately 70 Mbps capacity for seed server in case of random serving, whereas we can support this number of peers with almost no extra seeding if the serving has been done carefully until the flash crowd arrives.
We have presented an analytical model to forecast the performance of P2P streaming systems with scalable videos.
Given the characteristics of a P2P streaming system, our model computes the long-term throughput of the network in terms of the delivered video quality and the total served bitrate.
Our analysis can also asses the benefit of deploying a given amount of seeding resources, and help determining the cost-benefit tradeoff for provisioning a higher seeding capacity to sustain a desired level of video quality.
This is an important benefit of the proposed analytical model, since most current P2P streaming systems need to deploy auxiliary seed servers in order to offset the capacity shortage created by the asymmetry between peers' upload and download bandwidths.
In addition, the proposed analysis can estimate an upper bound on the capability of a P2P network for supporting a flash crowd of peers, which is the maximum number of peers that can be admitted to the system at once.
We have validated our analytical model using extensive simulations with realistic parameters of dynamic P2P streaming systems.
The results of our simulations confirm the accuracy of the proposed model.
