State-of-the-art flash-optimized KV stores frequently rely upon a log structure and/or compaction-based strategy to optimally organize content on flash.
However, these strategies lead to excessive I/O, beyond the write amplification generated within the flash itself, with both the application and the flash device constantly rearranging data.
In this paper, we explore the other extreme in the design space: minimal data management at the KV store and heavy reliance on the Flash Translation Layer (FTL) capabilities.
NVMKV is a scalable and lightweight KV store that leverages advanced capabilities that are becoming available in modern FTLs.
We demonstrate that NVMKV (i) performs KV operations at close to native device access speeds for get operations , (ii) outperforms state of the art KV stores by 50%-300%, (iii) significantly improves performance predictability for the YCSB KV benchmark when compared with the popular LevelDB KV store, and (iv) reduces data written to flash by as much as 1.7X and 29X for sequential and random write workloads relative to Lev-elDB, thereby dramatically increasing device lifetime.
Key-value (KV) stores are ubiquitous, having become the default data management software for many Internet services [8,10,21,22,32].
They serve application needs in a variety of different domains that demand highthroughput and low-latency data access [1,3].
The performance, capacity, and power consumption mix of flash-based storage makes it an attractive medium for KV stores [12,15,19,20,27].
To get the best performance from both HDDs and low-end SSDs, many KV stores use some form of log structured writing to optimize data layout on media.
Since log structured updates require eventual compaction or garbage collection, the consequence is auxiliary write amplification, i.e., additional write amplification introduced at the KV store besides the write amplification introduced by the Flash Translation Layer (FTL).
For instance, the SILT work introduces an auxiliary write amplification of 5.4 [27].
The recent LevelDB KV store from Google [22] also exhibits rather dramatic auxiliary write amplification.
in flash demonstrates that the cumulative write amplification is multiplicative, causing even a small user update to result in massive writing to the flash over time [33].
Three trends force us to rethink KV store design choices.
First, NAND flash endurance is getting poorer with every new media generation [25].
With fewer Program/Erase cycles to begin with, the auxiliary write amplification further reduces the device lifetime and increases the KV store's total cost of ownership.
Second, the gap between sequential and random write performance has significantly narrowed in state of the art SSDs today [26], calling into question the need for application level log structuring and compaction.
Third, modern FTLs are much more powerful than the traditional block devices.
New FTL interfaces have recently been developed to provide advanced capabilities to access data to (or from) NAND Flash [7,9,26,29,30].
In this paper, we explore a new design for a KV store -one that relies upon cooperative design with an FTL to minimize auxiliary write amplification and maximize application-level performance.
The resulting KV store, NVMKV, is lightweight and fully exploits native characteristics of the FTL to achieve get performance equivalent to raw device read speeds and put operations that are significant fractions of raw device write speeds.
NVMKV makes several novel contributions.
While many flash and disk optimized KV stores exist (see Figure 2), NVMKV is the first to leverage native FTL layer primitives such as atomic multi-block write, atomic RocksDB [10] LevelDB [22] SILT [27] FAWN-KV [12] SkimpyStash [20] BerkeleyDB [28] HashCache [15] MemcacheDB [5] MongoDB [8] FlashStore [19] multi-block persistent trim, exists, and iterate to implement KV functionality.
NVMKV demonstrates how strong consistency and atomic guarantees provided by the underlying FTL can be used to achieve atomicity and isolation, low write amplification, and performance close to that of the raw device.
NVMKV is also the first KV store that uses a small (close to zero), constant, amount of in-memory metadata that is independent of both the number of keys stored and the workload intensity.Our evaluation of NVMKV reveals the following.
NVMKV performs get operations at close to the native device access speeds.
Relative to LevelDB, a popular KV store in wide use today, NVMKV reduces auxiliary write amplification by as much as 1.7X and 29X for sequential and random write workloads respectively.
For the YCSB KV benchmark, NVMKV outperforms LevelDB by 50%-300% besides significantly reducing the variance of KV operation latencies when compared with LevelDB.
Finally, NVMKV provides significantly better performance when using a fourth of the user-level DRAM cache size compared to LevelDB and unlike LevelDB, without using any OS-level DRAM caching.
Modern FTLs are powerful software layers that include functions such as log-structuring, dynamic data remapping, indexing, transactional updates, and thin provisioning [26,29], which are superficially similar to the functionality being built into many KV stores.
For instance, FTLs implement an indirection map to manage the logical to physical block address mapping and write logging to guarantee durability on a medium that implicitly forces out-of-place writes.
There is ongoing effort to surface these advanced capabilities through standardized primitives for use by operating systems and user space software [7,9].
Table 1 lists some of the primitives that can be surfaced by a modern FTL.
Recent work has utilized such primitives for implementing efficient file systems [26], databases [29], and caching [30].
queries if an address is populated writes an address range as ACID tx.
deletes an address range as ACID tx.ITERATE returns all populated addresses Table 1: FTL Primitives.
These primitives can be used for either individual or ranges of (both sparse and non-sparse) locations.Additionally, batch operations of ATOMIC-WRITE, ATOMIC-TRIM, and combinations are also possible, allowing the write of some locations and the deletion of other locations as a single transaction.
FTLs maintain an indirection map translating logical block addresses to physical locations.
This mapping is required to organize data for minimal write amplification and best wear leveling.
Most KV stores also maintain a mapping engine that converts keys to storage addresses where the values are stored.To leverage an FTL based remapping engine for mapping key-value pairs, we extend the FTL indirection map to a sparse map, similar to that used in previous work [26].
A sparse map provides a few orders of magnitude more addressable logical addresses (LBAs) for the same physical capacity, thinly provisioning physical locations only for LBAs that have been written.
Leveraging the underlying sparse addressing, NVMKV replaces the indirection maps found in most KV stores with hashing functions over the sparse address space.
Through this approach, put and get operations are simply mapped to write and read operations in the FTL, respectively.
A delete of a key removes the KV pair from the storage device using the trim operation.
FTLs maintain both data and metadata, and in particular, persistent indirection maps to recover data upon restart.
Since FTLs operate as copy-on-write, they can provide high performance transactional write semantics [29].
KV stores can leverage this capability to ensure that puts of KV pairs are atomic without additional journaling, and providing nearly the same performance as that of conventional writes.
Access to the transactional persistence capabilities of the FTL can be provided through two primitives, ATOMIC-WRITE and ATOMIC-TRIM.
FTLs support highly parallel read/write operations to match the parallelism available inside the NAND die of most flash devices.
By utilizing the atomic operations, we can minimize locking within the KV store and better leverage the inherent parallelism within the flash device.
NVMKV is designed as a user space library that exports a KV API and leverages the publicly available FTL prim- itives (see Table 1) to access flash devices [9].
These primitives are implemented within Fusion-io's ioMemory FTL [23] and exported as a set of IOCTLs.
Leveraging sparse addressing is central to NVMKV since it allows minimizing I/O amplification during both get and put operations.
In the absence of collisions, each get or put operation translates into exactly one I/O to flash.
The elimination of an indirection map results in fixed (nearly zero) metadata at the KV store.To effectively utilize the sparse address space, NVMKV divides the sparse address into two parts: the Key Bit Range (KBR) and the Value Bit Range (VBR).
By default, NVMKV uses 36 bits for the KBR and 12 bits for the VBR in a 48 bit address space, with alternatives configurable by the user when the KV store is created.
The VBR defines the amount of contiguous address space (i.e., maximum value size) reserved for each KV pair, ensuring that KV pairs mapped into the sparse address space to not overlap each other in logical address space.
The KBR determines the maximum number logical hash slots that each KV pair can be placed into.
User supplied keys are mapped to LBA addresses through a simple hash model (Figure 3).
Keys can be variable length up to the maximum supported key size (2MB for a 12 bit VBR).
Since each VBR will contain exactly one KV pair, hash conflicts only occur in the KBR.
The design of NVMKV assumes that the KBR is kept sufficiently large, relative to the number of keys that can be stored in a flash device, to reduce the chances of a hash collision.
For example, 1TB of flash can contain a maximum of 2 billion 512B KV pairs.
With the default KBR of 36 bits which supports 64 billion hash slots, and uniform hashing of KV pairs across KBR space, the chances of a new KV pair inserted into even a full device causing a collision is˜3%is˜ is˜3%.
Collisions are handled within the library by deterministically computing an alternate hash location (via polynomial probing) within the KBR.
Up to eight hash locations are tried before the KV Store refuses to accept a new key.
Assuming that the hash function uniformly distributes keys, the probability of a PUT failing in the above example equals the probability of 8 consecutive collisions and is approximately (1/(64 billion / 2 billion) 8 = 1/2 40 is vanishingly small.
The sparse address bits can be increased proportionately as device capacities increase to maintain low hash collision probability.
The minimum unit of storage in NVMKV is a sector where keys and values smaller than 512B will consume a full 512B sector.
Each KV pair will also contain some metadata in a header stored on media.
NVMKV packs and stores the metadata, the key, and the value in a single sector if the sum of their individual sizes is less than or equal to the sector size.
Our instance of NVMKV implements a DRAM cache which can be used to hold KV pairs.
Separately, a collision cache holds information about recent hash collisions, reducing the need for additional flash lookups at collision time.
We evaluated the performance, overhead, and auxiliary write amplification of NVMKV, comparing it to the raw device and LevelDB 1.14 [22].
We used the FIO benchmark, the YCSB KV benchmark [17], and the LevelDB suite of micro-benchmarks for our workloads.
Our experiments were performed on a system with a Quad-Core 2.5 GHz AMD Opteron(tm) Processor, 8GB of DDR2 RAM, and a 825GB Fusion-io ioScale2 drive running Linux Ubuntu 12.04 LTS.
Our first experiment evaluates the overhead of the NVMKV stack and LevelDB relative to the raw flash device.
Get and put used 512B values and key sizes ranging from 1 byte to 128 bytes and for these sizes, NVMKV issues I/O operations of size 1KB.
The FIO tool was configured to generate 1KB I/Os to the raw device.
Figure 4 shows that as the thread count increases, the throughput for NVMKV's get operations tracks the FIO benchmark's read rate.
For put operations, NVMKV significantly outperforms both the asynchronous and synchronous versions of LevelDB.
Additional overheads in NVMKV such as checking for collisions cause performance to be lower than the underlying native device write performance extracted by FIO.
YCSB is a framework for comparing the performance of KV stores and implements six workload personalities A-F [17].
The YCSB dataset size was 10GB and we evaluated both KV stores with caches of size 256MB (C: 256 MB) and 1GB (C: 1 GB).
LevelDB implements write buffering and utilizes the OS page cache (both active during the experiment) while NVMKV does neither.
LevelDB was configured to perform asynchronous writes.
Figure 5 demonstrates throughput performance gains of 50%-300% with NVMKV relative to LevelDB when running the YCSB workloads.
These performance gains are even more significant when we consider that LevelDB does not provide durability while NVMKV does, and that LevelDB uses both a write buffer and the OS buffer cache for additional DRAM caching/buffering, while NVMKV does neither.
We do not report results for YCSB-E because it performs short range scans (short sequential scans at randomly chosen locations), an operation not currently supported by the YCSB Java binding for NVMKV.We measured how KV operation latencies varied over time for each of the workloads ( Figure 6).
We include data for both async (default) and sync modes for LevelDB writes.
There are a few interesting insights here.
First, NVMKV, which delivers atomic and durable updates, significantly outperforms even the LevelDB's best performing, but weaker, async mode.
Second, we note that the performance variance in LevelDB is greater relative to NVMKV with significant latency spikes.
We measured average/maximum KV read latencies of 0.33/1.38 and 0.33/1.39 ms for LevelDB and LevelDB-S respectively, relative to 0.14/0.66 ms for NMVKV.
Since the LevelDB latency spikes seem to occur periodically, we believe these are correlated with the internal compaction mechanism.
On the other hand, NVMKV offers much more consistent latency performance over time.Revisiting Figure 1, we see that for the LevelDB suite of microbenchmarks [22], NVMKV incurs 1.7X to 29X lower auxiliary write amplification than LevelDB.
The performance gains in NVMKV can be attributed to this reduction, as well as eliminating layers of indirection and metadata management overhead.
Besides NVMKV's performance improvements and endurance gains, atomic durability guarantees for KV operations are an added benefit.
The need for such guarantees is a topic of debate within the NoSQL community with KV stores implementing different levels of eventual to strict consistency and varying degrees of durability.
In NVMKV, we were able to provide strictly atomic and synchronous durability guarantees by leveraging the underlying FTL capability.
Thus, rather than adding complexity and sacrificing performance to achieve strict guarantees (and thereby feeling pressurized to give them up), we found that leveraging their presence in the underlying FTL helped us simplify the NVMKV design without sacrificing performance.
We also observed that by extracting more of the native performance of the flash device, we were able to deliver more KV operation performance than LevelDB while consuming less DRAM.
We believe that NVMKV represents a sound building block on which scale-out KV stores can be built.
This is an area we intend to explore further in the future.Due to lack of space, we did not include any descriptions of how the FTL supported the primitives that NVMKV relies upon.
However, the implementations of similar primitives have been discussed with descriptions of possible FTL implementation designs [26,29,30].
In particular, FlashTier discusses sparse maps, showing how an incremental extension to an existing FTL data structure can enable dramatic DRAM reductions in applications while only causing moderate additional DRAM consumption at the FTL.A limitation in our current design is the requirement to map individual KV pairs to separate sectors.
NVMKV is best utilized for KV pairs which consume over 256 bytes.
While many workloads fit this criterion, there are also many that do not.
For the second group of workloads, NVMKV will have poor capacity utilization.
One way to manage efficient storage of small KV pairs is to follow a multi-level storage mechanism, as provided in SILT [27], where small items are initially indexed separately and later compacted into larger units such as sectors.
This is also a target area for future work.
We explored a novel concept of a KV store designed cooperatively with an FTL to reduce redundant work across the two layers.
The result, NVMKV, is able to extract significant fractions of the raw device performance and outperform a state of the art KV store while minimizing auxiliary write amplification.
NVMKV is open source, available at https://github.com/opennvm/nvmkv.
We thank the reviewers for their feedback and comments.
We also would like to thank the people who helped build NVMKV and the FTL interface primitives.
