We give an O(n log 2 n)-time, linear-space algorithm that, given a directed planar graph with positive and negative arc-lengths, and given a node s, finds the distances from s to all nodes.
The best previously known algorithm requires O(n log 3 n) time and O(n log n) space.
2.1 Embedded Planar Graphs A planar embedding of a graph assigns each node to a distinct point on the plane, and assigns each edge to a simple arc between the points corresponding to its endpoints, with the property that no arc-arc or arc-point intersections occur except for those corresponding to edge-node incidence in the graph.
A graph is planar if it has a planar embedding.
Consider the set of points on the sphere that are not assigned to any node or edge; each connected component of this set is a face of the embedding.
Miller [17] gave a linear-time algorithm that, given a triangulated two-connected n-node planar embedded graph, finds a simple cycle separator consisting of at most 2 √ 2 √ n nodes, such that at most 2n/3 nodes are strictly enclosed by the cycle, and at most 2n/3 nodes are not enclosed.For an n-node planar embedded graph G that is not necessarily triangulated or two-connected, we define a Jordan separator to be a Jordan curve C that intersects the embedding of the graph only at nodes such that at most 2n/3 nodes are strictly enclosed by the curve and at most 2n/3 nodes are not enclosed.
The nodes intersected by the curve are called boundary nodes and denoted V c .
To find a Jordan separator with at most 2 √ 2 √ n boundary nodes, add artificial edges with sufficiently large lengths to triangulate the graph and make it two-connected without changing the distances in the graph.
Now apply Miller's algorithm.The internal part of G with respect to C is the embedded subgraph consisting of the nodes and edges enclosed by C, i.e. including the nodes intersected by C. Similarly, the external part of G with respect to C is the subgraph consisting of the nodes and edges not strictly enclosed by C, i.e. again including the nodes intersected by C.Let G 1 (G 0 ) denote the internal (external) part of G with respect to C.
Since C is a Jordan curve, the set of points of the plane strictly exterior to C form a connected region.
Furthermore, it contains no point or arc corresponding to a node or edge of G 1 .
Therefore, the region remains connected when these points and arcs are removed, so the region is a subset of some face of G 1 .
Since every boundary node is intersected by C, it follows that all boundary nodes lie on the boundary of a single face of G 1 .
Similarly, in G 0 , all boundary nodes lie on the boundary of a single face.
- ing A matrix M = (M ij ) is totally monotone if for ev- ery i, i 񮽙 , j, j 񮽙 such that i < i 񮽙 , j < j 񮽙 and M ij ≤ M ij 񮽙 , we also have M i 񮽙 j ≤ M i 񮽙 j 񮽙 .
Totally monotone matrices were introduced by Aggarwal et al. in [2], who showed that a wide variety of problems in computational geometry could be reduced to the problem of finding row-maxima or row-minima in totally monotone matrices.
Aggarwal et al. also give an algorithm, nicknamed SMAWK, that, given a totally monotone n× m matrix M , finds all rowmaxima of M in just O(n + m) time.
It is easy to see that by negating each element of M and reversing the order of its columns, SMAWK can be used to find the row minima of M as well.A matrix M = (M ij ) is convex Monge (concave Monge) if for every i, i 񮽙 , j, j 񮽙 such that i < i 񮽙 , j < j 񮽙 , we have M ij + M i 񮽙 j 񮽙 ≥ M ij 񮽙 + M i 񮽙 j (M ij + M i 񮽙 j 񮽙 ≤ M ij 񮽙 + M i 񮽙 j ).
It is immediate that if M is convex Monge then it is totally monotone.
It is also easy to see that the matrix obtained by transposing M and then reversing the order of the columns is also totally monotone.
Thus SMAWK can also be used to find the column minima and maxima of a convex Monge matrix.In [13] Klawe and Kleitman define a falling staircase matrix to be a lower triangular fragment of a totally monotone matrix.More precisely, (M, {f (i)} 0≤i≤n+1 ) is an n × m falling staircase matrix if1.
for i = 0, . . . , n + 1, f (i) is an integer with 0 = f (0) < f(1) ≤ f (2) ≤ · · · ≤ f (n) < f(n + 1) = m + 1.2.
M ij , is a real number if and only if 1 ≤ i ≤ n and 1 ≤ j ≤ f (i).
Otherwise, M ij is blank.3.
for i < k and j < l ≤ f (i), andM ij ≤ M il , we have M kj ≤ M kl .
Finding the row maxima in a falling staircase matrix can be easily done using SMAWK in O(n+m) time after replacing the blanks with sufficiently small numbers so that the resulting matrix is totally monotone.
However, this trick does not work for finding the row minima.
Aggarwal and Klawe [1] give an O(m log log n) time algorithm for finding row-minima in falling staircase matrices of size n × m. Klawe and Kleitman give in [13] a more complicated algorithm that computes the row-minima of an n × m falling staircase matrix in O(mα(n)+n) time, where α(n) is the inverse Ackerman function.
Again, it is not hard to see that both these algorithms can be used to find the column minima, by transposing and then reversing the order of the columns, as above.
For a directed graph G with arc-lengths 񮽙(·), a price function is a function φ from the nodes of G to the reals.
For an arc uv, the reduced length with respect to φ is 񮽙 φ (uv) = 񮽙(uv) + φ(u) − φ(v).
A feasible price function is a price function that induces nonnegative reduced lengths on all arcs of G.Feasible price functions are useful in transforming a shortest-path problem involving positive and negative lengths into one involving only nonnegative lengths, which can then be solved using Dijkstra's algorithm.
For any nodes s and t, for any s-to-t path P , 񮽙 φ (P ) = 񮽙(P ) + φ(s) − φ(t).
This shows that an s-to-t path is shortest with respect to 񮽙 φ (·) iff it is shortest with respect to 񮽙(·).
Moreover, the s-to-t distance with respect to the original lengths 񮽙(·) can be recovered by adding φ(t) − φ(s) to the s-to-t distance with respect to񮽙 φ (·).
Suppose φ is a feasible price function.
Running Dijkstra's algorithm with the reduced lengths and modifying the distances thereby computed to obtain distances with respect to the original lengths will be called running Dijkstra's algorithm with φ.An example of a feasible price function comes from single-source distances.
Suppose that, for some node r of G, for every node v of G, φ(v) is the r-to-v distance in G with respect to 񮽙(·).
Then for every arc uv, φ(v) ≤ φ(u) + 񮽙(uv), so 񮽙 φ (uv) ≥ 0.
Klein [14] gives a multiple-source shortest-path algorithm with the following properties.
The input consists of a directed planar embedded graph G with non-negative arclengths, and a face f .
For each node u in turn on the boundary of f , the algorithm computes (an implicit representation of) the shortest-path tree rooted at u.
The basic algorithm takes O(n log n) time and O(n) space on an n-node input graph.
In addition, given a set of pairs (u, v) of nodes of G where u is on the boundary of f , the algorithm computes the u-to-v distances.
The time per distance computed is O(log n).
In particular, given a set S of O( √ n) nodes on the boundary of a single face, the algorithm can compute all S-to-S distances in O(n log n) time.
In fact, the multiple-source shortest-path algorithm does not require that the arc-lengths be nonnegative if the input also includes a table of distances to all nodes from some node on the face f .
This observation follows from careful inspection of the algorithm [14] itself.
Alternatively, it also follows from the pricefunction technique of Section 2.4; the table of distances can be used to obtain nonnegative reduced lengths.
such that d 񮽙 i [v] is the r-to-v distance in G for every node v of G i distances rerooting 8 define a price function φ for G such that φ[v]is the r-to-v distance in G: single- source φ[v] = 񮽙 d 񮽙 0 [v] if v belongs to G 0 d 񮽙 1 [v] otherwise The high-level description of the algorithm appears in Figure 3.
After finding a Jordan separator and selecting a boundary node as a temporary source node, the algorithm consists of five major steps.
The recursive call step is straightforward.
Computing intra-part boundary distances uses the algorithm described in Section 2.5.
Computing single-source inter-part boundary distances is described in Section 4; it is based on the BellmanFord algorithm.
Single-source inter-part distances is described in Section 5, and is based on Dijkstra's algorithm.
It yields distances to all nodes from the temporary source node.
These distances constitute a feasible price function, as described in Section 2.4, that enables us, in rerooting single-source distances, to use Dijkstra's algorithm once more to finally compute distances from the given source.
In this section we describe how to efficiently compute the distances in G from r to all boundary nodes (i.e., the nodes of V c ).
This is done using δ 0 and δ 1 , the allpairs distances in G 0 and in G 1 between nodes in V c which were computed in the previous stage.
Figure 4: Pseudocode for the single-source inter-part boundary distances stage for calculating shortest-path distances in G from r to all nodes in V c using just δ 0 and δ 1 .
e j [v] = 񮽙 min w∈Vc {e j−1 [w] + δ 1 [w, v]}, if j is odd min w∈Vc {e j−1 [w] + δ 0 [w, v]}, if j is even 񮽙 , ∀v ∈ V c 5: B[v] ← e |Vc| [v] for all v ∈ V cThe rest of this section describes the algorithm, thus proving Theorem 4.1.
The following structural lemma stands in the core of the computation.
The same lemma has been implicitly used by previous planarityexploiting algorithms.
The proof is straightforward and omitted here.Lemma 4.1.
Let P be a simple r-to-v shortest path in G, where v ∈ V c .
Then P can be decomposed into at most |V c | subpaths P = P 1 P 2 P 3 . . . , where the endpoints of each subpath P i are boundary nodes, and P i is a shortest path in G i mod 2 .
Lemma 4.1 gives rise to a dynamic programming solution for calculating the from-r distances to nodes of C, which resembles the Bellman-Ford algorithm.
The pseudocode is given in Fig. 4.
Note that, at this level of abstraction, there is nothing novel about this dynamic program.
Our contribution is in an efficient implementation of Step 4.
The algorithm consists of |V c | iterations.
On odd iterations, it uses the boundary-to-boundary distances in G 1 , and on even iterations it uses the boundary-toboundary distances in G 0 .
Lemma 4.2.
After the table e j is updated by the algorithm, e j [v] is the length of a shortest path in G from r to v that can be decomposed into at most j subpaths P = P 1 P 2 P 3 . . . P j , where the endpoints of each subpath P i are boundary nodes, and P i is a shortest path inG i mod 2 .
Proof.
By induction on j. For the base case, e 0 is initialized to be infinity for all nodes other than r, trivially satisfying the lemma.
For j > 0, assume that the lemma holds for j − 1, and let P be a shortest path in G that can be decomposed into P 1 P 2 . . . P j as above.
Consider the prefix P 񮽙 , P 񮽙 = P 1 P 2 . . . P j−1 .
P 񮽙 is a shortest r-to-w path in G that can be decomposed into at most j − 1 subpaths as above for some boundary node w. Hence, by the inductive hypothesis, when e j is updated in Line 4, e j−1 [w] already stores the length of P 񮽙 .
Thus e j [v] is updated in line 4 to be at most e j−1 [w] + δ j mod 2 [w, v].
Since, by definition, δ j mod 2 [w, v] is the length of the shortest path in G j mod 2 from w to v, it follows that e j [v] is at most the length of P .
For the opposite direction, since for any boundary node w, e j−1 [w] is the length of some path that can be decomposed into at most j − 1 subpaths as above, e j [v] is updated in Line 4 to the length of some path that can be decomposed into at most j subpaths as above.
Hence, since P is the shortest such path, e j [v] is at least the length of P .
= e j−1 (v k ) + δ i (v k , v 񮽙 ).
Note that computing all minima in Line 4 is equivalent to finding the column-minima of A.
We define the upper triangle of A to be the elements of A on or above the main diagonal.
More precisely, the upper triangle of A is the portion {A kk : k ≤ 񮽙} of A. Similarly, the lower triangle of A consists of all the elements on or below the main diagonal of A. 1 ≤ k ≤ k 񮽙 ≤ 񮽙 ≤ 񮽙 񮽙 ≤ |V c | or 1 ≤ 񮽙 ≤ 񮽙 񮽙 ≤ k ≤ k 񮽙 ≤ |V c |), the convex Monge property holds:A kk + A k 񮽙 񮽙 񮽙 ≥ A kk 񮽙 + A k 񮽙 񮽙 .
Proof.
Consider the case 1 ≤ k ≤ k 񮽙 ≤ 񮽙 ≤ 񮽙 񮽙 ≤ |V c |, as in Fig. 5.
Since G i is planar, any pair of paths in G i from k to 񮽙 and from k 񮽙 to 񮽙 񮽙 must cross at some nodew of G i .
Let b k = e j−1 (v k ) and let b k 񮽙 = e j−1 (v k 񮽙 ).
Let ∆(u, v) denote the u-to-v distance in G i for any nodes u, v of G i .
Note that ∆(u, v) = δ i (u, v) for u, v ∈ V c .
l' k k' l wFigure 5: Nodes k < k 񮽙 < l < l 񮽙 in clockwise order on the boundary nodes.
Paths from k to l and from k 񮽙 to l 񮽙 must cross at some node w.
This is true both in the internal and the external subgraphs of G We have Proof.
We need to show how to find the column minima of the matrix A.
It follows directly from Lemma 4.3 that the lower triangle of A is a falling staircase matrix.
By [13], its column minima can be computed in O(|V c |α(|V c |)) time.
Another consequence of Lemma 4.3 is that the column-minima of the upper triangle of A may also be computed using the algorithm in [13].
To see this consider a counterclockwise ordering of the nodes ofA k, + A k 񮽙 ,, 񮽙 = (b k + ∆(v k , w) + ∆(w, v 񮽙 )) + (b k 񮽙 + ∆(v k 񮽙 , w) + ∆(w, v 񮽙 񮽙 )) = (b k + ∆(v k , w) + ∆(w, v 񮽙 񮽙 )) + (b k 񮽙 + ∆(v k 񮽙 , w) + ∆(w, v 񮽙 )) ≥ (b k + ∆(v k , v 񮽙 񮽙 )) + (b k 񮽙 + ∆(v k 񮽙 , v 񮽙 )) = (b k + δ i (v k , v 񮽙 񮽙 )) + (b k 񮽙 + δ i (v k 񮽙 , v 񮽙 )) = A k, 񮽙 + A k 񮽙 ,, The case (1 ≤ 񮽙 ≤ 񮽙 񮽙 ≤ k ≤ k 񮽙 ≤ |V c |) is similar.
|V c | v 񮽙 1 , v 񮽙 2 , . . . , v 񮽙 |Vc| such that v 񮽙 k = v |Vc|+1−k .
With this order, the upper triangle of A is in fact a falling staircase matrix.
Note that we never actually compute and store the entire matrix A as this would take O(|V c | 2 ) time.
We compute the entries necessary for the computation on the fly in O(1) time per element.Finally, we obtain A's column minima by comparing the two minima obtained for each column in each of the two triangular portions of A.
We thus conclude that A's column minima can be computed inO(2|V c | · α(|V c |) + |V c |) = O(|V c | · α(|V c |)) time.Hence the time it takes to compute the distances between r and all nodes of V c is O(|V c | 2 · α(|V c |).
We have thus proved Theorem 4.1.
The choice of separator ensures |V c | = O( √ n), so this computation is performed in O(nα(n)) time.
Single-Source Inter-Part Distances In the previous section we showed how to compute a table B that stores the distances from r to all the boundary nodes in G.
In this section we describe how to compute the distances from r to all other nodes of G.
We do so by computing tablesd 񮽙 0 and d 񮽙 1 where d 񮽙 i [v]is the r-to-v distance in G for every node v of G i .
Recall that we have already computed the table d i that stores the r-to-v distance in G i for every node v of G i .
The pseudocode given in Fig. 6 with the price function φ i in order to compute these from-r distances.The following two lemmas motivate the definition of G 񮽙 i and show that it captures the true from-r distances in G.
We omit the (simple) proof of Lemma 5.1.
Lemma 5.1.
Let P be an r-to-v shortest path in G, where v ∈ G i .
Then P can be expressed as P = P 1 P 2 , where P 1 is a (possibly empty) shortest path from r to a node u ∈ V c , and P 2 is a (possibly empty) shortest path from u to v that visits only nodes of G i .
i be the graph obtained from G i by removing arcs entering r, and adding an arc ru of length B[u] for every boundary node u.
The from-r distances in G 񮽙 i are equal to the from-r distance in G. Proof.
Distances in G 񮽙 i are not shorter than in G since each arc of G 񮽙 i corresponds to some path in G. For the opposite direction, consider the r-to-v distance in G. Let P 1 , P 2 , u be as in Lemma 5.1.
P 1 is a shortest path in G from r to some u ∈ V c .
By definition of G 񮽙 i , the length of the new arc ru in G 񮽙 i is equal to the length of P 1 in G. Furthermore, P 2 is a path in G 񮽙 i since it only consists of arcs in G i .
Since the shortest r-to-v path is simple, non of these arcs enters r, and therefore all of them are in G 񮽙 i .
Hence the length of the path in G 񮽙 i that consists of the new arc ru followed by P 2 equals the length of P in G. Figure 6: Pseudocode for the single-source intra-part distances stage for computing the shortest path distances from r to all nodes.Since G 񮽙 i contains arcs not in G i , we cannot use the from-r distances in d i as a feasible price function.
We slightly modify them to ensure non-negativity as shown by the following lemma.
Proof.
Let uv be an arc ofG 񮽙 i .
If uv is an arc of G i , then d i [v] ≤ d i [u] + 񮽙[uv], so 񮽙 φi [uv] ≥ 0.
Otherwise, u = r and 񮽙 φi [rv] = φ i [r] + B[v] − φ i [v] = p i + B[v] − d i [u]by definition of φ i ≥ 0 by definition of p iComputing the auxiliary graphs G 񮽙 i and the price functions φ i can be easily done in linear time.
Therefore, the time required for this stage is dominated by the O(n log n) running time of Dijkstra's algorithm.
We note that one may use the algorithm of Henzinger et al. [9] instead of Dijkstra to obtain a linear running time for this stage.
This however does not change the overall running time of our algorithm.
We will show that at each stage of our algorithm, the necessary information has been correctly computed and stored.
The recursive call in Line 4 computes and stores the from-r distances in G i .
The conditions for applying Klein's algorithm in Line 5 hold since all boundary nodes lie on the boundary of a single face of G i and since the from-r distances in G i constitute a feasible price function for G i .
The correctness of the single-source inter-part boundary distances stage in Line 6 and of the single-source inter-part distances stage in Line 7 was proved in Sections 4 and 5.
Thus, the r-to-v distances in G for all nodes v of G are stored ind 񮽙 0 for v ∈ G 0 and in d 񮽙 1 for v ∈ G 1 .
Note that d 񮽙 0 and d 񮽙 1agree on distances from r to boundary nodes.
Therefore, the price function φ defined in Line 8 is feasible for G, so the conditions to run Dijkstra's algorithm in Line 9 hold, and the from-s distances in G are correctly computed.
We have thus established the correctness of our algorithm.To bound the running time of the algorithm we bound the time it takes to complete one recursive call to shortest-paths.
Let |G| denote the number of nodes in the input graph G, and let |G i | denote the number of nodes in each of its subgraphs.
Computing the intra-subgraph boundary-to-boundary distances using Klein's algorithm takes O(|G i | log |G i |) for each of the two subgraphs, which is in O(|G| log |G|).
Computing the single-source distances in G to the boundary nodes is done in O(|G|α(|G|)), as we explain in Section 4.
The extension to all nodes of G is again done in O(|G i | log |G i |) for each subgraph.
Distances from the given source are computed in an additional O(|G| log |G|) time.
Thus the total running time of one invocation is O(|G| log |G|).
Therefore the running time of the entire algorithm is given byT (|G|) = T (|G 0 |) + T (|G 1 |) + O(|G| log |G|) = O(|G| log 2 |G|).
Here we used the properties of the separator, namely that |G i | ≤ 2|G|/3 for i = 0, 1, and that |G 0 | +|G 1 | = |G| + O( 񮽙 |G|).
We omit the formal proof of this recurrence.
Thus, the total running time of our algorithm is O(n log 2 n).
We turn to the space bound.
The space required for one invocation is O(|G|).
Each of the two recursive calls can use the same memory locations one after the other, so the space is given byS(|G|) = max{S(|G 0 |), S(|G 1 |} + O(|G|) = O(|G|) because max{|G 0 |, |G 1 |} ≤ 2|G|/3We have proved Theorem 1.1.
Paths of type Q 2 (dashed black) do not cross P .
Two LL paths (i.e., leaving and entering P from the left) are shown.
For i < i 񮽙 < j < j 񮽙 , the ij path and the i 񮽙 j 񮽙 path must cross at some node z.
In this section we show how to modify the algorithm of Emek et al. [4] to obtain an O(n log 2 n) running time for the replacement-paths problem.
This is another example of using a Monge property for finding row minima of a matrix.
Similarly to Section 4, we deal with a matrix whose upper triangle satisfies a Monge property.
However, the minima search problem is restricted to rectangular portions of that upper triangle.
Hence, each such rectangular portion is entirely Monge (rather than falling staircase) so the SMAWK algorithm of Aggarwal et al. [2] can be used (rather than Klawe and Kleitman's algorithm [13]).
The O(n log 3 n) time-complexity of Emek et al. arises from the recursive calls to the District procedure.
Given some 1 ≤ a < b ≤ n, District(a, b) computes the row and column minima of the rectangular portion of a matrix 񮽙 len d,d 񮽙 defined by rows a to (a+ b)/2 and columns (a + b)/2 to b. Initially, District is called with a = 1 and b = the length of the shortest path from s to t. Then, District(a, (a + b)/2) and District((a + b)/2, b) are called recursively.
Emek et al. show how to use a Monge property (which they refer to as the noncrossing property) to compute District(a, b) using a divide and conquer technique in O((b−a) log(b−a) log n) time, leading to a total of O(n log 3 n) time for all calls to District.
Our contribution is in showing that instead of divide-and-conquer one can use SMAWK to compute District(a, b) in O((b − a) log n) time, which leads to a total of O(n log 2 n) time.
We next describe in more de- Paths of type Q 2 (dashed black) do not cross P .
Two LR paths (i.e., leaving P from the left and entering P from the right) are shown.
For i < i 񮽙 < j < j 񮽙 , the ij 񮽙 path and the i 񮽙 j path must cross at some node z. tail the matrix 񮽙 len d,d 񮽙 and the procedure District(a, b).
Let P = (u 0 , u 1 , . . . , u p+1 ) be the shortest path from s = u 0 to t = u p+1 in the graph G. Consider the replacement s-to-t path Q that avoids the arc e in P .
Q can be decomposed as Q 1 Q 2 Q 3 where Q 1 is a prefix of P , Q 3 is a suffix of P , and Q 2 is a subpath from some u i to some u j that avoids any other vertex in P .
The first arc of Q 2 can be left or right of P and the last arc of Q 2 can be left or right of P (see [4] for a common formal definition of the left-of and right-of relations).
In all four cases Q 2 never crosses P (see Fig. 7 and Fig. 8).
For nodes x, y, the x-to-y distance is denoted by δ G (x, y).
The distances δ G (s, u i ) and δ G (u i , t) for i = 0, . . . , p + 1 are computed from P in O(p) time, and stored in a Proof.
Note that adding δ G (s, u i ) to all of the elements in the i th row or δ G (u j , t) to all elements in the j th column preserves Monge properties.
Therefore, it suffices to show that the upper triangle of PAD-query G,d,d 񮽙 satisfies a Monge property.When d = d 񮽙 , the proof is essentially the same as that of Lemma 4.3 because the Q 2 paths have the same crossing property as the paths in Lemma 4.3.
This is illustrated in Fig. 7.
We thus establish that the convex Monge property holds.When d 񮽙 = d 񮽙 , Lemma 4.3 applies but with the convex Monge property replaced with the concave Monge property.
To see this, consider the crossing paths in Fig. 8.
In contrast to Fig. 7, this time the crossing paths are i-to-j 񮽙 and i 񮽙 -to-j.
a) log n) time.
In the case of the concave Monge property, we cannot directly apply SMAWK.
By negating all the elements we get a convex Monge matrix but we are now looking for its row and column maxima.
As discussed in Section 2.3, SMAWK can be used to find row and column maxima of a convex Monge matrix.
