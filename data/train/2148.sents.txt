We introduce Scaph, a GPU-accelerated graph system that achieves scale-up graph processing on large-scale graphs that are initially partitioned into subgraphs at the host to enable iterative graph computations on the subgraphs on the GPU.
For active subgraphs to be processed on GPU at an iteration, the prior work always streams each in its entirety to GPU, even though only the neighboring information for its active ver-tices will ever be used.
In contrast, Scaph boosts performance significantly by reducing the amount of such redundant data transferred, thereby improving the effective utilization of the host-GPU bandwidth drastically.
The key novelty of Scaph is to classify adaptively at each iteration whether a subgraph is a high-value subgraph (if it is likely to be traversed extensively in the current and future iterations) or a low-value subgraph (otherwise).
Scaph then schedules a sub-graph for graph processing on GPU using two graph processing engines, one for high-value subgraphs, which will be streamed to GPU entirely and iterated over repeatedly, one for low-value subgraphs, for which only the neighboring information needed for its active vertices is transferred.
Evaluation on real-world and synthesized large-scale graphs shows that Scaph outperforms the state-of-the-art, Totem (4.12×), Graphie (8.93×), and Garaph (3.71×), on average.
Graph processing is used in a variety of real-world applications, including path navigation [23], social network analysis [9], and financial fraud detection [27].
Graph processing, typically memory-bound, often benefits substantially from memory optimizations [50].
Compared to CPU-based graph systems [15,16,30,36,40,46,51,68], GPU-accelerated graph systems can have high internal bandwidth and massive parallelism, therefore offering superior speedup [19,25,39,66], even for graph algorithms that involve substantial light-weight integer and comparison-based operations [28].
Unfortunately, many real-world graphs still cannot fit into GPU memory to enjoy high-performance in-memory graph processing.
For example, NVIDIA's high-end Tesla V100 has 32GB global memory [42], while real-world graphs such as Facebook's can easily reach the terabyte-scale [9].
This gap has spurred the development of many distributed graph systems, which partition a graph into sub-graphs and then assign these sub-graphs to different machines for distributed computing [13,15,16,33,36,67].
However, these distributed graph systems suffer from prohibitive communication overheads [8,15,58] and also require an extensive range of domain knowledge to maintain [11,16,24,38,56,59].
There is nowadays a viable alternative of turning a single machine plugged in with a GPU to support scale-up largescale graph processing.
Such a GPU-accelerated heterogeneous platform is easy to use and maintain [30,62,68].
In addition, we can take advantage of the large host memory (at the terabyte scale) to store large-scale graphs while still enjoying high-performance graph processing on GPU.In this paper, we focus on building graph systems on GPU-accelerated heterogeneous platforms to achieve scaleup graph processing for large graphs that cannot fit into GPU memory.
This would enable high-performance graph analytics on large-scale graphs everywhere by simply plugging a GPU into an off-the-shelf commodity PC.
In this case, a large graph must be partitioned into subgraphs at the host.
Any subgraphs to be processed on GPU must be streamed asynchronously to GPU when some previously transferred subgraphs are being concurrently processed on GPU (in an overlapping manner).
We consider vertex-centric graph processing [36], where a graph algorithm is performed in a sequence of iterations until convergence [15,36].
In each iteration, a graph algorithm processes only the active vertices (vertices with ongoing updates) in each subgraph, updates their neighbors (along their outgoing edges) and activates the neighbors whose values have been updated.
In this paper, we restrict ourselves to handle large-scale graphs that can entirely fit into the host memory.
Meanwhile, all the vertex data, including vertex states (active or not), are assumed to be resident in the GPU memory.
In contrast, the edge data of a graph are stored at the host and partitioned into subgraphs.
During graph processing, active subgraphs (containing all out-going edges of an active vertex) must be transferred to GPU for iterative processing.Achieving scale-up graph processing for large-scale graphs on GPU-accelerated heterogeneous platforms is challenging.
The power-law graphs [15] can result in substantial load imbalance among threads and warps [39].
Irregular data accesses made in graph algorithms often lead to non-coalesced memory accesses for GPU graph processing.
Fortunately, effective techniques for addressing these performance-limiting issues exist [14,17,34].
Currently, the performance bottleneck in a GPU-accelerated graph system has shifted to the limited host-GPU bandwidth, which was relatively sufficient in the past (e.g., ∼11.4GB/s for PCI-Express 3.0).
However, existing graph processing engines [17,26,34,47] focus still on overcoming the GPU memory capacity limitation to enable large-scale graph processing, without paying adequate attention to the effective utilization of the host-GPU bandwidth.Simple heuristics are used to reduce the number of data transfers.
Totem [14] partitions a graph into two subgraphs, one for the host and one for GPU, by keeping the amount of data transfers to a minimum at the expense of severe load imbalance.
Garaph [34] concurrently processes all active subgraphs on both the host and GPU.
Graphie [17] processes all subgraphs on GPU but re-processes only the recently processed subgraphs in the next iteration (before they are removed from GPU memory).
However, these graph systems always transfer an active subgraph in its entirety to GPU (even though only the neighboring information for its active vertices will usually be used), resulting in poor utilization of the host-GPU bandwidth.
To see this, Figure 1 compares the performance results of Graphie [17] for running three graph algorithms on a large graph on a PC with three generations of GPUs (one at a time).
We see little performance gains when increasingly more powerful GPUs are used.
For example, P100 has over 3× as many #SMX's and 4× as much memory as GTX980, but it offers small performance improvements.Recently, hardware vendors have launched several advanced interconnect technologies to mitigate the impact of the "bandwidth wall".
For example, compared to PCI-E 3.0, NVLINK 2.0 (50GB/s per link) and PCI-E 4.0 (32GB/s) are several times faster, but still cannot keep up with the growth in GPU computing capabilities.
Specifically, these advanced technologies cannot yet provide ∼500GB/s required by graph analytics under existing computing platforms [1].
In this work, we argue that we can improve the performance of large-scale graph processing on a GPU-accelerated architecture significantly by improving the effective utilization of the host-GPU bandwidth.
Our key observation is that the majority of the data in an active subgraph (once streamed to GPU) are never used in current and future iterations ( §2.2).
We introduce Scaph that achieves significantly improved performance than state of the art by adopting value-driven differential scheduling for active subgraphs.
The key novelty is to classify an active subgraph adaptively into a high-value subgraph (if it will be extensively traversed in current and future iterations) and a low-value subgraph (otherwise).
Thus, a high-value subgraph contains a significant amount of useful data (UD) to be used by active vertices in the current iteration and of potentially useful data (PUD) to be used by its future active vertices in future iterations.
On the other hand, a low-value subgraph contains a lot of never-used data (NUD) in current and future iterations.Unlike earlier graph systems [17,26,34,47], which transfer an active subgraph to GPU in its entirety (but with only its UD used usually), Scaph uses the host to stream an active sub-graph to GPU by using two graph processing engines for handling high-value and low-value subgraphs, respectively.
For the high-value subgraph, it will be transferred to GPU entirely.
Inspired by the data movement reduction in out-ofcore settings [2,53,69], we propose to compute each highvalue subgraph multiple times to exploit its PUD ahead of schedule for accelerating convergence.
Unlike these earlier efforts focusing on exploiting only the PUD in a subgraph, we present a GPU-context-friendly delayed scheduling to enable exploiting the PUD across subgraphs on GPU such that the value of the high-value subgraphs can be maximized.
For the low-value subgraph, only the neighboring information for its active vertices is transferred and scheduled once.In summary, this paper makes the following contributions:• Subgraph Value Characterization.
We quantify the value of a subgraph adaptively (dynamically) in terms of its UD and PUD used in current and future iterations.
• Value-Driven Differential Scheduling.
We propose a scheduler that adaptively distinguishes high-and low-value subgraphs in each iteration and dispatches a subgraph to an appropriate graph processing engine for acceleration.
• Value-Driven Graph Processing Engines.
We introduce two graph processing engines to squeeze the most value out of high-and low-value subgraphs to maximize the effective utilization of the host-GPU bandwidth in each case.
• Evaluation.
We evaluate Scaph on both real-world and synthesized large graphs.
Scaph outperforms state-of-the-art heterogeneous graph systems, Totem (4.12×) [14], Graphie (8.93×) [17], and Garaph (3.71×) [34], on average.
Scaph.
§4 describes value-driven differential scheduling while §5 discusses how to accomplish this effectively.
§6 presents results.
§7 discusses the related work.
Finally, §8 concludes.
We first review the background.
We then present some case studies to reveal why the poor host-GPU bandwidth utilization has limited the performance achieved by existing heterogeneous graph systems, finally motivating Scaph.
Figure 2 shows a representative GPU-accelerated heterogeneous architecture that integrates the hardware advantages of the host (with a larger host memory) and the GPU (with a stronger computing ability).
A GPU consists of multiple streaming multiprocessors (SMXs), each of which includes hundreds of cores.
Compared to the high-speed internal bandwidth (e.g., ∼720GB/s for NVIDIA Tesla P100 [41]) of GPU cores accessing global memory, a GPU is generally connected to the host with a relatively slow interface.
For example, the host-GPU bandwidth via PCI Express 3.0 can be limited to be as low as ∼11.4GB/s in practice [5].
This significant performance gap often severely limits the performance potential achieved on a GPU-accelerated heterogeneous architecture if the host-GPU data transfers are frequent [17,26].
This work makes use of a PCI Express interconnect since it is commonly used in the current commodity market.
Existing heterogeneous graph systems [17,26,47], with Graphie [17] as a representative compared against in our evaluation, generally use host memory to store large-scale graphs (partitioned into subgraphs) and rely on GPUs exclusively to accelerate graph analytics on these subgraphs.
Figure 3 depicts their generic graph processing engine used, with the function calls in blue executed on GPU.
Due to the limited GPU memory, a graph G is first divided into subgraphs, ˜ G 1 , · · · , ˜ G n (line 2).
During the entire iterative graph processing, the vertex data of G always reside in GPU memory, but the edge data of G, which are spread across these subgraphs, will be streamed to GPU on-demand [17,26,34,47].
At each iteration (lines 5 -12), ˜ G active represents the set of active subgraphs, i.e., the ones containing some out-going edges of an active vertex.
In each iteration, all active vertices Activate(e.destination_vertex) Figure 3: Existing graph processing engine on a GPUaccelerated heterogeneous architecture (with the function calls in blue executed on GPU and all the rest on the host) Afterward, these active vertices will be processed on the GPU (lines 13 -18) to activate more destination vertices possibly.
Note that Graphie [17] may schedule first the subgraphs processed at the end of the previous iteration as they are still in GPU memory (line 8).
This simple graph processing engine does not effectively utilize the limited, scarce host-GPU bandwidth since many vertices in an active subgraph are not active.
Simply transferring an entire subgraph to GPU (line 10) but consuming only a fraction of its data (lines 14 -15) will waste a considerable amount of the host-GPU bandwidth.
As a result, all the required data cannot arrive at the GPU promptly, limiting the performance that can be potentially achieved on GPU.1 Procedure SimpleSubgraphEngine(Graph G) 2 Load˜GLoad˜ Load˜G's subgraphs in { ˜ G 1 , · · · , ˜ G n } intoLet us examine the ratios of the unused over used data in the subgraphs transferred to GPU for three graph algorithms operating on two graphs, twitter (TW) [29] and uk-2007 (UK) [6], by Graphie [17] using the graph processing engine given in Figure 3.
Table 1 gives the results obtained through an offline trace analysis, showing that these ratios range from 6.29 to 36.17.
This indicates that the host-GPU bandwidth under Graphie is utilized rather ineffectively.
Consequently, as shown further in Figure 4, the performance of Graphie for Figure 5: UD, PUD, and NUD in a subgraph, which may change across the iterations, illustrated for SSSP.
The weight of an edge denotes its distance.
The shortest distance found so far by SSSP at a vertex is depicted next to it in orange.
[29] each graph algorithm (operating on TW) has plateaued as soon as #SMXs = 4.
However, mainstream GPU accelerators usually have far more than 4 SMXs.
For example, NVIDIA's Tesla K80 has 26 SMXs, while P100 has been integrated with 56 SMXs.
Thus, a significant gap remains between the poor provision of data and high-speed computation of GPU.
For a subgraph, its active vertices vary across the iterations.
However, from the perspective of an active vertex, it always contains three types of edge data, as illustrated in Figure 5:• Useful Data (UD).
These are the edge data associated with all the active vertices in a subgraph, i.e.,V 1 2 − → V 3 , V 1 3 − → V 4 ,and Figure 5.
UD will definitely be used in the current iteration (lines 15 -16 in Figure 3) and must be transferred to GPU [17,26,47].
V 2 1 − → V 4 in• Potentially Useful Data (PUD).
These are the edge data associated with all the future active vertices in future iterations in a subgraph.
In Figure 5, PUD will be just V 4 4 − → V 5 , since V 4 will be the only one activated by both V 1 and V 2 in current and future iterations.
Unlike UD, PUD is not actually used in the current iteration, but may be transferred repeatedly to GPU if not handled carefully (as in the case of Figure 3 where PUD is usually discarded).
• Never Used Data (NUD).
These are the edge data that will never be used again in a subgraph, associated with its vertices that have converged and will thus never be active.
In Note that the same vertex may be activated many times in different iterations.
Given a subgraph, its UD, PUD, and NUD computed at different iterations can vary dynamically.
Figure 6 shows the amount of UD, PUD, and NUD for the [29], partitioned sequentially into subgraphs of 32MB each.
Graphie [17], a representative of existing heterogeneous graph systems [17,26,47], wastes the host-GPU bandwidth in two ways ( Figure 3).
First, PUD, usually discarded by Graphie but needed in future iterations, is substantial in earlier iterations.
Second, NUD, which is becoming increasingly more dominant as the iteration progresses, is streamed to GPU redundantly.
For a subgraph, it will be cost-ineffective to stream just its UD, since its PUD cannot be exploited simultaneously.
Instead, our key insight for improving the effective utilization of the host-GPU bandwidth is to look beyond the current iteration, by considering not only its UD in the current iteration but also its PUD in future iterations.
Based on a cost-benefit analysis, we aim to leverage rather than discard its PUD (once streamed to GPU) in iterative graph processing.
Thus, the value of a subgraph at an iteration should be measured in terms of not only its UD but also its PUD.
Now, how do we extract the UD and PUD from a subgraph at a given iteration so that both can be transferred to GPU?
Extracting the UD from a subgraph is easy as its active vertices in the current iteration are known (lines 4 and 12 in Figure 3).
However, extracting precisely the PUD (without NUD) from a subgraph is difficult, as its future active vertices are not known yet during the current iteration.For a given subgraph, we propose to predict its PUD size at an iteration from the UD sizes in the current and past iterations.
This enables to adopt a value-driven differential scheduler that computes the value of a subgraph adaptively and schedules it depending on if it has a high value (when its UD and PUD are dominant) or a low value (otherwise).
Figure 7 shows the workflow of Scaph, in which all the subgraphs of a graph are computed on the GPU while the host is responsible for their preparation.
At each iteration, its dispatcher classifies a subgraph into either a high-value or lowvalue subgraph and sends it to its corresponding engine to facilitate value-driven differential scheduling.
Both engines schedule their subgraphs for acceleration on GPU independently but concurrently.
Value-Driven Subgraph Dispatcher.
Conceptually, the value of a subgraph at a given iteration is proportional to the amount of its UD and PUD.
The key insight here is that, for a given subgraph, although accurately computing its PUD is difficult, its PUD size can be approximated based on the UD sizes in the current and past iterations.
For a subgraph at a given iteration, Scaph's subgraph dispatcher ( §4), classifies it adaptively as a high-value subgraph if it contains a sufficient amount of UD and PUD to justify its transfer in its entirety to GPU and a low-value subgraph to request only its UD to be transferred to GPU otherwise.
This is done adaptively as the value of a subgraph changes as the iteration progresses.Value-Driven Subgraph Scheduler.
Scaph has two separate graph processing engines, described in §5, to process differentially high-and low-value subgraphs.
For a high-value subgraph, we use a queue-assisted multi-round processing engine, which streams it entirely from the host to GPU (if it is not in GPU memory) and exploits both its UD and PUD adequately to enable faster convergence.
For a low-value subgraph, Scaph relies on the graph processing engine given in Figure 3 but transfers only its UD to GPU, with the UD extracted in a NUMA-aware manner on the host.Scaph is essentially a hybrid graph system that allows outof-order computation of high-value subgraphs in each synchronous iteration.
The use of asynchronous execution allows fast convergence but also changes the vertex scheduling priority of subgraphs.
Therefore, a graph algorithm can use Scaph safely for preserving the convergence and the converged values, if it satisfies the correctness condition that the final vertex results are insensitive to the value propagation order.
In Section 4.1, we quantify the value of a subgraph.
In Section 4.2, we discuss how to estimate the value of a subgraph to support value-driven differential scheduling.
Graph computations proceed iteratively until convergence.
Conceptually, the value of a subgraph˜Gsubgraph˜ subgraph˜G can be measured in terms of its UD used in the current iteration and its PUD used in future iterations.
Therefore, the value of˜Gof˜ of˜G, denoted Val( ˜ G), from the current iteration Cur to the MAX-th iteration (beyond which˜Gwhich˜ which˜G is no longer active), is defined as:Val( ˜ G) = MAX ∑ i=Cur ∑ v∈˜Gv∈˜ v∈˜G.SetOfVertices D(v) * A i (v)(1)where D(v) represents the number of out-going edges of vertex v restricted tõ G and A i (v) ∈ {0, 1} indicates that v is active (inactive) in the i-th iteration when A i (v) = 1 (A i (v) = 0).
Val( ˜ G) represents the amount of computations arising from˜G from˜ from˜G from the current iteration until convergence.
According to Equation (1), the PUD of a subgraph is quantized by the number of its edges that will be used in future iterations.The value of a subgraph depends upon its active vertices and their degrees.
In the case of uniform degree distributions, the activation status of vertices can still differentiate the amount of UD, PUD, and NUD for a subgraph.
Scaph emphasizes value-driven data transfers, which should directly reflect how the bandwidth is effectively utilized in order to enable faster convergence.The intuition behind Val( ˜ G) is clear.
If Val( ˜ G) is high, ˜ G should be a high-value subgraph.
Then we should transfer˜G transfer˜ transfer˜G as a whole to GPU and also exploit its UD and PUD adequately by iterating over˜Gover˜ over˜G multiple times before it is removed from GPU memory.
Otherwise, ˜ G should be treated as a lowvalue subgraph.
In this case, we will opt to transfer only its UD to GPU and just iterate over the resulting˜Gresulting˜ resulting˜G once.If˜GIf˜ If˜G is a high-value subgraph, then the throughput of processing˜Gcessing˜ cessing˜G on GPU can be measured as follows:T HV ( ˜ G) = |UD| + λ|PUD| | ˜ G|/BW + t barrier(2)The denominator | ˜ G|/BW + t barrier , which represents the data transfer time for˜Gfor˜ for˜G, is used to approximate the time elapsed on processing˜Gprocessing˜ processing˜G by assuming a complete overlap between data transfers and computations on GPU.
As˜GAs˜ As˜G is transferred in its entirety to GPU, | ˜ G| denotes the amount of data thus transferred, BW represents the host-GPU bandwidth, and T barrier is the synchronization overhead for˜Gfor˜ for˜G (amortized by the number of active subgraphs processed).
The numerator |UD| + λ * |PUD| represents the amount of UD and PUD accessed wheñ G is iterated over on GPU.
We use a balancing factor λ to decay |PUD|, where 0 λ 1, to signify the actual amount of PUD accessed.If˜GIf˜ If˜G is a low-value subgraph, then we have:T LV ( ˜ G) = |UD| |UD|/BW + t barrier(3)This time, only the UD of˜Gof˜ of˜G is streamed to GPU.
Now, ˜ G is a high-value subgraph if T HV ( ˜ G) T LV ( ˜ G) and a low-value subgraph otherwise.
Thus, we need to analyze:|UD|+λ|PUD|(1 + t barrier |UD|/BW ) > | ˜ G|(4)To verify T HV ( ˜ G) T LV ( ˜ G), the key lies in determining |PUD|, which is difficult to obtain directly.
In fact, for a subgraph, its PUD is technically activated from its UD, motivating us to estimate the PUD of a subgraph heuristically based on the UD of the same subgraph.
In this work, we consider a subgraph to have a high value if either of the following two conditions (which we found to work well across all of our applications, as confirmed in §6) holds to simplify Equation (4):1 Procedure VDDSEngine(Graph G) 2 Distribute G's subgraphs { ˜ G 1 ,· · · , ˜ G n } to NUMA nodes 3 VertexInitialization(G) 4 ˜ G active ← FindActiveSubgraph(G) 5Transfer VertexStates from GPU to CPU When α is relatively large, which implies that the UD in a subgraph tends to be dominant, we can determine if it is a high-value subgraph by considering only its UD.
β is needed to identify the high-value subgraphs where the amount of UD is relatively low and that of PUD is potentially high.
Thus, β is often smaller than α.
As shown in Table 1, considering both together is often more effective than considering either alone.
In this work, α and β are set empirically as 50% and 30% to represent a nice point for yielding good results.
Figure 8 gives our value-driven differential scheduler, VDDSEngine(), for scheduling a graph G. Initially, G is partitioned into subgraphs, ˜ G 1 , · · · , ˜ G n , at the host and distributed across its NUMA nodes (to facilitate their scheduling).
Scaph uses two graph processing engines, as described in §5 below, HVSPEngine() for scheduling high-value subgraphs, and LVSPEngine() for scheduling low-value subgraphs.
In line 8, Scaph uses the above heuristic predictor to estimate the value of an active subgraph.
Note that both engines work independently but concurrently.
LVSPEngine() needs VertexStates in order to perform UD extraction for the active vertices in each subgraph.
The UD extraction can be overlapped effectively with the data transfers in HVSPEngine().
At the end of each iteration (line 15), Scaph will transfer back the updated vertices from the GPU to the CPU.
Edges, which are not modified, are thus not transferred.
Scaph has two graph processing engines.
We describe the one for handling high-value subgraphs in §5.1 and the one for handling low-value subgraphs in §5.2.
The PUD iñiñ G 1 can be exploited only if˜Gif˜ if˜G 2 and/or˜Gor˜ or˜G 3 are processed first.
The key to extracting the most value out of high-value subgraphs lies in how to fully exploit their PUD.
A useful idea of running each loaded subgraph multiple times is leveraged in the out-of-core settings [2,53,69] to exploit the intrinsic value in a subgraph for reducing the number of I/Os between memory and disk.
However, under a GPU-accelerated heterogeneous architecture, subgraphs must often be small enough (in several tens of millions of bytes [17,26]) against the ones in out-of-core settings, to enable fine-grained GPU scheduling.
In this case, simply iterating over such a small-sized subgraph multiple times is often ineffective, since it can exploit only the PUD of its active vertices activated by its other active vertices but not active vertices from other subgraphs.
In Scaph, we improve the PUD exploitation significantly by enabling exploiting the external value across the subgraphs.Our key observation is that: given a subgraph already available in GPU memory, scheduling it again after a period of delay can expose its PUD more fully than processing it repeatedly.
Figure 9 illustrates this with three subgraphs, exhibiting some complex inter-subgraph data dependencies (as is often the case in practice).
We see that  iñ G 1 can be activated by iñ G 2 and  iñ G 3 .
Once  iñ G 1 is activated,  iñ G 1 may get activated (as shown).
In this case, the edge data for →, →, and → are part of the PUD of˜Gof˜ of˜G 1 .
By processing˜Ging˜ ing˜G 1 after˜Gafter˜ after˜G 2 or˜Gor˜ or˜G 3 or both (even better), we can exploit such PUD to enable faster convergence.
That is, repeatedly processing˜Gprocessing˜ processing˜G 1 would not help.Queue-Assisted Multi-Round Processing.
The scheduling of high-value subgraphs at a given iteration is shown in Figure 10.
We use a k-level priority queue (PQ 1 , . . . , PQ k ) to enable re-scheduling a GPU-resident subgraph after some delay, where k indicates the maximum number of times some subgraphs have been processed in the current iteration.
Thus, k varies from iteration to iteration.
Figure 11 shows a case.In each differential scheduling iteration orchestrated by VDDSEngine (Figure 8), HVSPEngine(worklist) is invoked, where worklist contains all the high-value subgraphs in this iteration.
During the pre-processing (lines 2-6), each subgraph˜G subgraph˜ subgraph˜G i in worklist is examined in turn.
˜ G i will be enqueued into PQ 1 (if not already there) if˜Gif˜ if˜G i remains to be GPU-resident (i.e., in one of {PQ 1 , . . . , PQ k }) from the previous iteration and inserted into TransSet (waiting to be streamed to GPU) otherwise.
Thus, there are two concurrently executed modules, Subgraph Transferring and Subgraph Scheduling.
The Subgraph Transferring module (lines 7 -16) is responsible for streaming asynchronously the subgraphs in TransSet to GPU.
This is done by using some free GPU memory whenever possible (line 11) or making some free by dequeuing a subgraph from the multi-level queue (lines 13 -14).
Due to lines 4 and 16, all subgraphs in worklist are initially enqueued into PQ 1 , and thus assigned with the highest priority.The Subgraph Scheduling module (lines 17 -31) is responsible for scheduling the subgraphs in PQ 1 , · · · , PQ k .
The subgraphs in PQ 1 are processed first (for the first time in the current iteration) to exploit their UD (lines 21 -23).
If PQ 1 = / 0 (implying that some subgraphs are still being transferred to GPU asynchronously), the scheduler will dequeue a subgraph from a non-empty PQ i , where i is the smallest, to exploit its PUD (lines 25 -29), as this will be the i-th time that the subgraph is processed (in the i-th round) of the current iteration.
Simultaneously, the data transfers for PQ 1 and the computations for PQ 2 , · · · , PQ k are maximally overlapped, too.
In either case, the priority of a subgraph, once processed, drops by one (lines 30 -31).
This delayed re-scheduling at- Scheduling Priority ...PQk PQ2 PQ1 G9 G7 G4 G3 G8 G6 G5 Figure 11: Subgraph processing with a k-level priority queue.
PQ i represents a queue PQ with the i-th priority.
The smaller i is, the higher the priority is.
All the subgraphs streamed from the host to GPU enter into PQ 1 initially.tempts to maximize the PUD exploitation, by, e.g., increasing the chances for˜Gfor˜ for˜G 1 to be processed after˜Gafter˜ after˜G 2 and/or˜Gor˜ or˜G 3 in Fig- ure 9 (as motivated earlier).
Consider˜GConsider˜ Consider˜G 3 , which resides in PQ 1 , in Figure 11.
Once we have exploited its UD, we will move it to PQ 2 so that we can exploit its PUD after˜Gafter˜ after˜G 7 , ˜ G 4 , ˜ G 8 , and˜Gand˜ and˜G 6 have been processed.Our scheduler with a multi-level priority queue guarantees that subgraphs are scheduled fairly, preventing them from bearing too many useless computations in the sense that the data of a vertex is computed but not updated.Time and Space Complexity Analysis.
k is expected to be bounded by BW BW where BW is the internal bandwidth of GPU and BW is the host-GPU bandwidth.
In our computing platform, BW = 224GB/s and BW = 11.4GB/s.
Thus, k 20 is typically expected.As for the space complexity, a k-level priority queue is used to keep track of only the indices of the active subgraphs processed in an iteration.
Thus, the worst complexity is O( Mem GPU × sizeof(SubgraphIndex) ), where Mem GPU is the global memory size and | ˜ G| is the size of a subgraph˜Gsubgraph˜ subgraph˜G.
In our computing platform, we have used 4GB×4B 32MB = 0.5KB.
The key to exploiting the most value of low-value subgraphs is to extract their UD efficiently.
We use multiple CPU cores at the host to parallelize the UD extraction.
Due to non-uniform memory access (NUMA), however, scanning naively all the vertices in a subgraph to extract its UD can still be costly.
In addition, different subgraphs exhibit different amounts of UD.
Such scanning tasks are also prone to load imbalance.
Figure 12 gives our scheduler for low-value subgraphs.
In each value-driven scheduling iteration orchestrated by VDDSEngine() in Figure 8, LVSPEngine(worklist, VertexStates) is invoked, where worklist contains all the low-value subgraphs that are active in this iteration, with their active vertices indicated in VertexStates.
There are three modules, UD Extraction, Subgraph Transferring, and Subgraph Scheduling, which all execute concurrently.
The major contribution here is a NUMA-aware parallel UD extraction.UD Extraction.
Initially, all the subgraphs partitioned from a graph are evenly distributed to different NUMA nodes, with a NUMA node consisting of a CPU socket and its own memory banks (line 2 in Figure 8).
The UD extraction module is given in terms of lines 2 -4 and lines 17 -30.
To boost performance and improve intra-node load balancing, the UD extraction for each subgraph is done in its own thread, which is bound to the NUMA node storing the subgraph (line 3).
To improve inter-node load balancing (as a minor optimization), we also duplicate in a NUMA node an equal number of randomly selected subgraphs from the other nodes (if there is still some memory space available).
We adopt a simple bitmap-based approach to extract the UD from a subgraph˜Gsubgraph˜ subgraph˜G efficiently (lines 17-30).
All its vertices are stored in a bitmap, VertexStates( ˜ G).
bitmap, with 1 (0) indicating that the corresponding vertex iñ G is active (inactive).
To accelerate its construction, the total of active vertices is computed on GPU.Unlike high-value subgraphs, which can each be stored in the same-sized chunk in GPU memory ( §5.1), low-value subgraphs may give rise to UD-induced subgraphs of varying sizes.
To reduce fragmentation, Scaph further divides each chunk for storing a subgraph into smaller tiles (totaling 32 in our implementation).
To store a UD-induced subgraph in GPU memory, Scaph will try to find consecutive tiles first in a partially filled chunk and then in a vacant chunk.Subgraph Transferring.
As in the case of high-value subgraph streaming in Figure 10, this module proceeds similarly except that a multi-level queue is no longer used.Subgraph Scheduling.
As in the case of scheduling highvalue subgraphs to GPU in Figure 10, this module schedules UD-induced subgraphs (without using a multi-level queue).
We evaluate the efficiency and scalability of Scaph by answering the following four research questions (RQs):• RQ1: How much more efficient is Scaph over state-of-theart heterogeneous graph systems?
• RQ2: How effective is Scaph's value-driven differential scheduling in helping it achieve the overall performance?
• RQ3: How well does Scaph scale?
• RQ4: How much runtime overhead does Scaph introduce?
We compare Scaph with the following three state-of-the-art CPU-GPU heterogeneous graph systems:• Totem [14].
A graph is divided into two subgraphs, which are processed by CPU and GPU, respectively.
At the end of each iteration, the states of the active vertices that are activated reciprocally by the two subgraphs are exchanged.
• Graphie [17].
Like Scaph, a graph is initially partitioned at the host CPU and the subgraphs are then streamed to GPU for graph processing.
Unlike Scaph, however, all active subgraphs are transferred to GPU in their entirety.
• Garaph [34].
At an iteration, all the subgraphs that are partitioned from a graph are processed concurrently by both the host and GPU if the number of outgoing edges of all active vertices in the entire graph exceeds 50% of the total number of edges and on the host only otherwise.Subgraph Size.
For Totem, Graphie, and Garaph, the sizes of subgraphs are selected from their papers.
In Scaph, a graph is partitioned into subgraphs of 32MB each for several reasons.
First, the host-GPU bandwidth tends to be under utilized with smaller sizes.
Second, subgraphs will be streamed to GPU more frequently with larger sizes, as they tend to contain active vertices for more iterations.
Finally, the kernel launching overheads appear to be well hidden with 32MB.Graph Applications.
We consider the first three typical graph algorithms (from different categories) and the latter two actual graph workloads (with different complexities): (1) Single-Source Shortest Path (SSSP) [60]-Sequential traversal, (2) Connected Components (CC) [20]-Parallel traversal, (3) Minimum Spanning Tree (MST) [37]-Graph mutation, (4) Neural Network Digit Recognition (NNDR) [4], and (5) Graph-based Circuit Simulation (GCS) [25].
All these algorithms fit the correctness criteria discussed in §3, though NNDR and GCS are already typically executed in an asyn- chronous way, while the other algorithms are typically run in a synchronous, iterative manner.
Graph Datasets.
We use (1) 6 real-world graphs [6,31]) for performance evaluation, and (2) 5 large synthesized graphs (generated by the RMAT tool [7]) for scalability evaluation.
Table 2 gives all the graphs used.
For SSSP and MST that work on the weighted graphs, we randomly assign each edge of an unweighted graph with a weight ranging from 1 to 100.
Computing Platform.
We evaluate Scaph on a machine where the host is equipped with two Intel 14-core Xeon CPUs, E5-2680v4@2.40GHz with 512GB memory (256GB on each of the two NUMA nodes).
The GPU is NVIDIA P100 (with 56 SMXs, 3584 cores, and 16GB memory), connected to the host via the PCI Express 3.0 at 16x.
The host-GPU bandwidth is around 11.4GB/s.
We use NVCC V8.0.61 and g++ V5.4.0 to compile all the applications under "-O3".
The operating system is Ubuntu 14.04 with Linux kernel 4.13.
To answer RQ1, we compare Scaph against Totem [14], Graphie [17], and Garaph [34].
Table 3 depicts the results.Scaph vs. Totem.
The speedup of Scaph over Totem ranges from 2.23× (for SSSP on SK) to 7.64× (for CC on TW) with an average of 4.12×.
Totem's critical performance bottleneck lies in its severe load imbalance, as it partitions each graph into only two subgraphs, one for the host (with 512GB memory) and one for GPU (with only 16GB memory).
As a result, Totem cannot tap GPU's processing power to exploit adequately the UD and PUD in a graph.
Its bottleneck is to ask the CPU to process most of the graph data, which would have been processed more efficiently by the GPU otherwise.
A typical measurement for FB is for the CPU to handle 358.1GB and the GPU to handle only 16GB.
In contrast, Scaph streams all subgraphs dynamically to GPU with value-driven differential scheduling, thereby exploiting more adequately GPU's processing power, and consequently, the UD and PUD in all the subgraphs.
In the case of CC operating on FR, SK, and UK, their GPU portions under Totem are 39.6%, 36.7%, and 19.1%, respectively.
As a result, Scaph outperforms Totem by 5.51x (FR), 2.52x (SK), and 3.77x (UK).
Scaph vs. Graphie.
Scaph is faster than Graphie by 8.93× on average, with its speedup ranging from 3.03× (for NNDR on TW) to 16.41× (for SSSP on SK).
Both Graphie and Scaph process all subgraphs on GPU only.
So Graphie can be understood as a version of Scaph, where every subgraph is treated as a high-value subgraph except that only its UD is used but its PUD is exploited rather inadequately.
Graphie is inferior to Scaph for several reasons.
First, Graphie transfers an active subgraph entirely to GPU even though it contains only a few active vertices (i.e., a lot of NUD), wasting the host-GPU bandwidth.
Second, Graphie exploits the UD only but PUD inadequately in an active subgraph.Let us examine SSSP on SK, where the speedup of Scaph over Graphie is the highest (at 16.41×).
Graphie converges in 75 iterations, by transferring 18,019 subgraphs totaling 374.4GB data to GPU.
In contrast, Scaph converges in 16 iterations, by transferring 9,897 subgraphs totaling only 19.6GB data, comprising 13.2GB for 798 high-value subgraphs and 6.4GB for 9,099 low-value subgraphs.
For Scaph, its significantly improved utilization for the host-GPU bandwidth has resulted in its significantly improved overall performance.Scaph vs. Garaph.
Scaph is faster than Garaph by 3.71×, with an overall rang from 1.93× (for NNDR on TW) to 5.62× (for CC on FB).
Unlike Scaph, Garaph processes all the subgraphs on both the host and GPU if the active vertices in the entire graph have a lot of outgoing edges and on the host only otherwise ( §6.1).
Despite this, Garaph cannot distinguish highvalue from low-value subgraphs as Scaph does.
While being more effective than Graphie in reducing the amount of NUD transferred, Garaph is inferior to Scaph as it still transfers more NUD to GPU and exploits PUD less adequately.Let us examine CC on FB, where the speedup of Scaph over Garaph is the highest (at 5.62×).
Garaph processes all the subgraphs on the host only (as the outgoing edges of FB's active vertices over the total is under 6.9% at any iteration), by using a so-called notify-pull model.
In contrast, Scaph uses a fine-grained value-driven differential scheduler to identify high-value and low-value subgraphs even though it has active vertices only in its local regions at any iteration, so that the GPU's processing power is adequately exploited.T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 To answer RQ2, we consider four variations of Scaph: (1) Scaph-HVSP, where all the low-value subgraphs can be understood as being misidentified as high-value subgraphs, (2) Scaph-LVSP, where all the high-value subgraphs can be understood as being misidentified as low-value subgraphs, (3) Scaph-HBASE, which applies the differential processing, but every subgraph transferred to the GPU has kept computation with a specific number of times (without using queue-based delayed scheduling), as used in CLIP [2], and (4) Scaph-LBASE, a variation of Scaph-LVSP except that every subgraph is streamed to GPU entirely (without UD extraction), as used in Graphie [17].
Figure 13 gives the results.
We see that neither of Scaph-HVSP and Scaph-LVSP is always better than the other, and also Scaph is the best performer for all the algorithms on all the graphs.
Thus, Scaph's value-driven differential scheduling with heuristic subgraph identification is highly effective.Scaph-HVSP.
Scaph-HVSP achieves better speedups for the graphs where algorithms take longer iterations to converge, as this allows it to exploit PUD more adequately and thus stream less redundant data to GPU.
For example, each algorithm on SK has the longest number of iterations against on other graphs, thereby delivering considerable speedups.
We also see that Scaph-HBASE is significantly inferior to Scaph-HVSP.
This is because small subgraphs often contain very little PUD from themselves worthy of being exploited.
Our queue-based scheduling allows the availability of PUD from other subgraphs via delayed scheduling.
Thus, multitime processing under Scaph-HVSP can expose significantly more PUD than that under Scaph-HBASE (i.e., by simply applying the idea from CLIP) for boosting performance.Scaph-LVSP.
Just like Scaph-HVSP, Scaph-LVSP can be quite effective in some cases.
For example, the top two speedups achieved by Scaph-LVSP for MST are 5.26x and 3.58x on SK (14.8GB) and UK (27.61GB), respectively.
The corresponding speedups from Scaph are 5.99x and 4.19x. However, Scaph-LVSP can be rather ineffective for the graphs that can nearly fit into the 16GB GPU memory, since Scaph-LBASE will then make GPU-resident for nearly all the subgraphs.
For R28 with 16.78GB (unweighted) and 29.48GB (weighted), Scaph-LVSP offers little or even negative benefits for CC, NNDR, and GCS (on unweighted graphs) but positive ones for SSSP and MST (on weighted graphs).
Scaph.
Scaph obtains the best of both worlds, Scaph-HVSP and Scaph-LVSP.
For CC, SSSP, MST, NNDR, and GCS, the average speedups achieved by Scaph-HVSP (Scaph-LVSP) are To answer RQ3, we investigate Scaph's scalability in terms of #SMXs, graph sizes, memory sizes, and GPU generations.
We select Graphie as a reference on CC, MST, and NNDR.
#SMXs.
Figure 14(a) compares Scaph and Graphie in terms of CC, MST, and NNDR on UK [6] for varying #SMXs by using all the 8GB GPU memory available.
Scaph is significantly more scalable than Graphie for all the three graph algorithms, since Scaph can utilize the host-GPU bandwidth more effectively as already motivated earlier (Figures 1 and 4).
For example, Graphie-MST reaches its plateau when #SMXs = 2, but Scaph-MST continues to offer a scalable performance improvement.
CC and NNDR exhibit a similar trend.However, Scaph's scalability degrades gradually as #SMXs increases, due to the integrated impacts of the intrinsic random accesses of graph processing on GPU [5,25,57] and the increasingly more SMXs competing for the memory bandwidth.
As also shown in Figure 14(a), Groute [5], an in-memory graph system that can not handle over-subscription, on UK-2007@1M [6] (a sample graph with 1M vertices and 41M edges generated from UK), suffers from exactly the same scalability problem, which is beyond the scope of this work.
We leave addressing this problem in future work.Graph Sizes.
Figure 14(b) compares Scaph and Graphie as the graph size increases.
For CC and NNDR working on unweighted graphs, Scaph (Graphie) can store up to 4 billion (2 billion) edges in GPU memory.
For MST working on the weighted graph, these edge counts drop to roughly 2 billion and 1 billion.
Both Scaph and Graphie maintain their throughput well as the graph size increases but degrade visibly for the graphs that can no longer fit into GPU memory.
However , 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 Scaph has a slower performance reduction rate than Graphie, for two reasons.
First, Scaph can better tap GPU's processing power due to its use of a multi-level priority queue for exploiting PUD more adequately and overlapping data transfers and GPU computation more effectively.
Second, Scaph avoids transferring a large amount of NUD for low-value subgraphs.GPU Memory Capacities.
Figure 14(c) compares Scaph and Graphie for varying GPU memory sizes.
Graphie is highly sensitive to the GPU memory capacity used, which determines directly how many subgraphs can be resident on GPU at an iteration and how many of these get re-processed in the ensuing iteration (before they are removed from GPU memory).
In contrast, Scaph is nearly insensitive, since it exploits UD and PUD for high-value subgraphs and UD only for low-value subgraphs always.
Note that Scaph is significantly faster than Graphie (Table 3).
In Figure 14(c), Graphie improves over itself (normalized to 10GB) as the GPU memory size increases.GPU Generations.
Figure 14(d) characterizes the performance of Scaph on different GPU generations.
Compared to Graphie that shows few speedups as shown in Figure 1, Scaph enables the significant speedups for K40 (1.99×∼3.12×) and P100 (4.26×∼5.02×) against that of GTX980.Varying α and β.
Figure 14(e) shows the sensitivity of the performance results of Scaph with respect to α and β.
Here, A1 can be understood as Scaph-HVSP and A5 as Scaph-LVSP.
Looking at A3, we see that increasing α and β causes more subgraphs to be mis-identified as low-value subgraphs (A4 and A5) and decreasing α and β causes more subgraphs to be mis-identified as high-value subgraphs (A1 and A2).
Thus, A3 seems to represent a nice sweet spot for yielding good performance results.
As for the problem of finding an optimal setting, we leave it as future work.
We discuss Scaph's overheads incurred in its value-driven differential scheduling (VDDS) given in Figure 8, high-value subgraph processing (HVSP) given in Figure 10, and lowvalue subgraph processing (LVSP) given in Figure 12.
VDDS.
The cost of computing the subgraph value comes from computing the UD size for each iteration, on GPU, in line 8 of Figure 8.
This is negligible, as shown in Figure 15(a).
Figure 15(b), the cost incurred per iteration is small, representing an average of 0.79% of the total processing time.
This small overhead is more than offset by the benefit reaped.
In particular, the iteration count is reduced since most of the PUD can be computed ahead of schedule.
The per-iteration time can be improved mainly because most of the NUD is discarded (rather than transferred expensively).
LVSP.
The main overhead of LVSP lies in transferring a bitmap representation for all the active vertices in a subgraph from GPU to the host.
As shown in Figure 15(c), the average cost incurred per iteration represents 4.3% of the total graph processing time.
However, this cost increases relatively towards the last few iterations, reaching 57.4% at the end, where each subgraph has little UD to be acted upon.
Graph Partition.
Various partitions may show different value variations of subgraph at runtime.
Scaph adopts a greedy vertex-cut partition [15] with the time taken depending on the number of partitions.
It would be interesting future work to find a more reasonable partition method that can make most of UD and PUD exploited in the early stage of graph processing for faster convergence.Disk-based Heterogeneous Graph Systems.
The performance of Scaph is insensitive to the difference between CPU and GPU memory, given that the whole graph is assumed to fit into the CPU memory.
To support even larger graphs on a single machine, using the disk (e.g., SSD) as secondary storage is promising.
In this case, a new dimension of performance bottleneck will be the I/O inefficiency, which has been studied in prior work [2,32,35,55].
We can combine Scaph with these past disk-based solutions to cooperatively handle graphs that cannot fit into the host memory.Performance Profitability.
Scaph delivers performance benefits by processing all the subgraphs differentially.
Scaph is currently not expected to be applied to graph algorithms where the set of active vertices does not shrink as computation goes on.
For example, all vertices in PageRank are active in every iteration.
Thus, all the data of a subgraph can be regarded as UD without any PUD.
In fact, we can extend Scaph to distinguish these all-active subgraphs further for PageRank by considering not only the degrees and the activation but also the state variation rate for each vertex, which is a potential direction of future work.
Heterogeneous Graph Systems.
Such systems have been studied on a range of heterogeneous architectures equipped with varying hardware resources [21,35,44].
Compared to GPU-accelerated solutions [43,47], FPGA-accelerated alternatives are advantageous in energy-efficiency [10,61].
In developing Scaph, we focus on improving host-accelerator bandwidth utilization.
The basic idea behind can also be applied to improve the scalability of FPGA-accelerated heterogeneous graph systems with a few hardware specializations.Distributed Graph Systems.
The rationale is to aggregate multiple machines to enable processing large-scale graphs.
The main challenge lies in obtaining good graph partitions [3,8,16,48,52] so as to minimize the communication overheads across the machines.
Some recent studies take advantage of emerging high-speed networks (e.g., RDMA) to reduce communication overheads [49,58].
Aspire [54] designs a relaxed consistency model to exploit asynchronous parallelism for iterative algorithms.
Gemini [67] includes a series of adaptive runtime optimizations to enable obtaining an attractive scale-out efficiency.Disk-based Graph Systems.
Many disk-based graph systems [12,45,64,65] exist for supporting large-scale graph processing.
GraphChi [30] relies on parallel sliding windows to optimize disk accesses.
GridGraph [68] uses 2-level hierarchical partitioning to reduce the I/O overhead.
TurboGraph [18] applies a pin-and-slide model to exploit the multicore and I/O parallelism.
Due to the low disk-to-memory bandwidth, disk-based graph systems are often at least two orders-ofmagnitude slower than heterogeneous solutions.Data Movement Reduction.
Several previous studies leverage an analogous idea of running graph partitions multiple times for different purposes.
CLIP [2] iterates over each loaded subgraph multiple times to squeeze out the value of each subgraph so that less amount of disk I/O is required.
GraphQ [69] enables computing the local subgraphs multiple times in order to tolerate long latency across the compute nodes.
Unlike these efforts, Scaph emphasizes on a GPU context that often requires small-size subgraphs to enable fine-grained scheduling.
Thus, simply computing a subgraph multiple times is not sufficient to exploit its PUD fully.
Scaph enables value exploitation not only within a subgraph but also across the subgraphs via a delayed scheduling mechanism.In LUMOS [53], a subgraph in an iteration can be exploited asynchronously iff its updated values are independent of the subsequent iteration.
This dependency-aware technique allows enjoying the efficiency of asynchronous execution while ensuring synchronous processing semantics.
Applying this technique into Scaph can help identify the high-value subgraphs that contain across-iteration dependencies, so that Scaph can be extended to handle synchronous algorithms [22] safely by scheduling these high-value subgraphs once.
However, the downside is that many dependency-free low-value subgraphs may also be allowed to be computed multiple times, wasting the GPU computational and storage resources.Wonderland [63] uses graph abstraction as a bridge over on-disk subgraphs to speed up convergence.
However, under the context of small-sized subgraphs, such a graph abstraction is often hard to keep concise, and extracting it from the whole graph is also non-trivial.
PowerLayer [8] presents differentiated processing on high-degree and low-degree vertices to improve the trade-off between load balance and communication overheads in a distributed setting.
However, applying the idea of PowerLayer cannot often identify the value of a subgraph accurately while Scaph does with a fine-grained solution.
Mosaic [35] adopts a subgraph compression technique, which can be used to work together with Scaph to improve the bandwidth-efficiency of heterogeneous graph system further.
This paper tackles the challenge faced in achieving scale-up large-scale graph processing on a GPU-accelerated heterogeneous architecture.
We introduce Scaph, a value-driven heterogeneous graph system that differentially schedules the subgraphs partitioned from a graph according to their values in order to improve the effective utilization of the host-GPU bandwidth.
Scaph outperforms state of the art, as evaluated with representative graph algorithms operating on a range of graph datasets.
In addition, these performance benefits scale up as more computing resources are available.
We thank the anonymous reviewers for their insightful comments.
In particular, we thank our shepherd, Xiaosong Ma, for her valuable suggestions.
We would also like to thank Pengcheng Yao, Chuangyi Gui, Qinggang Wang, and Jieshao Zhao for their support.
This work is supported by the Na-
