Modern deep neural network (DNN) training jobs use complex and heterogeneous software/hardware stacks.
The efficacy of software-level optimizations can vary significantly when used in different deployment configurations.
It is oner-ous and error-prone for ML practitioners and system developers to implement each optimization separately, and determine which ones will improve performance in their own configurations.
Unfortunately, existing profiling tools do not aim to answer predictive questions such as "How will optimization X affect the performance of my model?"
.
We address this critical limitation, and proposes a new profiling tool, Daydream, to help programmers efficiently explore the efficacy of DNN optimizations.
Daydream models DNN execution with a fine-grained dependency graph based on low-level traces collected by CUPTI [49], and predicts runtime by simulating execution based on the dependency graph.
Daydream maps the low-level traces using DNN domain-specific knowledge and introduces a set of graph-transformation primitives that can easily model a wide variety of optimizations.
We show that Daydream is able to model most mainstream DNN optimization techniques and accurately predict the efficacy of optimizations that will result in significant performance improvements.
Recent years have witnessed the co-evolution of deep neural network (DNN) algorithms and the underlying hardware and software design.
ML researchers have developed many important models [20,26,27,73] at a rapid pace, creating a huge demand for computation power [69].
To meet the demand for fast DNN computation, computer architects respond with new, AI-optimized GPUs (e.g., NVidia Turing architecture [56]) and various domain-specific hardware accelerators from FPGAs (e.g., Microsoft Catapult [64]) to ASICs (e.g., Google TPU [34], Amazon Inferentia [70]).
However these accelerators might not be effective in improving performance without proper software optimizations across the full systems stack [84].
As a result, systems researchers have proposed many optimizations, targeting different bottlenecks across the system stack -for example, improving memory utilization [29,67], better overlapping of communication with computation [25,30,83], and increasing communication efficiency [16].
Moreover, researchers have also developed workload-centric optimizations to exploit the stochastic nature of DNN computation.
For example, precision reduction [18,23,42] aims to reduce runtime as well as memory consumption, and gradient compression [40,41] aims at reducing the communication overhead in distributed training.Despite these advances, the benefits of many proposed optimizations cannot be fully exploited due to two main reasons.
First, the efficacy of many proposed performance optimizations can drastically change when applied to different ML models and deployment configurations.
The hardware deployments that practitioners use might be completely different from the hardware configurations used by optimization and model inventors.
Differences in DNN models, accelerator type, compute capabilities, available memory, networking capabilities, and software library versions can all shift the major runtime bottlenecks.
Second, it is onerous for programmers to implement and evaluate various optimizations to identify the ones that actually work for their models.
As a result, it is common for users to ask what-if questions such as:Why did my DNN training workload run slowly?
Will optimization X improve the performance of my model?
Does GPU memory capacity limit the performance of my model?
Would upgrading to a faster network improve training throughput?
How will my workload scale with the number of GPUs?The central focus of this paper is to answer the following general question for DNN training workloads: Given a model and a deployment scenario, how can we efficiently explore the efficacy of potential solutions?
Systems researchers have tried to explore the impact of different potential performance bottlenecks (e.g., CPU, network, IO) in many non-ML contexts [5,17,43,59,60,74].
The basic approaches to explore the what-if questions are similar: decompose the workloads into atomic tasks, profile runtime statistics for each task, model the what-if question, and use simulation to estimate performance.These systems typically address what-if questions of the form: "How does runtime change if a task T is N times (or even infinitely) faster?"
[17,60].
Such questions can be simply modeled by shrinking task runtime.
While this basic approach seems sufficient to address the central question above for ML workloads, the diversity of DNN optimizations introduces three key requirements unique to these workloads, thus motivating the need for a novel solution.First, we need to track dependencies at a kernel-level abstraction i.e., one GPU kernel corresponds to one task (the smallest unit of execution in the dependency graph).
Such fine-grained abstraction is necessary because optimizations that improve hardware utilization typically target individual compute kernels (e.g., mixed precision [42]).
Meanwhile, accurate performance estimation has to consider both CPU and GPU runtime.
Certain optimizations, e.g., kernel fusion, require potentially removing existing CPU and GPU tasks from the dependency graph.
Existing tools do not provide such dependency tracking.
It is therefore important to track kernellevel dependencies among concurrently executing tasks.Second, we need to map tasks to DNN layers.
In contrast to prior works that explore what-if questions in non-ML contexts, predicting the performance of DNN optimizations requires domain knowledge about DNNs to properly model them.
For example, MetaFlow [33] and TASO [32] fuse DNN layers.
Modeling them requires a mapping from tasks to specific DNN layers.
However, collecting kernel-level traces on accelerators requires generic vendor-provided tools (e.g., NVProf [48], CUPTI [49]), which have no application specific knowledge.
We therefore need to have the ability to map low-level tasks to DNN layers.Third, we need the ability to easily model diverse DNN optimizations.
Modeling a DNN optimization might involve not just scaling or shrinking task durations, but also complicated transformations to the dependency graph.
For example, TicTac [25] reschedules communication tasks, BlueConnect [16] replaces the communication primitives to utilize parallel network channels, and the optimization proposed by Jung et al. [35] restructures the GPU kernel implementations.
Manually manipulating the kernel-level dependency graph could be extremely intricate and error-prone.
The system should enable users to flexibly and effectively model such diverse optimizations with minimal effort.We introduce Daydream, a new system that fulfills all three requirements described above, and achieves our goal of answering potential what-if questions for DNN workloads.
Constructing dependencies among potentially thousands of lowlevel tasks is not an easy problem: tasks can be spread across multiple execution threads (including both CPU threads and GPU streams), thus even for simple DNN workloads, this results in thousands of tasks to be tracked.
The intricacy comes from identifying dependencies across threads.
We make a key observation about DNN training workloads: despite the large number of tasks that need to be tracked, the number of concurrently executing threads is surprisingly quite limited.
Based on this observation, Daydream constructs the low-level dependency graph, which provides a realistic model of overlapping among CPU, GPU, and communication runtimes in a DNN training workload.
It uses a synchronization-free approach to map GPU tasks onto appropriate higher-level DNN layer abstractions.
We also introduce a set of graph-transformation rules, allowing programmers to effectively model various performance optimizations.
After modeling the optimization, Daydream simulates the execution based on the new dependency graph to predict the overall runtime.
In our evaluation, we show that Daydream is able to distinguish effective DNN optimizations from those that will bring limited improvements by accurately predicting their performance speedups.In summary, we make the following key contributions:• We make the observation that fine-grained tasks in DNN training workloads are highly sequential.
This greatly simplifies dependency graph construction, over thousands of tasks, as we only need to identify a limited number of inter-thread dependencies.
• Daydream introduces the abstraction of a kernelgranularity dependency graph that contains mappings back to DNN specific abstractions (layers), by collecting profiling data, instrumenting DNN frameworks, and exploiting information from vendor-provided tools like CUPTI.
Daydream also provides primitives to mutate the dependency graph in the form of simple graph transformations.
Taken together this enables programmers to both (i) model a diverse set of popular optimizations spanning kernel-and layer-level enhancements by using simple graph-transformation primitives, and (ii) estimate the efficacy of optimizations by simulating execution time based on optimization-induced graph mutations.
• We extensively evaluate Daydream, with five different optimizations on five DNN models across three distinct applications.
We show that Daydream can effectively detect which optimizations provide improvements and also accurately predict their magnitude for different DNN models and deployments.
For example, we estimate that using mixed precision will improve the iteration time of training BERT LARGE model by 17.2% (with <3% error), while the kernel fusion technique can improve it by 38.7% (with <7% error).
We can also accurately predict performance in distributed training with different number of workers and variable network bandwidth, based on runtime profiles collected from a single-GPU setting.
DNN training is an iterative algorithm, in which one iteration consists of three phases: (i) forward, (ii) backward, and (iii) weight update.
The forward phase takes training data samples as input and produces output based on current weights Table 1: Representative optimizations for DNN training.
We show how we can accurately estimate the performance of optimizations (shown in italics) in Section 6, and can effectively model many other optimizations (shown in bold) in Section 5.
(or parameters).
The error between the forward output and the input data labels is fed to the backward phase, which computes the gradients of weights with respect to the input data.
The weight update phase then uses the gradients to update weights accordingly.
In each iteration, the input data samples are randomly selected [11], forming a mini-batch of input.
Modern DNNs have millions of parameters [24], resulting in training times of days or even weeks [38].
To improve DNN training performance, researchers have proposed various strategies focusing on different optimization goals.
To understand the potential what-if questions and how to design a system to answer them, we study a list of software-level techniques that speedup DNN training from top systems and ML conferences in recent years.
Table 1 shows our summary.Exploiting computation power of hardware accelerators.
ML programmers often use large mini-batches, within the memory budget, for better hardware utilization and faster convergence.
This motivates strategies that reduce the memory footprint of DNN training and hence enables training with larger mini-batch sizes [14,29,67].
Researchers have also proposed some generic strategies to increase hardware utilization, including precision reduction [18,23,42], kernel/layer fusion [10,32,33], and improving low-level kernel implementation [13,35,37,72].
Meanwhile, libraries such as cuDNN [15], cuBLAS [45], MKL [75], Eigen [1], and NCCL [46] are also constantly evolving to provide operations and primitives that can better utilize underlying hardware.Scalable distributed training.
Data parallelism [11] is a simple and effective strategy to improve training performance.
Using multiple accelerators significantly reduces DNN training time to hours or even minutes [44].
This success is mainly based on the techniques that guarantee model convergence under extremely large mini-batch size [7,22,81].
One of the major performance bottlenecks for distributed training is communication, which can be optimized by compressing traffic [40,41,76,78], increasing network utilization [16,80], or increasing the overlap between communication and computation [25,30,83].
Exploring the efficacy of these optimizations without prediction requires a multi-machine cluster.
Our proposed design, Daydream, avoids the potential cost of cluster setup (i.e. extra machines, accelerators, high-speed communication), by predicting distributed training performance with profiles collected from a single-worker environment.
As the full ML system stack is constantly evolving, profiling tools play a key role in helping programmers identify the performance bottlenecks under different system configurations.Hardware profiling tools.
Modern DNN training heavily relies on hardware accelerators such as GPUs [56] and TPUs [34].
To help programmers develop highly efficient applications, hardware vendors provide profiling tools that can expose hardware performance counters.
For example, NVProf [48] provides programmers with information including start/end time, core utilization, memory throughput, cache miss rate, along with hundreds of other hardware counters for every GPU kernel.
CUPTI [49] enables programmers to extract and manipulate these counters at runtime.
Nsight [47] aims to provide details on the state of more fine-grained counters for recent GPU architectures [56].
Our proposed system, Daydream, relies on CUPTI to collect low-level traces for further analysis.Framework built-in tools.
For more intuitive profiling results, it is often desirable for a profiler to show runtime statistics for framework operations, or even DNN layers.
DNN frameworks have built-in tools to achieve this goal by correlating the hardware counters with runtime information collected in frameworks.
TensorFlow [3], coupled with the Cloud TPU Tool [21], can provide an execution timeline and runtime statistics for each TensorFlow operation.
Similarly, other mainstream frameworks (e.g., MXNet [12] and PyTorch [61]) provide built-in tools that can extract per-layer or per-operation runtime from both the CPU and the GPU.
The framework built-in tools render intuitive results for pro-these primitives.
These primitives include (i) task insertion/removal, (ii) task selection and update, and (iii) changing the policy for scheduling tasks.
The proposed primitives are simple yet powerful enough to represent many different optimizations as we will show in Section 5.
They play a key role in realizing our goal of efficiently exploring what-if questions.In summary, Daydream introduces the abstraction of a kernel-granularity dependency graph that contains mappings back to DNN specific abstractions (layers).
It tracks dependencies by collecting profiling data as well as instrumenting DNN frameworks.
Daydream also provides primitives to mutate the dependency graph in the form of simple graph transformations.
Altogether this enables programmers to both (i) model a diverse set of popular optimizations spanning kernel-and layer-level enhancements by using simple graphtransformation primitives, and (ii) estimate the efficacy of optimizations by simulating execution time based on optimizationinduced graph mutations.
We describe Daydream's design with an emphasis on how to construct Daydream's proposed graph abstraction: the kernelgranularity dependency graph with mappings back to DNN layers.
We also describe the primitives for mutating this graph to model different optimizations and how Daydream uses the graph to estimate the efficacy of various DNN optimizations.
Phase 1: Trace collection.
Constructing a kernel-level dependency graph requires low-level details for all tasks.
These details are extremely massive, differ across ML frameworks, and can be obtained by profiling a baseline workload.
Daydream collects low-level profiling data using CUPTI [49], a tool which provides details for all CPU/GPU tasks including name, start time, duration, CUDA stream ID, thread ID, etc.
We manually augment three popular frameworks (Caffe, MXNet, PyTorch) for use with CUPTI and modify the layer modules of these frameworks to collect timestamps of each layer, which will be used for task-to-layer mapping, described in Section 4.3.
Through our instrumentation, we also collect the necessary information (e.g., size of gradients) to construct the dependency graph of distributed training via a profile collected in a single worker setting.
Phase 2: Dependency graph construction.
Daydream constructs the dependency graph with details of tasks provided by the first phase.
A dependency could be induced by domain knowledge (e.g., a GPU task triggers a communication task), or by hardware/software implementation (e.g., a cudaLaunchKernel API triggers the corresponding GPU task).
Based on our analysis, we identify five different types of dependencies (described in Section 4.2.2), which are sufficient for Daydream to accurately simulate baseline execution.Phase 3: Graph transformation.
To estimate the efficacy of a given optimization, Daydream models the optimization by transforming the dependency graph.
Daydream provides a set of primitives (e.g. selection, insertion/removal) to represent these transformations.
We design these primitives in a way such that they are succinct (easy to use), flexible (able to depict a wide range of optimizations), and accurate (being able to achieve high prediction accuracy).
Input :Dependency graph: G(V, E) Output :The start time of each task u ∈ V 1 F ← / 0 // initialize the frontier task set 2 P ← {0} // initialize thread progress 3 Phase 4: Runtime simulation.
Daydream simulates the execution of optimizations to predict runtime based on the dependency graph.
Algorithm 1 shows the simulation process, which traverses the dependency graph and puts tasks into execution threads.
In each iteration, Daydream picks one task from the execution frontier (i.e. tasks that are ready to execute), dispatches it to its corresponding execution thread, and updates the thread progress.
The simulation determines the start time of each task and records the total execution time.
Constructing the dependency graph is essential to determine the node (task) set and edge (dependency) set.
Daydream's kernel-level dependency graph contains the following four types of tasks:GPU tasks.
Each GPU task in the graph corresponds to one GPU kernel.
Daydream also views CUDA memory copies as improve memory-bounded GPU kernels by 2× because the number of transferred bits is halved.
With Tensor Cores in the Volta and Turing architectures, AMP empirically yields up to 3× speedup on the most compute-intensive workloads [58].
To predict AMP performance, we simply select all the compute-intensive (e.g., sgemm, conv) kernels and memorybounded (e.g., elementwise, batchnorm, RELU) kernels, and shrink their duration by 3× and 2× respectively.
We show the pseudo code for modeling AMP in Algorithm 3.
Input :Dependency graph: FusedAdam Optimizer.
We use the FusedAdam optimizer [52] implemented in NVidia's Apex package [51] as an example for the kernel fusion optimization.
This optimizer fuses all kernels in one weight update phase into one unified kernel.
It is applicable to the models that use the Adam optimizer (e.g., GNMT, BERT).
Daydream uses the kernel-tolayer mapping to identify the CPU/GPU tasks that belong to a weight update phase.
We remove all these tasks, then insert a new GPU task whose duration is roughly estimated by the sum of all removed compute-intensive kernels.G(V, E) Output :A modified graph G(V, E) to model AMP 1 GPUTasks ← {G.Select( f uncPtr(IsOnGPU))} 2 foreach u ∈ GPUTasks doReconstructing Batchnorm.
Recently Jung et al. [35] proposed a technique that optimizes non-convolutional layers in state-of-the-art CNNs.
It first splits each batch normalization layer into two sub-layers, then fuses the first sub-layer with the previous convolutional layer, and the second sub-layer with the following activation and convolutional layers.
We remove the affected activation kernels when estimating performance, since they are memory-bound kernels now fused with compute-intensive convolutional kernels.
For the batch nomalization layers, we estimate that the GPU kernels will be improved by 2× since this optimization halves the amount of input data that these layers load from GPU memory.Distributed Training.
Using Daydream we can accurately predict distributed training performance with the profile based on the single-GPU environment.
We evaluate Daydream's prediction based on PyTorch, which uses collective communication primitives from the NCCl library [46].
PyTorch groups gradients from multiple layers into buckets before transferring them.
Hence, to predict distributed training performance, we need to insert one allReduce task for every bucket.
The dependencies of the inserted tasks are determined based on the layer-to-bucket mapping (which requires additional instrumentation to the PyTorch framework).
Priority-Based Parameter Propagation (P3).
P3 [30] is a technique that optimizes communication overhead by slicing and prioritizing.
We evaluate Daydream's prediction of P3 based on MXNet, which uses the parameter-server mechanism [39].
In order to model parameter slicing, we insert multiple push task and pull tasks between the backward and the forward GPU tasks for each layer.
The duration of the push/pull task is calculated from the slice size and the network bandwidth.
To model the priority scheduling, we override the schedule function with a priority queue.
In addition to the above optimizations, we show that Daydream is capable of modeling an additional set of diverse DNN optimizations.BlueConnect.
BlueConnect [16] optimizes communication by decomposing the allReduce primitives into a series of reduce-scatter and all-gather primitives.
These primitives run concurrently as they use parallel communication channels.
To predict the performance of BlueConnect, instead of inserting regular allReduce or push/pull tasks, we need to insert reduce-scatter and all-gather tasks, and assign them to corresponding network channels (the duration can be estimated according to formulas shown in [57]).
MetaFlow.
MetaFlow [33] is a layer-fusion technique to optimize DNN training by fusing DNN layers to simplify the DNN topology.
We select the GPU kernels of substituted layers, remove them, and insert GPU kernels of new layers to predict the performance of MetaFlow in Daydream.
The new layers are mostly existing layers with different dimensions; their GPU kernel durations can be inferred by profiling.vDNN.
Virtualized DNN [67] reduces GPU memory consumption by temporarily offloading intermediate data from GPU memory to CPU memory.
The offloaded data needs to be prefetched back to GPU to perform execution, which causes potential performance overhead due to PCIe traffic or late prefetching.
To predict the performance overhead using Daydream, we only need to insert additional CUDA memory copies, and override the schedule function to implement a custom prefetching policy.Gist.
Gist [29] reduces GPU memory consumption by storing encoded intermediate data and decoding before the data is used.
The encoding and decoding introduces performance overhead.
We insert extra encoding and decoding GPU kernels (along with cudaLaunchKernel calls in CPU) to estimate the performance overhead in Daydream.
The duration of the inserted encoding/decoding kernels can be estimated using existing element-wise kernels.Deep Gradient Compression (DGC).
DGC [40] is a technique that reduces communication overhead by compressing the gradients.
To estimate performance, we: (i) scale the duration of communication; (ii) insert the GPU tasks of compression and decompression.
The duration of inserted prediction error depends on the training workload itself.
Due to this limitation, it is hard for Daydream to accurately model algorithmic innovations (e.g., BPPSA [77] or 2nd Order Optimizations [68]), because these innovations use new GPU kernels at a massive scale, making the performance estimation with Daydream less accurate.
Estimating new GPU kernels runtime is beyond the current scope of Daydream.While Daydream cannot predict individual kernel runtime, it provides a high-level structure for kernel developers to estimate the overall performance.
Developers can profile their individual kernels, and then input the profiling results into Daydream to accurately estimate the overall runtime.
This approach saves the engineering effort of porting the kernel implementation into the DNN frameworks.Concurrent Kernels Existing GPU profilers such as CUPTI usually serialize GPU kernel execution, removing all concurrency, making our performance estimation somewhat conservative.
Despite this, we observe that the runtime for models with concurrent execution (e.g., GNMT) can still be predicted with high accuracy ( § 6.2).
This is because the majority of computation time goes to fully connected layers (including embedding layers), which have no concurrent kernels executed in parallel with them.
We leave a complete solution for concurrent kernels, requiring better support from profiling tools, as a part of future work.
To help programmers understand the performance of the hardware accelerators and develop highly efficient applications, hardware vendors provide profiling tools (e.g., NVProf [48], Nsight [47], and vTune [66]) that can reveal low-level performance counters (e.g., cache hit rate, memory speed or clock rate).
These tools are usually designed with general applications in mind, and expose hundreds of low-level performance counters.
The fundamental limitation of all these tools is that they do not utilize application-specific knowledge.The new generation of profiling tools feature the application-aware property, enabling them to deliver domainspecific (e.g., ML-specific) insights about performance to programmers.
The Cloud TPU Tool [21] is an example of such a profiling tool.
It correlates low-level TPU metrics with the DNN structure, and shows the performance for each DNN layer.
Similarly, MXNet [12] and PyTorch [61] also have their own built-in profiling tools.
These domain-specific tools can highlight performance hotspots, but are less efficient in finding optimization opportunities.
In contrast, Daydream is not only application-aware, but also optimization-aware, enabling Daydream to quantitatively estimate the efficacy of different optimizations without fully implementing them.Prior works have tried to explore what-if questions in other contexts by using low-level traces.
Curtsinger et al. proposed a causal profiler (COZ [17]) to identify potentially unknown optimization opportunities by running performance simulation with certain functions being virtually speed-up.
Unlike Daydream, COZ does not require dependencies among functions because it does not consider the cases where functions can be added or deleted (which is the case for many ML optimizations).
Pourghassemi et al. uses the idea of COZ to analyze the performance for web browser applications [63].
For data analytic frameworks, such as Spark [82], Ousterhout et al. use dependency analysis to understand the overhead caused by I/O, network, and stragglers [59,60].
Daydream is designed to address a more diversified set of what-if questions, and hence requires more powerful modeling.Prior works address what-if questions of the form "What if we can speedup task T by N times (or infinity)?"
, but they do not study whether existing optimizations can deliver this speedup.
In the ML context, given an optimization, accurately predicting the performance of individual tasks in the dependency graph, is still an open problem.
It requires additional knowledge about the kernel implementation and the architecture design.
Currently Daydream can not automatically estimate the runtime of new GPU kernels.
However, as we show in Section 6, even with rough estimates of per-kernel duration based on domain knowledge and reasonable assumptions, we can still achieve high overall prediction accuracy.
The efficacy of DNN optimizations can vary largely across different DNN models and deployments.
Daydream is a new profiler to effectively explore the efficacy of a diverse set of DNN optimizations.
Daydream achieves this goal by using three key ideas: (i) constructing a kernel-level dependency graph by utilizing vendor-provided profiling tools, while tracking dependencies among concurrently executing tasks; (ii) mapping low-level traces to DNN layers in a synchronizationfree manner; (iii) introducing a set of rules for programmers to effectively describe and model different optimizations.
Our evaluation shows that using Daydream, we can effectively model (i.e. predict runtime) the most common DNN optimizations, and accurately identify both optimizations that result in significant performance improvements as well as those that provide limited benefits or even slowdowns.
Daydream is a part of Project Fiddle at Microsoft Research (MSR).
We thank the MSR Lab LT, especially Ricardo Bianchini and Donald Kossmann, for their enthusiastic and unwavering support of Project Fiddle.
We also thank our shepherd, Swaminathan Sundararaman, the anonymous ATC reviewers, Jorgen Thelin, Shivaram Venkataraman, Deepak Narayanan, and the EcoSystem group members, especially James Gleeson, Geoffrey Yu, and Xiaodan (Serina) Tan for their constructive feedback and comments.
This work was also supported in part by the NSERC Discovery grant, the Canada Foundation for Innovation JELF grant, the Connaught Fund, and Huawei grants.
