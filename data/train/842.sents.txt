Heuristic forward search is the dominant approach to satis-ficing planning to date.
Most successful planning systems, however, go beyond plain heuristic search by employing various search-enhancement techniques.
One example is the use of helpful actions or preferred operators, providing information which may complement heuristic values.
A second example is deferred heuristic evaluation, a search variant which can reduce the number of costly node evaluations.
Despite the widespread use of these search-enhancement techniques however, we note that few results have been published examining their usefulness.
In particular, while various ways of using, and possibly combining, these techniques are conceivable , no work to date has studied the performance of such variations.
In this paper, we address this gap by examining the use of preferred operators and deferred evaluation in a variety of settings within best-first search.
In particular, our findings are consistent with and help explain the good performance of the winners of the satisficing tracks at IPC 2004 and 2008.
In the past decade, heuristic forward search in the state space has been the dominant approach to satisficing planning.
This is made evident by the fact that 4 out of 5 winners of the satisficing track in the biennial international planning competition (IPC) since 2000 have been heuristic forward-search planners.
There has also clearly been a propagation of ideas in the way that certain aspects of past successful planning systems have been adopted by more recent systems.
One example is the use of helpful actions in the venerable FF planner by Hoffmann and Nebel (2001), winner of the satisficing track at IPC-2000.
Helpful actions are actions that contribute to solving a simplified version of the task.
As they are likely to also contribute to solving the original task, they can be preferred over actions that are not considered helpful.
This search enhancement, which has been shown to improve FF's performance notably ( Hoffmann and Nebel 2001), has been adapted by the Fast Downward system (Helmert 2006), winner of the satisificing track at IPC-2004, under the name preferred operators.
Fast Downward, in turn, inspired many newer planners like LAMA ( , the satisficing winner at IPC-2008;Temporal Fast Downward (Röger, Eye- rich, and Mattmüller 2008), and systems used for experiments in various publications (Keyder and Geffner 2009;Richter, Helmert, and Westphal 2008;Helmert and Geffner 2008).
As a result, these systems all use the deferred evaluation popularised by Fast Downward.
Deferred evaluation is a variant of best-first where the successors of a node are not heuristically evaluated when they are generated, but later when they are expanded.
This may save time if many more nodes are generated than expanded.The starting point for this paper is to note that even though many current planning systems use the search enhancements mentioned above (preferred operators and deferred evaluation), there exists little data on their respective usefulness.
For example, the use of preferred operators (resp.
helpful actions) in Fast Downward is different from their use in FF.
While the authors of both planners report significantly improved performance compared to not using preferred operators, the question remains which usage is better, or how some of the many other conceivable approaches for using preferred operators would perform.
Deferred evaluation, on the other hand, has been suggested to be particularly useful in combination with preferred operators (Helmert 2006).
The question is open whether deferred evaluation is also advantageous in the absence of preferred operators, or when using preferred operators in a different fashion than the one examined by Helmert.We address these questions by examining the performance of various approaches to using preferred operators as well as their interaction with deferred evaluation.
One finding of this work is that deferred evaluation does not offer any benefit on standard benchmark tasks on average, but leads to strong improvement in some contexts and to strong deterioration in others.
On the other hand, we find that the best method for using preferred operators strongly depends on whether deferred evaluation is being employed or not.
Using an artificial search space, we demonstrate how the benefit that can be gleaned from preferred operators varies with different aspects of the search space.
One of the core features of the FF planner ( Hoffmann and Nebel 2001) is the use of helpful actions.
Helpful actions are operators that are deemed promising by the FF heuristic because they form part of or achieve a precondition of the relaxed plan, a solution for a simplified version of the task which ignores delete effects.
FF uses helpful actions to prune the search space and only evaluates successor states reached via helpful actions.
This restriction makes the search incomplete, so that FF restarts without this pruning technique if it fails.In the spirit of FF's helpful actions, Helmert (2006) subsequently coined the term preferred operators for all operators that are deemed promising for some reason.
We will in the following use that general term to subsume helpful actions.
The Fast Downward planner makes use of preferred operators in a dual-queue approach, where it keeps the states that are reached via a preferred operator (the preferred successors) in an additional open list.
It selects the next state to be expanded alternately from its regular open list containing all successors and from the open list containing just preferred successors, pruning duplicates upon removal.
A preferred successor is thus expanded sooner, on average, than an arbitrary successor of a state.
In particular, at least every second state selected for expansion is a preferred successor.
1 In contrast to FF which evaluates only preferred successors (as long as possible), Fast Downward may gain less leverage from its preferred operators, but at the advantage that its search stays complete.
The textbook version of best-first search keeps an open list of states to be expanded, ordered according to the heuristic estimates of their goal distance.
In each step, a state with smallest heuristic value is removed from the open list and expanded by applying all operators applicable in the state.
All successor states generated in this way are then evaluated and inserted into the open list.The Fast Downward planner (Helmert 2006) incorporates a variation of best-first search called deferred heuristic evaluation, where the successors of a state are not evaluated upon generation.
Instead, they are inserted into the open list with the heuristic estimate of their parent.
Only upon being removed from the open list for expansion are they evaluated, so that their heuristic value can in turn be used for their successors.
According to Helmert (2006), this technique can decrease the number of state evaluations substantially, especially when combined with preferred operators.
For instance, consider the way preferred operators are used within Fast Downward (see above).
If s is a preferred successor of a state s, then s will be expanded sooner than most of its siblings (as the fraction of preferred successors is usually small).
If there exists a path with non-increasing heuristic values from s to the goal, then the remaining siblings of s will never be evaluated.
By contrast, standard best-first search would evaluate all successors of s.While deferred evaluation may reduce the number of state evaluations compared to standard best-first search, it usually increases the number of state expansions.
This is due to the heuristic values being less informative, since they stem from the parent of a state rather than the state itself.
Note however that successor states need not be generated upon expansion of their parent.
Instead it is sufficient to store pointers in the open list to the parent state and to the creating operator, thus reducing memory requirements.
The FF planner uses preferred operators for pruning the search space while Fast Downward uses them in a dualqueue approach.
In both planning systems, preferred operators have been shown to improve performance ( Hoffmann and Nebel 2001;Helmert 2006).
However, the question is open as to which of the two usages leads to better results, assuming that all other aspects of a planner stay fixed.
Furthermore, other ways of using preferred operators are conceivable which have not been analysed in the literature to date.
An obvious idea is to use preferred operators for tie-breaking and expand, among states of equal heuristic value, preferred successors first.
The dual has also been proposed (Vidal 2004), namely using the heuristic values for tie-breaking among equal preferredness: choose preferred successors whenever they exist, otherwise expand non-preferred successors.
Within each of the two groups, choose according to heuristic values.
This latter method places more emphasis on preferred operators than the former, but is not as restrictive as expanding only preferred successors like FF does.The dual-queue approach can also be modified to give higher precedence to preferred successors.
For example, the IPC versions of both Fast Downward and its offspring LAMA used a setting that increases the use of preferred successors at certain points in time.
The planners keep a priority counter for each open list, initialised to 0.
Whenever a state is removed from a list, the priority of that list is decreased by 1.
At each iteration, the next state is removed from the list that has higher priority.
Unless the priorities are changed outside of this routine, this method will alternate between choosing a state from the list of preferred successors and from the list of all successors.
The setting used in Fast Downward and LAMA to increase the use of preferred operators is as follows: Whenever progress is made, i. e., whenever a state is discovered with a better heuristic estimate than the states before, the priority of the open list with only preferred successors is "boosted" by adding 1000.
Therefore, at least the next 1000 states will now be removed from the list of preferred successors, and only if none of them leads to further improvement does the turn go back to the regular open list.
If another improving state is found within the 1000 states, the boosts accumulate and it takes respectively longer until states from the regular queue are expanded again.In this paper, we address the question which of these different uses for preferred operators leads to best performance.
The answer may vary for different types of search; we examine greedy best-first search.
We look at both the standard implementation of best-first search (in the following called Eager), and the deferred-evaluation variant (in the following called Lazy).
The reason for examining both variants is that they can behave very differently, with Lazy being less informed than Eager, but paying a smaller price for wrong expansions.
Hence, the best use of preferred operators may be different for the two.
Along the way, we answer the question whether deferred evaluation is useful in general and whether synergies exist between preferred operators and deferred evaluation.
We have conducted a range of experiments to measure the relative performance of several uses for preferred operators (POs).
The uses we compare are • No POs, normal search without preferred operators.
• PO pruning, where the search space is restricted to only preferred successor states and the search restarts without POs if it exhausts the restricted search space without a solution.
This corresponds to the use made of preferred operators in the FF planner (though FF uses enforced hillclimbing rather than best-first search as the underlying search algorithm).
• Heur > PO, where preferred operators are used only to break ties among states of equal heuristic values.
• PO > Heur, where preferred operators are used whenever possible, breaking ties with heuristic values.
This approach has been used in the YAHSP planner (Vidal 2004).
• Boosted Dual Queue, the dual-queue approach where the open list with preferred successors is boosted whenever progress is made during the search, leading to more preferred successors being expanded.
This approach was used in the IPC versions of Fast Downward and LAMA.We ran experiments on all planning tasks from past international planning competitions between 1998 and 2006, totalling 1612 tasks.
Four different planning heuristics were used: the FF heuristic ( Hoffmann and Nebel 2001), the context-enhanced additive (cea) heuristic (Helmert and Geffner 2008), the causal graph (CG) heuristic (Helmert 2006) and the additive heuristic (Bonet and Geffner 2001).
We follow the Fast Downward implementation of the FF heuristic by defining preferred operators to be those applicable operators which appear in the relaxed plan.
By contrast, FF's "helpful actions" additionally include all applicable operators which add a precondition of an operator in the relaxed plan that is not true in the current state.
For the additive heuristic we generate preferred operators in the same way as for the FF heuristic.
(Without referring to relaxed plans, these preferred operators can alternatively be characterised as those applicable operators which contribute to the heuristic value, i.e. we select a best supporting action for the goal, according to the heuristic, and, recursively, a best supporting action for unsatisfied preconditions of selected actions.)
The preferred operators in the CG and cea heuristics are those applicable operators that change the current value of a goal variable to a value that is on a lowest-cost path (as computed by the heuristic) to the goal value in the domain transition graph of that variable.
If no such operator exists for a goal variable, the process recurses for variables that are involved in conditions for changing the value of the goal variable (Helmert 2006).
The experiments were conducted on a heterogeneous cluster of Intel Xeon and AMD Opteron CPUs, ranging from 2.2 GHz to 2.83 GHz.
(The magnitude of the experiments precluded experiments on homogeneous machines.)
To allow fair comparisons, for each given planning task all planner configurations using the same heuristic were run on the same CPU.
The timeout was 30 minutes and the memory limit 1.75 GB in all cases.For each planner configuration, we report results regarding coverage (the number of problems solved), the number of heuristic state evaluations, the runtime and the quality of the plans found.
We report evaluations rather than expansions to be able to compare the two search types Eager and Lazy in a meaningful way.
While the cost for an evaluation is the same for both search variants, the cost of an expansion differs greatly for the two.
To expand a state, Lazy only puts two pointers into the open list, while Eager generates and evaluates all successor states.
For Lazy, evaluations and expansions are the same; for both search types the number of evaluations is the number of states actually generated.
Most importantly, roughly 80% of the total runtime of the planner is spent calculating heuristic values, making evaluations a platform-independent indicator of the search effort.All reported results are measured via scores from the range 0-100, where best possible performance in a task is counted as 100, while failure to solve a task and worst performance are counted as 0.
For coverage, a solved task counts 100.
Evaluations and runtime are measured on a log scale due to the exponential nature of the problem.
Performance better than a lower limit (100 states for evaluations and 1 second for runtime) counts as 100, performance worse than an upper limit (1,000,000 states for evaluations and the timeout of 30 minutes for runtime) counts as 0.
In between, we interpolate with a logarithmic function.
Plan quality is measured using the criterion of IPC 2008 (scaled by a factor): a solved task counts as 100 · l * /l, where l is the length of the generated plan and l * is the shortest plan length found by any of the configurations.
The scores reported for domains are averaged over the tasks in the domain; when summarising results from multiple domains we show normalised averages such that each domain is weighed equally.
Table 1 shows the results for all configurations and heuristics.
We begin by discussing the coverage (number of problems solved).
The first thing we note is that the use of preferred operators almost always improves performance, and that the impact of preferred operators is significantly larger even than the choice of heuristic.
With suitable use of preferred operators, the weakest heuristics (CG and additive) perform better than the strongest heuristics without preferred operators (FF and cea).
Which use of preferred operators gives best results is dependent on the type of search: for eager search the simple dual-queue approach works best, for deferred evaluation the boosted dual-queue is best.
The Using preferredness as a tie-breaker among states of equal h-value ("Heur > PO") is a relatively subtle way of using preferred operators.
Compared to not using preferred operators, this option usually improves performance slightly for Eager and notably for Lazy.
However, for both search types the results are far from those obtainable via other options.
Using preferred operators whenever possible ("PO > Heur") leads with one exception to more improvement than "Heur > PO" for both search types.
For Lazy, this option works very well, giving consistently second-or third-best results among the 6 possible PO options.
For Eager, this configuration does not work quite as well and usually ranks fourth.
PO pruning is similarly good as "PO > Heur" for Lazy, and better than that option for Eager, ranking secondbest on average.
In particular, when using PO pruning the performance of the two search types is comparable.
The simple dual-queue approach is the best option for Eager, but gives mixed results for Lazy, ranging from second-best (CG heuristic) to fourth-best (cea and additive heuristic).
The boosted dual-queue approach is the best option for Lazy, but for Eager the performance is notably worse than that of the simple dual queue.Summarising the results, it seems that for Eager a certain amount of preferred operators is useful (e. g., "Heur > PO" or dual-queue), but strong uses of preferred operators can be counter-productive (e. g., "PO > Heur" and boosted dual-queue are worse than simple dual queue).
When using deferred evaluation, however, stronger use of preferred operators always seems to improve results.
The use of PO pruning works well for both search types, probably due to the significantly reduced branching factor as only preferred successors are evaluated (unless the restricted search fails, which we found to be rare).
Note that for Lazy, PO pruning and "PO > H" give similar results as they lead to identical behaviour in those cases where a goal can be found by only expanding preferred successors.
For Eager, however, "PO > H" is worse than PO pruning because with "PO > H" Eager evaluates all successors, whereas with PO pruning it evaluates only preferred successors.Comparing Eager with Lazy, we note that Eager performs better in most of the configurations; with boosted dual queue, however, deferred evaluation is better.
The best performance obtainable from each search variant (using the dual-queue setting for Eager, and boosted dual-queue for Lazy) is very similar.
Lazy consistently evaluates fewer states than Eager and is usually faster.
Eager, on the other hand, finds plans of better quality.
This is not surprising as Eager is better informed with respect to heuristic values than Lazy.
For both search variants, plan quality is improved when using preferred operators.
When using deferred evaluation, all successors of a state are placed in the open list with the same heuristic value (that of the parent).
Preferred operators are thus particularly useful for lazy search as they can help recognise the best successor of a state.
This is why stronger uses of preferred operators Perf consistently improve results for Lazy.
Eager search, on the other hand, evaluates all successors of a state at once.
Eager search thus has an estimate (through the h-values of the successors) of which successors are best, and preferred operators may not provide as much additional information to Eager than to Lazy.
Preferred operators can be helpful to decide among states of equal h-value, for example, as they act as a second type of information about which states likely lie on a path to the goal.
However, preferred operators may even be detrimental to the performance of Eager if they are ill-informed (i. e. if successors are deemed preferred even though they are in fact worse than some of their siblings).
We summarise the main findings that hold on average, as shown by Table 1.
• Preferred operators are very helpful.
For both search types, an average coverage improvement of 10-15% can be obtained with the respective best PO use; this is far more than the difference between the various heuristics.
• Best PO use depends on the search type.
While both eager and lazy search improve by a similar amount through using POs, Eager performs best with PO pruning and the standard dual-queue approach, while Lazy excels when using POs more strongly via the boosted dual queue.
• Deferred evaluation trades time for quality.
Both Eager and Lazy obtain similar coverage.
But Lazy is faster while Eager finds better plans.
Our work thus confirms a claim by Helmert (2006) which had been lacking empirical support to date.
In Table 2, we report individual results for a number of domain-heuristic combinations.
The top part of the table contains "typical" cases which reflect the general findings discussed above.
Preferred operators increase performance notably, and lazy and eager search perform similarly well.
While there is some variation (e. g. in Pipesworld-Tankage), in general it holds that "H > PO" is slightly better than not using POs, while the other PO uses are noticeably better; and PO pruning or dual-queue usage is best for Eager while the boosted dual-queue approach is best for Lazy.
The rest of the table contains exceptions from the general trend, which we discuss in turn.
Firstly, we examine cases which are outliers with regard to PO use.
In PSR-Large with the additive or FF heuristic, we find that both search types gain some improvement from subtle use of preferred operators in the "H > PO" setting.
However, stronger use of POs is detrimental, which becomes particularly evident in the settings "PO > H" and PO pruning.
The reason seems to be that few preferred operators are found in this domain; this holds for both the additive and the FF heuristic.
The setting "H > PO" uses the information about preferred operators only when it exists and adds to the knowledge about h-values, and thus improves results.
By contrast, restricting the search to POs means that we may often be precluded from expanding the current best state because it may not happen to be preferred.The Philosophers domain with the additive or FF heuristic is interesting because even the subtle use of POs via "H > PO" is detrimental for eager search, and with the exception of the dual-queue approach, all strong uses of POs lead to worse performance for both search types.
When examining the domain, we found that it contains large plateaus or near-plateaus, and many ill-informed preferred operators.
In large areas of the search space 80-100% of the operators are preferred, of which only 5-10% lead to states with better h-values.
Breadth-first search may be the best way to escape from plateaus, and when not using POs, this is what our search does (due to the open list acting FIFO on states of equal h-value).
When using the dual queue, the search performs breadth-first search half of the time, which still works very well.
By contrast, strong use of POs means that the search spends most of the time following ill-informed POs, and by not doing breadth-first search it may not manage to leave the plateaus as quickly.In Schedule with the FF heuristic, the use of POs in a dual-queue approach is a substantial improvement over not using POs, however, the subtle use of POs as tie-breakers in "H > PO" is notably worse than not using POs.
While we are at present not sure what causes this behaviour, one possible explanation is that POs are helpful in some parts of the search space but useless or even detrimental in others.It is noteworthy that the dual-queue approach may be the most "robust" way of using preferred operators.
For all other PO uses, bad cases exist where they perform significantly worse than not using POs.
By contrast, the dual-queue approach always performs fairly well and seems to be able to make use of helpful POs while not getting overly distracted by ill-informed POs.Lastly, the third part of Table 2 contains examples where the two search variants Eager and Lazy exhibit substantially different behaviour.
In Satellite and Logistics, a large branching factor in combination with an informative heuristic leads to lazy search being extremely useful, so that it dominates eager search notably with respect to coverage, evaluations and runtime.
We will discuss this effect in Satellite in more detail below.
On the other hand, Pathways is a domain where deferred evaluation seems to be particularly harmful, so that Eager performs significantly better than Lazy.
Table 3 provides a closer look at the effect of deferred evaluation in the Satellite domain when using the contextenhanced additive heuristic (results for the other heuristics are similar).
We show the number of evaluations for the last 12 instances of the domain.
With the configurations that are not shown (H > PO, PO > H, and boosted dual queue), Lazy performs exactly the same as with PO pruning, while Eager performs no better than without POs.
Eager obtains best performance with PO pruning in this domain as the pruning greatly reduces the large branching factor of the search space.
However, even in this setting Lazy outperforms Eager notably with regards to evaluations and, consequently, runtime.
Table 3 shows that with exception of the first two instances, Lazy evaluates substantially fewer states than Eager, with differences as high as two orders of magnitude in some cases.
On instance 34, eager search with the dual .
DQ 25 150K 1K 2K 71K 2K 71K 26 211K 1K 3K 116K 3K 80K 27 24K 1K 2K 96K 3K 96K 28 46K 2K 3K 195K 6K 195K 29 - 1K 3K - 7K 285K 30 - 3K 6K - 10K 420K 31 - 4K 5K - 14K - 32 - 10K - - 28K - 33 - 8K - - 41K - 34 - 4K 7K 265K 15K 265K 35 - 10K - - 26K - 36 -7K --27K - To more closely analyse how the benefit from preferred operators varies with different characteristics of the search space, we conducted a set of experiments on a manuallydesigned search space.
We chose the parameters of this artificial search space without much experimentation, as it was straightforward to find settings that show informative behaviour, i. e. where the task is not too easy and not too hard.
Other settings are possible and may often lead to similar results.
Our goal was to create a scenario where preferred operators can be useful but also harmful, depending on how well the preferredness of an operator predicts that it actually leads to a better state, and how well informed the heuristic is in comparison.
The parameters we vary are:• The quality of the heuristic.
We use an admissible heuristic that randomly deviates by up to a factor dev.
fac.
≤ 1 from the approximate goal distance agd (see below) of a state.
We experimented with values from 0.1 to 0.9.
in 0.1-step increases.
• The recognition rate rec.
rate for preferred operators, i. e., in what percentage of cases a useful operator (one that leads to a better state) is labelled as preferred.
For this parameter, we tested values between 0% and 100% using steps of 10%.
We fixed a branching factor of 25 and a typical solution depth of 50.
States are characterised by their approximate goal distance (agd).
The initial state has an agd-value of 50 and states with a value of 0 are labelled as goals.
For each state with agd-value d, we generate the values of the 25 successors randomly as follows: with probability 0.04 they are d − 1, with probability 0.16 they are d + 1, and with probability 0.8 they are d.
This means that there exist on average one successor with a lower agd-value, 20 successors with equal agd-values, and 4 successors with higher agdvalues than the parent.
Likewise, the decision whether or not an operator is labelled as preferred is made randomly: if an operator is useful, i. e. if it leads to a successor state with better agd-value, it is preferred in the percentage of cases given by rec.
rate; else it is preferred in 10% of the cases.
If the recognition rate decreases, this means that the relative chance of a preferred operator not being useful increases.
Figures 1 and 2 show the results of our experiment.
In Fig. 1, we fixed the heuristic quality at three different values (low in the left panel, medium in the centre and high on the right), and varied the recognition rate of preferred operators.
The numbers shown are averaged over 100 runs with different random seeds.
As can be seen, a low recognition rate leads to preferred operators being detrimental to the search, while with a high recognition rate, using preferred operators improves performance compared to not using them.
It is also evident that PO pruning suffers more strongly from illinformed preferred operators than the dual-queue approach.
The relative performance of the approaches shown in Fig. 1 is notably independent of the quality of the heuristic.
The dual-queue approach typically becomes better than not using POs if the recognition rate is more than 20-30%; PO pruning has a higher threshold of 40-50%.
Fig. 2 provides a different look on the same data, where we fix the recognition rate of preferred operators and vary the heuristic quality.
As can be seen in the centre and right panels, the advantage of using preferred operators decreases for high values of heuristic quality, i. e., preferred operators are most useful if the heuristic is not well informed.
But even given very high heuristic quality, POs can still notably improve performance if the recognition rate is high.
We have examined two search enhancement techniques for planning that are widely used, preferred operators and deferred evaluation.
Our findings include that the obvious method of using preferred operators as tie-breakers has little use, compared to other approaches; and that the dualqueue approach, which keeps preferred operators in a second open list, performs best amongst all tested approaches in a standard best-first search.
In particular, the dual-queue approach dominates pruning, the method that was proposed when preferred operators were first introduced.
With controlled experiments in an artificial search space, we showed that pruning performs badly if the preferred operators are ill-informed.
Our results thus suggest that FF might be improved by using the dual-queue approach, though experiments with FF's enforced hill-climbing search algorithm would be needed to confirm this hypothesis.Our work confirms previous claims that deferred evaluation can be very useful in the presence of large branching factors and that this method trades time for plan quality.
Our findings also help to explain the good performance of Fast Downward at IPC 2004 and LAMA at IPC 2008.
Both planners used deferred evaluation in combination with the boosted dual-queue option, a combination which led to best performance in our experiments.
However, the conceptually simpler approach of using standard best-first search with the unboosted dual queue performs equally well on average, albeit having different strengths and weaknesses.
Our results show that boosting the dual queue in Fast Downward and LAMA should always be done for best results, but it would be detrimental in a standard best-first search.
We thank Patrik Haslum for helpful discussions.NICTA is funded by the Australian Government, as represented by the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council, through the ICT Centre of Excellence program.This work was partly supported by the German Research Foundation (DFG) as part of the Transregional Collaborative Research Center "Automatic Verification and Analysis of Complex Systems" (SFB/TR 14 AVACS).
For more information, see http://www.avacs.org/.The computing resources for the experiments reported in this paper were provided by the Black Forest Grid Initiative.
