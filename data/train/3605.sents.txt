Automatic Machine Learning (AutoML) is a powerful mechanism to design and tune models.
We present Katib, a scal-able Kubernetes-native general AutoML platform that can support a range of AutoML algorithms including both hyper-parameter tuning and neural architecture search.
The system is divided into separate components, encapsulated as micro-services.
Each micro-service operates within a Kubernetes pod and communicates with others via well-defined APIs, thus allowing flexible management and scalable deployment at a minimal cost.
Together with a powerful user interface, Katib provides a universal platform for researchers as well as enterprises to try, compare and deploy their AutoML algorithms , on any Kubernetes platform.
Automatic Machine Learning (AutoML) determines the optimal hyper-parameters or the neural network structure for a specific task.
Thus it enables less technical users, and can discover state-of-art models that are almost as good as handcrafted ones ( [21], [14], [16], [4], [10]).
However, we have a long way before AutoML becomes mainstream.
The first is the diversity of AutoML algorithms.
Algorithms for hyperparameter tuning are generally different from those for neural architecture search (NAS).
Even within NAS, different algorithms follow separate structural mechanisms.
This diversity makes it difficult to reuse infrastructure and code, thus increasing the cost of deploying AutoML widely.
The second problem is the prohibitive computational cost.
The algorithm proposed by Zoph [23], for example, is expensive.
This is a very active area of research.To solve the first problem, we propose to build a general AutoML system.
We show that it is possible to integrate both hyper-parameter tuning and NAS into one flexible framework.
To help solve the second problem, we enable users to plug in their own optimized algorithms and we leverage micro-services and containers for scalability.
With the help of Kubernetes [1], each component can be encapsulated inside a container as a micro-service.
Our contributions can be summarized as follows:• We integrated various hyper-parameter tuning and neural architecture search algorithms into one single system.
• We standardized the interface to define and deploy AutoML workflows in Kubernetes based distributed systems.
The implementation of Katib is available at https://github.com/kubeflow/katib.
AutoML algorithms share the common ground that they run in an iterative manner.
The user first defines the search space, metrics target and maximum iterations.
The algorithm searches for the optimal solution until the target metrics or the maximum number of iterations is reached.
However, they may vary in terms of their internal mechanisms.
In hyperparameter tuning, we have a black-box function f (·) whose value is dependent on a set of parameters p.
The goal is to find a ˆ p such that f (p) can be minimized or maximized.
In each iteration, a search algorithm service Suggestion will generate a set of candidate hyperparameters.
The candidates are sent to Trial that provides training and validation services.
The performance metrics are then collected by Suggestion to improve its next generation.
In neural architecture search (NAS), a neural network is represented by a directed acyclic graph (DAG) G = {V, E}, where a vertex V i denotes the latent representation in i th layer and a directed edge E k = (V i ,V j ) denotes an operation o k whose input is V i and output is given to V j .
The value of a vertex V i depends on all the incoming edges:V i = g({o k (V j )|(V i ,V j ) ∈ E}) g(·)is a function to combine all the inputs.
It can vary in different algorithms.
In [23] and [16], g(·) means concatenating along the depth dimension.
In [4], [21] and [14], a weight is assigned to every edge so g(·) is naturally the weighted sum.Extensive research in NAS has lead to enormous diversity of NAS solutions.
In terms of the search objective, the algorithm may search for either the optimal network or the optimal cell.
The former constructs the whole graph directly while the latter generates a subgraph G , and the whole graph is built by duplicating the topology of G .
In terms of evolving strategy, some NAS algorithms adopt a generation approach while others use modification.
With the generation approach, the algorithm will propose a new neural architecture in each iteration.
With the modification approach, however, the algorithm will modify the current architecture by adding or deleting some parts of it instead of creating a brand-new candidate.
Based on this categorization, the latest NAS algorithms can be summarized as follows: Search for Network Search for Cell Evolve by Generation [23], [16], [20], [11], [2] [23], [16], [24], [22], [13] Evolve by Modification [9], [4], [18], [7], [3] [17], [21], [14], [10], [15], [6], [5] Table 1: Summary of neural architecture search algorithms Diverse as they are, those algorithms can be integrated into one system.
Compared with hyperparameter tuning, NAS only needs one extra ModelManager service to store, construct and manipulate models.
In each iteration, Suggestion provides the topology of the next candidate or the modification decisions of the previous architecture to ModelManager, which constructs the model and sends it to Trial.
Then the model is evaluated and the performance metrics are fed back to Suggestion, starting a new iteration.All these workflows can be summarized by We combine the requirements of these AutoML workflows and the ideas from Google's black-box optimization tool Vizier [8], with the design show in Figure 2.
The user starts from defining an AutoML task with Katib's interface, the details of which can be found at http://bit.ly/2E5B9pV.
A controller examines the task definition and spawns the necessary services.
The data communication between different containers is managed by Vizier Core.
The searching procedure follows exactly the workflow defined in Section 2.
Consider EnvelopeNet [10] as an example of non-standard NAS algorithm .
In EnvelopeNet, the neural networks are updated by pruning and expanding EnvelopeCells, which are convolution blocks connected in parallel.
And these modification decisions are based on feature statistics instead of validation accuracy.
The Suggestion and training containers will be pre-built so the user only needs to specify the structure of EnvelopeCells and other necessary parameters in StudyJob yaml file.
In each iteration, Vizier Core Manager first requests one or more modification decisions from Suggestion and sends them to ModelManager.
Then ModelManager calculates the current architectures, compiles the models into runnable objects, and sends them to Trials, which will carry out a truncated training process.
Once finished, a MetricsCollector is spawned to parse feature statistics from the training logs.
Finally, this information is fed back to Suggestion via the Vizier Core and a new iteration starts.
During the process, all the model topologies and metrics are stored in a database and presented to the user.
Katib is scalable.
The controller can spawn multiple parallel Trials in each iteration to accelerate the search.
These service components can be shared globally among all the users.The initial version provides hyper-parameter tuning with Bayesian optimization [19], Hyperband [12], grid search and neural architecture search with reinforcement learning ( [23]).
The user can also deploy customized tasks by creating her own algorithm for the Suggestion and the training container for each Trial.
We will add more algorithms such as EnvelopeNet [10] and integrate the support for advanced acceleration techniques such as parameter sharing [16].
This paper presents Katib, a distributed general AutoML system based on Kubernetes.
The key idea is to abstract AutoML algorithms into functionally isolated components and containerize each component as a micro-service.
With this extendable and scalable design, Katib can be a powerful tool for both advancing machine learning research and delivering turnkey AI solutions for enterprise users.
The authors would like to thank Cisco Systems for their support, especially Amit Saha for help with the paper.
