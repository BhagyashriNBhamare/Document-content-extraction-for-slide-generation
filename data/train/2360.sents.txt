Although data compression can benefit flash memory lifetime, little work has been done to rigorously study the full potential of exploiting data compressibility to improve memory lifetime.
This work attempts to fill this missing link.
Motivated by the fact that memory cell damage strongly depends on the data content being stored, we first propose an implicit data compression approach (i.e., compress each data sector but do not increase the number of sectors per flash memory page) as a complement to conventional explicit data compression that aims to increase the number of sectors per flash memory page.
Due to the runtime variation of data com-pressibility, each flash memory page almost always contains some unused storage space left by compressed data sectors.
We develop a set of design strategies for exploiting such unused storage space to reduce the overall memory physical damage.
We derive a set of mathematical formulations that can quantitatively estimate flash memory physical damage reduction gained by the proposed design strategies for both explicit and implicit data compression.
Using 20nm MLC NAND flash memory chips, we carry out extensive experiments to quantify the content dependency of memory cell damage, based upon which we empirically evaluate and compare the effectiveness of the proposed design strategies under a wide spectrum of data compressibility characteristics.
NAND flash memory cells gradually wear out with program/erase (P/E) cycling due to physical device damage caused by each P/E cycle, and cycling endurance drastically degrades with the technology scaling down.
Hence, how to maximize memory lifetime has been widely studied from different aspects, e.g., signal processing and error correction coding (ECC) [1][2][3], flash translation layer (FTL) [4][5][6][7][8][9][10], and system software stack [11][12][13].
Nevertheless, to our best knowledge, no prior work has thoroughly studied how data compressibility can be leveraged to improve flash memory lifetime.
It is actually not surprising, since this question appears to be trivial at first glance: In conventional practice, the sole objective of data compression is to improve storage efficiency (i.e., explicitly increase the number of data sectors that can be stored in one flash memory page).
This is referred to as explicit data compression in this work.
Due to the runtime variation of data compressibility, explicit data compression results in heterogeneity among flash memory pages in terms of the number of sectors per page, which can complicate FTL and/or file system design.
As a result, it is not uncommon that commercial flash-based storage devices do not use data compression at all.
If sophisticated FTL and/or file systems, which can employ explicit compression to improve storage efficiency, are indeed available, one may simply expect that storing data with an average compression ratio 1 of α can directly improve the flash memory lifetime by 1/α.
Therefore, one may easily draw the following conclusion: If we do not want to complicate the FTL and/or file system, we should simply leave the user data uncompressed, for which the data compressibility is totally irrelevant to flash memory lifetime; If we use complicated FTL and/or file systems to support explicit data compression, the flash memory lifetime improvement solely depends on the average data compression ratio.This work contends that the above intuitive conclusion is far from revealing the complete potential of how data compressibility can help to improve flash memory lifetime.
In essence, it overlooks two factors.
First, flash memory experiences content-dependent memory damage, i.e., the damage suffered by each memory cell depends on its content (e.g., '11', '10', '00', and '01' in MLC flash memory) being stored.
Once data compression leaves some unused storage space within flash memory pages, we can manipulate their data content in a damage-friendly manner to reduce physical damage.
Hence, conventional explicit data compression is not necessarily the only option of exploiting data compressibility to improve memory lifetime.
We propose implicit data compression as an alternative to complement with explicit data compression.
With implicit data compression, we compress each data sector but do not increase the number of data sectors per flash memory page.
Therefore, implicit compression has no impact on FTL and/or file system but meanwhile does not improve storage efficiency either.
Second, for multi-bit per cell (e.g., MLC and TLC) flash memory, physical damage depends on a variety of factors (e.g., distribution characteristics of compressed data size, relative placement or layout of different pages on the same memory wordline), which have not been considered in prior work.This paper presents a thorough study on exploiting data compressibility to reduce physical damage and hence improve flash memory lifetime.
Since random read latency is one of the most important metrics of flash-based storage devices, this work assumes that each compressed data sector must reside entirely in one flash memory page.
As a result, each flash memory page almost always contains some unused storage space left by compressed data sectors.
Motivated by the content dependency of flash memory cell damage, we present a set of design strategies that can exploit the unused storage space within a flash memory page to reduce the overall memory damage, for both explicit and implicit data compression.
Then we derive a set of mathematical formulations for quantitatively estimating flash memory damage reduction gained by the proposed design strategies.
These rigorous mathematical formulations build a framework that directly links flash memory lifetime with data compressibility characteristics (e.g., mean and deviation of data compression ratio) and memory cell damage content dependency.
Using 20nm MLC NAND flash memory chips, we carried out experiments to quantitatively measure the content-dependent memory cell damage factors, based upon which we empirically evaluated and compared the effectiveness of the proposed design strategies with either explicit or implicit compression.
In summary, the main contributions of this work include:1.
We propose an implicit data compression strategy as a viable complement to conventional explicit data compression for exploiting data compressibility to improve flash memory lifetime;2.
A set of design strategies are developed to leverage the unused storage space left by data compression within flash memory pages to reduce the memory cell physical damage;3.
We derive a set of mathematical formulations to accurately estimate the flash memory damage based upon the characteristics of data compressibility and content-dependent memory cell damage;4.
We quantitatively compare explicit data compression and implicit data compression under a wide spectrum of runtime data compressibility characteristics and show that it is important to fully understand the data compressibility characteristics in order to choose the appropriate design strategy.Finally, we note that, although this work focuses on flash memory, the developed design strategies and mathematical formulations are readily applicable to other emerging memory technologies, e.g., PCM and ReRAM, that experience similar content dependency of memory cell physical damage.
This section presents a set of design strategies that can exploit data compressibility to reduce memory cell damage.
We first discuss the content dependency of cyclinginduced memory damage that motivates us to propose implicit compression (i.e., compress the data without increasing the number of sectors per flash memory page) in addition to the conventional explicit compression (i.e., compress the data and increase the number of sectors per page as much as possible).
We further present different strategies on laying out the compressed data within flash memory pages that aim to leverage the contentdependent memory damage phenomenon for improving flash memory lifetime.
We note that this work only focuses on MLC memory, and the discussions could be readily extended to the more complicated TLC case.
NAND flash memory handles data programming and read in page units with a typical size of 4kB or 8kB.
For high-density MLC and TLC memory, different bits within each MLC/TLC memory cell belong to different pages.
This can be illustrated in Fig. 1 for MLC flash memory, where the two bits within each memory cell belong to lower and upper pages, respectively.
NAND flash memory cells wear out with P/E cycling due to the oxide damage caused by the electrons that pass through the gate oxide during each P/E cycle.
Although current practice estimates the memory cell damage solely dependent upon the number of P/E cycles endured by memory cells, actual physical damage further depends on the data content being programmed to memory cells.
This can be intuitively explained using Fig. 1: different data content (e.g., '11', '10', '00', and '01') correspond to different number of electrons that pass through the gate oxide, and hence different amount of physical damage [14,15].
To further demonstrate such content dependency, we carried out experiments using 20nm MLC NAND flash memory chips.
To evaluate the effect of writing the content D test ∈{'11','10','00','01'}, we program each flash memory block with the pattern as shown in Fig. 2 Since each compressed sector resides entirely in one memory page, each page will have a certain amount of unused storage space.
This subsection first discusses how we should determine the content of the unused storage space to minimize the overall damage, then discusses different options of laying out the compressed data within flash memory pages.
For MLC NAND flash memory, each pair of lower and upper pages together determine the memory cell content and hence the flash memory damage.
To minimize the flash memory damage, we should appropriately determine the data content in the unused storage space left by data compression.
Let S (l) and S (u) denote the unused storage space in lower and upper page, and b l and b u denote the two bits in the same memory cell and belong to lower and upper page, respectively.
Recall that the memory cell damage caused by the content '11', '10', '00', and '01' monotonically increase (where the left bit and right bit resides in lower and upper page, respectively), as illustrated in Fig. 3.
Therefore, for each memory cell, we should apply the following rules to minimize flash memory damage:• If b l ∈ S (l) and b u ∈ S (u) (i.e., we can freely set the values of both bits), we set b l = b u = 1 hence the least harmful content '11' is written to the cell;• If b l ∈ S (l) and b u / ∈ S (u) (i.e., we can only freely set the value of b l ), we always set b l as '1' regardless to the value of b u ;• If b l / ∈ S (l) and b u ∈ S (u) (i.e., we can only freely set the value of b u ), we always set b u = b l .
Since the memory cells covered by S (l) or S (u) experience less damage than the other memory cells, we should keep shifting the location of S (l) and S (u) within flash memory pages in order to equalize the damage among all the memory cells.
We define a parameter l head to represent the location from where the compressed data are continuously stored in the lower and upper pages.
We should keep changing l head in order to equalize the memory cell damage.
Since the storage device FTL module always keeps track of the P/E cycles of each memory block, we can fix a relationship between l head and P/E cycle number, e.g., let L denote the memory page size and N P/E denote the P/E cycle number, we can calculatel head = 񮽙 񮽙t · N P/E 񮽙mod L 񮽙 ,where t is a fixed constant integer.
As a result, the storage device controller does not need to record the value of l head for each memory block.In addition, as decompression is a process that is done serially, the length of the compressed data need not be kept in the FTL.
For each compressed memory page, the decompression process can be terminated once the decompressed data length reaches the page length.
Therefore, in order to support the proposed design strategy, the only overhead at the FTL layer is to calculate the l head for each memory page.
For MLC NAND flash memory, there are two different options for laying out the compressed data in lower and upper pages.
As illustrated in Fig. 4, the first option is to lay out the compressed data towards the same direction from l head in both the lower and upper pages, that we refer is referred to as unidirectional data layout.
The other option is to lay out the compressed data towards opposite directions in the lower and upper pages, that we refer to as bidirectional data layout.
As shown in Fig. 4, all the memory cells can be categorized into three types: (1) In each type-I memory cell, both bits belong to the compressed data; (2) In each type-II memory cell, one bit belongs to the compressed data while the other bit belongs to the unused storage space; (3) In each type-III memory cell, both bits belong to the unused storage space.
Apparently, the physical damage experienced by type-I, type-II, and type-III memory cells monotonically reduces.
Compared with unidirectional data layout, bidirectional data layout leads to more type-II memory cells and less type-I and type-III memory cells.
According to the discussion in Section 2.2.1, the content of each type-II memory cell can only belong to {'11', '10'} or {'11', '00'} if the lower or upper page bit belongs to unused storage space.
As shown in Fig. 3, '10' causes less damage than '00'.
Hence, the memory damage tends to be less if the lower page has more unused storage space (i.e., data being stored in the lower page have better compressibility).
This observation directly motivates us to propose conditional data exchange: Let D (l) and D (u) denote the compressed data that have been originally arranged by the storage device FTL to store in one pair of lower and upper pages.
If the length of D (l) is not larger than that of D (u) (i.e., |D (l) | ≤ |D (u) |), This section presents the mathematical formulations that can accurately estimate flash memory physical damage reduction when using the design strategies presented in Section 2, for both explicit and implicit data compression.
It is evident that different types of data can have different compressibility characteristics.
With the popular LZ77 [16] compression algorithm and sector size of 4kB, Fig. 5 shows the per-sector compression ratio distribution for some common types of data.
The results show that the compression ratio tends to approximately follows a Gaussian distribution.
We carried out further experiments to verify the accuracy of such distribution approximation.
Fig. 5 shows the absolute difference (denoted as "Appr.
error" in the figure) between the exact distribution and the approximate distribution for different types of data.
The corresponding mean square errors (MSE) for these types of data are all at the magnitude of 10 −5 .
Therefore, we can conclude that such a Gaussianbased approximation is reasonable with almost negligible inaccuracy.
Therefore, to facilitate the mathematical derivation, we set that per-sector data compression ratio follows a Gaussian distribution in this work.
We first introduce a parameter, called normalized content-dependent damage factor, to quantify the impact of different content on memory cell damage.
Let BER max denote the maximum memory raw BER that can be tolerated by the storage device error correction mechanism.
max .
In this work, we define the content-dependent damage factor ρ i for each content i by normalizing with the average damage caused by random content, i.e.,ρ i = η (r) max η (i) max , where i ∈ [0, 2 l − 1],(1)and hence the damage factor ρ r for random content is 1.
Using the measurement results shown in Fig. 3 as an example, assume the BER max is 5 × 10 −3 , we calculate the four damage factors as ρ 11 = 0.33, ρ 10 = 0.69, ρ 00 = 1.01, and ρ 01 = 1.58.
We first derive the mathematical formulations for estimating the distribution characteristics of the compressed data and unused storage space size in each page.
Let C s denote the size of each uncompressed data sector (e.g., 4kB), m s denote the number of uncompressed sectors in each page, and C p = m s · C s denote the size of each flash memory page (e.g., 8kB).
As pointed out above, the per-sector compression ratio x approximately follows a Gaussian distribution N(µ, σ 2 ).
Let m C (e) s = ∞ ∑ m (e) s =m s x · m (e) s ·C s · P 񮽙 m (e) s 񮽙 ,(2)P 񮽙 m (e) s 񮽙 = P 񮽙 x · m (e) s ≤ m s < x · 񮽙 m (e) s + 1 񮽙񮽙 = P 񮽙 x · m (e) s ≤ m s 񮽙 · 񮽙 1 − P 񮽙 x · 񮽙 m (e) s + 1 񮽙 ≤ m s 񮽙񮽙 .
Since x ∼ N(µ, σ 2 ), we have that x · m (e) s and x · 񮽙 m (e) s + 1 񮽙 follow N 񮽙 µm (e) s , (σ m (e) s ) 2 񮽙 and N 񮽙 µ(m (e) s + 1), (σ (m (e) s + 1)) 2 񮽙 , respectively.
Hence, P 񮽙 x · m (e) s ≤ m s 񮽙 and P 񮽙 x · 񮽙 m (e) s + 1 񮽙 ≤ m s 񮽙 is the CDF (cumulative distribution function) for the random variant x · m (e) s and x · 񮽙 m (e) s + 1 񮽙 .
Accordingly, we have that P 񮽙 m (e) s 񮽙 = 񮽙 1 + er f 񮽙 m s −m (e) s µ σ m (e) s √ 2 񮽙񮽙 · 񮽙 1 − er f 񮽙 m s −(m (e) s +1)µ σ (m (e) s +1) √ 2 񮽙񮽙 /4 ,(3)where er f (z) is the error function for Gaussian distribution, i.e., er f (z) = 1 √ π 񮽙 z −z e −t 2 dt.s ∼ N 񮽙 µ c (e) s , σ 2 c (e) s , where         µ c (e) s = µC s · ∑ m (e) s m (e) s P 񮽙 m (e) s 񮽙 , σ 2 c (e) s = (σC s ) 2 · ∑ m (e) s 񮽙 m (e) s P 񮽙 m (e) s 񮽙񮽙 2 .
(4)When using implicit compression, the number of compressed sectors per flash memory page always remains as m s and the length of compressed data per page is C(i) s = x · m s · C s .
Therefore, we have that the random variable C (i) s ∼ N 񮽙 µm s C s , σ 2 m 2 s C 2 s 񮽙 .
We further derive the mathematical formulations for calculating average memory cell damage per P/E cycle.
Based upon the above discussions, we should consider four different design scenarios: (1) UD: unidirectional data layout without conditional data exchange, (2) BD: bidirectional data layout without conditional data exchange, (3) UDC: unidirectional data layout with conditional data exchange, (4) BDC: bidirectional data layout with conditional data exchange.
Since the mathematical formulations can be derived with the same principle for all the scenarios, we first show the mathematical derivation in detail for UD (i.e., unidirectional data layout without conditional data exchange) and then present the results for the others without detailed derivations.
We first define two parameters x l and x u as the ratios between the compressed data size and flash memory page size for lower and upper pages, respectively.
Recall that both C .
Therefore, we can calculate the average memory cell damage per P/E cycle for the UD design scenario, which is normalized against the case of without using compression, asρ UD = 1 r 񮽙 z l + |x l − x u | ρ 00 + 2ρ 11 + ρ 10 4 + (1 − z u )ρ 11 񮽙 = 1 r 񮽙 1 − ρ 00 + 2ρ 11 + ρ 10 4 񮽙 z l + 1 r 񮽙 ρ 00 + ρ 10 − 2ρ 11 4 񮽙 z u + ρ 11 r = λ l UD r · z l + λ u UD r · z u + ρ 11 r ,(5)whereλ l UD = 1 − ρ 00 + 2ρ 11 + ρ 10 4 , λ u UD = ρ 00 + ρ 10 − 2ρ 11 4 .
In order to obtain the distribution of ρ UD , we must derive the distributions of z u and z l .
The CDF of z u can be written asF z u (z) = P(x l ≤ z, x u ≤ z) = P(x l ≤ z) · P(x u ≤ z) = F x l (z) · F x u (z),where F x l and F x u denote the CDF of x u and x l .
Since x u and x l follow the same Gaussian distribution (denoted as f N ), we have that F x l = F x u .
By taking the derivative of the CDF, we can obtain the PDF of z u asf z u (z) = F 񮽙 z u (z) = f N (z) · 񮽙 1 + er f 񮽙 z − ˜ µ √ 2 ˜ σ 񮽙񮽙 ≈ f N (z) · 񮽙 1 + z − ˜ µ √ 2 ˜ σ 񮽙 .
(6)Hence, f z u can be approximately expressed as the product of the PDF of a Gaussian distribution and a straight line with the slope of√ 2 ˜ σ .
Since z ∈ (0, 1], we could further approximate f z u to a PDF of a Gaus- sian distribution, i.e., z u ∼ N 񮽙 µ z u , σ 2 z u 񮽙 and f z u (z) = 1 σ zu √ 2π exp 񮽙 − (z−µ zu ) 2 2σ 2 zu .
The value of µ z u and σ z u can be obtained by solving       d( f zu (z)) dz 񮽙 񮽙 񮽙 z=µ zu ≈ d 񮽙 f N (z)· 񮽙 1+ z− ˜ µ √ 2 ˜ σ 񮽙񮽙 dz 񮽙 񮽙 񮽙 񮽙 񮽙 z=µ zu = 0, f z u (z)| z=µ zu ≈ f N (z) · 񮽙 1 + z− ˜ µ √ 2 ˜ σ 񮽙񮽙 񮽙 񮽙 z=µ zu .
(7)Accordingly, we have that   µ z u = ˜ µ + √ 6− √ 2 2 · ˜ σ , σ z u = 2 √ 2 √ 6+ √ 2 · ˜ σ · exp 񮽙 ( √ 6− √ 2) 2 8 񮽙 .
(8)We can obtain the PDF of z l in similar manner.
First, we can express the CDF of z l asF z l (z) = 1 − P (x l > z, x u > z) = 1 − (1 − P(x l ≤ z)) · (1 − P (x u ≤ z)) = 1 − (1 − F N (z)) 2 .
By taking the derivative of F z l , we obtain the PDF of z l asf z l (z) = 2 (1 − F N (z)) · f N (z) .
(9)Similar to the above derivations for the case of z u , we can approximate the PDF f z l as a Gaussian distribution To justify the Gaussian approximation of f z u (z) and f z l (z) (i.e., the PDF of z u and z l ) in the above derivations, Fig. 6 compares the Gaussian approximation and the exact PDF, where we considered three different sets of { ˜ µ, ˜ σ } (i.e., {0.2, 0.05}, {0.5, 0.1}, and {0.8, 0.02}, respectively) to cover a wide range of the compressed data length ratio and deviations.
As clearly shown in Fig. 6, the Gaussian approximation of f z u (z) and f z l (z) incurs almost negligible inaccuracy.N 񮽙 µ z l , σ 2 z l 񮽙 , where    µ z l = ˜ µ − √ 6− √ 2 2 · ˜ σ , σ z l = 2 √ 2 √ 6+ √ 2 · ˜ σ · exp 񮽙 ( √ 6− √ 2) 2 8 񮽙 .
(10)Since z l ∼ N 񮽙 µ z l , σ񮽙 µ UD = 1 r 񮽙 λ l UD · µ z l + λ u UD · µ z u + ρ 11 񮽙 , σ UD = 1 r 񮽙 񮽙 λ l UD · σ z l 񮽙 2 + 񮽙 λ u UD · σ z u 񮽙 2 .
(11)Given the same data compressibility, the use of implicit and explicit compression leads to different distribution of x u and x l , and different r, leading to different memory cell damage.
Using the same principle, we can derive the mathematical formulations that can calculate the normalized average memory cell damage per P/E cycle for the other three design scenarios.
Due to the page limit, we will directly present the final mathematical formulations without showing the derivation details.For the BD design scenario that uses bidirectional data layout without conditional data exchange, its average memory cell damage isρ (BD) ∼ N 񮽙 µ (BD) , σ 2(BD)񮽙 : 񮽙 :   µ (BD) = 1 r 񮽙񮽙 λ l (BD) + λ u (BD) 񮽙 · ˜ µ +C (BD) 񮽙 , σ (BD) = 1 r 񮽙 (λ l (BD) ) 2 + (λ u (BD) ) 2 · ˜ σ .  
µ (UDC) = 1 r 񮽙 λ l (UDC) · µ z l + λ u (UDC) · µ z u + ρ 11 񮽙 , σ (UDC) = 1 r 񮽙 (λ l (UDC) · σ z l ) 2 + (λ u (UDC) · σ z u ) 2 .
whereλ l (UDC) = 1 − ρ 11 + ρ 10 2 , λ u (UDC) = ρ 11 + ρ 10 2 − ρ 11 .
For the BDC design scenario that uses bidirectional data layout with conditional data exchange, its average memory cell damage isρ (BDC) ∼ N 񮽙 µ (BDC) , σ 2(BDC)񮽙 :   µ (BDC) = 1 r 񮽙 λ l (BD) · µ z l + λ u (BD) · µ z u +C (BD) 񮽙 , σ (BDC) = 1 r 񮽙 (λ l (BD) · σ z l ) 2 + (λ u (BD) · σ z u ) 2 .
This subsection discusses how we estimate the flash memory lifetime improvement based upon the average memory cell damage derived in the above section.
In this work, we assume ideal wear-leveling, i.e., all the memory blocks always experience the same number of P/E cycles, and quantitatively define memory lifetime as the P/E cycle number that one memory block can survive before reaching the maximum allowable BER.
Since it is common practice to use capacity overprovisioning in flash-based storage devices, we define an over-provisioning factor τ ≥ 1, i.e., the total physical storage capacity inside the storage device is τ× larger than the storage capacity visible to the host.
Let η denote the memory block P/E cycling endurance of the baseline scenario without using any data compression.
Straightforwardly, the overall memory lifetime of the baseline scenario is τ · η cycles.
Once data compression is used, the average memory cell damage becomes a random variable with a Gaussian distribution due to the Gaussian-like distribution of runtime data compression ratio.
As a result, the cycling endurance of each memory block and hence overall memory lifetime also become random variables.
Let P (t) b denote the probability that one memory block can survive (i.e., can ensure the storage integrity even for incompressible data) after t P/E cycles, referred to as memory block survival probability.
As the granularity of data erasure is in memory block units in NAND Flash memory, the number of P/E cycles is independent among memory blocks.
Hence, the survival of memory blocks is independent.
Let N denote the number of memory blocks visible to the host, then the storage device contains τ · N memory blocks in total.
Therefore, once P (t) b is known, based on the law of total probability, we can calculate the probability that the storage device can survive t cycles asSP (t) = (τ−1)N ∑ k=0 񮽙񮽙 Nτ k 񮽙 · 񮽙 P (t) b 񮽙 Nτ−k · 񮽙 1 − P (t) b 񮽙 k 񮽙 ,(12)which is called storage device survival probability.
Suppose each memory block contains M wordlines and let P (t) wl denote the survival probability of one wordline, we have that P(t) b = 񮽙 P (t) wl񮽙 M , i.e., one memory block survives only when all the wordlines inside this block survive.
In the following, we will discuss how we can estimate the memory wordline survival probability P (t)wl .
For the baseline scenario without using data compression, the storage device fails to survive once the accumulated average damage of each memory cell reaches η · ρ r (recall that ρ r = 1 is the normalized memory cell damage factor when storing random data).
When using data compression, let ρ w denote the memory cell damage per cycle, where ρ w could be ρ (UD) , ρ (BD) , ρ (UDC) , or ρ (BDC) dependent upon the design strategies being used.
By setting η · ρ r as the maximum tolerable accumulated memory cell damage, we can express the P/E cycling endurance of each wordline asT = max (t) , t · ρ w ≤ η · ρ r − ρ r .
(13)Since ρ w follows Gaussian distribution, t · ρ w also follows Gaussian distribution with mean of t · µ ρ w and variance of t 2 · σ 2 ρ w .
Therefore, we can calculate the wordline survival probability P (t) wl at t cycles asP (t) wl = 1 2 񮽙 1 + er f 񮽙 τ · η − 1 − t · µ ρ w tσ ρ w 񮽙񮽙 ,(14)where µ ρ w and σ ρ w can be obtained using the formulations presented above for the four different design scenarios.
With the formulations derived in Section 3, we studied the effectiveness of the design strategies presented in Section 2 for both explicit and implicit data compression.
Based upon our measurement results with 20nm MLC NAND flash memory chips, we set the damage factors ρ 11 = 0.33, ρ 10 = 0.69, ρ 00 = 1.01, and ρ 01 = 1.58, as discussed in Section 3.1.
As shown in (12), the storage device survival probability SP (t) depends on the overprovisioning factor τ and the total number of memory blocks N visible to the host.
Assume the storage capacity of 512GB visible to the host and a block size of 4MB, we have N equals 128k.
Each flash memory page has a size of 8kB, and we set the data sector size as 4kB.
We further set the over-provisioning factor τ as 1.2.
Based upon the memory chip measurement results and the overprovisioning factor of 1.2, we set the cycling endurance of the baseline scenario (i.e., without using data compression) as 8000.
Using the measured compression ratio distribution of different data types as shown in Fig. 5, we evaluated the effectiveness of the developed design strategies on improving memory lifetime over the baseline scenario.
Fig. 7 shows the results when using the four different design scenarios.
Recall that the design scenario UD uses unidirectional data layout without conditional data exchange, BD uses bidirectional data layout without conditional data exchange, UDC uses unidirectional data layout with conditional data exchange, BDC uses bidirectional data layout with conditional data exchange.
For the baseline scenario, the storage device lifetime remains 8000 regardless to the data types.
When using data compression with different design strategies, the storage device lifetime becomes a random variable, whose CDF (i.e., its survival probability) is calculated according to the formulations derived in Section 3.
As shown in Fig. 7, explicit compression always outperforms implicit compression, which can be intuitively justified because explicit compression always tries to fit as many sectors as possible into each flash page.
By comparing the data compressibility shown in Fig. 5 and the results shown in Fig. 7, we can clearly see that the difference between explicit compression and implicit compression strongly relies on the data compressibility.
The higher data compressibility is, the larger difference between explicit compression and implicit compression is.
In addition, the BDC design scenario always performs the best under both explicit and implicit data compression.When explicit compression is being used, the difference among different design strategies tends to diminish for data with better compressibility (e.g., LOG and HTML).
This can be explained as follows.
With highly compressible data, explicit compression can fit more compressed data and hence leave less unused storage space within each flash memory page.
As a result, there is a smaller room for these different design strategies to exploit the unused storage space, leading to almost the same storage device lifetime.
On the other hand, when implicit compression is being used, unidirectional data layout and bidirectional data layout tend to have noticeable different effect, especially for data with better compressibility.
As pointed out in Section 2.2.2, compared with bidirectional data layout, unidirectional data layout leads to more cells with random data content and '11'.
Although '11' causes the least memory cell damage, random data tend to cause relatively large damage, as shown in Fig. 3.
Based upon the content-dependent damage factors measured from our 20nm MLC flash memory chips, the penalty of having more random data can noticeably off-set the gain of having more '11'.
As a result, unidirectional data layout tends to be inferior to bidirectional data layout.
In the case of implicit compression, for data with worse compressibility (e.g., DLL and EXE ), most memory cells would store random data content in both unidirectional and bidirectional data layout.
As a result, bidirectional data layout will be inferior to unidirectional data layout in this scenario, which is shown in Fig. 7(a) and (c).
Meanwhile, as shown in the results, the benefit of using conditional data exchange is not significant.
Conditional data exchange aims to convert memory cell content from '00' to '10', since '10' causes less damage than '00'.
Nevertheless, as shown in Fig. 3, the damage difference between '00' and '10' is not significant, which explains the low effectiveness of conditional data exchange observed in our study.
The above results are based upon the measured compressibility characteristics of several different types of data.
To more thoroughly elaborate on the impact of data compressibility, we carried out further evaluations by considering a much wider range of data compressibility in terms of compression ratio mean and standard deviation.We first fix the data compression ratio standard deviation as 0.01, and Fig. 8 shows the corresponding storage device survival probability vs. lifetime for a wide range of compression ratio mean from 0.1 to 0.9.
Data with better compressibility (i.e., smaller compression ratio mean) lead to larger lifetime improvement in both explicit compression and implicit compression.
In addition, the advantage of explicit compression over implicit compression increases as the data have better compressibility.
At the compression ratio mean of 0.1 (i.e., the data can be compressed by 10:1 on average), explicit and implicit compression can improve the storage device lifetime by 9.6 and 4.8 times, respectively.
As shown in Fig. 8, for less compressible data (e.g., with the compression ratio mean of 0.7 and higher), explicit and implicit compression have almost the same effect.
This is because, with low data compressibility, explicit compression can hardly increase the number of compressed data sectors per page.
The results more clearly reveal the observations discussed above in Section 4.1: Under explicit compression, the difference between different de- sign strategies quickly shrinks as we reduce the compression ratio mean; Under implicit compression, bidirectional data layout is always noticeably more beneficial than unidirectional data layout.
By setting the storage device lifetime as the P/E cycles corresponding to 99.9% of storage device survival probability, Fig. 9 further plots the storage device lifetime gain over the baseline scenario without using compression under different compression ratio mean.Next, we examined the impact of data compression ratio standard deviation.
With the compression ratio mean of 0.5, Fig. 10 shows the storage device survival probability vs. P/E cycles when the compression ratio standard deviation varies from 0.01 to 0.14.
As the data compression ratio standard deviation increases, advantage of explicit compression over implicit compression becomes more significant, and the storage device lifetime improvement generally reduces.
In addition, the difference among the four different design scenarios reduces as the compression ratio standard deviation increases, for both explicit and implicit compression.
Again, the design scenario of BDC is the most effective for both explicit and implicit compression.By setting the storage device lifetime as the P/E cycles corresponding to 99.9% of storage device survival probability, Fig. 11 further shows the storage device lifetime gain over the baseline scenario under different compression ratio standard deviation.
It shows that the lifetime gain monotonically reduces as we increase the data compression ratio standard deviation for implicit data compression.
Nevertheless, for explicit data compression, the storage device lifetime gain first reduces and then saturates and even slightly increases as we increase the compression ratio standard deviation.
The figure more clearly reveals the dependency of comparison between explicit and implicit compression on compression ratio standard deviation.
The above quantitative studies show that the proposed implicit data compression is a viable complement to the conventional explicit data compression.
Although explicit data compression may noticeably complicate the design of FTL and/or OS, it always outperforms implicit data compression from the storage device lifetime perspective.
Nevertheless, the advantage of explicit compression over implicit compression strongly depends on the data compressibility.
As shown in the above evaluation results, the advantage of explicit compression over implicit compression reduces as the data compressibility drops, and becomes very small as the data compression ratio mean becomes sufficiently large (e.g., over 0.6∼0.7 in this study), particularly when the data compression ratio has a small standard deviation.
Our studies show that the bidirectional data layout outperforms the unidirectional data layout, especially when using the implicit data compression.
Nevertheless, we should emphasize that this conclusion may not be always true.
As pointed out above, compared with bidirectional data layout, unidirectional data layout result in more memory cells with random data content and '11'.
Hence, which data layout option is better is fundamentally dependent on the exact values of the contentdependent damage factors.
In this work, we extracted the content-dependent damage factors based upon measurements with 20nm MLC flash memory chips.
However, for further scaled technology nodes such as 16nm or the emerging 3D flash memory, content-dependent damage factors and their relative comparison may (largely) change.
This could essentially change the conclusion on the comparison between unidirectional data layout and bidirectional data layout.
In addition, the above results suggest that the design strategy of conditional data exchange is not very effective, which is again also essentially due to the content-dependent damage factors being used in this work.
For MLC NAND flash memory, the conditional data exchange will become more effective if the damage factors of '10' and '00' have a larger difference in future memory technology nodes.Therefore, when applying the developed design framework in practice, one should carry out sufficient measurements and experiments to fully understand the content dependency of NAND flash memory damage and runtime data compressibility characteristics, in order to determine the most appropriate design strategy for leveraging data compressibility to improve device lifetime.
Prior work [17][18][19] has studied the practical implementation of data compression in flash-based data storage systems, aiming to improve the storage system I/O speed performance and flash memory lifetime.
In [17], a blocklevel compression engine is devised to support on-line compression for SSD-based cache, which is transparent to the file system.
The authors of [18] develop a compression-aware FTL that can support compressionaware address mapping and garbage collection.
The authors of [19] implement a caching system with commodity SSD by integrating data compression and data deduplication.
All the prior work aimed to explicitly improve the storage efficiency, like the explicit data compression scenario being considered in this work.
Besides data compression, prior work [20,21] also investigated the practical implementation of data deduplication in flashbased storage systems.FTL plays an important role in determining the lifetime of flash-based data storage devices, hence it has been well studied.
The wear-leveling function in FTL aims to equalize the physical damage among all the flash memory block by appropriately allocating the memory blocks for erase and programming.
A variety of techniques have been proposed to optimize the design of the wear-leveling function (e.g., see [10,22,23]).
Aiming to reduce the write amplification and hence improve flash memory lifetime, the garbage collection function in FTL has been well studied (e.g., see [24]).
The log-structured approach to managing flash memory have been considered through direct management of raw flash memory chips [11] or by facilitating the operation of the FTL inside SSDs [13].
Such log-structured file system level management of memory chips lead to improved flash memory lifetime and storage system performance.The strength of fault tolerance, in particular ECC, also largely affect the storage device lifetime.
Although classical BCH codes are still widely used in commercial flash-based storage devices [25,26], the more powerful LDPC codes are receiving significant attention from the industry (e.g., see several industrial presentations at recent Flash Summit [2,3,27,28]).
A variety of techniques [29][30][31][32] have been developed to optimize the implementation of LDPC codes in future flash-based data storage devices.
This paper presents a thorough study on exploiting data compressibility to reduce cycling-induced flash memory cell physical damage and hence improve storage device lifetime.
This work is essentially motivated by the content dependency of flash memory cell damage.
We first present an unconventional implicit data compression strategy as a viable complement to explicit data compression being used in current practice, both of which represent different trade-offs between flash memory lifetime improvement and impact on FTL and system design complexity.
In addition, their effectiveness and comparison largely vary with the runtime data compressibility characteristics.
We further develop a set of design strategies that can exploit the unused storage space left by data compression within flash memory pages in order to minimize the overall memory physical damage.
Furthermore, we derive a set of mathematical formulations that can quantitatively estimate the effectiveness of the proposed design strategies.
Using 20nm MLC NAND flash memory chips, we carried out experiments to empirically evaluate the content dependency of flash memory cell damage.
Employing these quantized experimental results, we compare the effectiveness of the proposed design strategies when using either explicit or implicit compression.
Although this work focuses on flash memory, the proposed design strategies and developed mathematical formulations are readily applicable to other emerging memory technologies, e.g., PCM and ReRAM, that experience similar content dependency of memory cell damage.
We would like to thank our shepherd Sam H. Noh and the anonymous reviewers for their insight and suggestions for improvement.
This work was supported by the National Science Foundation under Grants No. 1162152 and 1406154, National Science Foundation CAREER award CNS-125394, and the Department of Defense award W911NF-13-1-0157.
