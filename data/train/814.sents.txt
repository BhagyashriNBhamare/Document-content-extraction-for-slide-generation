We present a new algorithm for probabilistic planning with no observability.
Our algorithm, called Probabilistic-FF, extends the heuristic forward-search machinery of Conformant-FF to problems with probabilistic uncertainty about both the initial state and action effects.
Specifically, Probabilistic-FF combines Conformant-FF's techniques with a powerful machinery for weighted model counting in (weighted) CNFs, serving to elegantly define both the search space and the heuristic function.
Our evaluation of Probabilistic-FF on several prob-abilistic domains shows an unprecedented, several orders of magnitude improvement over previous results in this area.
In this paper we address the problem of probabilistic planning with no observability ( Kushmerick et al. 1995), also known in the AI planning community as conditional (Ma- jercik & Littman 2003) or conformant (Hyafil & Bacchus 2004) probabilistic planning.
In such problems we are given an initial belief state in the form of a probability distribution over the world states W , a set of actions having (possibly) probabilistic effects, and a set of alternative goal states W G ⊆ W .
A solution to such a problem is a single sequence of actions that transforms the system into one of the goal states with probability exceeding a given threshold θ.
The basic assumption of the problem is that the system cannot be observed at the time of plan execution.
Such a setting is useful in controlling systems with uncertain initial state and non-deterministic actions, if sensing is expensive or unreliable.
Non-probabilistic conformant planning may fail due to non-existence of a plan that achieves the goals with 100% certainty.
Even if there is such a plan, that plan does not necessarily contain information about what actions are most useful to achieve (only) the requested threshold θ.The two most prominent steps made in the direction of conformant probabilistic planning are the probabilistic extensions of partial-order planning ( Kushmerick et al. 1995), and to fixed-length planning with problem reformulation into either probabilistic SAT (Majercik & Littman 1998) or probabilistic CSP (Hyafil & Bacchus 2004).
The state-ofthe-art performance of probabilistic planners has been ad- vancing much more slowly than that of deterministic planners, scaling from 5-10 step plans for problems with ≈20 world states to 15-20 step plans for problems with ≈100 world states.
Since probabilistic planning is inherently harder than its deterministic counterpart (Littman, Gold- smith, & Mundhenk 1998), such a difference in evolution rates is not surprising.
Yet, here we show that dramatic improvements in probabilistic planning can be obtained.We introduce Probabilistic-FF, a new probabilistic planner based on heuristic forward search in the space of implicitly represented probabilistic belief states.
The planner is based on combining the idea of lazy CNF-based belief state representation introduced in the (non-probabilistic) conformant planner Conformant-FF (Brafman & Hoffmann 2004) with recent techniques for probabilistic reasoning using weighted model counting (WMC) in propositional CNFs (Sang, Beame, & Kautz 2005).
This synergetic combination allows us to elegantly extend both Conformant-FF's search space and heuristic function to the probabilistic setting.The algorithms we present cover probabilistic initial belief states given as Bayes networks, deterministic and probabilistic actions, conditional effects, and standard action preconditions.
By the time of submission of this paper, our ongoing implementation supports probabilistic belief states, as well as deterministic actions with preconditions and conditional effects.
Implementing the presented techniques for probabilistic effects will be our next step.
So far, we can offer stunning results in domains with complex probabilistic initial states: in contrast to the SAT and CSP based approaches mentioned above, Probabilistic-FF can find 100-step plans for instances with billions of world states.
This success is based on consequent exploitation of problem structure, through our belief state representation and heuristic function -both of which are heavily based on the synergy between WMC and Conformant-FF's CNF-based techniques.
In fact, while without probabilities one could imagine to successfully replace the CNFs with BDDs, with probabilities, this seems hopeless.We detail our planning model, then in turn describe the belief state representation by Bayes networks, the encoding of Bayes networks as weighted CNFs, and the heuristic function; we give our empirical results, and conclude.
The probabilistic planning framework we consider adds probabilistic uncertainty to a subset of the classical ADL language, namely (sequential) STRIPS with conditional effects.
Such STRIPS planning tasks are described over a set of propositions X as triples (A, I, G), corresponding to the action set, initial world state, and goals.
I and G are sets of propositions, where I describes a concrete initial state w I , while G describes the set of goal states w ⊇ G. Actions a are pairs (pre(a), E(a)) of the precondition and the (conditional) effects.
A conditional effect e is a triple (con(e), add(e), del(e)) of (possibly empty) proposition sets, corresponding to the effect's condition, add, and delete lists, respectively.
The precondition pre(a) is also a proposition set, and an action a is applicable in a world state w if w ⊇ pre(a).
If a is not applicable in w, then the result of applying a to w is undefined.
If a is applicable in w, then all conditional effects e ∈ E(a) with w ⊇ con(e) occur.
Occurrence of a conditional effect e in w results in the world state w − del(e) + add(e).
Our probabilistic planning setting extends the above with (i) probabilistic uncertainty about the initial state, and (ii) an additional type of action having probabilistic effects.
1 In general, probabilistic planning tasks are quadruples (A, P I , G, θ), corresponding to the action set, initial belief state, goals, and goal satisfaction probability.
As before, G is a set of propositions.
The initial state is no longer assumed to be known precisely.
Instead, we are given a probability distribution over the world states, P I , where P I (w) describes the likelihood of w being the initial world state.
The action set A consists of two different types of actions.
A d ⊆ A is a set of deterministic actions that have precisely the same structure and semantics as the actions in STRIPS with conditional effects.
A p ⊆ A is a set of probabilistic actions.
Similarly to A d , actions a ∈ A p are pairs (pre(a), E(a)), but the effect set E(a) for such a has different structure and semantics.
Each probabilistic effect e ∈ E(a) is a pair (con(e), Λ(e)) of a propositional condition and a set of probabilistic outcomes.
Each probabilistic outcome ε ∈ Λ(e) is a triplet (p(ε), add(ε), del(ε)), where add and delete lists are as before, and p(ε) is the probability that outcome ε occurs as a result of effect e. Naturally, we require that probabilistic effects define probability distributions over their outcomes, that is, ε∈Λ(e) p(ε) = 1.
Without loss of generality, we assume that the conditions of probabilistic effects E(a) are mutually exclusive and exhaustive (Kushmerick, Hanks, & Weld 1995).
Unconditional probabilistic actions are modeled as having a single probabilistic effect e with con(e) = ∅.
As before, if a is not applicable in w, then the result of applying a to w is undefined.
If a is applicable in w, then there exists exactly one effect e ∈ E(a) such that con(e) ⊆ w, and for each ε ∈ Λ(e), applying a to w results in w + add(ε) − del(ε) with probability p(ε).
Considering the initial belief state, practical considera-1 Our formalism covers the features of the previously proposed languages for conformant probabilistic planning ( Kushmerick et al. 1995, Majercik & Littman 1998, Hyafil & Bacchus 2004).
tions force us to limit our attention only to compactly representable probability distributions P I .
While there are numerous alternatives for compact representation of structured probability distributions, Bayes networks (BNs) (Pearl 1988) to date is by far the most popular such representation model.
2 As excellent introductions to BNs abound (e.g., see (Jensen 1996)), it suffices to briefly define our notation.
A BN N = (G, P) represents a probability distribution as a directed acyclic graph G, where its set of nodes V stands for random variables (assumed discrete in this paper), and P, a set of tables of conditional probabilities (CPTs) -one table T X for each node X ∈ V .
For each possible value x ∈ Dom(X) (where Dom(X) denotes the domain of X), the table T X lists the probability of the event X = x given each possible value assignment to all of its immediate ancestors (parents) P a(X) in G. Thus, the table size is exponential in the in-degree of X. Usually, it is assumed either that this in-degree is small (Pearl 1988) or that the probabilistic dependence of X on P a(X) induces a significant local structure allowing a compact representation of T X ( Boutilier et al. 1996).
We assume that the initial belief state P I is described by a BN N I over our set of propositions X .
In Probabilistic-FF we allow N I to be described over the true multi-valued variables underlying our problem.
This significantly simplifies the process of specifying N I since the STRIPS propositions X do not correspond to the true random variables underlying problem specification.
3 Specifically, let k i=1 X i be a partition of X such that each proposition set X i uniquely corresponds to a multi-valued variable underlying our problem.
That is, for every world state w and every X i , if |X i | > 1, then there is exactly one proposition q ∈ X i that holds in w.
The variables of the BN N I describing our initial belief state P I are V = {X 1 , . . . , X k }, where Dom(X i ) = X i if |X i | > 1, and Dom(X i ) = {q, ¬q} if X = {q}.
It is not hard to see that the semantics of the actions a ∈ A can be specified in terms of V in a straightforward manner.
If |X i | > 1, then no action a can add a proposition q ∈ X i without deleting some other proposition q ∈ X i (and vice versa).
Thus, we can consider a as setting X i = q.
If |X i | = 1, then adding and deleting q ∈ X i has the standard semantics of setting X i = q and X i = ¬q, respectively.Finally, since our actions transform (probabilistic) belief states to belief states, achieving G with certainty is typically unrealistic.
Hence, θ specifies the required lower bound on probability of achieving G.
A sequence of actions a is called a plan if we have P a (G) ≥ θ for the belief state P a , obtained by applying a to the initial belief state P I .
Example 1 Say we have a robot and a block that physically can be at one of two locations.
This information is captured by the propositions rL 1 , rL 2 for the robot, and bL 1 , bL 2 for the block, respectively.
The robot can either move from one location to another, or do it while carrying the block.
If the robot moves without the block, then its move is guaranteed to succeed.
This provides us with a pair of (symmetrically defined) deterministic actions A d = {move-right, move-lef t}.
The action move-right has an empty precondition, and a single conditional effect e with con(e) = {rL 1 }, add(e) = {r L2 }, and del(e) = {rL 1 }.
If the robot tries to move while carrying the block, then this move succeeds with probability 0.7, while with probability 0.2 the robot ends up moving without the block, and with probability 0.1 this move of the robot fails completely.
This provides us with a pair of (again, symmetrically defined)probabilistic actions A p = {move-b-right, move-b-lef t}.
The action move-b-right has an empty precondition, and two conditional effects specified as follows:E(a) con(e) Λ(e) p(ε) add(ε) del(ε) ε1 0.7 {rL2, bL2} {rL1, bL1} e rL1 ∧ bL1 ε2 0.2 {rL2} {rL1} ε3 0.1 ∅ ∅ e ¬rL1 ∨ ¬bL1 ε 1 1.0 ∅ ∅Now, say the robot is known to be initially at one of the two possible locations with probability P (rL 1 ) = 0.9 and P (rL 2 ) = 0.1.
Suppose there is a correlation in our belief about the initial locations of the robot and the block.
We believe that, if the robot is at rL 1 , then P (bL 1 ) = 0.7 (and P (bL 2 ) = 0.3), while if the robot is at rL 2 , then P (bL 1 ) = 0.2 (and P (bL 2 ) = 0.8).
The initial belief state BN is defined over two variables R ("robot") and B ("block") with Dom(R) = {rL 1 , rL 2 } and Dom(B) = {bL 1 , bL 2 }, respectively, and it is depicted in Figure 1.
Probabilistic-FF constitutes a forward search in belief space.
The search states are belief states (that is, probability distributions over the world states w), and the search is restricted to belief states reachable from the initial belief state P I through some sequences of actions a.
A key decision one should make is the actual representation of the belief states.
Let P I be our initial belief state captured by N I , and let P a be a belief state resulting from applying to P I a sequence of actions a.
One of the well-known problems in the area of decision-theoretic planning is that the description of P a in terms of only X (that is, V) is getting less and less structured as the number of actions in a increases.
To overcome this limitation, we represent belief states P a as a BN N a that explicitly captures sequential application of a starting from P I .
Below we formally specify the structure of such a BN N a , assuming that all the actions a are applicable in the corresponding belief states of their application (and later showing that Probabilistic-FF makes sure this is indeed the case.)
Figure 2 illustrates this construction of N a on our running example with a = move-b-right, move-lef t.rL 1 rL 2 0.9 0.1 rL 1 rL 2 ε 1 ∨ ε 2 0 1 ε 3 ∨ ε 1 rL 1 1 0 rL 2 0 1 rL 1 rL 2 rL 1 1 0 rL 2 1 0 GFED @ABC R (0) / / & & N N N N N N N GFED @ABC R (1) / / GFED @ABC R (2) Y (1) 6 6 n n n n n n n n ( ( P P P P P P P P ε 1 ε 2 ε 3 ε 1 rL 1 ∧ bL 1 0.7 0.2 0.1 0 othrw 0 0 0 1 GFED @ABC B (0) / / 8 8 p p p p p p p GFED @ABC B (1) / / GFED @ABC B (2) bL 1 bL 2 rL 1 0.7 0.3 rL 2 0.2 0.8 bL 1 bL 2 ε 1 0 1 ¬ε 1 bL 1 1 0 bL 2 0 1 bL 1 bL 2 bL 1 1 0 bL 2 0 1Figure 2: Bayes network N a for our running example.Let a = a 1 , . . . , a m be a sequence of actions, numbered according to their appearance on a. For 0 ≤ t ≤ m, let V (t) be a replica of our state variables V, with X (t) ∈ V (t) corresponding to X ∈ V.
The variable set of N a is the union of V (0) , . . . , V (m) , plus some additional variables that we introduce for the probabilistic actions in a.First, for each X (0) ∈ V (0) , we set the parents P a(X (0) ) and conditional probability tables T X (0) to simply copy these of X in N I .
Now, suppose that a t is a deterministic action a ∈ A d , and let E X (a) ⊆ E(a) be the conditional effects of a that add and/or delete propositions associated with the domain of a variable X ∈ V.
If E X (a) = ∅, then we set P a(X (t) ) = {X (t−1) }, andT X(t) (X (t) = x | X (t−1) = x ) = ( 1, x = x , 0, otherwise(1)Otherwise, we setP a(X (t) ) = {X (t−1) } [ e∈E X (a) ˘ X (t−1) | con(e) ∩ Dom(X) = ∅ ¯ ,(2)and specify T X (t) as follows.
Let x e ∈ Dom(X) be the value that effect e ∈ E X (a) provides to X. For each π ∈ Dom(P a(X (t) )), if there exists e ∈ E X (a) such that con(e) ⊆ π, then we setT X(t) (X (t) = x | π) = ( 1, x = xe, 0, otherwise(3)Otherwise, we setT X(t) (X (t) = x | π) = ( 1, x = π[X (t−1) ], 0, otherwise(4)It is not hard to verify that Eqs.
3-4 are consistent, and, together with Eq.
1, capture the semantics of the conditional deterministic actions.
Alternatively, suppose that a t is a probabilistic action a ∈ A p .
For each such action we introduce a discrete variableY (t) with Dom(Y (t) ) = e∈E(a) Λ(e), P a(Y (t) ) = [ e∈E(a) ˘ X (i−1) | con(e) ∩ Dom(X) = ∅ ¯ ,(5)and, for each π ∈ Dom(P a(Y (t) )), we setT Y (t) (Y (i) = ε | π) = ( p(ε), con (e(ε)) ⊆ π 0, otherwise ,(6)where e(ε) denotes the effect e of a such that ε ∈ Λ(e).
We refer to the set of all such variables created for a as Y. Now, similarly to the case of deterministic actions, let E X (a) ⊆ E(a) be the (now probabilistic) effects of a that affect a variable X ∈ V.
The case of E X (a) = ∅ is similar to this for deterministic actions, that is, we set P a(X (t) ) = {X (t−1) }, and set T X(t) according to Eq.
1.
Alternatively, if E X (a) = ∅, let x ε ∈ Dom(X) be the value provided to X by ε, e(ε) ∈ E X (a).
Recall that the outcomes of effects E(a) are all mutually exclusive.
Hence, we set P a(X (t) ) = {X (t−1) , Y (t−1) }, andTX (i) (X (i) = x |X (i−1) = x , Y (i−1) = ε) = 8 > < > : 1, e(ε) ∈ EX (a) ∧ x = xε, 1, e(ε) ∈ EX (a) ∧ x = x , 0, otherwise(7)Here as well, it is not hard to verify that Eqs.
6-7 capture the precise semantics of our probabilistic actions and frame axioms.
This accomplishes our construction of N a over the variablesV a = Y m t=0 V (t) .
Theorem 1 Let (A, N I , G, θ) be a probabilistic planning task, and a be an m-step sequence of actions applicable in P I .
Let P be the probability distribution induced by N a on its variables V a .1.
The belief state P a corresponds to the marginal distribu-tion of P on V (m) , that is, P a = P (V (m) ).
2.
If G (m) is the (possibly partial) assignment provided by G to V (m) , then the probability that a achieves G is equal to P (G (m) ), that is, P a (G) = P (G (m) ).
At this point, it is worth bringing attention to the fact that all the variables in V (1) , . . . , V (m) are completely deterministic.
Moreover, the CPTs of these variables are all compactly representable (not exponential in the number of parents) due to the local structure induced by a large amount of context-specific independence (Boutilier et al. 1996).
Hence, the description complexity of N a is linear in the size of the input, and in the number of actions in a.Lemma 1 Let (A, N I , G, θ) be a probabilistic planning task, and a be a sequence of actions from A. Then, we have|N a | = O (|a| · (|N I | + α)), where α is the largest description size of an action in A. Given our representation of belief states as BNs, next we should select a mechanism for reasoning about these BNs.
Computing the probability of a query in a BN is well known to be #P-complete (Roth 1996).
While numerous exact algorithms for inference with BNs have been proposed in the literature (e.g., see (Darwiche 2000;Dechter 1999;Zhang & Poole 1994)), all these algorithms do not scale well on large networks exhibiting high density and tree-width.
Recent advances in the area, however, show that practical exact inference in BNs is far from being hopeless.
The main observation that guides these advances is that real-world domains typically exhibit a significant degree of deterministic dependencies and context-specific independencies.Targeting this property of BNs already resulted in developing powerful inference techniques (Chavira & Darwiche 2005;Sang, Beame, & Kautz 2005).
The general principle of these techniques is to (i) compile a BN N into a propositional logic formula φ(N ), (ii) associate some literals of φ(N ) with numerical weights derived from the CPTs of N , and (iii) perform an efficient weighted model counting on φ(N ) by reusing (and adapting) certain techniques that appear powerful in enhancing backtracking DPLL-style search for SAT.We have already shown that the BNs representing our belief states exhibit a large amount of both deterministic nodes and context-specific independence.
Together with the fact that queries of our interest correspond to computing probability of "evidence" G (m) in N a , this makes the modelcounting techniques especially attractive for our purposes.
Taking this route, in Probabilistic-FF we compile our BNs to weighted CNFs following the encoding scheme proposed in (Sang, Beame, & Kautz 2005), and answer probabilistic queries using Cachet ( Sang et al. 2004), one of the most powerful (if not the most powerful) system to date for weighted model counting in CNFs.For a detailed description of the weighted CNF encoding we refer the reader to (Sang, Beame, & Kautz 2005).
In general, this encoding is based on two types of propositional variables: state variables for the values of the BN variables, and chance variables, encoding the entries of conditional probability tables.
For each variable X (t) ∈ V (t) , we need up to γ · (|Dom(X (0) )| − 1) chance variables, where γ is the number of rows in T X(t) .
Clauses involving both state and chance variables encode the structure of the CPTs, while clauses involving state variables only encode the "exactly one" relationship between the state variables capturing the value of a single BN variable.Together with the weighted chance variables, the clauses of the encoding provide the input for a weighted model counting procedure.
A simple recursive DPPL-style procedure basic-WMC underlying Cachet ( Sang et al. 2004) is depicted in Figure 3.
The weight of a (partial) variable assignment is the product of weights of the literals in that assignment.
The weight of a formula φ is the sum of weights of all complete satisfying assignments to the variables of φ, and this is exactly the result of basic-WMC(φ).
Likewise, Theorem 3 in (Sang, Beame, & Kautz 2005) shows that if φ is a weighted CNF encoding of a BN N , and P (Q|E) is a general query with respect to N , then we have P (Q|E) = basic-WMC(φ∧Q∧E) basic-WMC(φ∧E) .
Note that, in our special case of empty evidence, P (Q|E) reduces to P (Q) = basic-WMC(φ ∧ Q), that is, a single call to the basic-WMC procedure.Lemma 2 Let (A, P I , G, θ) be a probabilistic planning task with a BN N I describing P I , and a be an m-step sequence of actions.
The probability P a (G) that a achieves G is:Pa(G) = WMC (ϕ(a) ∧ G(m)) .
(8)Besides the fact that weighted model counting is attractive for the kinds of BNs arising in our context, in what follows we show that weighted CNF representation of belief states goes extremely well with the ideas underlyingprocedure basic-WMC(φ) 1 if φ = ∅ return 1 2 if φ has an empty clause return 0 3 select a variable v ∈ φ 4 if v is a chance variable 5 return basic-WMC(φ|v=1) · weight(v) + 6 basic-WMC(φ|v=0) · (1 − weight(v)) 7 else 8return basic-WMC(φ|v=1) + basic-WMC(φ|v=0 Figure 3: Basic DPPL-style weighted model counting.Conformant-FF (Brafman & Hoffmann 2004), a state-ofthe-art approach for (non-probabilistic) conformant planning.
In what follows, we outline the Conformant-FF approach to belief space search, and then explain the extensions made for Probabilistic-FF.
Conformant-FF does a forward search in a nonprobabilistic belief space in which each belief state corresponds to a set of world states considered to be possible.
The main trick of Conformant-FF is the use of CNF formulas for an implicit representation of belief states.
Implicit, in this context, means that formulas ϕ(a) encode the semantics of executing action sequence a in the initial belief state, with propositional variables corresponding to facts with time-stamps.
Any actual knowledge about the belief states has to (and can) be inferred from these formulas; most particularly, a fact p is known to be true in a belief state iff ϕ(a) → p(m), where m is the time endpoint of the formula.
The only knowledge computed by Conformant-FF about belief states are these known facts, as well as (symmetrically) the facts that are known to be false.
This suffices to do STRIPS-style planning, i.e., to determine applicable actions and goal belief states.
This computation of only a partial knowledge constitutes a lazy kind of belief state representation, in comparison to other approaches that use explicit enumeration (Bonet & Geffner 2000) or BDDs (Bertoli et al. 2001) to fully represent belief states.The basic idea underlying Probabilistic-FF is (i) to extend Conformant-FF's belief state formulas to model our BNs, (ii) to use weighted model-counting to determine whether the probability of the goals in a belief state is high enough, and (iii) to adapt the heuristic function of Conformant-FF to the probabilistic setting.
Note the synergetic effect: Probabilistic-FF can re-use all of Conformant-FF's technology to recognize facts that are true or false with probability 1.
This fully serves to determine applicable actions, as well as detect whether part of the goal is already known.In more detail, given a probabilistic planning task (A, P I , G, θ), a belief state P a corresponding to some executable m-step action sequence a, and a proposition q ∈ X , we say that q is known in P a if P a (q) = 1, negatively known in P a if P a (q) = 0, and unknown in P a , otherwise.
We begin by determining whether q is known, negatively known, or unknown at time m. Re-using the Conformant-FF machinery, this classification requires up to two SAT tests of ϕ(N a ) ∧ ¬q(m) and ϕ(N a ) ∧ q(m), respectively.
The information provided by this classification is used threefold.
If a subgoal g is negatively known at time m, then we have P a (G) = 0.
On the other extreme, if all the subgoals of G are known at time m, then we have P a (G) = 1.
Finally, if some subgoals of G are known and the rest are unknown at time m, then we accomplish evaluating a by testing whetherPa(G) = WMC (ϕ(Na) ∧ G(m)) ≥ θ.
(9)Note also that having the sets of all (positively/negatively) known propositions at all time steps up to m allows us significantly simplify the CNF formula ϕ(N a ) ∧ G(m) by inserting into it the corresponding values of known propositions.
After evaluating the considered action sequence a, if we get P a (G) ≥ θ, then we are done.
Otherwise, the forward search continues, and the actions that are applicable in P a (and thus used to generate the successor belief states) are those whose preconditions are all known in P a .
Like the belief state representation, Conformant-FF's heuristic goal distance estimation process translates naturally to the new context.
For the estimation, Conformant-FF relaxes the planning task by ignoring all delete lists, and by ignoring all but one effect condition of each effect.
Under these simplifying assumptions, original FF's machinery (Hoffmann & Nebel 2001) can be extended with a DAGcalled "implication graph" -capturing the dependencies between unknown facts, as well as with a reasoning machinery that uses this implication graph to infer when unknown facts become known.
The extended machinery is sound and complete for relaxed tasks, and yields a heuristic function that is highly informative across a range of challenging domains (Brafman & Hoffmann 2004).
In the Probabilistic-FF context, we need to extend Conformant-FF's machinery with the abilities to determine when the goal set is sufficiently likely (rather than known to be true for sure).
To achieve that, we must introduce into relaxed planning some effective reasoning about the probabilistic initial state and effects of probabilistic actions.
It turns out that such a reasoning can be obtained by a certain natural weighted extension of the implication graph and the reasoning done with it.
In what follows, we explain the extended process in detail, starting with brief descriptions of the machinery used in original FF, and in Conformant-FF.
The heuristic of FF is based on the notion of relaxed plan, which is a plan that achieves the goals while assuming that all delete lists of actions are empty.
The heuristic value h(w) that FF provides to a world state w encountered during the search is the length of the relaxed plan to w. Formally, relaxed plans are computed as follows.
Starting from w, FF builds a relaxed planning graph as a sequence of alternating proposition layers P (t) and action layers A(t), where P (0) is the same as w, A(t) is the set of all actions whose preconditions are contained in P (t), and P (t + 1) is obtained from P (t) by including the add effects (with fulfilled conditions) of the actions in A(t).
The relaxed planning graph is constructed either until it reaches a propositional layer P (m) that contains all the goals, or until the construction reaches a fixpoint P (t) = P (t + 1) without reaching the goals.
The latter case corresponds to (all) situations in which a relaxed plan does not exist, and because existence of a relaxed plan is a necessary condition for existence of a real plan, the state w is excluded from the search space by setting h(w) = ∞.
In the former case of G ⊆ P (m), a relaxed plan is a subset of actions in A(1), . . . , A(m) that suffices to achieve the goals (under ignoring the delete lists), and it can be extracted by a simple backchaining loop: For each goal in P (m), select an action in A(1), . . . , A(m) that achieves this goal, and iterate the process by considering those actions' preconditions and the respective effect conditions as new subgoals.
The length of the relaxed plan (aka heuristic value h(w)) is the number of actions selected in this backchaining process.In Conformant-FF, this construction is extended with additional fact layers uP (t) containing the facts unknown at time t, and the reasoning is extended to when such unknown facts become known in the relaxed planning graph.
As the complexity of this type of reasoning is prohibitive, Conformant-FF further relaxes the planning task by ignoring not only the delete lists, but also all but one of the unknown conditions of each action effect.
That is, if action a appears in layer A(t), and for effect e of a we have con(e) ⊆ P (t) ∪ uP (t) and con(e) ∩ uP (t) = ∅, then con(e) ∩ uP (t) is arbitrarily reduced to contain exactly one literal, and reasoning is done as if con(e) had this reduced form from the beginning.
This relaxation converts implications c∈con(e)∩uP (t) c(t) → q(t + 1) that the (unknown) effects induce between unknown propositions into their 2-projections that take the form of binary implications c(t) → q(t + 1), with arbitrary c ∈ con(e) ∩ uP (t).
The set of all these binary implications forms a DAG.
Thus, checking whether a proposition q becomes known at time t can be done by a backchaining over the implication edges that end in q(t), followed by a SAT check to see if the initial belief state formula implies the disjunction of the reachable tree leafs.
Note that this process over-estimates the sets of propositions that become known at a given time point; Since it is easier to achieve a fact in relaxed planning, reasoning about when and what becomes known under relaxation is a non-sound but complete approximation of this reasoning without relaxation.
Hence, if Conformant-FF discovers that there is no relaxed plan to a world state w, then w can be safely excluded from the search.We now explain our extension of this reasoning process to the probabilistic setting.
Some parts are inherited from Conformant-FF, and the new parts are dedicated to processing probabilistic belief states and probabilistic actions.
We begin with our procedure for building a probabilistic relaxed planning graph (or PRPG, for short), then we show how one can extract a (probabilistic) relaxed plan from the PRPG.
Figure 4 depicts the main routine for building the PRPG.
Layers −n to −1 of PRPG correspond to the m-step action sequence a leading to the considered belief state P a .
The negative indices of the layers are used to simplify the presentation of reasoning about the past.
The PRPG is initialized with an empty implication set Imp, and P (−m) and uP (−m) containing the propositions that are known (hold with probability 1) and unknown (hold with probability less 1 but greater than 0) in the initial belief state.
The for loop (lines 3-19) then builds the sets P and uP for the subsequent time steps, incrementally extending the implication set Imp.
At each iteration −m ≤ t ≤ −1, the sets P (t + 1) and uP (t + 1) are made to contain the proposiprocedure build-PRPG (a −m , . . . , a −1 , A, ϕ(N I ), G, θ) 1 Imp := ∅, P (−m) := {q | q is known in ϕ(N I )}, 2 uP (−m) := {q | q is unknown in ϕ(N I )} 3 for t := −m · · · − 1 do 4 P (t + 1) := {q | q is known after a t } 5 uP (t + 1) := {q | q is unknown after a t } 6Imp ∪ = {(q(t), q(t + 1)) | q ∈ uP (t) ∩ uP (t + 1)} 7for all e ∈ E(a t ) s.t. con(e) ⊆ P (t) ∪ uP (t) do 8 if a t ∈ A d and con(e) ∩ uP (t) = ∅ then 9 select c ∈ con(e) ∩ uP (t) 10Imp ∪ = {(c(t), q(t + 1)) | q ∈ add(e) ∩ uP (t + 1)} 11 elseif a t ∈ Ap 12 for all ε ∈ Λ(e), q ∈ add(ε) ∩ uP (t + 1) 13introduce new fact ω ε q (t) with weight(ω ε q (t)) = p(ε); 14Imp ∪ = {(ω ε q (t), q(t + 1))} 15if con(e) ∩ uP (t) = ∅ then 16 select c ∈ con(e) ∩ uP (t) 17Imp ∪ = {(c(t), ω ε q (t))} 18 endif 19 endif 20 t := 0 21 while get-P(t, G) < θ do 22 A(t) := {a | a ∈ A, pre(a) ⊆ P (t)} 23 build-timestep(t, A(t)) 24 if P (t + 1) = P (t) and 25 uP (t + 1) = uP (t) and 26∀q ∈ uP (t + 1) : WImpleafs(q(t + 1)) = WImpleafs(q(t)) then tions that are known/unknown after applying the action a t , respectively.
The implication set Imp is extended as follows.
First, we add an implication q(t) → q(t + 1) for every proposition q that is unknown at both t and t + 1.
Then we consider each effect e ∈ E(a t ) that has no negatively known conditions at time t.
If action a t is deterministic and not all of its conditions are known at time t (lines 8-10), then for each considered effect e we add an implication c(t) → q(t + 1) for an arbitrarily chosen unknown condition of e, and each unknown at t + 1 add effect q of e. Alternatively, if action a t is probabilistic (lines 12-18), then we consider each probabilistic outcome ε of e. For each (unknown at t + 1) add effect q of ε, we introduce a chance proposition ω ε q (t) with weight(ω ε q (t)) = p(ε), and add an implication ω ε q (t) → q(t + 1).
In addition, if effect e is conditional and not all of its conditions are known at t, then we add an implication c(t) → ω ε q (t) for an arbitrarily chosen unknown condition of e.The next while loop (lines 21-29) constructs the relaxed planning graph from layer 0 onwards by iterative invocation of the build-timestep procedure that expands PRPG by a single level at each call.
This construction is controlled by two termination tests.
If the goal is achieved at layer t with probability higher than θ, then a relaxed plan can be extracted.
Otherwise, if the graph reaches a fix point, then we know that no relaxed (and thus, no real) plan exists.
The structure WImpleafs(q(t)) used in this termination test corresponds to the set of weighted (in a manner described below) leafs of the Imp subgraph rooted at q(t).
procedure build-timestep (t, A(t)) 1 P (t + 1) := P (t), uP (t + 1) := uP (t) 2 Imp ∪ = {(q(t), q(t + 1)) | q ∈ uP (t)} 3 for all a ∈ A d ∩ A(t), all e ∈ E(a) do 4 if con(e) ⊆ P (t) then P (t + 1) ∪ = add(e) endif 5 if con(e) ⊆ P (t) ∪ uP (t) and con(e) ∩ uP (t) = ∅ then 6 uP (t + 1) ∪ = add(e) 7 select c ∈ con(e) ∩ uP (t) 8Imp ∪ = {(c(t), q(t + 1)) | q ∈ add(e)} 9 endif 10 for all a ∈ Ap ∩ A(t), all e ∈ E(a) do 11 for all ε ∈ Λ(e), all q ∈ add(ε) do 12 uP (t + 1) ∪ = {q} 13 introduce new fact ω ε q (t) with weight(ω ε q (t)) = p(ε);14 Imp ∪ = {(ω ε q (t), q(t + 1))} 15 if con(e) ∪ uP (t) = ∅ do 16 select c ∈ con(e) ∩ uP (t) 17Imp ∪ = {(c(t), ω ε q (t))} 18 endif 19 for all q ∈ uP (t + 1) \ P (t + 1) do 20WImpleafs(q(t + 1)) := build-w-impleafs(q(t + 1), Imp)21 DS := {l | l ∈ WImpleafs(q(t + 1)) ∧ weight q(t+1) (l) = 1} 22 if ϕ(N I ) → W l∈DS l then P (t + 1) ∪ = {q} endif 23 uP (t + 1) \ = P (t + 1)procedure build-w-impleafs (q(t), Imp) 1.
for all state variables v ∈ Imp q(t) do weight(v) := 1 2.
for all v ∈ Imp q(t) in a reverse topological order of Imp q(t) do 3.
weight q(t) (v) := weight(v) 4.
if (max {weight q(t) (u) | (v, u) ∈ Imp q(t) } < 1)then 5.
weight q(t) (v) * = min {1 − δ , P (v,u)∈Imp q(t) weight q(t) (u)} 6.
return the set of all leafs l of Imp q(t) annotated with weight q(t) (l)procedure get-P (t, G) 1 if G ⊆ P (t) ∪ uP (t) then return 0 endif 2 if G ⊆ P (t) then return 1 endif 3 return WMC(ϕ(N I ) ∧ V g∈G\P (t) W l∈WImpleafs(g(t)) l)Figure 5: Building a new layer in the PRPG, and computing the goal likelihood at time step t.The build-timestep procedure is shown in Figure 5.
The first for loop (lines 3-9) proceeds over all effects of all deterministic actions "applicable" at time t. Effect whose condition is known at time t (possibly) make some additional facts to be known at time t + 1.
Effects whose condition depends on the actual initial state and/or outcome of probabilistic actions (possibly) add some additional such facts at time t + 1 (together with the appropriate binary implications in Imp).
The second for loop (lines 10-18) proceeds over all possible outcomes of all probabilistic actions "applicable" at layer t, providing all the propositions in the add lists of these outcomes with processing similar to this in lines 14-17 of build-PRPG.
Now, let Imp q(t ) be the subgraph of Imp formed by q(t ) and all its predecessors in Imp.
The third for loop (lines 19-22) checks whether an unknown proposition q becomes known at t + 1.
Specifically, it is checked whether the initial state formula implies one of the leafs l of Imp q(t+1) such that there exists a path in Imp q(t+1) from l to q(t + 1) with all edges of the path being contributed by deterministic actions only.
The latter is determined using the build-w-impleafs procedure ( Figure 5).
The build-w-impleafs procedure constructs a set WImpleafs(q(t)) of all leafs of Imp q(t) weighted with respect to q(t + 1).
Specifically, build-w-impleafs performs a BFS-style, top-down weight propagation from q(t) down to the leafs of Imp q(t) .
Together with acyclicity of Imp, the BFS-style traversal ensures that each variable v is processed only after processing all variables u such that Imp q(t) contains implication v → u.
When processed, v is assigned with a weight weight q(t) (v) derived from its own "global" weight weight(v) and the weights weight q(t) (v) of its immediate successors u in Imp q(t) .
We have:(1) weight q(t) (v) is an upper bound on the probability of achieving q at time t by a sequence of actions responsible for a path from v to q(t) in Imp; (2) weight q(t) (v) = 1 iff there exists a path from v to q(t)in Imp q(t) where all edges are due to deterministic actions.
(Otherwise, we always have weight q(t) (v) ≤ 1 − δ, for some arbitrary small 0 < δ 1.)
Theorem 2 Let (A, N I , G, θ) be a probabilistic planning task, and a be a sequence of actions applicable in P I .
If build-PRPG(a, A, ϕ(N I ), G, θ) returns FALSE, then there is no relaxed plan for (A, N I , G, θ) that starts with a.The proof of Theorem 2 is based on three observations.
Let T > 0 be the last layer of the PRPG upon the termination of build-PRPG.
First, for every −m ≤ t ≤ T , the sets P (t) and uP (t) contain all (and only all) propositions that are known (respectively unknown) after executing all the actions in the action layers up to A(t − 1).
Second, if build-PRPG returns FALSE, then the corresponding termination criterion would hold in all future iterations.
Finally, if the PRPG does contain a relaxed plan for (A, N I , G, θ), then we have get-P(T, G) ≥ θ, and thus (together with the second observation) build-PRPG cannot return FALSE.To see the latter, consider the details of the procedure get-P as in Figure 5.
This procedure is used to compute the goal likelihood at time t. First, if one of the subgoals is negatively known at time t, then the overall probability of achieving the goal is 0.
On the other extreme, if all the subgoals are known at time t, then the probability of achieving the goal is trivially 1.
This leaves us with the main case in which we are uncertain about some of the subgoals, and this uncertainty is either due to dependence of these subgoals on the actual initial world state, or due to achieving these subgoals using probabilistic actions (or due to both.)
The uncertainty about the initial state is fully captured by our weighted CNF ϕ(N I ).
Likewise, the new chance propositions ω ε q (t ) introduced in build-PRPG and build-timestep for the add lists of probabilistic outcomes are "chained up" in Imp to the propositions on these add lists, and "chained down" in Imp to the (relaxed) conditions of these outcomes, if any.
Therefore, if some add effect q of a probabilistic action at time t < t is relevant to achieving a subgoal g ∈ G at time t, then the corresponding chance variable ω ε q (t ) will appear in Imp g(t) , and its weight will be properly propagated by build-w-impleafs(g(t), Imp) down to the leafs of Imp g(t) .
(For unconditional probabilistic actions, ω ε q (t ) is in already a leaf in Imp.)
Example 2 To illustrate the treatment of probabilistic effects, consider the following simple throw-a-dice example.
The only propositions are q 1 , . . . , q 6 with the obvious meaning.
The initial state is empty, G = {q 6 }.
We have a single, probabilistic, action throw-a-dice that has a single unconditional effect with six equiprobable outcomes ε 1 , . . . , ε 6 , where add(ε i ) = {q i } and del(ε i ) = ∅.
Figure 6 depicts an excerpt of the implication graph constructed by t = 3.
The probability of throwing a six in three throws (that is, the probability of q 6 after a sequence of three actions throw-a-dice) is equal toWMC ω ε6 q6 (0) ∨ ω ε6 q6 (1) ∨ ω ε6 q6 (2) .
ω ε6 q6 (0) / / q 6 (1) / / q 6 (2) / / q 6 (3) ω ε6 q6 (1) 5 5 l l l l ω ε6 q6 (2) = = z z z z z z z. .
.
If build-PRPG succeeds at a level T , then a relaxed plan can be extracted from this PRPG using the extract-PRPlan procedure ( Figure 7).
The sets G (1), . . . , G(T ) used in this procedure store goals and subgoals arising at layers 1 ≤ t ≤ T .
First, the for loop in lines 1-7 processes the goals that remain unknown at T .
For each such goal g, and each 4 leaf l of the implication subgraph Imp g(T ) , the procedure selects an arbitrary path from l to g(T ), and adds to the relaxed plan all the actions responsible for the edges of this path.
The next for loop (lines 9-21) accomplishes the construction of the relaxed plan in a way similar in spirit to Conformant-FF.
Specifically, for each goal/subgoal g(t), if there is a deterministic action a in A(t − 1) that guarantees to always achieve g, then one such action is added to the relaxed plan.
Otherwise, the relaxed plan is extended with a set of all actions responsible for a set of implication paths from all leafs in DS ⊆ WImpleafs(g(t)) (similar to this in line 21 of build-timestep) to g(t).
In summary, if build-PRPG returns TRUE, then we set h(a) to the number of actions selected by extract-PRPlan.
Otherwise, we set h(a) = ∞, and the corresponding belief state P a is excluded from the search.
We implemented the techniques described in the previous sections in C, starting from the Conformant-FF code.
We call the implemented system Probabilistic-FF.
With θ = 1.0, Probabilistic-FF behaves exactly like Conformant-FF.
Otherwise, Probabilistic-FF uses Cachet (Sang, Beame, & Kautz 2005) for the weighted model counting.
The experiments were run on a PC running at 1.2GHz with 1GB main memory and 1024KB cache running Linux.
Table 1: Empirical results.
Times t in seconds (tW M C is time spent in WMC, which is none if the goal request is 1.0), search space size |S| (number of calls to the heuristic function), plan length l.Our first three domains are probabilistic versions of traditional conformant benchmarks: "Safe", "Cube", and "Bomb".
In Safe, each of n combinations of a safe must be tried to make sure the safe is open.
We implemented two probability distributions over the n combinations, a uniform one ("Safe-uni") and a distribution that declines according to a cubic function ("Safe-cub").
Table 1 shows that Probabilistic-FF can solve this very efficiently even with n = 70.
In Safe-cub, less combinations must be tried since a lot of "weight" is contained in the combinations at the start of the cubic distribution (the last combination has weight 0 and so needs not be tried even with θ = 1.0).
Observe that, rather than resulting in a performance decline, moving from θ = 1.0 to θ < 1.0 (from Conformant-FF to cases that require weighted model counting) even improves performance (except for θ = 0.75 in Safe-uni) since the plans become shorter.
In Cube, the task is to move into a corner of a 3-dimensional grid.
Again, we implemented uniform and cubic distributions (over the initial position in each dimension), and again, Probabilistic-FF scales well, easily solving instances on a 11 × 11 × 11 grid, becoming faster with decreasing θ.
(We also tried a version of Cube where the task is to move into the grid center; like Conformant-FF, Probabilistic-FF is bad at doing so, reaching its performance limit at n = 7.)
Our version of Bomb contains n bombs and m toilets, where each bomb may be armed or not armed independently, resulting in huge numbers of initially possible world states.
In the depicted instances, each bomb has a 0.02 chance of being armed.
Dunking a bomb in a toilet clogs the toilet, which must then be flushed.
Like Conformant-FF, Probabilistic-FF scales to large n, and becomes faster as m increases.
And again, we obtain the nice pattern of improved performance as we move from non-probabilistic (θ = 1.0) into probabilistic planning (specifically, θ ≤ 0.5; for θ ≤ 0.25, the initial state is good enough already).
We remark that, for all of Safe, Cube, and Bomb, Probabilistic-FF's plans are optimal (the shortest possible).
Our next three domains are adaptations of benchmarks from deterministic planning: "Logistics", "Grid", and "Rovers".
We assume that the reader is familiar with these domains.
Each "Log-x" instance contains 10 cities, 10 airplanes, and 10 packages, where each city has x locations.
The packages are with chance 0.88 at the airport of their origin city, and uniformly at any of the other locations in that city.
The effects of all loading and unloading actions are conditional on the (right) position of the package.
Note that higher values of x increase not only the space of world states, but also the initial uncertainty.
We again observe the nice scaling properties discussed above.
5 Grid is the complex grid world run in the AIPS'98 planning competition, featuring locked positions that must be opened with matching keys.
"Grid-x" here is a modification of instance nr.
2 (of 5) run at AIPS'98, with a 6 × 6 grid, 8 locked positions, and 10 keys of which 3 must be transported to a goal position.
Each lock uniformly has x possible shapes, and each of the 3 goal keys uniformly has x possible initial positions.
The effects of pickup-key, putdown-key, and open-lock actions are conditional.
The scaling behavior is similar as in the previous domains.
Our Rovers instances, finally, are modifications of instance nr.
7 (of 20) run at the AIPS'02 planning competition, with 6 waypoints, 3 rovers, 2 objectives, and 6 rock/soil samples.
From "Rovers" to "RoversPPP" we modify the instance/domain as follows.
"Rovers" is the original AIPS'02 domain and instance for compari-son.
In "RoversP", each sample is with chance 0.8 at its original waypoint, and with chance 0.1 at each of two others; each objective may be visible from 3 waypoints with uniform distribution (this is a probabilistic adaptation of the domain from (Bryce & Kambhampari 2004)).
"RoversPP" enhances this by conditional probabilities in the initial state, stating that whether or not an objective is visible from a waypoint depends on whether or not a rock sample (intuition: a large piece of rock) is located at the waypoint.
The probability of visibility is much higher if the latter is not the case (concretely, the visibility of each objective depends on the locations of two rock samples; if a rock sample is present then the visibility probability drops to 0.1).
"RoversPPP", finally, introduces the need to collect data about water existence.
Each of the soil samples has a certain likelihood (below 1) to be "wet".
An additional operator tests, for communicated sample data, if the sample was wet; if so, a fact "know-that-water", that is contained in the goals, is set to true.
The probability of being wet depends on the location of the sample.
Goal probabilities θ = 0.75 and θ = 1.0 can not be achieved, which is proved by the heuristic function, providing the correct answer in split seconds.As for Maxplan and CPP, due to technical problems (not on our side) with the CPP distribution, that were not resolved by the time of writing this paper, it was impossible to run CPP on our benchmarks.
We ran Maxplan in Safe, Cube, and Bomb.
In Safe with 10 combinations, we obtained a plan with 5 steps (goal likelihood 0.5) in 270 seconds.
Setting the bound to 6, Maxplan ran out of time (killed after 1 hour).
In Cube, Maxplan easily solved a 3 × 3 × 3 grid up to 6, where the goal is achieved with certainty.
It found plans within a few hundred seconds for 4 × 4 × 4 and 5 × 5 × 5 grids with a bound of 5, but with bound 6 ran out of time.
In Bomb, finally, 10 bombs 10 toilets could only be done up to bound 3 (270 seconds), and 10 bombs 1 toilet failed at bound 8.
We developed a synergetic combination of Conformant-FF with recent techniques for probabilistic reasoning.
The resulting planner scales well in several adaptations of known planning benchmarks, constituting an advance of several orders of magnitude to the state of the art in probabilistic planning with no observability and deterministic actions.
We do not claim that this solves the (this particular) problem once and for all: Probabilistic-FF inherits strengths and weaknesses from FF and Conformant-FF, like domains where FF's heuristic function yields bad estimates, or domains (e.g. the mentioned Cube-center variant) where Conformant-FF gets lost due to heuristic inaccuracy combined with a high degree of uncertainty.
We do believe that the synergy between WMC and Conformant-FF's CNF-based techniques constitutes a major step forward.Next, of course, we will implement the presented treatment of probabilistic effects, and evaluate its performance.
In the long term, we intend to work towards applicability in real-word settings, particularly in the space application settings that our Rovers domain hints at, in medicationtype treatment planning domains, and in the power supply restoration domain (e.g., (Bertoli et al. 2002)).
