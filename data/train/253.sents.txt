We introduce a world vision in which data is endowed with memory.
In this data-centric systems paradigm, data items can be enabled to retain all or some of their previous values.
We call this ability remembrance and posit that it empowers significant leaps in the security, availability, and general operational dimensions of systems.
With the explosion in cheap, fast memories and storage, large-scale remembrance will soon become practical.
Here, we introduce and explore the advantages of such a paradigm and the challenges in making it a reality.
Since the dawn of computing, data architectures have been essentially single-valued: each object instance is associated with only one value, namely, the value most recently assigned to it.
Upon the next assignment, the old value of the object is overwritten.
When one is given a digital object, its history (i.e., its past values and the actions that caused those values to change) is not usually available.
Further, objects are usually not self-aware.
Many subcommunities of computer science have explored the idea that it would be beneficial to retain the history of data items, or at least their old values.
For example, within the database community, we have seen version-based concurrency control [46], support for rollback to checkpoints [19,44], the total recall introduced in Postgres [43,42] that has evolved into time-travel databases [28,35,37] and temporal SQL [29,39], and provenance for scientific data and workflows [3,12,13,14,36].
Outside the database community, there are proposed or actual systems for storing and recalling past system configurations [49], old versions of source code [5,10], archival data [31], file system backups, old versions of individual files [26,30,32,11,40], old program and systems execution points in the form of checkpoints [33,44] and recent history of an execution [41], past states of the World Wide Web on the WayBack machine [1], YouTube, and gMail-style email repositories, and more [2].
One can also think of logs as a poor-man's version of remembrance, and logs are heavily used in database systems, file systems [34], telecommunications and other critical infrastructures, and all sorts of applications.
Though some of these This article is published under a Creative Commons License Agreement (http://creativecommons.org/licenses/by/3.0/).
You may copy, distribute, display, and perform the work, make derivative works and make commercial use of the work, but you must attribute the work to the author and CIDR 2009.
approaches to remembrance keep old data because it is faster or simpler to keep it than to discard or overwrite it (e.g., log-structured file systems and gMail users), other approaches have found novel uses for the historical information.
For example, many program crashes are due to transient failures, and many of these crashes can be avoided by retrying the last few instructions that were executed before the crash [41].
The US government has decided that Enron-style corporate fraud is such a threat to society's confidence in corporate America that companies need to keep a copy of every business email, spreadsheet, and report for several years [47], to enable after-the-fact prosecution of corporate evildoers.
The proliferation of loyalty cards for retail shopping shows the benefits that can be obtained by mining a log of all shopping transactions.
And the cultural anthropologists of the future will have a field day with the information obtainable from the WayBack machine [1].
Computer scientists have also proposed to endow objects with self-awareness, most notably in the programming language community [9,22,23,38,45,48], but also much closer to home: for over 30 years we have been fond of associating each data item with its metadata, and have even toyed with the idea of stronger notions of self-awareness [27].
However, even at their grandest, most of these remembrance approaches boil down to versioning systems that ensure the persistence of explicitly defined versions of data objects.
Because each approach was developed separately and for a different purpose, the result is a piecemeal coverage of past states of the world, with disparate, unintegrated interfaces reflecting a pastiche of different underlying assumptions.
As a result, we cannot just click a button to go back to the state of the world as of 2 PM this day a year ago.
Further, except for checkpointing, backup, data archival, and source code management, these versioning systems have not made it into the mainstream.
Even where remembrance-based approaches have made it into the mainstream, a data item and its previous versions are not considered as an inseparable unit.
Instead, each specific version is instantiated as a separate data item with its own unique identity.
Associations between data items and their previous versions are maintained externally with great effort.
These decisions were made for what were, at the time, good reasons.
We believe that it is time to revisit those decisions in the light of recent changes in technology, and examine what benefits could accrue from having universal support for remembrance.
It is also time to see what research contributions from the database community and other areas of computer science are needed in order to support universal integrated remembrance.When thinking about remembrance in computing systems, it may be helpful to draw an analogy with the evolution of wetware [20], i.e., living beings.
Single-celled organisms have little memory beyond what is hard-coded into their DNA.
Remembering little, they can learn very little.
Without much ability to learn, their potential to adapt dynamically to their environment is limited, and much of the pressure for adaptation to their environment is thrust onto reproductive processes that involve random changes in their DNA.
As organisms became more complex, their memory and their ability to learn increased, as did their self-awareness.
Sentient beings' memories retain vast amounts of contextual information with significant temporal components, often as explicit as the history of their formative elements.
This meta-information has been shown to be an essential building block of humans' ability to reason, at the core of our associative memory processes [21].
This outlines one of the main differences between knowledge and raw information.Although advanced wetware organisms such as humans can remember many events from their past history, they certainly cannot remember everything.
For example, young humans have trouble remembering many things that their parents consider important, while their parents have trouble remembering where they left their car keys, and their grandparents have trouble remembering where they parked their car at the mall.
At the dawn of the computer age, researchers were already positing that humans can benefit from computer assistance in remembering more of what happens to them [6].
Sixty years later, technology had advanced to the point that researchers could investigate so-called "total recall" paradigms for human activities such as the MyLifeBits [4,16,17,18,15] project, where both online and offline human activities are recorded in a database.
These approaches use technology to provide a partial record (e.g., video and audio) of what happens to a person.
"Total recall" is a misnomer for these experiments, which just remember what the human's recording device saw or heard, not how the human associated with the device felt about what was happening or how the human experienced the environment (e.g., hot, cold, sharp, soft, tense, relaxed, etc.).
Thus it may be more accurate to classify these approaches as providing total recall not for a human, but rather for a device they carried around with them.
However, even at the device level, such infrastructures currently lack self-awareness; for example, we cannot distinguish between the case where the video camera decided to turn itself off because its battery was low, and the case where the human switched off the camera.
This lack of self-awareness is not surprising, since these projects are intended to endow humans with total recall, not humans' digital devices and their fine-grained data and application states.
In the absence of system-level remembrance, only a partial temporal view of the life of a human or a digital artifact can be achieved.
Ultimately, while Memex-inspired [6] technology such as MyLifeBits is for assisting humans, system-wide remembrance for data items can offer leaps in data processing capabilities.We posit that, as computing evolves toward increasingly semanticsrich context-aware systems, a fundamentally novel data-with-memory model of remembrance will emerge.
In this model, digital entities ranging from simple memory records to complex structured objects are inherently endowed with the ability to retain full or partial memories of their past contexts.
Then history becomes an integral part of every entity.
By considering historical values for data containers to be an integral part of the container, we enable processing based on not only the current incarnation of a data item, but also its past history, trends, evolution, and lineage.
Data items stop being simple containers that live only in the current instant; they become aware of time, and have a knowledge of the evolution of their own state along the temporal axis.
Such capabilities are essential to support the transition from information processing to knowledge processing.In this paper, we define and explore this vision.
While the longterm benefits' fruition is linked to the emergence of strong semantics processing paradigms, noteworthy advantages to deploying such a data model exist also in the immediate future.
These range from significant leaps in the types and expressivity of the system policies now achievable, to self-healing systems that rely on remembrance to recover from undesirable events at the local level with minimal overall system impact, resulting in improved availability.
To realize these benefits, however, we will have to overcome major technology hurdles in efficiently recording, storing, searching, indexing, retrieving, processing, and ignoring history.
We must also seriously consider the possibility that intelligent behavior requires a well-developed ability to forget, as well as to remember; this raises the question of how we should choose what to forget.
At the extreme, we can imagine a system where every component, from the micro-level to the macro-level, remembers everything that has happened to it, from the moment of its creation through the moment of its destruction.
At the lowest levels of abstraction, memory locations can have remembrance, making it possible to query for older values of data stored at a particular location.
Individual data blocks in secondary storage can have memory of their previous context and values.
Higher logical structures such as files are no longer dumb containers of data; they retain contextual information about their contents throughout their lifetime.
More abstractly, a self-aware PDF file remembers the LaTex or Word documents from which it was created, even though they reside in separate containers.
Variables in an execution of a program can possess remembrance, including awareness of the environmental conditions that affected their execution.
The program itself can remember its executions and their effects, as can the larger configurations that include it.
Inside a database, we may choose to make tuples, schemas, constraints, triggers, stored procedures, and other metadata remembrance-capable, as in an extension of time-travel databases.
We can do the same for transactions and transaction executions.Even application-layer constructs and data containers can have remembrance.
A visitor to a web page can get not only the current instance of the web page, but also traverse the page chronologically.
Web searches can include the temporal dimension; this is already possible now with Google News where users can specify a particular time frame when searching for old news.
Queries no longer return a flat time-ignorant result, but rather a result that can be browsed along the temporal axis.Transfer, copying, and movement of data objects can preserve memory -a data container can retain its memory when it is moved to a new location.
The copy operation transfers old memory from the original source to the copied container, along with new memory of the copy operation.
Deletion removes the container, but its memories can live on, as in time-travel databases.Is this vision attainable, or even desirable?
Storage is cheap, but in practice, physical, logistical, legal, and pragmatic issues will limit what a computer system can and should remember.Physical limits.
Hardware does fail and bits do rot, even on magnetic disk.
When hardware dies, any memories stored on it will die with it, so memories that should not be lost should be stored in a fault-tolerant manner.
In a system that supports remembrance, users will expect their data to last forever, with no exceptions for bit rot, human error, or hardware failure.
Large-scale fault-tolerance for remembrance will probably be easiest to provide in a cloud computing context, such as in Google's initiative to preserve publicly sharable scientific data.No hardware support is available for remembrance at the level of physical bytes, either in L2 cache, memory, or disk.
Further, the astronomical cost of such support and the security issues that it would introduce outweigh the benefits that might result from, e.g., improved forensic analysis.
Thus we restrict our attention to support for remembrance at higher levels of abstraction, such as abstract locations.
On the other hand, storage has become very cheap, with the result that we already retain much more data than we used to.
The cost of accessing that data has dropped much more slowly, but that will not matter if historical information is rarely accessed.Performance limits.
At any level of abstraction, a naive implementation of remembrance will kill CPU, network, and system performance.
For example, suppose that a fetch of an object from disk brings not only the current value of the object, but also all its previous values and its other associated memories.
If the application does not need this extra information, it will occupy valuable buffer space and pollute the L2 cache, crippling performance.
Thus historical information should be available when wanted and invisible the rest of the time.
For example, we may prefer to keep historical versions of tuples on separate file pages from the latest versions.
At an extreme, we might choose to bury the historical information in logs and only dredge it up on request.
In other words, performance considerations may dictate that we develop a very clean user interface that provides the illusion of a self-aware and history-aware system.
In the relatively rare event that historical information is accessed through this interface, we will quickly cobble it together from searchable, indexable logs that we have shipped off to inexpensive remote self-organizing storage.
This extreme vision raises many new challenges in how to transparently move logs off to a cloud of inexpensive, fault-tolerant storage; how to transparently reorganize and/or index it in a manner conducive to future access patterns; and how to transparently decide where to place the data and its replicas.
These problems are particularly acute for multimedia data, such as sounds, images, and videos.
For example, suppose that we have a MyLifeBits record of our life.
How quickly can we get an answer to the question Where are my car keys?
or Where did I park the car?
Is it a security violation for the system to answer the question Where did my spouse park the car?
What about Where did my teenager park the car?
or simply Where is my teenager?
Imagine the benefit for a blind person if the system can answer questions of the form Where did I leave my house keys/wallet/comb?
These examples show that both the utility and the sensitivity of a remembrance are very context-sensitive.
Security and legal challenges.
As suggested by the examples above, some remembrances are very sensitive, and it will be hard to manage them in a user-friendly way that preserves confidentiality and privacy.
(In the security community, privacy refers to the ability of the owner of a piece of information to control what is done with it.)
For example, Microsoft Word's limited ability to remember the history of a document has already caused scandals where the recipient of a document was able to view previous drafts of the document, and publicized their contents.
Imagine the complications if a PDF of a recommendation letter was inseparable from the versions of the Word file from which it was created!
If we cannot erase sensitive remembrances from a disk, then the disk cannot be given to a new owner.
Similarly, shared computing facilities rely on our ability to erase the memory of executions of sensitive programs.
Digital rights management facilities often depend on computers' ability to forget digital information.
Laws such as HIPAA [8] require that certain kinds of electronic records only be kept for a limited period of time, and be destroyed thereafter.
Thus we need iron-clad ways to ensure that sensitive remembrances either do not fall into inappropriate hands, or are unintelligible if they do.Usability challenges.
Remembrances may be used directly by a self-aware computer system to tune and improve itself, or by a human who needs historical information.
Both kinds of interfaces will be challenging to provide at an appropriate level of abstraction.
This problem has been addressed in individual history-aware systems, but never in a manner intended to span multiple independent systems with autonomy.Philosophical challenges.
The preceding discussion raises fundamental questions about our current notions of what an object is and what object identity means.Limiting remembrance.
The foregoing discussion suggests that even if we could, it may be better for computer systems not to remember everything.
The fact that living beings tend to forget things also suggests that there is value in the ability to forget.
Psychological studies of humans who do remember everything [24,7] strongly support this conclusion, explaining that people cursed with a perfect memory (a medical condition called Superior Autobiographical Memory Syndrome or Hyperthymesia) find themselves overwhelmed by memories, distracted by memories, and/or unable to abstract away from the details of their memories.
Thus we postulate that systems with remembrance also need the ability to forget -immediately raising the question of what to forget.In some situations, specific hard-coded policies will dictate what to forget.
For example, companies often prefer to delete routine business documents once their mandated retention period has ended [25].
This policy is in place because companies have learned that the legal liabilities that result from retaining such documents, which can be subpoenaed in lawsuits, exceed their internal value to the company.
This raises the larger question of how a computer system can learn such policies automatically.An analogy to the short-term and long-term memory of humans may be helpful here.
We retain small details in memory for a short period because they are most likely to be useful during that period.
For example, a person (let us call her Alice) can remember what she ate for lunch today; that knowledge may guide her choice of food for dinner.
If Alice tried hard, she could probably remember what she ate for lunch two days ago.
But Alice will not be able to remember what she ate for lunch one month ago.
Her memory has automatically removed that detail because she has not made use of it for an extended period.
This suggests that as a default policy, routine remembrances in a computer system may fade away gradually, becoming less easily accessible over time, until they reach a threshold where they are forgotten entirely.
For example, the details of the execution of a program may no longer be worth remembering once the program has finished executing.
However, if the program is considered important, such as a financial transaction or a change in the system configuration, the remembrance may be retained for an extended period, in logs or in other forms.
Further, there may be high value in remembering the recent details of the execution of a program, because they can allow us to recover automatically from certain classes of failures [41].
In humans, detailed memories are often replaced by general patterns that we learn from them.
For example, we learn that Valentine's Day often involves giving and receiving flowers, cards, and candy, though we may forget the details of individual instances of Valentine's Day.
Similarly, before remembrances fade, the computer system should use data mining techniques to learn whatever useful patterns it can glean from them.
At the simplest level, such techniques can be used to improve the tuning of the system and plan future allocations of resources.
Further, with system-wide remembrance in place, we will have new opportunities for automated tuning and learning.
We have briefly presented our vision of the opportunities and challenges of a world where computing systems are self-aware and remember important aspects of their history and evolution.
While bits and pieces of this vision already exist in some applications, these pieces have never been tied together into a seamless continuum.
If we can make systems sentient of their old context, lineage, and values, and address the resulting challenges for performance and usability, then we have the potential to reach new levels of self-tuning in computer systems and support many exciting new user-level applications.
Hasan and Winslett were supported in part by NSF awards CNS-0716532 and CNS-0803280.
Sion was supported in part by the NSF through awards CNS-0627554, CNS-0716608, CNS-0708025, and IIS-0803197.
Sion would also like to thank Motorola Labs, Xerox PARC, IBM Research, the IBM Software Cryptography Group, CEWIT, and the Stony Brook Office of the Vice President for Research.
