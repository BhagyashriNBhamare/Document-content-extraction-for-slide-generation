Memory access is the major bottleneck in realizing multi-hundred-gigabit networks with commodity hardware, hence it is essential to make good use of cache memory that is a faster, but smaller memory closer to the processor.
Our goal is to study the impact of cache management on the performance of I/O intensive applications.
Specifically, this paper looks at one of the bottlenecks in packet processing, i.e., direct cache access (DCA).
We systematically studied the current implementation of DCA in Intel® processors, particularly Data Direct I/O technology (DDIO), which directly transfers data between I/O devices and the processor's cache.
Our empirical study enables system designers/developers to optimize DDIO-enabled systems for I/O intensive applications.
We demonstrate that optimizing DDIO could reduce the latency of I/O intensive network functions running at 100 Gbps by up to ~30%.
Moreover, we show that DDIO causes a 30% increase in tail latencies when processing packets at 200 Gbps, hence it is crucial to selectively inject data into the cache or to explicitly bypass it.
While the computer architecture community continues to focus on hardware specialization, the networking community tries to achieve greater flexibility with Software-defined Networking (SDN) together with Network Function Virtualization (NFV) by moving from specialized hardware toward commodity hardware.
However, greater flexibility comes at the price of lower performance compared to specialized hardware.
This approach has become more complex due to the end of Moore's law and Dennard scaling [14].
Furthermore, commercially available 100-Gbps networking interfaces have revealed many challenges for commodity hardware to support packet processing at multi-hundred-gigabit rates.
More specifically, the interarrival time of small packets is * Both authors contributed equally to the paper.
† This author has made all open-source contributions.shrinking to a few nanoseconds (i.e., less than Last Level Cache (LLC) latency).
Consequently, any costly computation prevents commodity hardware from processing packets at these rates, thereby causing a tremendous amount of buffering and/or packet loss.
As accessing main memory is impossible at these line rates, it is essential to take greater advantage of the processor's cache [81].
Processor vendors (e.g., Intel®) introduced new monitoring/controlling capabilities in the processor's cache, e.g., Cache Allocation Technology (CAT) [59].
In alignment with the desire for better cache management, this paper studies the current implementation of Direct Cache Access (DCA) in Intel processors, i.e., Data Direct I/O technology (DDIO), which facilitates the direct communication between the network interface card (NIC) and the processor's cache while avoiding transferring packets to main memory.
Our goal is to complete the recent set of studies focusing on understanding the leading technologies for fast networking, i.e., Peripheral Component Interconnect express (PCIe) [58] and Remote Direct Memory Access (RDMA) [37].
We believe that understanding & optimizing DDIO is the missing piece of the puzzle to realize high-performance I/O intensive applications.
In this regard, we empirically reverse-engineer DDIO's implementation details, evaluate its effectiveness at 100/200 Gbps, discuss its shortcomings, and propose a set of optimization guidelines to realize performance isolation & achieve better performance for multihundred-gigabit rates.
Moreover, we exploit a little-discussed feature of Xeon® processors to demonstrate that fine-tuning DDIO could improve the performance of I/O intensive applications by up to ~30%.
To the best of our knowledge, we are the first to: (i) systematically study and reveal details of DDIO and (ii) take advantage of this knowledge to process packets more efficiently at 200 Gbps.Why DCA matters?
Meeting strict Service Level Objectives (SLO) and offering bounded latency for Internet services is becoming one of the critical challenges of data centers while operating on commodity hardware [54].
Consequently, it is essential to identify the sources of performance variability in commodity hardware and tame them [51].
In computer systems, one of these sources of variability is the cache hierarchy, which can introduce uncertainty in service times, especially in tail latencies.
Additionally, the advent of modern network equipment [82] enables applications to push costly calculations closer to the network while keeping & performing only stateful functions at the processors [36,38], thereby making modern network applications ever more I/O intensive.
Hence, taming the performance variability imposed by the cache, especially for I/O, is now more crucial than before.
Moreover, as CPU core count goes up, it is important to be able to deliver appropriate I/O bandwidth to them.
Therefore, we go one level deeper [61] to investigate the impact of I/O cache management, done by DCA, on the performance of multi-hundred-gigabit networks.Contributions.
In this paper, we:1 Design a set of micro-benchmarks to reveal little-known details of DDIO's implementation * ( §4), 2 Extensively study the characteristics of DDIO in different scenarios and identify its shortcomings * ( §5), 3 Show the importance of balancing load among cores and tuning DDIO capacity when scaling up ( §6), 4 Measure the sensitivity of multiple applications (i.e., Memcached, NVMe benchmarks, NFV service chains) to DDIO ( §7), 5 Demonstrate the necessity and benefits of bypassing cache while receiving packets at 200 Gbps ( §8), 6 Discuss the lessons learned from our study that are essential for optimizing DDIO-enabled systems receiving traffic at multi-hundred-gigabit rates ( §9).
A standard method to transfer data from an I/O device to a processor is Direct Memory Access (DMA).
In this mechanism, a processor, typically instructed by software, provides a set of memory addresses, aka receive (RX) descriptors, to the I/O device.
Later, the I/O device directly reads/writes data from/to main memory without involving the processor.
For inbound traffic, the processor can be informed about newly DMA-ed data either by receiving an interrupt or polling the I/O device.
Next, the processor fetches the I/O data from main memory to its cache in order to process the data.
For outbound traffic, the processor informs the I/O device (via transmit (TX) descriptors) of data that is ready to be DMA-ed from main memory to the device.
The main source or destination of traditional DMA transfers is main memory, see Fig. 1a.
However, the data actually needs to be loaded into the processor's cache for processing.
Therefore, this method is inefficient and costly in terms of (i) number of accesses to main memory [43] (i.e., 2n + 5 for n cache lines [43]), (ii) access latency to the I/O data, and (iii) memory bandwidth usage.
Moreover, the negative impact of these inefficiencies becomes increasingly severe with higher link * The source code is available at: https://github.com/aliireza/ ddio-bench Figure 1: Different approaches of DMA for transferring data from an I/O device (e.g., NIC).
Red arrows show the path that a packet traverses before reaching the processing core.speeds.
For instance, a server has 6.72 ns to process small packets at 100 Gbps, whereas every access to main memory takes ~100 ns, 15× more expensive.
Therefore, placing the I/O data directly in the processor's cache rather than in main memory is desirable.
The advent of faster I/O technologies motivated researchers to introduce Direct Cache Access (DCA) [25,42,43].
DCA exploits PCIe Transaction Layer Packet Processing Hint [30], making it possible to prefetch portions of I/O data to the processor's cache, see Fig. 1b.
Potentially, this overcomes the drawbacks of traditional DMA, thereby achieving maximal I/O bandwidth and reducing processor stall time.
Although this way of realizing DCA can effectively prefetch the desired portions of I/O data (e.g., descriptors and packet header), it is still inefficient in terms of memory bandwidth usage since the whole packet is DMAed into main memory.
Additionally, this requires operating system (OS) intervention and support from the I/O device, system chipset, and processor [1].
To address these limitations and avoid ping-ponging data between main memory & the processor's cache, Intel rearchitected the prefetch hint-based DCA, introducing Data Direct I/O technology (DDIO) [28].
Intel introduced DDIO technology with the Xeon E5 family.
With DDIO, I/O devices perform DMA directly to/from Last Level Cache (LLC) rather than system memory, see Fig. 1c.
DDIO is also known as write-allocate-write-updatecapable DCA (wauDCA) [45], as it uses this policy to update cache lines in an n-way set associative LLC, where n cache lines form one set.
For packet processing applications, NICs can send/receive both RX/TX descriptors and the packets themselves via the LLC, thereby improving applications' response time & throughput † .
DDIO works as follows [41]: Writing packets.
When a NIC writes a cache line to LLC via PCIe, DDIO overwrites the cache line if it is already present in any LLC way (aka a PCIe write hit or write update).
Otherwise, the cache line is allocated in the LLC and DDIO writes the data into the newly allocated cache line (aka a PCIe write miss or write allocate).
In the latter case, DDIO is restricted to use only a limited portion of LLC when allocating cache lines.
It is possible to artificially increase this portion by warming up the cache with processor writes to the address of these buffers, then DDIO performs write-updates [16].
Reading packets.
A NIC can read a cache line from LLC if the cache line is present in any LLC way (aka a PCIe read hit).
Otherwise, the NIC reads a cache-line-sized chunk from system memory (aka a PCIe read miss).
To monitor DDIO and its interaction with I/O devices, Intel added uncore performance counters to its processors [29].
The Intel Performance Counter Monitor (PCM) tool (e.g., pcm-pcie.x * ) [86] can count the number of PCIe write hits/misses (represented as an ItoM event) and PCIe read hits/misses (represented as a PCIeRdCur event).
Next, we discuss the inherent problem of DDIO, which makes it hard to achieve low-latency for multi-hundred-gigabit NICs.
Researchers have shown some scenarios in which DDIO cannot provide the expected benefits [11,41,50,83].
Two typical cases occur when new incoming packets repeatedly evict the previously DMA-ed packets (i.e., not-yet-processed and already-processed packets) in the LLC.
Consequently, the processor has to load not-yet-processed packets from main memory rather than LLC and the NIC needs to DMA the already-processed packets from the main memory, thereby missing the benefits of DDIO.
Tootoonchian et al. referred to this problem as the leaky DMA problem [83].
To mitigate this problem, they proposed reducing the number of "in-flight" buffers (i.e., descriptors) such that all incoming packets fit in the limited portion of LLC used for I/O.
Thus, performance isolation can be done using only CAT (i.e., cache partitioning).
Unfortunately, reducing the number of RX descriptors is only a temporary solution due to increasing link speeds.
Multihundred-gigabit NICs introduce new challenges, specifically: 1 Packet loss.
At sub-hundred-gigabit link speeds reducing the number of RX descriptors may not result in a high packet loss rate, but at ≥100 Gbps packet loss increases due to the tight processing time budget before buffering/queuing happens.
For instance, every extra ~7 ns spent stalling or processing/accessing a packet causes another packet to be buffered when receiving 64-B packets at 100 Gbps.
When there are insufficient resources for immediate processing, increasing the number of RX descriptors permits packets to be buffered rather than dropped.
Delays in processing might occur because of interrupt handling, prolonged processing, or a sudden increase in the packet arrival rate [17]; therefore, multi-hundred-gigabit networks cannot avoid packet loss without having a sufficiently large number of descriptors.
Increasing the number of processing cores can reduce the packet loss rate, but applications that are compute-or memoryintensive require many cores to operate at the speed of the underlying hardware, e.g., Thomas et al. [81] mention that * The description of events can be found in [27] and pp. 63-66 of [41].
a server performing one DRAM access per packet needs 79 cores to process packets at 400 Gbps.
2 TX buffering.
One of the scenarios that makes DDIO inefficient is the eviction of already-processed packets.
Reducing the number of RX descriptors may solve this problem for systems that require a small number of TX descriptors, but this is not the case for 100-Gbps NICs.
Unfortunately, the de facto medium for DMA-ing packets (i.e., PCIe 3.0) induces some transmission limitations [58].
Consequently, packets often need to be buffered in the computer system for some time before being DMA-ed to the NIC.
This buffering can be realized by either a software queue or increasing the number of TX descriptors [35].
Unfortunately, either of these alternatives increases the probability of eviction of already-processed packets.
Therefore, completely solving the leaky DMA problem requires fine-tuning both the size of the software queue and the number of RX & TX descriptors.
3 PAUSE frames.
To alleviate packet loss, one can use Ethernet flow control mechanisms (e.g., PAUSE frames) that cause packets to be buffered earlier in the network, i.e., PAUSE frames stop the previous network node from transmitting packets for a short period.
However, these mechanisms are costly in terms of latency, making them less desirable than packet loss for time-critical applications.
The minimum and maximum pause duration of a 100-Gbps interface are 5.12 ns and 335.5 µs [56].
Our measurements show that a core that is simply forwarding packets at 100 Gbps with 1024 RX & TX descriptors causes the NIC to send ~179 k PAUSE frames while receiving ~80 M packets.
Dynamic reduction.
As reducing the number of RX buffers cannot fully solve the problem and it shifts the problem to another part of the network, most probably the previous node; therefore, an alternative is to dynamically reduce the pressure on the LLC when the number of I/O caused cache evictions starts to increase † .
These cache evictions can be tracked by monitoring either PCIe events or the length of the software queue.
After detecting a problem, the processor should fetch a smaller number of packets from the NIC (i.e., reducing the RX burst size).
Thus, the processor passes fewer free buffers to the NIC, reducing the number of DMA transactions.
Unfortunately, this approach does not perform well, hence we need a proactive solution, not a reactive one.
Is it sufficient to scale up?
Due to the demise of the Dennard scaling [14], processors are now shipped with more cores rather than higher clock frequencies.
Moreover, the per-core cache quota (i.e., LLC slices) has decreased in recent Xeon processors, i.e., the size of LLC slices reduced from 2.5 MiB to 1.375 MiB in the Xeon scalable family (i.e., Skylake) [55].
This reduction in per-core cache size directly affects the optimal number of descriptors as these are proportional to the limited space for DDIO.
For instance, using 18 cores, each having 256 RX descriptors, requires ~6.5 MiB, which is equal to ~26.6% of the LLC in this processor and greater than the available DDIO capacity (see §4.1).
Our approach.
To overcome these challenges, it is necessary to study and analyze DDIO empirically in order to make the best use of it.
A better understanding of DDIO and its implementation can help us optimize current computer systems and enables us to propose a better DCA design for future computer systems that could accommodate the ever-increasing NIC link speeds.
For instance, Fig. 2 demonstrates that tuning DDIO's capacity makes it possible to achieve a suitable performance while using a large number of descriptors (our approach), as opposed to using a limited number of descriptors (ResQ's approach proposed by Tootoonchian et al. [83]).
Figure 2: Using more DDIO ways ("W") enables 2 cores to forward 1500-B packets at 100 Gbps with a larger number of descriptors while achieving better or similar tail latency.
Initially, Intel announced that DDIO only uses 10% of LLC [28] and did not mention what part of the LLC is used (i.e., ways, sets, or slices [15]).
Recent Intel technical reports mention that DDIO only uses a subset of LLC ways, by default two ways [41,72].
However, it is still unclear whether this "subset" is fixed or whether it can be dynamically selected using a variant of Least Recently Used (LRU) policies [33,34,65,87].
Knowledge of these details could avoid I/O contention and optimize performance isolation [83] by performing precise cache management/partitioning [13,62] (e.g., way partitioning with CAT [59]).
This issue becomes increasingly critical for newer generations of Xeon processors that have lower LLC set-associativity (e.g., 11 ways in some Skylake processors, as opposed to 20 ways in Haswell processors), thereby using a larger portion ( 2 11 ≈ 18%) of the LLC for I/O.
Lower set-associativity makes the cache less flexible when the LLC is divided into multiple partitions, each of which could be used to accommodate different applications' code & data.
To clarify this, we assumed that the ways that are used for DDIO are fixed and then try to confirm this with an experiment in which we co-run an I/O and a cachesensitive application.
To increase the pressure on the LLC by DMA-ing more cache lines, we used an L2 forwarding DPDK-based application as the I/O intensive application.
Specifically, it receives large packets (1024-B) at a high rate (~82 Gbps) using a large number of RX descriptors (4096 RX descriptors).
For the cache-sensitive application, we chose water_nsquared from the Splash-3 benchmark suite [62,66,69] since it performs a large number of LLC accesses; hence, it interferes with the I/O application.Each application is run on a different core and CAT is used to allocate different cache ways to each core.
We allocate two fixed ways to the I/O application and two variable ways to the cache-sensitive application.
To avoid memory bandwidth contention, we also used Memory Bandwidth Allocation (MBA) technology [21] to limit the memory bandwidth of each core to 40%.
Fig. 3a shows the CAT configuration used in the experiment.
We start by allocating the two leftmost ways (i.e., bitmask of 0x600) to the cache-sensitive application and then we keep shifting the allocated ways one to the right until we cover all the LLC ways while measuring the LLC misses of the I/O application.
Fig. 3b shows the results of this experiment.
These results demonstrate that the cache-sensitive application interferes with the I/O application in two regions.
The first (see 0x0C0 in Fig. 3b) occurs when the cache-sensitive application uses the same ways as the I/O application, due to the code/data interference of the two applications.
However, the second (see 0x003 in Fig. 3b) cannot be explained with this same argument since the I/O application is limited to using other ways (i.e., 0x0C0).
Furthermore, since the CPU socket is isolated, no other application can cause cache misses.
CAT only mitigates the contention induced by code/data not DDIO.
Therefore, we conclude that the second interference is most probably due to I/O, which means DDIO uses the two rightmost ways in LLC (i.e., bitmask of 0x003).
The interference is proportional to the number of received packets per second × average packet size.
We expected to see roughly the same amount of cache misses for bitmasks of 0x180 and 0x060, as they are completely symmetrical in terms of way occupancy.
However, the undocumented LRU policy of the CPU may affect how the application uses the cache ways.
One of the established mechanisms to ensure performance isolation and mitigate cache contention is CAT, which limits different applications to a subset of LLC ways.
However, §4.1 showed that DDIO uses two fixed LLC ways.
Therefore, isolating applications using CAT may not fully ensure performance isolation, due to cache contention caused by I/O.
Such contention may occur in two common scenarios:1 I/O vs. Code/Data.
When an application is limited to using those ways which are also used by DDIO, then cache lines allocated in LLC for DDIO may evict the code/data of any application (i.e., either I/O or non-I/O application).
This issue was discussed by Tootoonchian et al. [83].
Their proposed framework, ResQ, uses only 90% of LLC to avoid interfering with DDIO's reserved space, but does not mention which part of LLC is isolated.
§4.1 showed the destructive (i.e., ~2.5×) impact on the cache misses of the I/O application due to a cache-hungry application overlapping with DDIO, see the rise in cache misses at the right side of Fig. 3b.
However, it did not show the impact of contention on the cache-hungry application; therefore, we repeated the experiment and measured the cache misses of the cache-sensitive application while using a lighter configuration.
Fig. 4 illustrates that the cache misses of the cache-sensitive application were similarly adversely affected.
Therefore, overlapping any application with DDIO ways in LLC can reduce the performance of both applications.
To tackle this, one can isolate the I/O portion of LLC (e.g., the two ways used for DDIO) by using CAT so that applications share the LLC without overlapping with I/O.
Comparing Fig. 3b and 4, we see that an unexpected rise (almost 3×) in cache misses occurs in a different region (i.e., bitmask of 0x600 in Fig. 4 as opposed to bitmask of 0x003 in Fig. 3b) when I/O is evicting code/data.
Hence, we speculate that CAT does not use a bijective function to map I/O & code/data to ways, thus f : code/data → Ways is not equivalent to g : I/O → Ways .
Specifically, I/O evicts code/data when the latter is located in the two leftmost ways whereas code/data evicts I/O when the latter is using the two rightmost ways.
Such information is useful to know, as it will give us an understanding of the eviction policy and the default priority of code/data and I/O.2 I/O vs. I/O.
When multiple I/O applications are isolated from each other with CAT, they could still unintentionally compete for the fixed ways allocated to DDIO.
§8.1 elaborates the negative impact of this type of contention.Security implication.
Since DDIO uses two fixed ways in LLC, it is possible to extend microarchitectural attacks to extract useful information from I/O data (e.g., NetCAT [44] and Packet Chasing [76,77]).
Furthermore, I/O applications can be vulnerable to performance attacks.
According to Intel [16,32], the current implementation of DDIO only affects the local socket.
Consequently, if a core accesses I/O data from an I/O device connected to a remote socket, the data has to traverse the inter-core interconnect, i.e., Intel QuickPath Interconnect (QPI) or Intel Ultra path Interconnect (UPI).
It was uncertain whether data traversing the inter-core interconnect is loaded into the LLC of the remote socket or not.
We clarified this by running the same experiment discussed in §4.2 while the NIC is connected to a remote socket.
The result (removed for brevity) showed that cache misses of neither application were affected by the I/O cache lines, hence packets coming through the UPI links do not end up in the local LLC.
Additionally, the cache misses of the I/O application dramatically increased to 20× greater than when receiving packets via the local socket without any contention.
Thus, DDIO is ineffective for the remote socket and it pollutes the LLC on the socket connected to the NIC.
Although [20,72] mention that DDIO uses two ways by default, there is no mention of whether it is possible to increase or decrease the number of ways used by DDIO.
A little-discussed Model Specific Register (MSR) called "IIO LLC WAYS" with the address of 0xC8B * is discussed in a few online resources [64,79] and server manuals [73,74].
For Skylake, the default value of this register is equal to 0x600 (i.e., two bits set).
While these bits cannot be unset, it is possible to set additional bits and the maximum value for this register on our CPU is 0x7FF (i.e., 11 bits set: the same as the number of LLC ways).
New values for this register follow the same format as CAT bitmasks.
On * One can read/write this register via msr-tools (e.g., rdmsr and wrmsr).
a processor with the Skylake microarchitecture, these new values should contain consecutive ones, while the Haswell microarchitecture does not require this (i.e., allowing any value in [0x60000, 0xFFFFF]).
To see whether this MSR register has an effect on performance, we measured the PCIe read/write hit rates (i.e., ItoM and PCIeRdCur events) while using different values for IIO LLC WAYS.
We calculate the hit rate based on the number of hits and misses during an experiment where an I/O application processes packets of 1024 B at 100 Gbps while using 4096 RX descriptors.
Fig. 5 shows that increasing the value of this MSR register leads to a higher PCIe read/write hit rate.
This suggests that increasing the value of this register could improve the ability of the system to handle packets at high rates.
We believe that the value of this register is positively correlated with the fraction of LLC used by DDIO.
Using the technique in §4.1, we could not detect the newly added I/O ways, thus we speculate that the newly added ways follow a different policy (e.g., LRU) than the first two ways used for I/O.
Therefore, we assume that the number of bits set specifies the number of ways used by DDIO.
Disabling DDIO.
DDIO is bundled as a part of Intel Virtualization Technology (Intel VT), hence it is possible to enable/disable it in BIOS for some vendors [16,23,88].
According to [44,72], DDIO can be disabled globally (i.e., by setting the Disable_All_Allocating_Flows bit in "iiomiscctrl" register) or per-root PCIe port (i.e., setting bit NoSnoopOpWrEn and unsetting bit Use_Allocating_Flow_Wr in "perfctrlsts_0" register).
Some brief discussions of the benefits of disabling DDIO exist [11,78], but we elaborate this more thoroughly in §7.
We implemented an element for FastClick, called DDIOTune, which can enable/disable/tune DDIO † .
This section scrutinizes the performance of DDIO in different scenarios while exploiting the tuning capability of DDIO.
The goal is to show where DDIO becomes a bottleneck and when tuning DDIO matters.
Therefore, we examined the impact of both system parameters (i.e., #RX descriptors, #cores, and processing time) and workload characteristics (i.e., packet size and rate) on DDIO performance.
All of these measurements were done 20 times for both Skylake and Haswell microarchitectures.
We observed the same behavior in both cases, but only discuss the Skylake results for the sake of brevity.
We initially focus on the performance of an L2 forwarding network function, as an example of an I/O intensive application.
Later, we discuss the impact of applications requiring more processing time per packet.
Size and RX Descriptors §3.1 discussed the negative consequence of a large number of RX descriptors on DDIO performance.
This section continues this discussion by looking at the PCIe read/write hit rate metrics for different numbers of RX descriptors and different packet sizes.
Fig. 6 shows the results of our experiments for PCIe write hit rate.
PCIe read hit rates (not included for brevity) demonstrate similar behavior.
When packets are >512 B, the PCIe read/write hit rates monotonically decrease with an increasing number of RX descriptors.
More specifically, sending 1500-B packets, even with a relatively small number of RX descriptors (i.e., 128), causes 10% misses for both PCIe read and PCIe write hit rates.
Furthermore, increasing the number of RX descriptors to 4096 makes DDIO operate at ~40% hit rate, hence 60% of packets require cache allocation and they had to be DMA-ed back to the NIC from main memory rather than LLC.
Note that the packet generator is generating packets as fast as possible.
Therefore, small packets show the case when the arrival rate is maximal, while large packets demonstrate maximal throughput, see Fig. 7 i.e., the number of received/processed packets per second, due to NIC and PCIe limitations.
Note that our testbed cannot exceed 90 Gbps when only one core is forwarding packets.
Unexpected I/O evictions.
In some cases (e.g., 1500-B packets with 128 RX descriptors in Fig. 6), the size of the injected data is smaller than the DDIO capacity (i.e., 187.5 KiB 4.5 MiB).
Even taking into account the TX descriptors and the FastClick's software queue, the maximum cache footprint of this workload is ~2 MiB.
However, DDIO still experiences ~10% misses.
We believe that this behavior may occur when an application cannot use the whole DDIO capacity due to (i) the undocumented cache replacement policy and/or (ii) the cache's complex addressing [15], thus multiple buffers may be loaded into the same cache set.
Processing Time §5.1 demonstrated that DDIO performs extremely poorly when a core does minimal processing at 100 Gbps.
Next, we focus on the worst-case scenario of the previous experiment (i.e., sending 1500-B packets with 4096 RX descriptors) while changing the packet rate.
To achieve 100 Gbps, we use two cores.
Fig. 8 shows the PCIe read and PCIe write hit rates.
The PCIe read metric results reveal that DDIO performs relatively well until reaching 98 Gbps.
However, the PCIe write results indicate that DDIO has to continually allocate cache lines in LLC for 25% of packets at most of these throughputs, due to insufficient space for all of the buffers.
Moreover, throughputs above 75 Gbps exacerbate this problem.
So far, we analyzed DDIO performance when cores performed minimal processing (i.e., swapping MAC addresses).
Now, we analyze DDIO performance for more compute/memory-intensive I/O applications.
Memoryintensive applications access memory frequently and execute few instructions per memory access.
The time to accessing memory differs depending upon the availability of a cache line in a given part of the memory hierarchy.
Therefore, we focus on the number of CPU cycles of the computation; noting that a memory access can be accounted for as given number of cycles.
Note that increasing the processing time can change the memory access pattern, as packets continue to be injected by the NIC while some packets are enqueued in the LLC.
To see the impact of different packet processing times on the performance of DDIO, we vary the amount of computation per packet by calling the std::mt1993 random number generator multiple times.
Ten such calls take ~70 cycles.
Fig. 9 illustrates the effect of increasing per-packet processing time on the PCIe metrics & achieved throughput.
These results demonstrate that increasing processing time slightly improves PCIe read hits rates up to ~60 calls, i.e., 400 cycles.
This is expected, as increasing processing makes the application less I/O intensive as the application provides buffers to the NIC at a slower pace.
However, increasing processing causes the available processing power (i.e., #cores) to become a bottleneck, substantially decreasing throughput.
Similarly, PCIe write hit rates increases after exceeding 60 calls, due to a decrease in throughput & amount of cache injection.
Therefore, DDIO performance matters most when an application is I/O bound, rather than CPU/memory bound.
When processing power limits an application's performance, the system should scale up/out.
However, this scaling can affect DDIO's performance.
To see the effect of scaling up, we measured the PCIe metrics while different numbers of cores were forwarding large packets.
Fig. 10 shows that when an application is I/O intensive, increasing the number Figure 9: Making an application more compute-intensive results in better PCIe metrics, but lower throughput.
In addition to forwarding packets, two cores call a dummy computation, while receiving 1500-B packets with a total of 4096 RX descriptors at 100 Gbps.of cores improves the PCIe read/write hit rate, as it enhances the packet transmission rate because of more TX queues and faster consumption of packets enqueued in the LLC.To avoid synchronization problems, every queue is bound to one core.
However, beyond a certain point (i.e., four cores in our testbed), increasing the number of cores causes more contention in the cache, as every core loads packets independently into the limited DDIO capacity.
Furthermore, since newer processors are shipped with more cores, scaling up, even with a small number of RX descriptors, eventually causes the leaky DMA problem-the same problem as having a large number of descriptors (see §3.1).
Fig. 11 shows PCIe metrics for 1, 2, and 4 cores while changing the number of DDIO ways.
Comparing the DDIO performance of different numbers of cores/DDIO ways, we conclude that increasing DDIO capacity leads to similar improvements for PCIe metrics.
Therefore, increasing the DDIO capacity rather than the number of cores is beneficial when an application's bottleneck is not processing power or number of TX queues.
Unless scaling up happens efficiently, some cores may receive more packets than others, causing The previous section focused on the PCIe read/write hit rates and showed that increasing link speed & packet size and the number of descriptors & cores could degrade these metrics.
PCIe read/write hit rates represent the percentage of I/O evictions (i.e., the performance of DDIO), but also indirectly affect application performance.
The correlation between PCIe metrics and meaningful performance metrics (e.g., latency and throughput) depends on an application's characteristics.
For instance, a low PCIe write hit rate can severely affect an application that requires the whole DMA-ed data.
Conversely, the impact is much less for an application that needs only a subset of the DMA-ed packet.
Fig. 2 showed one example of this correlation for the latter case, where the application only accessed the packet header.
These results showed that even when an application does not require the whole DMAed data, increasing the number of descriptors (i.e., causing a reduction in PCIe hit rate metrics) could negatively affect the 99 th percentile latency.
Note that we observed the same effect at median latency.
This section further elaborates this impact in two scenarios where a stateful network function is processing a realistic workload * via 18 cores with a run-tocompletion model [38,93].
The benefits of increasing cache performance are not limited to this model and could be even greater for a pipeline model where fewer cores handle the I/O.
Stateful service chain.
To evaluate the effect of increasing DDIO capacity, we chose a stateful service chain composed of a router, a network address port translator (NAPT), and a round-robin load balancer (LB) as a suitable chain to exploit hardware offloading capabilities of modern NICs while still keeping state at the processor.
In this case, we offload the * We replay the first 400 k packets of a 28-minute campus trace fifty times.
The full trace has ~800 M packets with an average size of 981 B.routing table of the router to the NIC and only handle the stateful tasks (i.e., NAPT + LB) and the basic functionality of the router in software.
We generated 2423 IP filter rules for the campus trace using the GenerateIPFlowDirector element in Metron [38] and use DPDK's Flow API technology [31] to offload them into a Mellanox NIC.
To examine the impact of load imbalance, we generate two different sets of rules with different load imbalance factors.
One distributes the rules among 18 cores in a round-robin manner while the other is load-aware and tries to reduce the flow imbalance in terms of bytes received by every core.
We calculated the number of packets received by each core for both cases and the maximum imbalance ratio of a core is 2.78× for the load-aware technique, while the round-robin technique causes 1.69× maximum load imbalance.
The load-aware method has a higher load imbalance because we generate rules for the whole trace, but only replay a subset of it.
Fig. 12 shows the 99 th percentile latency of this chain for different load balancing methods (with different load imbalance ratio), specifically increasing DDIO capacity reduces the 99 th percentile latency by ~21% when the load imbalance is higher.
However, when the load imbalance is lower, these improvements reduce to ~2%.
A higher load imbalance factor means that a core receives more packets than others, some of which could be evicted while enqueued in the LLC.
Hence, it is crucial to realize a good balance to get the most out of DDIO.
Furthermore, load imbalance is the root cause of many other performance degradations and is hard to prevent [8,10].
Figure 12: DDIO should be carefully tuned when the load imbalance factor is higher.
The results shows 99 th percentile latency of a stateful network function while 18 cores are processing mixed-size packets at 100 Gbps.
The throughputs were 94 & 97 Gbps for load-ware (higher imbalance) & roundrobin (lower imbalance) experiments, respectively.
The previous section showed that performance could be improved by tuning DDIO for I/O intensive network functions operating at ~100 Gbps.
However, these results cannot be generalized, as the improvements are highly dependent on the application's characteristics.
Moreover, there may be some applications that do not benefit from DDIO tuning.To investigate this, we measure the sensitivity of different applications to DDIO by enabling/disabling it (see §4.4).
Table 2 shows the results for four applications/benchmarks: (i) DPDK-based implementation of Memcached developed by Seastar [5], (ii) an NVMe benchmarking tool (i.e., fio [4]), (iii) L2 forwarding application, (iv) a stateful service chain, used in §6, which performs IP filtering in software rather than offloading it to the NIC, and (v) the stateful service chain with round-robin offloading used in §6.
We define sensitivity as "Low" if the maximum impact on the performance of an application is ≤ 5%.
For Memcached, we use the method recommended by Seastar [2] with 8 instances of memaslap clients running for 120 s and a Memcached instance with 4 cores.
For NVMe benchmarks, we tested a Toshiba NVMe (KXG50PNV1T02) with 4 × 1024-GB SSDs according to [3], where we report the average of 10 runs.
The L2 forwarding application forwards mixed-size packets, while using 4 cores with a total of 4096 RX descriptors.
The stateful service chain without offloading uses RSS to distribute packets among 18 cores (to increase the throughput) with 18 × 256 RX descriptors.
The results demonstrate that different applications have different levels of sensitivity to DDIO, which can be exploited by system developers to optimize their system in a multi-tenant environment, where multiple I/O applications co-exist, see §8.1.
The most sensitive application is L2 forwarding, which is the most I/O intensive application among these applications and can run at line rate.
Some applications (e.g., Memcached) experience less benefit from DDIO, as their performance may be bounded by other bottlenecks.
A more detailed sensitivity analysis of different applications remains as our future work.
Tuning DDIO occupancy was shown to substantially improve the performance of some applications.
However, increasing the portion of the cache used for I/O is only a temporary solution for two reasons: (i) I/O is only a part of packet processing and (ii) to achieve suitable performance many networking applications require a large amount of cache memory for code/data.
Moreover, many network functions would benefit from performing in-cache flow classification [92]; hence, there is a trade-off between allocating cache to I/O vs. code/data and this trade-off depends on the application's characteristics & cache size.Additionally, since DDIO is way-based, the granularity of partitions is quite coarse in recent Intel processors, due to low set-associativity.
Therefore, it is harder to partition the cache fairly between code/data & I/O.
These reasons, together with the recent trend in Intel processors of decreasing per-core LLC, eventually make the current implementation of DCA a major bottleneck to achieving low-latency service times.
Hence, DCA needs to deliver better performance even with a small fraction of the cache.
This makes it necessary to rethink the current DCA designs with an eye toward realizing network services running at multi-hundred gigabits per second.
Some possible directions/proposals for future DCA are: 1 Fine-grained placement: adopting CacheDirector [15] methodology (i.e., sending packets to the appropriate LLC slices) and only sending the relevant parts of these packets to the L2 cache, L1 cache, or potentially CPU registers [26]; 2 Selective DMA/DCA: only DMA relevant parts of the packet (as required by an application) to the cache and buffer the rest in either main memory, the NIC, or Top-of-Rack switch; and 3 I/O isolation: extend CAT to include I/O prioritization in addition to Code and Data Prioritization (CDP) technology [60] to alleviate I/O contention.
These ideas could be simulated in a cycle accurate simulator (e.g., gem5 [6,12]), which remains as our future work.
Next, we examine one potential solution in the current systems to better take advantage of DDIO.8.1 Bypassing Cache §3.1 explained that one way to prevent unnecessary memory accesses and the leaky DMA problem is to reduce the number of descriptors.
However, this could increase packet loss and generate more PAUSE frames at high link rates.
Unfortunately, both can have a severe impact on the service time as they postpone the service time by at least a couple of microseconds.
Taking these consequences into account, we believe future DCA technologies should perform cache injection more effectively: DMA should not be directed to the cache if this would cause I/O evictions; thus, buffering packets in local memory (at a cost of only several hundreds of nanoseconds) is preferable to dropping or enqueuing packets in previous nodes.
Additionally, bypassing cache would be beneficial in a multi-tenant scenario where performance isolation is desired.
For instance, low-priority and/or low-DDIO-sensitive applications could bypass cache to make room for high-priority and/or high-DDIO-sensitive applications.
In addition, one could prioritize [7] different traffic flows, thus only a subset of received traffic (and hence cores) would use cache for I/O.
Implementing a system to prioritize DDIO for different flows either in a programmable switch or modern NICs (e.g., Mellanox Socket Direct Adapters) remains as our future work.
Evaluation.
To evaluate the benefits of bypassing the cache, we use two methods: (i) disabling DDIO and (ii) exploiting DMA via a remote socket (see §4.3).
We set up a 200-Gbps testbed, see Fig. 13.
We first connect two 100-Gbps NICs to the same socket.
Next, we connect one of these NICs to a remote socket.
We run two instances of L2 forwarding application located on the first socket, each of which uses 4 cores and one NIC to forward mixedsize packets.
We chose four cores per NIC because our earlier experiments (see Fig. 10) showed that DDIO can achieve an acceptable performance while receiving 1500-B packets with four cores.
To reduce the contention for cache and memory bandwidth, we apply CAT & MBA to each application (similar to ResQ [83]).
We assume that one of the applications has a higher priority, and we measure its latency in five different scenarios: (i) without the presence of the lowpriority application, (ii) when the low-priority application pollutes the cache via 2-way DDIO (see Fig. 13a), (iii) when the low-priority application pollutes the cache via 4-way DDIO, (iv) when the low-priority application bypasses the cache by DMA-ing packets via a remote socket (see Fig. 13b), (v) when the low-priority application bypasses the cache via disabled DDIO.
Fig. 14 shows the 99 th percentile latency of the high-priority application-other percentiles show a similar trend with a smaller difference.
These results demonstrate that bypassing cache via a remote socket (i.e., case iv) achieves the same latency as when there is no low-priority application (i.e., case i).
However, when both applications are receiving traffic via DDIO (i.e., case ii), the 99 th percentile latency degrades ~30%.
We observe that bypassing cache has the same benefits as increasing DDIO capacity (i.e., case iii vs. case iv).
Furthermore, comparing cases (iv) and (v) indicates that disabling DDIO slightly pollutes the cache (as opposed to bypassing via a remote socket).
We speculate that disabling DDIO only affects the packets, not the descriptors.
Therefore, we conclude that bypassing cache can result in less variability in performance and potentially better performance isolation.
Additionally, it is clearly necessary to tune DDIO capacity when moving toward 200 Gbps.
This section summarizes our key findings, which could help system designers/developers to optimize DDIO for their applications.
Furthermore, our study should inspire computer architects to improve DCA's performance by offering increasing control.
Although we focused on packet processing, our work is not limited to network functions.
Our investigations could be equally useful in other contexts (e.g., HPC) that require high-bandwidth I/O when transferring data via RDMA and processing with GPUs.
We showed that current approaches to avoid DDIO becoming a bottleneck are only temporary solutions and they are inapplicable to multi-hundred-gigabit network applications.
We proposed a benchmarking method to understand the unknown & little-discussed details of DDIO.
Later, we characterized the performance of DDIO in different scenarios and showed the benefits of bypassing the cache.
We concluded that there is no one-size-fits-all approach to utilize DDIO.
Our study reveals:• The locations of LLC to which DDIO injects data ( §4.1).
• Co-locating an application's code/data with I/O in the cache could adversely impact its performance ( §4.2).
• The way that DDIO behavior changes for different system parameters and workload characteristics ( §5).
• If an application is I/O bound, adding excessive cores could degrade its performance (Fig. 10).
• If an application is I/O bound, carefully sizing the DDIO capacity can improve its performance and could lead to the same improvements as adding more cores (Fig. 11).
• If an application starts to become CPU bound, adding more cores can increase its throughput, but then it has to balance load among cores to maximize DDIO benefits (Fig. 12).
• If an application is truly CPU/memory bound, DDIO tuning is less efficient (Fig. 9).
However, it can be beneficial to buffer in DRAM incoming requests/packets which cannot be processed in time, rather than having the NIC issue PAUSE frames or drop packets.
• Going beyond ~75 Gbps can cause DDIO to become a bottleneck (Fig. 8).
Therefore, it is essential to bypass cache to realize performance isolation.
Bypassing cache could be done for low-priority traffic or applications that do not benefit from DDIO ( §8.1).
• Different applications have different levels of sensitivity to DDIO ( §7).
Identifying this level is essential to utilize system resources more efficiently, provide performance isolation, and improve performance.
The most relevant work to our study is ResQ [83], which we discussed thoroughly in §3.1 and §8.1.
This section discusses other efforts relevant to our work.
Injecting I/O into the cache.
The idea of loading I/O data directly to the processor's cache was initially proposed using cache injection techniques [52,63].
Later, it was used to enhance network performance on commodity servers and was referred to as DCA [25].
Amit Kumar et al. [42] investigated the role of coherency protocol in DCA.
Their results indicated that the benefit of DCA would be limited when the network processing rate cannot match the I/O rate.
In addition, [75] showed that DCA could cause cache pollution; hence they proposed an alternative cache injection mechanism to mitigate the problem.
A. Kumar et al. [43] characterized DCA for 10-Gbps Ethernet links.
Other works have discussed that DCA is insufficient due to architectural limitations [40,46,71].
For example, the work in [46] proposed a new I/O architecture that decouples and offloads I/O descriptor management from the NIC to an on-chip network engine.
Similarly, the work in [40] proposed a flexible network DMA interface which can support DCA.
Last but not least, Wen Su et al. [71] proposed an improvement to combine DCA with an integrated NIC to reduce latency.
Efforts toward realizing 100 Gbps.
Many have tried to tackle challenges to achieve suitable performance for fast networks, mostly in the context of NFV [49] and key-value stores [19,45].
Some research has exploited new features in modern/smart/programmable NICs (e.g., [38,47,84,94]) & switches (e.g., [36]) or proposed new features (e.g., [70]) to offload costly software processing.
A number of works investigate packet processing models (e.g., [9,39,93]).
CacheBuilder [80] and CacheDirector [15] have discussed the importance of cache management in realizing 100-Gbps networks.
HALO [92] exploited the non-uniform cache architecture (NUCA) characteristics of LLC to perform incache flow classification.
Last but not least, IOctopus [68] proposed a new NIC design and wiring for servers to avoid non-uniform DMA penalties.
Our work is complementary to these works.
Cache partitioning.
Many have tried to overcome cache contention by performing cache partitioning [53].
These efforts can be split into two main categories: (i) software techniques and (ii) hardware techniques.
The former group principally relies on physical addresses to partition cache based on sets [22,48,67] or slices [15].
This way of cache partitioning does not require any hardware support, but it is not very commonly used, due to its drawbacks (e.g., OS/App modification and costly re-partitioning).
The latter group mostly exploits way-partitioning (e.g., CAT) to partition the cache among different applications [13,18,62,89,90,91].
In addition to these techniques, Wang et al. [85] proposed a hybrid approach that combines both techniques to achieve finer granularity for partitioning.
To the best of our knowledge, there are only two works (ResQ [83] and CacheDirector [15]) that have specifically tried to exploit cache partitioning techniques to improve packet processing.
ResQ proposes to isolate a percentage of LLC that is used for I/O and CacheDirector exploits the NUCA used in Intel processors to distribute I/O more efficiently among different LLC slices.
Our work is complementary to these works, as most of them do not consider I/O when partitioning the cache.
DCA technologies were introduced to improve the performance of networking applications.
However, we systematically showed that the latest implementation of DCA in Intel processors (i.e., DDIO) cannot perform as needed with increasing link speeds.
We demonstrated that better I/O management is required to meet the critical latency requirements of future networks.
Our main goal is to emphasize that networking is, now more than before, tightly coupled with the capability of the current hardware.
Consequently, realizing time-critical multi-hundred-gigabit networks is only possible by (i) increasingly well-documented control over the hardware and (ii) improved holistic system design optimizations.
We would like to thank our shepherd, Mark Silberstein, and anonymous reviewers for their insightful comments.
We are grateful to Tom Barbette for helping us with his NPF tool.
This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.
The work was also funded by the Swedish Foundation for Strategic research (SSF).
This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 770889).
