The analysis of incomplete data is a long-standing challenge in practical statistics.
When, as is typical, data objects are represented by points in R d , incomplete data objects correspond to affine subspaces (lines or ∆-flats).
With this motivation we study the problem of finding the minimum intersection radius r(L) of a set of lines or ∆-flats L: the least r such that there is a ball of radius r intersecting every flat in L. Known algorithms for finding the minimum enclosing ball for a point set (or clustering by several balls) do not easily extend to higher-dimensional flats, primarily because "distances" between flats do not satisfy the triangle inequality.
In this paper we show how to restore geometry (i.e., a substitute for the triangle inequality) to the problem, through a new analog of Helly's theorem.
This "intrinsic-dimension" Helly theorem states: for any family L of ∆-dimensional convex sets in a Hilbert space, there exist ∆ + 2 sets L ⊆ L such that r(L) ≤ 2r(L).
Based upon this we present an algorithm that computes a (1 + ε)-core set L ⊆ L, |L | = O(∆ 4 /ε 2), such that the ball centered at a point c with radius (1 + ε)r(L) intersects every element of L.
The running time of the algorithm is O(n ∆+1 d poly(1/ε)).
For the case of lines or line segments (∆ = 1), the (expected) running time of the algorithm can be improved to O(nd poly(1/ε)).
We note that the size of the core set depends only on the dimension of the input objects and is independent of the input size n and the dimension d of the ambient space.
One of the great challenges in computational theory is the extraction of patterns from massive and high-dimensional data sets.
A common difficulty associated with such data sets is that entries are incomplete-a few questions are left blank on a questionnaire; weather records for a region omit the figures for one weather station for a short period because of a malfunction; stock exchange data is absent for one stock on one day because of a trading suspension; and so forth.
How should we process the partial data?
Statisticians approach this problem in a variety of ways: deleting incomplete entries; filling in incomplete entries based on the most similar complete entry ("hot deck imputation"); filling in incomplete entries with the sample mean ("mean substitution"); or using a learning algorithm or criterion (EM, max likelihood) to infer a missing entry [15].
All of these are attempts to cope concurrently with two difficulties: (1) The statistical relationship between the present and missing data is usually not known.
This precludes a universal answer to which approach is most statistically sound.
(2) There is a combinatorial explosion inherent in trying out all candidate assignments to the missing values.
The present paper offers a new approach to the problem of incomplete data: an approach rooted in the geometry of the data set.From the computational point of view, a data item with d representative features is typically represented by a point in R d , each dimension corresponding to a feature; frequently one obtains good results by approximating the similarity of two items by their Euclidean distance, after choosing a good scaling of the axes.
The most elementary form of data analysis for such a data set is to find the smallest ball that approximates the data set, whether in terms of the sum of distances to the center of the ball, maximum distance to the center, etc.
An immediate generalization is to find a small number of balls (a "clustering" of the data) which between them cover the points (different interpretations of "cover" lead to the well-known k-median problem, k-center problem, etc.).
There is copious work on these problems in the machine learning and algorithms literature.A data item that is lacking information about one or more features corresponds to a line or a flat in R d , whose dimension is the number of missing features.
There is no difficulty in assigning a distance between two such flats; it is simply the distance between the nearest points on the flats.
So we can seek to cluster the flats so as to minimize some objective function.
Right away there is a major difficulty: "distances" between flats do not satisfy the triangle inequality.
The problem is not that the triangle inequality is slightly violated, but that no relaxation of it holds.
No matter how far apart lines a and c are, there is always a line b that intersects both.
This problem defeats many existing algorithmic approaches for "clustering"-type tasks, and for good reasonthe geometry seems, in a genuine sense, to be absent.
What does it mean to cluster lines, if "a resembles b" and "b resembles c" imply nothing at all about a resembling c?In this paper we initiate work on data analysis for convex sets of low dimension inside an ambient space of possibly high dimension.
Specifically, we assume each input object is a convex subset of a flat of dimension ∆ within a Hilbert space.
For the existence theorems, this space may be infinitedimensional, while for the algorithmic statements, we take it to be R d for a value of d that we shall consider to be much higher than ∆.
Our measure of the similarity amongst a collection of convex sets L is the minimum intersection radius r(L), the least r such that there is a ball of radius r intersecting every flat in L.
The center of the optimum ball (selected arbitrarily in the degenerate case that it is not unique) is termed the minimum intersection center and denoted c(L).
Intuitively this center is the best explanation of the incomplete input data; the minimum intersection radius captures how well this center fits the data, in the sense that every incomplete data item can be completed to a point within that distance of the center.
Significantly, therefore, our model makes a functional prediction for reconstructing missing data: among all points of the flat, use that which is closest to c(L).
Thus in addition to the role of our model in learning aggregate properties of the data set, it also provides an inference mechanism about the missing features of individual records.Our approach can also be described under the shape fitting framework [1], where one asks for a shape, a point in our case, that best fits the input set, lines or flats, under some criterion.The core of our contribution is to show a way to restore geometry to the problem of analyzing incomplete data, in spite of the failure of the triangle inequality.
This restoration goes through a variant of Helly's theorem.
Suppose we blow up each line or flat to a cylinder or a slab that encloses all the points within distance r from .
Helly's theorem says that if every d + 1 of these have a common intersection, then all of them have a common intersection.
In other words, Helly's theorem restores geometry because if every subcollection of d + 1 out of the n "data flats" are within distance r of some "explanation point", then all of the n lines are within distance r of an explanation point.As it stands, however, this chain of reasoning is too weak.
The dimension of the ambient space, d, is typically of the order of hundreds or thousands, much larger than the maximum dimension ∆ of the individual data items.
We redress this gap by developing a version of Helly's theorem that takes into account the low dimension of the sets involved.
Beginning with the case of lines (∆ = 1), we show that if every 3 of the n "data lines" are within distance r of some "explanation point" then all of the n lines are within distance 2r of some explanation point.
Notice that we are now free of the "extrinsic" dimension of the ambient space, and depend only on the intrinsic dimensionality, ∆ = 1, of the data sets.
This result can be extended to any ∆-dimensional convex objects L, for any 0 ≤ ∆ ≤ d, as follows: if every subset of ∆+2 convex objects of dimension at most ∆ in a Hilbert space are within distance r of some point, then all of the objects are within distance 2r of some point.
This result is optimal in the sense that there exist configurations in which any ∆ + 1 convex objects of dimension at most ∆ in R d have a minimum intersection radius that is strictly smaller than 1/2 of that of L.
We call this result the intrinsic-dimension Helly theorem:THEOREM 1.1.
(INTRINSIC-DIMENSION HELLY THEOREM) For any n convex sets of dimension at most ∆ in a Hilbert space, L = { 1 , 2 , · · · , n }, there exist ∆ + 2 sets L ⊂ L such that r(L) ≤ 2r(L ).
Note that when r = 0 (i.e., when the sets of L intersect), Theorem 1.1 directly generalizes Helly's theorem (except that it is weaker by 1 in the case that the ambient space is of finite dimension d, and ∆ = d).
The implications of the theorem to the analysis of incomplete data are immediate: given a collection L of n convex sets of dimension at most ∆, a 2-approximation of the minimum intersection radius of L results from enumerating all subsets L of L of size ∆ + 2 and determining the largest r(L ).
Actually as will be seen later, the implicit O(n ∆+2 d poly(∆))-runtime can be replaced by an algorithm with expected running time 2 O(∆ log ∆) nd and a suitable center for L (not generally equal to c(L )) is identified as part of the same process.Next, we provide a method to achieve an approximation ratio of 1 + ε (for any ε > 0) for the minimum intersection radius.
A subset L ⊆ L is said to be an α-core set, with respect to r(L), if the minimum intersection radius r(L ) of L approximates r(L) within a multiplicative factor of α.
Theorem 1.1 says that when L is a set of lines in ddimensional Euclidian space, one can find a 2-core set L of size 3; and in general, if L consists of ∆-dimensional convex sets, there exists a 2-core set L of size ∆ + 2.
For general values of α = 1 + ε and for L consisting of ∆-dimensional flats in R d , we show that for any ε > 0 there exists a (1 + ε)-core set of size O(∆ 4 /ε 2 ).
Here and throughout the paper we assume that each convex set can be represented by a constant number of constraints.
Such a core set can be found in time O(n ∆+1 d poly(1/ε)).
For the case of lines (∆ = 1), the running time of the algorithm can be improved to O(nd poly(1/ε)).
Notice that the size of the (1 + ε)-core set only depends on ε and ∆, and is independent of the total input n or the dimension of the ambient space.To the best of our knowledge, this is the first work to address core sets for collections L that consist of ∆-dimensional convex sets.
We summarize the core set result by the following two theorems:THEOREM 1.2.
((1 + ε)-CORE SET FOR LINE SEGMENTS) Let ε > 0.
Let L be a set of lines or line segments { 1 , . . . , n } in R d .
There exist a subset L ⊆ L of size O(1/ε 2 ) such that r(L )(1 + ε) ≥ r(L).
The set L and a center c, such that the ball centered at c of radius (1 + ε)r(L ) intersects all lines or line segments in L, can be found in expected time O(nd poly(1/ε)).
THEOREM 1.3.
((1 + ε)-CORE SETS) Let ε > 0.
Let L be a set of convex sets of dimension ≤ ∆, L = { 1 , . . . , n }, in R d .
There exist a subset L ⊆ L of size O(∆ 4 /ε 2 ) such that r(L )(1+ ε) ≥ r(L).
The set L and a center c, such that the ball centered at c of radius (1 + ε)r(L ) intersects all sets in L, can be found in time O(n ∆+1 d poly(1/ε)).
As described above, the main focus of this paper is the near-optimal representation of a set of incomplete data entries (∆-flats) by a single ball of minimal radius.
Naturally, this is only the first step toward a more comprehensive theory that should provide algorithms for clustering incomplete data entries by providing several balls of small radius, at least one of which intersects each of the ∆-flats.
It is easy to see that minimizing this radius is NP-hard, from the NP-hardness of the k-center problem for points.
Our work already implies an initial result in this area: using our core set method, there is a straightforward O(n k )-time algorithm to obtain a 2-approximate k-clustering of n lines.
Due to space limitations, the details are omitted from this extended abstract and will appear in the full version of the paper.
Clustering and shape fitting problems on points have been actively studied in recent years.
One of the powerful techniques is to devise a core set, i.e., a small subset of representative points S of S such that the optimization problems on S is a good approximation to the optimal solution on S [1].
Precisely, a subset S is a (1 + ε)-core set of S if (1+ε)µ(S ) ≥ µ(S), where µ is a monotonic measure function.
Agarwal et al. provided a framework for computing a (1 + ε)-core set for a set of points S in R d with respect to many measure functions that depend on the extent of the point set, such as diameter, width, radius of the minimum enclosing ball, and volume of the smallest enclosing box [2].
The basic idea is to find a subset of points of size O(1/ε O(d) ) whose convex hull approximates the convex hull of S. For some of the problems such as the minimum enclosing ball or ellipsoid, there is an incremental algorithm that computes a (1 + ε)-core set of size that depends only on ε [8,7,13,14].
Thus one can apply bruteforth algorithms on the small core set S and obtain efficient approximation algorithms for the optimization problems on S. Indeed, many geometric optimization problems such as minimum enclosing ball, k-clustering, and various shape fitting problems can be solved efficiently by using a small core set [4,8,9,10,11,13,14].
However, to the best of our knowledge, no work has been done on devising a core set for lines or flats with respect to a natural quality measure.The study of core sets for points can not be directly applied to core sets for lines or flats.
For a set of lines or flats, there is no natural definition of "convex hull".
Our core set algorithms for lines or flats are more related with the incremental core set algorithm for points S in R d with respect to the radius of the minimum enclosing ball [8], which is described as follows.
The algorithm starts with S being a pair of furthest away points and computes the minimum enclosing ball of S .
If all the points are included in the minimum enclosing ball enlarged by a factor of (1+ε), then S is a core set.
Otherwise, a point outside the enlarged ball is added to S .
It can be shown that for each step, the radius of the minimum enclosing ball of S is increased by a factor of 1 + O(ε 2 ).
After O(1/ε 2 ) steps, the algorithm terminates.
However, there is a major difficulty to apply this algorithm for a set of lines L: there is a situation where by adding each extra line, the minimum intersection radius of the current subset L stays the same but the minimum intersection radius of L , r(L ), is still far away from the real value r(L).
A substantial part of this paper is devoted to showing that a carefully selected set of two lines (or ∆ + 1, more generally, ∆-flats) can improve the minimum intersection radius substantially.We also note that there has been work on "clustering points with lines" [3,5,10], where one finds a set of lines L such that the set of cylinders with radius r and axis as the lines of L covers all the input points S.
The problem we study in this paper can be phrased as "clustering lines with a point".
There does not exist an obvious connection between these two problems as a natural duality does not exist.
The remainder of the paper is organized as follows.
We start with a few preliminaries in Section 2.
In Section 3 we present the proof of Theorem 1.1.
In Section 4 we present the proof of Theorem 1.2.
The proof of Theorem 1.3 is very similar to that of Theorem 1.2, and is sketched in Section 5.
Due to space limitations the proof of some of our claims are omitted.
We denote by B r (c) a ball centered at a center c with radius r in R d .
We denote by d(.
, .)
the Euclidean distance function.
The distance between two points p, q is also written as |pq|.
The radius of the minimum intersection ball is called the minimum intersection radius, denoted as r(L).
Namely,B(L) = B r(L) (c(L)).
DEFINITION 2.3.
((1 + ε)-CORE SET) Let L be a set of convex sets in a Hilbert space.
A subset L of L is said to be a (1 + ε)-core set w.r.t. the minimum intersection radius of L if r(L) ≤ (1 + ε)r(L ).
We begin by noting that the minimum intersection radius and center of L can be found in polynomial time up to an absolute error δ using convex programming.
The proof of the following lemma appears in the Appendix.LEMMA 2.1.
Let L be a set of n convex sets with dimension at most ∆ in R d .
c(L) and r(L) can be computed to an ab- solute precision δ > 0 in time O( √ n(d 3 +d 2 n∆) log(n/δ)).
We now prove Theorem 1.1.
Proof.
For each (∆ + 2)-tuple i = {i 1 , . . . , i ∆+2 } in {1, . . . , n}, let B i be the minimum intersection ball of the subset L i = { i1 , .
.
.
, i∆+2 } centered at point c i , and let r i be the radius of B i .
Let the largest radius among the r i 's be r.
Now we claim that we can find a ball of radius 2r that intersects all the sets in L. Consider the set 1 and denote by I j the set of points on 1 with distance no more than 2r from j .
That is, I j = {p ∈ 1 |d(p, j ) ≤ 2r}.
I j is convex, since I j is the intersection of two convex objects, the set 1 and the set of points of distance 2r from j .
Notice that I j is a convex set of dimension ∆ in 1 .
Consider any (∆ + 1)-tuple i = {i 1 , . . . , i ∆+1 } in {2, . . . , n}.
Now we claim that the corresponding ∆ + 1 sets {I i 1 , . . . , I i ∆+1 } have non-empty intersection.
Let i be the (∆ + 2)-tuple obtained from i by adding an additional index of value 1, namely i = {1, i 1 , . . . , i ∆+1 }.
By the above discussion we have that r i ≤ r. Let c i be the center of the minimum intersection ball B i of L i .
Let c i be the point on the set 1 that is closest to c i .
As the point c i is within distance r from all sets in L i , we have that the point c i is within distance 2r from all the sets in L i .
This implies that c i is in I j for all j ∈ i .
See Figure 1.
Since for any (∆ + 1)-tuple i = {i 1 , . . . , i ∆+1 } the corresponding ∆ + 1 convex sets I j have non-empty intersection, and these sets are embedded in the ∆ dimensional set 1 , by Helly's Theorem [12] all the sets I j , 2 ≤ j ≤ n have a non-empty intersection.
Now let o be a point in j I j , 2 ≤ j ≤ n.
The ball centered at o with radius 2r intersects all n sets of L.The case of r = 0 is closest in form to the original Helly theorem: COROLLARY 3.1.
For any n convex sets of dimension at most ∆ in a Hilbert space,L = { 1 , 2 , · · · , n }, if every ∆ + 2 sets in L intersect, then all sets in L intersect.
THEOREM 3.1.
(OPTIMALITY) For any ∆, there exist a set of n convex sets L = { 1 , . . . , n } such that for every subset L ⊆ L of size less than ∆ + 2 it holds that r(L) > 2r(L ).
Proof.
For any ∆, consider the (∆ + 1)-dimensional simplex.Namely, the setΩ ∆+1 = {(p 1 , . . . , p ∆+2 ) | ∆+2 i=1 p i = 1}.
Our sets i will be subsets of Ω ∆+1 of dimension ∆.
|L| = ∆ + 2.
For i = 1 to ∆ + 2, define i = {(p 1 , . . . , p ∆+2 ) ∈ Ω ∆+1 | p i = 0}.
It is not hard to verify that every subset L ⊆ L of size ∆ + 1 has a non-empty intersection -if i ∈ L then the unit vector with '1' in the i'th coordinate is in L .
Thus r(L ) = 0.
However, the sets in L do not have a common intersection.
r(L) > 0.
Since this construction lies in R ∆+1 , the only choice of parameters for which Theorem 1.1 is not tight is the obvious case of ∆ = d. Theorem 1.1 indicates a straightforward algorithm to find a 2-approximation to the minimum intersection radius by simply taking the maximum radius of all (∆ + 2)-tuples of sets that include 1 .
One can improve the running time of this naive algorithm (and actually give an alternative proof to Theorem 1.1) using the methodology of 'LP-type' programming (e.g. [16]) to 2 O(∆ log ∆) nd.
The proof of the following lemma is given in the Appendix.LEMMA 3.1.
Let L be a set of n convex sets with dimension at most ∆ in R d .
Let ∈ L.The minimum radius ball B r (c) that covers the sets L with center c on , and a setL = {l 1 , . . . , l ∆+2 } ⊆ L for which r(L) ≤ 2r(L ) can be found in expected time 2 O(∆ log ∆) nd.
The proof of Theorem 1.2 we present shortly strongly builds upon the notion of a (1 +ε)-approximate intersection center, ε > 0.
In what follows we define (1 + ε)-approximate intersection centers, and state Theorem 4.1 which addresses a certain property of these centers.
We then prove Theorem 1.2 based on Theorem 4.1.
In what follows we will assume that L is a set of n lines.
The general case in which L also includes line segments is proven in a very similar manner and is given in detail in the Appendix.DEFINITION 4.1.
A (1 + ε)-approximate intersection ball of a set L in d-dimensionalEuclidean space is a ball of radius (1 + ε)r(L) that intersects all sets in L.
The center of a (1 + ε)-approximate intersection ball is called a (1 + ε)-approximate intersection center.
We denote by C ε (L) the set of (1 + ε)-approximate intersection centers of L. With Theorem 4.1, we now prove Theorem 1.2.
The idea is similar with the construction of a (1 + ε)-core set P for a set of points P in R d such that the radius of the minimum enclosing ball of P is bounded by (1 + ε) times that of P [8].
The basic idea in [8] is to add a point not covered by the minimum enclosing ball of the current core set such that the minimum radius is increased substantially.
However, a direct application of this idea does not work for the case of lines.
One can find a scenario where adding a line can not improve the minimum intersection radius.
We will show that a careful selection of two lines can always increase the minimum intersection radius by a substantial factor.OBSERVATION 4.1.
For any set L, C ε (L) is convex.Proof.
[Theorem 1.2] Let ε > 0.
Throughout this proof we assume that ε is sufficiently small.
In what follows we present an algorithm for finding L .
Our algorithm is greedy and strongly builds upon Theorem 4.1.
For a set L , let be the axis of the cylinder of radius ε 2 r(L ) which contains the collection of (1 + ε 2 /50 2 )-approximate intersection centers of L .
Roughly speaking, the main idea of our algorithm is as follows.
We start out by picking a subset of L of size 3 according to Lemma 3.1.
For these lines it holds that αr(L ) ≥ r(L) where α = 2.
This is a good starting point, but we still need to reduce the value α above to (1 + ε).
We do this in a series of steps.
In each step, a line or two are added to L and α reduces by a factor of (1 − 1 2 ε 2 /50 2 ).
Hence, after O(1/ε 2 ) such steps we are in a situation in which (1 + ε)r(L ) ≥ r(L) and we are done.
The second part of the theorem (regarding efficiency matters) will follow from the detailed description of the algorithm.We first focus on an iteration of the algorithm.
Let L be the subset defined by the algorithm so far.
Let c = c(L ) be the minimum intersection center of L , and let be the axis of the cylinder of radius ε 2 r(L ) which contains the collection of (1 + ε 2 /50 2 )-approximate intersection centers of L .
Recall from Theorem 4.1 that passes through c and is parallel to one of the sets in L .
If the ball centered at c of radius (1 + ε)r(L ) intersects L, halt and output the set L .
Otherwise, if there exists a line ∈ L such that r(L ∪ {}) ≥ (1 + ε 2 /50 2 )r(L ) add to L and proceed in an additional iteration of the algorithm (see remark at the end of the proof).
We are now in a situation that for every line ∈ L the radius of the minimum intersection ball of L ∪ {} is very close to the radius of the minimum intersection ball of L , namely, r(L ∪ {}) < (1 + ε 2 /50 2 )r(L ), this implies that the center of the minimum intersection ball of L ∪ {} is in the ε 2 r(L ) cylinder around .
In this case, we use the axis to find a pair of lines that when added to L will increase r(L ) substantially.
For each line i ∈ L \ L we now compute a certain interval I i on .
Namely, we define I i to be the set of points x on such that the ball of radius (1 + ε)r(L ) centered at x intersects the sets in L ∪ { i }.
It is not hard to verify that for each line i this interval is not empty.
Indeed, consider the minimum intersection centerc * i of L ∪ { i }.
As r(L ∪ {}) < (1 + ε 2 /50 2 )r(L ), i.e., c * iis a (1 + ε 2 /50 2 )-approximate center of L , it follows from Theorem 4.1 that the distance of c * i from is at most ε 2 r(L ).
Consider the projection of c * i onto the line .
Denote this projection by c i .
It now follows that the ball of radius(1 + ε/2 + ε 2 /50 2 )r(L ) ≤ (1 + ε)r(L ) centered at c i covers L ∪ { i }, which implies that c i ∈ I i .
If for all pairs of lines i and j the corresponding intervals intersect, then by Helly theorem, there is a point c in the intersection of all the intervals.
This implies that the ball of radius (1 + ε)r(L ) centered at c covers all the lines and we may halt the algorithm and output the set L .
Finally, if there are two lines i and j with corresponding intervals that do not intersect, then we claim that r(L ∪ { i , j }) ≥ (1 + ε 2 /50 2 )r(L ) and we may add both i and j to L and proceed in an additional iteration of the algorithm (see remark at end of proof).
Assume for contradiction that r(L ∪ { i , j }) < (1 + ε 2 /50 2 )r(L ) and let c * be the minimum intersection center of L ∪ { i , j }.
As the ball of radius (1 + ε 2 /50 2 )r(L ) centered at c * also covers L it follows that the distance of c * from is at most ε 2 r(L ).
As before, consider the projection of c * onto the line .
Denote this projection by c .
It now follows that the ball of radius(1 + ε 2 + ε 2 /50 2 )r(L ) ≤ (1 + ε)r(L ) centered at c covers L ∪ { i , j }.
This implies that the point c is in the intervals I i and I j corresponding to i and j , which is a contradiction.We still need to show, given a new set L how to find c(L ), r(L ) and the axis of C ε 2 /50 2 (L ).
Computing c(L ) and r(L ) can be done (with sufficient precision) in time d poly(1/ε) using Lemma 2.1 (here we use the fact that the total dimension of the lines involved in the computation is independent of d).
Regarding the line , by Theorem 4.1 we know that is parallel to one of the lines in L and passes through c = c(L ).
In the upcoming iteration of our algorithm, we may try each and every line in L (a constant number) and run the additional iteration with that center.
As we halt our algorithm, or proceed to add lines to L only if certain conditions hold.
We are sure to encounter these conditions once we have chosen the correct line .
The algorithm in the previous section can be extended to the general case of convex sets with dimension at most ∆, resulting in the proof of Theorem 1.3.
As in the previous section, the proof of Theorem 1.3 uses the notion of a (1+ε)-approximate intersection center.
In what follows we state Definition 5.1 and Theorem 5.1.
The proofs can be found in the Appendix.
DEFINITION 5.1.
Let be a convex set with dimension ≤ ∆ in R d .
A slab of radius r with axis in R d is defined as the set of points in R d which are of distance at most r from .
THEOREM 5.1.
The set of (1 + ε)-approximate intersection centers of a collection L of convex sets with dimension at most ∆, C ε (L), is included in a ∆-slab of width β ε(∆ + 1) 3 r(L), for some constant β.
Proof.
[Lemma 2.1] We can formulate this problem by convex programming.
Assume c is a point in R d and r is the intersection radius of c with respect to L. Each convex set i has dimension at most ∆ and is represented as follows.i is in a ∆-dimensional space B i , which has origin o (i) and is spanned by k unit vectors b(i) j , j = 1, · · · , ∆.
Therefore, each point in i can be represented by o (i) + ∆ j=1 λ (i) j b (i) j , where λ (i)j is a scalar.
Inside B i , the convex set i is specified by m i convex constraints f(i) j (λ (i) ) ≤ 0, j = 1, · · · , m i , where λ (i) = {λ (i) j }.
We can find the minimum intersection radius and center of L by solving the following optimization problem:min r s.t. r ≥ ||c − o (i) − ∆ j=1 λ (i) j b (i) j || , 1 ≤ i ≤ n ; f (i) j (λ (i) ) ≤ 0 , 1 ≤ j ≤ m i , 1 ≤ i ≤ n .
This is a convex optimization problem.
The total number of variables is n∆ + d + 1.
The total number of constraints is N + n, where N = i m i = O(n).
Thus one can find the solution up to precision δ > 0 in time O((N + n)(n∆ + d) 3 log(n/δ)), by using a generic interior point method [6].
A more careful analysis by exploring the sparsity of matrices shows a better bound on the running time O( √ n(d 3 + d 2 nk) log(n/δ)).
The details are omitted here.Proof.
[Lemma 3.1] Let L be a set of n convex sets of dimension ∆ in R d .
Let ∈ L.In what follows we study the problem of finding the minimum radius ball covering L with center on .
We show that this problem falls in the abstract framework of so called 'LP-type' problems, and can be solved by the randomized algorithm of [16] in expected running time 2 O(∆ log ∆) nd.
The algorithm of [16] not only finds the minimum radius ball (say B r (c) centered at c of radius r) covering L with center on , it also returns a subset L of L of size ∆ + 1 such that the minimum radius ball covering L with center on is also B r (c).
This implies thatr(L) ≤ 2r(L ∪ ).
Indeed, r ≥ r(L), and r(L ∪ ) ≥ r(L) 2otherwise by projecting onto and using the triangle inequality one could find a ball of radius less than r that covers L with center on .
We now sketch the proof that the problem at hand is an LP-type problem.
Throughout our proof we use the notation of [16] freely.
Our problem is defined by a couple (H, w) where H is the set of constraints corresponding to each set in L, and w is the function on subsets G of H which returns the minimum radius ball covering sets corresponding to G with center on (ties broken using a lexicographic order on ).
For various technical reasons we alter w as to satisfy basis regularity (as described in [16]).
To use the framework outlined in [16] it suffices to prove the following claims.Detailed proof is omitted form this extended abstract and will appear in the full version of the paper.
Roughly speaking the proofs follow the line of proof used in proving that the minimum enclosing ball of a set of n points is an LP-type problem.Claim 1: the combinatorial dimension of (H, w) is ∆ + 1.
This follows essentially from the argument that in the ∆ dimensional flat the minimum enclosing ball of n points has an exact coreset of size ∆ + 1.
Claim 2: Monotonicity and Locality, follow by the definition of w. Claim 3: Violation test and Basis Computation.
We present an algorithm which given a set of constraints G of size ≤ ∆ + 2, finds the value of w(G) along with a basis for G.
Here we use the fact that finding the minimum covering ball of k convex sets in k 2 dimensions can be done in time exp(O(k log k)) (see for example [16]), we also need a few additional ideas that tie this problem with our basis computation problem.Proof.
[Lemma 4.1] We take a point t on cp that moves infinitesimally away from c towards p.
We argue that there must be a line ∈ L such that d(c, ) = r(L) and the distance from a point t on cp to is non-decreasing, when t moves infinitesimally from c to p on cp.
Otherwise, the distances from t to all the lines in L are strictly less than r(L) when t moves infinitesimally away from c.
This contradicts with the fact that r(L) is the minimum intersection radius.
Now suppose for the line ∈ L, the distance from t to stays the same, when t moves infinitesimally away from c.
Then it must be that cp is parallel with .
Therefore the angle between and cp is zero.
The claim is true.If the distance from t to is strictly increasing when t moves infinitesimally away from c, the distance d(t, ) is monotonically increasing as t moves linearly from c to p.
Now we bound the angle between cp and as follows.
The point p is a (1 + ε)-approximate intersection center, r(L) < d(p, ) ≤ (1+ε)r(L).
We denote by the line that is parallel with and goes through center c. Let q be the point on for which pq perpendicular to , |pq| = d(p, ) ≤ (1 + ε) · r(L).
Let q be the point on for which qq is perpendicular to , |qq | = d(c, ) = r(L).
Finally, let s be the point on for which cs is perpendicular to , |cs| = d(c, ) = r(L).
See Figure 2.
Let H be the hyperplane with normal cs that passes through c, and let S be the (closed) half space defined by H that does not include .
The segment cp (not including c) is either entirely contained in S or entirely contained in the complement of S.
We now claim the cp is in S. Consider the ball B of radius r(L) around s.
The point c is on the boundary of both B and S. And the segment cp does not intersect the interior of B. Otherwise there would be a point on cp of distance less than r(L) to s.
This can only happen if cp is in S.If p is in S, the inner angle of the triangle pq q at vertex q , ∠pq q ≥ π/2.
Thus,¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡|pq | ≤ |pq| 2 − |qq | 2 ≤ r(L) (1 + ε) 2 − 1 = r(L) √ 2ε + ε 2 .
On the other hand, is perpendicular to both the line qq and the line pq.
Thus is perpendicular to the plane defined by the triangle pq q.
This implies that ∠pq c = π/2.
Thereforesin ∠pcq = |pq | |pc| ≤ √ 2ε + ε 2 · r(L) α √ ε · r(L) = √ 2 + ε/α.
Thus the angle between and line cp is ∠pcq ≤ arcsin(√ 2 + ε/α).
Proof.
[Lemma 4.2] Without loss of generality, we can assume that v i 's are the unit vectors along the k + 1 axes and passes through the origin.
We take the unit normal vector v of .
Then the angle between v j and , denoted by θ j , is π/2 − θ j , where θ j is the angle between vectors v j and v.
In order to minimize max i θ i = max i arcsin(sin θ i ) = max i arcsin(cosθ i ) = max i arcsin(v i · v), one chooses v = (1, 1, · · · , 1)/ √ k + 1.
Thus we have max i θ i ≥ arcsin(1/ √ k + 1).
Proof.
[ Lemma 4.3] Assume that there is a square R with side length 5 √ εr(L) inside C ε (L).
Denote by u the center of R, and a, b, c, d the four points on the boundary such that ua, ub, uc, ud are perpendicular to the four sides of R respectively, see Figure 3.
|ua| = |ub| = |uc| = |ud| = 5 √ εr(L)/2.
Now we claim that for any line ∈ L, d(u, ) < r(L).
Assume otherwise, there is a line ∈ L such that d(u, ) ≥ r(L).
We observe that when we move a point t continuously from u to a or from u to c, at least in one case the distance d(t, ) is monotonically increasing.
Similarly for nodes b, d.
We take such two vectors, say ua, ub.
The vectors ua, ub are perpendicular to each other.
Furthermore, a, b are both inside Also |ua| = |ub| = 5 √ εr(L).
By a similar argument as in Lemma 4.1, the angle from to the line ua (ub) is at most arcsin(2 √ 2 + ε/5) < π/4, a contradiction.C ε (L), so d(a, ) ≤ (1 + ε)r(L), d(b, ) ≤ (1 + ε)r(L).
Thus, d(u, ) < r(L), for any line ∈ L.
We conclude that u is an intersection center of L with radius less than r(L).
This gives a contradiction to the definition of r(L).
Proof.
[Theorem 4.1] We first study the case where the set of minimum intersection centers is included in a finite radius ball centered at the origin.
Then the set of approximate intersection centers is also included in a finite radius ball.
by Lemma 4.1 there is a line ∈ L such that the angle between and the line cp is arcsin( √ 2 + ε/α).
Take to be the line through c which is parallel with .
The distance from p to is at most r(L) √ 2ε + ε 2 .
Thus the distance from any point on the line segment pq to line is no more than r(L) √ 2ε + ε 2 , by simple geometry.
Take any approximate center t , assume its projection to line cp is t, and the projection of t on line is t .
The distance from t to isd(t , ) ≤ d(t , t) + d(t, t ) ≤ 20 √ εr(L) + r(L) √ 2ε + ε 2 ≤ 25 √ εr(L).
Thus the cylinder with axis and radius 25 √ εr(L) includes the collection of (1 + ε)-approximate centers.
If the set of minimum intersection centers is not included in a finite radius ball, then all the lines are parallel, and the cylinder that includes all the approximate centers must have a axis parallel with the lines in L (otherwise it can not cover the minimum intersection centers).
Thus if there is an approximate center v outside the cylinder with radius 25 √ εr(L), by a similar argument as above we can find a square with side length more than 5 √ εr(L) completely in- side C ε (L√ 2 + ε/(α − 2 √ 2 + ε)).
Proof.
For the case of line segments, the proof is basically the same as Lemma 4.1.
We can assume without loss of generality that the closest point s (q) to c (p) on is not the endpoints of the line segment.
If otherwise, we can find a pointˆcpointˆ pointˆc on the line segment cp such thatˆcthatˆ thatˆc is the furthest point from c on cp such thatˆcthatˆ thatˆc's closest point on is still s. Notice that r(L) < d(ˆ c, ) ≤ (1 + ε)r(L).
Since the distance d(t, ) is monotonically increasing, as t moves from c to p, the inner angle ofˆccwofˆofˆccw at c is at least π/2.
Thus |cˆc||cˆc| ≤ (1 + ε) 2 − 1r(L) = (2 + ε)εr(L).
Similarly, one can find the corresponding pointˆppointˆ pointˆp onˆcponˆ onˆcp.
|pˆp||pˆp| ≤ (2 + ε)εr(L).
Thus |ˆcˆp||ˆcˆ|ˆcˆp| ≥ (α−2 √ 2 + ε) √ εr(L).
Now we follow the arguments in the case of lines.
The angle between and cp is bounded by arcsin(√ 2 + ε/(α − 2 √ 2 + ε)).
LEMMA 6.2.
Suppose c is a minimum intersection center of a set L of convex sets with dimension at most ∆ and p is a(1 + ε)-approximate intersection center, |cp| ≥ α √ εr(L), α > 2 √ 2 + ε, then there exists a convex set ∈ L suchthat the angle between and the interval cp is bounded by arcsin(√ 2 + ε/(α − 2 √ 2 + ε)).
Proof.
The proof is very similar to the proof of Lemma 4.1 and 6.1.
There is a convex set ∈ L such that d(c, ) = r(L), d(p, ) ≤ (1 + ε)r(L) and d(t, ) > 1 monotonically nondecreasing as t moves from c to p. Let s be the point on closest to c and q the point on closest to p.
Now we know that |cs| = r(L) and |pq| ≤ (1 + ε)r(L).
If s and q are the same point, then the inner angle of scp at vertex c is at least π/2.
Thus |cp|≤ |sp| 2 − |cs| 2 ≤ (2 + ε)εr(L).
Thus α ≤ √ 2 + ε,which is a contradiction.
If s are q are different, we note that the line segment sq is completely inside , due to the fact that is convex.
Further, as a point t moves on cp from c to p, the distance between t and the line segment sq is non-decreasing.
By Lemma 6.1, the lines cp and sq have a small angle.
Thus the angle between cp and is no more than arcsin(√ 2 + ε/(α − 2 √ 2 + ε)).
LEMMA 6.3.
C ε (L) does not include a (∆+1)-dimensional cube with side length γ ε(∆ + 1)r(L), for a constant γ ≥ 6 √ 2 + ε.Proof.
Suppose otherwise, there is a (∆ + 1)-dimensional cube R with side length γ ε(∆ + 1)r(L) inside C ε (L).
Denote by u the center of this cube R. Again, we claim that d(u, ) < r(L) for any ∈ L, which contradicts with the definition of r(L).
Now suppose that there is a convex set ∈ L such that d(u, ) ≥ r(L), we argue a contradiction.
By Lemma 6.2, we can find ∆ + 1 orthogonal vectors of length γ ε(∆ + 1)r(L)/2 centered at u such that the angle between each vector to the convex set is no more than arcsin(√ 2+ε γ √ ∆+1/2−2 √ 2+ε) ≤ arcsin(1/ √ ∆ + 1).
This contradicts with Lemma 4.2.
Finally we show, with the above lemmas we can prove our Theorem.
In this final step, given a minimal intersection center c, we find a set of pairs (p 1 , q 1 ), (p 2 , q 2 ), . . . , (p ∆+1 , q ∆+1 ) in the following way.
p 1 , q 1 are the furthest pairs of points in C ε (L) such that the line segment p 1 q 1 intersects c. p 2 , q 2 are the furthest pairs of points in C ε (L) such that the line segment p 2 q 2 is perpendicular to the 1-flat spanned by p 1 , q 1 .
Similarly, p i , q i are the furthest pairs of points in C ε (L) such that the line segment p i q i is perpendicular to the (i − 1)-flat spanned by {p 1 , q 1 , p 2 , q 2 , · · · , p i−1 , q i−1 }.
Define d i = |p i q i |.
Now we claim that at least for one 1 ≤ i ≤ ∆ + 1, d i ≤ β ε(∆ + 1) 3 r(L), for some constant β.Suppose otherwise, d i > β ε(∆ + 1) 3 r(L) for 1 ≤ i ≤ ∆ + 1.
Now consider the convex polytope P spanned by the points {p 1 , q 1 , p 2 , q 2 , · · · , p ∆+1 , q ∆+1 }.
By the convexity of C ε (L), all the points in the interior of P are (1 + ε)-approximate centers.
Thus one can then find a (∆ + 1)-dimensional cube with side length γ ε(∆ + 1)r(L) inside P , with γ ≥ 6 √ 2 + ε.
The details are omitted here.
By Lemma 6.3 we have a contradiction.
Thus d i ≤ β ε(∆ + 1) 3 r(L) for some i. Therefore C ε (L) can be enclosed in a ∆-slab with axis as the flat spanned by p 1 q 1 , p 2 q 2 , · · · , p i−1 q i−1 , p i+1 q i+1 , · · · , p ∆+1 q ∆+1 and width β ε(∆ + 1) 3 r(L).
Proof.
[Theorem 1.3] The basic idea is the same as in Theorem 1.2.
We first focus on the existence of a small size core set.
We start with ∆ + 2 sets L ⊆ L according to Theorem 1.1 such that r(L) ≤ α·r(L), α = 2.
Let c = c(L ) be the minimum intersection center of L and be the axis of the slab that contains the collection of (1 + ε 2 4β 2 (∆+1) 3 )-approximate intersection centers of L for some constant β in Theorem 5.1.
Define I i to be a subset of such that a point p ∈ I i has distance at most (1 + ε)r(L ) away from the sets L { i }, i ∈ L \ L .
If among all I i , every ∆ + 1 of them have a non-empty intersection, then i I i = ∅, by Helly's theorem.
Thus the ball with radius (1 + ε)r(L ) centered at a point c ∈ i I i intersects with every set in L and we are done (L is the core set).
If there are ∆ + 1 sets 1 , 2 , · · · , ∆+1 such that their corresponding sets I j , j = 1, · · · , ∆ + 1 do not have a common intersection, then it can be verified in the same way as in Theorem 1.2 that r(L { 1 , · · · , ∆+1 }) ≥ (1 + ε 2 4β 2 (∆+1) 3 )r(L ).
Thus we add all the sets j , j = 1, · · · , ∆ + 1, to L and go to the next iteration.Thus, for each iteration, at most ∆ + 1 sets are added to L and the value α is decreased by a factor of (1 − ε 2 2β 2 (∆+1) 3 ).
After O(∆ 3 /ε 2 ) steps, there are O(∆ 4 /ε 2 ) sets in L such that r(L) ≤ (1 + ε)r(L ).
This concludes our proof for the existence of the (1+ε)-core set for convex sets of dimension at most ∆.
For an algorithm to compute this core set, notice that the axis of the slab containing C ε (L ) is not known.
Thus we will try each ∆ + 1 tuples of the remaining sets of L at each iteration.
We will terminate our algorithm once no ∆ + 1 tuple will increase the minimum intersecting radius (significantly).
The running time of the algorithm follows from the same arguments as in Theorem 1.2.
A center c such that the ball of radius (1 + ε)r(L ) intersects all sets in L can be found using Lemma 2.1 (convex programming).
The careful reader may have noticed that the running time of our algorithm for general ∆ is greater than that implied by standard convex programming (Lemma 2.1).
Indeed this is the case, however, in our algorithm in addition to returning an approximate center c, we also return the coreset L whose existence should be viewed as the main contribution of this Theorem.
The authors would like to thank the anonymous SODA referee for the useful comments on LP-type problems, and Lin Xiao for several helpful discussions on convex programming.
