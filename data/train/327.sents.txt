When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting.
While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied.
We perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions.
We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.
Consider a classification task where a learner is given training items x 1 , . . . , x l ∈ R d , represented by d-dimensional feature vectors.
The learner is also given the corresponding class labels y 1 , . . . , y l ∈ Y.
In this paper, we focus on binary labels Y ∈ {−1, 1}.
In addition, the learner is given some unlabeled items x l+1 , . . . , x l+u ∈ R d without the corresponding labels.
Importantly, the labeled and unlabeled items x 1 . . . x l+u are distributed in a peculiar way in the feature space: they lie on smooth, lower dimension manifolds, such as those schematically shown in Figure 1(a).
The question is: given this knowledge of labeled and unlabeled data, how will the learner classify x l+1 , . . . , x l+u ?
Will the learner ignore the distribution information of the unlabeled data, and simply use the labeled data to form a decision boundary as in Figure 1(b)?
Or will the learner propagate labels along the nonlinear manifolds as in Figure 1 When the learner is a machine learning algorithm, this question has been addressed by semisupervised learning [2,11].
The designer of the algorithm can choose to make the manifold assumption, also known as graph-based semi-supervised learning, which states that the labels vary slowly along the manifolds or the discrete graph formed by connecting nearby items.
Consequently, the learning algorithm will predict Figure 1(c).
The mathematics of manifold learning is wellunderstood [1,6,9,10].
Alternatively, the designer can choose to ignore the unlabeled data and perform supervised learning, which results in Figure 1(b).
When the learner is a human being, however, the answer is not so clear.
Consider that the human learner does not directly see how the items are distributed in the feature space (such as Figure 1(a)), but only a set of items (such as those in Figure 2(a)).
The underlying manifold structure of the data may not be immediately obvious.
Thus there are many possibilities for how the human learner will behave: 1) They may completely ignore the manifold structure and perform supervised learning; 2) They may discover the manifold under some learning conditions and not others; or 3) They may always learn using the manifold.For readers not familiar with manifold learning, the setting might seem artificial.
But in fact, many natural stimuli we encounter in everyday life are distributed on manifolds.
An important example is face recognition, where different poses (viewing angles) of the same face produce different 2D images.
These images can be quite different, as in the frontal and profile views of a person.
However, if we continuously change the viewing angle, these 2D images will form a one-dimensional manifold in a very high dimensional image space.
This example illustrates the importance of a manifold to facilitate learning: if we can form and maintain such a face manifold, then with a single label (e.g., the name) on one of the face images, we can recognize all other poses of that person by propagating the label along the manifold.
The same is true for visual object recognition in general.
Other more abstract stimuli form manifolds, or the discrete analogue, graphs.
For example, text documents in a corpus occupy a potentially nonlinear manifold in the otherwise very high dimensional space used to represent them, such as the "bag of words" representation.There exists little empirical evidence addressing the question of whether human beings can learn using manifolds when classifying objects, and the few studies we are aware of come to opposing conclusions.
For instance, Wallis and Bülthoff created artificial image sequences where a frontal face is morphed into the profile face of a different person.
When participants were shown such sequences during training, their ability to match frontal and profile faces during testing was impaired [8].
This might be evidence that people depend on manifold structure stemming from temporal and spatial proximity to perform face recognition.
On the other hand, Vandist et al. conducted a categorization experiment where the true decision boundary is at 45 degrees in a 2D stimulus space (i.e., an information integration task).
They showed that when the two classes are elongated Gaussian, which are parallel to, and on opposite sides of, the decision boundary, unlabeled data does not help learning [7].
If we view these two elongated Gaussian as linear manifolds, this result suggests that people do not generally learn using manifolds.This study seeks to understand under what conditions, if any, people are capable of manifold learning in a semi-supervised setting.
The study has important implications for cognitive psychology: first, if people are capable of learning manifolds, this suggests that manifold-learning models that have been developed in machine learning can provide hypotheses about how people categorize objects in natural domains like face recognition, where manifolds appear to capture the true structure of the domain.
Second, if there are reliable methods for encouraging manifold learning in people, these methods can be employed to aid learning in other domains that are structured along manifolds.
For machine learning, our study will help in the design of algorithms which can decide when to invoke the manifold learning assumption.
We designed and conducted a set of experiments to study manifold learning in humans, with the following design considerations.
First, the task was a "batch learning" paradigm in which participants viewed all labeled and unlabeled items at once (in contrast to "online" or sequential learning paradigm where items appear one at a time).
Batch learning allows us to compare human behavior against well-established machine learning models that typically operate in batch mode.
Second, we avoided using faces or familiar 3D objects as stimuli, despite their natural manifold structures as discussed above, because we wished to avoid any bias resulting from strong prior real-world knowledge.
Instead, we used unfamiliar stimuli, from which we could add or remove a manifold structure easily.
This design should allow our experiments to shed light on people's intrinsic ability to learn using a manifold.Participants and Materials.
In the first set of experiments, 139 university undergraduates participated for partial course credit.
A computer interface was created to represent a table with three bins, as shown in Figure 2(a).
Unlabeled cards were initially placed in a central white bin, with bins to either side colored red and blue to indicate the two classes y ∈ {−1, 1}.
Each stimulus is a card.
Participants sorted cards by clicking and dragging with a mouse.
When a card was clicked, other similar cards could be "highlighted" in gray (depending on condition).
Labeled cards were pinned down in their respective red or blue bins and could not be moved, indicated by a "pin" in the corner of the card.
The layout of the cards was such that all cards remained visible at all times.
Unlabeled cards could be re-categorized at any time by dragging from any bin to any other bin.
Upon sorting all cards, participants would click a button to indicating completion.Two sets of stimuli were created.
The first, used solely to acquaint the participants with the interface, consisted of a set of 20 cards with animal line drawings on a white background.
The images were chosen to approximate a linear continuum between fish and mammal, with shark, dolphin, and whale at the center.
The second set of stimuli used for the actual experiment was composed of 82 "crosshair" cards, each with a pair of perpendicular, axis-parallel lines, all of equal length, crossing on a white background.
Four examples are shown in Figure Procedure.
Each participant was given two tasks to complete.Task 1 was a practice task to familiarize the participant with the interface.
The participant was asked to sort the set of 20 animal cards into two categories, with the two ends of the continuum (a clown fish and a dachshund) labeled.
Participants were told that when they clicked on a card, highlighting of similar cards might occur.
In reality, highlighting was always shown for the two nearest-neighboring cards (on the defined continuum) of a clicked card.
Importantly, we designed the dataset so that, near the middle of the continuum, cards from opposite biological classes would be highlighted together.
For example, when a dolphin was clicked, both a shark and a whale would be highlighted.
The intention was to indicate to the participant that highlighting is not always a clear give-away for class labels.
At the end of task 1 their fish vs. mammal classification accuracy was presented.
No time limit was enforced.Task 2 asked the participant to sort a set of 82 crosshair cards into two categories.
The set of cards, the number of labeled cards, and the highlighting of cards depended on condition.
The participant was again told that some cards might be highlighted, whether the condition actually provided for highlighting or not.
The participant was also told that cards that shared highlighting may not all have the same classification.
Again, no time limit was enforced.
After they completed this task, a follow up questionnaire was administered.Conditions.
Each of the 139 participants was randomly assigned to one of 6 conditions, shown in Figure 3, which varied according to three manipulations:The number of labeled items l can be 2 or 4 (2 l vs. 4 l ).
For conditions with two labeled items, the labeled items are always (x 1 , y 1 = −1), (x 2 , y 2 = 1); with four labeled items, they are always (x 1 , y 1 = −1), (x 2 , y 2 = 1), (x 3 , y 3 = 1), (x 4 , y 4 = −1).
The features of x 1 . . . x 4 are those given in Figure 2(b).
We chose these four labeled points by maximizing the prediction differences made by seven machine learning models, as discussed in the next section.Unlabeled items are distributed on a uniform grid or manifolds (grid U vs. moons U ).
The items x 5 . . . x 82 were either on a uniform grid in the 2D feature space, or along two "half-moons", which is a well-studied dataset in the semi-supervised learning community.
No linear boundary can separate the two moons in feature space.
x 3 and x 4 , if unlabeled, are the same as in Figure 2(b).
Highlighting similar items or not (the suffix h).
For the moons U conditions, the neighboring cards of any clicked card may be highlighted.
The neighborhood is defined as within a radius of ǫ = 0.07 in the Euclidean feature space.
This value was chosen as it includes at least two neighbors for each point in the moons U dataset.
To form the unweighted graph shown in Figure 3, an edge is placed between all neighboring points.The rationale for comparing these different conditions will become apparent as we consider how different machine-learning models perform on these datasets.
We hypothesize that human participants consider a set of models ranging from simple to sophisticated, and that they will perform model selection based on the training data given to them.
We start by considering seven typical machine learning models to motivate our choice, and present the models we actually use later on.
The seven models are: (graph) Graph-based semi-supervised learning [1,10], which propagates labels along the graph.
It reverts to supervised learning when there is no graph (i.e., no highlighting).
(1NN,ℓ 2 ) 1-nearest-neighbor classifier with ℓ 2 (Euclidean) distance.
(1NN,ℓ 1 ) 1-nearest-neighbor classifier with ℓ 1 (Manhattan) distance.
These two models are similar to exemplar models in psychology [3].
Figure 4.
Their predictions on 2 l moons U are identical to 2 l moons U h, and on 4 l moons U are identical to 4 l moons U h, except that "(graph)" is not available.For conceptual simplicity and elegance, instead of using these disparate models we adopt a single model capable of making all these predictions.
In particular, we use a Gaussian Process (GP) with different kernels (i.e., covariance functions) k to simulate the seven models.
For details on GPs see standard textbooks such as [4].
In particular, we find seven different kernels k to match GP classification to each of the seven model predictions on all 6 conditions.
This is somewhat unusual in that our GPs are not learned from data, but by matching other model predictions.
Nonetheless, it is a valid procedure to create seven different GPs which will later be compared against human data.For models (1NN,ℓ 2 ), (multi-v), (multi-h), (single-v), and (single-h), we use diagonal RBF kernels diag(σ 2 1 , σ 2 2 ) and tune σ 1 , σ 2 on a coarse parameter grid to minimize classification disagreement w.r.t. the corresponding model prediction on all 6 conditions.
For model (1NN,ℓ 1 ) we use a Laplace kernel and tune its bandwidth.
For model (graph), we produce a graph kernel˜kkernel˜ kernel˜k following the Reproducing Kernel Hilbert Space trick in [6].
That is, we extend a base RBF kernel k with a graph component:˜ k(x, z) = k(x, z) − k ⊤ x (I + cLK) −1 cLk z(1)where x, z are two arbitrary items (not necessarily on the graph), k x = (k(x, x 1 ), . . . , k(x, x l+u )) ⊤ is the kernel vector between x and all l + u points x 1 . . . x l+u in the graph, K is the (l + u) × (l + u) Gram matrix with K ij = k(x i , x j ), L is the unnormalized graph Laplacian matrix derived from unweighted edges on the ǫNN graph defined earlier for highlighting, and c is the parameter that we tune.
We take the base RBF kernel k to be the tuned kernel for model (1NN,ℓ 2 ).
It can be shown that ˜ k is a valid kernel formed by warping the base kernel k along the graph, see [6] for technical details.
We used the GP classification implementation with Expectation Propagation approximation [5].
In the end, our seven GPs were able to exactly match the predictions made by the seven models in Figure 4.
We will use these GPs in the rest of the paper.
We now compare human categorization behaviors to model predictions.
We first consider the aggregate behavior for all participants within each condition.
One way to characterize this aggregate behavior is the "majority vote" of the participants on each item.
That is, if more than half of the participants classified an item as y = 1, the majority vote classification for that item is y = 1, and so on.
The first row in Figure 5 shows the majority vote for each condition.
In these and all further plots, blue circles indicate y = −1, red pluses y = 1, and green stars ambiguous, meaning the classification into positive or negative is half-half.
We also compute how well the seven GPs predict human majority votes.
The accuracies of these GP models are shown in Table 1 Of course, a majority vote only reveals average behavior.
We have observed that there are wide participant variabilities.
Participants appeared to find the tasks difficult, as their self-reported confidence scores were fairly low in all conditions.
It was also noted that strategies for completing the (graph) (1NN,ℓ2) Table 1: GP model accuracy in predicting human majority vote for each condition.task varied widely, with some participant simply categorizing cards in the order they appeared on the screen, while others took a much longer, studied approach.
Most interestingly, different participants seem to use different models, as the individual participant plots in the bottom three rows of Figure 5 suggest.
We would like to be able to make a claim about what model, from our set of models, each participant used for classification.
In order to do this, we compute per participant accuracies of the seven models on that participant's classification.
We then find the model M with the highest accuracy for the participant, out of the seven models.
If this highest accuracy is above 0.75, we declare that the participant is potentially using model M ; otherwise no model is deemed a good fit and we say the participant is using some "other" model.
We show the proportion of participants in each condition attributed to each of our seven models, plus "other", in Table 2: Percentage of participants potentially using each model Based on Figure 5, Table 1, and Table 2, we make some observations:1.
When there are only two labeled points, the unlabeled distribution does not encourage humans to perform manifold learning (comparing 2 l grid U vs. 2 l moons U ).
That is, they do not follow the possible implicit graph structure (2 l moons U ).
Instead, in both conditions they prefer a simple single vertical or horizontal decision boundary, as Table 2 shows 2 .2.
With two labeled points, even if they are explicitly given the graph structure in the form of highlighting, participants still do not perform manifold learning (comparing 2 l moons U vs. 2 l moons U h).
It seems they are "blocked" by the simpler vertical or horizontal hypothesis, which perfectly explains the labeled data.3.
When there are four labeled points but no highlighting, the distribution of unlabeled data still does not encourage people to perform manifold learning (comparing 4 l grid U vs. 4 l moons U ).
This further suggests that people can not easily extract manifold structure from unlabeled data in order to learn, when there is no hint to do so.
However, most participants have given up the simple single vertical or horizontal decision boundary, because it contradicts with the four labeled points.
Finally, when we provide the graph structure, there is a marked switch to manifold learning (comparing 4 l moons U vs. 4 l moons U h).
This suggests that a combination of the elimination of preferred, simpler hypotheses, together with a stronger graph hint, finally gives the originally less preferred manifold learning model a chance of being used.
It is under this condition that we observed human manifold learning behavior.
Do humans really learn using manifolds?
Could they have adopted a "follow-the-highlighting" procedure to label the manifolds 100% correctly: in the beginning, click on a labeled card x to highlight its neighboring unlabeled cards; pick one such neighbor x ′ and classify it with the label of x; now click on (the now labeled) x ′ to find one of its unlabeled neighbors x ′′ , and repeat?
Because our graph has disconnected components with consistently labeled seeds, this procedure will succeed.
The procedure is known as propagating-1NN in semi-supervised learning (Algorithm 2.7, [11]).
In this section we present three arguments that humans are not blindly following the highlighting.First, participants in 2 l moons U h did not learn the manifold while those in 4 l moons U h did, even though the two conditions have the same ǫNN highlighting.Second, a necessary condition for follow-the-highlighting is to always classify an unlabeled x ′ according to a labeled highlighted neighbor x. Conversely, if a participant classifies x ′ as class y ′ , while all neighbors of x ′ are either still unlabeled or have labels other than y ′ , she could not have been using follow-the-highlighting on x ′ .
We say she has taken a leap-of-faith on x ′ .
The 4 l moons U h participants had an average of 17 leaps-of-faith among about 78 classifications 3 , while strict follow-the-highlighting procedure would yield zero leaps-of-faith.
Third, the basic challenge of follow-the-highlighting is that the underlying manifold structure of the stimuli may have been irrelevant.
Would participants have shown the same behavior, following the highlighting, regardless of the actual stimuli?
We therefore designed the following experiment.
Take the 4 l moons U h graph which has 4 labeled nodes, 78 unlabeled nodes, and an adjacency matrix (i.e., edges) defined by ǫNN, as shown in Figure 3.
Take a random permutation π = (π 1 , . . . , π 78 ).
Map the feature vector of the ith unlabeled point to x πi , while keeping the adjacency matrix the same.
This creates the random-looking graph in Figure 6(a) which we call 4 l moons U h R condition (the suffix R stands for random), which is equivalent to the 4 l moons U h graph in structure.
In particular, there are two connected components with consistent labeled seeds.
However, now the highlighted neighbors may look very different than the clicked card.If we assume humans blindly follow the highlighting (perhaps noisily), then we predict that they are more likely to classify those unlabeled points nearer (in shortest path length on the graph, not Euclidean distance) a labeled point with the latter's label; and that this correlation should be the same under 4 l moons U h R and 4 l moons U h.
This prediction turns out to be false.
30 additional undergraduates participated in the new 4 l moons U h R condition.
Figure 6(b) shows the above behavioral evaluation, which does not exhibit the predicted correlation, and is clearly different from the same evaluation for 4 l moons U h in Figure 6(c).
Again, this is evidence that humans are not just following the highlighting.
In fact, human behavior in 4 l moons U h R is similar to 4 l moons U .
That is, having random highlighting is similar to having no highlighting in how it affects human categorization.
This can be seen from the last rows of Tables 1 and 2 We have presented a set of experiments exploring human manifold learning behaviors.
Our results suggest that people can perform manifold learning, but only when there is no alternative, simpler explanation of the data, and people need strong hints about the graph structure.We propose that Bayesian model selection is one possible way to explain these human behaviors.Recall we defined seven Gaussian Processes, each with a different kernel.
For a given GP with kernel k, the evidence p(y 1:l | x 1:l , k) is the marginal likelihood on labeled data, integrating out the hidden discriminant function sampled from the GP.
With multiple candidate GP models, one may perform model selection by selecting the one with the largest marginal likelihood.
From the absence of manifold learning in conditions without highlighting or with random highlighting, we speculate that the GP with the graph-based kernel˜kkernel˜ kernel˜k (1) is special: it is accessible in a participant's repertoire The graph-based GP has slightly lower evidence than several other GPs, which may be due to our specific choice of kernel parameters in (1).
In any case, there is no reason to prefer the GP with a graph kernel, and we do not expect humans to learn on manifold in 2 l moons U h. On the other hand, for 4 l moons U h, the evidence for the seven GP models on those four labeled points are: (graph) 0.0626, (1NN,ℓ 2 ) 0.0591, (1NN,ℓ 1 ) 0.0625, (multi-v) 0.0625, (multi-h) 0.0625, (single-v) 0.0341, (single-h) 0.0342.
The graph-based GP has a small lead over other GPs.
In particular, it is better than the evidence 1/16 for kernels that treat the four labeled points essentially independently.
The graph-based GP obtains this lead by warping the space along the two manifolds so that the two positive (resp.
negative) labeled points tend to co-vary.
Thus, there is a reason to prefer the GP with a graph kernel, and we do expect humans to learn on manifold in 4 l moons U h.We also explore the convex combination of the seven GPs as a richer model for human behavior: k(λ) = 7 i=1 λ i k i , where λ i ≥ 0, i λ i = 1.
This allows a weighted combination of kernels to be used, and is more powerful than selecting a single kernel.
Again, we optimize the mixing weights λ by maximizing the evidence p(y 1:l | x 1:l , k(λ)).
This is a constrained optimization problem, and can be easily solved up to local optimum (because evidence is in general non-convex) with a projected gradient method, given the gradient of the log evidence.
For the 2 l moons U h condition, in 100 trials with random starting λ values, the maximum evidence always converges to 1/4, while the optimum λ is not unique and occupies a subspace (0, λ 2 , λ 3 , λ 4 , λ 5 , 0, 0) with λ 2 +λ 3 +λ 4 +λ 5 = 1 and mean (0, 0.27, 0.25, 0.22, 0.26, 0, 0).
Note the weight for the graph-based kernel λ 1 is zero.
In contrast, for the 4 l moons U h condition, in 100 trials λ overwhelmingly converges to (1, 0, 0, 0, 0, 0, 0) with evidence 0.0626.
i.e., it again suggests that people would perform manifold learning in 4 l moons U h.Of course, this Bayesian model selection analysis is over-simplified.
For instance, we did not consider people's prior p(λ) on GP models, i.e., which model they would prefer before seeing the data.
It is possible that humans favor models which produce axis-parallel decision boundaries.
Defining and incorporating non-uniform p(λ) priors is a topic for future research.
