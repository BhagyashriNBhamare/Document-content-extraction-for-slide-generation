Sources of data uncertainty and imprecision are numerous.
A way to handle this uncertainty is to associate probabilis-tic annotations to data.
Many such probabilistic database models have been proposed, both in the relational and in the semi-structured setting.
The latter is particularly well adapted to the management of uncertain data coming from a variety of automatic processes.
An important problem, in the context of probabilistic XML databases, is that of answering aggregate queries (count, sum, avg, etc.), which has received limited attention so far.
In a model unifying the various (discrete) semi-structured probabilistic models studied up to now, we present algorithms to compute the distribution of the aggregation values (exploiting some regularity properties of the aggregate functions) and probabilistic moments (especially, expectation and variance) of this distribution.
We also prove the intractability of some of these problems and investigate approximation techniques.
We finally extend the discrete model to a continuous one, in order to take into account continuous data values, such as measurements from sensor networks, and present algorithms to compute distribution functions and moments for various classes of continuous distributions of data values.
The (HTML or XML) Web is an important source of uncertain data, for instance generated by imprecise automatic tasks such as information extraction.
A natural way to model this uncertainty is to annotate semistructured data with probabilities.
This is the basis of recent works that consider queries over such imprecise hierarchical information [3,15,16,19,21,23,27,28].
An essential aspect of query processing has been ignored in all these works, namely, aggregate queries.
This is the problem we consider here.
We provide a comprehensive study of query processing for a very general model of imprecise data and a very large class of aggregate queries.We consider probabilistic XML documents and the unifying representation model of p-documents [2,19].
A p-document can be viewed as a probabilistic process that randomly generates XML documents.
Some nodes, namely distributional nodes, specify how to perform this random generation.
We consider three kinds of distributional operators: cie, mux, det, respectively for conjunction of independent events (a node is selected if a conjunction of some probabilistic conditional events holds), mutually exclusive (at most one node selected from a set of a nodes), and deterministic (all nodes selected).
This model, introduced in [2,19], captures a very large class of models for probabilistic trees that had been previously studied.
For queries, we consider tree-pattern queries possibly with value joins and the restricted case of single-path queries.
For aggregate functions, we consider the standard ones, namely, sum, count, min, max, countd (count distinct) and avg (average).
A p-document is a (possibly very compact) representation of a probabilistic space of (ordinary) documents, i.e., a finite set of possible documents, each with a particular probability.
In the absence of a grouping operation à la SQL (GROUP BY), the result of an aggregate query is a single value for each possible document.
Therefore, an aggregate query over a pdocument is a random variable and the result is a distribution, that is, the set of possible values, each with its probability.
It is also interesting to consider summaries of the distribution of the result random variable (that is possibly very large), and in particular, its expected value and other probabilistic moments.
When grouping is considered, a single value (again a random variable) is obtained for each match of the grouping part of the query.
We investigate the computation of the distributions of random variables (in presence of grouping or not) and of their moments.Our results highlight an (expectable) aspect of the different operators in p-documents: the use of cie (a much richer means of capturing complex situations) leads to an increase in complexity.
For documents with cie nodes, we show the problems are hard (typically NP-or FP #P -complete).
For count and sum, in the restricted setting of single-path queries, we show how to obtain moments in P.
We also present Monte-Carlo methods that allow tractable approximations of probabilities and moments.
On the other hand, with the milder forms of imprecision, namely mux and det, the complexity is lower.
Computing the distribution for treepattern queries involving count, min and max is in P.
The result distribution of sum may be exponentially large, but the computation is still in P in both input and output.
On the other hand, computing avg or countd is FP #P -complete.
On the positive side, we can compute expected values (and moments) for most aggregate tree-pattern queries in P.
When we move to queries involving joins, the complexity of moment and distribution computation becomes FP #P -complete.A main novelty of this work is that we also consider probabilistic XML documents involving continuous probability distributions, which captures a very frequent situation occurring in practice.
We formally extend the probabilistic XML model by introducing leaves representing continuous value distributions.
We explain how the techniques for the discrete case can be adapted to the continuous case and illustrate the approach by results that can be obtained.The paper is organized as follows.
After presenting preliminaries and introducing the problems in Sections 2 and 3, we consider cie nodes in Section 4.
In Section 5, we consider monoid aggregate functions in the context of mux and det nodes.
Continuing with this model, we study complexity of distributions and moments in Section 6.
We briefly discuss approximation algorithms in Section 7.
Continuous probability distributions are considered in Section 8.
Finally, we present related work and conclude in Section 9.
A preliminary version of some of this work appeared in [1] (a national conference without proceedings).
We recall the data model and query languages we use.
We assume a countable set of identifiers I and one of labels L, such that I ∩ L = ∅.
The set of labels includes a set of data values (e.g., the integers, on which the aggregate functions will be defined).
A document is a pair d = (t, θ), consisting of a finite, unordered 1 tree t, where each node has 1 Ignoring the ordering of the children of nodes is a common simplification over the XML model that does not significantly [54] [26] [25] [12][55] [41] [32] 50[5] bonus [2] person [3] person a unique identifier v and a label θ(v).
We use the standard notions child and parent, descendant and ancestor, root and leaf in the usual way.
To simplify the presentation, we assume that the leaves of documents are labeled with data values and the other nodes by non-data labels, that are called tags.
The sets of nodes and edges of d are denoted, respectively, byMary [31] pdaI(d) and E(d), where E(d) ⊆ I(d) × I(d).
We denote the root of d by root(d) and the empty tree, that is, the tree with no nodes, by ε.
Example 1.
Consider the document d in Figure 1.
Identifiers appear inside square brackets before labels.
The document describes the personnel of an IT department and the bonuses distributed for different projects.
The document d indicates John worked under two projects (laptop and pda) and got bonuses of 37 and 50 in the former project and 50 in the latter one.An aggregate function maps a finite bag of values (e.g., rationals) into some domain (possibly the same or different).
In particular, we assume that any aggregate function is defined on the empty bag.
In the paper we study the common functions: sum, count, min, countd (count distinct), and avg (average) under the usual semantics.
Our results easily extend to max and topK .
Aggregate functions can be naturally extended to work on documents d: the result α(d) is α(B) where B is the bag of the labels of all leaves in d.
This makes the assumption that all leaves are of the type required by the aggregate function, e.g., rational numbers for sum.
Again to simplify, we ignore this issue here and assume they all have the proper type.
It is straightforward to extend our models and results with a more refined treatment of typing.As we will see some particular aggregate functions, the so-called monoid ones [10], play a particular role in our investigation, because they can be handled by a divide-andconquer strategy.
Formally, a structure (M, ⊕, ⊥) is called an abelian monoid if ⊕ is an associative and commutative binary operation with ⊥ as identity element.
If no confusion arises, we speak of the monoid M .
An aggregate function is a monoid one if for some monoid M and any a1, . . . , an ∈ M :α({|a1, . . . , an| }) = α({|a1| }) ⊕ · · · ⊕ α({|an| }).
It turns out that sum, count, min, max and topK are monoid aggregate functions.
For sum, min, max: α({|a| }) = a and ⊕ is the corresponding obvious operation.
For count: α({|a| }) = 1 and ⊕ is +.
On the other hand, it is easy to check that neither avg nor countd are monoid aggregate functions.change the results of this paper.Finally, we introduce tree pattern queries with joins, with join-free queries and single-path queries as special cases.
We then extend them to aggregate queries.We assume a countable set of variables Var.
A tree pattern (with joins), denoted Q, is a tree with two types of edges: child-edges, denoted E / , and descendent edges, denoted E // .
The nodes of the tree are labeled by a labeling function 2 λ with either labels from L or with variables from Var.
Variables that occur more than once are called join variables.
We refer to nodes of Q as n, m in order to distinguish them from the nodes of documents.A tree pattern query with joins has the form Q[¯ n], where Q is a tree pattern with joins and ¯ n is a tuple of nodes of Q (defining its output).
We sometimes identify the query with the pattern and write Q instead of Q[¯ n] if ¯ n is not important or clear from the context.
If ¯ n is the empty tuple, we say that the query is Boolean.
A query is join-free if every variable in its pattern occurs only once.
If the set of edges E / ∪ E // of a query is a linear order, the query is a single-path query.
We denote the set of all tree pattern queries, which may have joins, as TPJ.
The subclasses of join-free and single path queries are denoted as TP and SP, respectively.A valuation ν maps query nodes to document nodes.
A document satisfies a query if there exists a satisfying valuation, which maps query nodes to the document nodes in a way that is consistent with the edge types, the labeling, and the variable occurrences.
That is, (1) nodes connected by child/descendant edges are mapped to nodes that are children/descendants of each other; (2) query nodes with label a are mapped to document nodes with label a; and (3) two query nodes with the same variable are mapped to document nodes with the same label.Slightly differently from other work, we define that applying a query Q[¯ n] to a document d returns a set of tuples of nodes: Q(d) := {ν(¯ n) | ν satisfies Q}.
One obtains the more common semantics, according to which a query returns a set of tuples of labels, by applying the labeling function of d to the tuples in Q(d).
An aggregate TPJ-query has the form Q[α(n)], where Q is a tree pattern, n is a node of Q and α is an aggregate function.
We evaluate such a Q[α(n)] in three steps: First, the non-aggregate query Q := Q[n] over d, obtaining a set of nodes Q (d).
We then compute the bag B of labels of Q (d), that is B := {|θ(n) | n ∈ Q (d)| }.
Finally we apply α to B. Identifying the aggregate query with its pattern, we denote the value resulting from evaluating Q over d as Q(d).
If Q[n] is a non-aggregate query and α an aggregate function, we use the shorthand Q α to denote the aggregate query Q[α(n)].
More generally, we denote the set of aggregate queries obtained from queries in TPJ, SP, TP and some function α, as TPJ α , SP α , TP α , respectively.The syntax and semantics above can be generalized in a straightforward fashion to aggregate queries with SQL-like GROUP BY.
Such queries are written Q[¯ n, α(n)] and return an aggregate value for every binding of ¯ n to a tuple of document nodes.
Since we can reduce the evaluation of such queries to the evaluation of several simpler queries of the kind defined before, while increasing the data complexity by no more than a polynomial factor, we restrict ourselves to that simpler case.
Example 2.
Continuing with Example 1, one may want to compute the sum of bonuses for each person in the department.
A TP sum query Q that computes bonuses for Mary is in Figure 2.
The query result Q(d) is 59.
We next present discrete probability spaces over data trees (see [2] for a more detailed presentation) and formalize the problems we will study in the following sections.
A finite probability space over documents, px-space for short, is a pair S = (D, Pr), where D is a finite set of documents and Pr maps each document to a probabilityPr(d) such that {Pr(d) | d ∈ D} = 1.p-Documents: Syntax.
Following [2], we now introduce a very general syntax for representing compactly px-spaces, called p-documents.
A p-document is similar to a document, with the difference that it has two types of nodes: ordinary and distributional.
Distributional nodes are used for defining the probabilistic process that generates random documents but they do not actually occur in these.
Ordinary nodes have labels and they may appear in random documents.
We require the leaves to be ordinary nodes 3 .
More precisely, we assume given a set X of independent Boolean random variables with some specified probability distribution ∆ over them.
A p-document, denoted by P, is an unranked, unordered, labeled tree.
Each node has a unique identifier v and a label µ(v) in L ∪ {cie(E)}E ∪ {mux(Pr)} Pr ∪ {det} where L are labels of ordinary nodes, and the others are labels of distributional nodes.
We consider three kinds of the latter labels: cie(E) (for conjunction of independent events), mux(Pr) (for mutually exclusive), and det (for deterministic).
We will refer to distributional nodes labeled with these labels, respectively, as cie, mux and det nodes.
If a node v is labeled with cie(E), then E is a function that assigns to each child of v a conjunction e1 ∧ · · · ∧ e k of literals (x or ¬x, for x ∈ X ).
If v is labeled with mux(Pr), then Pr assigns to each child of v a probability with the sum equal to 1.
Example 3.
Two p-documents are shown in Figures 3 and 4.
The first one has only cie distributional nodes.
For example, node n21 has label cie(E) and two children n22 and n 24 , such that E(n22) = ¬x and E(n 24 ) = x.
The second p-document has only mux and det distributional nodes.
Node n52 has label mux(Pr) and two children n53 and n56, such that Pr(n53) = 0.7 and Pr(n56) = 0.3.x, z [25] [56][8]x [11] z ¬z, x [13] [52][41]x [55] ¬x ¬x [23] [32][54] [26] [21] cie [31] We denote classes of p-documents by PrXML with a superscript denoting the types of distributional nodes that are allowed for the documents in the class.
For instance, PrXML mux,det is the class of p-documents with only mux and det distributional nodes, like P on Figure 4.
p-Documents: Semantics.
The semantics of a p-document P, denoted by P, is a px-space over random documents, where the documents are denoted by P and are obtainable from P by a randomized three-step process.1.
We choose a valuation ν of the variables in X .
The probability of the choice, according to the distribution ∆, ispν = x in P,ν(x)=true ∆(x) · x in P,ν(x)=false (1 − ∆(x)).
2.
For each cie node labeled cie(E), we delete its children v where ν(E(v)) is false, and their descendants.
Then, independently for each mux node v labeled mux(Pr), we select one of its children v according to the corresponding probability distribution Pr and delete the other children and their descendants, the probability of the choice is Pr(v ).
We do not delete any of the children of det nodes.
4 3.
We then remove in turn each distributional node, connecting each ordinary child v of a deleted distributional node with its lowest ordinary ancestor v , or, if no such v exists, we turn this child into a root.The result of this third step is a random document P.
The probability Pr(P) is defined as the product of pν , the probability of the variable assignment we chose in the first step, with all Pr(v ), the probabilities of the choices that we made in the second step for the mux nodes.Example 4.
One can obtain the document d in Figure 1 by applying the randomized process to the p-document in Remark.
In our analysis, we only consider distributional nodes of the types cie, mux, and det. In [2] two more types of distributional nodes (ind and exp) are considered.
As shown there, the first kind can be captured by mux and [8] [55] [21] [54] [13] [11] [41] 0.7 0.3 [52][53] [25] 0.25 0.1 0.9[23][32][26][56]15 det, while the second is a generalization of mux and det and most results of PrXML mux,det can be extended to PrXML exp .
As proved in [2], PrXML cie is strictly more expressive than PrXML mux,det .
It was shown in [19,20] that data complexity of answering TP-queries is intractable for PrXML cie (FP #P -complete) whereas it is polynomial for PrXML mux,det .
Let Q α be an aggregate query and S = (D, Pr) be a px-space of documents.
Since Q α maps elements of the probability space S to values in the range of α, we can see Q α as a random variable.We therefore define the result of applying Q α to S as the distribution of this random variable, that is(Q α (S))(c) = Pr(d) d ∈ D, Q α (d) = c ,for c in the range of α.Since in applications px-spaces are given under the form of p-documents, we further extend the definition to p-documents by defining Q α ( P) := Q α ( P).
We denote the random variable over the p-document P corresponding to Q as Q(P).
Example 5.
Evaluation of the query Q[sum(n)] from Example 2 over the cie-document in Figure 3 gives the distribution {(0, 0.14175), (15, 0.80325), (44, 0.00825) (59, 0.04675)}, while evaluation over the mux-det-document in Figure 4 gives the distribution {(15, 0.3), (59, 0.7)}.
Computational Problems.
For an aggregate query Q, we are interested in the following three problems, where the input parameters are a p-document P with corresponding random document P and possibly a number c:Membership: Given a number c, is c in the carrier of Q(P),i.e., is Pr(Q(P) = c) > 0?
Probability computation: Given a number c, computePr(Q(P) = c).
Moment computation: Compute the moment E(Q(P) k ),where E is the expected value.Membership and probability computation can be used to return to a user the distribution Q( P) of an aggregate query.
Computing the entire distributions may be too costly or the user may prefer a summary of the distributions.
For example, a user may want to know its expected value E(Q(P)) and the variance Var(Q(P)).
In general the summary can be an arbitrary k-th moment E(Q(P) k ) and the moment computation problem addresses this issue.
5 In the following, we investigate these problems for the classes of cie documents and mux-det documents.
For each class, we further distinguish between aggregate queries of the types SP, TP, and TPJ with the functions min, count, sum, countd and avg.
We do not discuss max and topK since they behave similarly as min.
In the paper we mainly speak about data-complexity, when the input is a p-document and the query is fixed.
Occasionally we also consider combined complexity, when both the p-document and the query are inputs of the problem.
We now study the problems introduced in Section 3 for the most general class of p-documents, PrXML cie .
By definition, one approach is to first construct the entire px-space of a p-document P, then to apply the aggregate query Q to each document in P separately, and finally combine the results to obtain the distribution Q( P).
This approach is expensive, since the number of possible documents is exponential in the number of variables occurring in Our complexity results show that for practically all functions and all problems nothing can be done that would be significantly more efficient.
All the decision problems are NP-complete while computational problems are NP-hard or FP #P -complete.
The only exception is the computation of moments for aggregate single-path queries with sum and count.
The intractability is due to dependencies between nodes of p-documents expressed using variables.
We now show several general principles for p-documents that are used later on to support the results.Functions in #P and FP #P .
We recall here the definitions of some classical complexity classes (see, e.g., [24]) that characterize the complexity of aggregate functions on PrXML cie .
An N-valued function f is in #P if there is a non-deterministic polynomial-time Turing machine T such that for every input w, the number of accepting runs of T is the same as f (w).
A function is in FP #P if it is computable in polynomial time using an oracle for some function in #P. Following [9], we say that a function is FP #P -hard if there is a polynomial-time Turing reduction (that is, a reduction with access to an oracle to the problem reduced to) from every function in FP #P to it.
Hardness for #P is defined in a standard way using Karp (many-one) reductions.
For example, the function that counts for every propositional 2-DNF formula the number of satisfying assignments is in #P and #P-hard [25], hence #P-complete.
We notice that the usage of Turing reductions in the definition of FP #P -hardness implies that any #P-hard problem is also FP #P -hard.
Therefore, to prove FP #P -completeness it is enough to show FP #P -membership and #P-hardness.
Note also that #P-hardness clearly implies NP-hardness.
We now consider membership in FP #P .
We say that an aggregate function α is scalable if for every p-document P ∈ PrXML cie , one can compute in polynomial time a natural 5 The variance is the central moment of order 2; it is known that the central moment of order k can be tractably computed from the regular moments of order k.number M such that for every d ∈ P the product M · α(d)is a natural number.
The following result is obtained by adapting proof techniques of [13].
Proposition 6.
Let α be an aggregate function that is computable in polynomial time and Q be an aggregate TPJquery using α.
If α is scalable, then the following functions mapping p-documents to rational numbers are in FP #P :1.
for every c ∈ Q the function P → Pr(Q(P) = c);2.
for every k 1, the functionP → E(Q(P) k ).
The proposition above shows membership in FP #P of both probability and moment computation for aggregate queries in TPJ with all aggregate functions mentioned in the paper.Reducing Query Evaluation to Aggregation.
Now we show that for answering aggregate SP-queries it is possible to isolate aggregation from query processing.
Pr(P |= Q) = Pr(Q α (P ) = 1) = E(Q α (P ) k ).
Moreover, 1.
if Q ∈ TP, then Q α ∈ TP α ;2.
if P ∈ PrXML cie , then P ∈ PrXML cie ;3.
if P ∈ PrXML mux,det , thenP ∈ PrXML mux,det .
In [19] it has been proved that for every non-trivial Boolean tree pattern query, computing the probability to match ciedocuments is #P-hard.
By reducing #2DNF, we can show that for the more restricted case of mux-det documents, evaluation of tree pattern queries with joins can be #P-hard.
Lemma 9.
There is a Boolean TPJ-query with #P-hard data complexity over PrXML mux,det .
The result in [19] and the previous lemma yield immediately the following complexity lower bounds for probability and moment computation for TP and TPJ.Corollary 10.
For any faithful aggregate function α, there exist an aggregate TP-query Q1 and an aggregate TPJquery Q2, both with function α, such that each of the following computation problems is #P-hard:1.
probability computation for Q1 over PrXML cie ; 2.
k-th moments of Q1 over PrXML cie , for any k 1; 3.
probability computation for Q2 over PrXML mux,det ; 4.
k-th moments of Q2 over PrXML mux,det , for any k 1.
We are ready to present aggregation of PrXML cie .
We first show how to check for membership over PrXML cie .
Theorem 11 (Membership).
Let α be one of sum, min, count, avg, and countd.
Then membership over PrXML cie is in NP for the class TPJ α .
Moreover, the problem is NP-hard for any aggregate query in SP α .
The upper bound holds because, given a query, guessing a world and evaluating the query takes no more than polynomial time.
The lower bound follows from the next lemma.
We next show how to compute probability over PrXML cie .
Theorem 13 (Probability).
Let α be one of sum, count, min, avg, and countd.
Then probability computation over PrXML cie is in FP #P for the class TPJ α .
Moreover, the problem is #P-hard for every query in SP α .
Proof.
(Sketch) The FP #P upper bound follows from Proposition 6 and #P-hardness can be shown by a reduction of probability computation for DNF propositional formulas (that is known to be #P-hard), see the following lemma.The following lemma supports Theorems 13 and 15.
Lemma 14.
Let α be one of sum, count, min, avg, countd, and β be one of min, avg, countd.
Let Q α and Q β be SP-queries.
Then for every propositional DNF formula ϕ, one can compute in polynomial time a p-document Pϕ ∈ PrXML cie such that the following are equivalent: Proof.
Again, as for Theorem 13, the FP #P upper bound follows from Proposition 6.
Claim 2 follows from Lemma 14 and its analogues for min, countd, and avg.
Claim 3 follows from Corollary 10.1.
Pr(Q α (Pϕ) = 0) = 1 − Pr(ϕ); 2.
E(Q β (Pϕ) k ) = 1 − Pr(ϕ) for any k 1.
To prove Claim 1, we rely on Proposition 7, which reduces answering aggregate SP-queries to evaluating aggregate functions, and the following lemmas.The next lemma shows that the computation of the expected value for sum over a px-space, regardless whether it can be represented by a p-document, can be polynomially reduced to computation of an auxiliary probability.Lemma 16.
Let S be a px-space and V be the set of all leaves occurring in the documents of S. Suppose that the function θ labels all leaves in V with rational numbers and let sum(S) be the random variable defined by sum on S. ThenE(sum(S) k ) = (v 1 ,...,v k )∈V k k i=1 θ(vi) × Pr ({d ∈ S | v1, . . . , v k occur in d}) ,where the last term denotes the probability that a random document d ∈ S contains all the nodes v1, . . . , v k .
Intuitively, the proof exploits the fact that E(sum(S)) is a sum over documents of sums over nodes, which can be rearranged as a sum over nodes of sums over documents.The auxiliary probability introduced in the previous lemma can be in fact computed in polynomial time for px-spaces θ(vi), can be computed in time at most | P| k .
By Lemma 17, the second term can be computed in polynomial time.
This shows that for every k 1, the k-th moment of sum can be computed in polynomial time.
The claim for count follows as a special case, where all leaves carry the label 1.
Table 1 gives an overview of the data complexity results of this section.
The previous section highlighted the inherent difficulty of computing aggregate queries over cie-documents.
The intuitive reason for this difficulty is that the event variables used in a p-document can impose constraints between the structure of subdocuments in very different locations.
In contrast, mux,det-documents only express "local" dependencies.
As a consequence, for the special case of single path queries and monoid aggregate functions, mux-det documents allow for a conceptually simpler computation of distributions, which in a number of cases is also computationally efficient.The key to developing methods in this setting is Proposition 7, which reduces the evaluation of a single path aggregate We are going to show how a mux,det-document P can be seen as a recipe for constructing the px-space P in a bottomup fashion, starting from elementary spaces represented by the leaves and using essentially two kinds of operations, convex union and product.
Convex union corresponds to mux-nodes and product corresponds to det-nodes and regular nodes.
(To be formally correct, we would need to distinguish between two slightly different versions of product for det and regular nodes.
However, to simplify our exposition, we only discuss the case of regular nodes and briefly indicate below the changes necessary to deal with det-nodes.)
For any α, the distribution over the space described by a leaf of P is a Dirac distribution, that is, a distribution of the form δa, where δa(b) = 1 if and only if a = b. For monoid functions α, the two operations on spaces, convex union and product, have as counterparts two operations on distributions, convex sum and convolution, by which one can construct the distribution α( P) from the Dirac distributions of the leaves of P.
We sketch in the following both the operations on spaces and on distributions, and the way in which they are related.As the base case, consider a leaf node v with label l.
This is the simplest p-document possible, which constitutes an elementary px-space that contains one document, namely node v with label l, and assigns the probability 1 to that document.
Over this space, α evaluates with probability 1 to α({|l| }), hence, the probability distribution is δ α({ |l| }) .
As a special case, if α is a monoid aggregation function over M , the distribution of α over the space containing only the empty document ε is δ ⊥ , where ⊥ is the identity of M .
Inductively, suppose that v is a mux-node in P, the subtrees below v are P1, . . . ,Pn, and the probability of the i-th subtree Pi is pi (see Figure 5, left).
Without loss of generality we can assume that the pi are convex coefficients, that is, p1 + · · · + pn = 1, since we admit the empty tree as a special p-document.
Pv denote the subtree rooted at v.
Then the semantics of mux-nodes implies that the px-spacePv = (Dv, Prv)is the convex union of the spaces Pi = (Di, Pri), which means the following: (1) Dv is the disjoint union of the Di (in other words, for any d ∈ Dv, there is exactly one Di such that d ∈ Di); (2) for any document d ∈ Dv, we have that The set of surviving children in Pv is exactly the union of the sets of children having survived in each Pi and, consequently, for the probability q := Prv(d) we have that q = q1 · · · qn.
In summary, this shows that the probability space (Dv, Prv) is structurally the same as the product of the spaces (Di, Pri).
Prv(d) = piPri(d), where d ∈ Di.Suppose now that, in addition, α is a monoid aggregate function taking values in (M, ⊕, ⊥).
Then for any documentd = v l (d1, . . . , dn) ∈ Pv we have that α(d) = α(d1) ⊕ · · · ⊕ α(dn).
Hence, the probability that α(Pv) = c is the sum of all products Pr(α(P1) = c1) · · · Pr(α(Pn) = cn) such that c = c1 ⊕ · · · ⊕ cn.
Motivated by this observation, we define the following operation.
For any functions f , g : M → R, the convolution of f and g with respect to M is the functionf * M g : M → R such that (f * M g)(m) = m 1 ,m 2 ∈M : m 1 ⊕m 2 =M f (m1)g(m2).
(1)From our observation above it follows that the distribution α( Pv) is the convolution of the distributions α(Pi) with respect to M , that is,α( Pv) = α( P1) * M · · · * M α( Pn).
(2)For det-nodes v, the same equation applies, although the supporting arguments are a bit more complicated.
The crucial difference is that for det-nodes, Pv is a space of forests, not trees, since the trees (or forests) in the Pi are combined without attaching them to a new root.We summarize how one can use the operations introduced to obtain the distribution of a monoid aggregate function over a mux-det document.
Essentially the same relationship between distributions as spelled out in Theorem 18 exists also if we allow continuous distributions at the leaves of documents.
An evaluation algorithm then has to compute convex sums and convolutions, starting from continuous instead of Dirac distributions (we will discuss this in detail in Section 8).
The carrier of a function f : M → R is the set of elements m ∈ M such that f (m) = 0.
Since for any P the carrier of min( P) and of count( P) has at most as many elements as there are leaves in P, we can draw some immediate conclusions from Theorem 18.
Proof.
(Sketch) Claim 1 holds because computing a convex sum and convolutions with respect to "+" and min of two distributions is polynomial and all distributions involved in computing count( P) and min( P) have size O(| P|).
Claim 2 holds because, in addition, a convex sum and the convolution with respect to "+" of two distributions have at least the size of the largest of the two arguments.
Remark.
This principle has also been applied in [26] in the context of queries with aggregation constraints over probabilistic relational databases.
We investigate the three computational problems for aggregate queries for the restricted class of PrXML mux,det , drawing upon the principles developed in the preceding section.Theorem 20 (Membership).
Let α be one of sum, count, min, avg, and countd.
Then membership over PrXML mux,det is in NP for the class TPJ α .
Moreover, the problem is 1.
NP-hard for every query in SP sum , SP avg and SP countd ; 2.
of polynomial combined complexity for the classes SP min and SP count ; 3.
of polynomial data complexity for any query in TP min and TP count .
Proof.
(Sketch) The NP upper bound is inherited from the cie-case (Theorem 11).
Claim 1 can be shown by a reduction of subset-sum and exact cover by 3-sets.
Claims 2 and 3 follow from their counterparts (Claims 1 and 2, respectively) in Theorem 21.
We next consider probability computation.
Proof.
(Sketch) The FP #P upper bound is inherited from the cie-case (Theorem 13).
Claim 1 follows from Corollary 19, since, due to Proposition 7, for an aggregate SP-query Q α we have that Q α ( P) = α( PQ).
Regarding Claim 2, algorithms for count and min can be developed in a straightforward way, applying the techniques in [8] to evaluate TP-queries with aggregate constraints.
For a given p-document, there are only linearly many possible values for min and count, the probability of which can be computed in polynomial time by incorporating them in constraints.
Consequently, the entire distribution of min or count can be computed in polynomial time.Claim 3 can be shown by a reduction of the #K-cover problem for countd and the #Non-Negative-Subset-Average problem for avg.
6 Claim 5 follows from Corollary 10.
Finally, we consider moments over PrXML mux,det .
Theorem 22 (Moments).
Let α be one of sum, count, min, avg, and countd.
Then computation of moments of any degree over PrXML mux,det is in FP #P for the class TPJ α .
Moreover, the problem is 1.
of polynomial combined complexity for the class SP α ; 2.
of polynomial data complexity for the class TP α , if α = avg; 3.
#P-hard for some query in TPJ α .
Proof.
The FP #P upper bound is inherited from the cie-case (Theorem 15).
Claim 3 follows from Corollary 10.
Regarding Claim 1, all our algorithms first reduce aggregate query answering to function evaluation (see Proposition 7).
The algorithm for count and sum is a refinement for the one for the cie-case (Theorem 15).
The algorithm for min works on the entire distribution, which can be computed in polynomial time (Corollary 19).
For countd we apply similar techniques of regrouping sums to those that we used for sum in Lemma 16.
In doing so, we exploit the fact that the probability for a value (or sets of values of fixed cardinality) to occur in a query result over a mux,det-document can be computed in polynomial time, which follows from work in [19].
The algorithm for avg traverses p-documents in a bottomup fashion.
It maintains conditional moments of sum for each possible value of count and combines them in two possible ways, according to the node types.
7 Regarding Claim 2, moments for count and min can be computed directly from the distributions, which can be constructed in polynomial time as sketched in the proof of Theorem 21.2.
Algorithms for sum and countd can be based on a generalisation of the principle of regrouping sums (see Lemma 16) for tree pattern queries.
Analogously as for the case of singlepath queries, the crucial element for the complexity of the sum-algorithm is the difficulty of computing the probability that a node (or sets of nodes of fixed cardinality) occur in a query result.
For tree pattern queries without joins, these probabilities can be computed in polynomial time adapting the techniques in [19].
A variation of this principle, where the probabilities of a given set of values to occur in a query result is computed, gives an algorithm for countd.
6 The same problems has been used earlier in [26] to show #P-hardness of evaluating relational queries with countd and avg-constraints.
7 A technique that is similar in spirit has been presented in [18] for probabilistic streams.
Table 2 gives an overview of data complexity results of this section.
Without loss of generality, we only discuss how to estimate cumulative distributions Pr(Q α (P) c) and moments E(Q α (P) k ) for aggregate TPJ-queries.
Notice that by using cumulative distributions one can also approximate the probability of individual values: to estimate Pr(Q α (P) = c), we estimate Pr(Q α (P) c + γ) and Pr(Q α (P) c − γ) for a small γ (that depends on α and P) and subtract the second from the first.For instance, in order to approximate the cumulative probability Pr(Q countd (P) 100), one evaluates the query on independent random samples of worlds of P, and then use the ratio of resulting samples where countd is at most 100 as an estimator.
Similarly, for approximating E(Q countd (P)), one returns the average of countd over the results.Using Hoeffding's Bound [14] we obtain the following two propositions for approximating a point for the cumulative distribution of an aggregate query and moments of any degree, respectively.Proposition 23.
Let Q be an aggregate TPJ-query, P ∈ PrXML cie,mux,det a p-document and x ∈ Q.
Then for any rationals ε, δ > 0, it is sufficient to have O( 1 ε 2 log 1 δ ) samples so that with probability at least 1 − δ, the quantity Pr(Q(P) x) can be estimated with an additive error of ε.Observe that the number of samples in Proposition 23is independent of the size of Pr(Q(P) x) ε, since then an additive error of ε makes the estimate useless.
However, for probabilities above a threshold p0, it is enough to have the number of samples proportional to 1/p 2 0 (with additive error, say p0/10).
Proposition 24.
Let Q be an aggregate TPJ-query, f a function mapping Q to Q, such that f (Q(P)) ranges over an interval of width R and P ∈ PrXML cie a p-document.
Then, for any rationals ε, δ > 0, it is sufficient to have O( R 2 ε 2 log 1 δ ) samples so that, with probability at least 1 − δ, the quantity E(f (Q(P))) can be estimated with additive error of ε.As a consequence, if Q takes values in [0, R], choosing f (x) := x k yields that the k-th moment of Q(P) around zero can be estimated with O( R 2k ε 2 log 1 δ ) samples.
Observe that if the range R has magnitude polynomial in size of P, then we have a polynomial-time estimation algorithm.
For example, to approximate E(Q countd ( P)) it is enough to draw a quadratic number of samples, since the range R is at most the number of the leaves in P.
We generalize p-documents to documents whose leaves are labeled with (representations of) probability distributions over the reals, instead of single values.
We give semantics to such documents in terms of continuous distributions over documents with real numbers on their leaves.Continuous px-Spaces.
In the discrete case, a p-document defines a finite set of trees and probabilities assigned to them.In the continuous case, a p-document defines an uncountably infinite set of trees with a continuous distribution, which assigns probabilities to (typically infinite) sets of trees, the possible events, which form a σ-algebra.
We refer to a textbook on measure and probability theory such as [5] for the definitions of the concepts used in this section.From now on, we consider only documents whose leaves are labeled with real numbers.
We say that two documents d = (t, θ) and d = (t , θ ) are structurally equivalent, denoted d ∼st d , if t = t and θ(v) = θ(v ) for every v that is not a leaf of t.
That is, d and d differ only in the labels of the leaves.
Obviously, ∼st is an equivalence relation on the set of all documents.
Intuitively, the structure and the labels of inner nodes fix the structure of a document while the leaves contain values.A set of documents D is structurally finite (or sf for short) if (1) for any document d ∈ D and any d that is structurally equivalent to d, we have d ∈ D; (2) D consists only of finitely many ∼st-equivalence classes.
That is, intuitively, if it contains a document d, then it contains also all documents that have the same structure, but different values, and it contains only finitely many structurally distinct documents.Let D be an sf set of documents.
We define a σ-algebra AD on D and then probabilities on AD by doing so first for each ∼st-class and then for D as a whole.Let d0 = (t0, θ0) be a document, ¯ l := (l1, . . . , l k ) a tuple consisting of the leaf nodes of d0, and [d0]∼ st the equivalence class of d0 under ∼st. For every document d = (t, θ) withd ∈ [d0]∼ st we define θ( ¯ l) := (θ(l1), . . . , θ(l k )) a k-tuple of real numbers.In fact, this mapping of tuples of leaf values to tuples of numbers is a bĳection between [d0]∼ st and R k , which we denote as β.
The standard σ-algebra on R k is the algebra of Borel sets.
We use β to introduce a σ-algebra A0 on [d0]∼ st .
We say that D0 ∈ A0 for a set D0 ⊆ [d0]∼ st if and only if β maps D0 to a Borel set of R k .
In the same vein, we can identify probability distributions over R k with distributions over [d0]∼ st .
Note that, due to symmetry, the definition of A0 does not depend on the specific order of the leaves that is used by β.
Now, suppose that D = n i=1[di]∼ st and that Ai is the σ-algebra on [di]∼ st defined above.
Then we defineAD := {D1 ∪ · · · ∪ Dn | Di ∈ Ai}.
Clearly, since all the Ai are σ-algebras, AD is a σ-algebra.
Moreover, suppose that for each equivalence class [di]∼ st we have a probability distribution Pri and that p1, . . . , pn are convex coefficients (that is, pi 0 and p1 + · · · + pn = 1).
Then we define for every D ∈ ADPr(D ) := n i=1 pi · Pri(D ∩ [di]∼ st ).
Clearly, Pr is a probability on AD.
Conversely, every probability Pr over (D, AD) can be uniquely decomposed into probabilities Pri over the ∼st-classes of D such that Pr can be obtained from the Pri as described above.
Moreover, each Pri is essentially a probability over some R k .
p-documents.
To support (possibly continuous) distributions on leaves, we extend the syntax of p-documents by an additional type of distributional nodes, the cont nodes.
A cont node has the form cont(D), where D is a representation of a probability distribution over the real numbers.
In 0.6 [11] 0.25 0.75 [56] [51][52][53] contrast to the distribution nodes introduced earlier, a cont node can only appear as a leaf.Example 25.
Consider the PrXML cont,mux,det p-document in Figure 6.
The document collects results of (e.g., temperature) monitoring by sensors sa, sb and sc.
The data in the document are measurements at time 3 by sb and at time 5 by either sa or sc.
At time 3 the measurement is either 17, or a value in the interval from 15 to 19.
The fact the latter value is unknown and can be anywhere between 15 and 19 is represented by a continuous node cont(U ( [15; 19]), where U stands for the uniform distribution.
We know that both sensors sa and sc have an inherent imprecision and the real measurement is normally distributed around the one they sent.
We model it by a continuous node with a normal distribution cont(N (25; 1)) with mean 25 and variance 1.
∞ −∞ f = 1is the density function of a probability.
Clearly, in this case f |I 1 and f |Im are identical to 0.
Note that distributions defined by piecewise polynomial densities are a generalization of uniform distributions.
Piecewise polynomials are an example of a class of functions stable under convex sum, (classical) convolution, product, and integration.
We shall use this stability property to compute the distribution of aggregate query answers.When the symbol cont appears as a superscript of PrXML, possibly in combination with other symbols, it indicates a class of p-documents that have distributions on their leaves.
The symbol cont can be used with class symbols like the three above as arguments to specify the kind of distributions that can appear.
We define then a continuous probability distribution Pri over R k as the product distribution [5] of the Dij's, i.e., the unique distribution such thatPri(X1 × · · · × X k ) = Di1(X1) × · · · × D ik (X k ).
Using the inverse of the bĳection β discussed earlier, Pri can be translated into a probability distribution over [di]∼ st , the equivalence class of di under ∼st.Let D = ∪ n i=1 [di]∼ st .
We then define as already discussed the probability distribution Pr of P on the σ-algebra AD as:Pr(D ) := n i=1 pi · Pri(D ∩ [di]∼ st ).
Aggregating Continuous Probabilistic Data .
Having defined the semantics of continuous p-documents, we now show how the results for aggregate queries obtained in the discrete case can be lifted to the continuous case.
Our purpose here is not to give a comprehensive picture of the complexity, as in the discrete case, but to see what kind of tractability results can be obtained.
Let us restrict ourself to monoid aggregate functions, and p-documents of PrXML cont,mux,det , which is our main case of tractability in the discrete case.
For simplicity, we only deal with single-path queries.The following result is at the basis of the tractability of monoid aggregate query evaluation in PrXML cont,mux,det .
(X, Y ) is F × G. 3.
The cumulative distribution function of min(X, Y ) is F + G − F × G.Obviously, there is no hope of computing probabilities of aggregate query answers if it is not possible to somehow combine (either symbolically or numerically) the probability distributions of the leaves.
The preceding result hints that if we are able to efficiently apply a number of basic operations on our probability distribution functions, we are able to compute the distribution of the min, max or sum.
The following operations are required: convex sums (for mux nodes); convolution (for sum, in conjunction with det nodes); integration and multiplication (for min and max, in conjunction with det nodes).
One simple case where we can perform these operations efficiently is when cont leaves are piece-wise polynomials of a bounded degree.
For a fixed K > 0 let PP(K) be the set of all piecewise polynomial probability distributions whose polynomials have degree K.
It is reasonable to assume that such a bound K exists for every application.
This bound ensures that the piecewise polynomial representing the distribution of the query answer has degree polynomial in the size of the document.
Hence:Theorem 27.
For p-documents in PrXML cont,mux,det that are labeled with distributions in PP(K) we have:1.
The distribution of results of queries in SP sum can be computed in polynomial time in the combined size of the input and the output.2.
The distribution of results of queries in SP max and SP min can be computed in polynomial time.
3.
All moments of results of queries in SP sum , SP max , and SP min can be computed in polynomial time.Other results from the discrete case can be generalized to the continuous case.
For example, it can be shown that moments of queries in TP sum can be computed in polynomial time over PrXML cont,mux,det (and similarly for SP sum and PrXML cont,cie ), by replacing the cont nodes by the expected value of the represented distribution.
Related Work.
The probabilistic XML models that have been proposed in the literature can be grouped in two main categories, depending on the kind of supported probabilistic dependencies: PrXML mux,det -like local dependencies [15,16,23,28], or PrXML cie -like global dependencies [3,27], in the spirit of c-tables [17].
We used here the unifying framework of [2,19].
The complexity of non-aggregate query answering over PrXML mux,det and PrXML cie has been investigated in [19- 21, 27].
Several results presented here either extend or use these works.
The dynamic-programming algorithm for computing the probability of a Boolean tree-pattern query from [19][20][21] is in particular used for Claim 2 of Theorem 22.
The same authors have also studied in [8] the problem of treepattern query answering over PrXML mux,det documents with constraints expressed using aggregate functions, i.e., something similar to the HAVING queries of SQL.
We use their results for proving Claim 2 of Theorem 21.
Only a few works have considered aggregate queries in a setting of incomplete data.
In non-probabilistic settings aggregate queries were studied for conditional tables [22], for data exchange [4] and for ontologies [6].
In probabilistic settings, to the best of our knowledge, in addition to the aforementioned [8], only [26] studies aggregate queries.
Ré and Suciu consider the problem of evaluating HAVING queries (using aggregate functions) in "block-independent databases", which are roughly PrXML mux,det restricted to relations (limited-depth trees).
The complexity bounds of Claim 3 of Theorem 21 use similar arguments than the corresponding results for block-independent databases presented in [26].
In both [8] and [26], the authors discuss the filtering of possible words that do not satisfy a condition expressed using aggregate functions, and do not consider the problem of computing the distribution of the aggregation, or moments thereof.
Computation of the expected value of aggregate functions over a data stream of probabilistically independent data items is considered in [18].
This is a simpler setting than ours, but we use similar techniques in the proof of Theorem 22.
There is very little earlier work on querying continuous probability distributions.
The authors of [12] build a (continuous) probabilistic model of a sensor network to run subsequent queries on the model instead of the original data.
In [7], algorithms are proposed for answering simple classes of queries over uncertain information, typically given by a sensor network.
As noted in a recent survey on probabilistic relational databases [11], "although probabilistic databases with continuous attributes are needed in some applications, no formal semantics in terms of possible worlds has been proposed so far".
We proposed in Section 8 such a formal semantics.Conclusion.
We provided algorithms for, and a characterization of the complexity of, computing aggregate queries for both PrXML mux,det and PrXML cie models, i.e., very general and most interesting probabilistic XML models.
We also considered the expected value and other moments, i.e., summaries of the probability distribution of the results of aggregate functions.
In the case of PrXML mux,det , we have identified a fundamental property of aggregate functions, that of being monoid, that entails tractability.
The complexity of aggregate computations in many cases has led us to introduce polynomial-time randomized approximation schemes.
Finally, a last original contribution has been the definition of a formal continuous extension of probabilistic XML models.
We have shown how some of the results of the discrete case can be adapted.Because our work has many facets, it may be extended in a number of directions.
First, we intend to implement a system that manages imprecise data with aggregate functions.
In particular, we want the system to handle continuous probabilities, which are quite useful in practice.
A main novelty of the present work is the use of continuous probabilities for data values.
We are currently developing the theory in this direction.
Finally, observe that although a p-document (with continuous probabilities) represents uncountably infinite possible worlds, they only have finitely many possible structural equivalence classes, and in particular, they all are of bounded height and width.
It would be interesting to investigate extensions of the model without this restriction.
