Delay-sensitive Internet traffic, such as live streaming video, voice over IP, and multimedia teleconferencing, requires low end-to-end delay in order to maintain its interactive and streaming nature.
In recent years, the popularity of delay-sensitive applications has been rapidly growing.
This paper provides a protocol that minimizes the end-to-end delay experienced by inelastic traffic.
We take a known convex optimization formulation of the problem and use an optimization decomposition to derive a simple distributed protocol that provably converges to the optimum.
Through the use of multipath routing, our protocol can achieve optimal load balancing as well as increased robustness.
By carrying out packet level simulations with realistic topologies, feedback delays, link capacities, and traffic loads, we show that our distributed protocol is adaptive and robust.
Our results demonstrate that the protocol performs significantly better than other techniques such as shortest path routing or equal splitting among multiple paths.
With the rapid proliferation of broadband Internet access in the consumer market, new video communications and entertainment are gaining in popularity.
Interactive applications such as live video streaming, voice over IP, multimedia conferencing, or online gaming are promoted by the ISPs who are often offering IPTV, VoIP, and Internet service in one bundle.
In their recent report [1], Cisco predicts that in 2012, Internet video traffic will account for nearly 90% of all consumer IP traffic.
Representative of this trend, Internet video has jumped to 22% of the global consumer Internet traffic in 2007 from 12% in 2006.
To preserve the interactive nature of the video applications, and to enable real-time-playback, data delivery with a low latency is required.
As even a temporary delay in playback is likely to harm user experience, preventing delay jitter is equally important.
Despite the growing popularity of the new interactive applications, the current Internet is not well equipped to support the delivery of delay-sensitive traffic.
First, minimization of end-to-end latency by utilizing shorter, more direct routes has not received enough attention, and the existing solutions are not practical.
Second, transmissions are vulnerable to transient periods of congestion that cause temporary delays or losses which degrade the quality of live audio or video streams.Previous work in the Quality of Service (QoS) area [2] guarantees data delivery with the bandwidth and latency specified by the applications.
However, this approach requires coordination in the network because it uses circuit reservations.
Moreover, admission control needs to be used and if the delays increase above certain value, some sources are not allowed to transmit.
We believe that a simpler more practical solution that does not require per-flow resource reservation and allows users to transmit at all times is needed.A popular technique that improves reliability and robustness of data delivery is multipath routing [3].
This technique is beneficial because it allows rerouting of traffic if a path becomes congested or unavailable.
Much of the previous work studies how multipath routing can improve the throughput of the end hosts.
See e.g. [4] [5] [6] [7] [8].
While the key technique used in this paper is also multipath routing, the focus of our work is different.In this paper, we try to address the issues faced by delaysensitive traffic by redesigning routing and congestion control.
The goals are to (i) minimize the delay of the network traffic, (ii) if possible, meet the rate demands of the users while avoiding congestion, and (iii) improve the robustness of the network and resilience to transient performance degradation.
The desired design will ensure that the traffic uses paths with low delay while preventing congestion on the most popular shortest paths even when the loads in the network change.Optimization theory is a powerful tool that has yielded remarkable results in networking research.
It played a crucial role in the design of new distributed protocols for Network Utility Maximization (NUM) [9], [6], [10], reverse-engineering of existing protocols, particularly congestion control [11], [12], [13], and cross-layer optimized architectures in general [14].
In this paper we use convex optimization to optimize the delivery of delay-sensitive traffic.Our optimization problem is a special case of the formulation that appears in [15].
We chose an objective that penalizes a combination of the delay experienced by the traffic sources and excessive link utilization in the network.
The same problem receives treatment in [16] and [17].
However, our solution is much simpler and more practical.
We use optimization decompositions [9] to obtain a simple distributed solution that can be carried out by the routers and traffic sources in the network.
We also translate the distributed solution into a practical protocol and evaluate it through simulations in NS-2.
These packet level simulations allow us to demonstrate the performance of the protocol in an environment with realistic network topologies and feedback delays.The assumptions of our design are explained and justified in Section II.
Delay minimization is formulated as a centralized optimization problem in Section III and the centralized formulation is translated into a distributed network protocol in Section IV.
Finally, performance, robustness and dynamic properties of the protocol are evaluated in Section V and related work is discussed in Section VI.
Before we formulate the optimization problem that our protocol should be solving, let's consider the design decisions we need to make.
This section describes these design decisions and justifies them.
On one hand, we choose to use some simplifying assumptions that allow us to obtain an efficient distributed network protocol.
For example, we assume that all the traffic in our network is delay sensitive.
On the other hand, we need to extend the design of current networks to ensure that we have access to low delay paths.
This is achieved by allowing the sources to use multipath routing with flexible splitting of traffic over these paths.
The objective of our optimization is to minimize the average delay that a bit of data delivered in the network experiences.
This objective implicitly assumes that all the traffic in the network is delay sensitive.We choose to minimize the average delay for several reasons.
First, minimizing the average delay will result in shorter delay for more traffic than, say, minimizing the maximum delay.
The second reason is mathematical convenience that allows formulating the problem as convex optimization and solving it more easily.The assumption that all traffic in the network is delay sensitive is justified for two reasons.
First, as the proportion of video and audio transmissions increases, our protocol could be used to improve the delay experienced by all traffic.
Second, the protocol could be used in a virtual network [18] that carries delay-sensitive traffic exclusively.
Virtual networks subdivide network resources such as link capacities and router time among multiple virtual networks, each of which is used for a different application.
For example, one virtual network can carry delay-sensitive traffic, another bandwidth intensive traffic, etc.
The traffic sources in the network have by assumption the ability to split the traffic between multiple paths.
Moreover, the splitting ratios can change dynamically.The main benefit of having multiple paths is an improved ability to decrease the end-to-end delay, and better reliability.
Because the mathematics used in this paper does not specify whether the traffic sources are edge routers or end hosts, and whether the protocol is used in an interdomain or intradomain setting, we discuss the feasibility of using multiple paths and the changes required to the network architecture for these cases separately.Using multiple routes in the intradomain setting is easier since all the traffic originates and terminates within a single autonomous system.
Inside the network, routers can compute k shortest paths, or a management system can set-up multiple tunnels between the end hosts or edge routers.
Multipath routing in the interdomain case is also possible.
Today, most routing protocols only forward packets on a single path between a source and destination.
However, as is explained in [6], the sources can direct traffic on a particular path by, e.g., participating in a routing overlay or, if they are multihomed, simply directing the traffic to one of the available upstream providers.Dynamic load splitting improves performance as the sources are able to choose the path that has momentarily the lowest delay.
Moreover, in case of failures, the source can avoid paths that do not work or that became congested due to a sudden traffic shift.
If the sources are edge routers, they can rate limit the hosts and split the traffic among the available paths.
If the sources are end hosts, they have to be given access to the multiple paths across which the splitting occurs.
Developing a practical protocol that scales with the size of the network requires that we obtain a distributed protocol that can be performed by the sources and routers.
We assume that the routers in the network are cooperative and they are willing to share information about the network, such as the link propagation delays and current level of congestion, with the sources.
We assume that this signalling is performed honestly.
We also assume that the sources obey the signals provided by the routers.Cooperation is easy to achieve in the intradomain case where the routers are managed by a single network operator.
However, honest signalling may not be incentive compatible in the interdomain case where routers are managed by multiple competing entities.
This work could be extended to deal with misbehaving routers and traffic sources, but this is beyond this paper's scope.
In our optimization problem formulation we assume that the demands of the sources are inelastic, i.e., they do not change as the conditions in the network change.
It is important to notice that even with this assumption, the traffic sources are free to change their demands at any time.
When the demands change, the algorithm re-converges to the new optimal solution.Allowing the sources to change the demand is important as this corresponds to the requirements of applications in practice.
It was observed that the bandwidth requirements of streamed video remains relatively constant in 10-30 second intervals, but can vary significantly between these intervals [19] due to factors such as variable rate compression.
Our experimental results in Section V reflect this observation and demonstrate the performance of our protocol both for the case when the demands remain constant, and when the sources change them.
After introducing the notation used throughout the paper, this section formulates a central optimization problem that minimizes the average delay of network traffic delivery.
We show that this formulation is equivalent to the 'optimal routing' formulation presented in [15] and [16].
A particular link in our network is denoted by l, its capacity is denoted by c l , and its propagation delay by p l .
The network offers multipath routing, i.e., there are multiple paths connecting each source-destination pair.
The paths are encoded in routing matrices.
H i is the routing matrix for a source-destination pair i, where H i lj = 1 if path j connecting source-destination pair i uses link l, and H i lj = 0 otherwise.
H is the collection of all routing matrices H i .
H does not necessarily represent all possible paths in the physical topology, but a subset of paths chosen by the network operators.The traffic between each source-destination pair is split among the multiple paths.
z i j denotes the rate on path j connecting the source-destination pair i, and (Hz) l denotes the total load on link l. Each source-destination pair i has by assumption constant-bit-rate demand x i .
Since p l is the propagation delay, it does not include the queueing delay that traffic may encounter when traversing link l. Therefore, we introduce function f (·) that (i) models the queueing delay of a link, and (ii) penalizes a link's utilization that approaches its capacity.
How exactly is f (·) calculated is discussed later in this section.
The objective of our optimization problem is to minimize the aggregate end-to-end network delay encountered by the traffic.
In another words, we need to distribute the inelastic traffic among the available paths in the network in such a way that the end-to-end delay in seconds/bit for all bits in the network is minimized.
We also need to ensure that the constant demands x i of the traffic sources are met, and the link loads do not exceed link capacity.Since our algorithm needs to calculate the path rates on the paths that each source-destination pair uses, the variables are z i j .
The delay on a path is the sum of the link propagation delays and queueing delays for all links on the end-to-end path.
Therefore, the end-to-end delay on path j of sourcei is l H i lj (p l + f (·)).
The objective of the optimization which captures the average delay for all traffic is obtained by minimizing the products of the path rates and the associated delays, i.e., minimizei j z i j l H i lj (p l + f (·)).
This is equivalent to the minimization from [15]: minimize l D l (L l ) where L l = i j z i j H i lj is the load on link l and D l (L l ) = L l (p l + f (·)) is the cost associated with link l.The solution to the optimization problem, therefore, assigns path rates in a way that takes maximum advantage of low propagation-delay paths while keeping the queues small to minimize overall delay.
To ensure that link loads do not exceed link capacity, we introduce the constraint (Hz) l c l .
We also need to enforce that the sum of the path rates between each source-destination pair i be equal to the demand x i , i.e., j z i j = x i .
Our central optimization is in Figure 1.
The demands x i in the constraints j z i j = x i are constant.
First, this follows from the problem formulation which assumes inelastic traffic.
Second, if x i were variables, the objective of Fig. 1.
Central problem for optimized delivery of delay-sensitive traffic.minimize i j z i j l H i lj (p l + f (·)) subject to (Hz) l c l , ∀l j z i j = x i , ∀i z 0 variables z (1)(1) would have to change to include the demands.
Otherwise, since no traffic implies zero delays, all path rates would become zero.It remains to explain how to choose f (·).
We want to choose f (·) such that it represents queueing delays and heavily penalizes links that approach or exceed their capacity.
This penalty is introduced with the goal of avoiding long queues as well as improving the robustness of the solution.
For mathematical convenience, we also need the function be convex, nondecreasing and twice-differentiable.
Previous optimizations in traffic engineering [20], [21] use the M/M/1 queueing formulaf (L l , c l ) = 1 c l − L l ,(2)where L l is the load on link l, and c l its capacity.
Unfortunately, (2) is not defined for overutilized links.
Overutilization may occur during convergence.
This problem is solved by letting f (u l ) be a piecewise linear function [21] or an exponential approximation of the M/M/1 formula [22].
Here u l = L l c l is the utilization of link l. Both the functions are well defined for any non-negative values of u l .
In our work, we chose the piecewise linear approximation for mathematical convenience and the ability to obtain closed-form rate updates.
In order to have a unique solution to (1), we need to show that the problem is convex.
Since all the inequality constraints, i.e. the capacity constraints (Hz) l c l , ∀l are linear and therefore convex, and all the equality constraints, i.e the demand constraintsj z i j = x i , ∀iare affine, we only need to show that the objectivei j z i j l H i lj (p l + f (u l ))(3)is convex over z i j ∈ R + .
As pointed out above, (3) is equivalent tol L l (p l + f (u l )), which is convex since f (u l )is non-decreasing, twice differentiable and convex.
Hence (3) is convex.
In this section we will derive a practical protocol that converges to the solution of (1).
First, we will use an optimization decomposition to convert the central optimization derived in the previous section into a protocol that solves the problem in a distributed fashion.
In the course of finding the distributed solution, we will explain how mathematical convenience motivates our choice of f (u l ).
Finally, we will translate the distributed solution into a practical protocol which can be carried out by the routers and traffic sources.
We begin by taking the Lagrangian of (1):L (z, λ, q) = i j z i j l H i lj (p l + f (u l )) + l λ l ((Hz) l − c l ) + i q i   x i − j z i j   = i j z i j l H i lj (p l + λ l + f (u l )) − q i + i q i x i − l λ l c l .
(4)Here λ l is the dual variable for link l associated with the constraint i j H i lj z i j c l , and q i is the dual variable for source i associated with the constraint j z i j = x i .
Similarly as in [23], these dual variables can be thought of as prices for violating the respective constraints.
In the distributed algorithm, these prices can be calculated through a subgradient method by the sources and the routers.
The routers can then communicate the calculated prices to the traffic sources.Since the problem is convex, we will use the KKT conditions [24] to find the optimal assignments of rates z i j .
At time t in the distributed algorithm, the source i receives link prices λ l (t) from all distinct links in its paths and computes its own source price q i (t).
Then by the KKT condition for the lagrangian the optimal path rate z ii j (t) on path j must satisfy∂ ∂z i j (L (z, λ, q)) = 0.
(5)For real-time computations, we are interested in a solution of (5) that gives us a closed form expression for z i j (t).
This depends on the choice of f (u l ), and is not possible if we take f (u l ) to be the M/M/1 queueing formula or an exponential.
Therefore we approximate the M/M/1 formula with a piecewise linear function similar to the one introduced in [21].
The piecewise linear function is illustrated in Figure 2.
m( L l c lsimplification let m l = 1 c l m( L l c l ) and k l = 1 c l k( L l c l ).
Therefore, f (u l ) = m l L l + k l .
Similarly as in [21], the values of f (u l ) are small for low utilizations but increase rapidly as the load approaches or exceeds the capacity of the link.Note that the piecewise linear model is still convex.
To make it differentiable at the cutoff points, we need to define slopes at these points.
This can be easily done by letting them take the slope of either of the adjoining lines.
Now (4) can be rewritten as:L = i j z i j l H i lj (p l + λ l + m l (Hz) l + k l ) − q i + i q i x i − l λ l c l(6)Differentiating (6) w.r.t. z i j and after a few simple algebraic manipulations we obtain:∂L ∂z i j = l H i lj (p l + λ l + m l (Hz) l + k l ) − q i + z i j l H i lj m l + l H i lj m l (Hz) l − z i j .
(7)To compute optimal z i j , we set (7) to 0.
After rearranging, introducing iteration index t, and denoting (Hz) l as L l we obtain:z i j (t + 1) = z i j (t) + q i (t)− l H i lj (p l +λ l (t)+2m l (t)L l (t)+k l (t)) l H i lj m l (t) .
(8)Similarly as in [23] and [12], the link feedback subgradient update for the capacity constraint in (1), λ l (t), is:λ l (t + 1) = [λ l (t) − β λ (t) (c l − L l (t))] + .
(9)Similarly, the source subgradient update for the demand constraint, q i (t), is:q i (t + 1) =   q i (t) − β q (t)   j z i j (t) − x i     + .
(10)Here β λ (t) and β q (t) are the stepsizes at iteration t for the link price updates and the demand price updates respectively.
For diminishing step size, i.e β → 0 as t → ∞, the distributed algorithms's objective will converge to the global objective [24].
Equations (8), (9) and (10) define a mathematical algorithm which has to be converted into a network protocol that can be performed by the routers and traffic sources.The role of the routers is to monitor the performance on the links they are serving, calculate the prices associated with these links, and communicate these prices to the traffic sources.
The routers update the link prices iteratively with granularity T .
The prices are obtained by measuring the number of bits N T l that arrive on the link l during a time interval T , and comparing them to the link capacity c l .
The sources collect the feedback from the routers and adjust the rates z i j accordingly.
It is important to notice that the sources receive the feedback from the routers with a delay.
Source i receives the feedback from the links in path j with delay RTT i j , the round-trip-time of that path.
Therefore, we let source i update all its path rates at the timescale of the longest RTT.
We denote this time interval T i = max(RTT i j ), ∀j.
A closer inspection of (8) reveals that it is not sufficient for the source to learn λ l (t) from the routers.
The sources are unable to calculateF l (t) = p l + λ l (t) + 2m l (t)L l (t) + k l (t)and G l (t) = m l (t) without the help of the routers.
Therefore, we let the routers also calculate F l (t) and G l (t) along with the price λ l (t).
Therefore, combining (8), (9) and (10) we obtain the protocol in Figure 3.
Feedback price update at link l:λ l (t + T ) = [λ l (t) − β λ (c l − L l (t))] + ,where β λ is the feedback price stepsize, andL l (t) = N T l T .
Feedback computed at link l:F l (t) = p l + λ l (t) + 2m l (t)L l (t) + k l (t) G l (t) = m l .
Demand price computed at source i:q i (t + T i ) = q i (t) − β q j z i j (t) − x i + ,where β q is the demand price stepsize.Path rate update at source i, path j:z i j (t + T i ) = z i j (t) + q i (t + T i ) − l H i lj F l (t) l H i lj G l (t), where T i = max(RTT i j ), ∀j.
In the protocol in Figure 3, we dropped the dependence of the stepsizes β λ and β q on time.
Implementation of diminishing stepsizes would be impractical as the stepsizes would have to increase whenever a new flow enters or leaves the network.
Choosing a constant stepsize simplifies the protocol, but requires finding the proper values of the stepsizes.
[24] shows that even with constant stepsizes, the subgradient method converges to within of the optimal value, where is a decreasing function of the stepsize.
However, if the stepsizes are too large, the solution may end up too far from the optimum, while if the stepsizes are too small, the convergence of the protocol may be very slow.
The choice of the best numerical values of β λ and β q is further discussed in Section V. To avoid division by zero, i.e. l H i lj G l (t) = 0, we have m l ≥ 0 at all times.
The protocol in Figure 3 does not always assign the source rates z i j such that x i = j z i j (t).
While after the protocol converges the equality holds, during transients the price q i can be non-zero, and hence the demand constraint may be violated.
We solve this problem by projecting the source rates z i j onto the feasible region during the transients so that the demands are met and all rates are non-negative.
As shown in [15], projection onto the feasible region doesn't affect optimality of convergence.
In particular, after each path rate update, we project the rates of each source in the j dimensional space onto the closest point that satisfies the constraints.
The goal of this section is to demonstrate the performance of the protocol and its dynamic properties.
We use NS-2 to perform simulations on realistic network topologies and in the presence of feedback delay.
First, we will describe our experimental setup in NS-2 and the topologies we used.
Second, we will describe how we selected the values of the two stepsizes, β λ and β q .
Third, we will compare the performance of our protocol against two other simple schemes.
Finally, we will test the dynamic properties of the protocol by performing simulations with stochastic traffic.
We implement the protocol of Figure 3 in NS-2.
The link prices are updated every 5 ms and fed back to the sources in acknowledgement packets.
It is important to notice that the prices are delayed because they are attached by the routers to packets and returned to the traffic sources in the acknowledgements.
The sources then update their rates every max(RTT i j ) ms. Large router buffers increase propagation delays during congestion.
Since Section V-C compares our protocol against two schemes which are more likely to cause congestion, we chose short buffers (5 packets) for all evaluations.
In our simulations, we used both simple synthetic and more complicated realistic topologies.
One of the topologies, Abilene, is depicted in Figure 4.
The link capacities in Abilene were 100 Mb/s and delays approximated realistic values between the respective pairs of cities.
The routing in the network was defined as follows.
We selected 4 source-destination pairs, and connected each pair by 4 distinct paths.
The round trip delays on these paths ranged between 28 ms and 82 ms.Since the protocol can be easily deployed inside of a single autonomous system, we obtained autonomous systems' topologies along with link delays from the Rocketfuel topology mapping service [25].
The link capacities were 100 Mb/s if neither endpoint of the link has degree larger than 7, and 52 Mb/s otherwise.
We experimented with the topology of the Sprint network, using 25 source-destination pairs connected by 4 paths.
To obtain the paths connecting a source and destination in the Sprint topology, a third transit node was selected, then a shortest path connecting all the three nodes was calculated, and finally we removed any cycles from the paths.
The protocol has two step sizes: β λ , associated with the link capacity constraint, and β q , associated with the source demand constraint.
In order to find the proper value of β λ , we experimented with the protocol in MATLAB.
We chose MATLAB rather than NS-2 because it allowed us to sweep the parameter space quicker.
We experimented with link capacities of 1Mbps, 10Mbps, and 100Mbps, and chose demands that were sufficiently high to create congestion.
We observed the speed of convergence for different values of β λ .
The results are summarized in Table I. Taking a close look at the step sizes and capacities, we observe that higher link capacities require smaller β λ to achieve quick convergence.
We conclude that the optimal value of the stepsize is β λ = 1/c l .
We used the same method to find the optimal value for β q and found out that the stepsize of each source depends on its demand.
The optimal value is β q = 0.5/x i where x i denotes the demand of source i. To objectively assess the performance, we implemented two other simple protocols in NS-2 and compared the performance against our protocol on the Abilene topology.
The first protocol we compare against uses shortest path routing, and the second uses equal traffic splitting among multiple paths.
We describe these protocols next.In the shortest path routing protocol, each source i measures the propagation delays on the j paths that are available to it.
Then, all traffic is simply sent on the path with the lowest delay.
The protocol that uses equal splitting splits traffic equally among the j paths that are available to source i.In this experiment, we slowly increase the traffic demands from 20 Mbit/sec to 60Mbit/sec and observe the resulting losses and delays.
The losses are depicted in Figure 5.
We observe that the performance of the protocol that uses shortest path routing is the worst.
This is expected because the protocol cannot spread the loads on the longer less used paths when the loads increase beyond 30 Mbit/sec and congestion occurs.
The performance of the two protocols which use load balancing is nearly identical.
Our protocol performs slightly better because it dynamically adjusts the fraction of traffic sent on each path.The average propagation delay of the delivered traffic measured during the same experiment is depicted in Figure 6.
The delay of shortest path routing is the same as our protocol for demands below 30 Mbit/sec, i.e., in the region where the shortest path algorithm can meet the demands.
For larger demands, shortest path routing cannot deliver all the traffic.
The delay for equal splitting is larger because, unlike our protocol, it does not place greater load on the shorter paths during periods of low congestion.
We conclude that our protocol is able to balance the load in such a way so that the propagation delays are low when the network is not congested, but shifts traffic to the longer less used paths as congestion increases.
To assess the speed of convergence of the protocol, we compared the optimal value of the optimization objective with the achieved value after the sources start the transmission.
The simulation was performed on the Abilene topology using three different traffic demands for all the sources: 30 Mbit/sec, 45 Mbit/sec and 60 Mbit/sec respectively.
The results are depicted in Figure 7.
The plot shows that the objective achieved by our protocol converges quickly to within a few percent of the optimal value.
The optimal value was obtained by solving the optimization problem from Figure 1 numerically.We observe that the speed of convergence is better for slower demands.
This is because the amount of traffic that needs to be split among the available paths is smaller.
At first glance, the speed of convergence is slow.
However, this is not a major concern for two reasons.
First, in this experiment, all flows start at time 0, which makes the scenario more challenging.
In practice, such a synchronization is not likely.
Second, as mentioned in Section IV, our algorithm uses projections to meet the demands of the sources.
Therefore, even shortly after the start of the experiment, the demands of the sources are met, although the distribution of the load among the available paths is suboptimal with respect to delay during the transient period.The convergence of the average delays is depicted in Figure 8.
In the previous simulations we assumed that the traffic demands do not change.
Now we will relax this assumption and observe the behavior of the protocol when the demands change randomly.
We start at time 0 by choosing the demand of each source uniformly at random between 40 Mbit/sec and 60 Mbit/sec.
We keep the demands constant for a time interval of length t.
After the time interval elapses, we resample all demands from the same distribution.
This is repeated for each time interval.
The length of each time interval is selected uniformly at random between 35 sec and 60 sec.In this simulation, the demand adjustments are synchronized.
In practice, the demand adjustments may or may not be synchronized.
We selected synchronization for two reasons.
First, this is a more arduous test than changing the demands for each source independently.
Second, the optimal value of the objective changes less frequently, which makes it easier to compare against the achieved value of the objective.The simulation was performed on the Abilene topology, and the results are depicted in Figure 9.
The black curve depicts the optimal value of the objective, which was obtained by solving the optimization problem from Figure 1 numerically.
The red curve depicts the value achieved by our protocol.
We observe that after each demand adjustment, the achieved value quickly converges to the optimum.
Previous work that improves the latency of Internet traffic falls into three groups.
The first group consists of papers whose primary goal is to minimize these delays in context of 'optimal routing'.
The second and larger group consists of papers that decrease the delays as a byproduct of designs that address other problems.
Finally, the third group uses admission control to guarantee a desired quality of service (QoS).
[16] and [17] minimize propagation delays in the network by specifying for each node i what fraction of the traffic should leave node i on each of the links emanating from it.
While this work uses multipath routing and theoretically achieves the optimal values of delays, it has several drawbacks.
First, in [16] it is not possible to find a stepsize that would offer good convergence for a range of traffic conditions.
This is solved in [17], but the work requires more complicated calculations, such as estimation of both the lower and upper bound of the second derivative of the delays.
Second, the protocols require synchronized message passing where upstream nodes need message updates from downstream nodes before they can compute their own updates.
This also requires that the network be 'loop-free': node i cannot be upstream and downstream of node j at the same time to avoid deadlocks.
In comparison, our approach is more practical and does not require significant changes in current routing protocols.Proposals that help to improve end-to-end delay also include TCP protocols such as Vegas [11], or FAST [10] that decrease the amount of queueing in the router buffers.
Decreasing the physical length of the buffers [26] also decreases the delays.
Since the goal of these papers is not to optimize the delivery of delay-sensitive traffic, they do not offer optimal delays and robustness.QoS approaches are able to guarantee the delay and bandwidth requirements to the applications.
This is achieved by using admission control to select sources that are allowed to use the network.
For a survey of the work in this area see e.g. [2].
While our work does not provide delay or bandwidth guarantees, we are able to minimize the aggregate delays without using admission control.
In this paper we identified the need to develop a new protocol that would better serve the needs of interactive applications such as IPTV, VoIP or videoconferencing.
We started with the observation that these applications would benefit from low latency as well as better robustness and resilience to transient performance degradation.
With these goals in mind, we used optimization theory to design a new protocol that modifies routing and congestion control.We took a convex optimization problem that minimizes the latency by splitting traffic on multiple paths while making sure that the shortest most popular paths do not become congested.
Then we used optimization decomposition to translate the optimization problem into a distributed algorithm and a practical protocol with two tunable parameters.
Simulations in NS-2 allowed us to demonstrate the robustness and low latency of our solution.The robustness of the protocol was demonstrated by comparing it against two other simple algorithms that can be used to reduce latency, namely shortest path routing and equal splitting among paths.
We concluded that our protocol is able to shift traffic on longer paths before the shortest paths become congested.
We also observed that the protocol achieves a much lower latency than equal splitting, and is able to operate without any losses at higher loads than the other schemes.We see several possible extensions of this work.
Economic interpretation of prices in our protocol would provide monetary incentives for the sources and routers to behave honestly.
This would be especially helpful in interdomain deployments that span networks belonging to competing entities.
Another extension could simplify the protocol by having the sources to obtain the feedback from the network implicitly by measurement rather than having to rely on the explicit feedback from the routers.
