Most real-time systems require responsive interrupt handling.
Programming of interrupt handlers is challenging: in order to ensure responsiveness, it is often necessary to have interrupt processing enabled in the body of lower priority handlers.
It would be a programming error to allow the interrupt handlers to interrupt each other in a cyclic fashion; it could lead to an unbounded stack.
Until now, static checking for such errors could only be done using model checking.
However, the needed form of model checking requires a whole-program analysis that cannot check program fragments.
In this paper, we present a calculus that contains essential constructs for programming interrupt-driven systems.
The calculus has a static type system that guarantees stack boundedness and enables modular type checking.
A number of common programming idioms have been type checked using our prototype implementation.
Interrupt-driven systems Interrupts and interrupt handlers are often used in systems where fast response to an event is essential.
Embedded interrupt-driven systems generally have a fixed number of interrupt sources with a handler defined for each source.
When an interrupt occurs, control is transferred automatically to the handler for that interrupt source, unless interrupt processing is disabled.
If disabled, the interrupt will wait for interrupt processing to be enabled.
To get fast response times, it is necessary to keep interrupt processing enabled most of the time, including in the body of lower priority interrupt handlers.
This opens the door for interrupt handlers to be themselves interrupted, making it difficult to understand whether real-time deadlines can be met.
Conversely, to write reliable code with a given real-time property, it is simplest to disable interrupts in the body of interrupt handlers.
This may delay the handling of other interrupts and therefore make it difficult for the system to have other desired real-time properties.
The resultant tension between fast response times and easy-to-understand, reliable code drives developers to write code that is often difficult to test and debug.A nasty programming error A particularly nasty programming error is to allow the interrupt handlers to interrupt each other indefinitely, leading to an unbounded stack.
For example, consider the following two interrupt handlers.handler 1 { // do something enable-handling-of-interrupt-2 // do something else iret } handler 2 { // do something enable-handling-of-interrupt-1 // do something else iret } Suppose an interrupt from source 1 arrives first, and so handler 1 is called.
Before returning, handler 1 enables handling of interrupts from source 2, and unfortunately an interrupt from source 2 arrives before handler 1 has returned.
Thus handler 2 is called, and it, in turn, enables handling of interrupts from source 1 before returning, allowing a highly undesirable cycle and an unbounded stack.A traditional type system does not check for this kind of error.
The error is not about misusing data; it is about needing unbounded resources.
Until now, static checking for such errors could only be done using model checking.
However, the needed form of model checking is a whole-program analysis that cannot check program fragments [13,3].
The goal of this paper is to present a type system that guarantees stack boundedness and enables modular type checking.
To do that, we need a minimal setting in which to study interrupt-driven systems.The need for a new calculus For many programming paradigms, there is a small calculus which allows the study of properties in a language-independent way and which makes it tractable to prove key properties.
For example, for functional programming there is the λ-calculus [2], for concurrent programming there is Milner's calculus of communicating systems [7], for object-oriented programming there is the Abadi-Cardelli object calculus [1], and for mobile computation there is the π-calculus [8] and the ambient calculus [4].
These calculi do not offer any notion of interrupts and interrupt handling.
While such concepts might be introduced on top of one of those calculi, we believe that it is better to design a new calculus with interrupts at the core.
A new calculus should focus on the essential concepts and leave out everything else.What are the essential concepts?
While some modern, general-purpose CPUs have sophisticated ways of handling internal and external interrupts, the notion of an interrupt mask register (imr) is widely used.
This is especially true for CPUs that are used in embedded systems with small memory size, a need for low power consumption, and other constraints.
The following table lists some characteristics of four CPUs that are often used in embedded systems: Each of these processors have similar-looking imr's.
For example, consider the imr for the MCS-51 (which calls it the interrupt enable (IE) register):Well-known productEA -ET2 ES ET1 EX1 ET0 EX0The bits have the following meanings:-EA: enable/disable all interrupt handling, --: reserved (not used), and -each of the remaining six bits corresponds to a specific interrupt source.We will refer to the EA bit (and similar bits on other processors) as the master bit.
The idea is that for a particular interrupt handler to be enabled, both the master bit and the bit for that interrupt handler have to be enabled.
This semantics is supported by the Z86, Dragonball, MCS-51, and many other processors.
When an interrupt handler is called, a return address is stored on the stack, and the processor automatically turns off the master bit.
At the time of return, the processor turns the master bit back on.
Not all processors use this scheme: the strongARM does not have a master bit.
In this paper we focus on modeling processors that do have a master bit.In the rest of the paper, we will use a representation of the imr which is independent of particular architectures.
We represent the imr as a bit sequence b 0 b 1 . . . b n , where b i ∈ {0, 1}, b 0 is the master bit, and, for i > 0, b i is the bit for interrupts from source i which is handled by handler i. Notice that the master bit is the most significant bit, and that the bit for handler 1 is the second-most significant bit, and so on.
This layout is different from some processors, and it simplifies the notation used later.Our Results We present a calculus that contains essential constructs for programming interrupt-driven systems.
A program in the calculus consists of a main part and some interrupt handlers.
A program execution has access to:-an interrupt mask register that can be updated during computation, -a stack for storing return addresses, and -a memory of integer variables; output is done via memory-mapped I/O.
The calculus is intended for modeling embedded systems that should run "forever," and for which termination would be considered a disastrous error.
To model that, the calculus is designed such that no program can terminate; nontermination is guaranteed.Each element on the stack is a return address.
When we measure the size of the stack, we simply count the number of elements on the stack.For our calculus, we present a type system that guarantees stack boundedness and enables modular type checking.
A number of common programming idioms have been type checked using our prototype implementation.A type for a handler contains information about the imr on entry and the imr at the point of return.
Given that a handler can be called at different points in the program where the imr may have different values, the type of a handler is an intersection type [5,6] of the form:n j=1 (( imr) j δ j −→ ( imr ) j ).
where the j th component of the intersection means:if the handler is called in a situation where the imr can be conservatively approximated by ( imr) j , then at the point of return, the imr can conservatively be approximated by ( imr ) j , and during that call, the stack will grow by at most δ j elements, excluding the return address for the call itself.The annotations δ j help with checking that the stack is bounded.
Our type system with annotated types is an example of a type-based analysis [10].
Rest of the paper In Section 2 we introduce our interrupt calculus and type system via six examples.
In Section 3 we present the syntax and semantics of the interrupt calculus, and we prove that no program can terminate.
In Section 4 we present our type system and prove stack boundedness, and in Section 5 we conclude.
In the appendix we prove the theorem of type preservation which is a key lemma in the proof of stack boundedness.
We will introduce our interrupt calculus and type system via six examples of increasing sophistication.
The first five programs have been type checked using our prototype implementation, while the sixth program illustrates the limitations of our type system.
We will use the concrete syntax that is supported by our type checker; later, in Sections 3-4, we will use an abstract syntax.
Note that an imr value, say, 11 will in the concrete syntax be written as 11b, to remind the reader that it is a binary value.
Also, the type of a handlern j=1 (( imr) j δ j −→ ( imr ) j ).
will be written (( imr) 1 -> ( imr Figure 1 is a version of Example 3-5 from Wayne Wolf's textbook [14, p.113].
The program copies data from one device to another device.
The program uses memory-mapped I/O; two variables map to the device registers:) 1 : δ 1 ) ... (( imr) n -> ( imr ) n : δ n ).
The program in-indata: the input device writes data in this register and -outdata: the output device reads data from this register.
The line maximum stack size: 1 is a part of the program text.
It tells the type checker to check that the stack can never be of a size greater than one.
The number 1 is a count of return addresses on the stack; nothing else than return addresses can be put on the stack in our calculus.
The header of the handler contains the annotation 11b -> 11b : 0.
It is a type that says that if the handler is called in a situation where the imr can be conservatively approximated by 11, then it will return in a situation where the imr can be conservatively approximated by 11, and the stack will not grow during the call.
The value 11 should be read as follows.
The leftmost bit is the master bit, and the next bit is the bit for handler 1.
The value 11 means that handler 1 is enabled.The program in Figure 2 is much like the program in Figure 1, except that there are now two handlers.
The handlers cannot be interrupted so the maximum stack size is 1.
Notice that since there are two handlers, the imr has three bits.
They are organized as follows.
The leftmost bit is, as always, the master bit.
The next bit is the bit for handler 1, and the rightmost bit is the bit for handler 2.
The program in Figure 3 illustrates how to program a notion of prioritized handlers where handler 1 is of higher priority than handler 2.
While handler 1 cannot be interrupted by handler 2, it is possible for handler 2 to be interrupted by handler 1.
Handler 2 achieves that by disabling its own bit in the imr with the statement imr = imr and 110b, and then enabling the master bit with the statement imr = imr or 100b.
Thus, handler 2 can be interrupted before it returns.
Accordingly, the maximum stack size is 2.
The type for handler 1 is an intersection type that reflects that handler 1 can be called both from the main part and from handler 2.
If it is called from the main part, then the imr is 111, and if it is called from handler 2, then the imr is 110.
The type for handler 2 has annotation 1 because handler 2 can be interrupted by handler 1, which, in turn, cannot be interrupted.The program in Figure 4 illustrates how both handlers can allow the other handler to interrupt.
Each handler uses the discipline of disabling its own bit in the imr before setting the master bit to 1.
This ensures that the maximum stack size is two.The program in Figure 5 illustrates that n handlers can lead to a bounded stack where the bound is greater than n.
In this case we have two handlers and a maximum stack size of three.
A stack size of three is achieved by first calling handler 1, then calling handler 2, and finally calling handler 1 again.
While our type system can type check many common programming idioms, as illustrated above, there are useful programs that it cannot type check.
For example, the program in timer.
The OUT variable will be 0 for 60 seconds after a request for interrupt 2.
There are two interrupt handlers:-The first handler is for an external timer that is expected to request an interrupt once each second.
-The second handler is a trigger.
When it arrives, the OUT variable will become 0 for 60 seconds.
Then OUT will become 1, and remain so until the next trigger event.Our type system cannot handle this pattern where handler 2 disables itself and then enables handler 1, and where the main program disables handler 1 and enables handler 2.
Thus, while the program in Figure 6 has a maximum stack size of 2, it does not type check in our type system.
We will use an abstract syntax that is slightly more compact than the concrete syntax used in Section 2.
We use x to range over a set of program variables, we use i mr to range over bit strings, and we use c to range over integer constants.
| imr = imr ∧ i mr | imr = imr ∨ i mr | if0 (x) s 1 else s 2 | s 1 ; s 2 | skip (expression) e ::= c | x | x + c | x 1 + x 2The over bar notation h denotes a sequence h 1 . . . h n ; we will use the notationh(i) = h i .
We use a to range over m and h.
We identify programs that are equivalent under the smallest congruence generated by the rules:(s 1 ; s 2 ) ; m = s 1 ; (s 2 ; m) (s 1 ; s 2 ) ; h = s 1 ; (s 2 ; h) (s 1 ; s 2 ) ; s = s 1 ; (s 2 ; s).
With these rules, we can rearrange any m or h into one of the seven forms: loop s iret x = e; a imr = imr ∧ i mr; a imr = imr ∨ i mr; a (if0 (x) s 1 else s 2 ); a skip; a.
We use R to denote a store, that is, a partial function mapping program variables to integers.
We use σ to denote a stack generated by the grammar: σ ::= nil | a :: σ.
We define the size of a stack as follows: |nil| = 0 and |a :: σ| = 1 + |σ|.
If i mr = b 0 b 1 . . . b n , where b i ∈ {0, 1}, then we will use the notation i mr(i) = b i .
The predicate enabled is defined as follows:enabled (i mr, i) = (i mr(0) = 1) ∧ (i mr(i) = 1), i ∈ 1.
.
n.We use 0 to denote the imr value where all bits are 0.
We use t i to denote the imr value where all bits are 0's except that the ith bit is set to 1.
We will use ∧ to denote bitwise logical conjunction, ∨ to denote bitwise logical disjunction, ≤ to denote bitwise logical implication, and (·) • to denote bitwise logical negation.
Notice that enabled (t 0 ∨ t i , j) is true for i = j and false otherwise.
The imr values, ordered by ≤, form a lattice with bottom element 0.
A program state is a tuple h, R, i mr, σ, a.
We will refer to a as the current statement; it models the instruction pointer of a CPU.
We use P to range over program states.
If P = h, R, i mr, σ, a, then we use the notation P.stk = σ.
For p = (m, h), the initial program state for executing p is P p = h, λx.0, 0, nil, m, where the function λx.0 is defined on the variables that are used in the program p.A small-step operational semantics for the language is given by the reflexive, transitive closure of the relation → on program states:h, R, i mr, σ, a → h, R, i mr ∧ t • 0 , a :: σ, h(i) if enabled (i mr, i) (1) h, R, i mr, σ, iret → h, R, i mr ∨ t 0 , σ , a if σ = a :: σ (2) h, R, i mr, σ, loop s → h, R, i mr, σ, s; loop s (3) h, R, i mr, σ, x = e; a → h, R{x → eval R (e)}, i mr, σ, a (4) h, R, i mr, σ, imr = imr ∧ i mr ; a → h, R, i mr ∧ i mr , σ, a (5) h, R, i mr, σ, imr = imr ∨ i mr ; a → h, R, i mr ∨ i mr , σ, a (6) h, R, i mr, σ, (if0 (x) s 1 else s 2 ); a → h, R, i mr, σ, s 1 ; a if R(x) = 0 (7) h, R, i mr, σ, (if0 (x) s 1 else s 2 ); a → h, R, i mr, σ, s 2 ; a if R(x) = 0 (8) h, R, i mr, σ, skip; a → h, R, i mr, σ, a(9)We define the function eval R (e) as follows:eval R (c) = c eval R (x) = R(x) eval R (x + c) = R(x) + c eval R (x 1 + x 2 ) = R(x 1 ) + R(x 2 ).
Rule (1) models that if an interrupt is enabled, then it may occur.
The rule says that if enabled (i mr, i), then it is a possible transition to push the current statement on the stack, make h(i) the current statement, and turn off the master bit in the imr.
Notice that we make no assumptions about the interrupts arrivals; any enabled interrupt can occur at any time, and, conversely, no interrupt has to occur.Rule (2) models interrupt return.
The rule says that to return from an interrupt, remove the top element of the stack, make the removed top element the current statement, and turn on the master bit.Rule (3) is an unfolding rule for loops, and Rules (4)-(9) are standard rules for statements.
We say that a program p can terminate if P p → * P and there is no P such that P → P .
We say that a program state h, R, i mr, σ, a is consistent if and only if (1) σ = nil and a = m; or (2) σ = h k :: . . . :: h 1 :: m :: nil and a = h, for k ≥ 0, where k = 0 means σ = m :: nil.
Lemma 1.
(Consistency Preservation) If P is consistent and P → P , then P is consistent.Proof.
A straightforward cases analysis of P → P .
Lemma 2.
(Progress) If P is consistent, then there exists P such that P → P .
Proof.
There are two cases of P :-P = h, R, i mr, nil, m.
There are two cases of m:• if m = loop s, then Rule (3) gives P = h, R, i mr, nil, s; loop s, and• if m = s; m , then Rules (4)- (9) ensure that there exists a state P such that P → P .
-P = h, R, i mr, h k :: . . . :: h 1 :: m :: nil, h, k ≥ 0.
There are two cases of h:• if h = iret, then either k = 0 and s = m :: nil, and Rule (2) gives P = h, R, i mr ∨ t 0 , nil, m, or k > 0 and hence P = h, R, i mr ∨ t 0 , h k−1 :: . . . :: h 1 :: m :: nil, h k , and • if h = s; h , then Rules (4)- (9) ensure that there exists a state P such that P → P .
Proof.
Suppose a program p can terminate, that is, suppose P p → * P and there is no P such that P → P .
Notice first that P p is consistent by consistency criterion (1).
From Lemma 1 and induction on the number of execution steps in P p → * P , we have that P is consistent.
From Lemma 2 we have that there exists P such that P → P , a contradiction.
We will use imr values as types.
When we intend an imr value to be used as a type, we will use the mnemonic device of writing it with a hat, for example, imr.
We will use the bitwise logical implication ≤ as the subtype relation.
For example, 101 ≤ 111.
We will also use ≤ to specify the relationship between an imr value and its type.
When we want to express that an imr value i mr has type imr, we will write i mr ≤ imr.
The meaning is that imr is a conservative approximation of i mr, that is, if a bit in i mr is 1, then the corresponding bit in imr is also 1.
We use K to range over the integers, and we use δ to range over the nonnegative integers.We use τ to range over intersection types of the form:n j=1 (( imr) j δ j −→ ( imr ) j ).
We use τ to range over a sequence τ 1 . . . τ n ; we will use the notation τ (i) = τ i .
We will use the following forms of type judgments:TypeJudgment Meaning τ h : τ Interrupt handler h has type τ τ , imr K σ Stack σ type checks τ , imr K m Main part m type checks τ , imr K h : imr Handler h type checks τ , imr K s : imr Statement s type checks τ K P Program state P type checksThe judgment τ , imr K m means that if the handlers are of type τ , and the imr has type imr, then m type checks.
The integer K bounds the stack size to be at most K.
We can view K as a "stack budget" in the sense that any time an element is placed on the stack, the budget goes down by one, and when an element is removed from the stack, the budget goes up by one.
The type system ensures that the budget does not go below zero.The judgment τ , imr K h : imr means that if the handlers are of type τ , and the imr has type imr, then h type checks, and at the point of returning from the handler, the imr has type imr .
The integer K means that during the call, the stack will grow by at most K elements.
Notice that "during the call" may include calls to other interrupt handlers.The judgment τ , imr K s : imr has a meaning similar to that of τ , imr K h : imr .
A judgment for a program state is related to the concrete syntax used in Section 2 in the following way.
We can dissect the concrete syntax into four parts: (1) a maximum stack size K, (2) the types τ for the handlers, (3) a main part m, and (4) a collection h of handlers.
When we talk about a program (m, h) in the abstract syntax, the two other parts K and τ seem left out.
However, they reappear in the judgment: τ K P (m,h) .
Thus, that judgment can be read simply as: "the program type checks.
"For two sequences h, τ of the same length, we will use the abbreviation:τ h : τto denote the family of judgmentsτ h(i) : τ (i)for all i in the common domain of h and τ .
We will use the abbreviation:safe(τ , imr, K) =      ∀i ∈ 1 . . . n if enabled ( imr, i)then, whenever τ (i) = . . .( imr δ −→ imr ) . . . , we have imr ≤ imr ∧ δ + 1 ≤ K      .
The idea of safe(τ , imr, K) is to guarantee that it is safe for an interrupt to occur.
If an interrupt occurs at a time when the imr has type imr and the "stack budget" is K, then the handler for that interrupt should return with an imr that has a type which is a subtype of imr.
This ensures that imr is still a type for the imr after the interrupt.
Moreover, the stack should grow at most δ elements during the call, plus a return address for the call itself.As a mnemonic, we will sometimes use i mr r for the return imr value of an interrupt handler, and we will sometimes use i mr b for the imr value when an interrupt handler is called.τ h : τ i mr ≤ imr τ , imr K m τ K h, R, i mr, nil, m (10) τ h : τ i mr ≤ imr τ , imr K h : imr r imr r ≤ imr b τ , imr b K σ τ K h, R, i mr, σ, h (11) τ , imr K+1 m τ , imr K m :: nil (12) τ , imr K+1 h : imr r imr r ≤ imr b τ , imr b K+1 σ τ , imr K h :: σ (13) τ , ( imr) j ∧ t • 0 δ j h : ( imr ) j j ∈ 1.
.
n τ h : n j=1 (( imr) j δ j −→ ( imr ) j ) (14) τ , imr K s : imr τ , imr K loop s safe(τ , imr, K) (15) τ , imr K s : imr τ , imr K m τ , imr K s; m (16) τ , imr K iret : imr ∨ t 0 safe(τ , imr, K) (17) τ , imr K s : imr τ , imr K h : imr τ , imr K s; h : imr (18) τ , imr K x = e : imr safe(τ , imr, K) (19) τ , imr K imr = imr ∧ i mr : imr ∧ i mr safe(τ , imr, K)(20)τ ,imr K imr = imr ∨ i mr : imr ∨ i mr safe(τ , imr, K) (21) τ , imr K s 1 : imr τ , imr K s 2 : imr τ , imr K if0 (x) s 1 else s 2 : imr safe(τ , imr, K)(22)τ , imr K s 1 :imr 1 τ , imr 1 K s 2 : imr 2 τ , imr K s 1 ; s 2 : imr 2 (23) τ , imr K skip : imr safe(τ , imr, K)(24)Rules (10)- (11) are for type checking program states.
The actual imr value i mr is abstracted to a type imr which is used to type check the current statement.
In Rule (11), the last two hypotheses ensure that interrupts can return to their callers in a type-safe way.
This involves type checking the stack, which is done by Rules (12)-(13).
Rule (14) says that the type of a handler is an intersection type so the handler must have all of the component types of the intersection.
For each component type, the annotation δ j is used as the bound on how much the stack can grow during a call to the handler.
Notice that an intersection of different components cannot be reduced into a single component.
The rule type checks the handler with the master bit initially turned off.Rules (15)-(24) are type rules for statements.
They are flow-sensitive to the imr, and most of them have the side condition safe(τ , imr, K).
The side condition ensures that if an enabled interrupt occurs, then the handler can both be called and return in a type-safe way.
Theorem 2.
(Type Preservation) Suppose P is a consistent program state.
If τ K P , K ≥ 0, and P → P , then τ K P and K ≥ 0, where K = K + |P.stk| − |P .
stk|.
Theorem 3.
(Multi-Step Type Preservation) Suppose P is a consistent program state.
If τ K P , K ≥ 0, and P → * P , then τ K P and K ≥ 0, where K = K + |P.stk| − |P .
stk|.
Proof.
We need to prove that ∀n ≥ 0, if τ K P , K ≥ 0, and P → n P , then τ K P and K ≥ 0, where K = K + |P.stk| − |P .
stk|.
We proceed by induction on n.
In the base case of n = 0, we have P = P , so K = K + |P.stk| − |P.stk| = K. From P = P and K = K, we have τ K P and K ≥ 0.
In the induction step, assume that the property is true for n. Suppose τ K P , K ≥ 0, and P → n P → P .
From the induction hypothesis, we have τ K P and K ≥ 0, whereK = K + |P.stk| − |P .
stk|(25)From Lemma 1 we have that P is consistent.
From Theorem 2, we have τ K P and K ≥ 0, whereK = K + |P .
stk| − |P .
stk|(26)From Equations (25) and (26) Proof.
Notice first that P p is consistent.
From τ K P p , K ≥ 0, P p → * P , and Theorem 3, we have τ K P and K ≥ 0, where K = K + |P p .
stk| − |P .
stk|.
From K = K + |P p .
stk| − |P .
stk| and |P p .
stk| = 0, we have K = K − |P .
stk|, so, since K ≥ 0, we have |P .
stk| ≤ K, as desired.
Our calculus is a good foundation for studying interrupt-driven systems.
In tune with the need of embedded systems, no program can terminate.
Our type system guarantees stack boundedness, and a number of idioms have been type checked using our prototype implementation.
Our calculus can be viewed as the core of our ZIL language [9,12].
ZIL is an intermediate language that strongly resembles Z86 assembly language except that it uses variables instead of registers.
We use ZIL as an intermediate language in compilers.Future work includes implementing the type checker for a full-scale language, such as ZIL, and experimenting with type checking production code.
Another challenge is to design a more powerful type system which can type check the timer program in Figure 6.
It may be possible to integrate the interrupt calculus with a calculus such as the π-calculus.
This could give the advantage that existing methods, techniques, and tools can be used.To enable our type system to be used easily for legacy systems, we need a way to infer the types of all interrupt handlers.
Such type inference may be related to model checking.
An idea might be to first execute a variant of a model checking algorithm for pushdown automata [13,3], and then use the computed information to construct types (for a possibly related result, see [11]).
At present, it is open whether type inference for our type system is decidable.
2Lemma 3.
(Safe-Guarantee, Statements) If τ , imr K s : imr , then safe(τ , imr, K).
Proof.
By induction on the derivation of τ , imr K s : imr; we omit the details.
Proof.
By induction on the derivation of τ , imr K h : imr , using Lemma 3; we omit the details.Lemma 5.
(Safe-Guarantee, Main) If τ , imr K m, then safe(τ , imr, K).
Proof.
By induction on the derivation of τ , imr K m, using Lemma 3; we omit the details.Lemma 6.
(Safe-Weakening) If K 1 ≤ K 2 and safe(τ , imr, K 1 ), then safe(τ , imr, K 2 ).
that is, safe(τ , imr, K 2 ).
, and Lemma 6, we have safe(τ , imr, K 2 ).
Hence, τ , imr K2 x = e : imr.
-Rule (20).
The proof is similar to that for Rule (19).
-Rule (21).
The proof is similar to that for Rule (19).
From the induction hypothesis, we have τ , imr K2 s 1 : imr From the induction hypothesis, we have τ , imr K2 s 1 : imr 1 and τ , imr 1 K2 s 2 : imr 2 .
Hence, τ , imr K2 s 1 ; s 2 : imr 2 .
-Rule (24).
The proof is similar to that for Rule (19).
Proof.
We proceed by induction on the derivation of τ , imr K1 h : imr .
There are two subcases depending on which one of Rules (17)- (18) was the last one used in the derivation of τ , We can now prove Theorem 2, which we restate here:Suppose P is a consistent program state.
If τ K P , K ≥ 0, and P → P , then τ K P and K ≥ 0, whereProof.
There are nine cases depending on which one of Rules (1)-(9) was used to derive P → P .
-Rule (1).
We have τ , R, i mr, σ, a → h, R, i mr ∧ t • 0 , a :: σ, h(i) and enabled (i mr, i).
Since P is consistent, there are two subcases.
Subcase 1: We have P = h, R, i mr, nil, m and P = h, R, i mr ∧ t • 0 , m :: nil, h(i).
From τ K P and Rule (10), we have the derivation:From τ , imr K m, and Lemma 5, we have that:is true.
From safe(τ , imr, K) and enabled ( imr, i), it follows that:From τ h : τ and Rule (14), we have τ ,, and Lemma 8, we haveFrom τ , imr K m and Rule (12), we have τ ,imr K−1 m :: nil, and K = K + |P.stk| − |P .
stk| = K − 1 ≥ δ ≥ 0, we can use Rule (11) to derive τ K P .
Subcase 2: We have P = h, R, i mr, σ, h, P = h, R, i mr ∧ t • 0 , h :: σ, h(i).
From τ K P and Rule (11), we have the derivation:From τ , imr K h : imr r , and Lemma 4, we have thatthen, whenever τ (i) = . . .is true.
From safe(τ , imr, K) and enabled ( imr, i), it follows thatFrom τ h : τ and Rule (14), we have τ ,, and Lemma 8, we have and Rule (13), we have τ , imr K−1 h :: σ.imr K−1 h :: σ, and K = K + |P.stk| − |P .
stk| = K − 1 ≥ δ ≥ 0, we can use Rule (11) to derive τ K P .
-Rule (2).
We have h, R, i mr, σ, iret → h, R, i mr ∨ t 0 , σ , a, and σ = a :: σ .
Since P is consistent, there are two subcases.
Subcase 1: We have P = h, R, i mr, m :: nil, iret and P = h, R, i mr ∨ t 0 , nil, m. From τ K P , Rule (11), and Rule (12), we have the derivation:m, and K = K +|P.stk|−|P .
stk| = K +1, we can use Rule (10) to derive τ K P .
Subcase 2: We have P = h, R, i mr, h k :: σ , iret and P = h, R, i mr ∨ t 0 , σ , h k .
From τ K P , Rule (11), and Rule (13), we have the derivation:where τ , imr b K h k :: σ is derived as follows:σ, and K = K + |P.stk| − |P .
stk| = K + 1 we can use Rule (11) to derive τ K P .
-Rule (3).
We have h, R, i mr, nil, loop s → h, R, i mr, nil, s; loop s. From τ K P , Rule (10), and Rule (15), we have the derivation:From τ , imr K s : imr, τ , imr K loop s, and Rule (16) we have τ , imr K s; loop s. From τ h : τ , i mr ≤ imr, τ , imr K s; loop s, and K = K+|P.stk|−|P .
stk| = K, we can use Rule (10) to derive τ K P .
-Rule (4).
We have h, R, i mr, σ, x = e; a → h, R{x → eval R (e)}, i mr, σ, a.Since P is consistent, there are two subcases.
Subcase 1: P = h, R, i mr, nil, x = e; m and P = h, R{x → eval R (e)}, i mr, nil, m. From τ K P , Rule (10), and Rule (16), we have the derivation: From τ h : τ , i mr ≤ imr, τ , imr K h : imr r , imr r ≤ imr b , τ , imr b K σ, and K = K + |P.stk| − |P .
stk| = K, we can use Rule (11) to derive τ K P .
-Rules (5)-(9).
The proofs are similar to that for Rule (4); we omit the details.
Lemma 3.
(Safe-Guarantee, Statements) If τ , imr K s : imr , then safe(τ , imr, K).
Proof.
By induction on the derivation of τ , imr K s : imr; we omit the details.
Proof.
By induction on the derivation of τ , imr K h : imr , using Lemma 3; we omit the details.Lemma 5.
(Safe-Guarantee, Main) If τ , imr K m, then safe(τ , imr, K).
Proof.
By induction on the derivation of τ , imr K m, using Lemma 3; we omit the details.Lemma 6.
(Safe-Weakening) If K 1 ≤ K 2 and safe(τ , imr, K 1 ), then safe(τ , imr, K 2 ).
that is, safe(τ , imr, K 2 ).
, and Lemma 6, we have safe(τ , imr, K 2 ).
Hence, τ , imr K2 x = e : imr.
-Rule (20).
The proof is similar to that for Rule (19).
-Rule (21).
The proof is similar to that for Rule (19).
From the induction hypothesis, we have τ , imr K2 s 1 : imr From the induction hypothesis, we have τ , imr K2 s 1 : imr 1 and τ , imr 1 K2 s 2 : imr 2 .
Hence, τ , imr K2 s 1 ; s 2 : imr 2 .
-Rule (24).
The proof is similar to that for Rule (19).
Proof.
We proceed by induction on the derivation of τ , imr K1 h : imr .
There are two subcases depending on which one of Rules (17)- (18) was the last one used in the derivation of τ , We can now prove Theorem 2, which we restate here:Suppose P is a consistent program state.
If τ K P , K ≥ 0, and P → P , then τ K P and K ≥ 0, whereProof.
There are nine cases depending on which one of Rules (1)-(9) was used to derive P → P .
-Rule (1).
We have τ , R, i mr, σ, a → h, R, i mr ∧ t • 0 , a :: σ, h(i) and enabled (i mr, i).
Since P is consistent, there are two subcases.
Subcase 1: We have P = h, R, i mr, nil, m and P = h, R, i mr ∧ t • 0 , m :: nil, h(i).
From τ K P and Rule (10), we have the derivation:From τ , imr K m, and Lemma 5, we have that:is true.
From safe(τ , imr, K) and enabled ( imr, i), it follows that:From τ h : τ and Rule (14), we have τ ,, and Lemma 8, we haveFrom τ , imr K m and Rule (12), we have τ ,imr K−1 m :: nil, and K = K + |P.stk| − |P .
stk| = K − 1 ≥ δ ≥ 0, we can use Rule (11) to derive τ K P .
Subcase 2: We have P = h, R, i mr, σ, h, P = h, R, i mr ∧ t • 0 , h :: σ, h(i).
From τ K P and Rule (11), we have the derivation:From τ , imr K h : imr r , and Lemma 4, we have thatthen, whenever τ (i) = . . .is true.
From safe(τ , imr, K) and enabled ( imr, i), it follows thatFrom τ h : τ and Rule (14), we have τ ,, and Lemma 8, we have and Rule (13), we have τ , imr K−1 h :: σ.imr K−1 h :: σ, and K = K + |P.stk| − |P .
stk| = K − 1 ≥ δ ≥ 0, we can use Rule (11) to derive τ K P .
-Rule (2).
We have h, R, i mr, σ, iret → h, R, i mr ∨ t 0 , σ , a, and σ = a :: σ .
Since P is consistent, there are two subcases.
Subcase 1: We have P = h, R, i mr, m :: nil, iret and P = h, R, i mr ∨ t 0 , nil, m. From τ K P , Rule (11), and Rule (12), we have the derivation:m, and K = K +|P.stk|−|P .
stk| = K +1, we can use Rule (10) to derive τ K P .
Subcase 2: We have P = h, R, i mr, h k :: σ , iret and P = h, R, i mr ∨ t 0 , σ , h k .
From τ K P , Rule (11), and Rule (13), we have the derivation:where τ , imr b K h k :: σ is derived as follows:σ, and K = K + |P.stk| − |P .
stk| = K + 1 we can use Rule (11) to derive τ K P .
-Rule (3).
We have h, R, i mr, nil, loop s → h, R, i mr, nil, s; loop s. From τ K P , Rule (10), and Rule (15), we have the derivation:From τ , imr K s : imr, τ , imr K loop s, and Rule (16) we have τ , imr K s; loop s. From τ h : τ , i mr ≤ imr, τ , imr K s; loop s, and K = K+|P.stk|−|P .
stk| = K, we can use Rule (10) to derive τ K P .
-Rule (4).
We have h, R, i mr, σ, x = e; a → h, R{x → eval R (e)}, i mr, σ, a.Since P is consistent, there are two subcases.
Subcase 1: P = h, R, i mr, nil, x = e; m and P = h, R{x → eval R (e)}, i mr, nil, m. From τ K P , Rule (10), and Rule (16), we have the derivation: From τ h : τ , i mr ≤ imr, τ , imr K h : imr r , imr r ≤ imr b , τ , imr b K σ, and K = K + |P.stk| − |P .
stk| = K, we can use Rule (11) to derive τ K P .
-Rules (5)-(9).
The proofs are similar to that for Rule (4); we omit the details.
