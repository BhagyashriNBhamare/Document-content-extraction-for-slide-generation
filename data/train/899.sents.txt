Time-series data naturally arise in countless domains, such as meteorology, astrophysics, geology, multimedia, and economics.
Similarity search is very popular, and DTW (Dy-namic Time Warping) is one of the two prevailing distance measures.
Although DTW incurs a heavy computation cost, it provides scaling along the time axis.
In this paper, we propose FTW (Fast search method for dynamic Time Warping), which guarantees no false dismissals in similarity query processing.
FTW efficiently prunes a significant number of the search candidates, which leads to a direct reduction in the search cost.
Experiments on real and synthetic sequence data sets reveal that FTW is significantly faster than the best existing method, up to 222 times.
Time-series data naturally occur in many application domains, such as computational biology, meteorology, astrophysics, geology, multimedia and economics.
Even though the databases generated by the corresponding applications continue to grow in size, a common demand is to find similarities between time-series data sequences.
Moreover, these applications require a sequence-matching mechanism that is robust against noise while providing scaling of the time axis of the sequences.
Retrieving long sequences is very expensive given the large data sets involved, and various indexing and searching methods have been proposed to reduce this cost.
Most of the earlier works on high-speed sequence matching are based on the Euclidean distance function.
Since the Euclidean distance function treats sequence elements independently, it cannot be used to calculate the distance between sequences whose lengths and/or sampling rates are different.
Furthermore, it can be sensitive to outliers [2].
Recent applications have adopted Dynamic Time Warping (DTW) [5,22] to overcome these problems [13,21,19,12].
DTW is a transformation that allows sequences to be stretched along the time axis to minimize the distance between the sequences.
The distance of DTW is calculated by dynamic programming (See Fig- shall refer to it as the time warping matrix.
The warping path is the set of grid cells in the time warping matrix, which represents an alignment between the sequences.
Although DTW incurs a heavy computation cost, it is more robust against noise and provides scaling along the time axis.
This ability allows DTW to identify similarities far more accurately and so enhances the functionality of the applications that use it.The ideal method for DTW should fulfill the following requirements:1.
Fast: The exact DTW is quadratic, and prohibitive for long sequences.2.
No false dismissals: A search method that returns the qualifying sequences without any omissions is required.
It should achieve a high level of search performance even though it ensures no false dismissals.
The method should handle any sequence, even data sequences of different lengths and/or data sequences with lengths different from that of the query sequence.
4.
Support for any, as well as for no restriction on warping scope: The method in [14] is fast, because it cleverly exploits global constraints [22] that appear in dynamic programming (See Figure 2).
We would like to have a method that can exploit the restrictions on the warping scope, when the user so desires; but we also want a method that will be fast for the plain DTW, that is, even when the user specifies no restrictions warping scope.Our method, described below, bears all these characteristics, while none of the existing methods can claim the same.The problem we address in this paper is the following:Problem 1.
Given S time-series data sequences of unequal lengths {P1, P2, . . ., PS}, a query sequence Q, an integer k, and optionally a warping scope W , find the knearest neighbors of Q from the data sequence set by using DTW with W .
The warping scope is the area that the warping path is allowed to visit in the time warping matrix.
It can be limited by global constraints as shown in Figure 2.
Some constraints are discussed in [22] and [14].
Note that W can be the unrestricted warping scope as well as any restricted warping scope in the problem definition of this paper.In this paper, we propose FTW (Fast search method for dynamic Time Warping).
To obtain the exact time warping distance, we have to compute distances for all possible warping paths; this incurs a high computation cost.
In order to reduce the computation time, (1) we propose a new lower bounding measure that approximates the time warping distance, (2) we exclude the warping paths that will not yield fruitful search results by using a new algorithm for dynamic programming, and (3) the search algorithm gradually enhances the accuracy of the distance approximations.We carried out experiments on real and synthetic sequence data sets.
FTW pruned a significant number of data sequences at low computation cost, thus reducing the total search cost.
In fact, it is significantly faster than the best existing method, outperforming it by at least one order of magnitude, and occasionally up to 222 times.
Moreover, the superiority of FTW grows as data size and/or sequence length increases.
This tendency makes it more attractive for large and long sequence databases.The remainder of the paper is organized as follows.
Section 2 discusses related work.
Section 3 describes our proposed method, FTW.
We show how the approximate distance can be computed, and then introduce our data structure and search algorithm.
Section 4 reviews the results of the experiments, which clearly show the effectiveness of FTW.
Section 5 is a brief conclusion.
Agrawal et al. first proposed an approach for similarity sequence matching [1].
Their method extracts feature vectors from sequences, and indexes them using R*-trees.
Only a small number of features are extracted, since most multidimensional index structures cannot provide high enough performance for high-dimensional data because of the dimensionality curse problem [4,23].
Their work focus on whole sequence matching.
This was generalized to allow subsequence matching [8,18].
Keogh et al. presented an indexing method by using the Adaptive Piecewise Constant Approximation (APCA) [15].
APCA is a dimensionality reduction technique for sequence matching based on the Euclidean distance.
This technique uses constant-value segments to approximate sequences.
While many dimensionality reduction techniques have been proposed (e.g. DFT [20,10], Discrete Wavelet Transform [24] and Karhunen-Loeve Transform [9]), APCA gives especially high approximation quality.Recent applications require DTW for calculating the similarity of sequences [13,21,19,12].
To reduce the matching cost, many sequence-matching techniques based on dynamic programming have been proposed, especially in speech recognition [22] and bioinformatics [19].
These techniques trade off sequence retrieval speed against precision.
They must visit all data sequences of length N , and the time complexity is still basically O(N 2 ).
Chu et al. [6] proposed a search method based on distance approximation, which varies its accuracy during the course of query processing.
Although this method is efficient, it does not guarantee no false dismissals.Yi et al. proposed a lower bounding measure for DTW [26].
The distance from a query sequence to each data sequence is evaluated by using a lower bounding measure, after which a candidate set is constructed.
The current data sequence for each candidate is visited, and the exact distance between the data sequence and the query sequence is calculated.
The lower bounding measure is the sum of the squared differences between the maximum of the query sequence and elements in the data sequence that are greater than that maximum.
This lower bounding measure also employs elements in the data sequence that are smaller than the minimum of the query sequence.Kim et al. introduced a lower bounding measure employing 4-dimensional vectors [17] that represent the first, last, minimum, and maximum sequence elements.
The vectors can be readily indexed using any spatial access method.
The lower bounding measures proposed in [26] and [17] guarantee no false dismissal.
However, the results provided in [14] indicate that these approximations are coarse.
The number of exact distance calculations for the search is large, thus leading to high search costs.Keogh [14] proposed a search method based on global constraints that appear in dynamic programming.
Global constraints (e.g. the Sakoe-Chiba Band and the Itakura Parallelogram [22]) limit the scope of the warping path.
Zhu et al.'s search method [27] is also based on global constraints and represents an improvement over the one of [14].
The search methods of [14] and [27] compute the envelope of the query sequence from the scope of warping paths; they then derive the PAA (Piecewise Aggregate Approximation) [16,25] of the envelope (See Figures 3 and 4).
The lower bounding distance between each data sequence and the query sequence is defined as the Euclidean distance between the PAA of the envelope and the MBR (Minimum Bounding Rectangle) of the data sequence.
Although these search methods are efficient for warping paths with narrow scope, their search performance deteriorates as the warping scope becomes wider.
Since the optimum scope depends on the application and data set, search methods that give high search performance, even for wide scopes, are required.
We propose FTW (Fast search method for dynamic Time Warping), which guarantees no false dismissals.
The examples provided in this section assume that there is no restriction on the warping scope; recall that is the most expensive setting, that most competing methods can not handle.
Note that FTW can also improve search performance when the warping scope is limited by global constraints.
Unlike previous works [14,27], FTW can handle any sequence, even data sequences of different lengths and/or data sequences with lengths different from that of the query sequence.Range queries and k-nearest neighbor queries are essentially important for practical applications.
Although we mainly focus on k-nearest neighbor queries in this paper, FTW can efficiently support queries of both types.
Dynamic Time Warping (DTW) is a transformation that temporally warps sequences with the goal being to minimize the distance between the sequences.
Consider two sequences P = {p1, p2, . . . , pN } of length N and Q={q1, q2, . . . , qM } of length M .
Their time warping distance D dtw (P, Q) is defined as:D dtw (P, Q) = f (N, M) f (i, j) = 񮽙pi − qj 񮽙 + min ⎧ ⎨ ⎩ f (i, j − 1) f (i − 1, j) f (i − 1, j − 1) (1) f (0, 0) = 0, f(i, 0) = f (0, j) = ∞ (i = 1, . . . , N; j = 1, . . . , M)where 񮽙pi − qj 񮽙 = (pi − qj ) 2 is the distance between two numerical values.
Notice that any other choice (say, absolute difference: 񮽙pi −qj 񮽙 = |pi −qj |) would be fine; our upcoming algorithms are completely independent of such choices.
The time warping distance between two sequences is obtained by matching each sequence element of P to an element of Q in increasing time order.
Since the time warping distance is obtained by using a dynamic programming algorithm whose complexity is O(NM), it can be slow, especially for long sequences.
Our solution is based on three major ideas, described below.
First we give the intuition behind each of them, and in the three subsections we describe each of them in detail.
R i = (p L i : p U i ).
As described in Section 3.1, the complexity of DTW is O(NM), where N and M are sequence lengths.
Since computing the exact DWT distance is expensive, especially for long sequences, it is beneficial to use approximations in query processing.We operate on coarse representations of the sequences, which is obtained by segmentation.
Figure 6 shows an example of the representation, called approximate segments.
In this figure, we represent the coarse version of a sequence with three segments, each of which consists of its segment ranges and time intervals.We use the coarse version of sequences to estimate the time warping distance.
Figure 7(a-1) illustrates the approximate segments of P and Q, P A and Q A .
We compute the lower bounding distance from P A and Q A by using a dynamic programming approach as shown in Figure 7(a-2).
Our algorithms work for segments of arbitrary, even unequal sizes; but for exposition purposes, let's assume that all segments have the same size t. Query processing maintains the list of candidate k-nearest neighbors before reporting the final k-nearest neighbors.
The current k-th nearest neighbor distance, d cb (i.e., the current best), means the exact distance of the best k candidates so far.
When a similar sequence, which gives an exact distance less than d cb , is found, the list of candidate k-nearest neighbors has to be updated; this makes d cb smaller.
d cb keeps decreasing or remains unchanged; it never increases.The second idea is to exclude the warping paths that will not yield fruitful search results by using d cb .
In other words, we exclude the warping paths by using the exact distance between the query sequence and the candidate k-th nearest neighbor sequence.
This makes the distance computation more efficient as d cb becomes smaller.
Even if a given warping scope is not restricted (i.e. a global constraint is not used), we can dynamically reduce the warping scope by excluding warping paths that give distances exceeding d cb .
That is, d cb serves as the threshold for reducing the warping scope.
Now let us use Figure 7(a-2) again to give an example of this idea.
We assume a coarse representation of the time sequences.
Let g(i, j) be the lower bounding distance for the grid cell (i, j) (the exact definition of the distance g(i, j) will be given in Equation 2).
In the figure, ifg(1, 2) > d cb , we do not need to compute g(1, 3) since g(1, 3) ≥ g(1, 2) always holds.
Similarly, we can exclude g(4, 1) if g(3, 1) > d cb .
The gray area represents the reduced warping scope.
We can safely skip the distance computation for a warping path which goes through one or more white areas.
We dynamically compute the time warping distance by using d cb .
As well as for the approximate time warping distance calculation, this idea can be used for the exact time warping distance calculation (See Figure 7(c)).
Instead of operating on approximate segments of a single granularity, we propose to use multiple granularities, trying to balance the trade-off between accuracy and comparison speed.
As the approximate segment becomes more accurate, the approximate distance often increases, but the computation cost also grows.
There is a tradeoff between the approximation distance and the computation cost.
Accordingly, we gradually increase the granularity of sequences, which improves the accuracy of the approximate distance, during the course of query processing.
Figure 7 shows the gradual 'refinement' of the approximation.
In Figure 7 (a), we compute the approximate distance from the coarsest version of sequences as the first step of the refinement.
If the distance is greater than d cb , we can safely prune P .
Otherwise, we compute the distance from the more accurate sequences as the second refinement step (See Figure 7 (b)).
We need to compute the exact distance from the original sequences only if the approximate distance does not exceed d cb (See Figure 7 (c)).
We first introduce the approximate segments of sequences.
Let P = {p1, p2, . . . , pN } be a sequence of length N , then the i-th approximate segment p A i of P is defined as follows:p A i = {p R i , p T i }, p R i = {p L i , p U i } p L i = min{px, . . . , py}, p U i = max{px, . . . , py} x = 񮽙 1 ( i = 1) 񮽙 i−1 j=1 p T j + 1 (2 ≤ i ≤ n), y = i 񮽙 j=1 p T jwhere p R i and p T i denotes the segment range and time interval of P , p L i and p U i are the lower and upper bounds of p R i .
That is, p L i and p U i are the maximum and minimum values among the p T i elements within the subsequence from px to py.
Therefore, we approximate P by:P A = {p A 1 , p A 2 , . . . , p A n }where n denotes the number of segments.Example 1.
Let P be a data sequence of length N = 18, and p T i = 3 (i = 1, . . . , 6).
The approximate segments of P can be derived as follows: P = {3, 2, 3, 5, 7, 6, 6, 5, 7, 10, 12, 11, 13, 15, 14, 12, 13, 12}p R 1 = {2, 3}, p R 2 = {5, 7}, p R 3 = {5, 7}, p R 4 = {10, 12}, p R 5 = {13, 15}, p R 6 = {12, 13}Let Q be a query sequence of length M = 12, and q T i = 3 (i = 1, . . . , 4) The approximate segments of Q is computed as: Q = {7, 6, 7, 10, 12, 11, 9, 8, 11, 10, 8, 9}q R 1 = {6, 7}, q R 2 = {10, 12}, q R 3 = {8, 11}, q R 4 = {8, 10}We propose a new lower bounding distance measure based on a combination of dynamic programming and approximate segments.
Let P A = {p A 1 , . . . , p A n } and Q A = {q A 1 , . . . , q A m }.
We then introduce LBS (Lower Bounding distance measure with Segmentation) as follows:D lbs (P A , Q A ) = g(n, m) g(i, j) = g cell (i, j) + min ⎧ ⎨ ⎩ g(i, j − 1) g(i − 1, j) g(i − 1, j − 1) (2) g cell (i, j) = min(p T i , q T j ) · Dseg(p R i , q R j ) g(0, 0) = 0, g(i, 0) = g(0, j) = ∞where Dseg(p R i , q R j ) denotes the distance between p R i and q R j ; the distance of two ranges is intuitively the distance of their two closest points.
Formally:Dseg(p R i , q R j ) = ⎧ ⎨ ⎩ 񮽙p L i − q U j 񮽙 (p L i > q U j ) 񮽙q L j − p U i 񮽙 (q L j > p U i ) 0 ( otherwise)Notice that, if each range degenerates to a point, the distance Dseg becomes the original distance 񮽙 * , * * between two points.
Theorem 1.
Let P A and Q A be the approximate segments of sequences P and Q, respectively, thenD lbs (P A , Q A ) ≤ D dtw (P, Q)( 3 )Proof.Sinceg cell (1, 1) = min(p T 1 , q T 1 ) · Dseg(p R 1 , q R 1 )the following inequalities hold:g(1, 1) ≤ f (x, q T 1 ) (1≤ x ≤ p T 1 ) g(1, 1) ≤ f (p T 1 , y) (1≤ y ≤ q T 1 ) Since g cell (i, j) = min(p T i , q T j ) · Dseg(p R i , q R j ) we have min{g(i − 1, j − 1), g(i − 1, j)} ≤ f (x, y) (x = i−1 񮽙 k=1 p T k , j−1 񮽙 k=1 q T k < y ≤ j 񮽙 k=1 q T k ) min{g(i − 1, j − 1), g(i, j − 1)} ≤ f (x, y) ( i−1 񮽙 k=1 p T k < x ≤ i 񮽙 k=1 p T k , y = j−1 񮽙 k=1 q T k )Therefore, we obtaing(i, j) ≤ f (x, y) (x = i 񮽙 k=1 p T k , y = j 񮽙 k=1 q T k )Thus, g(n, m) ≤ f (N, M), which completes the proof.
To compute D lbs (P A , Q A ) efficiently, we introduce an algorithm called EarlyStopping (See Figure 8).
During search processing, we maintain the best-so-far distance, d cb ; lower bounding distances greater than d cb do not need to be computed using Equation (2).
EarlyStopping reduces the distance computation cost by using d cb .
Example 2.
Let us consider the approximate segments of P and Q as shown in Example 1.
The lower bounding distance between P and Q is efficiently computed by using EarlyStopping as shown in Figure 9.
Each value indicates g(i, j) in the figure.
If d cb = 40, the calculations of g(1, 3) and g(1, 4) are omitted since g(1, 2) = 174 is greater than d cb .
Similarly, the other white cells are also omitted since g(1, 2) > d cb and g(2, 2) = g(3, 2) = g(4, 1) = 54 > d cb .
As a result, the lower bounding distance between P and Q is computed as g(6, 4) = 45.
Consequently, we can safely prune P , since the lower bounding distance is greater than d cb .
EarlyStopping can be applied to the exact time warping distance calculation for data sequences as well as for the approximate distance calculation, and it reduces computation cost.
Thus, we utilize EarlyStopping for computing the exact distance D dtw (P, Q) of the sequences P and Q. Moreover, EarlyStopping supports global constraints [22].
When the warping scope is limited by global constraints, we change the initial values of begin [i] and end [i] in Figure 8 according to the given warping scope.
In the preceding section, we presented an algorithm computing an approximate distance with approximate segments of a single granularity.
However, we can use segments of multiple granularities for query processing.
Here, we describe the gradual refinement of the distance approximation with multiple granularities.
We use c different versions of P for query processing.
Let P A i be the coarse version of P , which is computed from the time interval ti as:1 < t1 < t2 < . . . < tc−1 < tc < N That is, P A c is the coarsest, while P A 1 is the most accurate.Various algorithms have been proposed to find the optimal representation of approximate segments of each sequences (e.g., [7]).
We will use equal-sized segments to approximate sequences for simplicity although segments of different time intervals would be acceptable to FTW.
Since we operate on equal-sized segments, the time interval of each i-th segment isp T i = 񮽙 t (1 ≤ i ≤ n − 1) tn − N (i = n)(4)where n = 񮽙N/t񮽙.
We propose a sequential structure to accelerate search performance.
The coarse version of P , P A , consists of the series of time intervals P T and the series of segment ranges P R .
While the search algorithm uses both P T and P R , the data structure does not includes P T since P T can be easily com- puted by using Equation (4).
The data structure is simply an array of feature data of a sequence P :F (P ) = {N, Set(P R )} Set(P R ) = {P R c , . . . P R 1 }The feature data F (P ) consists of the length of P , N , and the set of segment ranges of P , Set(P R ).
Our search algorithm is based on the following ideas:1.
The search algorithm first collects as candidates the knearest neighbors based on the lower bounding distance measure computed from approximate segments.FTW reduces computation cost by using the current k-th nearest neighbor distance, d cb ; consequently its efficiency is improved if the current value of d cb is close to the final k-th nearest neighbor distance early in the search process.
Therefore, the algorithm calculates the lower bounding distance between the query sequence and each data sequence from their approximate segments, and then collects the k sequences that have the shortest distance in the sense of the lower bounding distance.
The set of the k collected sequences is regarded as the initial candidate set for the k-nearest neighbor query.
We obtain the initial k-th nearest neighbor distance by computing the exact time warping distance between the collected sequences and the query sequence.
We pick the lowest granularity (i.e., the coarsest sequences) for this initial processing.2.
The search algorithm gradually enhances the accuracy of the distance approximations.Distance approximations incur a lower computation cost than exact distance calculations; moreover, we require a lower distance computation cost for coarser sequences.
The search algorithm first calculates an approximate distance of the coarsest data sequence of the time interval tc.
If the approximate distance is greater than d cb , the algorithm excludes the data sequence without computing its exact distance.
If the approximate distance does not exceed d cb , the algorithm then generates a more accurate approximation based on tc−1, and compares it with d cb .
That is, the search algorithm gradually enhances the accuracy of the distance approximations.
It determines the exact distance only when the approximate distance derived from t1 does not exceed d cb .
Figure 10 shows the search algorithm that uses the data structure described in Section 3.5.1.
We first compute the approximate segments of a given query sequence Q.
We then collect the candidate k-nearest neighbor sequences based on the lower bounding distance measure.
The set of the k collected sequences is used as the initial candidate set.
We obtain the initial k-th nearest neighbor distance by calculating the exact time warping distance between each candidate sequence and the query sequence.
We compute a tighter approximation distance, while reducing the time interval.
If the lower bounding distance is greater than d cb , we exclude the data sequence since it can't be one of the k-nearest neighbors.Although we described only the search algorithm for k-nearest neighbor queries, FTW can be applied to range queries.
It utilizes the current k-th nearest neighbor distance d cb for k-nearest neighbor queries, and the search range is used to handle range queries.
We conducted experiments to verify the superiority of FTW.
[14] claims that the method of [14] outperforms those of [26] and [17]; moreover, [27] confirms that Zhu et al.'s method is more effective than that of [14].
Thus, we show the results of a performance evaluation that compared FTW with the best existing method proposed in [27].
[27]'s method is denoted by LB PAA.
For LB PAA, we used the R*-tree [3], and each sequence was indexed according to 16 reduced dimensions as shown in [27].
We evaluated the search performance mainly based on wall clock time since the wall clock time for DTW exceeds the disk access time for retrieval processing; that is, the total execution time depends primarily on wall clock time.
The wall clock time was measured on an Intel Xeon 2.8GHz with 1GB of memory, running Linux.
We used 20-nearest neighbor queries.
Each result reported here for a particular database size and sequence length is the average of 100 trials.The data sets we used included the following real and synthetic data sets:1.
Temperature: Temperature measurements, from 55 sensors in buildings of Carnegie Mellon University.
Each sensor gives one value every 30 seconds.2.
FinTime: This is the financial time-series benchmark from [11].
We used historical stock market data for 100,000 securities.3.
RandomWalk: We generated 100,000 sequences by using random-walk models [25]: pi = pi−1 + xi where the p1 of each sequence is uniformly distributed in the range (0,10).
xi is normally distributed and the variance is 1.
For FTW, we used four different time intervals (t1 = 2, t2 = 8, t3 = 32, t4 = 128) for sequences of length 2048.
For sequences of length 512, we used three varieties (t1, t2 and t3).
Before discussing the search performance, we should mention the cost of constructing the index structures.
Our method required 381 seconds to construct the data structure for the data set of sequence length 2048 and size 100,000.
LB PAA required 531 seconds including the time for constructing the R*-tree.
To evaluate the search performance, we compared FTW with LB PAA.
We constructed index structures for the data sets of Temperature, FinTime, and RandomWalk.We present experimental results on search performance for when the data set size varies.
Figure 11 depicts the wall clock time of FTW and LB PAA for various data set sizes.
The global constraint we employed for this figure is the Itakura Parallelogram, which is widely used in practical situations [5,22].
The exact distance calculation also employed the same global constraint.
Database size varied from 25,000 to 100,000.
Note that the y-axis starts from a negative value to avoid the overlap between the x-axis and the line of FTW.
The experimental results show that FTW gives superior approximations for all data sets.
As the database size increases, the effectiveness of FTW increases, making it even more attractive for large data sets.
wall clock time.
We employed the Sakoe-Chiba Band for these figures, and let the width of the warping scope vary from 10% to 100% of the sequence length N .
In these figures, the data set size is 100,000, and the scale of the y-axis is logarithmic.
These figures indicate that FTW significantly reduces the search cost for all data sets.
FTW outperforms LB PAA significantly, by orders of magnitude.
Concretely, FTW is up to 222 times faster than LB PAA.
Figure 14 shows how often each approximation was used in FTW for the data set size of 100,000.
As shown in the figure, most of the data sequences are excluded by the approximations of t4 = 128 and t3 = 32.
The approximations of ti−1 incur a computation cost that is (ti/ti−1) 2 times that of the cost of ti; moreover, the exact time warping distance calculation requires a higher computation cost than the approximation of t1.
The coarser approximation provides reasonable approximation quality, and its calculation speed is high.
On the other hand, although the approximation with higher granularity is not very fast, it offers good approximation quality.
Accordingly, using approximations of various granularities has significant advantages in terms of approximation quality and calculation speed.
FTW efficiently prunes a large number of the search candidates, which leads to a significant reduction in the search cost.
Figure 15 shows the number of page accesses for FTW and LB PAA.
Our data structure is simply an array of feature data.
Although data sequences are accessed randomly, feature data are visited sequentially in query processing.
A sequential scan of feature data should significantly boost performance because of the sequential nature of their I/O requests.
In [23], Weber et al. indicate two speed-up factors for the phenomenon: a practical factor of 10 and a conservative one of 5.
Thus, we used these speed-up factors, SF , in our experimental evaluations.
The number of page accesses of FTW for a speed-up factor SF is given by:P ASF = P A f d SF + P A ds ,where P A f d and P A ds are the number of page accesses for feature data and data sequences, respectively.
Figure 15 indicates that FTW outperforms LB PAA for SF = 5 and SF = 10 in terms of page accesses.
We created four sequence data sets that contained sequences with different lengths, N : Random(2048, 32), Random(2048, 64), Random(2048, 128), and Random(2048, where Random(Nave, N dif f ) is a random function, Nave is the average sequence length in a data set, and N dif f is the difference between the minimum sequence length and the maximum sequence length in a data set.
All data sets had 100,000 sequences.
Figure 16 shows the wall clock time of a linear scan and FTW as a function of N dif f .
We employed the Itakura Parallelogram for this figure.
Clearly, FTW outperforms sequential scanning for all data sets.
Its search performance is superior even when N dif f is large.
We presented a new search method for DTW, called FTW, which meets all the requirements mentioned in the introduction.
FTW has been careful designed based on a new lower bounding distance measure that approximates the time warping distance.
Based on this lower-bounding idea, and several careful optimizations ('EarlyStopping', 'Refinement'), our method has all of the following specifications, that no competing method matches:1.
It is fast: In experiments on real and synthetic data sets, FTW clearly outperformed the best existing method, for all queries, achieving one or two orders of magnitude speed up.2.
It has no false dismissals (by our Theorem 1)3.
It can handle sequences of arbitrary lengths 4.
It allows for arbitrary warping restrictions, as well as no restrictions at allOur experimental results reveal that FTW is significantly faster than the best existing method, outperforming it by at least one order of magnitude, and occasionally up to 222 times.A promising but very challenging research direction is to extend FTW for a streaming setting.
The goal would be to monitor time sequences, looking for one or more prespecified patterns, where the (dis-)similarity score is determined by DTW.
