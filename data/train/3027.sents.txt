Modern cache hierarchies are tangled webs of complexity.
Multiple tiers of heterogeneous physical and virtual devices, with many configurable parameters, all contend to optimally serve swarms of requests between local and remote applications.
The challenge of effectively designing these systems is exacerbated by continuous advances in hardware, firmware, innovation in cache eviction algorithms, and evolving workloads and access patterns.
This rapidly expanding configuration space has made it costly and time-consuming to physically experiment with numerous cache configurations for even a single stable workload.
Current cache evaluation techniques (e.g., Miss Ratio Curves) are shortsighted: they analyze only a single tier of cache, focus primarily on performance, and fail to examine the critical relationships between metrics like throughput and monetary cost.
Publicly available I/O cache simulators are also lacking: they can only simulate a fixed or limited number of cache tiers, are missing key features, or offer limited analyses.
It is our position that best practices in cache analysis should include the evaluation of multi-tier configurations, coupled with more comprehensive metrics that reveal critical design trade-offs, especially monetary costs.
We are developing an n-level I/O cache simulator that is general enough to model any cache hierarchy, captures many metrics, provides a robust set of analysis features, and is easily extendable to facilitate experimental research or production level provisioning.
To demonstrate the value of our proposed metrics and simulator, we extended an existing cache simulator (PyMimircache).
We present several interesting and counter-intuitive results in this paper.
The vast configuration space of multi-tier caching enables the design of very complex systems.
Several tiers of cache and persistent storage can be allocated in numerous arrangements.
Moreover, devices can be partitioned into many differently sized cache segments for separate applications.
All of these devices can be implemented within, and interact with, any number of independent, large-scale infrastructures (e.g., cloud services, virtual machines, big data warehouses, distributed systems).
Furthermore, new storage technologies are constantly emerging (e.g., NVM, 3D flash), introducing additional complexity, greater capacities, and different cost/performance profiles.
Our ability to dynamically change hardware in live systems (e.g., adding or deleting RAM, SSD, NVM) has also been increasing, particularly in cloud environments and virtual machines [15,16,21], making it significantly easier to reconfigure a cache hierarchy.
Workloads continue to evolve as well, with complex and diverse access patterns that affect the frequency of data reuse and the size of working sets, two of the most influential factors in any caching system [4,14,46,48,58].
Research in cache algorithms and policies is also trying to keep up with these changes.
Machine learning and similar techniques that leverage historical data are being incorporated into caching systems to bolster prefetching [63], dynamically switch between replacement algorithms [45,46,53], or enhance existing eviction policies [2].
I/O classification has been used to enforce caching policies and improve file system performance [39].
Multi-tier cache eviction algorithms that are aware of some or all layers in the hierarchy at any given time are being developed [14].
The challenges of cache resource allocation and provisioning are being investigated as well [5,31].
Zhang et al. introduced CHOPT, a choice-aware, optimal, offline algorithm for data placement in multi-tier systems [65].
Algorithms such as CHOPT are promising solutions for efficiently finding optimal multi-tier configurations, but their bounding assumptions and inability to model all parameters limit the configuration space they can explore.Physically experimenting with various cache configurations is costly and time-consuming, with so many parameters to consider (e.g., number of tiers, device types and models, caching policies).
A well-known technique for evaluating cache performance without running experiments is Miss Ratio Curve (MRC) analysis [7,25,26,54].
MRCs plot the cumulative miss ratio of all requests in a given workload for some cache eviction algorithm(s) as a function of cache size.
Cache size usually ranges from one data block to the size required to store every unique block accessed in the workload, also known as the working set.
This technique has many uses, such as comparing eviction algorithms' performance for a given workload or identifying optimal cache size allocations.
However, MRCs evaluate the performance of only a single cache and are not capable of accurately modeling the complicated interactions between devices in a multi-tier cache.
Recent studies have shown that traditional MRCs are even sub-optimal for resource allocation in a single layer, since they admit data with poor locality into the cache.
[20].
It is vital that our methods of evaluating caches mature as storage technologies and cache hierarchies continuously evolve.
For example, examining performance metrics such as latency or using an MRC to analyze miss ratio as cache size increases may be misleading without also considering the monetary cost of purchasing and using the cache.
Cost has a non-linear, positive correlation with cache size, and is fundamentally the primary constraint when deciding how much cache to include in a system.
If this were not the case, everyone would cache all data in copious amounts of the fastest DRAM money can buy and back it up with a huge battery.
Furthermore, improved performance does not directly translate into cost efficiency, especially in a multi-tier system where devices' cost and performance characteristics can vary wildly.
The purchase cost of hardware is a simple example.
Ideally, we should be evaluating more comprehensive metrics such as the total cost of ownership, which combines other metrics such as power consumption, the cost of labor to maintain a system, and the projected lifetime of devices given access patterns.
It is also essential that we can freely evaluate the relationship between metrics (e.g., throughput/$) so we can make educated design decisions with full awareness of the inherent trade-offs.
The most complete solution would be an n-level I/O cache simulator that could quickly and accurately evaluate many configurations.
While there are some advanced CPU cache simulators available [18,27,38,43,56], storage cache simulators are scarce and lacking.
State-of-the-art storage cache simulators are mostly outdated; they either can simulate only a single layer or some fixed set of layers, have limited analysis features, are not easily extendable, or are simply not released to the public [1,23,59].
PyMimircache [62] is a popular open-source storage simulator with several useful features that is actively maintained.
However, even this simulator is inadequate; it also can simulate only a single layer of cache with no implementation of back-end storage, has no concept of write policy, and its analysis features are limited.
The main strength of PyMimircache is its ability to perform MRC analysis on multiple cache replacement algorithms.It is our position that best practices in cache research need to be broadened to reflect the growing multi-tier configuration space.
This paper makes the following contributions:1.
We explore current trends in cache analysis and propose that best practices in cache research including the analysis of multi-tier configurations and a more comprehensive set of evaluation metrics (e.g., monetary cost).2.
We describe the critical features an n-level I/O cache simulator should have and outline the design of a simulator we began to develop.3.
We extended PyMimircache to function as a multi-tier cache simulator, experimented with many configurations on a diverse set of real-world traces, and present initial results that support our position.
The fundamental strategy in engineering a cache hierarchy involves placing faster and typically lower-capacity devices in front of slower devices to improve the overall latency of accessing frequently reused data.
There is a tangible dollar cost per byte increase when purchasing hardware with better performance attributes.
Therefore, it follows that the cache size and speed are closely correlated with the purchase cost.
Straightforward logic dictates that performance is constrained by cost, so unless money is in endless supply, the best practice should be to evaluate these metrics together.
Surprisingly though, cost is often overlooked during analysis in favor of performance metrics such as raw throughput, latency, or hit/miss ratio [10, 13-15, 20, 44, 57].
The argument can be made that any improvement in cache performance translates into a reduction in cost when designing a cache, such that the relationship between cost and performance does not necessarily need to be considered.
This is situationally true, particularly when evaluating performance in a single-tier caching system.
However, in a more realistic, multi-tier storage or CPU cache hierarchy, the large configuration space and complex interactions between tiers produce scenarios where the relative performance per dollar between two configurations is vastly different, necessitating a more complex analysis (see Section 4 for examples).
Performance metrics have long been the standard in cache analysis.
Recently, additional metrics that are more relevant and informative for specific applications have gained popularity in storage research.
The 95 th (P95) or 99 th (P99) percentile latency, often referred to as tail latency, is an important quality of service (QoS) metric for cloud [49,61] and web [5,17,24,28] services, as well as at the hardware level [8,19,32,36].
Inter-cache traffic analysis has been used to design more efficient cache hierarchies in modern microprocessors [42].
Reducing the energy consumption of storage systems is beneficial for the environment, lowers operation costs, and promotes advancements in hardware design [9,34,47,52].
Even the total cost of ownership (TCO) can be difficult to calculate when considering all the factors that contribute to capital and operational expenditures (CapEx and OpEx) [33,35].
It is our position that cache analysis should be conducted using a diverse set of metrics whenever possible.
These metrics should be evaluated at various level of granularity: at each individual layer, some subset of layers, or globally.
Moreover, we need to create complex metrics (e.g., throughput/$) that allow for analysis of their informative relationships and reveals critical design trade-offs.
Simulator Design.
A general, n-level I/O cache simulator with a rich set of features is necessary to thoroughly explore the multi-tier caching configuration space and analyze our proposed metrics.
We are developing such a simulator that includes (but is not limited to) the following capabilities: (1) Write policy that determines where data is placed upon write requests.
We will support traditional write policies (e.g., write through, write back, write around), but also allow user-defined policies.
(2) Admission policy that controls if and how data is promoted and demoted throughout the hierarchy by request size, address space, or simply whether layers are inclusive or exclusive of each other.
(3) Eviction policy that decides which data to evict when a cache is full and new data needs to be brought in.
There will be support for single-layer or global policies, as well as the ability to easily add new policies.
(4) Trace sampling techniques (e.g., MiniatureSimulations [55]) that reduce the size of a trace to greatly decrease simulation time while maintaining similar cache behavior.
(5) Prefetching to retrieve data before it is requested with techniques like MITHRIL [64] that exploit historical access patterns.
The associated API will fully expose all data structures at request-level granularity or for any given real timestamp or virtual ones (where the trace has only ordered records without their original timing).
This will allow users to perform important analysis such as examining clean and dirty pages at any level, measure inter-reference recency, calculate stack distance metrics when relevant, or perform any type of analysis offered by our simulation framework on a subset of a trace.
The simulator will also be coupled with modern visualization tools that enable users to efficiently explore the large amount of data it produces.Multi-tier Cache Reconfiguration.
A major motivation for simulation is seeking optimal cache configurations.
However, efficiently reconfiguring a multi-tier cache hierarchy is another challenging problem.
In this work, we analyze various physical devices for simplicity, but manually swapping out devices is often not a feasible solution.
More likely, multi-tier caches may be dynamically reconfigured in cloud, distributed, and virtual environments, where storage can more easily be allocated through virtualization abstractions.
For example, distributed memory caching systems (e.g., Memcached) can greatly benefit from automatically reconfiguring cache nodes in response to changes in workload; but this process can significantly degrade performance as nodes are retired and data is migrated.
Hafeez et al. developed ElMem, an elastic Memcached system that uses a novel cache-merging algorithm to optimize data migration between nodes during reconfiguration [22].
Moving between configurations in any caching system has a temporarily negative impact on performance, until the new caches are fully warmed [12,66].
Therefore, efficient reconfiguration methods are essential to fully leverage any techniques that find optimal configurations (including simulations).
PyMimircache Extension.
To demonstrate the utility of our proposed simulator, we extended PyMimircache [62], a storage cache simulator with an easily extendable Python front-end and efficient C back-end.
We made several simplifying assumptions for this extension and experimented with a subset of the possible features we are proposing.
(1) We implemented a traditional write-through policy and an "optimistic" write-back policy as global write policies.
The write-through policy is consistent and reliable: a block is written to every cache layer and the back-end storage whenever there is a write request.
Our write-back policy is optimistic: it only writes to the first layer and assumes this data will be flushed to persistent storage at some point in time, outside of the critical path where it does not affect performance (i.e., we do not account for the write in any other layer).
This simplified version of write-back models the best-case performance scenario, which we found useful for exploring the potential effects of write policy.
A more realistic write-back would require asynchronous functionality that is not available in PyMimircache, and is a limitation of this work.
(2) All evicted blocks are discarded rather than demoted (moved or copied) to some lower layer of cache or back-end storage.
(3) Layers of DRAM are included in our simulations even though we are using block traces, which capture requests for data that was not found in DRAM.
This is a limitation of the traces we are using; the simulator we implement will be able to operate on any data item from any trace that includes some form of address accesses.
The simulator will support traces obtained from networks (e.g., NFS, HTML), distributed systems (e.g., HPC, Memcached), system calls, block traces, and potentially more.
(4) Throughput is limited by the system where traces were actually captured since we experiment with block traces.
For demonstration purposes, we ignore this limitation and assume requests are fed as fast as possible without using the original request timestamps.
This allows us to show how we can potentially evaluate throughput when using different hardware configurations.
(5) We consider each layer to have a portion of its capacity partitioned for caching to emulate various cache sizes at each layer using the specifications of a single device.A high-level description of how we extended PyMimircache is as follows: (1) We feed an original block I/O trace to an instance of PyMimircache, this is the top layer ("L1") of our cache hierarchy.
(2) This instance generates two output files: i) A log file "L1-log" containing counters for the following: read hits, write hits, read misses, write misses, data read, data written.
ii) We specify a new trace file called "L1-trace" which contains read requests that missed in L1, as well as all write requests.
As per our assumptions, write requests are not included when using write-back policy and evicted blocks from L1 are never included.
These intermediate trace files are stored in memory using Python-based virtual files to avoid disk I/O costs.
(3) After the L1 instance of PyMimircache completes, we feed the generated "L1-trace" from step 2 as input into another, separate instance of PyMimircache.
This emulates our L2 layer.
(4) We repeat steps 2-3 for L2, L3, etc. (5) When all layers have been processed, we aggregate all the log data into a single log file for that experiment.
(6) We have a higher-level script that we pass parameters to for each layer's device: purchase cost, capacity, and average read and write latencies.
This script records and calculates the following metrics for the cache configuration of an experiment: total purchase cost, partitioned device capacities, miss ratio per layer, and total read and write latency incurred.
It can be [re]run at any time using previously obtained simulation logs, and is separate from the actual simulation process.
Workloads.
In this section, we evaluate simulation results gathered using the Microsoft Research (MSR) traces.
These 36 traces, each about a week long, were collected from 36 different volumes on 13 production servers at MSR in Cambridge, Massachusetts, as described in detail by Narayanan et al. [40].
The percentage of total requests that access unique blocks (i.e., data used for the first time) in these traces range from 1% to 97%, which is representative of the frequency of data reuse.
The percentage of total requests that are writes range from nearly 0% to almost 100%, and is ideal for evaluating the effects of write policies.We are continuing to run additional experiments using 9 traces from the Department of Computer Science at Florida International University (FIU) [52] and 106 traces from CloudPhysics [54], but do not present results here due to space limitations.
Experimental setup.
We ran simulations on the MSR traces using between 1 and 3 layers of cache, in addition to the back-end storage device.
Each simulation consisted of a configuration of several parameters: cache and back-end sizes, eviction algorithms, and global write policy.
The capacity required to hold the entire working set of a trace dictated the cache and back-end storage sizes for every configuration.
The back-end size was always fixed to be the same size as the working set, since the data initially resides in the back-end.
The cache sizes selected for the first layer of cache are 100 evenly spaced sizes between 1 block (512 bytes) and the size of the working set for that trace.
100 is the default number of points for plotting MRCs with PyMimircache.
The second and third layers of cache are 10 evenly spaced sizes within the same range.
Using 10 cache sizes for these layers rather than 100 drastically reduced the time required to complete each experiment while still revealing the entire range of metrics (albeit with fewer data points within that range).
In this work, we only present results for configurations using a Least Recently Used (LRU) eviction policy at every layer, although we are varying these policies in our ongoing simulations.
We simulated each of the MSR traces using our extension of PyMimircache (see Section 3) and then calculated cost and performance metrics using the device specifications described in Table 1.
While these comprehensive traces represent a wide variety of workloads, they have only a relatively small working-set size that can easily fit in a modern server's RAM.
Therefore, to simulate larger workloads (e.g., bigdata, HPC), we treat the original MSR traces as if they were scaled-down spatial samples of larger traces.
We call this technique reverse-mini-sim: the reverse of the miniature simulations technique for down-scaling traces introduced by Waldspurger et al. [55].
Miniature simulations was shown to be fairly accurate at a sampling rate of 0.001 on the MSR traces, so we multiply the purchase cost (X axis) by a factor of 1,000 times: this simulates a workload whose working set size is 1,000× larger.Each data point in our figures represents a configuration with some set of cache sizes.
We assume that each layer consists of an independent device with a portion of its capacity partitioned for caching and the remaining capacity as unused.
For example, a cyan triangle in Figure 1 at Total Scaled Purchase Cost of around $235 represents the average throughput of all requests in a single simulation of the hm-1 trace with an L1 LRU cache of 61,865 blocks partitioned in device D2, an L2 LRU cache of 199,344 blocks partitioned in device S2, and back-end storage of device H3 partitioned to fit the working set of 687,396 blocks.Cache hierarchy depth.
Figure 1 shows (D1-H3, red) that too little RAM hurts performance but too much wastes money.
Adding a bit of SSD cache (D2-S2-H3, cyan) between DRAM and HDD (D2-H3, green) can help, but not always (some cyan dots are below the green line).
Consider the knee of D1-H3 (around X=$500): there are D2-S2-H3 configurations that provide higher throughput for the same cost, same throughput for less cost, and even both higher throughput and less cost.
Surprisingly, we also see that purchasing more of a cheaper DRAM (D3-H3, blue) for the same cost of a more expensive DRAM (D1-H3) yields overall better performance.
Therefore, we can sacrifice DRAM performance for a larger amount of DRAM to get better results.Solid-state drive (SSD) degradation.
Storage devices have an expected lifetime which is typically defined by some amount of I/O.
For example, it is well-known that the memory cells within SSDs can only be written to a finite number of times before they are no longer usable [29,37,41].
While the lifespan of devices is a parameter that should be considered when estimating the total cost of ownership of a storage system over some period of time, it is also important to evaluate the performance impact this aging process can have.
Studies have shown that SSD aging can increase average latency by around 2-3× [30].
To simulate this effect, we multiplied the latency specifications of device S2 and analyzed the results alongside simulations using its original specifications.
Figure 2 shows that while a new SSD (D1-S2-H2, green triangles) improves performance when inserted into a D1-H2 tier (red), when the SSD is aged (blue and cyan), performance is actually worse than not having the SSD at all.
For users with write-heavy workloads or infrastructures where these devices are expected to receive a lot of I/O traffic over a short period of time, choosing to exclude SSDs completely may not only save money, but also yield a similar or better average throughput over time.
Device specification variance.
Storage vendors want to convince consumers that their latest device is competitive.
They do so by publishing many device specifications: storage capacity, physical dimensions, hardware interface, durability, energy consumption, and performance metrics.
While most specifications are fairly standard, a wide variation of performance metrics can be found, even amongst the same type of device and vendor.
Some commonly found metrics are the minimum, average, median, or maximum values for latency, bandwidth, or throughput.
These metrics may also be further refined as random or sequential workloads, or separated by reads and writes.
These measurements are obtained via benchmarks using some specific workload(s), software environment, and hardware configuration, which are sometimes disclosed at varying levels of detail.
This poses a significant problem for consumers, who often are unable to reproduce vendors' performance results.
Given such a vast configuration space of variables that can affect performance and the understandable motivation for vendors to publish optimistic results, how can storage devices be reliably compared for their own usage?
A handful of independent, reputable websites have emerged by fixing these variables and benchmarking devices from different vendors, and producing realistic, trustworthy specifications: AnandTech [3], Tom's Hardware [50] and UserBenchmark [51].
In this experiment we show the difference between numbers reported by vendors and others.
Figure 3 shows that inserting an SSD tier between DRAM and HDD provides equal or better performance when using vendor reported specifications (green and cyan).
However, specifications obtained from Anandtech [3] (red and blue) show that the majority of the configurations yield worse average throughput.Write Policy.
The write policy of a cache hierarchy determines how and where data is written whenever there is a write request.
Write-through policy ensures data consistency by writing data to every cache and storage device in the hierarchy.
However, this incurs the write latency of every device and negatively impacts overall performance.
The write-back policy improves performance over write-through by only writing to the cache and then flushing data to back-end storage at a more favorable time.
The downside of write-back is that data is at risk of being lost in the event that a cache device fails or whole system loses power.
If reliability is more important, a write-through policy is the obvious choice, but how much impact will this have on performance?
Figure 4 compares write-through and write-back policies (policy implementations described in Section 3).
Using an optimistic write-back (wb) policy we achieve up to 6× better throughput for the same cost as write-through (wt) with the same devices.
Note that a more accurate write-back policy will account for the delayed writes, which will tie up the storage devices even during idle times.
Modern processors are designed with multiple cores, each containing multiple tiers of cache, as well as a shared cache.
Several architecture simulators have been developed and are widely used by industry and academia to facilitate engineering, research, and education [18,27,38,43,56].
One example of this is gem5, a popular architecture simulator that has been actively developed for nearly two decades [6].
It supports multiple ISAs and can accurately model complex multi-level non-uniform cache hierarchies with heterogeneous memories.
Architecture simulators such as gem5 have great value, but are fundamentally different than storage simulators.
Complex cache replacement algorithms that are designed specifically for storage devices (e.g., SAC [11] and GCaR [60]) could not be reasonably implemented in an architecture simulator.
Architecture simulators are typically driven by binaries or instruction-level traces, and could not operate on traces captured at the block, network, or system call layers.Conversely, storage cache simulators are scarce and lacking in features.
Accusim was developed to evaluate the performance impact of kernel prefetching [1].
It was designed specifically for file system caching and can not to model n tiers.
SimIdeal is a multitier simulator that implements several cache replacement and write policies [23].
It hard-codes the number of tiers to four and forces evictions to the immediate lower layer, and thus cannot support inclusive caching.
There are also a handful of outdated simulators such as Pantheon [59].
Unfortunately, there are few storage cache simulators available, and caching research is commonly done using proprietary simulators that are not available publicly.
Designing and evaluating cache hierarchies has become incredibly complex due to the expanding multi-tier configuration space.
In this work, we analyzed the deficiencies of single-tier cache analysis and common cache evaluation metrics.
We propose that best practices in cache research should include the analysis of multi-tier systems, as well as the evaluation of a more comprehensive set of metrics (particularly monetary cost) and their relationships.
We are developing an n-level I/O cache simulator with a rich set of features and analysis tools that is capable of modeling any cache hierarchy.
We extended PyMimircache to function as a multi-tier cache simulator and experimented with a wide variety of workload.
We presented interesting and counter-intuitive results that demonstrate the need for our proposed simulator and multi-tier analysis.
We thank the anonymous HotStorage reviewers, our shepherd Michael Mesnier, and Carl Waldspurger for their valuable feedback.
This work was made possible in part thanks to Dell-EMC, NetApp, and IBM support;and NSF awards CCF-1918225, CNS- 1900706, CNS-1729939, CNS-1755958, and CNS-1730726.
Our proposal includes redefining best practices in cache research and the development of a sophisticated simulator with many features.
We find the following discussion topics to be of interest and valuable to this line of work:1.
What features should be included in a multi-tier cache simulator?
We give a high-level view of the features we believe to be necessary in Section 3, but are we missing anything important?
2.
How a feature is implemented determines how useful it is for the research community.
For example, eviction policies can include a single layer, global awareness, or machine-learning algorithms; it can also have support for users to easily define their own.
What is important within each feature we implement?
3.
We consider monetary cost to be the primary metric when designing any caching system.
How important is monetary cost?
Are there more important metrics to consider?
4.
The ability to reconfigure a cache hierarchy in real-time is considerably valuable, especially in multi-tenant scenarios where resizing involves repartitioning across tenants.
What is a good time-frame for a simulator to produce valuable results that are not already stale?
What policy parameters could be changed dynamically?
5.
We propose best practices in cache research that should include the evaluation of multi-tier hierarchies, analyzed using a diverse set of metrics.
What else should be a part of best practice?
6.
Under what circumstances would evaluating caching (system performance, design, algorithms, etc.) not benefit from a multi-tier analysis?
7.
Are there any issues we are not taking into consideration (e.g., missing features, future technologies, algorithmic solutions)?
