Although our library can be used with the standard generic payload, more detail is captured using our extended version called PW TLM PAYTYPE.
In TLM 2.0, sockets are templated types which default to use the standard generic payload, but we can instead use PW TLM PAYTYPE.
Rather than explicitly extending the generic payload, we could have used the generic payload's own extensions (and these still work, as used for instance for extended commands such as loadlinked/store conditional), but we chose not to for efficiency reasons.
Socket definitions and calls now look like this (although CPP macros can tidy this up)://Providing the third template argument to a socket: tlm_utils::simple_initiator_socket <mytype, 64, PW_TLM_TYPES> ifetch_socket, data_socket; //Using the extended payload in the callbacks: void b_access(PW_TLM_PAYTYPE &trans, sc_time &delay)The extensions in PW TLM PAYTYPE assist with the following details: 1) deciding which fields are active so that only the correct fields have their hamming distance processed for wiring power, 2) establishing the trajectory of the transaction through the system so that traversed wire length is estimated, 3) keeping note of the components encountered so that the correct power and utlisation accounts can be incremented under DMI, 4) measuring the variance of metrics so that automated transition to DMI is enabled.
In a generic example, Fig. 2, the originator (CPU) will complete the address field of the payload and, for writes, also the data and byte-enabled fields.
Generally, multi_passthrough TLM sockets are used in complex system models: these support forwarding the transaction onwards through bus, cache and NoC (network-on-chip) subsystems.
The return path is always the reverse of the forward path owing to simple stack unwinding associated with method invocation.
So intermediate components forward the payload, perhaps with minor changes (e.g. address space manipulations at bus bridges or VM units) to the target destination.
This target will reply with a low-cost acknowledgement for a write and with the data for a read.Our TLM payload offers an API with three library calls for bus energy modelling.
These are pw_set_origin, pw_log_hop and pw_terminus.
Currently models invoke these on a payload at the beginning, intermediate steps and end of a its payload trajectory.
Rather than manual application, building these invocations into the TLM convenience sockets would be more convenient (sic).
The first argument where is the this pointer for the current component.
This is used to track the path through the system.The second argument is the flags that denote which fields in the payload are active.
They can also encode bidirectional data busses and multiplexed address-data busses.
When the physical nets of busses are re-used the transition count increases but there are fewer busses (eg.
the high order address bits might be mostly static on a dedicated address bus but are not if the same wires carry multiplexed address and data).
Most flags are sticky and apply to subsequent hops that do not change that flag.
In particular, if the flags argument is zero for the next hop then nothing has changed and the next hop has the same properties as the previous hop.The third argument is a bus reference.
Every transaction is considered to take place over a bus and a bus is a generic set of wires modelled with a bit_transition_tracker.
Wires present (ie.
payload fields) that are not used consume no energy, so it is not important to customise the instance of a bus to its use (e.g. the bus from CPU to memory has address and write data whereas the return bus has just read data).
The bus reference is needed so as to check which physical nets are transitioning with respect to their previous value.We could integrate a layout package to estimate wiring lengths in detail.
Currently we use the Rentian approach of [4] that provides a simple estimate of average connection length in a well-placed implementation according to α.A 1 2 where A is the area of the lowest common parent component to the source and destination of the signal and α is typically 0.3.
The capacitance per unit length of nets for on-chip and off-chip wiring is read from a configuration file (we use 0.3 pF/mm).
TLM 2.0 defined a DMI record called tlm::tlm_dmi which stores the start and end addresses and access times for a region of memory that can be accessed via a fast backdoor mechanism: the client simply reads from the raw host memory that contains the memory contents.
In addition, the target has a callback called invalidate_direct_mem_ptr whereby this record can be retracted.
However, using this DMI mechanism bypasses the target model and also all intermediate bus components, including caches, so their utilisation and energy accounts cannot be updated directly during DMI accesses.
In TLM POWER3, we make the forward trajectory of a TLM DMI-allowed call instantiate a chain of account records that contain the energy and utilisation and account number for each intermediate agent and the target.
The energy and utilisation are also updated on the return transit of the thread.
(For nonblocking calls, the updates are just made on the significant protocol phases.)
At the initiator we augment the DMI record with a count of the number of DMI calls made (scaled by the relative size of the transaction if the payload burst size is varying).
Aperiodically (eg.
at end of LT quantum), and on DMI invalidate, the count field is reset with the appropriate credits being made to the utilisation and energy accounts of each referenced component.
The invalidate DMI callback also performs such a flush and frees the agent list.
Operations such as store conditionals must not use DMI, so can be used as flush points.An alternative to building the agent records is to write DMI energy to a 'slush fund' account where it will appear (correctly) in the total for the system/subsystem but (incorrectly) in the slush fund of the originator of such transactions instead of the consuming component (which is ensured by our agent records).
We would perhaps use account number 4 in each component for this purpose.The energy and power figures in a call to the library can either be pre or post supply voltage scaling, where the former are multiplied by the supply voltage squared at the point of logging and the former are not scaled.
Given that a component (sc_module) inherits our pw_module_base, transaction energy logging in a component can be as simple as: alternative account to the default of '1'.
Multiplying by the supply voltage squared on every logging event is slow, and hence pre-computing this in the PVT method is preferred.
Account one is the default intra-component dynamic energy account.
The log of the utilisation itself takes the busy duration and an extra, optional second argument which is the advance over kernel simulation time needed for accurate rendering when loosely-timed.
The 'this->' prefix would either be missed out, but preferable is is to replace it with the agent handle returned from log_hop call.
This applies the energy and utilisation debits to the current component but also inserts their values in the agent list (if one is being constructed) so that they are accounted when subsequent calls are replaced with DMI.Using standard TLM 2.0, an initiator will start using DMI when calling get_direct_mem_ptr on the initiating socket after a transaction instantiates a valid, local DMI record.
Typically the initiator has no knowledge of the accuracy of the latency figures in its DMI record: naively, these will just be the result of the first call (which could be much slower owing to cache misses etc.).
We provide and use a 'confidence switcher' to ensure DMI is employed with fairly accurate values for latency as well as energy and utilisation in an extended DMI record.The confidence switcher (Fig. 3) is a simple library component designed to capture the value of a presumed-stationary Fig. 4.
Splash RADIX benchmark: probed processor power consumption.
Two runs using two cores followed by one run on a single core.
The end region of each run is the checking phase.
Spikes are other unix processes on the dual-core workstation (Intel Pentium D 3GHz).
Fig. 5.
Splash RADIX benchmark: TLM POWER3 total power consumption: we see one run using two cores followed by one run using a single core.
No unix operating system was present.statistic using a relatively small number of costly trials.
It has internal state and three user methods and is parametrised by an integer N (default is 1000) that averages a generic statistic over the second N measurements and then reports that average from then onwards while making pseudo-random occasional further measurements (with mean spacing every 1/N ) to check that the mean value has not significantly changed.
The first N measurements are not included in the average to avoid start-up transients.
This gives a performance boost of approximately N times in the overhead of this measurement.
A change by more than one percent and more than 2/N is considered significant and this raises an SC_ERROR or SC_WARNING according to another construction parameter.
Confidence switchers are used as much as possible, both in the POWER3 library and by the user models.
They can record bit transition density counts, latency times and and power and energy units.
Several kinds of report can be generated with the TLM POWER3 library.
These cover power consumption, utilisation and physical layout.The library will automatically add up the power and energy used globally, but further detail on individual sub-systems can also be reported by selecting other points in the hierarchy to trace and passing the associated component as an argument to a power trace function.
Each item traced generates either a fresh set of accounts that include that item and all of its children, or that item alone, or a fresh set of totals for each component (i.e. for that item alone and recursively for all its children separately).
Energy consumption and average power are primarily reported in a plain text file emitted at the end of simulation (or at other user request dump points).
This file can also be written in spreadsheet-friendly SYLK (SYmbolic Link) form.
As mentioned, utilisation, energy and transaction activity reports are available in VCD form.
The VCD generator can also output in a gnuplot-friendly multi-column file format.
The LT (loosely-timed) approach can upset the normal SystemC VCD report format owing to temporal decoupling (events are logged in their actual simulation order rather than their nominal correct order) but our VCD writer corrects this by writing the events to a circular RAM buffer whose temporal extent is greater than the LT quantum.
This also enables sensible energy plots to be made: energy events would be like Dirac pulses if rendered directly and cumulative energy plots are not especially informative, but our VCD writer implements a single-pole low-pass filter for the energy events so that they appear like exponentially-decaying pulses.
Alternatively it reports the flat average power given by the last energy quantum divided by the time to the next-logged quantum on that account.Physical layout is currently printed as a text file which just reports which components are inside which others and the resulting area for each component.
A graphical plot in .
svg form is being implemented.
We examined the performance of the first two testbench programs in the Splash-2 suite [14].
These are a radix sort and a L/U matrix decomposition that can run on 1 to 16 cores.
We compiled the Splash-2 programs to run bare metal supported by the standard linux libc and our own implementation of pthreads and a Simics/ANL (parmacs) shim layer [5].
Our testbench uses four OpenRISC processor cores [11] in verilated or fast ISS forms wrapped to use TLM 2.0 blocking calls served by an un-cached instance of DRAMSIM2 [9].
The cores log 250 pJ per instruction and run at 200 MHz unless paused waiting for other cores (50mW core power).
Each core has separate I and D L1 caches that included 17 RAMs each (tag and data for 8-way set associative and a write buffer).
Each Core, Cache and each of the other components shown in Fig 1 is a seprately annotated SystemC module that also inherits a TLM POWER3 base and communication between them is completely with blocking TLM 2.0 calls.
There are sixteen SystemC modules in three levels of hierarchy.
The individual RAMs had dimensions and power consumption computed according to the equations in Tab I which were formed from our own regression of 45nm CACTI runs [12].
.
about ten-to-one owing to SystemC kernel overhead (gprof reveals major costs (more than 20 percent of execution time) are in sc_core::sc_simcontext::crunch(bool) and b_transport.
This degradation occurs with and without the inclusion of caches but the performance of the modelled system does change as expected (ie.
program completes much faster with caches).
Using a maximal LT quantum so that the SystemC kernel is only entered during bus and mutex contention restores some of the performance.
Turning on our power library with its hamming distance computations, when N = 1000 for the confidence switcher causes a further 50 percent degradation in throughput.
This can perhaps be improved upon, but it is not overly bad.
Interestingly, the degradation was much worse in an early version where the island voltage was squared at every use rather than recomputing the transaction energies just on each PVT change.
Fig. 4 shows the measured power consumption of the processor (excluding DRAM) on a real Linux workstation as three idential runs of the RADIX benchmark were executed, the third one using only one CPU core.
A 0.05 ohm resistor was put in series with the 12 volt supply to the processor and its voltage drop and output voltage were logged at 60 Hz to record the energy consumption.
Fig. 5 shows the total power plot when the same C program was run on the SystemC model.
Some differences in general shape are obvious and need investigation.
Our framework provides an easy-to-use power estimation add on to SystemC TLM modelling.
The use of the confidence switcher to dynamically disable detailed modelling is novel.
The user may easily alter the system structure in radical ways, changing cache size, bus layout and so on.
Standard ELF binaries can be easily generated with GCC/binutils tool chain.
We also have a MIPS64 SMP system based around the same components.
Because wiring power is becoming a dominant aspect it is important to include it in rapid exploration tools.The benefit of rapidly exploring design options using SystemC was advocated in [1], but having performance predictions without power predictions is no longer acceptable.
A fairly-detailed TLM model with power annotation was constructed by [2] for a PowerPC-based SoC.
The activity for individual test transactions was extracted from VCD files and entered into a database.
This approach can be applied in our framework to generate the individual transaction energies.
Power estimation is also being performed for AMS (analog and mixed signal) subsystems within the SystemC framework [7].
We plan to further refine our API and library and release it for download.
Including the log_hop operations inside the convenience sockets would be sensible.
Also, further support for power islands might be needed, but currently we can use our chip number concept with zero volt supply setting to disable static power in regions.
Further work will be to integrate back-annotation flows from real layouts and compare these with the Rentian approach.
We would also like to extract net-level activity from the Verilated models to gain additional insight and confidence.We thank Matthieu Moy for providing the TLM POWER2 base platform.
