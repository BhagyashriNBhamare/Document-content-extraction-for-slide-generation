A heterogeneous information network is an information network composed of multiple types of objects.
Clustering on such a network may lead to better understanding of both hidden structures of the network and the individual role played by every object in each cluster.
However, although clustering on homogeneous networks has been studied over decades, clustering on heterogeneous networks has not been addressed until recently.
A recent study proposed a new algorithm, RankClus, for clustering on bi-typed heterogeneous networks.
However, a real-world network may consist of more than two types, and the interactions among multi-typed objects play a key role at disclosing the rich semantics that a network carries.
In this paper, we study clustering of multi-typed heterogeneous networks with a star network schema and propose a novel algorithm, NetClus, that utilizes links across multi-typed objects to generate high-quality net-clusters.
An iterative enhancement method is developed that leads to effective ranking-based clustering in such heterogeneous networks.
Our experiments on DBLP data show that NetClus generates more accurate clustering results than the baseline topic model algorithm PLSA and the recently proposed algorithm , RankClus.
Further, NetClus generates informative clusters, presenting good ranking and cluster membership information for each attribute object in each net-cluster.
Information networks, containing a large number of individual agents or components interacting with each other, are ubiquitous in many applications, e.g., the Internet that consists of a gigantic network of webpages, co-author networks and citation networks extracted from bibliographic data, user networks extracted from email systems, and friendship network extracted from web sites like Facebook 1 and Myspace 2 .
Clustering on an information network based on links between objects may give us a grand view of the huge network.
For example, communities can be detected by clustering on co-author network [11].
Most current studies [20,17,19,21] on information network are on homogeneous networks, i.e., networks consisting of single type of objects, as shown above.
However, in reality, objects could be of multiple types, forming a heterogeneous network.
A recent algorithm RankClus [16] deals with bi-typed heterogeneous networks.
Unfortunately, in reality there often exists more than two types of interacting objects in a network.
Among them, networks with star network schema (called star network) such as bibliographic network centered with papers and tagging network (e.g., http://delicious.com) centered with a tagging event are popular and important.
In fact, any n-nary relation set such as records in a relational database can be mapped into a star network, with each relation as the center object and all attribute entities linking to it.
Example 1.1 (Bibliographic Information Network) A bibliographic network consists of rich information about research papers, each written by a group of authors, using a set of terms, and published in a venue (a conference or a journal).
Such a bibliographic network is composed of four types of objects: authors, venues, terms, and papers.
Links exist between papers and authors by the relation of "write" and "written by", between papers and terms by the relation of "contain" and "contained in", between papers and venues by the relation of "publish" and "published by".
The topological structure of a bibliographic network is shown in the left part of Figure 1, which forms a star network schema, 1 http://www.facebook.com/ 2 http://www.myspace.com/ where paper is a center type and all other types of objects are linked via papers.
One possible way to cluster a heterogeneous network is to first extract from it a set of homogeneous networks and then apply traditional graph clustering algorithms.
However, such an extraction is an information reduction process: some valuable information, e.g., paper title or venue published in, is lost in an extracted co-author network.
Further, although clustering co-author network may discover author communities, a research network contains not only authors, but also venues, terms, and papers.
It is important to preserve such information by directly clustering on heterogeneous networks, which may lead to generating subnetwork clusters carrying rich information.
This motivates us to develop NetClus, a method that discovers net-clusters, i.e., a set of sub-network clusters induced from the original heterogeneous network (Figure 1).
The second weakness of current clustering algorithms is that they do not consider the importance of each object in the network and merely output the cluster label for each object.
As a result, clusters are difficult to understand, especially when the size of clusters are large.
NetClus not only discovers net-clusters but also gives ranking distribution for each type of objects in each cluster, which makes the cluster so discovered quite meaningful, as shown in the following example.
Example 1.2.
(Net-cluster of Database Area) A cluster of the database area consists of a set of database authors, conferences, terms, and papers, and can be obtained by NetClus on the bibliographic network extracted from DBLP dataset 3 .
NetClus also presents rank scores for authors, conferences, and terms in its own type.
With ranking distribution, users can easily grab the important objects in the area.
Table 1 shows the top ranked conferences, authors and terms in the area "database", generated from a 20-conf.
dataset (i.e., a "four-area" dataset) (see details in Sec. 5) using NetClus.Based on the above discussion, in this paper we study the problem of clustering heterogeneous information networks with star network schema and develop a novel clustering algorithm, called NetClus, with the following contributions.1.
A new kind of cluster, net-cluster, is proposed for heterogeneous information networks comprised of multiple types of objects.
In each cluster, statistical information such as the ranking distribution and membership prob-3 http://www.informatik.uni-trier.de/∼ley/db/ ability for each object are derived to facilitate users to navigate in the cluster.
2.
An effective and efficient algorithm, NetClus, is proposed, that detects net-clusters in a star network with arbitrary number of types, builds ranking-based generative model for each net-cluster and adjusts the membership of target objects according to their posterior probabilities in each net-cluster.
3.
Our algorithm is applied to the network extracted from the DBLP dataset, which shows our algorithm can give quite reasonable clustering and ranking results.
The clustering accuracy is much higher than the baseline methods.The rest of paper is organized as follows.
Section 2 is an introduction to related work.
In Section 3, we formally introduce several concepts related to heterogeneous networks and the clustering problem.
In Section 4, we systematically develop the NetClus algorithm.
Section 5 is experiment study and Section 6 concludes this study.
Clustering on networks and graphs has been widely studied in recent years.
Clustering on graphs, often called graph partition, aims at partitioning a given graph into a set of subgraphs based on different criteria, such as minimum cut, min-max cut [4] and normalized cut [14].
Spectral clustering [18] provides an efficient method to get graph partitions which is in fact an NP-hard problem.
Rather than investigate the global structure like spectral clustering, several density-based methods [19,21] are proposed to find clusters in networks which utilizes some neighborhood information for each object.
These methods are all based on the assumption that the network is homogeneous and the adjacent matrix of the network is already defined.SimRank [7] is able to calculate pairwise similarity between objects by links of a given network, which could deal with heterogenous network, such as bipartite network.
However, when the structure of network becomes more complex such as network with star network schema, SimRank cannot give reasonable similarity measures between objects any more.
Also, high time complexity is another issue of SimRank, which prevents it from being applied to large scale networks.An algorithm called RankClus [16] is newly proposed, which uses a ranking-clustering mutually enhancement methodology to cluster one type of objects in the heterogeneous network.
Although the algorithm is efficient comparing to other algorithms that need to calculate pairwise similarity, there are some weaknesses for RankClus: (1) it has not demonstrated the ability to clustering on networks with arbitrary number of types; and (2) the clusters generated by RankClus only contain one type of objects.
In contrast, our algorithm can generate net-clusters comprised of objects from multiple types, given any star network.Other related studies include topic model, such as PLSA [6], which purely uses text information and does not consider link information.
Some works such as author-topic model [15] utilizes additional information other than text by designing complex generative models that include additional types of objects.
Other works such as [10] intend to optimize a combined objective function with both text and graph constraints.
All of these studies are extensions Recently, a different view of clustering on heterogeneous networks [9, 1, 2] appears, which aims at clustering objects from different types simultaneously.
Given different cluster number needed for each type of objects, clusters for each type are generated by maximizing some objective function.
In this paper, net-cluster follows the original network topology and resembles a community that is comprised of multiple types of objects.
In this section, we define the problem of clustering in heterogeneous information networks and introduce several related concepts and necessary notations.
Definition 1.
Information Network.
Given a set of objects from T types X = {Xt} T t=1 , where Xt is a set of objects belonging to t th type, a weighted graph G = 񮽙V, E, W 񮽙 is called an information network on objects X , if V = X , E is a binary relation on V , and W : E → R + is a weight mapping from an edge e ∈ E to a real number w ∈ R + .
Specially, we call such an information network heterogeneous network when T ≥ 2; and homogeneous network when T = 1.
For convenience, we use Xt to denote both the set of objects belonging to the t th type, and the type name.
In the following sections, we will use Wx i x j to denote the weight of an edge 񮽙xi, xj񮽙 in E. V (G), E(G) and W (G) will be denoted as V , E and W of G, if they are not explicitly given.Definition 2.
Star Network Schema.
An information network G = 񮽙V, E, W 񮽙 on T + 1 types of objects X = {Xt} T t=0 is called with star network schema, if ∀e = 񮽙xi, xj񮽙 ∈ E, xi ∈ X0 ∧ xj ∈ Xt(t 񮽙 = 0), or vise versa.
G is then called a star network.
Type X0 is called the center type.
X0 is also called the target type and Xt(t 񮽙 = 0) are called attribute types.In Example 1.1, paper is the center type in the network and other types of objects only have links to the center type.
Many information networks in real applications fall into the class with star network schema.
For example, we can build network for tagging website 4 , where tagging event is the center type, user, webpage, and tag are linked to tagging events.
What's more, each tuple in a database table could be viewed as a center type and each entity attribute in the relation could be viewed as remaining types of objects.
Actually, a center object stands for a co-occurrence of different objects, which is able to catch multi-relation instead of binary relation among different objects.
In this paper, our algorithm is designed on networks with such topology.
4 e.g., http://www.delicious.com/ During clustering, center type objects are the objects first be clustered at each iteration, and links to other types of objects are used to help clustering center objects.
That is why they are called target type and attribute types.Definition 3.
Net-cluster.
Given a network G, a net- cluster C is defined as C = 񮽙G 񮽙 , pC 񮽙, where G 񮽙 is a sub- network of G, i.e., V (G 񮽙 ) ⊆ V (G), E(G 񮽙 ) ⊆ E(G), and ∀e = 񮽙xi, xj񮽙 ∈ E(G 񮽙 ), W (G 񮽙 )x i x j = W (G)x i x j .
Function pC : V (G 񮽙 ) → [0, 1] is defined on V (G 񮽙 ), for all x ∈ V (G 񮽙 ), 0 ≤ pC (x) ≤ 1, which denotes the probability that x belongs to cluster C, i.e., P (x ∈ C).
For convenience, we use V (C) to denote the object set V (G 񮽙 ) in network G 񮽙 and E(C) to denote the edge set E(G 񮽙 ).
Also, for x / ∈ V (G 񮽙 ), we define pC (x) = 0.
In this definition we adopt the idea of soft clustering, which means for each object x ∈ V (C), it can belong to several clusters with some probability pC k (x), k = 1, . . . , K and 񮽙 K k=1 pC k (x) = 1.
Though actually, for target objects x we restrict pC (x) as either 0 or 1, and they can belong to merely one cluster.
In fact, a net-cluster is a sub-network integrating statistical information for objects.
For each net-cluster resembling communities in real world, we argue that it has much simpler structure and can be modeled as a ranking-based generative model.
Therefore, every net-cluster is corresponding to a generative model, according to which generative probabilities of every target object in each cluster can be calculated.
Now, we can formalize our clustering problem as: given a heterogeneous information network G, and cluster number K, find K net-clusters C1, C2, . . . , CK , where񮽙 K k=1 V (C k ) = V (G), 񮽙 K k=1 E(C k ) = E(G), and ∀x ∈ V (G), 񮽙 K k=1pC k (x) = 1, such that target objects within each cluster are nearest to the cluster center under the new K dimensional measure space defined by posterior probabilities.
In this section, we introduce an efficient and effective algorithm, NetClus, which is a ranking-based iterative method.
The major difficulty that lies in clustering in heterogeneous information network is the definition and calculation of similarity between each pair of objects.
The general idea of NetClus is to avoid defining and calculating pairwise similarity between objects but map each target object into a very low dimensional space defined by current clustering result.
Then each target object in these clusters will be readjusted based on the new measure.
During each iteration, clustering results will be improved under new measure space and the quality of measure will be improved since it is derived from better clusters.
Here, we first introduce the general framework of NetClus, and each part of the algorithm will be explained in detail in the following sections.
The general idea of the NetClus algorithm given cluster number K is composed of the following steps:• Step 0: Generate initial partitions for target objects and induce initial net-clusters from the original network according to these partitions, i.e., {C 0 k } K k=1 .
• Step 1: Build ranking-based probabilistic generative model for each net-cluster, i.e., {P (x|C t k )} K k=1 .
• Step 2: Calculate the posterior probabilities for each target object (p(C t k |x)) and then adjust their cluster assignment according to the new measure defined by the posterior probabilities to each cluster.
• Step 3: Repeat Step 1 and 2 until the cluster does not change significantly, i.e.,{C * k } K k=1 = {C t k } K k=1 = {C t−1 k } K k=1 .
• Step 4: Calculate the posterior probabilities for each attribute object (p(C * k |x)) in each net-cluster.
According to many studies in real networks [5,12], preferential attachment and assortative mixing exist in many real networks, which means an object with a higher degree (i.e., high occurrences) has more probability to be attached with an edge, and in some cases higher occurrence objects are more tend to link to each other.
As in DBLP dataset, 7.64% of the most productive authors publishes 74.2% of all the papers, among which 56.72% papers are published in merely 8.62% of the biggest venues, which means large size conferences and productive authors are intended to co-appear via papers.
We extend the heuristic by using ranking, which denotes the overall importance of an object in a network, instead of degree.
The intuition is that degree may not represent global importance of an object well.
Examples include: webpage spammed by many low rank webpages linking to it (high-degree but low rank) will not have too much chance to get a link from a real important webpage, and authors publishing many papers in junk conferences will not increase his/her chance to publish a paper in highly ranked conferences.
Under this observation, we simplify the network structure by proposing a probabilistic generative model for target objects, where a set of highly ranked attribute objects are more likely to co-appear to generate a center object.
To explain this idea, we take bibliographic information network as a concrete example and show how the model works.
Bibliographic information network as illustrated in Example 1.1 is formalized as follows.
• Bibliographic Information Network: G = 񮽙V, E, W 񮽙.
• Nodes in G: V .
In bibliographic network, V is composed of four types of objects: author set denoted as A, conference set as C, term set as T , and paper set as D. Suppose the number of distinct objects in each type are |A|, |C|, |T |, and |D| respectively, objects in each type are denoted asA = {a 1 , a 2 , . . . , a |A| }, C = {c 1 , c 2 , . . . , c |C| }, T = {t 1 , t 2 , . . . , t |T | } and D = {d 1 , d 2 , . . . , d |D| }.
V is the union of all the objects in all types:V = A ∪ C ∪ T ∪ D.• Edges in G: E and W .
In bibliographic network, each paper is written by several authors, published in one conference, and contains several terms in the title.
Titles of papers are treated as a bag of terms, in which, the order of terms is unimportant but the number of occurrence of terms is.
Therefore, for each paper d i , i = 1, 2, . . . , |D|, it has three kinds of links, going to three types of attribute objects respectively.
For two objects from two arbitrary types, x i and x j , if there is a link between them, then edge 񮽙x i , x j 񮽙 ∈ E. Notice that the graph we consider here is an undirected graph.
Also, we use wx i x j to denote the weight of the link of edge 񮽙x i , x j 񮽙, which is defined as follows:wx i x j = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 1, if x i (x j ) ∈ A ∪ C and x j (x i ) ∈ D,and x i has link to x j c, if x i (x j ) ∈ T and x j (x i ) ∈ D and x i (x j ) appears c times in x j (x i ), 0, otherwise.In order to simplify the complex network with multiple types of objects, we try to factorize the impact of different types of attribute objects and then model the generative behavior of target objects.
The idea of factorizing a network is: we assume that given a network G, the probability to visit objects from different attribute types are independent to each other.
Still, the probability to visit an attribute object in G, say author ai, p(ai|G) can be decomposed into two parts:p(ai|G) = p(A|G) × p(ai|A, G), where the first part p(A|G) is the overall probability that type of author will be visited in G, and the second part p(ai|A, G) is the probability that an object ai will be visited among all the authors in the network G. Generally, given an attribute object x and its type Tx, the probability to visit x in G is defined as in Eq.
(1):p(x|G) = p(Tx|G) × p(x|Tx, G)( 1 )In practice, p(Tx|G) can be estimated by the proportion of objects in Tx compared with the whole attribute object set 񮽙 Tx for all attribute types.
Later we will show that the value of p(Tx|G) is not important and can be set to 1.
How to generate ranking distribution p(x|Tx, G) for type Tx in a given network G will be addressed in Section 4.4.
Also, we make another independence assumption that within the same type of objects, the probability to visit two different objects is independent to each other:p(xi, xj|Tx, G) = p(xi|Tx, G) × p(xj|Tx, G)where xi, xj ∈ Tx and Tx is some attribute type.
Now, we build the generative model for target objects given the ranking distributions of attribute objects in the network G. Still using bibliographic network as an example, each paper di is written by several authors, published in one conference, and comprised of a bag of terms in the title.
Therefore, a paper di is determined by several attribute objects, say xi1, xi2, . . . , xin i , where ni is the number of links di has.
The probability to generate a paper di is equivalent to generating these attribute objects with the occurrence number indicated by the weight of the edge.
Under the independency assumptions that we have made, the probability to generate a paper di in the network G is defined as follows:p(di|G) = 񮽙 x∈N G (d i ) p(x|G) W d i ,x = 񮽙 x∈N G (d i ) p(x|Tx, G) W d i ,x p(Tx|G) W d i ,xwhere NG(di) is the neighborhood of object di in network G, and Tx is used to denote the type of object x. Intuitively, a paper is generated in a cluster with high probability, if the conference it is published in, authors writing this paper and terms appeared in the title all have high probability in that cluster.
Once we get the generative model for each net-cluster, we can calculate posterior probabilities for each target object.
Now the problem becomes that suppose we know the generative probabilities for each target object generated from each cluster k, k = 1, 2, . . . , K, what is the posterior probability that it is generated from cluster k?
Here, K is the cluster number given by user.
As some target objects may not belong to any of K net-cluster, we will calculate K + 1 posterior probabilities for each target object instead of K, where the first K posterior probabilities are calculated for each real existing net-clusters C1, C2, . . . , CK , and the last one in fact is calculated for the original network G. Now, the generative model for target objects in G plays a role as background model, and target objects that are not very related to any clusters will have high posterior probability in background model.
In this section, we will introduce the method to calculate posterior probabilities for both target objects and attribute objects.According to the generative model for target objects, the generative probability for a target object d in the target type D in a sub-network G k = G(C k ) can be calculated according to the conditional rankings of attribute types in that sub-network:p(d|G k ) = 񮽙 x∈N G k (d) p(x|Tx, G k ) W d,x p(Tx|G k ) W d,x(2)where NG k (d) denotes for the neighborhood of object d in sub-network G k .
In Eq.
(2), in order to avoid zero probabilities in conditional rankings, each conditional ranking should be smoothed using global ranking with smoothing parameter λS, before calculating posterior probabilities for target objects:PS(X|TX , G k ) = (1 − λS)P (X|TX , G k ) + λSP (X|TX , G)where λS is a parameter that denotes how much we should utilize the ranking distribution from global ranking.Smoothing [22] is a well-known technology in information retrieval.
One of the reasons that smoothing is required in the language model is to deal with the zero probability problem for missing terms in a document.
When calculating generative probabilities of target objects using our rankingbased generative model, we meet a similar problem.
For example, for a paper in a given net-cluster, it may link to several objects whose ranking score is zero in that cluster.
However, if we simply assign the probability of the target object as zero in that cluster, we cannot use other informative objects to decide which cluster this target object is more likely belonging to.
In fact, in initial rounds of clustering, objects may be assigned to wrong clusters, if we do not use smoothing technique, they may not have the chance to go back to correct clusters (See the case of λS = 0 in Fig. 4(b)).
Once a clustering is given on the input network G, say C1, C2, . . . , CK , we can calculate the probability for each target object (say paper di) simply by Bayesian rule:p(k|di) ∝ p(di|k) × p(k).
where p(di|k) is the probability that paper di generated from cluster k, and p(k) denotes the relative size of cluster k, i.e., the probability that a paper belongs to cluster k overall.
Here, k = 1, 2, . . . , K, K + 1.
From this formula, we can see that type probability p(T |G) is just a constant for calculating posterior probabilities for target objects and can be neglected.In order to get the potential cluster size p(k) for each cluster k, we choose cluster size p(k) that maximizes loglikelihood to generate the whole collection of papers and then use the EM algorithm to get the local optimum for p(k).
logL = |D| 񮽙 i=1 log(p(di)) = |D| 񮽙 i=1 log[ K+1 񮽙 k=1 p(di|k)p(k)](3)We use the EM algorithm to get p(k) by simply using the following two iterative formulas:p (t) (k|di) ∝ p(di|k)p (t) (k); p (t+1) (k) = |D| 񮽙 i=1 p (t) (k|di)/|D|.
Initially, we can set p (0) (k) = 1 .
When posterior probability is calculated for each target object in each cluster C k together with the parent cluster C, where G(C) = G, each target object d can be represented as a K dimensional vector:񮽙 v(d) = (p(1|d), p(2|d), . . . , p(K|d)).
The center for each cluster C k can be represented using a K dimensional vector as well, which is the mean vector of all the target objects belonging to the cluster under the new measure.
Next, we calculate cosine similarity between each target object and each center of cluster, and assign the target object into the cluster with the nearest center.
A new sub-network G k can be induced by current target objects belonging to cluster k. Following the Net-Cluster definition (Definition 3), pC k (d) = 1 if object d is assigned to cluster C k , 0 otherwise.
The adjustment is an iterative process, until target objects do not change their cluster label significantly under the current measure.
Notice that, when measuring target objects, we do not use the posterior probability for background model.
We make such choices with two reasons: first, the absolute value of posterior probability for background model should not affect the similarity between target objects; second, the sum of the first K posterior probabilities reflects the importance of an object in determining the cluster center.The posterior probabilities for attribute objects x ∈ A ∪ C ∪ T can be calculated as follows:p(k|x) = 񮽙 d∈N G (x) p(k, d|x) = 񮽙 d∈N G (x) p(k|d)p(d|x) = 񮽙 d∈N G (x) p(k|d) 1 |NG(x)|It simply says, the probability of a conference belonging to cluster C k equals to the average posterior probability of papers published in the conference, which is similar for authors.
And pC k (x) in Net-Cluster definition is set to p(k|x).
In Table 2, we select four objects from four types in the DBLP "four-area" dataset to show their posterior probabilities, in four net-clusters and a background model, changing along iterations.
Initially, net-clusters are generated from random partitions of papers, each of which is very similar to the original network.
Therefore, conditional ranking distributions of each type in each cluster are also very similar to the original ones (background).
Thus, posterior proba- 5 .
However, as similar papers under new measure given by posteriors are grouped together, net-clusters in each area become more and more distinct and objects are gradually assigned with a high posterior probability in the cluster that they should belong to.
Definition 4.
Ranking Distribution and Ranking Function.
A ranking distribution P (X) on a type of objects X is a discrete probability distribution, which satisfies P (X = x) ≥ 0 (∀x ∈ X) and 񮽙 x∈X P (X = x) = 1.
A function fX : G → P (X) defined on an information network G is called a ranking function on type X, if given an information network G, it can output a ranking distribution P (X) on X.Ranking is usually used to evaluate the importance or relevance of objects in a collection.
For example, PageRank [3] and authority of HITS [8] stand for the static importance of webpages, while the rank of a document to a given query in text retrieval reflects the relevance of the document to that query.
Here, we use ranking distribution to represent the importance or visibility of objects within their own type in a given information network G.
The higher the rank is, the more possible an object will be visited.Ranking distributions are quite distinct from each other among different clusters.
For example, in computer science area, the ranking distribution of authors from the database area and the system area should be rather different.
In the best case, ranking distributions should be orthogonal to each other in different clusters.
As we illustrated in Section 4.2, within each cluster, by making independency assumptions between different objects, ranking distributions for each type can be used to build generative models for target objects.We now introduce two ranking functions using the bibliographic network as an example, and also give some properties of the two ranking functions for a simple 3-typed star network.
Simple ranking is namely the simple occurrence counting for each object normalized in its own type.
Given a network G, ranking distribution for each attribute type of objects is defined as follows:p(x|Tx, G) = 񮽙 y∈N G (x) Wxy 񮽙 x 񮽙 ∈Tx 񮽙 y∈N G (x 񮽙 ) W x 񮽙 y (4)where x is an object from type Tx.
For example, in bibliographic network, the rank score for a conference using simple ranking will be proportional to the number of its accepted papers.
5 Initial absolute posterior prob.
to background is sensitive to prior λP : the higher λP , the larger the value.
However, final posterior prob.
is not significantly affected by λP .
From the above simple case of network, intuitively for general star network, if a type of attribute objects has a small mutual information with other types of attribute objects, simple ranking is good for it.
For example, term type in bibliographic network has small mutual information with authors and conferences in the scale of computer science and database and information system area, and thus could use simple ranking.
Authority ranking for each object is a ranking function that considers the authority propagation of objects in the network, thus will represent more of the visibility over the whole network.
For a general star network G, the propagation of authority score from Type X to Type Y through the center type Z is defined as:P (Y |TY , G) = WY Z WZXP (X|TX , G)(5)where WTZ and WZX are the weight matrices between the two types of objects as indexed, and can be normalized when necessary.
Generally, authority score of one type of objects could be a combination of scores from different types of objects, e.g., that proposed in [13].
It turns out that the iteration method of calculating ranking distribution is the power method to calculate the primary eigenvector of a square matrix denoting the strength between pairs of objects in that certain type, which can be achieved by selecting a walking path (or a combination of multiple paths) in the network.Property 2.
Given a three-typed network with star net-work schema G = 񮽙X 񮽙 Y 񮽙 Z, E, W 񮽙,where Z is the center type, and ∀z, NG(z) = {x, y}(x ∈ X, y ∈ Y ), authority ranking P (X) and P (Y ) are calculated through Equation5 iteratively, then estimated joint distributionˆPdistributionˆ distributionˆP (X, Y ) = {ˆp{ˆp(x, y) = P (X = x)P (Y = y), x ∈ X, y ∈ Y } equals to the joint distribution represented by one rank matrix M ||M || 1 , such that ||WXZWZY − M ||F is minimized.Proof.
Let USV T = WXZ WZY be SVD of WXZ WZY , and U1 and V1 be the first columns of U and V corresponding to the largest singular value σ1, according to Eckart-Young theorem, M = σ1U1VT 1 = miñ M ||WXZ WZY − ˜ M ||,where rank( ˜ M ) = 1.
According to the authority ranking, P (X) = U1/||U1||1 and P (Y ) = V1/||V1||1,thus M/||M ||1 = σ 1 U 1 V T 1 ||σ 1 U 1 V T 1 || 1 = P (X)P (Y ) T ,where ||M ||1 is entrywise 1-norm of M .
Enlightened by this property holding for the simple network, we can have an intuition that authority ranking is Table 3: NMI between Attribute Types in Different Scale of DBLP network able to catch the largest component structure of a network under the constraints that the relation between objects are recovered by 1-dimensional ranking.
As a result, authority ranking should have better performance than simple ranking in most cases.
In the DBLP dataset, according to the rules that (1) highly ranked conferences accept many good papers published by many highly ranked authors and (2) highly ranked authors publish many good papers in highly ranked conferences, we determine the iteration equation as:C&A C&T A&T Level 1 0.4564 0.1389 0.2229 LevelP (C|TC , G) = WCDD −1 DA WDAP (A|TAG) P (A|TA, G) = WADD −1 DC WDC P (C|TC , G) (6)where DDA and DDC are the diagonal matrices with the diagonal value equaling to row sum of WDA and WDC.
Since all these matrices are sparse, in practice, the rank scores of objects need only be calculated iteratively according to their limited neighbors.
Table 3 in different scales of networks, namely the whole computer science network (level 1), the database and information system network (level 2), and the database network (level 3).
Top 1000 objects by occurrence frequency are used in the calculation.
If a type has low NMI with all other types, simple ranking is recommended; otherwise, authority ranking is used among types with high NMI.In both ranking functions, prior distributions for a certain type in different clusters can be integrated.
Priors for a given type X are given in the form PP (X|TX , k), k = 1, 2, . . . , K.
An User may give only a few representative objects to serve as priors, like terms and conferences in bibliographic data.
First, the prior is propagated in the network in a PageRank way, to propagate scores to objects that are not given in the priors.
Then, the propagated prior is linear combined with the ranking functions with parameter λP ∈ [0, 1]: the bigger the value, the more the final conditional ranking is dependent on prior.
Time complexity of NetClus is composed of the following parts.
First, computational complexity for global ranking for attribute objects is O(t1|E|) and that for global probability calculation for target objects is O(|E|), where |E| is the number of edges in network G and t1 is the iteration number for ranking.
For ranking, at each iteration, each link will be calculated once; and for global probability calculation, a link is still calculated once.
Second, time complexity for conditional ranking for attribute objects is O(t1|E k |), and for conditional probability for target objects is O(|E k |) in each cluster k.
When adding them together, for all sub-clusters, time complexity for one iteration of clustering should be O(t1|E| + |E|).
Third, time complexity for calculating posterior probability for each target object is O(t2(K + 1)N ), where N is the number of target objects, and t2 is the max Table 5: Top-5 Authors in "XML" Net-cluster iteration number in the EM algorithm.
Fourth, cluster adjustment for each target object is O(K 2 N ).
Since for each target object, it has a K dimensional measure, and we have to calculate similarity to K clusters' centers, which are also K-d.
Fifth, time complexity for posterior probability for each attribute object is O(K|E|).
For each attribute object, each link to target object should be used once to calculate the posterior probability for it.
Also, for each attribute type, we have to calculate a K-d measure.In all, the time complexity for NetClus is O((t1 + 1)|E| + t3((t1 + 1)|E| + t2(K + 1)N + K 2 N ) + K|E|), where t3 is max iteration number used for clustering adjustment, which can be summarized as O(c1|E| + c2N ).
When the network is very sparse, which is a real situation in most applications, the time complexity is almost linear to the objects in the network.
We now study the effectiveness and accuracy of NetClus and compare it with state-of-the-art algorithms.
We use real data set from DBLP and build bibliographic networks according to Example 1.1.
Two networks with different scales will be studied.
First, a big data set ("allarea" data set) covers all the conferences, authors, papers and terms from DBLP will be used.
Second, we also extract a small data set ("four-area" data set) which contains four areas that are most related to data mining, which are database, data mining, information retrieval and machine learning.
Five representative conferences for each area are picked, and all authors have ever published papers on any of the 20 conferences, all these papers and terms appeared in these titles are included in the network.
By using the smaller data set, we want to compare the clustering accuracy with several other methods.
Also, parameter study and ranking function study will be carried on based on the "four-area" data set.
We first show the ranking distributions in net-clusters we discovered using the "all-area" data set, which is generated by using authority ranking for conferences and authors, setting conference type as priors, and setting the cluster number as 8.
We show three net-clusters in Table 4.
Also, we can recursively apply NetClus to sub-networks derived from clusters and discover finer net-clusters.
Top-5 authors in a finer net-cluster about XML area, which is derived from database sub-network, are shown in Table 5.
In Section 4.4, we proposed two ranking functions, namely simple ranking and authority ranking.
Here, we study how low dimensional measure derived from ranking distributions improve clustering and how clustering can improve this new measure in turn (Figure 2).
Here, term is fixed to use simple ranking, and conference and author are set to use either authority ranking or simple ranking as two different settings.First, in order to measure how dissimilar conditional ranking distributions are among different clusters, we calculate average KL divergence, which is denoted as avgDKL(X), between each conditional ranking and global ranking for each attribute type X and trace the change of this measure during iterations of clustering.
avgKL(X) is defined as:avgDKL(X) = 1 K K 񮽙 k=1 DKL(P (X|TX , G k )||P (X|TX , G))Second, in order to measure the goodness of measure generated in each round of clustering, we use the compactness, C f , of target objects under each round of clustering for ranking function f , which is defined as the average ratio between within-cluster similarity and between-cluster similarity using the new measure:C f = 1 |D| K 񮽙 k=1 |D k | 񮽙 i=1 s(d ki , c k ) 񮽙 k 񮽙 񮽙 =k s(d ki , c k 񮽙 )/(K − 1) .
Third, we trace the accuracy of clustering results for target objects in each round of iteration, which is defined as:accuracy = 1 |D| D 񮽙 i=1 Ptrue(·|di) · P (·|di)However, since |D| is very large even in four-area data set, we manually randomly labeled 100 papers into four clusters and use this paper set to calculate the accuracy.
Fourth, at each iteration of clustering, we calculate the posterior probability for each paper by maximizing the loglikelihood of the whole collection.
Here, we also trace the log-likelihood logL along with the clustering iterations, which is defined in Equation 3.
From Figure 2, we can see authority ranking is better in every measure than simple ranking.As we know, in K-means like algorithm, the clustering results are sensitive to initial clustering.
We kept 30 times running records and mapped the relation between observable measure of log-likelihood (and compactness) and accuracy into Figure 3 to guide user to pick the best clustering results among several runnings with different initialization.
From Figure 3, we can see that linear relation exists among the two measures and accuracy.
Also, majority voting among different runnings can be used.
In our algorithm, there are two parameters: prior parameter (λP ) and smoothing parameter setting (λS).
We use clustering accuracy for sampled papers to test the impact of different settings of parameters to the algorithm.
By fixing one of them, we vary the other one.
From Figure 4(a) and 4(b), we find that the larger the prior parameter λP , the better the results, while when λP > 0.4, the impact becomes more stable 6 ; also, the impact of smoothing parameter is 6 Actually, the extremely poor quality when λP is very small very stable, unless it is not too small (less than 0.1) or too big (bigger than 0.8).
The results are based on 20 runnings.
Priors given for each of the four areas are around 2 or 3 terms.
For example, "database" and "system" are priors for database area, with uniform prior distribution.
In this section, we compare our algorithm with two other algorithms.
Since all of them cannot directly applied to heterogeneous network clustering with four types of objects, for each algorithm, we will simplify the network when necessary is partially caused by the improper accuracy measure at those occasions.
When the prior is not big enough to attract the papers from the correct cluster, the clusters generated not necessary have the same cluster label with the priors.to make all the algorithms comparable.
For PLSA [23], only the term type and paper type in the network are used.
Notice that we use the same term prior in both NetClus and PLSA.
The accuracy results for papers are in Table 6 Since RankClus can only cluster conferences, we choose to measure the accuracy of conference cluster.
For NetClus, cluster label is obtained according to the largest posterior probability, and NMI [16] is used to measure the accuracy.
The results are shown in Table 7, where d(a) > n means we select authors that have more than n publications.
Since majority authors only publish a few papers, which contains little information for disclosure of the relationship between two conferences and misleads the algorithm, we run RankClus algorithm by setting different thresholds to select subsets of authors.
All the results are based on 20 runnings.
In this paper, we address a new clustering problem to detect net-clusters on a special heterogeneous network with star network schema, which aims at splitting the original network into K layers and differs the concept from current clustering methods on heterogeneous networks.
A novel ranking-based algorithm called NetClus is proposed to find these clusters.
The algorithm makes assumption that within each net-cluster, target objects (i.e., objects from the center type) are generated by a ranking-based probabilistic generative model.
Each target object is then mapped into a new low dimensional measure by calculating their posterior probabilities belonging to each net-cluster through their generative models.
Our experiments on DBLP data show that NetClus generates more accurate clustering results than the baseline algorithms extended from the topic model and a previous ranking-based algorithm RankClus.
Further, NetClus generates more informative clusters, presenting good ranking information and cluster membership for each attribute object in each net-cluster.
In future, we will study how we can automatically set the number of cluster, by which hierarchy tree with arbitrary structure can be detected.
Another issue relates to the subspace selection for attribute objects at different scales, which is critical to efficiently and effectively clustering in any complex network.
