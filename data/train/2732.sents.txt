Deep learning is a popular technique for building models from large quantities of input data for applications in many domains.
With the proliferation of edge devices such as sensor and mobile devices, large volumes of data are generated at rapid pace all over the world.
Migrating large amounts of data into centralized data center(s) over WAN environments is often infeasible due to cost, performance or privacy reasons.
Moreover, there is an increasing need for incremental or online deep learning over newly generated data in real-time.
These trends require rethinking of the traditional training approach to deep learning.
To handle the computation on distributed input data, micro-clouds-small-scale clouds deployed near edge devices in many different locations-provide an attractive alternative for data locality reasons.
However , existing distributed deep learning systems do not support training in micro-clouds, due to the unique characteristics and challenges in this environment.
In this paper, we examine the key challenges of deep learning in micro-clouds: computation and network resource het-erogeneity at inter-and intra micro-cloud levels and their scale.
We present DLion, a decentralized distributed deep learning system for such environments.
It employs techniques specifically designed to address the above challenges to reduce training time, enhance model accuracy, and provide system scalability.
We have implemented a prototype of DLion in TensorFlow and our preliminary experiments show promising results towards achieving accurate and efficient distributed deep learning in micro-clouds.
Micro-clouds [4, 11-13, 42, 44, 47] are an emerging type of infrastructure to support the exponentially growing large amounts of data generated by edge devices such as surveillance cameras [21,34], mobile phones [33,35,58], or various sensors [32,42,53].
Individual micro-clouds consist of a small or medium number of servers.
Instead of storing and maintaining the huge amounts of data in a few monolithic public clouds, users and organizations can use micro-clouds across multiple locations, deployed close to edge devices for providing faster services with minimum latency.Deep learning (DL) is a popular technique to build models from large quantities of input data for applications in many domains [3,7,14,26,48,56].
Traditionally, DL models are trained on large quantities of data assembled in cluster or data center environments.
With recent advances in deep learning techniques, continuously generated data could be used for onlinelearning or incremental-learning [5,27,36,37,39,41,55].
Thus, instead of being a one-time training solution for a fixed set of training data, DL models could keep evolving using data continuously generated from a large number of edge devices across the globe.
However, migrating such large amounts of data into centralized cloud(s) over WAN environments for training is likely to be prohibitive due to cost, performance or privacy reasons.
For instance, such data is hard to move because of WAN bandwidth constraints, or because it could contain a lot of personal information such as pictures or videos generated by user devices or recorded using surveillance cameras as shown in Figure 1.
The need for geodistributed data analysis has also been shown for many other analytics tasks [17,20,25,38,49,50].
An attractive alternative is to carry out distributed deep learning across the micro-clouds, since they often provide (limited) computation and storage capabilities.
Recently, there has been growing interest in using the edge for DL inference [2,22,30,52], where models trained in the cloud are deployed at the edge for faster inference.
In this paper, we argue for the use of micro-cloud environments for DL training to efficiently build DL models in-situ and to support online and incremental learning.There are three major challenges of distributed deep learning in micro-clouds.
1.
Compute resource heterogeneity.
Different micro-clouds can have different number of servers equipped with different performance hardware.
In addition, servers in the microclouds can be shared by other applications, so the available compute capacity may dynamically change.
2.
Network resource heterogeneity.
Servers in a microcloud communicate with each other over LAN, whereas servers in different micro-clouds are connected via WAN.
Network capacities in LANs may vary due to network resource contention with other applications, while bandwidths in WANs are much more scare and fluctuating than in LANs.
3.
Scale.
A micro-cloud covers much smaller area than a public cloud, but the number of micro-clouds is much bigger than the number of data centers in a multi-DC public cloud.
The micro-clouds are also likely to be much more geographically distributed, leading to heavy communication over WANs.Existing distributed deep learning systems, however, do not fully address these challenges.
General purpose distributed DL systems like TensorFlow [1], MXNet [6] or CNTK [43] do not consider the heterogeneity or scale, resulting in much longer training time due to network bottleneck issue as cluster size increases.
Recent research [19,31,54] has addressed the network bottleneck issue by reducing the amounts of data transmitted over the network.
As a result, models can be trained faster, potentially at the cost of loss of model accuracy as cluster size increases.
Other recent research [18,23,24] tackles scalability issue by communicating with a small number of peers, but it does not consider network or compute resource heterogeneity.
Thus, none of the existing systems comprehensively considers all the challenges of micro-cloud environments: compute and network heterogeneity and scale.In this paper, we present DLion, a decentralized distributed deep learning system that is designed for deep learning in large-scale heterogeneous environments such as micro-clouds.
The goals of the system are to reduce training time, improve model accuracy, and handle system scalability.
It employs techniques specifically designed to address the above challenges, including compute capacity-aware batching, networkaware data exchange, and selective data propagation ( § 3).
We have implemented a prototype of DLion on top of TensorFlow and present our preliminary results in § 4.
Deep Learning.
We consider supervised learning using minibatch stochastic gradient descent (SGD) [40] to minimize the loss value of the function f over the training dataset x (eq.
1).
Learning: min x∈R n f (x; w) = 1 m m ∑ i=1 f i (x; w t )(1)A deep learning model consists of a set of parameters called weights, and operators.
The meaning of training a DL model is to find the best values for the weights, which lead to the smallest loss value.Gradient Calculation:g t = 1 m m ∑ i=1 w f i (x; w t ) (2)Weight Update: w t+1 = w t − ηg tThe weights are tuned by iterations of gradient g t calculation (eq.
2) and weight w t update (eq.
3) over minibatches.
A minibatch is composed of m training data samples from the training data x and batch size indicates the size of a minibatch.
An iteration indicates a cycle of gradient calculation and weight update over a minibatch.
An epoch indicates a set of iterations trained over one pass of the whole training data.
Batch size and learning rate η are tunable model parameters.Distributed Deep Learning.
Weight update follows eq.
4 in distributed deep learning systems.w t+1 = w t − η 1 k k ∑ j=1 1 m m ∑ i=1 w f i (x; w t )(4)k workers calculate their own gradients locally based on a minibatch size of m in parallel.
Weights are updated based on the average of the k gradients, where the total batch size of the model is m * k.Distributed Deep Learning Systems.
Distributed deep learning systems allow users to train their DL models using a cluster of multiple machines where training data are distributed.
General purpose DL systems [8] [18,23,24,31] such as Ako [54] synchronize models without PSs.
Workers exchange their local gradients with each other, and update their local weights based on the collected gradients.
The workload imposed on PSs can be offloaded to all the workers.
Hybrid distributed DL systems such as Gaia [19] employ the decentralized architecture to exchange gradients between PSs over WANs while learning in a centralized manner in LANs.
While some of the existing systems such as Gaia and Ako have addressed network bottleneck issue by exchanging small amount of gradients or sending full gradients to a subset of peers, none of them comprehensively consider all the challenges in a micro-cloud environment.
We propose DLion, a decentralized distributed deep learning system for learning in micro-clouds.
Figure 2 shows the system architecture of DLion.
The philosophy of decentralized architecture fits well to the heterogeneous environments of micro-clouds.
In DLion, there are no centralized components such as parameter servers.
Workers within a micro-cloud are connected over LANs, and those in different micro-clouds communicate over WANs.
There are three major goals in DLion, which are reducing training time, improving model accuracy, and providing system scalability for deep learning in micro-clouds.
In the rest of the section, we describe the techniques to deal with compute heterogeneity ( § 3.1), network heterogeneity ( § 3.2) and scalability ( § 3.3) encountered in micro-clouds environments.
Details of the experimental setup for our exploratory and preliminary experiments are specified in section 4.
We first describe how DLion makes use of heterogeneous compute resources in micro-clouds to reduce training time and adaptively increase model accuracy through consideration of available compute resources.The idea of compute capacity-aware batching is to have different batch sizes assigned to workers based on their computation capacities.
For faster learning, DLion maximizes data parallelism by assigning a batch size m j to worker j proportional to its compute capacity C j , so its time to process a minibatch,T j = m j C j, is close to an expected global unit processing time T unit , thus balancing the load proportionally across workers.
This approach could result in very large batch sizes being assigned to fast workers.
However, it has been shown that accuracy in large-batch training can degrade drastically beyond a certain batch size [28].
We observed this phenomenon in our experiment shown in Figure 3 where the accuracy drops between total batch size of 640 and 960.
To avoid this accuracy degradation, DLion makes sure the total batch size across workers ∑ k j=1 m j does not exceed a certain threshold T bs beyond which accuracy can drop.
DLion measures the computation power of each worker through preprofiling before training, and selects different batch sizes for individual workers accordingly.
The new weight update equation based on compute capacity-aware batching (eq.
5) adds two additional constraints as follow:w t+1 = w t − η 1 k k ∑ j=1 1 m j m j ∑ i=1 w f i (x; w t )(5)subject to |T j − T unit | ≤ ε and ∑ k j=1 m j ≤ T bs .
Adaptive model parameter tuning.
On top of compute capacity-aware batching, DLion applies techniques related to large-batch training for better model optimization such as: warm-up learning rate [15,57] increasing learning rate from η to η * k (# workers) early in the learning phase (WarmUpLR) increasing batch size [10,28] late in learning (SpeedUpBS) or throughout learning (IncreaseBS) decaying learning rate [46] late in learning (DecayLR).
DLion adaptively adjusts the DL model parameters, learning rate and batch size.
There is no comprehensive and integrated existing solution for applying these techniques.
Our system automatically applies them by determining when, how, and what to apply the techniques with comprehensive consideration of heterogeneous computation capacities of workers, batch size threshold T bs and learning progress.
For example, we make sure the total batch size is not greater than the threshold T bs while increasing IncreaseBS or speeding up batch size SpeedUpBS.
The threshold T bs is approximately 10 percents of training data size [46].
We performed exploratory experiments to see the effect of the techniques.
We trained a model stated in § 4 with different combinations of the techniques for 10 epochs.
Table 1 shows that the best combination WarmUpLR + SpeedUpBS + DecayLR results in faster training time and higher accuracy compared to other baselines that either do not use these techniques, or use them individually or pairwise combinations.
In this section, we describe how DLion deals with heterogeneous network bandwidth resources in micro-clouds to improve model accuracy and achieve faster training times.
Figure 4: When-to-do weight exchange (WE) for model synchronization; Weights are exchanged every 10 iterations during whole training (Periodic), first 2 epochs (Early), and last 2 epochs (Late).
WE early in training achieves comparable result with periodic WE.
Partial gradient exchange (PartialGrads) reduces training time for all cases.
A model is trained until it reaches 60% accuracy.The distributed DL model synchronization happens by exchanging data between workers during the training phase from-time-to-time.
There are two types of data, gradients and weights, that workers can exchange for model synchronization.
Existing decentralized systems like Ako and Gaia exchange only gradients, which can result in longer training time and accuracy drop as cluster size increases.
Recent work [51] has proposed a periodic weight exchange algorithm to compensate for drop in accuracy.DLion employs this direct model synchronization across workers though weight exchange (WE) in addition to gradient exchange (GE).
The key idea of network-aware data exchange is to adjust data size by controlling the quality of data and considering the available network bandwidth of individual workers in real-time.
We explore several decisions to understand factors contributing to the data dissemination, as follows.
When-to-do: when to exchange data, e.g., more frequently early or late in training, or periodically; what-todo: whether to exchange whole or partial data; whom-to-do: whether to exchange data with all workers or a subset of workers; and how-to-do: whether to exchange data synchronously or asynchronously.DLion uses a system parameter to control the contribution to the model update when adjusting data size, especially for gradient exchange (GE).
DLion increases the contribution threshold to reduce the size of partial gradients, but still to convey the important information by partial gradients.
When workers need to send gradients over WANs, the threshold is set higher, compared to sending them over LANs.
Figure 4 and Figure 5 show exploratory results for the four decisions regarding weight and gradient exchanges.
Figure 4 shows that frequent WE early in learning has a comparable performance with periodic WE with high frequency.
Also, partial GE helps to reduce the training time, compared to full GE.
DLion uses these insights to allocate more resources for model synchronization at the beginning of training phase, and concentrate more on gradient exchange later in the training phase.
In addition, Figure 5 shows the results regarding what-to-do, whom-to-do, and how-to-do.
Model synchronization by WE (MS) benefits every case except for partial WE (PartialMS).
In addition, exchanging weights only to the Figure 5: What-to-do, whom-to-do, and how-to-do model synchronization.
Partial weight exchange does not help to improve accuracy.
Rather, full weight exchange is much more effective in model optimization.
A model is trained with partial gradient exchange and periodic WE for 30 minutes.worst worker having the highest loss value (One) and asynchronous WE (Asynch) leads to higher accuracy as it was able to more iteration for a given training time because of small amount data exchange.
We next discuss how DLion can handle system scalability when learning over large number of micro-clouds over WANs.
We explicitly consider the compute and network heterogeneity in our scalability approach as well.In DLion, at each iteration, a subset of micro-clouds (senders) send their gradients to a subset of micro-clouds (receivers), instead of an all-to-all communication, where each micro-cloud would broadcast gradients to all micro-clouds.
We use a probabilistic model to select senders and receivers based on their compute capacities.
Since micro-clouds with higher capacities are more likely to generate more informative gradients over larger minibatches, they have a higher probability of being selected as senders.
Similarly, micro-clouds with lower capacities are more likely to generate less informative gradients over smaller minibatches, so they are likely selected as receivers.After the sender and receiver selection, we employ our network-aware data exchange techniques ( § 3.2) to reduce the data size.
Moreover, the workload of the gradients delivery from a sender micro-cloud to a receiver micro-cloud is offloaded to all the workers in each location, to avoid overloading only a single worker at each location.
Finally, we are also considering gossipping algorithms to more efficiently disseminate the data through the network.
Implementation.
We are implementing a prototype of DLion on TensorFlow.
Each worker trains DL models by using TensorFlow.
Messages are delivered via Redis, an in-memory data store, as a message broker for workers in DLion.
Redis provides persistence that is necessary for dynamic cluster configurations such as fault tolerance, worker join or leave in the future.
Currently, TensorFlow provides a static cluster configuration where it is hard to deal with the cluster dynamics.
Experimental setup.
We compared DLion with Gaia and Ako implemented in our prototype, and emulated micro-cloud environments using 4 local servers by using the linux tc and stress commands to throttle network bandwidth, and impose load on servers, respectively.
Network links are set to 1Gbps for LANs, and 100 Mbps for WANs.
High-performance servers have 24 cpus and low-performance servers have 8 cpus per server.
All servers have 60GB available memory and run on Ubuntu 16.04 installed with TensorFlow 1.4.1.
A worker runs in a server.
Dataset CIFAR10 [29] and a test model (2conv+2fc, model size is 17MB) are used for the purpose of preliminary experiments.
The test model with CIFAR10 is converged after 10-epoch training, and takes around 30 minutes in LAN, so we use those two as training termination conditions.
We use training time and accuracy as metrics to measure system performance and model optimization.
We evaluate the usefulness of the compute capacity-aware batching feature and compare DLion with existing distributed deep learning systems, Gaia and Ako, in compute resource heterogeneous environments.
We set up a cluster where there are three low-performance workers and one high-performance worker, and network resources are homogeneous.
Figure 6 shows the training time until reaching a target accuracy for the various systems.
We use two different accuracy targets (60% accuracy for Figure 6a and 70% for Figure 6b) as Gaia and Ako were unable to reach 70% accuracy in our experiments.
For Gaia, workers exchange significant partial gradients, and for Ako, workers exchange partitioned partial gradients at each iteration.
Figure 6a shows the comparison results where DLion is able to reach the target accuracy 73% and 74% faster than Gaia and Ako, respectively.
This is because both existing systems are unable to take advantage of compute heterogeneity and do not have any features to improve model accuracy like direct model synchronization through weight exchange and adaptive parameter tuning.
As shown in Figure 6b, we see that compute capacity-aware batching technique helps to reduce the training time by 34% by fully utilizing the heterogeneous compute resource of the cluster.
The highperformance worker was able to train a larger minibatch than the low-performance workers at each iteration.
Thus, using compute capacity-aware batching can learn more information faster than without using it.
We compare DLion using all four techniques with Gaia and Ako in network bandwidth heterogeneous environments.
We set up two micro-clouds with three workers in a micro-cloud and a worker in another micro-cloud.
All 4 workers have homogeneous compute resources.
For Gaia, workers in a micro-cloud train in a centralized manner by exchanging full gradients with a PS.
PSs in each micro-clouds exchange significant partial gradients according to its algorithm.
For Ako, all workers exchange partitioned gradients determined based on the smallest bandwidth between micro-cloud.
The test model is trained for 30 minutes for each of the three systems.
Figure 7 shows that DLion can achieve the highest accuracy during the same amount of training time, 42% and 25% higher than Gaia and Ako, respectively.
Gaia does not take into account the available network bandwidths when determining the size of gradients.
It waits until the significant gradients are delivered over WANs, resulting in more time to finish an iteration.
On the other hand, Ako considers the smallest network bandwidth in calculating the size of partial gradients.
As a result, it performs more iterations for a given amount of time than Gaia leading to higher accuracy.
However, there is inefficiency in network resource utilization of three workers in LAN.
If we selected the largest bandwidth for Ako, the accuracy would be lower due to network bottleneck issue in a WAN link between two micro-clouds.
There has been increasing need of data analytics based on deep learning in micro-clouds.
However existing distributed deep learning systems do not handle the characteristics of micro-clouds environments such as compute capacity and network bandwidth heterogeneity as well as system scalability.
In this paper, we have presented DLion, a decentralized deep learning system for fast learning and high accuracy in such environments.
We have conducted preliminary experiments to show the effect of DLion techniques handling the compute and network heterogeneity in micro-clouds.
Also, we have discussed design considerations for scalability solution.
Our preliminary results are promising, and show the benefit of explicitly addressing the challenges exposed by micro-cloud environments.In this paper, we presented promising preliminary results of DLion based on compute capacity-aware batching, adaptive model parameter tuning, network-aware data exchange features.
We are actively developing details of our DLion, and plan to open-source the system.
While this motivates the research on deep learning in large-scale heterogeneous environments such as micro-clouds, there are several discussion points and open issues: Large-scale system evaluation.
Scale is one of the major challenges in micro-clouds.
We plan to incorporate our proposed scalability solutions ( § 3.3) into DLion.
The experiments in this paper have been performed in a small-scale local cluster with a small size of deep learning model and dataset.
We plan to conduct more extensive and thorough experiments in large-scale environments where there are many micro-clouds to evaluate the scalability of DLion.
We will also use multiple large-scale deep learning models such as VGG16 [45] and ResNet50 [16] and datasets like ImageNet [9].
We expect DLion to outperform existing systems with larger models and datasets consuming larger computation and network resources because DLion carefully factors heterogeneous resources, the sizes of models and training data in training them.
Training data migration.
It is possible that data distribution across the micro-clouds may be skewed and may not match their compute capabilities.
For instance, it may happen that micro-clouds with small computation resources have larger volumes of training data compared to other micro-clouds with large computation resources.
This may require migrating training data across micro-clouds for better load balancing.
We will investigate the tradeoff of input data migration vs. load balancing in future work.
For instance, one approach could be to balance the size of training data for individual micro-clouds depending on their relative computation capacities by migrating small portions of training data to the closest large-capacity micro-clouds.
Dynamic environments.
We would like to cover dynamic environments where compute and network resources are dynamically changing over time to show the adaptability of DLion.
Our system and model parameters need to be adjusted based on the changing cluster resources.
We plan to exploit how the system dynamically adjusts the parameters based on environmental changes.
In addition, we will put more efforts in the assessment of the prediction accuracy based on profiling and the effectiveness of runtime parameter adjustment.
Fault tolerance and cluster dynamics Workers may join, leave, or fail during training, or micro-clouds may get disconnected.
To support such scenarios, we plan to explore a fault tolerance and cluster dynamics features in our system.
We have already separated the message passing module from the training core module based on TensorFlow to implement the feature.
We will continue to study on this to support worker failure recovery and cluster dynamics in DLion.Edge devices We assume workers as machines always powered on.
If various edges devices like mobile devices connected to micro-clouds are equipped with powerful computation components, we can consider the trade-off between available power (energy) of the devices and system performance in terms of training time and test accuracy.
