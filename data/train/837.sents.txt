Linear typing schemes guarantee single-threadedness and so the soundness of in-place update with respect to a functional semantics.
But linear schemes are restrictive in practice, and more restrictive than necessary to guarantee soundness of in-place update.
This has prompted research into static analysis and more sophisticated typing disciplines, to determine when in-place update may be safely used, or to combine linear and non-linear schemes.
Here we contribute to this line of research by defining a new typing scheme which better approximates the semantic property of soundness of in-place update for a functional semantics.
Our typing scheme includes two kinds of products (⊗ and ×), which allows data structures with or without sharing to be defined.
We begin from the observation that some data is used only in a "read-only" context after which it may be safely re-used before being destroyed.
Formalizing the in-place update interpretation and giving a machine model semantics allows us to refine this observation.
We define three usage aspects apparent from the semantics, which are used to annotate function argument types.
The aspects are (1) used destructively, (2) used read-only but shared with result, and (3) used read-only and not shared.
The distinctive advantage of pure functional programming is that program functions may be viewed as ordinary mathematical functions.
Powerful proof principles such as equational reasoning with program terms and mathematical induction are sound, without needing to use stores or other auxiliary entities, as is invariably required when reasoning about imperative programs.Consider the functional implementation of linked list reversal, as shown in Fig. 1 (for the moment, ignore the first argument to cons).
This definition of reversal is readily verified by induction and equational reasoning over the set of finite lists.
On the other hand, implementing reversal imperatively using pointers is (arguably) more cumbersome and error prone and, more seriously, would be harder to verify using complicated reasoning principles for imperative programs.The advantage of an imperative implementation, of course, is that it modifies its argument in-place whereas in a traditional functional implementation the result must be created from scratch and garbage collection is necessary in order salvage heap space.
We are interested in the possibility of having the best of both worlds by using a semantics-preserving translation of functional programs into imperative ones which use in-place update and need no garbage collection.
In previous work by the second author, a first-order functional language called LFPL was defined, together with such a translation.
LFPL relies on some user intervention to manage memory but without compromising the functional semantics in any way.
This works by augmenting (non-nil) constructors of inductive datatypes such as cons with an additional argument of an abstract "diamond" resource type 3 whose elements can be thought of as heap-space areas, loosely corresponding to Tofte-Talpin's notion of regions [TT97].
To construct an element of an inductive type, we must supply an value of the 3 abstract type.
The only way of obtaining values of type 3 is by deconstructing elements of recursive types in a pattern match.
The first argument to each use of cons in Fig. 1 is this value of type 3; the cons on the right hand side of the match is "justified" by the preceding cons-pattern.
The correspondence need not always be as local; in particular, values of type 3 may be passed as arguments to and returned by functions, as well as appearing in components of data structures.
We can give a compositional translation of LFPL into C by mapping 3 to the type void * (a universal pointer type), and implementing cons as Here, list_t is the type of pointers to a C struct with appropriately typed entries head, tail, and we have elided (compulsory) typecasts to reduce clutter.
As expected, nil is implemented as a function returning a null pointer.
When receiving a non-null argument of type list_t we can save its entries as local variables and subsequently use the pointer itself as an argument to cons().
This implements pattern matching.
For details of the translation, see [Hof00].
The main result of [Hof00] was that the semantics of the C translation agrees with the functional semantics of the soure program provided the latter admitted a linear typing for inductive types and 3 types, i.e., bound variables of inductive type are used at most once.
In particular, this linearity guarantees that the memory space pointed to by a 3-value is not needed anywhere else.
This prevents function definitions like:def list twice (list l) = match l with nil -> nil | cons(d,h,t) -> cons(d,0,cons(d,0,twice(l)))The functional semantics of twice maps a list l to a list twice as long as l with zero entries; on the other hand, the LFPL translation to C of the above code computes a circular list.
As one would expect, the translation of append in Fig. 1 appends one linked list to another in place; again, the translation of a non-linear phrase like append(l,l) results in a circular list, disagreeing with the functional semantics.
As an aside: we can implmenent twice in LFPL with the typing list[int*<>] -> list [int], where the argument list provides the right amount of extra space.
In recent unpublished work with Steffen Jost and Dilsun Kırlı, we have shown that the position of 3 types and their arguments can be automatically inferred, using integer linear programming.Linear typing together with the resource type 3 seems restrictive at first sight.
In particular, without dynamic creation of memory in the translation, no function can be written that increases the size of its input.
Yet surprisingly, a great many standard examples from functional programming fit very naturally into the LFPL typing discipline, among them, insertion sort, quick sort, tree sort, breadth-first traversal of a tree and Huffman's algorithm.
Moreover, in [Hof00] it was shown that every non size-increasing function on lists over booleans in the complexity class ETIME can be represented.In spite of this positive outlook, the linear typing discipline, as any other typing scheme, rejects many semantically valid programs.
In our context a program is semantically valid if its translation to imperative code computes its functional semantics.
We cannot hope to catch all semantically valid programs by a typing discipline, of course, but we can try to refine the type system to reduce the "slack", i.e., the discrepancy between the semantically valid programs and those which pass the typing discipline.In this paper we address one particular source for slack, namely the implicit assumption that every access to a variable is potentially destructive, i.e., changes the memory region pointed to or affected by this variable.
This is overly conservative: multiple uses of a variable need not compromise semantic validity, as long as only the last in a series of multiple uses is destructive (and moreover the results of the earlier accesses do not interfere with the ultimate destructive access).
A safe static approximation of this idea in the context of LFPL is the goal of this paper.
We present a type system which is more general than linear typing in that it permits multiple uses of variables in certain cases, yet is sound in the sense that for well-typed LFPL programs the imperative translation computes the functional semantics.
Some examples will help to motivate our type system.
A first example is the function sumlist : list[int] -> int which computes the sum of the elements in an integer list.def int sumlist(list[int] l) = match l with nil -> 0 | cons(d,h,t) -> h + sumlist(t)With the destructive pattern matching scheme of LFPL, we must consider that l is destroyed after evaluating sumlist(l), although the list would actually remain intact under any reasonable implementation.
We can avoid losing the list by returning it along with the result, rebuilding it as we compute the sum of the elements.
But this leads to a cumbersome programming style, and one has to remember that sumlist' : list[int] -> int * list [int] returns its argument unmodified.
A better solution is to say that from the definition above, we see that the list is not destroyed (because the 3-value d is not used), so we would like to assign sumlist a type which expresses that it does not destroy its argument.Not only should the sumlist function inspect its argument list without modifying it, but the result it returns no longer refers to the list.
This means that an expression likecons(d,sumlist(l),reverse(l))where d is of type 3, should also be soundly implemented, if we assume that evaluation occurs from left to right.
In other words, we can freely use the value of sumlist(l) even after l is destroyed.
This is not the case for functions which inspect data structures without modifying them, but return a result which contains some part of the argument.
An example is the function nth_tail which returns the nth tail of a list:def list nth_tail(int n, list l) = if n<=0 then l else match l with nil -> nil | cons(d,h,t) -> nth_tail(n-1, t)Unlike sumlist, the result of nth_tail may be shared (aliased) with the argument.
This means an expression likecons(d,nth_tail(2,l),nil)will be sound, butcons(d,nth_tail(2,l),cons(d',reverse(l),nil))will not be soundly implemented by the in-place update version, so the second expression should not be allowed in the language.
As a final example, consider again the append function in Fig. 1.
The imperative implementation physically appends the second list to the first one and returns the so modified first argument.
Thus, the first list l has been destroyed so we should treat that in the same way as arguments to reverse.
But the second list m is shared with the result, and so should be treated in the same way as arguments to nth_tail.
This suggests that we should consider the way a function operates on each of its arguments.These observations lead us to distinguish three usage aspects of variables, which are the central innovation in our type system.
The usage aspects are:-Aspect 1: modifying use, e.g., l in reverse(l) -Aspect 2: non-modifying use, but shared with result, e.g., m in append(l,m) -Aspect 3: non-modifying use, not shared with result, e.g., l in sumlist(l).
The numbers are in increasing order of "safety" or "permissiveness" in the type system.
Variables may only be accessed once with aspect 1.
Variables can be used many times with aspect 2, but this prevents an aspect 1 usage later if intermediate results are retained.
Finally, variables can be freely used with aspect 3, the pure "read-only" usage.
Perhaps surprisingly, these exact distinctions appear to be novel, but they are closely connected to several other analyses appearing in related work [Wad90,Ode92,Kob99] -see Section 5 for precise comparisons.Our type system decorates function arguments with usage aspects, and then tracks the way that variables are used.
For example, we have the following types:reverse : list[t]ˆ1 -> list[t] sumlist : list[t]ˆ3 -> int nth_tail : list[t]ˆ2 * intˆ3 -> list[t] append : list[t]ˆ1 * list[t]ˆ2 -> list[t]Heap-free types such as int will always have the read-only aspect 3.
Functions which have a heap-free result (like sumlist) may have aspect 3 for their non heap-free arguments, provided they are not modified when computing the result.
The strict linear type system in LFPL prevents sharing in data structures, which can lead to bad space behaviour in some programs.
The append function shows how we might be able to allow some limited sharing within data structures but still use an in-place update implementation, provided we take care over when modification is allowed.
For example, we would like to allow the expression let x=append(u,w) and y=append(v,w) in e provided that we don't modify both x and y in e; after either has been modified we should not refer to the other.
Similarly, we would like to allow a tree which has sharing amongst subtrees, in the simplest case a node constructed like this:let u=node(d,a,t,t) in e(where d:<> and a is a label).
This data structure should be safe so long as we do not modify both branches of u.
The kinds of data structure we are considering here have a DAG-like layout in memory.The "not modifying both parts" flavour of these examples leads us to include two kinds of products in our language.
Consider binary trees.
In a linear setting we have two kinds of trees, one corresponding to trees laid out in full in memory (⊗-trees), the other corresponding more to an object-oriented representation (×-trees) under which a tree can be sent messages asking it to return the outermost constructor or to evolve into one of its subtrees.
In ordinary functional programming these two are extensionally equivalent; in the presence of linearity constraints they differ considerably.
The ⊗-trees allow simultaneous access to all their components thus encompassing e.g., computing the list of leaf labellings, whereas access to the ×-trees is restricted to essentially search operations.
Conversely, ⊗-trees are more difficult to construct; we must ensure that their overall size is polynomially bounded which precludes in particular the definition of a function which constructs the full binary tree of depth n. On the other hand, the typing rules would allow construction of a full binary ×-tree, which is represented as a rather small DAG.
The novelty here is that we can reflect in the type system the kind of choices that a programmer would normally make in selecting the best data representation for a purpose.The product already in LFPL as studied to date is the tensor product (denoted by ⊗, resp.
* in code), accessed using a pattern matching construct:match p with x*y -> eThis allows both x and y to be accessed simultaneously in e. (Typing rules are shown in the next section).
Given a ⊗-product of two lists, we can access (maybe modify) both components; to be sound, the two lists must have no sharing.The cartesian product (denoted ×, resp.
X in code) which corresponds to the & connective of linear logic has a different behaviour.
We may access one component or the other, but not both; this means that the two components may have sharing.
With our usage aspects, we can be more permissive than allowing just access to one component of the product.
We can safely allow access to both components, so long as at most one component is modified, and if it is, the other one is not referenced thereafter.
The pairing rule for cartesian products has a special side condition which allows this.
Cartesian products are accessed via projection functions:fst : (t X u)ˆ2 -> t snd : (t X u)ˆ2 -> uThe usage aspect 2 here indicates that the result shares with the argument, which is the other part of enforcing the desired behaviour.To allow data structures with sharing, we can give constructors arguments of cartesian product types.
Ideally, we would allow the user to choose exactly where cartesian products are used and where ⊗-products are used, to allow the user to define datatypes appropriate for their application.
For the purpose of exposition in this paper, however, we will treat both lists and tree types as primitives, and consider just the ⊗-product style data structures as used in LFPL.O'Hearn and Pym's "bunched implications" [OP99] are also based on ⊗ and × coexisting.
In our case, × is not a true categorical product since the ×-pairing operation −, −− is partial; it was shown in [Hof99] that implementing e 1 , e 2 as a closure (λt.if t then e 1 else e 2 ) recovers the categorical product, but it requires heap space, violating heap size boundedness.
Syntax.
The grammar for the types and terms of our improved LFPL is given in Fig. 2.
For brevity, we use N also for the type of booleans (as in C).
Types not containing diamonds 3, lists L(−) or trees T(−) are called heap-free, e.g. N and N ⊗ N are heap-free.
We use x and variants to range over (a set of) variables and f to range over function symbols.
To simplify the presentation, we restrict the syntax so that most term formers can only be applied to variables.
In practice, we can define the more general forms such as f (e 1 , . . . , e n ) easily as syntactic sugar for nested let-expressions.
Also, compared with [Hof00] we will use a different translation scheme, where every non-nullary constructor of inductive type takes exactly one 3-argument, rather than a 3-argument for every ordinary argument of inductive type.
This is closer to the Java compilation described in [AH01].
A ::= N | 3 | L(A) | T(A) | A1 ⊗ A2 | A1 × A2 e ::= c | f (x1, . . . , xn) | x | let x = ex in e | if x then x1 else x2 | e1 ⊗ e2 | match x with (x1 ⊗ x2)⇒e | (e1, e2) | fst(e) | snd(e) | nil | cons(x d , x h , xt) | match x with nil⇒en|cons(x d , x h , xt)⇒ec | leaf(x d , xa) | node(x d , xa, x l , xr) | match x with leaf(x d , xa)⇒e l |node(x d , xa, x l , xr)⇒enA program consists of a series of (possibly mutually recursive) function definitions of the form f (x 1 , . . . , x n ) = e f .
These definitions must be well-typed.
To help ensure this, a program is given together with a signature Σ, which is a finite function from function symbols to first-order function types with usage aspects, i.e. of the form A i1 1 , . . . , A in n → A.
In the typing rules we will assume a fixed program with signature Σ.We keep track of usage aspects for variables as introduced above.
We write x i : A to mean that x:A will be used with aspect i ∈ {1, 2, 3} in the subject of the typing judgement.
A typing context Γ is a finite function from identifiers to types A with usage aspects.
If then we write Γ, ∆ for the disjoint union of Γ and ∆.
If such notation appears in the premise or conclusion of a rule below it is implicitly understood that these disjointness conditions are met.
We write e[x/y] for the term obtained from e by replacing all occurrences of the free variable y in e by x.
We consider terms modulo renaming of bound variables.In a couple of the typing rules we need some additional notation for manipulating usage aspects on variables.
The "committed to i" context ∆ i is the same as ∆, but each declaration x2 : A of an aspect 2 (aliased) variable is replaced with x i : A.
If we have two contexts ∆ 1 , ∆ 2 which only differ on usage aspects, so dom(∆ 1 ) = dom(∆ 2 ) and ∆ 1 (x) = ∆ 2 (x) for all x, then we define the merged context Γ= ∆ 1 ∧ ∆ 2 by dom(Γ ) = dom(∆ 1 ), Γ (x) = ∆ 1 (x), Γ [x] = min(∆ 1 (x), ∆ 2 (x)).
The merged context takes the "worst" usage aspect of each variable.Signatures.
We treat constructors as function symbols declared in the signature.
We also include primitive arithmetic and comparison operations in the signature.
Specifically, we can assume Σ contains a number of declarations:+, −, <, > : N 3 , N 3 → N nil A : L(A) cons A : 3 1 , A 2 , L(A) 2 → L(A) leaf A : 3 1 , A 2 → T(A) node A : 3 1 , A 2 , T(A) 2 , T(A) 2 → T(A) fst A×B : (A × B) 2 → A snd A×B : (A × B) 2 → Bfor suitable types A as used in the program.
The comma between argument types is treated as a ⊗-product, which means that these typings, and the corresponding elimination rules below, describe lists and trees with simultaneous access to subcomponents.
Hence they must be implemented without sharing unless the access is guaranteed to be read-only.
(For trees ST(A) with unrestricted sharing between components we could use the typing:sharednode A : 3 1 , (A × ST(A) × ST(A)) 2 → ST(A).
In this typing, there can be sharing amongst the label and subtrees, but still no sharing with the 3 argument, of course, since the region pointed to by the 3-argument is overwritten to store the constructed cell.)
Typing rules.
Now we explain the typing rules, shown in Fig. 2, which define a judgement of the form Γ e : A. Most rules are straightforward.
We use an affine linear system, so include weak.
In var, variables are given the default aspect 2, to indicate sharing.
If the result is a value of heap-free type, then with raise we can promote variables of aspect 2 to aspect 3 to reflect that they do not share with the result.
The rule drop goes the other way and allows us to assume that a variable is used in a more destructive fashion than it actually is.
Γ e1 : C Γ e2 : C Γ, x 3 : N if x then e1 else e2 : C (if) Γ, ∆1 e1 : A ∆2, Θ, x i : A e2 : B Either ∀z.∆1[z] = 3, or i = 3, ∀z.∆1[z] ≥ 2, ∆2[z] ≥ 2 Γ i , Θ, ∆ i 1 ∧ ∆2 let x = e1 in e2 : B (let) x1 2 : A1, x2 2 : A2 x1 ⊗ x2 : A1 ⊗ A2 (⊗-pair) Γ, ∆1 e1 : A1 Θ, ∆2 e2 : A2 condition Γ, Θ, ∆1 ∧ ∆2 (e1, e2) : A1 × A2 (×-pair)Γ, x1 The let rule is somewhat intricate.
The context is split into three pieces: variables specific to the definition e 1 , in Γ ; variables specific to the body e 2 , in Θ; and common variables, in ∆ 1 and ∆ 2 , which may be used with different aspects in e 1 and e 2 .
First, we type-check the definition to find its type A.
Then we type-check the body using some usage aspect i for the bound variable x.
The way the bound variable x is used in the body is used to commit any aliased variables belonging to e 1 .
For example, if x is used destructively in e 2 , then all aliased variables in Γ and ∆ 1 are used destructively in the overall expression; this accounts for the use of Γ i and ∆ i in the conclusion.
The aspects in ∆ 1 and ∆ 2 are merged in the overall expression, taking into account the way that x is used in e 2 .
The side condition prevents any common variable z being modified in e 1 or e 2 before it is referenced in e 2 .
More exactly, ∆ 1 [z] = 1 is not allowed (the value of the variable would be destroyed in the binding); ∆ 1 [z] = 3 is always allowed (the value of the variable has no heap overlap with the binding value), and ∆ 1 [z] = 2 is allowed provided neither i = 1 nor ∆ 2 [z] = 1 (the value of the common variable may have aliasing with e 2 , provided it is not partly or completely destroyed in e 2 : the modification may happen before the reference).
As an instance of let, we get a derived rule of contraction for aspect 3 variables.The only constructor rules we need are for the two kinds of pairs.
The rule for constructing a ×-pair ensures that all variables which are shared between the components have aspect at least 2.
The "condition " in rule ×-pair is:-∆ 1 [z] ≥ 2 and ∆ 2 [z] ≥ 2 for all z ∈ dom(∆ 1 ) = dom(∆ 2 ).
which ensures that no part of memory shared between the components is destroyed when the pair is constructed.
(A more liberal condition which considers evaluation order is possible, but we omit it here.)
In the destructor rules we type-check the branches in possibly extended contexts, and then pass the worst-case usage aspect as the usage for the term being destructed.
For example, if we destroy one half of a pair in pair-elim, so x 1 has usage aspect 1, then the whole pair is considered destroyed in the conclusion.
To establish the correctness of our typing rules, we need to formalize the intended in-place update interpretation of the language.
In [Hof00], a translation to 'C' and a semantics for the target sublanguage for 'C' was used.
Here we instead use an abstract machine model; this allows us to more easily consider alternative translations to other languages, such as the Java translation given in [AH01], or a typed assembly language interpretation, as given in [AC02].
The interpretation we consider here is closest to the Java translation from [AH01].
Let Loc be a set of locations which model memory addresses on a heap.
We use l to range over elements of Loc.
Next we define two sets of values, stack values SVal, ranged over by v, and heap values HVal, ranged over by h, thus:v ::= n | l | NULL | (v, v) h ::= v | {f 1 = v 1 . . . f n = v n }A stack value is either an integer n, a location l, a null value NULL, or a pair of stack values (v, v).
A heap value is either a stack value or an n-ary record consisting of named fields with stack values.
A stack S:Var SVal is a partial mapping from variables to stack values, and a heap σ:Loc HVal is a partial mapping from locations to heap values.
Evaluation of an expression e takes place with a given stack and heap, and yields a stack value and a possibly updated heap.
Thus we have a relation of the form S, σ e ; v, σ expressing that the evaluation of e under stack S and heap σ terminates and results in stack value v.
As a side effect the heap is modified to σ .
The only interesting cases in the operational semantics are the ones for the heap datatypes, which make use of 3-values as heap locations.
For example, the following rules for cons:S, σ e d ; l d , σ S, σ e h ; v h , σ S, σ et ; vt, σ S, σ cons(e d , e h , et) ; l d , σ [l d →{hd=v h , tl = vt}] S, σ e ; l, σ σ (l) = {hd=v h , tl=vt} S[x d → l, x h → v h , xt → vt], σ econs ; v, σ S, σ match e with nil⇒e nil |cons(x d , x h , xt)⇒econs ; v, σIn the constructor rule, the first argument e d of cons is a term of 3 type, which we evaluate to a heap location l d .
We then evaluate the head and the tail of the list in turn, propagating any changes to the heap.
Finally, the result is the location l d where we make the cons cell by updating the heap, using a record with hd and tl fields.
The match rule performs the opposite operation, breaking apart a cons-cell.
This operational semantics describes the essence of our in-place update interpretation of the functional language, without considering more complex translations or optimizations that might be present in a real compiler.
In this section we will prove that for a typable program, the imperative operational semantics is sound with respect to a functional (set-theoretic) semantics.Set-theoretic interpretation.
We define the set-theoretic interpretation of types [ ρ(f )(v 1 , . . . , v n ) = [ [e f ] ] ρ,ηwhere η(x i ) = v i , for any f ∈ dom(Σ).
Notice that this set-theoretic semantics does not say anything about space usage and treats 3 as a single-point type; its only purpose is to pin down the functional denotations of programs so that we can formally state a correctness result for the in-place update operational interpretation.Heap regions.
Given a stack value v, a type A and a heap σ we define the associated region R A (v, σ) as the least set of locations satisfying-R N (n, σ) = ∅, -R 3 (l, σ) = {l}, -R A×B ((v 1 , v 2 ), σ) = R A⊗B ((v 1 , v 2 ), σ) = R A (v 1 , σ) ∪ R B (v 2 , σ), -R L(A) (NULL, σ) = ∅, -R L(A) (l, σ) = {l} ∪ R A (h, σ) ∪ R L(A) (t, σ) when σ(l) = {hd = h, tl = t} (otherwise ∅), -R T(A) (l, σ) = {l} ∪ R A (v, σ), when σ(l) = {label = v}, -R T(A) (l, σ) = {l}∪R A (v, σ)∪R T(A) (t l , σ)∪R T(A) (t r , σ), when σ(l) = {label = v, left = t l , right = t r } (otherwise ∅).
It should be clear that R A (v, σ) is the part of the (domain of) σ that is relevant for v. Accordingly, if σ(l) = σ (l) for all l ∈ R A (v, σ) then R A (v, σ) = R A (v, σ ).
If A is a heap-free type, then R A (v, σ) = ∅.
Meaningful stack values in a heap.
Next, we need to single out the meaningful stack values and relate them to the corresponding semantic values.
A stack value is meaningful for a particular type and heap if it has a sensible interpretation in the heap for that type.
For instance, if σ(a) = {hd = 1, tl = NULL} then a would be a meaningful stack value of type L(N) with respect to σ and it would correspond to the semantic list [1].
Again, w.r.t. that same heap (a, a) would be a meaningful stack value of type L(A) × L(A) corresponding to the semantic pair([1], [1]) ∈ [ [L(A) × L(A)] ].
That same value (a, a) will also be a meaningful stack value of type L(A) ⊗ L(A) in case it will be used in a read only fashion.
This occurs for example in the term f (x⊗x) when f : (A⊗A) 3 → B.
This means that "meaningfulness" is parametrised by the aspect with which the value is going to be used.
No distinction is made, however, between aspects 2 and 3 in this case.
Given a stack value v, a type A, a heap σ, a denotation a ∈ [ [A] ] and an aspect i ∈ {1, 2, 3}, we define a five-place relation v σ A,i a which expresses that v is a meaningful stack value of type A with respect to heap σ corresponding to semantic value a in aspect i.
It is defined inductively as follows:-n σ N,i n , if n = n .
-l σ 3,i 0.
-(v 1 , v 2 ) σ A1×A2,i (a 1 , a 2 ) if v k σ A k ,i a k for k = 1, 2.
-(v 1 , v 2 ) σ A1⊗A2,i (a 1 , a 2 ) if v k σ A k ,i a k for k = 1, 2.
Additionally, R A1 (v 1 , σ)∩ R A2 (v 2 , σ) = ∅ in case i = 1.
-NULL σ L(A),i nil.
-l σ L(A),i cons(h, t), if σ(l) = {hd=v h , tl = v t }, l σ 3,i 0 and v h σ A,i h and v t σ L(A),i t. Additionally, R 3 (l, σ), R A (v h , σ), R L(A) (v t , σ) are pairwise disjoint in case i = 1.
-l σ T(A),i leaf(a) if σ(l) = {label=v a } and l σ 3,i 0 and v a σ A a. Additionally, R 3 (l, σ) ∩ R A (v a ) = ∅ in case i = 1.
-l σ T(A),i node(a, l, r) if σ(l) = {label=v a , left=v l , right=v r } and l σ 3,i 0 and v a σ A,i a and v l σ A,i l and v r σ A,i r.
Additionally R 3 (l, σ), R A (v a , σ), R T(A) (v l , σ), R T(A) (v r , σ) are pairwise disjoint in case i = 1.
Notice that σ A,2 and σ A,3 are identical, whereas σ A,1 prevents any "internal sharing" within ⊗-product types in the heap representation.
We extend this relation to stacks and valuations for a context, by defining S σ Γ η thus:-S(x) σ Γ (x),Γ [x] η(x) for each x ∈ dom(Γ ) -x = y and R Γ,x (S, σ) ∩ R Γ,y (S, σ) = ∅ implies Γ [x] ≥ 2, Γ [y] ≥ 2.
where as a shorthand, R Γ,x (S, σ) = def R Γ (x) (S(x), σ).
So S σ Γ η holds if stack S and heap σ are meaningful for the valuation η at appropriate types and aspects, and moreover, the region for each aspect 1 variable does not overlap with the region for any other variable.
(Informally: the aspect 1 variables are safe to update.)
Below we use the shorthand R Γ (S, σ) = def x∈dom(Γ ) R Γ,x (S, σ).
Correctness theorem.
With this definition of meaningfulness in place, we can prove that the evaluation of a term under a meaningful stack and heap gives a meaningful result corresponding to the set-theoretic semantics.
As usual, we prove a stronger statement to get an inductive argument through.Theorem 1.
Assume the following data and conditions:1.
a program P over some signature Σ with meaning ρ, 2.
a well-typed term Γ e : C over Σ for some Γ, e, C, 3.
a heap σ, a stack S and a valuation η, such that S σ Γ η Then S, σ e ; v, σ for some (uniquely determined) v, σ if and only if [ [e] ] η,ρ is defined.
Moreover, in this case the following hold:1.
R C (v, σ ) ⊆ R Γ (S, σ), 2.
if l ∈ R Γ (S, σ) then σ(l) = σ (l), 3.
if l ∈ R Γ,x (S, σ) and Γ [x] ≥ 2 then σ(l) = σ (l), 4.
v σ C,2 [ [e] ] η,ρ , 5.
S σ Γ 1 η implies v σ C,1 [ [e] ] η,ρ and R C (v, σ ) ∩ R Γ,x (S, σ) = ∅ when Γ [x] = 3.
This theorem expresses both the meaningfulness of the representation of semantic values on the machine, and the correctness of the operational semantics with respect to the set-theoretic semantics.
The five consequences capture the expected behaviour of the imperative operational semantics and the variable aspects.
In brief: (1) no new memory is consumed; (2) heap locations outside those reachable from input variables are unchanged in the result heap; (3) in-place updates are only allowed for locations in the regions of aspect 1 variables; (4) the operational semantics agrees with the set-theoretic result for aspects 2 and 3; (5) if the heap additionally has no variable-variable overlaps or "internal" sharing for aspect 2 variables, then the meaningfulness relation also holds in aspect 1 (in particular, there is no internal sharing within the result value v), and moreover, there is no overlap between the result region and the region of any aspect 3 variable.
This means it is safe to use the result in an updating context.
Specialising this perhaps daunting theorem to the particular case of a unary function on lists yields the following representative corollary:Corollary 1.
Let P be a program having a function symbol f : L(N) i → L(N).
If σ is a store and l is a location such that l points in σ to a linked list with integer entriesw = [x 1 , . . . , x n ] in σ then ρ(f )(w) is defined iff [x → l], σ f (x); v, σ for some v, σ and in this case v points in σ to a linked list with integer entries ρ(f )(w).
Additionally, one can draw conclusions about the heap region of the result list depending on the value of i.In further work (partly underway) we are examining the two kinds of product in more detail, and considering array types with and without sharing between entries.
We are also looking at dynamically allocated store via built-in functions new :→ 3 and dispose : 3 1 → N.
These built-ins can be implemented by interfacing to an external memory manager, for example, using malloc and free system calls augmented with a free list.
This allows more functions to be defined but breaks the heap-bounded nature of the system, in general.
We defined an improved version of the resource-aware linear programming language LFPL, which includes usage aspect annotation on types.
We also added datatypes based on cartesian products, to allow sharing in data structures on the heap.
Using an operational semantics to formalize the in-place update interpretation, we proved that evaluation in the language is both type-sound for a memory model and correct for a set-theoretic functional semantics.The philosophy behind LFPL is that of providing a static guarantee that efficient in-place implementations are used, while allowing as many programs as possible.
The guarantee is enforced by the type system.
This is in contrast to various other proposed systems which perform static analysis during compilation, or mix linear and non-linear typing schemes, to achieve compilations which are often efficient in practice, but which provide no absolute guarantee.
In its pure form, LFPL does not include any instructions for allocating heap space, so all computation is done in-place, using constant heap space.
This guarantee is provided for any program which can be written in the language.
Apart from differences in philosophy, our usage aspects and their semantic motivation from the memory model are somewhat novel compared with previous work.
Related ideas and annotations do already appear in the literature, although not always with semantic soundness results.
We believe that our system is simpler than much of the existing work, and in particular, the use of the resource type 3 is crucial: it is the appearance of 3 1 in the typing of constructors like cons that expresses that constructors are given an in-place update interpretation.
Without the resource type 3 there would be no aspect 1 component.Here is a necessarily brief comparison with some of the previous work.
The closest strand of work begins with Wadler's introduction of the idea of a sequential let [Wad90].
If we assume that e 1 is evaluated before e 2 in the expressionlet x=e 1 in e 2 [x]then we can allow sharing of a variable z between e 1 and e 2 , as long as z is not modified in e 1 and some other side-conditions which prevent examples like let x=y in append(x, y).
Our rule for let follows similar ideas.
Odersky [Ode92] built on Wadler's idea of the sequential let.
He has an observer annotation, which corresponds to our aspect 2 annotations: not modified, but still occurs in the result.
He too has side conditions for the let rule which ensure soundness, but there is no proof of this (Odersky's main result is a type reconstruction algorithm).
Kobayashi [Kob99] introduces quasi-linear types.
This typing scheme also allows sharing in let expressions.
It has a δ-usage which corresponds roughly to our aspect 3 usage.
Kobayashi's motivation was to statically detect points where deallocation occurs; this requires stack-managed extra heap, augmenting region analysis [TT97].
Kobayashi also allows non-linear use of variables (we might similarly add an extra aspect to LFPL to allow non-linear variables, if we accepted the use of a garbage collector).
Kobayashi proves a traditional type soundness (subject reduction) property, which shows an internal consistency of his system, whereas we have characterised and proved equivalence with an independently meaningful semantic property.
It might well be possible to prove similar results to ours for Kobayashi's system, but we believe that by considering the semantical property at the outset, we have introduced a rather more natural syntactic system, with simpler types and typing rules.There is much other related work on formal systems for reasoning or typechecking in the presence of aliasing, including for example work by Reynolds, O'Hearn and others [Rey78,OTPT95,Rey00,IO01]; work on the imperative λ-calculus [YR97]; uniqueness types [BS96], usage types for optimised compilation of lazy functional programs [PJW00] and program analyses for destructive array updates [DP93,WC98] as automated in PVS [Sha99].
There is also related work in the area of compiler construction and typed assembly languages, where researchers have investigated static analysis techniques for determining when optimisations such as in-place update or compile-time garbage collection are admissible; recent examples include shape analysis [WSR00], alias types [SWM00], and static capabilities [CWM99], which are an alternative and more permissive form of region-based memory management.
One of our future goals is to relate our work back to research on compiler optimizations and typed low-level languages, in the hope that we can guarantee that certain optimizations will always be possible in LFPL, by virtue of its type system.
This is in contrast to the behaviour of many present optimizing compilers, where it is often difficult for the programmer to be sure if a certain desirable optimization will performed by the compiler or not.
Work in this directions has begun in [AC02], where a typed assembly language is developed which has high-level types designed to support compilation from LFPL, to obviate the need for garbage collection.We see the work reported here as a step along the way towards a powerful high-level language equipped with notions of resource control.
There are more steps to take.
We want to consider richer type systems closer to those used in present functional programming languages, in particular, including polymorphic and higher-order types.
For the latter, recent work by the second author [Hof02] shows that a large class of functions on lists definable in a system with higherorder functions can be computed in bounded space.
Another step is to consider inference mechanisms for adding resource annotations, including the 3 arguments (we mentioned some progress on this in Section 1) and usage aspects, as well as the possibility of automatically choosing between ⊗-types and ×-types.
Other work-in-progress was mentioned at the end of the previous section.
We are supporting some of the theoretical work with the ongoing development of an experimental prototype compiler for LFPL; see the first author's web page for more details.
