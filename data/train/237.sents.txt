The SPARQL Query Language for RDF and the SPARQL Protocol for RDF are implemented by a growing number of storage systems and are used within enterprise and open web settings.
As SPARQL is taken up by the community there is a growing need for benchmarks to compare the performance of storage systems that expose SPARQL endpoints via the SPARQL protocol.
Such systems include native RDF stores, systems that map relational databases into RDF, and SPARQL wrappers around other kinds of data sources.
This paper introduces the Berlin SPARQL Benchmark (BSBM) for comparing the performance of these systems across architectures.
The benchmark is built around an e-commerce use case in which a set of products is offered by different vendors and consumers have posted reviews about products.
The benchmark query mix illustrates the search and navigation pattern of a consumer looking for a product.
After giving an overview about the design of the benchmark, the paper presents the results of an experiment comparing the performance of D2R Server, a relational database to RDF wrapper, with the performance of Sesame, Virtuoso, and Jena SDB, three popular RDF stores.
The Semantic Web is increasingly populated with larger amounts of RDF data.
Within the last year, the W3C Linking Open Data 1 community effort has stimulated the publication and interlinkage of datasets amounting to over 2 billion RDF triples.
Semantic Web technologies are also increasingly used within enterprise settings 2 .
A publicly accessible example of this trend is the Health Care and Life Science demonstration held at the 16th International World Wide Web Conference which involved datasets amounting all together to around 450 million RDF triples 3 .
A growing number of storage systems have started to support the SPARQL Query Language for RDF [1] and the SPARQL Protocol for RDF [2].
These systems include native RDF triple stores as well as systems that map relational databases into RDF 4 .
In the future, these systems may also include SPARQL wrappers around other kinds of data sources such as XML stores or object-oriented databases.As SPARQL is taken up by the community and as the amount of data that is accessible via SPARQL endpoints increases, there is a growing need for benchmarks to compare the performance of storage systems that expose SPARQL endpoints.The Berlin SPARQL Benchmark (BSBM) is a benchmark for comparing the performance of systems that expose SPARQL endpoints across architectures.
The benchmark is built around an e-commerce use case, where a set of products is offered by different vendors and consumers have posted reviews about products.The benchmark consists of a data generator and a test driver.
The data generator supports the creation of arbitrarily large datasets using the number of products as scale factor.
In order to be able to compare the performance of RDF triple stores with the performance of RDF-mapped relational databases or XML stores, the data generator can output RDF, XML and relational representations of the benchmark data.The test driver executes sequences of parameterized SPARQL queries over the SPARQL protocol against the system under test.
In order to simulate a realistic workload, the benchmark query mix emulates the search and navigation pattern of a consumer looking for a product.The rest of the paper is structured as follows: Section 2 gives an overview about existing benchmarks for Semantic Web technologies.
Section 3 describes the design of the Berlin SPARQL benchmark.
Section 3.1 gives an overview about the benchmark dataset.
Section 3.2 motivates the benchmark query mix and defines the benchmark queries.
Section 4 presents the results of an experiment comparing the performance of D2R Server, a relational database to RDF wrapper, with the performance of Sesame, Virtuoso, and Jena SDB, three popular RDF stores.
Section 5 discusses the contributions of the paper and outlines our next steps.
A benchmark is only a good tool for evaluating a system if the benchmark dataset and the workload are similar to the ones expected in the target use case [4].
Thus a variety of benchmarks for Semantic Web technologies have been developed.A widely used benchmark for comparing the performance, completeness and soundness of OWL reasoning engines is the Lehigh University Benchmark (LUBM) [6].
The LUBM benchmark has been extended in [7] to the University Ontology Benchmark (UOBM) by adding axioms that make use of all OWL Lite and OWL DL constructs.
As both benchmarks predate the SPARQL query language, they do not support benchmarking specific SPARQL features such as OPTIONAL filters or DESCRIBE and UNION operators.An early performance benchmark for SPARQL is the DBpedia Benchmark [8].
The benchmark measures the execution time of 5 queries that are relevant in the context of DBpedia Mobile [9] against parts of the DBpedia dataset.A recent SPARQL benchmark is SP 2 Bench [10].
SP 2 Bench uses a scalable dataset that reflects the structure of the DBLP Computer Science Bibliography.
The benchmark queries are designed for the comparison of different RDF store layouts and RDF data management approaches.A first benchmark for comparing the performance of relational database to RDF mapping tools with the performance of native RDF stores is presented in [11].
The benchmark focuses on the production of RDF graphs from relational databases and thus only tests SPARQL CONSTRUCT queries.Further information about RDF benchmarks and current benchmark results are found on the ESW RDF Store Benchmarking wiki page 5 .
This section introduces the design goals of the Berlin SPARQL Benchmark and gives an overview about the benchmark dataset, the query mix and the individual benchmark queries.The Berlin SPARQL Benchmark was designed along three goals: First, the benchmark should allow the comparison of different storage systems that expose SPARQL endpoints across architectures.
Testing storage systems with realistic workloads of use case motivated queries is a well established benchmarking technique in the database field and is for instance implemented by the TPC H benchmark [12].
The Berlin SPARQL Benchmark should apply this technique to systems that expose SPARQL endpoints.
As an increasing number of Semantic Web applications do not rely on heavyweight reasoning but focus on the integration and visualization of large amounts of data from autonomous data sources on the Web, the Berlin SPARQL Benchmark should not be designed to require complex reasoning but to measure the performance of queries against large amounts of RDF data.
The BSBM dataset is settled in an e-commerce use case in which a set of products is offered by different vendors and consumers have posted reviews about these products on various review sites.The benchmark dataset can be scaled to arbitrary sizes by using the number of products as scale factor.
The data generation is deterministic in order to make it possible to create different representations of exactly the same dataset.
Section 2 of the BSBM specification 6 contains the complete schema of the benchmark dataset, all data production rules and the probability distributions that are used for data generation.
Due to the restricted space of this paper, we will only outline the schema and the production rules below.The benchmark dataset consists of instances of the following classes: Product, ProductType, ProductFeature, Producer, Vendor, Offer, Review, Reviewer and ReviewingSite.Products are described by an rdfs:label and an rdfs:comment.
Products have between 3 and 5 textual properties.
The values of these properties consist of 5 to 15 words that are randomly chosen from a dictionary.
Products have 3 to 5 numeric properties with property values ranging from 1-2000 with a normal distribution.
Products have a type that is part of a type hierarchy.
The depth and width of this subsumption hierarchy depends on the chosen scale factor.
In order to run the benchmark against stores that do not support RDFS inference, the data generator can forward chain the product hierarchy and add all resulting rdf:type statements to the dataset.
Products have a variable number of product features which depend on the position of a product in the product hierarchy.
Each product type and product feature is described by an rdfs:label and an rdfs:comment.Products are offered by vendors.
Vendors are described by a label, a comment, a homepage URL and a country URI.
Offers are valid for a specific period and contain the price and the number of days it takes to deliver the product.
The number of offers for a product and the number of offers by each vendor follow a normal distribution.Reviews are published by reviewers.
Reviewers are described by their name, mailbox checksum and the country the reviewer live in.
Reviews consist of a title and a review text between 50-300 words.
Reviews have up to four ratings with a random integer value between 1 and 10.
Each rating is missing with a probability of 0.3.
The number of reviews for a product and the number of reviews published by each reviewer follow a normal distribution.
Table 1 summarizes the number of instances of each class in BSMB datasets of different sizes.
In order to simulate a realistic workload on the system under test, the Berlin SPARQL Benchmark executes sequences of parameterized queries that are motivated by the use case.
The benchmark queries are designed to emulate the search and navigation pattern of a consumer looking for a product.
In a real world setting, such a query sequence could for instance be executed by a shopping portal which is used by consumers to find products and sales offers.
First, the consumer searches for products that have a specific type and match a generic set of product features.
After looking at basic information about some matching products, the consumer gets a better idea of what he actually wants and searches again with a more specific set of features.
After going for a second time through the search results, he searches for products matching two alternative sets of features and products that are similar to a product that he likes.
After narrowing down the set of potential candidates, the consumer starts to look at offers and recent reviews for the products that fulfill his requirements.
In order to check the trustworthiness of the reviews, he retrieves background information about the reviewers.
He then decides which product to buy and starts to search for the best prize for this product offered by a vendor that is located in his country and is able to deliver within three days.
Tables 2 shows the BSBM query mix resulting from this search and navigation path.
?
product bsbm:productFeature %ProductFeature1% .
?
product bsbm:productFeature %ProductFeature3% .
?
product bsbm:productPropertyNumeric2 ?
p2 .
FILTER ( ?
p2> %y% ) }} ORDER BY ?
label LIMIT 10 OFFSET 10Query 5: Find products that are similar to a given product SELECT DISTINCT ?
product ?
productLabel WHERE { ?
product rdfs:label ?
productLabel .
%ProductXYZ% rdf:type ?
prodtype.
?
product rdf:type ?
prodtype .
FILTER (%ProductXYZ% !
= ?
product) %ProductXYZ% bsbm:productFeature ?
prodFeature .
?
product bsbm:productFeature ?
prodFeature .
%ProductXYZ% bsbm:productPropertyNumeric1 ?
origProperty1 .
?
product bsbm:productPropertyNumeric1 ?
simProperty1 .
FILTER (?
simProperty1 < (?
origProperty1 + 150) && ?
simProperty1 > (?
origProperty1 -150)) %ProductXYZ% bsbm:productPropertyNumeric2 ?
origProperty2 .
?
product bsbm:productPropertyNumeric2 ?
simProperty2 .
FILTER (?
simProperty2 < (?
origProperty2 + 220) && ?
simProperty2 > (?
origProperty2 -220)) } ORDER BY ?
productLabel LIMIT 5 SELECT ?
product ?
label WHERE { ?
product rdfs:label ?
label .
?
product rdf:type bsbm:Product .
FILTER regex(?
label, "%word1%|%word2%|%word3%")} Today, most enterprise data is stored in relational databases.
In order to prevent synchonization problems, it is preferable in many situations to have direct SPARQL access to this data without having to replicate it into RDF.
We were thus interested in comparing the performance of relational database to RDF wrappers with the performance of native RDF stores.
In order to get an idea which architecture performs better for which types of queries and for which dataset sizes, we ran the Berlin SPARQL Benchmark against D2R Server 7 , a database to RDF wrapper which rewrites SPARQL queries into SQL queries against an application-specific relational schemata based on a mapping, as well as against Sesame 8 , Virtuoso 9 , and Jena SDB 10 , three popular RDF stores.
The experiment was conducted on a DELL workstation (processor: Intel Core 2 Quad Q9450 2.66GHz; memory: 8GB DDR2 667; hard disks: 160GB (10,000 rpm) SATA2, 750GB (7,200 rpm) SATA2) running Ubuntu 8.04 64-bit as operating system.
We used the following releases of the systems under test: as underlying RDMS.
The load performance of the systems was measured by loading the N-Triple representation of the BSBM datasets into the triple stores and by loading the relational representation in the form of MySQL dumps into the RDMS behind D2R Server.
The loaded datasets were forward chained and contained all rdf:type statements for product types.
Thus the systems under test did not have to do any inferencing.The query performance of the systems was measured by running 50 BSBM query mixes (altogether 1250 queries) against the systems over the SPARQL protocol.
The test driver and the system under test (SUT) were running on the same machine in order to reduce the influence of network latency.
In order to enable the SUTs to load parts of the working set into main memory and to take advantage of query result and query execution plan caching, 10 BSBM query mixes (altogether 250 queries) were executed for warm-up before the actual times were measured.
This section summarizes the results of the experiment and gives a first interpretation.
The complete run logs of the experiment and the exact configuration of the SUTs are found on the BSBM website.
Table 5 summarizes the time it took to load the N-Triple files and the MySQL dumps into the different stores.
For all dataset sizes, loading the MySQL dump was significantly faster than loading the N-Triple representation of the benchmark data.
When comparing the load performance of the triple stores, it turns out that Sesame and Virtuoso are faster for small datasets while Jena SDB is faster for larger datasets.
Table 6 contains the overall times for running 50 query mixes against the stores.
It turns out that Sesame is the fastest RDF store for small dataset sizes.
Virtuoso is the fastest RDF store for big datasets, but is rather slow for small ones.
According to feedback from the Virtuoso team, this is due to the time spent on compiling the query and deciding on the query execution plan being independent of the dataset size.
Compared to the RDF stores, D2R Server showed an inferior overall performance.
When running the query mix against the 25M dataset, query 5 did hid the 30 seconds time out and was excluded from the run.
Thus there is no overall run time figure for D2R Server at this dataset size.
The picture drawn by the overall run times radically changed when we examined the run times of specific queries.
It turned out that no store is superior for all queries.
Sesame is the fastest store for queries 1 to 4 at all dataset sizes.
Sesame shows a bad performance for queries 5 and 9 against large datasets which prevents the store from having the best overall performance for all dataset sizes.
D2R Server, which showed an inferior overall runtime, is the fastest store for queries 6 to 10 against the 25M dataset.
Jena SDB turns out to be the fastest store for query 9, while Virtuoso is good at queries 5 and 6.
The tables below contain the arithmetic means of the query execution times over all 50 runs per query (in seconds).
The best performance figure for each dataset size is set bold in the tables.
This paper introduced the Berlin SPARQL Benchmark for comparing the performance of different types of storage systems that expose SPARQL endpoints and presented the results of an initial benchmark experiment.
The paper provides the following contributions to the field of SPARQL benchmarking:1.
The W3C Data Access Working Group has developed the SPARQL query language together with the SPARQL protocol in order to enable data access over the Web.
The Berlin SPARQL Benchmark is the first benchmark that measures the performance of systems that use both technologies together to facilitate data access over the Web.
2.
The Berlin Benchmark provides an RDF triple, an XML and a relational representation of the benchmark dataset.
The Benchmark can thus be used to compare the performance of storage systems that use different internal data models.
3.
Testing storage systems with realistic workloads of use case motivated queries is a well established technique in the database field and is for instance used by the widely accepted TPC H benchmark [12].
The Berlin SPARQL Benchmark is the first benchmark that applies this technique to systems that expose SPARQL endpoints.The benchmark experiment has shown that no store is dominant for all queries.
Concerning the performance of relational database to RDF wrappers, D2R Server has shown a very good performance for specific queries against large datasets but was very slow at others.
For us as the development team behind D2R Server it will be very interesting to further analyze these results and to improve D2R Server's SPARQL to SQL rewriting algorithm accordingly.
As next steps for the benchmark, we plan to:1.
Run the benchmark with 100M, 250M and 1B triple datasets against all stores in order to find out at which dataset size the query times become unacceptable.
2.
Run the benchmark against other relational database to RDF wrappers in order to make a contribution to the current work of the W3C RDB2RDF Incubator Group 11 .
Jena SDB storage layouts and setups.
4.
Extend the test driver with multi-threading features in order to simulate multiple clients simultaneously working against a server.
5.
Extend the benchmark to the Named Graphs data model in order to compare RDF triple stores and Named Graph stores.The complete benchmark specification, current benchmark results, benchmark datasets, detailed run logs as well as the source code of the data generator and test driver (GPL license) are available from the Berlin SPARQL Benchmark website at http://www4.wiwiss.fu-berlin.de/bizer/BerlinSPARQLBenchmark/
