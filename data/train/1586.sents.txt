While today's virtual datacenters have hypervi-sor based mechanisms to partition compute resources between the tenants co-located on an end host, they provide little control over how tenants share the network.
is opens cloud applications to interference from other tenants, resulting in unpredictable performance and exposure to denial of service attacks.
is paper explores the design space for achieving performance isolation between tenants.
We nd that existing schemes for enterprise datacenters suuer from at least one of these problems: they cannot keep up with the numbers of tenants and the VM churn observed in cloud datacenters; they impose static bandwidth limits to obtain isolation at the cost of network utilization; they require switch and/or NIC modiications; they cannot tolerate malicious tenants and compromised hypervisors.
We propose Seawall, an edge-based solution, that achieves max-min fairness across tenant VMs by sending trac through congestion-controlled, hypervisor-to-hypervisor tunnels.
By consolidating applications onto a common infrastructure, cloud datacenters achieve higher eeciency from the same resource pool and can scale up (or down) with changes in demand [].
Commodity virtualization stacks (e.g., Xen, HyperV) let existing applications run on the cloud with few modiications.
A key remaining obstacle, however, is the disparity in performance guarantees between the cloud and traditional datacenters.Since public clouds run arbitrary tenant code, they are at risk from malicious or subverted nodes.
For instance, Amazon Web Services (AWS) has already been used by spammers [] and been subject to denial of service attacks [].
e incentive to break cloud-hosted applications is rising as high-value applications move to the cloud.
Attacks from inside a cloud datacenter can pose greater threat, since they beneet from plentiful internal bandwidth.
Market research and experimental studies report high performance variation over time [] and user concerns regarding availability of shared services and consistent performance [].
Network performance isolation between tenants can be an important tool for both minimizing disruption from legitimate tenants that run network-intensive workloads and protecting against malicious tenants that launch DoS attacks.
Without such isolation, a tenant that sends a high volume of trac to shared services can deny service to ten- † Cornell University and Microso Research ‡ Microso Research ants that happen to be co-located at either end or share intermediate links.Perhaps because there has never been a need to keep apart many interacting parties, the commonly available techniques for network-level separation in Ethernet-based networks, VLANs and CoS (Class of Service) tags, cannot scale to cloud datacenters.
Every major cloud provider [, ] reports O(10 5 ) servers, to cores/server and O(10 4 ) tenants, while Amazon reports that of its cores are utilized on average.
In comparison, the number of VLANs possible is and most switches support classes.
Mapping tenants onto a small number of isolation primitives leads to fate sharing between tenants on the same VLAN/class.
Churn makes the problem worse.
e pay-as-you-go model encourages tenants to grow and shrink on demand.
AWS reports ~~~~K new instances created per day, or one new VM per server per day.
Modifying VLANs on all switches and hosts in the network upon each tenant change is unlikely to keep up with this churn.
In our enterprise the procedure for changing VLANs takes several days due to many human steps involved in checking against policy.
OpenFlow/NOX [, ] based centralized solutions, if well engineered to keep the work and state changes required per update small, may keep up.
However, doing so implies changing all the networking gear in the datacenter.ere is a trade-o between ensuring isolation and retaining high network utilization.
Bandwidth reservations, as realized by a host of mechanisms including RSVP and MPLS-TE, are either overly conservative at low load, thus achieving poor network utilization, or overly lenient at high load, thus achieving poor isolation.
It is preferable to enforce isolation only when congestion happens and allow best-eeort use of spare bandwidth at other times.Max-min fair allocation ts this bill, which can be accomplished by running only trac that is TCP or TCP-friendly (e.g., TFRC, DCCP).
However, in a cloud datacenter, the tenant controls the applications and trac, and, in some cases, may also control the networking stack in the guest OS.
We have found in interviews with users that banning trac, such as UDP, that is potentially unfriendly to TCP is undesirable because it may force code changes.
Further, even while conforming to the hose model, i.e., not sending more trac than the recipient can drain, a tenant can launch many concurrent TCP ows, avail of TCP's per--ow fair division of bandwidth and stomp on other users.Datacenter network topologies that provide full bisection bandwidth [, ], by using more switches and links in the core, only partially solve the problem.
Even if the core of the network has suucient bandwidth to not be the bottleneck, topologies such as fat-tree and VLL retain hotspots on the network path between the core and the receiver.
As shown in Figure , by selective targeting, an attacker can make a server or an entire rack unresponsive.
An ideal isolation solution for cloud datacenters has to scale, keep up with churn and retain high network utilization.
It has to do so without assuming well-behaved or TCP conformant tenants.
Since changes to the NICs and switches are expensive, take some time to standardize and deploy, and are hard to customize once deployed, edge or soware based solutions are preferable.is paper presents an initial design of Seawall that satises these constraints and requirements.
Seawall relies on running rate controllers at end hosts, to provide a scalable enforcement mechanism; and soware-based network monitoring, to provide exible, low cost end-to-end feedback.
Seawall places the rate controllers in the hypervisor to protect it against malicious tenant code.
Seawall employs recent advances in exploiting multi-queue NICs and multicore CPUs to achieve low overhead on end hosts.
Seawall is designed to be retrotted to real-world virtualized datacenters, providing performance isolation with no additional assumptions about hardware functionality and requiring only a small number of incremental changes to end host soware and switch conguration.
Unlike other end host approaches, which assume that hypervisors are trusted, Seawall can continue to guarantee performance isolation even when hypervisors are compromised.
is section examines existing schemes to apportion network bandwidth between diierent users in a cloud datacenter.
We outline the capabilities and shortcomings of existing mechanisms, the functionality that is already available at switches and end hosts (Table ).
We classify existing mechanisms as those that are local to a switch or a link and those that are end-to-end.
Ethernet provides VLANs and .
.
p CoS tags to segregate diierent users and types of trac.
VLANs provide reachability separation between diierent applications, such as wireless vs. wired, management vs. data, however, switches enforce no performance isolation between diierent VLANs that share the same Ethernet trunk.
.
.
p, when used with .
.
qaz, provides performance isolation of special trac classes, such as FCoE [].
Neither the VLAN address space, nor the numbers of supported .
.
p tags, scale to the number of tenants in today's cloud datacenters.
e scalability of .
.
p is constrained by both tag address space limitations and hardware.
Typical switches support up to eight tags, limited by the number of hardware queues.
ey map trac with each tag to a hardware queue and apply strict prioritization or deecit round robin (DRR) between the queues.
Trac mapped to the same tag shares fate since misbehaving or unresponsive trac can drown out other trac that is mapped to the same queue.High end Layer switches found in the core can police large numbers of ows, for example, K with Cisco Nexus [].
Policers are token bucket lters: they track the bandwidth utilization of each ow and mark or cap the bandwidth above a certain rate.
Such switches are expensive and the majority of datacenter switches do not support policing.
Trac along paths with no policer receives no protection.
Even where available, token bucket lters suffer from being unable to congure rates for all ows that achieves fairness across tenants.
For instance, to achieve high network utilization, an operator might congure policers to mark, rather than cap, ows above a certain threshold to harvest residual capacity.
However, the operator has no way to prevent a single sellsh ow from consuming the residual capacity.Compute nodes include virtual switches to multiplex their physical network connections between virtual machines.
Virtual switches have similar features as physical switches [, ], however, conguring large rule-sets can add CPU overhead.
NICs provide ooad hardware for ltering, rate limiting, and DRR that can reduce overhead, however current NICs support only a small number of hardware queues for DRR and less expressive ltering and rate limiting rules than datacenter switches.
CoS and policing only rate limit based on the local state of the network and do not consider downstream congestion.
End-to-end, feedback-based mechanisms, such as QCN and TCP, are more scalable, since rate controller state is held only at the edge, and more precise, since they can control individual ows without harming other ows.QCN is an emerging Ethernet standard for congestion control in datacenter networks [].
In QCN, switches can send congestion feedback directly to senders: upon detecting a congested link, the switch sends feedback to the heavy senders.
e feedback packet uniquely identiies the ow, enabling senders that receive feedback to rate limit speciic ows.
Since QCN feedback packets encode more detailed feedback about link utilization, QCN senders have more responsive control loops than those of TCP.
ough the QCN standard speciies implementing hardware-based rate controllers in NICs, recent work has proposed processing QCN feedback in soware, which can support an arbitrary number of ows and more exible reaction algorithms [].
Despite these advantages, QCN fails to meet the topology agnostic requirement.
To achieve full performance isolation, all links should support QCN.
However, it is unclear whether QCN will become a standard feature on future commodity switches.
Because QCN operates at Layer , while most datacenters contain many Layer domains joined by a Layer core, ows that span multiple subnets cannot receive QCN feedback without extensions to the protocol and gateways.TCP and UDP: TCP is scalable to many endpoints, achieves fair bandwidth allocation, avoids congestion, and supports arbitrary topologies.
Since UDP provides no rate control properties, some clouds, such as Azure, disallow tenants from using UDP [].
However, allowing only TCP trafc does not solve performance isolation since tenants can run any TCP stack they choose.
Malicious tenants can overwhelm the network by simply generating a ood of Figure A).
By deening fairness by the number of VMs that share a link, rather than the number of ows, the network would constrain the tenant's share in proportion to its size in VMs ( Figure B).
is notion of fairness prevents the above attacks and is consistent with how cloud providers allocate compute resources to a tenant in proportion to the number of VMs that it pays for.Billing: An economic incentive approach would count the amount of trac contributed by each tenant and bill her accordingly.
While billing disincentivizes tenants from waste and is an additional source of revenue for the provider, it is inadequate at enforcing isolation.
First, network bandwidth does not have a xed cost: when a link is idle, the marginal cost of network bandwidth is minimal.
When the link is congested, the cost of network bandwidth is high: the performance of other tenants suuers, potentially causing service failures and customer dissatisfaction.
Billing at a at rate is either too expensive at low network loads or not expensive enough when congestion happens.
Further, none of the cloud providers employ variable pricing for bandwidth, perhaps because it is complicated to explain and market.
Hence, billing cannot penalize for the severity of the collateral damage.
Second, billing operates on long timescales and does not protect against malicious code.
While a tenant that contributes excessive trac will eventually have to pay, billing does not provide run-time guarantees such as freedom from starvation.
Further, an attacker could launch attacks from compromised VMs and thus avoid being charged.Reservations: Bandwidth reservations, using mechanisms such as MPLS and RSVP, statically divide bandwidth among tenants.
While guaranteeing each tenant the bandwidth they ask for, reservations are not work-conserving.
A bottlenecked tenant cannot use more than his reservation even when there is spare capacity.
e variance in demands makes reserving for peak usage wasteful and reserving for average usage less performant.
Further, by operating at a higher granularity than that of a tenant's trac, most reservation schemes do not scale -MPLS, for example, is used in ISPs to engineer inter-PoP trac across pre-determined paths.
Some recent work tackles this scaling problem [].
.
Seawall uses a hypervisor-based rate controller, driven by feedback from the network and the receiving hypervisor, that regulates all trac sent from a tenant.
us, Seawall can control even tenants that send UDP trac or use misbehaving TCP stacks; malicious tenants cannot attack the rate controller directly by spoong feedback packets and cannot escape the rate controller without breaking hypervisor isolation.
e rate controller also protects against direct denial of service attacks: a recipient of unwanted trac can ask the sender's rate controller to block future trac to the recipient.
Seawall uses Layer (IP) feedback signaling, which can traverse arbitrary datacenter topologies.
is discussion focuses on intra-datacenter trac; we assume that the datacenter's Internet gateway participates in Seawall like any other compute node.Seawall rate controllers are implemented in the virtual NIC, the hypervisor component responsible for exporting a network device interface to a guest's network driver.
A rate controller takes as input the packets received and sent by the compute node and congestion feedback from the network and recipient.
On the receive path, the virtual NIC checks for congestion signals, such as ECN marks or lost packets, and sends this feedback to the sender.On the send path, the virtual NIC classiies incoming packets into per-(sourceVM, destinationVM, path) queues, with external destinations mapped onto the Internet gateway.
Path is needed for networks that use multipath (e.g., ECMP []) to assign packets with the same TCP/UDPtuple to diierent paths.
Rather than aliasing feedback information from diierent paths onto a common rate controller, Seawall maps -tuples to queues via a ow-traceroute.
Since ECMP deterministically maps a -tuple to a path, owtraceroute uses the same source, destination, protocol, and port numbers, in traceroute probes.
In practice, we nd that this mapping changes rarely and Seawall can cache it.
Seawall can use any rate control algorithm, such as TCP, TFRC, or QCN, to determine the rate of service for the transmit queues.
Such algorithms vary in their stability, reaction time, and tolerance to bandwidth delay products.
We defer choosing an appropriate algorithm for future work.We note that TCP-like rate control achieves max-min fairness between each contender.
In typical use, each contender is a ow, but as described above, each contender is a communicating pair of VMs.
It is easy to deduce that a tenant with N VMs can grab up to an N 2 proportion of bandwidth by communicating between all pairs.
To mitigate this, Seawall uses path feedback to estimate TCP-like fair rate for each (senderVM, link), i.e., a VM's share on each link along the path is independent of the destination.
e rate of service for each transmit queue is the minimum of the rates of links along the corresponding path.Interaction with guest OS.Since the rate controller changes the order in which packets drain from the virtual NIC it can cause head-of-line blocking in the guest's NIC driver.
e virtual NIC driver blocks waiting for the virtual NIC to acknowledge that packets have been sent to prevent overowing the NIC buuer before sending more packets.Fully solving this problem requires some participation from the guest: the rate controller could send positive or negative feedback (e.g., with window size or ECN) to an unmodiied guest running TCP, expose ow-speciic queues to the guest, or apply backpressure on a per-socket, rather than a per-NIC, basis [, ].
Since the same problem occurs with QCN, the same soware modiications to virtual NIC interface, network stack, and applications will work for both Seawall and QCN.
We have built a prototype rate controller for HyperV [].
e implementation does not depend on any HyperVspeciic functionality and only requires that the hypervisor provide a high-resolution timer and allow in-place modiications of packets from the guest.
us, we expect our techniques to generalize to other hypervisors.To ease development, deployment, and distribution, the rate controller was implemented as an NDIS packet lter driver rather than as changes to the virtual NIC.
Should we need to send control messages between the guest VM and lter driver, we plan to tunnel them over Ethernet.
e implementation took lines of code, compared with for the sample pass-through packet lter.
We have not yet implemented the extensions to prevent head of line blocking in the guest OS.
e rate controller installs directly above the physical NIC driver, where it interposes on all sent and received packets.
It implements a TCP-like algorithm and applies an encapsulation header around the transport headers, consisting of packet sequence number, packet acknowledgment number, and a single entry SACK.To verify performance isolation, we ran competing TCP and UDP ows over a Mb/s bottleneck link.
Enabling the rate controller on the UDP source forced the ow to be TCP-friendly, allowing the TCP connection to acquire its fair share of bandwidth.
Our prototype, which is an unoptimized work-inprogress, is CPU-bound, achieving only Mb/s throughput on a Gb/s link.
Since this is below our performance requirements, we are redesigning the rate controller to minimize CPU overhead.
Our preliminary eeorts suggest that changing the way the rate controller inserts data into packets will enable us to both exploit NIC ooads and reduce the complexity of our code.
By using a "bit-stealing" approach ( §....), our newer prototype achieves Mb/s.
Using encapsulation breaks NIC ooads, since NICs need to parse packet headers to implement ooads.
To determine the importance of preserving NIC ooads, we ran the NTttcp [] micro-benchmark to measure the through- put and CPU overhead of a TCP sender running on a machine equipped with a quad core ... Intel Coree Duo and an Intel LM NIC connected to a receiver over a Gb/s link.
Like most commodity NICs, this NIC ooads checksumming and transmit segmentation of TCP packets.
While NTttcp achieves line rate (above Mb/s) in all ooad congurations, the CPU utilization varies considerably.
Segmentation ooad decreases overhead by allowing the network stack to handle the same volume of trac with less bookkeeping.
When segmentation ooad is enabled, the network stack can divide the trac into packets larger than the MTU, thus reducing the total number of packets that pass through the soware stack.
e NIC hardware chops these large packets down to the MTU to maintain compatibility with the network.Segmentation ooad reduces overhead in both the guest and hypervisor.
Running segmentation ooad in the guest reduces overhead even in the absence of hardware support [].
However, these savings are minimal on our platform, only reducing utilization from ... to ... (Figure ).
By comparison, the reduction from enabling ooad in both the guest and the hypervisor is signiicantly higher, dropping utilization to ....
Segmentation ooad has the greatest impact; enabling just this reduces utilization to ....
To achieve our performance requirement, we will need to nd an alternate, ooad-compatible way to encode data from the rate controller.
An encoding breaks ooad if the hardware cannot parse the resulting packet header.
Conversely, ooad hardware can break an encoding if it overwrites or discards data.
e rate controller can satisfy both requirements by "stealing bits" from unused, redundant, or predictable bits in the TCP/IP headers.
For instance, a rate controller can encode data in any eld, such as the IP ID and TCP timestamp so long as it (() accounts for how the network and NIC interpret and update those elds, and (() upon receiving a packet, restores these elds to reasonable values for the guest.
To minimize the required space, the rate controller only encodes a sequence number, which suuces for detecting losses.
Other information, such as acknowledgments and RTT estimates, is exchanged on a separate connection between the source and destination hypervisors.Bit-stealing can only encode a limited amount of data, which can limit future improvements to the rate controller.
Placing rate controller data in the packet payload, i.e. after the transport layer headers, yields more space without breaking segmentation.
However, segmentation ooad breaks this encoding, since the data would only be included in one of the output packets.
Encoding the data as a TCP option solves this problem, since the ooad hardware would copy it to all output packets as part of the transport header.Changing the length of a packet may require allocating an additional packet buuer, which adds CPU overhead compared to the bit stealing approach.
Modifying the guest network stack to leave extra space in packets for hypervisorlevel data avoids this.
is optimization is straightforward and is an instance of device paravirtualization [].
In addition to enabling segmentation ooad, both encodings preserve compatibility with important switch functionality, such as ACLs, that reference TCP and UDP port numbers, and load balancing functionality, such as ECMP and receive side scaling [], which spread load using both IP addresses and TCP/UDP port numbers.Virtualization-aware NICs.NICs for virtualized datacenters include additional hardware ooads that allow guest VMs to directly access the hardware, bypassing the CPU and latency overheads of passing packets through the hypervisor.
Using the PCI SR-IOV interface, the hypervisor can bind VMs to dedicated virtual contexts that each provide the abstraction of a dedicated NIC [].
To prevent starvation and to provide proportional resource allocation, hypervisors can congure NICs to enforce rate limits for each virtual context.Seawall is compatible with virtual contexts given appropriate NIC or network support.
Our current prototype splits the rate controller into two components.
A rate selector outside the forwarding path (in HyperV, running in a userspace process within the root partition) continuously updates the rate limits for each ow based on congestion signals.
A rate limiter on the forwarding path (in HyperV, running in the lter driver) enforces these limits.
For guests that directly use a virtual context, the rate selector would instead congure the corresponding NIC rate limiter.
Alternatively, the rate selector could congure a matching policer in an upstream switch.
e rate limiters on existing SR-IOV NICs lack two pieces of functionality necessary to support Seawall.
First, Seawall's soware rate limiter passes congestion signals, based on monitoring received packets, to the rate selector so that the rate selector can determine the appropriate rate.
NIC rate limiters do not provide this monitoring functionality.
Second, the soware rate limiter enforces a separate rate limit for each destination and path.
NIC rate limiters are not suuciently selective: they enforce rate limits based only on source and are not multi-path aware.Hypervisor virtual switches are becoming increasingly sophisticated; this trend will likely enhance the capabilities of SR-IOV NICs.
In particular, Seawall can use hardware support for port mirroring, already available in SR-IOV NICs, to monitor congestion signals [].
Hypervisor compromise is a growing concern for cloud computing [].
Existing networks that shii trusted network functionality to end hosts typically rely on attestation to check the boot-time integrity of the hypervisor and its network stack [].
Due to the lack of performant mechanisms for detecting hypervisors that are compromised at runtime, Seawall uses defense in depth to limit the potential damage from such attacks.At a high level, Seawall preserves performance isolation by forcing compromised hypervisors to behave like uncompromised hypervisors in many situations.
Seawall achieves this by using uncompromised hypervisors to detect misbehavior, such as not reducing send rate in response to congestion feedback.
Upon detecting misbehavior, hypervisors report it to Seawall, which then contains or shuts down the compromised hypervisor.Seawall uses additional low level network invariants to prevent compromised hypervisors from escaping detection.
Compromised hypervisors might spoof packets to make it harder to detect an attack, or worse, falsely incriminate an innocent hypervisor, causing Seawall to shut it down.
A compromised hypervisor might also send packets, such as those with invalid destination addresses or with low TTLs, that are invisible to detection because they never reach an end host.
Seawall uses existing switch security features to prevent all of these attacks.
Responding quickly to an attack helps to minimize its impact but doing so makes the system vulnerable to false accusations.
Seawall requires a threshold of reports from f unique hypervisors before shutting down a hypervisor.
is requires the attacker to compromise f hypervisors before it can trick the hypervisor, but it also slows down the response to a real attack.
To mitigate this, Seawall incrementally sandboxes purported attackers with network packet lters to prevent them from sending more packets to their accusers.
is approach can provide a substantial security margin with existing datacenter switches.
ETTM [] is similar in attempting to push functionality towards the edges.
It leverages virtualization at the end hosts to implement NAT functionality among other things.
Unlike Seawall, it does not focus on performance isolation and is targeted at a diierent domain (branch oces and home networks) that lets it focus less on performance overheads and the possibility of hypervisor compromises.
SoUDC used hypervisor rate limiters to control the network utilization of diierent tenants within a shared datacenter [].
Seawall extends this work with an exploration of the design space given the constraints of deployed cloud datacenters.Recent work in hypervisor, network stack, and soware routers have shown that soware-based network processing, like that used in Seawall for monitoring and rate limiting, can be substantially more exible than hardware-based approaches, yet achieve high performance.
[] presents several soware optimizations of a hypervisor virtual switch and network stack to achieve comparable performance to direct I/O.
e Sun Crossbow network stack provides an arbitrary number of bandwidth-limited virtual NICs [].
Crossbow provides identical semantics regardless of underlying physical NIC and transparently leverages ooads to improve performance.
Seawall's sender-side rate controller can be incorporated into both of these network stacks.
is paper proposes Seawall, a scalable performance isolation system for cloud datacenter networks that fairly allocates network capacity between tenants; achieves elastic, network utilization; and is robust against malicious tenants.
Performance variation and service availability remain key concerns when deploying and maintaining cloud applications.
Since Seawall requires no special support from the network, it can be deployed in existing datacenters to improve both of these metrics.
