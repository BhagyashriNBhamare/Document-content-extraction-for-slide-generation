From small mobile devices to large-scale storage arrays, flash memory-based storage systems have gained a lot of popularity in recent years.
However, the uncoordinated use of resources by competing tasks in the flash translation layer (FTL) makes it difficult to guarantee predictable performance.
In this paper, we present AutoSSD, an autonomic SSD architecture that self-manages FTL tasks to maintain a high-level of QoS performance.
In AutoSSD, each FTL task is given an illusion of a dedicated flash memory subsystem , allowing tasks to be implemented oblivious to others and making it easy to integrate new tasks to handle future flash memory quirks.
Furthermore, each task is allocated a share that represents its relative importance, and its utilization is enforced by a simple and effective scheduling scheme that limits the number of outstanding flash memory requests for each task.
The shares are dynamically adjusted through feedback control by monitoring key system states and reacting to their changes to coordinate the progress of FTL tasks.
We demonstrate the effectiveness of AutoSSD by holistically considering multiple facets of SSD internal management, and by evaluating it across diverse work-loads.
Compared to state-of-the-art techniques, our design reduces the average response time by up to 18.0%, the 3 nines (99.9%) QoS by up to 67.2%, and the 6 nines (99.9999%) QoS by up to 76.6% for QoS-sensitive small reads.
Flash memory-based storage systems have become popular across a wide range of applications from mobile systems to enterprise data storages.
Flash memory's small size, resistance to shock and vibration, and low power consumption make it the de facto storage medium in mobile devices.
On the other hand, flash memory's low latency and collectively massive parallelism make flash storage suitable for high-performance storage for mission-critical applications.
As multi-level cell technology [5] and 3D stacking [38] continue to lower the cost per GB, flash storage will not only remain competitive in the data storage market, but also will enable the emergence of new applications in this Age of Big Data.Large-scale deployments and user experiences, however, reveal that despite its low latency and massive parallelism, flash storage exhibits high performance instabilities and variations [9,17].
Garbage collection (GC) has been pointed out as the main source of the problem [9,25,28,29,45], and Figure 1 illustrates this case.
It shows the performance degradation of our SSD model under small random writes, and it closely resembles measured results from commercial SSDs [2,23].
Initially, the SSD's performance is good because all the resources of the flash memory subsystem can be used to service host requests.
But as the flash memory blocks are consumed by host writes, GC needs to reclaim space by compacting data spread across blocks and erasing unused blocks.
Consequently, host and GC compete for resources, and the host performance inevitably suffers.However, garbage collection is a necessary evil for the flash storage.
Simply putting off space reclamation or treating GC as a low priority task will lead to larger performance degradations, as host writes will eventually block and wait for GC to reclaim space.
Instead, garbage collection must be judiciously scheduled with host requests to ensure that there is enough free space for future requests, while meeting the performance demands of current requests.
This principle of harmonious coexistence, in fact, extends to every internal management task.
Map caching [15] that selectively keeps mapping data in memory generates flash memory traffic on cache misses, but this is a mandatory step for locating host data.
Read scrubbing [16] that preventively migrates data before its corruption also creates traffic when blocks are repeatedly read, but failure to perform its duty on time can lead to data loss.
As more tasks with unique responsibilities are added to the system, it becomes increasingly difficult to design a system that meets its performance and reliability requirements [13].
In this paper, we present an autonomic SSD architecture called AutoSSD that self-manages its management tasks to maintain a high-level of QoS performance.
In our design, each task is given a virtualized view of the flash memory subsystem by hiding the details of flash memory request scheduling.
Each task is allocated a share that represents the amount of progress it can make, and a simple yet effective scheduling scheme enforces resource arbitration according to the allotted shares.
The shares are dynamically and automatically adjusted through feedback control by monitoring key system states and reacting to their changes.
This achieves predictable performance by maintaining a stable system.
We show that for small read requests, AutoSSD reduces the average response time by up to 18.0%, the 3 nines (99.9%) QoS by up to 67.2%, and the 6 nines (99.9999%) QoS by up to 76.6% compared to state-of-the-art techniques.
Our contributions are as follows:• We present AutoSSD, an autonomic SSD architecture that dynamically manages internal housekeeping tasks to maintain a stable system state.
( § 3)• We holistically consider multiple facets of SSD internal management, including not only garbage collection and host request handling, but also mapping management and read scrubbing.
( § 4)• We evaluate our design and compare it to the stateof-the-art techniques across diverse workloads, analyze causes for long tail latencies, and demonstrate the advantages of dynamic management.
( § 5)The remainder of this paper is organized as follows.
§ 2 gives a background on understanding why flash storages exhibit performance unpredictability.
§ 3 presents the overall architecture of AutoSSD and explains our design choices.
§ 4 describes the evaluation methodology and the SSD model that implements various FTL tasks, and § 5 presents the experimental results under both synthetic and real I/O workloads.
§ 6 discusses our design in relation to prior work, and finally § 7 concludes.
For flash memory to be used as storage, several of its limitations need to be addressed.
First, it does not allow in-place updates, mandating a mapping table between the logical and the physical address space.
Second, the granularities of the two state-modifying operationsprogram and erase-are different in size, making it necessary to perform garbage collection (GC) that copies valid data to another location for reclaiming space.
These internal management schemes, collectively known as the flash translation layer (FTL) [11], hide the limitations of flash memory and provide an illusion of a traditional block storage interface.The role of the FTL has become increasingly important as hiding the error-prone nature of flash memory can be challenging when relying solely on hardware techniques such as error correction code (ECC) and RAID-like parity schemes.
Data stored in the flash array may become corrupt in a wide variety of ways.
Bits in a cell may be disturbed when neighboring cells are accessed [12,41,44], and the electrons in the floating gate that represent data may gradually leak over time [6,35,44].
Sudden power loss can increase bit error rates beyond the error correction capabilities [44,47], and error rates increase as flash memory blocks wear out [6,12,19].
As flash memory becomes less reliable in favor of high-density [13], more sophisticated FTL algorithms are needed to complement existing reliability enhancement techniques.Even though modern flash storages are equipped with sophisticated FTLs and powerful controllers, meeting performance requirements have three main challenges.
First, as new quirks of flash memory are introduced, more FTL tasks are added to hide the limitations, thereby increasing the complexity of the system.
Furthermore, existing FTL algorithms need to be fine-tuned for every new generation of flash memory, making it difficult to design a system that universally meets performance requirements.
Second, multiple FTL tasks generate sequences of flash memory requests that contend for the resources of the shared flash memory subsystem.
This resource contention creates queueing delays that increase response times and causes long-tail latencies.
Lastly, depending on the state of the flash storage, the importance of FTL tasks dynamically changes.
For example, if the flash storage runs out of free blocks for writing host data, host request handling stalls and waits for garbage collection to reclaim free space.
On the other hand, with sufficient free blocks, there is no incentive prioritizing garbage collection over host request handling.
In this section, we describe the overall architecture and design of the autonomic SSD (AutoSSD) as shown in Figure 3.
In our model, all FTL tasks run concurrently, with each designed and implemented specifically for its job.
Each task independently interfaces the scheduling subsystem, and the scheduler arbitrates the resources in the flash memory subsystem according to the assigned share.
The share controller monitors key system states and determines the appropriate share for each FTL task.
AutoSSD is agnostic to the specifics of the FTL algorithms (i.e., mapping scheme and GC victim selection), and the following subsections focus on the overall architecture and design that enable the self-management of the flash storage.
The architecture of AutoSSD allows each task to be independent of others by virtualizing the flash memory subsystem.
Each FTL task is given a pair of request and response queues to send and receive flash memory requests and responses, respectively.
This interface provides an illusion of a dedicated (yet slower) flash memory subsystem and allows an FTL task to generate flash memory requests oblivious of others (whether idle or active) or the requests they generate (intensity or which resources they are using).
Details of the flash memory subsystem are completely abstracted by the scheduling subsystem, and only the back-pressure of the queue limits each task from generating more flash memory requests.
This virtualization not only effectively frees each task from having to worry about others, but also makes it easy to add a new FTL task to address any future flash memory quirks.
While background operations such as garbage collection, read scrubbing, and wear leveling have similar flash memory workload patterns (reads and programs, and then erases), the objective of each task is distinctly different.
Garbage collection reclaims space for writes, read scrubbing preventively relocates data to ensure data integrity, and wear leveling swaps contents of data to even out the damage done on flash memory cells.
Our design allows seamless integration of new tasks without having to modify existing ones and reoptimize the system.
The scheduling subsystem interfaces with each FTL task and arbitrates the resources of the flash memory subsystem.
The scheduler needs to be efficient with low overhead as it manages concurrency (tens and hundreds of flash memory requests) and parallelism (tens and hundreds of flash memory chips) at a small timescale.In AutoSSD, we consider these unique domain characteristics and arbitrate the flash memory subsystem resource through debit scheduling.
The debit scheduler tracks and limits the number of outstanding requests per task across all resources, and is based on the request windowing technique [14,20,34] from the disk scheduling domain.
If the number of outstanding requests for a task, which we call debit, is under the limit, its requests are eligible to be issued; if it's not, the request cannot be issued until one or more of requests from that task completes.
The debt limit is proportional to the share set by the share controller, allowing a task with a higher share to potentially utilize more resources simultaneously.
The sum of all tasks' debt limit represents the total amount of Figure 3a, no more requests can be sent to Chip 0, and Task B is at its maximum debit.
The only eligible scheduling is issuing Task A's request to Chip 2.
In the scenario of Figure 3b, while Task A is still under the debt limit, its request cannot be issued to a chip with a full queue.
On the other hand, a request from Task B can be issued as Chip 3's operation for Task B completes.parallelism, and is set to the total number of requests that can be queued in the flash memory controller.
Figure 3 illustrates two scenarios of the debit scheduling.
In both scenarios, the debt limit is set to 5 requests for Task A, and 3 for Task B.
In Figure 3a, no more requests can be sent to Chip 0 as its queue is full, and Task B's requests cannot be scheduled as it is at its debt limit.
Under this circumstance, Task A's request to Chip 2 is scheduled, increasing its debit value from 1 to 2.
In Figure 3b, the active operation at Chip 3 for Task B completes, allowing Task B's request to be scheduled.
Though Task B's request to Chip 1 is not at the head of the queue, it is scheduled out-of-order as there is no dependence between the requests to Chip 0 and Chip 1.
Task A, although below the debt limit, cannot have its requests issued until Chip 0 finishes a queued operation, or until a new request to another chip arrives.
Though not illustrated in these scenarios, when multiple tasks under the limit compete for the same resource, one is chosen with skewed randomness favoring a task with a smaller debit to debt limit ratio.
Randomness is added to probabilistically avoid starvation.Debit scheduling only tracks the number of outstanding requests, yet exhibits interesting properties.
First, it can make scheduling decisions without complex computations and bookkeeping.
This allows the debit scheduler to scale with increasing number of resources.
Second, although it does not explicitly track time, it implicitly favors short latency operations as they have a faster turn-around-time.
In scheduling disciplines such as weighted round robin [26] and weighted fair queueing (WFQ) [10], the latency of operations must be known or estimated to achieve some degree of fairness.
Debit scheduling, however, approximates fairness in the time-domain only by tracking the number of outstanding requests.
Lastly, the scheduler is in fact not workconserving.
The total debt limit can be scaled up to approximate a work-conserving scheduler, but the sharebased resource reservation of the debit scheduler allows high responsiveness, as observed in the resource reservation protocol for Ozone [36].
The share controller determines the appropriate share for the scheduling subsystem by observing key system states.
States such as the number of free blocks and the maximum read count reflect the stability of the flash storage.
This is critical for the overall performance and reliability of the system, as failure to keep these states at a stable level can lead to an unbounded increase in response time or even data loss.For example, if the flash storage runs out of free blocks, not only do host writes block, but also all other tasks that use flash memory programs stall: activities such as making mapping data durable and writing periodic checkpoints also depend on the garbage collection to reclaim free space.
Even worse, a poorly constructed FTL may become deadlocked if GC is unable to obtain a free block to write the valid data from its victim.
On the other hand, if a read count for a block exceeds its recommended limit, accumulated read disturbances can lead to data loss if the number of errors is beyond the error correction capabilities.
In order to prevent falling into these adverse system conditions, the share controller monitors the system states and adjusts shares to control the rate of progress for individual FTL tasks, so that the system is maintained within stable levels.AutoSSD uses feedback to adaptively determine the shares for the internal FTL tasks.
While the values of key system states must be maintained at an adequate level, the shares of internal tasks must not be set too high such that they severely degrade the host performance.
Once a task becomes active, it initially is allocated a small share.
If this fails to maintain the current level of the system state, the share is gradually increased to counteract the momentum.
The following control function is used to achieve this behavior:S A [t] = P A · e A [t] + I A · S A [t − 1](1)Where S A [t] is the share for task A at time t, S A [t − 1] is the previous share for task A, P A and I A are two nonnegative coefficients for task A, and e A [t] is the error value for task A at time t.
The error value function for GC is defined as follows:e GC [t] = max(0,target f reeblk − num f reeblk [t])(2)With target f reeblk set to the GC activation threshold, the share for GC S GC starts out small.
If the number of free blocks num f reeblk [t] falls far below target f reeblk , the error function e GC [t] augmented by P GC ramps up the GC share S GC .
After the number of free blocks num f reeblk [t] exceeds the threshold target f reeblk , the share S GC slowly decays given I GC < 1.
Addition to the GC share control, the error value function for read scrubbing (RS) is defined as follows:e RS [t] = max(0, max i∈blk (readcnt i [t]) − target readcnt ) (3)Where max i∈blk (readcnt i [t]) is the maximum read count across all blocks in the system at time t, and target readcnt is the RS activation threshold.In our design, the share for internal management schemes starts out small, anticipating host request arrivals and using the minimum amount of resources to perform its task.
If the system state does not recover, the error (the difference between the desired and the actual system state values) accumulates, increasing the share over time.It is important to note that the progress rate for a task depends not only on the share, but also on the workload, algorithm, and system state.
For example, the number of valid data in the victim block, the location of the mapping data associated with the valid data, and the access patterns at the flash memory subsystem all affect the rate of progress for GC.
A task's progress rate is, in fact, nonlinear to the share under real-world workloads, and computationally solving for the optimal share involves large overhead, if not impossible.
As a result, the two coefficients P and I for FTL tasks are empirically hand-tuned in this work.
We model a flash storage system on top of the DiskSim environment [1] by enhancing its SSD extension [3].
In this section, we describe the various components and configuration of the SSD, and the workload and test settings used for the evaluation.
Flash memory controller is based on Ozone [36] that fully utilizes flash memory subsystem's channel and chip parallelism.
There can be at most four requests queued to each chip in the controller.
Increasing this queue depth does not significantly increase intra-chip parallelism, as cached operations of flash memory have diminishing benefits as the channel bandwidth increases.
Instead, a smaller queue depth is chosen to increase the responsiveness of the system.
Table 1 summarizes the default flash storage configuration used in our experiments.
Of the 256GB of physical space, 200GB is addressable by the host system, giving an over-provisioning factor of 28%.
We implement core FTL tasks and features that are essential for storage functions, yet cause performance variations.
Garbage collection reclaims space, but it degrades the performance of the system under host random writes.
Read scrubbing that preventively relocates data creates background traffic on read-dominant workloads.
Mapping table lookup is necessary to locate host data, but it increases response time on map cache misses.Mapping.We implement an FTL with map caching [15] and a mapping granularity of 4KB.
The entire mapping table is backed in flash, and mapping data, also maintained at the 4KB granularity, is selectively read into memory and written out to flash during runtime.
The LRU policy is used to evict mapping data, and if the victim contains any dirty mapping entries, the 4KB mapping data is written to flash.
By default, we use 128MB of memory to cache the mapping table.
The second-level mapping that tracks the locations of the 4KB mapping Host request handling.
Upon receiving a request, the host request handler looks up the second-level mapping to locate the mapping data that translates the host logical address to the flash memory physical address.
If the mapping data is present in memory (hit), the host request handler references the mapping data and generates flash memory requests to service the host request.
If it is a miss, a flash memory read request to fetch the mapping data is generated, and the host request waits until the mapping data is fetched.
Host requests are processed in a non-blocking manner; if a request is waiting for the mapping data, other requests may be serviced out-of-order.
In our model, if the host write request is smaller than the physical flash memory page size, multiple host writes are aggregated to fill the page to improve storage space utilization.
We also take into consideration of the mapping table access overhead and processing delays.
Mapping table lookup delay is set to be uniformly distributed between 0.5µs and 1µs, and the flash memory request generation delay for the host task is between 1µs and 2µs.
Garbage collection.
The garbage collection (GC) task runs concurrently and independently from the host request handler and generates its own flash memory requests.
Victim blocks are selected based on costbenefit [40].
Once a victim block is selected, valid pages are read and programmed to a new location.
Mapping data is updated as valid data is copied, and this process may generate additional requests (both reads and programs) for mapping management.
Once all the valid pages have been successfully copied, the old block is erased and marked free.
GC becomes active when the number of free blocks drops below a threshold, and stops once the number of free blocks exceeds another threshold, similar to the segment cleaning policy used for the log-structured file system [40].
In our experiments, the two threshold values for GC activation and deactivation are set to 128 and 256 free blocks, respectively.
The garbage collection task also has a request generation delay, set to be uniformly distributed between 1µs and 3µs.
Read scrubbing.
The read scrubbing (RS) task also runs as its own stand-alone task.
Victims are selected greedily based on the read count of a block: the block with the most number of reads is chosen.
Other than that, the process of copying valid data is identical to that of the garbage collection task.
RS becomes active when the maximum read count of the system goes beyond a threshold, and stops once it falls below another threshold.
The default threshold values for the activation and deactivation are set to 100,000 and 80,000 reads, respectively.
Like the garbage collection task, the request generation delay (modeling the processing overhead of read scrubbing) is uniformly distributed between 1µs and 3µs.
We use both synthetic workloads and real-world I/O traces from Microsoft production servers [27] to evaluate the autonomic SSD architecture.
Synthetic workloads of 128KB sequential accesses, 4KB random reads, and 4KB random read/writes are used to verify that our model behaves expectedly according to the system parameters.From the original traces, the logical address of each host request is modified to fit into the 200GB range, and all the accesses are aligned to 4KB boundaries.
All the traces are run for their full duration, with some lasting up to 24 hours and replaying up to 44 million I/Os.
The trace workload characteristics are summarized in Table 2.
Prior to each experiment, the entire physical space is randomly written to emulate a pre-conditioned state so that the storage would fall under the steady state performance described in SNIA's SSS-PTS [2].
Furthermore, each block's read count is initialized with a non-negative random value less than the read scrubbing threshold to emulate past read activities.
Figure 4a shows the total bandwidth under 128KB sequential accesses with respect to changes in the channel bandwidth.
Figure 4b shows the performance (average response time and 3 nines QoS) and the utilization of the flash memory subsystem with respect to changes in the size of the in-memory map cache.
Figure 4c shows the performance (3 nines and 6 nines QoS) and the GC progress rate with respect to the GC share.
This section presents experimental results under the configuration and workload settings described in the previous section.
The main performance metric we report is the system response time seen at the I/O device driver.
We first validate our SSD model using synthetic workloads, and then present experimental results with I/O traces.
We replayed the I/O traces with the original request dispatch times, and with the dispatch times scaled down to increase the workload intensity.
Figure 4 illustrates the performance of the autonomic SSD architecture (AutoSSD) with debit scheduling under four micro-benchmarks.
Figure 4a plots the total bandwidth under 128KB sequential reads and 128KB sequential writes as we increase the channel bandwidth.
As the channel bandwidth increases, the flash memory operation latency becomes the performance bottleneck.
Write performance saturates early as the program latency cannot be hidden with data transfers.
At 1000MB/s channel bandwidth, the read operation latency also becomes the bottleneck, unable to further extract bandwidth from the configured four channels.
Traffic from GC and mapping management has a negligible effect for large sequential accesses, and RS task was disabled for this run to measure maximum raw bandwidth.
In Figure 4b, we vary the in-memory map cache size and measure the response times of 4KB random read requests when issued at 100K I/Os per second (IOPS).
As expected, the response time is the smallest when the entire map is in memory, as it does not generate map read requests once the cache is filled after cold-start misses.
However, as the map cache becomes smaller, the response time for host reads increases not only because it probabilistically stalls waiting for map reads from flash memory, but also due to increased flash memory traffic, which causes larger queueing delays.
Lastly, we demonstrate that the debit scheduling mechanism exerts control over FTL tasks in Figure 4c.
In this scenario, 4KB random read/write requests are issued at 20K IOPS with a 1-to-9 read/write ratio.
Both the response time of host read requests and GC task's progress (in terms of the number of erases per second while active) are measured at fixed GC shares from 20% to 80%.
As shown by the bar graph, more blocks are erased as the share for GC increases.
Furthermore, with more GC share, the overall host performance suffers, as evident by the increase in the 3 nines QoS.
Deceptively, however, assigning not enough share to GC will result in worse tail latency as shown by the 6 nines QoS.
GC needs to produce sufficient number of free blocks for the host to consume, and failure to do so will cause the host to block.
Using I/O traces, we evaluate the performance of AutoSSD and compare it to following three systems:Vanilla represents a design without virtualization and coordination, and all tasks dispatch requests to the controller through a single pair of request/response queue.
As the focus of this paper is response time characteristics, we only measure the performance of QoS-sensitive small reads (no larger than 64KB) in terms of the average response time, the 3 nines (99.9%) QoS figure, and the 6 nines (99.9999%).
Figure 5 compares the performance of the four systems under eight different traces.
Compared to RAIN, AutoSSD reduces the average response time by up to 18.0% under MSN-BEFS as shown in Figure 5a.
For the 3 nines QoS, AutoSSD shows improvements across most workloads, reducing it by 53.6% on average and as much as by 67.2% under RAD-AS (see Figure 5b).
For the 6 nines QoS, AutoSSD shows much greater improvements, reducing it as much as by 76.6% under RAD-AS (see Figure 5c).
Without coordination among FTL tasks, the Vanilla performance suffers, especially for the long tail latencies.
In terms of the 6 nines, AutoSSD performs well under bursty workloads such as RAD-AS and LM-TBE (large difference between average and median inter-arrival times in Table 2).
This is because AutoSSD limits the progress of internal FTL tasks depending on the state of the system, making resources available for the host in a non-work-conserving manner.
This is in contrast to the scheduling disciplines used by the other systems: Vanilla uses FIFO scheduling; RAIN, priority scheduling; and QoSFC, weighted fair queueing.
To better understand the overall results in Figure 5, we microscopically examine the performance under RAD-AS in Figure 6 and LM-TBE in Figure 7.
Figure 6a shows the average response time of three systems-RAIN, QoSFC, and AutoSSD-during a 10-second window, approximately 10 hours into RAD-AS.
GC is active during this window for all the three systems, and both RAIN and QoSFC exhibit large spikes in response time.
On the other hand, AutoSSD is better able to bound the performance degradation caused by an active garbage collection.
Figure 6b shows the number of free blocks and the GC share during that window for AutoSSD.
The sawtooth behavior for the number of free blocks is due to host requests consuming blocks, and GC gradually reclaiming space.
GC share is reactively increased when the number of free blocks becomes low, thereby increasing the rate at which GC produces free blocks.
If the number of free blocks exceeds the GC activation threshold, the share decays gradually to allow other tasks to use more resources.
In effect, AutoSSD improves the overall response time as shown in Figure 6c.
For LM-TBE, Figure 7a shows the average response time of the three systems during a 20-second window, approximately 15 hours into the workload.
Here we observe read scrubbing (RS) becoming active due to the read-dominant characteristics of LM-TBE.
We observe that both RAIN and QoSFC show large spikes in response time that lasts longer than the perturbation caused by GC for RAD-AS (cf. Figure 6a).
While GC is incentivized to select a block with less valid data, RS is likely to pick a block with a lot of valid data that are frequently read but not frequently updated: this causes the performance degradation induced by RS to last longer than that by GC.
AutoSSD limits this effect, while still decreasing the maximum read count in the system by dynamically adjusting the share of RS, as shown in Figure 7b.
Figure 7c shows the response time CDF of the three systems.
Figure 8 illustrates the delay causes for the flash memory requests generated by the host request handling task under MSN-BEFS.
Note that this is different from the response time of host requests: this shows the average wait time that a flash memory request (for servicing the host) experiences, broken down by different causes.
Category Figure 6a shows the average response time sampled at 100ms in the selected 10-second window.
Figure 6b shows the number of free blocks and the GC share of AutoSSD for the same 10-second window.
Figure 6c plots the response time CDF for the entire duration.
Flash represents flash memory latency, combining both flash array access latency and data transfers.
Sched is the time spent waiting to be scheduled, either waiting in the queue because the target queue is full, or waiting because the scheduler limits the progress in a non-workconserving manner (the case for AutoSSD).
The large Sched wait time for Vanilla is caused by uncoordinated sharing of resources, while that for AutoSSD is small as the scheduler reserves resources for host requests.
The remaining five categories are delays experienced due to resource blocking.
Most noticeably, the wait time caused by GC in RAIN is higher than the other systems.
When RAIN generates alternate flash memory requests to reconstruct data through parity, these additional requests can, in turn, be blocked again at another resource.
In ttFlash [45], this problem is overcome by statically limiting the number of active GC to one per parity group.
This technique is not used in our evaluation as a fixed cap on the number of allowed GC can quickly deplete free blocks, especially for high-intensity small random write workloads.
Next, we examine the effectiveness of the dynamic share assignment over the static ones.
Figure 9 shows the response time CDF of AutoSSD under MSN-BEFS with static shares of 5%, 10%, and 20% for GC, along with the share controlled dynamically.
As illustrated by the gray lines, decreasing the GC share from 20% to 10% improves the overall performance.
However, when further reducing the GC share to 5%, we observe that the curve for 5% dwindles as it approaches higher QoS and performs worse than the 10% curve.
This indicates that while a lower GC share achieves better performance at lower QoS levels, a higher GC share is desirable to reduce long-tail latencies as it generates free blocks at a higher rate, preventing the number of free blocks from becoming critically low.
This observation is in accordance with the performance under synthetic workload in Figure 4c.
Using feedback control to adjust the GC share dynamically shows better performance over all the static values, as it can adapt to an appropriate share value by reacting to the changes in the system state.
In this subsection, we present experimental results with higher request intensities.
Here, the request dispatch times are reduced in half, but other parameters such as the access type and the target address remain unchanged.
This experiment is intended to examine the performance of the four systems-Vanilla, RAIN, QoSFC, and AutoSSD-under a more stressful scenario.
Figure 10 compares the performance in the new setting.
AutoSSD reduces the average response time by up to 24.6% under MSN-BEFS (see Figure 10a), the 3 nines QoS figure by 48.6% on average and as much as 70.6% under MSN-CFS (see Figure 10b), and the 6 nines QoS figure by as much as 55.3% under MSN-CFS (see Fig- ure 10c).
With workload intensity increased, the overall improvement in long tail latency decreases due to a smaller wiggle room for AutoSSD to manage FTL tasks.
This is especially true for high-intensity workloads such as MSN-BEFS: with host requests arriving back-to-back (cf. halve the inter-arrival time in Table 2), debit scheduling has little advantage over other scheduling schemes.
However, AutoSSD nevertheless outperforms prior techniques across the diverse set of workloads.
Workloads such as RAD-AS and LM-TBE that showed the most reduction in long tail latency under the original intensity (cf. Figure 5c) still exhibit performance improvements with AutoSSD in the 6 nines, even with increased workload intensity.We examine DTRS more closely in Figure 11.
Fig- ure 11a shows the average response time of the three systems-RAIN, QoSFC, and AutoSSD-during a 20-second window, approximately 2 hours into the workload.
GC is active during this window for all the three systems, and AutoSSD is better able to bound the performance degradation caused by an active garbage collection, while both RAIN and QoSFC exhibit large spikes in response time.
Figure 11b shows the number of free blocks and the GC share during that window for AutoSSD.
Similar to the results in the previous section, the share for GC reactively increases at a lower number of blocks, and decays once the number of free blocks reaches a stable region.
Again, the number of free blocks shows a sawtooth behavior, and the ridges of GC share curve matches the valleys of the free block curve.
Fig- ure 11c plots the response time CDF of the three systems, demonstrating the effectiveness of our dynamic management.
There are several studies on real-time performance guarantees of flash storage, but they depend on RTOS support [7], specific mapping schemes [8,39,46], a number of reserve blocks [8,39], and flash operation latencies [46].
These tight couplings make it difficult to extend performance guarantees when system requirements Figure 11a shows the average response time sampled at 100ms in the selected 20-second window.
Figure 11b shows the number of free blocks and the GC share of AutoSSD for the same 20-second window.
Figure 11c plots the response time CDF for the entire duration.
and flash memory technology change.
On the other hand, our architecture is FTL implementation-agnostic, allowing it to be used across a wide range of flash devices and applications.
Some techniques focus on when to perform GC (based on threshold [31], slack [22], or host idleness [25,37]).
These approaches complement our design that focuses on the fine-grained scheduling and dynamic management of multiple FTL tasks running concurrently.
By incorporating workload prediction techniques to our design, we can extend AutoSSD to increase the share on background tasks when host idleness is expected, and decrease it when host requests are anticipated.Exploiting redundancy to reduce performance variation has been studied in a number of prior art.
Harmonia [30] and Storage engine [42] duplicate data across multiples SSDs, placing one in read mode and the other in write mode to eliminate GC's impact on read performance.
ttFlash [45] uses multiple flash memory chips to reconstruct data through a RAID-like parity scheme.
Relying on redundancy effectively reduces the storage utilization, but otherwise complements our design of dynamic management of various FTL tasks.Performance isolation aims to reduce performance variation caused by multiple hosts through partitioning resources (vFlash [43], FlashBlox [18]), improving GC efficiency by grouping data from the same source (Multistreamed [24], OPS isolation [29]), and penalizing noisy neighbors (WA-BC [21]).
These performance isolation techniques are complementary to our approach of finegrained scheduling and dynamic management of concurrent FTL tasks.The design of the autonomic SSD architecture borrows ideas from prior work on shared disk-based storage systems such as Façade [33], PARDA [14], and Maestro [34].
These systems aim to meet performance requirements of multiple clients by throttling request rates and dynamically adjusting the bound through a feedback control.
However, while these disk-based systems deal with fair sharing of disk resources among multiple hosts, we address the interplay between the foreground (host I/O) and the background work (garbage collection and other management schemes).
Aqueduct [32] and Duet [4] address the performance impact of background tasks such as backup and data migration in disk-based storage systems.
However, background tasks in flash storage are triggered at a much smaller timescale, and SSDs uniquely create scenarios where the foreground task depends on the background task, necessitating a different approach.
In this paper, we presented the design of an autonomic SSD architecture that self-manages concurrent FTL tasks in the flash storage.
By judiciously coordinating the use of resources in the flash memory subsystem, the autonomic SSD manages the progress of concurrent FTL tasks and maintains the internal system states of the storage at a stable level.
This self-management prevents the SSD from falling into a critical condition that causes long tail latency.
In effect, AutoSSD reduces the average response time by up to 18.0%, the 3 nines (99.9%) QoS by up to 67.2%, and the 6 nines (99.9999%) QoS by up to 76.6% for QoS-sensitive small reads.
We thank the anonymous reviewers for their constructive and insightful comments, and also thank Jaejin Lee, Jongmoo Choi, Hyeonsang Eom, and Eunji Lee for reviewing the early stages of this work.
This work was supported in part by SK Hynix and the National Research Foundation of Korea under the PF Class Heterogeneous High Performance Computer Development (NRF-2016M3C4A7952587).
Institute of Computer Technology at Seoul National University provided the research facilities for this study.
