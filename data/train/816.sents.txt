Explicit abstraction heuristics, notably pattern-database and merge-and-shrink heuristics, are employed by some state-of-the-art optimal heuristic-search planners.
The major limitation of these abstraction heuristics is that the size of the abstract space has to be bounded by a (large) constant.
Targeting this issue, Katz and Domshlak (2008b) introduced structural, and in particular fork-decomposition, abstractions, in which the planning task is abstracted by an instance of a tractable fragment of optimal planning.
At first view, however, the lunch was not free.
Some of the power of the explicit abstraction heuristics comes from pre-computing the heuristic function offline, and then determine h(s) for each evaluated state s by a very fast lookup in a "database".
In contrast, fork-decomposition offer a poly-time, yet far from being fast, computation.
In this contribution, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome.
Specifically, we show that an equivalent of the explicit abstractions' notion of "database" exists for the fork-decomposition abstractions as well, and this despite of their exponential-size abstract spaces.
Experimentally , we show that heuristic search with such "databased" fork-decomposition heuristics favorably competes with the state-of-the-art of optimal planning.
Heuristic search is nowadays one of the leading approaches to cost-optimal planning.
The difference between various cost-optimizing heuristic-search planners is mainly in the employed admissible heuristic functions.
In state-space search, such a heuristic estimates the cost of achieving the goal from a given state, and being admissible, it guarantees not to overestimate that cost.
During the last decade, numerous computational ideas evolved into new admissible heuristics for domain-independent planning; this includes deleterelaxing max heuristic h max (Bonet & Geffner 2001), critical path heuristics h m (Haslum & Geffner 2000), landmark heuristics h L and h LA (Karpas & Domshlak 2009), and abstraction heuristics such as pattern-database (Edelkamp 2001), merge-and-shrink (Helmert, Haslum, & Hoffmann 2007), and structural-pattern (Katz & Domshlak 2008b) heuristics.
In addition, these heuristics have been combined via additive ensembles of admissible heuristics (see, e.g., Edelkamp (2001), Haslum et al. (2005), Katz & Domsh- lak (2008a), Karpas & Domshlak (2009)).
Apart from being as informative as possible, the heuristic should also be computable in polynomial time.
In fact, the latter is a necessary, yet, in practice, insufficient condition.
If any reasonable time cap is put on the planner, the heuristic computation per search node has to be really fast.
For many problems, this has to be the case even if the heuristic is perfect, and this because of a large number of states evaluated along the optimal path.
This well-known issue can be exemplified by the results of the cost-optimal planning track of the IPC-2008 competition in which even the bestperforming heuristic search planner HSP * F (Haslum 2008) solved less problems within the time limit of 30 minutes than the baseline breadth-first search.
With this "heuristic selection" criterion of the fast per-search-node computation in mind, here we focus on abstraction heuristics, and most closely, on structural-pattern heuristics.Probably the most well-known abstraction heuristics are pattern database (PDB) heuristics that are based on projecting the planning task onto a subset of its state variables and then explicitly searching for optimal plans in the abstract space.
The limitation of PDB heuristics in practice is that both the size of the abstract space and its dimensionality have to be fixed.
1 The more recent, merge-and-shrink abstractions, generalize PDB heuristics to overcome the latter limitation; instead of perfectly reflecting just a few state variables, merge-and-shrink abstractions allow for imperfectly reflecting all variables.
The abstract space of mergeand-shrink is still searched explicitly, and thus it still has to be of fixed size.Targeting both limitations of the PDB abstractions, Katz and Domshlak (2008b) introduced the framework of structural patterns in which the planning task is abstracted to a tractable fragment of optimal planning.
In the extreme case where the problem itself belongs to that fragment, nothing will be abstracted at all, and the perfect heuristic will be computed in polynomial time.
Katz and Domshlak in- troduced a concrete instance of the structural patterns idea, called fork decomposition, in which the problem is projected (in a proper meaning of this term) onto fork and inverted fork subgraphs of the problem's causal graph.
The promise of fork-decomposition has been shown via a formal asymptotic performance analysis on selected domains, but the empirical relevance has yet to be shown.Examining the empirical effectiveness of the forkdecomposition heuristics, we have implemented and evaluated them using A * on numerous IPC domains.
On many domains, the quality of the fork-decomposition heuristics appeared to be very encouraging, to say the least.
However, in all domains we pretty quickly bumped into the wall of the per-node computation complexity.
While the planner was able to solve many challenging problems within a standard memory cap, it was typically running out of reasonable time limits.
Surprising it was not.
While pattern database and merge-and-shrink heuristics are also computationally prohibitive, they allow for pre-computing the heuristic values for all abstract states simultaneously, storing them in a lookup table, and then reusing that pre-search computation between search states.
In contrast, the abstract spaces induced by structural patterns can be as large as the original state space, and thus they cannot be pre-solved and prestored exhaustively.Overall, while the empirical (in)effectiveness of forkdecomposition was not surprising, given the good quality of its induced estimates, disappointing it was.
Here, however, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome.
Specifically, we show that an equivalent of the PDB's and merge-and-shrink's notion of "database" exists for the fork-decomposition abstractions as well, and this despite of their exponential-size abstract spaces.
In fact, the same effect could possibly be achieved also for other (to be developed in the future) structural-pattern heuristics.Unlike for PDBs and merge-and-shrink abstractions, structural-pattern databases (and in particular these for the fork-decomposition's fork and inverted fork abstractions) do not (and cannot) result in purely lookup computations of h(s).
In contrast, structural-pattern databases are based on a proper partition of heuristic computation (h-partition) into parts that can be shared between search states, and parts that shall be computed per individual state.
We prove existence of such effective h-partitions for both fork and inverted fork abstractions.
We then formally and empirically show that these h-partitions lead to fast pre-search and pernode computations that allow for exploiting the informativeness of the fork-decomposition heuristics in practice.
Experimentally, we show that A * equipped with the "databased" fork-decomposition heuristics favorably competes with the state-of-the-art of cost-optimal planning.
We consider planning problems captured by the SAS + formalism (Bäckström & Nebel 1995) with non-negative action costs.
Such a problem is given by a quintuple Π = V, A, I, G, c, where:• V is a set of state variables, each v ∈ V is associated with a finite domain D(v); the value provided to a variable v by a (possibly partial) assignment p to V is denoted by p [v].
Each complete assignment to V is called a state, and S = D(V ) is the state space of Π.
I is an initial state.
The goal G is a partial assignment to V ; a state s is a goal state iff G ⊂ s.• A is a finite set of actions.
Each action a is a pair pre(a), eff(a) of partial assignments to V called preconditions and effects, respectively.
By A v ⊆ A we denote the actions affecting the value of v. c : A → R + 0 is a real-valued, non-negative action cost function.An action a is applicable in a state s iff s[v] = pre(a) [v] whenever pre(a) [v] is specified.
Applying a changes the value of v to eff(a) [v] if eff(a) [v] is specified.
The resulting state is denoted by sa; by sa 1 , . . . , a k we denote the state obtained from sequential application of the (respectively applicable) actions a 1 , . . . , a k starting at state s.
Such an action sequence is an s-plan if G ⊆ sa 1 , . . . , a k , and it is a cost-optimal (or, in what follows, optimal) s-plan if the sum of its action costs is minimal among all s-plans.
The purpose of (optimal) planning is finding an (optimal) I-plan.
For a pair of states s 1 , s 2 ∈ S, by c(s 1 , s 2 ) we refer to the cost of a cheapest action sequence taking us from s 1 to s 2 in the transition system induced by Π; h * (s) = min s ⊇G c(s, s ) is the custom notation for the cost of the optimal s-plan in Π.
We also use two well-known graphical structures induced by a planning task Π with variables V .
• The causal graph CG(Π) of Π is a digraph over nodes V .
An arc (v, v ) is in CG(Π) iff v = v and there exists an action a ∈ A such that both eff(a) [v ] and either pre(a) [v] or eff(a) [v] are specified.
Our focus here is on state-dependent, admissible abstraction heuristics.
A heuristic is state-dependent if its estimate for a search node depends only on the problem state associated with that node, that is, h : S → R + 0 ∪ {∞}.
Most heuristics in use these days are state-dependent (though see, e.g., Richter et al. (2008) for a different case).
A statedependent heuristic h is admissible if h(s) ≤ h * (s) for all states s. Finally, an abstraction heuristic is based on mapping Π's transition system over states S to an abstract transition system over states S α .
The mapping is defined by an abstraction function α : S → S α that guarantees c α (α(s), α(s )) ≤ c(s, s ) for all states s, s ∈ S.
The (admissible) abstraction heuristic h α (s) is then the distance from α(s) to the closest abstract goal state.
Accuracy and low time complexity are both desired yet competing properties of heuristic functions.
For many powerful heuristics, and in particular abstraction heuristics, computing h(s) for each state s in isolation is infeasible-while computing h(s) is polynomial in the description size of Π, it is often not efficient enough to be performed at each search node.
Fortunately, for some heuristics this obstacle can be overcome to a large extent by sharing most of the computation between the evaluations of h on different states.
If that is possible, the shared parts of computing h(s) for all problem states s are pre-computed and memorized before the search, and then reused during the search by the evaluations of h on different states.
This mixed offline/online heuristic computation, called henceforth h-partition, is adopted by various optimal planners using backward critical-path heuristics h m for m > 1 (Haslum, Bonet, & Geffner 2005), landmark heuristics h L and h LA (Karpas & Domshlak 2009), and abstraction pattern-database and merge-and-shrink heuristics (Edelkamp 2001;Helmert, Haslum, & Hoffmann 2007).
Without an effective h-partition, optimal search with these heuristics would not scale up well, while with such an hpartition these heuristics constitute the state of the art of heuristic search planning.We define the time complexity of an h-partition as the complexity of computing h for a set of states.
Given a subset of k problem states S ⊆ S, the h-partition's time complexity of computing {h(s) | s ∈ S } is expressed as O(X + kY ), where O(X) and O(Y ) are, respectively, the complexity of the (offline) pre-search and (online) per-node parts of computing h(s) for a state s ∈ S .
As we briefly mentioned above, an abstraction heuristic for a problem Π maps Π's transition system over states S to an abstract transition system over states S α .
The mapping is defined by an abstraction function α : S → S α that guarantees c α (α(s), α(s )) ≤ c(s, s ) for all states s, s ∈ S.
Such "distance conservancy" is in particular guaranteed by homomorphism abstractions, obtained (only) by systematically contracting groups of states into abstract states.
The abstraction heuristic h α (s) is the distance from α(s) to the closest abstract goal state, and admissibility of h α is implied by the distance conservancy of α.The most well-studied abstraction heuristics are pattern database (PDB) heuristics.
Given a planning task Π over state variables V , such a heuristic is based on projecting Π onto a subset of its variables V α ⊆ V .
Such a homomorphism abstraction α maps two states s 1 , s 2 ∈ S into the same abstract state iffs 1 [V α ] = s 2 [V α ].
Inspired by the (similarly named) domain-specific heuristics for search problems such as (k 2 −1)-puzzles, Rubik's Cube, etc. (Culberson & Schaeffer 1998;Hernadvölgyi & Holte 1999;Felner, Korf, & Hanan 2004), PDB heuristics have been successfully exploited in domain-independent planning (Edelkamp 2001;2002;Haslum et al. 2007).
Apart from the need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space and its dimensionality.
First, the number of abstract states should be small enough to allow reachability analysis in S α by exhaustive search, and thus we must have |S α | = O(1).
The bound on |S α | is typically set explicitly given the time and memory limitations of the system.
Second, since PDB abstractions are projections, the explicit constraint on |S α | implies a fixed-dimensionality constraint on the abstract space, that is, |V α | = O(1).
In problems with, informally, many alternative resources, this limitation is a pitfall.
For instance, suppose {Π i } is a sequence of Logistics problems of growing size with |V i | = i.
If each package in Π i can be transported by some Θ(i) vehicles, then starting from some j, h α will not account at all for movements of vehicles essential for solving Π j (Helmert & Mattmüller 2007).
Note that, while the first limitation of projection abstractions can be overcome to a certain extent with additive ensembles of numerous projections, the dimensionality limitation remains an issue in additive PDBs as well.
Within these limitations, however, a very attractive side of PDB abstractions is the complexity of their natural h α -partition.
Instead of computing h α (s) = h * (α(s)) from scratch for each evaluated state s (which would not be practical for all but tiny projections), the practice is to pre-compute and store h * (α(s)) for all abstract states α(s) ∈ S α , and then the pernode computation of h α (s) boils down to hash-table lookup with a perfect hash function.
In our terms, the time and space complexity of that PDB h α -partition for a set of k states isO (|S α | (log (|S α |) + |A|) + k) and O(|S α |), respectively.This is precisely what makes PDB heuristics so attractive in practice.Aiming at preserving the attractiveness of the PDB h α -partition while eliminating the bottleneck of fixed dimensionality, Helmert, Haslum, and Hoffmann (2007) generalize the methodology of Dräger et al. (2006) and introduce what we call merge-and-shrink (MS) planning abstractions.
MS abstractions are homomorphisms that generalize PDB abstractions by allowing for more flexibility in selection of pairs of state to be contracted.
The problem's state space is viewed as the synchronized product of its projections onto the single state variables.
This product can be computed by iteratively composing two abstract spaces, replacing them with their product.
While in a PDB the size of the abstract space S α is controlled by limiting the number of product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites.The accuracy of the MS heuristics crucially depends on the order in which composites are formed and the choice of abstract states to contract.
This flexibility in the MS abstraction strategy also adds variability to the complexity of its natural h α -partition.
The time and space complexity for the linear abstraction strategy of Helmert et al. are respectivelyO (|V ||S α | (log (|S α |) + |A|) + k · |V |) and O(|S α |).
Similarly to PDB abstractions, the per-node computation of h α (s) with an MS abstraction α is just a lookup in a data structure storing h * (α(s)) for all abstract states α(s) ∈ S α .
Hence, while the pre-search computation with MS abstractions can be more costly than with PDBs, the online part of computing heuristic values is still extremely efficient.
This per-node efficiency allows Helmert et al. to demonstrate impressive practical effectiveness of MS abstractions on several IPC domains (Helmert, Haslum, & Hoffmann 2007).
Merge-and-shrink abstractions escape the fixeddimensionality constraint of PDBs, but not the constraint on the abstract space to be of a fixed size.
In attempt to eliminate both these constraints of "explicit" abstractions, Katz and Domshlak (2008b) introduce a framework of structural-pattern abstractions.
A structural-pattern abstraction α maps the problem in hand to an abstract problem from a tractable fragment of optimal planning.
This way, the dimensionality of S α is not limited even by |V |, and the size of the abstract space is not limited even by the size of the original space.
2 In particular, Katz and Domshlak (2008b) introduce a concrete family of structural-pattern abstraction, fork-decomposition, corresponding to two classes of tractable optimal planning.
A digraph forms fork/inverted-fork if it is connected, loop-less, and have all its arcs outgoing/incoming from/to a single node (called the root node of the graph).
Propositions 3-4 of Katz and Domshlak (2008b) state that an optimal plan for a planning task Π can be found in polynomial time if 1.
the causal graph of Π forms a fork, and the root variable is binary-valued, or 2.
the causal graph of Π forms an inverted fork 3 , and the domain of the root variable is fixed.
Now, given a general planning task Π, for each variable v i ∈ V , let V f i ⊆ V contain v i and all its immediate successors in CG(Π), and V i i ⊆ V contain v i and all its immediate predecessors in CG(Π).
At a high-level summary, the forkdecomposition of Π with |V | = n is obtained by (1) schematically constructing a set of projection abstractions{Π f i , Π i i } n i=1 , (2) reformulating the actions of {Π f i , Π i i } n i=1to single-effect actions so that the causal graphs of Π f i and Π i i become respectively forks and inverted forks rooted in v i , and (3) within each Π f i , abstracting the domain of v i to {0, 1}, and within each Π i i , abstracting the domain of v i to {0, 1, . . . , b} with b = O(1).
The problems {Π f i , Π i i } n i=1correspond to distanceconservative (though, after step (2), not necessarily homomorphic) abstractions of Π, and, similarly to PDB and MS abstractions, can be used either separately or as parts of heuristic ensembles.To see that the fork-decomposition escapes both limitations of the projection abstractions, note that both |V f i | and |V i i | can be Θ(|V |), and thus each induced |S α | can be Θ(|S|).
The latter is a blessing, but a mixed one, and both sides of the story are demonstrated in Table 1, showing the performance of four different planners on the planning tasks from the Logistics domain of IPC-2000.
The first planner, h F , is A * equipped with the additive fork-decomposition heuristic "all forks" under uniform action cost partitioning (Katz & Domshlak 2008b).
It is compared to (1) MS-10 5 -Helmert et al.'s A * with a merge-and-shrink abstrac-2 While dimensionality larger than |V |, and abstract-space size larger than |S| may appear excessive, see (Katz & Domshlak 2008a) for concrete examples of why this can be useful.3 Proposition 4 of Katz and Domshlak (2008b) also requires the problem to be what is called 1-dependent, but this requirement was later found unnecessary.
(3) Gamer planner.
MS-10 5 represents the stateof-the-art in optimal planning with abstraction heuristics, while Gamer and HSP * F were the two best-performing costoptimal planners at IPC-2008.
As it appears from Table 1, at least on the Logistics domain, the abstraction-based heuristic search favorably competes with the leading optimal-search planners: Both h F and MS-10 5 outperform HSP * F and Gamer on these planning tasks.
The comparison between h F and MS-10 5 , however, is more important to us here.
On the one hand, h F almost consistently expands less search nodes than MS-10 5 , with the difference hitting four orders of magnitude.
On the other hand, the time complexity of h F per search node is substantially higher than this of MS-10 5 , with the two expanding (approximately) 4 and 100000 nodes per second, respectively.
The outcome is simple: while with no time limits (and only memory limit of 1.5 GB) h F solves more tasks than MS-10 5 , the inverse holds with a time limit of one hour (see tasks 11-1 and 12-1).
Empirical (in)effectiveness of the fork-decomposition heuristics was not surprising, yet it was disappointing to see an informative heuristic being out of reach of practical planning.
Fortunately, this is not the end of the story.
We now show that the time-per-node complexity bottleneck of fork-decomposition heuristics can be successfully overcome.
Specifically, we show that an equivalent of PDB's and merge-and-shrink notion of "database" exists for fork-decomposition abstractions as well, despite of their exponential-size abstract spaces.
Of course, unlike with PDB and merge-and-shrink abstractions, structuralpattern databases (and in particular these for the forkdecomposition's fork and inverted fork abstractions) do not (and cannot) result in purely lookup computations of h α (s).
The online part of the h α -partition has to be non-trivial in the sense that its complexity cannot be O(1).
In the remainder of this section we prove existence of such effective h-partitions for both fork and inverted fork abstractions.
In the next section we empirically show that these hpartitions lead to fast pre-search and per-node computations that allow for successfully exploiting the informativeness of the fork-decomposition heuristics in practice.Theorem 1 Let Π = V, A, I, G, c be a planning task with a fork causal graph rooted in a binary-valued variable r.
There exists an h * -partition for Π such that, for any set of k states, the time and space complexity of that h * -partition is, respectively, For each v ∈ V \ {r}, let DTG 0 v and DTG 1 v be the subgraphs of DTG(v, Π) obtained by removing from the latter all the arcs labeled with 1 and 0, respectively.
O(d 3 |V | + |A r | + k · d|V |) and O(d 2 |V |), where d = max v D(v).
DTG 0 v /DTG 1 v if i is odd/even.
For each ϑ ∈ L i−1 , ϑ ∈ L i , L v (σ) contains an arc (ϑ, ϑ ) weighted with the dis- tance from ϑ to ϑ in DTG 0 v /DTG 1 v if i is odd/even.
3.
Set R = ∅.
For each σ ∈ * [σ(r)], a plan ρ σ for Π is constructed as follows.
i For each v ∈ V \ {r}, find a shortest path from s[v] to G[v] in L v (σ).
If no such path exists, go to the next σ ∈ * [σ(r)].
The i-th edge on this path (say, from ϑ ∈ L i−1 to ϑ ∈ L i ) corresponds to the shortest path from ϑ to ϑ in either DTG 0 v or DTG 1 v , and we denote this path byS i ϑ .
ii Set R = R∪{ρ σ }, ρ σ = S 1 ·a σ[2] ·S 2 ·. . .·a σ[m] ·S m ,where m = |σ|, S i is obtained by an arbitrary merge of {S i v } v∈V \{r} , and a x is the action that changes the value of r to x. 4.
If R = ∅, then "unsolvable".
Otherwise, return ρ = argmin ρσ∈R c(ρ σ ).
This algorithm can be formulated as follows.
(1) For each ϑ r ∈ D(r), v ∈ V \ {r}, and ϑ, ϑ ∈ D(v), let p ϑ,ϑ ;ϑr be the cost of the cheapest sequence of actions ϑ to ϑ provided r = ϑ r .
The whole set {p ϑ,ϑ ;ϑr } can be computed by a straightforward variant of the allpairs-shortest-paths, Floyd-Warshall algorithm in timeO(d 3 |V |).
(2) For each v ∈ V \ {r}, 1 ≤ i ≤ d + 1, and ϑ ∈ D(v), let g ϑ;i be the cost of the cheapest sequence of actions changing s[v] to ϑ provided a sequence σ ∈ * [σ(r)], |σ| = i, of value changes of r. Given {p ϑ,ϑ ;ϑr }, the set {g ϑ;i } is given by the solution of the recursive equationg ϑ;i =            p s[v],ϑ;s[r] , i = 1 min ϑ g ϑ ;i−1 + p ϑ ,ϑ;s[r] , 1 < i ≤ δ ϑ , i is odd min ϑ g ϑ ;i−1 + p ϑ ,ϑ;¬s[r] , 1 < i ≤ δ ϑ , i is even g ϑ;i−1 , δ ϑ < i ≤ d + 1, whereδ ϑ = |D(v)| + 1 if ϑ ∈ D(v).
Given that, h * (s) = min σ∈ * [σ(r)] [ c(σ) + v∈V \{r} g G[v];|σ| ].
Notice that step (1) is already state-independent, but the heavy step (2) is not.
However, the state dependence of step (2) can mostly be overcome.
For each v ∈ V \ {r}, ϑ ∈ D(v), 1 ≤ i ≤ d + 1, and ϑ r ∈ D(r), let g ϑ;i (ϑ r ) be the cost of the cheapest sequence of actions changing ϑ to G [v] provided the value changes of r induce a 0/1 sequence of length i starting with ϑ r .
Very similarly to before, the set {g ϑ;i (ϑ r )} is given by the solution of the recursive equationg ϑ;i (ϑ r ) =      p ϑ,G[v];ϑr , i = 1 min ϑ g ϑ ;i−1 (¬ϑ r ) + p ϑ,ϑ ;ϑr , 1 < i ≤ δ ϑ g ϑ;i−1 (ϑ r ), δ ϑ < i ≤ d + 1 ,(1) which can be solved in time O(d 3 |V |).
Note that this equation is now independent of the evaluated state s, and yet {g ϑ;i (ϑ r )} allow for computing h * (s) for a given state s viah * (s) = min σ∈ * [σ(r)] [ c(σ) + v∈V \{r} g s[v];|σ| (s[r]) ](2)With the new formulation, the only computation that has to be performed per search node is this of the final minimization over * [σ(r)], which is the lightest part of the whole algorithm anyway.
The major computation, notably this of {p ϑ,ϑ ;ϑr } and {g ϑ;i (ϑ r )}, can now be performed offline, and shared between the evaluated states.
The space required to store this information is O( 0 201 200 101 100 1 0 2 24 100 2 1 0 201 200 101 100 1 0 3 48 100 2 1 0 53 102 3 2 1 0 4 72 100 2 1 0 53 Theorem 2 Let Π = V, A, I, G, c be a planning task with an inverted fork causal graph rooted in r with |D(r)| = b = O(1).
There exists an h * -partition for Π such that, for any set of k states, the time and space complexity of thath * -partition is O(b|V ||A r | b−1 + d 3 |V | + k · |V ||A r | b−1 ) and O(|V ||A r | b−1 + d 2 |V |), respectively, where d = max v D(v).
Similarly to the proof of Theorem 1, the proof of Theorem 2 is also based on a modification of the poly-time algorithm of Katz and Domshlak (2008b) for computing h * (s) for state s of a task Π as in the theorem.
The latter algorithm finds h * (s) as follows.1.
For each v ∈ V \ {r}, and each ϑ, ϑ ∈ D(v), compute shortest (i.e., cost-minimal) paths from ϑ to ϑ in DTG(v, Π).2.
Enumerate all Θ(|A r | b−1 ) cycle-free paths from s[r] to G [r] in DTG(r, Π).
For each such path, construct a plan for Π based on that path for r, and the shortest paths computed in (1).
The cheapest such plan is a cost-optimal plan from s in Π, and thus induces h * (s).
This algorithm can be formulated as follows.
(1) For each v ∈ V \ {r}, and ϑ, ϑ ∈ D(v), let p ϑ,ϑ be the cost of the cheapest sequence of actions changing ϑ to ϑ .
The whole set {p ϑ,ϑ } can be computed using the Floyd-Warshall algorithm in timeO(d 3 |V |).
(2) For each cycle-free path π = a 1 · . . . · a m from s[r]to G [r] in DTG(v, Π), let g π be the cost of the cheapest plan from s in Π based on π, and the shortest paths computed in (1).
Each g π can be computed asg π = m i=1 c(a i ) + m i=0 v∈V \{r} p pre i [v],pre i+1 [v] ,where pre 0 · . . . · pre m+1 are the values needed from the parents of r along the path π.
That is, for each v ∈ V \ {r}, and 0 ≤ i ≤ m + 1, and G[v] is specified pre(ai) [v], 1 ≤ i ≤ m, and pre(ai) [v] is specified pre i−1 [v] otherwisepre i [v] = 8 > > > < > > > : s[v], i = 0 G[v], i = m + 1,From that, we have h * (s) = min π g π .
Notice that step (1) is state-independent, but step (2) is not so.
However, the dependence of step (2) on the evaluated state can be substantially relaxed.
As there are only O(1) different values of r, it is possible to consider cyclefree paths to G [r] from all values of r. For each such path π, and each parent variable v ∈ V \ {r}, we know what would be the first value of v required by π.
Given that, we can pre-compute the cost-optimal plans induced by each π and assuming the parents start at their first needed values.
The remainder of the computation of h * (s) is delegated to online, and the modified step (2) is as follows.For each ϑ r ∈ D(r) and each cycle-free path π = a 1 ·. . .· a m from ϑ r to G [r] in DTG(v, Π), let a "proxy" state s π be ipc4 20 20 100 17 85 17 85 16 80 16 80 15 75 11 55 17 85 20 100 blocks-ipc2 30 21 70 18 60 18 60 18 60 20 -ipc1 2 2 100 1 50 1 50 2 100 2 100 0 0 2 100 1 50 2 100 gripper-ipc1 20 7 35 7 35 7 35 7 35 7 35 6 30 20 100 7 35 7 35 logistics-ipc1 7 6 86 4 57 5 71 4 57 5 71 3 43 6 86 2 29 2 29 logistics-ipc2 22 22 100 16 73Table 2: A summary of the experimental results.
Per domain, S denotes the number of tasks solved by any planner.
Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from "solved by any" (%S).
Boldfaced results indicate the best performance within the corresponding domain.
The last three rows summarize the number of solved instances in total (s), the domain-normalized measure of solved instances in total (ˆ s), and the number of domains in which the planners achieved superior performance (w).
sπ[v] = 8 > < > : ϑr, v = r G[v], ∀1 ≤ i ≤ m : pre(ai)[v] is unspecified pre(ai)[v], i = argmin j {pre(aj)[v] is specified} , domain (D) S(D) h F h I h FI MS-10 4 MS-10 5 HSP * F Gamer blind hmax s %S s %S s %S s %S s %S s %S s %S s %S s %S airport-that is, the non-trivial part of s π captures the first values of V \ {r} needed along π.
4 Given that, let g π be the cost of the cheapest plan from s π in Π based on π, and the shortest paths {p ϑ,ϑ } computed in (1).
Each g π can be computed asg π = m i=1   c(a i ) + v∈V \{r} p pre i [v],pre i+1 [v]   ,where, for each v ∈ V \ {r}, and 1 ≤ i ≤ m + 1, and G[v] is specified pre(ai) [v], 2 ≤ i ≤ m, and pre(ai) [v] is specified pre i−1 [v] otherwise Storing the pairs (g π , s π ) accomplishes the offline part of the computation.
Now, given a search state s, we can computepre i [v] = 8 > > > < > > > : sπ[v], i = 1 G[v], i = m + 1,h * (s) = min π:sπ[r]=s[r]   g π + v∈V \{r} p s[v],sπ[v]   .
The dominant offline computation is computing {g π }.
and |S α | < 10 5 .
These four (baseline and MS) heuristics were implemented by within the same planning system as our fork-decomposition heuristics, allowing for a fairly unbiased comparison.
We also compare to the Gamer (Edelkamp & Kissmann 2009) and HSP * F (Haslum 2008) planners that were respectively the winner and the runner-up at the sequential optimization track of IPC-2008.
On the algorithmic side, Gamer is based on a bidirectional blind search using sophisticated symbolic-search techniques, and HSP * F uses A * with an additive critical-path heuristic.The summary of our experimental results is shown in Table 2.
The experiments were conducted on 3GHz Intel E8400 CPU with 2 GB memory, using a 1.5 GB memory limit and 30 minute timeout.
The only exception from that was Gamer for which we used similar machines but with 4 GB memory, and 2 GB memory limit; this was done to provide Gamer with the environment for which it was configured.
For each domain D, S(D) is the number of tasks in D that were solved by at least one planner in the suite.
For each domain D and planner p, s(D, p) (in columns s) is the number of tasks in D solved by p. Columns %S provide the same information in terms of percentage from "solved by any".
Boldfaced results indicate the best performance within the corresponding domain.
The last three rows summarize the performance of the planners via three measures.
First, s(p) is the total number of tasks solved in all the 23 domains; this is basically the measure used in planner evaluation in IPC-2008.
As domains are not equally challenging, and do not provide the same discrimination between the planners' performance, the second, "domain-normalized" measure of solved instances in totaî s(p) = D s(D, p)/S(D).
Finally, w(p) is the number of domains in which p achieved superior performance.Overall, Table 2 clearly suggests that heuristic search with "databased" fork-decomposition heuristics favorably competes with the state-of-the-art of optimal planning.
In particular, A * with the fork-decomposition heuristic h F exhibited the best overall performance according to all three measures.
The overall performance of A * with the other two heuristics h I and h FI was not as good as with h F , and yet, in terms of, e.g., absolute number of solved instances, h FI and h I came second and forth, respectively.
Likewise, h I still slightly outperforms h F on the Rovers and Trucks domains, so no clear-cut dominance between them.Finally, the contribution of employing h-partitions to the success of the structural-pattern heuristics was dramatic.
Looking back on the results for Logistics-ipc2 in Table 1, note that the timeout of 30 minutes would prevent us from solving instances 11-1 and 12-1, ending up with only 20 solved instances.
In contrast, with structuralpattern databases we manage to solve these two instances within the time limit, and the difference in running time is spectacular: While without employing h α -partition, solving instance 12-1 took us more than 7 hours, with structuralpattern databases we solved all the 22 instances from Table 1 together in less than 35 seconds.
This difference in runtimes is representative for all the domains in our evaluation, and thus h-partitions play a key role in bringing structural patterns to the state of the art in cost-optimal planning.
To evaluate the practical attractiveness of structural-pattern heuristics in general, and the corresponding h-partitions in particular, we have conducted an empirical study on a wide sample of planning domains from the international planning competitions [1998][1999][2000][2001][2002][2003][2004][2005][2006].
The selection of the domains was aimed at allowing a comparative evaluation with other baseline and state of the art approaches/planners that, at the moment, not all support all PDDL features.We implemented three additive fork-decomposition heuristics within a standard heuristic forward search framework of the Fast Downward planner (Helmert 2006), using the A * algorithm with full duplicate elimination.
The three heuristics were these outlined by Katz and Domsh- lak (2008b).
The h F heuristic corresponds to the ensemble of all (not clearly redundant) fork subgraphs of the causal graph, with the domains of the roots being abstracted using the "leave-one-value-out" binary-valued domain decompositions.
The h I heuristic is the same but for the inverted fork subgraphs, with the domains of the roots being abstracted using the "distance-to-goal-value" ternary-valued domain decompositions.
The ensemble of the h FI heuristic is the union of these for h F and h I .
The action cost partitioning for all three ensembles was set to "uniform".
We compare with two baseline approaches, namely blind search (A * with a heuristic function which is 0 for goal states and 1 otherwise) and A * with the h max heuristic (Bonet & Geffner 2001), as well as with state of the art abstraction heuristics, represented by the merge-and-shrink abstractions of Helmert, Haslum, and Hoffmann (2007).
The latter were constructed under the linear, f -preserving abstraction strategy suggested by these authors, and this under two fixed bounds on the size of the abstraction, |S α | < 10 4
