The popularity of Convolutional Neural Network (CNN) models and the ubiquity of CPUs imply that better performance of CNN model inference on CPUs can deliver significant gain to a large number of users.
To improve the performance of CNN inference on CPUs, current approaches like MXNet and Intel OpenVINO usually treat the model as a graph and use the high-performance libraries such as Intel MKL-DNN to implement the operations of the graph.
While achieving reasonable performance on individual operations from the off-the-shelf libraries, this solution makes it inflexible to conduct optimizations at the graph level, as the local operation-level optimizations are predefined.
Therefore, it is restrictive and misses the opportunity to optimize the end-to-end inference pipeline as a whole.
This paper presents NeoCPU, a comprehensive approach of CNN model inference on CPUs that employs a full-stack and systematic scheme of optimizations.
NeoCPU optimizes the operations as templates without relying on third-parties libraries, which enables further improvement of the performance via operation-and graph-level joint optimization.
Experiments show that NeoCPU achieves up to 3.45× lower latency for CNN model inference than the current state-of-the-art implementations on various kinds of popular CPUs.
The growing use of Convolutional Neural Network (CNN) models in computer vision applications makes this model architecture a natural focus for performance optimization efforts.
Similarly, the widespread deployment of CPUs in servers, clients, and edge devices makes this hardware platform an attractive target.
Therefore, performing CNN model inference efficiently on CPUs is of critical interest to many users.The performance of CNN model inference on CPUs leaves significant room for improvement.
Performing a CNN model * Equal contribution inference is essentially executing a computation graph consisting of operations.
In practice, people normally use highperformance kernel libraries (e.g. Intel MKL-DNN [27] and OpenBlas [51]) to obtain high performance for CNN operations.
While these libraries tune very carefully for common operations with normal input data shapes (e.g. 2D convolutions), they only focus on the (mostly, convolution) operations but miss the opportunities to further optimize the end-to-end model inference at the graph level.
The graph-level optimization is often handled by the deep learning frameworks, e.g. TensorFlow [5] and MXNet [8].
However, the graph-level optimization such as operation fusion and data layout planing that a framework can do is limited because the operation implementation is already predefined in the third-party libraries.
Therefore, the optimizations in the frameworks do not work in concert with the optimizations in the kernel library, which leaves significant performance gains unrealized in practice.
Furthermore, different CPU architectures rely on different high-performance libraries and integrating a library into a deep learning framework requires error-prone and time-consuming engineering effort.
Lastly, although those libraries are highly optimized, they present as third-party plug-ins, which may introduce contention issues with other libraries in the framework.
As an example, TensorFlow originally used the Eigen library [4] to handle computation on CPUs.
Later on, MKL-DNN was also introduced.
As a consequence, at runtime MKL-DNN threads coexist with Eigen threads, resulting in resource contention.
In summary, this kind of framework-specific approach for CNN model inference on CPUs is inflexible, cumbersome, and sub-optimal.
Because of the constraint imposed by the framework, optimizing the performance of CNN model inference end-to-end without involving a framework (i.e. a framework-agnostic method) is of obvious interest to many deep learning practitioners.
Recently, Intel launched a universal CNN model inference engine called OpenVINO Toolkit [16].
This toolkit optimizes CNN models in the computer vision domain on Intel processors (mostly x86 CPUs) and claims to achieve better performance than the deep learning frameworks alone.
Yet, OpenVINO could only provide limited graph-level optimization (e.g. operation fusion as implemented in ngraph [15]) as it still relies upon MKL-DNN to deliver performance gains for the carefully-tuned operations.
Therefore, the optimization done by OpenVINO is still not sufficient for most of the CNN models.Based on the previous observation, we argue that in order to further improve the CNN model inference performance on CPUs, being able to do the flexible end-to-end optimization is the key.
In this paper, we propose NeoCPU, a comprehensive approach to optimize CNN models for efficient inference on CPUs.
NeoCPU is full-stack and systematic, which includes operation-and graph-level joint optimizations and does not rely on any third-party high-performance libraries.
At the operation level, we follow the well-studied techniques to optimize the most computationally-intensive operations like convolution (CONV) in a template, which is applicable to different workloads on multiple CPU architectures and enables us for flexible graph-level optimization.
At the graph level, in addition to the common techniques such as operation fusion and inference simplification, we coordinate the individual operation optimizations by manipulating the data layout flowing through the entire model for the best end-to-end performance.
In summary, NeoCPU does the end-to-end optimization in a flexible and automatic fashion, while the existing works rely on third-party libraries and lack comprehensive performance tuning.NeoCPU is built upon a deep learning compiler stack named TVM [9] with a number of enhancements.
TVM enables the possibility of using own operation-level optimizations instead of third-party high-performance libraries, which make it flexible to apply our operation-and graph-level joint optimization.
However, there was only one customized operation-level optimization on ARM CPUs for convolutions with specific data shapes and no operation-and graph-level joint optimization in the original TVM stack before our work.
In addition, there exist other deep learning compilers such as Tensor Comprehensions [46] and Glow [40].
Unfortunately, they either do not target on CPUs or not optimize the CPU performance well, e.g. based on the paper description and our own experiments, Glow only optimizes the single-core performance for CPUs.
Therefore we do not incorporate those works as the baseline.
Table 1 summarizes the features of NeoCPU compared to others.
To the best of our knowledge, NeoCPU achieves competitive performance for CNN model inference on various kinds of popular CPUs.Specifically, this paper makes the following contributions:• Provides an operation-and graph-level joint optimization scheme to obtain high CNN model inference performance on different popular CPUs including Intel, AMD and ARM, which outperforms the current state-of-the-art implementations;Op-level opt Graph-level opt Joint opt Open-source [5] 3rd party limited OpenVINO [16] 3rd party limited ?
Original TVM [9] incomplete Glow [40] single core Table 1: Side-by-side comparison between NeoCPU and existing works on CNN model inferenceNeoCPU MXNet [8]/TensorFlow• Constructs a template to achieve good performance of convolutions, which is flexible to apply to various convolution workloads on multiple CPU architectures (x86 and ARM) without relying on high-performance kernel libraries;• Designs a global scheme to look for the best layout combination in different operations of a CNN model, which minimizes the data layout transformation overhead between operations while maintaining the high performance of individual operations.It is worth noting that, this paper primarily deals with direct convolution computation, while NeoCPU is compatible to other optimziation works on the computationally-intensive kernels, e.g. CONVs via Winograd [7,29] or FFT [52].
We evaluated NeoCPU on CPUs with both x86 and ARM architectures.
In general, NeoCPU delivers the best performance for 13 out of 15 popular networks on Intel Skylake CPUs, 14 out of 15 on AMD EYPC CPUs, and all 15 models on ARM Cortex A72 CPUs.
It is worthwhile noting that the baselines on x86 CPUs were more carefully tuned by the chip vendor (Intel MKL-DNN) but the ARM CPUs were less optimized.
While the selected framework-specific (MXNet and TensorFlow) and framework-agnostic (OpenVINO) solutions may perform well on one case and less favorably on the other case, NeoCPU runs efficiently across models on different architectures.In addition, NeoCPU produces a standalone module with minimal size that does not depend on either the frameworks or the high-performance kernel libraries, which enables easy deployment to multiple platforms.
NeoCPU is used in Amazon SageMaker Neo Service 1 , enabling model developers to optimize for inference on CPU-based servers in the cloud and devices at the edge.
Using this service, a number of application developers have deployed CNN models optimized for inference in production on several types of platforms.
All source code has been released to the open source TVM project 2 .
The rest of this paper is organized as follows: Section 2 reviews the background of modern CPUs as well as the typical CNN models; Section 3 elaborates the optimization ideas that we propose and how we implement them, followed by evaluations in Section 4.
We list the related works in Section 5 and summarize the paper in Section 6.
Although accelerators like GPUs and TPUs demonstrate their outstanding performance on the deep learning workloads, in practice, there is still a significant number of deep learning computation, especially model inference, taking place on the general-purpose CPUs due to the high availability.
Currently, most of the CPUs equipped on PCs and servers are manufactured by Intel or AMD with x86 architecture [1], while ARM CPUs with ARM architecture occupy the majority of embedded/mobile device market [2].
Modern CPUs use thread-level parallelism via multicore [21] to improve the overall processor performance given the diminishing increasing of transistor budgets to build larger and more complex uniprocessor.
It is critical to avoid the interference among threads running on the same processor and minimize their synchronization cost in order to have good scalability on multi-core processors.
Within the processor, a single physical core achieves the peak performance via the SIMD (single-instruction-multiple-data) technique.
SIMD loads multiple values into wide vector registers to process together.
For example, Intel introduced the 512-bit Advanced Vector Extension instruction set (AVX-512), which handles up to 16 32-bit single precision floating point numbers (totally 512 bits) per CPU cycle.
And the less advanced AVX2 processes data in 256-bit registers.
In addition, these instruction sets utilize the Fused-Multiply-Add (FMA) technique which executes one vectorized multiplication and then accumulates the results to another vector register in the same CPU cycle.
The similar SIMD technique is embodied in ARM CPUs as NEON [3].
As shown in the experiments, our proposed solution works on both x86 and ARM architectures.In addition, it is worth noting that modern server-side CPUs normally supports hyper-threading [37] via the simultaneous multithreading (SMT) technique, in which the system could assign two virtual cores (i.e. two threads) to one physical core, aiming at improving the system throughput.
However, the performance improvement of hyper-threading is applicationdependent [35].
In our case, we do not use hyper-threading since one thread has fully utilized its physical core resource and adding one more thread to the same physical core will normally decrease the performance due to the additional context switch.
We also restrict our optimization within processors using the shared-memory programming model as this is the typical system setting for CNN model inference.
The NonUniformed Memory Access (NUMA) pattern occurred in the context of multiple processors on the same motherboard is beyond the scope of this paper.
Convolutional neural networks (CNNs) are commonly used in computer vision workloads [23,26,33,36,[41][42][43].
A CNN model is normally abstracted as a computation graph, essentially, Directed Acyclic Graph (DAG), in which a node represents an operation and a directed edge pointing from node X to Y represents that the output of operation X serves as (a part of) the inputs of operation Y (i.e. Y cannot be executed before X).
Executing a model inference is actually to flow the input data through the graph to get the output.
Doing the optimization on the graph (e.g. prune unnecessary nodes and edges, pre-compute values independent to input data) could potentially boost the model inference performance.Most of the computation in the CNN model inference attributes to convolutions (CONVs).
These operations are essentially a series of multiplication and accumulation, which by design can fully utilize the parallelization, vectorization and FMA features of modern CPUs.
Existing works [19,24,27] have demonstrated that it is possible to achieve high performance of convolution operations on CPUs by arranging the data layout and consequently, the computation, in an architecture-friendly way.
The remaining challenge is how to manage the data layout flowing through these operations efficiently to get the high performance out of the end-to-end CNN model inference.The rest of the CNN workloads are mostly memory-bound operations associated to CONVs (e.g. batch normalization, pooling, activation, element-wise addition, etc.).
The common practice [9] is fusing them to CONVs so as to increase the overall arithmetic intensity of the workload and consequently boost the performance.The computation graph of CNN model training has no essential difference with inference, just being larger (adding in backwards operations) and with some more computationallytrivial operations (e.g. loss function).
Therefore, the optimization work done for CNN model inference is applicable to training as well.
This section describes our optimization ideas and implementations in detail.
The solution presented in this paper is endto-end for doing the CNN model inference.
Our proposed solution is generic enough to work for a wide range of common CNN models as we will show in the evaluation.
The basic idea of our approach is to view the optimization as an end-to-end problem and search for a globally best optimization.
That is, we are not biased towards a local performance optimal of a single operation as many previous works.
In order to achieve this, we first present how we optimized the computationally intensive convolution operations at low-level using a configurable template (Section 3.1).
This makes it flexible to search for the best implementation of a specific convolution workload on a particular CPU architecture, and to optimize the entire computation graph by choosing proper data layouts between operations to eliminate unnecessary data layout transformation overhead (presented in Section 3.2 and 3.3).
We implemented the optimization based on the TVM stack [9] by adding a number of new features to the compiling pass, operation scheduling and runtime components.
The original TVM stack has done a couple of generic graphlevel optimizations including operation fusion, pre-computing, simplifying inference for batch-norm and dropout [9], which are also inherited to this work but will not be covered in this paper.
Optimizing convolution operations is critical to the overall performance of a CNN workload as it takes the majority of computation.
This is a well-studied problem but the previous works normally go deep to the assembly code level for high performance [24,27].
In this subsection, we show how to take advantage of the latest CPU features (SIMD, FMA, parallelization, etc.) to optimize a single CONV without going into the tedious assembly code or C++ intrinsics.
By managing the implementation in high-level instead, it is then easy to extend our optimization from a single operation to the entire computation graph.
We started from optimizing CONV within one thread.
CONV is computationally-intensive which traverses its operands multiple times for computation.
Therefore, it is critical to manage the layout of the data fed to the CONV to reduce the memory access overhead.
We first revisit the computation of CONV to illustrate our memory management scheme.
A 2D CONV in CNN takes a 3D feature map (height × width × channels) and a number of 3D convolution kernels (normally smaller height and width but the same number of channels) to convolve to output another 3D tensor.
The calculation is illustrated in Figure 1, which implies loops of 6 dimensions: in_channel, kernel_height, kernel_width, out_channel, out_height and out_width.
Each kernel slides over the input feature map along the height and width dimensions, does element-wise product and accumulates the values to produce the corresponding element in the output feature map, which can naturally leverage FMA.
The number of kernels forms out_channel.
Note that three of the dimensions (in_channel, kernel_height and kernel_width) are reduction axes that cannot be embarrassingly parallelized.We use the conventional notation NCHW to describe the default data layout, which means the input and output are 4-D tensors with batch size N, number of channels C, feature map height H, feature map width W, where N is the outermost and W is the innermost dimension of the data.
The related layout of kernel is KCRS, in which K, C, R, S stand for the output channel, input channel, kernel height and kernel width.Following the common practice [27,45], we organized the k, in which c with split size x and k with split size y are the sub-dimensions of input channel C and output channel K, respectively.
It is worth noting that a significant amount of data transformation overhead needs to be paid to get the desired layout.
In addition to the dimension reordering, for better utilizing the latest vectorization instructions (e.g. AVX-512, AVX2, NEON, etc.), we split out_width to ow_outer and ow_inner using a factor reg_n and move the loop of ow_inner inside for register blocking.
For example, on a CPU featured AVX-512, we can utilize its 32 512-bit width registers ZMM 0 − ZMM 31 [28] as follows.
We maintain the loop hierarchy to use one ZMM register to store the kernel data while others storing the feature map.
The kernel values stored in one ZMM register (up to 512 bits, a.k.a, 16 output channels in float32) are used to multiply with a number of input feature map values continuously stored in the DRAM via AVX-512F instructions [28], whose results are then accumulated to other ZMM registers storing the output values.
Figure 1 illustrates this idea.
For other vectorized instructions, the same idea applies but the split factor of out_width (i.e. reg_n) may change.Algorithm 1 summarizes our optimization of CONV in single thread, which essentially is about 1) dimension ordering for friendly memory locality and 2) register blocking for good vectorization instruction utilization, as in previous works.
However, unlike others, we made it a template in high-level language , in which the block size (x, y), the number of utilized registers (reg_n), and the loop-unroll strategy (unroll_ker) are easily configurable.
Consequently, the computing logic can be adjusted according to different CPU architectures (cache size, registered vector width, etc.) as well as different workloads (feature map size, convolution kernel size, etc.).
This is flexible and enables graph-level optimization we will discuss later.
end for 25: end for It is a common practice to partition CONV into disjoint pieces to parallelize among multiple cores of a modern CPU.
Kernel libraries like Intel MKL-DNN usually uses off-the-shelf multithreading solution such as OpenMP.
However, we observe that the resulting scalability of the off-the-shelf parallelization solution is not desirable (Section 4.2.4).
Therefore, we implemented a customized thread pool to efficiently process this kind of embarrassing parallelization.
Basically, in a system of N physical cores, we evenly divided the outermost loop of the operation into N pieces to assign to N threads.
Then we used C++11 atomics to coordinate threads during fork-join and an single-producer-single-consumer lockfree queue between the scheduler and every working thread to assign tasks.
Active threads are guaranteed to run on disjoint physical cores via thread binding to minimize the hardware contention, and no hyper-threading is used as discussed in Section 2.1.
For the global data structure accessed by multiple threads such as the lock-free queues, we inserted cache line padding as needed to avoid false sharing between threads.
In summary, this customized thread pool employs deliberate mechanism to prevent resource contention and reduce the thread launching overhead, which makes it outperform OpenMP according to our evaluation.
In this subsection, we extend the optimization scope from a single operation to the entire computation graph of the CNN model.
The main idea here is to come up with a generic solution at the graph level to minimize the data layout transformation introduced by the optimization in Section 3.1.
Previous works [19,24,27] which focus on individual operation optimization normally do not consider about the data layout transformation overhead between highly optimized operations.Since NCHW[x]c is efficient for CONVs which takes the majority of the CNN model computation, we should make sure that every CONV is executed in this layout.
However, other operations between CONVs may only be compatible with the default layout, which makes each CONV transform the input data layout from default (NCHW or NHWC) to NCHW [x]c before the computation and transform it back at the end.
This transformation introduces significant overhead.Fortunately, from the perspective of the graph level, we can take the layout transformation out of CONV to be an independent node, and insert it only when necessary.
That is, we eliminate the transformation taking place in the CONV operation and maintain the transformed layout flow through the graph as far as possible.In order to determine if a data transformation is necessary, we first classify operations into three categories according to how they interact with the data layout as follows:1.
Layout-oblivious operations.
These operations process the data without the knowledge of its layout, i.e. it can handle data in any layout.
Unary operations like ReLU, Softmax, etc., fall in this category.2.
Layout-tolerant operations.
These operations need to know the data layout for processing, but can handle a number of layout options.
For example, CONV, in our case, can deal with NCHW, NHWC and NCHW [x]c layouts.
Other operations like Batch_Norm, Pooling, etc., fall in this category as well.3.
Layout-dependent operations.
These operations process the data only in one specific layout, that is, they do not The left side depicts the network with default data layout.
Each CONV node in pink needs to pay additional overhead to transform the data into a favorable layout to achieve good performance and then transform back to default.
The network in the right side is optimized at the graph level to minimize the data layout transformation during the runtime.
The CONV nodes in green do not need to transform any data before and after computation.tolerate any data transformation.
Therefore, the layout has to be transformed to a certain format before passing to a layout-dependent operation.
Transformation operations like Flatten, Reshape, etc, fall in this category.Operations between CONVs in typical CNN models are either layout-oblivious (e.g. ReLU, SoftMax, Concat, and ElemwiseAdd) or layout-tolerant (e.g. Batch_Norm, Pooling), making it possible to keep the data layout being NCHW[x]c across convolution layers.
Layout transformation from NCHW to NCHW [x]c happens before the first CONV.
Data layout between CONVs can be maintained the same (i.e. NCHW [x]c sharing the same x value) without transformation.
Only if getting to a layout-dependent operation, e.g. Flatten, the data layout is transformed back from NCHW[x]c to NCHW.In practice, we first traverse the computation graph to infer the data layout of each node as illustrated in the left side of Figure 2, then we alter the layout of CONVs from default to NCHW [x]c for better performance.
Note that in order to prevent further transformation, we make x a constant number (e.g. 16) across all CONVs.
However, this value may vary across different CONVs in order to get the optimal performance, which requires layout transformation.
We will explain more about this in Section 3.3.
Finally, the LayoutTransform nodes are inserted to the graph accordingly.
Thus, we still have NCHW input and output for the network, but the internal layouts between CONV layers are in optimized NCHW [x]c, as shown in the right part of Figure 2.
It is worth noting that, the layout of the model parameters such as convolution kernel weights and the mean and variance of Batch_Norm are invariant so can be pre-transformed during the compilation.We also illustrate this in the right part of Figure 2.
We implemented the ideas by introducing multiple graphlevel optimization passes to the TVM stack.
By keeping transformed data layout invariant between CONV layers as much as possible and pre-transforming the layout of convolution kernel weights at compilation time, we further improve the end-to-end performance of CNN model inference.
We came up with the aforementioned optimization schemes, especially, how to layout the data, based on our understanding of the hardware, e.g. cache size, vectorization unit width, memory access pattern, etc.
However, it is tedious and impractical to exhaust all possible optimal cases by hand.
As a trade-off, Section 3.2 assumes that the split factor of the channel, i.e. x in NCHW[x]c, stays the same during the entire network, while having various x values in different CONVs may lead to a better performance.
In addition, the split factor of the output width, i.e. reg_n, also needs to adjust for different vectorization instruction sets.Therefore, an automatic search for the best scheme is in demand to further improve the performance.
Basically, we should build a system to allow the domain experts to construct the search space for the machine to explore for the best scheme resulting in the shortest execution time.
The search is two-stage, first local to find optimization scheme candidates for the individual computationally-intensive operations, then global to select and combine the individual schemes for the optimal end-to-end results.
It is feasible to conduct this kind of search given the optimization template described in Section 3.1.
The first step is to find the optimal schedules for each computationally-intensive operations, i.e. CONVs in a CNN model.
We used a tuple (ic_bn, oc_bn, reg_n, unroll_ker) to represent a convolution schedule, whose items are chosen to cover different CPU architectures and generations for different convolution workloads.
The first two terms ic_bn and oc_bn stand for the split factors of input and output channels (i.e. x in the NCHW[x]c notation), which are relevant to the cache sizes of a specific CPU.
The third term reg_n is the number of SIMD registers to be used at the inner loop, which varies among different CPU architectures and generations.
Also, we observed that utilizing all SIMD registers in a single thread does not always return the best performance.
The last term unroll_ker is a boolean deciding whether to unroll the for loop involving convolution kernel computation (line 12 of Algorithm 1), as in some scenarios unrolling this loop may increase the performance by reducing branch penalties and such.
The local search uses the template discussed in 3.1.1 to find the best combination of these values to minimize the CONV execution time, similar to the kernel optimization step in [31].
Specifically, the local search works as follows:1.
Define the candidate lists of ic_bn and oc_bn.
3.
Define the candidate list of unroll_ker to be [True, False].4.
Walk through the defined space to measure the execution time of all combinations, each of which will be run multiple times for averaging to cancel out the possible variance rooted from the unexpected interference from the operating system and/or other processes.
This eventually generates a list of combinations ascendingly ordered by their execution time.It is worth noting that we designed the above tuple in a configurable way, which means that we can always revise the tuple (e.g. adding or removing items, modifying the candidate values of an item) as needed.
Empirically, the local search of a CNN model takes a few hours using one machine, which is acceptable as it is one-time work.
For example, it took about 6 hours to search for the 20 different CONV workloads of ResNet-50 on an 18-core Intel Skylake processor.
In addition, we can maintain a database to store the results for every convolution workload (defined by the feature map and convolution kernel sizes) on every CPU type to prevent repeating search for the same convolution in different models.Local search works well for each individual operation and indeed finds better optimization scheme than our manual work.
However, greedily adopting the local optimal of every operation may not lead to the global optimal.
Consider two consecutive CONV operations conv_0 and conv_1, if the output split factor (oc_bn) of conv_0 is different from the input split factor (ic_bn) of conv_1, a LayoutTransform node needs to be inserted to the graph as discussed in Section 3.2.
This transformation overhead can be too expensive to take advantage of the benefit brought by the local optimal, especially when the data size of the network is large.
On the other hand, if we maintain the same split factor throughout the entire network (as we did in Section 3.2), we may miss the opportunity to optimize some CONVs.
Therefore, a trade-off should be made using a global search.
In this subsection, we extend the optimization search to the entire computation graph.
The idea is to allow each CONV freely choosing the split factor x (i.e. ic_bn and oc_bn), and take the corresponding data layout transformation time into consideration.
According to Section 3.2, the operations between CONVs are either layout-oblivious or layout-tolerant, so they can use whatever x decided by the CONV operation.We extract a snippet of a typical CNN model in Figure 3 to illustrate the idea.
From the figure we see that each CONV has a number of candidate schemes specified by different (ic_bn and oc_bn) pairs.
The shortest execution time achieved by each pair can be obtained in the local search step.
The number of pairs is bound to 100 since both ic_bn and oc_bn usually have choices less than 10.
Choosing different schemes will introduce different data transformation overheads (denoted in dashed boxes between CONVs) or no transformation (if the oc_bn of the CONV equals the ic_bn of its successor).
For simplicity, in the figure we omit the operations which do not impact the global search decision such as ReLU, Batch_Norm between two CONVs.
However, operations like Elementwise_Add could not be omitted since it requires the layout of its two input operands (outputs of CONV j and CONV k in the figure) to be the same.Naively speaking, if a CNN model consists of n CONVs, each of which has k i candidate schemes, the total number of options of the global scheme will be ∏ n i=1 k i , very easy to become intractable as the number of layers n grows.
Fortunately, in practice, we can use a dynamic programming (DP) algorithm to efficiently solve this problem.
Note that when choosing the scheme for a CONV, we only need to consider the data layout of it and its direct predecessor(s) but not any other ancestor CONVs as long as the so-far globally optimal schemes up to the predecessor(s) are memorized.Therefore, a straightforward algorithm is constructed in Algorithm 2.
In practice, a lot of CNN models has the structure as simple as a list, in which each CONV only has one predecessor [33,41].
In this case, after a CONV is done, the intermediate states stored for its predecessor can be safely removed.
For networks with more complex structure like using Elementwise_Add to add two CONV outputs to feed to the next CONV [23], it is trickier since the schemes of a CONV may need to be saved for a future use (e.g. in Figure 3 CONV l needs the schemes of CONV j via Elementwise_Add).
1: Sort the nodes of the graph in topological order 2: Initialize the optimal schemes of the CONVs without dependency using the execution time of their candidate schemes 3: for CONV i in topological order do 4: for each candidate scheme CSI j of CONV i do j is the j th scheme of CONV i 5: t = execution_time(CSI j ) GSI j = MAX initialize global optimal scheme of CONV i under scheme j for each so-far globally optimal scheme GSX k of predecessor x do k is the k th scheme of CONV x 8:cur_opt = t + trans f orm_time(k, j) + GSX k 9:if cur_opt < GSI j then 10:GSI j = cur_opt 11:end if end for end for 14: end for 15: return last node's shortest scheme However, if the model structure becomes too complicated with many data dependency links between CONVs, the straightforward DP algorithm could go intractable, too.
For example, in the object detection model SSD [36], the number of states can reach the order of trillions due to the occurrence of many concatenation blocks.
In this case, we introduced an approximate solution to accelerate the search.
Particularly, we reduced our global search problem to the register allocation problem in the canonical compiler domain with minor modification as follows.
The register allocation problem is modeled as graph representation in which each node (variable) has a candidate list containing all possible register options, and each edge is associated with a cost matrix indicating the availability of registers between two nodes [20].
Similarly in our global search, each CONV has a list of candidate schemes and each edge is associated with the layout transformation cost matrix generated by the scheme lists of two CONVs.
For other non-CONV nodes like Elementwise_Add which require all inputs in the same layout, we fixed the layout of one input and convert all other input layouts to it.
Therefore, we defined the candidate list of a non-CONV node to be the same as the first input CONV and the cost matrix on the edge between these two nodes as all diagonal elements being 0 and all the other elements being infinite.
For the edges between this non-CONV node and other input nodes, cost matrices are generated from the first input node and other input nodes.
After such modification, all nodes and edges in our graph have the valid properties which are required by the register allocation modeling.
This enables us to apply a heuristic solver based on partitioned boolean quadratic programming (PBQP) to our problem as it is done in register allocation [20].
In order to verify the result of this approximation algorithm, we compared it with the result of DP (the guaranteed best) on some simple networks where DP is tractable.
It turns out that the approximation algorithm gets at least 88% of the best available result.
Empirically, a typical DP search completes in 1 minute for most CNN models.
In practice, we switch to the approximation algorithm if DP does not complete in 5 minutes.
The approximation algorithm completes quickly, e.g. in 10 seconds.
For the 15 popular networks we evaluated in Section 4, only SSD was done approximately.
This section evaluates the performance of our proposed solution, NeoCPU, by answering the following questions:1.
What is the overall performance of NeoCPU comparing with the start-of-the-art alternatives on various kinds of CPUs?
2.
What is the individual contribution of each optimization idea we proposed?All experiments were done on Amazon EC2 instances.
We evaluated NeoCPU on three kinds of CPUs, Intel Skylake (C5.9xlarge, 18 physical cores, featured with AVX-512), AMD EPYC (M5a.12xlarge, 24 physical cores, featured with AVX2) and ARM Cortex A72 (A1.4xlarge, 16 physical cores, featured with NEON).
Although testing on the cloud, our results of ARM CPUs apply to the ones at the edge devices such as Raspberry Pi and Amazon Echo Dot due to the same architecture.
All cores have uniformed memory access.NeoCPU was built on top of the code base of the TVM stack 0.4.0.
For CPUs with x86 architecture, we chose two framework-specific solutions and one framework-agnostic solution as baselines for comparison.
For the framework-specific solution, we investigated a wide range of options and figured out that MXNet 1.3.1 with Intel MKL-DNN v0.15 enabled has the widest model coverage with the best inference performance compared to others (e.g. Intel Caffe).
In addition, we chose TensorFlow 1.12.0 with ngraph v0.12.0-rc0 integration (empirically proved to be better than TensorFlow XLA on CPUs) due to its popularity.
TensorFlow is known to have better performance on CPUs than another popular deep learning framework PyTorch [14].
The latest Intel OpenVINO Toolkit 2018 R5.445 served as the framework-agnostic solution.
We used the official image-classification sample 3 and object-detection-ssd sample 4 for benchmarking.
For ARM CPUs, we chose MXNet 1.3.1 with OpenBlas 0.2.18 and TensorFlow 1.12.0 with Eigen fd68453 5 as the baselines.
No framework-agnostic comparison was performed as on ARM CPUs there is no counterpart of OpenVINO to x86 CPUs.
In addition, OpenMP 4.5 implemented in GCC 7.3 was used in the comparison with our own thread pool for multi-thread scalability.
As a note, all implementations used direct convolution.
Incorporating the advanced convolution algorithms to further improve the performance remains for future work.We ran the model inference on a number of popular CNN models, including ResNet [23], VGG [41], DenseNet [26], Inception-v3 [43], and SSD [36] using ResNet-50 as the base network.
Models consumed by MXNet and OpenVINO were from the Gluon Model Zoo 6 .
Models consumed by TensorFlow were obtained mostly from TF-SLim 7 and for some missing ones (e.g. ResNet-34, DenseNet-169) we manually created them.
The same model in different formats are semantically identical.
As inherited from the TVM stack, NeoCPU is compatible to both Gluon and TF-slim formats, and in the evaluation we used the former one.
The input data of the model inference are 224 × 224 images, except for the Inception Net (299 × 299) and SSD (512 × 512) by following the popular convention.
Since the most important performance criterion of model inference is the latency, we did all experiments with batch size 1, i.e. each time only one image was fed to the model, to measure the inference time.
Therefore, we fix the value N in NCHW [x]c as 1.
NeoCPU works for larger batch sizes as well, in which cases we just need to add the N value to our configuration tuple.Since our optimization does not change the semantics of the model, we do not expect any change of the model output.
As a sanity check, we compared the results generated by NeoCPU with other baselines (prediction accuracy for image classification models and mean accuracy prediction for object detection models) to validate the correctness.
We first report the overall performance we got for 15 popular CNN models comparing with the baselines on different CPUs in Table 2.
The results were obtained by averaging the execution times of 1000 samples, doing inference for one at a time.
In general, NeoCPU is more efficient across different models on different CPU architectures than any of the baselines (up to 11× speedup without considering the suspicious OpenVINO outliers which will be explained later).
Compared to the best available baseline result for each model, NeoCPU gets 0.94-1.15× performance on the Intel Skylake CPU, 0.92-1.72× performance on the AMD EYPC CPU, and 2.05-3.45× performance on the ARM Cortex A72 CPU.As framework-specific solutions, MXNet and TensorFlow were suboptimal for CNN inference on CPUs because it is lacking of flexibility to perform sufficient graph level optimization (e.g. flexible data layout management).
MXNet has active MKL-DNN support from Intel so it performed quite well on CPUs with the x86 architecture.
MXNet performed worse than TensorFlow on ARM due to the scalability issue (demonstrated in Figure 4c).
TensorFlow performs significantly worse on SSD as it introduces branches to this model, which requires dynamic decisions to be made during the runtime.
Comparatively, the framework-agnostic solution provided by the OpenVINO tries to further boost the performance by removing the framework limitation.
However, the performance of OpenVINO was unstable across models.
Although it gets appealing results on some cases, OpenVINO sometimes performed extremely slowly on certain models (e.g. 45× slower than us for ResNet-152 on AMD) for unknown reasons.
When summarizing the speedup results, we do not include these outliers.
It is also worth noting that the OpenVINO measures the execution time of SSD without taking into account a significant amount of operations including multibox detection.
Since OpenVINO is not open-sourced, we were not able to modify it for apples-to-apples comparison on the SSD model.
OpenVINO does not work for ARM CPUs as it relies on MKL-DNN which optimizes only for CPUs with x86 architecture.
NeoCPU outperforms the baselines mostly because of the advanced optimization techniques we presented in Section 3.
In addition, all baselines largely rely on the third-party libraries (MKL-DNN, OpenBlas, Eigen) to achieve good performance.
NeoCPU, on the other hand, is independent from those high-performance libraries, which gives us more room to optimize the model inference as a whole.
This subsection breaks up the end-to-end performance gain of NeoCPU by investigating the performance boost of each individual optimization technique we described in Section 3.
For the sake of space, in each comparison we only pick one network from a network family, respectively.
Other networks in the same family share the similar benefits.
We only report the performance results on Intel CPUs in Section 4.2.1-4.2.3.
The optimization effect applies to AMD and ARM CPUs, too.
Basically, Section 4.2.1 is the operation-level optimization, and Section 4.2.2 and 4.2.3 cover the operation-and graphlevel joint optimization.
Firstly, we compare the performance with and without organizing the data in a memory access and vectorized instruction utilization friendly layout (NCHW{x}c) for the CONV operations at the second row of Table 3: The individual speedup brought by our optimization compared to the NCHW baseline.
The speedup of row n was achieved by applying the optimization techniques till this row.intrinsics, which enables the subsequent optimization for various CNN models on different CPU architectures.
From row 2 of Table 3 we see significant improvement compared to the default data layout (NCHW), whose performance is normalized to baseline 1.
Both implementations are with proper vectorization and thread-level parallelization, as well as basic graph-level optimizations introduced by the original TVM stack, e.g. operation fusion, pre-computing, inference simplification, etc.
Secondly, we evaluate the performance boost brought by eliminating the data layout transformation overhead as discussed in Section 3.2.
The results were summarized at the third row of Table 3.
Compared to the layout optimization of CONV (second row of Table 3), layout transformation elimination further accelerates the execution time by 1.1 − 1.5×.
NeoCPU uses a systematic way to eliminate the unnecessary data layout transformation by inferring the data layout throughout the computation graph and inserting the layout transformation nodes only if needed, which is not seen in other works.
Next, we compare the performance between the optimization schemes produced by our search algorithm and the ones carefully picked by us manually.
By comparing the third and fourth row of Table 3, our algorithm (described in Section 3.3) is able to find the (approximately) best combination of data layouts which outperforms the manually picked results by 1.1 − 1.5×.
ResNet-50 (and its variants) gains more speedup from global search because the network structure is more complicated, hence leaving more optimization room.
In contrast, VGG-19 (and its variants) gains less since the structure of this model is relatively simple.
SSD utilizes the approximation algorithm and gets significant speedup, too.
The results also verify that, with automatic search, we can get rid of the tedious manual picking of parameters by producing even better results.
To the best of our knowledge, NeoCPU is the only one that does this level of optimization.
Lastly, we did a strong scalability experiment using the multithreading implementations backed by our own thread pool described at Section 3.1.2 and the commonly used OpenMP API implemented in the GCC compiler.
We also included the result of MXNet, TensorFlow and OpenVINO using Intel MKL-DNN, OpenBlas or Eigen (all realizing multi-threading via OpenMP) for comparison.
We configured OpenMP via environment variables to make sure that the jobs are statically partitioned and each thread runs on a disjoint core, which resemble the behavior of our thread pool for apples-to-apples comparison.
Figure 4 summarizes the number of images a model can inference one by one (i.e. batch size = 1) in a second as a function of the number of threads the model inference uses.
For the sake of space, we demonstrate one result for one CPU type.
The figure shows that our thread pool achieves better scalability than OpenMP in NeoCPU as well as in the baselines.
Although the tasks are embarrassingly parallelizable, each model inference consists of a number of parallelization regions.
The overhead of OpenMP to launch and suppress threads before and after a region is larger than our thread pool, which attributes to the less scalability of OpenMP.
Furthermore, sometimes we observed that the performance obtained by OpenMP jitters, or even drops, while adding threads.
In addition, the performance of OpenMP may differ across different implementations.
In summary, our evaluation suggests that in our use cases, it is preferable to have a self-customized thread pool with full control.
As deep learning demonstrates more and more power in the real-world applications, there is a significant amount of effort being made to accelerate the deep learning workloads on all kinds of hardware ranging from CPUs [24,27,44,53], GPUs [11,13], FPGAs [18,22,49], to special-purpose accelerators [12,32].
Modern deep learning frameworks normally leverage these optimized implementations to run deep learning training and inference on the corresponding hardware targets.
There are also works tailored for inference to address the inference-specific requirement such as low latency and small binary size on different hardware targets (e.g. GPUs [38], ASICs [22]).
NeoCPU is more flexible and combines the operation-and graph-level optimization intelligently.
Although this paper focuses on CPUs, the ideas are applicable to other hardware targets.NeoCPU is based on the TVM stack [9], an end-to-end framework inspired by Halide [39], which expresses a deep learning model into intermediate representations (IRs) and compiles to the machine code.
There are several other similar deep learning compilers such as TensorFlow XLA [34], Tensor Comprehensions [46], Glow [40] and DLVM [47].
However, so far none of them has reported CPU inference results on par with what we did (e.g. Glow only optimized single-core performance on CPUs).
We believe our proposed solution could be an integral part to these frameworks.We follow the well-studied ideas implemented in other high-performance libraries [27,51] to optimize the computationally-intensive CONV operations.
In addition to the libraries, there are also highly customized optimization works for convolutions and matrix multiplications on Intel CPUs [19,24].
These works are mostly about individual operation-level optimizations, which do not consider maintaining data layouts through the entire network.
Specifically, they carefully investigate the computation nature of convolutions as well as the available CPU resources to fine tune the operations.
This kind of optimization is able to maximize the convolution performance on the targeted CPUs but is not very flexible to extend to other platforms and to do joint optimization.
Unlike others, we make the optimization as a configurable template so that it is flexible to fit to different CPU architectures and enable the opportunity to surpass manually tuned performance via operation-and graph-level joint optimization.Our work utilizes auto search to look for optimal solutions.
Similar auto-tuning ideas were used in other works as well [10,46,48].
However, they all focused on performance tuning for single operations, while ours extends the scope to the entire CNN model to search for optimal solutions globally.
Recently, we also observed other work optimizing the DNN workloads at the graph level [30].
This work attempts to obtain better global performance using relaxed graph substitutions which may harm the local performance within a few operations.
Its non-greedy search idea is conceptually similar to ours and potentially applicable to our solution.
The approximation algorithm we employed to deal with the global search for the models with complicated structures (e.g. SSD) is inspired by the application of PBQP in the register allocation problem [6,17,20].
This paper leverages the previous idea and applies to a new domain by minor modification.
In this paper, we proposed an end-to-end solution to compile and optimize convolutional neural networks for efficient model inference on modern CPUs.
The experiments show that we are able to achieve up to 3.45× speedup on 15 popular CNN models on the various kinds of CPUs (Intel Skylake, AMD EPYC and ARM Cortex A72) compared to the performance of the state-of-the-art solutions.
The future work includes extending to other convolution computation algorithms such as Winograd and FFT, handling model inference in quantized values (e.g. INT8) and extending our operationand graph-level joint optimization ideas to work on other hardware platforms (e.g. NVidia GPUs compared with Ten- sorRT).
Supporting the optimized model inference in dynamic shapes (e.g. RNNs [25,50]) is another interesting direction to explore.
We would like to thank our shepherd Peter Pietzuch and the anonymous reviewers of the USENIX ATC program committee for their valuable comments which improved the paper a lot.
We are also grateful to Tianqi Chen and Animesh Jain for helpful discussion and constructive suggestion.
