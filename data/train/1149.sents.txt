This paper proposes a new classification method termed Rectified Nearest Feature Line Segment (RNFLS).
It overcomes the drawbacks of the original Nearest Feature Line (NFL) classifier and possesses a novel property that centralizes the probability density of the initial sample distribution, which significantly enhances the classification ability.
Another remarkable merit is that RNFLS is applicable to complex problems such as two-spirals, which the original NFL cannot deal with properly.
Experimental comparisons with NFL, NN(Nearest Neighbor), k-NN and NNL (Nearest Neighbor Line) using artificial and real-world datasets demonstrate that RNFLS offers the best performance.
Nearest Feature Line (NFL) [1], a newly developed nonparametric pattern classification method, has recently received considerable attention.
It attempts to enhance the representational capacity of a sample set of limited size by using the lines passing through each pair of the samples belonging to the same class.
Simple yet effective, NFL shows good performance in many applications, including face recognition [1] [2], audio retrieval [3], image classification [4], speaker identification [5] and object recognition [6].
On the other hand, feature lines may produce detrimental effects that lead to increased decision errors.
Compared with the well-known Nearest Neighbor (NN) classifier [7], NFL has obvious drawbacks under certain situations that limit its further potential.
The authors of [8] pointed out one of the problemsextrapolation inaccuracy, and proposed a solution called Nearest Neighbor Line (NNL).
This extrapolation inaccuracy may lead to enormous decision errors in a low dimensional feature space while a simple NN classifier easily reaches a perfect correct classification rate of 100%.
Another drawback of NFL is interpolation inaccuracy.
Distributions assuming a complex shape (two-spiral problem for example) often fall into this category, where, by the original NFL, the interpolating parts of the feature lines of one class break up the area of another class and severely damage the decision region.In this paper, a new nonparametric classification method, Rectified Nearest Feature Line Segment (RNFLS), is proposed that addresses both of the abovementioned drawbacks and significantly improves the performance of NFL.
The original NFL can conceptually be viewed as a two-stage algorithm -building representational subspaces for each class and then performing the nearest distance classification.
We focus mainly on the first stage.
To overcome extrapolation inaccuracy, Nearest Feature Line Segment subspace (NFLS-subspace) is developed.
For the interpolation inaccuracy, the "territory" of each sample point and each class is defined, and we obtain Rectified Nearest Feature Line Segment subspace(RNFLS-subspace) from NFLS-subspace by eliminating those feature line segments trespassing the territory of other classes.
As a result, RNFLS works well for all shapes of sample distribution, which is a significant improvement.Another remarkable advantage of RNFLS is that it centralizes the probability density of the initial sample distribution.
We show, in an experiment, that the decision region created by RNFLS gets closer to the one built by using the optimal Bayesian rule, bringing the correct classification rate higher.
Comparisons with NN, k-NN, NFL, NNL using artificial and real-world datasets demonstrate that the proposed RNFLS method offers remarkably superior performance.
The Nearest Feature Line (NFL) [1] method constructs a feature subspace for each class, consisting of straight lines passing through every pair of the samples belonging to that class.
The straight line passing through samples x i , x j of the same class, denoted by x i x j , is called a feature line of that class.
All the feature lines of class ω constitute an NFL-subspace to represent class ω, denoted byS ω = {x ω i x ω j |x i , x j ∈ ω, x i = x j },which is a subset of the entire feature space.
During classification, a query point q is classified to class ω if q assumes the smallest distance to S ω than to any other S ω , (ω = ω ) .
The distance from q to S ω isd(q, S ω ) = min xixj ∈Sω d(q, x i x j ).
(1)= minxixj ∈Sω q − p ij (2)where p ij is the projection point of q onto line x i x j .
The projection point can be computed byp ij = (1 − µ)x i + µx j ,(3)whereµ = (q − x i ).
(x j − x i ) (x j − x i ).
(x j − x i ) .
(4) NFL extends the samples of one class by adding the straight lines linking each pair.
A good argument for doing this is that it adds extra information to the sample set.
The extra information, however, is a double-edged sword.
When a straight line of one class trespasses into the territory of another class, it will lead to increased error probability.
There are two types of trespassing, causing two types of inaccuracies: extrapolation inaccuracy and interpolation inaccuracy.
Fig.1(a) shows a classification problem in which the extrapolation inaccuracy occurs.
The query q, surrounded by four "cross" sample points, is in the territory of the "cross" class, leading to the expectation that q should be classified to the "cross" class.
But the extrapolating part of feature line x 1 x 2 makes the distance from q to x 1 x 2 smaller.
Thus, d(q, S circle ) < d(q, S cross ), and NFL will assign q the label "circle", not "cross".
This is very likely to be a decision error.
Similarly, the interpolation inaccuracy caused by the interpolating part of a feature line is illustrated in Fig.1(b).
The above inaccuracies are drawbacks that limit the applicability of NFL.
In the following section we pursue a more systematic approach in which a new feature subspace for each class is constructed to avoid both drawbacks.
The original advantage of NFL that linearly extending the representational capacity of the original samples is retained in our method.
To avoid extrapolation inaccuracy, we propose to use line segments between pairs of the sample points to construct a Nearest Feature Line Segment subspace (NFLS-subspace) instead of the original NFL-subspace to represent each class.Let X ω = {x ω i |1 ≤ i ≤ N ω } be the set of N ω samples belonging to class ω.
The NFLS-subspace ( S ω ) representing class ω is S ω = { x ω i x ω j |1 ≤ i, j ≤ N ω },(5)where x ω i x ω j denotes the line segment connecting point x ω i and x ω j .
Note that a degenerative line segmentx ω i x ω i (1 ≤ i ≤ N ω ), which is a point in the feature space, is also a member of S ω .
The distance from a query point q to an NFLS-subspace S ω is defined asd(q, S ω ) = min xixj ∈ Sω k d(q, x i x j )(6)whered(q, x i x j ) = min y∈ xixj q − y.(7)And to calculate d(q, x i x j ), there are two cases.
If x i = x j , the answer is simply the point to point distance,d(q, x i x i ) = q − x i .
(8)Otherwise, the projection point p of q onto x i x j is to be located first by using Equ.
(3) and Equ.
(4).
Then, different reference points are chosen to calculate Fig.2 shows an example.d(q, x i x j ) according to the position parameter µ.
When 0 < µ < 1, p is an interpolation point between x i and x j , so d(q, x i x j ) = q − p.
When µ < 0, p is a "backward" extrapolation point on the x i side, so d(q, x i x j ) = q − x i .
When µ > 1, p is a "forward" extrapolation point on the x j side, so d(q, x i x j ) = q − x j .
In the classification stage, a query q is classified to class ω k when d(q, S ω k ) is smaller than the distance from q to any other S ωi (ω i = ω k ).
The next step is to rectify the NFLS-subspace to eliminate interpolation inaccuracy.
Our motivation is to have the inappropriate line segments removed from the NFLS-subspace S ω k for each class ω k ,.
The resulting subspace denoted by S * ω k is a subset of S ω k termed Rectified Nearest Feature Line Segment subspace (RNFLS-subspace).
Territory We begin with the definitions of two types of territories.
One is sample-territory, T x ∈ n , that is the territory of a sample point x; the other is class-territory, T ω ∈ n , that is the territory of class ω.
Suppose the sample set X is {(x 1 , θ 1 ), (x 2 , θ 2 ), ..., (x m , θ m )}, which means x i belongs to class θ i .
The radius r x k of the sample-territory T x k is,r x k = min ∀xi,θi =θ k x i − x k .
(9)Thus,T x k = {y ∈ n ||y − x k < r x k }.
(10)The class-territory T ω k is defined to beT ω k = θi=ω k T xi , (x i , θ i ) ∈ X.(11)In Fig.3, the points denoted by "circle" and "cross" represent the samples from two classes.
Each of the "cross"-points (y 1 , y 2 , y 3 ) has its own sample-territory as shown by the dashed circle.
The union of these sample-territories is T cross .
T circle is obtained in a similar way.Building RNFLS-subspace For class ω k , its RNFLS-subspace S * ω k is built from the NFLS-subspace S ω k by having those line segments trespassing the classterritories of other classes removed.
That isS * ω k = S ω k − U ω k ,(12)where '−' is the set difference operator, andU ω k = { x i x j |∃ω y , ω k = ω y ∧ x i x j ∈ S * ω k ∧ x i x j ∩ T ωy = φ} = { x i x j |∃(x y , θ y ) ∈ X, x i x j ∈ S * ω k ∧ ω k = θ y ∧ d(x y , x i x j ) < r xy }.
(13)Classifying using RNFLS-subspaces To perform classification using RNFLSsubspaces is similar to using NFLS-subspaces, since the only difference between an RNFLS-subspace and an NFLS-subspace is is still a set consisting of line segments.
The distance measure from a query point to the RNFLS-subspace remains the same.S * ω k = S ω k − U ω k , In many real-world pattern recognition problems, samples from one class tend to scatter around a certain center point because of systematic error and random noise.
Gaussian distribution is an example.
Two scattered classes may overlap each other, causing decision errors.
Compared with the original ideal sample distribution without noise, the NFLS-subspace of each class has an impressive property -distribution centralization, which can be viewed as the converse of scattering.
With the help of NFLS-subspace, the distribution overlapping is reduced, and the probability distribution grows closer to the original.
And so, we get a higher correct classification rate.The simplest case to show the centralization property is when the distribution is uniform in a two-dimensional feature space.
Suppose that the sample points of class ω are uniformly distributed in a disk D whose radius is R and the center is at O, as shown in Fig.4.
For the NFLS-subspace of the class, consider a small region M (a, r) (a ≤ R), that is a round area with an arbitrarily small radius r and distance a from O. Let N ω a be the probability of a randomly selected feature line segment of class ω passing through M (a, r).
Proposition 1 Given an arbitrarily small r, N ω a is decreasing on a.Proof We calculate N ω a in a polar coordinate system by choosing the center of M (a, r) as pole and − − → OM as polar axis.
For a line segment XY passing through M (a, r), given one endpoint X(ρ, θ) in D, the other endpoint Y has to appear in the corresponding 2 M1M2HG , as shown in Fig.4.
Thus we obtainN ω a = D 1 πR 2 A(ρ, θ)ρdρdθ = 2π 0 |M C| 0 1 πR 2 A(ρ, θ)ρdρdθ(14)where A(ρ, θ) is the probability that the randomly generated endpoint Y appears in 2 M1M2HG ,A(ρ, θ) = 1 πR 2 1 2 (2r + |GH|) · |M G| + o(r)(15)According to Equ.
(14) and (15) N ω a = 2r(R 2 − a 2 ) (πR 2 ) 2 2π 0 R 2 − a 2 sin 2 θ · dθ + o(r).
(16)Thus, for a fixed r, N ω a gets smaller when a gets larger.
Proposition 1 indicates that the distribution of line segments in the NFLSsubspace is denser at the center than at the boundary if the original sample points distribution is under a uniform density.
A Gaussian distribution can be viewed as a pile-up of several uniform distribution disks with the same center but different radius.
It is conjectured that this centralization property also applies to the Gaussian case, and can be extended to classification problems in which the overlapping is caused by noise scattering of two or more classes under similar distribution but different centers.
It reverses the scattering and achieves a substantial improvement.
The performance of the RNFLS method is compared with four classifiers -NN, k-NN, NFL and NNL -using two artificial datasets as well as a group of real-world benchmarks widely used to evaluate classifiers.
The results on these datasets, representing various distributions and different dimensions, demonstrate that RNFLS possesses remarkably stronger classification ability than the other four methods.
The two-spiral problem is now included by many authors as one of the benchmarks for evaluation of new classification algorithms.
The two-spiral curves in a two-dimensional feature space is described as followsspiral1 : x = kθ cos(θ) y = kθ sin(θ) spiral2 : x = kθ cos(θ + π) y = kθ sin(θ + π)(17)where θ ≥ π/2 is the parameter.
If the probability density of each class is uniform along the corresponding curve, an instance of such distribution is shown in Fig.5(a).
In our experiment, Gaussian noise is added to the samples so that the distribution regions of the two classes may overlap each other, as shown in Fig.5(b).
If the prior distribution density were known, according to the optimal Bayesian rule, Fig.5(d) should be the optimal decision region.
This, however, can hardly be achieved because the only information we have is from a finite number of sample points.
(a) (b) (c) (d) (e) (f)The original NFL is not a good choice for this classification problem.
We may imagine how fragmented the decision region is carved up because of its interpolation and extrapolation inaccuracy.
The decision region created by NN rule is shown in Fig.5(e).
When it comes to RNFLS, Fig.5(c) is the RNFLSsubspaces and Fig.5(f) is the corresponding decision region.
Compared with the decision region created by NN, RNFLS produces a much better one in which the boundary is smoother and some incorrect regions caused by isolated noise points is smaller.
This significant enhancement can be attributed to the centralization property.As a concrete test, let θ ∈ [π/2, 3π] and the Gaussian noise is of a variance σ = 1.7 and an expectation µ = 0.
We produce 500 points according to the well-defined distribution, where 250 belong to class ω 1 and the other 250 belong to class ω 2 .
Then, half of them are randomly chosen to form the sample set and the remaining half constitute the test set.
The classifiers, NN, k-NN(k=3), NFL, NNL and RNFLS, are applied to this task for 10 times, and Table 1 shows the results.
We test the RNFLS classifier on a group of real-world datasets as listed in Table 2.
All of the datasets are obtained from the U.C. Irvine repository [9].
Since we do not deal with the issue of missing data, instances with missing values are removed.
For the fairness of the procedure, attributes of the instances are standardized (normalized) by their means and standard deviations before submitted to the classifiers.
The performance in CCR is obtained using the leave-one-out procedure.
It can be seen that RNFLS performs well on both two-category and multicategory classification problems in both low and high dimensional feature spaces.
This is encouraging since these datasets represent real-world problems and none of them is specially designed to suit a specific classifier.
Since one common characteristic of real-world problems is distribution dispersing caused by noise, the centralization property of RNFLS helps improving the correct classification rate.
A new classification method RNFLS is developed.
It enhances the representational capacity of the original sample points and constitutes a substantial improvement to NFL.
It works well independent of the distribution shape and the feature-space dimension.
In particular, viewed as the converse of sample scattering, RNFLS is able to centralize the initial distribution of the sample points and offers a higher correct classification rates for common classification problems.Further investigation into RNFLS seems warranted.
In the rectification process it would be helpful to reduce the runtime-complexity, perhaps using some kind of probability algorithms.
It may also be helpful to treat the trespassing feature line segments more specifically, for example, finding a way to cut off a part of a trespasser instead of eliminating the whole feature line segments.
Also worth more investigation is the centralization property, which might be of great potential.
The research work presented in this paper is supported by National Natural Science
