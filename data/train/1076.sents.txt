Current methods for selectivity estimation fall into two broad categories, synopsis-based and sampling-based.
Synopsis-based methods, such as histograms, incur minimal overhead at query optimization time and thus are widely used in commercial database systems.
Sampling-based methods are more suited for ad-hoc queries, but often involve high I/O cost because of random access to the underlying data.
Though both methods serve the same purpose of selectivity estimation, their interaction in the case of selectivity estimation for conjuncts of predicates on multiple attributes is largely unexplored.
Our work aims at taking the best of both worlds, by making consistent use of synopses and sample information when they are both present.
To achieve this goal, we propose HASE, a novel estimation scheme based on a powerful mechanism called generalized raking.
We formalize selectivity estimation in the presence of single attribute synopses and sample information as a constrained optimization problem.
By solving this problem, we obtain a new set of weights associated with the sampled tuples, which has the nice property of reproducing the known selectivities when applied to individual predicates.
We discuss different variants of the optimization problem and provide algorithms for solving it.
We also provide asymptotic error bounds on the estimate.
Extensive experiments are performed on both synthetic and real data, and the results show that HASE significantly outperforms both synopsis-based and sampling-based methods.
Query optimizers in most relational database systems rely on cost estimation of various candidate query execution plans to select a good one.
Accurate plan costing can help avoid intolerably slow plans.
A key ingredient in cost estimation is to estimate the selectivity of various predicates.
In this paper, we are mainly concerned with selectivity estimation for conjunctive predicates of the form Q = P 1 ∧ P 2 . . . P m where each component P i is a simple predicate on a single attribute, taking the form of (attribute op constant) with op being one of the comparison operators <, ≤, =, 񮽙 =, ≥, or > (e.g., R.a = 100 or R.a ≤ 200).
In terms of methodology, existing work on selectivity estimation takes two fundamentally different approaches: one is based on synopsis data structures and the other is based on sampling.
Synopsis-based approaches seek to pre-compute summary data structures which capture statistics on the data (attribute value distributions).
Such synopses are stored in the database catalogs, and subsequently used for estimation when required.
A prominent example in this class of approaches is histograms, which have received heavy attention; numerous types of histograms [1,2] have been proposed in recent years aiming to improve the accuracy of histogram-based selectivity estimation.
Almost all major commercial database management systems (e.g., IBM r 񮽙 DB2 r 񮽙 Universal Database TM product(DB2 UDB), Oracle, SQL Server) keep some form of histograms in their catalogs and use them for selectivity estimation.Sampling-based approaches are more query-driven in nature, in the sense that data is not accessed until optimization time.
Given a query, a sample is derived from the database, and selectivities are estimated based on this sample.
There exists an extensive literature on sampling-based methods for selectivity estimation; see [3] for a comprehensive survey.
In recent years, all of the major commercial database system vendors have incorporated sampling capabilities into their engines [4].
Both approaches have their advantages and disadvantages.
Synopsis structures, such as histograms, only need to be computed once and can be used many times while incurring minimal overhead at selectivity estimation time.
However, it is difficult to capture all useful information in the limited space.
For example, the one-dimensional histograms commonly used in the commercial DBMS's do not provide correlation information between attributes.
Although it is possible to compute multi-dimensional histograms for some attribute combinations, it is generally not feasible to compute and store the multi-dimensional histograms for all attribute combinations, because the number of combinations is exponential in the number of attributes [5].
Without knowing of the query workload, deciding which combinations of attributes to choose in order to construct multidimensional histograms can be very difficult.
Sampling approaches, on the other hand, are able to provide such crucial information through a representative sample of the data.
The downside, however, is that sampling at selectivity estimation time incurs non-trivial cost, because in order to obtain a fairly accurate estimate, sometimes a significant portion of the data might have to be accessed.
Since sampling requires random access, which is much slower than sequential access, it is possible that the cost of sampling exceeds that of a sequential scan of the data when the sample size is relatively large.
(Haas et al. [4] show that under certain assumptions, the cost of sampling is greater than that of sequential scan when the sample rate is greater than 2% and tuple-level sampling is used.)
To the best of our knowledge, there is no previous work exploring the interaction of these two approaches in order to make consistent use of both sources of information.
This paper represents a first step in this direction.
In particular, we propose HASE (A Hybrid Approach to Selectivity Estimation), a novel method based on the powerful generalized raking procedure originally deployed in the context of survey sampling.
Sampling-based methods usually associate with each sampled tuple a sampling weight reflecting its inclusion probability (i.e., the probability of being selected to the sample), which is used to produce a selectivity estimate.
Given selectivities of individual predicates P i (which can be easily obtained from attribute synopses) in addition to the sample, we aim to obtain better estimates by adjusting sampling weights, in a way that is consistent with the information on individual selectivities obtained from the synopses.
In particular, we adjust the weights of the tuples in the sample, while maintaining the new weights as close as possible to the original weights.
We formalize this problem as a constrained optimization problem.
Its solution derives the new weights that can then be used to obtain improved selectivity estimates.We present a general numerical solution to this optimization problem, as well as an iterative solution based on the intrinsic structure of the problem.
We consider two different measures of "closeness" between the new weights and the original weights, namely the linear distance function and the multiplicative distance function, and compare them in terms of computational efficiency and interpretability.
We also provide asymptotic bounds on the estimation errors.The rest of this paper is organized as follows.
In Section 2, we formally define the problem of selectivity estimation for conjunctive predicates, and describe how selectivity estimates are obtained in existing approaches.
Section 3 presents HASE, our proposed approach based on generalized raking.
Experimental results on both synthetic and real data sets are presented in Section 4.
We briefly review existing approaches to selectivity estimation in Section 5.
Section 6 concludes this paper and discusses directions for future work.
In this section, we formally define the problem of selectivity estimation for conjunctive predicates and discuss two existing ways of conducting the estimation, one based on synopses and one on sampling.
We are interested in predicates taking the form of Q = P 1 ∧ P 2 ∧ · · · ∧ P m , where each P i (1 ≤ i ≤ m) is a simple predicate of the form (attribute op constant) with op being one of the comparison operators <, ≤, =, 񮽙 =, ≥, or >.
The selectivity s i (∈ [0,1]) is defined as the fraction of tuples on which predicate P i evaluates to true, i.e., s i = N i /N , where N is the number of tuples in the table, and N i is the number of tuples satisfying P i .
The selectivity of the conjuncts of predicates Q, denoted by s Q (∈ [0,1]), is the fraction of tuples satisfying all the P i 's simultaneously.
s Q is the quantity we would like to estimate.
When there is no ambiguity, we use s as a shorthand for s Q .
We measure the error of an estimatê s by the absolute relative errorE(ˆ s) = |ˆs|ˆs − s| s .
(1)Throughout the paper, we use the following scenario as a running example.
Consider a table R with N = 10, 000 tuples and three attributes A i (i = 1, 2, 3).
Let P 1 = (A 1 = 1), and P 2 = (A 2 = 1).
Suppose we need to estimate the selectivity of the following query: Q = P 1 ∧ P 2 .
If there are 500 tuples satisfying Q, then the true selectivity of Q is s = 500/10000 = 0.05.
Assume that we have access to synopsis structures for all individual attributes involved such that selectivity estimates s i (1 ≤ i ≤ m) can be obtained.
Without any information regarding the correlation between attributes, optimizers in current database systems estimate s Q based on the assumption that the values in distinct attributes are independently distributed.
In other words, knowing that a tuple satisfies a predicate on one attribute does not give any information as to whether it satisfies a predicate on another.
Therefore, s is estimated by taking a product of the selectivity estimates of individual predicates, i.e., ˆ s his = 񮽙 m i=1 s i .
In the running example, suppose we have access to single-attribute histograms on A 1 and A 2 , and therefore we can derive the selectivities of the two predicates, namely s 1 and s 2 , from the histograms.
Suppose s 1 = 0.6, and s 2 = 0.3.
If we assume A 1 and A 2 are independent, then the selectivity of Q is estimated to bê s his = s 1 · s 2 = 0.18, and the error is E(ˆ s his ) = |0.18 − 0.05|/0.05 = 260%.
This simple estimation scheme gives accurate estimates when the attributes are indeed independent.
Real-life data sets, however, almost always demonstrate a certain degree of correlation between attributes; therefore, making the attribute-value independence assumption often leads to erroneous estimates.
In the above example, treating the attributes A 1 and A 2 as independent incurs a large error (260%).
As another example, suppose we have the following query on a CAR table in a vehicle information database: Q = (MAKE = "BMW")∧(MODEL = "M3"), and we know through one-dimensional histograms that the selectivity of the predicate (MAKE = "BMW") is 0.1, and that the predicate (MODEL = "M3") has a selectivity of 0.01.
The optimizer then would estimate the selectivity of Q as 0.1 × 0.01 = 0.001, as per the attributevalue independence assumption.
Note, however, that there is strong correlation between the attributes MAKE and MODEL.
Because M3 is exclusively made by BMW, all tuples satisfying the predicate MODEL="M3" would also satisfy the predicate MAKE="BMW".
Therefore, the selectivity of Q is actually 0.01, 10 times that of the estimated selectivity.
Now let us look at how to obtain an estimate of the selectivity based on a sample of the data.
Suppose a random sample S of size n is taken from the queried table R of size N , where the inclusion probability (the probability of being selected into the sample) of the j-th tuple is π j .
The Horvitz-Thompson (HT) estimator [6] for the selectivity of the query Q, given the sample S, isˆ s spl = 1 N 񮽙 j∈S y j π j (2)where y j is an indicator variable such that y j = 1 if tuple j satisfies Q, and y j = 0 otherwise.
In the case of simple random sampling (SRS), where the inclusion probabilities are all equal to n/N , Eq.
(2) simplifies tô s spl = 1 n 񮽙 j∈S y j .
In our running example, suppose we take an SRS S of size n = 100 from table R. Clearly, the inclusion probabilities for tuples in R are all equal to 100/10000 = 0.01.
If 9 tuples in the sample satisfy Q, then the HT estimator isˆsisˆ isˆs spl = 9/100 = 0.09, and the error is E(ˆ s spl ) = 80%.
A major problem with the use of sampling is the I/O overhead incurred.
Since sampling requires random access to data, it is often the case that even if a very small sample is taken, the associated I/O cost is comparable to that of a full sequential scan of the data.
For example, if each page contains 50 tuples, and the sample rate is higher than 2%, essentially all pages have to be accessed because 50×2% = 1 (See [4] and [7] for a detailed analysis of this issue).
Recently, there has been work on using page-level sampling in conjunction with tuple-level sampling to reduce the sampling cost [4,7].
We take a complementary approach to this problem and attempt to decrease the sampling cost by utilizing existing synopsis information on the data.
Haas et al. [4] show that the expected fraction f of pages to be accessed for a sample rate of q is given byf = 1 − (1 − q) c ,where c is the number of tuples on each page.
It is evident that f decreases very fast as the sample rate drops, which means that if we can achieve the same level of accuracy with a lower sample rate, the I/O savings can be significant.
Our objective is to use the sample information in conjunction with the synopses to obtain better estimates.
To this end, we develop a hybrid approach, HASE, by applying generalized raking [8,9], a procedure originally utilized in survey sampling, to the problem of selectivity estimation.
Suppose we have obtained a sample of the data, and we also know the selectivities of individual predicates P i .
We begin with an estimator constructed based on the sample only, without reference to any additional information, such as the HT estimator (Eq.
(2)).
For each tuple j in table R, in addition to the variable of interest y j , we also associate with it an auxiliary vector x j to reflect the results of evaluating P i on j. Suppose each predicate P i divides tuples in R into two disjoint subsets, D i and ¯ D i , according to whether they satisfy the predicate or not.
We further define D m+1 = R, i.e., j ∈ D m+1 for all j. Let x j be a column vector of length m+1:x T j = (x j1 , . . . , x jm , x j,m+1 ), with the i-th (1 ≤ i ≤ m+1) element being 1 if j ∈ D i , and 0 otherwise.
For instance, in the running example,x T j = (1, 0, 1) indicates that tuple j satisfies P 1 , but not P 2 .
Let t T x = (t x1 , . . . , t xm , t x,m+1 ) = 1 N 񮽙 j∈R x j .
Clearly, t xi = 1 N 񮽙 j∈S x ji = s i (1 ≤ i ≤ m), the selectivity of predicate P i , and t x,m+1 = 1.
Therefore,t T x = (s 1 , s 2 , . . . , s m , 1)(3)Suppose s i can be obtained based on synopsis structures, and x j are observed for each tuple j ∈ S.
This allows construction of a new estimator (which we call the calibration estimator)ˆ s cal = 1 N 񮽙 j∈S w j y j ,(4)where the weights w j are as close to the weights d j = 1/π j as possible according to some distance metric (recall that π j is the inclusion probability of j), and where1 N 񮽙 j∈S w j x j = t x ,(5)meaning that the weighted average of the observed x j has to reproduce the known selectivities s i .
In light of the definition of x j and Eq.
(3), Eq.
(5) can be rewritten as1 N 񮽙 j∈S∩Di w j = s i , i = 1, 2, . . . , m + 1.
(6)where s m+1 = s.
Now w j has a natural representation interpretation: it is the number of tuples "represented" by the sampled tuple j.In our running example, Eq.
(6) 񮽙 j∈S w j = 1.
(7)Although in general, there can be many possible choices for the sets of weights {w j } satisfying the constraints in Eq.
(6), our goal is to select a set of new weights that are as close as possible to the original weights d i = 1/π i , which enjoy the desirable property of producing unbiased estimates.
By keeping the distance between the new weights and the original weights as small as possible, we expect the new weights to remain nearly unbiased.
We formulate this idea as a constrained optimization problem as described below.
Let D(x) be a distance function (with x = w j /d j ) that measures the distance between the new weights w j and the original weights d j .
We assure that D(x) satisfies the following requirements (for reasons that will become clear later): (i) D is positive and strictly convex, (ii) D(1) = D 񮽙 (1) = 0, and (iii) D 񮽙񮽙 (1) = 1.
The optimization problem we have to solve is:Minimize 񮽙 j∈S d j D(w j /d j ) ( 8 ) subject to 1 N 񮽙 j∈S w j x j = t x .
(9)Here, both x j and t x are defined as in Section 3.1.
Since D(w j /d j ) can have a large response to even a slight change in w j when d j is small, we minimize 񮽙j∈S d j D(w j /d j ) instead of 񮽙 j∈S D(w j /d j )in order to dampen this effect.
Also note that different distance functions can be used to measure the distance between {w j } and {d j }, as long as the distance function complies with conditions (i) to (iii).
In this paper, we consider the following two distance functions because of the computational efficiency and interpretability.
Both distance functions exhibit properties (i) to (iii).
We discuss the choice of distance functions in Section 3.5.
We now present algorithms to solve the constrained optimization problem.
A classical technique for solving constrained optimization problems is the method of Lagrange multipliers [10].
Note that the optimization problem can be rewritten as follows:Minimize񮽙 j∈S d j D(w j /d j ) − λ T ( 񮽙 j∈S w j x j − N t x )(10)with respect to w j (j ∈ S), where λ = (λ 1 , . . . , λ m , λ m+1 ) is a Lagrange multiplier.
Differentiating Eq.
(10) with respect to w j , we haveD 񮽙 (w j /d j ) − x T j λ = 0(11)Then we can solve the system formed by Eq.
(11) and (9) for w j .
To do this, we obtain from (11) thatw j = d j F (x T j λ),(12)where F (x) is the inverse function of D 񮽙 (x).
Conditions (i)-(iii) dictate that the inverse function always exists, and F (0) = F 񮽙 (0) = 1.
Substituting (12) into Eq.
(9), we have the calibration equations 񮽙j∈S d j F (x T j λ)x j = N t x ,(13)which can be solved numerically using Newton's method.Let φ(λ) = 񮽙 j∈S d j F (x T j λ)x j − N t x .
Then φ 񮽙 (λ) = ∂φ(λ)/∂λ = 񮽙 j∈S d j F 񮽙 (x T j λ)x j x T j .
We obtain successive estimates of λ, denoted by λ k (k = 0, 1, . . .), through the following iteration:λ k+1 = λ k + [φ 񮽙 (λ k )] −1 φ(λ k )(14)We take λ 0 = 0.
Since we haveφ(0) = 񮽙 j∈S d j F (0)x j − N t x = 񮽙 j∈S d j x j − N t x ,andφ 񮽙 (0) = 񮽙 j∈S d j F 񮽙 (0)x j x T j = 񮽙 j∈S d j x j x T j , the first iteration yields λ 1 = ( 񮽙 j∈S d j x j x T j ) −1 ( 񮽙 j∈S d j x j − N t x ).
The subsequent values of λ k can be obtained following Eq.
(14) until convergence.In summary, the procedure to estimate the selectivity of Q is presented in Algorithm 1.
Newton's method1: INPUT: Q, D, S, N , Ni(i = 1, . . . , m), dj(j ∈ S), stopping threshold 񮽙.
2: OUTPUT: ˆ s cal 3: for all j ∈ S do 4:Set the values of yj, xj according to the rules in Section 3.1; 5: end for 6: /*Solving the calibration equations using Newton's method*/ 7: λ0 := 0; k := 0; 8: repeat 9: Continuing the running example, the true frequencies obtained by evaluating the query Q on table R, and the observed frequency information based on a simple random sample S are given in Fig. 1 (both normalized so that all frequencies sum up to 1).
The last row and column in each table correspond to the marginal frequencies.
From Fig. 1, we know that the true selectivity of Q is 0.05 (the cell corresponding to P 1 = true ∧ P 2 = true in Fig. 1(a)), and the sampling-based selectivity estimate is 0.09 (the cell corresponding to P 1 = true ∧ P 2 = true in Fig. 1(b)).
Clearly, the marginal frequencies obtained from the sample do not agree with the true marginal frequencies; therefore, calibration is needed.
Applying Algorithm 1 to solve the calibration equations as shown in Eq.
(7), we obtain the following calibrated weights (using the multiplicative distance function):λ k+1 := λ k + [φ 񮽙 (λ k )] −1 φ(λ k ); 10: k := k + 1; 11: until ||λ k − λ k−1 || < < 12: for all j ∈ S do 13: wj := djF (x T j λ)w j 񮽙 60 for j ∈ S ∩ D 1 ∩ D 2 , w j 񮽙 102 for j ∈ S ∩ D 1 ∩ ¯ D 2 w j 񮽙 97 for j ∈ S ∩ ¯ D 1 ∩ D 2 , w j 񮽙 140 for j ∈ S ∩ ¯ D 1 ∩ ¯ D 2 .
The selectivity estimate can then be computed: ˆ s cal = 1 N 񮽙 j∈S w j y j = 1 N 񮽙 j∈S∩D1∩D2 w j = 60 Although Newton's method works well, it is not the only option to conduct the optimization.
Now we present an alternative algorithm for solving the calibration equations, which takes advantage of the intrinsic structure of the equations in (6) and does not require matrix inversion.Sincew j = d j F (x T j λ), Eq.
(6) becomes 1 N 񮽙 j∈S∩Di d j F (x T j λ) = s i , i = 1, . . . , m + 1.
(15)Observe that the i-th Eq.
(2 ≤ i ≤ m) can be solved for λ i assuming all other λ l (l 񮽙 = i) are known, and the first and last equations can be solved for λ 1 and λ m+1 assuming all other λ l (l 񮽙 = 1, l 񮽙 = m + 1) are known.
Hence we have the algorithm shown in Algorithm 2.
It is well known that such an iterative procedure converges to a proper solution [9], and in the case of multiplicative distance functions, this algorithm yields a variant of the classical iterative proportional fitting algorithm [11].
Replacing lines 6 to 11 in Algorithm 1 with Algorithm 2 results in a complete alternative estimation algorithm.
We now study the implications of the choice of distance functions D.
In general, different distance functions result in different calibration estimators.
However, it is well known [8] that regardless of the distance functions used (as long as the functions comply with conditions (i)-(iii)), the estimates obtained using the outcome of our specific optimization problem will converge asymptotically.
Therefore, for medium to large sized samples (empirically, with sample size greater than 30), the choice of distance function does not have a heavy impact on the properties of the estimator; one can expect only slight difference in the estimates produced by using different functions.
The main difference between the distance functions is thus their computational efficiency as well as interpretability.
for i = 2 to m do 8: SolveÈ j∈S∩D i djF (x T j λ) = si for λ (k+1) i , using values of λ (k) l (l = 1, . . . , m + 1, l 񮽙 = i); 9:end for 10:k := k + 1; 11:MaxChange := max{|λ(k) l − λ (k−1) l |}, l = 1, .
.
For the linear function, D lin , D 񮽙 (x) = x − 1; therefore, the inverse function is F (z) = z + 1.
In Algorithm 1, it is easy to verify that λ converges atλ 1 = ( 񮽙 j∈S d j x j x T j ) −1 ( 񮽙 j∈S d j x j − t x ).
Therefore, when the linear function is used, only one iteration is required, which makes the linear method the faster of the two distance functions considered here.
A major drawback of this function is that the weights can be negative.
This can lead to negative selectivity estimates.
For instance, in the running example, we take a sample of size 10 from R, and the observed frequencies are the following: P 1 = true ∩ P 2 = true: 2; P 1 = true, P 2 = f alse: 5; P 1 = f alse ∩ P 2 = true: 3; P 1 = f alse ∩ P 2 = f alse: 0.
Solving the calibration equation, we have w j = −500 for j ∈ S ∩ D 1 ∩ D 2 .
Therefore, the selectivity estimatê s cal = 2 × (−500)/10000 = −0.1.
Negative weights and selectivity estimates do not have a natural interpretation and thus are undesirable.
Note that, however, this usually only occurs for small-sized samples.
When the sample size gets large, all estimators with distance functions satisfying conditions (i)-(iii) are asymptotically equivalent and give positive weights and selectivity estimates.For the multiplicative function, D mul , D 񮽙 (x) = log x; the inverse function is therefore F (z) = e z .
When the multiplicative function is used, it may require more than one iteration, but our experience indicates that it often converges after only a few iterations (typically two in our experiments).
An advantage of using this function is that it always leads to positive weights becausew j = d j F (x T j λ) = d j exp{x T j λ} > 0.
We will contrast the effects of both functions on the estimation accuracy in Section 4.
Let π jl be the probability that both j and l are included in the sample, and π jj = π j .
We assume that the sampling scheme is such that the π jl 's are strictly positive.
Let β be a vector satisfying the equation񮽙 j∈R d j x j (y j − x T j β) = 0 and let ∆ jl = π jl − π j π l , 񮽙 j = y j − x T j β.We have the following result on the error bounds of the estimation error.
(0, 1), the selectivity s Q is bounded by (ˆ s cal − z α/2 񮽙 V (ˆ s cal ), ˆ s cal + z α/2 񮽙 V (ˆ s cal ) with probability 1 − α,) = 񮽙 j∈R 񮽙 j∈R (∆ jl /π jl )(w j 񮽙 j )(w l 񮽙 l ).
Proof Sketch: When the linear distance function is used, w j = d j (1 + x T j λ).
We know from Section 3.5 that the solution of the calibration equation converges at λ = ( 񮽙j∈S d j x j x T j ) −1 ( 񮽙 j∈S d j x j − t x ).
Therefore, w j = d j [1 + x T j ( 񮽙 j∈S d j x j x T j ) −1 ( 񮽙 j∈S d j x j − t x )].
LetˆβLetˆ Letˆβ S be the solution to the equation񮽙 j∈S d j x j (y j − x T j ˆ β S ) = 0.
Then the estimatorˆsestimatorˆ estimatorˆs cal can be written asˆ s cal = 1 N 񮽙 j∈S w j y j = ˆ s spl + 1 N (t x − 񮽙 j∈S d j x j ) T ˆ β S ,which takes the form of a generalized regression estimator (GREG) [12].
Applying results on the asymptotic variance of GREG [12], we obtain the asymptotic variance of the estimatorˆsestimatorˆ estimatorˆs cal :V (ˆ s cal ) = 񮽙 j∈R 񮽙 j∈R (∆ jl /π jl )(w j 񮽙 j )(w l 񮽙 l ).
Since it has been shown that all estimators with distance functions satisfying conditions (i)-(iii) are asymptotically equivalent [8], all estimators have the same asymptotic variance V (ˆ s cal ).
When the sample S is large enough, the Central Limit Theorem applies.
Therefore, for a given constant α ∈ (0, 1), s Q is bounded by (ˆ s cal − z α/2 񮽙 V (ˆ s cal ), ˆ s cal + z α/2 񮽙 V (ˆ s cal ) with probability 1 − α. 񮽙
In our discussion, we have assumed that we have knowledge of the selectivities s i of individual predicates P i based on single-attribute synopsis structures.
In fact, the estimation procedure can be easily extended so that multi-attribute synopsis structures can also be utilized when they are present.
Suppose that a multi-dimensional synopsis [13,2] exists on a set of attributes A.
It is relatively easy to derive lower-dimensional synopses from higher-dimensional synopses, i.e., synopses on any subset(s) of A can be obtained from the synopsis on A. Let A Q be the set of attributes involved in query Q.
If A ∩ A Q 񮽙 = ∅, the synopsis on A can be utilized.
Let U = A ∩ A Q , and let P U be the conjuncts of predicates in which attributes in U are involved.
Then the selectivity s U of P U can be estimated based on the synopsis on U.
We augment the auxiliary vector x j by an additional element reflecting whether j satisfies P U .
Changes are also made accordingly to t x , with the addition of an element with value s U .
The algorithms for solving the calibration equations presented above can then be applied in order to obtainˆsobtainˆ obtainˆs cal .
In this section, we report the results of an experimental evaluation of the proposed estimation procedure.
We compare the accuracy of HASE with that of the synopsis-based and samplingbased approaches using synthetic as well as a real data set.
The real data set we use is the Census Income data obtained from the UCI KDD Archive [14].
-Synthetic data are used to study the properties of the HASE in a controlled manner.
We generate a large number of synthetic data sets by varying the following parameters: Data skew: The data in each attribute are generated from a Zipfian distribution with parameter z ranging from 0 (uniform distribution) to 3 (highlyskewed distribution).
The number of distinct values in each attribute is fixed to 10.
Correlation: By default, the data are independently generated for each attribute.
We introduce correlation between a pair of attributes by transforming the data such that the correlation coefficient between the two attributes is approximately ρ.
The parameter ρ ranges from 0 to 1, representing an increasing degree of correlation.
In particular, ρ = 0 corresponds to the case where there is no correlation between the two attributes; ρ = 1 indicates that the two attributes are fully dependent, i.e., knowing the value of one attribute enables one to perfectly predict the value of the other attribute.
This is achieved by first independently generating the data for both attributes (say, A 1 and A 2 ) and then performing the following transformation.
For each tuple with A 1 = a 1 and A 2 = a 2 , we replace a 2 by a 1 × ρ + a 2 × 񮽙 1 − ρ 2 , suitably rounded.
For three or more attributes, we create data such that the correlation coefficient between any pair of attributes is approximately ρ.
The real data set Census Income contains weighted census data extracted from the 1994 and 1995 population surveys conducted by the U.S. Census Bureau.
It has 199,523 tuples and 40 attributes representing demographic and employment related information.
Out of the 40 attributes, 7 are continuous, and 33 are nominal.
-We evaluate HASE on two different query workloads.
The first set of queries consist of 100 range queries where each predicate in the query takes the form of (attribute <= constant) with randomly chosen constant.
The second set of queries consist of 100 equality queries where each predicate takes the form of (attribute = constant) where constant is randomly chosen.
-We use simple random sampling as the sampling scheme in our experiments for both the sampling-based approach and HASE.
All numbers reported are averages of 30 repetitions.
-We use the exact frequency distributions of individual attributes as the synopses.
-The absolute relative error defined in Eq.
(1) is used as the error metric.
In all experiments, similar trends are observed for both range and equality queries; we only report the results on range queries because of space limitations.
We first study the effects of various parameters in the case of two attributes (i.e., only two predicates on two different attributes are involved in the query), and then show the effect of the number of attributes on the estimation accuracy.
The individual selectivities are obtained based on the frequencies of values in each attribute.
Since our results indicate that the number of tuples T in the table does not have a significant effect on the accuracy of the estimators, only the results for T = 100, 000 are shown here.Correlation We study the effect of the correlation between attributes on the estimation accuracy by varying the correlation coefficient ρ from 0 to 1, representing an increasing degree of correlation.
Fig. 2(a) presents a typical result.When the two attributes are totally uncorrelated (ρ = 0), the accuracy of the synopsis-based approach is very high, with an error close to zero, better than the other two methods.
This is because in such cases, the attribute-value independence assumption holds true, and the selectivity estimate for the query is indeed the product of the individual selectivities of the two predicates.
The accuracy of this approach deteriorates when the degree of correlation increases and the actual relationship between the two attributes deviates further from the independence assumption.The accuracy of the sampling-based approach actually improves when the two attributes become more correlated.
The reason is as follows.
When the degree of correlation increases, the number of distinct value combinations 3 in the two attributes decreases, as the data become more "concentrated".
Therefore, the sample space (containing all distinct value combinations) becomes smaller, and thus sampling becomes more efficient (i.e., for a given sample rate, it is more likely to include in the sample a tuple satisfying the query).
The accuracy of HASE also increases with the degree of correlation.
Since HASE utilizes sample information, the preceding argument for the samplingbased approach also applies.
Besides, as the degree of correlation increases, the benefit of adjusting the weights in accordance with known single-attribute synopses becomes more evident.
In the extreme case where the two attributes are fully dependent (ρ = 1), it essentially produces the exact selectivity, provided 3 (a, b) is considered a value combination if ∃j ∈ R such that A1 = a and A2 = b. that there is at least one tuple in the sample satisfying the query.
To see why this is the case, consider the following queryQ = P 1 ∩ P 2 = (A 1 = a) ∩ (A 2 = b).
Full dependency dictates that if there is at least one tuple in the table satisfying this query, then for any other value c (c 񮽙 = a)in A 1 andd (d 񮽙 = b) in A 2 , both (A 1 = a) ∩ (A 2 = d) and (A 1 = c) ∩ (A 2 = b)evaluate to false.
This implies that s = s 1 = s 2 .
Therefore, if in the auxiliary vector x j for tuple j, we have x j1 = 1 (which corresponds to A 1 = a), then y j (the variable indicating whether j satisfies Q) must also be 1, and vice versa.
Since we know s 1 , we have 1 N 񮽙 j∈S w j x j1 = s 1 as a constraint in the optimization problem.
If we can find a set of w j that satisfy this constraint, then the calibration estimator 1 N 񮽙 j∈S w j y j must also yield s 1 , which means we have a perfect selectivity estimate.
One exception to this analysis is that when there is no tuple j ∈ S satisfying Q, we may no longer be able to produce the exact estimate.
In such cases, all y j (j ∈ S) are 0; therefore, regardless of the weights, the calibration estimator 1 N 񮽙 j∈S w j y j will also be zero, which may be different from the exact selectivity.In all cases, HASE produces significantly more accurate estimates than the sampling-based method, with a 50%-100% reduction in error.
Both distance functions give very close estimates, verifying the claim that estimators using different distance functions are asymptotically equivalent.
In the following discussion, we only show the results for the case of the linear distance function.Data skew We study the effect of data skew by varying the Zipfian parameter z from 0 (uniform) to 3 (highly-skewed), a typical result is shown in Fig. 2(b).
The errors of both HASE and the sampling approach increase as the data becomes increasingly more skewed.
The reason is that when the data skew in each attribute increases, the frequencies of some value combinations decrease.
As a result, when we query on those value combinations with low occurrence frequencies, it becomes increasingly possible that no sampled tuple can satisfy the query.
This gives rise to more errors, because with no sampled tuple satisfying the query, the estimate has to be zero, whereas the actual selectivities are not.
Note that this situation is different from the case of increasing correlation as discussed above.
The main effect of increasing the skew is a decrease in the frequencies of some value combinations, not necessarily reducing the number of value combinations present in the table.
Increasing correlation, on the other hand, generally results in a reduction in the number of value combinations.
Therefore, increasing skew and increasing correlation have different effects on the accuracy of HASE as well as the sampling-based approach.Another interesting observation from Fig. 2(b) is that the accuracy of the synopsis-based approach remains virtually the same regardless of the data skew.
The reason is as follows.
Assuming independence between attributes, the synopsisbased approach estimates the selectivity byˆsbyˆ byˆs his = s 1 * s 2 .
In Fig. 2(b), the two attributes are fully dependent, which implies that the actual selectivitys = s 1 = s 2 .
Thus, E(ˆ s his ) = (s − s 1 s 2 )/s 1 = 1 − s 1 .
The average error over a large number of (uniformly) randomly selected equality queries is therefore 1 − avg(s 1 ).
In our case, since there are 10 distinct values in each attribute, avg(s 1 ) = 1/10 = 0.1.
the average error of the estimate is thus 1 − 0.1 = 0.9.
Therefore, the accuracy of this approach does not change with data skew in this case.Sample rate Fig. 3(a) shows a typical result on how the three approaches behave as we increase the sample rate.
The number of attributes in the data set is 2.
The accuracy of the synopsis-based approach remains unchanged across the range of sample rates, because it does not depend on sampling.
The accuracy of both HASE and the sampling-based approach improves with increasing sample rate, as one would expect.
For all sample rates, HASE outperforms both the synopsis-based and the sampling-based approaches.
It is also worth noting that using HASE, we can achieve the same level of accuracy with a much smaller sample rate than that required by the sampling-based approach.
For example, in Fig. 3(a), the sampling-based approach has an error of 0.07 when the sample rate is 0.005.
HASE achieves approximately the same level of accuracy with a sample rate of 0.001, resulting in a reduction by a factor of 5.
This translates into more significant I/O savings because of the non-linear relationship between the I/O cost and the sample rate as discussed in Section 2.3.
Number of attributes We vary the number of attributes involved in the query from 2 to 5 to study the impact of the number of attributes on the estimation accuracy.
A typical result is shown in Fig. 3(b).
Clearly, the accuracy of all three approaches decreases as the number of attributes increases.
This is not surprising, because having more attributes would introduce more sources of errors.
A space of higher dimensionality requires a much larger sample to cover a fixed portion of the space, in comparison with a space of lower dimensionality.
Note from Fig. 3(b), however, that HASE outperforms the other two approaches for all number of attributes, and has a lower rate of decrease in accuracy.
Since the Census Income data has 40 attributes, there are 40 × 39 = 1560 attribute pairs.
We randomly choose 100 attribute pairs and record the accuracy of the three approaches as the sample rate increases.
The result is shown in Fig. 4.
The trends are similar to those for the synthetic data, with HASE significantly outperforming both the synopsis-based and the sampling-based approaches.
The error response to the number of attributes is also similar to that for the synthetic data, and is therefore omitted here.
The issue of selectivity estimation has been extensively studied in the literature and a large variety of methods have been proposed [15,1,16,17].
Histograms are probably the most widely used form of synopses in commercial database systems (e.g., DB2 UDB, Oracle, SQL Server, etc.).
See [18] for an excellent survey on this topic.
Aside from histograms, other types of synopses have also been proposed in the literature, such as wavelet-based synopses (e.g., [16]) and parametric synopses (e.g., [19]).
Markl et al. [20] propose a method to consistently utilize various multidimensional synopsis structures for selectivity estimation of conjunctive predicates.
This work is close in spirit to ours in that both of them address the issue of consistent utilization of various sources of information for selectivity estimation.
However, their focus is on reconciling the estimates obtained from different synopsis structures, whereas we attack the problem of utilizing both synopses and sample information.Olken [3] provides a survey of techniques on sampling from databases.
Lipton et al. [15] propose an adaptive sampling (a.k.a. sequential sampling) approach to selectivity estimation.
Haas and Swami [21] improve the sequential sampling approach by establishing tighter termination conditions.
There has also been work on estimating the number of distinct values via sampling [22][23][24].
Recently, Haas et al. [4] and Chaudhuri et al. [7] address the efficiency of sampling and propose techniques to utilize page-level sampling in conjunction with tuple-level sampling.
Techniques have also been developed to use sampling to construct synopsis structures [25,24,7].
Note that sampling is used here only for fast construction of data synopses, which are then used for selectivity estimation; they do not consider the issue of direct utilization of both sampling and synopses for selectivity estimation.
Existing work on selectivity estimation can be classified as either synopsis-based or sampling-based, depending on whether the basis for estimation is the synopsis structures stored in the database or sample information.
The presence of both sources of information presents a unique challenge, as it is nontrivial to make consistent use of them in order to obtain better estimation.
To the best of our knowledge, we are the first to tackle this challenge.
We proposed HASE, a new estimation procedure based on generalized raking, and the problem is formulated as a constrained optimization problem.
We then presented two algorithms to solve it.
We also discussed the implications of different distance functions, and provided asymptotic error bounds on the selectivity estimate thus obtained.
The experiments demonstrated the effectiveness of the proposed approach.For future work, we would like to consider extending HASE to handle the selectivity estimation of more complex queries, such as joins and aggregations.
We also plan to extend HASE to handle the case where multi-attribute synopses (e.g., multi-dimensional histograms) are available.
It would also be interesting to study in our framework how to best divide the efforts between constructing histograms and sampling for a given query workload.
The authors would like to thank our friend and mentor Prof. Kenneth C. Sevcik for his comments and encouragement during the course of this work.
We miss him dearly.
IBM, DB2, DB2 Universal Database are trademarks or registered trademarks of International Business Machines Corporation in the United States, other countries, or both.
Other company, product, or service names may be trademarks or service marks of others.
