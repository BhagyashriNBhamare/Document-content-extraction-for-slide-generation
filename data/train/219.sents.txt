Delta compression techniques are commonly used to succinctly represent an updated version of a file with respect to an earlier one.
In this paper, we study the use of delta compression in a somewhat different scenario, where we wish to compress a large collection of (more or less) related files by performing a sequence of pairwise delta compressions.
The problem of finding an optimal delta encoding for a collection of files by taking pairwise deltas can be reduced to the problem of computing a branching of maximum weight in a weighted directed graph, but this solution is inefficient and thus does not scale to larger file collections.
This motivates us to propose a framework for cluster-based delta compression that uses text clustering techniques to prune the graph of possible pairwise delta encodings.
To demonstrate the efficacy of our approach, we present experimental results on collections of web pages.
Our experiments show that cluster-based delta compression of collections provides significant improvements in compression ratio as compared to individually compressing each file or using tar+gzip, at a moderate cost in efficiency.
Delta compressors are software tools for compactly encoding the differences between two files or strings in order to reduce communication or storage costs.
Examples of such tools are the diff and bdiff utilities for computing edit sequences between two files, and the more recent xdelta [16], vdelta [12], vcdiff [15], and zdelta [26] tools that compute highly compressed representations of file differences.
These tools have a number of applications in various networking and storage scenarios; see [21] ¡ This project was supported by a grant from Intel Corporation, and by the Wireless Internet Center for Advanced Technology (WICAT) at Polytechnic University.
Torsten Suel was also supported by NSF CAREER Award NSF CCR-0093400.
for a more detailed discussion.
In a communication scenario, they typically exploit the fact that the sender and receiver both possess a reference file that is similar to the transmitted file; thus transmitting only the difference (or delta) between the two files requires a significantly smaller number of bits.
In storage applications such as version control systems, deltas are often orders of magnitude smaller than the compressed target file.Delta compression techniques have also been studied in detail in the context of the World Wide Web, where consecutive versions of a web page often differ only slightly [8,19] and pages on the same site share a lot of common HTML structure [5].
In particular, work in [2,5,7,11,18] considers possible improvements to HTTP caching based on sending a delta with respect to a previous version of the page, or another similar page, that is already located in a client or proxy cache.In this paper, we study the use of delta compression in a slightly different scenario.
While in most other applications, delta compression is performed with respect to a previous version of the same file, or some other easy to identify reference file, we are interested in using delta compression to better compress large collections of files where it is not obvious at all how to efficiently identify appropriate reference and target files.
Our approach is based on a reduction to the optimum branching problem in graph theory and the use of recently proposed clustering techniques for finding similar files.We focus on collections of web pages from several sites.
Applications that we have in mind are efficient downloading and storage of collection of web pages for off-line browsing, and improved archiving of massive terabyte web collections such as the Internet Archive (see http://archive.org).
However, the techniques we study are applicable to other scenarios as well, and might lead to new general-purpose tools for exchanging collections of files that improve over the currently used zip and tar/gzip tools.
In this paper, we study the problem of compressing collections of files, with focus on collections of web pages, with varying degrees of similarity among the files.
Our approach is based on using an efficient delta compressor, in particular the zdelta compressor [26], to achieve significantly better compression than that obtained by compressing each file individually or by using tools such as tar and gzip on the collection.
Our main contributions are:The problem of obtaining optimal compression of a collection of ¡ files, given a specific delta compressor, can be solved by finding an optimal branching on a directed graph with ¡ nodes and ¡ ¢ edges.
We implement this algorithm and show that it can achieve significantly better compression than current tools.
On the other hand, the algorithm quickly becomes inefficient as the collection size grows beyond a few hundred files, due to its quadratic complexity.We present a general framework, called cluster-based delta compression, for efficiently computing nearoptimal delta encoding schemes on large collections of files.
The framework combines the branching approach with two recently proposed hash-based techniques for clustering files by similarity [3,10,14,17].
Within this framework, we evaluate a number of different algorithms and heuristics in terms of compression and running time.
Our results show that compression very close to that achieved by the optimal branching algorithm can be achieved in time that is within a small multiplicative factor of the time needed by tools such as gzip.We also note three limitations of our study: First, our results are still preliminary and we expect additional improvements in running time and compression over the results in this paper.
In particular, we believe we can narrow the gap between the speed of gzip and our best algorithms.
Secondly, we restrict ourselves to the case where each target file is compressed with respect to a single reference file.
Additional significant improvements in compression might be achievable by using more than one reference file, at the cost of additional algorithmic complexity.
Finally, we only consider the problem of compressing and uncompressing an entire collection, and do not allow individual files to be added to or retrieved from the collection.The rest of this paper is organized as follows.
The next subsection lists related work.
In Section 2 we discuss the problem of compressing a collection of files using delta compression, and describe an optimal algorithm based on computing a maximum weight branching in a directed graph.
Section 3 provides our framework called clusterbased delta compression and outlines several approaches under this framework.
In Section 4, we present our experimental results.
Finally, Section 5 provides some open questions and concluding remarks.
For an overview of delta compression techniques and applications, see [21].
Delta compression techniques were originally introduced in the context of version control systems; see [12,25] for a discussion.
Among the main delta compression algorithms in use today are diff and vdelta [12].
Using diff to find the difference between two files and then applying gzip to compress the difference is a simple and widely used way to perform delta compression, but it does not provide good compression on files that are only slightly similar.
vdelta, on the other hand, is a relatively new technique that integrates both data compression and data differencing.
It is a refinement of Tichy's block-move algorithm [24] that generalizes the well known Lempel-Ziv technique [27] to delta compression.
In our work, we use the zdelta compressor, which was shown to achieve good compression and running time in [26].
The issue of appropriate distance measures between files and strings has been studied extensively, and many different measures have been proposed.
We note that diff is related to the symmetric edit distance measure, while vdelta and other recent Lempel-Ziv type delta compressors such as xdelta [16], vcdiff [15], and zdelta [26] are related to the copy distance between two files.
Recent work in [6] studies a measure called LZ distance that is closely related to the performance of Lempel-Ziv type compressing schemes.
We also refer to [6] and the references therein for work on protocols for estimating file similarities over a communication link.Fast algorithms for the optimum branching problem are described in [4,22].
While we are not aware of previous work that uses optimum branchings to compress collections of files, there are two previous applications that are quite similar.
In particular, Tate [23] uses optimum branchings to find an optimal scheme for compressing multispectral images, while Adler and Mitzenmacher [1] use it to compress the graph structure of the World Wide Web.
Adler and Mitzenmacher [1] also show that a natural extension of the branching problem to hypergraphs that can be used to model delta compression with two or more reference files is NP Complete, indicating that an efficient optimal solution is unlikely.We use two types of hash-based clustering techniques in our work, a technique with quadratic complexity called min-wise independent hashing proposed by Broder in [3] (see also Manber and Wu [17] for a similar technique), and a very recent nearly linear time technique called localitysensitive hashing proposed by Indyk and Motwani in [14] and applied to web documents in [10].
Delta compressors such as vcdiff or zdelta provide an efficient way to encode the difference between two similar files.
However, given a collection of files, we are faced with the problem of succinctly representing the entire collection through appropriate delta encodings between target and reference files.
We observe that the problem of finding an optimal encoding scheme for a collection of files through pairwise deltas can be reduced to that of computing an optimum branching of an appropriately constructed weighted directed graph ¡ .
Formally, a branching of a directed graph ¡ is defined as a set of edges such that (1) contains at most one incoming edge for each node, and (2) does not contain a cycle.
Given a weighted directed graph, a maximum branching is a branching of maximum edge weight.
Given a collection of includes an extra null node corresponding to the empty file that is used to model the compression savings if a file is compressed by itself (using, e.g., zlib, or zdelta with an empty reference file).
Given the above formulation it is not difficult to see that a maximum branching of the graph ¡ gives us an optimal delta encoding scheme for a collection of files.
Condition (1) in the definition of a branching expresses the constraint that each file is compressed with respect to only one other file.
The second condition ensures that there are no cyclical dependencies that would prevent us from decompressing the collection.
Finally, given the manner in which the weights have been assigned, a maximum branching results in a compression scheme with optimal benefit over the uncompressed case.
We implemented delta compression based on the optimal branching algorithm described in [4,22], which for dense graphs takes time proportional to the number of edges.
Ta- ble 1 shows compression results and times on several collections of web pages that we collected by crawling a limited number of pages from each site using a breadth-first crawler.The results indicate that the optimum branching approach can give significant improvements in compression over using cat or tar followed by gzip, outperforming them by a factor of ' to ( .
However, the major problem with the optimum branching approach is that it becomes very inefficient as soon as the number of files grows beyond a few dozens.
For the cbc.ca data set with ( 6 % pages, it took more than an hour ()& 3 ( 6 ( ¢ ¡) to perform the computation, while multiple hours were needed for the set with all sites.
Figure 2 plots the running time in seconds of the optimal branching algorithm for different numbers of files, using a set of files from the gcc software distribution also used in [12,26].
Time is plotted on a logarithmic scale to accomodate two curves: the time spent on computing the edge weights (upper curve), and the time spent on the actual branching computation after the weights of the graph have been determined using calls to zdelta (lower curve).
While both curves grow quadratically, the vast majority of the time is spent on computing appropriate edge weights for the graph ¡ , and only a tiny amount is spent on the actual branching computation afterwards.
Thus, in order to compress larger collections of pages, we need to find techniques that avoid computing the exact weights of all edges in the complete graph ¡ .
In the next sections, we study such techniques based on clustering of pages and pruning and approximation of edges.
We note that another limitation of the branching approach is that it does not support the efficient retrieval of individual files from a compressed collection, or the addition of new files to the collection.
This is a problem in some applications that require interactive access, and we do not address it in this paper.
As shown in the previous section, delta compression techniques have the potential for significantly improved compression of collections of files.
However, the optimal algorithm based on maximum branching quickly becomes a bottleneck as we increase the collection size ¡ , mainly due to the quadratic number of pairwise delta compression computations that have to be performed.
In this section, we describe a basic framework, called Cluster-Based Delta Compression, for efficiently computing near-optimal delta compression schemes on larger collections of files.
We first describe the general approach, which leads to several different algorithms that we implemented.
In a nutshell, the basic idea is to prune the complete graph , and then find the best delta encoding scheme within this subgraph.
More precisely, we have the following general steps: The assignment of weights in the second step can be done either precisely, by performing a delta compression across each remaining edge, or approximately, e.g., by using estimates for file similarity produced during the document analysis in the first step.
Note that if the weights are computed precisely by a delta compressor and the resulting compressed files are saved, then the actual delta compression after the last step consists of simply removing files corresponding to unused edges (assuming sufficient disk space).
The primary challenge is Step (1), where we need to efficiently identify a small subset of file pairs that give good delta compression.
We will solve this problem by using two sets of known techniques for document clustering, one set proposed by Broder [3] and Manber and Wu [17], and one set proposed by Indyk and Motwani [14] and applied to document clustering by Haveliwala, Gionis, and Indyk [10].
These techniques were developed in the context of identifying near-duplicate web pages and finding closely related pages on the web.
While these problems are clearly closely related to our scenario, there are also a number of differences that make it nontrivial to apply the techniques to delta compression, and in the following we discuss these issues.
The compression performance of a delta compressor on a pair of files depends on many details, such as the precise locations and lengths of the matches, the internal compressibility of the target file, the windowing mechanism, and the performance of the internal Huffman coder.
A number of formal measures of file similarity, such as edit distance (with or without block moves), copy distance, or LZ distance [6] have been proposed that provide reasonable approximations; see [6,21] for a discussion.
However, even these simplified measures are not easy to compute with, and thus the clustering techniques in [3,17,10] ¤ ¨ £ ¢ ¤ ¦ !
§ " © # $ ¦ !
§ " © ¤ ¤ ¦ ¨ § © ¤ .
(Note that shingle containment is not symmet- ric.)
Thus, both of these measures assign higher similarity scores to files that share a lot of short substrings, and intuitively we should expect a correlation between the delta compressibility of two files and these similarity measures.
In fact, the following relationship between shingle intersection and the edit distance measure can be easily derived: We refer to [9] for a proof and a similar result for the case of edit distance with block moves.
A similar relationship can also be derived between shingle containment and copy distances.
Thus, shingle intersection and shingle containment are related to the edit distance and copy distance measures, which have been used as models for the corresponding classes of edit-based and copy-based delta compression schemes.
While the above discussion supports the use of the shingle-based similarity measures in our scenario, in practice the relationship between these measures and the achieved delta compression ratio is quite noisy.
Moreover, for efficiency reasons we will only approximate these measures, introducing additional potential for error.Given We now describe the first set of techniques, called minwise independent hashing, that was proposed by Broder in [3].
(A similar technique is described by Manber and Wu in [17].)
The simple idea in this technique is to approximate the shingle similarity measures by sampling a small subset of shingles from each file.
However, in order to obtain a good estimate, the samples are not drawn independently from each file, but they are obtained in a coordinated fashion using a common set of random hash functions that map shingles of length ¡ t o integer values.
We then select in each file the smallest hash values obtained this way.We refer the reader to [3] for a detailed analysis.
Note that there are a number of different choices that can be made in implementing these schemes:Choice of hash functions: We used a class of simple linear hash functions analyzed by Indyk in [13] and also used in [10].
.
This results in a significant speedup over the optimal algorithm in practice, although the algorithm will eventually become inefficient due to the quadratic complexity.
The second set of techniques, proposed by Indyk and Motwani [14] and applied to document clustering by Haveliwala, Gionis, and Indyk [10], is an extension of the first set that results in an almost linear running time.
In particular, these techniques avoid the pairwise comparison between all ¡ files by performing a number of sorting steps on specially designed hash signatures that can directly identify similar files.The first step of the technique is identical to that of the min-wise independent hashing technique for fixed sample size.
-bit signature).
If two files agree on their signature, then this is strong evidence that their intersection is above some threshold.
It can be formally shown that by repeating this process a number of times that depends on and the chosen threshold, we will find most pairs of files with shingle intersection above the threshold, while avoiding most of the pairs below the threshold.
For a more formal description of this technique we refer to [10].
The resulting algorithm consists of the following steps:( We note two limitations.
First, the above implementation only identifies the pairs that are above a given fixed similarity threshold.
Thus, it does not allow us to determine the U best neighbors for each node, and it does not provide a good estimate of the precise similarity of a pair (i.e., whether it is significantly or only slightly above the threshold).
Second, the method is based on shingle intersection, and not shingle containment.
Addressing these limitations is an issue for future work.
In this section, we perform an experimental evaluation of several cluster-based compression schemes that we implemented based on the framework from the previous section.
We first introduce the algorithm and the experimental setup.
In Subsection 4.2 we show that naive methods based on thresholds to do not give good results.
The next three subsections look at different techniques that resolve this problem, and finally Subsection 4.6 presents results for our best two algorithms on a larger data set.
Due to space constraints and the large number of options, we can only give a selection of our results.
We refer the reader to [20] for a more complete evaluation.
We implemented a number of different algorithms and variants.
In particular, we have the following options: Sample size: fixed size vs. fixed rate.Similarity measure: intersection vs. containment.Edge pruning rule: threshold vs. best neighbors vs. heuristics.Edge weight: exact vs. estimated.We note that not every combination of these choices make sense.
For example, our LSH implementations do not support containment or best neighbors, and require a fixed sample size.
On the other hand, we did not observe any benefit in using multiple hash functions in the MH scheme, and thus assume a single hash function for this case.
We note that in our implementations, all samples were treated as sets, rather than multi-sets, so a frequently occurring string is presented at most once.
2 All algorithms were implemented in C and compiled using gcc 2.95.2 under Solaris 7.
Experiments were run on a E450 Sun Enterprise server, with two UltraSparc .
The pages were crawled in a breadthfirst crawl that attempted to fetch all pages reachable from the www.poly.edu homepage, subject to certain pruning rules to avoid dynamically generated content and cgi scripts.
The first experiments that we present look at the performance of MH and LSH techniques that try to identify and retain all edges that are above a certain similarity threshold.In Table 2 we look at the optimum branching method and at three different algorithms that use a fixed threshold to select edges that are considered similar, for different thresholds.
For each method, we show the number of similar edges, the number of edges included in the final branching, and the total improvement obtained by the method as compared to compressing each file individually using zlib.
The results demonstrate a fundamental problem that arises in these threshold-based methods: for high thresholds, the vast majority of edges is eliminated, but the resulting branching is of poor quality compared to the optimal one.
For low thresholds, we obtain compression close to the optimal, but the number of similar edges is very high; this is a problem since the number of edges included in ¡ £ 2 Intuitively, this seems appropriate given our goal of modeling delta compression performance.
determines the cost of the subsequent computation.
3 Unfortunately, these numbers indicate that there is no real "sweet spot" for the threshold that gives both a small number of similar edges and good compression on this data set.
We note that this result is not due to the precision of the sampling-based methods, and it also holds for thresholdbased LSH algorithms.
A simplified explanation for this is that data sets contain different clusters of various similarity, and a low threshold will keep these clusters intact as dense graphs with many edges, while a high threshold will disconnect too many of these clusters, resulting in inferior compression.
This leads us to study several techniques for overcoming this problem: In summary, using a fixed threshold followed by an optimal branching on the remaining edges does not result in a very good trade-off between compression and running time.
We now look at the case where we limit the number of remaining edges in the MH algorithm by keeping only the By using the containment measure values computed by the MH clustering as the weights of the remaining edges in ¡ £, we can further decrease the running time, as shown in For LSH algorithms, we experimented with a simple heuristic for reducing the number of remaining edges where, after the sorting of the file signatures, we only keep a subset of the edges in the case where more than ' files have identical signatures.
In particular, instead of building a complete graph on these files, we connect these files by a simple linear chain of directed edges.
This somewhat arbitrary heuristic (which actually started out as a bug) results in significantly decreased running time at only a slight cost in compression, as shown in Table 5.
We are currently looking at other more principled approaches to thinning out tightly connected clusters of edges.
Finally, we present the results of the best schemes identified above on the large data set of ' 6 % 2 & R 6 % pages from the poly.edu domain.
We note that our results are still somewhat preliminary and can probably be significantly improved by some optimizations.
We were unable to compute the optimum branching on this set due to its size.The In this paper, we have investigated the problem of using delta compression to obtain a compact representation of a cluster of files.
As described, the problem of optimally encoding a collection using delta compression based on a single file can be reduced to the problem of computing a maximum weight branching.
However, while providing superior compression, this algorithm does not scale to larger collections, motivating us to propose a faster cluster-based delta compression framework.
We studied several file clustering heuristics and performed extensive experimental comparisons.
Our preliminary results show that significant compression improvements can be obtained over tar+gzip at moderate additional computational costs.Many open questions remain.
First, some additional optimizations are possible that should lead to improvements in compression and running time, including faster sampling and better pruning heuristics for LSH methods.
Second, the cluster-based framework we have proposed uses only pairwise deltas, that is, each file is compressed with respect to only a single reference file.
It has been shown [5] that multiple reference files can result in significant improvements in compression, and in fact this is already partially exploited by tar+gzip with its ( 7 ' ¢ ¡ window on small files.
As discussed, a polynomial-time optimal solution for multiple reference files is unlikely, and even finding schemes that work well in practice is challenging.
Our final goal is to create general purpose tools for distributing file collections that improve significantly over tar+gzip.In related work, we are also studying how to apply delta compression techniques to a large web repository 4 that can
