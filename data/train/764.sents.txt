In this work, we reexamine the time scale Laplace transform as defined by Bohner and Peter-son [M. Bohner, A. Peterson, Dynamic Equations on Time Scales: An Introduction with Applications, Birkhäuser, Boston, 2001; M. Bohner, A. Peterson, Laplace transform and Z-transform: Unification and extension, Methods Appl.
Anal.
9 (1) (2002) 155-162].
In particular, we give conditions on the class of functions which have a transform, develop an inversion formula for the transform, and further, we provide a convolution for the transform.
The notion of convolution leads to considering its algebraic structure-in particular the existence of an identity element-motivating the development of the Dirac delta functional on time scales.
Applications and examples of these concepts are given.
In their initial work on the subject, Bohner and Peterson [3] The time scale exponential function itself is of type II.
In their work on the stability of the time scale exponential function, Pötzsche, Siegmund, and Wirth [6] show that the time scale polynomials h k (t, 0) are of type I.Throughout this work, we will assume that T is a time scale with bounded graininess, that is, 0 < μ min 񮽙 μ(t) 񮽙 μ max < ∞ for all t ∈ T.
We set μ min = μ * and μ max = μ * .
Under this assumption, Pötzsche, Siegmund, and Wirth [6] as well as Hoffacker and Gard [4] show that by choosing λ ∈ H, where H denotes the Hilger circle given by we obtain lim t→∞ e λ (t, 0) = 0.
See Fig. 1.
This limit condition will play a crucial role in the analysis of the transform.
However, the expression for the transform has a slight complication since the function 񮽙z is time varying and not constant.
Fortunately, this is only a minor problem to overcome as we shall see.To give an appropriate domain for the transform, which of course is tied to the region of convergence of the integral in ( where H 񮽙 max denotes the complement of the closure of largest Hilger circle corresponding to μ * .
Note that if μ * = 0 this set is a right half plane; see Fig. 2.
Furthermore, if z ∈ D, then 񮽙z ∈ H min ⊂ H t since for all z ∈ D, 񮽙z satisfies the inequality񮽙 񮽙 񮽙 񮽙 񮽙z + 1 μ * 񮽙 񮽙 񮽙 񮽙 < 1 μ * .
Finally, if Re μ * (z) > Re μ * (c), then Re μ (z) > Re μ (c)for all t ∈ T since the Hilger real part is an increasing function in μ.
This observation will play a key role later.Lemma 1.1.
If 񮽙z ∈ H and Re μ (z) > Re μ (c), then ((񮽙z) ⊕ c) ∈ H. Proof.
Since Re μ (z) > Re μ (c) implies |1 + zμ(t)| > |1 + cμ(t)|, we see 񮽙 񮽙 񮽙 񮽙 (񮽙z ⊕ c) + 1 μ(t) 񮽙 񮽙 񮽙 񮽙 = 񮽙 񮽙 񮽙 񮽙 1 + cμ(t) μ(t)(1 + zμ(t)) 񮽙 񮽙 񮽙 񮽙 = |1 + cμ(t)| μ(t)|1 + zμ(t)| < 1 μ(t).
2 Theorem 1.1 (Domain of the transform).
The integral񮽙 ∞ 0 e σ 񮽙z (t, 0)f (t) t converges absolutely for z ∈ D if f (t) is of exponential type II with exponential constant c. Proof.
If 񮽙z ∈ H, then 1 |1+zμ(t)| 񮽙 1.
Thus, 񮽙 񮽙 񮽙 񮽙 񮽙 ∞ 񮽙 0 e σ 񮽙z (t, 0)f (t) t 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 ∞ 񮽙 0 񮽙 񮽙 e σ 񮽙z (t, 0)f (t) 񮽙 񮽙 񮽙t 񮽙 M ∞ 񮽙 0 񮽙 񮽙 񮽙 񮽙 1 1 + zμ(t) 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 e 񮽙z (t, 0)e c (t, 0) 񮽙 񮽙 񮽙t 񮽙 M ∞ 񮽙 0 񮽙 񮽙 e 񮽙z⊕c (t, 0) 񮽙 񮽙 񮽙t = M ∞ 񮽙 0 exp 񮽙 t 񮽙 0 log |1 + μ(τ )(񮽙z ⊕ c)| μ(τ ) 񮽙τ 񮽙 񮽙t = M ∞ 񮽙 0 exp 񮽙 t 񮽙 0 log 񮽙 񮽙 1+μ(τ )c 1+μ(τ )z 񮽙 񮽙 μ(τ ) 񮽙τ 񮽙 񮽙t 񮽙 M ∞ 񮽙 0 e −αt dt = M α , where α = 񮽙 񮽙 񮽙 񮽙 log 񮽙 񮽙 1+μ * c 1+μ * z 񮽙 񮽙 μ * 񮽙 񮽙 񮽙 񮽙 .
2The same estimates used in the proof of the preceding theorem can be used to show that if f (t) is of exponential type II with constant c and Re μ (z) > Re μ (c), then lim t→∞ e 񮽙z (t, 0)f (t) = 0.
As we look towards an inverse for the transform, we would like to know which functions are the transform of some function.
To answer this question, the following properties are needed.
Theorem 1.2.
Let F denote the generalized Laplace transform for f : T → R.(1) F (z) is analytic in Re μ (z) > Re μ (c).
(2) F (z) is bounded in Re μ (z) > Re μ (c).
(3) lim |z|→∞ F (z) = 0.
Proof.
For the first, we seed dz L{f }(z) = d dz ∞ 񮽙 0 e σ 񮽙z (t, 0)f (t) t = ∞ 񮽙 0 d dz 񮽙 1 1 + μ(t)z exp 񮽙 t 񮽙 0 log 񮽙 1 1+μ(τ )z 񮽙 μ(τ ) 񮽙τ 񮽙񮽙 f (t) t = ∞ 񮽙 0 񮽙 t 񮽙 0 −1 1 + μ(τ )z 񮽙τ − μ(t) 1 + μ(t)z 񮽙 e σ 񮽙z (t, 0)f (t) t = − ∞ 񮽙 0 񮽙 σ (t) 񮽙 0 1 1 + μ(τ )z 񮽙τ 񮽙 e σ 񮽙z (t, 0)f (t) t = −L{gf }(z),whereg(t) = 񮽙 σ (t) 0 1 1+μ(τ )z 񮽙τ .
The second equality follows from the Lebesgue Dominated Convergence Theorem.
These calculations show we get the familiar formula when T = R that derivatives of the transform correspond to multiplication by powers of t in the function.
For T = Z, the calculations show that (in the shifted version) derivatives of the transform correspond to multiplication by powers of t + 1 in the function.The second claim is an immediate consequence of the preceding theorem since it shows |F (z)| < M α .
As for the third, a direct calculation yields Proof.lim |z|→∞ F (z) = lim |z|→∞ ∞ 񮽙 0 e σ 񮽙z (t, 0)f (t) t = ∞ 񮽙 0 lim |z|→∞ e σ 񮽙z (t, 0)f (t) t = 0.
L 񮽙 f 񮽙 (t) 񮽙 = ∞ 񮽙 0 f 񮽙 (t)e σ 񮽙z (t, 0) )t = f (t)e 񮽙z (t, 0) 񮽙 񮽙 ∞ t=0 − ∞ 񮽙 0 f (t)(񮽙z)(t)e 񮽙z (t, 0) )t = −f (0) + z ∞ 񮽙 0 f (t)e σ 񮽙z (t, 0) )t = zF (z) − f (0).
Now z → ∞ above yields lim z→∞ ∞ 񮽙 0 f 񮽙 (t)e σ 񮽙z (t, 0) )t = 0 = lim z→∞ 񮽙 zF (z) − f (0) 񮽙 , i.e., f (0) = lim z→∞ zF (z).
On the other hand, z → 0 yieldslim z→0 ∞ 񮽙 0 f 񮽙 (t)e σ 񮽙z (t, 0) )t = ∞ 񮽙 0 f 񮽙 (t) t = lim t→∞ f (t) − f (0) = lim z→0 񮽙 zF (z) − f (0) 񮽙 ,i.e., lim t→∞ f (t) = lim z→0 zF (z).
2 Using Theorem 1.2 we can establish an inversion formula for the transform.
As is the case with T = R, these properties are not sufficient to guarantee that F (z) is the transform of some continuous function f (t), but they are necessary as we have just seen.
For sufficiency, we have the following: Re μ (c) and F (z) → 0 uniformly as |z| → ∞ in this region.
Suppose F (z) has finitely many regressive poles of finite order {z 1 , z 2 , . . . , z n } and˜Fand˜ and˜F R (z) is the transform of the functioñ f (t) on R that corresponds to the transformF (z) = F T (z) of f (t) on T.
If c+i∞ 񮽙 c−i∞ 񮽙 񮽙˜F񮽙˜ 񮽙˜F R (z) 񮽙 񮽙 |dz| < ∞, then f (t) = n 񮽙 i=1 Res z=z i e z (t, 0)F (z), has transform F (z) for all z with Re(z) > c.Proof.
The proof follows from the commutative diagram between the appropriate function spaces in Fig. 3.
Let C be the collection of Laplace transforms over R, and D the collection of transforms over for G a rational function and for τ ∈ T a constant.
Let C p-eo (R, R) denote the space of piecewise continuous functions of exponential order, and C prd-e2 (T, R) denote the space of piecewise rightdense continuous functions of exponential type II.
Each of θ , γ , θ −1 , γ −1 maps functions involving the continuous exponential to the time scale exponential and vice versa.
For example, γ maps the functioñT, i.e., C = { ˜ F R (z)} and D = {F T (z)}, where˜Fwhere˜ where˜F R (z) = G(z)e −zτ and F T (z) = G(z)e 񮽙z (τ, 0) C p-eo (R, R) L R 񮽙 񮽙 L −1 R C C prd-e2 (T, R) θ 񮽙 θ −1 񮽙 L T 񮽙 񮽙 L −1 T = θ • L −1 R • γ −1 D γ 񮽙 γ −1F (z) = e −za z to the function F (z) = e 񮽙z (a,0) z , while γ −1 maps F (z) back tõ F (z)in the obvious manner.
If the representation of F (z) is independent of the exponential (that is, τ = 0), then γ and its inverse will act as the identity.
For example,γ 񮽙 1 z 2 + 1 񮽙 = γ −1 񮽙 1 z 2 + 1 񮽙 = 1 z 2 + 1 .
θ will send the continuous exponential function to the time scale exponential function in the following manner: if we write˜fwrite˜ write˜f (t) ∈ C p-eo (R, R) as˜fas˜ as˜f (t) = n 񮽙 i=1 Res z=z i e zt˜Fzt˜ zt˜F R (z), then θ 񮽙˜f񮽙˜ 񮽙˜f (t) 񮽙 = n 񮽙 i=1 Res z=z i e z (t, 0)F T (z).
To go from˜Ffrom˜ from˜F R (z) to F T (z), we simply switch expressions involving the continuous exponential iñ F R (z) with the time scale exponential giving F T (z) as was done for γ and its inverse.
θ −1 will then act on g ∈ C prd-e2 (T, R),g(t) = n 񮽙 i=1 Res z=z i e z (t, 0)G T (z), as θ −1 񮽙 g(t) 񮽙 = n 񮽙 i=1Res z=z i e zt˜Gzt˜ zt˜G R (z).
For example, for the unit step functioñ u a (t) on R, we know from the continuous result that we may write the step function as˜uas˜ as˜u a (t) = Res z=0 e zt · e −az z , so that if a ∈ T, then θ 񮽙˜u񮽙˜ 񮽙˜u a (t) 񮽙 = Res z=0 e z (t, 0) e 񮽙z (a, 0) z .
With these operators defined on these spaces, we can see that the claim in the theorem follows.
For a given time scale Laplace transform F T (z), we begin by mapping tõ F R (z) via γ −1 .
The hypotheses on F T (z) and˜Fand˜ and˜F R (z) are enough to guarantee the inverse of˜Fof˜ of˜F R (z) exists for all z with Re(z) > c (see [1]), and is given by˜fas˜ as˜u a (t) = Res z=0 e zt · e −az z , so that if a ∈ T, then θ 񮽙˜u񮽙˜ 񮽙˜u a (t) 񮽙 = Res z=0 e z (t, 0) e 񮽙z (a, 0) z .
Res z=z i e zt˜Fzt˜ zt˜F R (z).
f (t) = n 񮽙 i=1 Res z=z i e z (t, 0)F T (z), whereby (γ • L R • θ −1 )(f (t)) = F T (z) as claimed.
2Before looking at a few examples, some remarks are in order.
First, it is reasonable to ask if there is a contour in the complex plane around which it is possible to integrate to obtain the same results that we have obtained here through a more operational approach.
At present, we leave this as a very interesting although nontrivial open problem.
It is well known that there are such contours when T = R or T = Z.
In fact, it can easily be shown that if T is completely discrete, then if we choose any circle in the region of convergence which encloses all of the singularities of F (z), we will obtain the inversion formula.
However, in general, we do not know whether or not there exists a contour which gives the formula, and if so, what it is.Second, it is possible to use the technique we have presented here to define and find inverses for any time scale transform.
These would include the Fourier, Mellin, and many other transforms.
Once the inverse is known for T = R and the appropriate time scale integral is developed to give the correct transform analogues for any T, the diagram becomes completed and the inversion formula for any T is readily obtained.Finally, notice in our construction, for any transformable function f (t) on T, there is a shadow functioñ f (t) defined on R.
That is, to determine the appropriate time scale analogue of the functioñ f (t) in terms of the transform, we use the diagram to map its Laplace transform on R to its Laplace transform on T.Example 1.1 (New representation for h k (t, 0)).
Suppose F (z) = 1 z 2 .
The hypotheses of Theorem 1.4 for F (z) are readily verified, so that F (z) has an inverse given byL −1 {F } = f (t) = Res z=0 e z (t, 0) z 2 = t.For F (z) = 1 z 3 , we haveL −1 {F } = f (t) = Res z=0 e z (t, 0) z 3 = t 2 − 񮽙 t 0 μ(τ ) τ 2 = h 2 (t, 0).
The last equality is justified since the functionf (t) = t 2 − 񮽙 t 0 μ(τ ) τ 2 ,is the unique solution to the initial value problem f 񮽙 (t) = h 1 (t, 0), f (0) = 0.
In a similar manner, we can use an induction argument coupled with Theorem 1.4 to show that the inverse transform of F (z) = 1 z k+1 , for k a positive integer, is h k (t, 0).
In fact, it is easy to show that the inversion formula gives the claimed inverses for any of the elementary functions that Bohner and Peterson have in their table in [3].
These elementary functions become the proper time scale analogues or shadows of their continuous counterparts.
f (t) = 1 2a h 1 (t, 0) sin a (t, 0).
However, closer inspection shows that this is incorrect.
To see this, note that the Laplace transform of˜fof˜ of˜f (t) is˜Fis˜ is˜F (z) = z (z 2 + a 2 ) 2 .
To find the proper analogue f (t) of˜fof˜ of˜f (t), we search for a time scale function with the same transform as˜fas˜ as˜f (t).
To do this, we note thatf (t) = L −1 {F } = L −1 񮽙 z (z 2 + a 2 ) 2 񮽙 = 2 񮽙 i=1 Res z=z i ze z (t, 0) (z 2 + a 2 ) 2 = 1 2a sin a (t, 0) t 񮽙 0 1 1 + (μ(τ )a) 2 񮽙τ − 1 2 cos a (t, 0) t 񮽙 0 μ(τ ) 1 + (μ(τ )a) 2 τ,so that in general the correct analogue involves the time scale cosine function as well. 񮽙
.
Then (zI − A) −1 = 񮽙 1 (z−2) 1 (z−2)(z−3) 0 1 (z−3) 񮽙 , so that e A (t, 0) = 񮽙 e 2 (t, 0) e 3 (t, 0) − e 2 (t, 0) 0 e 3 (t,δ H a (t) = 񮽙 1 μ(a) , t = a, 0, t񮽙 = a,has F (z) as a transform, while if a is right dense, the Dirac delta functional has F (z) as a transform as we shall see later.
If two functions f and g have the same transform, then are f and g necessarily the same function?
As on R, the answer to this question is affirmative if we define our equality in an almost everywhere (a.e.) sense.
In order to do so, however, we must clarify what is meant by a.e. on a time scale.In his initial work on the time scale Lebesgue integral, Guseinov [5] defines the Carathéodory extension of the set function that assigns each time scale interval its length to be the Lebesgue 񮽙-measure on T. To construct his outer measure, Guseinov does a Vitali covering of subsets of T by finite or countable systems of intervals of T, and then naturally defines the outer measure to be the infimum of the sums of the lengths of the intervals that cover the subsets.
If there is no such covering, he defines the outer measure of the set to be infinite.
As a consequence of this definition, any time scale will have infinite 񮽙-measure.However, Guseinov points out that his choosing the 񮽙-measure to be infinite is merely to preserve the monotonicity of the outer measure.
The monotonicity will also be preserved if for a subset E of T that cannot be covered, we define the outer measure of E to be the outer measure of the maximal coverable subset of E, call it F , plus some positive extended real number c chosen independently of E. For his purposes, Guseinov chooses c = ∞, but for our purposes, it is convenient to choose c = 0.
By doing this, the Lebesgue 񮽙-measure μ 񮽙 can be decomposed nicely.
For any subset E of T, decompose E as E = D ∪ S, where D = {t ∈ T: t is right dense}, S= {t ∈ T: t is right scattered}.
Since c = 0 above, we may writeμ 񮽙 (E) = m(E ∩ D) + c(E ∩ S),where m(D) denotes the usual Lebesgue measure of the set D and c(S) is the measure given by c(S) = 񮽙 s∈S μ(s).
Notice that with this decomposition, the sets of measure zero can only consist of right dense points since μ 񮽙 (E) = 0 if and only if m(E ∩ D) = 0 and E ∩ S = ∅.
Thus, to show that a property holds almost everywhere on a time scale, it is necessary to show that the property holds for every right scattered point in the time scale, and that the set of right dense points for which the property fails has Lebesgue measure zero.We are now in a position to prove the uniqueness theorem.
∞ 񮽙 0 e σ 񮽙z (t, 0)f (t) t = ∞ 񮽙 0 e σ 񮽙z (t, 0)g(t) t,so that the function h = f − g has transform of zero.
That is, if we let F denote the inversion operator given in Theorem 1.4 and G denote the transform operator, then h ∈ ker G. But, it follows that (F • G)(h) = F (0) = 0 = h, where we note that the function h(t) = 0 a.e. also satisfies the equations above.
Thus, f = g a.e. 2 In this section, we turn our attention to the convolution on an arbitrary time scale T via the Laplace transform.
We need to define the delay of a function on T. Definition 2.1.
The delay of the function x : T → R by σ (τ ) ∈ T, denoted by x(t, σ (τ )), is given byu σ (τ ) (t)x 񮽙 t, σ (τ ) 񮽙 = n 񮽙 i=1 Res z=z i X(z)e z 񮽙 t, σ (τ ) 񮽙 .
Here, u ξ (t) : T → R is the time scale unit step function activated at time t = ξ ∈ T; see [2].
Notice that u σ (τ ) (t)x(t, σ (τ )) has transform X(z)e σ 񮽙z (τ, 0).
Indeed, u σ (τ ) (t)x 񮽙 t, σ (τ ) 񮽙 = n 񮽙 i=1 Res z=z i X(z)e z 񮽙 t, σ (τ ) 񮽙 = n 񮽙 i=1 Res z=z i 񮽙 X(z)e σ 񮽙z (τ, 0) 񮽙 e z (t, 0) = L −1 񮽙 X(z)e σ 񮽙z (τ, 0) 񮽙 .
This allows us to use the term delay to describe x(t, σ (τ )), since on T = R, the transformed function X(z)e −zτ corresponds to the function u τ (t)x(t − τ ).
With the delay operator now defined, we define the convolution of two time scale functions.Definition 2.2.
The convolution of the functions f : T → R and g ∈ C prd-e2 (T, R), denoted f * g, is given by(f * g)(t) = t 񮽙 0 f (τ )g 񮽙 t, σ (τ ) 񮽙 τ.Theorem 2.1 (Convolution Theorem).
The transform of a convolution product that is absolutely integrable is the product of the transforms, with the Laplace integral converging in the regionRe μ * (z) > Re μ * ( ˆ c), wherê c = max{c f , c g } and c f , c g are the exponential constants corresponding to f and g, respectively.Proof.
If we assume absolute integrability of all functions involved, then by the delay property of the transform previously mentioned, we obtainL{f * g} = ∞ 񮽙 0 e σ 񮽙z (t, 0) 񮽙 (f * g)(t) 񮽙 񮽙t = ∞ 񮽙 0 񮽙 t 񮽙 0 f (τ )g 񮽙 t, σ (τ ) 񮽙 񮽙τ 񮽙 e σ 񮽙z (t, 0) )t = ∞ 񮽙 0 f (τ ) 񮽙 ∞ 񮽙 σ (τ ) g 񮽙 t, σ (τ ) 񮽙 e σ 񮽙z (t, 0) )t 񮽙 񮽙τ = ∞ 񮽙 0 f (τ )L 񮽙 u σ (τ ) (t)g 񮽙 t, σ (τ ) 񮽙񮽙 񮽙τ = ∞ 񮽙 0 f (τ ) 񮽙 G(z)e σ 񮽙z (τ, 0) 񮽙 񮽙τ = ∞ 񮽙 0 f (τ )e σ 񮽙z (τ, 0) )τ G(z) = F (z)G(z).
For the convergence of the integral, note that for f and g of exponential type II with constants c f and c g , respectively, we have񮽙 񮽙 (f * g)(t) 񮽙 񮽙 = 񮽙 񮽙 񮽙 񮽙 񮽙 t 񮽙 0 f (τ )g 񮽙 t, σ (τ ) 񮽙 񮽙τ 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 t 񮽙 0 񮽙 񮽙 f (τ ) 񮽙 񮽙 · 񮽙 񮽙 g 񮽙 t, σ (τ ) 񮽙񮽙 񮽙 񮽙τ 񮽙 Me c g (t, 0) t 񮽙 0 e c f (τ, 0)e c g 񮽙 0, σ (τ ) 񮽙 񮽙τ 񮽙 Me c g (t, 0) t 񮽙 0 e c f (τ, 0)e 񮽙c g (t, 0))τ 񮽙 M |c f − c g | e c g (t, 0) 񮽙 e c f 񮽙c g (t, 0) − 1 񮽙 񮽙 M |c f − c g | 񮽙 e c f (t, 0) + e c g (t, 0) 񮽙 񮽙 2M |c f − c g | e ˆ c (t, 0),so that f * g is of exponential type II with constantˆcconstantˆ constantˆc.
2Example 2.1.
Suppose f (t) = f (t, 0) is one of the elementary functions: that is, f (t) is one of h k (t, 0), e a (t, 0), sin a (t, 0), cos a (t, 0), cosh a (t, 0), or sinh a (t, 0).
Direct calculations show that the delay f (t, σ (τ )) of each of these functions is given by h k (t, σ (τ )), e a (t, σ (τ )), sin a (t, σ (τ )), cos a (t, σ (τ )) cosh a (t, σ (τ )), and sinh a (t, σ (τ )).
Thus, convolving any two of these functions will give the results obtained by Bohner and Peterson in [3].
Example 2.2.
Now suppose g(t, σ (τ )) = 1.
In this instance, we see that for any f ∈ C prd-e2 (T, R), the transform of h(t) = 񮽙 t 0 f (τ )τ is given byL{h} = L{f * 1} = F (z)L{1} = F (z) z ,another result obtained by direct calculation in Bohner and Peterson [3].
f (t) = 1 2α sin α (t, 0) t 񮽙 0 1 1 + (μ(τ )α) 2 񮽙τ − 1 2 cos α (t, 0) t 񮽙 0 μ(τ ) 1 + (μ(τ )α) 2 τ,for α > 0, and g(t) = e β (t, 0).
We wish to compute (f * g)(t).
By the Convolution Theorem, the convolution product has transform F (z)G(z).
By Example 1.2,F (z) = z (z 2 + α 2 ) 2 .
Hence (f * g)(t) = L −1 񮽙 F (z)G(z) 񮽙 = 3 񮽙 i=1 Res z=z i ze z (t, 0) (z 2 + α 2 ) 2 (z − β) = 1 2α 2 (α 2 + β 2 )(α 4 + β 2 ) × 񮽙 t 񮽙 0 (−βμ + 1)(α 2 )(α 4 + β 2 ) 1 + (μα) 2 񮽙τ + (α − 1)β 3 + 2α 3 β 񮽙 cos α (t, 0) + 1 2α(α 2 + β 2 )(α 4 + β 2 ) 񮽙 t 񮽙 0 β(α 4 + β 2 ) + α 2 (α 4 + β 2 )μ 1 + (μα) 2 񮽙τ + α 4 + 񮽙 −α 2 + α + 1 񮽙 β 2 񮽙 sin α (t, 0) + β (α 2 + β 2 ) 2 e β (t, 0).
It is worth noting that the convolution product is both commutative and associative.
Indeed, the products f * g and g * f have the same transform as do the products f * (g * h) and (f * g) * h, and since the inverse is unique, the functions defined by these products must agree almost everywhere.At first glance, one may think that the identity is vested in the Hilger delta.
Unfortunately, this is not the case.
It can be easily shown that any identity for the convolution will of necessity have transform 1 by the Convolution Theorem.
But the Hilger delta does not have transform 1 since its transform is just the exponential.
Thus, we develop an analogue of the Dirac delta in the next section in order to establish an identity element.
Let f, g : T → R be given functions with f (x) having unit area.
To define the delta functional, we construct the following functional.
Let C ∞ c (T) denote the C ∞ (T) functions with compact support.
For g σ ∈ C ∞ c (T) and for all 񮽙 > 0, define the functional F :C ∞ c (T, R) × T → R by F 񮽙 g σ , a 񮽙 = 񮽙 񮽙 ∞ 0 δ H a (x)g ρ (σ (x)) x, if a is right scattered, lim 񮽙→0 񮽙 ∞ 0 1 񮽙 f 񮽙 x 񮽙 񮽙 g(σ (x)) x, if a is right dense.The time scale Dirac delta functional is then given by񮽙 δ a (x), g σ 񮽙 = F 񮽙 g σ , a 񮽙 .
We need to see how the delta functional acts on functions from C ∞ c (T).
For this purpose, let g : T → R be given such that g σ ∈ C ∞ c (T).
If a is right dense, considerf (x) = 񮽙 1 񮽙 , if a 񮽙 x 񮽙 a + 񮽙, 0, else,with the understanding that any sequence of 񮽙's we choose will be under the restriction that a + 񮽙 ∈ T for each 񮽙 in the sequence.
Then for h(x) = g σ (x), we have (for any antiderivativeH (x) of h(x)), F 񮽙 g σ , a 񮽙 = lim 񮽙→0 ∞ 񮽙 0 f (x)h(x) x = lim 񮽙→0 񮽙 a+񮽙 a h(x) x 񮽙 = lim 񮽙→0 H (a + 񮽙) − H (a) 񮽙 = H 񮽙 (a) = h(a) = g 񮽙 σ (a) 񮽙 = g(a).
If a is right scattered, thenF 񮽙 g σ , a 񮽙 = ∞ 񮽙 0 δ H a (x)g ρ 񮽙 σ (x) 񮽙 񮽙x = g ρ 񮽙 σ (a) 񮽙 = g(a).
Thus, in functional terms, the Dirac delta functional acts as񮽙 g σ , δ a 񮽙 = g(a),independently of the time scale involved.
Also, if g(t) = e 񮽙z (t, 0), then 񮽙δ a , g σ 񮽙 = e 񮽙z (a, 0), so that for a = 0, the Dirac delta functional δ 0 (t) has Laplace transform of 1, thereby producing an identity element for the convolution.
However, the present definition of the delay operator only holds for functions.
It is necessary to extend this definition for the Dirac delta functional.
To maintain consistency with the delta function's action on g(t) = e σ 񮽙z (t, 0), it follows that for any t ∈ T, the shift of δ a (τ ) is given by δ a (t, σ (τ )) := δ t (σ (τ )).
With this definition, our claim holds:(δ 0 * g)(t 0 ) = t 0 񮽙 0 δ 0 (τ )g 񮽙 t, σ (τ ) 񮽙 񮽙τ = g(t 0 , 0) = g(t 0 ) = t 0 񮽙 0 g(τ )δ t 0 񮽙 σ (τ ) 񮽙 񮽙τ = t 0 񮽙 0 g(τ )δ 0 񮽙 t 0 , σ (τ ) 񮽙 񮽙τ = (g * δ 0 )(t 0 ).
In other words, when we perform the convolution 񮽙g, δ σ 񮽙, we must give meaning to this symbol and do so by defining δ σ (t) to be the Hilger delta when t is right scattered and the Dirac delta if t is right dense.
While this ad hoc approach does not address convolution with an arbitrary (shifted) distribution on the right, this will suffice (at least for now) since our eye is on solving generalizations of canonical partial dynamic equations which will involve the Dirac delta distribution.We now turn to uniqueness of the inverse of the Dirac delta.
We would like an analogue of the diagram given in the proof of Theorem 1.4 where we move from the space of continuous functions to its dual space (or at least restricted to the dual space of those functions that are infinitely differentiable with compact support).
As frequently happens when we move from a space to its dual space, the diagram also becomes the dual of the original one.
Indeed, the diagram in terms of the dual space is shown in Fig. 4.
The mappings γ and γ −1 in the dual diagram act on the transform spaces just as in Fig. 3.
It is worth comparing Figs. 3 and 4 since X ⊂ X * .
So how do we reconcile the two diagrams?
We first must clarify what is meant by the identity map between the two dual spaces.
The Dirac delta is defined in terms of its action on any function: in both dual spaces, 񮽙δ a , g σ 񮽙 = g(a).
To make this agree with the map for the function spaces, note that for f ∈ C prd-e2 (T, R) and g ∈ C p-eo (R, R), the action of f (t) on h σ (t) = e σ 񮽙z (t, 0) should be the same as the action of g(t) oñ h(t) = e −zt .
For example, the function on T that acts on h σ (t) with a result of 1z 2 +1is the function f (t) = sin 1 (t, 0), while the function on R that acts oñ h(t) with the same result is g(t) = sin(t).
As another example, the function on T that acts on h σ (t) with a result of h(τ ) z is the time scale unit step function, while on R, the function that results iñiñ h(τ )z is the continuous step function.
Thus, in terms of the dual spaces, the map on the left hand side of the diagram is the identity map, while in the function spaces this identity map maps g into f σ by the switching of the exponentials between the two domains.
The preceding discussion provides the basis for the uniqueness of the transform of the delta functional.
Through the diagram we see that if another functional had the same transform, then there would be two such functionals over C ∞ c (R, R) which had the same transform, which we know to be false.
(C ∞ c (R, R)) * L R 񮽙 񮽙 L −1 R T f (R) (C ∞ c (T, R)) * I 񮽙 I 񮽙 L T 񮽙 񮽙 L −1 T = I • L −1 R • γ −1 T f (T) γ 񮽙 γ −1Next, we examine more properties of the Dirac delta.
We have already noted that the functional has transform 1 (as desired) since 񮽙δ 0 , e σ 񮽙z (t, 0)񮽙 = 1.
Second, the transform of the derivative of the delta functional is familiar: = 񮽙 δ 0 , g σ 񮽙 .
L 񮽙 δ 񮽙 0 񮽙 = ∞ 񮽙 0 δ 񮽙 0 e σ 񮽙z (t, We now demonstrate a powerful use of the Dirac delta as applied to the Green's function analysis that is ubiquitous in the study of boundary value problems.Consider the operator L : S → C rd given byLx(t) = 񮽙 px 񮽙 񮽙 񮽙 (t) + q(t)x σ (t),where p, q ∈ C rd , p(t) 񮽙 = 0 for all t ∈ T, and S = {x ∈ C 1 (T, R): (px 񮽙 ) 񮽙 ∈ C rd (T, R)}.
In Bohner and Peterson [2], it is shown that if the homogeneous boundary value problem and ψ is the solution ofLψ = 0, ψ 񮽙 σ 2 (b) 񮽙 = δ, ψ 񮽙 񮽙 σ (b) 񮽙 = −γ,then the solution of the nonhomogeneous problem above can be written in the formx(t) = σ (b) 񮽙 a G 񮽙 t, σ (s) 񮽙 h 񮽙 σ (s) 񮽙 s,where G(t, σ (s)) is the Green's function for the boundary value problem and is given by In fact,Lx = σ (b) 񮽙 a L 񮽙 G 񮽙 t, σ (s) 񮽙񮽙 h 񮽙 σ (s) 񮽙 񮽙s = σ (b) 񮽙 a δ σ (t) (s)h 񮽙 σ (s) 񮽙 񮽙s = h 񮽙 σ (t) 񮽙 .
