Information-maximization clustering learns a probabilistic classifier in an unsupervised manner so that mutual information between feature vectors and cluster assignments is maximized.
A notable advantage of this approach is that it only involves continuous optimization of model parameters, which is substantially easier to solve than discrete optimization of cluster assignments.
However, existing methods still involve non-convex optimization problems, and therefore finding a good local optimal solution is not straightforward in practice.
In this paper , we propose an alternative information-maximization clustering method based on a squared-loss variant of mutual information.
This novel approach gives a clustering solution analytically in a computationally efficient way via kernel eigenvalue decomposition.
Furthermore, we provide a practical model selection procedure that allows us to objectively optimize tuning parameters included in the kernel function.
Through experiments , we demonstrate the usefulness of the proposed approach.
The goal of clustering is to classify data samples into disjoint groups in an unsupervised manner.
K-means is a classic but still popular clustering algorithm.
However, since k-means only produces linearly separated clusters, its usefulness is rather limited in practice.To cope with this problem, various non-linear clustering methods have been developed.
Kernel k-means (Girolami, 2002) performs k-means in a feature space induced by a reproducing kernel function.
Spectral clustering (Shi & Malik, 2000) first unfolds non-linear data manifolds by a spectral embedding method, and then performs k-means in the embedded space.
Blurring mean-shift (Fukunaga & Hostetler, 1975) uses a non-parametric kernel density estimator for modeling the data-generating probability density and finds clusters based on the modes of the estimated density.
Discriminative clustering ( Xu et al., 2005;Bach & Harchaoui, 2008) learns a discriminative classifier for separating clusters, where class labels are also treated as parameters to be optimized.
Dependencemaximization clustering (Song et al., 2007;Faivi- shevsky & Goldberger, 2010) determines cluster assignments so that their dependence on input data is maximized.These non-linear clustering techniques would be capable of handling highly complex real-world data.
However, they suffer from lack of objective model selection strategies 1 .
More specifically, the above non-linear clustering methods contain tuning parameters such as the width of Gaussian functions and the number of nearest neighbors in kernel functions or similarity measures, and these tuning parameter values need to be heuristically determined in an unsupervised manner.
The problem of learning similarities/kernels was addressed in earlier works, but they considered supervised setups, i.e., labeled samples are assumed to be given.
Zelnik-Manor & Perona (2005) provided a useful unsupervised heuristic to determine the similarity in a data-dependent way.
However, it still requires the number of nearest neighbors to be determined man-ually (although the magic number '7' was shown to work well in their experiments).
Another line of clustering framework called information-maximization clustering (Agakov & Barber, 2006;Gomes et al., 2010) exhibited the state-of-the-art performance.
In this informationmaximization approach, probabilistic classifiers such as a kernelized Gaussian classifier (Agakov & Barber, 2006) and a kernel logistic regression classifier ( Gomes et al., 2010) are learned so that mutual information (MI) between feature vectors and cluster assignments is maximized in an unsupervised manner.
A notable advantage of this approach is that classifier training is formulated as continuous optimization problems, which are substantially simpler than discrete optimization of cluster assignments.
Indeed, classifier training can be carried out in computationally efficient manners by a gradient method (Agakov & Barber, 2006) or a quasi-Newton method (Gomes et al., 2010).
Furthermore, Agakov & Barber (2006) provided a model selection strategy based on the common information-maximization principle.
Thus, kernel parameters can be systematically optimized in an unsupervised way.However, in the above MI-based clustering approach, the optimization problems are non-convex, and finding a good local optimal solution is not straightforward in practice.
The goal of this paper is to overcome this problem by providing a novel informationmaximization clustering method.
More specifically, we propose to employ a variant of MI called squaredloss MI (SMI), and develop a new clustering algorithm whose solution can be computed analytically in a computationally efficient way via eigenvalue decomposition.
Furthermore, for kernel parameter optimization, we propose to use a non-parametric SMI estimator called least-squares MI (LSMI) ( Suzuki et al., 2009), which was proved to achieve the optimal convergence rate with analytic-form solutions.
Through experiments on various real-world datasets such as images, natural languages, accelerometric sensors, and speech, we demonstrate the usefulness of the proposed clustering method.
In this section, we describe our novel clustering algorithm.
Suppose we are given d-dimensional i.i.d. feature vectors of size n,{x i | x i ∈ R d } n i=1 ,which are assumed to be drawn independently from a distribution with density p * (x).
The goal of clustering is to give cluster assignments,{y i | y i ∈ {1, . . . , c}} n i=1 , to the feature vectors {x i } n i=1, where c denotes the number of classes.
Throughout this paper, we assume that c is known.In order to solve the clustering problem, we take the information-maximization approach (Agakov & Bar- ber, 2006;Gomes et al., 2010).
That is, we regard clustering as an unsupervised classification problem, and learn the class-posterior probability p * (y|x) so that 'information' between feature vector x and class label y is maximized.The dependence-maximization approach ( Song et al., 2007;Faivishevsky & Goldberger, 2010) is related to, but substantially different from the above information-maximization approach.
In the dependence-maximization approach, cluster assignments {y i } n i=1 are directly determined so that their dependence on feature vectors {x i } n i=1 is maximized.
Thus, the dependence-maximization approach intrinsically involves combinatorial optimization with respect to {y i } n i=1 .
On the other hand, the informationmaximization approach involves continuous optimization with respect to the parameter α included in a class-posterior model p(y|x; α).
This continuous optimization of α is substantially easier to solve than discrete optimization of {y i } n i=1 .
Another advantage of the information-maximization approach is that it naturally allows out-of-sample clustering based on the discriminative model p(y|x; α), i.e., a cluster assignment for a new feature vector can be obtained based on the learned discriminative model.
As an information measure, we adopt squared-loss mutual information (SMI).
SMI between feature vector x and class label y is defined bySMI := 1 2 ∫ c ∑ y=1 p * (x)p * (y) ( p * (x, y) p * (x)p * (y) − 1 ) 2 dx,(1)On Information-Maximization Clustering where p * (x, y) denotes the joint density of x and y, and p * (y) is the marginal probability of y. SMI is the Pearson divergence (Pearson, 1900) from p * (x, y) to p * (x)p * (y), while the ordinary MI (Cover & Thomas, 2006) is the Kullback-Leibler divergence (Kullback & Leibler, 1951) from p * (x, y) to p * (x)p * (y):MI := ∫ c ∑ y=1 p * (x, y) log p * (x, y) p * (x)p * (y) dx.
(2)The Pearson divergence and the Kullback-Leibler divergence both belong to the class of Ali-Silvey-Csiszár divergences (which is also known as f -divergences, see (Ali & Silvey, 1966;Csiszár, 1967)), and thus they share similar properties.
For example, SMI is nonnegative and takes zero if and only if x and y are statistically independent, as the ordinary MI.In the existing information-maximization clustering methods (Agakov & Barber, 2006;Gomes et al., 2010), MI is used as the information measure.
On the other hand, in this paper, we adopt SMI because it allows us to develop a clustering algorithm whose solution can be computed analytically in a computationally efficient way via eigenvalue decomposition, as described below.
Here, we give a computationally-efficient clustering algorithm based on SMI (1).
We can express SMI asSMI = 1 2 ∫ c ∑ y=1 p * (x, y) p * (x, y) p * (x)p * (y) dx − 1 2 (3) = 1 2 ∫ c ∑ y=1 p * (y|x)p * (x) p * (y|x) p * (y) dx − 1 2 .
(4)Suppose that the class-prior probability p * (y) is set to be uniform:p * (y) = 1/c.
Then Eq.
(4) is expressed as c 2 ∫ c ∑ y=1 p * (y|x)p * (x)p * (y|x)dx − 1 2 .
(5)Let us approximate the class-posterior probability p * (y|x) by the following kernel model:p(y|x; α) := n ∑ i=1 α y,i K(x, x i ),(6)where K(x, x ′ ) denotes a kernel function with a kernel parameter t.
In the experiments, we will use a sparse variant of the local-scaling kernel (Zelnik-Manor & Perona, 2005):K(x i , x j ) =          exp ( − ∥x i − x j ∥ 2 2σ i σ j ) if x i ∈ N t (x j ) or x j ∈ N t (x i ), 0otherwise,where N t (x) denotes the set of t nearest neighbors for x (t is the kernel parameter), σ i is a local scaling factor defined asσ i = ∥x i − x (t)i ∥, and x (t) i is the t-th nearest neighbor of x i .
Further approximating the expectation with respect to p * (x) included in Eq.
(5) by the empirical average of samples{x i } n i=1, we arrive at the following SMI approximator:SMI := c 2n c ∑ y=1 α ⊤ y K 2 α y − 1 2 , (8)where ⊤ denotes the transpose,α y := (α y,1 , . . . , α y,n ) ⊤ , and K i,j := K(x i , x j ).
For each cluster y, we maximize α ⊤ y K 2 α y under 2 ∥α y ∥ = 1.
Since this is the Rayleigh quotient, the maximizer is given by the normalized principal eigenvector of K (Horn & Johnson, 1985).
To avoid all the solutions {α y } c y=1 to be reduced to the same principal eigenvector, we impose their mutual orthogonality: α ⊤ y α y ′ = 0 for y ̸ = y ′ .
Then the solutions are given by the normalized eigenvectors ϕ 1 , . . . , ϕ c associated with the eigenvalues λ 1 ≥ · · · ≥ λ n ≥ 0 of K.
Since the sign of ϕ y is arbitrary, we set the sign asϕ y = ϕ y × sign(ϕ ⊤ y 1 n ),where sign(·) denotes the sign of a scalar and 1 n denotes the n-dimensional vector with all ones.On the other hand, sincep * (y) = ∫ p * (y|x)p * (x)dx ≈ 1 n n ∑ i=1 p(y|x i ; α) = α ⊤ y K1 n ,and the class-prior probability p * (y) was set to be uniform, we have the following normalization condition:α ⊤ y K1 n = 1/c.
Furthermore, probability estimates should be nonnegative, which can be achieved by rounding up negative outputs to zero.
Taking these issues into account,cluster assignments {y i } n i=1 for {x i } n i=1are determined asy i = argmax y [max(0 n , ϕ y )] i max(0 n , ϕ y ) ⊤ 1 n ,where the max operation for vectors is applied in the element-wise manner and [·] i denotes the i-th element of a vector.
Note that we used K ϕ y = λ y ϕ y in the above derivation.We call the above method SMI-based clustering (SMIC).
Since the above clustering approach was developed in the framework of SMI maximization, it would be natural to determine the kernel parameters so that SMI is maximized.
A direct approach is to use the above SMI estimator SMI also for kernel parameter choice.
However, this direct approach is not favorable because SMI is an unsupervised SMI estimator (i.e., SMI is estimated only from unlabeled samples{x i } n i=1).
In the model selection stage, however, we have already obtained labeled samples {(x i , y i )} n i=1 , and thus supervised estimation of SMI is possible.
For supervised SMI estimation, a non-parametric SMI estimator called least-squares mutual information (LSMI) ( Suzuki et al., 2009) was shown to achieve the optimal convergence rate.
For this reason, we propose to use LSMI for model selection, instead of SMI (8).
LSMI is an estimator of SMI based on paired samples{(x i , y i )} n i=1 .
The key idea of LSMI is to learn the following density-ratio function,r * (x, y) := p * (x, y) p * (x)p * (y) ,(9)without going through density estimation of p * (x, y), p * (x), and p * (y).
More specifically, let us employ the following density-ratio model:r(x, y; θ) := ∑ ℓ:y ℓ =y θ ℓ L(x, x ℓ ),(10)where L(x, x ′ ) is a kernel function with kernel parameter γ.
In the experiments, we will use the Gaussian kernel:L(x, x ′ ) = exp ( − ∥x − x ′ ∥ 2 2γ 2 ) .
(11)The parameter θ in the above density-ratio model is learned so that the following squared error is minimized:1 2 ∫ c ∑ y=1 ( r(x, y; θ) − r * (x, y) ) 2 p * (x)p * (y)dx.
(12)Among n cluster assignments {y i } n i=1 , let n y be the number of samples in cluster y. Let θ y be the parameter vector corresponding to the kernel bases {L(x, x ℓ )} ℓ:y ℓ =y , i.e., θ y is the n y -dimensional subvector of θ = (θ 1 , . . . , θ n ) ⊤ consisting of indices {ℓ | y ℓ = y}.
Then an empirical and regularized version of the optimization problem (12) is given for each y as follows:min θy [ 1 2 θ ⊤ y H (y) θ y − θ ⊤ y h (y) + δθ ⊤ y θ y ] ,(13)where δ (≥ 0) is the regularization parameter.H (y)is the n y × n y matrix and h (y) is the n y -dimensional vector defined as H(y) ℓ,ℓ ′ := n y n 2 n ∑ i=1 L(x i , x (y) ℓ )L(x i , x (y) ℓ ′ ), h(y)ℓ := 1 n ∑ i:yi=y L(x i , x (y) ℓ ),wherex (y) ℓis the ℓ-th sample in class y (which corresponds to θ(y) ℓ ).
A notable advantage of LSMI is that the solution θ (y) can be computed analytically as θ(y) = ( H (y) + δI) −1 h(y).
Then a density-ratio estimator is obtained analytically as follows:r(x, y) = ny ∑ ℓ=1 θ (y) ℓ L(x, x (y) ℓ ).
The accuracy of the above least-squares densityratio estimator depends on the choice of the kernel parameter γ and the regularization parameter δ.
They can be systematically optimized based on crossvalidation as follows ( Suzuki et al., 2009).
The sam-ples Z = {(x i , y i )} n i=1 are divided into M disjoint sub- sets {Z m } M m=1of approximately the same size.
Then a density-ratio estimator r m (x, y) is obtained using Z\Z m (i.e., all samples without Z m ), and its out-ofsample error (which corresponds to Eq.
(12) without irrelevant constant) for the hold-out samples Z m is computed asCV m := 1 2|Z m | 2 ∑ x,y∈Zm r m (x, y) 2 − 1 |Z m | ∑ (x,y)∈Zm r m (x, y).
This procedure is repeated for m = 1, . . . , M , and the average of the above hold-out error over all m is computed.
Finally, the kernel parameter γ and the regularization parameter δ that minimize the average holdout error are chosen as the most suitable ones.Based on the expression of SMI given by Eq.
(3), an SMI estimator called LSMI is given as follows:LSMI := 1 2n n ∑ i=1 r(x i , y i ) − 1 2 ,(14)where r(x, y) is a density-ratio estimator obtained above.
Since r(x, y) can be computed analytically, LSMI can also be computed analytically.We use LSMI for model selection of SMIC.
More specifically, we compute LSMI as a function of the kernel parameter t of K(x, x ′ ) included in the cluster-posterior model (6), and choose the one that maximizes LSMI.MATLAB implementation of the proposed clustering method is available from 'http://sugiyamawww.cs.titech.ac.jp/˜sugi/software/SMIC'.
In this section, we qualitatively compare the proposed approach with existing methods.
The basic idea of spectral clustering (Shi & Malik, 2000) is to first unfold non-linear data manifolds by a spectral embedding method, and then perform kmeans in the embedded space.
More specifically, given sample-sample similarity W i,j ≥ 0, the minimizer of the following criterion with respect to {ξ i } n i=1 is obtained under some normalization constraint:n ∑ i,j W i,j 1 √ D i,i ξ i − 1 √ D j,j ξ j 2 ,where D is the diagonal matrix with i-th diagonal element given by D i,i := ∑ n j=1 W i,j .
Consequently, the embedded samples are given by the principal eigenvectors of D − 1 2 W D − 1 2 , followed by normalization.
Note that spectral clustering was shown to be equivalent to a weighted variant of kernel k-means with some specific kernel ( Dhillon et al., 2004).
The performance of spectral clustering depends heavily on the choice of sample-sample similarity W i,j .
Zelnik-Manor & Perona (2005) proposed a useful unsupervised heuristic to determine the similarity in a data-dependent manner, called local scaling:W i,j = exp ( − ∥xi−xj ∥ 2 2σiσj ), where σ i is a local scaling factor de-fined as σ i = ∥x i − x (t)i ∥, and x (t) i is the t-th nearest neighbor of x i .
t is the tuning parameter in the local scaling similarity, and t = 7 was shown to be useful (Zelnik-Manor & Perona, 2005;Sugiyama, 2007).
However, this magic number '7' does not seem to work always well in general.If D − 1 2 W D − 1 2is regarded as a kernel matrix, spectral clustering will be similar to the proposed SMIC method described in Section 2.3.
However, SMIC does not require the post k-means processing since the principal components have clear interpretation as parameter estimates of the class-posterior model (6).
Furthermore, our proposed approach provides a systematic model selection strategy, which is a notable advantage over spectral clustering.
Blurring mean-shift (Fukunaga & Hostetler, 1975) is a non-parametric clustering method based on the modes of the data-generating probability density.In the blurring mean-shift algorithm, a kernel density estimator (Silverman, 1986) is used for modeling the data-generating probability density:p(x) = 1 n n ∑ i=1 K ( ∥x − x i ∥ 2 /σ 2 ) ,where K(ξ) is a kernel function such as a Gaussian kernel K(ξ) = e −ξ/2 .
Taking the derivative of p(x) with respect to x and equating the derivative at x = x i to zero, we obtain the following updating formula for sample x i (i = 1, . . . , n):x i ←− ∑ n j=1 W i,j x j ∑ n j ′ =1 W i,j ′ , where W i,j := K ′ ( ∥x i − x j ∥ 2 /σ 2 )and K ′ (ξ) is the derivative of K(ξ).
Each mode of the density is regarded as a representative of a cluster, and each data point is assigned to the cluster which it converges to.
Carreira-Perpiñán (2007) showed that the blurring mean-shift algorithm can be interpreted as an EM algorithm (Dempster et al., 1977), whereW i,j /( ∑ n j ′ =1 W i,j ′ )is regarded as the posterior probability of the i-th sample belonging to the j-th cluster.
Furthermore, the above update rule can be expressed in a matrix form as X ←− XP , where X = (x 1 , . . . , x n ) is a sample matrix and P := W D −1 is a stochastic matrix of the random walk in a graph with adjacency W (Chung, 1997).
D is defined asD i,i := ∑ n j=1 W i,j and D i,j = 0 for i ̸ = j.If P is independent of X, the above iterative algorithm corresponds to the power method (Golub & Loan, 1996) for finding the leading left eigenvector of P .
Then, this algorithm is highly related to the spectral clustering which computes the principal eigenvectors of D − 1 2 W D − 1 2 (see Section 3.1).
Although P depends on X in reality, Carreira-Perpiñán (2006) insisted that this analysis is still valid since P and X quickly reach a quasi-stable state.An attractive property of blurring mean-shift is that the number of clusters is automatically determined as the number of modes in the probability density estimate.
However, this choice depends on the kernel parameter σ and there is no systematic way to determine σ, which is restrictive compared with the proposed method.
Another critical drawback of the blurring mean-shift algorithm is that it eventually converges to a single point (Cheng, 1995), and therefore a sensible stopping criterion is necessary in practice.
Although Carreira-Perpiñán (2006) gave a useful heuristic for stopping the iteration, it is not clear whether this heuristic always works well in practice.
In this section, we experimentally evaluate the performance of the proposed and existing clustering methods.
First, we illustrate the behavior of the proposed method using artificial datasets described in the top row of Figure 1.
The dimensionality is d = 2 and the sample size is n = 200.
As a kernel function, we used the sparse local-scaling kernel (7) for SMIC, where the kernel parameter t was chosen from {1, . . . , 10} based on LSMI with the Gaussian kernel (11).
The top graphs in Figure 1 depict the cluster assignments obtained by SMIC, and the bottom graphs in Figure 1 depict the model selection curves obtained by LSMI.
The results show that SMIC combined with LSMI works well for these toy datasets.
Next, we systematically compare the performance of the proposed and existing clustering methods using various real-world datasets such as images, natural languages, accelerometric sensors, and speech.We compared the performance of the following methods, which all do not contain open tuning parame- ters and therefore experimental results are fair and objective: K-means (KM), spectral clustering with the self-tuning local-scaling similarity (SC) (Zelnik-Manor & Perona, 2005), mean nearest-neighbor clustering (MNN) (Faivishevsky & Goldberger, 2010), MI-based clustering for kernel logistic models (MIC) (Gomes et al., 2010) with model selection by maximumlikelihood MI ( Suzuki et al., 2008), and the proposed SMIC.The clustering performance was evaluated by the adjusted Rand index (ARI) (Hubert & Arabie, 1985) between inferred cluster assignments and the ground truth categories.
Larger ARI values mean better performance, and ARI takes its maximum value 1 when two sets of cluster assignments are identical.
In addition, we also evaluated the computational efficiency of each method by the CPU computation time.We used various real-world datasets including images, natural languages, accelerometric sensors, and speech: The USPS hand-written digit dataset ('digit'), the Olivetti Face dataset ('face'), the 20-Newsgroups dataset ('document'), the SENSEVAL-2 dataset ('word'), the ALKAN dataset ('accelerometry'), and the in-house speech dataset ('speech').
Detailed explanation of the datasets is omitted due to lack of space.For each dataset, the experiment was repeated 100 times with random choice of samples from a pool.
Samples were centralized and their variance was normalized in the dimension-wise manner, before feeding them to clustering algorithms.The experimental results are described in Table 1.
For the digit dataset, MIC and SMIC outperform KM, SC, and MNN in terms of ARI.
The entire computation time of SMIC including model selection is faster than KM, SC, and MIC, and is comparable to MNN which does not include a model selection procedure.
For the Table 1.
Experimental results on real-world datasets (with equal cluster size).
The average clustering accuracy (and its standard deviation in the bracket) in terms of ARI and the average CPU computation time in second over 100 runs are described.
The best method in terms of the average ARI and methods judged to be comparable to the best one by the t-test at the significance level 1% are described in boldface.
Computation time of MIC and SMIC corresponds to the time for computing a clustering solution after model selection has been carried out.
For references, computation time for the entire procedure including model selection is described in the square bracket.
face dataset, SC, MIC, and SMIC are comparable to each other and are better than KM and MNN in terms of ARI.
For the document and word datasets, SMIC tends to outperform the other methods.
For the accelerometry dataset, MNN and SMIC work better than the other methods.
Finally, for the speech dataset, MIC and SMIC work comparably well, and are significantly better than KM, SC, and MNN.Overall, MIC was shown to work reasonably well, implying that model selectoin by maximum-likelihood MI is practically useful.
SMIC was shown to work even better than MIC, with much less computation time.The accuracy improvement of SMIC over MIC was gained by computing the SMIC solution in a closedform without any heuristic initialization.
The computational efficiency of SMIC was brought by the analytic computation of the optimal solution and the class-wise optimization of LSMI (see Section 2.4).
The performance of MNN and SC was rather unstable because of the heuristic averaging of the number of nearest neighbors and the heuristic choice of local scaling.
In terms of computation time, they are rela- Class-imbalance was realized by setting the sample size of the first class m times larger than other classes.
The results for m = 1 are the same as the ones reported in Table 1.
tively efficient for small-to medium-sized datasets, but they are expensive for the largest dataset, digit.
KM was not reliable for the document and speech datasets because of the restriction that the cluster boundaries are linear.
For the digit, face, and document datasets, KM was computationally very expensive since a large number of iterations were needed until convergence to a local optimum solution.Finally, we performed similar experiments under imbalanced setup, where the the sample size of the first class was set to be m times larger than other classes.The results are summarized in Table 2, showing that the performance of all methods tends to be degraded as the degree of imbalance increases.
Thus, clustering becomes more challenging if the cluster size is imbalanced.
Among the compared methods, the proposed SMIC still worked better than other methods.Overall, the proposed SMIC combined with LSMI was shown to be a useful alternative to existing clustering approaches.
In this paper, we proposed a novel informationmaximization clustering method, which learns classposterior probabilities in an unsupervised manner so that the squared-loss mutual information (SMI) between feature vectors and cluster assignments is maximized.
The proposed algorithm called SMI-based clustering (SMIC) allows us to obtain clustering solutions analytically by solving a kernel eigenvalue problem.
Thus, unlike the previous information-maximization clustering methods (Agakov & Barber, 2006;Gomes et al., 2010), SMIC does not suffer from the problem of local optima.
Furthermore, we proposed to use an optimal non-parametric SMI estimator called leastsquares mutual information (LSMI) for data-driven parameter optimization.
Through experiments, SMIC combined with LSMI was demonstrated to compare favorably with existing clustering methods.
We would like to thank Ryan Gomes for providing us his program code of information-maximization clustering.
MS was supported by SCAT, AOARD, and the FIRST program.
MY and MK were supported by the JST PRESTO program, and HH was supported by the FIRST program.
