Dimensionality reduction via feature projection has been widely used in pattern recognition and machine learning.
It is often beneficial to derive the projections not only based on the inputs but also on the target values in the training data set.
This is of particular importance in predicting mul-tivariate or structured outputs which is an area of growing interest.
In this paper we introduce a novel projection framework which is sensitive to both input features and outputs.
Based on the derived features prediction accuracy can be greatly improved.
We validate our approach in two applications.
The first is to model users' preferences on a set of paintings.
The second application is concerned with image categorization where each image may belong to multiple categories.
The proposed algorithm produces very encouraging results in both settings.
Consider the pattern recognition task of predicting an output quantity y given an input feature vector x.
If the input space is high-dimensional and contains irrelevant features, the design of an appropriate pattern recognition system becomes a non-trivial problem.
Thus it is desirable to employ a preprocessing step in which input features are first projected into a new feature space that is compact, noise-free, and highly indicative.
As an outcome, learning algorithms based on the new features are often efficient and effective.
Projection methods such as principal component analysis (PCA), linear discriminant analysis (LDA) (see [2]), canonical correlation analysis (CCA) (e.g., [3,1]) and partial least squares (PLS) [9,4] have been applied successively in various applications.Among all the algorithms, PCA is probably the most common choice, which aims to find the principle components preserving the covariance structure of input features.
However, the unsupervised manner indicates the uncovered components not necessary helpful for predictions.In this paper we are interested in supervised projection methods, since it is often beneficial to ensure feature projections sensitive to the predicted quantities.
In particular we consider a very general setting where the outputs are multivariate, i.e., for an example x the corresponding output is a vector y = [y 1 , . . . , y L ] .
Note that the usual univariate output is a special case of the framework.Multi-output problem is very common in real-world applications, typically involving multiple predictive tasks based on the same input space.
One example is to model people's preferences on a set of products.
This is a typical multi-output problem since for each product many persons' preferences have to be estimated.
Since people's tastes are usually correlated, it is desired to the interdependency between individuals.
Another example is the problem of multi-label image categorization, where each image is allowed to be associated with multiple categories, which often have semantic correlations.
There are many other multioutput problems where the dependency of outputs should be explored.
For instance, tracking the positions of different parts of a person's hands or arms has multi-dimensional outputs which are dependent of each other, since the freedoms of different parts are mutually restricted.In all these applications it is desired to exploit the dependency between the multiple outputs for prediction and multivariate data analysis.
This paper introduces a novel framework, multi-output regularized projection (MORP), which maps the input features into a new feature space that not only retains the information of inputs, but also captures the dependency of outputs as well.
The algorithm exposes the inherent structure of input features which are highly informative for predictions.The paper is organized as follows.
In Section 2 we formulate the data projection as an optimization problem in the linear case and then propose an regularized version to prevent overfitting, which is generalized to nonlinear mapping by using kernels.
Then we discuss it connections to related work in Section 3.
Finally we report the experiments in Section 4 and conclude the paper in Section 5.
We consider a set of N examples.
For i = 1, . . . , N , each example i is described by an M -dimensional feature vector x i ∈ X , and is associated with an L-dimensional output vector y i ∈ Y.
We denote the input data as a matrix X = [x 1 , . . . , x N ] ∈ R N ×M , and the output data as Y = [y 1 , . . . , y N ] ∈ R N ×L , where [·] denotes matrix transpose.
We aim to derive a mapping Ψ : X → V that projects the input features into a K-dimensional latent space.
We propose to project input features into a new feature space V that preserves the statistical structure of X as much as possible, and meanwhile explains Y very well.
Thus we solve the following optimization problem:min A,B,V (1 − β)X − VA 2 + βY − VB 2 (1) subject to: V V = I,where V ∈ R N ×K gives the K-dimensional projections of examples.
A ∈ R K×M , B ∈ R K×L are the factor loadings for X and Y, respectively.
0 ≤ β ≤ 1 is a tuning parameter determining how much the indexing should be biased by the outputs.
V V = I restricts the K latent variables to be linearly independent and have identical variances.
Clearly, the cost function is a trade-off between the reconstruction error of both X and Y.
The following proposition states the interdependency between A, B and V at the optimum.
To improve readability, we put all proofs into the appendix.
Since X 2 F and Y 2 F are both fixed, and the objective function is convex, Proposition 2.1 suggests that the problem (1) can be considered to be an optimization problem only with respect to V:max V Tr V KV(2)subject to: V V = INote an ambiguity arises in (1) and (2).
If V is the solution, then V = VR is also a solution, given an arbitrary rotation matrix R.
The following theorem summarizes the situation.
is K i=1 λ i ; (ii) V has the form [˜ v 1 , . . . , ˜ v K ]R, where R is an arbitrary K × K orthogonal rotation matrix.To remove the ambiguity, we are focusing on the solutions given by the eigenvectors without any rotation, i.e. v j = ˜ v j , j = 1, . . . , N .
Thus the original optimization problem (1) has an equivalent form: 1max v∈R N v Kv (3) subject to : v v = 1,By setting the Lagrange's derivative to be zero, we obtain the standard form of an eigenvalue problem Kv = λv.
Let v 1 , . . . , v N be the eigenvectors of K with the eigenvalues sorted in a non-increasing order, then an optimal solution to (1) is given as V = [v 1 , . . . , v K ], A = XV, and B = YV.
Instead of uncovering the latent projections of observed examples, this paper focuses on learning the mapping functions Ψ : X → V that are able to map new input features into a meaningful space, thus we restrict the latent variables as linear mappings of X, and solve the following problem we have an optimization problem with respect to w max w∈R M w X KXw (5) subject to : w X Xw = 1min A,B,V (1 − β)X − VA 2 + βY − VB 2 (4) subject to: V V = I, V = XW where W = [w 1 , . . . , w K ] ∈ R M ×K .
Plugging v = Xw into (3),Setting the derivative of its Lagrange with respect to w to be zero, we reach a generalized eigenvector problem 2 :X KXw = λX Xw (6)which produces M generalized eigenvectors w 1 , . . . , w M , as well as the eigenvalues λ 1 ≥ . . . ≥ λ M .
The first K eigenvectors are used to form the mapping functionsψ j (x) = λ j w j x, j = 1, . . . , K(7)where the scaling with λ j reflects the relative importance of projection dimensions.
FinallyΨ(x) = [ψ 1 (x), . . . , ψ K (x)] maps x into a K-dimensional space.
However, similar to other linear systems, the learned mapping functions can be ill-posed when X has the rank lower than M , which typically happens when the dimensionality of input features is very high, namely N M .
Under a mild assumption 3 rank(K) = N , the maximization in (3) is equivalent to minimizing v K −1 v.
Then we regularize the problem (5) as the followingmin w∈R M w X K −1 Xw + γw 2 subject to : w X Xw = 1(8)where w 2 = w w is the Tickhonov regularizer [7] typically applied in ill-posed problems, and γ is a nonnegative scalar which is usually very small.
The corresponding generalized eigenvalue problem isX K −1 X + γI w = λX Xw(9)which gives generalized eigenvectors w 1 , . . . , w M with eigenvalues λ 1 ≤ . . . ≤ λ M .
Note since the objective is the inverse of some maximization problem, we sort eigenvalues in a non-decreasing order, and take the first K eigenvectors to form the mapping.
The following theorem implies that we can also derive a nonlinear mapping Ψ using the kernel trick.
α ∈ R N such that w = X α = N i=1 (α) i x i .
If {x 1 , . . . , x N } are linearly independent, such an α is unique.Let X be a reproducing kernel Hilbert space (RKHS) with the kernel function κ x (x i , x j ) = x i , x j , then based on Theorem 2.3 we have v = Xw = XX α = K x α where K x is the N × N kernel matrix with (K x ) i,j = κ x (x i , x j ).
Then an equivalent form of (8) is min α∈R N α K x K −1 K x α + γα K x α subject to : α K 2 x α = 1(10)whereK = (1 − β)K x + βYY .
The corresponding gen- eralized eigenvalue problem is K x K −1 K x + γK x α = λK 2 x α(11)With the eigenvalues sorted asλ 1 ≤ . . . ≤ λ N , the first K eigenvectors [α 1 , . . . , α K ]give the mappings, where thej-th function is ψ j (x) = w j x = N i=1 (α j ) i κ x (x i , x).
Now the algorithm is readily able to deal with nonlinear mappings.
We consider a nonlinear function Φ : X → F, which maps x into a high-dimensional or even infinite-dimensional feature space F, and let X = [φ(x 1 ), . . . , φ(x N )], then the kernel is accordingly defined as κ x (x i , x j ) = φ(x i ), φ(x j ).
Finally, we can directly work with kernels (e.g. Gaussian kernel), without knowing φ(·) explicitly.
Sometimes the outputs are not just vector-valued, but also have some complex structure like sequences or graphs.
Similar to the case of X, one can consider an proper kernel κ y (y i , y j ) = ϕ(y i ), ϕ(y j ) to characterize the structure of outputs, where ϕ(·) maps output vectors y into a RKHS space.
Let Y = [ϕ(y 1 ), . . . , ϕ(y N )], thenK = (1 − β)K x + βYY = (1 − β)K x + βK y (12)In various problems, we can design the κ y (·, ·) tailored to the nature of data.
This is a very general setting where nonlinear dependency of outputs can be explored.
The methods discussed in the previous sections are special cases which use the linear kernel κ y (y i , y j ) = y i , y j .
It is usually convenient to seek for the eigenvectors with the largest eigenvalues, which is numerically stabler and more efficient.
Thus we transform the optimization problem (10) to obtain another equivalent form.
Let v = K x α, then problem (10) becomesmin v v (K −1 + γK −1 x )v(13)subject to : v v = 1After some matrix derivation, we can get(K −1 + γK −1 x ) −1 = K(γK + K x ) −1 K x .
Then the objective in (13) becomes maximizing v T K(γK + K x ) −1 K x v, which leads to the following standard eigenvalue problem:K(γK + K x ) −1 K x v = λv(14)In practice, since γ is usually very small, the eigenvalue problem (14) can be approximated as Kv = λv.
Compared to (14), the simplified version is much more efficient, since the multiplication of matrices and matrix inverse are both removed.
After obtaining the leading eigenvectors v j , j = 1, . . . , k with the largest eigenvalues λ j (due to the maximization), we can recover the coefficient vectors as Finally, incorporating the eigenvalues that reflect the relative importance of latent dimensions, we obtain the final feature mapping functionsα j = K −1 x v j , j = 1, . . . , K(15)ψ j (x) = λ j N i=1 (α j ) i κ x (x i , x), j = 1, . . . , K.(16) The proposed framework MORP becomes identical to PCA if β = 0.
This connection is also clear from (11), which becomes identical to kernel PCA [5] when β = 0.
In the other extreme case β = 1, the feature mapping is enforced to entirely explain the dependency of outputs.
Then the MORP algorithm is in spirit similar to kernel dependency estimation (KDE) [8], which first performs PCA on K y , and then uses input features to regress the eigenvectors.
Due to the regularization in the post-regression phase, the uncovered projections are usually not orthogonal.
In contrast to KDE's two-step strategy, our algorithm is derived in a single optimization framework.
The proposed MORP is a very general framework.
Compared with PCA, it makes the projections sensitive to output quantities, which is desired in supervised learning tasks.
Compared with KDE, MORP retains the structure of the input features and thus prevents to be overfitted by the outputs, which makes the derived mapping functions more stable and potentially generalizable to new output dimensions.In the literature there are some other supervised projection methods, like linear discriminant analysis (LDA) (e.g., [6]), canonical correlation analysis (CCA) (e.g., [3,1]) and partial least squares (PLS) [9,4].
MORP substantially differs from them.
LDA is focusing on single classification problem where the output is one-dimensional.
CCA finds the correlations between two representatives of the same examples (e.g., inputs X and outputs Y in our setting) by minimizing v x −v y 2 subject to both v x and v y being unitary and linear mappings of x i and y i (see a recent discussion in [1]).
However, it does not require the projections v x and v y to promise low-reconstruction error of x and y and thus ignores the intra correlation of either.
Instead, MORP takes into account all the inter and intra dependencies, since the projections minimize the reconstruction error of inputs and outputs simultaneously.
PLS can be seen as penalized CCA (see [6]), which purely focuses on the regression of known output quantities, while does not consider the generalization for new dimension of outputs.
We evaluate the proposed MORP on preference prediction and image categorization.
In both settings each example is an image, from which color histogram (216-dim.)
, correlagram (256-dim.)
, first and second color moments (9-dim.)
and Pyramid wavelet texture (10-dim.)
are extracted to form a 491-dimensional feature vector x.
In both settings K x is based on RBF kernel while K y is based on linear kernel, and both matrices are re-scaled to ensure equal traces.
For MORP β = 0.5 and γ = 0.001.
Note that γ is found uncritical as long as it is very small.
We collected 190 users' ratings on 642 paintings in a survey, where each user expresses "like" and "dislike" for some randomly presented paintings.
On average each user had rated 89 paintings, thus there are missing entries in Y.
This is a typical multi-output classification problem, since for each paining many users' opinions need to be predicted.
We examine the performance of various feature projection algorithms that map the original features into a 20-dim.
space, where the new features are fed into SVMs.
In the experiment, a set of users are selected as test users.
For each test user, we withdraw some ratings so that 20 ratings are left, then a SVM trained on the 20 examples is employed to predict the rest of ratings.
We compare MORP with kernel CCA and kernel PCA.
The two supervised methods, i.e., MORP and CCA, make use of the 190-dimensional outputs, with the missing and withdrawn entries filled with zeros.
The derived new features are fed into SVMs with linear kernels.
We also perform SVMs employing the same RBF kernel with the original 491-dim.
features.The first metric for evaluation is Top-N accuracy, i.e. the proportion of truly liked paintings among the N top-ranked paintings.
Due to the missing entries in y, we actually count the fraction of known liked paintings in the top ranked N paintings.
The quantity is smaller than the true accuracy because unknown liked paintings are missing in the measurement.
However, in our survey, the presenting of paintings to users is completely random, thus the distributions of rated/unrated paintings in both unranked and ranked lists are also random.
This randomness dose not change the relative performances of the studied methods and thus the comparison still makes sense.
The other metric is the ROC curve, which reflects the ranking quality of predictions and insensitive to the missing entries.The experiment employs 10-fold cross validation, in which each fold is set as active users.
For each active user the accuracy is averaged over 10 tests-in each time the 20 seen ratings are randomized.
Finally the mean and variance over the 10 folds are presented in Figure 1.
MORP significantly outperforms others in terms of both accuracy and ROC, because it explores the dependency between users.
We also found that supervised projections, i.e., MORP and CCA, are generally better than unsupervised PCA.
Note that in the left panel the ROC curves of PCA and original features are almost overlapped.
The experiment is based on a subset of Corel image database, containing 1021 images that have been manually assigned into 35 categories based on their contents.
In average, each image belongs to 3.6 categories and each category contains 98 positive examples.
We treat each category as a binary classification problem.
We employ AUC score and macro/micro F1 value to measure the accuracy.
AUC is the size of area under the ROC curve, ranging from 0 to 1.
F1 measures have been widely used in text categorization which combines precision and recall and is suitable when positive examples are much less than negative ones.
Macro F1 is the simple average over all the categories while micro F1 is average weighted by the size of positive examples in each category.
In all the cases larger values indicating better performances.In each run of the experiment, we randomly pick up 25 categories and have 500 examples labeled.
Projection methods with RBF kernels are trained on the 500 examples to learn the mapping functions, which are then employed to compute new features for all the 1021 images.
In setting (I) we train linear SVM classifiers to predict the rest 521 images' labels, while in setting (II) we perform classification with 5-fold cross validation on the unlabeled 521 images with respect to the remaining 10 categories (one fold training and 4 folds test).
Note that the second setting examines the generalization of supervised projection methods on new output dimensions.
The whole experiments are repeated by 10 runs with randomization, and the classification accuracy under different dimensionality of projections are shown in Figure 2.
We can see that MORP outperforms CCA and PCA in all the cases.
In particular, the results in the lower panels indicate that the features derived by MORP are generalized well to new predictive problems.
In this paper we propose a novel feature projection algorithm for predicting multivariate outputs.
The projections retain the statistical structure of not only input features but also the outputs.
We present the kernel version of the mappings such that nonlinear dependency can be captured.
The algorithm achieves very good results in user preference prediction and image categorization.
Currently we mainly exploit the linear dependency of outputs in the empirical study.
As suggested in Section 2.5, the algorithm is generally applicable for outputs with richer structures, like sequences or graphs.
In the future its applications to modeling structured outputs should be further studied.
(2) is L(V, ˜ Λ) = K i=1 v i Kvi − 2 i =j˜λi =j˜ =j˜λi,jv i vj − K j=1˜λj j=1˜ j=1˜λj,j(v i vi − 1)where ( ˜ Λ)i,j = ˜ λi,j and V = [v1, . . . , vK ].
Setting its derivative with respect to vj to be zero, we obtain∂L ∂vj = 2Kvj − 2 K i=1˜λii=1˜ i=1˜λi,jvj = 0, j = 1, . . . , K, which can be rewritten as KV = V ˜ Λ.
Since˜ΛSince˜ Since˜Λ is a symmetric matrix, we have˜Λhave˜ have˜Λ = R ΛR where Λ is a diagonal matrix and R is an orthogonal rotation matrix satisfying RR = R R = I.
Then KV = VR ΛR yields KVR = VRΛ.
Since Λ is diagonal, it is easy to see that the columns of˜Vof˜ of˜V = VR are the eigenvectors of K. Thus the optimal V is formed by an arbitrary rotation of K's eigenvectors, i.e. V = ˜ VR.
Inserting V back to the objective function, then the value of objective function are Tr(Λ), i.e., sum of the K corresponding eigenvalues of K.
It is easy to see the maximal Tr(Λ) is the sum of the K largest eigenvalues, which proofs (i).
In this case, V is an arbitrary rotation of the K largest eigenvectors, thus conclusion (ii) holds.Proof.
( Theorem 2.3) Let J(w) denote the cost function in (8).
Obviously J(w) achieves the minimum at the first eigenvector w = w1 of the generalized eigenvalue problem (9).
Consider w as the projection of w1 on the subspace span{x1, . . . , xN }.
Then we can write w1 = w + w ⊥ , where w ⊥ is orthogonal to the subspace spanned by xi.
Then we have w 1 xi = w xi + w ⊥ xi.
Since w1 2 = w 2 + w ⊥ 2 ≥ w 2 , then J(w1) ≥ J(w ).
However, J(w1) achieves the minimum, meaning J(w1) ≤ J(w ).
Therefore J(w1) = J(w ), and w ⊥ = 0.
So far we have proofed that the first eigenvector (with the smallest eigenvalue) is a linear combination of xi.
Given eigenvectors wj, j = 1, . . . , n − 1, it is known that the n-th eigenvector is obtained by first deflating the matrix K † = K −1 − n−1 j=1 λjX wjw j X and then solving the problem min w∈R M w XK † X w + γw 2 , subject to: w X Xw = 1.
Following the same procedure as before we proof that the eigenvector wn also lies in the span of xi.
