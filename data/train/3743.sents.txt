Streaming computations are by nature long-running, and their workloads can change in unpredictable ways.
This in turn means that maintaining performance may require dynamic scaling of allocated computational resources.
Some modern large-scale stream processors allow dynamic scaling but typically leave the difficult task of deciding how much to scale to the user.
The process is cumbersome, slow and often inefficient.
Where automatic scaling is supported, policies rely on coarse-grained met-rics like observed throughput, backpressure, and CPU utilization.
As a result, they tend to show incorrect provi-sioning, oscillations, and long convergence times.
We present DS2, an automatic scaling controller for such systems which combines a general performance model of streaming dataflows with lightweight instrumen-tation to estimate the true processing and output rates of individual dataflow operators.
We apply DS2 on Apache Flink and Timely Dataflow and demonstrate its accuracy and fast convergence.
When compared to Dhalion, the state-of-the-art technique in Heron, DS2 converges to the optimal, backpressure-free configuration in a single step instead of six.
We present DS2, a low-latency, robust controller for dynamic scaling of streaming analytics applications, which can vary the resources available to a computation so as to handle variable workloads quickly and efficiently.Static provisioning is a poor fit for continuous, longrunning streaming applications: it forces users to choose a single point on the spectrum between allocating resources for worst-case, peak load (which is inefficient) and suffering degraded performance during load spikes.
Fixing resources a priori almost inevitably leads to a system which is over-or under-provisioned for much of its execution.
* Work done while visiting the Systems Group at ETH Zürich Figure 1: Effect of Dhalion's scaling decisions on the source rate when trying to match the target throughput of an under-provisioned word count dataflow.The solution is to dynamically scale the system in response to load, an idea used extensively in cloud environments [30,31].
This requires both a mechanism for scaling the computation, and a scaling controller which decides when and how to scale.
This paper focuses on the latter; DS2 is designed to be mechanism-agnostic.
A scaling controller makes two kinds of decisions.
First, it detects symptoms of over-or under-provisioning (e.g. backpressure) and decides whether to make a change.
Detection is often straightforward and addressed by conventional monitoring tools.
Second, the controller must identify the causes of symptoms (e.g. a bottlenecked or idle operator) and propose a scaling action.The second decision is challenging, involving performance analysis and prediction.
Streaming systems supporting a form of automatic dynamic scaling (e.g. Google Cloud Dataflow [26,5], Heron [27,13], Pravega [11], Spark Streaming [45], and IBM System S [15]) and research prototypes (e.g. Seep [12] and StreamCloud [17]) focus on the first decision and either ignore or provide speculative, often ad-hoc solutions for the second.A good scaling controller should provide the SASO properties [19] familiar from Control Theory: Stability (not oscillating between different configurations), Accuracy (finding the optimal configuration for the given workload), Short settling times to reach the optimal configuration, and not Overshooting.Speculative scaling decisions which do not provide these properties can be bad for streaming systems.
First, they lead to temporary over-or under-provisioning, and the resulting sub-optimal resource utilization incurs unnecessary costs.
Second, oscillations can in turn degrade performance due to frequent scaling actions.
Finally, speculative scaling can be slow to converge, resulting in Service Level Objective (SLO) violations or load shedding.
Figure 1 illustrates these problems in the state-of-theart Dhalion controller [13] of Heron, using the same word count dataflow as in the original paper.
The dashed line shows the target throughput (source output rate), while the solid line tracks the achieved throughput, which varies due to backpressure as Dhalion changes the computation scale.
Dhalion performs six scaling decisions, taking more than 30 minutes to converge.We make the following contributions in this paper.
First, we review how existing dynamic scaling techniques can lead to inaccurate, unstable, or slow provisioning decisions.
We identify the causes of these effects ( § 2), which we attribute to the lack of a comprehensive performance model, dependence on heuristics, and use of coarse-grained, externally-observed execution metrics.Second, we propose DS2, a general model and controller for automatic scaling of distributed streaming dataflows ( § 3).
DS2 can accurately estimate parallelism for all dataflow operators within a single scaling decision, and operates reactively online.
As a result, DS2 eliminates oscillation and overprovisioning when making scaling decisions.
DS2 bases scaling decisions on real-time performance traces, and is general: it relies neither on specific signals like backpressure, as in [13], nor simplistic assumptions like 1-1 operator selectivity, as in [41].
Third, DS2 gives leverage on existing state-of-the-art approaches: when used in Heron, it identifies the optimal backpressure-free configuration in a few seconds and one step, while Dhalion performs six steps to reach an overprovisioned configuration in the same scenario ( § 5.2).
Fourth, we apply DS2 on Apache Flink ( § 5.3) and demonstrate fully-automatic scaling of streaming dataflows under dynamic workload.Finally, we show that DS2 is accurate and converges quickly for both Apache Flink and Timely Dataflow ( § 5.4 and § 5.5).
In all experiments DS2 takes at most three steps to reach the optimal configuration.
Designing a scaling controller with SASO properties is non-trivial, and existing dynamic scaling techniques for stream processing do not achieve them.
Here, we summarize existing approaches, and then examine why they frequently lead to inaccurate, unstable, and slow scaling decisions, before proposing our solution.Many stream processors [45,8,40,27,4,43] have elastic runtimes and allow job reconfiguration by migrating or by externalizing state, but the majority relies entirely on manual intervention for both symptom detection and scaling actions.
Table 1 summarizes those systems that do provide some form of automatic scaling (for details also see [10]).
We categorize them by (i) metrics used for symptom detection, (ii) policy logic for deciding when to scale, (iii) type of scaling action which defines which operators to scale and by how much, and (iv) optimization objective (i.e. latency or throughput SLO).
We identify two areas in which current systems fall short of the controller properties we would like: first, the metrics used do not provide enough information to make fast and accurate decisions as to how to rescale the system, and second, the policies used for scaling (and the models they are based on) are often simplistic and rule-based.
Limited metrics: Most systems rely on coarse-grained externally observed metrics to detect suboptimal scaling: CPU utilization, throughput, queue sizes, etc.CPU and memory utilization can be inadequate metrics for streaming applications, particularly in cloud environments due to multi-tenancy and performance interference [38].
StreamCloud [17] and Seep [12] try to mitigate the problem by separating user time and system time, but preemption can make these metrics misleading: high CPU usage by a task running on the same physical machine as a dataflow operator can trigger incorrect scaleups (false positives) or prevent correct scale-downs (false negatives), for example.
Google Cloud Dataflow [26] uses CPU utilization only for scale-down decisions but could still suffer from false negatives.
CPU usage is also unsuitable for systems like Timely [32,33], where operators spin waiting for input.These metrics also imply continuous threshold tuning, a cumbersome and error-prone process.
Incorrect scaling decisions can often arise from slightly misconfigured thresholds, even on fine-grained metrics [13].
Dhalion [13] and IBM Streams [15] also use backpressure and congestion to identify bottlenecks.
These signals are only helpful where a bottleneck exists.
If the dataflow is using resources unnecessarily, such metrics will not trigger reconfiguration.
Moreover, in under-provisioned dataflows, backpressure will only detect a single bottle- neck; for this reason and to minimize the effects of incorrect decisions [39,13], each scaling action only configures one operator, increasing convergence time.Simplistic performance models: scaling policy is generally expressed in simple rules, using predefined thresholds and conditions, e.g. CPU utilization > 50 and backpressure =⇒ scale up.
This results in a simple performance model with poor predictive accuracy, which is unable to consider the structure of the dataflow graph or computational dependencies among operators.
We note the exceptions of Nephele [29] and DRS [14], which use queuing theory models.
Both systems show poor prediction quality in some cases, while Nephele also seems to suffer from temporary over-provisioning and slow convergence.Since the controller cannot accurately estimate how much to scale an operator, scaling actions are mostly speculative.
The system applies pessimistic strategies which introduce only small changes to the number of provisioned resources [12,15] and most policies configure a single operator at a time.
This delays convergence to a steady state significantly, as all steps of the scaling process are repeated many times: SLO monitoring, decision making, state migration, and redeployment.
[13] shows that, from the point that backpressure is observed, Heron needs almost an hour to reach a steady state that can handle the input rate.More aggressive strategies apply configurations, blacklisting them if they degrade performance.
[39] allows arbitrary scaling steps but requires a user-defined function to calculate the new number of instances whereas [2] supports exponential increase in resources.
StreamCloud [17] tries to estimate the optimal number of VMs in a single step, but using very coarse-grained scaling (a subgraph of the dataflow topology).
Google Cloud Dataflow is the only system we know with fully automatic scaling per operator, although the details of the model used have not been disclosed.A better approach: stepping back, it seems a more promising approach for making scaling decisions would take into account both (i) each operator's true processing and output capabilities, regardless of backpressure or other effects, and (ii) the dataflow topology and how scaling each operator will affect downstream operators.
Figure 2 gives an intuition of how this works showing the execution timelines of operator instances in a simple dataflow.
Solid lines show useful work performed by an instance (e.g. record processing) while dotted lines show it waiting for input or output.
Edges across timelines represent data transfer.In this example, o 1 is a bottleneck slowing down both the source and o 2 by pausing their execution.
Backpressure means that an external observer sees o 1 processing 10 rec/s and o 2 processing 100 rec/s.
Based on this, a policy might provision three additional instances for o 1 to reach a target of 40 rec/s, but it could not accurately estimate how much to scale o 2 and would need to make a speculative decision or apply an extra reconfiguration step.A better approach would measure the useful time of an operator's timeline and would determine the true rate of o 1 as 10 rec/s and that of o 2 as 200 rec/s, inferring that when increasing the parallelism of o 1 to 4, it also needs to double the parallelism of o 2 to keep up with the output rate.
Note this can be calculated globally, i.e. for all operators in the dataflow, in a single step.DS2 does precisely this, obtaining rate measurements of each operator by lightweight instrumentation (already present in many streaming systems).
In the rest of the paper we define this notion, extend it to more complex dataflow graphs with multiple sources, and show how DS2 implements it to provide fast, accurate, and stable reconfiguration of streaming dataflows.
DS2 identifies the optimal level of parallelism for each operator in the dataflow on the fly, while the computation executes, based on real-time performance traces.
It maintains a changing provisioning plan, i.e. the number of resources allocated to each operator.
It therefore works online and in a reactive setting.Note that we do not target offline computation of an initial resource provisioning plan (as in [7]).
Such initial configurations quickly become sub-optimal in a live system where workloads and/or internal operator states change continuously.
However, for static workloads known a priori, DS2 could use historical performance metrics and offline micro-benchmarks (as in [20,21,16]) to estimate the optimal levels of parallelism before deployment.In this section we define the scaling problem ( § 3.1), describe the DS2 model ( § 3.2), and discuss the model assumptions ( § 3.3) and properties ( § 3.4).
We target distributed streaming dataflow systems like Flink [9] and Heron [27] that execute data-parallel computations on shared-nothing clusters.
Such a computation can be represented as a logical directed acyclic graph G = (V, E), where vertices in V denote operators and edges in E are data dependencies between them.
A vertex with no incoming edges (no upstream operators) is a source and a vertex with no outgoing edges (no downstream operators) is a sink.A dataflow computation runs as a physical execution plan which maps dataflow operators to provisioned compute resources (or workers).
Let the graph G = (V , E ) represent the execution plan.
Vertices in V are operator (or task) instances of a corresponding vertex in V and edges are data channels.
The assignment of tasks to workers is system-specific.
We show in § 5 that DS2's scaling policy is independent of this assignment.
Figure 3 illustrates a logical graph and its corresponding physical graph for a dataflow with a source, a sink, and three operators.
Operators o1, o2 and o3 execute with two, one and three instances, respectively.The Scaling Problem.
Given a logical dataflow with sources s 1 , s 2 , ..., s n and rates λ 1 , λ 1 , ..., λ n , identify the minimum parallelism π i per operator such that the physical dataflow can sustain all source rates.Source operators generate records at a rate λ s , defined by application data sources (sensors, stock market feeds, etc.).
To maximize system throughput, the execution plan must sustain the full source rate.
This means that each operator must be able to process data without stalling its upstream operators from producing output.Like any controller, DS2 targets workload changes on a timescale greater than its convergence time, and reacting to spikes or other changes on a shorter timescale than the convergence time would cause inefficient fluctuations.
In these latter cases, the use of backpressure, buffering, or load shedding leads to more stable results than dynamic scaling at the cost of increased latency or lost data.
We consider operator instances as repeatedly performing three activities in sequence: deserialization, processing, and serialization.
This fits all types of operators in most modern streaming dataflow systems, including Heron, Flink, and Timely.
When an operator instance is scheduled for execution, it pulls records from its input, deserializes them, applies its processing logic, and serializes the results (if any), which are pushed to the output.
Serialization and deserialization are optional and happen only when data is moved between operator instances executed within different OS processes, otherwise data is usually exchanged via shared memory (e.g. queues).
The model is based on the concept of useful time, which we define for an operator instance as follows:Useful Time.
The time spent by an operator instance in deserialization, processing, and serialization activities.Useful time excludes time spent waiting on input or output.
Such waiting does occur in practice, for different reasons depending on the design of the reference system.
In Flink, an operator instance may block on input when the input buffers are empty, or on output when there is no free space in the (bounded) output buffers.
In Timely, operator instances may continuously "spin" checking their input queues until new records appear.
In Heron, instances may be forced to wait due to a backpressure signal from a slow downstream operator.In all cases, the useful time amounts to the time an operator instance runs for if executed in an ideal setting where it never has to wait to obtain input or push output.
In general, useful time differs from the total observed time the instance needs to process and output records, and plays a key role in solving the problem of § 3.1.
Based on this distinction, we define the true processing and output rate of an operator instance as follows:True Rates.
The true processing (resp.
output) rate corresponds to how many records an operator instance can process (resp.
output) per unit of useful time.Intuitively, the true rates denote the capacity of the operator instance, i.e. the maximum processing and output rate the instance could sustain for the current workload.
In contrast, the observed rates are those measured by simply counting the number of records processed and output by the instance over a unit of elapsed time, which might include waiting.
More precisely:Observed Rates.
The observed processing (resp.
output) rate corresponds to how many records an operator instance processes (resp.
outputs) per unit of observed time.Although the observed rates are more sensitive to changing workloads, due to the potential change in waiting time, true rates typically have lower variance, especially within short time periods (e.g. a few seconds of execution) as they represent the average "cost" to process and output a single record.
This cost naturally can depend on factors like the size of the record, its content, and the state maintained by the operator instance, but the average cost can be estimated using appropriate instrumentation of the operator without needing to saturate it.We define all rates in our model relative to windows of size W seconds of observed time.
We denote the useful time for an operator instance W u , where 0 ≤ W u ≤ W .
More precisely: where λ p and λ o are the true processing and output rate respectively (undefined when W u = 0), λ p and λ o are the observed processing and output rates (undefined when W = 0), and R prc (resp.
R psd ) is the total number of records the instance processed (resp.
pushed) in W .
For a specific operator instance and a window W , the following inequalities hold:λ p = R prc W u (1) λ o = R psd W u(2)λ p = R prc W (3) λ o = R psd W (4) Symbol Description G logical dataflow graph m number of operators in G (m > 1) n number of source operators in G (0 < n < m) W0 ≤ λ p ≤ λ p and 0 ≤ λ o ≤ λ o , since 0 ≤ W u ≤ W .
In general, the less an operator instance waits on its input and output the smaller the difference between the observed and true rates.
Table 2 summarizes the notation.We instantiate the model with (i) the logical dataflow graph G, (ii) the output rate of each data source, and (iii) the true processing and output rates (λ p and λ o ) of each operator instance.
G is static (known at compile time) and does not change during execution, since the logical dataflow is unaffected by the scaling decisions.
The output rates of the data sources are continuously monitored outside the reference system, and the true rates of the operator instances are computed based on system-generated traces, as we explain in § 4.1.
The output of DS2 is the optimal parallelism, i.e. number of instances, for each logical operator in the graph G, subject to the constraints of the problem in § 3.1.
The calculation proceeds as follows: let A be the adjacency matrix of G.
A i j = 1 iff the i-th operator outputs to the j-th operator, otherwise A i j = 0.
We consider operators numbered in topological order from i = 0 to i = m − 1, where m is the total number of operators in G.
This means that if o i outputs to o j and, hence, A i j = 1, then 0 ≤ i < j < m.
Since G is acyclic (cf. § 3.1),o i [λ p ] = k=p i ∑ k=1 λ k p (5) o i [λ o ] = k=p i ∑ k=1 λ k o(6)where λ k p and λ k o are the true processing and output rates of the k-th instance of o i , as given by Eq.
1 and Eq.
2.
The optimal level of parallelism π i for an operator o i is now computed using the ratio of the aggregated true output rate of its upstream operators (when they keep up with their inputs) to the average true processing rate per instance of o i .
More formally:π i = ∑ ∀ j: j<i A ji · o j [λ o ] * · o i [λ p ] p i −1 , n ≤ i < m (7)where m is the total number of operators in G, and n is the number of source operators in G, 0 < n < m. o j [λ o ] * denotes the aggregated true output rate of an operator o j , when o j itself and all operators before it (in topological order) are deployed with their optimal parallelism to keep up with their inputs.
It is recursively computed as follows:o j [λ o ] * =          o j [λ o ] = λ j src , 0 ≤ j < n o j [λ o ] o j [λ p ] · ∑ ∀u:u< j A u j · o u [λ o ] * , n ≤ j < m(8)where λ j src is the output rate of the j-th source operator, 0 ≤ j < n.Note that o j [λ o ] * depends on (i) the ratio o j [λ o ] o j [λ p ], which denotes the selectivity of o j , and (ii) the estimated true output rate of the upstream operators (∀u : u < j in the summation).
The latter implies that o j [λ o ] * and, hence, π i can be efficiently computed for all operators in the dataflow with a single traversal of G, starting from the sources.
This property is important in practice, as it allows us to estimate the required number of instances for all operators in the dataflow in the same scaling decision.
DS2 makes the following assumptions about the dataflow system it is controlling:Data-parallel operators.
An operator's output can be produced by partitioning its input on a key and applying the operator logic separately to each partition.
Other than this, the operator's internal logic can be any user-defined function.
Data-parallelism is essential for effective scaling decisions: executing multiple operator instances entails partitioning its state into chunks of data processed in parallel.
In contrast, non-data-parallel operators do not benefit from scaling.
System users could tag such operators for DS2 to ignore, or their lack of parallelism could be identified online by comparing input and output rates before and after scaling.
As with existing systems, we leave the integration of such operators for future work.No data or computation imbalance.
Our scaling model addresses neither data skew across operator instances nor computational stragglers.
Both these types of imbalance can trigger backpressure which cannot be tackled by changing the degree of parallelism of one or more operators.
Several robust solutions to the skew and straggler problems exist and have been incorporated into real systems.
Techniques such as partial key grouping [35] introduced in Storm [34] and further evaluated in [25], and work-stealing for straggler mitigation in MapReduce [28] and Google Dataflow [26] are complementary to DS2.
In § 4.2 we describe how DS2 could be integrated in a general controller for streaming applications which would not only handle dynamic scaling but also include skew and straggler handling components.Stable workloads during scaling.
Like existing scaling mechanisms, DS2 operates with the understanding that workload characteristics remain stable between a scaling decision being made and the new parallelism configuration being deployed.
This window is the time taken for DS2 to make a decision (which we evaluate in § 5) plus the time to deploy the new configuration, which depends on the dataflow system in use.
In practice, we find this timescale is dominated by the latter in current systems.
DS2 estimates the optimal parallelism for each operator assuming perfect scaling, that is, the true processing and output rates change linearly with the number of instances.
In general, however, true rates are described by non-linear, most commonly sub-linear functions.
Superlinear speedups are possible [16] (e.g. when state fits in cache after a scale-up) but are rare in practice.
When this "perfect scaling" assumption holds, DS2 estimations (Eq.
7) correspond to bounds and the model enjoys the following two properties: Property 1.
No overshoot: a scale-up decision will not result in over-provisioning.
The estimated optimal number of instances π i for an under-provisioned operator is always less than or equal to the minimum required to keep up with the target rate Consider an operator initially configured with parallelism p and aggregated processing rate λ < r t , where r t is the target rate, as shown in Figure 4a (left).
Assuming linear scaling, our model assigns π instances to reach the target rate r t .
Property 1 states that there exists no π < π such that π matches r t .
Indeed such a π can only exist in W next if the aggregated processing rate scales super-linearly, as shown in Figure 4a (right).
r t = ∑ ∀ j: j<i A ji · o j [λ o ] * in Eq.
7.
Similarly, if an operator is initially configured with parallelism p and aggregated processing rate λ > r t , as in Figure 4b (left), our model assigns π < p instances to scale down to r t .
Property 2 states that there exists no π > π such that π matches the target r t .
As shown in Figure 4b (right), such a π would violate the assumption of non-superlinear aggregated true processing rate.Together, these properties imply that repetitive applications of DS2 do not oscillate: they will monotonically converge to the target rate from below or above, ensuring stability without the need to blacklist previous decisions, and simplifying the scaling mechanism significantly.When true rates are linear and the target rate r t is accurately estimated for each operator, DS2 converges in at most one step.
When one of these two conditions does not hold, for example, true rates do not scale well due to other overheads (e.g. worker coordination) or dataflow operators have data-dependent output rates, DS2 needs more steps to converge to a stable configuration.
In each of these steps, DS2 tries to minimize the error of its previous decision to get closer to the target, as any typical controller does.
We omit the details of this process here and we only show empirically (in § 5.4) that DS2 needs at most three steps to converge in all our experiments.
Further reducing the number of steps requires good approximation of non-linear rates, which could be gradually learned by DS2 using machine learning techniques, opening an interesting direction for future work.
The DS2 controller consists of about 1500 lines of Rust running as a standalone process.
Here we describe the instrumentation requirements it imposes and discuss the issues encountered integrating it with three different stream processing engines: Flink, Timely dataflow, and Heron.
DS2 requires a subset of the instrumentation required by bottleneck detection tools for stream processors like SnailTrail [23].
The stream processor must periodically collect and report records processed, records produced, and useful time (serialization, deserialization, processing) or waiting time per operator instance.Flink gathers some of the metrics required by DS2 (e.g. records read and produced) by default but we extended its runtime so that each operator instance maintains local counters for (de)serialization and processing duration as well as for buffer wait time, reporting them to DS2 in configurable intervals.
For record-at-a-time systems like Flink, tracking and emitting metrics for every record might incur significant overhead.
Instead, we aggregate measurements per input buffer for all operators, except for sources where we aggregate per output buffer.
Specifically, we have implemented a MetricsManager module which is responsible for gathering, aggregating, and reporting policy metrics.
We assign one MetricsManager instance per parallel thread executing operator logic.
Each 13th USENIX Symposium on Operating Systems Design and Implementation 789 thread maintains local counters for records read, records produced, (de)serialization duration, processing duration, and waiting for input and output buffers.
Source operator instances send their current local counters to the MetricsManager every time an output buffer gets full and regular operator instances send their local counters every time they receive a new input buffer for processing.
The MetricsManager maintains a data structure with the current aggregate metrics of its operator instance and reports them to the outside world in configurable intervals.Timely [32] outputs raw tracing information, which we aggregate in configurable intervals to produce metrics for DS2.
We use a similar MetricsManager, as in Flink, which receives streams of logged events coming from Timely workers and aggregates them on the fly.
Each Timely worker logs individual events of different types, such as scheduling an operator or sending a message over a data channel, along with their timestamp in nanoseconds.
Recall that operator instances in Timely are not blocked on their input or output queues; instead, they are continuously spinning, i.e. they are scheduled for execution (in a round-robin fashion) even if there are no data records to process.
Spinning results in a huge amount of scheduling event logs, which quickly saturate the MetricsManager, although most of these logs are not needed for computing the true rates.
To tackle this problem, we modified Timely's logger to trace and send to the MetricsManager only the "useful" scheduling events, i.e. those that correspond to an operator instance doing some "useful work" for the actual computation.Heron also by default outputs detailed, aggregated metrics [22], which are periodically collected and fed into DS2.
The aggregation window depends on how frequently Heron samples its metrics and can be configured.
DS2 is mechanism-agnostic and can be integrated with any stream processor capable of dynamically varying resources and migrating state.
Figure 5 shows the high-level architecture of such an integration.
Instrumented streaming jobs periodically report metrics to a repository.
DS2 consists of a Scaling Policy component implementing the model of § 3.2, and a Scaling Manager monitoring the repository, invoking the policy when new metrics are available, and sending scaling commands to the stream processor.
While DS2 currently only offers scaling functionality, it could be easily extended with skew and straggler mitigation techniques as shown in Figure 5.
In this case, the system would consist of multi-purpose Manager and Policy components, where the first detects the problem type (e.g., presence of skew) and the latter invokes the appro- priate policy.
Note that DS2 collects metrics from each operator instance separately, thus skew detection can be effortlessly implemented by the Manager.
We have integrated DS2 with Apache Flink, which employs a simple scaling mechanism: when instructed, Flink takes a savepoint, a consistent snapshot of the job state, halts the computation, and redeploys it with the updated parallelism [24].
We demonstrate this integration in action and evaluate it under a dynamic source rate in § 5.3.
Operational issues in real deployments that are not captured by the model must be handled by the implementation instead.
To deal with factors that might affect scaling decisions in practice, the Scaling Manager provides the following configuration parameters:Policy interval defines the frequency with which metrics are gathered and the policy invoked.
Tuning the policy interval allows the scaling manager to aggregate metrics meaningfully, e.g. to ensure enough data is available to compute averages for processing and output rates.
Long intervals give stable metrics but also increase reaction time.
The interval must also be tuned based on the reconfiguration mechanism of the reference system.
In our experiments, we found 5-30s intervals reasonable for Flink and Timely.
For Heron, we found the default 60s suitable.Warm-up time is the number of consecutive policy intervals ignored after a scaling action, since rate measurements can be unstable at the start of a computation or before backpressure builds up.Activation time specifies when DS2 applies a scaling decision, as the number of consecutive policy decisions considered by the scaling manager before issuing a scaling command.
Activation time plus an appropriate policy interval mitigates the effects of irregularities in some streaming computations, such as non-incremental tumbling windows or data-dependent operators.
For instance, consider naively-implemented window operators that buffer records and only apply the computation logic after the window fires.
As long as input is simply assigned to a window, the operator's processing rate will appear high but once the window fires and the actual computation is performed the processing rate will suddenly drop.
DS2 can consider several consecutive policy decisions and, for example, compute the maximum or median parallelism across intervals before applying a scaling action.Target rate ratio defines a maximum allowed difference between the observed source rate achieved by the policy and the target rate, addressing the practical issue that processing and output rates might be affected by overheads not captured by instrumentation.
For instance, adding workers to a distributed computation might incur higher coordination, channel selection cost, or resource contention, and so a computation might need more resources to achieve the target rate than the policy indicates.
DS2 estimates the additional resources required by computing the ratio between the currently achieved rate and the target rate.
DS2 also ignores minor changes (e.g. changing an operator's parallelism by one or two), which can be triggered by noisy metrics.
External disruptions, such as garbagecollection in Java-based systems or disk I/O, can also influence rates measurements.
For example, when integrating DS2 with Flink, we took care to properly configure task managers, heap memory, and network buffers.
We are also aware that system performance might degrade after a scaling action (though we have not observed this in practice).
If this were to happen, DS2 rolls back to the previous configuration.
Similarly, consecutive decisions resulting in very small improvements indicate a performance issue (e.g. data skew, stragglers) that cannot be improved by scaling.
DS2 can limit the number of decisions to prevent further reconfiguration.
Even though the scaling model assumes no data imbalance and the current implementation of DS2 does not offer skew mitigation functionality, it is worth discussing how the system behaves if skew actually appears in a streaming application it is controlling.
In such a case, the system makes a scaling decision assuming data balance ( § 3.3) by averaging true processing and output rates.
Thus, DS2 proposes a configuration which might not meet the target throughput but at the same time will not overprovision the system.
Further, due to DS2's ability to limit the number of decisions ( § 4.2.2), the policy is guaranteed to converge.
We have verified the above behavior experimentally on Flink varying the skew parameter in the Dhalion benchmark from 20% to 50% and 70%.
In all cases, DS2 converged after two steps to the configuration which would be optimal if there was no skew, but which in this experiment did not meet the target throughput.
DS2's policy can be applied on streaming systems regardless of their execution model.
In Flink and Heron each dataflow operator is assigned a number of worker threads that define its level of parallelism, i.e. the number of parallel instances executing the operator's logic.
In this case, Eq.
7 can be directly used to configure operator parallelism independently.
In Timely, on the other hand, parallelism is configured globally for the whole dataflow.
Each worker runs every operator in the dataflow graph according to a round-robin scheduling strategy.For Timely, DS2 estimates the optimal number of total workers by summing up the optimal level of parallelism, as given by Eq.
7, for all operators in the dataflow.
The intuition here is simple: an operator that needs π i instances to keep up with its input actually needs π i · 100% computing power per unit of time.
In an execution model like Timely's where operators share computing resources (worker threads), the total computing power needed so that the system can keep up with its input is ∑ ∀i π i · 100%.
We experimentally validate the accuracy of DS2 decisions on Timely in § 5.5.
Our evaluation covers DS2 in use with three different streaming systems: Heron, Flink, and Timely Dataflow.
We start our evaluation by comparing DS2 with the stateof-the-art Dhalion scaling controller used in Heron, with the benchmark in the original Dhalion publication [13].
We then demonstrate DS2 in action through end-to-end, dynamic scaling experiments with Flink, followed by measurements of DS2 convergence and accuracy in using both Flink and Timely.
Finally, we evaluate the overhead of the instrumentation used by DS2.
We run all Flink and Timely experiments on up to four machines, each with 16 Intel Xeon E5-2650 @2.00GHz cores and 64GB of RAM, running Debian GNU/Linux 9.4.
We use Apache Flink 1.4.1 configured with 12 TaskManagers, each with 3 slots (maximum parallelism To demonstrate generality across diverse computations and streaming operators, we selected six queries from the Nexmark benchmarking suite of Apache Beam [42,36,37].
Specifically, we test the policy with Queries 1-3, 5, 8, and 11, which contain various representative streaming operators: stateless streaming transformations, i.e. map and filter in Q1 and Q2 respectively, a stateful record-ata-time two-input operator (incremental join) in Q3, and various window operators: sliding window in Q5, tumbling window join in Q8, and session window in Q11.
These queries specify computations both in processing and event time domains [5].
For the comparison with Dhalion ( § 5.2) and the end-to-end experiment on Flink ( § 5.3), we use the wordcount dataflow as specified in Dhalion's paper [13].
We compare the accuracy and convergence steps of DS2 with Dhalion, recreating the benchmark in [13].
We run Heron with Dhalion and its dynamic resource allocation policy enabled.
The source operator of the three-stage wordcount topology (Source, FlatMap, Count) produces sentences at a fixed rate of 1M per minute.
The FlatMap and Count operators are rate-limited to simulate bottlenecks: each FlatMap instance splits at most 100K sentences per minute, and each Count instance counts up to 1M words per minute (the same ratios as in the Dhalion paper).
We start under-provisioned with one instance per operator and let Heron stabilize without backpressure.
We have already seen how the source rate evolves to match the target throughput in this experiment in Figure 1.
Figure 6 shows the parallelism of FlatMap and Count over time, from the start until convergence.
Dhalion makes six scale-up decisions (each involving a single operator) and reaches a stable configuration with 22 FlatMap instances and 30 Count instances after 2000 seconds.We then apply DS2 on the same initial underprovisioned configuration using a 60s decision interval, no warm-up, one interval activation time, and 1.0 target ratio (cf. § 4).
DS2 indicates a required parallelism of 10 for FlatMap and 20 for Count, which indeed is the minimum configuration that handles 1M sentences per minute.
Note that DS2 correctly estimates the optimal parallelism in a single step, after only one minute of collecting the default Heron performance metrics.Dhalion requires several re-configuration steps, each affecting a single operator, and reaches a final configuration that is significantly over-provisioned, even in this simple wordcount dataflow.
In contrast, DS2 correctly identifies the optimal configuration in a single step and two orders of magnitude less time than Dhalion.Besides those discussed in § 2, another reason Dhalion takes so long to reach a backpressure-free configuration is that its reaction time depends on the size of the operator queues.
By default, Heron has a 100MiB buffer per operator queue, which may take some time to fill (depending on the workload) before backpressure kicks in and Dhalion can react.
In contrast, DS2 only depends on the decision interval where metrics are aggregated, arbitrarily specified by the user and typically much smaller.
We now show DS2 driving Apache Flink, in order to demonstrate the benefits of DS2 when combined with Persons Flink Timely Flink Timely Flink Timely Q1 4M 5M - - Q2 4M 5M - - Q3 - 500K 3M 100K 800K Q5 500K 2M - - Q8 - 420K 4M 120K 4M Q11 1M 9M -- a fast re-configuration mechanism such as that in Flink.Here, DS2 uses a 10s decision interval, 30s warmup time, one interval activation time, and 1.0 target ratio.
DS2 hence ignores the first three decisions after reconfiguration, applying a decision immediately after.We use the same wordcount dataflow as before, this time with two phases corresponding to scale-up and scale-down scenarios respectively.
In phase 1, the source rate is 2M sentences per second and Flink starts underprovisioned with 10 FlatMap instances and 5 Count instances.
In this state, FlatMap can not keep up with the source rate, neither can Count handle FlatMap's output rate.
Once Flink has reached a backpressure-free configuration, we keep the source rate stable for 10 minutes.
During the second phase, we decrease the source rate to 1M sentences per second and keep it stable for another 10 minutes.
Figure 7 shows observed source rate and operator parallelism over time.
DS2 applies two scale-up actions.
First, at 40s it re-deploys the dataflow with 14 FlatMap instances and 7 Count instances.
This happens right after the warm-up and activation time, and Flink takes around 30s to snapshot state and restart from the savepoint [24].
At 150s DS2 acts again to increase FlatMap to 19 and Count to 11 instances.
This time Flink takes about 50s to redeploy the backpressure-free configuration at 200s.
At 803s (3s into the second phase) DS2 reacts to the reduced source rate by reducing the configuration to 7 FlatMap and 4 Count instances at 845s.
At 900s it makes a final decision to increase Count parallelism by one, and Flink successfully applies the change at 930s, reaching the new optimal configuration.This shows that DS2 plus an efficient re-configuration mechanism can offer robust dynamic scaling for streaming dataflows, allowing the reference system to react to changes in its workload in just a few seconds -significantly faster than any other systems we are aware of.
We now show DS2 convergence from both over-and under-provisioned states on more complex dataflows.
We use the same Flink configuration as before, and execute each query with fixed source rates (cf. Table 3) and initial configurations of varying parallelism.
We run each query-configuration combination for 5 minutes and evaluate DS2 with 30s decision interval, 30s warm-up time, 1.0 target ratio, and five intervals activation (i.e. we consider the policy to have converged if the decision is unchanged over 5 consecutive intervals).
Table 4 shows the indicated parallelism per decision step for the main operator of each query on Flink.
Note that queries Q3, Q5, Q8, and Q11 include many operators, but we show results for the main operator of each for simplicity.
DS2 converges in one step for simple queries and initial configurations close to optimal (e.g. Q1 with parallelism 12), and in at most three steps for complex queries and initial configurations far from optimal (e.g. Q5 with initial parallelism 8).
In all cases, DS2 takes at most three steps to converge.
It needed three steps in 3 experiments (with Q2, Q5, and Q11), two steps in 14 experiments, and a single step in 19 out of 36 total experiments.
We also ran the same queries using Timely Dataflow and the results were similar.This shows that DS2 provides two important SASO properties: stability and short settling time.Intuitively, one DS2 step moves close to optimal by estimating ideal linear scaling ( § 3.4).
For far-from-optimal initial configurations, the second step "refines" this decision with a more accurate measurement, and the third step compensates for uncaptured overheads.
We next show accuracy: DS2 converges to configurations that exhibit no backpressure (and thus keep up with the source rates) while minimizing resource usage.
In particular, we show that for a given dataflow, fixed input rate, and initial configuration, DS2 identifies the optimal parallelism regardless of whether the job is initially underor over-provisioned.
We further show that there exists no other backpressure-free configuration with lower parallelism than the one DS2 computes.
Finally, we show that this configuration gives low latency by minimizing waiting time per operator instance.We set source rates as in Table 3 and parallelism given by the convergence experiment.
Figure 8 plots observed source rates (top) and per-record latency (bottom) for the main operator of each Nexmark query on Flink with different configurations.
For queries with two sources (Q3 and Q8), we show results for the higher-rate source (results for the low-rate sources are similar).
In all cases, DS2 successfully identifies the lowest parallelism that can keep up with the source rate.
Further increasing the parallelism does not significantly improve latency and would waste resources, while lower parallelism would Table 3 with instrumentation disabled (vanilla) and enabled (instr) for both Flink (10a) and Timely (10b).
cause backpressure.Timely does not have a backpressure mechanism so data sources are never delayed and the observed source rates are always equal to the initial fixed rate (instead, queues grow when the system cannot keep up).
We therefore simply show CDFs of per-epoch latencies with different configurations for Timely.
Figure 9 shows these for Q3, Q5, and Q11; results are similar for other queries.
Each epoch in the CDFs corresponds to 1s of data, which must be processed in less than 1s.
The optimal parallelism indicated by DS2 is p = 4 in all queries, regardless of the starting configuration.
For Q3 (left) and Q11 (right), p = 4 is clearly the configuration that can keep up with the 1s target (vertical line in the plots) using minimum required resources.
For Q5, 18% of the epochs are above the target by up to 0.5s. Here, the larger percentage of epochs that cannot keep up is because of the window operator, which stashes data and then forwards it at certain time points.
This manifests as load spikes, which require additional resources for the system to keep up.
Longer decision intervals smooth out the spikes but tend to affect policy decisions towards higher optimal configurations, which is why DS2 indicated p = 4 (cf. § 4.2).
In summary, DS2 identified optimal configurations in all experiments and never overshot (provisioned more resources than needed), thereby exhibiting the remaining two SASO properties: accuracy and no overshoot.
Finally, we evaluate instrumentation overhead.
We run the Nexmark queries for 5 minutes with source rates from Table 3 and a 10s decision interval -the smallest we use in this paper, which results in the most frequently aggregated logs and has the highest potential overhead on the system performance.We measure per-record latency in Flink using its builtin metric and per-epoch latency in Timely using 1s eventtime epochs.
Figure 10 shows boxplots for both systems.
Individual columns show latency with logging completely off (vanilla) and instrumentation activated (instr).
Overheads are small: at most 13% on Flink (40ms absolute difference) and at most 20% on Timely (5ms absolute difference) across all queries.
Performance penalties are an acceptable trade-off for a good scaling policy, and could be further reduced with a larger decision interval and pre-aggregation of metrics.
Note that Heron incurs no overhead since it gathers the required metrics by default.
In this paper we have described and evaluated DS2, a novel automatic scaling controller for distributed streaming dataflows.
Unlike existing scaling approaches, which rely on coarse-grained metrics and simplistic models, DS2 leverages knowledge of the dataflow graph, the computational dependencies among operators, and estimates the operators' true processing and output rates.DS2 uses a general performance model that is mechanism-agnostic and broadly applicable to a range of streaming systems.
We have implemented DS2 atop different stream processing engines: Apache Flink, Timely Dataflow, and Apache Heron, and showed that it is capable of accurate scaling decisions with fast convergence, while incurring negligible instrumentation overheads.An interesting question for future work is what kind of scaling and adaptation mechanisms are a good match for a controller like DS2.
The efficiency of DS2's model means that responsiveness is often limited by the latency of the scaling mechanism of the stream processor (when it is not determined by the granularity of measurement).
All the stream processors we test against implement scaling actions by checkpointing the dataflow, redeploying, and restoring from the checkpoint.
A faster, more dynamic reconfiguration mechanism might allow DS2 to operate on shorter timescales than the tens of seconds it allows in current systems.We will release DS2 as open source, together with all code and data used to produce the results in this paper.
We thank Nicolas Hafner for his help with the Nexmark queries implementation on Timely.
We would also like to thank the anonymous OSDI reviewers for their insightful comments, and our shepherd Matei Zaharia for his guidance in improving the paper.
This work was partially supported by the Swiss National Science Foundation, a Google Research award, and a gift from VMware Research.
Vasiliki Kalavri is supported by an ETH Postdoctoral fellowship.
