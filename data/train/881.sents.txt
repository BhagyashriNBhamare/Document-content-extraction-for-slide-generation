As CPU cores become building blocks, we see a great expansion in the types of on-chip memory systems proposed for CMPs.
Unfortunately, designing the cache and protocol controllers to support these memory systems is complex, and their concurrency and latency characteristics significantly affect the performance of any CMP.
To address this problem, this paper presents a microarchitecture framework for cache and protocol controllers, which can aid in generating the RTL for new memory systems.
The framework consists of three pipelined engines-request-tracking, state-manipulation, and data movement-which are programmed to implement a higher-level memory model.
This approach simplifies the design and verification of CMP systems by decomposing the memory model into sequences of state and data manipulations.
Moreover, implementing the framework itself produces a polymorphic memory system.
To validate the approach, we implemented a scalable, flexible CMP in silicon.
The memory system was then programmed to support three disparate memory models-cache coherent shared memory, streams and transactional memory.
Measured overheads of this approach seem promising.
Our system generates controllers with performance overheads of less than 20% compared to an ideal controller with zero internal latency.
Even the overhead of directly implementing a fully programmable controller was modest.
While it did double the controller's area, the amortized effective area in the system grew by roughly 7%.
With the advance to the multi-core era and replication of processor cores on a single die, the surrounding "un-core" logic, such as cache, memory controllers, and network interfaces, is growing in importance.
In particular, implementing the necessary data sharing and communication protocols for multi-core processors involves handling a large amount of transient state that is not necessarily visible to the high-level protocol.
As a result, the design of controllers that implement such protocols is usually complex, because they are part of the large distributed on-chip memory system and must provide global guarantees on consistency, ordering and forward progress.
Moreover, since the system's programming model defines the data sharing and communication semantics and the realization of this model is often tailored to a specific system, the implementation differs from one system to another, preventing controllers from being reused.To address these issues, this paper proposes a microarchitectural framework for the design of on-chip memory systems and, in particular, protocol controllers.
This approach is based on breaking down the functionality of the on-chip memory system into a set of basic operations and providing the necessary means for combining and sequencing these operations.
The system is programmed to perform protocol actions in the memories and controllers by appropriately combining these basic operations.
Having such a framework in place provides multiple benefits: first, it reduces the design time for creating and implementing controllers for a specific protocol, by converting the hardware design problem into a software programming problem.
Programmed values in the memory system are converted into constants and propagated into the logic at synthesis time, facilitating generation of controllers for multiple protocols.
Second, it enables run-time alteration of the memory system behavior to fix or patch design errors after fabrication, even after deployment of the system, as well as enabling run-time tuning of the memory system performance.
Third, a direct implementation of the RTL for the whole framework along with the "program storage" effectively creates a flexible, polymorphic memory system that can support a broad class of memory models.
Last but not least, this framework provides a platform for developing future tools such as protocol checkers or optimizers for increasing the verification coverage and performance of the generated system.
framework as a configurable controller along with eight Tensilica [15] [16] processors to create a polymorphic CMP, which we fabricated using ST 90nm CMOS technology.
Before the chip was taped out, we validated the resulting RTL design (and associated memory system configurations) to ensure that the system would correctly implement three distinct memory models: cache-coherence shared memory, streaming and transactional memory.
The overhead for using this design approach was small.
For all three memory systems, performance with the generated protocol controller is within 20% of the performance of an idealized controller, where internal protocol operations were assumed to take place at zero latency.
The area overhead of directly implementing the flexible controller was modest, less than twice the area of a controller optimized at design time for a specific protocol.
While this overhead might seem large, the resulting protocol controller area was only 14% of the core area.The next section reviews some of the previous work on memory system design and programmable protocol controllers, focusing on the work relevant to Smart Memories project [8] that we build upon.
Section 3 then presents our memory system architecture, and Section 4 describes the overall organization and microarchitecture of the protocol controller that makes everything work.
Section 5 then maps a number of distinct memory models onto this architecture, and Section 6 evaluates the performance and area overhead of this approach to controller design.
All memory access protocols at the hardware implementation level perform a limited set of operations: they move data from one physical location to another; they associate and update state information with data that guides the data movement operations, and they preserve necessary ordering between different operations in order to conform to the high-level protocol properties.
Controllers, as the primary engines executing protocol actions in the memory system, are responsible for completing these actions, and have been extensively studied in the literature.
There have been many proposals for designing high-performance, low-latency protocol controllers, especially for coherence protocols, and several micro-architectural techniques for both hard-wired and programmable controllers have been developed [24] [25] [26].
Particularly, programmable protocol controllers have been the subject of extensive research and have successfully been implemented in many academic and industrial projects: FLASH [13], Typhoon [12], Impulse [18], S3.mp [19] and Alewife [20] are just a few examples.
While our philosophy toward programmable controllers is similar to this previous work, and we leverage some of their approach (event driven execution, dispatch on message types, etc.), our goal is to create a programmable memory hierarchy all the way down to the processor's first level cache interface.
Hence protocol controllers in our system have to sustain a very high throughput and are very latency sensitive.
Therefore using a general-purpose processor for executing protocol actions (the approach of the MAGIC chip in FLASH, or the NP in Typhoon) would not be adequate.It is not surprising that commercial multi-core processors primarily use hardwired solutions for control of their on-chip memory resources, although these solutions often include the same kind of message driven execution seen in the programmable machines.
For instance, Niagara's memory controller uses one or two coded packets (a kind of microcode) sent across a crossbar to manage the transaction according the packet's code [29].
In addition, in many controllers complex operations are broken into many smaller operations.
In the controller for AMD's Opteron processor, a single read transaction generated du to a cache miss might result in thirteen messages from three different message classes: two Request messages, three Probe messages, and eight Response messages [28].
Similarly, the IBM Cell processor's Memory Flow Controller (MFC) transfers data to and from each compute element by way of a set of primitive commands [27].
Another old controller idea that we use is the notion of patchable microcode.
The use of microcode for patching and detecting design errors in the processor and memory system has also been proposed both in industry and academia and is widely used in processors [21] [22] [23].
Creating a hardware framework for the on-chip memory system that we can microcode, allows us to implement the protocols in software, which also allows to use software patches to fix memory system errors after deployment.
There has also been a lot of work in the literature in creating novel cache/local memory systems.
Discussions about message passing and shared memory protocols have given way to proposed new programming models such as streams and Transactional Memory (TM).
Streaming systems such as Imagine [3] and IBM Cell [4] share some characteristics with message passing machines, in that all communication is explicit, and also share some traits of shared memory machines, in that they generally have a shared address space and use high performance, low latency networking to connect the processors to each other and to the memory.
However, the total local memory is often small, so these machines implement their entire local memory in fast onchip SRAM, and forgo building a cache hierarchy entirely.
To fill and spill this local memory, implementations often contain sophisticated DMA engines that support gather/scatter operations as part of the memory hierarchy [3] [4].
On the cache coherent front, modern shared memory machines are moving to support a larger number of threads to help hide memory latency [1] [2], which requires the memory system to sustain and track several memory requests from different threads in order to tolerate long memory access latencies.
There has also been a lot of research in extending speculative execution techniques to the memory system.
Thread Level Speculative (TLS) systems such as Hydra [5] and Stampede [6] extend conventional memory systems with mechanisms to track and buffer results of speculative computations and to detect logical data dependencies between speculative threads.
Most recently Transactional Memory (TM) has generalized and formulated these systems into a transactional programming model [7] [10] [14].
There have been many proposals for implementing transactional systems, in hardware.
Since any implementation must either buffer the speculative values that are written, or the old values that have been overwritten by speculative data, the hardware must store a significant amount of information to track dependencies and it must also support inter-transaction communication (such as write-set broadcasting) to commit or flush data.
We use all of these memory models to demonstrate the capabilities of our memory control framework.
Our work on creating a protocol controller framework builds upon ideas from polymorphic computing, which tries to map different programming models to a malleable hardware substrate.
For example, the TRIPS polymorphic architecture can be configured to better leverage instruction, data or thread level parallelism [9].
The Stanford Smart Memories (SM) project took a different approach where they directly mapped a stream machine and a thread-level speculative machine on a reconfigurable hardware substrate [8].
We build on the SM work, which showed how storing a small number of state / meta-data bits and updating them on each access could maintain state information needed for a wide variety of memory models.
Unfortunately this work focused on the processor "Tile" and did not explain how to flexibly implement the different protocols that are needed for these different memory models.
The approach in this paper addresses this limitation, providing a method of constructing the needed protocol controllers.
Since our controllers assume local memories contain state similar to what was described in the Smart Memories system, we review this information next.As described in [8] the basic unit of the architecture is the Tile.
Each Tile contains two processor elements, 16 local memory blocks and a crossbar interconnect to connect memories to processors and the outside world ( Figure 1).
The memory mats are the basic storage element in the system and are connected through an Inter-Mat Communication Network (IMCN)-a fast path for exchanging memory control and state information.Memory mats are aggregated (using IMCN) to implement composite storage structures such as instruction and data caches ( Figure 1c).
Meta-data bits in the tag storage are used to encode state information according to the protocol, such as cache line state and LRU information in shared memory model, or a transaction's read and write sets in a TM model.
When implementing a streaming model, the memory mats are aggregated into addressable scratchpads.
Moreover, having head/tail pointers in the memory mats allows them to efficiently implement hardware FIFOs, which can be used to capture producer-consumer locality between processors.
It also can simplify some protocol/runtime operations.
For example, hardware FIFOs are used to augment cache structures in order to store addresses of a transaction's write set, which is used at transaction commit time for broadcasting address/data pairs [10].
The protocol controller described in the next section assumes that local memories have the needed meta-data bits to store protocol state and also have simple hardware that can modify the state, if needed, on each access.
We also assume that the local memory or processor can create a small number of request types when it needs help from the protocol controller.
In our attempt to create a design framework for memory systems, we associated meta-data with the local storage and decided to take a RISC-like approach for the protocol controller design: instead of providing complex pre-defined operations, we provided a small number of basic operations and implemented complex data and state manipulations by executing a set of these basic operations.
As was mentioned earlier, a general-purpose RISC processor would be too slow, but fortunately only a small number of primitive operations are needed to support all the models that we investigated, and these could be accomplished in a small number of programmable FSM/pipelined engines.
This approach works because across many different memory models the functions of all protocol controllers are very similar: at their core all protocol engines track and move data.
One can recognize such similarity at two levels: at the high level, many protocol actions that implement a memory model have the same conceptual functionality.
Table 1 lists a few of these actions, indicating which other protocol actions they resemble.
At a lower level, the hardware operations that are combined to form the protocol actions are also the same.
These primitive operations can be categorized into five different classes: Data/State read and write -Accessing data and state storages for performing data transfers, state inquiries and updates, according to the specific protocol action  Communication -Sending and receiving messages over available communication infrastructure Ordering -Guaranteeing a specific order between requests from same processor or different processors, according to the specific protocol or memory consistency model  Tracking -Keeping track of the outstanding requests in the system so that each request can be completed after corresponding reply is received.
This is also necessary for enforcing ordering between different requests  Association and interpretation of state information -This is the major differentiating factor among memory models; indicates how the state associated with data is interpreted and controls the flow of operation according to the specific protocol These operations are essentially the RISC instructions, the basic blocks, for composing protocol actions.
One can describe the activities occurring in the memory system hardware upon receiving any protocol request/reply message as a composition of the above operations in an appropriate sequence.
We implement these operations in two structures, the local memory hardware which is associated with each processor, and the protocol controller that connects a number of local memory hardware units to the network.
Having described the local memory earlier, we describe the protocol controller next.
In our framework, processors and main memory controllers communicate with the protocol controller by sending and receiving request/reply messages.
Each request message when received invokes a "subroutine" in the controller that executes a series of basic memory operations.
One creates a memory model by defining the set of messages that the protocol controller needs to handle, and then composing the required actions for each message from the basic operations described above.
number, which is used to detect conflicting requests and enforce serialization.
The Uncached-request Status Holding Registers (USHR), on the other hand, keep the tracking information of requests that do not require such serialization, for instance DMA transfers.
Tracking resources are sized according to the expected occupancy of the controller.
For example, one can determine appropriate sizes by simulating the controller with an infinite number of tracking registers and statistically determining the minimum required size for a given performance level.
The State Update (S-Unit) performs read, write and manipulation of the state information associated with data blocks, such as cache tags and cache line states.
It has a dedicated port to Tile memory mats in order to read and write data and meta-data bits.
The Data Movement Engine (D-Unit) provides necessary functions for reading and writing data blocks from memory mats into an internal data buffer (Line Buffer).
It also checks and updates meta-data bits that are associated with an individual data word.Communication primitives are implemented in processor and network interface units.
The Processor interface unit (P-Unit) receives and decodes request messages from all processors sharing this controller and passes them to the execution core.
The Network interface unit (N-Unit) consists of separate transmitter and receiver logic that composes and decodes messages that are communicated over the system network.
It has necessary interfaces to the line buffer to read and write blocks of data that are transmitted or received over the network.The controller is also equipped with independent DMA channels, which are programmable request generator engines.
Each channel is associated with a processor supported by the controller and is programmed by writes into control registers.
A dedicated interrupt unit (INT-Unit) can send individual interrupt requests to any processor in the group when necessary.
Processors can also generate interrupts for one another by writing into control registers in the INT-Unit.
The conceptual programming model of the controller is the execution of a set of subroutines triggered by an input message to the controller.
Each subroutine comprises a few basic operations and is executed by one of the internal functional units.
After executing the subroutine, each functional unit invokes another subroutine in the next functional unit(s) by passing an appropriate request type to it.
Subroutines are chained to one another until processing of the input message is completed.
Sequential execution semantic is maintained within each subroutine.
The request message invokes a coherence request subroutine in the network receive unit.
This subroutine calls the Read-Exclusive subroutine in the T-Unit.
After performing necessary serialization actions, this subroutine calls the Snoop subroutine in the S-Unit to snoop cache tags.
It is also possible for a functional unit to call two or more subroutines in different units concurrently to parallelize processing of different parts of the same request message.
Processing ends after the the N-Unit transmitter sends a coherence reply message to the main memory controller.
The functional units form a macro-level pipeline to process incoming messages.
Requests are passed from one functional unit to another after the executing subroutine completes its necessary steps and invokes the next subroutine.
Each of the functional units is organized as a shallow pipeline.
The pipelining allows the unit to overlap processing steps of different requests internally and further increases the throughput of the controller.The first stage of each unit's pipeline contains a configuration memory that holds the controlling microcode for the pipeline.
This memory is indexed by the type of the request passed to the unit and dictates all the operations that are performed in that unit.
At the output of the unit a shallow queue ensures that the pipeline can be completely drained and all the in-flight operations can be completed.
An arbiter located in front of each unit decides what request is passed into the unit at each clock cycle.
Figure 4 shows the S-Unit pipeline as an example.
Configuration memory is located at the first stage of the pipeline (Access Generator).
It generates necessary signals for accessing memory mats in the Tiles.
In addition to identifying mats that need to be accessed, these access signals individually control operations on the data and meta-data arrays, as well as the read-modify-write logic that allows the mats to update their own state bits.
Up to two mat accesses can be generated in the AG stage.
The S-Unit can send a mat access to a single Tile, all Tiles simultaneously, or can send one access to a specific Tile and another access to the remaining Tiles.
Two pipeline stages are used for accessing the memory mats which include the roundtrip time over the Tile crossbar.
At the end of the pipeline a lookup table serves as the decision-making logic.
It analyzes the information retrieved from memory mats and generates necessary requests for next functional unit.
Figure 5 shows the micro-architecture of the data movement engine.
It consists of four parallel data pipes, one connected to each Tile.
Each data pipe has its own input and output queues.
A dispatch unit decodes an incoming request and uses a lookup table to issue necessary operations to each data pipe.
For example, a block transfer request is converted into a block read operation on the source pipe and a block write operation on the destination pipe.
All data pipes are connected to the line buffer and data is staged through the line buffer between the two pipes.
A small FSM generates necessary replies for the processors if the data transfer was due to a processor's request.
Each data pipe has four stages and is very similar to the S-Unit: the Access Generator has a configuration memory that generates the necessary memory mat access signals.
Condition Check logic is a lookup table that compares the meta-data bits collected from the mats with predefined patterns and generates necessary subroutine calls for subsequent units.
Data pipes supports 32-bit (single mat) and 64-bit (double mat) accesses.
64-bit accesses use two adjacent memory mats for faster data transfers.
Deadlocks are avoided in the system by carefully considering a set of constraints.
First, the controller provides some deadlock avoidance guarantees in the hardware.
The output queue at the final stage of each functional unit guarantees that the in-flight requests in that pipeline can be completed without needing to stall the functional unit.
The arbiter in front of each unit considers necessary buffering space in the output queue and throttles the input requests if there is no buffering space available.The network interconnect supports multiple virtual channels.
Priorities for processing virtual channel messages are adjustable at the network interfaces.
However, it is the designer's responsibility to distinguish between protocol request and reply messages [17] and assign them to different virtual channels.
Since the connection between the S-Unit and the D-Unit forms a loop inside the controller, there is a potential that the controller can live-lock, where subroutines in the S-Unit and D-Unit invoke each other repeatedly.
Once again it is the responsibility of the protocol designer to ensure that the controller is programmed such that all possible subroutine chains eventually terminate.
Live-lock avoidance can be guaranteed by simply forbidding subroutine foo in, for example, the D-Unit from calling subroutine bar in the SUnit that might possibly invoke foo.
In the controllers we have implemented, this has not been a difficult constraint to satisfy.
An overview of these protocols is described in the next section.
When implementing a memory protocol, operations are divided between different parts of memory system, namely processor interface logic, local memory mats, protocol controller and main memory controller.
These components communicate by sending and receiving messages.
Table 2 lists and describes all the messages exchanged between protocol controller and processors as well as main memory controller for our three different memory models.
In this section we demonstrate how the protocol controller is programmed to handle a few of these messages.
Figure 6 shows an example of an indexed DMA scatter operation to main memory.
First, the DMA channel issues an index read operation that returns the address of the destination memory to it.
After identifying the destination, the DMA channel generates line-size requests for transferring a single data element (source address, element size and number of elements are programmed by writing DMA control registers).
The T-Unit allocates a tracking register and stores tracking information for the operation and passes it to the D-Unit.
The D-Unit extracts the appropriate memory block from the source memory into the line buffer entry and requests the N-Unit to send scatter message to main memory.
The N-Unit reads the data from the line buffer entry and sends it over the network to the main memory controller.
After data is written to the destination address, a scatter reply message confirms completion of the operation.
This message is decoded by the network interface unit and is passed to the T-Unit.
The T-Unit retrieves the tracking information, releases the tracking register and sends an acknowledgement to the originating DMA channel.
Figure 7 shows another example of servicing a cache miss request from processor.
A cache miss is received and decoded by the processor interface unit and is then passed to the tracking unit.
The T-Unit looks up the tracking information of outstanding cache misses to serialize the cache miss properly against prior requests.
If no collision is found, it allocates an MSHR entry and saves the tracking information of the cache miss.
The cache miss is then passed to the S-Unit, which evicts a cache line in the source cache to open up space for the cache refill.
The S-Unit simultaneously snoops the tags of caches in other Tiles to enforce coherence properties and to consider the possibility of a cache-tocache transfer.
The Data movement engine performs the necessary writeback or cache-to-cache transfer based on the decision made in the S-Unit.
The Network interface sends the write back request to main memory controller as well as the cache miss request.
Refills are handled similarly.
We evaluated the performance overhead of our protocol controller framework by simulating our reconfigurable CMP, which directly implemented it.
The processor simulator and software tool chain were supplied by Tensilica [15] [16] while the memory system and interconnect simulator was developed using the Xtensa Modeling Protocol (XTMP).
In order to evaluate the performance impact of the framework, we back-annotated the memory system simulator with latency numbers extracted from the actual system RTL, and compared the results to simulations where all internal controller operations took zero cycles, but external operation timing (e.g. mat read) remained unchanged.
For correctness checking, the RTL was extensively checked using both applications and random stress cases as part of the tape-out flow.
Table 3 lists the benchmark applications for three different models that we used to evaluate system performance.
Table 4 describes system parameters used for performance simulations.
Figure 8 shows speedups for three different memory models.
For almost all benchmarks, our system shows good performance scaling, achieving at least 50% parallel efficiency.
In cache Table 4.
System parameters used for simulation coherent mode, speedups range from 18 to 27 for a 32-processor configuration; in streaming mode, speedups range from 18 to 26.
The exception is mp3d in transactional mode, which does not scale beyond 8 parallel processors.
The reason for this is frequent accesses to shared data structures, which cause transaction dependency violations and transaction re-execution.
On the other hand, in a cache-coherent version of mp3d, these accesses are not protected by locks and as a result they cause data races.
Since mp3d performs randomized computation and reports results only after statistical averaging of many steps of computation, the data races should not alter the results significantly.
The reason for this uncommon programming decision is performance: in conventional cache-coherent architectures, fine-grain locking is expensive.To estimate the performance overhead of reconfigurability, we repeated the same set of simulations on a "zero latency" model, where internal protocol operations take zero cycles, and calculated the difference with the real case.
Note that this difference is an upper bound for the overhead estimate, because in any realistic fixed function design the protocol controller latency cannot be zero.
Figure 9 illustrates the performance scaling of benchmark applications for both real and "idealized" controllers, with Table 5 summarizing this information.
For each benchmark the overhead is averaged for system configurations ranging from 1 to 32 processors.
In most cases the difference is less than 20%.
The exception is the cache-coherent version of mp3d.
The reason for this is once again frequent accesses to shared data structures without locks which cause frequent data races and put significant stress on the memory system.
On the other hand, as one might expect, streaming applications are least sensitive to the controller latencies because of their latency tolerant nature.The 8-processor polymorphic CMP test chip parameters are summarized in Table 6.
The test chip contains 4 Tiles, each with 2 Tensilica processors, and a shared protocol controller.
The total chip area is 60.5 mm 2 , and the core area, which includes tiles and protocol controller, is 51.7 mm 2 ( Table 7).
To evaluate the hardware overhead of building a reconfigurable protocol controller rather than using it to generate the desired controller, we performed a series of simple experiments, in which we tailored the protocol controller to a specific memory protocol by converting all the internal configuration memories into constant values.
Our synthesis tool then removed the memories and propagated the constant values into the logic, eliminating unnecessary parts and creating an "instance" of the controller tailored to that specific memory protocol.
Figure 10 overhead, the percentage of the area consumed by the protocol controller is relatively small: around 12% of the test chip area and less than 14% of the core area (Table 7).
Thus, protocol programability area overhead is less than 7% of the total system area.
As we move towards CMPs with many replicated cores, designing the memory system and the associated communication interfaces and protocols becomes one of the most important and difficult microarchitecture tasks.
We provide a framework that helps address this problem.
By creating a standard set of hardware units with simple operations, we convert this hardware design problem to a software programming problem.
By defining the messages that each hardware unit must handle and the sequence of steps, the "subroutine," that needs to be run for each message, one completely defines the protocol's operation down to the RTL level.
If we are correct in that this framework allows one to create any memory model, its greatest strength will be moving memory design and verification conceptually up a level.
Instead of worrying about gates, a designer would only need to worry about the state that needs to be stored, and the operations that need to be executed.
The overhead of using this method appears modest.
Compared with a customized protocol controller with no internal delay the performance difference for most applications was less than 20%.
Somewhat surprisingly, our results indicate it is feasible to directly implement the programmable framework on silicon.
While that doubles the controller area, the controller consumes only 14% of the core area.
This work would have not been possible without support from DARPA, Tensilica and ST Microelectronics.
The authors also would like to thank Megan Wachs, Han Chen, Wajahat Qadeer, Rehan Hameed, Kyle Kelley, Francois Labonte, Jacob Chang and Don Stark for their help and support.
