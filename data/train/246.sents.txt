The paper is a collection of remarks, some rather plain, on various issues with factor graphs.
In particular , it is pointed out that powerful signal processing techniques such as gradient methods, Kalman filtering , and the particle filter can be used and combined in factor graphs.
Based on prior work by Wiberg et al. [1] [2], factor graphs were introduced in [3] and [4].
The main point of these papers was to show that many known algorithms in coding, artificial intelligence, and signal processing may be viewed as instances of the summary-product algorithm that operates by message passing in the factor graph.Factor graphs can of course also be used to develop new algorithms for particular applications, but the literature on such applications is still quite limited; examples include [5] and [6].
For the past two years, we have been applying factor graphs to a wide variety of practical problems in areas ranging from communications over biomedical signal processing to fire detection devices.
(Most of this is as yet unpublished, but see [7] and [8].)
It has become apparent to us that the factor graph notation is indeed extremely helpful for the development of practical algorithms.
A key issue in most such applications is the coexistence of discrete and continuous variables; another is the harmonic cooperation of a variety of different signal processing techniques.
Some remarks on these topics are given in the present paper.The paper is structured as follows.
In Section 2, we briefly review Forney-style factor graphs (called normal graphs in [9]), which have become our preferred notation.
Section 3 is a collection of remarks on some unrelated little issues that may be helpful to non-experts; none of this is new.
In Section 4, we address some general issues with continuous variables.
In Section 5, we outline the use of gradient methods for local message computations in factor graphs.
Some conclusions are offered in Section 6.
This paper is not an introduction to factor graphs; for such an introduction, see, e.g., [4] [9] [10] [11].
A Forney-style factor graph (FFG) is a diagram as in Figure 1 that represents the factorization of a function of several variables.
E.g., assume that some function f (u, w, x, y, z) can be factored asf (u, w, x, y, z) = f A (u, w, x)f B (x, y, z)f C (z).
(1)This factorization is expressed by the FFG shown in Figure 1.
In general, an FFG consists of nodes, edges, and "half edges" (which are connected only to one node), and there are the following rules:• There is a node for every factor.
• There is an edge (or half edge) for every variable.
• The node representing some factor g is connected with the edge (or half edge) representing some variable x if and only if x is an argument of g.Implicit in these rules is the assumption that no variable appears in more than two factors.
This restriction is easily circumvented, however.
For example, consider the factorizationf (x) = f A (x)f B (x)f C (x).
(2)We expand this intof (x) = f A (x)f B (x )f C (x )δ(x − x )δ(x − x ), (3) f A x = x f C f B xFigure 2: Cloning of variables.X - µ inX f Y ?
µ inY Z - µ outZ Figure 3: Message passing.which is represented by the FFG in Figure 2.
(The "δ" in (3) may be either a Kronecker delta or a Dirac delta, depending on the context.)
The free use of auxiliary variables (such as x and x in Figure 2) is typical for Forney-style factor graphs.Other than in Figures 1 and 2, we will usually denote the variables by capital letters and specific values ("realizations") of the variables with lowercase letters.For FFGs, the basic sum-product rule for the computation of messages has a particularly simple form.
For example, in Figure 3, the message out of node f along the edge Z isµ outZ (z) = x y f (x, y, z) µ inX (x) µ inY (y).
(4)We also recall that the sum in (4) may be replaced by other summary operators such as integration or maximization, cf. [4].
In any case, a message is a "summary" of the graph "behind" it.
Consider a situation as in Figure 4, where two codes share the coded symbols X k , k = 1, 2, 3, . . . In such cases, the correct handling of extrinsic information and intrinsic information is usually considered an issue that requires attention.
Not so with factor graphs: the correct handling of extrinsic information and intrinsic information is automatic.
Consider a situation as in Figure 5, where two binary symbols, X A and X B , are mapped to a 4-AM symbol Z. Let f : Z 2 × Z 2 → {−3, −1, +1, +3} be this mapping and assume that x A is mapped to the Code 1· · · · · · = = Code 2 · · · · · · X k X k+1 Channel . . . Y k Y k+1. . . - - f - - µ outX A - µ outX B - µ outZ - - f - µ inX A - µ outX B µ inZδ f (x A , x B , z) = 1, if f (x A , x B ) = z 0, else(5)The computation of all messages in and out of the node (cf. Figure 6) is immediate from the sumproduct rule (4).
For example, we haveµ inX A (x A ) = x B ,z δ f (x A , x B , z) µ outX B (x B ) µ inZ (z),(6)which expands toµ inX A (0) = µ outX B (1) · µ inZ (+3) + µ outX B (0) · µ inZ (+1) (7) µ inX A (1) = µ outX B (0) · µ inZ (−1) + µ outX B (1) · µ inZ (−3).
(8) Consider an equality constraint between a variable X that takes values in some finite set X and a r r r r r r r r ?
b ?
b ?
b ?
b r ?
r r ?
r r ?
r r ?
r Figure 8: Quantizer.real variable Y , cf. Figure 7.
Such an equality constraint translates into a factor δ(x − y), which should be read as a Kronecker delta in x and a Dirac delta in y.
According to the sum-product rule (4), the message out of the X-edge isµ outX (x) = y δ(x − y) µ inY (y)(9)= µ inY (x),(10)which amounts to sampling the incoming density µ inY at the elements of X .
The message out of the Y -edge isµ outY (y) = x∈X δ(y − x) µ inX (x),(11)a sum of weighted Dirac deltas.
Let X be a variable that takes on values in some finite set X .
Consider a quantizer q : X → Y : x → q(y).
The set Y of possible values of Y may be a finite subset of R, or it may consist of subsets (intervals) of R.
Such a quantizer may be present in the original system or it may have been introduced to make some message computations more tractable (cf. Section 3.5).
For the latter purpose, a quantizer as in Figure 8 may be attractive.
The messages through such a quantizer node are easily computed to beµ outY (y) = x:q(x)=y µ inX (x)(12)andµ outX (x) = µ inY (q(x)).
(13) Consider the problem of computing messages through a node that represents the constraint where, for > 0, the variables X take values in finite subsets X of R, and X 0 takes arbitrary values in R.
The literal application of the sum-product rule (4) yields sums with about m =1 |X | terms, which is infeasible for large alphabets and/or large m. Note that such a node may be decomposed as in Figure 9, but this decomposition does not, by itself, solve the complexity problem.In practical applications, however, the computations can usually be reduced to a manageable level.
One way to achieve this is to insert a quantizer (e.g., as in Figure 8) between the "small" sum constraint nodes in Figure 9.
By properly adjusting such quantizers to the noise level of the application, the performance loss can usually be kept negligible.In summary, sum constraints among finitealphabet real variables can usually be handled computationally.
One should not forget, though, that sum constraints among many variables dilute information: messages through such nodes tend to be uninformative.
For continous variables, literal application of the sum-product or max-product update rules often leads to intractable integrals.
Dealing with continuous variables thus involves the choice of suitable message types and of the corresponding (exact or approximate) update rules.
So far, the following message types have proved useful:Constant messages.
The message is a "harddecision" estimate of the variable.
This message type appears, e.g., if a decision-feedback equalizer is represented as a message passing algorithm.
Another example isˆθisˆ isˆθ in Section 5 below.Quantized messages are an obvious choice (cf. Section 3.5).
However, quantization is usually infeasible in higher dimensions.Mean and variance of (exact or assumed) Gaussian Messages.
This is the realm of Kalman filtering.
Kalman filtering as message passing in a factor graph was briefly treated in [4] and worked out in more detail in [10], see also [11] [12] [13].
The derivative of the message at a single point is the data type used for gradient methods, see Section 5.
List of samples.
A probability distribution can be represented by a list of samples from the distribution.
This data type is the basis of the particle filter [14]; its use for message passing algorithms in general graphs seems to be largely unexplored, but promising.Note that all these message types are consistent with the axiom that a message is a summary of everything "behind" the transmitting node.
With these message types, it is possible to integrate most good known signal processing techniques as local message computations in a factor graph.
In the next section, we outline such a translation for gradient methods.
The use of gradient methods for local message computations in factor graphs is illustrated in Fig Note thatd dθ log f (θ) = f (θ) f (θ) (17) = f A (θ)f B (θ) + f B (θ)f A (θ) f A (θ)f B (θ) (18) = d dθ log f B (θ) + d dθ log f A (θ) .
(19)Figure 10 may be a part of some bigger FFG.
In this case, the nodes f A and f B may be summaries of the graph "behind" them.
The functions f A and f B may be infeasible to represent, or to compute, in their entirety, but it may be easy to evaluate d dθ log f A (θ) (and likewise for f B ) at any given point θ.A general gradient method to find a solutionˆθsolutionˆ solutionˆθ of (16) operates as follows.1.
The equality constraint node in Figure 10 broadcasts some initial estimatê θ.
The node f A replies by sending Figure 10: On gradient methods.
and the node f B replies accordingly.d dθ log f A (θ) θ=ˆθθ=ˆ θ=ˆθ f A P P P P P P P P P P i ˆ θ P P q f B ) ˆ θ 1 = Θ - ˆ θ θ is computed asˆ θ new = ˆ θ old + s · d dθ log f (θ) θ=ˆθθ=ˆ θ=ˆθ old (20)where s ∈ R is a positive step-size parameter.3.
The procedure is iterated as one pleases.As always with message passing algorithms, there is much freedom in the scheduling of the individual operations.
It can be shown, for example, that the standard LMS algorithm may be obtained in this way from a suitable factor graph.
If second derivatives are also available, Newton-type methods can be used.
• "Turbo signal processing"-iterative message passing in a graphical model-allows to combine and to extend many of the best known algorithms for detection and estimation problems, including gradient methods, Kalman filtering, and particle filters.
Our accumulating experience confirms that the graphical framework helps to see such algorithmic options in practical problems.
• As factor graphs develop into a general tool for signal processing, there appear to be similar developments (with similar graphical models) in statistics, artificial intelligence, and neural networks, cf.[15] [13].
• Dealing with continuous variables involves many design choices, and this is a vast field of research.
Nevertheless, many good practical solutions are apparent already.
• Forney-style factor graphs provide an especially elegant notation.
