A distributed consensus algorithm allows n processes to reach a common decision value starting from individual inputs.
Wait-free consensus, in which a process always terminates within a finite number of its own steps, is impossible in an asynchronous shared-memory system.
However, consensus becomes solvable using randomization when a process only has to terminate with probability 1.
Randomized consensus algorithms are typically evaluated by their total step complexity, which is the expected total number of steps taken by all processes.
This work proves that the total step complexity of ran-domized consensus is Θ(n 2) in an asynchronous shared memory system using multi-writer multi-reader registers.
The bound is achieved by improving both the lower and the upper bounds for this problem.
In addition to improving upon the best previously known result by a factor of log 2 n, the lower bound features a greatly streamlined proof.
Both goals are achieved through restricting attention to a set of layered executions and using an isoperimetric inequality for analyzing their behavior.
The matching algorithm decreases the expected total step complexity by a log n factor, by leveraging the multi-writing capability of the shared registers.
Its correctness proof is facilitated by viewing each execution of the algorithm as a stochastic process and applying Kolmogorov's inequality.
Coordinating the actions of processes is crucial for virtually all distributed applications, especially in asynchronous systems.
At the core of many coordination problems is the need to reach consensus among processes, despite the possibility of process failures.
A (binary) consensus algorithm allows processes starting with input values in {0, 1} to agree on the same output value (agreement); to rule out trivial solutions, this common output must be one of the inputs (validity).
Consensus is a fundamental task in asynchronous systems, and can be employed to implement arbitrary concurrent objects [21]; consensus is also a key component of the state-machine approach for replicating services [22,28].
Perhaps the most celebrated result in distributed computing shows that no deterministic algorithm can achieve consensus in an asynchronous system, if one process may fail [20,21,23].
Due to the importance of the consensus problem, much research was invested in trying to circumvent this impossibility result.
One successful approach is to allow randomized algorithms in which a nonfaulty process terminates only with probability 1.
(The agreement and validity properties remain the same.)
Randomized consensus algorithms are typically evaluated by their total step complexity, which is the expected total number of steps taken by all processes.Many randomized consensus algorithms have been suggested, in different communication models and under various assumptions about the adversary (see [4]).
In particular, algorithms were designed to solve randomized consensus in asynchronous shared-memory systems, against a strong adversary that can observe the results of local coin flips before scheduling the processes [1,2,5,26].
The total step complexity of the best previously known algorithm is O(n 2 log n) [13].
A lower bound of Ω( n 2 log 2 n ) on the expected total number of coin flips was proved by Aspnes [3]; this implies the same lower bound on the total step complexity.Closing the gap of Θ(log 3 n) between the lower and the upper bounds and determining the total step complexity of randomized consensus remained an intriguing open question.In this paper, we prove that Θ(n 2 ) is a tight bound on the total step complexity of solving randomized consensus under the strong adversary in asynchronous shared memory systems, where processes communicate by reading and writing to multi-writer multi-reader registers.The Ω(n 2 ) lower bound is obtained by considering a restricted set of schedules with a round-based structure, called layers [25].
We focus on configurations at the end of each layer and classify them according to their valence [20,25], namely, the decisions that can be reached in layered extensions.
As opposed to deterministic algorithms, where the valence of a configuration binds the extension to reach a certain decision value v, in a randomized algorithm the valence only implies that some execution will decide v with high probability [3].
This introduces a category of null-valent configurations, from which no decision value is reached with high probability.
When a decision is reached, the configuration must be univalent, so the proof aims to avoid univalent configurations.
An important case in the proof is when the configuration is null-valent, where we derive an isoperimetric inequality in order to control a one-round coin-flipping game for reaching another null-valent configuration.To show that the lower bound is tight, we present a randomized algorithm for consensus, which has an O(n 2 ) total step complexity.
We give a shared coin algorithm with a constant agreement parameter, which leverages a single binary multi-writer register (in addition to n single-writer multireader registers); this can be used to obtain a randomized consensus algorithm with the same total step complexity [5].
We prove that all processes output the same value with high probability by viewing any schedule of the algorithm as a stochastic process, and applying Kolmogorov's inequality.Related Work.
The previous Ω( n 2 log 2 n ) lower bound [3] relies on a lower bound for coin flipping games.
In this proof, the adversary schedules processes step-by-step, and the results of the games are analyzed through hyperbolic functions, resulting in a long and unwieldy proof.
In contrast, our approach considers only the configurations at the end of layers, allowing powerful results about product probability spaces to be applied, and streamlining the analysis of the behavior of executions.Our general proof structure follows a proof by Bar-Joseph and Ben-Or [12] of an Ω( p n/ log n) lower bound on the expected number of rounds in a randomized consensus algorithm for the synchronous message passing model.
In particular, like them, we treat null-valent configurations by considering one-round coin-flipping games and applying an isoperimetric inequality.
Unlike their proof, our proof handles the more complicated shared-memory model and exploits the fact that in an asynchronous system, processes can be hidden in a one-round coin flipping game without having to fail for the rest of the execution.The layered approach was introduced by Moses and Rajsbaum [25], who used it to study deterministic consensus.
They showed that the layered approach can unify the impossibility proof for asynchronous consensus [20,21,23] with the lower bound on the number of rounds needed for solving synchronous consensus [16,19].
Their work considered the message-passing model as well as the shared-memory model with single-writer registers.
We take the layered approach one step further and extend it to randomized algorithms, deriving the lower bound on their total step complexity within the same framework as the results for deterministic algorithms.
Besides incorporating randomization into the layered model, our proof also deals with the technical challenge of allowing processes to access multi-writer registers.Abrahamson [1] presented a randomized algorithm for solving consensus in asynchronous systems using shared memory, which had an exponential running time.
The first polynomial algorithm for solving randomized consensus was presented by Aspnes and Herlihy [5].
They described an algorithm that uses a shared coin in order to reach agreement and has a total step complexity of O(n 4 ).
The amount of shared memory required by this algorithm was later bounded by Attiya, Dolev and Shavit [7].
Aspnes [2] presented an algorithm for randomized consensus with O(n 4 ) total step complexity, which also uses bounded space.
These algorithms were followed by an algorithm of Saks, Shavit and Woll [26] with O(n 3 ) total step complexity and later, an algorithm of Bracha and Rachman [13] with O(n 2 log n) total step complexity.Organization.
In Section 2, we adapt the layered model to incorporate randomization and a strong adversary.
Section 3 defines the key notions used in our lower bound proofpotence and valence-and proves that layered executions in the multi-writer shared-memory model are potence connected.
The lower bound proof appears in Section 4, while Section 5 presents the algorithm.
We consider a standard model of an asynchronous sharedmemory system, where n processes, p1, . . . , pn, communicate by reading and writing to shared multi-writer multi-reader registers (cf. [8,24]).
Each step consists of some local computation, including an arbitrary number of local coin flips (possibly biased) and one shared memory event, which is a read or a write to some register.A configuration C consists of the local states of all the processes, and the values of all the registers.
Like many impossibility results, our proof relies on having configurations that are indistinguishable to all processes, except some set P .
We denote C ¬P ∼ C if the state of all processes that are not in P is equal in both C and C , and the values of all the registers are equal.
We write C ¬p ∼ C when P = {p}.
For the purpose of the lower bound, we restrict our attention to a constrained set of executions, which proceed in layers.
An f -layer is a sequence of at least n − f distinct process id's.
When executing a layer L, each process p ∈ L takes a step, in the order specified by the layer.An f -execution τ = L 1 , L 2 , · · · is a (finite or infinite) sequence of f -layers.
We will consider only configurations that are reachable by finite f -executions, namely, after some finite sequence of f -layers.Since we consider randomized algorithms, for each configuration C there is a fixed probability for every step a process will perform when next scheduled.
Denote by X C i the probability space of the steps that process pi will preform, if scheduled by the adversary.
The probability space X C i depends only on the local state of p i in configuration C, and therefore, delaying p i does not change this probability space.
Let X C = X C 1 × X C 2 × · · · × X C n be the product probability space.
An element y ∈ X C represents a possible result of the local coin flips from a configuration C.We assume a strong adversary that observes the processes' local coin flips, and chooses the next f -layer knowing what is the next step each process will take.
The adversary applies a function σ to choose the next f -layer to execute for each configuration C and y ∈ X C , i.e.,σ : {(C, y) | C is a configuration and y ∈ X C } → {L | L is a layer}.
When the configuration C is clear from the context we will use the abbreviation σ( y) = L y .
Denote by (C, y, L y ) the configuration that is reached by applying steps of the processes in L y , after a specific y ∈ X C is chosen.
Then C • σ is a random variable whose values are the configurations (C, y, L y ), when y is drawn from the probability space X C .
An f -adversary α = σ 1 , σ 2 , · · · is a (finite or infinite) sequence of functions.Given a configuration C and a finite prefix α = σ 1 , σ 2 , · · · , σ of the adversary α, C • α is a random variable whose values are the configurations that can be reached by the algorithm.
For every y 1 ∈ X C let P ( y 1 ) = Pr[ y1 is chosen] denote the probability of y1 in the probability space X C .
The probability that a configuration C is reached is defined inductively 1 :Pr[C • α is C ] = X y 1 ∈X C P ( y 1 ) · Pr[(C, y 1 , L y 1 ) • α is C ],where α is the remainder of the prefix after σ1, i.e., α = σ 2 , · · · σ , and the basis of the induction for α 1 = σ 1 is:Pr[C • α1 is C ] = X y 1 ∈X C P ( y1) · χ C ( y1),where χ C ( y1) = χ C (C, α1, y1) characterizes whether the configuration C is reached if y1 is chosen, i.e.,χ C ( y 1 ) =  1 (C, y1, L y 1 ) is C 0 otherwiseThe probability of deciding v when executing the algorithm under α from the configuration C is defined as follows: if C is a configuration in which there is a de-cision v, then Pr[decision from C under α is v] = 1, if C is a configuration in which there is a decision ¯ v, then Pr[decision from C under α is v] = 0, otherwise, Pr[decision from C under α is v] = X y 1 ∈X C P ( y1) · Pr[decision from (C, y1, L y 1 ) under α is v],where α is the remainder of the adversary after σ 1 , i.e.,α = σ 2 , σ 3 , · · · .
In order to derive our lower bound, we are interested in the probability of reaching each of the possible decision values, from a given configuration.
As the algorithm proceeds towards a decision, we expect the probability of reaching a decision to grow, where for a configuration in which a decision is reached, this probability is 1.
Let k ≥ 0 be an integer, and definek = 1 n √ n − k (n − f ) 3 .
Our proof makes use of adversaries that have a probability of 1 − k for reaching a certain decision value.
As the layer number k increases, the value of k decreases, and the probability 1 − k required for a decision grows.An adversary with a high probability of deciding is defined as follows.Definition 1.
An f -adversary α from a configura- tion C that is reachable from an initial configuration by an f -execution with k ≥ 0 layers, is v-deciding if Pr[decision from C under α is v] > 1 − k .
Next, we classify configurations according to the probabilities of reaching each of the possible decisions from them.
We adapt the notion of potence [25] to fit randomized algorithms.Instead of considering all possible adversaries, we further restrict our attention to a certain subset of them, which will be specified later.Definition 2.
A configuration C is (v, k, S)-potent, for v ∈ {0, 1} and a set S of f -adversaries, if there is a v- deciding adversary α ∈ S from C. Definition 3.
A configuration is (v, k, S)-valent if it is (v, k, S)-potent but not (¯ v, k, S)-potent.
Such a configuration is (k, S)-univalent.
A configuration is (k, S)-bivalent if it is both (0, k, S)- potent and (1, k, S)-potent.
A configuration is (k, S)-null-valent if it is neither (0, k, S)-potent nor (1, k, S)-potent.We often say that C is v-potent (v-valent, bivalent, nullvalent) with respect to S. Figure 1 illustrates the valence of configurations.Note that a configuration can have certain valency with respect to one set of adversaries S and another valency with respect to another set S .
For example, it can be univalent with respect to S and null-valent with respect to S ; however, this cannot happen when S ⊆ S .
(Another example appears in Lemma 1 below.)
We will sometimes use the notation v-potent or v-valent, when the set S and the layer number k are clear from the context.The set of f -adversaries we consider, denoted SP , is induced by a subset of processes P ⊆ {p1, . . . , pn}.
A layer is P -free, for some set of processes P , if it does not include any process p ∈ P .
An adversary α is in S P , if all of the layers it may choose are P -free.
A layer is full with respect to SP if it contains the n − |P | distinct process identifiers {p1, . . . , pn} \ P ; otherwise, the layer is partial.Restricting a set of adversaries can only eliminate possible adversaries, and therefore cannot introduce potence that does not exist in the original set of adversaries, as formalized in the following simple lemma.Lemma 1.
If a configuration C is v-valent with respect to S P , then it is not ¯ v-potent with respect to S P ∪{p} for any process p. Proof.
Assume towards a contradiction, that there is a process p such that C is ¯ v-potent with respect to S P ∪{p} .
Then there exists a ¯ v-deciding adversary α in S P ∪{p} , i.e.,6 - 1 − k 1 − k 1 1 max α Pr[decision from C under α is 0]Pr[decision in C • α is ¯ v] > 1 − k .
But α is also an adversary in SP because S P ∪{p} ⊆ SP , which implies that C is ¯ v-potent also with respect to SP , contradicting the fact that C is v-valent with respect to SP .
Let C be a configuration, and fix y1 ∈ X C .
We consider the configurations that can be reached by applying different layers.
We define a relation between these configurations, based on their potence, which generalizes notions suggested by Moses and Rajsbaum [25].
Definition 4.
For a given y1, configurations (C, y1, L) and (C, y1, L ) have shared potence with respect to SP , if they are both v-potent with respect to SP for some v ∈ {0, 1}.
Definition 5.
For a given y1, configurations (C, y1, L) and (C, y1, L ) are potence connected with respect to SP , if there is a sequence of layers L = L0, L1, . . . , L h = L such that for every i, 0 ≤ i < h, there exists a process p such that the configurations (C, y1, Li) and (C, y1, Li+1) have shared potence with respect to S P ∪{p} .
Note that potence connectivity is a transitive relation.
Also, if (C, y1, L) and (C, y1, L ) have shared potence with respect to S P ∪{p} for some process p, then, in particular, they are potence connected.The following claims show specific configurations that are potence connected, under the assumption that for every process p and layer L, the configuration (C, y1, L) is univalent with respect to SP and S P ∪{p} .
This implies thatif (C, y1, L) ¬P ∪{p} ∼ (C, y1, L ), then (C, y1, L) and (C, y1, L )have shared potence with respect to S P ∪{p} , since they must have the same valence with respect to S P ∪{p} , but are not null-valent.
Claim 2.
If L = [pi 1 , pi 2 , . . . , pi ]is a layer where for some j, 1 ≤ j < , pi j and pi j+1 both write to the same register R, andL = [pi 1 , . . . , pi j−1 , pi j+1 , . . . , pi ] is the layer L after removing pi j , then (C, y1, L) and (C, y1, L ) have shared potence with respect to S P ∪{p i j } .
Proof.
It is clear that (C, y1, L) ¬P ∪{p i j } ∼ (C, y1, L ), which implies that (C, y1, L) and (C, y1, L ) have shared potence with respect to S P ∪{p i j } .
Claim 3.
If L = [pi 1 , pi 2 , . . . , pi ] is a layer, p is a pro- cess not in L, and L = [pi 1 , pi 2 , . . . , pi , p]is the layer L after adding p at the end, then (C, y1, L) and (C, y1, L ) have shared potence with respect to S P ∪{p} .
Proof.
If p performs a read operation, then (C, y1, L) ¬P ∪{p} ∼ (C, y1, L ), which implies that these two configurations have shared potence with respect to S P ∪{p} , and the claim follows.If p performs a write operation to register R, then the states of all processes not in P ∪{p} are the same in (C, y1, L) and in (C, y1, L ), but the value of R may be different.If (C, y1, L) and (C, y1, L ) do not have shared potence with respect to S P ∪{p} , then since we assume they are univalent with respect to S P ∪{p} , we have that for some v ∈ {0, 1}, (C, y1, L) is v-valent with respect to S P ∪{p} and (C, y1, L ) is ¯ v-valent with respect to S P ∪{p} .
In particular, there is a ¯ v-deciding adversary α ∈ S P ∪{p} from (C, y1, L ).
Adding p at the beginning of α yields a ¯ vdeciding adversary from (C, y1, L), which is in SP .
However, by Lemma 1, (C, y1, L) is v-valent with respect to SP , which is a contradiction.Claim 4.
If L = [pi 1 , pi 2 , . . . , pi ] is a layer and L = [pi 1 , . . . , pi j−1 , pi j+1 , pi j , pi j+2 , . . . , pi ]is the layer L after swapping pi j and pi j+1 , then (C, y1, L) and (C, y1, L ) are potence connected with respect to SP .
Proof.
If pi j and pi j+1 access different registers or if theyboth read, then (C, y1, L) ¬P ∼ (C, y1, L ).
If pi j reads register R and pi j+1 writes to R, then (C, y1, L) ¬P ∪{p i j } ∼ (C, y1, L ).
Both cases imply that (C, y1, L) and (C, y1, L ) are potence connected with respect to SP .
The remaining case is when pi j and pi j+1 both write to the same register R, which is proved by reverse induction on j.Basis:If j = −1, let L0 = L, L1 = [pi 1 , . . . , pi −2 , pi] be the layer L after removing pi −1 , and L2 = L .
By Claim 2, (C, y1, L0) and (C, y1, L1) are potence connected with respect to SP , and by Claim 3, (C, y1, L1) and (C, y1, L2) are potence connected with respect to SP .
By the transitivity of potence connectivity, this implies that (C, y1, L0) and (C, y1, L2) are potence connected with respect to SP .
Induction step: LetL0 = L = [pi 1 , pi 2 , . . . , pi ]andL1 = [pi 1 , . . . , pi j−1 , pi j+1 , pi j+2 , . . . , pi ]be the layer L0 after removing pi j .
By Claim 2, (C, y1, L0) and (C, y1, L1) are potence connected with respect to SP .
LetL2 = [pi 1 , . . . , pi j−1 , pi j+1 , pi j+2 , . . . , pi , pi j ]be the layer L1 after adding pi j at the end.
By Claim 3, (C, y1, L1) and (C, y1, L2) are potence connected with respect to SP .
For every m, 3 ≤ m ≤ − j + 1, letLm = [pi 1 , . . . , pi j−1 , pi j+1 , pi j+2 , . . . , pi j , pi −m+3 , . . . , pi ]be the previous layer L m−1 after swapping p i j with the process before it, until it reaches p i j+1 .
Specifically,L −j+1 = L = [p i 1 , . . . , p i j−1 , p i j+1 , p i j , p i j+2 , . . . , p i ].
By the induction hypothesis, (C, y 1 , L m ) and (C, y 1 , L m+1 ) are potence connected with respect to SP , for every m, 2 ≤ m < − j + 1.
This implies that (C, y1, L0) and (C, y 1 , L −j+1 ) are potence connected with respect to S P .
The following lemma shows that given a configuration C and a y ∈ X C , if there is a layer that extends C into a v-valent configuration and a layer that extends C into a ¯ vvalent configuration, then there is a layer that extends C into a non-univalent configuration, possibly by failing one additional process.Lemma 5.
Let C be a configuration andy 1 ∈ X C .
If there are layers L v and L ¯ v , such that (C, y 1 , L v ) is (v, k + 1, S P )-valent and (C, y 1 , L ¯ v ) is (¯ v, k + 1, S P )-valent, then there is a layer L such that (C, y1, L) is either not (k+1, SP )- univalent or not (k + 1, S P ∪{p} )-univalent, for some process p.Proof.
Assume towards a contradiction that for every layer L and every process p, the configuration (C, y 1 , L) is univalent with respect to both S P and S P ∪{p} .
Let L F be the full layer with respect to SP consisting of all processes not in P , according to the order of their id's.
Then, L F is univalent with respect to S P , say it is (¯ v, k + 1, S P )-valent.
(Otherwise, we follow the same proof with We start with the layer L F and repeatedly swap processes until we reach the layer L , in a chain of configurations which, by Claim 4, are potence connected with respect to SP .
From L , we repeatedly remove the last process until reaching the layer L v , in a chain of configurations which, by Claim 3, are potence connected with respect to S P .
This implies that (C, y 1 , L F ) and (C, y 1 , L v ) are potence connected with respect to SP .
L ¯ v .)
Assume L v = [p i 1 , .
Since (C, y1, Lv) is (v, k + 1, SP )-valent, and (C, y1, L F ) is (¯ v, k + 1, S P )-valent, it follows that there are layers L 1 andL 2 such that (C, y 1 , L 1 ) is (v, k + 1, S P )-valent, (C, y 1 , L 2 )is (¯ v, k + 1, S P )-valent, and (C, y 1 , L 1 ) and (C, y 1 , L 2 ) have shared potence with respect to S P ∪{p} for some process p.By Lemma 1 and our assumption, (C, y1, L1) is (v, k + 1, S P ∪{p} )-valent, and (C, y 1 , L 2 ) is (¯ v, k +1, S P ∪{p} )-valent, and hence, they cannot have shared potence with respect to S P ∪{p} .
This yields a contradiction and proves the lemma.
A deciding configuration has to be univalent, so our proof aims to avoid univalent configurations.
We first show that some initial configuration is not univalent (Lemma 6); namely, it is either bivalent or null-valent.
Ideally, we would like to prove that a non-univalent configuration can be extended by a single layer to a non-univalent configuration, by (permanently) failing at most one more process.
Doing so would allow us to construct a layered execution with f layers, each containing at least n − f process steps, which implies the desired lower bound.However, while this can be done (with high probability) in the case of a null-valent configuration (see Lemma 11), this is not true in the case of a bivalent configuration.
From a bivalent configuration, we have both a v-deciding adversary and a ¯ v-deciding adversary.
However, we cannot use them in Lemma 5 to obtain a non-univalent configuration, since the first layer of a v-deciding adversary may lead to a ¯ vvalent configuration.
Such a ¯ v-valent configuration, which is reached while following a v-deciding adversary, will be called ¯ v-switching.
We prove that a bivalent configuration can be extended by a single layer to a non-univalent or switching configuration, by failing at most one more process (Lemma 7).
We also prove that there is a small probability of deciding in a switching configuration and thus, the execution can be extended (with high probability) from a switching configuration by at least one layer (Lemma 8).
We extend the execution in this manner, with high probability, for f layers.
Since n − f processes take a step in each layer, we obtain the bound of an expected Ω(f (n − f )) steps (Theorem 12).
We start by applying Lemma 1 to prove that some initial configuration is not univalent.Lemma 6.
There exists an initial configuration C that is not univalent with respect to either S ∅ or S {p} , for some process p.Proof.
Assume that all initial configurations are univalent with respect to S ∅ .
Consider the initial configurations ∼ Ci, and hence, C i−1 and C i have the same valence with respect to S {p i } .
By Lemma 1, C i−1 is not 1-potent with respect to S {p i } , and C i is not 0-potent with respect to S {p i } .
Hence, they are null-valent with respect to S {p i } .
C 0 , C 1 , · · · , C n such that in C i , 0 ≤ i ≤ n, We formally define switching configurations as follows.Definition 6.
Let C be a (v, k, S P )-potent configuration, α = σ1, σ2, . . . be a v-deciding adversary from C in SP , and y1 ∈ X C such that the configuration (C, y1, σ1( y1)) is (¯ v, k + 1, S P )-valent.
Then (C, y 1 , σ 1 ( y 1 )) is a ¯ v-switching configuration with respect to S P from C by y 1 and α.Lemma 5 implies that a bivalent configuration can be extended with one layer to a configuration which is either vswitching or not univalent.Lemma 7.
If a configuration C reachable by an fexecution is (k, SP )-bivalent, then there is an adversary σ such that for every y 1 ∈ X C , (C, y 1 , σ( y 1 )) is either vswitching, or not (k+1, S P )-univalent or not (k+1, S P ∪{p} )-univalent, for some process p.Proof.
Assume that for every layer L and every process p, the configuration (C, y 1 , L) is univalent with respect to S P and S P ∪{p} .
Consider the extension of C with L F .
Fix y 1 ∈ X C and assume that D = (C, y 1 , L F ) is (¯ v, k + 1, S P )-valent.
Since C is bivalent, there is a v-deciding adversary α = σ1, σ2, · · · in SP .
Consider the configuration C = (C, y1, σ1( y1)).
By the assumption, C is univalent.
If it is (¯ v, k + 1, S P )-valent then it is ¯ v-switching with respect to S P from C by y 1 and α.
Otherwise, it is (v, k+1, S P )-valent.
Since D is (¯ v, k+1, S P )-valent, by Lemma 5, there exists a layer L and a process p such that (C, y1, L) is either not (k + 1, SP )-univalent or not (k + 1, S P ∪{p} )-univalent.Handling switching configurations is more delicate, and it is done in the next lemma.Lemma 8.
Let C be a ¯ v-switching configuration with respect to S P from C by y 1 and α.
Then with probability at least 1 − 1 n √ n , C can be extended with at least one layer to a configuration which is either v-switching, or not univalent with respect to S P , or not univalent with respect to S P ∪{p} , for some process p.Proof.
Let α = σ1σ2 . . .; note that C = (C, y1, σ1( y1)).
Denote C 0 = C, C 1 = C and for every k ≥ 2 fix y k ∈ X C k−1 and let C k = (C k−1 , y k , σ k ( y k )).
Assume that for every k ≥ 1, C k is univalent, and let C be the first configuration which is v-valent with respect to SP .
If such a configuration does not exist then the execution either reaches a configuration that decides ¯ v, or does not reach any decision.
Since α is v-deciding from C, the probability that C does not exist is at most k ≤ 1n √ n .
Since C is the first configuration which is v-valent, C −1 is ¯ v-valent.
Therefore, there is a ¯ v-deciding adversary β = ρ1, ρ2, . . . from C −1 in SP .
Consider the configura- tion D = (C −1 , y , ρ 1 ( y )).
If D is not (k + , S P )-univalent then we are done.
If D is ¯ v-valent, then since C is v-valent, by Lemma 5, there exists a layer L such that (C −1 , y , L)is either not (k + , SP )-univalent or not (k + , S P ∪{p} )-univalent, for some process p, in which case we are also done.
Otherwise D is v-valent, which implies that it is v-switching with respect to S P from C −1 by y and β.
The remaining part of the lower bound proof deals with null-valent configurations, and it relies on results about oneround coin-flipping games.
Formally, a U -valued one-round coin flipping game of m players is a functiong : {X 1 ∪ ⊥} × {X 2 ∪ ⊥} × · · · × {X m ∪ ⊥} −→ {1, 2, . . . , U },where X i , 1 ≤ i ≤ m, is the i-th probability space.A t-hiding adversary may hide at most t of the random choices in X 1 , · · · , X m , by replacing them with a ⊥.
Formally, let X = X1 × · · · × Xm be the product probability space.
For every y ∈ X, and I ⊆ {1, · · · , m}, the vector reached when the adversary hides the coordinates of I is defined as follows:y I (i) = 8 < : y(i), i / ∈ I ⊥, i ∈ I.For every possible outcome of the game u ∈ {1, . . . , U }, define the set of all vectors in X for which no t-hiding adversary can force the outcome of the game to be u, asW u = { y ∈ X|g( yI ) = u for every I ⊆ {1, · · · , m} such that |I| ≤ t } .
We prove that there is high probability for hiding values in a way that forces one of the outcomes when U = 3, i.e., for some u ∈ {1, 2, 3}, Pr[ y ∈ W u ] is very small.
The proof relies on an isoperimetric inequality, following a result of Schechtman [27].
The space (X, d) is a finite metric space where for every x and y in X, d ( x, y) is the Hamming distance between x and y (the number of coordinates in which x and y differ).
For A ⊆ X, B(A, t) is the ball of radius t around the set A, i.e., B(A, t) = { y ∈ X| there is z ∈ A such that d( y, z) ≤ t}.
Lemma 9.
Let X = X 1 ×· · ·×X m be a product probability space and A ⊆ X such that Pr[ x ∈ A] = c. Let λ0 = q 2m log 2 c , then for ≥ λ0,Pr[ x ∈ B(A, )] ≥ 1 − 2e − (−λ 0 ) 2 2m .
Proof.
Consider an element x as a random functionx : D = {1, . . . , m} → X1 ∪ · · · ∪ Xm such that x(i) ∈ Xi.
Define a sequence of partial domains ∅ = D 0 ⊂ D 1 ⊂ · · · ⊂ D m = D such that D i = {1, . . . , i}.
Let f : X → R be a function that measures the distance of elements from the given subset A ⊆ X, i.e.,f ( x) = d(A, x).
Choose a random element of w ∈ X according to the given distribution.
Define the sequence Y 0 , . . . , Y m byY i = E[f ( x) | x| D i = w].
Specifically, Y 0 = E[f ( x)] with probability 1, and Ym = f ( w) with the probability of choosing w.
It is well known that Y0, . . . , Ym is a Doob martingale (see [18, Chapter VI]).
Notice that X 1 , ..., X m are independent and therefore for every i ∈ D, the random variable x(i) is independent of other values of x.The function f satisfies the Lipschitz condition, i.e., for every x, y that differ only in By Azuma's inequality we have thatPr[|f ( x) − E[f ( x)]| > λ] < 2e − λ 2 2m .
We now claim that E[f ( x)] ≤ λ 0 .
Assume the contrary,that E[f ( x)] > λ 0 .
Since λ 0 = q 2m log 2 c, we have that2e − λ 2 0 2m = c. For every x ∈ A we have f ( x) = 0, therefore Pr[|f ( x) − E[f ( x)]| > λ 0 ] ≥ Pr[f ( x) = 0] = c,which contradicts Azuma's inequality.
Hence, for every ≥ λ 0 we havePr[ x / ∈ B(A, )] = Pr[f ( x) > ] ≤ Pr[|f ( x) − E[f ( x)]| > − λ0] < 2e − (−λ 0 ) 2 2m, which completes the proof.Lemma 10.
In a 3-valued one-round coin-flipping game of m players, for t = 6 p 2m log(m 3 ) there exists a t-hiding adversary and u ∈ {1, 2, 3}, such that the adversary can force the outcome of the game to u with probability greater than 1 − 1 m 3 .
Proof.
Recall that for every u ∈ {1, 2, 3}, the set W u is the set of all vectors in X for which no t-hiding adversary can force the outcome of the game to be u.
So our goal is to prove that Pr[ y ∈ W u ] < 1 m 3 for some u ∈ {1, 2, 3}.
Denote B u = B(W u , t 3Pr[ y ∈ B u ] ≥ 1 − 2e − (t−λ 0 ) 2 2m = 1 − 2e − 2m log (2m 3 ) 2m = 1 − 2e − log (2m 3 ) = 1 − 1 m 3 .
Since T u∈{1,2,3} B u = ∅ we have that Pr[ y ∈ B 1 ∩ B 2 ] + Pr[ y ∈ B 1 ∩ B 3 ] + Pr[ y ∈ B 2 ∩ B 2 ] ≤ 1 2 · P u∈{1,2,3} Pr[ y ∈ B u ], which implies that Pr[ y ∈ ∪ u∈{1,2,3} B u ] = X u∈{1,2,3} Pr[ y ∈ B u ] − X u =u ∈{1,2,3} Pr[ y ∈ B u ∩ B u ] +Pr[ y ∈ ∩ u∈{1,2,3} B u ] ≥ 1 2 · X u∈{1,2,3} Pr[ y ∈ B u ] ≥ 3 2 · (1 − 1 m 3 ) > 1This contradiction implies that for some u ∈ {1, 2, 3} we have Pr[ y ∈ W u ] < 1 m 3 .
We use one-round coin-flipping games to show that, with high probability, a null-valent configuration C can be extended with one f -layer to a null-valent configuration.
In order to achieve the above, we may need to hide up to 6 p 2n log (2n 3 ) processes in the layer other than the processes in P , although they are not permanently failed.
Therefore, we assume that f ≥ 12 p 2n log (2n 3 ), and will always make sure that |P | ≤ f 2 .
This will allow us to hide f 2 ≥ 6 p 2n log (2n 3 ) additional processes (not in P ), in executions in S P .
Lemma 11.
If a configuration C reachable by an fexecution is (k, S P )-null-valent, then with probability at least 1 −1 (n−|P |) 3 , there is an f -adversary σ1 such that C • σ1 is (k + 1, SP )-null-valent.
Proof.
Let C be a (k, S P )-null-valent configuration, and let L F be an f -layer that is full with respect to S P .
For every y 1 ∈ X C , we classify the configurations (C, y 1 , L F ) into three categories:1.
The configuration (C, y 1 , L F ) is (0, k + 1, S P )-potent.
2.
The configuration (C, y 1 , L F ) is (1, k + 1, S P )-valent.
3.
The configuration (C, y1, L F ) is (k +1, SP )-null-valent.
This can be considered as a 3-valued one-round coinflipping game of m players, where m = n − |P |.
This implies that n − f ≤ m ≤ n. By Lemma 10, we can hide 6 p 2m log(2m 3 ) processes and force the resulting configuration into one of the above categories with probability at least 1 − 1 m 3 .
Hiding processes is done by choosing a partial layer L y 1 in S P that does not contain any step by the hidden processes, but only a step of each process that is not hidden.
We define an adversary σ 1 that for each y 1 ∈ X, chooses the corresponding partial layer, i.e., σ1( y1) = L y 1 .
Our claim is that the category that can be forced is the third one.
Assume, towards a contradiction, that the category that can be forced is the first one.This implies that the probability overy 1 ∈ X that (C, y1, L y 1 ) is (0, k + 1, SP )-potent is at least 1 − 1 m 3 .
Therefore with probability at least 1− 1 m 3 , a y1 ∈ X is chosen such that there exists a 0-deciding adversary α from (C, y1, L y 1 ) for which:Pr[decision from (C, y1, L y 1 ) under α is 0] > 1 − k+1Therefore with probability at least 1 − 1 m 3 , there exists an adversary α = σ 1 , α from C such that:Pr[decision from C under α is 0] = X y 1 ∈X P ( y 1 ) · Pr[decision from (C, y 1 , L y 1 ) under α is 0] > " 1 − 1 m 3 « · (1 − k+1 ) ≥ " 1 − 1 (n − f ) 3 « · " 1 − 1 n √ n + k + 1 (n − f ) 3 « = 1 − 1 n √ n + k (n − f ) 3 + 1 (n − f ) 3 n √ n − k + 1 (n − f ) 6 > 1 − 1 n √ n + k (n − f ) 3 = 1 − k ,where the last inequality holds for sufficiently large n, since (n − f ) 6 ≥ (n − f ) 3 n √ n and k = O(n).
This contradicts the assumption that C is (k, S P )-nullvalent.
A similar argument holds for the second category.Hence, with probability at least 1− 1 m 3 , the third category can be forced, namely, we can reach a configuration that is (k + 1, S P )-null-valent.
We can now put the pieces together and prove the lower bound on the total step complexity of any randomized consensus algorithm.Theorem 12.
The total step complexity of any f -tolerant randomized consensus algorithm in an asynchronous system, where n−f ∈ Ω(n) and f ≥ 12 p 2n log (2n 3 ), is Ω(f (n−f )).
Proof.
We show that the probability of forcing the algorithm to continue f 2 layers is at least 1 − 1 √ n .
Therefore the expected number of layers is at least (1− 1 √ n ) · f 2.
Each of these layers is an f -layer containing at least n − f steps, implying that the expected total number of steps is at leastΩ((1 − 1 √ n ) · f 2 · (n − f )), which is in Ω(f (n − f )) since n − f ∈ Ω(n).
We argue that for everyk, 0 ≤ k ≤ f 2, with probability at least 1−k 1 n √ n , there is a configuration C reachable by an fexecution with at least k layers, which is either v-switching or non-univalent with respect to S P where |P | ≤ k +1.
Once the claim is proved, the theorem follows by taking k = f 2 , since the probability of having an f -execution with more than f 2 layers is at least 1 − f2 1 n √ n > 1 − 1 √ n.
We prove the claim by induction on k. Basis: k = 0.
By Lemma 6, there exists an initial configuration C that is not univalent with respect to S ∅ or S {p} , for some process p.Induction step: Assume C is a configuration reachable by an f -execution with at least k layers, that is either vswitching or non-univalent with respect to S P where |P | ≤ k + 1.
We prove that with probability at least 1 − 1 n √ n , C can be extended with at least one layer to a configuration C that is either v-switching or non-univalent with respect to S P where |P | ≤ k + 2.
This implies that C exists withprobability (1 − k 1 n √ n )(1 − 1 n √ n ) ≥ 1 − (k + 1) 1 n √ n.
If C is bivalent, then by Lemma 7, there exists an adversary σ and a process p such that C • σ is either v-switching or not (k + 1, SP )-univalent or not (k + 1, S P ∪{p} )-univalent.If C is v-switching, then by Lemma 8, there exists a finite adversary α and a process p such that with probability at least 1 − 1 n √ n , C • α is either ¯ v-switching, or not univalent with respect to S P , or not univalent with respect to S P ∪{p} .
If C is null-valent, then by Lemma 11, there exists an adversary σ1 such that the configuration C • σ1 is not (k + 1, SP )-univalent with probability at least 1 − 1 m 3 .
Since m ≥ n − f ∈ Ω(n), we have that 1 − 1m 3 ≥ 1 − 1 (n−f ) 3 > 1 − 1 n √ n .
Finally, taking f ∈ Ω(n) and n − f ∈ Ω(n), we get a lower bound of Ω(n 2 ) on the total step complexity.
This section presents a randomized consensus algorithm with O(n 2 ) total step complexity, by introducing a shared coin algorithm with a constant agreement parameter and O(n 2 ) total step complexity.
In a shared coin algorithm with agreement parameter δ, each process outputs a decision value −1 or +1, such that for every v ∈ {−1, +1}, there is a probability of at least δ for all processes to output the same value v [5].
Using a shared coin algorithm with O(n 2 ) total step complexity and a constant agreement parameter, in the framework of Aspnes and Herlihy [5], implies a randomized consensus algorithm with O(n 2 ) total step complexity.As in previous shared coin algorithms [13,26], in our algorithm the processes flip coins until the amount of coins that were flipped reaches a certain threshold.
An array of n single-writer multi-reader registers records the number of coins each process has flipped, and their sum.
A process reads the whole array in order to track the total number of coins that were flipped.Each process decides on the value of the majority of the coin flips it reads.
Our goal is for the processes to read similar sets of coins, in order to agree on the same majority value.
For this to happen, we bound the total number of coins that are flipped (by any process) after some process observes that the threshold was exceeded.
A very simple way to guarantee this property is to have processes frequently read the array in order to detect quickly that the threshold was reached.
This, however, increases the total step complexity.The novel idea of our algorithm in order to overcome this conflict, is to utilize a multi-writer register called done that serves as a binary termination flag; it is initialized to false.
A process that detects that enough coins were flipped, sets done to true.
This allows a process to read the array only once in every n of its local coin flips, but check the register done before each local coin flip.The pseudocode appears in Algorithm 1.
In addition to the binary register done, it uses an array A of n single-writer multi-reader registers, each with the following components (all initially 0): count: how many flips the process performed so far.sum: the sum of coin flips values so far.For the proof, fix an execution α of the algorithm.
We will show that all processes that terminate agree on the value 1 for the shared coin with constant probability; by symmetry, the same probability holds for agreeing on −1, which implies the algorithm has a constant agreement parameter.The total count of a specific collect is the sum of A [1].
count, . . . , A [n].
count, as read in this collect.
Note that the total count in Line 8 is ignored, but it can still be used for the purpose of the proof.Although we only maintain the counts and sums of coin flips, we can (externally) associate them with the set of coin flips they reflect; we denote by FC the collection of coin flips that are written in the shared memory by the first time that true is written to done.
The size of F C can easily be bounded, since each process flips at most n coins before checking A.Lemma 13.
FC contains at least n 2 coins and at most 2n 2 coins.For a set of coins F we let Sum(F ) be the sum of the coins in F .
We denote by F i the set of coin flips read by the collect of process p i in Line 8.
This is the set according to which the process p i decides on its output, i.e., p i returns Sum(F i ).
Since each process may flip at most one more coin after true is written to done, we can show:Lemma 14.
For every i, F C ⊆ F i , and F i \ F C contains at most n − 1 coins.We now show that there is at least a constant probability that Sum(FC ) ≥ n.
In this case, by Lemma 14 all processes that terminate agree on the value 1, since Fi contains at most n − 1 additional coins.We partition the execution into three phases.
The first phase ends when n 2 coins are written.
We assume a stronger adversary that can choose these n 2 coins out of n 2 + n − 1 coins that were flipped.
The second phase ends when true is written to done.
In the third phase, each process reads the whole array A and returns a value for the shared coin.Since F C is the set of coins written when done is set to true, then it is exactly the set of coins written in the first and second phases.
Let F f irst be the first n 2 coins that are written, and Figure 2.)
This implies that Sum(F C ) = Sum(F f irst ) + Sum(F second ).
Therefore, we can bound (from below) the probability that Sum(FC ) ≥ n by bounding the probabilities that Sum(F f irst ) ≥ 3n andF second = F C \ F f irst .
(SeeSum(F second ) ≥ −2n.
Consider the sum of the first n 2 + n − 1 coin flips.
After these coins are flipped, the adversary has to write at least n 2 of them, which will be the coins in F f irst .
If the sum of the first n 2 + n − 1 coin flips is at least 4n then Sum(F f irst ) ≥ 3n.
We bound the probability that this happens using the Central Limit Theorem.Lemma 15.
The probability that Sum(F f irst ) ≥ 3n is at least 1 8 √ 2π e −8 .
Proof.
There are N = n 2 + n − 1 coins flipped when n 2 coins are written to F f irst .
By the Central Limit Theorem, the probability for the sum of these coins to be at least x √ N , e −8 the sum of these N coins is at least 4 √ N , which is more than 4n, and hence Sum(F f irst ) ≥ 3n.
converges to 1 − Φ(x),We now need to bound Sum(F second ).
Unlike F f irst , whose size is determined, the adversary may have control over the number of coins in F second , and not only over which coins are in it.
However, by Lemma 13 we have |FC | ≤ 2n 2 , therefore |F second | ≤ n 2 , which implies that F second must be some prefix of n 2 additional coin flips.
We consider the partial sums of these n 2 additional coin flips, and show that with high probability, all these partial sums are greater than −2n, and therefore in particular Sum(F second ) > −2n.
Formally, for every i, 1 ≤ i ≤ n 2 , let Xi be the ith additional coin flip, and denote Sj = P j i=1 Xi.
Since |F second | ≤ n 2 , there exist k, 1 ≤ k ≤ n 2 , such that S k = Sum(F second ).
If Sj > −2n for every j, 1 ≤ j ≤ n 2 , then specifically Sum(F second ) = S k > −2n.
The bound on the partial sums is derived using Kolmogorov's inequality.
This bounds the probability of agreeing on the same value for the shared coin as follows.
.
Since Sum(FC ) = Sum(F f irst ) + Sum(F second ), this implies that the probability that Sum(FC ) ≥ n is at least 3 √ 2πe −8 .
By Lemma 14, for every i, Fi \ FC contains at most n − 1 coins, which implies that if Sum(FC ) ≥ n then Sum(Fi) ≥ 1, and therefore if pi terminates, then it will decide 1.
Hence with probability at least 3 √ 2πe −8 , Sum(FC ) ≥ n and all processes which terminate agree on the value 1.
By symmetry, all processes which terminate agree on the value −1 with at least the same probability.Clearly, Algorithm 1 flips O(n 2 ) coins.
Moreover, all work performed by processes in reading the array A can be attributed to coin flips.
This can be used to show that Algorithm 1 has O(n 2 ) total step complexity.Using Algorithm 1 in the framework of Aspnes and Herlihy [5] implies: Theorem 18.
There is a randomized consensus algorithm with O(n 2 ) total step complexity.
We have proved that Θ(n 2 ) is a tight bound on the total step complexity of solving randomized consensus, under a strong adversary, in an asynchronous shared-memory system using multi-writer registers.Aspnes [3] shows an Ω( n 2 log 2 n ) lower bound on the expected total number of coin flips.
Our layering approach as presented here, does not distinguish between a deterministic step and a step involving a coin flip, leaving open the question of the optimal number of coin flips.Our algorithm exploits the multi-writing capabilities of the register.
The best known randomized consensus algorithm using single-writer registers [13] has O(n 2 log n) total step complexity, and it is intriguing to close the gap from our lower bound.In addition to settling the open question of the asymptotic total step complexity, we consider an important contribution of this paper to be in introducing new techniques for investigating randomized consensus algorithms.
These techniques provide a new context and opportunities for exploring several research directions, two of which are discussed next.One direction is to study the individual step complexity of randomized consensus; namely, the expected number of steps performed by a single process.
Aspnes and Waarts [6] present a shared coin algorithm in which each process performs O(n log 2 n) expected number of steps; their algorithm has O(n 2 log 2 n) total step complexity.
Their shared coin algorithm is similar to [13], but the local flips of a process are assigned increasing weights, which allows fast processes to terminate earlier.
We believe that the individual step complexity can be reduced using multi-writer registers.Our results may also provide new insight into the expected step complexity of randomized consensus under weak adversaries that cannot observe the local flips or the contents of the shared memory.
Several papers presented randomized consensus algorithms for this type of adversaries, e.g., [9-11, 14, 15], with the best algorithms having O(n polylog(n)) total step complexity and O(polylog(n)) individual step complexity.
It will be intriguing to find tight bounds for this model as well.
