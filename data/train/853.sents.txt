One of the key problems in reinforcement learning (RL) is balancing exploration and exploitation.
Another is learning and acting in large or even continuous Markov decision processes (MDPs), where compact function approximation has to be used.
In this paper, we provide a provably efficient , model-free RL algorithm for finite-horizon problems with linear value-function approximation that addresses the exploration-exploitation tradeoff in a principled way.
The key element of this algorithm is the use of a hypothesized online linear-regression algorithm in the recently proposed KWIK framework.
We show that, if the sample complexity of the KWIK online linear-regression algorithm is polynomial, then the sample complexity of exploration of the RL algorithm is also polynomial.
Such a connection provides a promising approach to efficient RL with function approximation via studying a simpler setting.
One of the key problems in reinforcement learning (Sut- ton & Barto 1998) is the exploration-exploitation tradeoff, which strives to balance two competing types of behavior of an autonomous agent in an unknown environment: the agent can either make use of its current knowledge about the environment to maximize its cumulative reward (i.e., to exploit), or sacrifice short-term rewards to gather information about the environment (i.e., to explore) in the hope of increasing future long-term return.Exploration can be framed as a dual control problem, and (in principle) can be solved optimally in a Bayesian manner.
However, this approach is computationally intractable and it is often not obvious how to select a prior distribution for learning (Duff 2002).
We only consider non-Bayesian approaches in this paper.
Thrun (1992) surveyed a number of popular exploration rules, including the competencemap approach for continuous MDPs (Thrun & M ¨ oller 1992), but little can be said about their performance guarantees.
In fact, some of them have provably poor performance in certain situations.
Recently, there has been a growing interest in formally analyzing the sample complexity of exploration (Kakade 2003) in finite-state Markovian environments.
This line of work has significantly advanced understanding of the exploration-exploitation dilemma, but has not been merged with approaches for function approximation needed for scaling up.
In contrast, this paper is concerned with intelligent exploration in large or even continuous environments where compact function approximation has to be used.
In particular, we focus on the special case where the value function is represented as a linear combination of predefined features.
1 In contrast to previous work on linear value-function approximation, our algorithm explicitly addresses the question of balancing exploration and exploitation, and we formally analyze its sample complexity.
This algorithm works by reducing a finite-horizon reinforcement-learning problem to a sequence of related online linear regression problems, each of which is solved by a hypothesized admissible algorithm, denoted A 0 , in the recently proposed KWIK framework (Strehl & Littman 2008).
Roughly speaking, A 0 is admissible if it predicts the target value of an example near-accurately except in a polynomial (in relevant quantities that we will make clear) number of examples where it signals "I don't know".
The main contribution of this paper is an efficient reduction to KWIK online linear regression from reinforcement learning with linear value-function approximation and an efficient, built-in exploration scheme.
This reduction shows how important questions in reinforcement learning, such as the sample complexity of exploration and value-function approximation error, may depend on the related quantities in the simpler setting of KWIK online linear regression.
Thus, this connection allows us to study a simpler problem as a means to solve the significantly more difficult RL problem.Although A 0 is hypothetical right now, a close relative has been successfully created by Strehl & Littman (2008).
In particular, the existing algorithm requires that the function to be learned be precisely linear, whereas our algorithm must be tolerant to nearly linear target functions.
However, the close relationship between the two problems gives us hope that A 0 can be created under some mild conditions.
The rest of the paper is organized as follows.
Section 2 defines the KWIK online regression problem and specifies the conditions for the admissible algorithm A 0 .
Section 3 reviews the RL notation briefly, and then describes the reduction in detail.
We provide a few theoretical results including the sample complexity of exploration as well as error bounds for the learned linear value functions, both of which scale up nicely.
Finally, Section 4 discusses the relationship between this work to previous results, and Section 5 concludes the paper by pointing out a few research directions.
KWIK stands for "Know What It Knows", and represents a new framework for learning that is particularly well suited for use in RL settings.
An essential element of a KWIK learner is that it is able to compute certain quantities to measure how confident it is in its predictions.
A simple example is confidence bounds for parameter estimation, which is widely used in statistics and machine learning.
Such confidence information is particularly useful for a few purposes.
In reinforcement learning, for instance, confidence bounds are used to select actions to guide exploration (Auer 2002;Strehl & Littman 2005).
We first define the KWIK Online Regression Framework, adopting terminology from Strehl & Littman (2008).
We use 񮽙x񮽙 to denote the Euclidean norm of a vector x ∈ R d where d ∈ N is the dimension.
Definition 1 At timestep t = 1, 2, 3, · · · , a KWIK online regression agent acts according to the following protocol:• First, it receives an input vector x t ∈ R d .
• Second, it provides an outputˆyoutputˆ outputˆy t ∈ [−1, 1] ∪ {Ξ}, whereΞ is a special value indicating that the agent is not confident in its prediction and thus refuses to predict a numeric value between −1 and 1.
We calî y t valid ifˆyifˆ ifˆy t 񮽙 = Ξ.
• Finally, the agent observes the (possibly noisy) ground truthy t ∈ [−1, 1].
The problem becomes a KWIK online linear regression problem if we must impose certain assumptions on the functional relation between x t and y t .
Assumption 1 We make the following assumptions for the KWIK online regression problem:A. (Bounded-input assumption) 񮽙x t 񮽙 ≤ 1 for all t. B. (Semi-linearity assumption) There exist some (unknown) vector w * ∈ R d and a small number ξ ∈ [0, 1) such that 񮽙w * 񮽙 ≤ 1 and |E[y t |x t ] − w * · x t | ≤ ξ for all t.We call the quantity ξ the slack value.
Assumption 1A is reasonable as practical problems often have bounded inputs and we can re-scale the inputs so that this assumption holds.
Assumption 1B essentially states that the target function being learned is "almost" linear, and the distance to being linearity is measured by the slack value ξ.
This assumption is less restrictive than it might appear at the first glance.
In practice, with an expanded set of features we can often approximate a learning target by a function linear in the features.
This is a common trick to capture nonlinearity via linear functions in many situations including kernelbased learning (Shawe-Taylor & Cristianini 2004).
Note that we make no assumption on the sequence of inputs x t , except that 񮽙x t 񮽙 is at most 1.
In particular, x t can depend on previous inputs {x 1 , · · · , x t−1 }, which is fundamentally different from the usual i.i.d. assumption made in supervised-learning problems (e.g., see Hastie, Tibshirani, & Friedman (2003)).
As we shall see later, this difference is important when we move to the RL setting in the next section.
We next define admissibility of KWIK online regression algorithms.Definition 2 A KWIK online regression algorithm A is admissible if, for any given 񮽙, δ > 0, the following two conditions are satisfied with probability at least 1 − δ:• Whenever A predicts a validˆyvalidˆ validˆy t 񮽙 = Ξ, we have that|ˆy|ˆy t − E[y t |x t ]| ≤ 񮽙 + ξ.
• The number of timesteps t for whichˆywhichˆ whichˆy t = Ξ is bounded by some function ζ(d, 1//, 1/δ) that is polynomial in d, 1//, and 1/δ.
We call ζ the sample complexity of A.
An admissible, polynomial-time algorithm is proposed recently by Strehl & Littman (2008) for KWIK online linear regression when ξ = 0; namely, the target function, E[y t |x t ], is a linear function of x t .
When we allow ξ > 0, however, the problem becomes significantly more difficult, as illustrated by the following example.Example 1 Fix any ξ > 񮽙 > 0.
The input dimension is d = 1, and the target function f to learn is the zero function (which is trivially linear): f (x) ≡ 0.
Let the input at time t be x t = min{tβ, 1} for some small β > 0, then the corresponding output is y t = f (x t ) = 0.
Assume that the learner knows f is noise free.
But since it does not know that f is exactly linear, it has to predict conservatively to handle the worst-case situations.
At time t where ξ β < t < 1 β , the learner has to say "I don't know" since, based on the training data up to time t − 1, the possible range of y t , which is[− 2tξ t−1 , 2tξ t−1 ], is too wide to guarantee a prediction error of at most ξ + 񮽙.
By letting β ↓ 0, the number of Ξ does not depend on 񮽙 or ξ, and is unbounded.It may be appealing to change Definition 2 to tolerate a prediction error of cξ+񮽙 for some constant c > 1.
While this modification may allow admissible KWIK online regression algorithms which may be useful on its own, it will lead to an exponential growth of value-function approximation error in the reduction given in Section 3.
Therefore, in order to allow for the existence of algorithms that are admissible in the sense of Definition 2, we have to make certain assumptions on the process that generates the data [񮽙x t , y t 񮽙] t∈N .
Such assumptions may place restrictions on, for example, the accumulative loss of the best linear hypothesis, w * · x, on the sequence of data.
Whether these assumptions are reasonable depends on the applications at hand.
For the purpose of this paper, they are denoted abstractly by C. Under certain conditions C on the process generating the samples [񮽙x t , y t 񮽙] t∈N , there exists a KWIK online linear regression algorithm A 0 that is admissible and takes polynomial running time in every timestep.
We denote its sample complexity by ζ 0 (d, 1//, 1/δ), and its perstep computation complexity by τ 0 (d, 1//, 1/δ).
In the rest of the paper, we assume that condition C holds whenever we apply the base algorithm A 0 .
Given the background terminology and assumptions in the previous section, we consider reinforcement learning that involves sequential prediction and control.
After a brief introduction to notation and terminology, we describe a reduction to KWIK online linear regression from RL.
We consider environments modeled as finite-horizon Markov decision processes (Puterman 1994), or MDPs for short.
An MDP M can be described by a six-tuple 񮽙S, A, T, R, H, µ 0 񮽙, where S is a set of states, A is a finite set of actions, T is the transition function with T (s, a, s 񮽙 ) denoting the probability of reaching s 񮽙 from s by taking action a, R is a bounded reward function with R(s, a) ∈ [0, 1] denoting the expected immediate reward gained by taking action a in state s, H ∈ N is the horizon, and µ 0 is a start-state distribution.
2 An MDP is said to be finite (or infinite) if the state space S is finite (or infinite).
For convenience, define the set of stages as[H] = {1, 2, · · · , H}.
An episode is a sequence of H state transitions: 񮽙s 1 , a 1 , r 1 , s 2 , · · · , s H , a H , r H , s H+1 񮽙,π : S × [H] → A.Specifically, π(s, h) ∈ A is the action the agent will take if s is the current state at stage h. Given a policy π, we define the state-value function, V π h (s), as the expected cumulative reward received by executing π starting from state s at stage h until the episode terminates at stage H. Similarly, the state-action value function (a.k.a. the Q-function), Q π h (s, a), is the expected cumulative reward received by taking action a in state s at stage h and following π until the episode terminates at stage H.
A reinforcement-learning agent attempts to learn an optimal policy π * whose value functions at stage h are denoted by V * h (s) and Q * h (s, a), respectively.
It is known thatV * h = max π V π h and Q * h = max π Q π h .
A greedy policy at stage h, denoted π Q h , with respect to a value function Q h is one that selects actions with maximum Q-values; namely, π Q h (s, h) = arg max a Q h (s, a).
The greedy policy with respect to Q * h is optimal for stage h.
The Bellman equation plays a central role to many RL algorithms including the one we will describe: for any s ∈ S, a ∈ A, h ∈ [H],Q * h (s, a) = R(s, a) + 񮽙 s 񮽙 ∈S 񮽙 T (s, a, s 񮽙 ) max a 񮽙 ∈A Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙where Q * H+1 is understood to be the zero function.Given the complete model of a finite MDP (i.e., the sixtuple), standard algorithms exist for finding the optimal value function and the optimal policy, including linear programming, value iteration, and policy iteration (Puterman 1994).
However, if the transition and/or reward functions are unknown, the agent has to learn the optimal value function or policy by interacting with the environment.
Algorithms such as Q-learning with 񮽙-greedy exploration (Sutton & Barto 1998) do not address the exploration problem efficiently and may be highly inefficient in some domains (Thrun 1992;Koenig & Simmons 1996).
Recently, there has been a growing interest in formally analyzing the efficiency of exploration strategies in finite MDPs.
For any fixed 񮽙, Kakade (2003) defines the sample complexity of exploration of an RL algorithm A to be the number of timesteps t such that the non-stationary policy at time t, A t , is not 񮽙-optimal from the current state s t at time t (i.e., V At (s t ) ≤ V * (s t ) − 񮽙).
An algorithm A is then said to be PAC-MDP (Probably Approximately Correct in Markov Decision Processes) if, for any 񮽙 > 0 and δ ∈ (0, 1), its sample complexity of exploration is less than some polynomial in |S|, |A|, 1//, 1/δ, and 1/(1 − γ), with probability at least 1 − δ ( Strehl et al. 2006).
Examples of PAC-MDP algo- In this subsection, we propose an algorithm, A H , for Hhorizon reinforcement learning in which a linear value function is used.
In particular, we assume a set of d features are predefined: φ : S × A 񮽙 → [−1, 1] d .
A Q-function can then be represented compactly by a weight vectorw h ∈ R d for each h ∈ [H]: ˆ Q h (s, a) = w h · φ(s, a).
Such a linear approximation scheme is widely used to solve large-scale RL problems (Lagoudakis & Parr 2003;Sutton & Barto 1998).
Similarly to the previous section, we make a semilinearity assumption for the value function at every stage.
Remember that outputs in the KWIK online regression (c.f., Definition 1) are in [−1, 1], we need to re-scale the value function to the same range by dividing Q * h by H:Assumption 3 (Semi-linearity assumption) For every stage h ∈ [H], there exist some (unknown) vector w * h ∈ R d and a small number ξ h > 0 such that 񮽙w * h 񮽙 ≤ 1 and 񮽙 񮽙 񮽙 񮽙 Q * h (s, a) H − w * h · φ(s, a) 񮽙 񮽙 񮽙 񮽙 ≤ ξ h(1)for all s and a. Whether it is required to know ξ h depends on whether such information is needed by A 0 .
Algorithm 1 gives a formal description of A H .
Basically, A H learns the optimal value functions Q * h by treating them as H related KWIK online linear regression problems.
It runs H copies of the base algorithm A 0 to update the weight vector w h for each stage h. By the Bellman equation, the Q-function at stage h is defined recursively as the sum of immediate reward at stage h and the expected optimal Q-value of the next states.
Therefore, the algorithm im-S 1 S 2 S 3 S 4Stage h = 2:(w 2 is used to compute q 2,L and q 2,R ) (3) choose greedy a 2 = R since both q 2,L and q 2,R are valid (4) use r 1 + q 2,a2 to update w 1 as this backup value is "trusted"Stage h = 1:(w 1 is used to compute q 1,L and q 1,R )(1) choose exploratory a 1 = L sinceq 1,L = Ξ(2) no backup is needed as this is the first horizonStage h = 3:(w 3 is used to compute q 3,L and q 3,R )(5) choose exploratory a 3 = R since q 3,R = Ξ (6) do not use r 2 +q 3,a3 to update w 2 as this value is "unknown" proves its value-function estimates by performing Bellmanbackup-style updates.
A central idea behind the efficiency of the reduction is that we only use a backup value when it is "known".
A backup value is "known" when the prediction made by A 0 is valid, and thus by Assumption 2 must be near-accurate to the true value.
Figure 1 gives a simple H-horizon example for H = 3.
It illustrates how to choose actions and how to select backup values to do learning.
A quick observation about A H is that, if the per-step computation complexity of A 0 is τ 0 , then the per-step computation complexity of A H is O(|A| τ 0 ), when execution of lines 17-21 are amortized to every timestep.
So, the perstep computation complexity scales nicely from regression problems to sequential decision making in MDPs, in contrast to algorithms such as sparse sampling (Kearns, Man- sour, & Ng 2002) that scales exponentially in the horizon length.
We next turn to the more difficult questions of sample complexity of exploration and value-function approximation error bounds.
Observe state s h .
for all a ∈ A do 6:Use A (h) 0 to compute q h,a ∈ [0, H] ∪ {Ξ} as a prediction for Q * h (s h , a).
Here, if A (h) 0gives a valid prediction, then this prediction has to be multiplied by H to obtain q h,a due to the normalization (1) we have used.
end for 8: if q h,a = Ξ for some a ∈ A then 9:a h ← a // do exploration 10: L h ← FALSE // Q * h (s h , a h ) is "unknown" else 12: a h ← arg max a q h,a // do exploitation 13: L h ← TRUE // Q * h (s h , aH 񮽙 h=1 񮽙 h · ζ 0 񮽙 d, 1 񮽙 h , 1 δ h 񮽙񮽙 ;III.
With probability at least 1 − 񮽙 H l=1 δ l , all valid Q-value predictions at stage h differ from the true values by at mostH · H 񮽙 l=h (񮽙 l + ξ l ) .
Before proving this theorem, we first mention a few implications of it.
The following corollary, which follows immediately from Theorem 1, indicates that the sample complexity and error bounds of the KWIK online linear regression algorithm A 0 scale nicely to the analogous quantities in the more complicated, H-horizon RL problem.
If we let 񮽙 h = 񮽙 0 , δ h = δ 0 , and ξ h = ξ 0 for all stage h in Theorem 1, then:I.
The number of Ξ outputted at stage h is O 񮽙 Hζ 0 񮽙 d, 1 񮽙0 , 1 δ0 񮽙񮽙 ;II.
The total number of Ξ outputted during the whole run ofA H in all stages is O 񮽙 H 2 ζ 0 񮽙 d, 1 񮽙0 , 1 δ0 񮽙񮽙 ;III.
With probability at least 1 − Hδ 0 , all valid Q-value predictions at stage h differ from the true values by at mostH(H − h + 1) (񮽙 0 + ξ 0 ) = O(H 2 (񮽙 0 + ξ 0 )).
Using Corollary 1, we can prove the following theorem about the sample complexity of exploration of A H .
Our focus is to provide the first polynomial sample complexity bound, although it is possible to improve the bounds using a more careful analysis.
O 񮽙 H 3 񮽙 ζ 0 񮽙 d, H 3 񮽙 , H δ 񮽙 log 1 δ 񮽙episodes, with probability at least 1 − δ.
Due to space limitation, we only provide proof sketches for the theorems given in the previous subsection.
Before proving the sample complexity of exploration bound, we first provide two useful lemmas.
The first is simple and the proof is omitted.Lemma 1 Let f 1 and f 2 be two real-valued functions on the same finite domain X; namely, f i :X 񮽙 → R, for i = 1, 2.
If max x∈X |f 1 (x) − f 2 (x)| ≤ ∆ for some ∆ > 0, then |max x∈X f 1 (x) − max x∈X f 2 (x)| ≤ ∆.
Lemma 2 Let π be a policy for an H-horizon MDP.
Let s 1 be a fixed start state of an episode, and s h be the state visited at stage h of this episode.
Then,V * 1 (s 1 ) − V π 1 (s 1 ) = E π 񮽙 H 񮽙 h=1 񮽙 Q * h (s h , π * (s h , h)) − Q * h (s h , π(s h , h)) 񮽙 񮽙 ,where E π stands for the expectation with respect to the probability distributions of trajectories ρ = [s 1 , s 2 , · · · , s H , s H+1 ] generated by policy π.PROOF.
We let r h denote the reward received at stage h by following π.
Note that both s h and r h are random variables whose distributions are completely determined by policy π as s 1 is fixed.
Then,V * 1 (s 1 ) = Q * 1 (s 1 , π * (s 1 , 1)) = Q * 1 (s 1 , π(s 1 , 1)) + 񮽙 Q * 1 (s 1 , π * (s 1 , 1)) − Q * 1 (s 1 , π(s 1 , 1)) 񮽙 = E π [r 1 + V * 2 (s 2 )] + 񮽙 Q * 1 (s 1 , π * (s 1 , 1)) − Q * 1 (s 1 , π(s 1 , 1)) 񮽙 .
We apply the derivation above for V * h (s h ) recursively up to stage H, and obtainV * 1 (s 1 ) = E π [r 1 + r 2 + · · · + r H ] +E π 񮽙 񮽙 H h=1 񮽙 Q * h (s h , π * (s h , h)) − Q * h (s h , π(s h , h)) 񮽙񮽙 .
By definition, V π 1 (s 1 ) = E π [r 1 + r 2 + · · · + r H ], which immediately proves the lemma. 񮽙
Proof of Theorem 1.
The theorem can be proved by mathematical induction.
For h = H, the theorem is ensured by Assumption 2.
For the induction step, assume the theorem holds for all stages l > h where h < H and we consider stage h. Due to operations of Algorithm 1, the transitions from s h to s h+1 in all episodes can be categorized into two groups: (i) L h+1 = TRUE, and (ii) L h+1 = FALSE.
Transitions belonging to case (i) consist of a stream of data for A (h) 0 to run according to the KWIK online regression protocol defined in Definition 1, and Assumption 2 guarantees that there are at most ζ 0 (d, 1// h , 1/δ h ) timesteps for which Ξ is outputted.On the other hand, by induction hypothesis, case (ii) happens at most 񮽙 H l=h+1 ζ 0 (d, 1// l , 1/δ l ) times.
Therefore, the total number of Ξ outputted in stage h is at mostζ 0 (d, 1// h , 1/δ h ) + H 񮽙 l=h+1 ζ 0 (d, 1// l , 1/δ l ),which is what we desire to prove for part I.Part II follows directly from part I. For part III, the target function to learn at stage h is given by˜Qby˜ by˜Q h (s, a) = R(s, a) + 񮽙 s 񮽙 ∈S T (s, a, s 񮽙 ) max a 񮽙 ∈AˆQ ∈Aˆ ∈AˆQ h+1 (s 񮽙 , a 񮽙 ),wherê Q h+1 is the function learned by A H in stage h + 1.
3 By the induction hypothesis, we have񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h+1 (s 񮽙 , a 񮽙 ) − Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙 񮽙 񮽙 ≤ H 񮽙 H l=h+1 (񮽙 l + ξ l ) for all (s 񮽙 , a 񮽙 ) whenever Ξ is not outputted.
LetˆQLetˆ LetˆQ h be 3Strictly speaking, ˆ Q h+1 may change over time and thus˜Qthus˜ thus˜Q h is not a stationary learning target.
But, this fact does not affect our analysis as long asˆQasˆ asˆQ h+1 is always bounded betweenQ * h+1 − H P H l=h+1 (񮽙 l + ξ l ) and Q * h+1 + H P H l=h+1 (񮽙 l + ξ l ).
the function A (h) 0learns, then for any (s, a) we have񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h (s, a) − ˜ Q h (s, a) 񮽙 񮽙񮽙 ≤ H(񮽙 h + ξ h ) due to Assumptions 2 and 3.
Combining all these facts, we have for any (s, a):񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h (s, a) − Q * h (s, a) 񮽙 񮽙 񮽙 ≤ 񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h (s, a) − ˜ Q h (s, a) 񮽙 񮽙 񮽙 + 񮽙 񮽙 񮽙˜Q񮽙˜ 񮽙˜Q h (s, a) − Q * h (s, a) 񮽙 񮽙 񮽙 ≤ H (񮽙 h + ξ h ) + 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 s 񮽙 ∈S T (s, a, s 񮽙 ) 񮽙 max a 񮽙 ∈AˆQ ∈Aˆ ∈AˆQ h+1 (s 񮽙 , a 񮽙 ) − max a 񮽙 ∈A Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 ≤ H(񮽙 h + ξ h ) + max s 񮽙 ∈S 񮽙 񮽙 񮽙 񮽙 max a 񮽙 ∈AˆQ ∈Aˆ ∈AˆQ h+1 (s 񮽙 , a 񮽙 ) − max a 񮽙 ∈A Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙 񮽙 񮽙 񮽙 ≤ H(񮽙 h + ξ h ) + H H 񮽙 l=h+1 (񮽙 l + ξ l ) = H H 񮽙 l=h (񮽙 l + ξ l ),where the last inequality is due to Lemma 1.
1 − 񮽙 H h=1 δ h = 1 − δ 2 , we have for each h, Q * h (s h , π * (s h , h)) − Q * h (s h , π i (s h , h)) ≤ Q * h (s, π * (s h , h)) − ˆ Q h (s h , π i (s h , h)) +O 񮽙 H 2 (񮽙 h + ξ h ) 񮽙 = Q * h (s, π * (s h , h)) − ˆ Q h (s h , π i (s h , h)) + O 񮽙 񮽙 H 񮽙 ≤ Q * h (s, π * (s h , h)) − ˆ Q h (s h , π * (s h , h)) + O 񮽙 񮽙 H 񮽙 ≤ O 񮽙 H 2 (񮽙 h + ξ h ) 񮽙 + O 񮽙 񮽙 H 񮽙 = O 񮽙 񮽙 H 񮽙 , (2)where the first and last inequalities are due to Corollary 1(III), and the second due to the fact that π i is greedy with respect tô Q h when no Ξ is outputted.
For any fixed start state s 1 , Lemma 2 asserts thatV * 1 (s 1 ) − V πi 1 (s 1 ) = E πi 񮽙 H 񮽙 h=1 񮽙 Q * h (s h , π * (s h , h)) − Q * h (s h , π i (s h , h)) 񮽙 񮽙 .
Combined with Equation (2) and the fact that case (i) happens with probability p i , the equality above impliesV * 1 (s 1 ) − V πi 1 (s 1 ) = O(񮽙 + Hp i ).
When p i ≤ p 0 for some threshold p 0 = O( 񮽙 H ), we have V * 1 (s 1 ) − V πi 1 (s 1 ) = O(񮽙) and also E s1∼µ0 [V * (s 1 ) − V πi (s 1 )] = O(񮽙), indicating that the policy π i is indeed O(񮽙)-optimal.We claim that with high probability p i > p 0 will hold only a polynomial number of episodes.
Specifically, Corollary 1 asserts that Ξ is outputted O (H 2 ζ 0 (d, H 3 񮽙 , H δ )) times.
Using the inequality of Hoeffding (1963), with probability at least 1 − δ 2 , the number of episodes i with p i > p 0 isO 񮽙 H 2 ζ 0 (d, H 3 񮽙 , H δ ) p 0 log 1 δ 񮽙 .
Substituting p 0 = O( 񮽙 H ) and applying the union bound to the two cases (p i > p 0 and p i ≤ p 0 ) gives the lemma.
While we have focused on finite-horizon RL problems in this paper, it is often easier to model environments by discounted MDPs (Puterman 1994;Sutton & Barto 1998), which are specified by a five-tuple, 񮽙S, A, T, R, γ񮽙, where γ ∈ [0, 1) is a discount factor.
Changes in notation and terminology are necessary since there is no notion of horizon in this setting.
Specifically, we only need to consider stationary policies and value functions: a policy is a mapping from states to actions: π : S 񮽙 → A; the value functions, such as Q π γ (s, a) and Q * γ (s, a), are defined as the expected cumulative γ-discounted reward.An observation for discounted MDPs is that rewards in the future are exponentially down-weighted.
Since rewards are bounded, rewards received after a large number of timesteps contribute little to the value of the current state.
Therefore, we may transform a γ-discounted MDP M γ into an H-horizon MDP M H so that the optimal value functions of M H and M γ differ by at most 񮽙, providedH = Ω 񮽙 log 1 񮽙(1−γ) 1 − γ 񮽙 .
It is worth mentioning that even if the optimal value function in M γ , Q * γ , is near linear, the intermediate value functions in M H , Q * h , need not be near linear.
We note that this problem may be resolved by using different sets of features at different stages.
That is, we require features φ h : S × A 񮽙 → [−1, 1] d at stage h, and assume that Q * h satisfies Assumption 3 with small slack ξ h .
Algorithm A H can then be applied.
Our work in this paper is most related to the original KWIK online linear regression framework proposed by Strehl & Littman (2008).
The only difference in problem formulation is that we make a semi-linearity assumption while they assume exact linearity.
This change is necessary if we allow Bellman-backup-style updates on the value functions since the backup value (i.e., is unavoidably biased and it is unreasonable to assume the target function at stage h − 1 remains linear for all possible biases introduced in stage h.
The second significant difference is how KWIK online linear regression is applied to RL problems.
Strehl & Littman (2008) adopt a model-based approach: they assume the MDP state transitions are governed by a set of linear equations with white, Gaussian noise, and then apply KWIK online linear regression to learn the transition matrices, and finally solve the learned MDP model to obtain a policy that either explores or exploits.
Even if an MDP can be accurately modelled as a linear system and approximate planning is concerned, however, solving a continuous MDP remains a challenging task (Chow & Tsitsiklis 1989;Kearns, Mansour, & Ng 2002;Kocsis & Szepesvári 2006).
In contrast, the model-free approach taken in this paper avoids this problem completely by learning the value function directly.
With a learned linear value function, finding the greedy action takes only O(|A| d) time per step.Another related work is metric-E 3 (Kakade, Kearns, & Langford 2003), which also addresses the problem of efficient exploration in continuous MDPs.
They also use a model-based approach, and develop sample complexity of exploration in terms of the so-called cover number of an MDP-a number that describes how complex the MDP is.
Similarly, they also make an assumption on the availability of an efficient, continuous MDP solver, which might limit the use of their algorithm in practice for the same reason.The KWIK online linear regression framework we described is related to the online learning model of linear functions, where a rich set of beautiful results have been established in the last two decades (e.g., the works by Bernstein (1992), Cesa-Bianchi, Long, & Warmuth (1996), Kivinen & Warmuth (1997), Klasner & Simon (1995), Littlestone, Long, & Warmuth (1995), Long (1997), andVovk (1998)).
In this model, input data are not assumed to be i.i.d. (as what the paper does), and the outputs are roughly a linear function of the inputs.
Cumulative absolute and squared error bounds are developed under various assumptions.
The main difference between that model and ours is that we require the learner to be aware of the accuracy of its prediction.
However, ideas and results in the online learning area may turn out useful for our framework as well.Recently, Peters & Schaal (2007) proposes an interesting reduction from RL to reward-weighted regression.
While they consider the specific task of following a given trajectory in rigid-body systems whose dynamics are governed by a set of equations with unknown parameters, this paper focuses on learning in general MDPs where the learner is not provided with such near-optimal trajectories.A H is similar to delayed Q-learning ( Strehl et al. 2006) in that both of them allow a value backup to happen only if the backup value is "trusted".
In finite MDPs, it is sufficient for delayed Q-learning to become confident by averaging over a large set of samples.
In the case of using linear function approximation, A H relies on A 0 to do so.
In turn, A 0 is expected to resort to more complicated reasoning.
The connection between KWIK online regression and reinforcement learning opens a number of interesting directions.
First, we are currently developing a concrete algorithm A 0 under mild conditions C, and plan to compare it against the standard RL algorithms with other exploration strategies.
With such an algorithm at hand, it is possible to improve the bounds provided in Section 3.2 to make them more practical.Second, it is important to find more efficient reductions for discounted RL that do not involve an intermediate conversion to an H-horizon problem, as described in Section 3.4.
Also, a more careful analysis is needed for the discounted case.Third, it is observed that a similar reduction in Algorithm 1 applies to KWIK online nonlinear regression.
This fact allows us to use more expressive classes of nonlinear value functions, which may lead to better policies in some problems.Finally, we expect fruitful applications of KWIK online regression to (associative) bandit problems (Abe, Bier- mann, & Long 2003;Auer 2000;2002;Long 1997).
These problems are classic settings for studying the explorationexploitation dilemma, and have found important applications in the fast growing market of Internet sponsored search (e.g., (Gonen & Pavlov 2007)).
We appreciate the helpful discussion with Alex Strehl.
The anonymous reviewers also provided constructive comments and links to relevant works that have improved the paper.
The work is primarily supported by NSF-ITR-0325281.
