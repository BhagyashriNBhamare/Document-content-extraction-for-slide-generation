Establishing trust amongst agents is of central importance to the development of well-functioning multi-agent systems.
Trust (or reputation) mechanisms can help by aggregating and sharing trust information between agents.
Unfortunately these mechanisms can often be manipulated by strategic agents.
Existing mechanisms are either very robust to manipulation (i.e., manipulations are not beneficial for strategic agents), or they are very informative (i.e., good at aggregating trust data), but never both.
This paper explores this trade-off between these competing desiderata.
First, we introduce a metric to evaluate the informativeness of existing trust mechanisms.
We then show analytically that trust mechanisms can be combined to generate new hybrid mechanisms with intermediate robustness properties.
We establish through simulation that hybrid mechanisms can achieve higher overall efficiency in environments with risky transactions and mixtures of agent types (some cooperative , some malicious, and some strategic) than any previously known mechanism.
We aim to design trust mechanisms that have good informativeness as well as strategyproofness properties.
A mechanism is informative if it aggregates the available information well, such that agents using it can successfully separate good from bad trading partners.
A mechanism is strategyproof if agents cannot improve their utility in the system by manipulating the trust mechanism.
Strategyproofness is important here because we consider mechanisms that must rely on information provided voluntarily by the agents and where the outcome of individual transactions cannot be monitored centrally.
Depending on the particular trust mechanism, agents might be able to manipulate by spreading bad information about other agents in the system, or by creating fake agents (sybils) that spread good information about themselves.Existing aggregation methods represent distinct tradeoffs between robustness and informativeness.
This can be problematic for overall system efficiency.
On the one hand, if a mechanism is not informative then it is not very helpful in identifying good and bad agents, resulting in poor trading decisions and low overall efficiency.
On the other hand, if a mechanism can be easily manipulated, then many agents may choose to influence a mechanism to their advantage, which in turn decreases overall efficiency as well.
In real environments with risky transactions, there is likely to be a mixture of different kinds of agents.
Some agents will be highly trustworthy and cooperative, likely to complete a transaction in good faith.
Some agents will be less trustworthy and malicious, with a greater probability of participating in an incomplete or fraudulent transaction.
Depending on how costly manipulations are, some of the malicious agents will act strategically and manipulate a trust mechanism to their advantage.Previous research has primarily focused on a formal analysis of the strategyproofness properties of different mechanisms.
However, a formal instrument for measuring and comparing informativeness was missing.
In this paper, we propose a simple metric for measuring the informativeness of a trust mechanism, independent from how this information is being used for making decisions in the environment.
This gives us a way to evaluate how well different mechanisms aggregate trust information.
We then combine exist-ing transitive-trust mechanisms introducing new hybrid transitivetrust mechanisms, enabling a new continuum of tradeoffs between the competing desiderata of informativeness and strategyproofness.
This is desirable in order to make the tradeoff that is best for a given environment with a particular agent population.
We establish analytically that these hybrid mechanisms have intermediate strategyproofness properties and we show experimentally that they also have good informativeness properties.
Ultimately, however, we are interested in the overall efficiency resulting from the use of hybrid mechanisms.
We study this in two different simulated domains (file-sharing with viruses, and website surfing).
Our results show that in some settings, hybrid mechanisms can outperform previously known mechanisms, with efficiency gains up to 5%.
Many transitive-trust mechanisms have been introduced in the literature (for a recent survey see Friedman et al. [8]).
The most well known mechanism is PageRank [10] originally used by Google to rank websites.
However, PageRank was soon found to be highly susceptible to manipulation, and thus subsequent work has primarily focused on solving the manipulability problem [6,7,13].
Altman et al. [1] presented the first axiomatic approach to the design of trust mechanisms, providing systematic insight into the design space.
Guha et al. [9] present the first large-scale empirical study on trust mechanisms using transitive trust networks.
Sami and Resnick [11] study the dynamics of transitive trust mechanisms in environments with risky transactions, looking to limit the cumulative effect of an attack by a powerful adversarial.
We consider multi-agent systems where agents engage in risky transactions with many other agents, but rarely have repeat interactions with the same other agent.
An agent who contacts another agent puts itself at risk in terms of whether the second agent will complete the transaction correctly or not.
A good outcome leads to a gain in utility by the first agent, a bad outcome a loss in utility.
DEFINITION 1 (AGENT MODEL).
Each agent vi has a (private) type θi ∈ [0,1], which represents its goodness, or trustworthiness.
This is the probability that an agent will generate a good outcome when participating in a transaction with another agent.By sharing their direct experiences via the trust mechanism, the agents can help each other identify and thus avoid bad agents.
DEFINITION 2 (AGENT INFORMATION & REPORTS).
Given a set of agents V = {v1, ..., vn}, let Vi denote the agents that vi has direct trust information ti about, where ti :Vi → [0, 1], i.e., ti(vj ) is the trust agent vi has in agent vj .
Agent vi makes reports ( ˆ Vi, ˆ ti) to a transitive-trust mechanism.
Agent vi is truthful if and only if (Vi, ti) = ( ˆ Vi, ˆ ti).
DEFINITION 3 (TRUST GRAPH).
A trust graph G = (V, E, w) is a set of vertices V and directed edges (vi, vj ) ∈ E, vi, vj ∈ V .
Each edge (vi, vj ) has an associated weight w(vi, vj ) ∈ [0, 1].
In a trust graph, vertices are individual agents, and the weight of an edge (vi, vj ) corresponds to the last claim that vi has made regarding its direct trust in agent vj (see Figure 1(a) for a simple example).
To simplify notation we sometimes use i, j directly instead of vi, vj.
A trust graph is constructed to correspond to agent reports as follows: for each vertex vi, given report ( ˆ Vi, ˆ ti), create a directed edge (vi, vj ) ∈ E for each vj ∈ ˆ Vi and define w(vi, vj ) = ˆ ti(vj).
If agent vi has reported truthfully, we call the corresponding trust graph a vi-truthful trust graph.
If all agents have reported truthfully, we call the corresponding trust graph a truthful trust graph.DEFINITION 4 (TRANSITIVE-TRUST MECHANISM).
Let GV denote the set of trust graphs G = (V, E, w) on V .
A transitivetrust mechanism M is a function that for every set of agents V and for every individual agent vi ∈ V maps GV to a vector of trust scores for all other agents vj ∈ V, vj 񮽙 = vi.
More formally: G, vi) denotes the trust score assigned to agent vj from the perspective of vi.
We let M (G, vi) denote the vector of all trust scores from agent vi's perspective.M : GV × V → [0, 1] n−1 .
Each Mj (This allows for personalized trust mechanisms where the trust score assigned to some agent vj depends on which agent's perspective vi 񮽙 = vj is adopted.
This makes sense for environments where it is reasonable to expect that I trust my own direct experiences more than the reported experiences of other agents.Ultimately, we care about the overall system efficiency.
In the experimental section, we measure the efficiency of a trust mechanism as the fraction of transactions by non-strategic agents that are successful.
This obviously depends on the strategyproofness and informativeness properties of the mechanism.
The informativeness of a mechanism is formally defined in Section 4.
Following earlier work, we consider two different classes of manipulations by strategic agents.DEFINITION 5 (MISREPORT).
Given trust graph G = (V, E, w), define the set E−v = {(x, y) : (x, y) ∈ E, x 񮽙 = v} (i.e., the set of all edges in G that do not start at v).
A misreport strategy for agent v ∈ V is a tuple σ = (Ev, wv) where Ev = {(v, u) : u ∈ V } and wv : Ev → [0, 1].
Applying the strategy σ to G results in trust graph G ↓ σ = G ′ = (V, E−v ∪ Ev, w ′ )where w ′ (e) = w(e) for all e ∈ E−v, and w ′ (e ′ ) = wv(e ′ ) for all e ′ ∈ Ev.Please see Figure 1(b) for an example of a misreport attack.
DEFINITION 6 (SYBIL MANIPULATION).
Given a trust graph G = (V, E, w), a sybil manipulation for agent v ∈ V is a tuple σ = (S, ES, wS) where S = {s1, ..., sm} is a set of sybil agents, ES is a set of edges ES = {(x, y) : x ∈ S ∪ {v}, y ∈ V ∪ S}, and wS : ES → [0, 1] are the weights on the edges in ES.
Applying the sybil manipulation σ to G results in a modified trust graphG ↓ σ = G ′ = (V ∪ S, E ∪ ES, w ′ ), where w ′ (e) = w(e) for e ∈ E, and w ′ (e ′ ) = wS(e ′ ) for e ′ ∈ ES.A sybil manipulation (introduced by Cheng and Friedman [6]) involves the creation of multiple fake nodes and associated fake edges in the trust graph.
Figure 1 (c) shows an example of a sybil manipulation.
Note that in general, an agent can manipulate a trust mechanism via a combination of misreports and sybil manipulations.
For these combinations, G ↓ σ is defined analogously.We can now define appropriate concepts of strategyproofness.
We use two different concepts, similar to the ones introduced in Cheng and Friedman [6].
The first one, rank-strategyproofness, compares the relative trust scores of agents.
The second one, valuestrategyproofness, considers an agent's absolute trust score.
DEFINITION 7 (RANK-STRATEGYPROOF).
A transitive-trust mechanism is rank-strategyproof if for any vi-truthful trust graph G = (V, E, w) where vi ∈ V , and for every strategy σ by node vis.t.
G ↓ σ = G ′ , for all vj 񮽙 = vi, for all v k 񮽙 = vi : Mi(G, vj ) < M k (G, vj) ⇒ Mi(G ′ , vj ) < M k (G ′ , vj), i.e., an agent cannot increase its position in a rank-order from the perspective of any such agent vj 񮽙 = vi.
DEFINITION 8 (VALUE-STRATEGYPROOF).
A transitive-trust mechanism is value-strategyproof if for any vi-truthful trust graph G = (V, E, w) with vi ∈ V , and for every strategy σ by node vis.t.
G ↓ σ = G ′ , for all vj 񮽙 = vi: Mi(G, vj ) ≥ Mi(G ′ , vj ), i.e., an agent cannot increase its absolute trust score from the perspective of any agent vj 񮽙 = vi.Rank-strategyproofness is appropriate, for example, when an agent can choose from a list of trading partners and only the relative trust scores are important to identify the most trustworthy agent.
Value-strategyproofness is appropriate, for example, when agents adopt a threshold approach in deciding which other agents to transact with; e.g., any agent with a trust score above some threshold may be acceptable.
It is easy to show that neither of these concepts dominates one another.
For many applications, however, rank-strategyproofness seems a more natural requirement, but it is also harder to achieve.
We now review four transitive-trust mechanisms that have been introduced in this form or very similarly before.
The trust scores produced by the mechanisms are normalized to be in [0,1].
[10]).
Given a trust graph G = (V, E, w), PageRank conducts a random walk from a random node vi ∈ V that at each step, with probability λ (for λ ∈ [0, 1)) follows a random outlink with probability proportional to weight w(vi, vj ), as a fraction of the total weight on all outlinks, and with probability 1 − λ jumps to another node with uniform probability.
If the random walk reaches a node with no outgoing links then PageRank randomly jumps to another node in the trust graph with uniform probability.
The trust score Mj (G, vi) = π(G, vj) of a node vj is the same, irrespective of vi, and is given by the probability π(G, vj ) of being in node vj in the stationary distribution of the Markov process described by the random walk.DEFINITION 9 (PAGERANKSome mechanisms use pre-trusted nodes in their algorithms.
This is reasonable for many domains, e.g., in P2P networks the administrator of the mechanism might own some trusted servers.
DEFINITION 10 (HITTINGTIME [13]).
Given a trust graph G, the hitting time of a node vj , H(vj ), is the number of steps before a random walk on G first reaches vj .
A hitting time trust mechanism has a set of pre-trusted nodes, and after each time step, the random walk jumps back to one of the pre-trusted nodes with some probability λ.
The random variable J denotes the number of time steps before the random walk performs a jump.
The trust score of node vj is the probability that the random walk reaches v before jumping, i.e., ∀i : Mj (G, vi) = P r(H(vj) < J).
DEFINITION 11 (MAXFLOW MECHANISM [6]).
Given a trust graph G = (V, E, w) and nodes vi, vj ∈ V , let MF (vi, vj ) denote the maximum flow from node vi to node vj .
The max-flow transitive-trust mechanism sets Mj(G, vi) = MF (vi, vj).
Given a trust graph G = (V, E, w), define the trust graphG ′ = (V, E, w ′ ) with w ′ (i, j) = 1 w(i,j), i.e., all edge weights are flipped such that low trust scores lead to high edge weights in G ′ .
Now, let SP G ′ (vi, vj) denote the length of the shortest path between agents vi and vj in G ′ .
The shortest-path mechanism setsMj (G, vi) = 1 SP G ′ (v i ,v j ) .
Each of theses mechanisms makes a distinct tradeoff between informativeness and strategyproofness.
Previous research has already established their strategyproofness properties: ShortestPath is best being rank-strategyproof and value-strategyproof; MaxFlow and HittingTime are both value-strategyproof, and finally PageRank is last with no formal strategyproofness properties (see Table 1).
We investigate the informativeness properties of all four mechanisms in Section 4.
We find that the order of the mechanisms with respect to informativeness is roughly reversed.
This makes intuitive sense: the more information a mechanism ignores when computing trust scores, the better its strategyproofness properties but the worse its informativeness properties.
This illustrates the trade-off we make when designing trust mechanisms.
We now introduce the idea of a hybrid transitive-trust mechanism, which is defined as a linear combination of two mechanisms.
Given transitive-trust mechanisms M 1 and M 2 , we let M α (M 1 , M 2 ) denote the α-hybrid of those mechanisms.
Given a trust graph G = (V, E, w) and vi, vj ∈ V , let M 1 j (G, vi) denote the trust value of vj from vi's perspective under M 1 , and let M 2 j (G, vi) denote the trust value of vj from vi's perspective under M 2 .
The reputation of vj from vi's perspective underM α (M 1 , M 2 ) is M α j (G, vi) = (1 − α)M 1 j (G, vi) + αM 2 j (G, vi).
For a hybrid mechanism Mα(M 1 , M 2 ) we will by convention always combine two mechanisms in which M 1 is more strategyproof than M 2 .
Often times, but not always, M 2 will be more informative than M 1 .
Thus, as α is increased from 0 to 1, the opportunities for manipulation increase, but we also expect the mechanism to become more informative, at least when no strategic agents are present.
We will look for non-trivial hybrids (with 0 < α < 1) that have better efficiency than either extreme mechanism.
LEMMA 1.
If mechanisms M 1 and M 2 are value- strategyproof, then M α (M 1 , M 2 ) is value-strategyproof.
PROOF.
If M 1 and M 2 are both value-strategyproof, then for any vi-truthful trust graph G = (V, E, w) with vi ∈ V , for every strategy σ by node vi s.t.G ↓ σ = G ′ , for all vj 񮽙 = vi, we have M 1 i (G, vj ) ≥ M 1 i (G ′ , vj ) and M 2 i (G, vj ) ≥ M 2 i (G ′ , vj ).
Thus, it follows that (1 − α)M 1 i (G, vj ) + αM 2 i (G, vj ) ≥ (1 − α)M 1 i (G ′ , vj ) + αM 2 i (G ′ , vj ), for any α ∈ [0, 1].
Unfortunately this does not hold true for the property of rankstrategyproofness.LEMMA 2.
If mechanisms M 1 and M 2 are rank-strategyproof, then M α (M 1 , M 2 ) is not necessarily rank-strategyproof.
PROOF.
By counterexample.
Assume a truthful trust graph with two agents 1 and 2 and with only one edge from agent 1 to agent 2.
M 1 always assigns a trust score of 1 to agent 2 and a trust score of 0.2 to agent 1 (and all other agents).
M 1 is trivially rankstrategyproof.
M 2 always assigns a trust score of 1 to agent 1, and assigns trust score 0.5 to agent 2 if an edge exists from agent 1 to agent 2 and trust score 0 otherwise.
M 2 is rank-strategyproof because agent 1 is always the highest-ranked agent, and agent 2 cannot affect the final ranking.
Now, for α = 0.5, agent 1 has trust value 0.6 while agent 2 has trust value 0.75.
If agent 1 now removes the link to agent 2, then agent 2's trust value is lowered to 0.5, and agent 1 becomes ranked higher than agent 2, thus proving thatM α (M 1 , M 2 ) is not rank-strategyproof.
For the design of hybrid mechanisms, we adopt relaxed notions of strategyproofness (similar to concepts adopted by [2]).
DEFINITION 14 (ε-VALUE-STRATEGYPROOFNESS).
A tran-sitive-trust mechanism is ε-value-strategyproof for ε > 0 if for any vi-truthful trust graph G = (V, E, w) with vi ∈ V and for all manipulation strategies σ for vi givingG ′ = G ↓ σ, for all vj 񮽙 = vi, Mi(G, vj ) + ε ≥ Mi(G ′ , vj ).
DEFINITION 15 (ε-RANK-STRATEGYPROOFNESS).
A transitive-trust mechanism is ε-rank-strategyproof for ε > 0 if for any vi-truthful trust graph G = (V, E, w) with vi ∈ V and for all manipulation strategies σ for vi s.t.G ′ = G ↓ σ, for all vj 񮽙 = vi, v k ∈ V , Mi(G, vj ) + ε ≤ M k (G, vj ) ⇒ Mi(G ′ , vj ) ≤ M k (G ′ , vj ).
In words, an ε-value-strategyproof mechanism is one in which an agent cannot increase its trust score by more than ε under any manipulation strategy and for any trust graph.
An ε-rank-strategyproof mechanism is one in which an agent cannot overcome more than a difference of ε in trust scores between itself and any other agent, whatever the trust graph and for any manipulation strategy.
THEOREM 1.
If transitive-trust mechanisms M 1 and M 2 are ε1 and ε2-value-strategyproof respectively, thenM α (M 1 , M 2 ) is (1 − α)ε1 + αε2 -value-strategyproof.
PROOF.
Let M 1 i , M 2 i denote the trust scores of vi (as viewed by some other agent) under mechanisms M 1 and M 2 when vi is truthful.
LetM α i = (1 − α)M 1 i + αM 2 i .
Let M α i , M 1 i and M 2 idenote the trust scores after vi has performed manipulations.
Then:M α i − M α i = (1 − α)(M 1 i − M 1 i ) + α(M 2 i − M 2 i ) ≤ (1 − α)ε1 + αε2,and we see that M α is(1 − α)ε1 + αε2 -value-strategyproof.
We can now prove a corresponding corollary for specific hybrid trust mechanisms:COROLLARY 1.
Hybrid mechanism M α (Hitting, PageRank) is 0.5α-value-strategyproof.
HittingTime mechanism is valuestrategyproof [13].
Moreover, Bianchini et al. [4] establish that PageRank is 0.5-value-strategyproof.
By Theorem 1, we have that M α (Hitting, PageRank) is 0.5α-value-strategyproof.
COROLLARY 2.
Hybrid mechanism M α (Max-Flow, PageRank) is 0.5α-value-strategyproof.
COROLLARY 3.
Hybrid mechanism M α (Shortest, PageRank) is 0.5α-value-strategyproof.
PROOF.
Max-Flow and Shortest-Path are both valuestrategyproof and thus Corollaries 2 and 3 also follow from Theorem 1.
Establishing rank-strategyproofness properties for hybrid transitive-trust mechanisms requires a more delicate argument.
For this, we introduce the following property: DEFINITION 16 (UPWARDS VALUE-PRESERVANCE).
A tran-sitive-trust mechanism is upwards value-preserving if for any trust graph G = (V, E, w), for any vi ∈ V , for every strategy σ by node vi s.t.G ↓ σ = G ′ , for all vj 񮽙 = vi, for all v k 񮽙 = vi we have M k (G, vj ) > Mi(G, vj ) ⇒ M k (G ′ , vj ) ≥ M k (G, vj ).
This property requires that an agent cannot decrease the trust score of a higher ranked agent.
Note that the ShortestPath mechanism is easily seen to be upwards value-preserving: if vi has a lower trust score than v k from vj 's perspective, then the path from vj to v k is shorter than then path from vj to vi; thus, vi cannot be on the path between agents v k and vj , and therefore vi cannot affect v k 's trust score.
2 THEOREM 2.
If transitive-trust mechanisms M 1 and M 2 are value-strategyproof and M 1 satisfies upwards value-preservance, then M α (M 1 , M 2 ) is α-rank-strategyproofα i > M α j , i.e., (1 − α)M 1 i + αM 2 i > (1 − α)M 1 j + αM 2 j .
With this assumption, it impossible that bothM 1 i < M 1 j and M 2 i < M 2 j .
Thus, we only need to consider the following two cases:Case 1: M 1 i > M 1 j :Because M 1 and M 2 are both valuestrategyproof, agent vj cannot increase its own trust score, i.e.,M α j ≤ M α j .
Because M 1 is upwards value-preserving, agent vj also cannot decrease vi's trust score under M 1 .
However, agent vj can decrease agent vi's trust score under M 2 .
ButM 2 i − M 2 i ≤ 1 since M 2 i ≤ 1 and M 2 i ≥ 0.
So, we have that M α i − M α i ≤ α.Putting all these arguments together we get:M α i − M α j ≥ M α i − M α j ≥ M α i − α − M α j ≥ α − α = 0.
And thus, M α (M 1 , M 2 ) is α-rank-strategyproof in case 1.
Case 2: M 1 i < M 1 j and M 2 i > M 2 j : For α = 0 or α = 1 there is nothing to be shown.
For 0 < α < 1 we show that M α i − M α j ≥ α is impossible to begin with:M α i − M α j = αM 2 i + (1 − α)M 1 i − αM 2 j − (1 − α)M 1 j ≤ α + (1 − α)M 1 i − (1 − α)M 1 j = α − (1 − α)(M 1 j − M 1 i ) < α.
Thus, M α is α-rank-strategyproof in case 2 as well.
COROLLARY 4.
Hybrid mechanism M α (Shortest, Max-Flow) is α-rank-strategyproof.
PROOF.
ShortestPath and MaxFlow are value-strategyproof [3,7].
Moreover, ShortestPath is upwards value-preserving.
Thus, the corollary follows from Theorem 2.
In this section we analyze the informativeness of the four existing trust mechanisms as well as our new hybrids.
A trust mechanism shall help agents to discriminate good from bad trading partners.
Similar to ideas by Bolton et al. [5], we call a mechanism informative if it discriminates well between good and bad agents, and non-informative if it doesn't.
A perfectly informative mechanism would be one that is perfectly discriminative in the sense that it has a strictly monotonic relationship between the trust scores Mj (G, vi) and the true agent types θj.
With limited information, no mechanism can be perfectly informative and thus we want to measure how close our mechanism comes to this goal.
The correlation between the true agent types and the trust scores a mechanism produces tells us how discriminative the mechanism is.
A random mechanism results in a correlation of 0.
Assuming a linear relationship, a perfectly discriminative mechanism would result in a correlation of 1.
Thus, all mechanisms that perform better than random have informativeness values between 0 and 1.
We define the informativeness of a mechanism M on graph G as the correlation between the true agent types and the trust scores produced by mechanism M .
More formally, we offer the following natural definition: DEFINITION 17 (INFORMATIVENESS).
Let Θ−i denote the (n − 1)-dimensional vector of all agents' types except for agent i. Let Θ n − = Θ−1, Θ−2, ..., Θ−n denote the vector resulting from combining all Θ−i vectors to a vector of dimension (n−1) n .
Given a trust graph G = (V, E, w), and transitive-trust mechanism M , let M (G) denote the (n − 1) n -dimensional vector of all agents' trust scores from all other agents' perspectives produced by M , i.e., M (G) = M (G, v1), M (G, v2), M (G, v3), ..., M (G, vn).
We define the informativeness of mechanism M on graph G as:Inf (M, G) = correlation(Θ n − , M (G)) = n i=1 j񮽙 =i (Mj(G, vi) − ˜ M )(θj − ˜ θ) (n(n − 1) − 1)sM s θ ,where˜Mwhere˜ where˜M and˜θand˜ and˜θ are the sample means of the trust scores and the agent types; sM and s θ are the sample standard deviations.
It is apparent from the definition that the informativeness of a mechanism is defined with respect to a particular trust graph G. Thus, to perform an informativeness measurement, we first have to specify how G is generated in our experiments.
In this section, we focus on a mechanism's ability to aggregate data and do not consider its strategyproofness.
Thus, we will not consider strategic agents.
Also, we want to measure informativeness independent from how the trust scores are being used by the agents when making decisions in the environment.
Thus, we start our analysis with an artificial experiment where a random trust graph is constructed according to the following process.We simulate a multi-agent system with 50 agents.
Each agent's type θi is chosen uniformly at random from [0,1].
We limit the number of interaction partners for each agent, which in real networks is small relative to the total graph size.
We use parameter κ to denote the size of the "interaction set" for each agent.
At the beginning of the experiment, each agent's interaction set is chosen randomly.
We let our simulation run for τ time steps.
At each time step, each agent i picks a random partner agent j from its interaction set.
The outcome of the interaction between i and j is good with probability θj and bad with probability 1 − θj.
Every agent keeps track of the total number of interactions and the number of successful interactions with each partner agent.
At the end of each time step, for each agent i, we set the edge weight of edge (i, j) equal to the fraction of successful interactions i had with j divided by the total number of interactions i had with j.
After τ time steps, we stop the interactive part of the experiment and consider the resulting trust graph G as the basis for the analysis.
For each mechanism M that we consider, for each agent i and each agent j 񮽙 = i, we compute the trust scores Mj(G, vi).
We then calculate the informativeness metric, i.e., the correlation between the true agent types and the trust scores computed by the mechanisms.
3 The informativeness metric is sensitive to the parameters of the trust graph generation process, in particular to the number of time steps, τ , and to the size of the interaciton sets, κ.
In Figure 2 we present two graphs that show some patterns that are representative for our experiments without strategic agents.
For both graphs, we plot the log of the number of time steps on the x-axis, and the informativeness scores on the y-axis.
Figure 2(a) shows results for κ = 5 and Figure 2(b) shows results for κ = 50, i.e., here each agent could interact with each other agent in the system.
The legend on Figure 2(a) holds for both graphs.We see immediately that as the number of time steps increases, the informativeness scores increase for all mechanisms.
This is expected because over time each agent gets better and better information about the type of each agent in its interaction set.
Note that the last data points in both graphs correspond to an infinite number of time steps which we simulated by setting the edge weights equal to the true agents' types.
It is interesting to note that all mechanisms, except for MaxFlow, reach informativeness of 1 when κ = 50 and τ = ∞.
However, for practical purposes this is less relevant, because we generally only have little information available.In both graphs, we clearly see that the shortest path mechanism performs worst (except when τ = ∞).
This is expected and nicely illustrates the trade-off between informativeness and strat- egyproofness.
The order of the other basic mechanisms is much less clear.
In general, PageRank and HittingTime are very close together, which makes sense given that both mechanisms use similar algorithms to compute trust scores.
The MaxFlow mechanism shows the largest variation in informativeness and is particularly sensitive to κ, the size of the interaction set.
In Figure 2(a) where κ = 5, MaxFlow has the highest informativeness, while in Figure 2(b) where κ = 50, it has the second lowest informativeness.
To explore this effect, we ran additional experiments for more values of κ (not shown here).
It turns out that an interesting cross-over effect happens at κ = 10: for κ ≤ 10, the MaxFlow algorithms has informativeness as good as or better than PageRank and HittingTime, for κ ≥ 15, MaxFlow has informativeness significantly worse than PageRank and HittingTime.
We now analyze the informativeness of two hybrid mechanisms: M α (Shortest, Hitting) and M α (Shortest,PageRank).
We use these hybrids because the trade-off is clear in this case: shortest path has the best strategyproofness properties but the worst informativeness properties.
We have shown analytically in the last section that the hybrids have intermediate strategyproofness properties and we expected the same result for informativeness.Thus, it is perhaps surprising that, for many settings, the hybrids perform as well with respect to informativeness, or even better, than HittingTime or PageRank.
In Figure 2(a), we see that M α (Shortest,PageRank) has informativeness scores that are as good or even higher than those of PageRank, and M α (Shortest, Hitting) has scores that are consistently higher than those of HittingTime.
In contrast, in Figure 2(b), we see that both hybrids have intermediate informativeness, i.e., the informativeness scores of M α (Shortest, Hitting) lie between those of ShortestPath and HittingTime, and the scores of M α (Shortest,PageRank) lie between those of ShortestPath and PageRank.
Further analysis (data not shown here) shows that another interesting cross-over effect happens: for large values of κ, both hybrids have intermediate informativeness as we expected.
But for small values of κ, the informativeness of the hybrids is as good or even better than that of HittingTime or PageRank respectively.
At first sight, it is counterintuitive that a hybrid mechanism could have informativeness even higher than any of its component mechanisms.
A possible explanation is that both component mechanisms measure different aspects of the trust graph, and the hybrid benefits from both perspectives, i.e., both sources of information.
A deeper analysis of this effect will be interesting to conduct in future research.
In this section we analyze the efficiency of hybrid mechanisms.
We would like to investigate whether hybrids with intermediate informativeness and intermediate strategyproofness properties can achieve higher performance than any of the "pure" mechanisms.
We measure the efficiency of a trust mechanism as the the fraction of transactions by non-strategic agents that are successful.
Note this is no longer independent of how agents use trust scores for acting in their environment.
We consider two simulated domains: (1) combatting the spread of bad files (e.g., viruses or trojans) in a filesharing network (2) ranking website quality based on link structure.
As before, we use 50 simulated agents.
Agents are divided into cooperative and malicious agents: cooperative agents have type θi = 0.95, while malicious agents have types drawn uniformly at random from [0, 0.5].
A subset γ of the malicious agents are also strategic, i.e, they also consider manipulating the trust mechanism to their benefit.
4 We let γ denote the fraction of the total agent population that is strategic.
Properly simulating the behavior of strategic agents is difficult.
We model strategic behavior by assuming a heterogenous fixed cost for manipulation (e.g., some agents are more adept than others at hacking the P2P filesharing software).
As α increases, the manipulability of the mechanism increases linearly with α, leading to higher rewards for manipulating agents.
Since agents will only manipulate if the benefit exceeds their cost, we assume that the percentage of manipulating agents increases linearly with α.
For a manipulating agent, we determine in each context the optimal "attack" on the trust mechanism.Virus Distribution Experiment: Imagine a file-sharing network with good and malicious agents.
Malicious agents have bad files that are infected by viruses.
A trust mechanism helps to separate users with good files from users with bad files.
In our experiments we use 100 simulated agents, of which 80% are malicious.
We vary γ, i.e., the proportion of strategic agents, between 0 and 0.8.
In P2P filesharing settings, the total number of agents in the system is too large for any single agent to track all interactions.
We model this by limiting the number of maximum outgoing edges of all agents in the trust graph by κ = 3.
This "memory set" is uniformly randomly selected for each agent at the beginning of the simulation.
We initialize the system by constructing a sparse trust graph.
Each agent randomly chooses another agent j from its memory set, and lays down an edge with weight 1 to j with probability θj.
We repeat this process until each agent has exactly one outgoing edge.
We then start the experiment itself and run it for 100 time steps.
Each time step, agent i obtains a set of three randomly selected agents drawn from the entire set of agents.
With probability 0.9, the agent uses the trust mechanism to select agent j with the highest trust score; with probability 0.1 the agent simply selects a random agent.
This ǫ-greedy selection policy encourages agents to explore and discover agents outside their memory set.
Once j is selected, with probability θj , agent j sends a good file (otherwise it sends a bad file).
After the interaction has taken place, agent i makes a report to the trust mechanism, updating the weight of its edge to agent j to be the fraction of successful transactions over the total number of transactions.Strategic agents in this setting only employ misreport strategies because Mα(MShortest, MHitting) is robust against sybil manipulations.
By cutting all their outlinks they do not affect their own trust scores, but could lower the trust scores of agents ranked above them, thus improving their relative rank.Website Ranking Experiment: This experiment uses a trust mechanism to rank websites according to their quality, helping web surfers differentiate between high quality and low quality websites.
We assume that the set of surfers and the set of website owners coincides, i.e., each surfer has one pre-trusted website.
We have 80% malicious agents (low quality websites) and we vary γ, the proportion of strategic agents (website owners), between 0 and 0.8.
We limit each agent to interacting with a randomly chosen memory set of size κ = 5.
For each agent, for 10 time steps, we sample from that agent's memory set and update the edge weights according to an outcome of a simulated transaction.
Strategic agents (website ownders) employ the misreport manipulation as well as a sybil manipulation (5 sybils) in the optimal star-shaped pattern [4].
We leave the trust graph unchanged over the duration of the experiment.
We run the experiment for 100 time steps.
At each timestep, each surfer is provided with five randomly selected websites and considers their trust scores.
We use the threshold-based selection rule: the surfer is willling to visit any website that has a trust score higher than a certain threshold (which we set to the median trust score across all agents).
In Figure 3 we present efficiency and informativeness results (averaged over 10 trial runs) for the virus distribution experiment.
In Figure 3(a), we plot the informativeness of the mechanisms on the y-axis, this time varying the maximum out-degree κ on the xaxis.
We see that the overall pattern is similar to the one we have described in Section 4.
The ShortestPath mechanism has lowest informativeness and MaxFlow has highest informativeness.
The mechanisms HittingTime and PageRank are close together and are slightly less informative than MaxFlow.
We also see that the two hybrids M α (Shortest, Hitting) and M α (Shortest, PageRank) have intermediate informativeness.Consider now Figure 3(b), where we plot overall efficiency on the y-axis and vary the maximum out-degree κ on the x-axis.
We see that the ordering of the mechanisms is the same as in Figure 3(a), except for the MaxFlow mechanism, which on average performs slightly worse than HittingTime and PageRank, even though it had better informativeness.
Thus, without strategic agents and with the exception of MaxFlow, the informativeness of a mechanism seems to be a very good predictor of its efficiency.
We have already seen in Section 4 that the MaxFlow mechanism is very sensitive to various parameter settings.
A more detailed analysis of the properties of MaxFlow is subject to our ongoing research.
We now analyze the efficiency of our hybrid mechanisms in the presence of strategic agents.
In Figure 4(a) we display the results for the virus distribution experiment, and in Figure 4(b) the results for the WebRank experiment.
On the x-axis we plot the mixing factor α ∈ [0, 1] and on the y-axis we plot efficiency.We see that with 0% strategic agents, efficiency increases almost monotonically as we move from ShortestPath to HittingTime or PageRank.
This is expected because ShortestPath is a very uninformative mechanism and without strategic agents has no benefits over the other mechanisms.
However, the situation is different when strategic agents are present, i.e., for γ ≥ 0.2.
Now we see that for α-values towards 1, the efficiency decreases significantly.
This is also expected because HittingTime and PageRank are both suscep- tible to the manipulations performed by strategic agents and thus, the more weight we give those mechanisms, the more successful are the strategic agents at manipulating the hybrids.
5 The most important finding from these experiments is that initially, the efficiency goes up as we increase α and the efficiency peak in both cases does not occur for one of the base mechanisms.
Instead, the efficiency peak in Figure 4(a) is around α = 0.5 with a relative efficiency increase up to 2%.
In Figure 4 the peak is around α = 0.02 with a relative efficiency increase up to 7%.
Thus, when strategic agents are present, the optimal hybrid mechanisms achieve higher overall efficiency than either of the component mechanisms.
In this paper, we have introduced hybrid transitive-trust mechanisms, which allow for a continuum of design tradeoffs between existing point solutions in the literature.
We have shown analytically that these hybrids have intermediate strategyproofness properties.
We have presented a simple metric to measure informativeness of trust mechanisms and via simulations we found that hybrid mechanisms have intermediate or sometimes even better informativeness than any of their component mechanisms.
Finally, we have performed efficiency experiments to study the overall effect of using hybrid mechanisms.
Our experimental results suggest that in some domains it is possible to improve efficiency by blending together two mechanisms, making a tradeoff between informativeness and strategyproofness that is optimal for a given population of agents.
Note that the optimal α depends on the agent population and how costly it is for strategic agents to actually manipulate the mechanism.
Our current experimental methodology is deliberately simplistic: as we increase blend parameter α from 0 to 1, we also increase the fraction of strategic agents that choose to manipulate.
This models a simple cost-benefit tradeoff.
As a next step, we will 5 Note that in Figure 4(b), for γ = 0.6 and γ = 0.8, the efficiency increases again as we move from α = 0.9 to α = 1.
This happens because at α = 0.9, the strategic agents affect the hybrid twice, via ShortestPath and via PageRank.
As we have seen in Figure 2, ShortestPath is particularly bad when it has little information.
For α = 0.9, the strategic agents cannot influence their trust scores under ShortestPath, but the mechanism still stuffers significantly from the missing information due to many misreport attacks.
Close to α = 1, ShortestPath loses effect, and as we have seen in Figure 2, PageRank is significantly better at coping with little information in the trust graph which explains the efficiency increase at the end.
instead assume a model in which this cost-benefit analysis is made explicit.
In future work we will also consider the computational requirements of the trust mechanisms.
For practical applications, informativeness and strategyproofness are important, but in any case it must be feasible to run the mechanisms on real-sized graphs.
