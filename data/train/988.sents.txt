While a number of User-Level Protocols have been developed to reduce the gap between the performance capabilities of the physical network and the performance actually available, applications that have already been developed on kernel based protocols such as TCP have largely been ignored.
There is a need to make these existing TCP applications take advantage of the modern user-level protocols such as EMP or VIA which feature both low-latency and high bandwidth.
In this paper, we have designed, implemented and evaluated a scheme to support such applications written using the sockets API to run over EMP without any changes to the application itself.
Using this scheme, we are able to achieve a latency of 28.5 ¤ s for the Data-gram sockets and 37 ¤ s for Data Streaming sockets compared to a latency of 120 ¤ s obtained by TCP for 4-byte messages.
This scheme attains a peak bandwidth of around 840 Mbps.
Both the latency and the throughput numbers are close to those achievable by EMP.
The ftp application shows twice as much benefit on our sockets interface while the web server application shows up to six times performance enhancement as compared to TCP.
To the best of our knowledge, this is the first such design and implementation for Gigabit Ethernet.
Networks of Workstations (NOWs) have been accepted as a viable alternative to mainstream supercomputing for a broad subset of computation intensive applications.
Much of the success of these NOWs lies in the use of commodity based components, giving a high ratio of performance to cost to the end users.
With the advent of modern high speed interconnects such as Myrinet [4], Gigabit Ethernet [7] and Quadrics [13], the bottleneck in the communication ¥ This research is supported by Sandia National Labs (contract number 12652 dated 31 Aug) and NSF grant # EIA-9986052 has shifted to the messaging software at the sending and the receiving side.Earlier generation protocols relied upon the kernel for processing the messages.
This caused multiple copies and many context switches [17].
Thus, the communication latency was high.
Researchers have been looking at the alternatives by which one could increase the communication performance delivered by the NOWs in the form of lowlatency and high-bandwidth user-level protocols such as FM [12] for Myrinet [4], U-Net [18] for ATM and Fast Ethernet, GM [5] for Myrinet, and others.In the past few years, several industries have taken up the initiative to standardize high-performance user-level protocols such as the Virtual Interface Architecture (VIA) [8,3,10].
It has also led to the development of the InfiniBand Architecture (IBA) [1].
These developments are minimizing the gap between the performance capabilities of the physical network and that obtained by the end users.One such physical network which is of particular interest is Gigabit Ethernet [7] as most of world's networks today use Ethernet.
Gigabit Ethernet offers an excellent opportunity to build Gbps networks over the existing Ethernet installation base due to its backward compatibility with Ethernet.
However, the user applications have not been able to take advantage of the high performance of Gigabit Ethernet because a vast majority of them still use the sockets interface, which has traditionally been implemented on kernelbased protocols like TCP and UDP.One way to get around this problem would be to develop a very low overhead user-level protocol similar to VIA over Gigabit Ethernet.
This motivated us towards the development of Ethernet Message Passing (EMP) protocol [16,15], using which the applications can take full advantage of the bandwidth offered by Gigabit Ethernet with minimum latency.
While this approach is good for writing new applications, it might not be so beneficial for the already existing socket applications which were developed over a span of several years.Sockets is a generalized library which can be implemented over numerous protocols.
In this paper, we take on a challenge of developing a low overhead, user-level sockets interface on Gigabit Ethernet which uses EMP as the underlying protocol.
There is no exact parallel between EMP and TCP or UDP.
We analyze the semantic mismatches between the two protocols like connection management and unexpected message arrival to name a few.
To capture these differences, we suggest various approaches for two commonly used options with sockets, namely data streaming and datagram.
Finally, we suggest several performance enhancement techniques while providing these options and analyze each of them in detail.Using our approach one will be able to transport the benefits of Gigabit Ethernet to the existing sockets application without necessitating changes in the user application itself.
Our sockets interface is able to achieve a latency of 28.5 ¤ s for the Datagram sockets and 37 ¤ s for Data Streaming sockets compared to a latency of 120 ¤ s obtained by TCP for 4-byte messages.
We also attained a peak bandwidth of around 840 Mbps using our interface.
In addition we tested our implementation on real applications like ftp and web server.
For ftp we got almost twice the performance benefit as TCP while the web server application showed as much as six times performance enhancement.
In the past few years, a large number of user-level protocols have been developed to reduce the gap between the performance capabilities of the physical network and that achievable by an application programmer.
The Ethernet Message Passing (EMP) protocol specifications [16,15] have been developed at The Ohio Supercomputing Center and The Ohio State University to fully exploit the benefits of Gigabit Ethernet.EMP is a complete zero-copy, OS-bypass, NIC-level messaging system for Gigabit Ethernet (Figure 1).
This is the first protocol of its kind on Gigabit Ethernet.
It has been implemented on a Gigabit Ethernet network interface chipset based around a general purpose embedded microprocessor design called the Tigon2 [11] (produced by Alteon Web Systems, now owned by Nortel Networks).
This is a fully programmable NIC, whose novelty lies in its two CPUs.In EMP, message transmission follows a sequence of steps (Figure 1).
First the host posts a transmit descriptor to the NIC (T1), which contains the location and length of the message in the host address space, destination node, and an MPI [6] specified tag.
Once the NIC gets this information (T2-T3), it DMAs this message from the host (T4-T5), one frame at a time, and sends the frames on the network.
Message reception follows a similar sequence of steps (R1-R6) with the difference that the target memory location in the host for incoming messages is determined by performing tag matching at the NIC (R4).
Both the source index of the sender and an arbitrary user-provided 16-bit tag are used by the NIC to perform this matching, which allows EMP to make progress on all messages without host intervention.
EMP is a reliable protocol.
This mandates that for each message being sent, a transmission record be maintained (T3).
This record keeps track of the state of the message including the number of frames sent, a pointer to the host data, the sent frames, the acknowledged frames, the message recipient and so on.EMP is a zero-copy protocol as there is no buffering of the message at either the NIC or the host, in both the send and receive operations.
It is OS bypass in that the kernel is not involved in the bulk of the operations.
However, to ensure correctness, each transmit or receive descriptor post must make a call to the operating system for two reasons.
First, the NIC accesses host memory using physical addresses, unlike the virtual addresses which are used by application programs.
Only the operating system can make this translation.
Second, the pages to be accessed by the NIC must be pinned in physical memory to protect against the corruption that would occur if the NIC wrote to a physical address which no longer contained the application page due to kernel paging activity.
We do both operations in a single system call (T2/R2).
One of the main features of this protocol is that it is a complete NIC based implementation.
This gives maximum benefit to the host in terms of not just bandwidth and latency but also CPU utilization.
The traditional communication architecture involves just the application and the libraries in user space, while protocol implementations such as TCP/UDP, IP, etc reside in kernel space [9] (Figure 2a).
This approach not only entails multiple copies for each message, but also requires a context switch to the kernel for every communication step, thus adding a significant overhead.
Most of the current NIC drivers, including the standard Acenic driver on Alteon NICs, use this style of architecture.
The LANE driver supplied by GigaNet for its cLAN adapters used a simple approach.
They provide a IP-to-VI layer which maps IP communications onto VI NICs ( Figure 2b).
However, TCP is still required for reliable communications, multiple copies are necessary, and the entire setup is in the kernel as with the traditional architecture outlined in Figure 2a.
Although this approach gives us the compatibility, it does not give any performance improvement.Some other socket implementations over VIA [9,14] take good advantage of the user-level protocol.
But, the motivation for our work is to provide a high performance sockets layer over Gigabit Ethernet given the advantages associated with Gigabit Ethernet.
M-VIA [10], while providing a VIA interface over Gigabit Ethernet, is a kernel-based protocol and hence the current sockets interface over VIA will not be able to exploit the benefits of Gigabit Ethernet.
To the best of our knowledge EMP is the only complete OS-bypass, zero-copy and NIC-driven protocol over Gigabit Ethernet.
Thus, we focus our research on the EMP protocol.The solution proposed in this paper creates an intermediate layer which maps the sockets library onto EMP.
This layer ensures that no change is required to the application itself.
This intermediate layer will be referred to as the "EMP Substrate".
Figure 2c provides an overview of the proposed Sockets-over-EMP architecture.
While implementing the substrate to support sockets applications on EMP, we faced a number of challenges.
In this section, we mention a few of them, discuss the possible alternatives, the pros and cons of each of the alternatives and the justifications behind the solutions.
The mismatches between TCP and EMP are not limited to the syntax alone.
The motivation for developing TCP was to obtain a reliable, secure and fault tolerant protocol.
However, EMP was developed to obtain a low-overhead protocol to support high performance applications on Gigabit Ethernet.While developing the EMP substrate to support applications written using the sockets interface (on TCP and UDP), it must be kept in mind that the application was designed around the semantics of TCP.
We have identified the following significant mismatches in these two protocols and given solutions so as to maintain the semantics for each of the mismatches with regard to TCP.
More importantly, this has been done without compensating much on the performance given by EMP.
TCP is a connection based protocol, unlike EMP.
At first sight, this does not appear to be too much of a problem as the connection can always be assumed to be present.
However, by doing so, we overlook certain essential features of the connection requests.In TCP, when a connection request is sent to the server, it contains information about the client requesting the connection.
In this approach, this information is not available since there's no explicit message for the connection.In our solution, the client sends an explicit message to the server containing information about the client requesting the connection.
However, this puts an additional requirement on the substrate to post descriptors for the connection management messages too.
When the application calls the listen() call, the substrate posts a number of descriptors equal to the usual sockets parameter of a backlog which limits the number of connections that can be simultaneously waiting for an acceptance.
When the application calls accept(), the substrate blocks on the completion of the descriptor at the head of the backlog queue.
Like most other user-level protocols, EMP has a constraint that before a message arrives, a descriptor must have been posted so that the NIC knows where to DMA the arriving message.
However, EMP is a reliable protocol.
So, when a message arrives, if a descriptor is not posted, the message is dropped by the receiver and eventually retransmitted by the sender.
This facility relaxes the descriptor posting constraint to some extent.
However, allowing the nodes to retransmit packets indefinitely might congest the network and harm performance.
Posting a descriptor before the message arrives is not essential for the functionality, but is crucial for performance issues.
In our solution, we explicitly handle unexpected messages at the substrate, and avoid these retransmissions.
We examined three separate mechanisms to deal with this.Separate Communication Thread: In the first approach, we post a number of descriptors on the receiver side and have a separate communication thread which watches for descriptors being used and reposts them.
This approach was evaluated and found to be too costly.
With both threads polling, the synchronization cost of the threads themselves comes to about 20 ¤ s. Also, the effective percentage of CPU cycles the main thread can utilize would go down to about 50%, assuming equal priority threads.
In case of a blocking thread, the Operating System scheduling granularity makes the response time too coarse (order of milliseconds) for any performance benefit.Rendezvous Approach: The second approach (similar to the approach indicated by [9]) is through rendezvous communication with the receiver as shown in Figure 3.
Initially, the receive side posts a descriptor for a request message, not for a data message.
Once the sender sends the request, it blocks until it receives an acknowledgment.
The receiver on the other hand, checks for the request when it encounters a read() call, and posts two descriptors -one for the expected data message and the other for the next request, and sends back an acknowledgment to the sender.
The sender then sends the data message.Effectively, the sender is blocked till the receiver has synchronized and once this is done, it is allowed to send the actual data message.
This adds an additional synchronization cost in the latency.Though this approach is straight-forward, it has a few disadvantages.
One of the high points of TCP is its datastreaming option.
In this option, the message boundaries supplied by the transmitter are not enforced at the receiver.
When a message arrives, the receiving node has the option of reading any number of bytes at any time.
For example, if the sender sends 10 bytes of data, TCP allows the user to read it as two sets of 5 bytes each, potentially into different user buffers.
In EMP, when a message arrives, the data is directly transferred to the user space, and thus this option is disabled.Eager In Section 5.1, we have proposed an extension of this idea (with additional credits) to enhance its performance.The first solution, using a separate communication thread, was not found to give any significant benefit in performance.
However, the second and third approaches, namely the rendezvous and eager with flow control respectively, were found to give significant benefit in latency and bandwidth.
Both these approaches have been implemented in the substrate, giving the user an option of choosing either one of them.
Applications built using the sockets interface use a number of standard UNIX system calls including specialized ones such as listen(), accept() and connect(), and generic overloaded calls such as open(), read() and write().
The generic functions are used for a variety of external communication operations including local files, named pipes and other devices.
In the substrate, these calls were mapped to the corresponding EMP calls (sets of calls).
This mapping can be done in a number of ways.Function Overriding: In this approach, the TCP function calls are directly mapped to the corresponding EMP function calls by overriding them.
This approach works for calls such as listen(), which have just one interpretation.
But, for calls such as read() and write(), this approach does not work, as the read can be on a socket or on a file.
Overriding cannot distinguish between these two interpretations.Application changes: In this approach, minor changes are made to the application by adding a parameter which allows the substrate to distinguish between a call to the EMP library and one to the libc library.
This approach gives the flexibility of using both sockets over EMP as well as over TCP.
However, since the aim of the substrate was to avoid any changes to the application, this approach was not used.File descriptor tracking: In the approach used in our substrate, no changes are made to the application.
We cause our functions to be loaded into the application before the standard C library, and monitor library calls which change the state of file descriptors, including open(), close() and socket().
In this way, on a read() or write(), for instance, our functions can decide whether to call into the EMP substrate or to pass the parameters on to the standard system function of the same name.
While implementing the substrate, the functionality of the calls was taken into account so that the application does not have to suffer due to the changes.
However, these adjustments do affect the performance the substrate is able to deliver.
In order to improve the performance given by the substrate, we have come up with some techniques, which are summarized below.
More details on these techniques are are included in [2].
As mentioned earlier (Section 4.1.2), the scheme we have chosen for handling unexpected messages can be extended to enhance its performance.The sender is given a certain number of credits (tokens).
It loses a token for every message sent and gains a token for every acknowledgment received.
If the sender is given credits, the substrate has to make sure that there are enough descriptors and buffers pre-posted for unexpected message arrivals on the receiver side.
In this way, the substrate can tolerate up to outstanding write() calls before the corresponding read() for the first write() is called ( Figure 5).
One problem with applying this algorithm directly is that the acknowledgment messages also use up a descriptor and there is no way the receiver would know when it is reposted, unless the sender sends back another acknowledgment, thus forming a cycle.
To avoid this problem, we have proposed the following solutions:Blocking the send: In this approach, the write() call is blocked until an acknowledgment is received from the receiver, which would increase the time taken for a send to a round-trip latency.Piggy-back acknowledgment: In this approach, the acknowledgment is sent along with the next data message from the receiver node to the sender node.
This approach again requires synchronization between both the nodes.
Though this approach is used in the substrate when a message is available to be sent, we cannot always rely on this approach and need an explicit acknowledgment mechanism too.Post more descriptors: In this approach, ¡ ¢ number of descriptors are posted where is the number of credits given.
It can be proved that at any point of time, the number of unattended data and acknowledgment messages will not exceed 2 .
On the basis of the same, this approach was used in the substrate.
As mentioned earlier, TCP supports the data streaming option, which allows the user to read any number of bytes from the socket at any time (assuming that at-least so many bytes have been sent).
To support this option, we use a temporary buffer to contain the message as soon as it arrives and copy it into the user buffer as and when the read() call is called.
Thus, there would be an additional memory copy in this case.However, there are a number of applications which do not need this option.
To improve the performance of these applications, we have provided an option in the substrate which allows the user to disable this option.
In this case, we can avoid the memory copy for larger message sizes by switching to the rendezvous approach to synchronize with the receiver and DMA the message directly to the user buffer space.
This approach is referred to as Datagram sockets.
To improve performance, we delay the acknowledgments so that an acknowledgment message is sent only after half the credits have been used up, rather than after every message.
This reduces the overhead per byte transmitted and improves the overall throughput.These delayed acknowledgments bring about an improvement in the latency too.
When the number of credits given is small, half of the total descriptors posted are acknowledgment descriptors.
So, when the message arrives, the tag matching at the NIC takes extra time to walk through the list that includes all the acknowledgment descriptors.
This time was calculated to be about 550 ns per descriptor.
However, with the increase in the number of credits given, the fraction of acknowledgment descriptors decreases, and thus reducing the effect of the time required for tag matching.
EMP supports a facility for unexpected messages.
The user can post a certain number of unexpected queue descriptors, and when the message comes in, if a descriptor is not posted, the message is put in the unexpected queue and when the actual receive descriptor is posted, the data is copied from this temporary memory location to the user buffer.
The advantage of this unexpected queue is that the descriptors posted in this queue are the last to be checked during tag matching, which means that access to the more time-critical pre-posted descriptors is faster.The only disadvantage with this queue is the additional memory copy which occurs from the temporary buffer to the user buffer.
In our substrate, we use this unexpected queue to accommodate the acknowledgment buffers.
The memory copy cost is not a concern, since the acknowledgment messages do not carry data payload.
Further, there is the additional advantage of removing the acknowledgment messages from the critical path.These enhancements have been incorporated in the substrate and are found to give a significant improvement in the performance.
For the purpose of this paper, the experimental test-bed used included 4 Pentium III 700MHz Quads, each with a Cache Size of 1MB and 1GB main memory.
The interconnect was a Gigabit Ethernet network with Alteon NICs on each machine connected using a Packet Engine switch.
The linux kernel version used was 2.4.18.
This section gives the performance evaluation of the basic substrate without any performance enhancement and shows the advantage obtained incrementally with each performance enhancement technique.In Figure 6 the basic performance given by the substrate for data streaming sockets is labeled as DS and that for datagram sockets is labeled as DG.
DS DA refers to the performance obtained by incorporating Delayed Acknowledgments as mentioned in Section 5.3.
DS DA UQ refers to the performance obtained with both the Delayed Acknowledgments and the Unexpected Queue option turned on (Section 5.4).
For this experiment, for the Data Streaming case, we have chosen a credit size of 32 with each temporary buffer of size 64KB.
With all the options turned on, the substrate performs very close to raw EMP.
The Datagram option performs the closest to EMP with a latency of 28.
Figure 7 shows the drop in latency with delayed acknowledgment messages.
The reason for this is the decrease in the amount of tag matching that needs to be done at the NIC with the reduced number of acknowledgment descriptors.
For a credit size of 1, the percentage of acknowledgment descriptors would be 50%, which leads to an additional tag matching for every data descriptor.
However, for a credit size of 32, the percentage of acknowledgment descriptors would be 6.25%, thus reducing the tag matching time.The bandwidth results have been found to stay in the same range with each performance evaluation technique.
Figure 8 shows the latency and the bandwidth achieved by the substrate compared to TCP.
The Data Streaming label corresponds to DS DA UQ (Data Streaming sockets with all performance enhancements turned on).
Again, for the data streaming case, a credit size of 32 has been chosen with each temporary buffer of size 64 Kbytes.
In default, TCP allocates 16 Kbytes of kernel space for the NIC to use for communication activity.
With this amount of kernel space, TCP has been found to give a bandwidth of about 340 Mbps.
However, since the modern systems allow much higher memory registration, we changed the kernel space allocated by TCP for the NIC to use.
With increasing buffer size in the kernel, TCP is able to achieve a bandwidth of about 550 Mbps (after which increasing the kernel space allocated does not make any difference).
Further, this We have measured the performance of the standard File Transfer Protocol (ftp) given by TCP on Gigabit Ethernet and our substrate.
To remove the effects of disk access and caching, we have used RAM disks for this experiment.With our substrate, the FTP application takes 6.84 secs for Data Streaming and Datagram sockets, compared to the 11.8 secs taken by TCP for transferring a 512MB file.
For small files, FTP takes 13.6 ¤ s for Data Streaming and Datagram sockets, compared to the 25.6 ¤ s taken by TCP [2].
The application is not able to achieve the peak bandwidth illustrated in Section 6.2, due to the File System overhead.There is a minor variation in the bandwidth achieved by the data streaming and the datagram options in the standard bandwidth test.
The overlapping of the performance achieved by both the options in ftp application, is also attributed to the file system overhead.Note that this application requires both a socket read as well as a file read, thus requiring the substrate to be compatible with UNIX sockets.
We have attained it by overloading the function name-space as mentioned in Section 4.2.
We have measured the performance obtained by the Web Server application for a 4 node cluster (with one server and three clients).
The experiment was designed in the following manner -the server keeps accepting requests from the clients.
The clients connect to the server and send in a request message (which can typically be considered a file name) of size 16 bytes.
The server accepts the connection and sends back a message of size bytes to the client.
We have shown results for varying from 4 bytes to 8 Kbytes.
Once the message is sent, the connection is closed (as per HTTP/1.0 specifications).
However, this was slightly modified in the HTTP/1.1 specifications, which we also discuss in this section.
A number of things have to be noted about this application.
First, the latency and the connection time results obtained by the substrate in the micro-benchmarks play a dominant role in this application.
For connection management, we use a data message exchange scheme as mentioned earlier.
This gives an inherent benefit to the Sockets-over-EMP scheme since the time for the actual request is hidden, as the connection message descriptors are pre-posted.
Figure 9 gives the results obtained by the Web Server application following the HTTP/1.0 specifications.In the substrate, once the "connection request" message is sent by the substrate, the application can start sending the data messages.
This reduces the connection time of the substrate to the time required by a message exchange.
However, in TCP, the connection time requires intervention by the kernel and is typically about 200 to 250 ¤ s. To cover this disadvantage, TCP has the following enhancements: the HTTP 1.1 specifications allow a node to make up to 8 requests on one connection.
Even with this specification, our substrate was found to perform better than the base TCP application [2].
In the worst case, if the web server allows infinite requests on a single connection, the web server application boils down to a simple latency test which has been plotted in Section 6.2.
Ethernet forms a major portion of the world's networks.
Applications written using the sockets library have not been able to take advantage of the high performance provided by Gigabit Ethernet due to the traditional implementation of sockets on kernel based protocols.In this paper, we have designed and developed a lowoverhead substrate to support socket based applications on EMP.
For short messages, this substrate delivers a latency of 28.
Compared to the basic TCP, latency obtained by this substrate shows performance improvement up to 4 times.
A peak bandwidth of over 840 Mbps is obtained by this substrate, compared to 550 Mbps achieved by the basic TCP, a performance improvement by a percentage of up to 53%.
For the ftp and Web server applications, compared to the basic TCP implementation, the new substrate shows performance improvement by a factor of 2 and 6, respectively.
These results demonstrate that applications written using TCP can be directly run on Gigabit Ethernetconnected cluster with this substrate.We are currently working on utilizing and evaluating the proposed substrate for a range of commercial applications in the Data center environment.
We also plan to develop a similar substrate for the emerging InfiniBand interconnect so that a range of applications should be able to take advantage of the low-latency and high-bandwidth associated with interconnects for the next generation clusters.
We would like to thank all the students working at the Network of Workstations Laboratory at The Ohio State University, for all the valuable suggestions they had given through the course of the project.
Without their help, this paper would not have been possible.
We would also like to thank the reviewers for their comments and suggestions which helped us make this paper better.
