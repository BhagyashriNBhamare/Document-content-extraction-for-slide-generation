In the case of dense NAND flash such as TLC, the LSB, CSB and MSB 1 pages in a wordline can be combined to form a larger logical page called melded-page.
In this paper, we propose melding TLC/QLC pages to improve the performance of SSD by mitigating the overheads involved in the read operation.
Melded-pages are read in their entirety.
The controller schedules the write requests in such a way that, during reads later, requests for data present in LSB, CSB and MSB pages are guaranteed to be present in the request queue.
This method works reliably when the workload performs its read operations in large chunks or has a predictable I/O pattern.
We obtain performance improvements of up to 45% on workloads that use large block sizes such as the Hadoop Distributed File System (HDFS).
Big data solutions that exhibit such read patterns can vastly benefit from melding pages.
Solid State Drives based on NAND flash use transistors to store information persistently.
Data is stored as a threshold voltage in each flash cell 2 .
Due to its energy efficiency, small form factor and resilience to physical shock, they are the storage medium of choice in a variety of mobile devices.
Internal parallelism facilitates SSDs to deliver a significantly higher I/O performance when compared to traditional magnetic disks, and are hence used in data centers.
Modern flash memories have transistors that allow it to store multiple bits, thus enabling the production of SSDs with higher capacities and a low cost-per-bit.
Cells with an ability to store three bits are being widely used, with Intel and Micron announcing even the availability of the commercial SSD with quad-level cells.However, such high-density SSDs suffer from longer latencies to program and read data, resulting in reduced throughputs, when compared to flash memories that store a single bit per cell.
When a read request arrives via the host interface, the controller needs to determine the physical page address of the requested data in the underlying flash memory.
To do this, the controller runs a firmware called the Flash Translation Layer to enable this address translation.
It maintains a Flash Translation Table to manage the mappings from logical addresses to physical addresses.
This process of mapping adds to read latency.The controller then sends this address along with the read command to the control unit in the flash chip.
Block and page decoders are then used to access the correct wordline [6].
However, before reading the data from the selected wordline, all other wordlines in the selected block are made conductive.
This setup operation is required because the sense amplifiers are connected to the selected cells on a shared bitline [3,7,10].
A voltage, greater than the maximum threshold voltage of the transistor, called the passthrough voltage is applied to all the unselected cells to ensure they are conductive.
The conductivity of the selected cell can only be determined after this setup operation.
The operation to set up a block to ensure the readability of the chosen wordline further adds to the read overheads.The controller may also choose to perform various postprocessing operations after the page is read.
One such task involves detecting and correcting bit errors.
Such operations also add to the read latencies.Furthermore, in a dense SSD, such as TLC flash used in this paper, one, two and four read reference voltages are required to read LSB, CSB and MSB pages respectively.
Table 1 shows the latencies to read different page types from a triple-level cell [5].
Ideally, one would expect the latency to read an MSB page to be four times that of an LSB page and two times for a CSB page.
This is not observed in practice due to overheads involved in the read operation.Our approach to alleviate the effects of read overheads involves combining the LSB, CSB and MSB pages in a wordline into a single larger page called a melded-page.
When a read request arrives, the entire melded-page is read in one command cycle.
The scheduling algorithm ensures that the request for all the data in a melded-page is present in the request queue (or will arrive soon).
To guarantee this, the scheduler modifies the order in which the pages are programmed.
It must be noted that data is written to melded-page at page granularity, as opposed to meldedpage granularity, using shadow programming sequence and data stripping across parallel units.
The pattern in which data is programmed to the flash memory remains unchanged.
The high throughput of a NAND flash SSD can be attributed to its internal parallelism.
There are four levels of parallelism in an SSD.
When a write request arrives, the controller stripes the data across channels, packages, dies, and planes.
Multiple schemes to stripe data and allocate this striped data across parallel units exist [4].
Figure 2 shows the Plane-ChannelPackage-Die allocation scheme.
The exact method used depends on the manufacturer.
Melded-page mitigates overheads by reading all data in a wordline of a TLC flash in one command cycle.
The process of writing pages in shadow program sequence and striping I/O request across parallel units may prevent the data, when requested later during the read operation, to be in the same melded-page.
Reading an entire melded-page when only a part of it is used to fulfill a read request has a negative impact on the read performance.
To prevent this, the scheduler rearranges the order in which striped data is pipelined for writes in a manner which compensates for the change in order due to the processes mentioned earlier.
This reordering of writes by the scheduler ensures that, later, during reads, requests for the data present in LSB, CSB and MSB pages are guaranteed to be present in the read request queue.As an example, let us consider an SSD with just one parallel unit.
Also assume that we know the requests or read stripes 0, 1 and 2 will all be in the read request queue.
With this knowledge, we reorder the way in which the stripes are written as shown in Figure 4.
Stripe 3 is written before 1 and stripes 4 and 5 are written before 2, so that 0, 1 and 2 can all be in the same melded page.
We take another example of an SSD with two channels, each having two packages.
Assuming the resources are allocated using a Plane-Channel-Package-Die allocation scheme, Figure 6 and Figure 5 illustrates the spread of data across flash memory with and without the scheduler rearranging the order in which the pages are programmed.
Only the contents of package 0 are shown completely for the sake of simplicity.
In general, the scheduling algorithm depends upon the I/O write patterns, striping policies and page allocation strategies.
Figure 6: An example of how the striped data is spread across parallel units with scheduling (to guarantee that later, while reading, requests for data in LSB, CSB and MSB pages are all present in the read request queue.)
Ideally, for a system to benefit from melded pages, it must perform large read operations (or, read large blocks), in a predictable pattern.
Systems that use large block sizes such as Hadoop Distributed File System (HDFS) are good use cases for melded-pages.
Hadoop and Spark are popular open-source cluster-computing frameworks used for big data analytics and large-scale data processing.
One of the most popular distributed data storage system to manage large data sets is Hadoop Distributed File System [9].
Like most filesystems, HDFS allows the user to create and delete directories, and read, write and delete files.
HDFS uses master-slave architecture.
A master NameNode server manages the filesystem and DataNodes are responsible for serving the client's read and write requests.When an application or user writes a file, the system splits it into smaller chunks, knows as blocks.
These blocks are stored in various slave nodes in the HDFS cluster.
The block size, by default, is 128MB [1].
With an exception of the final data block for a file, which uses as only as much space as it is needed, a block is the smallest unit of data that HDFS uses to read or write.When an application or a user reads a file, the HDFS client raises a request to the NameNode for the list of DataNodes that host the requested file blocks.
The client then contacts the topologically closest DataNode and directly initiates the transfer of the desired blocks.Since HDFS performs all write and read operations in large chunks and in a sequential manner, the scheduler can predict read patterns and rearrange write requests to guarantee that, later, all requests for data present LSB, CSB, and MSB pages are present in the read request queue.
The use of meldedpages shows a considerable improvement in read-throughput of SSDs in DataNodes.
The scheduler only reorders those requests that can benefit from melding pages.
In the case of HDFS, for example, only write requests, to SSD, which correspond to the writing of HDFS block data needs to be reordered.
The controller identifies such requests with the help of hints provided to it, by the host, known as directives.The specifications for NVM Express v1.3 [2] defines a framework for directive support.
It introduces a mechanism to exchange extra metadata between the host and the controller.
I/O command directives add an ability for the host to tag write commands with directive type and an associated directive specific value.Currently, the NVM Express specs define two directive types; identity and stream [2].
We propose a new use case for steams.
For a controller that supports melded-pages, all write requests to SSD generated from writing an HDFS block forms a stream and are tagged as such.
This is used by the controller to identify write requests that need to be reordered by the scheduler.It is possible for the host application to start writing a new HDFS block while another HDFS block is already being written.
Write requests to SSD with data from different HDFS blocks are interleaved in this case.
The host uses a different stream identifier to prevent the melding of pages that contain data from various HDFS blocks.
The controller opens a new stream for each HDFS block and pages that originate from different sources are written to different SSD blocks.
Stream identifiers and allocated blocks are released after the SSD completes writing an entire HDFS block.
The following are the limitations of using melded-pages.
To benefit form melding pages, the scheduler needs to predict the read pattern of the workload.
This enables the scheduler to combine multiple pages (three in case of TLC flash), which are guaranteed to be read simultaneously in the future, and program them to the same wordline.
Melded-pages cannot be used if this prediction is not possible.
The host provides directives to the SSD controller, along with the write request, to enable or disable the use of melded-pages for that request.
Read requests which are fulfilled from different parallel units have a better performance than those served from a single die/plane.
Also, the lack of contentions in I/O bus for requests served form parallel units in different channels results in the best performance.
Thus, melding pages in a single plane when they could have been assigned different parallel units impacts read throughput.When request sizes are large enough, all data stripes cannot be assigned to pages in different parallel units.
Pages are melded only in this case.
The minimum size of a request to give an improvement in read-throughput with the use of melded-pages is discussed later in section 3.2.1.
In case of TLC flash, an operation to read from a meldedpage retrieves three times the amount of data in one command cycle when compared to reading a normal page.
Page buffers in each plane must be three times as large to accommodate this.
The cost of this hardware change is justifiable only if majority of I/O operations performed by the workload can take advantage of melded-pages.
For the purposes of evaluation, we run Hadoop v2.9.1 with Hadoop Distributed File Systems.
The workload used does not matter since the I/O pattern of reads and writes from the perspective of the host is consistently sequential due to large block sizes used by HDFS.
For our simulation environment, we use SimpleSSD [5], a high-fidelity simulator, and implement Melded-Pages on top of it.
Table 2 summarizes the configuration of the SSD.
The following describes various parameters used in the implementation.
We assume the time it takes to read a melded page is 166µs.
Assuming X denotes the latency due to the overheads involved in accessing a page and Y is the amount of time to apply a read reference voltage and test the conductivity of a cell, to read an LSB page, which requires only one read reference voltage, the latency can be approximated to X +Y .
Similarly, the latency to read a CSB page, which requires two read reference voltages, is X + 2Y .
Reading an MSB page from a TLC flash requires four reference voltages.
Hence, the latency to read an MSB page can be approximated to X + 4Y .
The total time to read all three pages will hence be 3X + 7Y .
However, the latency to read an entire melded-page in one go is X + 7Y .
In this work, we approximate the value of X to 40µs and Y to 18µs.
To quantify the performance improvements with the use of melded-pages, we measure the read throughputs of the SSD.
We estimate the minimum read size that an application must request to benefit from melded-pages and then report the improvements in read-throughput performance for reading HDFS blocks.
As mentioned in section 2.4, melding pages is only beneficial for workloads that read a large amount of data from an SSD.To determine the minimum size of data, that needs to be read in a predictable pattern, to benefit from melded-pages, we increment the read size and measure the time to complete each read operation.
We simulate experiments on an SSD with 4KB page size and with a data frequency of 800MT/s.
The SSD has eight channels and each channel has eight parallel units, giving us a total of sixty-four parallel units.
The results are tabulated in Small reads have better throughput if the pages that hold the requested data is placed in different parallel units as opposed to melding them on the same die/plane.
Requests that are served from pages on parallel units in different channels show the best performance.
For an SSD, with an aforementioned configuration, the minimum size of data that must be read to benefit from melded-pages is 2MB (table 3).
Figures 7 and 8 show read throughputs with various SSD configuration for reading an HDFS block.
We vary the page size, channel width, data frequency and report results for every combination with and without the use of mended-pages.
We observe a significant improvement in the readthroughputs of up to 45% while reading an HDFS block.
This is the upper limit of the performance that can be achieved by using melded-pages with our configuration.
Increasing data frequency or channel width will not improve the performance any further because of the latencies to read from the page.
In other cases, when reading large melded-page sizes, the throughput of the channel can become a bottleneck.
Increasing the channel's operating frequency/width or reducing the number of parallel units per channel solves this problem.
In this paper, we presented a solution to improve the read throughput of SSDs using melded pages, if knowledge of I/O patterns is available.
For this to work, the scheduler rearranges the writes to ensure that the request for all data in a wordline is present in the read request queue.
For workloads that use distributed file system, we can improve the read performance by up to 45%.
