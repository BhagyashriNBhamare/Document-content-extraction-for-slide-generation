We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context.
We consider two popular approaches, the one-step Temporal Difference fix-point computation (TD(0)) and the Bellman Residual (BR) minimization.
We describe examples, where each method out-performs the other.
We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general.
We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008).
Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.
We consider linear approximations of the value function of the policy in the framework of Markov Decision Processes (MDP).
We focus on two popular methods: the computation of the projected Temporal Difference fixed point (TD(0), TD for short), which Antos et al. (2008); Farahmand et al. (2008); Sutton et al. (2009) have recently presented as the minimization of the mean-square projected Bellman Equation, and the minimization of the meansquare Bellman Residual (BR).
In this article, we present some new analytical and empirical data, that shed some light on both approaches.
The paper is organized as follows.
Section 1 describes the MDP linear approximation framework and the two projection methods.
Section 2 presents small MDP examples, where each method outperforms the other.
Section 3 highlights a simple relation between the quantities TD and BR optimize, and show that while BR enjoys a performance guarantee, TD does not in general.
Section 4 contains the main contribution of this paper: we describe a unified view in terms of oblique projections of the Bellman equation, which simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008).
Eventually, Section 5 presents some simulations, that address the following practical questions: which of the method gives the best approximation?
and how useful is our analysis for selecting it a priori?
The model We consider an MDP with a fixed policy, that is an uncontrolled discrete-time dynamic system with instantaneous rewards.
We assume that there is a state space X of finite size N .
When at state i ∈ {1, .
.
, N }, there is a transition probability p ij of getting to the next state j. Let i k the state of the system at time k.
At each time step, the system is given a reward γ k r(i k ) where r is the instantaneous reward function, and 0 < γ < 1 is a discount factor.
The value at state i is defined as the total expected return: v(i) := lim N →∞ E N −1 k=0 γ k r(i k ) i 0 = i .
We write P the N × N stochastic matrix whose elements are p ij .
v can be seen as a vector of R N .
v is known to be the unique fixed point of the Bellman operator: T v := r + γP v, that is v solves the Bellman Equation v = T v and is equal to L −1 r where L = I − γP .
Author manuscript, published in "N/P" TD or BR?
The unified oblique projection view Approximation Scheme When the size N of the state space is large, one usually comes down to solving the Bellman Equation approximately.
One possibility is to look for an approximate solutionˆvsolutionˆ solutionˆv in some specific small space.
The simplest and best understood choice is a linear parameterization: ∀i, ˆ v(i) = m j=1 w j φ j (i) where m ≪ N , the φ j are some feature functions that should capture the general shape of v, and w j are the weights that characterize the approximate valuê v. For all i and j, write φ j the N -dimensional vector corresponding to the j th feature function and φ(i) the mdimensional vector giving the features of state i. For any vector of matrix X, denote X ′ its transpose.
Thefollowing N × m feature matrix Φ = (φ 1 . . . φ m ) = (φ(i 1 ) . . . φ(i N ))′ leads to write the parameterization of v in a condensed matrix form: ˆ v = Φw, where w = (w 1 , ..., w m ) is the m-dimensional weight vector.
We will now on denote span (Φ) this subspace of R N and assume that the vectors φ 1 , ..., φ m form a linearly independent set.Some approximationˆvapproximationˆ approximationˆv of v can be obtained by minimizingˆvmizingˆ mizingˆv → ˆ v − v for some norm · , that is equivalently by projecting v onto span (Φ) orthogonally with respect to · .
In a very general way, any symmetric positive definite matrix Q of R N induces a quadratic norm · Q on R N as follows:v Q = √ v ′ Qv.It is well known that the orthogonal projection with respect to such a norm, which we will denote Π ·Q , has the following closed form: Π ·Q = Φπ ·Q where π ·Q = (Φ ′ QΦ) −1 Φ ′ Q is the linear application from R N to R m that returns the coordinates of the projection of a point in the basis (φ 1 , . . . , φ m ).
With these notations, the following relations π ·Q Φ = I and π ·Q Π ·Q = π ·Q hold.In an MDP approximation context, where one is modeling a stochastic system, one usually considers a specific kind of norm/projection.
Let ξ = (ξ i ) be some distribution on X such that ξ > 0 (it assigns a positive probability to all states).
Let Ξ be the diagonal matrix with the elements of ξ on the diagonal.
Consider the orthogonal projection of R N onto the feature space span (Φ) with respect to the ξ-weighted quadraticnorm v ξ = N j=1 ξ i v i 2 = √ v ′ Ξv.For clarity of exposition, we will denote this specific projection Π := Π ·Ξ = Φπ where π := π ·Ξ = (Φ ′ ΞΦ) −1 Φ ′ Ξ.Ideally, one would like to compute the "best" approximation ˆ v best = Φw best with w best = πv = πL −1 r.This can be done with algorithms like TD(1) / LSTD(1) (Bertsekas & Tsitsiklis, 1996;Boyan, 2002), but they require simulating infinitely long trajectories and usually suffer from a high variance.
The projections methods, which we focus on in this paper, are alternatives that only consider one-step samples.TD (0) Sutton et al. (2009), when the inverse exists, the above computation is equivalent to minimizing forˆvforˆ forˆv ∈ span (Φ) the TD errorE T D (ˆ v) := ˆ v − ΠTˆvΠTˆ ΠTˆv ξ down to 0 3 .
BR minimization method The principle of the Bellman Residual (BR) method is to look forˆvforˆ forˆv ∈ span (Φ) so that it minimizes the norm of the Bellman Residual, that is the quantityE BR (ˆ v) := ˆ v − T ˆ v ξ .
Sincêv is of the form Φw, it can be seen that E BR (ˆ v) = Φw − γP Φw − r ξ = Ψw − r ξ using the notation Ψ = LΦ.
Using standard linear least squares arguments, one can see that the minimum BR is obtained forˆvforˆ forˆv BR = Φw BR withw BR = (Ψ ′ ΞΨ) −1 Ψ ′ Ξr.
(2)Note that in this case, the above inverse always exists (Schoknecht, 2002).
, and the weight of the best approximation is w best = πv = 1 5 r 1 + 2+γ 5(1−γ) r 2 .
This example has been proposed by Bertsekas & Tsitsiklis (1996) in order to show that fitted Value Iteration can diverge if the samples are not generated by the stationary distribution of the policy.
In (Bertsekas & Tsitsiklis, 1996), the authors only consider the case r 1 = r 2 = 0 Figure 1.
Error ratio (in log scale) between the TD/BR projection methods and the best approximation for Example 1, with respect to the discount factor γ and the parameter θ of the reward (Left).
It turns out that these surfaces do not depend on θ so we also draw the graph with respect to γ only (Right).
so that this diverging result was true even though the exact value function v(0) = v(1) = 0 did belong to the feature space.
In the case r 1 = r 2 = 0, the TD and BR methods do calculate the exact solution (we will see later that this is indeed a general fact when the exact value function belongs to the feature space).
We thus extend this model by taking (r 1 , r 2 ) 񮽙 = (0, 0).
As a scaling of the reward is translated exactly in the approximation, we consider the general form (r 1 , r 2 ) = (cos θ, sin θ).
Consider the TD solution: one hasΦ ′ Ξ = 1 2 1 , (I − γP )Φ = (1 − 2γ 1 − γ), thus (Φ ′ ΞΨ) = 52 − 3γ and Φ ′ Ξr = r1 2 + r 2 .
Eventually the weight of the TD approximation is w T D = r1+2r2 5−6γ .
One notices here that the value γ = 5/6 is singular.
Now, consider the BR solution.
One can see that (Ψ ′ ΞΨ) −1 = (1−2γ) 2 +(2−2γ) 2 2andΨ ′ Ξr = (1−2γ)r1+(2−2γ)r2 2.
Thus, the weight of the BR approximation isw BR = (1−2γ)r1+(2−2γ)r2 (1−2γ) 2 +(2−2γ) 2 .
For all these approximations, one can compute the squared error e with respect to the optimal solution v:For any weight w ∈ {w best , w T D , w BR }, e(w) = v − Φw 2 ξ = 1 2 (v(1) − w) 2 + 1 2 (v(2) − 2w) 2 .
In Fig- ure 1, we plot the squared error ratios e(wT D )e(w best ) and e(wBR) e(w best ) on a log scale (they are by definition greater than 1) with respect to θ and γ.
It turns out that these ratios do not depend on θ (instead of showing this through painful arithmetic manipulations, we will come back to this point and prove it later on).
This Figure also displays the graph with respect to γ only.
We can observe that for any choice of reward function and discount factor, the BR method returns a better value than the TD method.
Also, when γ is in the neighborhood of 5 6 , the TD error ratio tends to ∞ while BR's stays bounded.
This Example shows that there exists MDPs where the BR is consistenly better than the TD method, which can give an unbounded error.
One should however not conclude too quickly that BR is always better than TD.
The literature contains several arguments in favor of TD, one of which is considered in the following Example.Example 2 Sutton et al. (2009) recently described a 3-state MDP example where the TD method computes the best projection while BR does not.
The idea behind this 3-state example can be described in a quite general way 4 : Suppose we have a k + l-state MDP, of which the Bellman Equation has a block triangular structure:v 1 = γP 1 v 1 + r 1 / v 2 = γP 21 v 1 + P 22 v 2 + r 2where v 1 ∈ R k and v 2 ∈ R l (the concatenation of the vectors v 1 and v 2 form the value function).
Suppose also that the approximation subspace span (Φ) is R k × S 2 where S 2 is a subspace of R l .
For the first component v 1 , the approximation space is the entire space R k .
With TD, we obtain the exact value for the k first components of the value, while with Bellman residual minimization, we do not: satisfying the first equation exactly is traded for decreasing the error in satisfying the second one (which also involves v 1 ).
In an optimal control context, the example above can have quite dramatic implications, as v 1 can be related to the costs at some future states accessible from those states associated with v 2 , and the future costs are all that matters when making decisions.Overall, the two methods generate different types of biases, and distribute error in different manners.
In order to gain some more insight, we now turn on to some analytical facts about them.
Though several works have compared and considered both methods (Schoknecht, 2002;Lagoudakis & Parr, 2003;Munos, 2003;Yu & Bertsekas, 2008), the following simple fact has, to our knowledge, never been emphasized per se:Proposition 1The BR is an upper bound of the TD error, and more precisely:∀ˆv∀ˆv ∈ span (Φ) , E BR (ˆ v) 2 = E T D (ˆ v) 2 + T ˆ v − ΠTˆvΠTˆ ΠTˆv 2 ξ .
Proof This simply follows from Pythagore, as ΠTˆvΠTˆ ΠTˆv − T ˆ v is orthogonal to span (Φ) andˆvandˆ andˆv − ΠTˆvΠTˆ ΠTˆv belongs to span (Φ).
This implies that if one can make the BR small, then the TD Error will also be small.
In the limit case where one can make the BR equal to 0, then the TD Error is also 0.
One of the motivation for minimizing the BR is historically related to a well-known result of Williams & Baird (1993): ∀ˆv∀ˆv, v − ˆ v ∞ ≤ 1 1−γ T ˆ v − ˆ v ∞ .
Since one considers the weighted quadratic norm in practice 5 , the related result 6 that really makes sense here is:∀ˆv∀ˆv, v − ˆ v ξ ≤ √ C(ξ) 1−γ T ˆ v − ˆ v ξ where C(ξ) := max i,jpij ξi is a "concentration coefficient", that can be seen as some measure of the stochasticity of the MDP 7 .
This result shows that it is sound to minimize the BR, since it controls (through a constant) the approximation error v − ˆ v BR ξ .
On the TD side, there does not exist any similar result.
Actually, the fact that one can build examples (like Example 1) where the TD projection is numerically unstable implies that one cannot prove such a result.
Proposition 1 allows to understand better the TD method: by minimizing the TD Error, one only minimizes one part of the BR, or equivalently this means that one does not care about the term T v − ΠT v 2 ξ , which may be interpreted as a measure of adequacy of the projection Π with the Bellman operator T .
In Example 1, the approximation error of the TD projection goes to infinity because this adequacy term diverges.
In , the authors use an algorithm based on the TD Error and make an assumption on this adequacy term (there called the inherent Bellman error of the approximation space), so that their algorithm can be proved convergent.A complementary view on the potential instability of TD, has been referred to as a norm incompatibility issue (Bertsekas & Tsitsiklis, 1996;Guestrin et al., 2001), and can be revisited through the notion of concentration coefficient.
Stochastic matrices P statisfy P ∞ = 1, which makes the Bellman operator T γ-contracting, and thus its fixed point is well-defined.
The orthogonal projection with respect to · ξ is such that Π ξ = 1.
Thus P and Π are of norm 1, but for different norms.
Unfortunately, a general (tight) bound for linear projections is Π ∞ ≤ 5 Mainly because it is computationnally easier than doing a max-norm minimization, see however ( Guestrin et al., 2001) for an attempt of doing max-norm projection.
6 The proof is a consequence of Jensen's inequality and the arguments are very close to the ones in (Munos, 2003).7 If ξ is the uniform law, then there always exists such a C(ξ) ∈ (1, N ) where one recalls that N is the size of the state space; in such a case, C(ξ) is minimal if all next-states are chosen with the uniform law, and maximal as soon as there exists a deterministic transition.
See (Munos, 2003) for more discussion on this coefficient.
√ N 2 (Thompson, 1996) and it can be shown 8 that P ξ ≤ C(ξ) (which can thus also be of the order of √ N ).
Consequently, ΠP ∞ and ΠP ξ may be greater than 1, and thus the fixed point of the projected Bellman equation may not be well-defined.
A known exception where the composition ΠP has norm 1, is when one can prove that P ξ = 1 (as for instance when ξ is the stationary distribution of P ) and in this case we know from Bertsekas & Tsitsiklis (1996); Tsitsiklis & Van Roy (1997) that v − ˆ v T D ξ ≤ 1 1 − γ 2 v − ˆ v best ξ .
(3)Another notable such exception is when Π max = 1, as in the so-called "averager" approximation (Gordon, 1995).
However, in general, the stability of TD is difficult to guarantee.
In the TD approach, we consider finding the fixed point of the composition of an orthogonal projection Π and the Bellman operator T .
Suppose now we consider using a (non necessarily orthogonal) projection Π onto span (φ), that is any linear operator that satisfies Π 2 = Π and whose range is span (Φ).
In their most general form, such operators are called oblique projections and can be written Π X = Φπ X with π X = (X ′ Φ) −1 X ′ .
The parameter X specifies the projection direction: precisely, Π X is the projection onto span (Φ) orthogonally to span (X).
As for the orthogonal projections, the following relations π X Φ = I and π X Π X = π X hold.
Recall that L = I − γP .
We are ready to state the main result of this paper:Proposition 2 Write X T D = ΞΦ and X BR = ΞLΦ.
(1)The TD fix point computation and the BR minimization are solutions (respectively with X = X T D and X = X BR ) of the projected equationˆvequationˆ equationˆv X = Π X T ˆ v X .
(2) When it exists, the solution of this projected equation is the projection of v onto span (Φ) orthogonally to span (L ′ X), i.e. formallyˆvformallyˆ formallyˆv X = Π L ′ X v.Proof We begin by showing part (2).
WritingˆvWritingˆ Writingˆv X = Φw X , the fixed point equation is: Φw X = Π X (r + γP Φw x ).
Multiplying on both sides by π X , one obtains: w X = π X (r + γP Φw x ) and therefore w X = (I − γπ X P Φ) −1 π X r. Using the definition of π X , one 8 One can prove that for all x, P x 2 ξ ≤ x 2 ξP ≤ C(ξ)x 2 ξ .
The argument for the first inequality involves Jensen's inequality and is again close to what is done in (Munos, 2003).
obtains:w X = (I − γ(X ′ Φ) −1 X ′ P Φ) −1 (X ′ Φ) −1 X ′ r = (X ′ Φ)(I − γ(X ′ Φ) −1 X ′ P Φ) −1 X ′ r = (X ′ (I − γP )Φ) −1 X ′ r (4) = (X ′ LΦ) −1 X ′ Lv = π L ′ X vwhere we enventually used r = Lv.The proof of part (1) now follows.
The fact that TD is a special case with X = ΞΦ is trivial by construction since then Π X is the orthogonal projection with respect to · ξ .
When X = ΞLΦ, one simply needs to observe from Equations 2 and 4 and the definition of Ψ = LΦ that w X = w BR .
Beyond its nice and simple geometric flavour, a direct consequence of Proposition 2 is that it allows to derive tight error bounds for TD, BR, and any other method for general X. For any square matrix M , write σ(M ) its spectral radius.Proposition 3 For any choice of X, the approximation error satisfies:v − ˆ v X ξ ≤ Π L ′ X ξ v − ˆ v best ξ (5) = σ(ABCB ′ )v − ˆ v best ξwhere A = Φ ′ ΞΦ, B = (X ′ LΦ) −1 and C = XLΞ −1 L ′ X are matrices of size m × m.Thus, for any X, the amplification of the smallest error v − ˆ v best ξ depends on the norm of the associated oblique projection, which can be estimated as the spectral radius of the product of small matrices.
A simple corollary of this Proposition is the following: if the real value v belongs to the feature space span (Φ) (in such a case v = ˆ v best ) then all oblique projection methods find it (ˆ v X = v).
v − ˆ v X = (I − Π L ′ X )v = (I − Π L ′ X )(I − Π ΞΦ )v.where we used the fact that Π L ′ X Π ΞΦ = Π ΞΦ since Π L ′ X and Π ΞΦ are projections onto span (Φ).
Taking the norm,one obtains v − ˆ v X ξ ≤ I − Π L ′ X ξ v − Π ΞΦ v ξ = Π L ′ X ξ v − ˆv best ξ where we used the definition ofˆv ofˆ ofˆv best , and the fact that I − Π L ′ X ξ = Π L ′ X ξ since Π L ′ X is a (non-trivial) projection (see e.g. (Szyld, 2006)).
Thus Equation 5 holds.In order to evaluate the norm in terms of small size matrices, one will use the following Lemma on the pro-jection matrix Π L ′ X = Φπ L ′ X : Lemma 1 (Yu & Bertsekas (2008)) Let Y be an N × m matrix, and Z a m × N matrix, then Y Z 2 ξ = σ (Y ′ ΞY )(ZΞ −1 Z ′ ) .
Thus,Π L ′ X 2 ξ = Φπ L ′ X 2 ξ = σ[(Φ ′ ΞΦ)(π L ′ X Ξ −1 (π L ′ X ) ′ )] = σ[Φ ′ ΞΦ(X ′ LΦ) −1 X ′ LΞ −1 L ′ X(Φ ′ L ′ X) −1 ] = σ[ABCB ′ ].
Proposition 2 is closely related to the work of (Schoknecht, 2002), in which the author derived the following characterization of the TD and BR solutions: This "orthogonal projection" characterization and our "oblique projection" characterization are in fact equivalent.
On the one hand for BR, it is immediate to notice that Π ·Q BR = Π L ′ XBR .
On the other hand for TD, writingY = L ′ X T D , one simply needs to notice that Π L ′ XT D = Π Y = Φ(Y ′ Φ) −1 Y ′ = Φ(Y ′ Φ) −1 (Φ ′ Y ) −1 (Φ ′ Y )Y ′ = Φ(Φ ′ Y Y ′ Φ) −1 Φ ′ Y Y ′ = Π ·Q T D .
The work of Schoknecht (2002) suggests that TD and BR are optimal for different criteria, since both look for somê v ∈ span (Φ) that minimizesˆv minimizesˆminimizesˆv − v for some (semi)norm · .
Curiously, our result suggests that neither is optimal, since neither uses the best projection direction X * := L ′−1 ΞΦ for whichˆv whichˆ whichˆv X * = Π L ′ X * v = Π ΞΦ v = ˆ v best and this supports the empirical evidence that there is no clear "winner" between TD and BR.Our main results, stated in Propositions 2 and 3, constitutes a revisit of the work of Yu & Bertsekas (2008), where the authors similarly derived error bounds for TD and BR.
Our approach mimicks theirs: 1) we derive a linear relation between the projectionˆvtionˆ tionˆv, the real value v and the best projectionˆvprojectionˆ projectionˆv best , then 2) analyze the norm of the matrices involved in this relation in terms of spectral radius of small matrices (through Lemma 1, which is taken from (Yu & Bertsekas, 2008)).
From a purely quantitative point of view, our bounds are identical to the ones derived there.
Two immediate consequences of this quantitative equivalence are that, as in (Yu & Bertsekas, 2008), (1) our bound is tight in the sense that there exists a worst choice for the reward for which it holds with equality, and (2) it is always better than that of Equation 3 from Bertsekas & Tsitsiklis (1996); Tsitsiklis & Van Roy (1997).
However, our work is qualitatively different: by highlighting the oblique projection relation betweenˆvbetweenˆ betweenˆv and v, not only do we provide a clear geometric intuition for both methods, but we also greatly simplify the form of the results and their proofs (see (Yu & Bertsekas, 2008) for details).
Last but not least, there is globally a significant difference between our work and the two works we have just mentionned.
The analysis we propose is unified for TD and BR (and even extends to potential new methods through other choices of the parameter X), while the results in (Schoknecht, 2002) and (Yu & Bertsekas, 2008) are proved independently for each method.
We hope that our unified approach will help understanding better the pros and cons of TD, BR, and related alternative approaches.
In order to further compare the TD and the BR projections, we have made some empirical comparison, which we describe now.
We consider spaces of dimensions n = 2, 3, .
.
, 30.
For each n, we consider projections of dimensions k = 1, 2, .
.
, n. For each (n, k) couple, we generate 20 random projections (through random matrices 10 Φ of size (n, k) and random weight vectors ξ) and 20 random (uncontrolled) chain like MDP: from each state i, there is a probability p i (chosen randomly uniformly on (0, 1)) to get to state i + 1 and a probability 1 − p i to stay in i (the last state is absorbing); Using this raw data on 20 × 20 problems, we compute for each (n, k) couple some statistics, which we describe now.
All the graphs that we display shows the dimension of the space N and of the projected space m on the x − y axes.
The z axis correspond to the different statistics of interest.
consistently greater than 1 2 , which means that the TD method is usually better than the BR method.
Figure 3 presents the ratio of time the bounds we have presented in Propostion 4 correctly guesses which method is the best (i.e. the expectation of the indicator func-tion of [e T D < e BR ] = [b T D < b BR ]).
Unless the feature space dimension is close to the state space dimension, the bounds do not appear very useful for such a decision.
Figure 4 displays the expectation of e T D /e BR .
One can observe that, on average, this expectation is bigger than 1, that is the BR tends to be better, on average, than the TD error.
This may look contradictory with our interpretation of Figure 2, but the explanation is the following: when the BR method is better than the TD method, it is by a larger gap than when it is the other way round.
We believe this corresponds to the situation when the TD method in unstable.
Figure 5 allows to confirm this point: it shows the expectation of the relative approximation errors with respect to the best possible error, that is the expectation of e T D /e and e BR /e.
One observes on all charts that this average relative quality of the TD fix point has lots of pikes (corresponding to numerical instabilities), while that of the BR method is smooth.
We have presented the TD fix point and the BR minimization methods for approximating the value of some MDP fixed policy.
We have described two original examples: in the former, the BR method is consistently better than the TD method, while the latter (which generalizes the spirit of the example of Sutton et al. (2009)) is best treated by TD.
Proposition 1 highlights the close relation between the objective criteria that correspond to both methods.
It shows that minimizing the BR implies minimizing the TD error and some extra "adequacy" term, which happens to be crucial for numerical stability.Our main contribution, stated in Proposition 2, provides a new viewpoint for comparing the two projection methods, and potential ideas for alternatives.
Both TD and BR can be characterized as solving a projected fixed point equation and this is to our knowledge new for BR.
Also, the solutions to both methods are some oblique projection of the value v and this is to our knowledge new for TD and BR.
Eventually, this simple geometric characterization allows to derive some tight error bounds (Proposition 3).
We have discussed the close relations of our results with those of Schoknecht (2002) and Yu & Bertsekas (2008), and argued that our work simplifies and extends them.
Though apparently new to the Reinforcement Learning community, the very idea of oblique projections of fixed point equations has been studied in the Numerical Analysis community (see e.g. Saad (2003)).
In the future, we plan to study more carefully this literature, and particularly investigate whether it may further contribute to the MDP context.Concerning the practical question of choosing among the two methods TD and BR, the situation can be summarized as follows: the BR method is sounder than the TD method, since the former has a performance guarantee while the latter will never have one in general.
Extensive simulations (on random chainlike problems of size up to 30 states, and for many projection of all the possible space sizes) further suggest the following facts: (a) the TD solution is more often better than the BR solution; (b) however sometimes, TD failed dramatically; (c) overall, this makes BR better on average.
Equivalently, one may say that TD is more risky than BR.Even if TD is more risky, there remains several reasons inria-00537403, version 1 -19 Nov 2010 why one may want to use it in practice, and which our study did not focus on.
In large scale problems, one usually estimates the m × m linear systems through sampling.
Sampling based methods for BR are more constraining since they generally require double sampling.
Independently, the fact, highlighted by Propostion 1, that the BR is an upper bound of the TD error, suggests two things.
First, we believe that the variance of the BR problem is higher than that of the TD problem; thus, given a fixed amount of samples, the TD solution might be less affected by the corresponding stochastic noise than the BR one.
More generally, the BR problem may be harder to solve than the TD problem, and from a numerical viewpoint, the latter may provide better solutions.
Eventually, we only discussed the TD(0) fix point method, that is the specific variant of TD(λ) (Bertsekas & Tsitsiklis, 1996;Boyan, 2002) where λ = 0.
Values of λ > 0 solve some of the weaknesses of TD (0): it can be show that the stability issues disappear for values of λ close to 1, and the optimal projectionˆvprojectionˆ projectionˆv best is obtained when λ = 1.
Further analytical and empirical comparisons of TD(λ) with the algorithms we have considered here (and with some "BR(λ)" algorithm) constitute future research.Eventually, a somewhat disappointing observation of our study is that the bounds of Proposition 3, which are the tightest possible bounds independent of the reward function, did not prove useful for deciding a priori which of the two methods one should trust better (recall the results showed in Figure 3).
Extending them in a way that would take the reward into account, as well as trying to exploit our original unified vision of the bounds (Propositions 2 and 3) are some potential tracks for improvement.
The author would like to thank Janey Yu for helpful discussions, and the anonymous reviewers for providing comments that helped to improve the presentation of the paper.
