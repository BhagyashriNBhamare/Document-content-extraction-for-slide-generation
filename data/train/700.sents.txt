The maximum cardinality of a frequent set as well as the minimum cardinality of an infrequent set are important characteristic numbers in frequent (item) set mining.
Gunopulos et al. [10] have shown that finding a maximum frequent set is NP-hard.
In this paper I show that the minimization problem is also NP-hard.
As a next step I investigate whether these problems can be approximated.
While a simple greedy algorithm turns out to approximate a minimum infrequent set within a logarithmic factor one can show that there is no such algorithm for the maximization problem.
Finding sets of items that appear concurrently in at least a specified number of records in a given database is an important task in data mining.
This socalled frequency criterion for sets is used as an additional condition for different interestingness predicates.
Examples are association rules [2], correlations [5], or emerging patterns [7].
Algorithms usually perform an exhaustive enumeration of the family of frequent sets or of a reduced family like closed frequent sets or maximal frequent sets.
Such an exhaustive enumeration tends to be very time-consuming because both, the search space and the output size, can be exponential in the size of the input database.
The running time as well as the semantic significance of the produced output depend on the user-specified frequency parameter.
Thus it is of great value to know as much as possible about the results of an exponential time pattern mining algorithm prior to its application.
This knowledge can be used to readjust the frequency parameter and thus improve performance and semantic value of the mining algorithm.For that purpose frequent sets of maximum cardinality resp.
infrequent sets of minimum cardinality can be used.
Many mining algorithms tend to run exponentially long in the cardinality of a longest pattern, i.e. the size of a maximum frequent set and for level-wise algorithms the size of a minimum infrequent set determines the level where pruning starts.
So knowing either of the two would allow to upper bound the running time resp.
skip initial search levels.
In terms of result quality both indicate whether the chosen frequency threshold provides a significant gain of information for the resulting patterns.
If for instance the minimum cardinality of an infrequent set is 18 in a database containing 20 items this is an indication for a weak parameter choice.On the one hand, both optimization problems are NP-hard.
For the maximization problem this was shown by Gunopulos et al. in [10].
For the minimization problem this is shown in Section 3 of this paper.
On the other hand, computing approximate solutions would suffice for the described motivations.
In this paper I show that not even a reasonable approximation algorithm for a maximum frequent set is likely to exist based on recent results from computational complexity [12], while for a minimum infrequent set a simple greedy algorithm reaches a logarithmic approximation factor.
By another recent complexity result [8] this factor cannot be improved substantially.
Note that in contrast to approaches that aim at approximating the set of all frequent sets (like in [1]) we consider different problems each aiming to compute only one set.
To the best of my knowledge this is the first investigation on the approximability of these problems.The rest of the paper is organized as follows: Section 2 introduces basic definitions and notations.
In Section 3, the two optimization problems are defined formally and their NP-hardness is discussed.
Section 4 points out the hardness of approximating the maximization problem, while Section 5 proves the logarithmic performance of the greedy algorithm for the minimization problem.
Finally, Section 6 concludes with a summary and ideas for possible future work.
A hypergraph is a triple (V, H, μ) with V a finite set called ground set, H ⊆ 2 V a family whose elements are called hyperedges, and μ : H → N a mapping representing the multiplicity of each hyperedge.
So H can be seen as a multiset, and thus we mean by its cardinality |H| the sum H∈H μ(H).
For the purpose of computational problems we assume a hypergraph to be given as incidence matrix, and thus define size((V, H, μ)) = |V ||H| as the input size.
If μ(H) = 1 for all H ∈ H we omit μ and (V, H) is called proper.A graph is a hypergraph G = (V, E) with |e| = 2 for all e ∈ E.
The elements of V are called vertices, and the elements of E are called edges 1 .
G is called bipartite if V can be partitioned into V 1 , V 2 such that all edges are of the form {v, w} with v ∈ V 1 and w ∈ V 2 .
A graph of this form is denoted by (V 1 , V 2 , E).
A set of vertices X = X 1 ∪ X 2 with X 1 ⊆ V 1 , X 2 ⊆ V 2 is denoted by (X 1 , X 2 )and is called a bipartite clique if for all x 1 ∈ X 1 and all x 2 ∈ X 2 there is anedge {x 1 , x 2 } ∈ E.
It is called balanced if |X 1 | = |X 2 |.
The size of a balanced bipartite clique (X 1 , X 2 ) is |X 1 | = |X 2 |.
An optimization problem is a computational problem formally given by a 4-tuple P = (X, (S x ) x∈X , c, goal) with a set of instances X, a set of feasible solutions S x for all instances, a target function c :x∈X S x → N, and goal ∈ {min, max}.
The task is then, given an instance x ∈ X, compute a feasible solution y ∈ S x with c(y) = goal{c(y ) : y ∈ S x }.
If goal = min, P is called a minimization problem.
If goal = max, P is called a maximization problem.As examples consider the following two well known NP-hard optimization problems (see [9]): max balanced clique is the following maximization problem: Given a bipartite graph G, compute a balanced bipartite clique in G of maximum cardinality.
Here the instances are bipartite graphs, the feasible solutions for a graph G are balanced bipartite cliques in G, and the target function maps a balanced bipartite clique (X, Y ) to its size |X|.
min set cover is the following minimization problem: Given a hypergraph (V, H) withH = V , compute a family H ⊆ H of minimum cardinality covering V , i.e., H = V .
Let P = (X, (S x ) x∈X , c, goal) be an optimization problem.
A deterministic algorithm A for P can be thought of as a mapping from the instances X to the set of all possible outputs x∈X S x .
Then A is called an α-approximation algorithm for P with α : X → R ≥1 if for all x ∈ X with goal{c(y) : y ∈ S x } = OPT it holds that A(x) ∈ S x , i.e., the algorithm produces only feasible solutions, A runs in polynomial time, and1 α(x) OPT ≤ c(A(x)) ≤ α(x)OPT .
For such an algorithm we say that A approximates P within a factor of α.
If α(x) ≡ 1, A solves the problem exactly.
Note that the first inequality applies only to maximization problems, while the second applies only to minimization problems.
Since we require A to produce always feasible solutions, it holds that A(x) ≤ OPT in case goal=max and OPT ≤ A(x) in case goal=min.In frequent set mining (or frequent itemset mining) [2] the input is a hypergraph D = (I, T , μ) called dataset and a positive integer t ∈ {1, . . . , |T |} called frequency threshold.
Sometimes the elements of I are called items and the elements of T are called transactions.
For X ⊆ I the support set of X is defined asT [X] = {T ∈ T : X ⊆ T } .
X is called t-frequent in D if |T [X] | ≥ t.
We are now ready to give a formal definition of the problems of interest: Given a hypergraph (I, T , μ) and a frequency threshold t ∈ {1, . . . , |T |} we define max frequent set as the maximization problem to compute a t-frequent set X ⊆ I of maximum cardinality and min infrequent set as the minimization problem to compute a set X ⊆ I of minimum cardinality that is not t-frequent.
Remark 1.
In Section 1 we only discussed the use of the maximum resp.
the minimum cardinality of a frequent resp.
infrequent set.
Here we require the construction of an actual set in each problem.
However, these two tasks are polynomially equivalent.
In particular a maximum frequent set can be constructed by iteratively trying to remove an element and then checking whether the maximum cardinality has changed.Next we recall the construction used in [10] to prove hardness of max frequent set.
In Section 4 we will reuse this construction, which is a transformation from the NP-hard max balanced clique problem to max frequent set that uses a canonical correspondence between hypergraphs and bipartite graphs: For a given bipartite graphG = (V, U, E) construct a hypergraph D = (V, T , μ) with T = {Γ (u) : u ∈ U } μ : T → |{u ∈ U : Γ (u) = T }|where Γ (u) denotes the set of all neighbors of u, i.e., Γ (u) = {v ∈ V : {v, u} ∈ E}.
Note that size(D) ≤ size(G).
Furthermore, the maximum cardinality of a balanced bipartite clique in G is the maximum t such that there is a t-frequent set X in D with |X| ≥ t, which can easily be computed from D with an algorithm solving max frequent set.
This implies: To analyze min infrequent set we define the following generalized version of min set cover: min general set cover is the following minimization problem: Given a hypergraph (V, H) and a positive integer p ∈ {0, . . . , |V | − 1}, compute a minimum family of hyperedges H covering at least |V | − p elements of V , i.e., |V \ H | ≤ p.min general set cover contains the NP-hard problem min set cover as a special case (p = 0), and thus it is itself NP-hard.
Moreover, we have the following equivalence:Theorem 2.
min infrequent set is polynomially equivalent to min general set cover.Proof.
Construct a polynomial transformation f from min general set cover to min infrequent set by transposing the given incidence matrix and changing 0-entries to 1-entries and vice versa.
The frequency parameter t is set to p + 1.
Note that because of the parameter ranges of t and p this mapping is bijective.
For an instance ((V, H), p) this results in: So the original hyperedges act as items and every element v of the original ground set becomes a hyperedge, that contains all the sets, which v is not an element of (in the original min general set cover instance).
Now we claim that an (p + 1)-infrequent set in (H, V) corresponds to a set of hyperedges covering at least all but p elements of (V, H) and vice versa (see Fig. 1).
For a subset H ⊆ H it holds thatf : ((V, H), p) → ((H, V, μ), p + 1) V = {H \ H [{v}]: v ∈ V } μ : H → |{v ∈ V : H = H \ H [{v}]}| .
H (p+1)-infrequent in (H, V) ⇔ |V [H ] | < p + 1 ⇔ |{v ∈ V : H \ H [{v}] ⊇ H }| < p + 1 ⇔ |{v ∈ V : ∀H ∈ H v / ∈ H}| < p + 1 ⇔ |V \ H | < p + 1⇔ H covers at least all but p elements .
So an infrequent set of size k corresponds to a subfamily of size k covering sufficient many elements and vice versa.
Furthermore, f is a bijection implying polynomial equivalence.This implies the main result of this section completing our problem introduction:Corollary 3.
min infrequent set is NP-hard.
Since max frequent set is NP-hard, the next step is to ask for an approximation algorithm.
Proving negative results for the approximation of hard problems has been very successful in recent years.
New results have in common that they use so called 'probabilistically checkable proofs' [4] as a characterization of NP.As indicated by the proof of Theorem 1 the following result proved by Khot [12] for max balanced clique is of particular importance for our purpose:Unless there are probabilistic algorithms with an arbitrary small exponential time complexity for all problems in NP there is no polynomial approximation scheme for max balanced clique, i.e., the infimum of all constants k such that there is a k-approximation algorithm for max balanced clique is bounded away from 1.
It was known before that such a result, once achieved, can be boosted via derandomized graph products (introduced in [3]).
So that the result of Khot implies in fact:Theorem 4 (Khot [12]).
Unless for all > 0 and all decision problems in NP there is a probabilistic algorithm A accepting a YES-instance resp.
rejecting a NO-instance of size n with probability at least 2/3 in time 2 n the following holds: There is a constant δ BC > 0 such that there is no algorithm approximating max balanced clique within a factor of size(x) δBC for instances x.
Now suppose there is an algorithm A approximating max frequent set within a factor of α(size(x)) for instances x.
Then one can construct a hypergraph D from a given bipartite graph G as for Theorem 1 and find t APX the maximum t ∈ {1, . . . , |T |} for which |A(D, t)| ≥ t by running A at most |T | times.
Let (X, Y ) be a maximum balanced bipartite clique in G = (V, U, E) with size t OPT .
Any set of transactions corresponding to a subset Y ⊆ Y contains the t OPT items corresponding to X-in particular those with |Y | = t OPT /α(size(D)) = t * .
This implies for the maximum cardinality of a t * -frequent set in D, denoted as mfs(D, t * ), (G)), because the transformed instance is of equal or smaller size.
Since all necessary computations can be performed in polynomial time, we have a polynomial algorithm approximating max balanced clique within a factor of α(size(x)) for instances x and hence Corollary 5.
Under the same assumptions as in Theorem 4 with the same constant δ BC > 0 there is no algorithm approximating max frequent set within a factor of size(x) δBC for instances x.mfs(D, t * ) ≥ t OPT ⇒ |A(D, t * )| ≥ t OPT /α(size(D)) = t * .
But then t APX ≥ t * = t OPT /α(size(D)) ≥ t OPT /α(sizeAlthough stronger than P = NP the stated complexity assumption is still widely believed and thus we have a strong indication that there is no algorithm for max frequent set with a reasonable approximation factor.
The transformation in Theorem 2 maps instances of min general set cover to instances of min infrequent set with the same optimum value and vice versa and there is also a bijection between feasible solutions.
So an approximation algorithm for either one of the two problems will grant the same approximation factor for the other.
To analyze the approximability of the two problems we will use another related coverage problem: max coverage is the following maximization problem: Given a hypergraph (V, H) and a positive integer k, compute a family H ⊆ H of k hyperedges covering a maximum number of elements.Using the known fact that the approximation ratio of the greedy algorithm for this problem is (1−e −1 ) (see for instance [6]), one can analyze the approximation performance of the greedy approach for min general set cover.Theorem 6.
min general set cover can be approximated in polynomial time within a factor of ln(|V | − p)+1 for instances ((V, H), p).
Proof.
The following algorithm uses the greedy algorithm G for max coverage, to achieve the desired approximation rate for min general set cover.
Denote with n the number of elements |V | and with gsc(V, H, p) the minimum cardinality of a hyperedge set covering at least n − p elements.1.
i ← 1, S ← ∅, V 1 ← V, H 1 ← H 2.
while |V i | > p do 3.
k i ← min{j : | G(V i , H i , j)| ≥ e 1−i (1 − 1 e )(|V | − p)} 4.
H Δ ← G(V i , H i , k i ) S ← S ∪ H Δ , H i+1 ← H i \ H Δ , V i+1 ← V i \ H Δ 5.
i ← i + 1 6.
return SObviously S covers at least n − p elements after termination.
We claim that also |S| ≤ (ln(n − p)+1) gsc(V, H, p).
To see this, we first analyze the number of iterations and then the number of hyperedges added to the S in every iteration.
(i) The algorithm terminates after at most ln(n − p)+1 iterations.
Proof of (i): First show |V i | ≤ p + e 1−i (n − p) by induction on i. For i = 1 this is true, because |V | = |V 1 | = n.
Now assume that |V i | ≤ p + e 1−i (n − p) for a given i.
In line 3 k i is chosen such that at least e 1−i (1 − e −1 )(n − p) elements will be covered.
So|V i+1 | ≤ p + 1 e i−1 (n − p) − e − 1 e i (n − p) = p + 1 e i (n − p) .
Since the algorithm terminates when |V i | ≤ p (and |V i | cannot be fractional), it is for the number of iterations t: So O covers all but p elements.
Let mc(V , H , k) denote the maximum number of elements one can cover with k hyperedges in (V , H ).
Since in iteration i it ist ≤ min{i ∈ N : e i > n − p} = ln(n − p)+1 (i) (ii)For all iterations i it is k i ≤ gsc(V, H, p).
Require: Dataset D = (I, T , μ) and frequency threshold t Ensure: X infrequent and |X| ≤ (ln(|T | − t) + 1)OPT, with OPT the minimum cardinality of a set that is not t-frequent in D1.
X ← ∅ 2.
while |T | ≥ t do 3.
i ← i ∈ I with |T [{i}] | = min{|T [{i }] | : i ∈ I} 4.
X ← X ∪ {i} 5.
I ← I \ {i} 6.
T ← T [{i}] 7.
return X |V i | ≤ p + e 1−i (n − p), |O| elements can still cover at least e 1−i (n − p) elements.
It follows mc(V i , H i , gsc(V, H, p)) ≥ e 1−i (n − p) ⇒| G(V i , H i , gsc(V, H, p))| ≥ (1 − 1 e )e 1−i (n − p)and because k i is selected in line 3 as the minimum number satisfying this⇒k i ≤ gsc(V, H, p) .
(ii) Since k i sets are added to S in every iteration i, it follows from (i) and (ii) that |S| ≤ (ln(n − p)+1)gsc(V, H, p).
The polynomial running time is obvious, because the polynomial time greedy algorithm is called in every iteration at most |H| times.Remark 2.
The formulation of the algorithm in the above proof was tailor-made for the surrounding analysis.
In fact it only selects remaining hyperedges covering a maximum number of remaining elements and thus the simple greedy strategy stopping, when all but p elements are covered, will select the same hyperedges or possibly even some less.Algorithm 1 takes this into account and incorporates the transformation between min infrequent set and min general set cover.
Note that this transformation switches the roles of ground set and hyperedges so that the resulting approximation factor does not depend on the number of items but on the number of transactions.
This constitutes the following result: Corollary 7.
min infrequent set can be approximated within a factor of ln(|T | − t)+1 for instances (I, T , μ), t.The approximation ratio achieved above is close to optimal.
Otherwise, since min general set cover contains min set cover as a special case for p = 0, a better ratio would imply the existence of subexponential time algorithms with extremely small exponents for every problem in NP by the following theorem: Theorem 8 (Feige [8]).
For all > 0 there is no algorithm approximating min set cover within a factor of (1 − ) ln |V | for instances (V, H), unless for all problems in NP there is an algorithm running in time n O(log log n) for instances of size n.
In this paper, we have analyzed the algorithmical tasks to approximate a maximum frequent resp.
a minimum infrequent set.
This investigation is motivated by the need for an efficient parameter evaluation procedure that can be applied before a possibly exponential time pattern mining algorithm.
We turned to approximation algorithms because both problems are NP-hard.
In case of the maximization problem this was well-known.
In case of the minimization problem we proved this hardness by showing it to be equivalent to a generalized version of the min set cover problem.Using recent results from computational complexity we have argued that a nontrivial approximation algorithm for max frequent set is unlikely to exist.
For min infrequent set we gave a polynomial time greedy algorithm, which was proven to compute an infrequent set of cardinality smaller than ln(m − t) + 1 times the minimum cardinality of an infrequent set for instances with frequency threshold t and m transactions.
Slavík proved in [14] that the approximation ratio of the greedy algorithm for min set cover can in fact be bounded by ln n − ln ln n + 0.79.
It is likely that his tight analysis can be transfered to min general set cover, which is a task for possible future work.
The fact that the approximation factor depends on the number of transactions and not on the number of items indicates that the algorithm is useful for gene expression data [13], which can contain up to 100,000 items but typically only about 1000 transactions.
In general, knowing the approximation factor allows valuable conclusions.
If the cardinality of the returned set is c this implies that all sets of cardinality smaller than c/(ln(m − t)+1) are frequent.
In turn, this provides a lower bound on the number of frequent sets and for level-wise algorithms determines an earliest level where pruning can occur so that search need not to be started before this level.Other important characteristics that can be used for parameter evaluation are the number of frequent resp.
closed or maximal frequent sets resulting from a given parameter, all of which are hard counting problems [15,10].
It is an interesting question whether the positive results from computing the permanent of a 0-1 matrix can be transfered to those problems.
For 0-1-permanent the existence of a fully polynomial randomized approximation scheme has been shown [11].
Another question is, how quick parameter evaluation can be done in other domains with similar problems as frequent set mining (exponential output size and even greater search space).
Examples for such domains are pattern mining tasks with structured data like sequences or graphs.
