We define a type system, which may also be considered as a simple Hoare logic, for a fragment of an assembly language that deals with code pointers and jumps.
The typing is aimed at local reasoning in the sense that only the type of a code pointer is needed, and there is no need to know the whole code itself.
The main features of the type system are separation logic connectives for describing the heap, and polymorphic answer types of continuations for keeping track of jumps.
Specifically, we address an interaction between separation and answer types: frame rules for local reasoning in the presence of jumps are recovered by instantiating the answer type.
However, the instantiation of answer types is not sound for all types.
To guarantee soundness, we restrict instantiation to closed types, where the notion of closedness arises from biorthogonality (in a sense inspired by Krivine and Pitts).
A machine state is orthogonal to a disjoint heap if their combination does not lead to a fault.
Closed types are sets of machine states that are orthogonal to a set of heaps.
We use closed types as well-behaved answer types.
Low-level programming languages, such as intermediate or assembly languages, are challenging to reason about, since the abstractions that one can rely on in high-level languages are not present: storage and pointers must be managed explicitly, and the control flow may be similarly unstructured.
In recent years, there has been substantial progress in applying sophisticated type theories and logics to low-level languages, strongly motivated by verifying systems code and proof-carrying code [2,19].
In particular, separation logic [9,22] (see Reynolds's paper [27] for an overview) makes it possible to reason about heaps in a modular fashion: only the portion of the heap affected by the program fragment at hand needs to be considered, while frame rules can be used to infer that the remainder of the heap stays unchanged.
The central idea in separation logic is the spatial conjunction * : a Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
POPL'06 January 11-13, 2006 formula P * Q is true in a given heap if the heap can be split into two disjoint subheaps, such that P holds in one and Q in the other.The heap-splitting semantics of * then gives rise to frame rules: in a Hoare triple {P } c {Q} for a command c, another formula R can be added with the separating conjunction: {P } c {Q} {P * R} c {Q * R} Any part of the heap that is not mentioned in the specification of the command c cannot be altered by it, so we can assume that R stays invariant (ignoring "modifies" clauses, which are not about the heap).
However, the very format of frame rules assumes purely functional control behaviour: c is assumed to have a single entry and exit point, to which we can then add R.
If c could jump to some external label, the above rule would not be sound, unless * R were also somehow added to the precondition of the label.
(Recall that under the tight interpretation of separation logic it is generally not possible to jump to a label whose precondition is Q while passing along some extra heap, say Q * R.)In assembly language, of course, all control is jumping.
It appears, then, that in a language with arbitrary jumps we need to keep track of the control flow, perhaps using control flow analysis or a control effect system [10,14].
The problem is in fact similar to effect masking in effect systems, in that we need to keep track of all possible exits from and entries into the code.
However, it is not necessary to design a control effect system for assembly.
Since everything is in continuation-passing style, we can use the answer types of continuations to keep track of jumps.The notion of answer type of continuations may sound surprising to some readers in the light of the well-known classical typing [8] of control operators like call/cc.
The type of call/cc is Peirce's law ((A → ⊥) → A) → A where ⊥ is some empty type that may logically be read as falsity.
However, these classical typings are a feature of control operators in direct style.
If everything is converted to continuation passing style, there are non-trivial answer types for continuations.
The continuations never return to their point of call, but the answer type is not about returning to the point of call.
Rather, it refers to the global answer of the whole program, or more operationally, its behaviour.
A type like A − − * B, for instance, can be read as stating that the code is requiring some new heap satisfying A, and will then behave according to B.
The type B itself may again require more heap, or express a precondition on existing heap.
If the last instruction in the code is a jump to some label f , then the type of f determines what the behaviour is going to be.
In other words, the operation of jumping is polymorphic in the answer type.The general idea of answer type polymorphism [29,30] in the setting of the λ-calculus is as follows.
An expression in continuation-passing style expects a continuation, which it may then apply to a value.
If the expression has no control effects, it always applies the continuation; hence its type is of the form∀α.
(A → α) → αwhere α is not free in A.
The expression could also have a less rigid control behaviour.
It may for example expect two continuations [28], exactly one of which must be called (say for successful and abnormal exit).
This situation may be expressed with a typing like the following:∀α.
(A → α) → (A 񮽙 → α) → αagain assuming that α is not free in A or A 񮽙 .
By instantiating the answer type, additional levels of statepassing can be added [29].
Let S be a type of stores, and instantiate α to S → α 񮽙 .
Then we have∀α 񮽙 .
(A → S → α 񮽙 ) → (A 񮽙 → S → α 񮽙 ) → S → α 񮽙Note that the instantiation is done consistently for the two possible exits.
Thus answer type polymorphism in continuation-passing style gives us a kind of semantic control flow analysis.
It immediately makes powerful reasoning principles like parametricity [26] available.
Specifically, equivalences have been shown for continuation passing [29,30] using "Theorems for free" techniques [32].
The resemblance of answer type instantiation to frame rules may become a little more perspicuous with some uncurrying:∀α 񮽙 .
(((A × S) → α 񮽙 ) × ((A 񮽙 × S) → α 񮽙 ) × S) → α 񮽙To be sure, the analogy to continuation passing in the λ-calculus is a little naive, because in a language with assignment not everything is as well-behaved as in the λ-calculus.
In fact, if we read the × as analogous to logical conjunction, there is the evident problem that frame laws with (additive) conjunction rather than separating conjunction are unsound.
That is the main problem that will have to be addressed: only some substitutions for the answer type can be allowed.
These types correspond to frame rules with separating conjunction, but seen from the inside, so to speak, due to continuation-passing style.
Such types can be characterized as being orthogonal to a set of heaps, where a machine state is orthogonal to a heap if their combination does not lead to a run-time error.
The general notion of orthogonality between a term and its context is due to Krivine, Pitts and others [11,15,23,31], and is variously called biorthogonality, ⊥⊥-closure, or 񮽙񮽙-closure.
In a sense, we generalize orthogonality from stacks to heaps.
The connection to framing is that we define orthogonality using separating conjunction in such a way that only the interaction with disjoint heaps can be observed.In brief, the contributions of this paper include the following: • We give Hoare-style typing for code pointers with strong update; • frame rules are derived from answer type polymorphism in continuation-passing style; • the technique of biorthogonality (also called 񮽙񮽙-closure) is adapted to a language with heaps.
The paper is organized as follows.
We consider a small fragment of an assembly language; to clarify its intended meaning, we give an operational semantics in Section 2, before defining the type system in Section 3.
We discuss closed types (in the sense of biorthogonality) in Section 4, as a preparation for giving a realizability semantics based on this idea on top of the operational semantics in Section 5.
The main idea of this semantics is to recover frame rules, addressed in Section 6.
We then discuss a limitation of the present paper, the absence of dynamic recursion through the heap (Section 7), and conclude with a discussion of related and possible future work in Section 8.
We consider a very idealized fragment of an assembly language, with a straightforward operational semantics, which is fairly standard.
Despite its simplicity, it is sufficient for our purpose here, since it contains operations for strong update of code pointers.
Code blocks (basic blocks) are sequences of instructions ending in a jump.
They are defined by the following grammar:c ::= jmp f | jmp [p] | movc f p; c | movh p q; cThese instructions are the minimum we need for moving code pointers into the heap, updating them and jumping to them.
The instruction jmp f jumps to a fixed label f in the code segment, while jmp [p] jumps to a code pointer p in the heap (the brackets are intended to indicate indirect addressing).
A code address f from the code segment can be stored in the heap using movc f p, while the instruction movh p q moves the contents of heap cells.
We use c to range over basic blocks, f over immutable code addresses, and p, q, . . . over pointers.
A function mapping x to y is written as x 񮽙 → y, and we write dom(g) for the domain of definition of the function g. Both heaps and code segments will be modelled as such finite functions.A program consists of a currently executing basic block and a finite mapping from addresses fi to basic blocks ci, which we write as {f1 񮽙 → c1, . . . , fn 񮽙 → cn} Strictly speaking, one could distinguish between the program in which the fi are labels for the ci and the mapping in memory at runtime; we conflate them for simplicity.
A machine state or configuration 񮽙c | h | s񮽙 consists of a current code block c, a mutable data heap h (which may include code pointers), and an immutable code segment s. (The executing code block is in reality the code pointed to by the instruction pointer, but writing it separately makes the operational semantics more readable.)
More precisely, such machine states with a currently executing code block will be called active states.
In addition, we will also need a notion of passive state of the form 񮽙h | s񮽙 for a heap h and a code segment s.States can be combined to form larger ones, provided that their code segments agree on their intersection and their heaps are disjoint (as required by the * operation on heaps).
Two passive states together form another passive state, while an active and a passive state together form an active one.
In the latter case, control starts off in the part coming from the active state, but could later pass to the formerly passive one.The operational semantics is defined as a small-step transition relation 񮽙 between active machine states, given by the rules in There are two special code addresses exit and error, which are never in the domain of a code segment.
There is no code associated with them, and the semantics gets stuck if a jump to either of them is attempted.
Intuitively, one could think of these addresses as belonging to the surrounding operating system; jumping to them leaves the user code space and lets one observe termination, just as reduction to a value does in the context of a functional language.
The operational semantics also gets stuck if a memory access outside the current heap or code segment is attempted.
It should be straightforward to add more details to this semantics in the form of registers and a richer instruction set for data, building on assembly languages from the literature [20].
(Since registers cannot be aliased, they are more innocuous than the heap from the point of view of separation.)
However, the typing problems of code pointers that we are concerned with manifest themselves already in the semantics of this small fragment.񮽙
jmp f | h | s񮽙 񮽙 񮽙s(f ) | h | s񮽙 where f / ∈ {exit, error} 񮽙jmp [p] | h | s񮽙 񮽙 񮽙s(h(p)) | h | s񮽙 񮽙movc f p; c | h | s񮽙 񮽙 񮽙c | h[p 񮽙 → f ] | s񮽙 񮽙movh p q; c | h | s񮽙 񮽙 񮽙c | h[q 񮽙 → h(p)] | s񮽙As an example of the operational semantics, consider the following transition sequence, where looping arises from a code pointer update:񮽙movc f p; jmp f | p 񮽙 → g | f 񮽙 → jmp [p]񮽙 񮽙 񮽙jmp f | p 񮽙 → f | f 񮽙 → jmp [p]񮽙 񮽙 񮽙jmp [p] | p 񮽙 → f | f 񮽙 → jmp [p]񮽙 񮽙 񮽙jmp [p] | p 񮽙 → f | f 񮽙 → jmp [p]񮽙 񮽙 · · · The type system that we will use for the code pointer fragment is based on separation logic and bunched typing [21,24].
It uses Hoare-style typing, in which assignment changes the types; alternatively, it may be seen as a fragment of Hoare logic with only simple type-like assertions.
For simplicity, only a subset without recursion is considered, even though the operational semantics allows recursion, including recursion through the heap.
As with the operational semantics, the type system is only intended as a fragment to be integrated into a richer type theory.
Its purpose is to provide a foundational interface with continuationpassing semantics.
Derivations contain many − − * and ∀ introductions and eliminations that are operationally a "no-op", but more convenient idioms arise as derivable rules.The main design decisions concern code pointers.
The aim here is to reason locally only about the type (or specification) of a code pointer in the heap, without having to know what the actual code pointed to is.We assume that the code segment is immutable, and hence not a resource in the same way that the data heap is.
For the data heap, we specify exactly what the heap that we have access to contains.
For the code segment, we only demand that it contain enough code of the right type, but it may contain more code that we do not (yet) know about.
Of course, there may be situations when one wants to treat the code heap as a resource, for instance when dynamically loading code.
But it would be cumbersome to have to treat the code in the same explicit way as the data heap all the time.We distinguish between heap types A, behaviour types B and closed types C; see Figure 2 for their grammars.Heap types use the standard separation logic connectives.
The only novelty are points-to assertions of the form p 񮽙 → B, whose intended meaning is that the heap cell p points to code with type B.
This does not mean that the machine instructions are in the heap; rather, p points to some heap cell containing an address in the code segment.
B types specify the behaviour of code in terms of the arrow types of BI.
They also include type variables α, which are used for answer type polymorphism.
Other forms of polymorphism, in particular location polymorphism, would be natural features of a richer language, but are beyond the scope of the present paper.Types of the form C, where the use of → is disallowed, form a subset of well-behaved behaviour types (the sense of well-behaved will be defined as ⊥⊥-closedness in Section 4).
The rules of the type system are in Figure 3.
Γ | A 񮽙 c : BThe intended meaning is that the basic block c may rely on code typed according to Γ, and a heap typed with A in order to behave as typed by B.The rules for introduction and elimination of both arrow types (→ and − − * ) are silent in that there is no syntax for λ-abstraction.
They could also be written as two-way rules:Γ | A 񮽙 * A 񮽙 c : B Γ | A 񮽙 񮽙 c : A − − * B Γ | A 񮽙 ∧ A 񮽙 c : B Γ | A 񮽙 񮽙 c : A → BSince there are no λ-abstractions that bind any variables, arrow types are quite different from what may be familiar from functional languages.
They do not introduce a function, but express the preconditions of a jump.
An arrow type also does not delay execution, as it would in a call-by-value language.Since we assume the code segment to be immutable, its typing is reminiscent of declarations.
Code segments are typed using contexts of the form f1 񮽙 B1, . . . , fn 񮽙 Bn.
This typing ascribes the type Bi to the constant code pointer fi.
The code pointed to does not own any heap (emp), since we do not have closures.
If the code wants to make any assumptions about the heap, it has to use arrow types.Informally, one could read a type for a code pointer in the heap as an abbreviation for an existential(p 񮽙 → B) ≡ ∃f.(p 񮽙 → f ) ∧ (f f B)where the (f f B) is interpreted solely in the code segment, so that there is no conflict between the two pointers.
The existential for the code address is left implicit in indirect addressing.The typing rules contain the usual BI rules for weakening and contracting in a context A(−), e.g., a heap A * (A1 ∧ A1) can be contracted to A * A1.We also impose the following structural congruence, written as ≡ and used by the rule (≡) on types:• emp and * form a commutative monoid• true and ∧ form a commutative monoid• ((A * A 񮽙 ) − − * B) ≡ A − − * (A 񮽙 − − * B) • ((A ∧ A 񮽙 ) → B) ≡ A → (A 񮽙 → B)The interpretation of the separation logic connectives on the data heap is standard.
Existing work, for instance dealing withA ::= p 񮽙 → B | A * A | A ∧ A | emp | true (heap types) B ::= A − − * B | A → B | ∀α.B | α (behaviour types) C ::= α | A − − * C | ∀α.C (closed types) Γ ::= − | Γ, f B (code segments)Figure 2.
Syntax of typesCode blocks Γ | A 񮽙 c : B (JMP) Γ, f B, Γ 񮽙 | emp 񮽙 jmp f : B (INDJMP) Γ | p → ∀α.
((p 񮽙 → α) − − * C) 񮽙 jmp [p] : C where α is not free in C Γ | p 񮽙 → B * q 񮽙 → B 񮽙 c : C (MOVHEAP) Γ | p 񮽙 → B * q 񮽙 → B 񮽙 񮽙 movh p q; c : C Γ, f B, Γ 񮽙 | p 񮽙 → B 񮽙 c : C (MOVCODE) Γ, f B, Γ 񮽙 | p 񮽙 → B 񮽙 񮽙 movc f p; c : C Γ | A 񮽙 * A 񮽙 c : B (− − * I) Γ | A 񮽙 񮽙 c : A − − * B Γ | A 񮽙 񮽙 c : A − − * B (− − * E) Γ | A 񮽙 * A 񮽙 c : B Γ | A 񮽙 ∧ A 񮽙 c : B (→I) Γ | A 񮽙 񮽙 c : A → B Γ | A 񮽙 񮽙 c : A → B (→E) Γ | A 񮽙 ∧ A 񮽙 c : B Γ | A(A1) 񮽙 c : B (WEAKEN) Γ | A(A1 ∧ A2) 񮽙 c : B Γ | A(A1 ∧ A1) 񮽙 c : B (CONTR) Γ | A(A1) 񮽙 c : B Γ | A 񮽙 c : B (≡) Γ | A 񮽙 񮽙 c : B 񮽙 where A ≡ A 񮽙 and B ≡ B 񮽙 Γ | A 񮽙 c 񮽙 B (∀I) Γ | A 񮽙 c : ∀α.Bwhere α is not free inΓ or A Γ | A 񮽙 c : ∀α.B (∀E) Γ | A 񮽙 c : B[C/α] Code segments 񮽙 s : Γ (EMPTY) 񮽙 ∅ : − 񮽙 s : Γ Γ | emp 񮽙 c : B (ADDCODE) 񮽙 s ∪ {f 񮽙 → c} : Γ, f Bwhere f / ∈ dom(s) Figure 3.
Typing rules for the assembly language fragment data structures as well as allocation and deallocation of storage, should be easy to adapt [4,17].
The main difference from the languages typically used in the separation logic literature is that in assembly language there are no stack variables and everything is in the heap.
This does not cause any additional difficulties, apart from the notational inconvenience of having to refer to the heap all the time.
The rule for an indirect jumpΓ | p → ∀α.
((p 񮽙 → α) − − * C) 񮽙 jmp [p]: C deserves some explanation, since it is a deliberate restriction to avoid a knotty problem.
When we jump to some code pointer in the heap, the same pointer is still in the heap, and so it is passed to the code.
This is a form of self-application through the heap, so to type it in full generality, we would need recursive types.Since a semantic account of recursive types is beyond the scope of this paper, we compromise by restricting the indirect jumping to an idiom that avoids the recursion: the code pointed to by p may use p, but it cannot make any assumptions about the type of the code pointed to, due to the quantification.
This restriction breaks the potential recursion.
The idiom is motivated by return addresses: if p is some default location in which return addresses are passed in the function calling convention, then the code pointed to by p needs access to p to store the next return address when it calls the next function.
That means we cannot hide p from the code using some sort of frame rule.
However, in this idiom, all that the code does is overwrite p, so it can assume p to point to any type.
Other idioms are possible, but return addresses are a useful one for our purposes here, since once we can type return addresses, function calls arise as an idiom, and frame rules for functions can then be formulated in Section 6.
We revisit the more general form of jumping in Section 7.
The typing rules are akin to the so-called small axioms of separation logic.
In essence, such axioms only mention the heap that changes.
Whatever is left unchanged by the operation can then be added by frame laws.
In our setting, all framing takes place at the answer type.
For instance, consider the rule for storing a code pointer in the heap:Γ | p 񮽙 → B 񮽙 c : C Γ | p 񮽙 → B 񮽙 񮽙 movc f p; c : Cwhere Γ is of the form Γ = Γ1, f B, Γ2.
This rule states that the heap must consist exactly of the code pointer p to be overwritten.
Specializing C to A − − * C, we can use the rule for cases where there is additional heap A.Γ | (p 񮽙 → B) * A 񮽙 c : C (− − * I) Γ | p 񮽙 → B 񮽙 c : A − − * C (MOVCODE) Γ | p 񮽙 → B 񮽙 񮽙 movc f p; c : A − − * C (− − * E) Γ | (p 񮽙 → B 񮽙 ) * A 񮽙 movc f p; c : CHowever, we have to be careful how we specialize C. Suppose we allowed any type here, including A → C.
Then we could pick p 񮽙 → B for A and infer:Γ | (p 񮽙 → B) ∧ (p 񮽙 → B 񮽙 ) 񮽙 c : C (→I) Γ | p 񮽙 → B 񮽙 c : (p 񮽙 → B 񮽙 ) → C (MOVCODE) Γ | p 񮽙 → B 񮽙 񮽙 movc f p; c : (p 񮽙 → B 񮽙 ) → C (→E) Γ | (p 񮽙 → B 񮽙 ) ∧ (p 񮽙 → B 񮽙 ) 񮽙 movc f p; c : C (CONTR) Γ | (p 񮽙 → B 񮽙 ) 񮽙 movc f p; c : CNote that using → instead of − − * allows us to contract the disjunction ∧ in the last step.
This inference states that by storing f into p we can satisfy the precondition of the code c even though it may be inconsistent.
For instance, if our logic includes assertions like p 񮽙 → 1, we could have (p 񮽙 → 1) ∧ (p 񮽙 → 2) that holds for no heap.Specializing answer types with − − * is analogous to frame laws (using * ) in separation logic; specializing the answer with → amounts to unsound rules using ∧ in place of * .
Although the format of the inference above may look unfamiliar due to continuation-passing style, it is comparable to the unsoundness of ∧ rather than * in a putative frame law for traditional (direct-style) Hoare logic; a simple example of which is the following spurious inference:{x 񮽙 → 2} [x] := 3 {x 񮽙 → 3} ???
{x 񮽙 → 2 ∧ x 񮽙 → 2} [x] := 3 {x 񮽙 → 2 ∧ x 񮽙 → 3} Contraction {x 񮽙 → 2} [x] := 3 {false}The choice of possible answer types is behind many of the design decisions in the type system in Figure 3.
Answer types include type variables α, but the rule for ∀-elimination only allows these variables to be instantiated to closed types C.
The closed types do not include →-types, thus avoiding the framing problem outlined above.
The next section supports this choice with a more semantic view.
We need to abstract from code by considering only its interaction with all disjoint heaps.
To do so, we use a notion of ⊥⊥-closure of a type, which includes all machine states that cannot be distinguished by the interaction with disjoint heaps.It is not as evident as in functional languages what termination should mean in the context of assembly language.
Here we assume that successful termination consists of a jump to a special label called exit.
A notion of termination relative to a context will be built into an orthogonality relation −⊥− analogous to the ones used by Krivine, Pitts and others [11,15,23].
The contexts are built from disjoint heaps, using separating conjunction.
Since a heap may also contain pointers to known or unknown code, the heaps will be paired with a code segment that may overlap the current code segment; we refer to such pairs as passive states.
h # h 񮽙 iff dom(h) ∩ dom(h 񮽙 ) = ∅.
For code segments s and s 񮽙 , we write s s s 񮽙 iff s(f ) = s 񮽙 (f ) for all f ∈ dom(s) ∩ dom(s 񮽙 ).
Since code segments are assumed immutable, we do not demand that they be disjoint, only that they do not disagree on any code in their possible overlap.
We use A, A1 and A2 for sets of passive states, and B for sets of active states (since that is how in Section 5, types of the form A, respectively B, will be interpreted).
Given the definition of orthogonality, we define the biorthogonal or ⊥⊥-closure of a set of states (analogous to similar definitions in the literature [11,15,23,31]).
Let A be a set of passive states of the form 񮽙h | s񮽙, and B be a set of active states of the form 񮽙c | h | s񮽙.
We define the orthogonal A ⊥ of A and the orthogonal B ⊥ of B as follows:A ⊥ = {{c | h | s | c | h | s ⊥ h 񮽙 | s 񮽙 񮽙 for all 񮽙h 񮽙 | s 񮽙 ∈ A} B ⊥ = {{h 񮽙 | s 񮽙 | c | h | s񮽙 ⊥ ⊥h 񮽙 | s 񮽙 񮽙 for all 񮽙c | h | s ∈ B}The set B ⊥⊥ is called the biorthogonal or ⊥⊥-closure of B. B is called closed if B ⊥⊥ ⊆ B.As an example, consider an active state that loops, like Ω = 񮽙jmp f | ∅ | {f 񮽙 → jmp f }}񮽙{q 񮽙 → f } | {f 񮽙 → jmp exit}}The definition of (−) ⊥⊥ automatically entails a number of set-theoretic properties irrespective of the details of −⊥−; for instance, (−) ⊥⊥ is a closure operator [31].
Moreover, it enjoys some algebraic laws with regard to the separation logic connectives * and − − * , which we define as operations on sets of states: Definition 4.5 Let A1, A2 and A be sets of passive states, and B a set of active states.
We define:A1 * A2 = {{h1 * h2 | s1 ∪ s2񮽙 | h1 # h2, s1 񮽙 s2,and 񮽙hi | si ∈ Ai} A − − * B = {{c | h | s | h 񮽙 | s 񮽙 ∈ A, h # h 񮽙 , s s 񮽙 implies 񮽙c | h * h 񮽙 | s ∪ s 񮽙 ∈ B} emp = {{∅ | s񮽙 | s is any code segment}On the heap part of the states, this is the standard reading of the connectives.
The definition of (−) ⊥ (Definition 4.4 above) is analogous to the one of − − * in its use of the implication.
As one would expect, − − * is right adjoint to * : Lemma 4.6 Let A1 and A2 be sets of passive states and B a set of active states.
Then we have:(A1 * A2) − − * B = A1 − − * (A2 − − * B)The set of all active states that do not go wrong is the same as emp ⊥ , so that we have this lemma: Lemma 4.7 Let A be a set of passive states.
The operations (−) ⊥ and − − * satisfy: • emp ⊥ = {{c | h | s | c | h | s } • A ⊥ = A − − * (emp ⊥ ).
Proof Let 񮽙c | h | s񮽙 ∈ emp ⊥ .
The next lemma gives two rules linking the separating connectives * and − − * with the orthogonal (−) ⊥ .
Intuitively, an active state that can be combined with two disjoint passive states with types A1 and A2 is the same as an active state that, when first given an A1 state, could then be combined with an A2 state.
A passive state of type A together with one that can be combined with an active one of type B amounts to a passive state that can be combined with an active state of type A − − * B.Lemma 4.8 Let A1, A2 and A be sets of passive states and B a set of active states.
Then:(A1 * A2) ⊥ = A1 − − * A ⊥ 2 A * B ⊥ ⊆ (A − − * B) ⊥ Proof (A1 * A2) ⊥ = A1 − − * A ⊥2 follows from A ⊥ = A − − * emp ⊥ and the fact that − − * is right adjoint to * :(A1 * A2) ⊥ = (A1 * A2) − − * emp ⊥ = A1 − − * (A2 − − * emp ⊥ ) = A1 − − * (A ⊥ 2 )To prove A * B ⊥ ⊆ (A − − * B) ⊥ , we use the previous equality and the fact that (−) ⊥⊥ is a closure operator (so X ⊆ X ⊥⊥ for any X ).
Since (A − − * C) ⊥⊥ ⊆ (A * C ⊥ ) ⊥ = A − − * C ⊥⊥ ⊆ A − − * Cand hence the desired inclusion.
£ Lemma 4.9 is crucial for our approach to frame rules by way of instantiation of answer type variables by closed types.
What we have established in this section is a well-behaved and sufficiently rich set of possible answer types, which the semantics in the next Section will build on.
Using the notion of ⊥⊥-closed set from Section 4, we can now define a semantics for the type system from Figure 3 in Section 3.
Each type is interpreted as a set of untyped realizers.
More specifically, a type of the form A is interpreted as a set of passive states, and a behaviour type B as a set of active states (much as in Definition 4.5) .
A context Γ is interpreted as a set of code segments.
All interpretations are relative to a type environment η that maps type variables to sets of active states.
We require the type environment to be closed, that is, for each α, η(α) is a ⊥⊥-closed set of active states.
[[(A1 * A2) − − * B]] η = [[A1 − − * (A2 − − * B)]] η [[(A1 ∧ A2) → B]] η = [[A1 → (A2 → B)]] ηThe code segment is treated intuitionistically: moving from a code segment s to a larger one s 񮽙 with s ⊆ s 񮽙 does not change the type, unlike the tight interpretation of data heaps.Lemma 5.4 Let s ⊆ s 񮽙 be code segments.
Then: • 񮽙c | h | s񮽙 ∈ [[B]] η implies 񮽙c | h | s 񮽙 񮽙 ∈ [[B]] η • 񮽙h | s񮽙 ∈ [[A]] η implies 񮽙h | s 񮽙 񮽙 ∈ [[A]] η • s ∈ [[Γ]] η implies s 񮽙 ∈ [[Γ]] η Lemma 5.5 If s ∈ [[Γ, f B, Γ 񮽙 ]] η, then 񮽙s(f ) | ∅ | s񮽙 ∈ [[B]] η.
| h | s񮽙 ∈ [[B]] η, then also 񮽙c | h | s񮽙 ∈ [[B]] η.With the help of the preceding lemmas, we now prove soundness.
The type system in Figure 3 is sound with respect to the realizability semantics in Figure 4.
If we can infer a type for code in a judgement Γ | A 񮽙 c : B, then the code realizes B whenever it is placed in a machine state that is equipped with a code segment that realizes the code context Γ and a heap that realizes A.Theorem 5.7 Let η be a closed type environment.
•If Γ | A 񮽙 c : B is derivable, then s ∈ [[Γ]] η and 񮽙h | s񮽙 ∈ [[A]] η implies 񮽙c | h | s񮽙 ∈ [[B]] η.
• If 񮽙 s : Γ is derivable, then s ∈ [[Γ]] η.Proof (sketch) The proof proceeds by induction over the derivation of typing judgements.
Only a few cases are given here, emphasising the role of closed types.First, consider the typing rule for moving a label into a code pointer:Γ, f B, Γ 񮽙 | p 񮽙 → B 񮽙 c : C (MOVCODE) Γ, f B, Γ 񮽙 | p 񮽙 → B 񮽙 񮽙 movc f p; c : C Let s ∈ [[Γ, f B, Γ 񮽙 ]] η and 񮽙h | s񮽙 ∈ [[p 񮽙 → B 񮽙 ]] η.
The latter implies that h = {p 񮽙 → f 񮽙 } for some f 񮽙 .
We need to show that 񮽙movc f p; c | {p 񮽙 → f 񮽙 } | s񮽙 ∈ [[C]] η Since [[C]] η is ⊥⊥-closed by Lemma 5.2, it is sufficient to show membership of ([[C]] η) ⊥⊥ .
So suppose we have some 񮽙h1 | s1񮽙 ∈ ([[C]] η) ⊥ with dom(h) ∩ dom(h1) = ∅ and s(g) = s1(g) for all g ∈ dom(s) ∩ dom(s1).
We need to show that 񮽙movc f p; c | {p 񮽙 → f 񮽙 } * h1 | s ∪ s1񮽙 񮽙The first transition step of this state is:񮽙movc f p; c | {p 񮽙 → f 񮽙 } * h1 | s ∪ s1񮽙 񮽙 񮽙c | {p 񮽙 → f } * h1 | s ∪ s1񮽙 Now 񮽙{p 񮽙 → f } | s񮽙 ∈ [[p 񮽙 → B]] η due to s ∈ [[Γ, f B, Γ 񮽙 ]] η.
So by the induction hypothesis, 񮽙c | {p 񮽙 → f } | s񮽙 ∈ [[C]] η.
Since 񮽙h1 | s1񮽙 ∈ ([[C]] η) ⊥ , that implies 񮽙c | {p 񮽙 → f } * h1 | s ∪ s1񮽙 񮽙 so 񮽙movc f p; c | {p 񮽙 → f 񮽙 } * h1 | s ∪ s1񮽙 񮽙 as well.
As this holds for any 񮽙h1 | s1񮽙 ∈ ([[C]] η) ⊥ , we have that 񮽙movc f p; c | {p 񮽙 → f 񮽙 } | s񮽙 ∈ ([[C]] η) ⊥⊥ ⊆ [[C]] η as required.Next, consider the rule for an indirect jump along a code pointer:(INDJMP) Γ | p → ∀α.
((p 񮽙 → α) − − * C) 񮽙 jmp [p] : C We assume that α is not free in C. Let s ∈ [[Γ]] η and 񮽙h | s񮽙 ∈ [[p → ∀α.
((p 񮽙 → α) − − * C]] η.
Then we have h = {p 񮽙 → f } for some f with s(f ) = c such that 񮽙c | ∅ | s񮽙 ∈ [[∀α.
((p 񮽙 → α) − − * C]] η Let C = [[∀α.
((p 񮽙 → α) − − * C)]] η.
Then 񮽙c | ∅ | s񮽙 ∈ [[(p 񮽙 → α) − − * C]] (η[α → C]) = [[p → ∀α.
((p 񮽙 → α) − − * C)]] η − − * [[C]] ηFurthermore, the next transition step causes c to run:񮽙jmp [p] | {p 񮽙 → f } | s񮽙 񮽙 񮽙c | {p 񮽙 → f } | s񮽙 Since 񮽙{p 񮽙 → f } | s񮽙 ∈ [[p → ∀α.
((p 񮽙 → α) − − * C)]] η and trivially s s s, we then have by the definition of − − * that񮽙c | {p 񮽙 → f } | s񮽙 ∈ [[C]] η 񮽙c | h | s񮽙 ∈ [[A → B]] η iff 񮽙h | s 񮽙 񮽙 ∈ [[A]] η and s ⊆ s 񮽙 implies 񮽙c | h | s 񮽙 񮽙 ∈ [[B]] η 񮽙c | h | s񮽙 ∈ [[A − − * B]] η iff 񮽙h 񮽙 | s 񮽙 񮽙 ∈ [[A]] η with h # h 񮽙 and s s s 񮽙 implies 񮽙c | h * h 񮽙 | s ∪ s 񮽙 񮽙 ∈ [[B]] η 񮽙c | h | s񮽙 ∈ [[α]] η iff 񮽙c | h | s񮽙 ∈ η(α) 񮽙c | h | s񮽙 ∈ [[∀α.B]] η iff for all ⊥⊥-closed sets C of active states , 񮽙c | h | s񮽙 ∈ [[B]] (η[α → C]) 񮽙h | s񮽙 ∈ [[p 񮽙 → B]] η iff h = {p 񮽙 → f } and s(f ) = c with 񮽙c | ∅ | s񮽙 ∈ [[B]] η 񮽙h | s񮽙 ∈ [[A1 ∧ A2]] η iff 񮽙h | s񮽙 ∈ [[A1]] η, 񮽙h | s񮽙 ∈ [[A2]] η 񮽙h | s񮽙 ∈ [[A1 * A2]] η iff 񮽙h1 | s1񮽙 ∈ [[A1]] η, 񮽙h2 | s2񮽙 ∈ [[A2]] ηwhere h = h1 * h2, s1 ∪ s2 = s with h1 # h2 and s1 񮽙 s2񮽙h | s񮽙 ∈ [[emp]] η iff h = ∅ 񮽙h | s񮽙 ∈ [[true]] η iff h is any heap s ∈ [[Γ, f B]] η iff s ∈ [[Γ]] η and s(f ) = c with 񮽙c | ∅ | s񮽙 ∈ [[B]] η s ∈ [[−]] ηiff s is any code segment 񮽙jmp [p] | {p 񮽙 → f } | s񮽙 񮽙 񮽙c | {p 񮽙 → f } | s񮽙 we have 񮽙jmp [p] | {p 񮽙 → f } | s񮽙 ∈ [[C]] η as well, and we are done with this case.
For a direct jump, we do not need to assume a closed answer type:(JMP) Γ, f B, Γ 񮽙 | emp 񮽙 jmp f : B Suppose s ∈ [[Γ, f B, Γ 񮽙 ]] η and 񮽙h | s񮽙 ∈ [[emp]] η.
Now 񮽙h | s񮽙 ∈ [[emp]] η implies h = ∅ and s ∈ [[Γ, f B, Γ 񮽙 ]] η implies 񮽙s(f ) | ∅ | s񮽙 ∈ [[B]] η.
The next machine transition is: 񮽙jmp f | ∅ | s񮽙 񮽙 񮽙s(f ) | ∅ | s񮽙 By Lemma 5.6, that implies 񮽙jmp f | h | s񮽙 ∈ [[B]] η.For building up code segments from code blocks, we need to show the soundness of (ADDCODE).
Suppose Instantiation of the answer type to types of the form A − − * C gives us frame rules whose soundness follows from the soundness of the type system, Theorem 5.7.
In particular, we define function types as the evident continuation-passing type with a return address r:A1 ⇒r A2 ≡ ∀α.
(r 񮽙 → (∀αr.
(r 񮽙 → αr) − − * A2 − − * α)) − − * A1 − − * αThe answer type polymorphism [29,30] in this idiom means that the function must eventually invoke its return continuation or loop; it cannot jump to a different continuation instead.We then have a derivable frame rule for the function call idiom:Γ, f A1 ⇒r A2, Γ 񮽙 񮽙 jmp f : (A1 * A 񮽙 ) ⇒r (A2 * A 񮽙 )See Figure 5 for the derivation, which uses instantiation of the answer type variable and congruence for − − * types.Frame laws for continuation passing with quantified answer types are not restricted to purely functional idioms.
Figure 6 shows how to frame in A * − for an indirect jump of the form jmp [p].
Syntactically, the derived rule looks different from the usual Hoare logic one, since there are two − * A on the left of the 񮽙; they are however in a covariant and a contravariant position.Using the function type idiom ⇒r with the return address passed in r, the identity function (λx.x) can be compiled into the code jmp [r].
We type it as:− | emp 񮽙 jmp [r] : A ⇒r AWe then have the typing񮽙 {f 񮽙 → jmp [r]} : f f A ⇒r Afor the code segment holding the identity function at address f .
To write more involved examples, it would be useful to extend the language and typing fragment with more features that make it less cumbersome to pass around pointers.
Even the typing for jmp [p] used so far is a restriction to a special case, and we return to this point in the next section.
The rule for indirect jump in Figure 3 was explicitly designed so that the code being jumped to cannot subsequently make recursive calls to itself by an indirect jump through the pointer to itself that is still in the heap.
Peter Landin calls this recursion through the heap "tying a knot in the store".
A full account of mutual recursion between code and heap types is beyond the scope of this paper, so we only sketch what extensions are needed to accommodate general jumping, what could be typed using them, and how they relate to the type system studied in the previous sections.
Typing Figure 5.
Derivation of a frame law for the function call idiom Figure 6.
Framing for an indirect jump knots in the store with a Hoare-style typing is more challenging than in strongly typed functional languages like ML.
Since the Hoare typing tracks the state of the heap, it must be different before and after the knot has been tied.Γ, f A1 ⇒r A2, Γ 񮽙 | emp 񮽙 jmp f : A1 ⇒r A2 Γ, f A1 ⇒r A2, Γ 񮽙 | emp 񮽙 jmp f : ∀α.
(r 񮽙 → (∀αr.
(r 񮽙 → αr) − − * A2 − − * α)) − − * A1 − − * α (∀E) Γ, f A1 ⇒r A2, Γ 񮽙 | emp 񮽙 jmp f : (r 񮽙 → (∀αr.
(r 񮽙 → αr) − − * A2 − − * A 񮽙 − − * α 񮽙 )) − − * A1 − − * A 񮽙 − − * α 񮽙 (≡) Γ, f A1 ⇒r A2, Γ 񮽙 | emp 񮽙 jmp f : (r 񮽙 → (∀αr.
(r 񮽙 → αr) − − * (A2 * A 񮽙 ) − − * α 񮽙 )) − − * (A1 * A 񮽙 ) − − * α 񮽙 (∀I) Γ, f A1 ⇒r A2, Γ 񮽙 | emp 񮽙 jmp f : ∀α 񮽙 .
(r 񮽙 → (∀αr.
(r 񮽙 → αr) − − * (A1 * A 񮽙 ) − − * α 񮽙 )) − − * (A2 * A 񮽙 ) − − * α 񮽙 Γ, f A1 ⇒r A2, Γ 񮽙 | emp 񮽙 jmp f : (A1 * A 񮽙 ) ⇒r (A2 * A 񮽙 )(INDJMP) Γ | p → ∀αp.
((p 񮽙 → αp) − − * α) 񮽙 jmp [p] : α (≡) Γ | emp * p → ∀αp.
((p 񮽙 → αp) − − * α) 񮽙 jmp [p] : α (− − * I) Γ | emp 񮽙 jmp [p] : (p → ∀αp.
((p 񮽙 → αp) − − * α)) − − * α (∀I) Γ | emp 񮽙 jmp [p] : ∀α.
(p → ∀αp.
((p 񮽙 → αp) − − * α)) − − * α (∀E) Γ | emp 񮽙 jmp [p] : (p → ∀αp.
((p 񮽙 → αp) − − * A − − * α)) − − * A − − * α (≡) Γ | emp 񮽙 jmp [p] : (p → ∀αp.
(((p 񮽙 → αp) * A) − − * α)) − − * A − − * α (− − * E) Γ | emp * p → ∀αp.
(((p 񮽙 → αp) * A) − − * α) 񮽙 jmp [p] : A − − * α (− − * E) Γ | emp * (p → ∀αp.
(((p 񮽙 → αp) * A) − − * α)) * A 񮽙 jmp [p] : α (≡) Γ | (p → ∀αp.
(((p 񮽙 → αp) * A) − − * α)) * A 񮽙 jmp [p] : αAs a concrete example, here is some code in C that ties a knot in the store which then causes the function f to loop:void (*p)() = 0; void f() { (*p)(); } main() { p = &f; f(); }Initially, there is no recursion; f only becomes recursive by virtue of the assignment p = &f;.
If our specification for f requires it to be recursive, then we would not be able to call f until after the assignment to p.The same principle works with labels in a language with code pointers, like Gnu C, as in the following fragment (using the && operator to turn a label into a pointer):void *p; p = &&f; goto f; ... f: goto *p;Note that f: goto *p; does not by itself imply a loop; only the assignment creates the loop.To type the general form of an indirect jump, we need recursively typed heaps.
We write µφ.A for a recursively-defined heap, and assume that we can roll and unroll the recursion via a congruence µφ.A ≡ A[(µφ.A)/φ]A possible rule for general jumping is then:(µJMP) Γ | µφ.p 񮽙 → (φ − − * C) 񮽙 jmp [p] : C Given the operational semantics of jmp [p], 񮽙jmp [p] | h | s񮽙 񮽙 񮽙s(h(p)) | h | s񮽙 the precondition µφ.p 񮽙 → (φ − − * C)is plausible: it states that the heap contains at address p a pointer to some code that, given the same heap (including p itself), will produce an answer of type C. Assuming this rule, the code segment consisting of{f 񮽙 → jmp [p]} has type f f µφ.p 񮽙 → (φ − − * C)) − − * CAssuming this heap, we can then type the code that saves f itself in p and then jumps to f ; see Figure 7 for derivations for the code segment containing f and the current code that sets up the knot and then jumps to f .
Here C can be any type, in particular ∀α.α, which indicates that this code loops (which it does, see Section 2).
A jump in its most general form implies a self-application through the heap, so that it is instructive to compare the derivation in Figure 7 with the typing of the λ-term(λx.xx) (λx.xx)using a recursive type µα.
(α → B), in particular the unrolling that needs to happen before the self-application xx can be typed.
(µJMP) − | µφ.p 񮽙 → (φ − − * C) 񮽙 jmp [p] : C (≡) − | emp * (µφ.p 񮽙 → (φ − − * C)) 񮽙 jmp [p] : C (− − * I) − | emp 񮽙 jmp [p] : (µφ.p 񮽙 → (φ − − * C)) − − * C (EMPTY) 񮽙 ∅ : − (ADDCODE) 񮽙 {f 񮽙 → jmp [p]} : f f (µφ.p 񮽙 → (φ − − * C)) − − * C (JMP) f f (µφ.p 񮽙 → (φ − − * C)) − − * C | emp 񮽙 jmp f : (µφ.p 񮽙 → (φ − − * C)) − − * C (− − * E) f f (µφ.p 񮽙 → (φ − − * C)) − − * C | emp * µφ.p 񮽙 → (φ − − * C) 񮽙 jmp f : C (≡) f f (µφ.p 񮽙 → (φ − − * C)) − − * C | µφ.p 񮽙 → (φ − − * C) 񮽙 jmp f : C (≡) f f (µφ.p 񮽙 → (φ − − * C)) − − * C | p 񮽙 → ((µφ.p 񮽙 → (φ − − * C)) − − * C) 񮽙 jmp f : C (MOVCODE) f f (µφ.p 񮽙 → (φ − − * C)) − − * C | p 񮽙 → B 񮽙 movc f p; jmp f : Cp 񮽙 → (∀αp.
(p 񮽙 → αp) − − * C) p 񮽙 → (p 񮽙 → ((µφ.
(p 񮽙 → (φ − − * C))) − − * C) − − * C) = p 񮽙 → ((µφ.
(p 񮽙 → (φ − − * C))) − − * C) = µφ.
(p 񮽙 → (φ − − * C))The operational semantics of indirect jumping remains the same, but what is different is the view of what is passed along with the jump.
To summarize, it appears that, subject to the requirement for contravariant recursive types, recursion through dynamically created knots in the heap would fit quite smoothly into the present type system.
We have recovered frame rules from answer type polymorphism over ⊥⊥-closed types.
The potential recursion through code pointers in the heap remains a challenge, but may be amenable to similar techniques.
Work on typed assembly language [16], and typing heaps, such as substructural type systems [18], is part of the general background of the present paper.
Recent work on L3, a Linear Logic with Locations [17], also uses a relational style of semantics, rather than subject reduction, for soundness of a type system for a language with heap operations.
Type systems and logics for assembly languages usually assume more restricted control flow than pointers to unknown code; however, in recent work Ni and Shao have studied a language with embedded code pointers [20].
Much of the typing presented here is based on separation logic and bunched typing [21,24] in particular.
Code has to work not just given the currently known code segment, but all larger ones, to which it may gain access through code pointers, so it appears the semantics could be formulated as a possible-worlds semantics [25].
Apart from separation logic, substructural type systems and answer type polymorphism, one of the crucial ideas is biorthogonality.
It was invented by Krivine [11] and independently by Pitts, who defines a notion of 񮽙񮽙-closure of relations [23] in his relational parametricity for operational semantics.
Vouillon and Melliés have recently built on Krivine's work [15,31] by giving elegantly simple semantics for polymorphic and recursive types in an operational setting, as an alternative to approaches based on indexing [1].
Lindley and Stark [13] use Pitts's 񮽙񮽙-closure for termination proofs.Both in Krivine's and Pitts's version, orthogonality is a relation between a term and its continuation, which is syntactically represented as a stack of frames [7].
(Krivine also considers operating systems-level features such as the run-time clock [12] in realizability.)
In the present paper, we transfer the idea of orthogonality to assembly language, where there is no surrounding evaluation context; rather the continuation in this sense is the rest of the heap and code segment, and spatial separation is built into the definition of orthogonality.
It seems quite likely that Melliés's and Vouillon's recent work on recursive types in an operational setting [15] could be adopted to deal with the recursive types that are necessary to cover indirect jumps in full generality, as in Section 7.
In fact, this possible direction is one of the reasons why a framework inspired by their use of biorthogonality was used in this paper.
Principled operational techniques are also appropriate in this context since the literature on low-level languages typically presents them in terms of operational semantics.
Conversely, the mutual recursion between code (which operates on heaps) and heaps (which can point to code) provides concrete motivation for recursive types combined with polymorphism.Frame rules via answer type polymorphism should generalize from function calls to more general forms of jumping.
It remains to be seen if it can cover the hypothetical frame rule [22] or higherorder frame rules, perhaps even for non-functional control structure such as coroutines or system calls.
BI-style typing of continuations may also overcome some of the limitations of earlier work on linear continuation passing [3,5].
Multiplicative quantification over locations may be an alternative to biorthogonality for obtaining well-behaved answer type polymorphism [4].
The distinction between an immutable code segment and a writable heap could be seen as an instance of permissions [6] in separation logic, in that f fB corresponds to an execute permission and f 񮽙 → B to a read and write permission in operating systems.
By using execute permissions, it may be possible to unify the code segment and the heap to give greater flexibility than in the system presented here, for instance for dynamically loading code.
Thanks to Josh Berdine, Cristiano Calcagno and Peter O'Hearn for discussions, and to the anonymous POPL referees for comments.
