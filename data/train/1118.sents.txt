Symmetry of information establishes a relation between the information that x has about y (denoted I(x : y)) and the information that y has about x (denoted I(y : x)).
In classical information theory, the two are exactly equal, but in algorithmical information theory, there is a small excess quantity of information that differentiates the two terms, caused by the necessity of packaging information in a way that makes it accessible to algorithms.
It was shown in [Zim11] that in the case of strings with simple complexity (that is the Kolmogorov complexity of their Kolmogorov complexity is small), the relevant information can be packed in a very economical way, which leads to a tighter relation between I(x : y) and I(y : x) than the one provided in the classical symmetry-of-information theorem of Kolmogorov and Levin.
We give here a simpler proof of this result.
This result implies a van Lambalgen-type theorem for finite strings and plain complexity: If x is c-random and y is c-random relative to x, then xy is O(c)-random.
We show that a similar result holds for prefix-free complexity and weak-K-randomness.
In classical information theory, the information that X has about Y is equal to the information that Y has about X, i.e., I(X : Y ) = I(Y : X).
1 In algorithmical information theory, there exists a similar relation, but it is less perfect:I(x : y) ≈ I(y : x).
In this paper we take a closer look at the level of approximation indicated by "≈."
In the line above, x and y are binary strings of lengths n x and, respectively, n y , and I(x : y), the information in x about y, is defined by I(x : y) = C(y | n y ) − C(y | x, n y ), where C(·) is the plain Kolmogorov complexity.
The precise form of the equation is given by the classical Kolmogorov-Levin theorem [ZL70], which states that |I(x : y) − I(y : x)| = O(log n x + log n y ).
This is certainly an important result, but in some situations, it is not saying much.
Suppose, for example, that x has very little information about y, say, I(x : y) is a constant (not depending on x and y).
What does this tell about I(y : x)?
Just from the definition and ignoring I(x : y), I(y : x) can be as large as n x − O(1), and, somewhat surprisingly, given that I(x : y) = O(1), I(y : x) can still have the same order of magnitude.
As an example, consider y a c-random string (meaning C(y | n y ) ≥ n y − c), whose length n y is also c-random.
Let x be the binary encoding of n y .
Then I(x :y) = C(y | n y ) − C(y | x, n y ) = O(1), but I(y : x) = C(x | n x ) − C(x | y, n x ) = n x − O(1).
This example shows that even though, for some strings, the relation (1) is trivial, it is also tight.
In many applications, the excess term O(log n x + log n y ) does not hurt too much.
But sometimes it does.
For example, it is the reason why the KolmogorovLevin Theorem does not answer the following basic "direct product" question: If x and y are c-random n-bit strings and I(x : y) ≤ c, does it follow that xy isO(c)-random?The answer is positive (providing the finite analog of the van Lambalgen theorem).
It follows from a recent result of the author [Zim11], which establishes a more precise symmetry-of-information relation for strings with simple complexity, i.e., for strings x such that C (2) (x | n x ) is small, say, bounded by a constant.
2 Note that all random strings have simple complexity, and that there are other types of strings with simple complexity as well.
The main result in [Zim11] implies that if x and y are strings of equal length that have simple complexity bounded by c and I(x : y) ≤ c, then I(y : x) ≤ O(c).
The proof method in [Zim11] is based on randomness extraction.
Alexander Shen (personal communications) has observed that in fact a proof of the above result in [Zim11] can be obtained via the standard method used in the proof of Kolmogorov-Levin Theorem (see [DH10,LV08]).
We present here such a proof.We slightly extend the symmetry-of-information relation from [Zim11] to strings x and y that may have different lengths.
We prove the following (with the convention that log 0 = 0):Theorem 1 (Main Theorem).
For all strings x and y,I(y : x) ≤ I(x : y) + O(log I(x : y)) + O(log |n x − n y |) + δ(x, y), where δ(x, y) = O(C (2) (x | n x ) + C (2) (y | n y )).
Thus, for strings x and y with simple complexity, I(y : x) ≤ I(x : y) + O(log I(x : y)) + O(log |n x − n y |).
As mentioned, Theorem 1 implies an analog of the van Lambalgen Theorem for finite strings and plain Kolmogorov complexity.
Recall that van Lambalgen Theorem [vL87] states that for infinite sequences x and y the following two statements are equivalent:(1) x is random and y is random relative by x, (2) x ⊕ y is random.
(Here, random means Martin-Löf random, and x ⊕ y is the infinite sequence obtained by alternating the bits of x and y).
Theorem 1 implies immediately the following.Theorem 2 ([Zim11]).
Let x and y be two strings of length n such that x is c-random and y is c-random relative to x.
Then xy is O(c)-random.Formally, if C(x | n) ≥ n − c, C(y | x) ≥ n − c, then C(xy | 2n) ≥ 2n − O(c).
(The constant in the O(·) notation depends only on the universal Turing machine.)
Is there a van Lambalgen Theorem analog for prefix-free complexity?
To adress this question, we note that there are two notions of randomness for the prefix-free complexity K, weak K-randomness, and strong K-randomness (see [DH10]).
An n-bit string x is (1) weakly c-(K-random) if K(x | n) ≥ n − c, and (2) strongly c-(K-random) if K(x | n) ≥ n + K(n) − c.For weak K-randomness, we prove in Section 4 the following analog of the van Lambalgen Theorem.
For strong K-randomness, the question remains open.Theorem 3.
Let x and y be two strings of length n such that x is weakly c-(K-random) and y is weakly c-(K-random) relative to x.
Then xy is weakly O(c)-(K-random).
Formally, if K(x | n) ≥ n − c, K(y | x) ≥ n − c, then K(xy | 2n) ≥ 2n − O(c).
(The constant in the O(·) notation depends only on the universal Turing machine.)
The proofs of Symmetry of Information Theorems have a combinatorial nature.
To fix some ideas, let us first sketch the proof of the classical KolmogorovLevin Theorem which establishes relation (1).
The setting of the theorem, as well as for the rest of our discussion, is as follows: x and y are binary strings of lengths n x and, respectively, n y .
An easy observation (see Section 2.2), shows that a relation between I(y : x) and I(x : y) can be deduced from a "chain rule:" C(xy | n x , n y ) ≥ C(x | n x ) + C(y | x, n y ) − (small term).
In our case, to obtain (1), it is enough to show thatC(xy | n x , n y ) ≥ C(x | n x ) + C(y | x, n y ) − O(log n x + log n y ).
(2)Let us consider a 2 nx × 2 ny table with rows indexed by u ∈ {0, 1} nx and columns indexed by v ∈ {0, 1} ny .
Let t = C(xy | n x , n y ).
We assign boolean values to the cells of the table as follows.
The (u, v)-cell in the table is 1 if C(uv | n x , n y ) ≤ t and it is 0 otherwise.
The number of cells equal to 1 is less than 2 t+1 , because there are only 2 t+1 programs of length ≤ t. Let m be such that the number of 1's in the x row is in the interval (2 m−1 , 2 m ].
Note that, given x, n y and t, we can enumerate the 1-cells in the x-row and one of them is the (x, y) cell.
It follows that C(y | x, n y ) ≤ m + O(log t).
Now consider the set of rows that have at least 2 m−1 1s.
The number of such rows is at most 2 t+1 /2 m−1 = 2 t−m+2 .
We can effectively enumerate these rows if we are given n x , n y , m and t.
Since the x row is one of these rows, it follows that C(x | n x ) ≤ t − m + O(log n y + log m + log t).
Adding equations (3) and (4) and keeping into account that log m ≤ log n y and log t ≤ O(log n x + log n y ), the relation (2) follows.
A careful inspection of the proof reveals that the excess term O(log n x + log n y ) is caused by the fact that the enumerations involved in describing y and x need to know m (which is bounded by C(y | n y )) and t = C(xy | n x , n y ).
In case x and y are c-random, it is more economical to use randomness deficiencies.
In particular instead of using t = C(xy | n x , n y ), we can do a similar argument based on the randomness deficiency of xy, i.e., w = (n x + n y ) − C(xy | n x , n y ).
This is an observation of Chang, Lyuu, Ti and Shen [CLTS09], who attribute it to folklore.
For strings with simple complexity, it is advantageous to express t as t = C(x | n x ) + C(y | x) − w because the first two terms have a short description.
This yields a proof of the Main Theorem which we present in Section 3.
The Kolmogorov complexity of a string x is the length of the shortest effective description of x.
There are several versions of this notion.
We use here mainly the plain complexity, denoted C(x), and the conditional plain complexity of a string x given a string y, denoted C(x | y), which is the length of the shortest effective description of x given y.
The formal definitions are as follows.
We work over the binary alphabet {0, 1}.
A string is an element of {0, 1} * .
If x is a string, n x denotes its length.
Let M be a Turing machine that takes two input strings and outputs one string.
For any strings x and y, define the Kolmogorov complexity of x conditioned by y with respect to M , asC M (x | y) = min{|p| | M (p, y) = x}.
There is a universal Turing machine U with the following property: For every machine M there is a constant c M such that for all x, C U (x | y) ≤ C M (x | y) + c M .
We fix such a universal machine U and dropping the subscript, we write C(x | y) instead of C U (x | y).
We also write C(x) instead of C(x | λ) (where λ is the empty string).
If n is a natural number, C(n) denotes the Kolmogorov complexity of the binary representation of n.
We use C (2) (x|n x ) as a shorthand for C(C(x | n x ) | n x ).
For two strings x and y, the information in x about y is denoted I(x : y) and is defined as I(x : y) = C(y | n y ) − C(y | x, n y ).
Prefix-free complexity K is defined in a similar way, the difference being that the universal machine is required to be prefix-free.
In this paper, the constant hidden in the O(·) notation only depends on the universal Turing machine.In this paper, by convention log 0 = 0.
All the forms of the Symmetry of Information that we are aware of have been derived from the chain rule and this paper is no exception.
The chain rule states that C(xy | n x , n y ) ≈ C(x | n x ) + C(y | x, n y ).
For us it is of interest to have an accurate estimation of the "≈" relation.
It is immediate to see thatC(xy | n x , n y ) ≤ C(y | n y ) + C(x | y, n x ) + 2C (2) (y | n y ) + O(1).
If we show a form of the converse inequalityC(xy | n x , n y ) ≥ C(x | n x ) + C(y | x, n y ) − (small term),(5)then we deduce thatC(x | n x ) − C(x | y, n x ) ≤ C(y | n y ) − C(y | x, n y ) + 2C (2) (y | n y ) + (small term), i.e., I(y : x) ≤ I(x : y) + 2C (2) (y | n y ) + (small term).
Thus our efforts will be directed to establishing forms of the relation (5) in which (small term) is indeed small.
We demonstrate Theorem 1, using the standard proof method of the KolmogorovLevin Theorem.Let t x = C(x | n x ), t y = C(y | x, n y ) and t = C(xy | n x , n y ).
We take w = t x + t y − t, which is the term called (small term) in equation (5), and plays the role of randomness deficiency.
Our goal is to show that w = O(log I(x : y)) + O(log |n x − n y |) + δ(x, y), from which the Main Theorem follows (using the discussion in Section 2.2).
We assume that w > 0, otherwise there is nothing to prove.
We will need to handle information (|n x − n y |, t x , t y , w, b)), where b is a bit that indicates if n x > n y or not.
Let Λ be a string encoding in a self-delimiting way this information, and let λ be the length of Λ.
Note that λ ≤ 2 log w + O(C (2) (x | n x ) + C (2) (y | n y ) + log I(x : y) + log |n x − n y |).
We build a 2 nx × 2 ny boolean table, with rows and columns indexed by the strings in {0, 1} nx and, respectively, {0, 1} ny , and we set the value of cell (u, v) to be 1 if C(uv | n x , n y ) ≤ t, and to be 0 otherwise.
Let S be the set of 1-cells, and S u be the set of 1-cells in row u. Note that |S| ≤ 2 t+1 .
Let m be defined as 2 m−1 < |S x | ≤ 2 m .
We take F to be the set of "fat" rows, i.e., the set of rows having more than 2 m−1 1-cells.
We have |F | < |S|/2 m−1 ≤ 2 t−m+2 .
Note that x is in F , and that the elements of F can be effectively enumerated given the information Λ and m.
It follows that, given Λ and m, the string x can be described by its index in the enumeration of F .
This index can be written on exactly t − m + 2 bits, so that knowing t which can be deduced from Λ, we can reconstruct m.
It follows thatC(x | n x , Λ) ≤ t − m + 2 + O(1).
(6)Next we note that y is in S x and the elements of S x can be enumerated given x and Λ.
It follows that y can be described by its index in the enumeration of S x .
We obtain C(y | x, Λ) ≤ m + O(1).
From Equations (6) and (7), we conclude thatC(x|n x , Λ) + C(y|x, Λ) ≤ t + O(1) = t x + t y − w + O(1).
Note that t x = C(x | n x ) ≤ C(x | n x , Λ) + O(λ) and t y = C(y | x, n y ) ≤ C(y | x, Λ) + O(λ).
We obtain t x + t y ≤ t x + t y − w + O(λ).
Taking into account the estimation for λ, we have w −O(log w) = O(C (2) (x | n x )+C (2) (y | n y )+log I(x : y) + log |n x − n y |), from which, w = O(C (2) (x | n x ) + C (2) (y | n y ) + log I(x : y) + log |n x − n y |), as desired.
In this section we prove Theorem 3.
We introduce first the following notation: d is a self delimited encoding of the integer d.
We can have |d| ≤ 2 log d + 4.
For a string u, let A u (d) = {v ∈ {0, 1} n | K(uv | n) ≤ 2n − d}, and letF (d) = {u ∈ {0, 1} n | |A u (d)| ≥ 2 n−d }.
Consider the following program p 1 , which has x of length n on the oracle tape (as conditional information):On input the pair of positive integers (d, i), encoded as d bin(i), check if i is written on n − d bits.
If not, diverge.
If yes, enumerate strings u of length n such that K(xu | n) ≤ 2n − 2d.
Output the i-th string in the enumeration.Note that the domain of p 1 is prefix-free.
Let c 1 be the length of program p 1 .
Let p 2 be the following program, which has n on the oracle tape (as conditional information):On input the pair of positive integers (d, i), encoded as d bin(i), check if i is written on n − d bits.
If not, diverge.
If yes, enumerate the elements of F (d) and output the i-th one.Program p 2 is prefix-free.
Let c 2 be its length.Let d 0 = max{2(c + 4 + c 1 ), 2(c + 4 + c 2 )}.
If there is some i such thatp 1 (d 0 , i) outputs u, then K(u | x) ≤ n − d 0 + 2 log d 0 + 4 + c 1 < n − c. So, there is no i such that p 1 (d 0 , i) outputs y.
This can happen only if either (a) K(xy | n) > 2n − 2d 0 , or (b) K(xy | n) ≤ 2n − 2d 0 but there are 2 n−d0 strings u which are enumerated before y.If (a), we are done.
We show that (b) is impossible.
Indeed, if (b) holds, then |A x (d 0 )| ≥ 2 n−d0 and thus x is in F (d 0 ).
Also |F (d 0 )| ≤ 2 2n−2d0 /2 n−d0 = 2 n−d0 .
It follows that for some i, program p 2 on input (d 0 , i) outputs x.
But then K(x | n) < n − d 0 + 2 log d 0 + 4 + c 2 < n − c, which is a contradiction.
For symmetry of information, the argument from [Zim11] based on randomness extractors, a concept introduced and studied intensively in computational complexity, can be replaced by the simpler and more direct argument suggested by Sasha Shen and presented here.
Nevertheless, we still think that extractors play a role in Kolmogorov complexity theory.
They have been used in Kolmogorov complexity extraction (see [FHP + 11,HPV11,Mus12,Zim10d,Zim09], and the survey paper [Zim10a]) and, of course, this is not surprising given the analogy between Kolmogorov complexity extraction and randomness extraction.
But they have also been used for obtaining results that are not immediately connected to randomness extraction, such as counting results regarding dependent and independent strings [Zim10b], and independence amplification [Zim10c].
I am grateful to Alexander Shen who has noticed that the main theorem can be obtained with the method used in the standard proof of the Kolmogorov-Levin Theorem.
