A direct addressed cache is a hardware-software design for an energy-efficient microprocessor data cache.
Direct addressing allows software to access cache data without a hardware cache tag check.
These tag-unchecked loads and stores save the energy of a tag check when the compiler can guarantee an access will be to the same line as an earlier access.
We have added support for tag-unchecked loads and stores to C and Java compilers.
For Mediabench C programs , the compiler eliminates 16-76% of data cache tag accesses, with half of the benchmarks avoiding over 40% of the data tag checks.
For SPECjvm98 Java programs, the compiler eliminates 18-63% of data cache tag checks.
These tag check reductions translate into data cache energy savings of 9-40%, and overall processor and cache energy savings of 2-8%.
Reducing energy consumption is an important goal for processors that will be used in battery-powered devices.
Caches consume a large portion of total energy in processors targeted at low-power applications.
For example, 16% of the total processor and cache power for the StrongARM microprocessor is dissipated in the data cache [6].
Commercial low-power processors usually employ associative caches [2,6,10,13,18].
For associative caches, a significant portion of the total access energy is spent checking multiple tags to find where data resides in the cache.
For example, the highly-associative low-power cache designs used by the StrongARM and Xscale processors expend over 50% of the total cache access energy in the tag check [20].
In this paper, we propose a new hardware-software interface to reduce the energy cost of accessing cache data.
Direct addressing allows software to access cache data without the hardware performing a cache tag check.
These tagunchecked loads and stores save the energy of performing a tag check when the compiler can guarantee an access will be to the same line as an earlier access.
If the compiler cannot determine this information, or if cache lines are evicted due to interrupts or cache invalidations, direct addressing gracefully degrades back to conventional tag-checked accesses.We have implemented compiler support for direct addressing in the SUIF C compiler [8], and in FLEX, a Java bytecode-to-native compiler [7].
We evaluate our compiler algorithms using C programs from Mediabench, and Java programs from SPECjvm98.
Our results show we can eliminate 16-76% of all data cache tag accesses in C, and 18-63% of data cache tags checks in Java.
We have developed a detailed energy model of a power-optimized microprocessor and caches.
The reduction in cache tag checks results in data cache energy savings of 9-40% in C and 9-31% in Java.
The total processor plus cache energy savings is 2-8%.
The paper is structured as follows.
First we review current cache design in Section 2.
Section 3 describes the changes needed to implement direct addressing.
General compiler algorithms to support direct addressing are discussed in Section 4.
The algorithms and results specific to C are described in Section 5, and the algorithms and results for Java in Section 6.
Section 7 compares direct addressing to hardware schemes that remove tag checks.
Finally, we discuss related work and conclude.
Figure 1 shows the structure of a conventional virtually-indexed, virtually-tagged set-associative RAMtagged cache (for brevity, only virtual caches are considered here, but direct addressing can be applied to other types of caches).
An index taken from the virtual address is used to select a set consisting of several ways, and the tag field of the virtual address is compared against the tags in all ways to determine the location of the data.
An Ò-way associative cache performs Ò tag checks and Ò data reads in parallel, discarding all but one of the data values depending on the tag compares.
An alternative approach, used in many low-power microprocessors [2,6,10,13,18], is to store the tags in content- addressable memory (CAM).
The tag is broadcast across the cache lines and only the line whose tag matches has its data read out.
The energy consumption of a 32-way CAM-tag search is approximately the same as a 2-way set associative RAM-tag search [20,2] but has lower miss rates.
Caches are often subbanked to save energy and reduce delay, and a CAM-tag cache subbank is shown in Figure 2.
Although CAM-tag caches reduce miss rates and hence total absolute memory access energy, they expend relatively greater energy in tag checks.
Detailed HSpice simulations of a 16 KB CAM-tagged data cache divided into 1 KB subbanks, shows that the tag check consumes 54% of cache energy for loads and 43% for stores.For both RAM and CAM tag caches, searching tags is expensive.
If we could shortcut the process, by letting software tell the hardware in which way the line is located, we could save significant energy.
The problem is how to let software directly access cache lines without compromising inter-process protection and while preserving correct operation in the face of cache replacements or other cache coherence actions.
ing.
The lwlda instruction causes da2 to memoize the location of the data.
A subsequent lwda that used da2 would not power up the CAM bank on the left, but use the shaded DAR to pick this line.
Our approach to eliminating tag checks is to let software tell the hardware to remember the location of a cache line, so when software accesses the line again, hardware can access the data directly without searching tags.
We augment the processor state with some number of direct address registers (DARs).
These registers are set and used by software, and contain enough information to specify the exact location of a cache line in the cache data RAM as well as a valid bit.
The exact width and data layout of the DARs is hidden from software to avoid exposing the implementationdependent structure of the cache.
In particular, software is only made aware of the length of a cache line, but not the total cache capacity or associativity.
Table 1 shows the instruction extensions for using DARs.
Software places values in the DARs as an optional sideeffect of performing a load or store.
A tag-unchecked load or store specifies a full effective virtual address in addition to a DAR number.
If the DAR is valid, its contents are used to avoid a tag search; if it is invalid, hardware falls back to a full tag search using the entire virtual address.
The implementation described here uses a separate DAR specifier in each instruction, which takes 3 bits from the 16-bit immediate offset.
An alternative encoding is to implicitly associate a DAR with some set of base registers, which reduces ISA changes at the cost of complicating compiler register allocation.
We do not consider this option further in this paper.Direct addressing is only used for data caches.
Instruction caches have very regular access patterns and are only accessed via the program counter, and hence are amenable to software-invisible micro-architectural techniques to remove tag checks [16,18,19].
(l|s)wlda rt, off(rs), daLoad or store word, load direct address.
Perform regular load or store, and also set the direct address register da to the location of the referenced line.
(l|s)wda rt, off(rs), daLoad or store word, using direct address.
Data from the cache line pointed to by da is transferred to register rt (or the contents of rt is stored into the line specified by da).
The line offset bits of rs + off are used to pick the proper word in the line.
If da is invalid, the instruction acts like (l|s)wlda, accessing memory and setting the da register.
Jump register and invalidate direct address registers.
It acts like a jump register instruction, and also clears the valid bit on the DARs specified in the bitmask.
It is used on function return to invalidate the DARs used by the function.
As an example, consider the function entry code in Fig- ure 4, and a transformation of that code using direct addressing.
The swlda instruction sets up the da0 DAR, which is then used by the following swda instructions to eliminate cache tag checks.
Note that no additional instructions were added and that performance is identical.
At minimum, a DAR need only record the matching way within the cache set.
In this case, the effective address is used to obtain the subbank number, the set index, and the offset within the cache line.
In some implementations, however, it will be advantageous to also record subbank and set index information in the DARs and to physically distribute the DARs among the cache subbanks.
This avoids recalculating and retransmitting these portions of the virtual address for tag-unchecked accesses.The DARs incur additional area, energy, and delay overheads.
The primary energy penalty is the parasitic load of the DARs on the signal lines driving the cache, but this should be a negligible fraction of overall cache access energy.
The delay penalty is a single mux to select either one of the DARs or the normal cache access signal.For a RAM-tag cache, the DARs can record way hit/miss information locally in each way (each way is a subbank).
For a tag-unchecked access, the DAR specifier is broadcast to the ways, which replay the hit/miss information recorded in the local DAR latches without performing a tag check.The area and energy overhead of the DAR bits is small compared to the cache itself.
The delay penalty is only a fraction of a gate delay as the DAR hit/miss signal can be folded into the existing precharged tag comparator.For a CAM-tag cache, a DAR would be implemented as a unary bit vector with a single bit set on the matching row.
Each cache row would locally store one bit per DAR.
The DARs would be written with the local hit/miss signals generated by the CAM tag in each row.
For regular accesses, the parasitic energy overhead of the DARs is small because at most only one row's hit signal transitions high and one row's hit signal transitions low on any search.
There is an additional energy cost to writing a DAR, where the DAR clock line has to transition high and low, but this overhead is small compared to the saving from not driving multiple bits of address across the tag array when the DAR is next used.
As with the RAM-tag cache, the delay penalty is small if the DAR hit/miss signal is folded into the precharged match comparator.
The DARs must be kept coherent with the state of the cache.
If a line pointed to by a DAR is evicted, the DAR contents are no longer valid and cannot be used in a tagunchecked access.
Lines may be evicted either as a result of cache line replacement, or by external invalidate requests to maintain cache coherence with other processors or DMA I/O traffic.To maintain coherence, each DAR can be tagged with the address of the cache line to which it points.
On any eviction, the DAR tags are searched associatively and matching DARs are invalidated.
The next use of an invalid DAR will cause a regular tag-checked access (which will usually miss).
The DAR address tags need hold only a portion of the entire address allowing only a partial compare against the victim address, trading off some additional spurious invalidations for reduced complexity.
In the extreme case, the DAR tags can be omitted with all DARs invalidated on any eviction.The validity of the DARs can be checked right after the instruction decode of a tag-unchecked access.
If the register is not valid, the access is converted into a regular tagchecked access early in the instruction pipeline, well before reaching the memory access stage.
This avoids any additional memory access latency for checking valid bits.
Direct addressing has been implemented in two compiler systems, a SUIF-based C compiler and the FLEX Java native compiler.
This section describes compiler algorithms common to both systems.Both compilers use the same two step approach to eliminate tag checks with direct addressing.
First, find two references, one of which dominates the other, so all paths that cause the subordinate access to be executed cause the dominant reference to be executed first.
Second, prove that the two references always point to the same cache line.
The second reference can then skip the tag check, by having the dominant reference write a DAR that the subordinate reference reads.
Any other code between the two references, including assignments, control flow, or even function calls, can not affect correctness because hardware will invalidate DARs that point to lines that get evicted between the definition and the use of a DAR (as discussed in Section 3.2 above).
Both compilers control the stack pointer, ensuring it remains aligned to a cache boundary to simplify the determination of when two stack variables are on the same cache line.
This allows easy transformation of function entry/exit code (as in Figure 4), spill code, parameter passing code, and access to automatic variables.
The C and Java compilers use different methods to determine if two references to non-stack data (heap and static data) are to the same cache line.
These are discussed in Sections 5.1 and 6.1 respectively.
Each dominant reference with at least one subordinate reference to the same cache line is marked as a candidate for a DAR.
The DAR allocation problem is an instance of the standard register allocation problem -DAR candidates that are live at the the same program point interfere and need to be allocated to different hardware DARs.
DAR allocation is simpler than processor register allocation because DARs can not be spilled.
Instead of spilling, a DAR is simply not allocated to a problematic DAR candidate.The metric of utility we use for allocation is the number of tag checks eliminated by a certain DAR candidate minus the number of tag checks eliminated by the DAR candidates with which it interferes.
This causes small, non-interfering ranges to get good coverage, and the most important variables in regions of heavy DAR use are prioritized.
The compilers analyze one function at a time, and the DARs are caller invalidated-at function exit, the compiler invalidates the DARs used in the function.
If a function has a DAR live (say da3), and it makes a function call, the called function might invalidate da3, forcing a tag check on the use of that register.
To reduce the impact of inter-procedural DAR invalidates, we randomly permute the DAR numbers used by the allocator.
So one function might use registers in the order 7,2,3,6,0,5,1,4, another in the order 5,1,0,7,2,6,4,3.
Random permutation is much simpler than interprocedural analysis, and makes collisions between register numbers much less likely than if every function used the same order.
Interference is very low, and is quantified for C programs in Table 2 and for Java programs in Table 3.
We employ alignment and distance analysis for C to determine if two references are to the same cache line.
This section first describes alignment and distance analysis in our C compiler, and then discusses the results of our experiments.
Alignment analysis attempts to determine the address alignment of each static memory reference relative to a cache line boundary.
A value of 24 would indicate that the associated memory reference always accesses an address that is 24 bytes offset from the start of the cache line.
A load or store instruction is considered aligned when its cache alignment is the same for each dynamic execution of the instruction.
For instance, a global scalar resides in a static memory location and therefore always occupies a set alignment within the cache.
For the majority of memory operations however, this will not be the case.
Consider the loop in Figure 5(a).
Here, the store instruction will access sequential cache locations in each loop iteration and is therefore unaligned.In order to increase the percentage of aligned memory operations, our compiler performs a series of alignmentincreasing transformations.
One of the most important is loop unrolling.
The code in loop with unrolling.
After unrolling the loop by a factor consistent with the size of the cache line, we can guarantee that each memory operation in the loop only accesses the cache with a certain alignment.
This is the case in our example assuming that 񮽙 is an array of 64-bit data, and the cache line size is 32 bytes.Since inner loops comprise the majority of dynamically executed instructions, it is very important that we uncover as much alignment information as possible from the body of an inner loop.
Loop unrolling is effective for array references when the array is a local or global variable.
However, if the array in Figure 5 is passed as an argument to the enclosing function, then loop unrolling does not enable the analysis to guarantee alignment for the memory references within the loop since the base of the array is unknown.
Even worse is the case when the base of the array is actually aligned differently for different invocations of the function.To overcome this limitation, our compiler inserts a preloop that runs for a small number of iterations until the references within the loop reach a known alignment.
The code then jumps to an unrolled version of the loop where the alignment of references within the unrolled body are guaranteed ( Figure 5(c)).
Using this technique, the alignment analysis can determine the alignment for the majority of dynamically executed memory accesses.
In order to limit the number of pre-loop iterations that are executed, our compiler also uses profile-driven feedback to determine the best conditions to begin execution of the unrolled loop.One disadvantage of using loop unrolling to obtain alignment information is that too much unrolling can increase Icache pressure [11].
We did not measure the impact of this effect.
Distance analysis attempts to determine the byte distance between the addresses of two static memory references.
The algorithm is implemented as a dataflow analysis that operates on low-level address calculations.
If the difference between address calculations is a constant, then we know the distance between the references.In the initial compiler passes, when array accesses are represented at a high level, we tag them with their source array to aid in distance analysis.
We use this tag once the array access has been decomposed into pointer manipulation.
For accesses of the form 񮽙񮽙񮽙񮽙 and 񮽙񮽙񮽙 ·񮽙񮽙, our tagging allows us to compute the distance as 񮽙.
This pattern occurs very frequently in unrolled loops.We deal with aliasing using local information.
To be conservative, we assume a pointer variable can point to any globally visible address.
So a DAR definition and use will not span a pointer store to a base with a globally visible address.Once we know the distance, we can use the alignment to determine if two references are to the same cache line.
We find the alignment of the dominant reference relative to the cache line boundary and then find the distance between the subordinate access and the dominant access.
Simple arithmetic indicates if the references are on the same cache line.
An important special case is when the distance is 0, in which case we do not need to consult the alignment information.
We used the SUIF compiler [8] to output instrumented C code.
It acts like a C compiler with C as its target architecture.
A disadvantage of this approach is that the instrumented C code does not capture stack references for function entry/exit, spill code and parameter passing.
This will tend to underestimate the benefit of direct addressing as stack references provide many direct addressing opportunities, as quantified below in the Java evaluation.The instrumented code has loops unrolled and is augmented with statistics gathering code.
Every load and store in the program is analyzed and converted into a function call to our model.
We verify at runtime that our static analysis was accurate.
Figure 6 shows how many tag checks were eliminated for loads and stores for the Mediabench programs.
From the number of tag checks eliminated, we computed the Dcache energy savings based on our extracted layout for the cache [15].
This model has tag search consuming 54% of a load and 43% of a store, broken down further into 10%/8% (load/store) for address bus, 25%/40% for data access, and 11%/9% for data bus.
The results vary widely, with over 76% of checks eliminated for g721 decode (39.7% savings in data cache energy), down to 16.5% for epic.
Direct addressing saves some energy on every application and even the small 8.7% energy savings on epic is likely to be larger than any overhead direct addressing introduces.One reason for the spread is that some codes are more difficult to analyze, mostly due to pointer manipulation.
One example is mpeg2 decode, for which the compiler was unsuccessful on the code as distributed with Mediabench.
The code had one key loop which was manually unrolled, with a key matrix traversed in column-major order.
By making four small edits to the source code to express the loop in a natural way, and to traverse the matrix in row-major order (which is also better for cache performance), the percentage of tag checks eliminated went from 6.2% to 37%.
Table 2 shows the data cache energy saved, and also the energy saving for the whole processor core including instruction and data caches.
The energy consumption of the data cache relative to the entire core is highly dependent on the implementation.
Our core design is highly optimized for low-power, consuming 100-250 pJ per instruction at 300 MHz in a 0.25 񮽙m technology (񮽙100 mW).
For our design, we measured average data cache tag energy at 10% of the total core energy for Mediabench [15].
The Table clearly shows the importance of offset information.
While the results vary across benchmarks, most of the benefit of the DARs is not just from the program reusing the same location (0off column).
Our initial experiments indicated that 8 DARs captured most direct addressing opportunities across a range of benchmarks.
The 8DARlim column shows how many more tag checks could be eliminated with an unlimited number of DARs versus the 8 used for the rest of the results.
We compute this number by emitting liveness information for DAR candidates and doing post-hoc optimal register allocation.
Only toast is able to soak up many more tag checks with more registers (it can profitably use 44).
Every benchmark could make use of at least two DARs.
Random permutation of register numbers makes the interference of function calls very small, as seen in the f() column.
Finally, we see that each DAR value written is usually reused several times (r/w column), sometimes over 13 times, but averaging around 2-3 times.
Java bytecodes are normally interpreted directly or fed to a just-in-time compiler, but instead we used the FLEX compiler to compile Java bytecodes to MIPS assembly code.
Java-to-native compilation is a good alternative for lowpower environments if Java's dynamic loading capabilities are not usually needed, as the code can be highly optimized for low energy consumption.The FLEX implementation used the same dominance analysis and DAR allocation algorithms as the SUIF imple- mentation.
The following sections first describe how heap memory references are mapped onto cache lines for Java programs, and then discuss the results of our experiments.
Our approach to finding references to the same cache line is different in Java than it was in C. Java's type-safety and object-orientation means there is additional pointer information available to the compiler.All memory for Java objects comes from the system allocator.
We modify the memory allocator to ensure that small objects are never split across cache lines and that larger objects are always aligned to the start of a cache line.
The compiler can then simply determine cache-line equivalence based on object type and member field offset.
This determination is performed on a very low-level representation just prior to instruction selection, so even access to object header words (like the class descriptor and hashcode) are visible to this "cache-line equivalence" analysis.
This modified allocation policy potentially introduces fragmentation, which the allocator could deal with, e.g., by tracking "holes" and filling them in with small objects.This type-based analysis is very simple, but accounts for a large number of eliminated tag checks in strongly objectoriented benchmarks like jess or jack.
For more traditionally coded benchmarks, such as compress, there is need for further cache-line equivalence analysis of indexed array operations.As with the C implementation, loops are unrolled in Java to expose more direct addressing opportunities.
The unrolling strategy in Java is simpler: each loop which mentions an array is unrolled times, where C is the cache line length, and E is the element size of the array with the smallest elements in the loop.
This may over-unroll some loops, but guarantees that almost all the direct addressing opportunities are exposed.
If the first element accessed in the loop is not cache-line aligned, extra checks are placed within the unrolled loop to catch cache-line boundary crossings.To further expose direct addressing opportunities and improve performance, the FLEX compiler inlines small final methods.
FLEX outputs the MIPS instruction extensions for direct addressing (Table 1).
Due to the limited number of offset bits in the instruction encoding, some loads (that use the global pointer) take one instruction while some loads (to data that is further than 32 KB from any register) take two instructions.
The GNU assembler was modified to accept these instructions, and our extended MIPS ISA simulator models the state of the DARs (with dynamic correctness checks of DAR use).
The Java runtime is written in C, and was compiled with gcc 2.7.2 with a MIPS target.
The run- time is linked with the assembled Java code to give a MIPS binary that is run on the simulator.
The Java garbage collector was disabled for all runs.
The collector, like the runtime, is written in C.
The collector moves large amounts of data in memory with exact knowledge of object size and alignment, and so we expect that it could make heavy use of direct addressing.
Modifying the collector was beyond the scope of these experiments, but including the modified collector should only improve the relative benefits of direct addressing.Instead of modifying the system memory allocator to ensure cache alignment of heap data, we instead used conventional malloc and modified our checking code to ensure that all references are to the same 32-byte block of memory regardless of alignment.
Table 3 shows the percentage of tag checks eliminated for Java SPECjvm98 programs.
Unlike our C evaluation, we ran each Java binary on the detailed energy simulator [15] to get exact energy dissipation numbers (except for mpegaudio which ran for too long and was estimated at 10%, as with the C benchmarks).
Data cache tag check energy consumption was computed to be almost exactly 10% for every benchmark except raytrace, which has many memory accesses, and dissipates 13% of its energy in data cache tag checks.
The nSP column shows how many of our eliminated tag checks are to non-stack memory accesses.
Most of the stack accesses are function entry/exit, and these are easy for the compiler to transform.
The data for Java shows that stack references are about half (46-79%) of all memory references for SPECjvm98, and our analysis eliminates 67-82% of tag checks for these references.
This gives an indica- tion of the expected improvement if stack accesses were included in the SUIF C evaluation.
Table 3, like Table 2, shows the necessity of offset information.
The number of zero offset references (where the dominant and subordinate access are to the same location) is lower in Java than in C because much of the tag check elimination comes from stack accesses on function entry and exit.
These accesses load or store registers to sequential locations on the stack.The f() column is the percentage of accesses that have to be tag checked because a function call between a DAR definition and use invalidated the DAR.
As with our C benchmarks, random permutation of DAR numbers keeps this interference low.Finally in Table 4, the mSP column shows that by ignoring spill code and parameter accesses, we are not missing a major opportunity.
The generally low numbers indicate that the register allocator is not doing excessive spilling.Mpegaudio sticks out because there is excessive spilling in this benchmark.
Transforming the spill code to use direct addressing would get us a large part of the 52.0% of stack references which are not being analyzed.
This would bring mpegaudio into the 50-60% tag elimination range of the other applications.In order to transform spill code, we would generalize our direct register analysis and allocation to work on the postregister allocated version of the program (all the needed information is still available in FLEX).
Table 4.
All benchmarks were run with -s10, which is the middle sized spec input.
Jinst is the percentage of instructions executed in Java code.
The remainder executed in the runtime.
Jrefs is the percentage of memory references issued in Java.
JavaSP is the percentage of Java memory references that are to the stack.
RunSP is the percentage of memory references made to the stack by the Java runtime.
mSP is the maximum possible contribution to the tag unchecked references if we converted every remaining stack access-namely spill code and parameter access.
# inst/ld/st are the numbers of instructions, loads and stores from the Java code, not including the runtime.
In this section, we compare our direct addressing scheme for eliminating tag checks at compile time with dynamic hardware alternatives that are invisible to software.
One approach is for the hardware to remember the tag of the last cache line that was accessed and to compare this against the tag of the next memory access before enabling the tag search [2].
The main disadvantage of this scheme is that it adds a wide tag compare into the critical path of every cache access, adding several gate delays to this latency-sensitive path.
A variant of this scheme is to remember the last line accessed within each cache subbank, and only power up cache tags if a different line is accessed within each subbank.
Table 5 compares results for the C and Java benchmarks using these two schemes.
Using 8 DARs usually removes more tag checks than a hardware single last line buffer without the additional access latency, although with pgp the hardware scheme is significantly better.
The hardware and software techniques can be combined, with the last line buffer used in cases where the DARs were not specified or unsuccessful.
In this case, accesses will incur the additional cache access latency of the hardware scheme.
The results in the fourth column of Table 5 show that combining the techniques usually does better than using each alone, indicating that they are capturing different types of cache line reuse.The fifth column in Table 5 shows the results for the persubbank last line buffer (16 subbanks).
This removes many more tag checks than the previous schemes, but requires an extra tag comparator in each subbank and incurs the additional memory access latency.
Finally, the sixth column shows the effect of adding 8 DARs to the per-subbank last line buffers.
Here, there is little additional benefit (except for mipmap) as the hardware scheme has captured most of the available cache line reference locality.The results for the Java benchmarks are similar, with the hardware last line scheme eliminating roughly the same number of tag checks as the 8 DAR scheme, but with the additional memory access latency.
There is a smaller benefit to combining the hardware and software schemes for the Java programs, because the DARs only give benefit to the hardware schemes where the analysis was successful, as in jess and jack.
Again, the per-subbank last line scheme performs well, removing 80-90% of all tag checks.
The ARM instruction set includes load/store multiple instructions that can be used to avoid tag checks for sequential accesses to the same cache line [18].
These instructions are typically only used for procedure call/return, whereas our model allows significantly greater flexibility.
For example, the results we presented for the C Mediabench code were for non-stack accesses which are much less amenable to load/store multiple.Some researchers [2,14] have described hardware L0 caches designed for low power access.
These schemes have performance impacts, whereas the direct addressing scheme does not affect performance.
Direct addressing can also be combined with some of these hardware schemes to save further power.Other researchers [4,9,17] have developed software caching schemes that use compile-time information to reduce software tag checks.
FlexCache [17] adds HotPage registers, which are similar to DARs except they also hold a tag along with the direct address.
They are used as a small compiler-managed hardware tag array for a software associative cache.
The HotPage-likely compiler analysis implements static software way-prediction to index the likely HotPage register holding the translation for a given memory access.
The speculation is checked by a hardware compare of the virtual address with the HotPage tag.
The authors mention that an additional optimization, HotPagepredictable analysis [4], could avoid this tag check but do not include compiler algorithms or results.
In contrast, our work removes tag checks from a hardware associative cache scheme with no performance penalty, and our compiler analysis avoids tag checks by statically guaranteeing two accesses are to the same line.Fisher [12] and Ellis [3] were the first to use loop unrolling to improve the alignment of memory references in a loop body.
Their work was done in the context of a clustered VLIW in which main memory was divided among separate banks.
Their architecture supported a fast path to memory when data were located on a cluster's local memory bank.
Alignment of memory operations was therefore an important factor in machine performance.Barua et al. expanded on these ideas and introduced Modulo Unrolling [1].
This work introduced precise equations for determining the unroll factors for loop nests.
In Modulo Unrolling, outer loops may be unrolled to create aligned references outside the inner loop.
This work was done in the conext of the RAW machine [5] in which processor memory is distributed across processing tiles.
As is the case with the clustered VLIW, access to a local bank is faster than access to a remote bank.
Direct addressed caches provide a new hardwaresoftware interface to use energy of cache accesses.
Direct addressing uses compile-time information plus a minimal amount of hardware to remove data cache tag checks, thus saving energy.
Our implementations of direct addressing in a C and Java compiler resulted in data cache energy savings from 9-40% for C and 9-31% for Java.
In contrast to other cache energy saving techniques, direct addressing does not change the performance of the processor, it just reduces the amount of microarchitectural work the processor performs.
This work was funded by DARPA PAC/C award F30602-00-2-0562, NSF Grant CCR-0073510, DARPA/AFRL Contract F33615-00-C-1692, NSF Grant CCR00-86154, NSF Grant CCR00-63513, and NSF CAREER Grant CCR-0093354.
