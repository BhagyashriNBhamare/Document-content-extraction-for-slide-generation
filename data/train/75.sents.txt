We show that any n × n matrix A over any finite semiring can be preprocessed in O(n 2+ε) time, such that all subsequent vector multiplications with A can be performed in O(n 2 /(ε log n) 2) time, for all ε > 0.
The approach is combinatorial and can be implemented on a pointer machine or a (log n)-word RAM.
Some applications are described.
Matrix-vector multiplication is an absolutely fundamental operation, with countless applications in computer science and scientific computing.
Efficient algorithms for matrix-vector multiplication are of paramount importance.
However, the sheer size of the matrix can be an issue: if the matrix is dense, then Ω(n 2 ) time is certainly required for an n × n matrix.Suppose one allows for a slightly superquadratic (n 2+ε , for arbitrarily small ε > 0) preprocessing of the matrix.
How quickly can matrix-vector multiplication be done then?
This question has been studied since the beginning of scientific computing, but with an almost exclusive focus on special, structured matrices (such work is briefly surveyed in the next section).
To our knowledge, the general case of matrix-vector multiplication has not received attention.We first show how to matrix-vector multiply over the Boolean semiring in sub-quadratic time with preprocessing, for arbitrary matrices.
More precisely, the following theorem is proved.
Theorem 1.1 For all ε ∈ (0, 1/2), every n × n Boolean matrix A can be preprocessed in O(n 2+ε ) time such that every subsequent multiplication of A with an arbitrary Boolean n-vector x can be performed in O(n 2 /(ε log n) 2 ) time, on a pointer machine or a (log n)-word RAM.Both the preprocessing and matrix-vector multiplication algorithm are of the combinatorial/"nonalgebraic" variety.
At a high level, the preprocessing algorithm encodes O(n 2 /(ε log n) 2 ) lookup tables for the matrix in a directed graph that "correlates" the tables; the multiplication algorithm looks at one entry from each of the tables, using the graph to merge results.An immediate corollary of the theorem is a new combinatorial O(n 3 /(log n) 2 ) Boolean matrix multiplication algorithm, obtained by performing a matrix-vector multiplication for n times.
In fact, the theorem implies that Boolean matrix multiplication can be computed on-line in O(n 3 /(log n) 2 ) time, in the following sense: given an n × n matrix A, and supposing that the columns of an n × n matrix B are revealed one at a time, one can maintain the product of A with the currently known B in O(n 2 /(log n) 2 ) time per column.
Another application of our algorithm is that graphs can be preprocessed so that certain queries can be answered faster.
For example, one can determine if a given vertex subset is a dominating set in O(n 2 /(log n) 2 ) time.Building on our results in the Boolean setting, we generalize our algorithms to show that matrix-vector multiplication over any finite semiring can be sped up with preprocessing.
Theorem 1.2 Let (R, +, ×) be a semiring on K elements.
For all ε ∈ (0, 1), every n × n matrix A over R can be preprocessed in O(n 2+ε log 2 K ) time such that every subsequent matrix-vector multiplication can be performed in O(n 2 /(ε log n) 2 ) steps on a pointer machine or a (log n)-word RAM, assuming operations in R take constant time.
It would be infeasible to properly summarize all of the prior work on matrix preprocessing here; we shall settle for mentioning a few relevant (theory-based) references.
For more information on this topic, the reader is referred to Pan [P01].
As mentioned earlier, the major theoretical work in matrix preprocessing has focused on structured matrices.
For example, consider the case of Vandermonde matrices.
In 1955, Motzkin [M55] introduced the idea of preprocessing a polynomial such that subsequent evaluations of the polynomial can be done more efficiently.
Note that, after verifying a given matrix is Vandermode, computing a matrix-vector product with that matrix is equivalent to evaluating a univariate degree-n polynomial at n points.
Hence Motzkin's ideas applied to preprocessing for Vandermonde matrix-vector multiplication.
Later work on polynomial evaluation used ideas similar to the Fast Fourier Transform, obtaining algorithms for Vandermonde matrix-vector multiplication that run in O(n · poly(log n)) time after preprocessing (cf. [MB72, F72, BM75]).
In more recent work, Gohberg and Olshevsky [GO94] have generalized previous results, showing that several classes of structured matrices (including Vandermonde) admit matrix-vector multiplication in O(n log 2 n) time, after preprocessing.
Substantial works continuing in this direction have discovered fast matrix-vector multiplication algorithms for a large variety of structured matrices (e.g. Olshevsky and Shokrollai [OS00] and Pan [P00, P01]).
Besides the work on structured matrices, another related result is that of Savage [S74] who showed thirty years ago that n × n matrix-vector multiplication over semirings with s elements can be performed in O(n 2 / log s n) arithmetic operations, without preprocessing(!)
.
The key phrase here is "arithmetic operations": while only o(n 2 ) additions and multiplications are indeed performed, all n 2 entries of the matrix must obviously be read 1 .
Thus Savage's algorithm is not sub-quadratic when one counts its steps on a typical machine model.
(In interesting contrast, Winograd [W70] showed that in general, the number of arithmetic operations necessary to multiply an n×n matrix A with an n-vector x is the optimal Ω(n 2 ) bound.
The catch is that his proof requires the underlying algebra to have an unbounded number of elements.)
Let us briefly review some past combinatorial (non-algebraic) approaches to matrix multiplication.
The "Four Russians" algorithm of Arlazarov et al. [ADKF70] performs Boolean n × n matrix multiplication in O(n 3 / log n) time.
(For a reference in English, cf. Aho, Hopcroft, and Ullman's book [AHU74].)
Savage [S74] and Santoro [S80] observed that this time bound extends to a wide range of algebraic structures, assuming constant time arithmetic.
This was eventually improved slightly to O(n 3 / log 3/2 n) time by Atkinson and Santoro [AS88].
Rytter [R85] (and independently Basch, Khanna, and Motwani [BKM95]) gave an O(n 3 / log 2 n) algorithm for Boolean matrix multiplication on the (log n)-word RAM.
Chan [C06] has recently suggested (see the Discussion in the citation) that the original Arlazarov et al. algorithm may be modified to run in O(n 3 / log 2 n) on a pointer machine, although the (allegedly messy) details are not provided.Why Our Approach is Different.
To our knowledge, none of the above algorithms can be modified to solve the matrix-vector multiplication problem in O(n 2 / log 2 n) time.
We shall attempt to give some brief intuition as to why this is the case.
All above algorithms partition both input matrices into small blocks, where multiplications for the blocks are preprocessed and solved in advance.
For example, the log 2 n speedup in matrix multiplication comes from a product of two log n speedups, one from each input matrix.
In our problem, as only one matrix is being preprocessed, a log 2 n speedup of matrix-vector multiplication is not possible in the same manner 2 .
Therefore we believe our approach to be a truly different method, in this regard.A Note on Word Tricks.
It is sometimes the case that logarithmic speedups come from "word tricks" that exploit the word parallelism of a RAM.
We do not explicitly use word tricks in our algorithms; however, we do need a form of table lookup.
The only "suspicious" operation required is that a list of pointers to the neighbors of a node v in a graph can be obtained in O(deg(v)) time, where deg(v) is the degree of v.
In the literature, this is known as a neighborhood query and can be implemented on a pointer machine or (log n)-word RAM using a simple adjacency list representation.
Our algorithm uses this operation to encode multiple lookup tables in a common graph.
If one assumes that a neighborhood query requires Θ(deg(v) · α(n)) time for some function α, our algorithm still takes only O(n 2 α(n)/ log 2 n) to perform a matrix-vector multiplication.Semirings.
We recall the definition of a semiring.
A semiring is a triple (R, +, ×) such that R is a set with distinguished elements 0 and 1, and + and × are binary operations over R, satisfying the axioms:• (R, +) is a commutative monoid with identity 0,• x × 0 = 0 × x = 0• (R, ×) is a monoid with identity 1,• × distributes over +.
More succinctly, a semiring is essentially a ring that is not required to have additive inverses.
(Thus every ring is also a semiring.)
For brevity we often refer to the semiring (R, +, ×) as just "R" when there is no chance of confusion.
We begin by considering matrix-vector multiplication in the Boolean semiring (where R = {0, 1}, + is OR, and × is AND).
The main ideas introduced here carry over to the general case.
In the following, let n be a positive integer, A be an n × n Boolean matrix, x be a Boolean n-vector, and V = {1, . . . , n}.
Definition 3.1 For S ⊆ V , the neighborhood of S in directed graph (V, E) is the setN (S) := {j ∈ V | (∃k ∈ S)[(k, j) ∈ E]}.
Recall there is a simple correspondence between Boolean matrix-vector multiplication and neighborhood computations in a directed graph.Lemma 3.1 Let G A = (V, E) be the directed graph corresponding to A, and let x be an indicator vector for a subset S ⊆ V .
Then A T x is the indicator vector for N (S).
Proof.
Let j ∈ V .
Then (A T x)[j] = 1 ⇐⇒ n k=1 (A T [j, k] ∧ x[k]) = 1 ⇐⇒ n k=1 (A[k, j] ∧ x[k]) = 1 ⇐⇒ (∃k ∈ S)[(k, j) ∈ E and k ∈ S] ⇐⇒ j ∈ N (S).񮽙
Therefore a Boolean matrix-vector multiplication is equivalent to computing the neighborhood of a given node subset.
From here on, we focus on the problem of preprocessing a graph such that neighborhood queries for a given node subset can be done in O(n 2 /(log n) 2 ) time.
In this section, we establish Theorem 1.1 from the Introduction.
Given a graph G, the preprocessing phase constructs a new graph H on O(n 1+ε /(ε log n)) nodes and O(n 2+ε /(ε log n) 2 ) edges.
Neighborhood subset queries shall be handled by performing local operations on portions of H. Intuitively, the graph H encodes O(n 2 /(ε log n) 2 ) different lookup tables, one for each (ε log n)×(ε log n) block of the adjacency matrix of G.Without loss of generality, assume that ε log n divides n. For i = 1, . . . , n/ε log n, defineP i = {(i − 1) · ε log n + 1, . . . , i · ε log n},and define Π to be the partition {P 1 , . . . , P n/ε log n } of [n].
The graph H has two layers of nodes, both having O(n 1+ε /(ε log n)) nodes.
In particular both layers have n/ε log n "groups", of O(n ε ) nodes each 3 .
Each group corresponds to a part P i of Π, and each of the O(n ε ) nodes in a group of H represents one of the possible subsets of P i .
Therefore the number of nodes is O(n 1+ε /(ε log n)).
Our query algorithm uses the simple fact that a subset of nodes of G can be represented as a set of n/(ε log n) nodes in a layer of H. Definition 3.2 Let x be a Boolean n-vector, and let 񮽙 ∈ {1, 2}.
The 񮽙th layer node representation of x is the unique list of vertices v 1 , . . . , v n/(ε log n) in H such that• for all i, v i is in the ith group of layer 񮽙, and• the indicator vector for the subset corresponding to v i is equal to x[(i−1)ε log n+1, . . . , iε log n], i.e. bits (i − 1)ε log n + 1 through iε log n of x.That is, each v i corresponds to a distinct set of ε log n bits of x.Definition 3.3 Let S be a set of n/(ε log n) nodes, from either the first or second layer of H, such that each node is from a different group.
The vector representation of S is the indicator vector for the set obtained by taking the union of all node sets corresponding to the nodes in S.We remark that on a computational model with Θ(poly(log n)) cost per random access, the two representations above can be computed in O(n · poly(log n)) time.
In one matrix-vector multiplication, both of these representations are computed only once.We now specify where the edges of H are placed.First Layer Edges of H. Let v be a node in the first layer.
Recall v corresponds to a subset S v of V , of size at most ε log n. Note that N (S v ) can easily be determined in O(n · ε log n) time 4 .
Let x 1 , . . . , x n/(ε log n) be the second-layer node representation of N (S v ).
Then the edges out of v are defined to be (v, x 1 ), . . ., (v, x n/(ε log n) ).
The number of outgoing edges from nodes in the first layer is O(n 1+ε /(ε log n) · n/(ε log n)) = O(n 2+ε /(ε log n) 2 ).
As there are O(n 1+ε /(ε log n)) nodes in the first layer, the above edges can be determined in O(n 1+ε /(ε log n) · n · ε log n) = O(n 2+ε ) time.Second Layer Edges of H.
In the second layer, there is an edge (u, v) between u and v in the same group if and only if, when construed as subsets of V , u is a subset of v. Therefore, the second layer consists of n/(ε log n) disjoint copies of the same n ε -node DAG.
(In particular, this DAG is the transitive closure of the directed hypercube on 2 ε log n nodes.)
Thus the number of edges between nodes in the second layer is O(n/(ε log n) · n 2ε ) = O(n 1+2ε /(ε log n)).
The figure below gives a bird's eye view of H.Processing Neighborhood Subset Queries.
Neighborhood queries for a node subset are performed on H as follows.1.
Given a node subset S as a Boolean n-vector, determine the first-layer node representation of S. Let T be the set of n/(ε log n) nodes in this representation.2.
Put a mark on every node of the set N (T ) in H. Recall that the degree of each node in the first layer is n/(ε log n).
Assuming the neighbors of a node v can be marked in O(deg(v)) time, the nodes of N (T ) can be marked in O(|T | · n (ε log n) ) = O(n 2 /(ε log n) 2 ) time.
(Note this stage is the bottleneck in the algorithm's runtime.)
n ε n ε log n n ε n ε . . . . . .
ε log n ε log n ε log n Figure 1: The graph H. Both layers have n 1+ε /(ε log n) nodes, divided into n/(ε log n) groups.
Each node in H corresponds to a subset of nodes in G, with cardinality at most ε log n.
In the example above, the vertices labelled 00 · · · 00 represent empty sets, and the vertices labelled 11 · · · 11 represent the set of all ε log n nodes in that respective part.
Since the empty set node in the first layer has no neighbors, its edges point to the empty sets of the nodes in the second layer.3.
Fix a topological order on nodes to be used for all groups (recall that each group in the second layer is a copy of a certain DAG).
Each group of n ε nodes in the second layer is processed separately as follows.
Specially mark the first node u in the topological order such that all marked nodes in the group have an edge to u. To do this, obtain a count t of the total number of marked nodes in the group, then for each node in order, count its marked predecessors and compare that count with t.
This needs at most O(n 2ε log n) time per group, as each edge is accessed at most once.
The total runtime for this stage is O 񮽙 n ε log n · n 2ε log n 񮽙 = O(n 1+2ε ).
For ε < 1/2, this is less than O(n 2 /(ε log n) 2 ).4.
Observe that each group has exactly one specially marked node.
Compute the vector representation of the set of specially marked nodes, and finally, erase all node markings.This concludes the description of the neighborhood query algorithm.
Observe that the runtime of the query algorithm is dominated by the second step.
The next lemma proves correctness.Lemma 3.2 The vector representation of the set of specially marked nodes is N (S).
Proof.
For j = 1, . . . , n/(ε log n), let S j denote the subset of S restricted to part P j .
For each i = 1, . . . , n/(ε log n), consider the ith group of nodes in the second layer of H, representing a subset of nodes of G in part P i .
When a node v in the ith group is marked, then there is a j such that the node representing S j in the first layer has an edge to v. By construction of H, the nodes of G in the subset corresponding to v are exactly the neighbors of S j that lie in P i .
It follows that the collection of all marked nodes in the ith group corresponds to all neighbors of S that lie in the part P i .
More precisely, the union of the node sets in G corresponding to the marked nodes in group i of H gives bits (i − 1) · ε log n + 1 through i · ε log n of the indicator vector for N (S).
Over all n/(ε log n) groups, these unions give all the bits in the indicator vector for N (S).
We claim that the third step in the algorithm accomplishes exactly this, in that for each group, the algorithm specially marks the node corresponding to the union of the sets represented by marked nodes in that group.
By finding a node u in a group such that all marked nodes in that group have edges to it, this implies that the node subset in G corresponding to u is a superset of all node sets in G corresponding to the marked nodes.
By finding the first node in the topological ordering with this property, the node u is the smallest cardinality node subset in G that is a superset of all sets in G corresponding to the marked nodes.
That is, u corresponds to the union of the sets represented by marked nodes in the current group. 񮽙
Beyond on-line matrix multiplication, another application of faster Boolean matrix-vector multiplication is that certain types of graph queries can be answered more rapidly than one might initially believe is possible.
Here we give a few examples of such queries.Recall that a subset S of vertices is dominating if and only if every node in the graph is either in S or has a neighbor in S, S is independent if and only if there is no edge between any two nodes in S, and S is a vertex cover if every edge has an endpoint in S. Note S is independent if and only if V − S is a vertex cover.
The following proposition is straightforward.Proposition 1 S ⊆ V is a dominating set if and only if N (S) ∪ S = V .
S ⊆ V is independent if and only if N (S) ∩ S = ∅.
Hence the Boolean matrix-vector multiplication algorithm can be used to more efficiently determine if a given query subset S is dominating, independent, or a vertex cover.Corollary 3.1 A graph G can be preprocessed in O(n 2+ε ) time such that one can determine in O(n 2 /(ε log n) 2 ) time if a given subset of nodes is dominating, independent, or a vertex cover.The problem of finding an independent set or dominating set is quite general.
As a result, certain other graph queries can also be performed in subquadratic time as well.
For example, one can determine if a given query node participates in a triangle.Corollary 3.2 A graph G can be preprocessed in O(n 2+ε ) time such that one can determine in O(n 2 /(ε log n) 2 ) time if a given query node is in a 3-cycle.
Proof.
Let v u be the neighborhood vector for the query node u, obtainable in O(n) time.
Determine if the set of vertices denoted by v u is independent, in O(n 2 / log 2 n) time.
But this set is independent if and only if every pair of neighbors of u do not have an edge between them, i.e. if and only if u does not participate in a 3-cycle. 񮽙
Note that in the absence of preprocessing, these tasks require Ω(n 2 ) time on dense graphs.
The matrix-vector multiplication scheme can be extended to all finite semirings, with a few modifications.
We recall the statement of Theorem 1.2 from the Introduction for convenience: Theorem 1.2: Let (R, +, ×) be a semiring on K elements.
For all ε ∈ (0, 1), every n × n matrix A over R can be preprocessed in O(n 2+ε log 2 K ) time such that every subsequent matrixvector multiplication can be performed in O(n 2 /(ε log n) 2 ) steps, assuming operations in R take constant time.Proof.
As in the Boolean case, we build a graph H with two layers, and each layer has n/(ε log n) groups of nodes, where each group represents a part P i .
However, four changes are made to the preprocessing phase:1.
The number of nodes in a group is now K ε log 2 n = n ε log 2 K , for all of the possible values of a vector over R with ε log n components.2.
There are no edges between nodes in the second layer.3.
The edges from the first layer to the second layer are redefined as follows.
Each ε log n part of an input vector, when multiplied with the corresponding n × ε log n submatrix of A, contributes an n-vector to the overall matrix-vector product.
For a node u in the first layer, let x u be its corresponding ε log n vector over R, and let y u be the corresponding nvector obtained from left-multiplying x u by the proper n × ε log n submatrix of A. Finally, let v 1 , . . . , v n/(ε log n) be the second layer node representation of the vector y u .
Then u has outgoing edges to v 1 , . . . , v n/(ε log n) .
For each node u, its neighbors can be computed in O(n · ε log n) steps.
Therefore O(n 1+ε log 2 K /(ε log n) · n · ε log n) = O(n 2+ε log 2 K ) semiring operations and computation steps suffice for determining the edges.4.
For every node u in the second layer, maintain a variable c u that is set to 0 ∈ R at the start of a query.Multiplication of A with an n-vector x is performed as follows:1.
For all n/(ε log n) nodes in the first layer node representation of x, starting with the node in group 1, compute the node's list of neighbors u 1 , . . . , u n/(ε log n) in the second layer, and set c u i := c u i + 1 for all i, where + is over the semiring.
These counts and neighbor queries can be computed in O(n 2 /(ε log 2 n)) additions and computation steps, using fast neighborhood queries and the assumption that operations in R take constant time.2.
For each node u in the second layer, let y u be its corresponding ε log n vector over R. Set y u := c u × y u .
That is, y u is obtained by semiring-multiplying the scalar c u with each component of y u .
This takes O(ε log n) arithmetic operations for each node in the second layer.3.
For each group i = 1, . . . , n/(ε log n), compute the sum of all y u .
That is, determinez i = u in group i c u y u ,and output the block vector [z 1 z 2 · · · z n/(ε log n) ] T .
The multiplications take O(n ε ·poly(log n)) time per group, and the sum takes O(n ε ·poly(log n)) time as well.
(Note that no table lookup is required to achieve these runtimes.)
Hence the number of steps taken by stages 2 and 3 is O(n 1+ε · poly(log n)).
The runtime analysis is already embedded in the description of the algorithm.
We now prove that the output vector is indeed Ax.
We need to show that for the ith group of ε log n components of Ax, the corresponding sum z i is equal to it.
Without loss of generality, let us only consider group 1.
Then formally our objective is to show (Ax)[1, . . . , ε log n] = z 1 .
Observe that by construction, the number of times that an increment occurs to some c u in group 1 is n/(ε log n).
Let u 1 , . . . , u n/(ε log n) be the temporal sequence of second layer nodes in group 1 whose c u i 's were incremented, i.e. u 1 is a neighbor of a first layer node in group 1, u 2 is a neighbor of a first layer node in group 2, etc. (Notice that the sequence has repetitions, for n ε < n/(ε log n).)
Then, letting y u i be the vector corresponding to u i , y u i = A i x i , where A i is the matrix A restricted to rows 1, . . . , ε log n and columns (i − 1)ε log n + 1, . . . , iε log n, andx i = x[(i − 1)ε log n + 1, . . . , iε log n].
By distributivity of R, and commutativity of +, We have demonstrated how preprocessing makes it possible to achieve sub-quadratic matrix-vector multiplication for all constant-sized semirings.
Our method implies an on-line combinatorial matrix multiplication algorithm, in the sense that a Θ(log 2 n) speedup is achieved even if the columns of one input matrix are only revealed one at a time.We conclude with three interesting open problems.
First, it might be of more practical importance if one could obtain a speedup of matrix-vector multiplication for sparse matrices.
A time bound of the form O(m/poly(log n) + n) is desirable, but it is not clear how to extend our ideas to this case.
Another interesting question is whether or not the algebraic methods for matrix multiplication (such as Strassen's [S69], Coppersmith-Winograd's [CW90], etc.) can be applied to our problem of matrix-vector multiplication with preprocessing.
This possibility seems unlikely to us, but we have not rigorously ruled it out.
The power of algebraic matrix multiplication algorithms stems from the redundancy in multiplying different pairs of the same collections of vectors.
Such redundancy is not present in matrix-vector multiplication.Finally, is it possible to combine our preprocessing techniques with others to achieve a nonalgebraic o(n 3 / log 2 n) Boolean matrix multiplication algorithm?
Achieving such an algorithm is an important open problem at the intersection of theory and practice that has resisted many efforts.
The answer would be yes if, for example, it were possible to preprocess an n × n matrix so that it can be multiplied with an arbitrary n × log n matrix in o(n 2 / log n) time.
I thank Avrim Blum for introducing me to the problem of preprocessing a graph such that vertex cover queries can be efficiently computed.
This investigation led to the present work.
Thanks to the SODA referees for helpful comments and references to the literature.
