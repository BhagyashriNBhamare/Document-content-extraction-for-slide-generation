SLAOrchestrator is a new system designed to reduce the price increases necessary to support performance SLAs in cloud analytics systems.
SLAOrchestrator is designed for SLAs that guarantee per-query execution times.
Its core architecture consists of a double learning loop that improves both SLAs and resource management over time.
It further utilizes an efficient combination of elastic query scheduling and multi-tenant resource pro-visioning algorithms to reduce the costs of performance guarantees.
A variety of shared-nothing systems for data analytics are available as cloud services today, including Amazon Elastic MapReduce (EMR) [5], Amazon Redshift [4], Azure's HDInsight [8], and Azure Data Lake Analytics [46].
When using those systems, users upload their data to the cloud and issue queries on that data.
Queries can include relational operators and various user-defined computations.
A key challenge with these services, however, is that users must decide on a desired configuration: how many service instances they want to pay for and how powerful these instances should be.The service configuration dramatically impacts price [2] and performance [53], yet it is known to be very difficult for users to select correctly [24].
Since users do not know what configuration to purchase, one approach is to offer performance-based service level agreements (SLAs), where the system promises to meet a given per-query latency or pay a penalty [41,42].
Previous research has addressed the challenge of selecting and enforcing SLAs in various ways.
One line of work assumes each tenant fits on a single server and the challenge is to pack tenants on a restricted set of servers [17,34,47], migrating tenants as needed [16], ordering queries for execution [12,36], controlling admission [56,42], and dispatching queries to servers [11,37].
Other approaches assume the workload is known and re- Figure 1: A time-changing set of tenants executes ad-hoc, analytical queries subject to performance SLAs.
Static resource allocation (EMR+SLAs), even with a buffer (EMR+SLA+Buffer) leads to large cost increases.
Our improving SLAs (EMR+Improving SLAs), especially with multi-tenancy and other optimizations (SLAOrchestrator), bring costs down.quire profile runs of queries, possibly restricted to processing samples [53,25,18,23].
Knowledge of the workload and profile runs are reasonable assumptions in a transaction-processing system with a fixed set of stored procedures or in an analytics system that runs predefined reports, but not for ad-hoc analytical workloads.Another line of work focuses purely on enforcing SLAs, assuming that SLAs are pre-defined [12,11,56].
SLA runtimes are artificially generated by, for example, offering a performance guarantee 10x the true latency [12], or by setting SLAs to be the performance of past executions [29].
Without the right SLAs, the best enforcement does not help: If the cloud provider overprovisions the underlying system, the user has to bear large costs, making the cloud provider less competitive and encouraging the user to take her business elsewhere.
If the cloud provider underprovisions the underlying system, the cloud provider has to pay penalties for missed SLAs and thus loses money in the long term or must raise prices to compensate.In this paper, we address the problem of selecting and enforcing SLAs for ad-hoc analytical queries over systems with multiple nodes.
We develop SLAOrchestrator, a system that enables a cloud provider to offer querylevel, performance SLAs for ad-hoc data analytics.
Instead of relying on outside-generated SLAs [12,11,56], SLAOrchestrator uses our PSLAManager from prior work [41] to show the user what is possible and the price tag associated with various options.
SLAOrchestrator generates, updates over time, and enforces SLAs in a way that successfully brings down the cost, close to that of the original service without SLAs.
Figure 1 shows our system in action given a set of random tenants and EC2 prices.
1 The x-axis shows time and the y-axis shows the ratio of the service cost with SLAs to the service cost without SLAs.
When we add performance SLAs to Amazon EMR and let the cloud provision the number of Virtual Machines (VMs) purchased under the covers, costs grow dramatically either due to SLA violations (EMR+SLAs) or over-provisioning (EMR+SLAs+Buffer).
Since guarantees depend on the quality of the SLAs (measured by how close runtime estimates are to the real runtimes on the purchased resources), a key component of our approach is to improve SLAs over time (EMR+Improving SLAs).
We complement these improving SLAs with novel resource scheduling and provisioning algorithms that minimize costs due to over-or under-provisioning given a per-query SLA (SLAOrchestrator).
SLAOrchestrator achieves its goal through three key techniques that form the core contributions of this work.
First, SLAOrchestrator is designed on the core idea of a double nested, learning loop.
In the outer loop, every time a tenant arrives, the system generates a performance SLA given its current model of query execution times.
That model improves over time as more tenants use the system.
The SLA is in effect for the duration of a query session, which is the time from the moment a user purchases an SLA and issues their first query until the user stops their data analysis and leaves the system.
In the inner loop, SLAOrchestrator continuously learns from user workloads to improve query scheduling and resource provisioning decisions and reduce costs during query sessions.
To drive this inner loop, we introduce a new subsystem, that we call PerfEnforce.
We present the overall system architecture in Section 2.
Second, the PerfEnforce subsystem comprises a new type of query scheduler.
Unlike traditional schedulers, which must arbitrate resource access and manage contention, PerfEnforce's scheduler operates in the context of seemingly unbounded, elastic cloud resources.
Its goal is cost-effectiveness.
It schedules queries in a man- 1 We present the detailed experimental setup in Section 5 and the exact SLA function in Section 3.1.
[50].
The last variant models the problem as an online learning problem.
We present the query scheduler in Section 3.
Third, PerfEnforce also includes a new resource provisioning component.
We evaluate two variants of resource provisioning: The first one strives to maintain a desired resource utilization level.
The other one observes tenant query patterns and adjusts, accordingly, both the size of the overall resource pool and the tuning parameters of the query scheduler above.
We present the resource provisioning algorithms in Section 4.
We evaluate all techniques in Section 5 and discuss related work in Section 6.
As Figure 1 shows, SLAOrchestrator is able to reduce the costs associated with performance guarantees, bringing those costs down close to the basic service costs without guarantees.
Figure 2 shows SLAOrchestrator's system architecture.
In this section, we present the details of that architecture and SLAOrchestrator's double nested learning loop.
SLAOrchestrator runs on top of a distributed, sharednothing, data management and analytics engine (Analytics Service) such as Spark [7] or Hive [26].
We use our own Myria system [54] in the evaluation.
Similar to how tenants use Amazon EMR today, in SLAOrchestrator, tenants upload their data to the service and analyze it by issuing declarative queries.
While modern systems support complex queries, in this paper, we focus on relational select-project-join queries as proof-of-concept.
However, there is nothing in our approach that precludes more complex queries in principal.
On top of the Analytics Service, SLAOrchestrator includes an SLA generator (PSLAManager [41]), which generates performance SLAs for tenants.
It also contains a dynamic scaling engine (PerfEnforce), which drives the scheduling and provisioning decisions for the underlying Analytics Service.Analytics Analytics Service: Storage Once a user purchases an SLA and before they can query their data, PerfEnforce prepares their data by ingesting it into fast networked storage, EBS volumes in our prototype.
Figure 3 motivates our choice.
The figure shows the median query execution times across three runs for a variety of storage options available on Amazon Web Services (AWS).
The y-axis shows the runtime relative to local storage.
Queries on the x-axis are sorted by local storage runtimes in ascending order.
The 70 queries shown are based on a 100SF TPC-H SSB dataset on Myria [54] running on 32 i2.xlarge instances.
As the figure shows, fast networked storage, such as EBS-HighIOPS, provides performance competitive with ephemeral storage, even on a cold cache query, without the need to dynamically migrate (or replicate) data fragments as VMs are added and removed from the shared pool.
This type of storage is also affordable at less than 20% of the cost of a VM.
Because we seek dynamism and must support dataintensive processing, fast networked storage is appealing.During query execution, PerfEnforce attaches EBS volumes to different VMs and detaches them as needed.
Each EBS volume holds a partition of the data, resulting in a standard shared-nothing configuration.
To avoid data shuffling overheads due to scaling, PerfEnforce ingests multiple copies of each table.
Each copy is partitioned across a subset of EBS volumes such that, when a query executes over a set of k containers, it uses the version of its data spread across k EBS volumes.
Due to space constraints, we refer to our technical report for further details on EBS data placement and its negligible impact on performance [43].
SLA Generation To generate SLAs, we use a system from our prior work, the PSLAManager [41], but our system could work with others.
PSLAManager takes as input a database schema and statistics associated with a database instance for a tenant (we use the term user and tenant interchangeably).
It generates a performancebased SLA specific to a database instance as shown in Figure 4 for the TPC-H Star Schema Benchmark [40].
Each tier has a fixed hourly price, which maps to a predefined set of storage and compute resources, along with sets of grouped queries where each group contains a time threshold ("Runtime" in the figure).
The time threshold represents the performance guarantee for its respective group of queries and corresponds to query time estimates made by the SLA generator for the corresponding resource configuration.
For each resource configuration, we only consider varying the number of instances, but consistently use a standard network, and EBS-HighIOPS for storage across all configurations.Each tier represents a performance summary for a specific set of containers the service can use for tenant queries, which we call a configuration.
Tiers can correspond to different types and numbers of containers, but we use a single type in our experiments.
We refer to all possible configurations that the system can use to execute a query as the set conf igs.
For example, conf ig = {2, 4, . . . , 64}, represents all even numbers of containers up to a maximum of 64.
The system shows tiers for a pre-defined subset of these configurations.
Later, it can schedule queries using the full set of configurations.
The price of each tier is at least the sum of the hourly cost of the containers and network storage.When a tenant purchases a performance SLA, she unknowingly purchases a configuration.
The system starts a query session for the tenant and the latter starts paying the corresponding fixed hourly price.
During the session, the tenant issues queries.
The queries get queued up and execute one after the other, each one running in the entire set of containers in the purchased configuration.
As we present in Section 3, PerfEnforce changes these allocations over time based on how fast they execute compared with the initial SLA time.
To drive the SLA generation, SLAOrchestrator maintains a log of all past queries executed in the system.
Initially, it executes queries from a 100GB dataset generated by the Parallel Data Generation Framework(PDGF) [45].
The system runs queries on all configurations that it will sell to populate the query log.
With this information, SLAOrchestrator builds a model of query execution times.
Each query is represented by a feature vector.
Features correspond to query plan properties including the number of tables being joined, their sizes, the query cost estimates from the query optimizer, the number of containers in the configuration, etc.
SLAOrchestrator learns a function from that feature vector to a query execution time.
In our work, we use a simple linear model as in prior work [41,53].
More complex models are possible but we find a simple linear model to yield good results for the select-project-join queries that we focus on in this paper.
With this model, predictions are made by learning the coefficients (a weight vector, w) [9] given the query features,x q : y(x q , w) = D d=1 w d · x q d .
With our previous PSLAManager work [41], we observed that when a new tenant joins the system, estimates for that tenant's queries are likely to be inaccurate because the system has limited information about the tenant data and queries (only statistics on base data).
However, as the tenant starts to execute queries, the system can quickly learn the properties of the data and can specialize its model to that data.
PerfEnforce uses this information to dynamically adjust query scheduling and resource provisioning decisions in the context of an existing SLA.
We call this the Inner Learning Loop.
The effect of this learning is also that the system updates the SLA that it offers after each query session.
This is SLAOrchestrator's Outer Learning Loop.
The benefit of more precise SLAs to tenants is the overall reduction in the service cost.
We use TensorFlow [1] to build this model and train on the PDGF dataset.
We generate 4000 queries (500 per configuration) and record the features as well as runtimes into the System Model.
Figure 2 shows in more detail how SLAOrchestrator components interact with one another.
Steps 1 through 6 denote the Inner Learning Loop: (1) Each tenant query, q is issued through the service front-end.
(2) PSLAManager determines q's promised SLA time based on the service tier that the user previously purchased.
(3) PerfEnforce uses query scheduling algorithms in conjunction with the System Model to determine the number of containers to schedule for q. (4) PerfEnforce schedules q on the Analytics Service.
(5) The Analytics Service sends metadata about the query to the Query Log.
(6) The System Model parses the Query Log metadata and stores features for the learning models.
Once a tenant completes their session, SLAOrchestrator initiates the Outer Learning Loop.
In Step 7, the PSLAManager system takes the information from the System Model and generates an improved SLA.In the next two sections, we focus on the PerfEnforce subsystem and its query scheduling (Section 3) and resource provisioning (Section 4) algorithms, which are part of SLAOrchestrator's inner learning loop.
Every time a new tenant purchases a service tier, PerfEnforce begins a query session for that tenant.
The initial state of the query session indicates the configuration (i.e., number of containers) that corresponds to the purchased service tier.
Many sessions are active at the same time and PerfEnforce receives streams of queries from these active tenants.
Each query is associated with a possibly imperfect SLA.
That is, the query may run significantly faster or slower than the SLA time if scheduled on the purchased set of containers.
PerfEnforce's goal is to determine how many containers to actually use for each query with the goal to minimize operation costs.
In this section, we present PerfEnforce's query scheduling algorithm.
Consider a cloud service operation interval T = [t start , t end ].
The total operating cost to the cloud during that interval is the cost of the resources used for the service and the cost associated with SLA violations for tenants active during that interval.
Thus, PerfEnforce's goal is to minimize the following cost function :cost(T ) = costR(T ) + u∈U (T ) (penalty(u))(1)where U (T ) is the set of all tenants active during time interval, T , and cost R (T ), is given by:costR(T ) = t end −1 t=t start costt(resources)(2)where cost t (resources) represents the cost of resources for time interval [t, t + 1], which depends on the size and the price of individual compute instances.The SLA penalty, penalty(u), is the amount of money to refund to user u in case there are any SLA violations.
In this paper, we use the following formulation:S 1 |Wu| q∈Wu max(0, t real (q) − t sla (q) t sla (q) ) * α * pu (3)where W u is the sequence of queries executed by user u, t real (q) is the real query execution time of query q, For each query q ∈ W u and for each user u, PerfEnforce's query scheduling algorithm must determine the number of containers from the shared pool to allocate to the query.
PerfEnforce begins with using the number of containers that corresponds to the purchased service tier.
It observes the resulting query runtimes and dynamically adjusts the number of containers for subsequent queries by using a scaling algorithm.
It runs a scaling algorithm separately for each tenant.To minimize resource costs, the scaling algorithm should schedule queries on the smallest possible number of containers.
To avoid SLA penalties, however, it must schedule queries on sufficiently large numbers of containers to ensure that the real query execution time, t real (q), is below the SLA time, t sla (q).
We define the query performance ratio as t real (q) t sla (q) and the goal of the query scheduling algorithm is thus to execute each query in the configuration that yields a performance ratio of 1.0.
In practice, if the query scheduling algorithm aims for query performance ratios of X, it will yield a query performance ratio distribution around X as illustrated in Figure 5.
To illustrate our point, we plot synthetic Gaussians.
Real distributions are noisier.
Since we can adjust the mean of the distribution (a.k.a. setpoint), X, the quality of the scheduling algorithm is determined by the tightness of the distribution around X.
In other words, if the distribution is wide (large standard deviation σ), then the system is either wasting resources for many queries (Figure 5a) or causing a large number of SLA violations.
A good query scheduling algorithm should yield a tight distribution as in Figure 5b).
A reactive algorithm observes errors between the real and SLA runtimes and adjusts the number of containers accordingly for each subsequent query.
We implement a Proportional Integral (PI) controller and a Multi-ArmedBandit (MAB) as our reactive methods.
Both of these techniques have successfully been used in other resource allocation contexts [31,33,35,37].
A limitation of the these techniques is that the configuration size chosen for a new query depends only on the rewards or errors of previous queries ignoring the features of the current query.
We use the reactive methods as baseline.Proportional Integral Control (PI).
Feedback control [28] in general, and PI controllers in particular, are commonly used to regulate a system in order to ensure that it operates at a given reference point.
With a PI controller, at each time step, t, the controller produces an actuator value u(t).
In our scenario, this is the number of containers to use for the current query.
The actuator value, causes the system to produce an output y(t + 1) at the next time step.
We compute y(t) as the average query performance ratio over some time window of queries w:y(t) = 1 |w| q∈w t real (qj ) t sla (qj )where |w| is the number of queries in w.
The goal is for the output, y(t), to be equal to some desired reference output r(t), 1.0 in our setting.The error e(t) = y(t) − r(t) captures a percent error between the current and desired average runtime ratios.
Since the number of containers to spin up and remove given such a percent error depends on the configuration size, we add that size to the error computation as follows:e(t) = (y(t) − r(t))u(t).
The PI controller, chooses the next number of containers as a combination of the initial configuration size u(0), the most recently observed error, e(t), and the sum of all accumulated errors t x=0 e(x).
k p and k i are tunable controller parameters, which determine how strongly the controller reacts to recent errors and how much it weighs history: u(t + 1) = u(0) + t x=0 k i e(x) + k p e(t) Multi-Armed Bandits (MAB).
In a MAB problem, the system must repeatedly choose among k different options, or arms.
At each timestep t, the system makes a decision by selecting one of k arms, a t , and receives a reward, r t [50].
In our setting, each arm is a configuration from the set conf igs.
The arm choice is the decision to schedule the next query using a given configuration size.The goal is to maximize the total reward across many timesteps.
In the bandit setting, the algorithm must learn the reward distributions for different arms through a process of trial and error [9].
At each timestep, the system must thus choose to either select the arm with the highest estimated reward (exploitation) or try another arm (exploration) in order to acquire more information and maximizing the reward across all timesteps [50].
To help balance between exploration and exploitation, we use a heuristic known as Thompson Sampling [10].
During initialization, we define priors describing the expected reward of each arm.
In our setting, we do not make assumptions for each configuration.
Instead, we initialize the model for each arm using a uniform distribution, a noninformative prior.
At timestep t, the system constructs a posterior distribution for each arm based on observed rewards, P (θ|a, r 0 , ..., r t−1 ), where θ represents the model parameters.
For each query submitted, the system samples from a candidate posterior distribution, defined asˆθasˆ asˆθ.
Given that our prior is based on a uniform distribution, we use a t-distribution to represent our posterior.
This t-distribution takes the reward mean, variance, and count as input.
As the system samples from this posterior, we select the arm with the highest expected reward, arg max a E[P (r t | ˆ θ α )].
To address the limitations of the reactive techniques, we consider two other scaling algorithms that both include additional context, x q , for each incoming query, where x q is a D−dimensional vector of features describing the query,x q = (x q1 , ..., x q D ) T .
To generate the feature vector, we use the query optimizer of the back-end query execution engine and include information from the query plans (e.g. number of columns, estimated costs, estimated rows, estimated width, and the number of workers scheduled to run the query).
Contextual Multi-Arm Bandit (CMAB).
This approach is a variant of the multi-armed bandit problem that includes contextual information.
In a CMAB problem, at each timestep t, the algorithm receives a feature vector, x q , as input, and uses it to determine the best arm, a t .
CMAB does this by building a model for each configuration that predicts the reward in that configuration given a query feature vector.
The expected value of the reward for each arm and feature vector thus becomes:q (a) = E[r t |a t , x q , θ].
Where θ represents the parameters of the generated model [10].
As with MAB, PerfEnforce uses the Thomson sampling heuristic to balance exploration and exploitation.
At each timestep t, PerfEnforce builds a predictive model for each state by computing a bootstrap sample over all previous observations.
PerfEnforce selects the action that corresponds to the state with the best predicted reward (i.e., reward closest to 1.0).
In our prototype implementation, we use the REPTree model from Weka [22] as used in BanditDB [37].
For the first N queries in a tenant's session, we begin with a "warmup" phase where we execute queries a small number of times in each configuration to initialize the observations for that configuration.
PerfEnforce runs the "warm-up" session at the start of the query session, which could impact performance for some queries.Online Learning The CMAB technique described above presents two practical challenges.
First, it is difficult to determine the number of queries that should be used to initialize the distributions for each state.
At least one query must be executed in each state, which can be either unnecessarily expensive or undesirably slow.
The overhead especially penalizes short query sessions as early queries undergo larger amounts of exploration.
Second, the observations collected are independent for each state.
If one configuration suddenly results in slower or faster runtimes, this knowledge does not propagate to other states.Because of the above limitations, we propose a different algorithm for our setting.
We build a single model of query execution times with the configuration size as a feature.
As a user executes queries, we always schedule those queries in configuration sizes expected to yield the best performance ratio and use the resulting query execution times to update our global model.As described in the previous section, SLAOrchestrator maintains a model of query execution time that it uses for SLA generation.
The idea here is for PerfEnforce to continuously update that model, during a tenant's query session, based on the measured query execution times.
To update the model, PerfEnforce uses stochastic gradient descent.
For each data point, it slowly updates the weight vector based on the gradient of a loss function, E: w (τ +1) = w (τ ) − ηE [9].
Where τ represents the nth data point and η represents the learning rate.
Importantly, PerfEnforce maintains a separate model of query execution time for each dataset so as to specialize its model to the properties of that dataset.
If the underlying data significantly changes, the model could take time to adjust to changes, depending on the learning rate.
Since we primarily focus on analytic sessions, we do not evaluate how this model adapts to updates.
Training this model is relatively cheap, taking approximately 2.38s for a single epoch.
Each prediction takes ∼10ms.
Setpoint Adjustment With all algorithms above, PerfEnforce strives to schedule queries such that their performance ratios form a tight distribution around a desired setpoint.
An important question is how to tune the value of that setpoint.
If the setpoint is 1.0 and the mean of the distribution falls on that setpoint, 50% of all queries will miss their SLA times.
The setpoint can be lowered such that more, perhaps 90% of all queries, meet their SLA time.
Lowering the setpoint, however, will increase the number of containers used for those queries and will thus raise resource costs.
In SLAOrchestrator, we adjust the setpoint dynamically.
We do so at the same time as we make cluster provisioning decisions as described next.
With the above query scheduler, the total number of containers needed to service the active set of tenants varies over time.
To reduce operation costs, PerfEnforce includes a resource provisioning component that determines when to grow and shrink the shared pool of compute resources.
Provisioning is particularly challenging as it can take time to spin up new virtual machines.
We observe that it takes approximately 12 seconds to spin up a virtual machine with a pre-loaded image on Amazon.
We consider this start up time throughout our evaluation.Utilization Provisioning: The most common approach to resource provisioning is to maintain a desired resource utilization level.
Typically, this means adding (or removing) resources when CPU, I/O, network and memory usage move beyond (or below) set thresholds [3,8,21,48].
We posit, however, that measuring resource utilization levels directly is not the right approach for PerfEnforce because tenants are allocated resource containers.
As such, some tenants might execute I/O intensive workloads while others may run CPU intensive workloads, leading to very different resource utilization levels for various containers.
In general, high resource utilization does not imply a higher demand for resources [14].
Instead of aiming for a given CPU or I/O utilization goal, we aim for an average VM utilization target, Z.
The utilization of each machine is measured by the percentage of time it is actively running queries (wall clock time).
For our target Z, we aim for an average utilization across all shared VMs, AvgU tilization.To determine the number of machines the system should provision to meet Z, we implement a PI controller where the set point is Z. Besides wall clock time, we note that other metrics for system state could be used as well.
For example, the system could target a desired percentage of idle machines or a desired tenant query queue length.Simulation-based Provisioning: For a more proactive approach to provisioning, we propose to explicitly consider tenant recent workloads rather than only measure resource utilization.
Specifically, we propose to build models of tenant workloads and estimate the smallest number of shared resources to support these modeled workloads.
This approach should be more effective than simply looking at utilization, since the latter is tightly coupled with the specific set of executed queries and the query scheduler's resource allocation decisions, which are themselves constrained by the amount of shared resources.
To estimate the best number of shared resources to support tenant modeled workloads, we use simulations.
This approach is not new and has been recently used in the "What-If" engine from Tempo [52], where the goal is to simulate the performance of many configurations of the MapReduce Resource Manager.
We aim to understand how such a provisioning algorithm in combination with a learning query scheduler can help make profitable decisions in a multi-tenant service.In this provisioning approach, we model each tenant, u, with a tuple (Q u , λ u ), where Q u is a set of queries that the tenant may issue and λ u is the tenant's average think time between consecutive queries.
PerfEnforce learns Relative RMSE T1 (10GB) T2 (25GB) T3 (50GB) T4 (100GB) Figure 6: SLA improvements across query sessions both values from a recent window of each tenant's query session.
Based on these models, PerfEnforce then generates random sessions for each active tenant.
To generate a random session, PerfEnforce samples queries from the recent query workload and also samples the think time based on a Poisson distribution.
During these simulations, we also evaluate the costs of dynamically shifting the setpoint.
In general, these simulations help PerfEnforce discover whether setpoint adjustments are necessary for active tenants or whether nodes should be added or removed to further save on costs.
We run SLAOrchestrator and execute all queries on Amazon EC2 using i2.xlarge (4 ECU, 30 GB Memory) instance types priced at $0.12/hr.
We consider eight types of query scheduling configurations, each with a different number of compute instances: configs = {4, 8, 12, 16, 20, 24, 28, 32}.
For multi-tenant experiments, we run simulations with up to thousands of servers and use the query times measured on EC2.
For our underlying shared-nothing, database management system, we use Myria [54].
Myria uses PostgreSQL as its storage subsystem.To generate each tenant's query sequence, W u , we alternate between different patterns of queries.
For example, one tenant might run small, lightweight queries for a majority of the session before switching to queries with larger joins and higher latencies.
Thus, for some random number of queries, k, we define the following three discrete distributions: (1) number of joins, (2) number of projected attributes, and (3) selectivity factor.
For the next k queries, we sample from each of these distributions and generate a query that meets all the sampled characteristics.
Once k queries are generated, we define new distributions for the next random interval of queries.
We use both uniform and skewed (zipfian) distributions.
Unless stated otherwise, all the query workloads throughout the evaluation are generated in this fashion.
A key tenet of SLAOrchestrator is the idea that the system should update SLAs because they rapidly improve as a tenant queries a database.
We validate this hypothesis in this section.
Figure 6 shows the error of the SLA predictions for four tenants, each with a different, random star schema [45] and database instance: T1 = 10GB, T2 = 25GB, T3 = 50GB, T4= 100GB.
We generate a set of SPJ queries with random selection predicates for each tenant.
As tenants execute queries, SLAOrchestrator updates the query time model separately for each database.
After each query batch, PSLAManager re-generates an updated SLA.
As the figure shows, in all cases, the RRMSEs (relative root mean squared errors) between the real runtimes and the predicted SLA runtimes decreases rapidly after the first batch and then improves more slowly.
We compute the error on a sample of queries generated by the PSLAManager for the tenant SLA.
We measure the RRMSE as:1 |W| q∈W ( (t real (q)−t sla (q)) t sla (q)) 2 .
The prediction errors observed before running an initial batch of queries (Query Batch 0), are highly dependent on the similarity between the tenant's database and the synthetic database used to train our offline base model.
In our experiments, while databases differ in their schemas and table sizes, we find that table sizes have the greatest impact on prediction errors.
Our offline model is trained on a generated 100GB PDGF dataset.
We observe a higher initial RRMSE error (approx. 2.4-2.5) for tenants T1 and T2 with the smaller databases.
The goal of each query scheduler is to ensure a tight distribution (small σ) of query performance ratios around a µ close to 1.0 (later in Section 5.4, we consider dynamic setpoint tuning).
In this section, we evaluate how the different scheduling algorithms perform in the face of different tenant workloads.
All tenant workloads are based on the 100GB TPC-H SSB benchmark [40].
We evaluate the algorithms using different-quality SLAs as shown in Table 1, which could correspond, for example, to different model qualities as shown in Section 5.1.
We first evaluate the PI-Control scheduling algorithm on four different SLA types and, in each case, on 10 different, randomly generated, tenant query sessions.
We execute the PI controller on each tenant's query session independently and measure the resulting query performance distribution for that tenant.
We then compute the average µ and σ across these 10 distributions and plot them in the first row of Figure 7.
The y-axis represents the distance between µ and 1.0, while the x-axis displays the standard deviation of the query performance distributions.
Because the PI controller has three tunable parameters (k p , k i and w), each point in the figure corresponds to one such parameter combination.
For each graph, we also plot the average distribution of an Oracle, which always selects the best configuration size for each query.
The best parameter combinations are those closest to the Oracle.
If any technique's parameters result in a distribution with a higher σ or a µ farther from 1.0, this error impacts cost, which ultimately depends on the cost function.
As the figure shows, for all SLAs, the PI-Control algorithm results in average distributions that are far from the average distribution of the Oracle.
There are no best set of parameters that work across all workloads.In the second row of Figure 7, we show the average distributions for MAB, CMAB, and online learning across the same set of SLA types and tenant query sessions.
Note, the ranges for the axes are much smaller for these graphs compared to the PI-Controller, which shows that these techniques result in average distributions much closer to the Oracle.
For both bandit techniques, we execute each tenant's query session 20 times due to their variance when sampling.
For online learning, we vary the learning parameter, η.
In the first 4 columns of the figure, we omit the performance ratios for the first 20 queries for all scaling techniques, since the bandits require an initial "warm-up" phase, where they need to try each configuration at least two times.For the SG SLA, the bandit techniques result in average distributions nearly identical to the Oracle.
Since both techniques rely on learning a distribution of query performance ratios per configuration, they quickly find the optimal configuration during the warm-up phase and select this configuration for a majority of the queries.
Since the online learning technique is directly predicting the runtimes for each query, the prediction errors result in average distributions that are slightly farther away from the Oracle.
For the PLJ SLA, all techniques perform similarly as most of the runtimes are meet at configurations that are close to the purchased tier.
In contrast, online learning outperforms both bandit-based methods for the NLJ and Initial SLAs.
The NLJ SLA underestimates runtimes, which requires the schedulers to accurately choose across a wider set of configuration options.
For these more difficult cases, context is critical as the scheduling algorithm must make different decisions for different queries.
There is no single best configuration.
As a result, CMAB and the online learning approach both outperform the simpler MAB scheduler.
Online learning further outperforms CMAB because this technique is able to quickly learn the performance correlations between configurations, which is crucial for the initial SLA as it requires scaling for each query.The final column shows the average performance ratio distributions when using the initial SLA and including the queries in the warm-up period.
As the figure shows, the online learning technique significantly outperform the bandit-based methods because it has the extra benefit of starting to learn from the offline model and learning more quickly because it learns a single model for all configurations.
These results show that the PI controller is ill-suited to our problem and we do not consider (SG SLA) SLA assumes a good prediction model and tests sensitivity to small errors (or variance) in query times.
Generated by taking the real query execution times at the purchased tier and adding a small Gaussian error: N = (σ = 0.1 * t real (q), µ = 0).
We skew SLA runtimes for some query types.
We introduce large positive/negative errors to the real runtimes on queries with a large number of joins (> 3 joins) and with a runtime >100 seconds.
We update the runtime to t real (q) + |e| (or −|e|), where e is sampled from a Gaussian distribution, N = (σ = 0.3 * t real (q), µ = 0).
For other queries, we still inject small errors as in SG SLA.
Initial SLA This is the least accurate SLA, where runtimes are generated by an initial offline-trained model.
Table 1: SLAs used in experiments.
We now evaluate how query scheduling algorithms can adapt to changing conditions.
Recall, the goal of these query schedulers is to ensure a query performance ratio distribution close to 1.0.
We generate a query sequence by selecting one query and running it repeatedly several times.
Each time we run this query, we record the query performance ratio.
Once we reach the 250th iteration, we increase the query's runtime by 25% (essentially slowing down the system) for the rest of the session, running up to 1000 iterations.
Table 2 shows that the online learning technique reacts the fastest to this change in conditions, leading to an overall mean performance ratio closest to 1.0.
We omit additional experiments with different changing workloads due to space constraints.
We first evaluate each provisioning algorithm in combination with the Oracle query scheduler to ensure that query runtime penalties are not a side-effect of the query scheduler's mispredictions.
We launch each multi-tenant tenant session based on session parameters summarized in Table 3.
We introduce up to 100 tenants in a session and Table 3: Parameters of multi-tenant experiments simulate a shared cluster with thousands of containers/VMs.
We sample arrival times, think times, and session durations from Poisson distributions defined by their corresponding parameters λ arrival , λ thinktime and λ terminate .
PerfEnforce always keeps at least a minimum of 4 machines launched at all times, to ensure that there are enough machines available to execute queries.
Each provisioning algorithm monitors the shared resources and tenants for M minutes before adding or removing VMs from the pool.
Our step function S provides no service credit if the system misses the runtime by 10%.
For each additional 20% increment and given a threshold from x% to y%, we increase the credit to y%.
As described in Section 2, each submitted query gets allocated a set of containers.
We schedule one container (running a Myria process) per VM.
For each query, the system assigns the tenant's EBS volumes to a set of VMs in the pool.
After the query completes, the volumes are detached from the VMs, making them available to other tenants.
We find it takes 4 seconds to mount a volume to Utilization and Simulation-based Provisioning We compare the multi-tenant session costs when provisioning VMs using either the utilization-based or simulationbased approaches.
We launch 100 VMs with 10 initial tenants and set the provisioning monitoring time to 20min.
Figure 8 shows the results and the other experimental parameters.
For different average utilizations, Z, we show the results for the best parameter values V kp and V ki .
The y-axis shows the cost per time unit, while the x-axis shows the value of the α parameter.
Recall from Equation 3 that we define α as a tunable parameter that amplifies the weight of the SLA penalty compared to the resource cost.
The hash pattern in each bar represents the proportion of the cost due to Cost R , the cost of resources.
Other costs come from SLA violations.
Error bars show variance across 10 runs.
As expected, the utilization-based method requires tuning depending on the α value.
Simulation-based provisioning has the double-benefit of avoiding any tuning and more cost effectively provisioning shared resources compared to the utilization-based approach.Combining Scheduling and Provisioning We now evaluate the performance of simulation-based provisioning in conjunction with various query scheduling algorithms on the initial SLA.
In Figure 9, we vary alpha (xaxis) and measure the total cost compared with an Oracle query scheduler (y-axis).
As a baseline, we also include a naive query scheduler, static, which simply schedules each query on the configuration initially purchased by the user.
We also include utilization-based provisioning at Z = .25 (using online learning as the query scheduler).
We still initialize the session with 10 tenants, but we start with a larger pool of 320 VMs, allowing enough room to have each initial tenant schedule queries on up to 32 containers.
In this experiment, since we also include CMAB, we extend the session times to 180 to ensure the algorithm has more time to operate in steady state (beyond the warm-up phase).
Overall, simulation-based provisioning continues to outperform the utilization-based approach even with a less perfect scheduler.
Even when penalties are high, simulation-based provisioning reduces costs by 11% and more for lower penalties.
Additionally, the online learning-based scheduler yields similar costs to the Oracle scheduler (a 4% overhead).
As expected, it significantly outperforms the static scheduler and CMAB when SLA penalties are expensive, with 20% cost savings.
CMAB does worse because it causes more SLA violations.
For small α, the CMAB approaches result in costs lower than even the Oracle scheduler.
This is because the CMAB's warm-up phase initially schedules queries on all available configurations (even small configurations), which then causes the simulation approach to provision less resources.
Throughout the session, resources are not added back in due to the low α value.
Finally, we evaluate the benefits of dynamic setpoints together with the relative benefits of the other optimizations.
Figure 10a shows the results.
In the figure, we start with SLAOrchestrator as initially shown in Figure 1.
We then remove optimizations one at a time in order.
First, we remove the ability to use dynamic setpoints, followed by removing SLA improvements, scheduling and provisioning.
We remove these optimizations to run SLAOrchestrator as a simpler multi-tenant system.
To emphasize the differences between optimizations, we use α = 2 and α = 3.
In this experiment, we start the session with 5 tenants and 80 VMs (ensuring 16 nodes per tenant, the amount they have purchased).
New tenants arrive approximately every 5 minutes, and tenants finish their session after 180 minutes.
As seen in the figures, removing each optimization increases the cost.
This is especially apparent for α = 3, where SLAOrchestrator decreases the cost by up to 59% compared to the case with no optimizations.
The SPJ query workloads we use throughout the evaluation allow us to demonstrate a proof of concept for SLAOrchestrator.
Incorporating more complex query workloads (e.g., considering aggregates and subqueries), would impact the online learning and CMAB techniques as they would require more advanced models and more extensive feature engineering than those considered in this work.
Second, for a more thorough provisioning evaluation, running SLAOrchestrator against real tenant traces would help to better understand how the system would behave under more bursty workloads.
Elastic Scaling for Performance Guarantees Performance guarantees have been the focus of real-time database systems [30], where the goal is to schedule queries in a fixed-size cluster and minimize deadlines.
Provisioning and admission control methods have enabled OLAP and OLTP systems to make profitable choices with respect to performance guarantees [12,11,56], possibly postponing or even simply rejecting queries.
Work by Das et.
al [14], uses telemetry to determine whether to scale up containers within a single node, whereas our goal is to scale the number of containers per query.
Ernest [53], CherryPick [2], Morpheus [29] and CloudScale [48] find good configurations for analytical workloads, but require representative workloads or repeated tenant usage patterns.
Several systems have studied performance SLAs through dynamic resource allocation, including feedback control [33], and TIRAMOLA [31] which use reinforcement learning techniques.
Others leverage decisions based on resource utilization goals [13,15,19,39,51,57].
Tempo [52] simulates the performance of many MapReduce Resource Manager configurations to meet a global system objective, but jobs can be preempted to allow high priority tenants to finish first.Multi-Tenant Workload Consolidation Related work addresses bad tenant packings by either finding a good initial placement strategy or dynamically migrating tenants [15,17,32,34,51,55].
Finding a good tenant placement strategy is not the focus of our work.
Instead, we focus on algorithms that help determine when to launch or turn off machines.Query Runtime Prediction Previous work has relied on techniques to find whether a query will miss a deadline [56], build gray-box performance models [20], use historical traces of workloads [18], use benchmarks to profile resources [58], or run smaller samples of the workload [53].
Herodotou et.
al. [24], assumes a previously profiled workload to predict the runtime throughout different sized clusters.
Jalaparti et.
al. [27] generates resource combinations given performance goals.
Instead of building an analytical model, we use a model that does not require an extensive understanding of a single system.
We also focus on ad-hoc queries with no prior profiles.Provisioning In terms of provisioning, some rely on machine learning techniques such as the hill-climbing approach seen in Marcus et.
al. [37], which allows machines to learn an optimal time to wait before they shut down.
Neural networks for dynamic allocation [38] or dynamic provisioning [44] have also been used, but have distinct goals.
One focuses on allocating resources with minimal use of electrical power while the other assumes predictable workloads.
We presented SLAOrchestrator, a new system designed to minimize the price of performance SLAs in cloud analytics systems.
SLAOrchestrator uses a double learning loop that improves SLAs and resource management over time.
To support the latter, the system also includes an efficient combination of elastic query scheduling and multi-tenant resource provisioning algorithms that work toward minimizing service costs.
Experiments demonstrate that SLAOrchestrator dramatically reduces service costs for a common type of per-query latency SLAs.
