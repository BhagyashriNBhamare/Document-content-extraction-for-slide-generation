The important normalized maximum likelihood (NML) distribution is obtained via a normalization over all sequences of given length.
It has two short-comings: the resulting model is usually not a random process, and in many cases, the normalizing integral or sum is hard to compute.
In contrast, the recently proposed sequentially normalized maximum likelihood (SNML) models always comprise a random process and are often much easier to compute.
We present some results on SNML type models in the Markovian and linear-Gaussian model classes.In the linear-Gaussian case, the resulting sequentially normalized least squares (SNLS) model is particularly interesting.
The associated sequentially minimized squared deviations are smaller than both the usual least squares and the squared prediction errors used in the so called predictive least squares (PLS) criterion.
The SNLS model is asymptotically optimal within the given class of distributions by reaching the lower bound on the logarithmic prediction errors, given by the stochastic complexity, up to lower-order terms.
Consider the model class M k = {f (x n ; θ)}, θ = θ 1 , . . . , θ k , and data sequences x n = x 1 , . . . , n, for n = 1, 2, . . . .
Let m be a large enough integer such that the ML estimatê θ t = ˆ θ(x t ) can be computed for t > m.
The number log 1/f (x n ; ˆ θ n )has been considered as the ideal target for the code length obtainable with the model class, [1], which, however, is not attainable, because f (x n ; ˆ θ n ) is not a probability distribution.
This leads to the minmax problem min q max x n log f (x n ; ˆ θ n ) q(x n ) , with the solution due to Shtarkov, known as the normalized maximum likelihood (NML) universal model, [2],ˆ f NML (x n ; M γ ) = f (x n ; ˆ θ(x n )) C n (1) C n = 񮽙 f (y n ; ˆ θ(y n ))dy n .
However, the normalizing coefficient can be evaluated easily only for restricted model classes, and the model does not define a random process.
This means that it cannot be used for prediction and its evaluation for data compression is difficult.
Now, consider for all t > m, the problemmin q(x|x t−1 ) max x log f (x t−1 , x; ˆ θ(x t−1 , x)) q(x | x t−1 ).
The solution is given by the conditional NML modeîmodeî f (x t | x t−1 ) = f (x t ; ˆ θ(x t )) K t (x t−1 )(3)K t (x t−1 ) = 񮽙 f (x t−1 , x; ˆ θ(x t−1 , x))dx.This is proved the same way as the solution to Shtarkov's problem: First, replacing the numerator by the density function (3) does not change the solution, and the maximized ratio of the two density functions (3) and q(x | x t−1 ), which is not smaller than unity, is made unity when the latter is selected equal to the former.
It is clear that the normalizing coefficient K t (x t−1 ), which in general is a function of x t−1 , is easier to calculate, at least numerically, than the normalizing coefficient in the NML universal model.
For another type of normalization, where the numerator f (x t ; ˆ θ(x t )) is replaced by the conditional density f (x t | x t−1 ; ˆ θ(x t )), see [3].
Putting together the conditional NML densities gives the sequentially normalized maximum likelihood (sNML) model:f SNML (x n ) = f m (x m ) n t=m+1ˆf t=m+1ˆ t=m+1ˆf (x t | x t−1 ),(4)where f m (x m ) is a suitably chosen initial distribution.
The result is, by construction, a random process.
We begin with an example involving the Bernoulli class B = {P (x; p)}, where the parameter p = P (1).
The ML estimate is given byˆpbyˆ byˆp(x n ) = n 1 /n, where n 1 = t x t is the number of 1's in x n .
If n 0 = n − n 1 the maximized likelihood isP (x n ; n 1 /n) = n 1 n n1 n 0 n n0 .
The conditional NML predictive probability can be written asˆPasˆ asˆP (1 | x n ) = (n 1 + 1) e(n 1 ) (n 0 + 1) e(n 0 ) + (n 1 + 1) e(n 1 ) ,(5)where e(n 0 ) = (1 + 1/n 0 ) n0 and e(n 1 ) = (1 + 1/n 1 ) n1 ; take e(k) = 1 for k = 0.
For instance, in the problem considered by Laplace, given a sequence of '1's, the successive probabilities of yet another '1' are P Lap (1 | x n ) = n 1 + 1 n + 2 ,which gives the same sequence as 1 2 , 2 3 , 3 4 , 4 5 , . . ., i.e., the certainty of 80 %, which is achieved by the Laplace probability on the fourth step, is achieved by conditional NML already on the second step.The same conditional probability functionˆPfunctionˆ functionˆP (1 | x n ) was found in [2], where it was shown to behave similarly to the Krichevski-Trofimov predictive probabilityP KT (1 | x n ) = n 1 + 1/2 n + 1 .
It was also found later in [4], in effect, as the solution to the following minmax problemmin θ max x log f (x t−1 , x; ˆ θ(x t−1 , x)) f (x | x t−1 ; θ) .
(6)Neither Krichevski-Trofimov predictive probability nor the related Laplace probability has been shown to have any particular optimality property, except asymptotically.
Takimoto and Warmuth [4] showed that for the Bernoulli models, the regret of the sNML model (4) satisfies for all sequences the inequalityR(f SNML , x n ) := ln 1/f SNML (x n ) − ln 1/f (x n ; ˆ θ(x n )) ≤ 1 2 ln(n + 1) + 1 2 .
(7)We conclude this section by noting that in the Bernoulli case, the alternative version of normalization mentioned in Sec. 2, where the numerator of (3) is replaced by f (x t | x t−1 ; ˆ θ(x t )), agrees with the Laplace probability, see [3].
In the rest of the paper, we are concerned with deriving a model selection criterion for a class of normal modelsf (y n | X n ; σ 2 , b) = (2πσ 2 ) −n/2 exp − 1 2σ 2 n 1 (y t − b ′ ¯ x t ) 2 ,induced by the regression equationsy t = b ′ ¯ x t + ǫ t ,(8)where the prime indicates transposition,b ′ = (b(1), . . . , b(k)), with k ∈ N.
The deviations (ǫ t ) n t=1are taken as an i.i.d. sequence generated by a normal distribution of zero-mean and variance σ 2 .
The columns ¯ x t = (x t,1 , . . . , x t,k ) ′ of real valued elements, defining the regressor matrices X t , are either non-random, or ¯ x t = (y t−1 , . . . , y t−k ) ′ as in AR models.For each t = 1, 2, . . . n, let k(t) be the largest integer such that the least squares estimate b t = (b t,1 , . . . , b t,k(t) ) ′ can be uniquely solved.
Hence, typically k(t) = min{t, k} except for AR models, where k(t) = min{t − 1, k}.
We let m be the smallest integer t such that k(t) = k.Central to this work are the following three representations of data for t = 1, 2, . . . n, and k ≥ k(t):y t = b ′ t−1 ¯ x t + e t = k(t) i=1 b t−1,i x t,i + e t ,(9)y t = b ′ n ¯ x t + ˆ ǫ t (n) = k(t) i=1 b n,i x t,i + ˆ ǫ t (n),(10)y t = b ′ t ¯ x t + ˆ e t = k(t) i=1 b t,i x t,i + ˆ e t .
(11)The predictor b ′ t−1 ¯ x t of y t in the first case is called the 'plug-in' predictor, in which the parameters are calculated from the data available up to t − 1.
The plug-in model defines a conditional normal density function for t > m,f (y t | y t−1 , X t ; b t−1 , ˆ σ 2 t−1 ) = 1 񮽙 2πˆσ2πˆσ 2 t−1 exp − e 2 t 2ˆσ2ˆσ 2 t−1 , wherê σ 2 t−1 = 1 t−1 t−1 i=1ˆǫi=1ˆ i=1ˆǫ 2 i (t − 1), and y t−1 = y 1 , . . . , y t−1 .
The resulting joint density function obtained by multiplying the conditional densities of y m+1 , . . . , y n , and ignoring constant terms, defines (by its negative logarithm) the so-called predictive minimum description length (PMDL) criterion, studied in [5], [6], [7], and [8].
Its special case for constant variancêσ 2 t−1 = σ 2 is the predictive least squares (PLS) criterion, PLS(n, k) = n t=m+1 (y t − b ′ t−1 ¯ x t ) 2, studied in [9] and [8].
The second representation (10) is traditional, and it, too, has associated model selection criteria, including AIC [10], and BIC [11],BIC(n, k) = n 2 logˆσlogˆ logˆσ 2 n + k + 1 2 log n,where k + 1 is the number of parameters (including the variance).
The BIC criterion is obtained by an approximation of a joint density function of the data where the negative logarithm of the maximized likelihood f (y n | X n ; b n , ˆ σ 2 n ) determines the first term.
In the AIC criterion the second term is k + 1, the number of parameters.
Both criteria are often multiplied by 2/n, so that the first term is simply the logarithm of the residual sum of squares.Also involving the second representation, the normalized maximum likelihood (NML) criterion is obtained directly as the normalized version of the maximized likelihood, where the normalizing term is given by C n,k = [12].
In order to make the integral finite, the range of integration Y has to be restricted, which requires hyper-parameters.
A solution which eliminates the effect of the hyper-parameters to model selection by a second normalization is presented in [13], see also [14,15].
The corresponding parameterfree criterion isy n ∈Y f (y n | X n ; b n , ˆ σ 2 n ) dy n [1], [2],NML(n, k) = n − k 2 logˆσ logˆ logˆσ 2 n n − k + k 2 logˆR logˆ logˆR k + 1 2 log(k(n − k)), wherê R = b ′ n X n X ′ n b n /n.The third representation, which we are interested in, is new.
The sum of squared deviationsêdeviationsˆdeviationsê 2 t is smaller than either the sum of the traditional least squaresˆǫsquaresˆ squaresˆǫ 2 t (n), or the sum of the squared prediction errors e 2 t .
However, since the parameters of the corresponding conditional density function f (y t | y t−1 , X t ; b t , ˆ σ 2 t ) involve at each step t > m the response variable y t , it too needs to be normalized in order to obtain a proper density function.
We study the asymptotic behavior of the resulting sequentially normalized least squares criterion for both fixed designs and random ones appearing in AR models.
The criterion involves no approximations and is free of any hyperparameters which tend to affect the outcome especially for small samples.
In order to obtain a meaningful model selection criterion with a capability to find a balance between goodness of fit and complexity, we convert the squared deviations into a density model.
Consider first the simple case where the variance σ 2 is fixed.
The non-normalized conditionalsf (y t | y t−1 , X t ; σ 2 , b t ) = 1 √ 2πσ 2 exp − (y t − ˆ y t ) 2 2σ 2 ,(12)are obtained by replacing the parameter vector b in the conditional normal density function f (y t | y t−1 , X t ; σ 2 , b) by the least squares estimate b t .
For each fixed k, for t > m, where m is the smallest value for t for which k(t) = k, the well known recursions exist, see for instance [16],b t = V t t j=1 ¯ x j y j = b t−1 + V t−1 1 + c t ¯ x t (y t − ¯ x ′ t b t−1 ) (13) V t = (X t X ′ t ) −1 = V t−1 − V t−1 ¯ x t ¯ x ′ t V t−1 /(1 + c t ) (14) c t = ¯ x ′ t V t−1 ¯ x t d t = ¯ x ′ t V t ¯ x t 1 − d t = 1/(1 + c t ).
(15)The last equality was shown in [7] and [8] with the interpretation that the quantity 1 − d t is the ratio of the (Fisher) information in the first t−1 observations relative to all the t observations, [8].
This also implies that 0 ≤ d t ≤ 1.
By (13) we obtainˆyobtainˆ obtainˆy t = ¯ x ′ t [V t−1 ¯ x t (y t − ¯ x ′ t b t−1 )/(1 + c t ) + b t−1 ] = c t /(1 + c t )(y t − ¯ x ′ t b t−1 ) + ¯ x ′ t b t−1 = (1 − d t )¯ x ′ t b t−1 + d t y t .
(16)which is a weighted average of the plug-in prediction ¯ x ′ t b t−1 and the true value y t .
This gives the remaining error asêobtainˆ obtainˆy t = ¯ x ′ t [V t−1 ¯ x t (y t − ¯ x ′ t b t−1 )/(1 + c t ) + b t−1 ] = c t /(1 + c t )(y t − ¯ x ′ t b t−1 ) + ¯ x ′ t b t−1 = (1 − d t )¯ x ′ t b t−1 + d t y t .
(16)which is seen to be smaller than the plug-in prediction error by a constant factor.
The normalization of (12) is straightforward, and the result is a normal density function, the mean given by the plug-in predictor and the variance by τ = (1 + c t ) 2 σ 2 .
If we in (12) replace the variance by the minimized variancê s t /t and try to normalize the result the normalizing integral will be infinite.
To make it finite would require hyper-parameters.
Consider instead the maximization problemmax σ 2 n t=m+1 f (y t | y t−1 , X t ; σ 2 , b t ).
(18)The maximizing σ 2 isˆτisˆ isˆτ n = ˆ s n − ˆ s m n − m = 1 n − m n t=m+1ê t=m+1ˆt=m+1ê 2 t ,which gives the maximized product (2πeˆτ2πeˆτ n ) −(n−m)/2 .
By normalizing over y t , we get the normalized conditional density functionˆf functionˆ functionˆf (y t | y t−1 , X t )= K −1 (y t−1 )ˆ τ −1/2 t−1 1 + (y t − ˆ y t ) 2 ˆ τ t−1 −(t−m)/2 .
The normalizing integral is given byK(y t−1 ) = √ π 1 − d t Γ t − m − 1 2 /Γ t − m 2 .
The proof is omitted.
We need t > m + 1 to make the normalizer non-zero.
For t > m + 1, the conditional density function is given byˆf byˆ byˆf (y t | y t−1 , X t ) = K −1t−1 ˆ τ −(t−m)/2 t ˆ τ −(t−m−1)/2 t−1 .
We see that again the predictor that maximizes the conditional density function is the plug-in predictor ¯ x ′ t b t−1 .
By putting the initial density function as some prespecified function q(y m+1 | X m+1 ), which will not play a role in comparison of different models, we get the desired parameter-free density functionˆffunctionˆ functionˆf (y n | X n ) = q(y m+1 | X m+1 ) n t=m+2ˆf t=m+2ˆ t=m+2ˆf (y t | y t−1 , X t ).
The negative logarithm of this gives the sequentially normalized least squares (SNLS) criterion:SNLS(n, k) = n − m 2 ln(2πeˆτ2πeˆτ n ) + n t=m+1 ln(1 + c t ) + 1 2 ln n + O(1),(19)where Stirling's formula has been applied to the Gamma function, and constant terms are implicit in the O(1) term.
The SNLS criterion can be used for subset selection and order estimation for both small and large data sets.
One of its distinguished properties is the fact that unlike the regular NML universal model it has no hyper-parameters.
We conclude this section by a large data set behavior of the SNLS model.
¯ x t satisfy 1 n X n X ′ n = 1 n n i=1 ¯ x i ¯ x ′ i → Σ(20)with Σ non-singular, thenSNLS(n, k) = n − m 2 ln(2πeˆτ2πeˆτ n ) + 2k + 1 2 ln n + o(ln n).
The proof of this and all subsequent theorems are left to the full version.
The first theorem shows the mean square deviations in the three representations of data (9), (10), and (11), which are of some interest, and which we will need later on.
Since we need the recursive formulas (13), (14), (15) we give the results for t > m. 1 n − m n t=m+1 Ee 2 t = σ 2 1 + 1 n − m n t=m+1 c t (21) 1 n − m n t=m+1 Eê 2 t = σ 2 1 − 1 n − m n t=m+1 d t (22) 1 n − m n t=1 EˆǫEˆǫ 2 t (n) − m t=1 EˆǫEˆǫ 2 t (m) = σ 2 , (23)where the expectation is with the parameters b and σ.The next theorem shows the asymptotic optimality of the SNLS model in terms of logarithmic prediction errors, see [9], both in the mean and almost surely, in the case where the regressor matrix is fixed.
(20) hold, and let the data be generated by (8).
Then E SNLS(n, k) = n − m 2 ln(2πeσ 2 )+ k + 1 2 ln n+o(ln n),(24)for almost all parameters b and σ.
Also,SNLS(n, k) = n − m 2 ln(2πeσ 2 ) + k + 1 2 ln n + o(ln n)(25)almost surely.
We then consider the case where the data are generated by an AR model,y t = k i=1 a i y t−i + ǫ t , t ≥ 1,(26)in which the regressor matrix is random, determined by the the data y n , and where we write the coefficients as a i to avoid confusing them with b i , where the subindex refers to time i.
The following theorem shows the almost sure asymptotic optimality of the SNLS model also in this case.
(26), where the roots of the polynomial 1 − k i=1 a i z i are outside the unit circle, and ǫ t is an i.i.d. zero-mean Gaussian process with variance σ 2 .
The process is also assumed to be ergodic and stationary with E¯ x t ¯ x ′ t = Σ nonsingular.
Then forˆσforˆ forˆσ 2 n = (1/n) n i=1ˆǫi=1ˆ i=1ˆǫ 2 i (n), we have lnˆτlnˆ lnˆτ n = lnˆσlnˆ lnˆσ 2 n − k n − m ln n (1 + o(1)),(27)almost surely, andSNLS(k, n) = n − m 2 ln(2πeˆσ2πeˆσ 2 n ) + k + 1 2 ln n + o(ln n)almost surely.
We study the behavior of the proposed SNLS model selection criterion in a simulation study where the AIC, BIC, PLS, and SNLS (Eq.
(19)) methods are used to estimate the order of an AR model.
The scripts, in R language, needed to reproduce the experiments are available for download 1 .
The true order was varied between k * = 1, . . . , 10, and the sample sizes were n = 100, 200, 400, 800, 1600, 3200.
The parameters of the AR models are generated by sampling parameter vectors uniformly at random from the range [−1, 1] k * and rejecting combinations that result in unstable processes, until 3000 accepted (stable) models were produced per each (n, k * ) pair.
The criteria were evaluated for orders up to k = 15, and the order minimizing each criterion was chosen as the estimate.
Tables 1 and 2 report the percentage of correctly estimated orders for each true order k * and sample size n. For the lowest orders, k = 1, 2, the BIC criterion is clearly the most accurate one and wins for almost all sample sizes; this was expected since BIC is known to have a tendency to underestimate rather than overestimate the order.
Likewise, it is not too surprising that AIC, which a priori favors more complex models than the other criteria, wins for the smallest sample size whenever k ≥ 5.
For the orders k = 3, 4, 5, BIC, PLS, NML, and SNLS share the first place, the last one somewhat more often than the others.
For orders k = 5, . . . , 10, Table 2, SNLS is clearly the best method, with the exception of the smallest sample sizes.
This work was supported in part by the Academy of Finland under the project Civi and by the Finnish Funding Agency for Technology and Innovation under the projects Kukot and PMMA.
In addition, this work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence.
