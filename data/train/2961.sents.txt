Storage systems are designed and optimized relying on wisdom derived from analysis studies of file-system and block-level workloads.
However, while SSDs are becoming a dominant building block in many storage systems, their design continues to build on knowledge derived from analysis targeted at hard disk optimization.
Though still valuable, it does not cover important aspects relevant for SSD performance.
In a sense, we are "searching under the streetlight", possibly missing important opportunities for optimizing storage system design.
We present the first I/O workload analysis designed with SSDs in mind.
We characterize traces from four repositories and examine their 'temperature' ranges, sensitivity to page size, and 'logical locality'.
Our initial results reveal nontrivial aspects that can significantly influence the design and performance of SSD-based systems.
Characterizations of file-system and block-level workloads are often used in the design and optimization of various levels of the storage stack.
Aspects such as file and object placement, metadata management, replication, caching, array configuration, and power management are optimized according to our understanding of the target workloads.
The storage community has invested considerable effort in studying storage workloads.
These studies naturally focus on those aspects of the workload relevant to optimization of the underlying storage component.Analyses of file-system workloads examine the distributions of file size, age, and functional lifetime, correlating them with file extension and directory structure [3,12,16].
On the other hand, I/O workload analyses focus on factors crucial for managing low level devices and servers: interreference gaps, working set sizes, and access skew are relevant for cache management, while request sizes, idle and inter-arrival times and read/write ratio are important for hard disk performance [6,7,10,15,19].
Sequentialitythe portion and nature of sequential accesses-is relevant for both: sequential accesses can be an order of magnitude faster than random accesses on hard disks, but they can also pollute the cache and render it utterly useless.
Thus, they often receive special attention in workload analysis [11].
Existing studies have been focusing on the same attributes and characteristics for many years.
The storage landscape is changing, however, as flash-based solid-state drives (SSDs) are gradually replacing hard disks in many data centers.
Although they present a similar block interface, SSDs differ from hard disks in several ways.
The most notable are their fast random access, asymmetric read and write performance, and out-of-place updates, which require dynamic mapping between logical and physical locations as well as garbage collection, both implemented in the flash translation layer (FTL).
Consequently, SSDs have different optimization goals than hard disks.
For example, rather than organizing data to optimize for fast sequential reads, data is organized on SSDs with the objective of maximizing parallelism and minimizing garbage collection overheads.
The need for dynamic mapping leads SSD optimizations to target data movement, rather than data placement [4,20].
At the same time, FTL design allows for complex optimizations and fast adjustment to workload changes.This shift in storage device optimization calls for a new understanding of I/O workloads.
Although existing analyses are still useful, they were designed for optimizing hard disks.
Even analyses and discussions made in the context of SSD optimization [14,17] consider traditional metrics such as working set size, request rate, and interreference gaps.
Relying only on aspects relevant for hard disk optimizations means we might be "searching under the streetlight," missing interesting attributes or phenomena relevant for SSD performance, as well as opportunities for optimizing the entire storage system.
This work is a first attempt to characterize I/O workloads with SSDs in mind.
We analyze over one hundred traces from four public repositories [1,7,9,13], with the goal of identifying characteristics relevant to SSD design and performance.
We consider temperature range (rather than just the amount of "hot" data they contain), logical locality (rather than simple spatial locality or sequentiality), and their sensitivity to increasing SSD page sizes.
Our results expose nontrivial characteristics with substantial implications for SSD design and performance.
We analyze block-level traces of storage workloads from four online repositories: the University of Massachusetts (SPC traces) [1], Microsoft Research at Cambridge [13], Microsoft production servers [7] and Florida International University [9].
We focus on standard attributes available in all these traces: request arrival time (relative to the beginning of the trace), volume, offset (in bytes relative to the beginning of the volume), request size (in bytes), and I/O operation (either read or write).
The focus of our analysis is the block-device level.
Thus, in workloads that access multiple volumes attached to the same server, we analyzed each volume as a separate trace.
Our analysis includes only traces with at least 10,000 requests and at least 5,000 write requests, 122 traces in all.
As part of our analysis, we categorize the workloads according to the server's function.
Table 1 summarizes the characteristics of each repository.
Workload skew is traditionally leveraged to optimize performance by use of caches.
If the cache is large enough to store the application's working set, it can serve most requests without accessing the underlying storage.
In this context, rules of thumb such as the 80-20 rule, which states that 80% of requests access 20% of the data, are often used to estimate the required cache size for a workload.
Thus, traditional analyses characterize the skew of a workload by assessing its working set size or amount of hot (frequently accessed) data.
While those are sufficient for optimizing hard disk and cache performance, SSD optimization can benefit from more detailed analysis.Motivation.
In the context of FTL design, separating hot and cold data into different logical partitions has been shown to minimize write amplification (caused by out-ofplace updates) and, respectively, garbage collection costs and cell wear [5].
Such separation is also useful for other optimizations such as wear leveling and page reuse [21].
Consequently, many FTLs separate data into two partitions.
Stoica and Ailamaki [18] have shown that several temperatures can be grouped into the same partition without increasing write amplification, as long as the access skew within each partition is sufficiently small.
We focus on this analysis and characterize workloads according to the minimum number of partitions they require.
We also aim to quantify the cost of allocating fewer partitions when the optimal number is too high.
We are specifically interested in the common case of two partitions, for hot and cold data.
Definitions.
When a logical page is written to an SSD, the FTL marks its previous physical location as invalid, and chooses a chip and a plane to write the page to, according to its striping the load balancing schemes.
It then writes the page to an active block in the plane-a block that has been erased and was not fully written yet.
Separating data into p logical partitions requires p active blocks in each plane.Following the notation of Stoica and Ailamaki [18], let f i be the update frequency of page i. Ideally, each partition would contain pages with the same update frequency.
This is clearly unrealistic, as modern chips have as few as 512 blocks per plane [2].
Instead, logical pages with several access frequencies are grouped into each partition.
Let f r (p) be the update frequency ratio in partition p, which is the ratio between the maximum and minimum update frequencies of pages stored in p. f r denotes the maximal ratio across all partitions.
Ensuring f r ≤2 is sufficient for minimizing garbage collection overhead [18].
Methodology and results.
To see whether this optimization is realistic given modern chip organization, we calculate the number of partitions required to ensure f r ≤2.
We rank the logical sectors in each workload according to their update frequency, and greedily assign them to partitions: we start with the first partition, assigning to it the page i with maximal f i , as well as all pages { j} with f j ≥ f i /2.
We then assign the next page to the second partition, and so on.Our results show that the minimal number of required partitions is evenly distributed between 2 and 16, and that it varies between categories.
Figure 1(a) shows the 25th, 50th and 75th percentiles for each category, with the vertical lines showing the minimum and maximum required number of partitions for each category.
For example, 9 partitions are sufficient for 75% of DBMS workloads, while 75% of mail server workloads require at least 14 partitions.
We repeated this experiment with varying restrictions on f r .
As expected, more partitions are required to maintain a lower ratio.
The minimum number of required partitions ranges from 3 to 26 for f r ≤1.5, and from 2 to 11 for f r ≤3.
To understand the implications of these findings, consider a state-of-the-art enterprise SSD with 512 blocks per plane and 28% overprovisioning.
Each plane will have 400 available logical blocks.
Thus, to maintain 16 partitions, 4% of the blocks must be allocated as active blocks, and on average 2% of the logical capacity will be unutilized.
In addition to this overhead, one must consider the cost of maintaining and classifying pages into several partitions in the SSD's RAM.SSD designs might limit the number of partitions in order to exploit specialized hardware, or to minimize partitioning overhead.
To estimate the effect of sub-optimal partitioning, we repeated the procedure described above to exhaustively find f min (N)-the lowest ratio for which the greedy partitioning scheme results in N-a predetermined number of partitions.
Figure 1(b) shows the distribution of f min (N) across all traces, for N between 2 and 32.
With a limit of 2 and 4 partitions, the lowest ratio can be as high as 1000 1 and 33.8, respectively, with respective medians 77.7 and 6.6.
However, when the number of partitions is limited to a modest 8, f min (N) does not exceed 5, and is 2 or less for 28% of the traces.Implications.
Previous studies have shown that garbage collection overheads are similar for a ratio of 2 and for uniform accesses, and demonstrated that these overheads increase considerably with higher ratios [18].
Though the exact impact of values as high as 5 or 77 are yet to be determined, it is clear that separating data into only two partitions is far from optimal.
Our results show that workloads and categories differ greatly in the number of partitions they require and in the cost of suboptimal partitioning.
This lays the groundwork for more detailed analysis of workload skew.
I/O workload granularity is determined by the page size of the underlying device and the access granularity of the file system generating the requests.
A sector size of 512B was the default granularity for many years, and is also the access granularity of all the workloads we examined, with one exception: all the requests in workloads from the FIU repository are of 4KB pages, which is the new standard size for sectors and file-system pages alike.Motivation.
While hard disk sector sizes have remained stable for many years, SSD page size has increased rapidly, 1 We did not continue the search beyond f r =1000 because this value was encountered in only a few extreme cases.
from 2KB to 16KB in a few years [2].
This trend is expected to continue as flash feature sizes decrease, to allow more aggressive write optimizations and bit error correction.
Nonetheless, file systems continue to operate at 4KB granularity, to allow fine-grained allocation and access.
Mapping these 4KB logical pages to larger physical pages is the responsibility of the FTL, and although many optimizations have been suggested to alleviate the overhead of this mapping, all FTLs employ some form of logical address alignment.
Alignment causes accesses to two or more logical pages to refer to the same aligned page, with two major implications.
First, page lifetime-the time between consecutive writes to the same page-decreases, as pages are updated more frequently than in the original workload.
In addition, the access distribution of the workload is modified: accesses to different pages, with different original access frequencies, are now considered as accesses to the same larger page.Consider, for example, two workloads that access pages 0-3, as described in Table 2.
Workloads A and B have the same access distribution and CDF, although individual pages are ranked differently in each workload: hot data are clustered in a small logical address space in workload A but not in B.
Now consider workloads A' and B', which result from aligning A and B to a 2-page boundary.
Although A and B both had the same skew, alignment increased the skew of A, yet it decreased that of B.Methodology and results.
We study the effect of increasing device access granularity: for each workload and each physical page size from 4KB to 64KB, we align the requested logical address to page boundaries, and aggregate accesses to the same physical page.
We then repeat the experiments in Section 3 for each page size.
Figure 2 shows the median access frequency ratio when the number of partitions is restricted and the page size varies from 512B to 64KB (the figure of the maximum looks similar).
Interestingly, increasing the page size from 512B to 4KB increases the skew considerably, but increasing it further has only a minor effect.
We also calculated the number of partitions required for f r ≤2 with different page sizes.
Our results (omitted for lack of space) show that for individual workloads, increasing the page sizes beyond 4KB resulted in a difference of only 1 or 2 par- titions.
Some workloads behave like example workload A for some page sizes and like B for others.
Implications.
The implications of changes in workload skew are well understood when it comes to caching, tiering, partitioning, and other optimizations.
Our initial results indicate that increasing the page size beyond the standard operating system size of 4KB has little effect on workload skew, and thus on most optimizations.
Analyzing the effect of increased page size on lifetime and on read access patterns is part of our future work.
Locality has always been important in workload analysis.
Temporal locality-repeated accesses to the same logical address-has been studied in the form of interreference gaps, working set size, and functional lifetime [7,10,15,16].
Spatial locality-accesses to nearby logical addresses-is usually studied in the form of sequential access: the portion of sequential accesses and their "run length."
Spatial locality in random accesses is harder to characterize, and is usually analyzed by studying the differences between offsets of consecutive requests [6,7,10,15].
This is often referred to as "seek distance," as the motivation for its study is to estimate disk arm movements when serving a given workload.Motivation.
Although sequential accesses are known to improve SSD throughput, we focus instead on novel characterization of random accesses in SSDs, where spatial locality is critical despite the lack of moving parts.
Consider, for example, an SSD that stripes pages across chips in a RAID organization, and must update a parity page whenever a data page is written.
A possible optimization is to delay parity updates until more pages in the same stripe are updated, to minimize write overhead [8].
To evaluate the efficiency of such optimizations, one might ask, "What is the probability that another page in the same stripe will be written in the near future?
"As another example, consider a block-mapping FTL, which groups together pages with consecutive logical addresses.
Pages are first written to a log, and are periodically garbage collected and grouped according to their logical address.
In this context, the expected probability of writing to the same logical block can help optimize garbage collection.Definitions.
We characterize the logical spatial locality (logical locality, for short) of a workload as follows.
For a given write to logical page i at time T , we calculate the probability P D,T that a logical page j, |i − j|≤D, will be written at time t ′ ≤t+T .
The logical locality is given as a For any predetermined D and T we say that a write request hits a previous write request if the time between them is less than T , and we can find a pair of pages, one in each request, whose logical addresses are within D pages from one another (see example in Figure 3).
Thus, while traditional measures consider only the last and first logical pages in consecutive requests, logical locality takes into account the entire address range of requests within a larger time frame.Methodology and results.
We calculate logical locality by defining a 'window size' of D max and T max (1024 and 5000 in our experiments) and storing a sliding window of the last T max requests in memory while traversing each workload.
When a new request hits one of the recorded requests, we update the CDF accordingly.
We present the logical locality in a three dimensional graph, where the axes are distance (D) and time (T ).
Figure 4, discussed below, shows an example.
The graph shows how P D,T increases as we increase D and T : a new request hits older requests as we move left on the T axis, or requests at a larger offset as we move right on the D axis.Note that in the calculation described above, two or more pages written in the same request will be treated as two requests at the same time, which consequently hit one another: for every one of these pages, i, there is another page (i-1, i+1, or both) written at time t+0.
This often causes P D,T to be very close to 1 for all D and T .
This result does answer the question of future writes in the vicinity of the current page, but it does not provide meaningful To prevent large requests from 'masking' other forms of locality, we do not consider pages in the same request as hitting one another-we count only hits in previous requests.
Although this approach does not capture locality within the same request, it provides a better answer to the question above: we already know the size of the current request, and we can use P D,T to make an educated guess about the future.
The cumulative probabilities P D,T can be complemented with the distribution of request sizes for a more complete picture of the locality in the trace.
Figure 4 shows logical locality of the workload of the 'casa' server from the FIU repository, which stores home directories.
The full results (a) show that when the window is large enough, P D,T approaches 1.
A close-up view (b) shows how P D,T increases in each axis, with several noticeable "steps."
Comparing the cross-sections in each axis (c) shows that P D,T converges much faster in time (right).
This is common in other traces, suggesting that locality does not increase much after a short time, implying that optimizations such as delayed garbage collection or parity updates are feasible even with only a short wait.To confirm our hypothesis, we calculate δ P D -the maximal increase in P D,T in a workload for a given distance D.
In other words, we measure the potential improvement in P D,T when waiting for the maximal time T .
Figure 5 summarizes our results for all workloads.
It shows that as we increase the distance from the adjacent page (D = 1) to 16 and 64 pages, P D,T can grow more.
Conversely, the potential improvement in P D,T remains similar as we increase T (figure omitted for lack of space).
In summary, to capture more hits, it is better to consider a larger distance, than to wait for a longer time.Implications.
Our initial findings expose a variety of behaviors in the logical locality of the workloads.
We are currently investigating possible aggregate metrics for representing logical locality, as well as correlations between logical locality and other workload characteristics.
We believe that device-specific analysis is inevitablewhat makes an analysis interesting are the implications of its findings for system design.
Thus, workload analyses must evolve with the systems they target, and continuously consider additional trace attributes, emerging technologies and hybrid designs.We present a first attempt at characterizing I/O workloads with SSDs in mind.
Our initial results expose nontrivial insights when examining new characteristics.
Some implications of our findings for SSD design and optimization are straightforward, while others are yet to be determined.
Additional analysis is clearly due, and we are continuing to refine our metrics and search for additional correlations and insights.
We thank Abdalla Amaria, Khaled Fahom, Baraa Hleihil, Kujan Lauz, Muhammad Musa, Roni Refael, and Ali Tabaja for their help with the traces.
We thank Assaf Schuster, Eitan Yaakobi and the anonymous reviewers for their insightful suggestions.
This work is partially funded by the Microsoft Economic Center at the Technion.
