Information retrieval is, in general, an iterative search process, in which the user often has several interactions with a retrieval system for an information need.
The retrieval system can actively probe a user with questions to clarify the information need instead of just passively responding to user queries.
A basic question is thus how a retrieval system should propose questions to the user so that it can obtain maximum benefits from the feedback on these questions.
In this paper, we study how a retrieval system can perform active feedback, i.e., how to choose documents for relevance feedback so that the system can learn most from the feedback information.
We present a general framework for such an active feedback problem, and derive several practical algorithms as special cases.
Empirical evaluation of these algorithms shows that the performance of traditional relevance feedback (presenting the top K documents) is consistently worse than that of presenting documents with more diversity.
With a diversity-based selection algorithm, we obtain fewer relevant documents, however, these fewer documents have more learning benefits.
In ad hoc information retrieval, a user often needs to interact with the retrieval system several times to obtain satisfactory results for one information need, which provides opportunities for the retrieval system to actively participate in this iterative retrieval process.
Most traditional retrieval systems just passively respond Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
SIGIR'05, August 15-19, 2005, Salvador,Brazil.
Copyright 2005 ACM 1-59593-034-5/05/0008 ...$5.00.
to user queries and put the responsibility of refining/improving the search solely on the user.
But there has been evidence showing that a retrieval system can play an active role in this process, e.g., obtaining user feedback explicitly or implicitly when the user browses these documents, and exploiting such information to improve the performance in the next round of search [6,8].
Ideally, a retrieval system should collaborate with the user in the whole interactive search period to improve the accuracy and reduce the number of interactions.When explicit feedback is possible, a natural way for the retrieval system to actively participate in the retrieval process is to clarify the user's information need by probing the user with well-designed questions.
A question could be whether a document or passage is relevant, or whether a term describes the user's information need.In this scenario, a basic question is how the retrieval system should intelligently propose the questions so that it can learn most from the user's answers to these questions.
In this paper, we study how a retrieval system can perform active feedback, i.e., how to choose documents for relevance feedback so that the system can learn most from the feedback information.Relevance feedback is known to be effective for improving retrieval performance [16,18,4].
Previous work on relevance feedback focuses on query updating techniques such as query term reweighting and query expansion.
The issue of choosing documents for relevance feedback has not been well addressed.
Traditionally, relevance feedback methods just choose the top ranked documents for feedback, which is not necessarily the best strategy from the learning perspective.
For example, if the top two documents have identical contents, the learning benefits of these two documents will be nearly equal to that of any one of them.
Thus a very interesting research question is how to select appropriate documents for user judgment to maximize the learning benefits, which is the focus of the study in this paper.Active feedback is essentially an application of active learning in ad hoc information retrieval.
Active learning has been extensively studied in machine learning [17,22,3].
It has been applied to text categorization in several previous studies [12,14,23], and recently to adaptive information filtering [29].
But there has been little work on applying it to ad hoc retrieval, partly because there are two special challenges in applying active learning to ad hoc retrieval.
First, in ad hoc retrieval, we do not have any training examples available to guide the retrieval system for actively selecting the documents for feedback; the query is the only information that can be exploited.
Second, it is unclear how we can define an objective function that optimizes ranking performance rather than classification accuracy.
An interesting recent work on applying active learning to ad hoc retrieval is [5], where a user is assumed to iteratively choose clusters, and the active learning task for the system is to design good clusters, a different task from active feedback.
The TREC HARD Track [1] has stimulated some recent work along the line of active feedback including [15,20].
In this paper, we frame the problem of active relevance feedback as a statistical decision problem, and examine several special cases in refining the framework.
We derive several practical algorithms for active feedback, including the Top K, Gapped Top K and K Cluster Centroid algorithm.
We empirically evaluate these three algorithms using the TREC2003 HARD data , AP88-89 and AP90.
The results show that the performance of the Top K algorithm (i.e., the traditional way of relevance feedback) is consistently worse than that of Gapped Top K algorithm and K Cluster Centroid algorithm which present documents with more diversity.
In general, with a diversity-based selection algorithm, we obtain fewer relevant documents, but these fewer documents have more learning benefits.The remaining sections are organized as follows.
In Section 2, we present the active feedback framework and derive several practical algorithms.
In Section 3, we describe our evaluation methods and three algorithms we tested.
We discuss the experiment results in Section 4 and conclude our work in Section 5.
The problem of active feedback is essentially a decision problem in which we choose the best subset of documents for relevance judgment by the user.
To formalize this problem, we follow the risk minimization framework for retrieval [9] and treat it as the following optimization problem:D * = arg min D Θ L(D, U, θ)p(θ|U , q, C) dθwhere D = {d1, ..., d k } is a subset of the document collection C, q is a query, U is a user variable, θ is the set of parameters of the query language model and document language models.
p(θ|U , q, C) is the posterior probability distribution of all the parameters, and L(D, U, θ) is a loss function reflecting how much we can expect to learn by requesting relevance judgments on D from user U.
In general, the loss function may also depend on other factors such as any relevance judgments available from previous iterations of retrieval, but here we ignore those factors for the convenience of presentation.Without refining the language models p(θ|U , q, C), which is not the focus of this paper, we study how to define the loss function for active feedback.
Clearly, the actual value of a set of documents D for learning depends on not only D but also the judgments the user would make.
Let J = {0, ..., m} be the set of all possible relevance levels that a user may assign to each presented document (0 for "completely non-relevant").
For example, for binary judgments, J = {0, 1}.
The loss function can now be written asL(D, U, θ) = j∈J k l(D, j, θ)p( j|D, θ, U )where j = (j1, ..., j k ) and ji is a possible judgment for document d i in D; p( j|D, θ, U) is the probability that the user U would assign judgments j to all the documents in D; and l(D, j, θ) is a loss function that indicates how much we can learn from the judgments j on D.
In other words, l(.)
tells us how good (D, j) is as a set of labeled examples for learning.Now assuming that the user would judge each document independently, we haveL(D, U, θ) = j∈J k l(D, j, θ) k i=1 p(j i |d i , θ, U)Note that this assumption is reasonable if a user explicitly judges a document, but it is unlikely to hold when we infer a user's judgments based on, say, clickthrough data [6], as obviously a user would not open a redundant (but relevant) document.Thus our general framework for active feedback is the following decision rule:D * = arg min D Θ [ j∈J k l(D, j, θ) k i=1 p(j i |d i , θ, U )]p(θ|U, q, C) dθIn the remaining part of the section, we discuss some interesting special cases.
We will assume that the relevance judgments are all binary, though most derivations can be easily generalized to multilevel judgments.
Let us first simplify the loss function by assuming that the value of each judged example for learning is independent of each other.
The total value of a set of examples (D, j) can thus be written as the sum of the value of each individual example, i.e.,l(D, j, θ) = k i=1 l(di, ji, θ)where l(di, ji, θ) is the loss for a single judged document (di, ji).
After some algebraic manipulation, we haveL(D, U, θ) = k i=1 j i l(d i , j i , θ)p(j i |d i , θ, U)And the active feedback decision rule isD * = arg min D Θ k i=1 j i l(di, ji, θ)p(ji|di, θ, U)p(θ|U , q, C) dθ = arg min D k i=1 j i Θ l(di, ji, θ)p(ji|di, θ, U)p(θ|U , q, C) dθThe optimal set D can thus be obtained by ranking all the documents according to the following risk function and taking the k documents with the least risk:r(d i ) = j i Θ l(d i , j i , θ)p(j i |d i , θ, U)p(θ|U , q, C) dθwhich can be interpreted as the expected value of di for learning over all possible judgments.We now examine the assumptions underlying two simple methods for defining r(d i ) -"Top K" and "Uncertainty Sampling".
Let us assume that the loss of any relevant example (document) and that of any non-relevant example (document) are both constants.
We further assume that the former is smaller than the latter, which is to say that a relevant example is more useful for learning than a non-relevant one.
Formally,∀d i ∈ C, we have l(d i , 1, θ) = C1, l(di, 0, θ) = C0, and C1 < C0.The risk function now becomesr(d i ) = C 0 + (C 1 − C 0 ) Θ p(j i = 1|d i , θ, U )p(θ|U, q, C) dθSince C1 − C0 < 0, clearly the optimal set D * is precisely the k documents with the highest probabilities of being judged as relevant (i.e., with the highest expected values of p(j i = 1|d i , θ, U)).
That is, we should simply rank all the documents in C according to the estimated relevance status of each document and select the top k documents that are most likely relevant for feedback.
We have thus obtained the "Top K" method as a special case under three assumptions 1 : (1) independent loss function; (2) constant loss for any relevant (non-relevant) document; and (3) a relevant document has a smaller loss than a non-relevant one.
The results are not really surprising because assumption 2 basically says that all relevant (non-relevant) examples are equally good for learning.
However this analysis suggests that we may expect other methods to perform better than Top K if the underlying feedback algorithm does not satisfy all these three assumptions, e.g., independent loss function.
In [12,11], a similar document selection problem is studied, though a set of documents are selected for labeling to train a text classifier instead of a ranking function.
Authors propose to select the most uncertain documents for labeling.
In [28], a similar idea, i.e., selecting most uncertain objects, is used to guide the hidden annotation for content-based image information retrieval.
Using our general active feedback framework, we can derive the uncertainty sampling method by assuming the following loss function:l(d i , 1, θ) = log p(R = 1|d i , θ) ∀d i ∈ C l(di, 0, θ) = log p(R = 0|di, θ) ∀di ∈ Cwhere R ∈ {0, 1} is a binary relevance variable with 1 indicating "relevant".
This loss function essentially says that a relevant example is more useful if the predicted probability of relevance is smaller according to our current model, and similarly, a non-relevant example is more useful if the predicted probability of relevance is larger.
In other words, an example is more useful if our prediction has less confidence.With such a loss function, and assuming p(R|d i , θ) is an approx- imation of p(j i |d i , θ, U), the risk function becomes r(d i ) = − Θ H(R|d i , θ)p(θ|U , q, C)dθwhere H is the entropy function.
This means that, in order to obtain D, we should rank documents in the descending order of the expected entropy of the corresponding relevance variable R.
That is, we would pick documents with the highest uncertainty.We have thus obtained the "Uncertainty Sampling" method as a special case under two assumptions: (1) independent loss function; (2) an example is more useful for learning if our prediction of relevance is more uncertain.
This method relies on explicitly predicting the probability of relevance, which is often not feasible in ad hoc retrieval.
Our assumption about an independent loss on each example is not realistic.
For instance, if two examples are completely identical, their total value is clearly less than the sum of their individual values, and is probably close to the value of one of the examples.
Thus we need to model the interactions between documents with a dependent loss function.
Unfortunately, the exact form of such a loss function highly depends on the specific feedback algorithms.
Nevertheless, intuitively, given a fixed size of D, increasing the representativeness of documents in D appears to be always desirable.
At the same time, we can also reasonably assume that relevant examples are more useful than non-relevant examples.
Thus one possible way to refine a dependent loss function is to associate 1 Top K as an active feedback method was first discussed in [10].
the value of D for learning with the relevance status and diversity of D.
That is, we write our loss function asL(D, U , θ) ≈ − k i=1 p(ji = 1|di, θ, U) − λ∆(D, θ)where ∆(D, θ) is a function that measures the diversity of documents in D and λ is a parameter indicating the tradeoff between the relevance and diversity.According to this loss function, the active feedback decision rule isD * = arg min D − Θ k i=1 p(ji = 1|di, θ, U)p(θ|U , q, C) dθ −λ Θ ∆(D, θ)p(θ|U , q, C) dθ (1)That is, we need to select D to simultaneously optimize both relevance (the first term) and diversity (the second term).
A possible greedy algorithm is to first optimize the relevance term by selecting top N (N > K) documents according to relevance-based ranking, and then to further select K most diverse documents from the N documents.
We now discuss several simple methods along this line.
Suppose we let N = (G+1)K, where G is a small positive integer.
To capture the diversity, we partition the N documents into K clusters based solely on the relevance scores so that our first cluster would have the first G + 1 documents and the second one have the next G + 1 documents, and so on so forth.
From each cluster, we then select a document with the highest relevance score to form our feedback document set D.
We refer to this method as "Gapped Top K" since it corresponds to selecting the top K documents with a gap of G documents in between any two documents.
An interesting property of this method is that when G = 0, it is essentially the regular Top K method.
Maximal Marginal Relevance (MMR) ranking is a greedy algorithm for ranking documents based on relevance and at the same time avoiding redundancy [2,26].
Specifically, we iteratively select a document which optimizes the following MMR function:r(d|D) = αs(d) + (1 − α) max d ∈D sim(d, d )where s(d) is a relevance scoring function, sim (d, d ) is a similarity function, and α is a parameter for trading off between relevance and redundancy.This method can also be regarded as performing an implicit clustering and then selecting a document from a cluster with the highest relevance value.
The first document selected will be the top ranked one based on relevance.
Since the next document to be selected must be far from this selected first document, we can interpret the first document as implicitly defining a cluster with the first document being the centroid, and none of the other documents in the cluster will be selected since they are all too close to the selected first document.
As we select the second document, we again have another cluster which further excludes some documents from being selected.
However, it is unclear what the clustering boundary is exactly as it is affected by not just the similarity function, but also the relevance scores of documents and the parameter α.
The MMR method can also cover the Top K method as a special case when α = 1.
A more direct method to maximize diversity is to perform explicit clustering.
Specifically, we can first select the top N documents according to the relevance scores.
Then we partition these N documents into K clusters and construct D by selecting one representative document from each cluster.To optimize the relevance term in equation 1, we restrict N to a relatively small number.
In this way, we ensure that each of the N documents has a reasonably high probability of relevance.
The diversity is ensured through clustering and choosing only one document from each cluster.
There are several different ways to choose a representative document from each cluster.
One is to choose the centroid document, which maximizes the average similarity between the chosen document and other documents in the cluster.
Another choice is to choose the document with the highest relevance score.
We use two data sets for experiments.
One is the Associated Press (AP) news data on TREC disks 1, 2, and 3.
The other is the TREC2003 HARD (High Accuracy Retrieval from Documents) track data set [1].
TREC2003 HARD track puts search into context, which allows a retrieval system to actively infer a user's information need and improve retrieval performance [20].
Our experiment process simulates two runs of the HARD track experiment setup [1].
For the HARD track data set, we use 48 topics that have relevance judgments.
For the AP data set, we use 92 topics from topics 1-50 and 101-150, which have relevance judgments on both the AP88-89 and AP90 data set.
We use the title of each topic as the query. 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙
We use the Lemur toolkit as our retrieval system [24] and the KL-Divergence language retrieval model as our retrieval model [9,25].
K is fixed to 6 in most experiments, and all parameters are set to default values [24] unless otherwise stated.
Our baseline run is regular retrieval without any feedback.
It allows us to test whether we can improve performance by performing feedback.
From the baseline retrieval results, we use different active relevance feedback algorithms to select a set of documents for relevance feedback.
Using the known relevance judgments available from these TREC data to simulate a user's judgments, we obtain relevance judgments on the selected documents.
These judgments are then used to perform feedback using the mixture model approach implemented in Lemur [27].
This method only uses relevant documents for query model updating, which can be a limitation of our study.
The retrieval results in the second run using different active feedback algorithms are compared for evaluation.
This experiment procedure is illustrated in Figure 1.
As a first step of studying active feedback, we evaluate three representative active feedback algorithms discussed in Section 2.
The first one is Top K, which chooses top K documents from the baseline run retrieval, and is also what existing retrieval systems would normally do.
The second one is Gapped Top K , which is to choose gapped top K documents from the baseline run results.
For example, if we set the gap to 3 and K to 6, we will end up choosing the 1st, 5th, 9th, ..., 21st documents from the retrieval results.
The third one is K cluster centroid, which represents the most direct way of modeling diversity.
We use the K-Medoid clustering algorithm [7] to cluster the top N documents.
And we use J-Divergence [13] of two documents as the distance function.
J-Divergence is a divergence metric similar to KL-Divergence.
But unlike the nonsymmetry of KL-Divergence, J-Divergence is symmetric.
The formula of J-Divergence is as follows.J(d i ||d j ) = w p(w|θ i ) log p(w|θ i ) p(w|θ j ) + w p(w|θ j ) log p(w|θ j ) p(w|θ i )Evaluation of these methods allows us to examine whether presenting a diverse set of documents for feedback leads to more effective feedback than presenting the top k documents with the highest relevance values.
To measure the performance of a ranking method, we use two standard ad hoc retrieval measures: (1) Mean Average Precision (MAP): This is the commonly used non-interpolated average precision and serves as a good measure of the overall ranking accuracy since it is sensitive to the rank of every relevant document.
(2) Precision at 10 documents (pr@10): This measure does not average well and only gives us the precision at one single cutoff point.
But it reflects the utility perceived by a user who may only read up to top 10 documents.
In all cases, the reported figure is the average over all the topics.Since the task of active feedback involves identifying a certain number of relevant documents by the user, an interesting question is whether we should include such relevant documents when computing the retrieval precision of an active feedback algorithm.
While this is also a problem for relevance feedback evaluation, it is especially a challenge for evaluating active feedback algorithms because the set of relevant documents used for feedback can usually be controlled in regular relevance feedback evaluation, but must vary in evaluating active feedback algorithms.In our evaluation, we decided to include all the judged documents, including both relevant and non-relevant documents, because if we exclude them, we would have a potentially different test set for each method.
In particular, it would be unfair for a method that tends to present more "easy" relevant documents for feedback; indeed, the retrieval task would become artificially harder for such a method due to the fact that more "easy to retrieve" relevant documents would be excluded.However, including such judged documents also has a problem -it does not accurately reflect the actual utility of a method as perceived by a user.
Indeed, a user would presumably not really care about where the judged feedback documents are ranked because the user has already seen them.
Thus any improvement in the ranking of a seen relevant document does not really bring any real benefit to the user.
In order to see more clearly how much a method can improve the ranking of unseen documents, we can run the active feedback algorithms on one document database (i.e., the training database) to obtain relevance judgments and then use another similar document database (i.e., the testing database) to test the retrieval performance [21].
Thus, in addition to the regular evaluation on the HARD track data set and AP88-89 with all the judged documents included, we also use AP88-89 for training and AP90 for testing to compare different methods, assuming that the contents in these two databases are sufficiently similar.
As we mentioned in Section 2.2.1, Top K can be considered as a special case of Gapped Top K (i.e. when the gap equals to 0).
We do experiments varying the gap to test whether a non-zero gap can perform better than Top K.
The results on the HARD data set and AP88-89 data set are shown in Table 1, where we show the MAP, the precision at 10 documents, and the number of judged relevant documents per query.From the results, we can see Top K (gap = 0) is clearly not the best strategy.
Actually, when we choose small gaps (gap ≤ 6), the performance is consistently better than Top K, which strongly suggests that top K is really a poor choice for active relevance feedback.
We may also note that, as we increase the gap, we obtain fewer relevant documents than we could obtain with Top K.
But using these fewer relevant documents for feedback can achieve better retrieval performance, which means these fewer relevant documents have more learning benefits.
The same phenomenon is also observed when active learning is applied in the classification problem [19].
One explanation of this phenomenon is that when we increase the gap, we obtain more diverse documents, thus the judgments become more informative.
Here we use the clustering algorithm to select more diverse documents for active relevance feedback.
We cluster the top N documents into K clusters and choose the K cluster centroid for relevance feedback.
When N = K, we again have Top K as a special case.
We vary N for fixed K (= 6) to test if presenting documents with higher diversity is beneficial.
The results are shown in Table 2.
The variation of N causes a different tradeoff point for relevance and diversity.
If we choose a bigger N , we pay more attention to diversity, while if we choose a smaller N , we pay more attention to relevance.
We see that the optimal values are different for the two databases.
Comparing Top K (N = K) with other results in the Table again shows that Top K is mostly the worst among all the results, suggesting that the relevance judgments obtained with clustering are more effective for feedback than those obtained using Top K. Moreover, with a large N , we actually obtain fewer judged relevant documents, but these fewer relevant documents are better examples for learning.
Since the effectiveness of the underlying feedback mechanism ( the mixture model method in our case) is an important factor that may affect our evaluation, we compare several different feedback algorithms with the non-feedback baseline in Table 3.
The performance for the Gapped Top K and the K Cluster Centroid is the best performance from Table 1 and Table 2, respectively.From these results, we can see that the performance of both active feedback and pseudo feedback are better than that of baseline retrieval.
We also see that the Top K relevance feedback performs better than using the top K documents for pseudo feedback.
All these results show that the underlying feedback mechanism is effective.Among active feedback algorithms, K cluster centroid outperforms Gapped Top K algorithm, which in turn outperforms Top K algorithm, although the improvement appears to be quite small.
A very interesting observation is that the K cluster centroid algorithm obtains the fewest number of relevant documents from user feedback, yet its performance is the best.
This suggests that selecting diverse documents leads to more effective learning.As mentioned in Section 3.4, when comparing different active feedback algorithms, it is more reasonable to use one document database for active feedback (training), and the other document database for measuring retrieval performance (testing).
Thus we further compare these methods using AP88-89 as the training set and AP90 as the testing set.
Specifically, we perform baseline retrieval on AP88-89 database, select a document subset for relevance Table 4: Average performance of different retrieval algorithms on AP90 data set.
The best performance is shown in bold.
A double star (**) indicates that the performance is significantly better than that of Top K according to Wilcoxin signed rank test at the level of 0.05.
feedback using different active relevance feedback algorithms, update the query model, all on AP88-89, and then retrieve documents from AP90.
The experiment results are shown in Table 4.
The results again show that the results of the Top K algorithm is the worst among three active relevance feedback algorithms.
Although the performance difference is mostly insignificant according to the Wilcoxin signed rank test except in the case of pr@10 for Gapped Top K, there are more topics for which Gapped Top K and K Cluster Centroid are better than Top K than the other way in all cases.
In the case of MAP, it is 42 topics vs. 31 topics (with 19 cases tied) for both Gapped Top K and K Cluster Centroid.
In the case of pr@10, it is 12 topics vs. 3 topics (with 77 cases tied) and 9 topics vs. 5 topics (with 78 cases tied) for Gapped Top K and K Cluster Centroid, respectively.
The large number of tied cases indicates that our query expansion feedback mechanism is conservative.
Indeed, as we show later in Table 6, when we change the query expansion parameter to perform more aggressive query expansion, the performance improvement is generally amplified.
The performance of all active feedback algorithms is also better than that of pseudo feedback and baseline retrieval.
The results shown so far are all obtained by fixing K = 6.
We now examine how choosing a different K may affect our conclusions.
We compare Top K, Gapped Top K (gap=3), and K cluster centroid (N = 100) for several different values of K in Table 5.
The results show that our conclusion, i.e., the performances of Gapped Top K and K Cluster Centroid are better than that of Top K, is relatively insensitive to the choice of K. Indeed, the Top K results are almost always the worst among the three methods.
Also, on the HARD data, the K cluster centroid method consistently outperforms the other two methods with fewer judged relevant documents.
In the results shown so far, the improvement of Gapped Top K and K Cluster Centroid over Top K is not so significant.
We find that the feedback algorithm parameter is an important factor.
In [27], the new query modeî θ Q isˆθisˆ isˆθ Q = (1 − α) ˆ θQ + α × ˆ θFHere, α controls how much weight we give to feedback documents.
In all the previous results, we set α to 0.5.
But since the feedback documents are judged to be relevant by users, we can give more weight to these feedback documents.
So we did another set of experiments by varying α and keeping all other parameters fixed.
The results are shown in Table 6.
From these results, we can see clearly that the α can amplify the effect of feedback.
And when α is increased, the improvement of Gapped Top K and K Cluster Centroid over Top K is also amplified.
This paper presents the first serious study of the problem of active relevance feedback, in which a retrieval system actively chooses the best documents for relevance feedback.
Ad hoc information retrieval is largely an interactive process.
Active relevance feedback allows a retrieval system to actively probe a user and clarify the user's information need, thus can improve retrieval performance.We formulate the problem of active feedback as a statistical decision problem and study several special cases.
We analyze the assumptions made in each case.
We derive three specific algorithms for active relevance feedback, i.e., Top K, Gapped Top K, and K Cluster Centroid algorithm.
We evaluate these algorithms using the TREC2003 HARD data set, AP88-89 and AP90 data set.
Experiment results show that the Top K algorithm, which is what an existing retrieval system normally uses for relevance feedback, is not optimal for active relevance feedback, and is actually often worse than both the Gapped Top K algorithm and the K Cluster Centroid algorithm.
Compared with the Top K algorithm, Gapped Top K algorithm and K Cluster Centroid algorithm emphasize returning more diversified documents.
The results show that with fewer judged relevant documents, both Gapped Top K and K Cluster Centroid outperform the Top K algorithm, suggesting that the diversity in the presented documents is a desirable property.
Although the difference is generally small, the overall consistency strongly supports our conclusions.
Our work represents only a very preliminary exploration of this important topic.
There are several interesting directions to explore.
(1) It would be interesting to study how to learn from non-relevant documents judged by the user so as to make full use of user efforts and feedback.
(2) Another interesting question is how to optimize performance over the entire search session, rather than just one iteration.
(3) We may explore other approaches for selecting documents.
For example, MMR strategy is also a promising strategy.
(4) We can try to combine pseudo feedback and active feedback.
Since those highly ranked documents are very likely relevant, we do not really need to present them for judgments; instead, we can propose documents ranked below a few top documents for feedback (essentially the uncertainty sampling strategy).
In this way, feedback can be based on those top-ranked documents, which we assume to be relevant, and the obtained relevance judgments through active feedback.
We thank Karen Spärck Jones, Stephen Robertson and the anonymous reviewers for their useful comments.
This material is based in part upon work supported by the National Science Foundation under award numbers CAREER-IIS-0347933 and ITR-IIS-0428472.
