Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field.
However, traditional erasure codes employ device-level redundancy to protect against sector failures, and hence incur significant space overhead.
Recent sector-disk (SD) codes are available only for limited configurations due to the relatively strict assumption on the coverage of sector failures.
By making a relaxed but practical assumption, we construct a general family of erasure codes called STAIR codes, which efficiently and provably tolerate both device and sector failures without any restriction on the size of a storage array and the numbers of tolerable device failures and sector failures.
We propose the upstairs encoding and downstairs encoding methods, which provide complementary performance advantages for different configurations.
We conduct extensive experiments to justify the practical-ity of STAIR codes in terms of space saving, encod-ing/decoding speed, and update cost.
We demonstrate that STAIR codes not only improve space efficiency over traditional erasure codes, but also provide better computational efficiency than SD codes based on our special code construction.
Mainstream disk drives are known to be susceptible to both device failures [25,37] and sector failures [1,36]: a device failure implies the loss of all data in the failed device, while a sector failure implies the data loss in a particular disk sector.
In particular, sector failures are of practical concern not only in disk drives, but also in emerging solid-state drives as they often appear as worn-out blocks after frequent program/erase cycles [8,14,15,43].
In the face of device and sector failures, practical storage systems often adopt erasure codes to provide data redundancy [32].
However, existing erasure codes often build on tolerating device failures and provide device-level redundancy only.
To tolerate additional sector failures, an erasure code must be constructed with extra parity disks.
A representative example is RAID-6, which uses two parity disks to tolerate one device failure together with one sector failure in another non-failed device [21,39].
If the sector failures can span a number of devices, the same number of parity disks must be provisioned.
Clearly, dedicating an entire parity disk for tolerating a sector failure is too extravagant.To tolerate both device and sector failures in a spaceefficient manner, sector-disk (SD) codes [27,28] and the earlier PMDS codes [5] (which are a subset of SD codes) have recently been proposed.
Their idea is to introduce parity sectors, instead of entire parity disks, to tolerate a given number of sector failures.
However, the constructions of SD codes are known only for limited configurations (e.g., the number of tolerable sector failures is no more than three), and some of the known constructions rely on exhaustive searches [6,27,28].
An open issue is to provide a general construction of erasure codes that can efficiently tolerate both device and sector failures without any restriction on the size of a storage array, the number of tolerable device failures, or the number of tolerable sector failures.In this paper, we make the first attempt to develop such a generalization, which we believe is of great theoretical and practical interest to provide space-efficient fault tolerance for today's storage systems.
After carefully examining the assumption of SD codes on failure coverage, we find that although SD codes have relaxed the assumption of the earlier PMDS codes to comply with how most storage systems really fail, the assumption remains too strict.
By reasonably relaxing the assumption of SD codes on sector failure coverage, we construct a general family of erasure codes called STAIR codes, which efficiently tolerate both device and sector failures.Specifically, SD codes devote s sectors per stripe to coding, and tolerate the failure of any s sectors per stripe.
We relax this assumption in STAIR codes by limiting the number of devices that may simultaneously contain sector failures, and by limiting the number of simultaneous sector failures per device.
The new assumption of STAIR codes is based on the strong locality of sector failures found in practice: sector failures tend to come in short bursts, and are concentrated in small address space [1,36].
Consequently, as shown in §2, STAIR codes are constructed to protect the sector failure coverage defined by a vector e, rather than all combinations of s sector failures.With the relaxed assumption, the construction of STAIR codes can be based on existing erasure codes.
For example, STAIR codes can build on Reed-Solomon codes (including standard Reed-Solomon codes [26,30,34] and Cauchy Reed-Solomon codes [7,33]), which have no restriction on code length and fault tolerance.We first define the notation and elaborate how the sector failure coverage is formulated for STAIR codes in §2.
Then the paper makes the following contributions:• We present a baseline construction of STAIR codes.Its idea is to run two orthogonal encoding phases based on Reed-Solomon codes.
See §3.
• We propose an upstairs decoding method, which systematically reconstructs the lost data due to both device and sector failures.
The proof of fault tolerance of STAIR codes follows immediately from the decoding method.
See §4.
• Inspired by upstairs decoding, we extend the construction of STAIR codes to regularize the code structure.
We propose two encoding methods: upstairs encoding and downstairs encoding, both of which reuse computed parity results in subsequent encoding.
The two encoding methods provide complementary performance advantages for different configuration parameters.
See §5.
• We extensively evaluate STAIR codes in terms of space saving, encoding/decoding speed, and update cost.
We show that STAIR codes achieve significantly higher encoding/decoding speed than SD codes through parity reuse.
Most importantly, we show the versatility of STAIR codes in supporting any size of a storage array, any number of tolerable device failures, and any number of tolerable sector failures.
See §6.
We review related work in §7, and conclude in §8.
We consider a storage system with n devices, each of which has its storage space logically segmented into a sequence of continuous chunks (also called strips) of the same size.
We group each of the n chunks at the same position of each device into a stripe, as depicted in Fig- ure 1.
Each chunk is composed of r sectors (or blocks).
Thus, we can view the stripe as a r × n array of sectors.
Using coding theory terminology, we refer to each sector as a symbol.
Each stripe is independently protected by an erasure code for fault tolerance, so our discussion focuses on a single stripe.
Storage systems are subject to both device and sector failures.
A device failure can be mapped to the failure of an entire chunk of a stripe.
We assume that the stripe can tolerate at most m (< n) chunk failures, in which all symbols are lost.
In addition to device failures, we assume that sector failures can occur in the remaining n − m devices.
Each sector failure is mapped to a lost symbol in the stripe.
Suppose that besides the m failed chunks, the stripe can tolerate sector failures in at most m 񮽙 (≤ n − m) remaining chunks, each of which has a maximum number of sector failures defined by a vector e = (e 0 , e 1 , · · · , e m 񮽙 −1 ).
Without loss of generality, we arrange the elements of e in monotonically increasing order (i.e., e 0 ≤ e 1 ≤ · · · ≤ e m 񮽙 −1 ).
For example, suppose that sector failures can only simultaneously appear in at most three chunks (i.e., m 񮽙 = 3), among which at most one chunk has two sector failures and the remaining have one sector failure each.
Then, we can express e = (1, 1, 2).
Also, let s = 񮽙 m 񮽙 −1 i=0 e i be the total number of sector failures defined by e.
Our study assumes that the configuration parameters n, r, m, and e (which then determines m 񮽙 and s) are the inputs selected by system practitioners for the erasure code construction.Erasure codes have been used by practical storage systems to protect against data loss [32].
We focus on a class of erasure codes with optimal storage efficiency called maximum distance separable (MDS) codes, which are defined by two parameters η and κ (< η).
We define an (η, κ)-code as an MDS code that transforms κ symbols into η symbols collectively called a codeword (this operation is called encoding), such that any κ of the η symbols can be used to recover the original κ uncoded symbols (this operation is called decoding).
Each codeword is encoded from κ uncoded symbols by multiplying a row vector of the κ uncoded symbols with a κ × η generator matrix of coefficients based on Galois Field arithmetic.
We assume that the (η, κ)-code is systematic, meaning that the κ uncoded symbols are kept in the codeword.
We refer to the κ uncoded symbols as data symbols, and the η − κ coded symbols as parity symbols.
We use systematic MDS codes as the building blocks of STAIR codes.
Examples of such codes are standard Reed-Solomon codes [26,30,34] and Cauchy Reed-Solomon codes [7,33].
Given parameters n, r, m, and e (and hence m 񮽙 and s), our goal is to construct a STAIR code that tolerates both m failed chunks and s sector failures in the remaining n − m chunks defined by e. Note that some special cases of e have the following physical meanings:• If e = (1), the corresponding STAIR code is equivalent to a PMDS/SD code with s = 1 [5,27,28].
In fact, the STAIR code is a new construction of such a PMDS/SD code.
• If e = (r), the corresponding STAIR code has the same function as a systematic (n, n − m − 1)-code.
• If e = (, , · · · , ,) with m 񮽙 = n − m and some constant 񮽙 < r, the corresponding STAIR code has the same function as an intra-device redundancy (IDR) scheme [10,11,36] that adopts a systematic (r, r − 񮽙)-code.We argue that STAIR codes can be configured to provide more general protection than SD codes [6,27,28].
One major use case of STAIR codes is to protect against bursts of contiguous sector failures [1,36].
Let β be the maximum length of a sector failure burst found in a chunk.
Then we should set e with its largest element e m 񮽙 −1 = β.
For example, when β = 2, we may set e as our previous example e = (1, 1, 2), or a weaker and lower-cost e = (1, 2).
In some extreme cases, some disk models may have longer sector failure bursts (e.g., with β > 3) [36].
Take β = 4 for example.
Then we can define e = (1, 4), so that the corresponding STAIR code can tolerate a burst of four sector failures in one chunk together with an additional sector failure in another chunk.
In contrast, such an extreme case cannot be handled by SD codes, whose current construction can only tolerate at most three sector failures in a stripe [6,27,28].
Thus, although the numbers of device and sector failures (i.e., m and s, respectively) are often small in practice, STAIR codes support a more general coverage of device and sector failures, especially for extreme cases.STAIR codes also provide more space-efficient protection than the IDR scheme [10,11,36].
To protect against a burst of β sector failures in any data chunk of a stripe, the IDR scheme requires β additional redundant sectors in each of the n − m data chunks.
This is equivalent to setting e = (β, β, · · · , β) with m 񮽙 = n − m in STAIR codes.
In contrast, the general construction of STAIR codes allows a more flexible definition of e, where m 񮽙 can be less than n − m, and all elements of e except the largest element e m 񮽙 −1 can be less than β.
For example, to protect against a burst of β = 4 sector failures for n = 8 and m = 2 (i.e., a RAID-6 system with eight devices), the IDR scheme introduces a total of 4 × 6 = 24 redundant sectors per stripe; if we define e = (1, 4) in STAIR codes as above, then we only introduce five redundant sectors per stripe.
For general configuration parameters n, r, m, and e, the main idea of STAIR encoding is to run two orthogonal encoding phases using two systematic MDS codes.
First, we encode the data symbols using one code and obtain two types of parity symbols: row parity symbols, which protect against device failures, and intermediate parity symbols, which will then be encoded using another code to obtain global parity symbols, which protect against sector failures.
In the following, we elaborate the encoding of STAIR codes and justify our naming convention.We label different types of symbols for STAIR codes as follows.
Figure 2 shows the layout of an exemplary stripe of a STAIR code for n = 8, r = 4, m = 2, and e = (1, 1, 2) (i.e., m 񮽙 = 3 and s = 4).
A stripe is composed of n − m data chunks and m row parity chunks.
We also assume that there are m 񮽙 intermediate parity chunks and s global parity symbols outside the stripe.
Let d i,j , p i,k , p 񮽙 i,l , and g h,l denote a data symbol, a row parity symbol, an intermediate parity symbol, and a global parity symbol, respectively, where Figure 2 depicts the steps of the two orthogonal encoding phases of STAIR codes.
In the first encoding phase, we use an (n + m 񮽙 , n− m)-code denoted by C row (which is an (11,6)-code in Figure 2).
We encode via C row each row of n − m data symbols to obtain m row parity symbols and m 񮽙 intermediate parity symbols in the same row: Phase 1: For i = 0, 1, · · · , r − 1,0 ≤ i ≤ r − 1, 0 ≤ j ≤ n − m − 1, 0 ≤ k ≤ m − 1, 0 ≤ l ≤ m 񮽙 − 1, and 0 ≤ h ≤ e l − 1.
d i,0 , d i,1 , · · · , d i,n−m−1 Crow =⇒p i,0 , p i,1 , · · · , p i,m−1 , p 񮽙 i,0 , p 񮽙 i,1 , · · · , p 񮽙 i,m 񮽙 −1 ,where C =⇒ describes that the input symbols on the left are used to generate the output symbols on the right using some code C.
We call each p i,k a "row" parity symbol since it is only encoded from the same row of data symbols in the stripe, and we call each p 񮽙 i,l an "intermediate" parity symbol since it is not actually stored but is used in the second encoding phase only.In the second encoding phase, we use a (r+e m 񮽙 −1 , r)-code denoted by C col (which is a (6,4)-code in Figure 2).
We encode via C col each chunk of r intermediate parity symbols to obtain at most e m 񮽙 −1 global parity symbols:Phase 2: For l = 0, 1, · · · , m 񮽙 − 1, p 񮽙 0,l , p 񮽙 1,l , · · · , p 񮽙 r−1,l C col =⇒ e m 񮽙 −1 񮽙 񮽙񮽙 񮽙 g 0,l , g 1,l , · · · , g e l −1,l , * , · · · , * ,where " * " represents a "dummy" global parity symbol that will not be generated when e l < e m 񮽙 −1 , and we only need to compute the "real" global parity symbols g 0,l , g 1,l , · · · , g e l −1,l .
e m′-1 d 0,0 d 1,0 d 3,0 d 2,0 d 0,1 d 1,1 d 3,1 d 2,1 d 0,2 d 1,2 d 3,2 d 2,2 d 0,3 d 1,3 d 3,3 d 2,3 d 0,4 d 1,4 d 3,4 d 2,4 d 0,5 d 1,5 d 3,5 d 2,5 p 0,1 p 1,1 p 3,1 p 2,1 p 0,0 p 1,0 p 3,0 p 2,0 p′ 0,2 p′ 1,2 p′ 3,2 p′ 2,2 p′ 0,0 p′ 1,0 p′ 3,0 p′ 2,0 p′ 0,1 p′ 1,1 p′ 3,1 p′ 2,1 r n n g 0,2 g 1,2n mg 0,1 g 0,0Encode with row each g h,l is in essence encoded from all the data symbols in the stripe, and thus we call it a "global" parity symbol.We point out that C row and C col can be any systematic MDS codes.
In this work, we implement both C row and C col using Cauchy Reed-Solomon codes [7,33], which have no restriction on code length and fault tolerance.From Figure 2, we see that the logical layout of global parity symbols looks like a stair.
This is why we name this family of erasure codes STAIR codes.In the following discussion, we use the exemplary configuration in Figure 2 to explain the detailed operations of STAIR codes.
To simplify our discussion, we first assume that the global parity symbols are kept outside a stripe and are always available for ensuring fault tolerance.
In §5, we will extend the encoding of STAIR codes when the global parity symbols are kept inside the stripe and are subject to both device and sector failures.
In this section, we justify the fault tolerance of STAIR codes defined by m and e.
We introduce an upstairs decoding method that systematically recovers the lost symbols when both device and sector failures occur.
The proof of fault tolerance of STAIR codes builds on the concept of a canonical stripe, which is constructed by augmenting the existing stripe with additional virtual parity symbols.
To illustrate, Figure 3 depicts how we augment the stripe of Figure 2 into a canonical stripe.
Let d * h,j and p * h,k denote the virtual parity symbols encoded with C col from a data chunk and a row parity chunk, respectively, where 0 ≤ j ≤ n − m − 1, 0 ≤ k ≤ m − 1, and 0 ≤ h ≤ e m 񮽙 −1 −1.
Specifically, we use C col to generate virtual parity symbols from the data and row parity chunks as follows:For j = 0, 1, · · · , n − m − 1, d 0,j , d 1,j , · · · , d r−1,j C col =⇒ d * 0,j , d * 1,j , · · · , d * e m 񮽙 −1 −1,j ; and for k = 0, 1, · · · , m − 1, p 0,k , p 1,k , · · · , p r−1,k C col =⇒ p * 0,k , p * 1,k , · · · , p * e m 񮽙 −1 −1,k .
The virtual parity symbols d * h,j 's and p * h,k 's, along with the real and dummy global parity symbols, form e m 񮽙 −1 augmented rows of n + m 񮽙 symbols.
To make our discussion simpler, we number the rows and columns of the canonical stripe from 0 to r + e m 񮽙 −1 − 1 and from 0 to n + m 񮽙 − 1, respectively, as shown in Figure 3.
Referring to Figure 3, we know that the upper r rows of n + m 񮽙 symbols are codewords of C row .
We argue that each of the lower e m 񮽙 −1 augmented rows is in fact also a codeword of C row .
We call this the homomorphic property, since the encoding of each chunk in the column direction preserves the coding structure in the row direction.
We formally prove the homomorphic property in Appendix.
We use this property to prove the fault tolerance of STAIR codes.
We prove that for a STAIR code with configuration parameters n, r, m, and e, as long as the failure pattern is within the failure coverage defined by m and e, the corresponding lost symbols can always be recovered (or decoded).
In addition, we present an upstairs decoding method, which systematically recovers the lost symbols for STAIR codes.For a stripe of the STAIR code, we consider the worstcase recoverable failure scenario where there are m failed chunks (due to device failures) and m 񮽙 additional chunks that have e 0 , e 1 , · · · , e m 񮽙 −1 lost symbols (due to sector failures), where 0 < e 0 ≤ e 1 ≤ · · · ≤ e m 񮽙 −1 .
We prove that all the m 񮽙 chunks with sector failures can be recovered with global parity symbols.
In particular, we show that these m 񮽙 chunks can be recovered in the order of e 0 , e 1 , · · · , e m 񮽙 −1 .
Finally, the m failed chunks due to device failures can be recovered with row parity chunks.
Virtual parity symbols Step 5d 0,0 d 1,0 d 3,0 d 2,0 d 0,1 d 1,1 d 3,1 d 2,1 d 0,2 d 1,2 d 3,2 d 2,2 d 0,3 d 1,3 d 3,3 d 2,3 d 0,4 d 1,4 d 3,4 d 2,4 d 0,5 d 1,5 d 3,5 d 2,5 p 0,1 p 1,1 p 3,1 p 2,1 p 0,0 p 1,0 p 3,0 p 2,0 p * 0,1 p * 1,1 p * 0,0 p * 1,0 d * 0,5 d * 0,4 d * 0,1 d * 0,2 d * 0,0 d * 0,3 d * 1,5 d * 1,4 d * 1,1 d * 1,2 d * 1,0 d * 1,3 r p′ 0,2 p′ 1,2 p′ 3,2 p′ 2,2 p′ 0,0 p′ 1,0 p′ 3,0 p′ 2,0 p′ 0,1 p′ 1,1 p′ 3,1 p′ 2,1 g 0,2 g 1,2 g 0,1 g 0,Step 6Step 8Step 8Step 9Step 10Step 12Step 11Step 9Step 10Step 12Step 11Step 4 Step 4Step 2Step 3 Step 1Step 4Step 7 Step 6Step 2Step 3 Step 1Step 5 d 0,0 d 1,0 d 3,0 d 2,0 d 0,1 d 1,1 d 3,1 d 2,1 d 0,2 d 1,2 d 3,2 d 2,2 d 0,3 d 1,3 d 2,3 d 0,4 d 1,4 d 2,4 d 0,5 d 1,5 g 0,2 g 1,2 g 0,1 g 0,0 We demonstrate via our exemplary configuration how we recover the lost data due to both device and sector failures.
Figure 4 shows the sequence of our decoding steps.
Without loss of generality, we logically assign the column identities such that the m 񮽙 chunks with sector failures are in Columns n − m − m 񮽙 to n − m − 1, with e 0 , e 1 , · · · , e m 񮽙 −1 lost symbols, respectively, and the m failed chunks are in Columns n − m to n − 1.
Also, the sector failures all occur in the bottom of the data chunks.
Thus, the lost symbols form a stair, as shown in Figure 4.
The main idea of upstairs decoding is to recover the lost symbols from left to right and bottom to top.
First, we see that there are n − m − m 񮽙 = 3 good chunks (i.e., Columns 0-2) without any sector failure.
We encode via C col (which is a (6,4)-code) each such good chunk to obtain e m 񮽙 −1 = 2 virtual parity symbols (Steps 1-3).
In Row 4, there are now six available symbols.
Thus, all the unavailable symbols in this row can be recovered using C row (which is a (11,6)-code) due to the homomorphic property (Step 4).
Note that we only need to recover the m 񮽙 = 3 symbols that will later be used to recover sector failures.
Column 3 (with e 0 = 1 sector failure) now has four available symbols.
Thus, we can recover one lost symbol and one virtual parity symbol using C col (Step 5).
Similarly, we repeat the decoding for Column 4 (with e 1 = 1 sector failure) (Step 6).
We see that Row 5 now contains six available symbols, so we can recover one unavailable virtual parity symbol (Step 7).
Then Column 5 (with e 2 = 2 sector failures) now has four available sym- Detailed Descriptions1 d 0,0 , d 1,0 , d 2,0 , d 3,0 ⇒ d * 0,0 , d * 1,0 2 d 0,1 , d 1,1 , d 2,1 , d 3,1 ⇒ d * 0,1 , d * 1,1 3 d 0,2 , d 1,2 , d 2,2 , d 3,2 ⇒ d * 0,2 , d * 1,2 4 d * 0,0 , d * 0,1 , d * 0,2 , g 0,0 , g 0,1 , g 0,2 ⇒ d * 0,3 , d * 0,4 , d * 0,5 5 d 0,3 , d 1,3 , d 2,3 , d * 0,3 ⇒ d 3,3 , d * 1,3 6 d 0,4 , d 1,4 , d 2,4 , d * 0,4 ⇒ d 3,4 , d * 1,4 7 d * 1,0 , d * 1,1 , d * 1,2 , d * 1,3 , d * 1,4 , g 1,2 ⇒ d * 1,5 8 d 0,5 , d 1,5 , d * 0,5 , d * 1,5 ⇒ d 2,5 , d 3,5 9 d 0,0 , d 0,1 , d 0,2 , d 0,3 , d 0,4 , d 0,5 ⇒ p 0,1 , p 0,2 10 d 1,0 , d 1,1 , d 1,2 , d 1,3 , d 1,4 , d 1,5 ⇒ p 1,1 , p 1,2 11 d 2,0 , d 2,1 , d 2,2 , d 2,3 , d 2,4 , d 2,5 ⇒ p 2,1 , p 2,2 12 d 3,0 , d 3,1 , d 3,2 , d 3,3 , d 3,4 , d 3,5 ⇒ p 3,1 , p 3,2Table 1: Upstairs decoding: detailed steps for the example in Figure 4.
Steps 4, 7, and 9-12 use C row , while Steps 1-3, 5-6, and 8 use C col .
bols, so we can recover two lost symbols (Step 8).
Now all chunks with sector failures are recovered.
Finally, we recover the m = 2 lost chunks row by row using C row (Steps 9-12).
Table 1 lists the detailed decoding steps of our example in Figure 4.
We now generalize the steps of upstairs decoding.
(1) Decoding of the chunk with e 0 sector failures: It is clear that there are n − (m + m 񮽙 ) good chunks without any sector failure in the stripe.
We use C col to encode each such good chunk to obtain e m 񮽙 −1 virtual parity symbols.
Then each of the first e 0 augmented rows must now have n − m available symbols: n − (m + m 񮽙 ) virtual parity symbols that have just been encoded and m 񮽙 global parity symbols.
Since an augmented row is a codeword of C row due to the homomorphic property, all the unavailable symbols in this row can be recovered using C row .
Then, for the column with e 0 sector failures, it now has r available symbols: r − e 0 good symbols and e 0 virtual parity symbols that have just been recovered.
Thus, we can recover the e 0 sector failures as well as the e m 񮽙 −1 − e 0 unavailable virtual parity symbols using C col .
(2) Decoding of the chunk with e i sector failures (1 ≤ i ≤ m 񮽙 − 1): If e i = e i−1 , we repeat the decoding for the chunk with e i−1 sector failures.
Otherwise, if e i > e i−1 , each of the next e i − e i−1 augmented rows now has n − m available symbols: n − (m + m 񮽙 ) virtual parity symbols that are first recovered from the good chunks, i virtual parity symbols that are recovered while the sector failures are recovered, and m 񮽙 − i global parity symbols.
Thus, all the unavailable virtual parity symbols in these e i −e i−1 augmented rows can be recovered.
Then the column with e i sector failures now has r available symbols: r − e i good symbols and e i virtual parity symbols that have been recovered.
This column can then be recovered using C col .
We repeat this process until all the m 񮽙 chunks with sector failures are recovered.
(3) Decoding of the m failed chunks: After all the m 񮽙 chunks with sector failures are recovered, the m failed chunks can be recovered row by row using C row .
In §4.2, we describe an upstairs decoding method for the worst case.
In practice, we often have fewer lost symbols than the worst case defined by m and e. To achieve efficient decoding, our idea is to recover as many lost symbols as possible via row parity symbols.
The reason is that such decoding is local and involves only the symbols of the same row, while decoding via global parity symbols involves almost all data symbols within the stripe.
In our implementation, we first locally recover any lost symbols using row parity symbols whenever possible.
Then, for each chunk that still contains lost symbols, we count the number of its remaining lost symbols.
Next, we globally recover the lost symbols with global parity symbols using upstairs decoding as described in §4.2, except those in the m chunks that have the most lost symbols.
These m chunks can be finally recovered via row parity symbols after all other lost symbols have been recovered.
We thus far assume that there are always s available global parity symbols that are kept outside a stripe.
However, to maintain the regularity of the code structure and to avoid provisioning extra devices for keeping the global parity symbols, it is desirable to keep all global parity symbols inside a stripe.
The idea is that in each stripe, we store the global parity symbols in some sectors that originally store the data symbols.
A challenge is that such inside global parity symbols are also subject to both device and sector failures, so we must maintain their fault tolerance during encoding.
In this section, we propose two encoding methods, namely upstairs encoding and downstairs encoding, which support the construction of inside global parity symbols, while preserving the homomorphic property and hence the fault tolerance of STAIR codes.
These two encoding methods produce the same values for parity symbols, but differ in computational complexities for different configurations.
We show how to deduce parity relations from the two encoding methods, and also show that the two encoding methods have complementary performance advantages for different configurations.
We letˆgletˆ letˆg h,l (0 ≤ l ≤ m 񮽙 − 1 and 0 ≤ h ≤ e l − 1) be an inside global parity symbol.
Figure 5 illustrates how we place the inside global parity symbols.
Without loss of generality, we place them at the bottom of the rightmost data chunks, following the stair layout.
Specifically, we choose the m 񮽙 = 3 rightmost data chunks in Columns 3-5 and place e 0 = 1, e 1 = 1, and e 2 = 2 global parity symbols at the bottom of these data chunks, respectively.
That is, the original data symbols d 3,3 , d 3,4 , d 2,5 , and d 3,5 are now replaced by the inside global parity symbolsˆg symbolsˆ symbolsˆg 0,0 , ˆ g 0,1 , ˆ g 0,2 , andˆgandˆ andˆg 1,2 , respectively.
To obtain the inside global parity symbols, we extend the upstairs decoding method in §4.2 and propose a recovery-based encoding approach called upstairs encoding.
We first set all the outside global parity symbols to be zero (see Figure 5).
Then we treat all m = 2 row parity chunks and all s = 4 inside global parity symbols as lost chunks and lost sectors, respectively.
Now we "recover" all inside global parity symbols, followed by the m = 2 row parity chunks, using the upstairs decoding method in §4.2.
Since all outside global parity symbols are set to be zero, we need not store them.
The homomorphic property, and hence the fault tolerance property, remain the same as discussed in §4.
Thus, in failure mode, we can still use upstairs decoding to reconstruct lost symbols.
We call this encoding method "upstairs encoding" because the parity symbols are encoded from bottom to top as described in §4.2.
In addition to upstairs encoding, we present a different encoding method called downstairs encoding, in which we generate parity symbols from top to bottom and right to left.
We illustrate the idea in Figure 6, which depicts m′ intermediate parity chunks m row parity chunks Figure 5: Upstairs encoding: we set outside global parity symbols to be zero and reconstruct the inside global parity symbols using upstairs decoding (see §4.2).
d 0,0 d 1,0 d 3,0 d 2,0 d 0,1 d 1,1 d 3,1 d 2,1 d 0,2 d 1,2 d 3,2 d 2,2 d 0,3 d 1,3 ĝ 0,0 d 2,3 d 0,4 d 1,4 ĝ 0,1 d 2,4 d 0,5 d 1,5 ĝ 1,2 ĝ 0,2 p 0,1 p 1,1 p 3,1 p 2,1 p 0,0 p 1,0 p 3,0 p 2,0 p * 0,1 p * 1,1 p * 0,0 p * 1,0 d * 0,5 d * 0,4 d * 0,1 d * 0,2 d * 0,0 d * 0,3 d * 1,5 d * 1,4 d * 1,1 d * 1,2 d * 1,0 d * 1, Step 7Step 7Step 7Step 4Step 1Step 2Step 7Step 4Step 1Step 2Step 7Step 4Step 1Step 2Step 3Step 3Step 1Step 2Step 6Step 4Step 1Step 2Step 5Step 4d 0,0 d 1,0 d 3,0 d 2,0 d 0,1 d 1,1 d 3,1 d 2,1 d 0,2 d 1,2 d 3,2 d 2,2 d 0,3 d 1,3 d 2,3 d 0,4 d 1,4 d 2,4 d 0,5 d 1,5 g0,2=0 g1,2=0g0,1=0 g0,0=0 the sequence of generating parity symbols.
We still set the outside global parity symbols to be zero.
First, we encode via C row the n − m = 6 data symbols in each of the first r − e m 񮽙 −1 = 2 rows (i.e., Rows 0 and 1) and generate m + m 񮽙 = 5 parity symbols (including two row parity symbols and three intermediate parity symbols) (Steps 1-2).
The rightmost column (i.e., Column 10) now has r = 4 available symbols, including the two intermediate parity symbols that are just encoded and two zeroed outside global parity symbols.
Thus, we can recover e m 񮽙 −1 = 2 intermediate parity symbols using C col (Step 3).
We can generate m + m 񮽙 = 5 parity symbols (including one inside global parity symbol, two row parity symbols, and two intermediate parity symbols) for Row 2 using C row (Step 4), followed by e m 񮽙 −2 = 1 and e m 񮽙 −3 = 1 intermediate parity symbols in Columns 9 and 8 using C col , respectively (Steps 5-6).
Finally, we obtain the remaining m + m 񮽙 = 5 parity symbols (including three global parity symbols and two row parity symbols) for Row 3 using C row (Step 7).
Table 2 shows the detailed steps of downstairs encoding for the example in Figure 6.
In general, we start with encoding via C row the rows from top to bottom.
In each row, we generate m + m 񮽙 symbols.
When no more rows can be encoded because of insufficient available symbols, we encode via C col the columns from right to left to obtain new intermediate parity symbols (initially, we obtain e m 񮽙 −1 symbols, followed by e m 񮽙 −2 symbols, and so on).
We alternately encode rows and columns until all parity symbols are Detailed Descriptions Table 2: Downstairs decoding: detailed steps for the example in Figure 6.
Steps 1-2, 4, and 7 use C row , while Steps 3 and 5-6 use C col .1 d 0,0 , d 0,1 , d 0,2 , d 0,3 , d 0,4 , d 0,5 ⇒ p 0,0 , p 0,1 , p 񮽙 0,0 , p 񮽙 0,1 , p 񮽙 0,2 2 d 1,0 , d 1,1 , d 1,2 , d 1,3 , d 1,4 , d 1,5 ⇒ p 1,0 , p 1,1 , p 񮽙 1,0 , p 񮽙 1,1 , p 񮽙 1,2 3 p 񮽙 0,2 , p 񮽙 1,2 , g 0,2 = 0, g 1,2 = 0 ⇒ p 񮽙 2,2 , p 񮽙 3,2 4 d 2,0 , d 2,1 , d 2,2 , d 2,3 , d 2,4 , p 񮽙 2,2 ⇒ ˆ g 0,2 , p 2,0 , p 2,1 , p 񮽙 2,0 , p 񮽙 2,1 5 p 񮽙 0,1 , p 񮽙 1,1 , p 񮽙 2,1 , g 0,1 = 0 ⇒ p 񮽙 3,1 6 p 񮽙 0,0 , p 񮽙 1,0 , p 񮽙 2,0 , g 0,0 = 0 ⇒ p 񮽙 3,0 7 d 3,0 , d 3,1 , d 3,2 , p 񮽙 3,0 , p 񮽙 3,1 , p 񮽙 3,2 ⇒ ˆ g 0,0 , ˆ g 0,1 , ˆ g 1,2 , p 3,0 , p 3,1formed.
We can generalize the steps as in §4.2.2, but we omit the details in the interest of space.It is important to note that the downstairs encoding method cannot be generalized for decoding lost symbols.
For example, referring to our exemplary configuration, we consider a worst-case recoverable failure scenario in which both row parity chunks are entirely failed, and the data symbols d 0,3 , d 1,4 , d 2,2 , and d 3,2 are lost.
In this case, we cannot recover the lost symbols in the top row first, but instead we must resort to upstairs decoding as described in §4.2.
Upstairs decoding works because we limit the maximum number of chunks with lost symbols (i.e., at most m + m 񮽙 ).
This enables us to first recover the leftmost virtual parity symbols of the augmented rows first and gradually reconstruct lost symbols.
On the other hand, we do not limit the number of rows with lost symbols in our configuration, so the downstairs method cannot be used for general decoding.
Note that both upstairs and downstairs encoding methods always generate the same values for all parity symbols, since both of them preserve the homomorphic property, fix the outside global parity symbols to be zero, and use the same schemes C row and C col for encoding.Also, both of them reuse parity symbols in the intermediate steps to generate additional parity symbols in subsequent steps.
On the other hand, they differ in encoding complexity, due to the different ways of reusing the parity symbols.
We analyze this in §5.3.
Before relocating the global parity symbols inside a stripe, each data symbol contributes to m row parity symbols and all s outside global parity symbols.
However, after relocation, the parity relations become uneven.
That is, some row parity symbols are also contributed by the data symbols in other rows, while some inside global parity symbols are contributed by only a subset of data symbols in the stripe.
Here, we discuss the uneven parity relations of STAIR codes so as to better understand the encoding and update performance of STAIR codes in subsequent analysis.To analyze how exactly each parity symbol is generated, we revisit both upstairs and downstairs encoding methods.
Recall that the row parity symbols and the inside global parity symbols are arranged in the form of stair steps, each of which is composed of a tread (i.e., the horizontal portion of a step) and a riser (i.e., the vertical portion of a step), as shown in Figure 7.
If upstairs encoding is used, then from Figure 4, the encoding of each parity symbol does not involve any data symbol on its right.
Also, among the columns spanned by the same tread, the encoding of parity symbols in each column does not involve any data symbol in other columns.
We can make similar arguments for downstairs encoding.
If downstairs encoding is used, then from Figure 6, the encoding of each parity symbol does not involve any data symbol below it.
Also, among the rows spanned by the same riser, the encoding of parity symbols in each row does not involve any data symbol in other rows.As both upstairs and downstairs encoding methods generate the same values of parity symbols, we can combine the above arguments into the following property of how each parity symbol is related to data symbols.Property 1 (Parity relations in STAIR codes): In a STAIR code stripe, a (row or inside global) parity symbol in Row i 0 and Column j 0 (where 0 ≤ i 0 ≤ r − 1 and n − m − m 񮽙 ≤ j 0 ≤ n − 1) depends only on the data symbols d i,j 's where i ≤ i 0 and j ≤ j 0 .
Moreover, each parity symbol is unrelated to any data symbol in any other column (row) spanned by the same tread (riser).
Figure 8 illustrates the above property.
For example, p 2,0 depends only on the data symbols d i,j 's in Rows 0-2 and Columns 0-5.
Note thatˆgthatˆ thatˆg 0,1 in Column 4 is unrelated to any data symbol in Column 3, which is spanned by the same tread as Column 4.
Similarly, p 1,1 in Row 1 is unrelated to any data symbol in Row 0, which is spanned by the same riser as Row 1.
We have proposed two encoding methods for STAIR codes: upstairs encoding and downstairs encoding.
Both of them alternately encode rows and columns to obtain the parity symbols.
We can also obtain parity symbols using the standard encoding approach, in which each parity symbol is computed directly from a linear combination of data symbols as in classical Reed-Solomon codes.
We now analyze the computational complexities of these three methods for different configuration parameters of STAIR codes.
STAIR codes perform encoding over a Galois Field, in which linear arithmetic can be decomposed into the basic operations Mult XORs [31].
We define 1 , 2 ) ( 1 , 1 , 1 , 1 1 , 2 ) ( 1 , 1 , 1 , 1 1 , 2 ) ( 1 , 1 , 1 , 1 1 , 2 ) ( 1 , 1 , 1 , 1 Mult XOR(R 1 , R 2 , α) as an operation that first multiplies a region R 1 of bytes by a w-bit constant α in Galois Field GF (2 w ), and then applies XOR-summing to the product and the target region R 2 of the same size.
For example, Y = α 0 · X 0 + α 1 · X 1 can be decomposed into two Mult XORs (assuming Y is initialized as zero): Mult XOR(X 0 , Y, α 0 ) and Mult XOR(X 1 , Y, α 1 ).
Clearly, fewer Mult XORs imply a lower computational complexity.
To evaluate the computational complexity of an encoding method, we count its number of Mult XORs (per stripe).
For upstairs encoding, we generate m · r row parity symbols and s virtual parity symbols along the row direction, as well as s inside global parity symbols and (n − m) · e m 񮽙 −1 − s virtual parity symbols along the column direction.
Its number of Mult XORs (denoted by X up ) is:Xup = row direction 񮽙 񮽙񮽙 񮽙 (n − m) × (m · r + s) + column direction 񮽙 񮽙񮽙 񮽙 r × [(n − m) · e m 񮽙 −1 ].
(1)For downstairs encoding, we generate m · r row parity symbols, s inside global parity symbols, and m 񮽙 · r − s intermediate parity symbols along the row direction, as well as s intermediate parity symbols along the column direction.
Its number of Mult XORs (denoted by X down ) is:X down = row direction 񮽙 񮽙񮽙 񮽙 (n − m) × 񮽙 (m + m 񮽙 ) · r 񮽙 + column direction 񮽙 񮽙񮽙 񮽙 r × s .
(2)For standard encoding, we compute the number of Mult XORs by summing the number of data symbols that contribute to each parity symbol, based on the property of uneven parity relations discussed in §5.2.
We show via a case study how the three encoding methods differ in the number of Mult XORs.
Figure 9 depicts the numbers of Mult XORs of the three encoding methods for different e's in the case where n = 8, m = 2, and s = 4.
Upstairs encoding and downstairs encoding incur significantly fewer Mult XORs than standard encoding most of the time.
The main reason is that both upstairs encoding and downstairs encoding often reuse the computed parity symbols in subsequent encoding steps.
We also observe that for a given s, the number of Mult XORs of upstairs encoding increases with e m 񮽙 −1 (see Equation (1)), while that of downstairs encoding increases with m 񮽙 (see Equation (2)).
Since larger m 񮽙 often implies smaller e m 񮽙 −1 , the value of m 񮽙 often determines which of the two encoding methods is more efficient: when m 񮽙 is small, downstairs encoding wins; when m 񮽙 is large, upstairs encoding wins.In our encoding implementation of STAIR codes, for given configuration parameters, we always pre-compute the number of Mult XORs for each of the encoding methods, and then choose the one with the fewest Mult XORs.
We evaluate STAIR codes and compare them with other related erasure codes in different practical aspects, including storage space saving, encoding/decoding speed, and update penalty.
The main motivation for STAIR codes is to tolerate simultaneous device and sector failures with significantly lower storage space overhead than traditional erasure codes (e.g., Reed-Solomon codes) that provide only device-level fault tolerance.
Given a failure scenario defined by m and e, traditional erasure codes need m + m 񮽙 chunks per stripe for parity, while STAIR codes need only m chunks and s symbols (where m 񮽙 ≤ s).
Thus, STAIR codes save r×m 񮽙 −s symbols per stripe, or equivalently, m 񮽙 − s r devices per system.
In short, the saving of STAIR codes depends on only three parameters s, m 񮽙 , and r (where s and m 񮽙 are determined by e).
Figure 10 plots the number of devices saved by STAIR codes for s ≤ 4, m 񮽙 ≤ s, and r ≤ 32.
As r increases, the number of devices saved is close to m 񮽙 .
The saving reaches the highest when m 񮽙 = s.We point out that the recently proposed SD codes [27,28] are also motivated for reducing the storage space over traditional erasure codes.
Unlike STAIR codes, SD codes always achieve a saving of s − s r devices, which is the maximum saving of STAIR codes.
While STAIR codes apparently cannot outperform SD codes in space saving, it is important to note that the currently known constructions of SD codes are limited to s ≤ 3 only [6,27,28], implying that SD codes can save no more than three devices.
On the other hand, STAIR codes do not have such limitations.
As shown in Figure 10, STAIR codes can save more than three devices for larger s.
We evaluate the encoding/decoding speed of STAIR codes.
Our implementation of STAIR codes is written in C.
We leverage the GF-Complete open source library [31] to accelerate Galois Field arithmetic using Intel SIMD instructions.
Our experiments compare STAIR codes with the state-of-the-art SD codes [27,28].
At the time of this writing, the open-source implementation of SD codes encodes stripes in a decoding manner without any parity reuse.
For fair comparisons, we extend the SD code implementation to support the standard encoding method mentioned in §5.3.
We run our performance tests on a machine equipped with an Intel Core i5-3570 CPU at 3.40GHz with SSE4.2 support.
The CPU has a 256KB L2-cache and a 6MB L3-cache.
We compare the encoding performance of STAIR codes and SD codes for different values of n, r, m, and s. For SD codes, we only consider the range of configuration parameters where s ≤ 3, since no code construction is available outside this range [6,27,28].
In addition, the SD code constructions for s = 3 are only available in the range n ≤ 24, r ≤ 24, and m ≤ 3 [27,28].
For STAIR codes, a single value of s can imply different configurations of e (e.g., see Figure 9 in §5.3), each of which has different encoding performance.
Here, we take a conservative approach to analyze the worst-case performance of STAIR codes, that is, we test all possible configurations of e for a given s and pick the one with the lowest encoding speed.Note that the encoding performance of both STAIR codes and SD codes heavily depends on the word size w of the adopted Galois Field GF (2 w ), where w is often set to be a power of 2.
A smaller w often means a higher encoding speed [31].
STAIR codes work as long as n + m 񮽙 ≤ 2 w and r + e m 񮽙 −1 ≤ 2 w .
Thus, we choose w = 8 since it suffices for all of our tests.
However, SD codes may choose among w = 8, w = 16, and w = 32, depending on configuration parameters.
We choose the smallest w that is feasible for the SD code construction.
We consider the metric encoding speed, defined as the amount of data encoded per second.
We construct a stripe of size roughly 32MB in memory as in [27,28].
We put random bytes in the stripe, and divide the stripe into r × n sectors, each mapped to a symbol.
We obtain the averaged results over 10 runs.Figures 11(a) and 11(b) present the encoding speed results for different values of n when r = 16 and for different values of r when n = 16, respectively.
In most cases, the encoding speed of STAIR codes is over 1000MB/s, which is significantly higher than the disk write speed in practice (note that although disk writes can be parallelized in disk arrays, the encoding operations can also be parallelized with modern multi-core CPUs).
The speed increases with both n and r.
The intuitive reason is that the proportion of parity symbols decreases with n and r. Compared to SD codes, STAIR codes improve the encoding speed by 106.03% on average (in the range from 29.30% to 225.14%).
The reason is that STAIR codes reuse encoded parity information in subsequent encoding steps by upstairs/downstairs encoding (see §5.3), while such an encoding property is not exploited in SD codes.We also evaluate the impact of stripe size on the encoding speed of STAIR codes and SD codes for given n and r.
We fix n = 16 and r = 16, and vary the stripe size from 128KB to 512MB.
Note that a stripe of size 128KB implies a symbol of size 512 bytes, the standard sector size in practical disk drives.
Figure 12 presents the encoding speed results.
As the stripe size increases, the encoding speed of both STAIR codes and SD codes first increases and then drops, due to the mixed effects of SIMD instructions adopted in GF-Complete [31] and CPU cache.
Nevertheless, the encoding speed advantage of STAIR codes over SD codes remains unchanged.
Stripe Size STAIR, s=1 STAIR, s=2 STAIR, s=3 STAIR, s=4Figure 12: Encoding speed of STAIR codes and SD codes for different stripe sizes when n = 16 and r = 16.
We measure the decoding performance of STAIR codes and SD codes in recovering lost symbols.
Since the decoding time increases with the number of lost symbols to be recovered, we consider a particular worst case in which the m leftmost chunks and s additional symbols in the following m 񮽙 chunks defined by e are all lost.
The evaluation setup is similar to that in §6.2.1, and in particular, the stripe size is fixed at 32MB.
Figures 13(a) and 13(b) present the decoding speed results for different n when r = 16 and for different r when n = 16, respectively.
The results of both figures can be viewed in comparison to those of Figures 11(a) and 11(b), respectively.
Similar to encoding, the decoding speed of STAIR codes is over 1000MB/s in most cases and increases with both n and r. Compared to SD codes, STAIR codes improve the decoding speed by 102.99% on average (in the range from 1.70% to 537.87%).
In practice, we often have fewer lost symbols than the worst case (see §4.3).
One common case is that there are only failed chunks due to device failures (i.e., s = 0), so the decoding of both STAIR and SD codes is identical to that of Reed-Solomon codes.
In this case, the decoding speed of STAIR/SD codes can be significantly higher than that of s = 1 for STAIR codes in Figure 13.
For example, when n = 16 and r = 16, the decoding speed increases by 79.39%, 29.39%, and 11.98% for m = 1, 2, and 3, respectively.
We evaluate the update cost of STAIR codes when data symbols are updated.
For each data symbol in a stripe being updated, we count the number of parity symbols being affected (see §5.2).
Here, we define the update penalty as the average number of parity symbols that need to be updated when a data symbol is updated.Clearly, the update penalty of STAIR codes increases with m.
We are more interested in how e influences the update penalty of STAIR codes.
Figure 14 For STAIR codes, we plot the error bars for the maximum and minimum update penalty values among all possible configurations of e. symbols, which are further encoded from almost all data symbols (see §5.2).
We compare STAIR codes with SD codes [27,28].
For STAIR codes with a given s, we test all possible configurations of e and find the average, minimum, and maximum update penalty.
For SD codes, we only consider s between 1 and 3.
We also include the update penalty results of Reed-Solomon codes for reference.
Figure 15 presents the update penalty results when n = 16 and r = 16 (while similar observations are made for other n and r).
For a given s, the range of update penalty of STAIR codes covers that of SD codes, although the average is sometimes higher than that of SD codes (same for s = 1, by 7.30% to 14.02% for s = 2, and by 10.47% to 23.72% for s = 3).
Both STAIR codes and SD codes have higher update penalty than Reed-Solomon codes due to more parity symbols in a stripe, and hence are suitable for storage systems with rare updates (e.g., backup or write-once-read-many (WORM) systems) or systems dominated by full-stripe writes [27,28].
Erasure codes have been widely adopted to provide fault tolerance against device failures in storage systems [32].
Classical erasure codes include standard Reed-Solomon codes [34] and Cauchy Reed-Solomon codes [7], both of which are MDS codes that provide general constructions for all possible configuration parameters.
They are usually implemented as systematic codes for storage applications [26,30,33], and thus can be used to implement the construction of STAIR codes.
In addition, Cauchy Reed-Solomon codes can be further transformed into array codes, whose encoding computations purely build on efficient XOR operations [33].
In the past decades, many kinds of array codes have been proposed, including MDS array codes (e.g., [2][3][4]9,12,13,20,22,29,41,42]) and non-MDS array codes (e.g., [16,17,23]).
Array codes are often designed for specific configuration parameters.
To avoid compromising the generality of STAIR codes, we do not suggest to adopt array codes in the construction of STAIR codes.
Moreover, recent work [31] has shown that Galois Field arithmetic can be implemented to be extremely fast (sometimes at cache line speeds) using SIMD instructions in modern processors.Sector failures are not explicitly considered in traditional erasure codes, which focus on tolerating devicelevel failures.
To cope with sector failures, ad hoc schemes are often considered.
One scheme is scrubbing [24,36,38], which proactively scans all disks and recovers any spotted sector failure using the underlying erasure codes.
Another scheme is intra-device redundancy [10,11,36], in which contiguous sectors in each device are grouped together to form a segment and are then encoded with redundancy within the device.
Our work targets a different objective and focuses on constructing an erasure code that explicitly addresses sector failures.To simultaneously tolerate device and sector failures with minimal redundancy, SD codes [27,28] (including the earlier PMDS codes [5], which are a subset of SD codes) have recently been proposed.
As stated in §1, SD codes are known only for limited configurations and some of the known constructions rely on extensive searches.
A relaxation of the SD property has also been recently addressed as a future work in [27], which assumes that each row has no more than a given number of sector failures.
It is important to note that the relaxation of [27] is different from ours, in which we limit the maximum number of devices with sector failures and the maximum number of sector failures that simultaneously occur in each such device.
It turns out that our relaxation enables us to derive a general code construction.
Another similar kind of erasure codes is the family of locally repairable codes (LRCs) [18,19,35].
Pyramid codes [18] are designed for improving the recovery performance for small-scale device failures and have been implemented in archival storage [40].
Huang et al.'s and Sathiamoor- thy et al.'s LRCs [19, 35] can be viewed as generalizations of Pyramid codes and are recently adopted in commercial storage systems.
In particular, Huang et al.'s LRCs [19] achieve the same fault tolerance property as PMDS codes [5], and thus can also be used as SD codes.
However, the construction of Huang et al.'s LRCs is limited to m = 1 only.
To our knowledge, STAIR codes are the first general family of erasure codes that can efficiently tolerate both device and sector failures.
We present STAIR codes, a general family of erasure codes that can tolerate simultaneous device and sector failures in a space-efficient manner.
STAIR codes can be constructed for tolerating any numbers of device and sector failures subject to a pre-specified sector failure coverage.
The special construction of STAIR codes also makes efficient encoding/decoding possible through parity reuse.
Compared to the recently proposed SD codes [5,27,28], STAIR codes not only support a much wider range of configuration parameters, but also achieve higher encoding/decoding speed based on our experiments.In future work, we explore how to correctly configure STAIR codes in practical storage systems based on empirical failure characteristics [1,25,36,37].
The source code of STAIR codes is available at http://ansrlab.cse.cuhk.edu.hk/software/stair.
We would like to thank our shepherd, James S. Plank, and the anonymous reviewers for their valuable comments.
This work was supported in part by grants from the University Grants Committee of Hong Kong (project numbers: AoE/E-02/08 and ECS CUHK419212).
yWe formally prove the homomorphic property described in §4.1.
We state the following theorem.Theorem 1 In the construction of the canonical stripe of STAIR codes, the encoding of each chunk in the column direction via C col is homomorphic, such that each augmented row in the canonical stripe is a codeword of C row .
Proof: We prove by matrix operations.
We define the matricesAlso, we define the generator matrices G row and G col for the codes C row and C col , respectively, as:where I is an identity matrix, and A and B are the submatrices that form the parity symbols.
The upper r rows of the stripe can be expressed as follows:The lower e m 񮽙 −1 augmented rows are expressed as follows:We can see that each of the lower e m 񮽙 −1 rows can be calculated using the generator matrix G row , and hence is a codeword of C row . 񮽙
We formally prove the homomorphic property described in §4.1.
We state the following theorem.Theorem 1 In the construction of the canonical stripe of STAIR codes, the encoding of each chunk in the column direction via C col is homomorphic, such that each augmented row in the canonical stripe is a codeword of C row .
Proof: We prove by matrix operations.
We define the matricesAlso, we define the generator matrices G row and G col for the codes C row and C col , respectively, as:where I is an identity matrix, and A and B are the submatrices that form the parity symbols.
The upper r rows of the stripe can be expressed as follows:The lower e m 񮽙 −1 augmented rows are expressed as follows:We can see that each of the lower e m 񮽙 −1 rows can be calculated using the generator matrix G row , and hence is a codeword of C row . 񮽙
