In recent years, privacy-preserving toll collection has been proposed as a way to resolve the tension between the desire for sophisticated road pricing schemes and drivers' interest in maintaining the privacy of their driving patterns.
Two recent systems in particular, VPriv (USENIX Security 2009) and PrETP (USENIX Security 2010), use modern cryptographic primitives to solve this problem.
In order to keep drivers honest in paying for their usage of the roads, both systems rely on unpredictable spot checks (e.g., by hidden roadside cameras or roaming police vehicles) to catch potentially cheating drivers.
In this paper we identify large-scale driver collusion as a threat to the necessary unpredictability of these spot checks.
Most directly, the VPriv and PrETP audit protocols both reveal to drivers the locations of spot-check cameras-information that colluding drivers can then use to avoid paying road fees.
We describe Milo, a new privacy-preserving toll collection system based on PrETP, whose audit protocol does not have this information leak, even when drivers misbehave and collude.
We then evaluate the additional cost of Milo and find that, when compared to naïve methods to protect against cheating drivers, Milo offers a significantly more cost-effective approach.
Assessing taxes to drivers in proportion to their use of the public roads is a simple matter of fairness, as road maintenance costs money that drivers should expect to pay some part of.
Gasoline taxes, currently a proxy for road use, are ineffective for implementing congestion pricing for city-center or rush-hour traffic.
At the same time, the detailed driving records that would allow for such congestion pricing also reveal private information about drivers' lives, information that drivers do seem to * smeiklej@cs.ucsd.edu † kmowery@cs.ucsd.edu ‡ s@cs.ucsd.edu § hovav@cs.ucsd.edu have interest in keeping private.
(In the U.S., for example, some courts have recognized drivers' privacy interests by forbidding the police from using a GPS device to record a driver's movements without a search warrant [1].)
The VPriv [39] and PrETP [4] systems for private tolling, proposed at USENIX Security 2009 and 2010 respectively, attempt to use modern cryptographic protocols to resolve the tension between sophisticated road pricing and driver privacy.
At the core of both these systems is a monthly payment and audit protocol.
In her payment, each driver commits to the road segments she traversed over the month and the cost associated with each segment, and reveals the total amount she owes.
The properties of the cryptography used guarantee that the total is correct assuming the segments driven and their costs were honestly reported, but that the specific segments driven are still kept private.To ensure honest reporting, the systems use an auditing protocol: throughout the month, roadside cameras occasionally record drivers' locations; at month's end, the drivers are challenged to show that their committed road segments include the segments in which they were observed, and that the corresponding prices are correct.
So long as such spot checks occur unpredictably, drivers who attempt to cheat will be caught with high probability given even a small number of auditing cameras.
In the audit protocols for both VPriv and PrETP, however, the authority reveals to each driver the locations at which she was observed.
(The driver uses this information to open the appropriate cryptographic commitments.)
If the cameras aren't mobile, or are mobile but can be placed in only a small set of suitable locations (e.g., overpasses or exit signs along a fairly isolated highway), then the drivers will easily learn where the cameras are (and, perhaps more importantly, where they aren't).
Furthermore, if drivers collude and share the locations at which they were challenged, then a few audit periods will suffice for colluding drivers to learn and map the cameras' locations.We believe the model of large-scale driver collusion is a realistic one.
For example, drivers already collaborate to share the locations of speed cameras [37] and red-light cameras [38]; if we extend this behavior to consider maps of audit cameras, then we see that the unpredictable spot checks required in the analysis of VPriv and PrETP are difficult to achieve in the real world when drivers may collude on a large scale.
When drivers know where cameras are (and where they aren't), they will not pay for segments that are camera-free, and may even change driving patterns to avoid the cameras.
By collaborating, drivers can discover and share camera locations at acceptable cost; in fact, if the cameras are revealed to them directly in the course of the audit protocol then they can do so without incurring a single fine.Finally, one might argue that an appropriate placement of audit cameras at chokepoints will make them impossible to avoid, even if their location is known; the price charged for traversing such a chokepoint could then be made sufficiently high that it subsidizes the cost of maintaining other, unaudited road segments.
This alternative arrangement may seem superficially appealing, but it is ultimately incompatible with driver privacy.
If drivers cannot avoid a chokepoint they cannot but be observed by authorities when they cross it; in other words, this approach would be feasibly enforceable only when most drivers are regularly observed at the chokepoints.
In fact, what we have described is precisely the situation today in many cities, where tolls are collected on bridges and other unavoidable chokepoints.Our contribution We show, in Section 4, how to modify the PrETP system to obtain our own system, Milo, in which the authority can perform an agreed-upon number of spot checks of a driver's road-segment commitments without revealing the locations being checked.
To achieve this, we adapt a recent oblivious transfer protocol due to Green and Hohenberger [28] that is based on blind identity-based encryption.
We have implemented and benchmarked our modifications to the audit protocol, showing (in Section 5) that they require a small amount of additional work for each driver and a larger but still manageable amount of work for the auditing authority.Basic fairness demands that drivers whom the authority accuses of cheating be presented with the evidence against them: a photo of their car at a time and location for which they did not pay.
This means that drivers who intentionally incur fines will inevitably learn some camera locations; in some cases, a large coalition of drivers may therefore profitably engage in such misbehavior.
Here the information about camera locations is leaked not by the audit protocol but by the legal proceedings that follow it.Finally, if the cameras are themselves visible then drivers will discover and share their locations, regardless of the cryptographic guarantees of the audit protocol.
1 All that is necessary is for one driver to spot the camera at any point during the month; the colluding drivers can then ensure that their commitments take this camera into account.
We discuss this further in Section 6.
In summary, our paper makes three concrete contributions:• we identify large-scale driver collusion as a realistic threat to privacy-preserving tolling systems;• we modify the PrETP system to avoid leaking camera locations to drivers during challenges; and• we identify and evaluate other ways to protect against driver collusion and compare their costs to that of Milo.
In this section we present an overview of the Milo system.
We discuss both the organizational structure of the system, as well as the security goals it is able to achieve.
As our system is built directly on top of PrETP we have approximately maintained its structure, with the important differences highlighted below.
Milo consists of three main players: the driver, represented by an On-Board Unit (OBU); the company operating the OBU (abbreviated TSP, for Toll Service Provider); and finally the local government (or TC, for Toll Charger) responsible for setting the road prices and collecting the final tolls from the TSP, as well as for ensuring fairness on the part of the driver.
The interactions between these parties can be seen in Figure 1.
In some respects, the organization of Milo is similar to that of current toll collection systems.
The driver will keep a certain amount of money in an account with the TSP; at the end of every month the driver will then pay some price appropriate for how much she drove and the amount of money remaining in the account will need to be replenished.
The major difference, of course, is that the payments of the driver do not reveal any information about their actual locations while driving.
2 In addition, we will require that the TC perform occasional spot checks to guarantee that drivers are behaving honestly.The OBU is a box installed in the car of the driver, which is responsible for collecting location information, computing the prices associated with the roads, and forming the final payment information that is sent to the TSP Figure 1: An overview of how the Milo system works.
As we can see, the OBU deals with the TSP for payment purposes (using the Pay protocol), but for spot checks it interacts with the TC (using the Audit protocol).
The TC conducts these audits using both the information recorded by the cameras it operates along the roads and the OBU's payment information, which is forwarded on from the TSP after it has been checked to be correct (using the VerifyPayment protocol).
at the end of each month.
Its work in this stage is described formally in our Pay algorithm, which we present in Section 4.
The TSP is responsible for the collection of tolls from the driver.
At the end of each month, the TSP will receive a payment message from the OBU as specified above.
It is then the job of the TSP to verify that this payment information is correct, using the VerifyPayment algorithm outlined in Section 4.
If the payment information is found to be correctly formed then the TSP can debit the appropriate payment from the user's account; otherwise, they can proceed in a legal manner that is similar to the way in which traffic violations are handled now.The TC, as mentioned, is the local government responsible for setting the prices on the roads, as well as the fines for dishonest drivers who are caught.
The TC is also responsible for performing spot checks to ensure that drivers are behaving honestly.
Although this presents a new computational burden for the TC (as compared to PrETP, for example, which has the TSP performing the spot checks), we believe that it is important to keep all location information completely hidden from the TSP, as it is a business with incentive to sell this information.
Since the TC already sees where each car is driving regardless of which body performs the spot checks (since it is the one operating the cameras), having it perform the audits itself minimizes the privacy lost by the driver.Note, however, that the formal guarantees of correctness, security, and privacy provided by our system do not depend on having the TSP and TC not collaborate.
In fact, both roles could be performed by a single organization.
Since in practice businesses such as E-ZPass play the role of TSP, we recommend the separation of duties above to avoid giving the TSP an incentive to monetize customers' driving records.
Of course, this assumes that regulation or the courts will forbid the government from misusing the information it collects.
In any privacy-preserving system, there are two goals which are absolutely essential to the success of the system: maintaining privacy, while still keeping users of the system honest.
We discuss what this means in the context of electronic toll collection in the following two points:• Driver privacy: Drivers should be able to keep their locations completely hidden from any other drivers who may want to intercept (and possibly modify) their payment information on its way to the TSP.
With the exception of the random spot checks performed by the audit authority (in our case the TC), the locations of the driver should also be kept private from both the TC and the TSP.
This property should hold even for a malicious TSP; as for the TC, we would like to guarantee that, as a result of the audit protocol, it learns only whether the driver was present at certain locations and times of its choice, even if it is malicious.
The number of these locations and times about which the TC can query is fixed and a parameter of the audit protocol.
An honest-butcurious TC will query the driver at those locations and times where she was actually observed, but a malicious TC might query for locations where no camera was present; see Section 4.3 for further discussion.
• Driver honesty: Drivers should not be able to tamper with the OBU to produce incorrect location or price information; i.e., pretending they were in a given location, using lower prices than are actually assigned, or simply turning off the OBU to pretend they drove less than they actually did.
This property should hold even if drivers are colluding with other dishonest drivers, and should in fact hold even if every driver in the system is dishonest.These security goals should look fairly similar to those outlined in previous work (e.g., PrETP or VPriv [39], and inspired by the earlier work of Blumberg, Keeler, and shelat [8]), but we note the consideration of possibly colluding drivers as an essential addition.
We also note that we do not consider physical attacks (i.e., a malicious party gaining physical access to a driver's car) in this model, as we believe these attacks to be out of scope.For ideal privacy, the locations of each driver would be kept entirely private even from the TC.
This does not seem to be possible, however, as it would allow drivers to behave dishonestly without any risk of getting caught.
Because each camera does take away some degree of privacy from the driver, we would like to minimize the number of cameras operated by the TC; at the same time, we need to keep it high enough so that the TC will have a very good chance of catching any cheating drivers.
We believe this to be a fundamental limitation on the value of any privacy-preserving tolling system, however, as they are privacy preserving only when the spot-check cameras do not monitor such a large fraction of trips that the records themselves constitute a substantial privacy violation.
As Blumberg, Keeler, and shelat write, "Extensive camera networks are simply not compatible with the kinds of privacy we demand since they collect too much information.
If misused, they can provide adequate data for real-time tracking of vehicles" [8].
Finally, we note that these security properties are both achieved by Milo, under the assumption that cameras are randomly placed and invisible to drivers (i.e., the only way camera locations can leak to drivers is during the audit protocol).
We discuss the potential issues with this assumption in Section 6.
Because our scheme follows closely the PrETP construction [4], we employ the same modern cryptographic primitives as they do: commitment schemes and zeroknowledge proofs, in addition to the more familiar primitive of digital signatures [26].
In addition, to keep the spotcheck camera locations hidden from drivers, we make use of another primitive, blind identity-based encryption, in a manner that is inspired by the oblivious transfer protocol of Green and Hohenberger [28].
A commitment scheme is essentially the cryptographic relative of an envelope, and consists of two main phases: forming the commitment and opening the commitment.
First, to form a commitment to a certain value, a user Alice can put the value in the envelope and then seal the envelope; to keep the analogy going, let's also assume she sealed it in some special way such that only she can open it.
The sealed envelope then acts as her commitment, which she can send on to another user Bob.
When the time comes, Alice can reveal the committed value by opening the envelope and showing Bob its contents.
There are two properties that commitment schemes satisfy: hiding and binding.
The hiding property says that, because Alice is the only one who can unseal the envelope, Bob will not be able to learn any information about its contents before she reveals them.
In addition, the binding property says that, because the envelope is sealed, Alice will not be able to open it, change the value inside, and give it back to Bob without him noticing.
In other words, when Alice finally reveals the opening of the commitment, Bob can be satisfied that those were the values inside all along.
We will use the notation c = Com(m; r) to mean that c is a commitment to the message m using some randomness r. (Note that there are also some parameters involved, but that here and in the primitives that follow we omit them for simplicity.)
One more property we will require of our commitment schemes is that they are additively homomorphic.
This means that there is an operation on commitments, call it , such that if c 1 is a commitment to m 1 and c 2 is a commitment to m 2 , then c 1 c 2 will be a commitment to m 1 + m 2 .
This property can be achieved by a variety of schemes; to best suit our purposes, we work with FujisakiOkamoto commitments [18,22], which rely on the Strong RSA assumption for their security.
Our second primitive, zero-knowledge proofs [24,25], provides a way for someone to prove to someone else that a certain statement is true without revealing anything beyond the validity of the statement.
For example, a user of a protected system might want to prove the statement "I have the password corresponding to this username" without revealing the password itself.
The two main properties of zero-knowledge proofs are soundness and zero knowledge.
Soundness guarantees that the verifier will not accept a proof for a statement that is false; in the above example, this means that the system will accept the proof only if the prover really does have the password.
Zero knowledge, on the other hand, protects the prover's privacy and guarantees that the system in our example will not learn any information about the password itself, but only that the statement is true.
A non-interactive zero-knowledge proof (NIZK for short) is a particularly desirable type of proof because, as the name indicates, it does not require any interaction between the prover and the verifier.
For a given statement S, we will use the notation π = NIZKProve(S) to mean a NIZK formed by the prover for the statement S. Similarly, we will use NIZKVerify(π, S) to mean the process run by the verifier to check, using π, that S is in fact true.
In our system we will need to prove only one type of statement, often called a range proof, which proves that a secret value x satisfies the inequality lo ≤ x < hi, where lo and hi are both public.
For this we can use Boudot range proofs and their extensions [11,34], which are secure in the random oracle model [6] and assuming the Strong RSA assumption.
Finally, to maintain driver honesty even in the case of possible collusions between drivers (as discussed in Section 2), we use an additional cryptographic primitive: identity-based encryption [10,42].
Intuitively, identitybased encryption (IBE for short) extends the notion of standard public-key encryption by allowing a user's public key to be, rather than just a random collection of bits, some meaningful information relevant to their identity; e.g., their e-mail address.
This is achieved through the use of an authority, who possesses some master secret key msk and can use it to provide secret keys corresponding to given identities on request (provided, of course, that the request comes from the right person).
When we work with IBE, we will use the syntax C = IBEnc(id; m) to mean an identity-based encryption of the message m, intended for the person specified in the identity id.
We will similarly use m = IBDec(sk id ;C) to mean the decryption of C using the secret key for the identity id.Because of how IBE is integrated into our system, we will need the IBE to be augmented by a blind extraction protocol: a protocol interaction between a user and the authority at the end of which the user obtains the secret key corresponding to some identity of her choice, but the authority does not learn which identity was requested (and also does not learn the secret key for that identity).
This process of getting the secret key will be denoted as sk id = BlindExtract(id), keeping in mind that the authority learns neither id nor sk id .
As we show in Section 4, this property (introduced by Green and Hohenberger [28]) is crucial for guaranteeing that drivers do not learn where the TC has its cameras.Furthermore, we would like our IBE to be anonymous [2], meaning that given a ciphertext C, a user cannot tell which identity the ciphertext is meant for (so, in particular, they cannot check to see if a guess is correct).
Again, as we show in Section 4, this property is necessary to ensure that the TSP cannot simply guess and check where the driver was at a given time, and thus potentially learn information about her whereabouts.To the best of our knowledge, there are two blind and anonymous IBEs in the cryptographic literature: the first due to Camenisch, Kohlweiss, Rial, and Sheedy [13] and the second to Green [27]; both are blind variants on the Boyen-Waters anonymous IBE [12].
While either of these schemes would certainly work for our purposes, we chose to come up with our own scheme in order to maximize efficiency.
Our starting point is the BonehFranklin IBE [10], which is already anonymous [2, Section 4.5].
We then introduce a blind key-extraction protocol for Boneh-Franklin, based on the Boldyreva blind signature [9].
Finally, we "twin" the entire scheme to essentially run two copies in parallel; this is just to facilitate a "Twin Diffie-Hellman" style security proof [15].
We give a full description of our scheme in the full version of our paper [36], as well as a proof of its security in a variant of the Green-Hohenberger security model.
Our IBE is conveniently efficient, but we stress that the Milo system could be instantiated with any provably secure IBE that is both blind and anonymous (and in particular the schemes of Camenisch et al. and Green which, while not as efficient as our scheme, have the attractive properties that they use significantly weaker assumptions and do not rely on random oracles in their proofs of security).
In the broadest sense, our blind IBE can be viewed as a special case of a secure two-party computation between the OBU and the TC, at the end of which the TC learns whether or not the driver paid honestly for a given segment, and the driver learns nothing (and in particular does not learn which segment the TC saw her in).
As such, any efficient instantiation of this protocol as a secure two-party computation would be sufficient for our purposes.
One promising approach, suggested by an anoymous reviewer, uses an oblivious pseudorandom function (OPRF for short) as a building block.
With an OPRF, a user with access to a seed k for a PRF f and another user with input x can securely evaluate f k (x) without the first user's learning x or f k (x), and without the second user's learning the seed k; this can be directly applied to our setting by treating the seed k as a value known by the OBU, and the input x as the segment in which the TC saw the driver.
An efficient OPRF was recently given by Jarecki and Liu [32].
Compared to our approach, the OPRFs of Jarecki and Liu may require increased interaction (which has implications for concurrent security) and potentially more computation than ours.
In this section, we describe the various protocols used within our system and how they meet the security goals described in Section 2.2; we note that only Algorithm 4.3 substantially differs from what is used in PrETP.
There are three main phases we consider: the initialization of the OBU, the forming and verifying of the payments performed by the OBU and the TSP respectively, and the audit between the TC and the OBU.
Below, we will detail the functioning of each of these algorithms; first, though, we give some intuition for how our scheme works and why the use of blind identity-based encryption means the audit protocol does not leak the locations of spot-check cameras to drivers.In the audit protocol, the driver needs to show that her actual driving is consistent with the fee she chose to pay.
To do this, she must upload her (claimed) driving history to the TSP's server; if she didn't, the TSP would have nothing to check the correctness of.
Obviously, simply uploading this history in the clear would provide no privacy.
The VPriv system sidesteps this by having the driver upload the segments anonymously (using an anonymizing service such as Tor [20]), accompanied by a "tag" that will allow her to claim them as her own.
We instead follow PrETP in having the driver upload a commitment of sorts to each of her segments.
In addition, the driver commits to the cost associated with each segment using the additively homomorphic commitment scheme.
Checking that the total payment is the sum of the fees for each committed segment is now easy: using the homomorphic operation , the TSP can compute a commitment to the sum of the committed fees; the driver then provides the opening of this sum commitment, showing that its value is the fee she paid.
3 What remains is to prove that the committed segments the driver uploaded to the server are in fact the segments she drove, and that the committed fee she uploaded alongside each is in fact the fee charged for driving it.
Following VPriv, PrETP, and de Jonge and Jacobs' system (see Section 7), we rely on spot check cameras.
The TC's cameras observed the driver at a few locations over the course of the month.
It now challenges the driver to show that these locations are among the committed segments, and that the corresponding committed fees are correct.
If the driver cannot show a commitment that opens to one of these spot check locations, she has been caught cheating; if the spot check locations are unpredictable then a simple probability analysis (see Section 6.1) shows that a cheating driver will likely be caught.
In PrETP, the spot check has the TC sending to the driver the locations and times where she was observed; the driver returns the index and opening of the corresponding committed segments.
This, of course, leaks the spot check locations to the driver.
To get around this, we must somehow transmit the appropriate openings to the TC without the driver finding out which commitments are being opened.Identity-based encryption allows us to achieve exactly the requirement above.
Along with each of her commitments, the driver encrypts the opening of the commitment using IBE; the identity to which a commitment is encrypted is the segment location and time.
She sends these encrypted openings to the TC along with the commitments themselves.
(Note that it is crucial the ciphertext not reveal the identity to which it was encrypted, since otherwise the TSP and TC would learn the driver's entire driving history.
This is why we require an anonymous IBE.)
Now, if the TC had the secret key for the identity corresponding to the place and time where the driver was spotted, it could decrypt the appropriate ciphertext, obtain the commitment opening value, and check that the corresponding commitment was properly formed.
But the TC can't ask the driver for the secret key, since this would also leak the spot-check location.
Instead, it engages with the driver in a blind key-extraction protocol.
The TC provides as input the location and time of the spot check and obtains the corresponding secret key without the driver learning which identity (i.e., location and time) was requested.
By undertaking the blind extraction protocol only a certain number of times, the driver limits the number of spot checks the TC can perform.Note that this is essentially an oblivious transfer protocol; our solution is in fact closely related to the oblivious transfer protocol of Green and Hohenberger [28], who introduced blind IBE.Before any of the three phases can take place, the TC first decides on the segments used for payment and how much each one actually costs.
It starts by dividing each road into segments of some appropriate length, for example one city block in denser urban areas or one mile along a highway in less congested areas.
Because prices might change according to time of day, the TC also decides on a division of time into discrete quanta based on some "time step" when a new segment must be recorded by the OBU (even if the location endpoint has not yet been reached).
For example, if two location endpoints are set as Exit 17 and Exit 18 on a highway and the time step is set to be a minute, then a driver traveling between these exits for more than a minute will have segments with the same location endpoints, but different time endpoints.
In particular, if this driver starts at 22:00 and takes about three minutes to get from one exit to the other, she will end up with three segments: 4• (exit 17, exit 18), (22:00, 22:01) ;• (exit 17, exit 18), (22:01, 22:02) ; and• (exit 17, exit 18), (22:02, 22:03) .
Each segment is of the form (loc 1 , loc 2 ), (time 1 , time 2 ) ; in the future, we denote these segments as (where, when), where where represents the physical limits of the segment and when represents the particular time quantum during which the driver was in the segment where.
For each of these segments, the TC will have assigned some price; this can be thought of as a publicly available function f : (where, when) → [0, M], where M is the maximum price assigned by the TC.
Before any payments can be made, there are a number of parameters that need to be loaded onto the OBU.
To start, the OBU will be given some unique value to identify itself to the TSP; we refer to this value as tag.
Because the OBU will be signing all the messages it sends, it first needs to generate a signing keypair (vk tag , sk tag ); the public verification key will need to be stored with both the TSP and TC, while the signing key will be kept private.
We will also use an augmented version of the BlindExtract protocol (mentioned in Section 3.3) in which the OBU and TC will sign their messages to each other, which means the OBU will need to have the verification key for the TC stored as well (alternatively, they could just communicate using a secure channel such as TLS).
In addition, the OBU will need to generate parameters for an IBE scheme in which it possesses the master secret key msk, as well as to load the parameters for the commitment and NIZK schemes (note that it is important the OBU does not generate these latter parameters itself, as otherwise the driver would be able to cheat).
Finally, the OBU will also need to have stored the function f used to define road prices.
Once the OBU is set up with all the necessary parameters, it can begin making payments.
As the driver travels, the GPS picks up location and time information, which can then be matched to segments (where, when).
For each of these segments, the OBU first computes the cost for that segment as p = f (where, when).
It then computes a commitment c to this value p; we will refer to the opening of this commitment as open c .
Next, the OBU computes an identity-based encryption C of the opening open c along with a confirmation value 0 λ , using the identity id = (where, when).
Finally, the OBU computes a non-interactive zero-knowledge proof π that the value contained in c is in the range [0, M].
This process is then repeated for every segment driven, so that by the end of the month the OBU will end up with a set of tuples(c i ,C i , π i ) n i=1.
In addition to this set, the OBU will also need to compute the opening open final for the commitment c final = c 1 c 2 · · · c n ; i.e., the opening for the commitment to the sum of the prices, which effectively reveals how much the driver owes.
The OBU then creates the final message m = tag, open final ,(c i ,C i , π i ) i, signs it to get a signature σ m , and sends to the TSP the tuple (m, σ m ).
This payment process is summarized in Algorithm 4.1.
The parameter λ , set to 160 for 80-bit security, is explained below.Once the TSP has received this tuple, it first looks up the verification key for the signature using tag.
If it is satisfied that this message came from the right OBU, then it performs several checks; if not, it aborts and alerts the OBU that something went wrong (i.e., the message was manipulated in transit) and it should resend the tuple.
Next, it checks that each commitment c i was properly In terms of privacy, the hiding property of the commitment scheme and the zero knowledge property of the NIZK scheme guarantee that the driver's information is being kept private from the TSP.
Furthermore, the anonymity of the IBE scheme guarantees that, although the segments are used as the identity for the ciphertexts C i , the TSP will be unable to learn this information given just these ciphertexts.
In addition, some degree of honesty is guaranteed.
First, because the message was signed by the OBU, the TSP can be sure that the tuple came from the correct driver and not some other malicious driver trying to pass herself off as someone else (or cause the first driver to pay more than she owes).
Furthermore, if all the checks pass then the binding property of the commitment scheme and the soundness property of the NIZK scheme guarantee that the values contained in the commitments are to valid prices and so the TSP can be somewhat convinced that the price p final given by the driver is the correct price she owes for the month.
The TSP cannot, however, be convinced yet that the driver did not simply turn off her OBU or otherwise fake location or price information; for this, it will need to forward the payment tuple to the TC, which initiates the audit phase of the protocol.
As we argued in Section 2.2, although the audit protocol does take away some degree of privacy from the driver, this small privacy loss is necessary to ensure honesty within the system.
We additionally argued that the TC should not reveal to the driver the locations of the cameras and furthermore believe that the driver should not even learn the number of cameras at which the TC saw her, as even this information would give her opportunity to cheat (for more on this see Section 6).
We therefore assume that the TC makes some fixed number of queries k for every driver, regardless of whether or not it has in fact seen the driver k times.
To satisfy this assumption, if the TC has seen the driver on more than k cameras, it will just pick the first k (or pick k at random, it doesn't matter) and query on those.
If it has seen the driver on fewer than k cameras, we can designate some segment to be a "dummy" segment, which essentially does not correspond to any real location/time tuple.
The TC can then query on this dummy segment until it has made k queries in total; because the part of the protocol in which the TC performs its queries is blind, the OBU won't know that it is being queried on the same segment multiple times.
After the TSP has forwarded the OBU's payment tuple to the TC, the TC first checks that the message really came from the OBU (and not, for example, from a malicious user or even the TSP trying to frame the driver).
As with the TSP, if this check fails then it can abort the protocol and alert the OBU or TSP.
It then extracts the tuples (c i ,C i , π i ) from m and begins issuing its random spot checks to ensure that the driver was not lying about her whereabouts.
This process is outlined in Algorithm 4.3.
Because there were a certain number of cameras the driver passed, the TC will have a set of tuples (loc i , time i ) of its own that correspond to the places and times at which the TC saw the driver.
First, for every pair (loc, time), the TC will need to determine which segment this pair belongs to; this then gives it a set (where i , when i ) of tuples that the driver would have logged if they were behaving honestly (unless the set has been augmented by the dummy segment as described above, in which case the OBU clearly will not have logged this segment).
After the TC has this set of tuples, it uses the identitybased encryption C j contained within every tuple sent by the OBU.
Recall from Algorithm 4.1 that the identity corresponding to each encryption is the segment (where j , when j ), and that the encryption itself is of the opening of the commitment c j (contained in the same tuple), along with a confirmation value 0 λ .
Therefore, if the TC can obtain the secret key sk id from the OBU for the identity id = (where j , when j ), then it can successfully decrypt the ciphertext and obtain the opening for the commitment, which it can then use to check if the driver is recording correct price information.
Because the TC does not know which ciphertext corresponds to which segment, however, once the TC obtains this secret key it will then need to attempt to decrypt each C j .
To prevent drivers from using a single commitment to pay for two segments, we require that it be computationally difficult to find a ciphertext C that has valid decryptions under two identities id 1 and id 2 .
For our IBE, it is sufficient to encrypt a confirmation value 0 λ along with the message (where λ = 160 for 80-bit security), since messages are blinded with a random oracle hash that takes the identity as input.
On decryption, one checks that the correct confirmation value is present.
Note that we do not require CCA security.If C j does decrypt properly for some j, then the TC checks that the value contained inside is the opening of the commitment c j .
If it is, then the TC further checks that the price p j is the correct price for that road segment by computing f (where j , when j ).
If this holds as well, then the TC can be satisfied that the driver paid correctly for the segment of the road on which she was seen and move on to the next camera.
If it does not hold, then the TC has reason to believe that the driver lied about the price of the road she was driving on.
If instead the opening is not valid, the TC has reason to believe that the driver formed either the ciphertext C j or the commitment c j incorrectly.
Finally, if none of the ciphertexts properly decrypted using sk id (i.e., C j did not decrypt for any value of j), then the TC knows that the driver simply omitted the segment (where j , when j ) from her payment in an attempt to pretend she drove less.
In any of these cases, the TC believes the driver was cheating in some way and can undertake legal proceedings.
If all of these checks pass for every camera, then the driver has successfully passed the audit and the TC is free to move on to another user.In terms of driver honesty, the addition of BlindExtract allows the TC to obtain sk id without the OBU learning the identity, and thus the location at which they were caught on camera.
As argued in Section 2, this is absolutely cru- cial for maintaining driver honesty, both individually and in the face of possible collusions.
In terms of privacy, if the OBU and TC sign their messages in the BlindExtract phase, then we can guarantee that no malicious third party can alter messages in their interaction in an attempt to learn the segment in which the driver was caught on camera (or, alternatively, frame the driver by corrupting sk id ).
As mentioned in Section 2, whereas the cameras do take away some part of the driver's privacy, they are necessary to maintain honesty; we also note that no additional information is revealed throughout the course of this audit interaction provided both parties behave honestly.
One potential downside of this protocol, however, is that the TC is not restricted to querying locations at which it had cameras; it can essentially query any location it wants without the driver's knowledge (although the driver is at least aware of how many queries are being made).
We believe that our system could be augmented to resist such misbehavior through an "audit protocol audit protocol" that requires the TC to demonstrate that it actually has camera records corresponding to some small fraction of the spot check it performs, much as its own audit protocol requires the driver to reveal some small fraction of its segments driven.
This "audit audit" could be performed on behalf of drivers by an organization such as EFF or the ACLU; alternatively, in some legal settings an exclusionary rule could be introduced that invalidates evidence obtained through auditing authority misbehavior.
The numbers for encryption and decryption represent the time taken to encrypt/decrypt a pair of 1024-bit numbers using the curve y 2 = x 3 + x mod p at the 80-bit security level, and the numbers for blind extraction represent the time to complete the computation required for each side of the interactive protocol.
In order to achieve a more effective audit protocol, an extra computational burden is required for both the OBU and the TC.
In this section, we consider just how great this additional burden is; in particular, we focus on our blind identity-based encryption protocol from the full version of our paper [36], as well as Algorithm 4.3 from Section 4.3.
The benchmarks presented for these protocols were collected on two machines: a MacBook Pro running Mac OS X 10.6 with a 2.53 GHz Intel Core 2 Duo processor and 4 GB of RAM, and an ARM v5TE running Linux 2.6.24 with a 520 MHz processor and 128 MB of RAM.
We believe that the former represents a fairly conservative estimate for the amount of computational resources available to the TC, whereas the latter represents a machine that could potentially be used as an OBU.
For the bilinear groups needed for blind IBE we used the supersingular curve y 2 = x 3 + x mod p for a large prime p (which has embedding degree 2) within version 5.4.3 of the MIR-ACL library [41], and for the NIZKs and commitments we used ZKPDL (Zero-Knowledge Proof Description Language) [35], which itself uses the GNU multi-precision library [23] for modular arithmetic.
Table 1 shows the time taken for each of the unit operations performed within the IBE scheme.
As mentioned in Section 4, in the context of our system the creation of the parameters will be performed when the OBU is initialized, the encryption will be performed during the Pay protocol (line 4 of Algorithm 4.1), and both blind extraction and decryption will be performed in the audit phase between the TC and the OBU (lines 6 and 9 of Algorithm 4.3 respectively).
We consider the computational costs for the OBU and the TC separately, as well as the communication overhead for the whole system.
5 OBU computational costs.
During the course of a month (or however long an audit period is), the OBU is required to spend time performing computations for two distinct phases of the Milo protocol.
The first phase is the Pay protocol, which consists of computing the commitments to segment prices, encrypting the openings of the commitments, and producing a zero-knowledge proof that the value in the commitment lies in the right range.
From Table 1, we know that encryption takes roughly a second when encrypting 1024-bit number on the ARM.
As these correspond to "medium security" in PrETP [4, Table 2], and our commitments and zero-knowledge proofs are essentially identical to theirs, we can use the relevant timings from PrETP to see that the total time taken for the Pay protocol should be at most 20 seconds per segment.
As long as the time steps are at least 20 seconds and the segment lengths are at least half a mile (assuming people drive at most 90 miles per hour), the calculations can therefore be done in real time.The second phase of computation is the end of the month audit protocol.
Here, the OBU is responsible for acting as the IBE authority to answer blind extraction queries from the TC.
As we can see in Table 1, each query takes the OBU approximately 175 milliseconds, independent of the number of segments.
If the TC makes a small, fixed number of queries, say ten, for each vehicle, then the OBU will spend only a few seconds in the Audit protocol each month.TC computational costs.
In the course of the Audit protocol, the TC has to perform a number of complex calculations.
In particular, the cost of challenging the OBU for each camera is proportional to the number of segments the OBU reported driving.To obtain our performance numbers for the audit protocol, we considered the driving habits of an average American, both in terms of time spent and distance driven.
For time, we assumed that an average user would have a commute of 30 minutes each way, meaning one hour each day, in addition to driving between two and three hours each weekend.
For distance, we assumed that an average user would drive around 1,000 miles each month.
While we realize that these averages will vary greatly between locations (for example, between a city and a rural area), we believe that these measures still give us a relatively realistic setting in which to consider our system.
Table 2 gives the time it takes for the TC to challenge the OBU on a single segment for several segment lengths and time steps; we can see that the time taken grows approximately linearly with the number of segments.
To determine the number of segments, we considered they are essentially the same as they were within PrETP; the numbers they provide should therefore provide a reasonably accurate estimate for the cost of the TSP within our system as well.both fine-grained and coarse-grained approaches.
For the fine-grained approach, we considered a time step of one minute.
Using our assumptions about driving habits, this means that in a 30-day month with 22 weekdays, our average user will drive approximately 1,320 segments.
Adding on an extra 680 segments for weekends, we can see that a user might accumulate up to 2,000 segments in a month.
In the way that road prices are currently decided, however, a time step of one minute seems overly short, as typically there are only two times considered throughout the day: peak and off-peak.
We therefore considered next a time step of one hour, keeping our segment length at 1 mile.
Here the number of miles driven determines the number of segments rather than the minutes spent in the car, and so we end up with approximately 1,000 segments for the month.
Finally, we considered a segment length of 2 miles, keeping our time step at one hour; we can see that this results in approximately half as many segments as before, around 500 segments.
Longer average physical segment lengths would result in an even lower number of segments (and therefore better performance).
Communication overhead.
Looking at Table 3, we can see that the size of a payment message is approximately 6kB per segment; furthermore, this size is dominated by the NIZK (recall that each segment requires a commitment, a NIZK, and a ciphertext), which accounts for over 90% of the total size.
For our parameter choices in Table 2, this would result in a total payment size of approximately 11MB in the worst case (with 2000 segments) and 3MB in the best case (with 500 segments).
In PrETP, on the other hand, the authors claim to have sizes of only 1.5kB per segment [4, Section 4.3].
Using their more compact segments with our ciphertexts added on would therefore result in a segment size of only 2kB, which means the worst-case size of the entire payment message would be under 4MB (and the best-case size approximately 1MB).
Finally, we can see that the overhead for the rest of the Audit protocol is quite small: each blind IBE key sent from the OBU to the TC is only 494 bytes; if the TC makes ten queries per audit, then the total data transferred in the course of the protocol is about 5kB.
If we continue to assume that the TC always queries the user on ten cameras, then the entire auditing process will take less than 10 minutes per user in the worst case (when there are 2,000 segments) and less than 2 minutes in the best case (when there are 500 segments).
If we consider pricing computational resources as according to Amazon EC2 [3], then to approximately match the computer used for our benchmarks would cost about 10 cents per hour.
Between 6 and 30 users can be audited within an hour, so Length Time step Segments Time for TC (s) 1 mile 1 minute 2000 55.68 1 mile 1 hour 1000 33.51 2 miles 1 hour 500 10.45 Table 2: The average time, in seconds and over a run of 10, for the TC to perform a single spot check given segment lengths and time steps; we consider only the active time spent and not the time waiting for the OBU.
Essentially all of the time was spent iterating over the segments; as such, the time taken grows approximately linearly with the number of segments.
To determine the approximate number of segments given segment lengths and time steps, we assumed that an average user would drive for 1,000 miles in a 30-day month, or about 33 hours (1 hour each weekday and an extra 11 hours over four weekends).
Size ( : Size of each of the components that needs to be sent between the OBU and the TC, in bytes.
Each segment of the payment consists of a NIZK, commitment, and ciphertext; all the segments are forwarded to the TC from the TSP at the start of an audit.
In the course of the Audit protocol the OBU must also send blind IBE keys to the TC.each user ends up costing the system between one-third of a cent and 2 cents each month; this is an amount that the TSP could easily charge the users if need be (although the cost would presumably be cheaper if the TC simply performed the computations itself).
We therefore believe that the amount of computation required to perform the audits, in addition to being necessary in guaranteeing fairness and honesty within the system, is reasonably practical.
Finally, to examine how much Milo would cost if deployed in a real population we consider the county of San Diego, which consists of 3 million people possessing approximately 1.8 million vehicles, and almost 2,800 miles of roads [16,17,44].
As we just saw, Milo has a computational cost of up to 2 cents per user per month, which means a worst-case expected annual cost of $432,000; in the best case, wherein users cost only one-third of a cent per month, the expected annual cost is only $72,000.
In the next section, we can see how these costs compares to that of the "naïve" solution to collusion protection; i.e., one in which we attempt to protect against driver collusion through placement of cameras as opposed to prevention and protection at the system level.
Previously proposed tolling systems did not take collusion into account, as they allow the auditing authority to transmit camera locations in the clear to drivers.
Given these locations, colluding drivers can then share their audit transcripts each month in order to learn a greater number of camera locations than they would have learned alone.
Furthermore, websites already exist which record locations of red light cameras [38] and speed cameras [37]; one can easily imagine websites similar to these that collect crowd-based reports of audit camera locations.
With cameras whose locations are fixed from month to month, the cost to cheat is therefore essentially zero (just check the website!)
and so we can and should expect enterprising drivers to take advantage of the system.
In contrast, Milo is specifically designed to prevent these sorts of trivial collusion attacks.In addition to learning camera locations through the course of the audit phase, drivers may also learn camera locations from simply seeing them on the road.
This is also quite damaging to the system, as drivers can learn the locations of cameras simply by spotting them.
After pooling together the various locations and times at which they saw cameras, cheating drivers can fix up their driving record in time to pass any end-of-month audit protocol.To prevent such cheating, a system could instead require the OBU to transmit the tuples corresponding to segments as they are driven, rather than all together at the end of the month.
Without an anonymizing service such as Tor (used in VPriv [39]), transmitting data while driving represents too great a privacy loss, as the TSP can easily determine when and for how long each driver is using their car.
One possible fix might seem to be to continually transmit dummy segments while the car is not in use; transmitting segments in real time over a cellular network, however, leaks coarse-grained real-time location information to nearby cell towers (for example, staying connected to a single tower for many hours suggests that you are stationary), thus defeating the main goal of preserving driver privacy.Finally, we note that there exists a class of expensive physical attacks targeting any real-world implementation of a camera-based audit protocol.
For example, against fixed-location cameras, cheating drivers could disable their OBU for specific segments each month, revealing information about those segments.
Against mobile cameras, a driver could follow each audit vehicle and record its path, sharing with other cheating drivers as they go.
One can imagine defenses against these attacks and even more fanciful attacks in response; these sort of attacks quickly become very expensive and impractical, however, and provide tell-tale signs of collusion (e.g., repeated cheating, suspicious vehicles).
We therefore do not provide a system-level defense against them.
With Milo, we have modified the PrETP system to avoid leaking the locations of cameras as part of the audit protocol.
An alternative approach is to leave PrETP (or one of the other previously proposed solutions) in place and increase the number of audit cameras and their mobility, thus reducing the useful information leaked in audits even when drivers collude.
Whereas deploying Milo would increase computational costs over PrETP, deploying the second solution would increase the operational costs associated with running the mobile audit cameras.
In this section, we compare the costs associated with the two solutions.
Even with intentionally conservative estimates for the operating costs of mobile audit cameras, Milo appears to be competitive for reasonable parameter settings; as Moore's law makes computation less expensive, Milo will become more attractive still.Hardening previous tolling systems against trivial driver collusion is possible if we consider using continuously moving, invisible cameras.
Intuitively, if cameras move randomly, then knowing the position and time at which one audit camera was seen does not allow other cheating drivers to predict any future camera locations.
The easiest way to achieve these random spot checks is to mount cameras on special-purpose vehicles, which then perform a random walk across all streets in the audit area.
Even this will not generate truly random checks (as cars must travel linearly through streets and obey traffic laws); for ease of analysis we assume it does.
Furthermore, we will make the assumptions that the audit vehicles will never check the same segment simultaneously, operate 24 hours a day (every day), and are indistinguishable from other cars; tolling segments are 1 mile; and non-audit vehicles drive all road segments with equal probability.
These assumptions are by no means realistic, but they present a stronger case for moving cameras and so we use them, keeping in mind that any more realistic deployment will have higher cost.Using a probability analysis similar to that of VPriv [39, Section 8.4], we consider an area with M miles of road and C audit vehicles.
If both audit vehicles and other drivers are driving all roads uniformly at random, then a driver will share a segment with an audit vehicle with probability p = C M with each mile driven.
If the driver travels m miles in a tolling period, she will be seen at least once by an audit vehicle with a probability of Figure 2: A cost comparison of using the Milo system against using mobile cameras within previously proposed systems.
We know, from Section 5.1, that Milo has a worst-case computational cost of $432,000 per year and a best case of $72,000; for the other systems, we ignore computation completely (i.e., we assume it is free).
Even with the minimal costs we have assigned to operating a fleet of audit vehicles 24 hours a day and assuming worst-case computational costs, Milo becomes equally cheap when the probability of catching cheating drivers is 83%, and becomes significantly cheaper as the probability approaches 100%.
For Milo's best-case cost, it becomes cheaper as soon as more than one camera is used.1 − (1 − p) m = 1 − M −C M m .
(To determine the overall cost of this type of operation, we return to San Diego County (discussed already in Section 5.1); recall that it consists of 1.8 million vehicles driving on 2,800 miles of road, in which the average distance driven by one vehicle is 1, 000 miles in a month.
Using Equation 1, with one audit vehicle (C = 1), the probability that a driver gets caught is 1 − (2799/2800) 1000 ≈ .3, so that a potentially cheating driver still has a 70% chance of completely avoiding any audit vehicles for a month.
If we use two audit vehicles, then this number drops to 49%.
Continuing in this vein, we need 13 audit vehicles to guarantee a 99% chance of catching drivers who intentionally omit segments.
Achieving these results requires the TC to employ drivers 24 hours a day, as well as purchase, maintain, and operate a fleet of audit vehicles.
To consider the cost of doing so, we estimate the depreciation, maintenance, and operation cost of a single vehicle to be approximately $12,500 a year [45].
Furthermore, California has a minimum wage of $8.00/hr; paying this to operate a single vehicle results in minimum annual salary costs of $70,080, ignoring all overtime pay and benefits.
Each audit vehicle will therefore cost at least $82,500 per year (ignoring a number of additional overhead costs).
Finally, we compare the cost of operating these mobile cameras with the cost of the Milo system.
Because Milo leaks no information about camera locations to drivers, cameras can in fact stay at fixed locations; as long they are virtually invisible, drivers have no opportunities to learn their locations and so there is no need to move them continuously.
We therefore consider placing invisible cameras at random fixed locations, and can calculate the probability of drivers being caught by Milo using Equation 1, where we now use C to represent the number of cameras (and continue to assume that drivers drive 1,000 miles at random each month).
Figure 2 compares the cost of Milo with fixed cameras and the cost of previous systems with mobile cameras as the probability of detecting cheating increases.
We used a per-camera annual cost of $10,000.
6 As we can see, in the worst case, Milo achieves cost parity with mobile cameras at a detection probability of 83% and becomes vastly cheaper as the systems approach complete coverage, while in the best case it achieves cost parity as soon as more than a single camera is used (which gives a detection probability of around 30%).
With either of these numbers, we remember that our assumptions about the cost of operating these vehicles significantly underrated the actual cost; substituting in more realistic numbers would thus cause Milo to compare even more favorably.
In addition, future developments in computing technology are almost guaranteed to drive down the cost of computation, while fuel and personnel costs are not likely to decrease, let alone as quickly.
Therefore, we believe that Milo is and will continue to be an effective (and ultimately cost effective) solution to protect against driver collusion.
The study of privacy-preserving traffic enforcement and toll collection was initiated in papers by Blumberg, Keeler, and shelat [8] and Blumberg and Chase [7].
The former of these papers gave a system for traffic enforcement (such as red-light violations) and uses a private set-intersection protocol at its core; the latter gave a system for tolling and road pricing, and uses general secure function evaluation.
Neither system keeps the location of enforcement or spot-check devices secret from drivers.
In an important additional contribution, these papers formalized the "implicit privacy" that drivers currently enjoy: The police could tail particular cars to observe their whereabouts, but it would be impractical to apply such surveillance to more than a small fraction of all drivers.
7 6 This number was loosely choosen based upon purchase costs for red light violation cameras.
Note that the choice does not affect the differential system cost, as both systems must operate the same number of cameras to achieve a given probability of success.
7 We would like to correct one misconception, lest it influence future researchers.
Blumberg, Keeler, and shelat write, "the standards of suspicion necessary to stop and search a vehicle are much more lax than those required to enter and search a private residence."
In the U.S., the same standard -probable cause -governs searches of both vehicles and residences; the difference is only that a warrant is not required before the search of a car, as "it is not practicable to secure a warrant because the vehicle can be quickly moved out of the locality or jurisdiction in which the warrant must be sought" (Carroll v. United Another approach to privacy-preserving road pricing was given by Troncoso et al. [43], who proposed trusted tamper-resistant hardware in each car that calculates the required payment, and whose behavior can be audited by the car's owner.
The Troncoso et al. paper also includes a useful survey of pay-as-you-drive systems deployed at the time of its publication.
See Balasch, Verbauwhede, and Preneel [5] for a prototype implementation of the Troncoso et al. approach.De Jonge and Jacobs [19] proposed a privacypreserving tolling system in which drivers commit to the path they drove without revealing the individual road segments.
De Jonge and Jacobs' system uses hash functions for commitments, making it very efficient.
Only additive road pricing functions are allowed (i.e., ones for which the cost of driving along a path is the sum of the cost of driving along each segment of the path); this makes possible a protocol for verifying that the total fee was correctly computed as the sum of each road segment price by revealing, essentially, a path from the root to a single leaf in a Merkle hash tree.
(This constitutes a small information leak.)
In addition, de Jonge and Jacobs use spot checks to verify that the driver faithfully reported each road segment on which she drove.More recently, Popa, Balakrishnan, and Blumberg proposed the VPriv [39] privacy-preserving toll collection system.
VPriv takes advantage of the additive pricing functions it supports to enable the use of homomorphic commitments whereby the drivers commit to the prices for each segment of their path as well as the sum of the prices.
Then, the product of the commitments is a commitment of the sum of the prices.
This eliminates the need for a protocol to verify that the sum of segment prices was computed correctly.
Like previous systems, VPriv uses (camera) spot checks to ensure that drivers faithfully reveal the segments they drove.
The downside to VPriv is that, for the audit protocol, drivers must upload the road segments they drove to the server; to avoid linking these to their IP address, they must use an anonymizing network such as Tor.
Balasch et al. proposed PrETP [4] to address some of the shortcomings in VPriv.
In PrETP, drivers do not reveal the road segments they drove in the clear, and so do not need an anonymizing network.
Instead, they commit to the segments and, using a homomorphic commitment scheme, to the corresponding fees; in the audit protocol, they open the commitments corresponding to the road segments on which spot-check cameras observed them.In each of the system of de Jonge and Jacobs, VPriv, and PrETP, drivers are challenged in the audit protocol to prove that they committed to or otherwise uploaded the segments for which there is photographic evidence States, 267 U.S. 132 (1925), at 153).
that they were present.
As discussed in Sections 1 and 2, this revealing of camera locations enables several attacks which allow drivers to pay less than their actual tolls.
Additionally, camera placement and tolling areas must be restricted to ensure driver privacy, for example, by using "virtual trip lines" [30].
In recent work, Hoepman and Huitema [29] observed that in both VPriv and PrETP the audit protocol allows the government to query cars about locations where there was no camera, a capability that could be misused, for example, to identify whistleblowers.
They propose a privacy-preserving tolling system in which vehicles can be spot-checked only where their presence was actually recorded, and in which overall driver privacy is guaranteed so long as the pricing provider and aggregation provider do not collaborate.
Like VPriv, Hoepman and Huitema's system requires road segments to be transmitted from the car to the authority over an anonymizing network.Besides tolling, there are other vehicular applications that require privacy guarantees; see, generally, Hubaux, CˇapkunCˇapkun, and Luo [31].
One important application is vehicle-to-vehicle ad hoc safety networks [14]; see Freudiger et al. [21] for one approach to location privacy in such networks.
Another important application is aggregate traffic data collection.
Hoh et al. [30] propose "virtual trip lines" that instruct cars to transmit their location information and are placed to minimize privacy implications; Rass et al. [40] give an alternative construction based on cryptographic pseudonym systems.Vehicle communication is one class of ubiquitous computing system.
Location privacy in ubiquitous computing generally is a large and important research area; see the recent survey by Krumm [33] for references.
In recent years, privacy-preserving toll collection has been proposed as a way to implement more fine-grained pricing systems without having to sacrifice the privacy of drivers using the roads.
In such systems drivers do not reveal their locations directly to the toll collection authorities; this means there needs to be a mechanism in place to guarantee that the drivers are still reporting their accumulated fees accurately and honestly.
Maintaining this balance between privacy and honesty in an efficient and practical way has proved to be a challenging problem; previous work, however, such as the VPriv and PrETP systems, has demonstrated that this problem is in fact tractable.
Both these systems employ modern cryptographic primitives to allow the driver to convince the collection authority of the accuracy of her payment without revealing any part of her driving history.
To go along with this collection mechanism, a series of random spot checks (i.e., the authority challenging the driver to prove that she paid for segments in which she was caught on camera) must be performed in order to maintain honesty and fairness throughout the system.In this paper, we have identified large-scale driver collusion as a realistic and damaging threat to the success of privacy-preserving toll collection systems.
To protect against these sorts of collusions, we have presented Milo, a system which achieves the same privacy properties as VPriv and PrETP, but strengthens the guarantee of driver honesty by avoiding revealing camera locations to drivers.
We have also implemented the new parts of our system to show that achieving this stronger security guarantee does not add an impractical burden to any party acting within the system.Finally, along more practical lines, we have considered a naïve approach to protecting against collusions and shown that, from both a cost and effectiveness consideration, it is ultimately less desirable and more cumbersome than Milo.The weaknesses we identify in previous systems are caused by the gap between the assumption made by the cryptographic protocols (that spot checks are unpredictable) and the real-world cameras used to implement them -cameras that are physical objects that can be identified and may be difficult to move.
If drivers are able to avoid some cameras, more of them will be required; if too many spot-check cameras are deployed, the records they generate will themselves degrade driver privacy.
We believe that it is important for work on privacy-preserving tolling to address this limitation by carefully considering how the spot checks it relies on will be implemented.
We gratefully acknowledge Matthew Green, our shepherd, as well as our anonymous reviewers for their valuable feedback.
This material is based on work supported by the National Science Foundation under Grant No.
CNS-0963702, and by the MURI program under AFOSR Grant No.
FA9550-08-1-0352; the first author was also supported in part by a fellowship from the Powell Foundation.
