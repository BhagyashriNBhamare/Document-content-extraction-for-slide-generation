Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks.
A major challenge for link-based clustering in such networks is its potential to generate many different results, carrying rather diverse semantic meanings.
In order to generate desired clustering, we propose to use meta-path, a path that connects object types via a sequence of relations , to control clustering with distinct semantics.
Nevertheless, it is easier for a user to provide a few examples ("seeds") than a weighted combination of sophisticated meta-paths to specify her clustering preference.
Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks , where a user first provides a small set of object seeds for each cluster as guidance.
Then the system learns the weights for each meta-path that are consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths.
A probabilistic approach is proposed to solve the problem , and an effective and efficient iterative algorithm, PathSelClus, is proposed to learn the model, where the clustering quality and the meta-path weights are mutually enhancing each other.
Our experiments with several clustering tasks in two real networks demonstrate the power of the algorithm in comparison with the baselines.
With the advent of massive social and information networks, link-based clustering of objects in networks becomes increasingly important since it may help discover hidden knowledge in large networks.
Link-based clustering groups objects based on their links instead of attribute values.
This is especially useful when attributes of objects cannot be fully obtained.
Most existing link-based clustering algorithms are on homogeneous networks where links carry the same semantic meaning and only differ in their strengths (i.e., weights).
However, most real-world networks are heterogeneous, where objects are of multiple types and are linked via different types of relations or sequences of relations, forming a set of meta-paths [21].
These meta-paths imply diverse semantics, and thus clustering on different meta-paths will generate rather different results, as shown below.
Example 1.1.
(Meta-path-based clustering) A toy heterogeneous information network is shown in Figure 1, which contains three types of objects: organization (O), author (A) and venue (V), and two types of links: solid line represents the affiliation relation between author and organization, whereas the dashed one the publication relation between author and venue.
Authors are then connected (indirectly) via different meta-paths.
For example, A − O − A is a meta-path denoting a relation between authors via organizations (i.e., colleagues), whereas A − V − A denotes a relation between authors via venues (i.e., publishing in the same venues).
A question then raises: which type of connections should we use to cluster the authors?Obviously, there is no unique answer to this question: Different meta-paths lead to different author connection graphs, which may lead to different clustering results.
In Figure 2 This toy example shows that all the three clusterings look reasonable but they carry diverse semantics.
It should be a user's responsibility to choose her desired meta-path(s).
However, it is often difficult to ask her to explicitly specify one or a weighted combination of meta-paths.
Instead, it is easier for her to give some guidance in other forms, such as giving one or a couple of examples for each cluster.
For example, it may not be hard to give a few known conferences in each cluster (i.e., field) if one wants to cluster them into K research areas (for a user-desired K), or ask a user to name a few restaurants if one wants to cluster them into different categories in a business review website (e.g., Yelp).
On the other hand, since we are dealing with heterogeneous networks, the previous work on user-guided clustering or semisupervised learning approaches on (homogeneous) graphs [11,30,31] cannot apply.
We need to explore meta-paths that represent heterogeneous connections across objects, leading to rich semantic meanings, hence diverse clustering results.
With user guidance, a system will be able to learn the most appropriate meta-paths or their weighted combinations.
The learned meta-paths will in turn provide an insightful view to help understand the underneath mechanism for the formation of a specific type of clustering.
For example, which meta-path is more important to determine a restaurant's category?
-the meta-path connecting them via customers, or the one connecting them via text in reviews, or the kNN relation determined by their locations?In this paper, we integrate the meta-path selection with the userguided clustering for better clustering a user-specified type of objects, i.e., the target objects, in a heterogeneous information network, where the user guidance is given as a small set of seeds in each cluster.
For example, to cluster authors into 2 clusters in Example 1.1, a user may seed {1} and {5} for two clusters, which implies a selection of meta-path A − O − A; or seed {1}, {2}, {5}, and {6} for four clusters, which implies a combination of both meta-paths A − O − A and A − V − A with about equal weights.
Our goal is to (1) determine the weight of each meta-path for a particular clustering task, which should be consistent with the clustering results implied by the limited user guidance, and (2) output the clustering result according to the user guidance and under the learned weights for each meta-path.
We propose a probabilistic model that models the hidden clusters for target objects, the user guidance, and the quality weights for different meta-paths in a unified framework.
An effective and efficient iterative algorithm PathSelClus is developed to learn the model, where the clustering quality and the meta-paths quality mutually enhance each other.
The experiments with different tasks on two real networks show our algorithm outperforms the baselines.
Our contributions are summarized as follows: 1.
We propose to integrate meta-path selection with user-guided clustering for arbitrary heterogeneous networks, and study a specific form of guidance: seeding some objects in each cluster; 2.
A probabilistic model is proposed to put hidden clusters, user guidance, and the quality of meta-paths into one unified framework, and an iterative algorithm is developed where the clustering result and weights for each meta-path are learned alternatively and mutually enhance each other; and 3.
Experiments on real heterogeneous information networks have shown the effectiveness and efficiency of our algorithm over baselines, and the learned weights of meta-paths provide knowledge for better understanding of the cluster formation.
In this section, we introduce preliminary concepts in heterogeneous information networks and define the problem of integrating meta-path selection with user-guided object clustering.
A heterogeneous information network [22] is an information network with multiple types of objects and/or multiple types of links.
Here we introduce two heterogeneous information networks that are used in the experiment section in this paper, which are the D-BLP network and the Yelp network.
Example 2.1.
(The DBLP bibliographic network 1 ) DBLP is a typical heterogeneous information network (see schema in Figure 3(a)), which contains 4 types of objects, namely paper(P), author (A), term (T), and venue (V) including conferences and journals.
Links exist between authors and papers by the relation of "write" and "written by", between papers and terms by "mention" and "mentioned by", and between venues and papers by "publish" and "published by".
"Citation" relation between papers can be added further using other data source, such as Google scholar.
Yelp is a website where users can write reviews for businesses.
The Yelp network (see schema in Figure 3(b)) used in this paper contains 4 types of objects, namely business (B), user (U), term (T), and review (R).
Links exist between users and reviews by the relation of "write" and "written by", between reviews and terms by "mention" and " mentioned by", between businesses and reviews by "commented by" and "comment", and between users by "friendship" (not included in our dataset).
Following the work [21], we use the concept of meta-path to describe the possible relations that can be derived from a heterogeneous network between two types of objects in a meta level.
Metapath is defined by a sequence of relations in the network schema, and can be described by a sequence of object types when there is no ambiguity.
For example, A − P − A is a meta-path denoting the co-authorship between authors, and A − P − V is a meta-path denoting the publication relation between the author and the venue type.
Note that, a single relation defined in the network schema can be viewed as a special case of meta-path, e.g., the citation relation P → P .
Link-based clustering is to cluster objects based on their connections to other objects in the network.
In a heterogeneous information network, we need to specify more information for a meaningful clustering.First, we need to specify the type of objects we want to cluster, which is called the target type.
Second, we need to specify which type of connection, i.e., meta-path, to use for the clustering task, where we call the object type that the target type is connecting to via the meta-path as the feature type.
For example, when clustering authors based on the venues they have published papers in, the target type is the author type, the meta-path to use is A − P − V , and the feature type is the venue type.In a heterogeneous information network, target objects could link to many types of feature objects by multiple meta-paths.
For example, authors could connect to other authors by meta-path A − P − A, or connect to terms by meta-path A − P − T .
The meta-path selection problem is then to determine which meta-paths or their weighted combination to use for a specific clustering task.
User guidance is critical for clustering objects in the network.
In this study, we consider the guidance in the form of object seeds in each cluster given by users.
For example, to cluster authors based on their (hidden) research areas, one can first provide several representative authors in each area.
On one hand, these seeds are used as guidance for clustering all the target objects in the network.
On the other hand, they provide information for selecting the most relevant meta-paths for the specific clustering task.
Note that in practice, a user may not be able to provide seeds for every cluster, but only for some clusters they are most familiar with, which should be handled by the algorithm too.
In all, given a heterogeneous information network G, a user needs to specify the following as inputs for a clustering task:1.
The target type for clustering, type T .
2.
The number of clusters, K, and the object seeds for each cluster, say L1, . . . , LK, where L k denotes the object seeds for cluster k, which could be an empty set.
These seeds will be used as the hints to learn the purpose/preference of the clustering task.
3.
A set of M meta-paths starting from type T , denoted as P1, P2, . . . , PM , which might be helpful for the clustering task.
These meta-paths can be determined either according to users' expert knowledge, or by traversing the network schema starting from type T with a length constraint.For each meta-path Pm, we calculate the adjacency matrix Wm, which we call relation matrix, between the target type T and the feature type Fm, by multiplying adjacency matrices for each relation along the meta-path.
For example, the relation matrix W for meta-path A − P − V , denoting the number of papers published by an author in a venue, is calculated by W = WAP × WPV , where WAP and WPV are the adjacency matrices for relation A − P and P − V respectively.The output of the algorithm includes two parts: (1) to determine the weight αm ≥ 0 of each meta-path Pm for a particular clustering task, which should be consistent with the clustering result implied by the limited user guidance, and (2) to output the clustering result according to the user guidance and under the learned weights for each meta-path, that is, to associate each target object ti in T with a K-dimensional soft clustering probability vector, θi = (θi1, . . . , θiK), where θ ik is the probability of ti belonging to cluster k, i.e., θ ik ≥ 0 and 񮽙 K k=1 θ ik = 1.
In this section, we propose a probabilistic approach to model the problem into a unified framework.A good clustering result is determined by several factors: first, the clustering result should be consistent with the link structure; second, the clustering result should be consistent with the user guidance; third, the importance of each meta-path is implied by the user-guided clustering, which should be modeled and learned to further enhance the clustering quality.
In the following, we first introduce the modeling for the three aspects respectively, and then propose a unified model that takes consideration of all of them.
To model the consistency between a clustering result and a relation matrix, we propose a clustering-based generative model for relationship generation.For a meta-path Pm, let its corresponding relation matrix between the target type T and the feature type Fm be Wm. For each target object ti, we model its relationships as generated from a mixture of multinomial distributions, where the probability of ti ∈ T connecting to fj,m ∈ Fm is conditionally independent on ti given the hidden cluster label of the relationship is known.
Let πij,m = P (j|i, m) be the generative probability of the relationship starting from ti and ending at fj,m, where񮽙 j πij,m = 1, then π ij,m = P (j|i, m) = 񮽙 k P (k|i)P (j|k, m) = 񮽙 k θ ik β kj,m(1)where θ ik = P (k|i) denotes the probability of ti belonging to cluster k and β kj,m = P (j|k, m) denotes the probability of fj,m appearing in cluster k.
In other words, let πi,m = (πi1,m, . . . , π i|Fm|,m ) be the generative probability vector for target object ti, then each πi,m can be factorized as a weighted summation of ranking distributions of feature objects in each cluster.
The factorization idea is similar to that of PLSA [10], PHITS [7], and RankClus [22], but is built on meta-path-encoded relationships rather than immediate links.
This extension will capture more and richer link-based features for clustering target objects in heterogeneous networks.
By assuming each target object ti is independent with each other and each relationship generated by ti is independent with each other, the probability of observing all the relationships between all the target objects and feature objects is the production of the probability of all the relationships following meta-path Pm:P (Wm|Πm, Θ, Bm) = 񮽙 i P (w i,m |π i,m , Θ, Bm) = 񮽙 i 񮽙 j (π ij,m ) w ij,m(2)where Πm = ΘBm is the probability matrix with cells as πij,m's, Θ is the parameter matrix for θ ik 's, Bm is the parameter matrix for β kj,m 's.
and wij,m is the weight of the relationship between ti and fj,m. Note that, each meta-path Pm corresponds to a different generative probability matrix Πm to model the relationship generation.
The factorization of these probability matrices share the same soft clustering probabilities Θ, but different ranking distributions Bm in different meta-paths.
Further, we take the user guidance in the form of object seeds for some clusters as the prior knowledge for the clustering result Θ, by modeling the prior as a Dirichlet distribution rather than treating them as hard labeled ones.For each target object ti, its clustering probability vector θi is assumed to be a multinomial distribution, which is generated from some Dirichlet distribution.
If ti is labeled as a seed in cluster k * , θi is then modeled as being sampled from a Dirichlet distribution with parameter vector λe k * + 1, where e k * is a K-dimensional basis vector, with the k * th element as 1 and 0 elsewhere.
If ti is not a seed, θi is then assumed as being sampled from a uniform distribution, which can also be viewed as a Dirichlet distribution with parameter vector of 1.
The density of θi given such priors is:P (θ i |λ) ∝ 񮽙 񮽙 k θ 1 {t i ∈L k } λ ik = θ λ ik * , if t i is labeled and t i ∈ L k * , 1, if t i is not labeled.
(3)where1 {t i ∈L k } is an indicator function, which is 1 if ti ∈ L k holds, otherwise 0.
The hyper-parameter λ is a nonnegative value, which controls the strength of users' confidence over the object seeds in each cluster.
From Eq.
(3), we can find that:• when λ = 0, the prior for θi of a labeled target object becomes a uniform distribution, which means no guidance information will be used in the clustering process.
• when λ → ∞, the prior for θi of a labeled target object converges to a point mass, i.e.,P (θi = e k * ) → 1 or θi → e k * ,which means we will assign k * as the hard cluster label for ti.In general, a larger λ indicates a higher probability of that θi is around the point mass e k * , and thus a higher confidence for the user guidance.
Different meta-paths may lead to different clustering results, therefore it is desirable to learn the quality for each meta-path for the specific clustering task.
We propose to learn the quality weight for each meta-path by evaluating the consistency between its relation matrix and the user-guided clustering result.In deciding the clustering result for target objects, a meta-path may be of low quality for the following reasons:1.
The relation matrix derived by the meta-path does not contain an inherent cluster structure.
For example, target objects are connecting to the feature objects randomly.
2.
The relation matrix derived by the meta-path itself has a good inherent cluster structure, however, it is not consistent with the user guidance.
For example, in our motivating example, if the user gives a guidance as: K = 2, L1 = {1}, L2 = {2}, then the meta-path A − O − A should have a lower impact in the clustering process for authors.The general idea of measuring the quality of each meta-path is to see whether the relation matrix Wm is consistent with the detected hidden clusters Θ and thus the generative probability matrix Πm, which is a function of Θ, i.e., Πm = ΘBm.
In order to quantify the weight for such quality, we model the weight αm for meta-path Pm as the relative weight for each relationship between target objects and feature objects following Pm.In other words, we treat our observations of the relation matrix as αmWm rather than original Wm. A larger αm indicates a higher quality and a higher confidence of the observed relationships, and thus each relationship should count more.Then, we assume the multinomial distribution πi,m has a prior of Dirichlet distribution with parameter vector φ i .
In this paper, we consider a discrete uniform prior, which is a special case of Dirichlet distribution with parameters as an all-one vector, i.e., φ i,m = 1.
The value of αm is determined by the consistency between the observed relation matrix Wm and the generative probability matrix Πm.
The goal is to find the α * m that maximizes the posterior probability of πi,m for all the target objects ti, given the observation of relationships wi,m with relative weight αm:α * m = arg max αm 񮽙 i P (π i,m |αmw i,m , θ i , Bm)(4)The posterior of πi,m = θiBm is another Dirichlet distribution with the updated parameter vector as αmwi,m + 1, according to the multinomial-Dirichlet conjugate: πi,m|αmwi,m, θi, Bm ∼ Dir(αmwij,m+1, . . . , αmw i|Fm|,m +1) (5) which has the following density function:P (π i,m |αmw i,m , θ i , Bm) = Γ(αmn i,m + |Fm|) 񮽙 j Γ(αmw ij,m + 1) 񮽙 j (π ij,m ) αm w ij,m(6)where ni,m = 񮽙 j wij,m, the total number of path instances from ti following meta-path Pm.By modeling αm in such a way, the meaning of αm is quite clear: • αmwij,m + 1 is the parameter of jth dimension for the new Dirichlet distribution.
• The larger αm, the more likely it will generate a πi,m with a distribution as the observed relationship distribution, i.e., πi,m → w i,m /ni,m when αm → ∞, where ni,m is the total number of path instances from ti following meta-path Pm.
• The smaller αm, the more likely it will generate a πi with a uniform distribution (which means randomly), i.e., πi,m → (1/|Fm|, . . . , 1/|Fm|) when αm → 0, where |Fm| is the total size of feature objects in meta-path Pm.Note that, we do not consider negative αm's in this model, which means relationships with a negative impact in the clustering process are not considered, and the extreme case of αm = 0 means the relationships in a meta-path are totally irrelevant for the clustering process.
By putting all the three factors together, we have the joint probability of observing the relation matrices with relative weights αm's, and the parameter matrices Πm's and Θ:P ({αmWm} M m=1 , Π 1:M , Θ|B 1:M , Φ 1:M , λ) = 񮽙 i 񮽙 񮽙 m P (αmWm|Πm, θ i , Bm)P (Πm|Φm) 񮽙 P (θ i |λ)(7)where Φm is the Dirichlet prior parameter matrix for Πm, and an all-one matrix in our case.
We want to find the maximum a posteriori probability (MAP) estimate for Πm's and Θ, which maximizes the logarithm of posterior probability of {Πm} M m=1 , given the observations of relation matrices with relative weights {αmWm} M m=1 and Θ, plus a regularization term over θi for each target object denoting the logarithm of prior density of θi:J = 񮽙 i 񮽙 񮽙 m log P (π i,m |αmw i,m , θ i , Bm) + 񮽙 k 1 {t i ∈L k } λ log θ ik 񮽙(8)By substituting the posterior probability formula in Eq.
(6) and the factorization form for all πi,m, we get the final objective function:J = 񮽙 i 񮽙 񮽙 m 񮽙 񮽙 j αmw ij,m log 񮽙 k θ ik β kj,m + log Γ(αmn i,m + |Fm|) − 񮽙 j log Γ(αmw ij,m + 1) 񮽙 + 񮽙 k 1 {t i ∈L k } λ log θ ik 񮽙(9) In this section, we introduce the learning algorithm, PathSelClus, for the model (Eq.
(9)) proposed in Section 3.
It is a two-step iterative algorithm, where the clustering result Θ and the weights for each meta-path α mutually enhance each other.
In the first step, we fix the weight vector α, and learn the best clustering results Θ under this weight.
In the second step, we fix the clustering matrix Θ and learn the best weight vector α.
When α is fixed, the terms only involving α can be discarded in the objective function Eq.
(9), which is then reduced to:J 1 = 񮽙 m αm 񮽙 i 񮽙 j w ij,m log 񮽙 k θ ik β kj,m + 񮽙 i 񮽙 k 1 {t i ∈L k } λ log θ ik(10)The new objective function can be viewed as a weighted summation of the log-likelihood for each relation matrix under each meta-path, where the weight αm indicates the quality of each meta-path, plus a regularization term over Θ representing the user guidance.
Θ and the augmented parameter Bm's can be learned using the standard EM algorithm, as follows.
In each relation matrix, we use zij,m to denote the cluster label for each relationship between a target object ti and a feature object fj,m.
According to the generative process described in Section 3.1, P (zij,m = k) = θ ik , and fj,m is picked with probability β kj,m .
The conditional probability of the hidden cluster label given the old Θ t−1 and B t−1 m values is:p(z ij,m = k|Θ t−1 , B t−1 m ) ∝ θ t−1 ik β t−1 kj,m(11)M-step: We have the updating formulas for Θ t and B t m as:θ t ik ∝ 񮽙 m αm 񮽙 j w ij,m p(z ij,m = k|Θ t−1 , B t−1 m ) + 1 {t i ∈L k } λ (12) β t kj,m ∝ 񮽙 i 񮽙 j w ij,m p(z ij,m = k|Θ t−1 , B t−1 m ) (13)From Eq.
(12), we can see that the clustering membership vector θi for ti is determined by the cluster labels of all its relationships to feature objects, in all the relation matrices.
Besides, if ti is labeled as a seed object in some cluster k * , θi is also determined by the label.
The strength of impacts from these factors is determined by the weight of each meta-path αm, and the strength of the cluster labels λ, where αm's are learned automatically by our algorithm, and λ is given by users.
Once given a clustering result Θ and the augmented parameter Bm's, we can calculate the generative probability matrix Πm for each meta-path Pm by: Πm = ΘBm.
By discarding the irrelevant terms, the objective function of Eq.
(9) can be reduced to:J 2 = 񮽙 i 񮽙 񮽙 m 񮽙 񮽙 j αmw ij,m log π ij,m + log Γ(αmn i,m + |Fm|) − 񮽙 j log Γ(αmw ij,m + 1) 񮽙 񮽙(14)It is easy to check that J2 is a concave function, which means there is a unique α that maximizes J2.
We use gradient descent approach to solve the problem, which is an iterative algorithm with the updating formula as:α t m = α t−1 m + η t m ∂J 2 ∂αm 񮽙 񮽙 񮽙 αm =α t−1 m, where the partial derivative of αm can be derived as:∂J 2 ∂αm = 񮽙 i 񮽙 j w ij,m log π ij,m + 񮽙 i ψ(αmn im + |Fm|)n i,m − 񮽙 i 񮽙 j ψ(αmw ij,m + 1)w ij,mwhere ψ(x) is the digamma function, the first derivative of log Γ(x).
The step size η t m is usually set as a small enough number, to guarantee the increase of J2.
In this paper, we follow the trick used in non-negative matrix factorization (NMF) [12], and setη t m = α t−1 m − 񮽙 i 񮽙 j w ij,m log π ij,m. By using the above step size, we can get updating formula for αm as:α t m = α t−1 m 񮽙 i 񮽙 ψ(α t−1 m n im + |Fm|)n i,m − 񮽙 j ψ(α t−1 m w ij,m + 1)w ij,m 񮽙 − 񮽙 i 񮽙 j w ij,m log π ij,m(15)which guarantees to be a non-negative value.
Also, by looking at the denominator of the formula, we can see that a larger loglikelihood of observing relationships wij,m under model probability πij,m, which means a smaller denominator as log-likelihood is negative, generally leads to a larger αm.
This is also consistent with the human intuition.
The PathSelClus algorithm is then summarized in Algorithm 1.
Overall, it is an iterative algorithm that optimizes Θ and α alternatively.
The optimization of Θ contains an inner loop of EMalgorithm, and the optimization of α contains another inner loop of gradient descent algorithm.
We discuss some details of the algorithm implementation in the following.
The Weight Setting of Relation Matrices.
Given a heterogeneous information network G, we calculate the relation matrix Wm for each given meta-path Pm by multiplying adjacency matrices along the meta-path.
It can be shown that, scaling Wm by a factor of 1/cm leads to a scaling of the learned relative weight αm by a factor of cm.
Therefore, the performance of the clustering result will not be affected by the scaling of the relation matrix, which is a good property of our algorithm.
Initialization Issues.
For the initial value of α, we set it as an allone vector, which assumes all the meta-paths are equally important.
For the initial value of Θ in the clustering step given α, if ti is not labeled, we assign a random clustering vector to θi; while if ti is labeled as a seed for a cluster k * , we assign θi = e * k .
Time Complexity Analysis.
The PathSelClus algorithm is very efficient, as it is proportional to the number of relationships that are used in the clustering process, which is about linear to the number of target objects for short meta-paths in sparse networks.
Formally, for the inner EM algorithm that optimizes Θ, the time complexity is O(t1(K 񮽙 m |Em| + K|T | + K 񮽙 m |Fm|)) = Input: Network: G,O(t1(K 񮽙 m |Em|)),where |Em| is the number of non-empty relationships in relation matrix Wm, |T | and |Fm| are the numbers of target objects and feature objects in meta-path Pm, which are typically smaller than |Em|, and t1 is the number of iterations.
For the inner gradient descent algorithm, the time complexity is O(t2( 񮽙 m |Em|)), where t2 is the number of iterations.
The total time complexity for the whole algorithm is thenO(t(t1(K 񮽙 m |Em|) + t2( 񮽙 m |Em|))),where t is the number of outer iterations, which usually is a small number.
In this section, we will compare PathSelClus with several baselines, and show the effectiveness and efficiency of our algorithm.
In this paper, we use two real information networks for performance test, the DBLP network and the Yelp network.
For each network, we design multiple clustering tasks provided with different user guidance, which are introduced in the following.
1.
The DBLP Network.
For the DBLP network introduced in Example 2.1, we design three clustering tasks in the following.
• DBLP-T1: Cluster conferences in the "four-area dataset" [23], which contains 20 major conferences and all the related papers, authors and terms in DM, DB, IR, and ML fields, according to the research areas of the conferences.
• DBLP-T3: Cluster 165 authors who have been ever advised by Christos Faloutsos, Michael I. Jordan, Jiawei Han, and Dan Roth (including these professors), according to their research groups.
The candidate meta-paths are the same as in DBLP-T2.2.
The Yelp Network.
For the Yelp network introduced in Example 2.2, we are provided by Yelp a sub-network 3 , which include 6900 businesses, 152327 reviews, and 65888 users.
Hierarchical categories are provided for each business as well, such as "Restaurants", "Shopping" and so on.
For Yelp network, we design three clustering tasks in the following.
• Yelp-T1: We select 4 relatively big categories ("Health and Medical", "Food", "Shopping", and "Beauty and Spas"), and cluster 2224 businesses with more than one reviews according to two meta-paths: B −R−U −R−B and B −R−T −R−B.• Yelp-T2: We select 6 relatively big sub-categories under the first-level category "Restaurant" ("Sandwiches", "Thai", "American (New)", "Mexican", "Italian", and "Chinese"), and cluster 554 businesses with more than one reviews according to the same two meta-paths.
• Yelp-T3: We select 6 relatively big sub-categories under the first-level category "Shopping" ("Eyewear & Opticians", "Books, Mags, Music and Video", "Sporting Goods", "Fashion", "Drugstores", and "Home & Garden"), and cluster 484 businesses with more than one reviews according to the same two meta-paths.
First, we study the effectiveness of our algorithm under different tasks, and compare it with several baselines.
Three baselines are used in this paper.
Since none of them has considered the meta-path selection problem, we will use all the meta-paths as features and prepare them to fit the input of each of these algorithms.
The first one is user-guided information theoretic-based k-means clustering (ITC), which is an adaption of seeded k-means algorithm proposed in [4], by replacing Euclidean distance to KL-divergence as used in information theoretic-based clustering algorithms [8,2].
ITC is a hard clustering algorithm.
For the input, we concatenate all the relation matrices side-by-side into one single relation matrix, and thus we get a very high dimensional feature vector for each target object.The second baseline is the label propagation (LP) algorithm proposed in [31], which utilizes link structure to propagate labels to the rest of the network.
For the input, we add all the relation matrices together to get one single relation matrix.
As LP is designed for homogeneous networks, we confine our meta-paths to ones that start and end both in the target type.
LP is a soft clustering algorithm.The third baseline is the cluster ensemble algorithm proposed in [18], which can combine soft clustering results into a consensus, which we call ensemble_soft.
Different from the previous two baselines that directly combine meta-paths at the input level, cluster ensemble combines the clustering results for different meta-paths at the output level.
Besides, we also use majority voting as another baseline (ensemble_voting), which first maps each clustering result for each target object into a hard cluster label and then pick the cluster label that is the majority over different meta-paths.
As we can use either ITC or LP as the clustering algorithm for each ensemble method, we then get four ensemble baselines in total: ITC_soft, ITC_voting, LP_soft, and LP_voting.
Two evaluation methods are used to test the clustering result compared with the ground truth, where the soft clustering is mapped into hard cluster labels.The first measure is accuracy, which is used when seeds are available for every cluster and is calculated as the percentage of target objects going to the correct cluster.
Note that, in order to measure whether the seeds are indeed attracting objects to the right cluster, we do not map the outcome cluster labels to the given class labels.
The second measure is normalized mutual information (N-MI), which does not require the mapping relation between ground truth labels and the cluster labels obtained by the clustering algorithm.
The normalized mutual information of two partitions X and Y is calculated as:NMI(X, Y ) = I(X;Y ) √ H(X)H(Y ), where X and Y are vectors containing cluster labels for all the target objects.
Both measures are in the range of 0 to 1, and a higher value indicates a better clustering result in terms of the ground truth.
We first test the clustering accuracy when cluster seeds are given for every cluster.
In this case, all the three baselines can be used and compared.
Performances under different numbers of seeds in each cluster are tested.
Each result is the average of 10 runs.The accuracy for all the 6 tasks for two networks are summarized in Table 1 and Table 2 respectively.
From the results we can see that, PathSelClus performs the best in most of the tasks.
Even for the task such as DBLP-T3 where other methods give the best clustering result, PathSelClus still gives clustering results among the top.
This means, PathSelClus can give consistently good results across different tasks in different networks.
Also, by looking at the clustering accuracy trend along with the number of seeds used in each cluster, we can see that, more seeds generally leads to better clustering results.
We then test the clustering accuracy when cluster seeds are only available for some of the clusters.
We perform this study on DBLP-T3 using PathSelClus, which includes 4 clusters, and the results are shown in Fig. 4.
We can see that even if user guidance is only given to some clusters, those seeds can still be used to improve the clustering accuracy.
In general, the fewer number of clusters with seeds, the worse the clustering accuracy, which is consistent with the human intuition.
Note that, label propagation-based methods like LP cannot deal with partial cluster labels.
However, in reality it is quite common that users are only familiar with some of the clusters and are only able to give good seeds in those clusters.
That is another advantage of PathSelClus.
Now, we study the scalability of our algorithm using synthetic datasets, due to that we can manipulate the size of network flexibly.
In Fig. 5(a), we keep the size of target objects and the total number of relationships they issued as fixed, and vary the size of feature objects.
We can see that the average running time for one iteration of the inner EM algorithm is about linear to the size of the feature objects; and the average running time for one iteration of the inner gradient descent algorithm is almost constant, as it is only linear to the number of relationships in the network.
In Fig. 5(b), we keep the size of feature objects as fixed, and vary the number of target objects.
We keep the average relationships for each target object as constant.
From the result we can see that the average running time for one iteration of both the inner EM algorithm and the gradient descent algorithm is linear to the size of target objects, since the number of relationships is also increasing linearly with the size of target objects.
From the efficiency test, we can see that PathSelClus is very scalable and can be applied to large-scale networks.
In this section, we study the impact of the only parameter in the algorithm, λ, to the performance of our algorithm.
We select DBLP-T1 and Yelp-T2 as the test tasks.
From the results in Fig. 6, we can see that the clustering results is in general not sensitive to the value of λ, as long as it is a positive value.
In practice, we set it as 100 for our experiments.
Notice that in Fig. 6, we do not show the accuracy value when λ = 0, as when there is no guidance from users, the accuracy cannot be correctly defined.
One of the major contributions of PathSelClus is that it can select the right meta-paths for a user-guided clustering task.
We now show the learned weights of meta-paths for some of the tasks.In DBLP-T1 task, the total weight αm for meta-path V − P − A − P − V is 1576, and the average weight per relationship (a concrete path instance following the meta-path) is 0.0017.
The total weight for meta-path V − P − T − P − V is 17001, while the average weight per relationship is 0.0003.
This means that generally the relationships between two conferences that are connected by an author are more trustable than the ones that are connected by a term, which is consistent with human intuition since many terms can be used in different research areas and authors are typically more focused on confined research topics.
However, as there are much more relationships following V − P − T − P − V than following V − P − A − P − V , the former overall provide more information for clustering.In the Yelp network, similar to DBLP-T1 task, in terms of the average weight for each relationship, meta-path B − R − U − R − B is with higher weight than B − R − T − R − B; while in terms of total weight, meta-path B − R − T − R − B is with higher weight.
An interesting phenomenon is that, for Yelp-T2 task, which tries to cluster restaurants into different categories, the average weight for relationships following B − R − U − R − B is 0.1716, much lower than the value (0.5864) for Yelp-T3 task, which tries to cluster shopping businesses into finer categories.
This simply says that most users actually will try all different kinds of food, therefore they will not be served as a good connection between restaurants as they are in other categories.
In this section, we briefly discuss some interesting issues.
1.
The Strength of Meta-Path Selection.
Different meta-paths in heterogeneous networks could be viewed as different sources of information for defining link-based similarity between objects.
There are several ways to handle different meta-paths: (1) to combine them at relation matrix level, such as in baselines ITC and LP; (2) to combine the clustering results at the output level, such as in ensemble baselines; (3) to learn and improve the quality weights for each meta-path iteratively, such as in PathSelClus.
Only the third approach is able to select different meta-paths according to different clustering tasks, while the other two can only output an "average" clustering result using all the information.
It turns out that, in most cases, the third approach is more flexible to combine information from different sources, and its advantage has been shown in the experiment section.
2.
Meta-Paths vs. Path Instances.
In this paper, we only consider the different semantics encoded by different meta-paths.
In practice, different concrete paths (path instances) between two objects may also differ from each other, e.g., two objects may be linked via a "bridge" or via a "hub", indicating different meanings.
The difference between the two concepts, i.e., meta-path and path instance, is similar to the difference between a source of features and a concrete feature in a vector space.
Due to limited scope, this paper only discusses the selection of meta-paths.
It is possible to select path instance at the object level, and the concrete method is left for future research.
Recently, there are many clustering algorithms proposed for networks, such as spectral clustering-based methods [19,15], linkbased probabilistic models [7,1], modularity function-based algorithms [17,16], and density-based algorithms [26,25] on homogenous networks; and ranking-based algorithms [22,23], nonnegative matrix factorization [12,24], spectral clustering-based methods [13], and probabilistic approaches [14] on heterogeneous networks.
However, while all these clustering methods use the information given in the networks, none considers that different users may have different purposes for clustering, nor do they ask users to help select different information for link-based clustering.
In this paper, we show that different types of relationships encoded by meta-paths have different semantic meanings in determining the similarity between target objects, and the selection of these metapaths should be done with user guidance in order to derive userdesired clustering results.There are several lines of research on how to add user guidance to derive good clustering results, consistent with users' demand in vector space or networked data.Clustering with constraints.
In [4,5,11], clustering algorithms that consider constraints either in the form of seeds in each cluster or pairwise constraints as must-link or cannot-link are proposed.
A probabilistic model with an HMRF (hidden Markov random field) as a hidden layer that models the must-link and cannot-link between objects is proposed to solve the problem [5].
This approach can also be extended to graph data with the use of kernels instead of vector-based features [11].
However, these methods assume there is one trustable information source to either define the feature of each object or define the network structure between objects.
The goal is to output the clustering result that is consistent with both the similarity defined by the data as well as the user guidance.
In this paper, we dig further and study which type of information source encoded with meta-paths is more trustable in a heterogeneous network.Semi-supervised learning on graphs.
In [30,31], algorithms that propagate labels for a small portion of objects into the rest of the network are proposed, which are based on harmonic functions defined between objects using the network structure.
Again, this kind of methods totally trust the given network and determine the best labels of the rest of the nodes according to the cost function defined on the network.Semi-supervised metric learning.
In [6,3], algorithms that learn the best distance metric functions according to the constrains for the clustering task are proposed.
This line of problem is closer to the meta-path selection problem, but still differs significantly.
First, they study features of objects in vector space instead of network; second, the metric functions should be given in an explicit format, which is very difficult to determine in a network scenario.
In this paper, we are not finding an explicit metric function that determines the similarity between any two target objects, instead, we model and learn the quality weight for each meta-path in the clustering process, which can be viewed as an implicit way to determine the similarity between two target objects.User-guided clustering in relational data.
CrossClus [28] deals with another type of guidance from users: the attribute set of the target object type.
The algorithm extracts a set of highly relevant attributes in multiple relations connected via linkages defined in the database schema, and then use the whole attribute set as the feature set to apply traditional vector space-based clustering algorithm.
CrossClus works for relational data with complete attributes, but not for purely link-based clustering.Cluster ensemble [20,18] is a method that combines clustering results of different methods or different datasets to a single consensus.
Most of these cluster ensemble methods try to find a mean partition given different partitions of target objects.
However, in reality these clusterings may conflict with each other, representing different purposes of clustering tasks, and a consensus does not necessarily lead to a clustering desired by users.
In this study, we do not combine clustering results at the output level, but use intermediate clustering results as feedback to adjust the weight of each meta-path, and thus the clustering results and the quality weight for each meta-path can mutually enhance each other.Our work also differs from traditional feature selection [9] and recently emerged semi-supervised feature selection [29,27], which focus on vector space features, and do not have an immediate extension of solutions to our problem.
For our meta-path selection problem, each meta-path provides a source of features instead of a concrete feature, and we have shown that simple combinations of features from different sources may lead to no good solution.
Link-based clustering for objects in heterogeneous information networks is an important task with many applications.
Different from traditional clustering tasks where similarity functions between objects are given and with no ambiguity, objects in heterogeneous networks can be connected via different relationships, encoded by different meta-paths.
In this paper, we integrate the meta-path selection problem with the user-guided clustering problem in heterogeneous networks.
An algorithm PathSelClus that can utilize very limited guidance from users in the form of seeds in some of the clusters and automatically learn the best weights for each metapath in the clustering process, is proposed.
The experiments on different tasks on real datasets have demonstrated that our algorithm can output the most stable and accurate clustering results compared with the baselines.
Also, the learned weights for each meta-path are very insightful to explain the hidden similarity between target objects under a particular clustering task.
