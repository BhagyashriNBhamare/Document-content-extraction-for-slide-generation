Succinct is a data store that enables efficient queries directly on a compressed representation of the input data.
Succinct uses a compression technique that allows random access into the input, thus enabling efficient storage and retrieval of data.
In addition, Succinct natively supports a wide range of queries including count and search of arbitrary strings, range and wildcard queries.
What differentiates Succinct from previous techniques is that Succinct supports these queries without storing indexes-all the required information is embedded within the compressed representation.
Evaluation on real-world datasets show that Succinct requires an order of magnitude lower memory than systems with similar functionality.
Succinct thus pushes more data in memory, and provides low query latency for a larger range of input sizes than existing systems.
High-performance data stores, e.g. document stores [1,6], key-value stores [5,9,23,24,26,38,39,43] and multiattribute NoSQL stores [3,19,21,25,35,48], are the bedrock of modern cloud services.
While existing data stores provide efficient abstractions for storing and retrieving data using primary keys, interactive queries on values (or, secondary attributes) remains a challenge.To support queries on secondary attributes, existing data stores can use two main techniques.
At one extreme, systems such as column oriented stores, simply scan the data [10,36].
However, data scans incur high latency for large data sizes, and have limited throughput since queries typically touch all machines 1 .
At the other extreme, one can construct indexes on queried attributes [3,6,35].
When stored in-memory, these indexes are not only fast, but can achieve high throughput since it is possible to execute each query on a single machine.
The main disadvantage of indexes is their high memory footprint.
Evaluation of popular open-source data stores [6,35] using real-world datasets ( §6) shows that indexes can be as much as 8× larger than the input data size.
Traditional compression techniques can reduce the memory footprint but suffer from degraded throughput since data needs to be decompressed even for simple queries.
Thus, existing data stores either resort to using complex memory management techniques for identifying and caching "hot" data [5,6,26,35] or simply executing queries off-disk or off-SSD [25].
In either case, latency and throughput advantages of indexes drop compared to in-memory query execution.We present Succinct, a distributed data store that operates at a new point in the design space: memory efficiency close to data scans and latency close to indexes.
Succinct queries on secondary attributes, however, touch all machines; thus, Succinct may achieve lower throughput than indexes when the latter fits in memory.
However, due to its low memory footprint, Succinct is able to store more data in memory, avoiding latency and throughput degradation due to off-disk or off-SSD query execution for a much larger range of input sizes than systems that use indexes.Succinct achieves the above using two key ideas.
First, Succinct stores an entropy-compressed representation of the input data that allows random access, enabling efficient storage and retrieval of data.
Succinct's data representation natively supports count, search, range and wildcard queries without storing indexesall the required information is embedded within this compressed representation.
Second, Succinct executes queries directly on the compressed representation, avoiding data scans and decompression.
What makes Succinct a unique system is that it not only stores a compressed representation of the input data, but also provides functionality similar to systems that use indexes along with input data.Specifically, Succinct makes three contributions:• Enables efficient queries directly on a compressed representation of the input data.
Succinct achieves this using (1) a new data structure, in addition to adapting data structures from theory literature [32,[44][45][46], to compress the input data; and (2) a new query algorithm that executes random access, count, search, range and wildcard queries directly on the compressed representation ( §3).
In addition, Succinct provides applications the flexibility to tradeoff memory for faster queries and vice versa ( §4).
• Efficiently supports data appends by chaining multiple stores, each making a different tradeoff between write, query and memory efficiency ( §4): (1) a small log-structured store optimized for fine-grained appends; (2) an intermediate store optimized for query efficiency while supporting bulk appends; and (3) an immutable store that stores most of the data, and optimizes memory using Succinct's data representation.
• Exposes a minimal, yet powerful, API that operates on flat unstructured files ( §2).
Using this simple API, we have implemented many powerful abstractions for semi-structured data on top of Succinct including document store (e.g., MongoDB [6]), key-value store (e.g., Dynamo [23]), and multi-attribute NoSQL store (e.g., Cassandra [35]), enabling efficient queries on both primary and secondary attributes.We evaluate Succinct against MongoDB [6], Cassandra [35], HyperDex [25] and DB-X, an industrial columnar store that supports queries via data scans.
Evaluation results show that Succinct requires 10−11× lower memory than data stores that use indexes, while providing similar or stronger functionality.
In comparison to traditional compression techniques, Succinct's data representation achieves lower decompression throughput but supports point queries directly on the compressed representation.
By pushing more data in memory and by executing queries directly on the compressed representation, Succinct achieves dramatically lower latency and higher throughput (sometimes an order of magnitude or more) compared to above systems even for moderate size datasets.
Succinct exposes a simple interface for storing, retrieving and querying flat (unstructured) files; see Figure 1.
We show in §2.1 that this simple interface already allows us to model many powerful abstractions including MongoDB [6], Cassandra [35] and BigTable [19], enabling efficient queries on semi-structured data.
The application submits and compresses a flat file using compress; once compressed, it can invoke a set of powerful primitives directly on the compressed file.
In particular, the application can append new data using append, can perform random access using extract that returns an uncompressed buffer starting at an arbitrary offset in original file, and count number of occurrences of any arbitrary string using count.
Arguably, the most powerful operation provided by Succinct is search which takes as an argument an arbitrary string (i.e., not necessarily word-based) and returns offsets of all occurrences in the uncompressed file.
For example, if file contains abbcdeabczabgz, invoking search(f, "ab") will return offsets [0,6,10].
While search returns an array of offsets, we provide a convenient iterator interface in our implementation.
What makes Succinct unique is that search not only runs on the compressed representation but is also efficient, that is, does not require scanning the file.Succinct provides two other search functions, again on arbitrary input strings.
First, rangesearch returns the offsets of all strings between str1 and str2 in lexicographical order.
Second, wildcardsearch(f, prefix, suffix, dist) returns an array of tuples.
A tuple contains the offset and the length of a string with the given prefix and suffix, and whose distance between the prefix and suffix does not exceed dist, measured in number of input characters.
Suppose again that file f contains abbcdeabczabgz, then wildcardsearch(f, "ab", "z", 2) will return tuples [6,9] for abcz, and [10,13] for abgz.
Note that we do not return the tuple corresponding to abbcdeabcz as the distance between the prefix and suffix of this string is greater than 2.
Consider a logical collection of records of the form (key, avpList), where key is a unique identifier, and avpList is a list of attribute value pairs, i.e., avpList = ((attrName1, value1),... (attrNameN, valueN)).
To enable queries using Succinct API, we encode avpList within Succinct data representation; see Figure 2.
Specifically, we transform the semi-structured data into a flat file with each attribute value separated by a delimiter unique to that attribute.
In addition, Succinct internally stores a mapping from each attribute to the corresponding delimiter, and a mapping from key to offset into the flat file where corresponding avpList is encoded.A1 A2 A3 key1 key2 key3 V11 V12 V13 V21 V22 V23 V31 V32 V33 V11⋆V12•V13 †> V21⋆V22•V23 †> V31⋆V32•V33 †> Attr Delimiter A1 A2 A3 ⋆ • † + + key → offset pointers + end-of-record delimiter (>)Figure 2: Succinct supports queries on semi-structured data by transforming the input data into flat files (see §2.1).
Succinct executes get queries using extract API along with the key→offset pointers, and put queries using the append API.
The delete queries are executed lazily, similar to [8,10], using one explicit bit per record which is set upon record deletion; subsequent queries ignore records with set bit.
Applications can also query individual attributes; for instance, search for string val along attribute A2 is executed as search(val•) using the Succinct API, and returns every key whose associated attribute A2 value matches val.Flexible schema, record sizes and data types.
Succinct, by mapping semi-structured data into a flat file and by using delimiters, does not impose any restriction on avpList.
Indeed, Succinct supports single-attribute records (e.g., Dynamo [23]), multiple-attribute records (e.g., BigTable [19]), and even a collection of records with varying number of attributes.
Moreover, using its key → offset pointers, Succinct supports the realistic case of records varying from a few bytes to a few kilobytes [17].
Succinct currently supports primitive data types (strings, integers, floats), and can be extended to support a variety of data structures and data types including composite types (arrays, lists, sets).
See [16] for a detailed discussion.
We describe the core techniques used in Succinct.
We briefly recall techniques from theory literature that Succinct uses, followed by Succinct's entropy-compressed representation ( §3.1) and a new algorithm that operates directly on the compressed representation ( §3.2).
Existing techniques.
Classical search techniques are usually based on tries or suffix trees [13,47].
While fast, even their optimized representations can require 10-20× more memory than the input size [33,34].
Burrows-Wheeler Transform (BWT) [18] and Suffix arrays [12,40] are two memory efficient alternatives, but still require 5× more memory than the input size [33].
FM-indexes [27][28][29][30] and Compressed Suffix Arrays [31,32,[44][45][46] b a n a n a $ Input6 5 3 1 0 4 2 AoS2Input (a) $ a$ ana$ anana$ banana$ na$ nana$ AoS 0 1 2 3 4 5 6b a n a n a $ Input Illustration of search using AoS and AoS2Input (dashed arrows).
Suffixes being sorted, AoS allows binary search to find the smallest AoS index whose suffix starts with searched string (in this case "an"); the largest such index is found using another binary search.
The result on the original input is showed on the right to aid illustration.
for large datasets.
We describe the basic idea behind Compressed Suffix Arrays.Let Array of Suffixes (AoS) be an array containing all suffixes in the input file in lexicographically sorted order.
AoS along with two other arrays, AoS2Input and Input2AoS 2 , is sufficient to implement the search and the random access functionality without storing the input file.
This is illustrated in Figure 3 and Figure 4.
Note that for a file with n characters, AoS has size O(n 2 ) bits, while AoS2Input and Input2AoS have size n⌈log n⌉ bits since the latter two store integers in range 0 to n−1.
The space for AoS, AoS2Input and Input2AoS is reduced by storing only a subset of values; the remaining values are computed on the fly using a set of pointers, stored in NextCharIdx array, as illustrated in AoS b a n a n a $ Input4 3 6 2 5 1 0 Input2Aos (a) $ a$ ana$ anana$ banana$ na$ nana$ 0 1 2 3 4 5 6 AoS 0 1 2 3 4 5 6b a n a n a $ Input The NextCharIdx array is compressed using a twodimensional representation; see Figure 8.
Specifically, the NextCharIdx values in each column of the twodimensional representation constitute an increasing sequence of integers 3 .
Each column can hence be independently compressed using delta encoding [2, 7, 11].
Succinct uses the above data representation with three main differences.
We give a high-level description of these differences; see [16] for a detailed discussion.First, Succinct uses a more space-efficient representation of AoS2Input and Input2AoS by using a sampling by "value" strategy.
In particular, for sampling rate α, rather than storing values at "indexes" {0,α,2α,... } as in Figure 6 and Figure 7, Succinct stores all AoS2Input values that are a multiple of α.
This allows storing each sampled value val as val/α, leading to a more space-efficient representation.
Using α = 2 for example of Figure 6, for instance, the sampled AoS2Input values are {6,0,4,2}, which can be stored as {3,0,2,1}.
Sampled Input2AoS then becomes {1,3,2,0} with i-th value being the index into sampled AoS2Input where i is stored.
Succinct stores a small amount of additional information to locate sampled AoS2Input indexes.
[4] and following pointers, we get the original AoS entry "banana$".
(b) Since suffixes are sorted, only the first AoS index at which each character occurs (e.g., {($,0),(a,1),(b,4),(n,5)}) need be stored; a binary search can be used to locate character at any index.Second, Succinct achieves a more space-efficient representation for NextCharIdx using the fact that values in each row of the two-dimensional representation constitute a contiguous sequence of integers 4 .
Succinct uses its own Skewed Wavelet Tree data structure, based on Wavelet Trees [32,44], to compress each row independently.
Skewed Wavelet Trees allow looking up NextCharIdx value at any index without any decompression.
The data structure and lookup algorithm are described in detail in [16].
These ideas allow Succinct to achieve 1.25-3× more space-efficient representation compared to existing techniques [7,11,31].
Finally, for semi-structured data, Succinct supports dictionary encoding along each attribute to further reduce the memory footprint.
This is essentially orthogonal to Succinct's own compression; in particular, Succinct dictionary encodes the data along each attribute before constructing its own data structures.
Succinct executes queries directly on the compressed representation from §3.1.
We describe the query algorithm assuming access to uncompressed data structures; as discussed earlier, any value not stored in the compressed representation can be computed on the fly.Succinct executes an extract query as illustrated in Figure 7 on Input2AoS representation from §3.1.
A strawman algorithm for search would be to perform two binary searches as in Figure 3.
However, this algorithm suffers from two inefficiencies.
First, it executes binary searches on the entire AoS2Input array; and sec- [2]) and corresponding suffix (AoS [2]="nana$").
We then remove the first character since the difference between the desired index and the closest sampled index was 1; hence the result "ana$".
ond, each step of the binary search requires computing the suffix at corresponding AoS index for comparison purposes.
Succinct uses a query algorithm that overcomes these inefficiencies by aggressively exploiting the two-dimensional NextCharIdx representation.Recall that the cell (colID, rowID) in twodimensional NextCharIdx representation corresponds to suffixes that have colID as the first character and rowID as the following t characters.
Succinct uses this to perform binary search in cells rather than the entire AoS2Input array.
For instance, consider the query search("anan"); all occurrences of string "nan" are contained in the cell 〈n,an〉.
To find all occurrences of string anan, our algorithm performs a binary search only in the cell 〈a,na〉 in the next step.
Intuitively, af- ter this step, the algorithm has the indexes for which suffixes start with "a" and are followed by "nan", the desired string.
For a string of length m, the above algorithm performs 2(m − t − 1) binary searches, two per NextCharIdx cell [16], which is far more efficient than executing two binary searches along the entire AoS2Input array for practical values of m.
In addition, the algorithm does not require computing any of the AoS suffixes during the binary searches.
For a 16GB file, Succinct's query algorithm achieves a 2.3× speed-up on an average and 19× speed-up in the best case compared to the strawman algorithm.Range and Wildcard Queries.
Succinct implements rangesearch and wildcardsearch using the search algorithm.
To implement rangesearch(f, str1, str2), we find the smallest AoS index whose suffix starts with string str1 and and the largest AoS index whose suffix starts with string str2.
Since suffixes are sorted, the returned range of indices necessarily contain all strings that are lexicographically contained between str1 and str2.
To implement wildcardsearch(f, prefix, suffix, dist), we first find the offsets of all prefix and suffix occurrences, and return all possible combinations such that the difference between the suffix and prefix offsets is positive and no larger than dist (after accounting for the prefix length).
Succinct incorporates its core techniques into a writefriendly multi-store design that chains multiple individual stores each making a different tradeoff between write, query and memory efficiency.
This section describes the design and implementation of the individual stores and their synthesis to build Succinct.
Inv.
Index#Machines n − 2 1 1 %Data(est.) > 99.98% < 0.016% < 0.001% Memory ≈ 0.4× ≈ 5× ≈ 9×Succinct design overview.
Succinct chains three individual stores as shown in Figure 9; Table 1 summarizes the properties of the individual stores.
New data is appended into a write-optimized LogStore, that executes queries via in-memory data scans; the queries are further sped up using an inverted index that supports fast fine-grained updates.
An intermediate store, SuffixStore, supports bulk appends and aggregates larger amounts of data before compression is initiated.
Scans at this scale are simply inefficient.
SuffixStore thus supports fast queries using uncompressed data structures from §3; techniques in place ensure that these data structures do not need to be updated upon bulk appends.
SuffixStore raw data is periodically transformed into an immutable entropy-compressed store SuccinctStore that supports queries directly on the compressed representation.
The average memory footprint of Succinct remains low since most of data is contained in the memory-optimized SuccinctStore.
LogStore is a write-optimized store that executes data append via main memory writes, and other queries via data scans.
Memory efficiency is not a goal for LogStore since it contains a small fraction of entire dataset.
One choice for LogStore design is to let cores concurrently execute read and write requests on a single shared partition and exploit parallelism by assigning each query to one of the cores.
However, concurrent writes scale poorly and require complex techniques for data structure integrity [39,41,42].
Succinct uses an alternative design, partitioning LogStore data into multiple partitions, each containing a small amount of data.
However, straightforward partitioning may lead to incorrect results if the query searches for a string that spans two partitionsLogStore thus uses overlapping partitions, each annotated with the starting and the ending offset corresponding to the data "owned" by the partition.
The overlap size can be configured to expected string search length (default is 1MB).
New data is always appended to the most recent partition.LogStore executes an extract request by reading the data starting at the offset specified in the request.
While this is fast, executing search via data scans can still be slow, requiring tens of milliseconds even for 250MB partition sizes.
Succinct avoids scanning the entire partition using an "inverted index" per partition that supports fast updates.
This index maps short length (default is three character) strings to their locations in the partition; queries then need to scan characters starting only at these locations.
The index is memory inefficient, requiring roughly 8× the size of LogStore data, but has little affect on Succinct's average memory since LogStore itself contains a small fraction of the entire data.
The speed-up is significant allowing Succinct to scan, in practice, up to 1GB of data within a millisecond.
The index supports fast updates since, upon each write, only locations of short strings in the new data need to be appended to corresponding entries in the index.
SuffixStore is an intermediate store between LogStore and entropy-compressed SuccinctStore that serves two goals.
First, to achieve good compression, SuffixStore accumulates and queries much more data than LogStore before initiating compression.
Second, to ensure that LogStore size remains small, SuffixStore supports bulk data appends without updating any existing data.Unfortunately, LogStore approach of fast data scans with support of inverted index does not scale to data sizes in SuffixStore due to high memory footprint and data scan latency.
SuffixStore thus stores uncompressed AoS2Input array ( §3) and executes search queries via binary search (Figure 3).
SuffixStore avoids storing AoS by storing the original data that allows random access for comparison during binary search, as well as, for extract queries; these queries are fast since AoS2Input is uncompressed.
SuffixStore achieves the second goal using excessive partitioning, with overlapping partitions similar to LogStore.
Bulk appends from LogStore are executed at partition granularity, with the entire LogStore data constituting a single partition of SuffixStore.
AoS2Input is constructed per partition to ensure that bulk appends do not require updating any existing data.
SuccinctStore is an immutable store that contains most of the data, and is thus designed for memory efficiency.
SuccinctStore uses the entropy-compressed representation from §3.1 and executes queries directly on the compressed representation as described in §3.2.
SuccinctStore's design had to resolve two additional challenges.First, Succinct's memory footprint and query latency depends on multiple tunable parameters (e.g., AoS2Input and Input2AoS sampling rate and string lengths for indexing NextCharIdx rows).
While default parameters in SuccinctStore are chosen to operate on a sweet spot between memory and latency, Succinct will lose its advantages if input data is too large to fit in memory even after compression using default parameters.
Second, LogStore being extremely small and SuffixStore being latency-optimized makes SuccinctStore a latency bottleneck.
Hence, Succinct performance may deteriorate for workloads that are skewed towards particular SuccinctStore partitions.Succinct resolves both these challenges by enabling applications to tradeoff memory for query latency.
Specifically, Succinct enables applications to select AoS2Input and Input2AoS sampling rate; by storing fewer sampled values, lower memory footprint can be achieved at the cost of higher latency (and vice versa).
This resolves the first challenge above by reducing the memory footprint of Succinct to avoid answering queries off-disk 6 .
This also helps resolving the second challenge by increasing the memory footprint of overloaded partitions, thus disproportionately speeding up these partitions for skewed workloads.We discuss data transformation from LogStore to SuffixStore and from SuffixStore to SuccinctStore in §5.
We have implemented three Succinct prototypes along with extensions for semi-structured data ( §2.1) -in Java running atop Tachyon [37], in Scala running atop Spark [51], and in C++.
We discuss implementation details of the C++ prototype that uses roughly 5,200 lines of code.
The high-level architecture of our Succinct prototype is shown in Figure 10.
The system consists of a central coordinator and a set of storage servers, one server each for LogStore and SuffixStore, and the remaining servers for SuccinctStore.
All servers share a similar architecture modulo the differences in the storage format and query execution, as described in §3.
The coordinator performs two tasks.
The first task is membership management, which includes maintaining a list of active servers in the system by having each server send periodic heartbeats.
The second task is data management, which includes maintaining an up-to-date collection of pointers to quickly locate the desired data during query execution.
Specifically, the coordinator maintains two set of pointers: one that maps file offsets to partitions that contain the data corresponding to the offsets, and the other one that maps partitions to machines that store those partitions.
As discussed in §2.1, an additional set of key → offset pointers are also maintained for supporting queries on semi-structured data.Clients connect to one of the servers via a light-weight Query Handler (QH) interface; the same interface is also used by the server to connect to the coordinator and to other servers in the system.
Upon receiving a query from a client, the QH parses the query and identifies whether the query needs to be forwarded to a single server (for extract and append queries) or to all the other servers (for count and search queries).
In the case of an extract or append query, QH needs to identify the server to which the query needs to be forwarded.
One way to do this is to forward the query to the coordinator, which can then lookup its sets of pointers and forward the query to the appropriate server.
However, this leads to the coordinator becoming a bottleneck.
To avoid this, the pointers are cached at each server.
Since the number of pointers scales only in the number of partitions and servers, this has minimal impact on Succinct's memory footprint.
The coordinator ensures that pointer updates are immediately pushed to each of the servers.
Using these pointers, an extract query is redirected to the QH of the appropriate machine, which then locates the appropriate partition and extracts the desired data.In the case of a search query, the QH that receives the query from the client forwards the query to all the other QHs in the system.
In turn, each QH runs multiple tasks to search all local partitions in parallel, then aggregates the results, and sends these results back to the initiator, that is, to the QH that initiated the query (see Figure 10).
Finally, the initiator returns the aggregated result to the client.
While redirecting queries using QHs reduces the coordinator load, QHs connecting to all other QHs may raise some scalability concerns.
However, as discussed earlier, due to its efficient use of memory, Succinct requires many fewer servers than other in-memory data stores, which helps scalability.Data transformation between stores.
LogStore aggregates data across multiple partitions before transforming it into a single SuffixStore partition.
LogStore is neither memory nor latency constrained; we expect each LogStore partition to be smaller than 250MB even for clusters of machines with 128GB RAM.
Thus, AoS2Input for LogStore data can be constructed at LogStore server itself, using an efficient linear-time, linearmemory algorithm [50].
Transforming SuffixStore data into a SuccinctStore partition requires a merge sort of AoS2Input for each of the SuffixStore partitions, scanning the merged array once to construct Input2AoS and NextCharIdx, sampling AoS2Input and Input2AoS, and finally compressing each row of NextCharIdx.
Succinct could use a single over-provisioned server for SuffixStore to perform this transformation at the SuffixStore server itself but currently does this in the background.Failure tolerance and recovery.
The current Succinct prototype requires manually handling: (1) coordinator failure; (2) data failure and recovery; and (3) adding new servers to an existing cluster.
Succinct could use traditional solutions for maintaining multiple coordinator replicas with a consistent view.
Data failure and recovery can be achieved using standard replicationbased techniques.
Finally, since each SuccinctStore contains multiple partitions, adding a new server simply requires moving some partitions from existing servers to the new server and updating pointers at servers.
We leave incorporation of these techniques and evaluation of associated overheads to future work.
We now perform an end-to-end evaluation of Succinct's memory footprint ( §6.1), throughput ( §6.2) and latency ( §6.3).
Compared systems.
We evaluate Succinct using the NoSQL interface extension ( §2.1), since it requires strictly more space and operations than the unstructured file interface.
We compare Succinct against several open-source and industrial systems that support search queries: MongoDB [6] and Cassandra [35] using secondary indexes; HyperDex [25] using hyperspace hashing; and an industrial columnar-store DB-X, using in-memory data scans 7 .
We configured each of the system for no-failure scenario.
For HyperDex, we use the dimensionality as recommended in [25].
For MongoDB and Cassandra, we used the most memory-efficient indexes.
These indexes do not support substring searches and wildcard searches.
HyperDex and DB-X do not support wildcard searches.
Thus, the evaluated systems provide slightly weaker functionality than Succinct.
Finally, for Succinct, we disabled dictionary encoding to evaluate the performance of Succinct techniques in isolation.Datasets, Workloads and Cluster.
We use two multiattribute record datasets, one smallVal and one largeVal from Conviva customers as shown in Table 2.
The workloads used in our evaluation are also summarized in Table 2.
Our workloads closely follow YCSB workloads; in particular, we used YCSB to generate query keys and corresponding query frequencies, which were then mapped to the queries in our datasets (for each of read, write, and search queries).
All our experiments were performed on Amazon EC2 m1.xlarge machines with 15GB RAM and 4 cores, except for DB-X where we used pre-installed r2.2xlarge instances.
Each of the system was warmed up for 5 minutes to maximize the amount of data cached in available memory.
Figure 11 shows the amount of input data (without indexes) that each system fits across a distributed cluster with 150GB main memory.
Succinct supports inmemory queries on data sizes larger than the system RAM; note that Succinct results do not use dictionary encoding and also include pointers required for NoSQL interface extensions ( §2.1, §5).
MongoDB and Cassandra fit roughly 10-11× less data than Succinct due to storing secondary indexes along with the input data.
HyperDex not only stores large metadata but also avoids touching multiple machines by storing a copy of the entire record with each subspace, thus fitting up to 126× less data than Succinct.
We now evaluate system throughput using a distributed 10 machine Amazon EC2 cluster.
Figure 12 shows throughput results for smallVal and LargeVal datasets across the four workloads from Table 2.
Workload A.
When MongoDB and Cassandra can fit datasets in memory (17GB for smallVal and 23GB for LargeVal across a 150GB RAM cluster), Succinct's relative performance depends on record size.
For small record sizes, Succinct achieves higher throughput than MongoDB and Cassandra.
For MongoDB, the routing server becomes a throughput bottleneck; for Cassandra, the throughput is lower because more queries are executed off-disk.
However, when record sizes are large, Succinct achieves slightly lower throughput than MongoDB due to increase in Succinct's extract latency.When MongoDB and Cassandra data does not fit in memory, Succinct achieves better throughput since it performs in-memory operations while MongoDB and Cassandra have to execute some queries off-disk.
Moreover, we observe that Succinct achieves consistent performance across data sizes varying from tens of GB to hundreds of GB.Workload B. MongoDB and Succinct observe reduced throughput when a small fraction of queries are append queries.
MongoDB throughput reduces since indexes need to be updated upon each write; for Succinct, LogStore writes become a throughput bottleneck.
Cassandra being write-optimized observes minimal reduction in throughput.
We observe again that, as we increase the data sizes from 17GB to 192GB (for SmallVal) and from 23GB to 242GB (for LargeVal), Succinct's throughput remains essentially unchanged.Workload C. For search workloads, we expect MongoDB and Cassandra to achieve high throughput due to storing indexes.
However, Cassandra requires scanning indexes for search queries leading to low throughput.
The case of MongoDB is more interesting.
For datasets with fewer number of attributes (SmallVal dataset), MongoDB achieves high throughput due to caching being more effective; for LargeVal dataset, MongoDB search throughput reduces significantly even when the entire index fits in memory.
When MongoDB indexes do not fit in memory, Succinct achieves 13-134× higher throughput since queries are executed in-memory.
As earlier, even with 10× increase in data size (for both smallVal and LargeVal), Succinct throughput reduces minimally.
As a result, Succinct's performance for large datasets is comparable to the performance of MongoDB and Cassandra for much smaller datasets.
Workload D.
The search throughput for MongoDB and Cassandra becomes even worse as we introduce 5% appends, precisely due to the fact that indexes need to be updated upon each append.
Unlike Workload B, Succinct search throughput does not reduce with appends, since writes are no more a bottleneck.
As earlier, Succinct's throughput scales well with data size.
Note that the above discussion holds even when MongoDB and Cassandra use SSDs to store the data that does not fit in memory.
When such is the case, throughput reduction is lower compared to the case when data is stored on disk; nevertheless, the trends remain unchanged.
Specifically, Succinct is able to achieve better or comparable performance than SSD based systems for a much larger range of input values.
We now compare Succinct's latency against two sets of systems: (1) systems that use indexes to support queries (MongoDB and Cassandra) on a distributed 10 node Amazon EC2 cluster; and (2) systems that perform data scans along with metadata to support queries (HyperDex and DB-X) using a single-machine system.
To maintain consistency across all latency experiments, we only evaluate cases where all systems (except for HyperDex) fit the entire data in memory.Succinct against Indexes.
Figure 13 shows that Succinct achieves comparable or better latency than MongoDB and Cassandra even when all data fits in memory.
Indeed, Succinct's latency will get worse if record sizes are larger.
For writes, we note that both MongoDB and Cassandra need to update indexes upon each write, leading to higher latency.
For search, MongoDB achieves good latency since MongoDB performs a binary search over an in-memory index, which is similar in complexity to Succinct's search algorithm.
Cassandra requires high latencies for search queries due to much less efficient utilization of available memory.Succinct against data scans.
Succinct's latency against systems that do not store indexes is compared in Fig- ure 14.
HyperDex achieves comparable latency for get queries; search latencies are higher since due to its high memory footprint, HyperDex is forced to answer most queries off-disk.
DB-X being a columnar store is not optimized for get queries, thus leading to high latencies.
For search queries, DB-X despite optimized inmemory data scans is around 10× slower at high percentiles because data scans are inherently slow.
Figure 15 shows the throughput versus latency results for Succinct, for both get and search queries for a fully loaded 10 machine cluster with smallVal 192GB dataset.
The plot shows that Succinct latency and throughput results above are for the case of a fully loaded system.
Our evaluation results for workload A and B used records of sizes at most 1300bytes per query.
We now discuss Succinct's performance in terms of throughput for long sequential reads.
We ran a simple microbenchmark to evaluate the performance of Succinct over a single extract request for varying sizes of reads.
Succinct achieves a constant throughput of 13Mbps using a single core single thread implementation, irrespective of the read size; the throughput increases linearly with number of threads and/or cores.
This is essentially a tradeoff that Succinct makes for achieving high throughput for short reads and for search queries using a small memory footprint.
For applications that require large number of sequential reads, Succinct can overcome this limitation by keeping the original uncompressed data to support sequential reads, of course at the cost of halving the amount of data that Succinct pushes into main memory.
The results from Figure 11 show that Succinct will still push 5-5.5× more data than popular open-source systems with similar functionality.
Succinct's goals are related to three key research areas:Queries using secondary indexes.
To support point queries, many existing data stores store indexes/metadata [3,6,25,35] in addition to the original data.
While indexes achieve low latency and high throughput when they fit in memory, their performance deteriorates significantly when queries are executed off-disk.
Succinct requires more than 10× lower memory than systems that store indexes, thus achieving higher throughput and lower latency for a much larger range of input sizes than systems that store indexes.Queries using data scans.
Point queries can also be supported using data scans.
These are memory efficient but suffer from low latency and throughput for large data sizes.
Most related to Succinct is this space are columnar stores [10,15,22,36,49].
The most advanced of these [10] execute queries either by scanning data or by decompressing the data on the fly (if data compressed [14]).
As shown in §6, Succinct achieves better latency and throughput by avoiding expensive data scans and decompression.Theory techniques.
Compressed indexes has been an active area of research in theoretical computer science since late 90s [27-30, 32, 44-46].
Succinct adapts data structures from above works, but improves both the memory and the latency by using new techniques ( §3).
Succinct further resolves several challenges to realize these techniques into a practical data store: (1) efficiently handling updates using a multi-store design; (2) achieving better scalability by carefully exploiting parallelism within and across machines; and (3) enabling queries on semi-structured data by encoding the structure within a flat file.
In this paper, we have presented Succinct, a distributed data store that supports a wide range of queries while operating at a new point in the design space between data scans (memory-efficient, but high latency and low throughput) and indexes (memory-inefficient, low latency, high throughput).
Succinct achieves memory footprint close to that of data scans by storing the input data in an entropy-compressed representation that supports random access, as well as a wide range of analytical queries.
When indexes fit in memory, Succinct achieves comparable latency, but lower throughput.
However, due to its low memory footprint, Succinct is able to store more data in memory, avoiding latency and throughput reduction due to off-disk or off-SSD query execution for a much larger range of input sizes than systems that use indexes.
This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy, Cisco, Cray, Cloudera, EMC, Ericsson, Facebook, Guavus, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal, Samsung, Splunk, Virdata and VMware.
