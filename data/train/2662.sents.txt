The microservice and DevOps approach to software design has resulted in new software features being delivered immediately to users, instead of waiting for long refresh cycles.
On the downside, software bugs and performance regressions have now become an important cause of downtime.
We propose app-bisect, an autonomous tool to troubleshoot and repair such software issues in production environments.
Our insight is that the evolution of microservices in an application can be captured as mutations to the graph of microservice dependencies, such that a particular version of the graph from the past can be deployed automatically, as an interim measure until the problem is permanently fixed.
Using canary testing and version-aware routing techniques, we describe how the search process can be sped up to identify such a candidate version.
We present the overall design and key challenges towards implementing such a system.
Web applications hosted in the platform as a Service (PaaS) clouds are being re-architected to be microservice-oriented [24,35].
In addition, developers and system administrators are adopting new software practices such as continuous integration and deployment of new features instead of periodic upgrade cycles [19][20][21][22][23].
Such practices have collectively come to be termed as DevOps.
The frequency of such deployments varies anywhere from few times a week to 50 times a day [15].
In order to continuously incorporate user feedback, application owners now choose to release early and release often foregoing rigorous testing in exchange for hastening the time to market a product feature.
In such a rapidly evolving application deployment, bugs and performance regressions occur more often than usual.
Application availability is impacted because debugging modern cloud-based distributed applications is often a time consuming task.There are many performance profiling tools [1,[3][4][5]10,16,[29][30][31]37] to help developers identify software issues causing degradation in throughput and response times to the end user.
However, they provide only monitoring capabilities.
There are also design patterns like circuit breakers [13,36] that enable distributed applications to prevent transient and component-level errors from cascading.
Persistent failures, those stemming from software bugs and under-tested features, continue to require human intervention.
Instead of forcing applications to function poorly while waiting for a software update, we argue that an interim measure is possible.
Specifically, applications should be automatically reverted to an older version that was known to provide a better end user experience.Inspired by autonomous computing [7,9,11,12,14,17], we present app-bisect 1 : an autonomous system service that can identify and heal microservice-based applications deployed in the cloud.
By representing the application as a graph of microservices that evolve over time, any update to a microservice can be viewed as a mutation of the graph.
Typical of canary testing, app-bisect systematically tests previous mutations by deploying them in production and diverting a portion of user traffic to the various versions.
On finding the least destructive combination of versions, one that offers the desired end user experience, app-bisect diverts all user traffic to it until the human operator intervenes.Time traveling through the "changelog" of an application's persistent state is not a new concept.
Chronus [18] helps debug failures caused by configuration errors in single-machine applications, by performing a binary search over previously recorded states of the application, to locate the point in time when the configuration error was made.
App-bisect, on the other hand, targets distributed applications, searching through the updates to the service dependency graph.
Debugging is a secondary goal.
Business continuity is its primary focus, with the intent of bringing the application or its older version back online.
Service oriented architectures (SOA) in the past have attempted to use autonomic service substitution [2,6] in case of a service failure.
However, these approaches resort to looking for substitutes providing the same service from the current pool of executing services.
In a microservices environment, functionality is rarely duplicated.
At the same time, compared to a SOA application, the microservice-based application is highly dynamic, with new services, features and fixes being constantly deployed.
Autonomous healing in this scenario faces a different set of challenges compared to prior work in SOA systems.In the following sections, we discuss the scope, design and implementation challenges towards realizing an autonomous system-level service for troubleshooting and healing microservice-based applications.
Figure 1 illustrates a simplified view of a microservice deployment.
We are primarily focused on applications deployed on Platform-as-a-service (PaaS) clouds [32- 34].
We assume that applications follow the microservices and DevOps principles as described below.
Our focus is primarily on user facing applications where end user's experience is the key indicator of application performance.
Microservices and DevOps.
In the microservice approach, applications are structured as a loosely-coupled collection of small, well-defined stateless services that communicate with each other only through well-defined APIs.
As exemplified by the Netflix architecture [35], this approach simplifies the design of scalable and robust cloud applications.
Typical applications are composed of hundreds of instances of heterogeneous microservices, backed by scalable data stores.As shown in Figure 1, services register themselves with a registry service like Zookeeper [8] that can be used for service discovery as well.
The service discovery node enables services to discover the locations of other services.
For scalability purposes, clients (services) periodically fetch the locations of other services in the system.
When invoking other services, load balancing is typically performed at the client-side instead of querying the centralized registry for every invocation.From a code development and operations perspective, each microservice has its own branch with updates to the service being committed to that branch.
Developers and operations personnel work closely to quickly test and deploy new features, monitor user experience in controlled fashion using techniques like canary testing, and finally incorporate changes based on monitoring into the next iteration.
The tight feedback loop created by this process enables software to evolve faster in response to user needs [19][20][21][22][23].
Fault model.
We take a high-availability approach to our problem by treating a performance degradation as application downtime.
For the purposes of our discussion, performance is purely associated with end user experience.
In other words, we consider metrics like end-toend response time and application throughput.
We treat an application experiencing prolonged periods (on the order of tens of minutes) of performance loss as a failure.We do not consider transient issues such as temporary drops in performance while one or more services are scaling-out, spurious timeouts, hard to reproduce nondeterministic bugs, etc.
However, we take into account scenarios where there is a high frequency of errors when invoking a microservice's API, frequent crashes of a microservice, or other recurring error events in log files.Our fault-model does not include scenarios where the performance issue is caused by failures in the underlying infrastructure such as hardware failures, network partitions, transient link congestion, etc.
For example, if there is a long outage in a certain section of the data center, application performance issues in the affected portion of the data center are ignored until the system issues are resolved.Detection & Recovery.
Performance management requires monitoring, analyzing and maintaining the performance of an application.
In a standard setup, the data from the monitoring subsystem is continuously fed into a real-time analytics subsystem.
The analytics subsystem generates alerts whenever it observes performance issues.
The maintenance subsystem acts on the alerts, typically by launching new instances (i.e., auto scaling) or alerting the human operator.
The mean time to recover (MTTR) an application from a performance bug depends on how quickly the human operator responds to the issue.
If the alert was raised at an hour when the operator is not available (e.g., midnight), the time to repair (and hence recover) is high, resulting in loss of traffic and revenue.
Our goal is to minimize the MTTR by routing traffic through a previous version of the application where the issue is not present.
We introduce app-bisect, a system-level applicationagnostic tool to react to performance issues that arises in microservice applications.
App-bisect is intended to operate in unsupervised mode.
The high-level operation of app-bisect is as follows: upon activation, it systematically tests various past-versions of the application's microservices by co-deploying them along with their present-version counter parts, until it finds a combination where the performance issue is not present.
It then decommissions all other versions, effectively rolling back parts of the application to the past.As an autonomous system tasked with maintaining application performance, app-bisect needs to know when to repair an application and when not to.
By operating at the system-level, app-bisect leverages knowledge of both the application deployment as well as the underlying infrastructure.
This visibility enables app-bisect to repair applications only when the performance issue can be attributed to the application and not the underlying infrastructure.In the remainder of this section, we describe various design points, the challenges and related implementation details.
The application's response to the user is a composition of outputs from various microservices constituting the application.
While the application certainly depends on all its microservices, individual microservices may also depend on each other for their functions.
Given that the microservices can be developed and updated independently, when updates are made to a microservice's data model or API, developers maintain backward compatibility by continuing to support the previous version until all dependent microservices are updated.
During build or deployment time, every microservice specifies its dependencies on the minimum required version of other microservices in the application (Fig- ure 2).
These dependencies are provided in manifest files and are already enforced in several platform as a service clouds [32][33][34].
With every new update to a microservice, its version number in the manifest file is updated.
The PaaS platform deploys the instances of respective versions of microservices accordingly.
At runtime, microservices advertise their versions and discover the presence of appropriate dependents using systems like Zookeeper [8] that enable service registration and discovery.Treating the application as a Linux installation.
When deploying an older version of one microservice, older versions of other dependent services may need to be deployed as well.
App-bisect takes a package management approach used commonly in various Linux distributions.
It records the microservice dependencies and the application topology at every update to a microservice.
When a previous version of one microservice is deployed, it also deploys appropriate versions of other dependent microservices with the latest version possible.
As shown in Figure 2, the dependency on a range of versions enables app-bisect to start with the latest possible version that may include additional bug fixes compared to the earliest possible version.
A straightforward approach to restoring application performance is to identify the root cause, the exact update to a microservice that resulted in the current performance degradation.
The microservice is reverted to a version prior to the update.
At the same time, other dependent microservices in the application are reverted to the latest version possible while maintaining overall compatibility.Unfortunately, it is non-trivial to trace the root cause of performance degradation in a microservice-based deployment.
Even well engineered data center scale applications like Facebook [4] and LinkedIn [10] resort to sophisticated techniques to identify the root cause.It is relatively easy to test a simple three-tiered web application where the cause and effect can be observed.
For example, ordering for an item would result in changes to the database immediately.
However microservice-based applications are typically event-based.
Worker services take items off a task queue and service them one after another.
In such a system, how does one correlate the action and the eventual result?
There is no cause-effect correlation in such systems.
A problem that manifests in one microservice may have been caused by another service in the call chain.
At other times, a bug introduced in an update to one service may manifest only after other microservices are updated to a point where they start activating the bug prone code path.
App-bisect takes an inverted canary testing approach to auto healing.
It starts by identifying the most recently updated microservice.
As shown in Figure 3, it deploys the previous version of the microservice and routes portion of the user traffic across the previous version, very similar to the canary testing process conducted in modern web applications when testing new features.
Appbisect's philosophy is to not attempt to identify the root cause.
Rather, it focuses on identifying a deployment graph that does not exhibit the performance degradation.Is this searching for a needle in a haystack?
Theoretically, in an application with n microservices with m updates to each service, the search space of all possible deployment combinations is O(n m ).
Deploying and testing each version is infeasible and beats the purpose of a fast auto-response tool.
Fortunately, the search space can be drastically pruned by taking into account the dependencies across microservices.Help!
My app is back to Hello World!
App-bisect continues to test various past versions until it finds one that meets the performance requirements specified in the performance monitor.
However, the reader may ask how far back in time will app-bisect go?
With every rollback, there is potential for loss of features that were exposed, until now, to the end user.
The application owner can bound the search process; specifically, as shown in Fig- ure 4, lower bounds, known as global restore points can be created to signify point in time until which the application owner is willing to rollback the application.
Appbisect searches through various combinations of the application from the current state until the global restore point.What about data consistency?
One question that arises in this context is what happens to the state stored in the data stores as essentially different versions of the same microservices are accessing the same data store.
Our approach to searching for a substitute version leverages the canary testing practice used by modern web applications.
Canary testing is typically conducted directly in production deployments.
Hence the microservices are engineered from the very beginning to handle scenarios where multiple versions can co-exist, accessing the same data store backends.
App-bisect piggybacks on this capability to provide a gradual feature downgrade in certain parts of the application.
One drawback of this approach is that the search is limited to microservice versions that do not involve changes to the data model in the data store.
At any given point in time during the search, there are two or more versions of multiple microservices in the application.
Requests have to flow through a specific chain of microservices, where the dependencies are satisfied.
This may not seem like a challenge as an application capable of handling canary testing would certainly have this capability built into it.
However, app-bisect does not have control over the application layer code.
Hence, it has to control the flow of information, such only a particular chain of specifically picked versions of microservices handle all aspects of a user request.
In order to route requests through a specific chain of microservices, app-bisect leverages the software defined networking substrate in public cloud data centers [25][26][27][28] networks to achieve version-aware routing.
The combination of host IP address and the edge-switch port number can be used to uniquely identify a particular microservice and its respective version.
App-bisect uses this information to setup flow forwarding rules that route requests through a particular chain of microservices.Sharing microservices between deployments.
During the search process, two chains of microservices being tested may have one or more microservices in common.
While it may be operationally efficient to share such instances of such microservices, version-aware routing becomes hard.
Specifically, if two microservice call chains diverge from a given microservice, app-bisect cannot decide where to route egress requests.
It requires application layer support to perform such intelligent routing.
On the other hand, as shown in Figure 3, when two microservice call chains converge to the same final set of microservices, app-bisect can reuse those microservice instances while still being able to route requests in a version-aware manner.
A version of the application needs to be available always while app-bisect deploys, tests and destroys previous versions of the candidate microservices in the application.
We chose to let the original (latest) version of the application remain operational for this purpose.
Alternatively, a version corresponding to the restore point version could be deployed at the risk of unnecessarily losing features and bug fixes that are unrelated to the component performing poorly.When a restore point is available, to speed up the search, app-bisect performs a binary search between the restore point and the version of the application corresponding to the latest update.
As shown in Figure 4, the search algorithm starts by picking a random update in the highlighted search region.
If a restore point is not available, app-bisect starts from the most recent update to a microservice.
The alternate version of the microservice and its corresponding dependencies are deployed.
As described earlier (and shown in Figure 3), using canary testing techniques, a portion of incoming traffic is redirected to the alternate version of the application deployment and its performance is monitored over a small period of time (e.g., 1-2 minutes).
If the alternate version does not trigger alerts from the performance monitoring tool, app-bisect proceeds to search the second half of the commit history between the alternate version and the latest version.
If the alternate version also performs poorly, app-bisect proceeds to search the first half of the commit history between the restore point and the alternate version.
Throughout the course of the search, the latest version of the application remains deployed, ensuring that some version of the application is always available to the user until an alternative version with better performance is found.Sacrificing efficiency for speed.
When a restore point is not provided, the search process can take a very long time.
App-bisect parallelizes the search by testing multiple deployments simultaneously.
This approach can be thought of as an n-ary version of the typically binary style canary testing.
The upside to this approach is that the triage can be completed quickly, thereby reducing the impact on application availability.
Release early, release often, and listen to your consumers 2 is the philosophy that is driving the fast adoption of microservices and DevOps principles among the developer community.
However, its not all free lunch.
While the common web application has now become a distributed resilient application, the complexity of troubleshooting issues has also spread from a single machine to a deployment spanning data centers.
App-bisect is an attempt to tackle this complexity based on futuristic visions of autonomous computing.
Controversial Points: Feature upgrades tend to be pushed out much more frequently nowadays than they used to be in the past.
In this context, there are two controversial parts to this paper.
The first is the belief that distributed applications are better off traveling back to a slimmer (feature wise) past rather than sticking to a slow and bloated present.
The second controversial part is the notion that an autonomic system can decide to enforce this belief.
What assurances does the developer community need to embrace such autonomic tools in production?Feedback: App-bisect cannot not guarantee a bounded completion time for the search process.
Neither can a developer when asked to diagnose a software issue whose root causes are unknown.
However, we are looking for community feedback on pruning the search space and speeding up the search process through intelligent insights into the application.
For example, app-bisect could apply machine learning techniques to continuously understand the changes in performance, as the application evolves.
How can this learning be leveraged when troubleshooting the application?Open Issues: One of the unsolved issues in the paper is how to automatically route requests in a version-aware manner when multiple microservice chains share one or more microservices.
As an example, consider the scenario where two microservices (a producer and a consumer) use asynchronous notifications and work queues.
When creating instances of different versions of producers and consumers, it is possible that the different producers append tasks to a globally shared queue service like Amazon's Simple Queue Service.
Consumer services could end up consuming tasks from incompatible producers, resulting in erroneous execution or data corruption.Idea Falling Apart: Microservices can have complex dependencies that span the persistent data stores.
Unless the microservices are stateless and data models in the persistent stores can handle downgrades gracefully, appbisect may not work.
As discussed earlier, app-bisect's usefulness depends on its time to discover and isolate bugs.
More importantly, app-bisect should not introduce additional failures or corrupt any data.
