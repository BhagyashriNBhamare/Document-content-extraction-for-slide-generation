Latent semantic indexing (LSI) is a well-known unsuper-vised approach for dimensionality reduction in information retrieval.
However if the output information (i.e. category labels) is available, it is often beneficial to derive the indexing not only based on the inputs but also on the target values in the training data set.
This is of particular importance in applications with multiple labels, in which each document can belong to several categories simultaneously.
In this paper we introduce the multi-label informed latent semantic indexing (MLSI) algorithm which preserves the information of inputs and meanwhile captures the correlations between the multiple outputs.
The recovered "latent seman-tics" thus incorporate the human-annotated category information and can be used to greatly improve the prediction accuracy.
Empirical study based on two data sets, Reuters-21578 and RCV1, demonstrates very encouraging results.
Information retrieval and pattern recognition often suffer from the problem of high dimensionality of the data, for the reason of learnability or computational efficiency.
Therefore dimensionality reduction in terms of semantic indexing or feature projection is of great importance and is commonly applied to solve real world problems [2,1,5].
Among various methods, latent semantic indexing (LSI) turns out to be a successful approach and is widely applied to document analysis and information retrieval [2].
To apply LSI, documents are represented in a vector space model, and singular value decomposition (SVD) is performed to find the sub-eigenspace with large eigenvalues.
It is shown that LSI can find the best subspace in terms of Frobenius norm of matrix.
Thus the technology behind LSI is also called principal component analysis (PCA) in the sense that each "latent semantic" can be viewed as a "component" to represent the data (see, e.g. [4]).
LSI is purely unsupervised and is not capable to incorporate some additional knowledge.
There are at least two reasons for further improvements on this issue.
First, considerable information about the content of documents is reflected by document's labels, which is often annotated by human experts.
This is particularly the case in the multi-label setting where each document is assigned to multiple categories.
The semantic correlations of assignments for variant categories and the hierarchical structure of categories expresses the semantic relationships between documents.
Therefore, it is desired to have a LSI technique that can be informed by this additional knowledge and produce semantically more meaningful latent factors.Second, the unsupervision of LSI leads to results that may be or may not be useful in discriminative analysis like automatic text categorization.
However in one specific classification or regression problem, output information is in general very important and should be incorporated into the feature mapping or selection process.
In particular we consider problems with multiple labels: For an input x the corresponding output is no longer a scalar but a vector y = [y 1 , . . . , y L ] T .
Thus the text categorization system solves many related tasks at the same time.
In this setting the dependencies between multiple labels are worth considering for multivariate data analysis, and can be used to improve the indexing for these specific tasks.
Furthermore, training a system with multiple labels might lead to smaller parameter variance and the prediction for a particular label is improved if the labels are correlated.This setting is very common in real-world applications.
One example is the problem of multi-label document categorization, where each document is allowed to be associated with more than one category and where categories often have semantic correlations [8].
The well-known text data set Reuters-21578 contains such documents, and the new text data corpus RCV1 has additionally a topic hierarchy [7].
These two data sets will be used in the experiments.In this paper we introduce a supervised LSI called multilabel informed latent semantic indexing (MLSI).
MLSI maps the input features into a new feature space that retains the information of original inputs and meanwhile captures the dependency of output dimensions.
The mapping is derived by solving an optimization problem for linear projections, and can be easily extended for nonlinear mappings with kernels.
We use this method as a preprocessing step and achieve encouraging results on the multi-label text classification problems.
We consider a set of N documents.
For i = 1, . . . , N , each document i is described by an M -dimensional feature vector x i ∈ X , and is associated with an L-dimensional output vector y i ∈ Y.
We denote the input data as a matrix [·] T denotes matrix transpose.
We aim to derive a mapping Ψ : X → V that projects the input features into a K-dimensional latent space.X = [x1, . . . , xN ] T ∈ R N ×M , and the output data as Y = [y 1 , . . . , y N ] T ∈ R N ×L , whereIn the following, lower-case bold Roman letters denote column vectors, and upper-case ones denote matrices.
In particular, I is reserved for identity matrix.
Eigenvalues are usually denoted as λ and it should be clear from the context which matrix they are corresponding to.
· denotes Frobenius norm for matrices and 2-norm for vectors, and Tr [·] denotes trace for square matrices.
The paper is organized as follows.
In Section 2 we formulate the data projection as an optimization problem in the linear case and then propose a regularized version to prevent overfitting, which is generalized to nonlinear mapping by using kernels.
Then we point out its connections to related work in Section 3 and report the experimental results in Section 4.
In Section 5 we conclude the paper.
We begin by introducing an optimization explanation for LSI, and then take into account the output information.
In LSI, we aim at finding a linear mapping from the input space X to some low-dimensional latent space V, while most of the structure in the data can be explained and recovered.
We can achieve this by taking a latent variable model and solving the following optimization problem which minimizes the reconstruction error (see, e.g., [4]):min A,V X − VA 2(1) subject to:V T V = I,where V ∈ R N ×K and A ∈ R K×M , given K ≤ M .
Each column of V corresponds to one latent variable or latent semantic, and by V T V = I we constrain that they are uncorrelated and each has unit variance 1 .
For each document in X (represented as one row in X), the corresponding row in V explicitly gives its projection in V.
A is sometimes called factor loadings and gives the mapping from latent space V to input space X .
At the optimum, VA leads to the best K-rank approximation of the observations X.
The derived indexing explains the covariance of input data, which is however not necessarily relevant to the output quantities.
Thus LSI may or may not be beneficial to supervised learning problems.
Generally speaking, it is more desirable to consider the correlation between input X and output Y, and the intra-correlation within Y (if multiple labels).
Therefore, we turn to supervised indexing in the next subsection, incorporating both input X and output Y.
The unsupervised indexing problem (1) explicitly represents the projections of input data X in matrix V. To consider the label information, we can enforce the projections V in problem (1) sensitive to Y as well.
Thus in supervised LSI we solve the following optimization problem:min A,B,V (1 − β)X − VA 2 + βY − VB 2(2)subject to:V T V = I,where V ∈ R N ×K gives the K-dimensional projections of documents, for features of both X and Y; A ∈ R K×M , B ∈ R K×L are the factor loadings for X and Y, respectively.
0 ≤ β ≤ 1 is a tuning parameter determining how much the indexing should be biased by the outputs.
As before, V T V = I restricts the K latent variables to be uncorrelated and have unit variance.
Clearly, the cost function is a trade-off between the reconstruction error of both X and Y.
We wish to find the optimal indexing that gives the minimum reconstruction error.
The second part in the objective function of problem (2) enforces the latent semantics to explain the dependency structure of multiple labels.
The following theorem states the interdependency between A, B and V at the optimum.
(a) A = V T X, B = V T Y; (b) V = [v 1 , . . . , v K ]R, where R is an arbitrary K × K orthogonal rotation matrix;(c) At the optimum, the objective function in (2) equals toTr [C] − Tr V T CV , or equivalently, N i=K+1 λ i .
To improve readability, we put all proofs into Appendix.
Theorem 1 states that the leading eigenvectors of C form a solution for matrix V, and any arbitrary rotation for V does not change the optimum.
Therefore to remove the ambiguity, we focus on the solution given by the leading eigenvectors of C, i.e., V = [v 1 , . . . , v K ].
Problem (2) can thus be achieved by solving the eigenvalue problem Cv = λv for the first K leading eigenvectors, which is equivalent to solving 2 : v1, . . . , vK ], A = V T X, and B = V T Y gives the optimal solution for problem (2).
max v∈R N v T Cv (3) subject to : v T v = 1.
Then V = [ To complete the MLSI algorithm, we still need to consider two things.
Firstly, the indexing should not rely on the labels, since for new documents we have no target information yet.
Secondly, the stability of indexing should be taken into account, because otherwise overfitting is likely to occur.
It is not hard to see that solving problem (3) only gives the projections for training data with both features in X and Y.
We wish to construct a mapping Ψ : X → V that is able to handle the input features of any new documents, thus we add a linear constraint to problem (2) and restrict the latent variables as linear mappings of X, i.e.,V = XW.Therefore we have vi = Xwi, for i = 1, . . . , K, if we denote (3), we have an optimization problem with respect to w:W = [w1, . . . , wK ] ∈ R M ×K .
Plugging v = Xw intomax w∈R M w T X T CXw (4) subject to : w T X T Xw = 1.
Similar to other linear systems, the learned mappings can be unstable when the span{x 1 , . . . , x N } has a lower rank than M , due to the small size of training set or dependence between input features 3 .
As a result, a disturbance of w with an arbitrary w * ⊥ span{x1, . . . , xN } does not change the objective function of optimization since (w + w * ) T x i = w T x i , but may dramatically change the projections of unseen test documents which are not in the spanned space.
To improve the stability, we have to constrain w in some way.Suppose rank(C) = N , then maximizing (3) is equivalent to minimizing v T C −1 v. 4 We introduce the Tikhonov 2 Solving problem (3) itself only gives the first eigenvector v1 of C.
The full optimization problem should be recursively computing vj by maximizing v T Cv with the constraint v T v = 1 and v ⊥ span{v 1 , . . . , v j−1 }.
Here we state the problem as (3) for simplicity and also because its Lagrange formulism directly leads to the eigenvalue problem.
3 This will be a crucial problem when we consider nonlinear mapping in the dual form (cf. Section 2.4), since the dimensionality of data point x in the reproducing kernel Hilbert space (RKHS) could be very high, or even infinite (e.g., in case of RBF kernel).
See, e.g., [12].
4 This equivalence holds whenever C is positive definite and thus invertible.
It is easy to show that matrix C is at least positive semi-definite, since we haveu T Cu = (1 − β)u T XX T u + βu T YY T u = (1 − β)X T u 2 + βY T u 2 ≥ 0, ∀u ∈ R N .
In case that C is not positive definite, it suffices to use pseudo-inverse instead, or makes it so by adding a tiny positive scalar to diagonal entries.
regularization [14] into problem (4) as the followingmin w∈R M w T X T C −1 Xw + γw 2 (5) subject to : w T X T Xw = 1,where w 2 = w T w is a penalty term and γ is a tuning parameter.
The following theorem shows that the regularization term w 2 removes the ambiguity of mapping functions by restricting w in the span of x i , i = 1, . . . , N , and thus improves the stability of mapping functions.Theorem 2.
If w is an eigenvector of the generalized eigenvalue problem (5), then w must be a linear combination of x i , i = 1, . . . , N , namelyw = X T α = N i=1 (α) i x i where α ∈ R N .
Problem (5) is easily solvable by setting the derivative of its Lagrange formulism with respect to w to be zero.
Then we obtain a generalized eigenvalue problemX T C −1 X + γI w = ˜ λX T Xw,(6)which gives generalized eigenvectors w 1 , . . . , w M with eigen-values˜λvalues˜ values˜λ 1 ≤ . . . ≤ ˜ λ M .
Note we sort eigenvalues in a nondecreasing order, since we take the K eigenvectors with the smallest eigenvalues to form the mapping.
The first K eigenvectors are used to form the mapping functions as the followingψj(x) = w T j x, j = 1, . . . , K,(7)where in this paper we focus on the projection directions and ignore possible scaling factors.
As the main results we obtain Ψ(x) = [ψ1(x), . . . , ψK (x)] T which maps x into a K-dimensional space.
In problem (6) we are interested in the eigenvectors with the smallest eigenvalues, whose computation is however the most unstable part in solving an eigenvalue problem.
Thus we let λ = 1/ ˜ λ and turn the problem into an equivalent one:X T Xw = λ X T C −1 X + γI w,(8)where we are seeking the K eigenvectors with the largest eigenvalues.
This gives the MLSI algorithm in primal form, as summarized in Table 1.
Table 1: MLSI in primal form Input X ∈ R N ×M , Y ∈ R N ×L , 0 ≤ β ≤ 1, γ ≥ 0, K > 0 Steps (i) Calculate C = (1 − β)XX T + βYY T ;(ii) Solve the generalized eigenvalue problem:X T Xw = λ X T C −1 X + γI w, obtain eigenvectors w 1 , . . . , w K with largest K eigenvalues λ 1 ≥ . . . ≥ λ K .
Output indexing function ψ j (x) = w T j x, j = 1, . . . , K So far we have considered linear mappings that project inputs x into a meaningful space V. However, Theorem 2 implies that we can also derive a nonlinear mapping Ψ.Let a kernel function k x (·, ·) be the inner product in X , i.e., k x (x i , x j ) = x i , x j = x T i x j , then from Theorem 2, v = Xw = XX T α = Kxα,whereK x is the N × N kernel matrix satisfying (K x ) i,j = kx(xi, xj).
w 2 can also be calculated with kernel Kx:w 2 = w T w = α T XX T α = α T K x α.Similarly, we can define a kernel function k y (·, ·) for inner product in Y and obtain a kernel matrix Ky = YY T .
Then we can calculate the matrix C using kernels:C = (1 − β)K x + βK y ,(9)and express the dualf ormalism of problem (5) with respect to coefficients α asmin α∈R N α T K x C −1 K x α + γα T K x α(10)subject to : α T K 2 x α = 1, which gives rise to again a generalized eigenvalue problemKxC −1 Kx + γKx α = ˜ λK 2 x α.
(11)We obtain the generalized eigenvectors α 1 , . . . , α N , with˜λ with˜ with˜λ 1 ≤ . . . ≤ ˜ λ N .
The first K eigenvectors are applied to form the mappings.
The j-th mapping function, j = 1, . . . , K, is given byψ j (x) = w T j x = N i=1 (α j ) i k x (x i , x).
As before we define λ = 1/ ˜ λ and change (11) to the following equivalent form:K 2 x α = λ K x C −1 K x + γK x α,(12)and hence we can choose the K eigenvectors with the largest eigenvalues.
The MLSI algorithm in dual form is summarized in Table 2.
Input X ∈ R N ×M , Y ∈ R N ×L , 0 ≤ β ≤ 1, γ ≥ 0, K > 0 Steps (i) (Kx)i,j = kx(xi, xj), (Ky)i,j = ky(y i , y j ), C = (1 − β)K x + βK y ;(ii) Solve the generalized eigenvalue problem:K 2 x α = λ K x C −1 K x + γK x α, obtain eigenvectors α 1 , . . . , α K with largest eigenvalues λ 1 ≥ . . . ≥ λ K .
Output indexing function ψ j (x) = N i=1 (α j ) i k x (x i , x), j = 1, . . . , KSeveral advantages of dual MLSI can be seen from Table 2.
First of all, in contrast of solving a generalized eigenvalue problem for M × M matrices in primal MLSI, in dual MLSI we only need to solve a similar problem for N × N matrices.
In a general indexing problem, the input dimension M (i.e., number of words) is much larger than the number of documents N , and therefore working in dual form is more efficient.
In the experiments we will use the dual form for indexing.
Second, MLSI in dual form is ready to deal with nonlinear mappings.
For this we consider a nonlinear mapping φ : x ∈ X → φ(x) ∈ F , which maps x into a high-dimensional or even infinite-dimensional feature spaceF , and change X to be [φ(x 1 ), . . . , φ(x N )] T .
Then the kernel function is accordingly defined ask x (x i , x j ) = φ(x i ), φ(x j ) F ,where we still have K x = XX T .
Therefore, we can directly work with kernels (e.g., RBF kernel k x (x i , x j ) = exp(−−x i − xj 2 /2σ 2 )), without knowing φ(·) explicitly.
Similarly, we can define a nonlinear mapping for Y and directly work on the corresponding kernel matrix K y .
Although this paper mainly considers the linear kernel to explore the linear correlation of inputs and multivariate labels, the formulism implies that the method can generally handle more complex inputs and outputs (e.g., images) by using some other suitable kernels.
The proposed algorithm MLSI is seen to solve the same optimization problem as LSI when β = 0, as seen in (1) and (2).
Therefore MLSI takes as special case the unsupervised LSI, or more specifically, kernel PCA [10,11].
Kernel PCA is the dual form of PCA and turns out to solve the eigenvalue problem K x α = λα with kernel matrix (K x ) i,j = kx(xi, xj).
To build this connection, we see from (9) that C = Kx holds when β = 0 in MLSI.
Therefore from Table 2 it is easy to check that MLSI solves the generalized eigenvalue problemK 2 x α = λ(1 + γ)K x α,which is identical to kernel PCA since Kx is invertible.
Under this situation, the regularization term controlled by γ is just a rescaling of the cost function, as can be seen in (10).
Hence γ is just a nuisance parameter and we obtain rescaled eigenvalues compared to kernel PCA.
From this perspective, MLSI in general performs label informed kernel PCA or supervised kernel PCA, since it can be viewed as directly modifying the kernel matrix C with label information.
In the literature there are some other well-known supervised projection methods, like linear discriminant analysis (LDA) (e.g., [13]), canonical correlation analysis (CCA) (e.g., [6,3]) and partial least squares (PLS) [15,9].
MLSI substantially differs from them.
LDA is focusing on single classification problem where the output is one-dimensional, while in contrast MLSI considers predictions with multivariate labels and is thus more general.
CCA finds the correlations between two representatives of the same documents (e.g., inputs X and outputs Y in our setting) by minimizing vx − vy 2 subject to both vx and vy being unitary and linear mappings of x i and y i (see a recent discussion in [3]).
However, it does not require the projections v x and v y to promise low-reconstruction error of x and y and thus ignores the intra correlation of either (especially y).
Instead, MLSI takes into account all the inter and intra dependencies, since the projections minimize the reconstruction error of inputs and outputs simultaneously.
PLS can be seen as a penalized CCA, but it cannot find a space of larger dimensionality than that of Y, thus its generalization performance on new dimensions of outputs is restricted (see discussions in [14]).
Instead, MLSI can find in principle N orthogonal dimensions (if K x is positive definite).
In this section we evaluate the proposed MLSI algorithm based on the task of multi-label text classification, in which we allow one document to be assigned to multiple labels.
One can treat each classification problem separately, but these problems could have correlations between each other and could be solved simultaneously.
We solve this problem by applying MLSI and encoding the labelling information into the mapping, and then each classification problem is solved independently using the projected features.
By incorporating the output information that may be difficult to reveal from inputs, the indexing is biased by the specific classification tasks and is thus more suitable for discriminate analysis.
We compare the classification performance using features learned by MLSI and normal LSI, where in the latter case no labelling information is used in indexing.
Experiments are performed on two text data sets taken from Reuters-21578 and RCV1, respectively, followed by detailed discussions.
Our first data set is a text corpus which contains all the documents in Reuters-21578 that are associated with multiple categories.
Eliminating those minor categories that contain less then 50 documents, we have 47 categories to work with.
Picking up all the words that occur at least in 5 documents, we finally obtain 1600 documents with 6076 words that are used in computing TFIDF feature vectors.
In average, each document is assigned to 2.48 categories, and each category has 85 positive documents.The other data set is a subset of the RCV1-v2 text data set, provided by Reuters and corrected by Lewis et al. [7].
The data set contains topics, regions and industries information to each document and a hierarchical structure for topics and industries.
Since it is common that one document is assigned to multiple topics, this is an ideal data set for multi-label text classification.
We use topics as the classification tasks and simply ignore the topic hierarchy.
A small part of the data set is chosen, and similar preprocessing as for Reuters-21578 is done by picking up words with more than 5 occurrences and topics with more than 50 positive assignments.
We end up with 3588 documents with 5496 words, and have 79 topics left.
In average, each topic contains 180 positive documents, and each document belongs to 3.96 topics.
In the following we denote "Reuters" and "RCV1" for these two data sets respectively.
We have two settings in this experiment.
In the first setting (I), we randomly pick up 70% categories for classification and employ 5-fold cross-validation with one fold training and 4 folds testing.
This is a standard classification setting, and our goal is to evaluate whether the feature mappings are generalizable to new data points.
The second setting (II) aims to test the generalization performance of the projection methods on new categorization tasks.
For this we consider the classification problems for the rest 30% categories.
To make a fair comparison, we perform 5-fold cross-validation on previous unseen data (with the same size as training data), using the feature mappings derived from setting (I).
We will compare the following three methods in our experiment:1.
Original Features: A linear SVM with all the text features is trained for each category, and this serves as the baseline for comparison.
2.
LSI: Standard unsupervised projection is performed which maps the input data into a low-dimensional space.
Then a linear SVM is trained on this projected space.3.
MLSI: Additional label information for training data is used for making a supervised mapping.
Then the same SVM is trained on this projected space.In both of the projection methods LSI and MLSI, we use the dual form in this experiment simply because this gives much improved efficiency.
In case of linear kernels, this will give the same results as that in primal form.
The classification performance is compared using F 1 Macro, Micro and AUC (Area Under Curve) score.
F 1 -measure defines a trade-off between precision and recall, and is known to be a good metric for classification evaluation.
In case of multiple outputs, F1 Macro is just the arithmetic average of F 1 measures of all output dimensions, and F 1 Micro can be seen as a weighted average.
Alternatively, AUC score is the area under the ROC (receiver operating characteristics) curve, which plots sensitivity versus 1-specificity.
It is known to measure the objective quality of ranking for specific classification problems.
A higher AUC indicates a better ranking.
It is also averaged over all the output dimensions.
We also tried classification accuracy, but didn't get informative comparison because most of the classification problems are very unbalanced (more than 90% of data are negative examples).
We choose all the parameters for these algorithms as follows.
We use LIBSVM with linear kernel and fix C = 100, which gives Original Features the best performance and is then fixed for the other two methods.
For MLSI we set the parameter β to 0.5 after we scale K x and K y to ensure they have equal traces for balance.
γ is simply fixed as 0 to give the best performance.
For both settings we repeat the experiments 50 times with randomization, and the performance versus dimensionality of projection is shown with means and standard deviations in Figure 1 and Figure 2 for Reuters and RCV1, respectively.The first observation from these figures is that MLSI outperforms LSI in all the cases for setting (I).
This indicates that the mapping functions in MLSI are generalizable to new test data, by incorporating the output information for the training data.Another encouraging observation is that MLSI in most cases can even lead to better classification performance than Original Features, which uses at least 50 times more features.
MLSI in this case can not only greatly accelerate the classification tasks, but also improve the performance.
This is especially true for F1 Macro and AUC score, where a large gap can be observed for all the figures.
For F1 Micro the effect of MLSI is mixed, and an interesting decrease can be observed in Figure 1(e) and Figure 2(e).
Consider the difference between F 1 Macro and F 1 Micro measures, these results indicate that MLSI is particularly useful for classification problems with small positive training examples, since by randomly choosing training data we are more likely to choose small positive examples for them.
For large classes that have lots of training data, SVMs with full features can already do a very good job.MLSI has two tunable parameters β and γ that controls the kernel combination weights and the strength of regularization, respectively.
For previous figures it is assumed fixed, and in this last experiments we study the classification performance when they varied.
Since we can see similar results for both data sets on all the evaluation measures, we A first impression from Figure 3 is that the curves are rather smooth (except when β approaching 1 in setting (II)).
This indicates that the performance is not very sensitive to small changes of β value.
When β increases from 0 to 1, it is seen that all the curves first increase and then decrease, indicating that a good trade-off should be identified for best performance.
When β approaches 0, MLSI tends to be LSI and thus unsupervised.
Outputs are ignored in this case, and poor performance is observed for both settings.
On the other hand when β approaches 1, the mappings tend to solely explain outputs Y, ignoring the intrinsic structure of inputs X.
This also leads to poor performance, especially for setting (II) because the mappings are not good to generalize to new outputs.
Overfitting occurs in this case, where a sharp decrease can be observed with even a much worse performance than LSI (β = 0).
Finally, β = 0.5 is seen to be a good trade-off for both settings.
From our experiences, a slightly larger β (e.g., 0.6) is better for setting (I), and a slightly smaller β (e.g., 0.4) is more stable for setting (II).
For γ we have the observation that small γ leads to better performance for setting (I), while an appropriately chosen γ is necessary for setting (II).
This reflects its regularization effect, since for setting (II) new categories are considered and setting γ = 0 will lead to overfitting.
In this paper we propose a novel indexing algorithm MLSI for multi-label informed latent semantic indexing.
The mappings are supervised and retain the statistical information of not only input features but also the multivariate outputs.
We present both the primal and the dual formalisms for the linear mappings, and nonlinear mappings can also be derived by using reproducing kernels.
The final solution ends up as a simple generalized eigenvalue problem that can be easily solved.
The algorithm is applied for multi-label text classification with very encouraging results.
Currently we are mainly exploiting linear dependency of inputs as well as outputs.
In the near future we plan to apply the algorithm to other types of objects like images with suitable kernels (e.g., RBF kernels), and define kernels to explore richer structured outputs.
Applying the rule C 2 = Tr CC T for an arbitrary matrix C, we obtainJ(A, B, V) : = (1 − β)X − VA 2 + βY − VB 2 = (1 − β)Tr XX T − 2VAX T + VAA T V T + βTr YY T − 2VBY T + VBB T V T .
Let the derivative of J with respect to A and B be zero, we have∂J ∂A = 2(1 − β)(V T X − V T VA) = 0 ⇒ A = V T X, ∂J ∂B = 2β(V T Y − V T VB) = 0 ⇒ B = V T Y,which proves (a).
Then we use the results (a) to replace A and B in J and obtain Jopt = Tr [C] − Tr V T CV , which is first part of (c).
Since Tr [C] is fixed, this suggests that problem (2) can be considered to be an optimization problem only with respect to V:max V∈R N ×K Tr V T CV(13)subject to: V T V = I.For notation simplicity, we denote the optimal solution for V as˜Vas˜ as˜V = [˜ v 1 , . . . , ˜v K ] for a moment.
The Lagrange formalism of problem (13) isL( ˜ V, ˜ Λ) = K i=1˜v i=1˜ i=1˜v T i C˜vC˜v i − K i=1˜λ i=1˜ i=1˜λ i,i (˜ v T i ˜ v i − 1) − 2 i>j˜λ i>j˜ i>j˜λ i,j ˜ v T i ˜ v j ,where ( ˜ Λ) i,j = ˜ λ i,j is a symmetric matrix if we define˜λdefine˜ define˜λ i,j = ˜ λ j,i for i < j. Setting its derivative with respect tõ v i to be zero, we obtain∂L ∂ ˜ v i = 2C˜v2C˜v i − 2 K j=1˜λ j=1˜ j=1˜λ i,j ˜ v j = 0, i = 1, . . . , Kwhich can be rewritten as C ˜ V = ˜ V ˜ Λ.
Since˜ΛSince˜ Since˜Λ is a symmetric matrix, we have˜Λhave˜ have˜Λ = R T ΛR where Λ is a diagonal matrix and R ∈ R K×K is an orthogonal rotation matrix satisfying RR T = R T R = I. ThenC ˜ V = ˜ VR T ΛR ⇒ C ˜ VR T = ˜ VR T ΛSince Λ is diagonal, it is easy to see that the columns of V = ˜ VR T are the eigenvectors of C. Thus the optimal˜Voptimal˜ optimal˜V is formed by an arbitrary rotation of C's eigenvectors, i.e. ˜ V = VR.
Inserting˜VInserting˜ Inserting˜V back into the objective function, we have the value of objective function as Tr [Λ], i.e., sum of the K corresponding eigenvalues of C.
It is easy to see that the maximal Tr [Λ] is the sum of the K largest eigenvalues, which proves second part of (c).
In this case, ˜ V is an arbitrary rotation of the K largest eigenvectors, thus conclusion (b) holds.
Let J(w) denote the cost function in (5), i.e., J(w) := w T X T C −1 Xw + γw 2 .
Obviously J(w) achieves the minimum at the first eigenvector w of the generalized eigenvalue problem (6).
Denote w as the projection of w on the subspace span{x 1 , . . . , x N }, then we can write w = w + w ⊥ , where w ⊥ is orthogonal to the subspace.
Compare J(w ) with J(w).
We havew T x i = w T x i + w T ⊥ x i = w T x i ,so Xw = Xw, which means J(w ) and J(w) agree on the first term.
Since w 2 = w 2 + w ⊥ 2 ≥ w 2 , J(w) ≥ J(w ) holds.
However, this must be an equation since J(w) achieves the minimum.
Therefore we have w ⊥ = 0, and hence w ⊥ = 0, which means w is actually a linear combination of x i , i = 1, . . . , N .
So far we have proved the theorem for the first eigenvector (with the smallest eigenvalue).
Given eigenvectors w j , j = 1, . . . , n− 1, it is known that the n-th eigenvector is obtained by first deflating the matrix C −1 with C † = C −1 − n−1 j=1 λ j Xw j w T j X T , and then solving the following problem min w∈R M w T X T C † Xw + γw 2 subject to : w T X T Xw = 1.
Following the same procedure as before, we can prove that the eigenvector w n also lies in the span of x i , i = 1, . . . , N .
