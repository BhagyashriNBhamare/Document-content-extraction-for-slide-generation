In this paper, we study the prize-collecting version of constrained forest problems with an arbitrary 0-1 connectivity requirement function and a submodular penalty function.
Our framework generalizes the Prize Collecting Generalized Steiner Tree framework of Ha-jiaghayi and Jain [HJ06] to incorporate more general connectivity requirements and penalty functions.
We generalize their primal-dual algorithm using submod-ular function minimization to give a 3-approximation algorithm, and devise an LP rounding algorithm with a performance guarantee of 2.54.
In this section, we further discuss our prize-collecting forest problem (PCF).
We begin by discussing the connection of PCF to the 0-1 proper functions of [GW95].
Next, we show that the result of Hajiaghayi and Jain [HJ06] is a special case of our model.
Finally, we show that PCF does not reduce to the PCGST of Hajiaghayi and Jain.
We also note that the model of Hayrapetyan et al. [HST05] is a special case of our model, but defer details to the full version of the paper.
Proper connectivity requirement functions were first defined in [GW95].
A function f : 2 V → {0, 1} is proper if f (V ) = 0, f (V − S) = f (S) for all S ⊆ V , and for disjoint sets A and B, f (A ∪ B) ≤ max(f (A), f (B)).
We now argue that proper functions are closely related to the penalty function we have introduced.
Given an arbitrary 0-1 connectivity function f : 2 V → {0, 1}, we can ensure that our algorithm returns a feasible solution for the function by setting π(S) = ∞ whenever there is S ∈ S such that f (S) = 1 and π(S) = 0 otherwise.
This penalty function obeys the emptyset, monotonicity, submodularity, and inactivity properties.
However, in order to obey the complement property, we cannot have f (S) 񮽙 = f (V − S), and in order to obey the union property we cannot have f (A) = f (B) = 0 and f (A ∪ B) = 1 for disjoint A and B. Thus for our model of penalty function, if we enforce that a feasible solution is returned for the connectivity function f , that function must be proper.
[HJ06] Hajiaghayi and Jain [HJ06] consider the the prize-collecting generalized Steiner tree problem in which there are a set of pairs of vertices (s 1 , t 1 ), . . . , (s k , t k ) to connect along with penalties π i for not connecting the pair (s i , t i ).
The goal is to find a set of edges F that minimizes the cost of the edges in F plus the penalties of the pairs of vertices not connected by F .
We can map this problem into our framework as follows.
The connectivity requirement function f for our model is the function f (S) = 1 iff there exists some i such that |S ∩ {s i , t i }| = 1, and the penalty π for a family S for our model is the sum of penalties of pairs which are separated by some set in S; that is π(S) = i:∃S∈S:|S∩{si,ti}|=1 π i .
The resulting penalty, as it turns out, satisfies all properties that we require from the penalty function: it is easy to see that the emptyset, inactivity, and monotonicity properties are satisfied.
Hajiaghayi and Jain show that the submodularity property is satisfied (Lemma 2.3 of [HJ06]).
The complement property is not hard to see: V − S separates no more pairs than S. Similarly, for any two S 1 , S 2 , their union S 1 ∪ S 2 separates no more pairs than do S 1 and S 2 , so the union property is satisfied.
Hajiaghayi and Jain with a variation on the penalty of a solution.
There are k pairs of vertices with penalties π 1 = · · · = π k = 1.
The penalty of the solution is the minimum of the number of unconnected pairs and a fixed number l ≤ k.
It means that the penalty increases linearly with the number of unconnected pairs, but is upper bounded by l.
This problem does not fit Hajiaghayi-Jain's model.
We next show that the problem can be cast into our framework.The connectivity requirement function f is same as the one described in Section 2.2.
The penalty function is also the one described in the section except that it is upper bounded by l.
It is easy to see that the penalty function satisfies emptyset and monotonicity properties.
The union, complement, and inactivity properties also follow the same argument as in Section 2.2.
For submodularity of the penalty function, we will prove the equivalent statement that for families S and T ⊇ S and for any subset S ∈ T , π(S ∪ {S}) − π(S) ≥ π(T ∪ {S}) − π(T ).
If S separates more than l pairs, then the inequality is trivially true.
Let S separate a ≤ l pairs which are to be connected and S separates an additional b pairs (not already separated by S).
Clearly T separates a + ≥ a pairs and T ∪{S} separates at most a + + b pairs.
So the left hand side of the inequality is min(a + b, l) − a, and the right hand side is at mostmin(a + + b, l) − min(a + , l).
One can easily check that min(a + b, l) − a ≥ min(a + + b, l) − min(a + , l).
This proves the submodularity of the function π(·).
In this section, we give a primal-dual approximation algorithm to solve our problem.
The LP relaxation of the integer program (PCF-IP) is:min e c e x e + S π(S)z S (PCF-LP) s.t. e∈δ(S) x e + S:S∈S z S ≥ f (S) for all S ⊆ V, x e , z S ≥ 0 for all e, S.The dual of the LP relaxation (PCF-LP) ismax S⊆V f (S) · y S (PCF-D) s.t. S:e∈δ(S) y S ≤ c e ∀e ∈ E (type-(e)) S:S∈S y S ≤ π(S) ∀S ⊆ 2 V (type-(f)) y S ≥ 0 ∀S ⊆ V.Our algorithm is similar to Algorithm A in [HJ06], though it differs in how it finds the tight family of subsets and how it performs the reverse delete step.
We next give the idea behind our algorithm.
For its precise description, see Algorithm 1.
We introduce some terminology first.
We call an edge e tight if its corresponding inequality (of type-(e)) in (PCF-D) holds with equality, i.e., S:e∈δ(S) y S = c e .
Similarly, we call a family S ⊆ 2 V tight if its corresponding inequality (of type-(f)) holds with equality, i.e., S∈S y S = π(S).
For a family S ⊆ 2 V , we call T the closure of S (denoted closure(S)) if T ⊇ S and T is closed under taking unions and complements (and hence intersections and set differences too).
We keep track of three types of components, (i) active components (C a )-components that need an edge out of them (f (S) = 1) but do not have one yet, (ii) inactive components (D i )-components having f (S) = 0, and (iii) marked components (D m )-components that needed an edge out of them originally but our algorithm has decided not to have such an edge and pay the penalty; these are denoted by f (S) = 1 → 0.
To start off, there are n components corresponding to n singleton vertices.
We set them active or inactive depending on whether their connectivity requirement is 1 or 0 respectively.
This is the initialization phase.In the dual-increase phase, we uniformly increase the duals of all active subsets in C a .
In this process, either another edge becomes tight or another family of subsets becomes tight.
(See Section 5 for details on how to check tightness of edges and families.)
If an edge e connecting two components C u and C v becomes tight, we add e to the set of candidate edges for the output solution, remove C u and C v from the family C a of active sets (if they are there), make a new component C u ∪ C v , and set it active (and add it to C a ) if f (C u ∪C v ) = 1, and inactive otherwise (and add it to D i ).
If, on the other hand, a family S of subsets {S 1 , S 2 , . . . , S |S| } becomes tight, we mark all these subsets, remove them from the family C a of active sets (if they are there), and add them to the family D m of marked sets.When there is no active component remaining whose dual can be increased, we enter the reverse delete phase of our algorithm.
We consider edges added to the solution in the reverse order.
Consider the two components C 1 and C 2 resulting from deletion of edge e in the current solution.
If e is the only edge out of C i (i = 1 or 2) and C i is not in the closure of inactive sets and marked sets then we retain the edge e; otherwise, we delete it from the solution.
The idea is that if e is redundant or only necessary for sets whose penalty has been paid for (because they are either inactive or marked), it should be deleted from the solution to decrease the cost of edges in the solution.Let F ′ be the set of edges remaining after the reverse delete procedure and let C(V, F ′ ) be the connected components of F ′ .
Then our algorithm returns F ′ as its set of edges and closure(C(V, F ′ )) as the family of subsets on which it will pay the penalty.
In this section, we prove that Algorithm 1 has a performance guarantee of 3 for the prize-collecting forest problem.
We first ensure that the penalty paid by the solution produced by the algorithm is no more than the cost of the optimum solution.
We then show in Section 4.3 that the cost of edges finally selected by the algorithm (after reverse delete) is no more than twice the cost of the optimum solution.
This will prove a performance guarantee of 3.
by OPT The proof in this section will break naturally into two parts.
We first prove that the penalty of all marked and inactive subsets is bounded by the value of our dual solution, and follow that with the proof that the actual penalty of the algorithm is no more than the penalty of all marked and inactive subsets.
We begin by inferring some properties of the penalty function.Lemma 4.1.
Let S be any collection of subsets with S 1 , S 2 (not necessarily different subsets) in S, and let S be a subset such that f (S) = 0.
Thenπ(S) = π(S ∪ {S 1 ∪ S 2 }) = π(S ∪ {S c 1 }) = π(S ∪ {S}).
Data: A graph G = (V, E), edge cost function c e : E → Q ≥0 , connectivity requirement function f : 2 V → {0, 1}, and penalty function π : Find an edge e = (u, v) with u ∈ C u ∈ M,2 2 V → Z ≥0 .
Result: F ′ ⊆ E minimizing e∈F ′ c e + π ({S ⊆ V : f (S) = 1, δ(S) ∩ F ′ = ∅}).
Let F = ∅ (4 v ∈ C v ∈ M, C u 񮽙 = C v minimizing ε e = (c e − d u − d v )/(f (C u ) + f (C v )).
Find a non-tight family S = {S 1 , . . . , S |S| } 5 with each S i ∈ C a ∪ {S : y S > 0} minimizing ε f = (π(S) − S∈S )/( S∈S f (S)).
Set ε = min{ε e , ε f }.
6Set y C = y C + ε for all C ∈ C a .7 if ε = ε e then 8 Let F ← F ∪ {e = {u, v}}, 9 M ← M ∪ {C u ∪ C v } − {C u , C v }, and C a ← C a \ {C u , C v }; if f (C u ∪ C v ) = 1 then 10 C a ← C a ∪ {C u ∪ C v } 11 else 12 D i ← D i ∪ {C u ∪ C v } 13 end 14 else 15 D m ← D m ∪ {S 1 , S 2 , . . . , S |S| }, 16 C a ← C a − {S 1 , S 2 , . . . , S |S| } end 17 end 18Let F l = F = {e 1 , e 2 , . . . , e l }.
Edges are 19 arranged in order they were added to F .
for j = l down to 1 do Consider the graph H j = (V j , E j ) where Return F ′ = F 0 as the set of edges.21 V j = C(V, {e 1 , e 2 , . . . , e j−1 }) (components) and E j = F j \ {e 1 , e 2 , . . . , e j−1 }.
Let u j (e j ) and v j (e j ) be the two endpoints of e j in H j .
if δ(u j (e j )) = {e j }∧u j (e j ) ∈ closure(D i ∪D m ) 22 ∨ δ(v j (e j )) = {e j } ∧ v j (e j ) ∈ closure(D i ∪ D m ) then F j−1 = F j .
23 else 24 F j−1 = F j \ e j .
Algorithm 1: A primal-dual algorithm for PCF.Let F ′ be the final set of edges returned by the algorithm, with C(V, F ′ ) the set of components of F ′ .
Using Lemma 4.1, we note that for a given family S of subsets of V , π(S) = π(closure(S)).
We will prove that π(D i ∪ D m ) ≤ S⊆V y S .
Since the dual value is bounded above by OPT, this will prove thatπ(closure(D i ∪ D m )) ≤ OPT.
Then we show that closure(C(V, F ′ )) ⊆ closure(D i ∪ D m ), which by the monotonicity of π will prove that the penalty paid by the solution returned by the algorithm is at most OPT.The following lemma states that if two families are tight, then their union is also tight.
This is the same as Corollary 2.2 in [HJ06].
Lemma 4.2.
Let y be a feasible solution to dual program (PCF-D), and S and T be tight families w.r.t. y.
Then S ∪ T is also tight w.r.t. y.Since D m is a union of several tight families, it is tight by Lemma 4.2, i.e.,S∈Dm y S = π(D m ).
Thus, π(closure(D i ∪ D m )) = π(D i ∪ D m ) = π(D m ) = S∈Dm y S ≤ S⊆V y S .
The second equality follows from Lemma 4.1, since the sets in D i are inactive.
The bound on the penalty of the closure of marked and inactive subsets follows.Recall that F ′ is the set of edges finally returned by the algorithm and C(V, F ′ ) denotes the set of connected components in the graph with vertices V and edges F ′ .
We now prove that the penalty incurred by the algorithm, π(closure(C(V, F ′ ))), is no more than π(closure(D i ∪ D m )).
We will prove this bound by showing that C(V, F ′ ) ⊆ closure(D i ∪ D m ).
Since the penalty function is monotonically increasing and the closure function is idempotent, the bound on the penalty of the algorithm will follow.Note that in Algorithm 1, line 23 and 25, we define a series of sets of edgesF = F l ⊇ F l−1 ⊇ · · · ⊇ F 1 ⊇ F 0 = F ′ .
We need to prove that C(V, F j ) ⊆ closure(D i ∪ D m )for all j = l, l−1, . . . , 1, 0.
Since F ′ = F 0 , this will prove the required claim.
We state it as the following lemma.Lemma 4.3.
For all j, C(V, F j ) ⊆ closure(D i ∪ D m ).
This finishes the proof that the penalty paid by the algorithm is at most the cost of the optimal solution.
The minimal augmentation property of the reverse delete step We present another simple property of the reverse delete step, which we will need for proving the final performance guarantee.Lemma 4.4.
If e j cannot be deleted in reverse delete step when it is e j 's turn, then e j cannot be deleted later keeping both endpoint components in closure(D i ∪ D m ).
The contrapositive of Lemma 4.4 tells that if we can delete e j at some later instance than it was considered for deletion, then it could also have been deleted when it was considered.
In particular, F j−1 \ {e 1 , e 2 , . . . , e j−1 } gives a minimal augmentation to the set {e 1 , e 2 , . . . , e j−1 }.
In this section, we will prove that the cost of edges in the output is no more than twice the value of the dual solution.
Let F ′ be the final set of edges output by the algorithm.
We want to show thate∈F ′ c e ≤ 2 − 2 n S⊆V y S .
(4.1)Our proof follows the standard outline given in [GW95].
The only novelty is the proof that all "leaf components" must be active.Since all the edges that we include in our solution are tight, the left hand side can be rewritten as (after changing order of summation)e∈F ′ S:e∈δ(S) y S = S⊆V y S · |F ′ ∩ δ(S)| ≤ 2 − 2 n S⊆V y S .
(4.2)We will prove this by induction on the number of iterations of our algorithm.
The claim is initially true (both sides are zero).
For the induction step, let us say that this inequality holds after i steps of the algorithm and e j−1 was the last edge added to the solution at this point in the algorithm.
Let H ′ j be a graph whose vertices correspond to the connected components C(V, {e 1 , . . . , e j−1 }) and whose edges correspond to the set F ′ \ {e 1 , . . . , e j−1 } = F j−1 \ {e 1 , e 2 , . . . , e j−1 }; that is, edge e in H ′ j joins two vertices if the corresponding edge joins two connected components in C(V, {e 1 , . . . , e j−1 }).
Let C a denote the set of active components in H ′ j and for C ∈ C(V, {e 1 , e 2 , . . . , e j−1 }), let d(C) denote the degree of C in H ′ j .
In the (i + 1)-st iteration, if the dual variables grow by ε, then the left hand side of Equation (4.2) increases by ε × Ca∈Ca d(C a ) and the right hand side increases by ε × 2 − 2 n |C a |.
Therefore proving induction step reduces to provingCa∈Ca d(C a ) ≤ 2 − 2 n |C a |.
(4.3)This is equivalent to proving that the average degree of active components contained in C(V, {e 1 , . . . , e j−1 }) in the graph H ′ j is at most 2 − 2 n .
We will prove that0 or 1 → 0 0 1 → 0 C e C ′ − C C ′ Components of (V, F j−1 )Components of (V, {e 1 , e 2 , . . . , e j−1 }) Figure 1: Illustration of the proof that there are no inactive/marked leaves.there are no non-active leaves in H ′ j , which implies, by a standard argument [GW95], that the average degree of active components is at most (2 − 2/n).
For the sake of contradiction, assume that there is some non-active leaf v having a single incident edge e in H ′ j (see Figure 1 for an example).
Let C be the corresponding component; since we assume C is not active, either it is marked or inactive.
In either case, it is in closure(D m ∪D i ).
By Lemma 4.3, all components of C(V, F j−1 ) (which are same as components of H ′ j since Lemma 4.4, edge e must have been deleted in the reverse delete step, which is a contradiction to the fact that it is in F ′ .
This proves that there are no non-active leaves in H ′ j , hence, the average degree of active components in H ′ j is at most 2 − 2/n, proving the claim and Equation (4.1).
F j−1 = {e 1 , . . . , e j−1 } ∪ F ′ ) are in closure(D m ∪ D i ).
Consider the component C ′ of C(V, F j−1 ) such that C ⊆ C ′ .
Then since C ∈ closure(D m ∪ D i ) and all components of C(V, F j−1 ) ∈ closure(D m ∪ D i ), it must be the case that C ′ − C ∈ closure(D m ∪ D i ).
Since both C and C ′ − C are in closure(D m ∪ D i ), by We now argue that our algorithm can be implemented in polynomial time.
The central difficulty is that of finding which of the 2 2 n type-(f) dual constraints corresponding to family of subsets will next become tight given our dual increase scheme.
We will address this in Section 5.3; however, we need some preliminary discussion first.
Algorithm 1 makes at most 2n iterations of the while statement.
This is because in each iteration of the while loop, we decrease the sum of the number of active components and the number of components.
If an edge becomes tight, then we decrease the total number of components (and may also decrease the number of active components), and if a family of subsets becomes tight, we again decrease the number of active components.
To start with, we have n components, all of which can be active, which gives rise to a bound of 2n.
At any point, the collection of sets that have positive dual form a laminar family (that is, for any pair of sets with positive dual, either the two are disjoint or one contains the other).
This implies that at the end of the algorithm there are at most 2n sets with positive dual.
Each step through the while statement takes polynomial time.
Checking the tightness of an edge e requires us to look at all the components with positive dual and adding the duals which have the edge e in their boundary.
We will show in Section 5.3 how to check the tightness of a family of subsets via binary search and submodular function minimization, which will be implementable in polynomial time.The reverse delete step is at most n iterations and each iteration requires us to check whether two end point components of an edge are in the closure of certain sets.
A polynomial-time algorithm for checking whether a set is in the closure of certain sets is standard and we omit the details in this version of the paper.
Thus the whole reverse delete step also runs in polynomial time.
Let D + denote the family of subsets of V having non-zero dual.
We will prove a property of the family of subsets which ought to go tight next in order to significantly narrow down our search for tight families from 2 2 n candidates to 2 2n candidates.
We show that a family containing a nonactive set whose dual value is zero does not need to be considered for determining a tight family of subsets.Lemma 5.1.
Let S be a family of subsets of V containing a non-active subset S ∈ S with y S = 0.
If S goes tight next while we increase the dual variables of active sets, then there exists a subfamily S ′ of S that will go tight no later than S goes tight.Proof.
Let S = {S 1 , S 2 , . . . , S k } be the family to go tight next the earliest, S k be a non-active subset such that y S k = 0, and the first j subsets of S (without loss of generality) be active sets where 1 ≤ j ≤ k − 1.
We show that the family S ′ = S − {S k } goes tight no later than S. Indeed, the increase in the dual variables needed for S ′ to go tight is ε S ′ = 1 j π(S ′ ) − k−1 i=1 y Si and the increase in the dual variables needed for S to go tight isε S = 1 j π(S) − k i=1 y Si .
Using monotonicity of the function π and the fact that y S k = 0, we conclude that ε S ′ ≤ ε S proving that the family S ′ goes tight no later than the family S. Therefore, when looking for which family becomes tight next, we only need to consider subfamilies of active sets and sets with positive duals (subfamilies of C a ∪D + ).
We now proceed to present a procedure to find which family of subsets goes tight next.5.3 The method to find a tight family We want to find the maximum value of ε such that if we increase all active duals by ε, all constraints of type-(f) in the dual program are still satisfied.
We can replace "all constraints of type-(f)" to "all constraints of type-(f) corresponding to subfamilies of C a ∪ D + " in the above statement, since the first constraint to be violated as we increase dual variables will correspond to a subfamily of C a ∪ D + as proved in Lemma 5.1.
This problem can be written in the form of the following program: maximize ε,s.t. S∈S y S + ε|active(S)| ≤ π(S); ∀S ⊆ C a ∪ D + .
Here, active(S) denotes active sets contained in S. Let ε 0 be the optimum value of ε above.
We use binary search to find the correct value of ε 0 .
We keep a lower bound ε low on the value of ε 0 (take ε low = 0 to start with), and an upper bound ε high on the value of ε 0 (take ε high = π(S)− P S∈S yS |active(S)| for a particular family to start with, say π({S})−yS 1 for some S ∈ C a ).
Since ε 0 ∈ [ε low , ε high ], ε 0 ∈ [ε low , ε middle ] or ε 0 ∈ [ε middle , ε high ]where ε middle = (ε low + ε high )/2.
To determine which interval ε 0 lies in, we use submodular function minimization.
If ε 0 ≥ ε middle , then the minimum of the function π ′ (S) def = π(S) − S∈S y S − ε middle |active(S)| over all S ⊆ C a ∪ D + is nonnegative.
If ε 0 < ε middle , then the minimum of the function π ′ (S) over all subfamilies of C a ∪ D + is strictly negative (because some constraint is violated).
Also note that both the conditions above are if and only if conditions.
Therefore testing whether ε 0 ≥ ε middle reduces to checking whether the minimum of the function π ′ (S) over subfamilies of C a ∪D + is nonnegative.
The function π ′ (S) is a submodular function for a fixed value of ε middle since its first term is submodular and the last two terms are modular.
We apply a polynomial-time submodular function minimization algorithm [IFF01, Sch00] to minimize the function and hence determine which of the two interval ε 0 lies in.
Note that we can do this in oracle polynomial-time and time polynomial in n, since the ground set of π ′ are the sets in C a ∪ D + , and we have previously argued that there are at most 2n such sets.
This further implies that we need at most O(n 2 ) space to query the oracle for π(·).
Once our interval [ε low , ε high ] becomes "sufficiently" small, we stop the binary search and find the right value of ε 0 in that interval.
This is the intuitive idea, which it turns out takes a bit of care to turn into a formal proof.
We need the following lemma to start.Lemma 5.2.
The value of each non-zero dual variable y S and each amount ǫ of dual increase in each iteration of the algorithm can be expressed by rational numbers whose numerators are integers of value at most (max S π(S) + max e∈E c e ) · (6n 2 )!
and whose denominators are integers of value at most (6n 2 )!
.
Thus these variables can be expressed with bit complexity at most O(log(max S π(S)) + log max e∈E c e + n 2 log n).
If we do binary search until the window size |ε high − ε low | is smaller than (1/(6n 2 )!)
2 , there can be at most one rational number in the window with denominator at most (6n 2 )!
.
We now want to determine the exact value of ε in this interval.
We use a procedure Best Approximation from Grötschel, Lovász, and Schrijver [GLS88, Theorem 5.1.9], which determines a rational number β of denominator at most N that minimizes |α − β| for input α.
It does so in time polynomial in the encoding size of N and α.
We run one more submodular minimization to halve the window size of the binary search, then apply this procedure with N = (6n 2 )!
and α set to whichever of ε high or ε low was last modified.
This guarantees that Best Approximation will find ε 0 .
Therefore, finding the right value of ε 0 can be done in time polynomial in log(max S π(S)) + log(max e c e ) + n 2 log n.
The number of iterations of binary search required to bring the interval length down to 1/((6n 2 )!)
2 is at most O(log(max S π(S)) + n 2 log n).
Furthermore, each iteration of binary search requires polynomial time, so the overall time of finding a tight family can be bounded by a polynomial in log(max S π(S)) + log(max e c e ) + n 2 log n.Once we find the right value of ε 0 , we solve the problem of minimizing π(S) − S∈S y S − ε 0 |active(S)| over subfamilies of C a ∪ D + .
As argued above, the minimum value will be zero and the corresponding minimizer family will be the candidate family to go tight next.
Thus, the family to go tight next can be found in polynomial time.
In this section, we present an LP-rounding algorithm with a better performance guarantee when the connectivity requirement function f is proper.
Notice that formulation (PCF-LP) has an exponential number of constraints and a doubly exponential (2 2 n where n = |V |) number of variables, so even solving the LP is a challenging task.
In fact, it is not even clear that a basic solution of (PCF-LP) admits a polynomial-size description -a basic solution of the LP may yet set 2 n of the z S variables to be positive!
Our result is built on three components.
First, we show that by suitably transforming (PCF-LP), one can obtain a compact, convex programming relaxation of the problem that is equivalent to (PCF-LP) and has only x e variables.
Next, we give a simple procedure to round any fractional solution to this convex program to an integer solution losing a factor of at most 2.54.
Since the convex program is equivalent to the LP (PCF-LP), the rounding procedure may also be viewed as a rounding procedure for (PCF-LP) with the property that one only needs the x-component of the fractional solution in order to implement it.
Finally, we show that the convex program can be solved efficiently using the ellipsoid method.
The key ingredient that we need to run the ellipsoid method is a procedure that computes a subgradient of the objective function at any point x. Although the convex program yields a compact relaxation of the prize-collecting problem, this poses several challenges since computing the subgradient at a given point requires solving an exponential-size linear program.
Nevertheless, we will prove certain structural properties and show that a subgradient can indeed be computed in polynomial time.
The idea behind the compact formulation is simple.
Suppose we fix the values of the x e variables.
Then, one can consider an optimal setting of the z S variables corresponding to these x e values and denote the penalty incurred for these z S values (which is a function of only the x e variables) by g(x).
So we can restate the objective function as e c e x e + g(x), and our problem is to minimize e c e x e + g(x) subject to the constraints that 0 ≤ x e ≤ 1 for every edge e.
More formally, we obtain the following program: x e ∀S ⊆ V, z S ≥ 0 ∀S.Here (Pen-P) is the LP that determines the penalty incurred at a fractional solution x.
It is easy to see that (PC-CP) is equivalent to (PCF-LP), and it is straightforward to show that the objective function of (PC-CP) is convex.
Let OPT LP denote the optimal value of (PC-CP) (and (PCF-LP)).
mThe convex program (PC-CP) can be solved efficiently using the ellipsoid method provided that one can find a subgradient of the objective function at any given point.for any feasible x ′ .
It turns out that a subgradient can be computed if we can solve the following dual of (Pen-P).
But this dual has 2 n variables and 2 2 n constraints!
One helpful and important fact is that since π is submodular and the feasible region of (Pen-D) is the polymatroid associated with π, we can use the Edmonds' greedy algorithm [Edm70] to obtain an optimal solution to both (Pen-D) and (Pen-P).
This still does not solve the problem fully since the ground set of the polymatroid is still of size 2 n , and the naive greedy algorithm iterates through all ground elements.
The main idea, now, is to show that there exists an optimal solution to (Pen-P) with at most n non-zero z values and an optimal solution to (Pen-D) with at most n non-zero y values, and in fact, one where the sets S with y S > 0 form a sparse laminar collection.
Using the solution y, we can compute a subgradient of h in (PC-CP) in polynomial time, and thereby implement the ellipsoid method.
Note that we do not require f to be proper in order to solve (PC-CP); this is only needed for the rounding argument.
The algorithm to compute an optimal dual solution is as follows.
For a set S and collection A, let confl(S, A) denote the number of "conflicts" between S and A, that is, |{S ′ ∈ A : S ∩ S ′ 񮽙 = ∅, S \ S ′ 񮽙 = ∅, S ′ \ S 񮽙 = ∅}|.
We initialize y S ← 0 for all sets S, and L ← ∅.
WhileWe very briefly argue the correctness of the algorithm; the details are deferred to the full version of the paper.
One way to prove this is to show that the above algorithm is in a sense equivalent to a run of Edmonds' greedy algorithm.
The above algorithm considers sets in increasing order of x(δ(S)) value, instead of decreasing f (S) − x(δ(S)) value as required by the greedy algorithm.
But one can show that ordering sets in this way, and setting their values as in the greedy algorithm (so as to satisfy (6.4)), also works.
Intuitively, this is because a set S with f (S) = 0 adds 0 value to the penalty of any collection, so these sets can be "swapped around" without affecting optimality.
Next, one can show (Lemma 6.4) that although the algorithm only considers sets that preserve the laminarity of L, there is an underlying valid ordering of the sets (by increasing x(δ(.))
value) that yields the same solution y. Combining these two properties shows that the solution constructed y is optimal to (Pen-D).
Let L = {S 1 , . . . , S ℓ }, ℓ ≤ n, where x(δ(S 1 )) ≤ . . . ≤ x(δ(S ℓ )).
Using this laminar family, one can construct a sparse optimal solution to (Pen-P) that satisfies the properties stated in Section 6.2.
Let L >0 = {S ∈ L : y S > 0} = {T 1 , . . . , T k }, where T j = S i(j) and i(0) := 0 < i(1) ≤ . . . ≤ i(k) < i(k + 1) := ℓ + 1.
Let S i = {S 1 , . . . , S i } for i = 0, . . . , ℓ (with S 0 := ∅).
Define S 0 = ∅ = T 0 , and let T j = z-clos(S i(j) ), w j := x(δ(T j )) for j = 0, . . . , k, and w k+1 := 1.
Theorem 6.2.
(i) y is an optimal solution to (Pen-D).
(ii) The solution z = (z S ) S⊆2 V given by z Tj = (w j+1 − w j ) for j = 0, . . . , k, and z S = 0 for any other S ⊆ 2 V is an optimal solution to (Pen-P).
AcknowledgmentsThe first author would like to thank Ara Hayrapetyan and Zoya Svitkina for useful discussions.
The convex program (PC-CP) can be solved efficiently using the ellipsoid method provided that one can find a subgradient of the objective function at any given point.for any feasible x ′ .
It turns out that a subgradient can be computed if we can solve the following dual of (Pen-P).
But this dual has 2 n variables and 2 2 n constraints!
One helpful and important fact is that since π is submodular and the feasible region of (Pen-D) is the polymatroid associated with π, we can use the Edmonds' greedy algorithm [Edm70] to obtain an optimal solution to both (Pen-D) and (Pen-P).
This still does not solve the problem fully since the ground set of the polymatroid is still of size 2 n , and the naive greedy algorithm iterates through all ground elements.
The main idea, now, is to show that there exists an optimal solution to (Pen-P) with at most n non-zero z values and an optimal solution to (Pen-D) with at most n non-zero y values, and in fact, one where the sets S with y S > 0 form a sparse laminar collection.
Using the solution y, we can compute a subgradient of h in (PC-CP) in polynomial time, and thereby implement the ellipsoid method.
Note that we do not require f to be proper in order to solve (PC-CP); this is only needed for the rounding argument.
The algorithm to compute an optimal dual solution is as follows.
For a set S and collection A, let confl(S, A) denote the number of "conflicts" between S and A, that is, |{S ′ ∈ A : S ∩ S ′ 񮽙 = ∅, S \ S ′ 񮽙 = ∅, S ′ \ S 񮽙 = ∅}|.
We initialize y S ← 0 for all sets S, and L ← ∅.
WhileWe very briefly argue the correctness of the algorithm; the details are deferred to the full version of the paper.
One way to prove this is to show that the above algorithm is in a sense equivalent to a run of Edmonds' greedy algorithm.
The above algorithm considers sets in increasing order of x(δ(S)) value, instead of decreasing f (S) − x(δ(S)) value as required by the greedy algorithm.
But one can show that ordering sets in this way, and setting their values as in the greedy algorithm (so as to satisfy (6.4)), also works.
Intuitively, this is because a set S with f (S) = 0 adds 0 value to the penalty of any collection, so these sets can be "swapped around" without affecting optimality.
Next, one can show (Lemma 6.4) that although the algorithm only considers sets that preserve the laminarity of L, there is an underlying valid ordering of the sets (by increasing x(δ(.))
value) that yields the same solution y. Combining these two properties shows that the solution constructed y is optimal to (Pen-D).
Let L = {S 1 , . . . , S ℓ }, ℓ ≤ n, where x(δ(S 1 )) ≤ . . . ≤ x(δ(S ℓ )).
Using this laminar family, one can construct a sparse optimal solution to (Pen-P) that satisfies the properties stated in Section 6.2.
Let L >0 = {S ∈ L : y S > 0} = {T 1 , . . . , T k }, where T j = S i(j) and i(0) := 0 < i(1) ≤ . . . ≤ i(k) < i(k + 1) := ℓ + 1.
Let S i = {S 1 , . . . , S i } for i = 0, . . . , ℓ (with S 0 := ∅).
Define S 0 = ∅ = T 0 , and let T j = z-clos(S i(j) ), w j := x(δ(T j )) for j = 0, . . . , k, and w k+1 := 1.
Theorem 6.2.
(i) y is an optimal solution to (Pen-D).
(ii) The solution z = (z S ) S⊆2 V given by z Tj = (w j+1 − w j ) for j = 0, . . . , k, and z S = 0 for any other S ⊆ 2 V is an optimal solution to (Pen-P).
The first author would like to thank Ara Hayrapetyan and Zoya Svitkina for useful discussions.
