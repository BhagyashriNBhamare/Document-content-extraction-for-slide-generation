The modern Linux IO stack is a collection of the arbitration layers; the IO scheduler, the command queue manager, and the writeback cache manager shuffle the incoming requests at their own disposal before passing them to the next layers.
Despite the compound uncertainties from the multiple layers of arbitration, it is essential for the software writers to enforce a certain order in which the data blocks are reflected to the storage surface, storage order, in many cases such as in guaranteeing the durability and the atomicity of a database transaction [47,26,35], in filesystem journaling [67,41,65,4], in soft-update [42,63], or in copy-onwrite or log-structure filesystems [61,35,60,31].
Enforcing a storage order is achieved by an extremely expensive approach: dispatching the following request only * This work was done while the author was a graduate student at Hanyang University.
after the data block associated with the preceding request is completely transferred to the storage device and is made durable.
We call this mechanism a Transfer-andFlush.
For decades, interleaving the write requests with a Transfer-and-Flush has been the fundamental principle to guarantee the storage order in a set of requests [24,16].
We observe a phenomenal increase in the performance and the capacity of the Flash storage.
The performance increase owes much to the concurrency and the parallelism in the Flash storage, e.g. the multi-channel/way controller [73,6], the large size storage cache [48], and the deep command queue [19,27,72].
A state of the art NVMe SSD reportedly exhibits up to 750 KIOPS random read performance [72].
It is nearly 4,000× of a HDD's performance.
The capacity increase is due to the adoption of the finer manufacturing process (sub-10 nm) [25,36], and the multi-bits per cell (MLC, TLC, and QLC) [5,11].
Meanwhile, the time to program a Flash cell has barely improved, and is even deteriorating in some cases [22].
The Transfer-and-Flush based order-preserving mechanism conflicts with the parallelism and the concurrency in the Flash storage.
It disables the parallelism and the concurrency feature of the Flash storage and exposes the raw Flash cell programming latency to the host.
The overhead of the Transfer-and-Flush mechanism will become more significant as the Flash storage employs a higher degree of parallelism and the denser Flash device.
Fig. 1 illustrates an important trend.
We measure the sustained throughput of orderless random write (plain buffered write) and the ordered random write in EXT4 filesystem.
In ordered random write, each write request is followed by fdatasync().
X-axis denotes the throughput of orderless write which corresponds to the rate at which the storage device services the write requests at its full throttle.
This usually matches the vendor published performance of the storage device.
The number next to each point denotes the sustained throughput of the ordered write.
The Y-axis denotes the ratio between the two.
In a single channel mobile storage for smartphone (SSD A), the performance of ordered write is 20% of that of unordered write (1351 IOPS vs. 7000 IOPS).
In a thirty-two channel Flash array (SSD G), this ratio decreases to 1% (2296 IOPS vs. 230K IOPS).
In SSD with supercap (SSD E), the ordered write performance is 25% of that of the unordered write.
The Flash storage uses supercap to hide the flush latency from the host.
Even in a Flash storage with supercap, the overhead of Transferand-Flush is significant.Many researchers have attempted to address the overhead of storage order guarantee.
The techniques deployed in the production platforms include non-volatile writeback cache at the Flash storage [23], no-barrier mount option at the EXT4 filesystem [15], and transactional checksum [56,32,64].
Efforts such as transactional filesystem [50,18,54,35,68] and transactional block device [30,74,43,70,52] save the application from the overhead of enforcing the storage order associated with filesystem journaling.
A school of work address more fundamental aspects in controlling the storage order, such as separating the ordering guarantee from durability guarantee [9], providing a programming model to define the ordering dependency among the set of writes [20], and persisting a data block only when the result needs to be externally visible [49].
Despite their elegance, these works rely on Transfer-and-Flush when it is required to enforce the storage order.
OptFS [9] relies on Transfer-and-Flush in enforcing the order between the journal commit and the associated checkpoint.
Featherstitch [20] relies on Transfer-and-Flush to implement the ordering dependency between the patchgroups.In this work, we revisit the issue of eliminating the Transfer-and-Flush overhead in the modern IO stack.
We develop a Barrier-Enabled IO stack, in which the filesystem can issue the following request before the preceding request is serviced and yet the IO stack can enforce the storage order between them.
The barrier-enabled IO stack consists of the cache barrier-aware storage device, the order-preserving block device layer, and the barrier enabled filesystem.
For cache barrier-aware storage device, we exploit the "cache barrier" command [28].
The barrier-enabled IO stack is built upon the foundation that the host can control a certain partial order in which the cache contents are flushed.
The "cache barrier" command precisely serves this purpose.
For the order-preserving block device layer, the command dispatch mechanism and the IO scheduler are overhauled so that the block device layer ensures that the IO requests from the filesystem are serviced preserving a certain partial order.
For the barrier-enabled filesystem, we define new interfaces, fbarrier() and fdatabarrier(), to separate the ordering guarantee from the durability guarantee.
They are similar to fsync() and fdatasync(), respectively, except that they return without waiting for the associated blocks to become durable.
We modify EXT4 for the order-preserving block device layer.
We develop dual-mode journaling for the order-preserving block device.
Based upon the dual-mode journaling, we newly implement fbarrier() and fdatabarrier() and rewrite fsync().
Barrier-enabled IO stack removes the flush overhead as well as the transfer overhead in enforcing the storage order.
While large body of the works have focused on eliminating the flush overhead, few works have addressed the overhead of DMA transfer to enforce the storage order.
The benefits of the barrier-enabled IO stack include the followings;• The application can control the storage order virtually without any overheads, including the flush overhead, DMA transfer overhead, and context switch.
• The latency of a journal commit decreases significantly.
The journaling module can enforce the storage order between the journal logs and the journal commit block without interleaving them with flush or with DMA transfer.
• Throughput of the filesystem journaling improves significantly.
The dual-mode journaling commits multiple transactions concurrently and yet can guarantee the durability of the individual journal commit.By eliminating the Transfer-and-Flush overhead, the barrier-enabled IO stack successfully exploits the concurrency and the parallelism in modern Flash storage.
The device is available if the command queue is not full.
The storage controller inserts the incoming command at the command queue.
The storage controller removes the command from the command queue and services it ( i.e. transfers the associated data block between the host and the storage).
When the transfer finishes, the device signals the host.
The contents of the writeback cache are committed to the storage surface either periodically or by an explicit request from the host.
We define four types of orders in the IO stack; Issue Order, I , Dispatch Order, D, Transfer Order, X , and Persist Order, P.
The issue order I = {i 1 , i 2 , . . . , i n } is a set of write requests issued by the file system.
The subscript denotes the order in which the requests enter the IO scheduler.
The dispatch order D = {d 1 , d 2 , . . . , d n } denotes a set of the write requests dispatched to the storage device.
The subscript denotes the order in which the requests leave the IO scheduler.
The transfer order, X = {x 1 , x 2 , . . . , x n }, is the set of transfer completions.
The persist order, P = {p 1 , p 2 , . . . , p n }, is a set of operations that make the data blocks in the writeback cache durable.
We say that a partial order is preserved if the relative position of the requests against a designated request, barrier, are preserved between two different types of orders.
We use the notation '=' to denote that a partial order is preserved.
The partial orders between the different types of orders may not coincide due to the following reasons.
• I = D.
The IO scheduler reorders and coalesces the IO requests subject to the scheduling principle, e.g. CFQ, DEADLINE, etc.
When there is no scheduling mechanism, e.g. NO-OP scheduler [3] or NVMe [13] interface, the dispatch order may be equal to the issue order.
• D = X .
The storage controller can freely schedule the commands in its command queue.
In addition, the commands can be serviced out-of-order due to the errors, the time-outs, and the retry.
• X = P.
The writeback cache of the storage is not FIFO.
In Flash storage, persist order is governed not by the order in which the data blocks are made durable but by the order in which the associated mapping table entries are updated.
The two may not coincide.Due to all these uncertainties, the modern IO stack is said to be orderless [8].
Enforcing a storage order corresponds to preserving a partial order between the order in which the filesystem issues the requests, I , and the order in which the associated data blocks are made durable, P.
It is equivalent to collectively enforcing the partial orders between the pair of the orders in the adjacent layers in Fig. 2.
It can be formally represented as in Eq.
1.
(I = P) ≡ (I = D) ∧ (D = X ) ∧ (X = P) (1)The modern IO stack has evolved under the assumption that the host cannot control the persist order, i.e. X = P.
This is due to the physical characteristics of the rotating media.
For rotating media such as HDDs, a persist order is governed by disk scheduling algorithm.
The disk scheduling is entirely left to the storage controller due to its complicated sector geometry which is hidden from outside [21].
When the host blindly enforces a certain persist order, it may experience anomalous delay in IO service.
Due to this constraint of X = P, Eq.
1 is unsatisfiable.
The constraint that the host cannot control the persist order is a fundamental limitation in modern IO stack design.
The block device layer adopts the indirect and the expensive approach to control the storage order in spite of the constraint X = P. First, after dispatching the write command to the storage device, the caller postpones dispatching the following command until the preceding command is serviced, i.e. until the associated DMA transfer completes.
We refer to this mechanism as Wait-on-Transfer.
Wait-on-Transfer mechanism ensures that the commands are serviced in order and to satisfy D = X .
Wait-on-Transfer is expensive; it blocks the caller and interleaves the requests with DMA transfer.
Second, when the preceding command is serviced, the caller issues the flush command and waits for its completion.
The caller issues the following command only after the flush command returns.
This is to ensure that the associated data blocks are persisted in order and to satisfy X = P.
We refer to this mechanism as Waiton-Flush.
The modern block device layer uses Wait-onTransfer and Wait-on-Flush in pair when it needs to enforce the storage order between the write requests.
We call this mechanism as Transfer-and-Flush.
The cost of Transfer-and-Flush is prohibitive.
It neutralizes the internal parallelism of the Flash storage controller, stalls the command queue, and exposes the caller to DMA transfer and raw cell programming delays.
We examine how the EXT4 filesystem controls the storage order in an fsync().
In Ordered journaling mode (default), the data blocks are persisted before the journal transaction.
Fig. 3 illustrates the behavior of an fsync().
The filesystem issues the write requests for a set of dirty pages, D. D may consist of the data blocks from different files.
After issuing the write requests, the application thread blocks waiting for the completion of the DMA transfer.
When the DMA transfer completes, the application thread resumes and triggers the JBD thread to commit the journal transaction.
After triggering the JBD thread, the application thread sleeps again.
When the JBD thread makes journal transaction durable, the fsync() returns.
It should be emphasized that the application thread triggers the JBD thread only after D is transferred.
Otherwise, the storage controller may service the write request for D and the write requests for journal commit in an out-of-order manner, and the storage controller may persist the journal transaction prematurely (before D is transferred).
A journal transaction is usually committed using two write requests: one for writing the coalesced chunk of the journal descriptor block and the log blocks and the other for writing the commit block.
In the rest of the paper, we will use JD and JC to denote the coalesced chunk of the journal descriptor and the log blocks, and the commit block, respectively.
In committing a journal transaction, JBD needs to enforce the storage orders in two relations: within a transaction and between the transactions.
Within a transaction, JBD needs to ensure that JD is made durable ahead of JC.
Between the journal transactions, JBD has to ensure that journal transactions are made durable in order.
When either of the two conditions are violated, the file system may recover incorrectly in case of unexpected failure [67,9].
For the storage order within a transaction, JBD interleaves the write request for JD and the write request for JC with Transfer-andFlush.
To control the storage order between the transactions, JBD thread waits for JC to become durable before it starts committing the following transaction.
JBD uses Transfer-and-Flush mechanism in enforcing both intratransaction and inter-transaction storage order.In earlier days of Linux, the block device layer explicitly issued a flush command in committing a journal transaction [15].
In this approach, the flush command blocks not only the caller but also the other requests in the same dispatch queue.
Since Linux 2.6.37, the filesystem (JBD) implicitly issues a flush command [16].
In writing JC, JBD tags the write request with REQ FLUSH and REQ FUA.
Most storage controllers have evolved to support these two flags; with these two flags, the storage controller flushes the writeback cache before servicing the command and in servicing the command it directly persists JC to storage surface bypassing the writeback cache.
In this approach, only the JBD thread blocks and the other threads that share the same dispatch queue can proceed.
Our effort can be thought as a continuation to this evolution of the IO stack.
We mitigate the Transferand-Flush overhead by making the storage device more capable: supporting a barrier command and by redesigning the host side IO stack accordingly.
The order-preserving block device layer consists of the newly defined barrier write command, order-preserving dispatch module, and Epoch-based IO scheduler.
We overhaul the IO scheduler, the dispatch module, and the write command so that they can preserve the partial order between the different types of orders, I = D, D = X , and X = P, respectively.
Order-preserving dispatch module eliminates the Wait-on-Transfer overhead and the newly defined barrier write command eliminates the wait-on-flush overhead.
They collectively together preserve the partial order between the issue order I and the persist order P without Transfer-and-Flush.
The order-preserving block device layer categorizes the write requests into two categories, orderless write and order-preserving write.
The order-preserving requests are the ones that are subject to the storage ordering constraint.
Orderless request is the one which is irrelevant to the ordering dependency and which can be scheduled freely.
We distinguish the two to avoid imposing unnecessary ordering constraint in scheduling the requests.
The details are to come shortly.
We refer to a set of the order-preserving requests that can be reordered with each other as an epoch [14].
We define a special type of order-preserving write as a barrier write.
A barrier write is used to delimit an epoch.
We introduce two new attributes REQ ORDERED and REQ BARRIER for the bio object and the request object to represent an order-preserving write and a barrier write.
REQ ORDERED attribute is used to specify the order-preserving write.
Barrier write request has both REQ ORDERED and REQ BARRIER attributes.
The orderpreserving block device layer handles the request differently based upon its category.
Fig. 4 illustrates the organization of Barrier-Enabled IO stack.
The "cache barrier," or "barrier" for short, command is defined in the standard command set for mobile Flash storage [28].
With barrier command, the host can control the persist order without explicitly invoking the cache flush.
When the storage controller receives the barrier command, the controller guarantees that the data blocks transferred before the barrier command becomes durable after the ones that follow the barrier command do.
A few eMMC products in the market support cache barrier command [1,2].
The barrier command can satisfy the condition X = P in Eq.
1 which has been unsatisfiable for several decades due to the mechanical characteristics of the rotating media.
The naive way of using barrier is to replace the existing flush operation [66].
This simple replacement still leaves the caller under the Waiton-Transfer overhead to enforce the storage order.Implementing a barrier as a separate command occupies one entry in the command queue and costs the host the latency of dispatching a command.
To avoid this overhead, we define a barrier as a command flag.
We designate one unused bit in the SCSI command for a barrier flag.
We set the barrier flag of the write command to make itself a barrier write.
When the storage controller receives a barrier write command, it services the barrier write command as if the barrier command has arrived immediately following the write command.With reasonable complexity, the Flash storage can be made to support a barrier write command [30,57,39].
When the Flash storage has Power Loss Protection (PLP) feature, e.g. a supercapacitor, the writeback cache contents are guaranteed to be durable.
The storage controller can flush the writeback cache fully utilizing its parallelism and yet can guarantee the persist order.
In Flash storage with PLP, we expect that the performance overhead of the barrier write is insignificant.For the devices without PLP, the barrier write command can be supported in three ways; in-order writeback, transactional writeback, or in-order recovery.
In in-order writeback, the storage controller flushes the data blocks in epoch granularity.
The amount of data blocks in an epoch may not be large enough to fully utilize the parallelism of the Flash storage.
The in-order writeback style of the barrier write implementation can bring the performance degradation in cache flush.
In transactional writeback, the storage controller flushes the writeback cache contents as a single unit [57,39].
Since all epochs in the writeback cache are flushed together, the persist order imposed by the barrier command is satisfied.
The transactional writeback can be implemented without any performance overhead if the controller exploits the spare area of the Flash page to represent a set of pages in a transaction [57].
The in-order recovery method relies on a crash recovery routine to control the persist order.
When multiple controller cores concurrently write the data blocks to multiple channels, one may have to use sophisticated crash recovery protocol such as ARIES [46] to recover the storage to consistent state.
If the entire Flash storage is treated as a single log device, we can use simple crash recovery algorithm used in LFS [61].
Since the persist order is enforced by the crash recovery logic, the storage controller can flush the writeback cache at the full throttle as if there is no ordering dependency.
The controller is saved from performance penalty at the cost of complexity in the recovery routine.In this work, we modify the firmware of the UFS storage device to support the barrier write command.
We use a simple LFS style in-order recovery scheme.
The modified firmware is loaded at the commercial UFS product of the Galaxy S6 smartphone 1 .
The modified firmware treats the entire storage device as a single log structured device.
It maintains an active segment in memory.
FTL appends incoming data blocks to the active segment in the order in which they are transferred.
When an active segment becomes full, the controller stripes the active segment across the multiple Flash chips in log-structured manner.
In crash recovery, the UFS controller locates the beginning of the most recently flushed segment.
It scans the pages in the segment from the beginning till it encounters the page that has not been programmed successfully.
The storage controller discards the rest of the pages including the incomplete one.Developing a barrier-enabled SSD controller is an engineering exercise.
It is governed by a number of design choices and should be addressed in a separate context.
In this work, we demonstrate that the performance benefit achieved by the barrier command well deserves its complexity if the host side IO stack can properly exploit it.
Order-preserving dispatch is a fundamental innovation in this work.
In order-preserving dispatch, the block device layer dispatches the following command immediately after it dispatches the preceding one ( Fig. 5) and yet the host can ensure that the two commands are serviced in order.
We refer to this mechanism as Wait-on-Dispatch.
The order-preserving dispatch is to satisfy the condition D = X in Eq.
1 without Wait-on-Transfer overhead.The dispatch module constructs a command from the requests.
The dispatch module constructs the barrier write command when it encounters the barrier write request, i.e. the write request with REQ ORDERED and REQ BARRIER flags.
For the other requests, it constructs the commands as it used to do in the legacy block device.Implementing an order-preserving dispatch is rather simple; the block device driver sets the priority of a barrier write command as ordered.
Then, the SCSI compliant storage device services the command satisfying the ordering constraint.
The following is the reason.
SCSI standard defines three command priority levels: head of the queue, ordered, and simple [59].
With each, the storage controller puts the incoming command at the head of the command queue, at the tail of the command queue or at an arbitrary position determined at its disposal, respectively.
The default priority is simple.
The command with simple priority cannot be inserted in front of the existing ordered or head of the queue command.
Exploiting the command priority of existing SCSI interface, the order-preserving dispatch module ensures that the barrier write is serviced only after the existing requests in the command queue are serviced and before any of the commands that follow the barrier write are serviced.The device can temporarily be unavailable or the caller can be switched out involuntarily after dispatching a write request.
The order-preserving dispatch module uses the same error handling routine of the existing block device driver; the kernel daemon inherits the task and retries the failed request after a certain time interval, e.g. 3 msec for SCSI devices [59].
The ordered priority command has rarely been used in the existing block device implementations.
This is because when the host cannot control the persist order, enforcing a transfer order with ordered priority command barely carries any meaning from the perspective of ensuring the storage order.
In the emergence of the barrier write, the ordered priority plays an essential role in making the entire IO stack an order-preserving one.The importance of order-preserving dispatch cannot be emphasized further.
With order-preserving dispatch, the host can control the transfer order without releasing the CPU and without stalling the command queue.
IO latency can become more predictable since there exists less chance that the CPU scheduler interferes with the caller's execution.
∆ WoT and ∆ WoD in Fig. 5 illustrate the delays between the consecutive requests in Wait-onTransfer and Wait-on-Dispatch, respectively.
In Wait-onDispatch, the host issues the next request W i+1 (WoD) immediately after it issues W i .
In Wait-on-Transfer, the host issues the next request W i+1 (WoT ) only after W i is serviced.
∆ WoD is an order of magnitude smaller than ∆ WoT .
Epoch-based IO scheduling is designed to preserve the partial order between the issue order and the dispatch order.
It satisfies the condition I = D.
It is designed with three principles; (i) it preserves the partial order between the epochs, (ii) the requests within an epoch can be freely scheduled with each other, and (iii) an orderless request can be scheduled across the epochs.When an IO request enters the scheduler queue, the IO scheduler determines if it is a barrier write.
If the request is a barrier write, the IO scheduler removes the barrier flag from the request and inserts it into the queue.
Otherwise, the scheduler inserts it to the queue as is.
When the scheduler inserts a barrier write to the queue, it stops accepting more requests.
Since the scheduler blocks the queue after it inserts the barrier write, all orderpreserving requests in the queue belong to the same epoch.
The requests in the queue can be freely re-ordered and merged with each other.
The IO scheduler uses the existing scheduling discipline, e.g. CFQ.
The merged request will be order-preserving if one of the components is order-preserving request.
The IO scheduler designates the last order-preserving request that leaves the queue as a new barrier write.
This mechanism is called EpochBased Barrier Reassignment.
When there are not any order-preserving requests in the queue, the IO scheduler starts accepting the IO requests again.
When the IO scheduler unblocks the queue, there can be one or more orderless requests in the queue.
These orderless requests are scheduled with the requests in the following epoch.
Differentiating orderless requests from the orderpreserving ones, we avoid imposing unnecessary ordering constraint on the irrelevant requests.
Fig. 6 illustrates an example.
The circle and the rectangle that enclose the write request denote the order-preserving flag and barrier flag, respectively.
An The order-preserving block device layer now satisfies all three conditions, I = D, D = X and X = P in Eq.
1 with an Epoch-based IO scheduling, an orderpreserving dispatch and a barrier write, respectively.
The order-preserving block device layer successfully eliminates the Transfer-and-Flush overhead in controlling the storage order and can control the storage order with only Wait-on-Dispatch overhead.
The barrier-enabled IO stack offers four synchronization primitives: fsync(), fdatasync(), fbarrier(), and fdatabarrier().
We propose two new filesystem interfaces, fbarrier() and fdatabarrier(), to separately support ordering guarantee.
fbarrier() and fdatabarrier() synchronize the same set of blocks with fsync() and fdatasync(), respectively, but they return without ensuring that the associated blocks become durable.
fbarrier() bears the same semantics as osync() in OptFS [9] in that it writes the data blocks and the journal transactions in order but returns without ensuring that they become durable.fdatabarrier() synchronizes the modified blocks, but not the journal transaction.
Unlike fdatasync(), fdatabarrier() returns without persisting the associated blocks.
fdatabarrier() is a generic storage barrier.
By interleaving the write() calls with fdatabarrier(), the application ensures that the data blocks associated with the write requests that precede fdatabarrier() are made durable ahead of the data blocks associated with the write requests that follow fdatabarrier().
It plays the same role as mfence for memory barrier [53].
Refer to the following codelet.
Using fdatabarrier(), the application ensures that the "world" is made durable only after "Hello" does.write(fileA, "Hello") ; fdatabarrier(fileA) ; write(fileA, "World") ;The order-preserving block device layer is filesystem agnostic.
In our work, we modify EXT4 for barrier enabled IO stack.
Committing a journal transaction essentially consists of two saparate tasks: (i) dispatching the write commands for JD and JC and (ii) making JD and JC durable.
Exploiting the order-preserving nature of the underlying block device, we physically separate the control plane activity (dispatching the write requests) and the data plane activity (persisting the associated data blocks and journal transaction) of a journal commit operation.
Further, we allocate the separate threads to each task so that the two activities can proceed in parallel with minimum dependency.
The two threads are called as commit thread and flush thread, respectively.
We refer to this mechanism as Dual Mode Journaling.
Dual Mode Journaling mechanism can support two journaling modes, durability guarantee mode and ordering guarantee mode, in versatile manner.
The commit thread is responsible for dispatching the write requests for JD and JC.
The commit thread writes each of the two with a barrier write so that JD and JC are persisted in order.
The commit thread dispatches the write requests without any delay in between ( Fig. 7(b)).
In EXT4, JBD thread interleaves the write request for JC and JD with Transfer-and-Flush ( Fig. 7(a)).
After dispatching the write request for JC, the commit thread inserts the journal transaction to the committing transaction list and hands over the control to the flush thread.The flush thread is responsible for (i) issuing the flush command, (ii) handling error and retry and (iii) removing the transaction from the committing transaction list.
The behavior of the flush thread varies subject to the durability requirement of the journal commit.
If the journal commit is triggered by fbarrier(), the flush thread returns after removing the transaction from the committing transaction list.
It returns without issuing the flush command.
If the journal commit is triggered by fsync(), the flush thread involves more steps.
It issues a flush command and waiting for the completion.
When the flush completes, it removes the the associated transaction from the committing transaction list and returns.
BarrierFS supports all journal modes in EXT4; WRITEBACK, OR-DERED and DATA.The dual thread organization of BarrierFS journaling bears profound implications in filesystem design.
First, the separate support for the ordering guarantee and the durability guarantee naturally becomes an integral part of the filesystem.
Ordering guarantee involves only the control plane activity.
Durability guarantee requires the control plane activity as well as data plane activity.
BarrierFS partitions the journal commit activity into two independent components, control plane and data plane and dedicates separate threads to each.
This modular design enables the filesystem primitives to adaptively adjust the activity of the data plane thread with respect to the durability requirement of the journal commit operation; fsync() vs. fbarrier().
Second, the filesystem journaling becomes concurrent activity.
Thanks to the dual thread design, there can be multiple committing transactions in flight.
In most journaling filesystems that we are aware of, the filesystem journaling is a serial activity; the journaling thread commits the following transaction only after the preceding transaction becomes durable.
In dual thread design, the commit thread can commit a new journal transaction without waiting for the preceding committing transaction to become durable.
The flush thread asynchronously notifies the application thread about the completion of the journal commit.
In fbarrier() and fsync(), BarrierFS writes D, JD, and JC in a piplelined manner without any delays in between ( Fig. 7(b)).
BarrierFS writes D with one or more order-preserving writes whereas it writes JD and JC with the barrier writes.
In this manner, BarrierFS forms two epochs {D, JD} and {JC} in an fsync() or in an fbarrier() and ensures the storage order between these two epochs.
fbarrier() returns when the filesystem dispatches the write request for JC.
fsync() returns after it ensures that JC is made durable.
Order-preserving block device satisfies prefix constraint [69].
When JC becomes durable, the order-preserving block device guarantees that all blocks associated with preceding epochs have been made durable.
An application may repeatedly call fbarrier() committing multiple transactions simultaneously.
By writing JC with a barrier write, BarrierFS ensures that these committing transactions become durable in order.
The latency of an fsync() reduces significantly in BarrierFS.
It reduces the number of flush operations from two in EXT4 to one and eliminates the Wait-on-Transfer overheads (Fig. 7).
In fdatabarrier() and fdatasync(), BarrierFS writes D with a barrier write.
If there are more than one write requests in writing D, only the last one is set as a barrier write and the others are set as the orderpreserving writes.
An fdatasync() returns after the data blocks, D, become durable.
An fdatabarrier() returns immediately after dispatching the write requests for D. fdatabarrier() is the crux of the barrierenabled IO stack.
With fdatabarrier(), the application can control the storage order virtually without any overheads: without waiting for the flush, without waiting for DMA completion, and even without the context switch.
fdatabarrier() is a very light-weight storage barrier.An fdatabarrier() (or fdatasync()) may not find any dirty pages to synchronize upon its execution.
In this case, BarrierFS explicitly triggers the journal commit.
It forces BarrierFS to issue the barrier writes for JD and JC.
Through this mechanism, fdatabarrier() or fdatasync() can delimit an epoch as desired by the application even in the absence of any dirty pages.
A buffer page may have been held by the committing transaction when an application tries to insert it to the running transaction.
We refer to this situation as page conflict.
Blindly inserting a conflict page into the running transaction yields its removal from the committing transaction before it becomes durable.
The EXT4 filesystem checks for the page conflict when it inserts a buffer page to the running transaction [67].
If the filesystem finds a conflict, the thread delegates the insertion to the JBD thread and blocks.
When the committing transaction becomes durable, the JBD thread identifies the conflict pages in the committed transaction and inserts them to the running transaction.
In EXT4, there can be at most one committing transaction.
The running transaction is guaranteed to be free from page conflict when the JBD thread has made it durable and finishes inserting the conflict pages to the running transaction.In BarrierFS, there can be more than one committing transactions.
The conflict pages may be associated with different committing transactions.
We refer to this situation as multi-transaction page conflict.
As in EXT4, BarrierFS inserts the conflict pages to the running transaction when it makes a committing transaction durable.
However, to commit a running transaction, BarrierFS has to scan all buffer pages in the committing transactions for page conflicts and ensure that it is free from any page conflicts.
When there exists large number of committing transactions, the scanning overhead to check for the page conflict can be prohibitive in BarrierFS.To reduce this overhead, we propose the conflict-page list for a running transaction.
The conflict-page list represents the set of conflict pages associated with a running transaction.
The filesystem inserts the buffer page to the conflict-page list when it finds that the buffer page that it needs to insert to the running transaction is subject to the page conflict.
When the filesystem has made a committing transaction durable, it removes the conflict pages from the conflict-page list in addition to inserting them to the running transaction.
A running transaction can only be committed when the conflict-page list is empty.
We examine the degree of concurrency in journal commit operation under different storage order guarantee mechanisms: BarrierFS, EXT4 with no-barrier option (EXT4 (no flush)), EXT4 with supercap SSD (EXT4 (quick flush)), and plain EXT4 (EXT4 (full flush)).
With no-barrier mount option, the JBD thread omits the flush command in committing a journal transaction.
With this option, the EXT4 guarantees neither durability nor ordering in journal commit operation since the storage controller may make the data blocks durable out-oforder.
We examine this configuration to illustrate the filesystem journaling behavior when the flush command is removed in the journal commit operation.
In Fig. 8, each horizontal line segment represents a journal commit activity.
It consists of the solid line segment and the dashed line segment.
The end of the horizontal line segment denotes the time when the transaction reaches the disk surface.
The end of the solid line segment represents the time when the journal commit returns.
If they do not coincide, it means that the journal commit finishes before the transaction reaches the disk surface.
In EXT4 (full flush), EXT4 (quick flush), and EXT4 (no flush), the filesystem commits a new transaction only after preceding journal commit finishes.
The journal commit is a serial activity.
In EXT4 (full flush), the journal commit finishes only after all associated blocks are made durable.
In EXT4 (quick flush), the journal commit finishes more quickly than in EXT4 (full flush) since the SSD returns the flush command without persisting the data blocks.
In EXT4 (no flush), the journal commit finishes more quickly than EXT (quick flush) since it does not issue the flush command.
In journaling throughput, BarrierFS prevails the remainders by far since the interval between the consecutive journal commits is as small as the dispatch latency, t D .
The concurrencies in journaling in EXT4 (no flush) and in EXT4 (quick flush) have their price.
EXT4 (quick flush) requires the additional hardware component, supercap, in the SSD.
EXT4 (quick flush) guarantees neither durability or ordering in the journal commit.
BarrierFS commits multiple transactions concurrently and yet can guarantee the durability of the individual journal commit without the assistance of additional hardware.The barrier enabled IO stack does not require any major changes in the existing in-memory or on-disk structure of the IO stack.
The only new data structure we introduce is the "conflict-page-list" for a running transaction.
Barrier enabled IO stack consists of approximately 3K LOC changes in the IO stack of the Linux kernel .
As the closest approach of our sort, OptFS deserves an elaboration.
OptFS and barrier-enabled IO stack differ mainly in three aspects; the target storage media, the technology domain, and the programming model.
First, OptFS is not designed for the Flash storage but the barrier-enabled IO stack is.
OptFS is designed to reduce the disk seek overhead in a filesystem journaling; via committing multiple transactions together (delayed commit) and via making the disk access sequential (selective data mode journaling).
Second, OptFS is the filesystem technique while the barrier enabled IO stack deals with the entire IO stack; the storage device, the block device layer and the filesystem.
OptFS is built upon the USENIX Association 16th USENIX Conference on File and Storage Technologies 219 legacy block device layer.
It suffers from the same overhead as the existing filesystems do.
OptFS uses Wait-onTransfer to control the transfer order between D and JD.
OptFS relies on Transfer-and-Flush to control the storage order between the journal commit and the associated checkpoint in osync().
Barrier-enabled IO stack eliminates the overhead of Wait-on-Transfer and Transferand-Flush in controlling the storage order.
Third, OptFS focuses on revising the filesystem journaling model.
BarrierFS is not limited to revising the filesystem journaling model but also exports generic storage barrier with which the application can group a set of writes into an epoch.
To date, fdatasync() has been the sole resort to enforce the storage order between the write requests.
The virtual disk managers for VM disk image, e.g., qcow2, use fdatasync() to enforce the storage order among the writes to the VM disk image [7].
SQLite uses fdatasync() to control the storage order between the undo-log and the journal header and between the updated database node and the commit block [37].
In a single insert transaction, SQLite calls fdatasync() four times, three of which are to control the storage order.
In these cases, fdatabarrier() can be used in place of fdatasync().
In some modern applications, e.g. mail server [62] or OLTP, fsync() accounts for the dominant fraction of IO.
In TPC-C workload, 90% of IOs are created by fsync() [51].
With improved fsync() of BarrierFS, the performance of the application can increase significantly.
Some applications prefer to trade the durability and the freshness of the result for the performance and scalability of the operation [12,17].
One can replace all fsync() and fdatasync() with ordering guarantee counterparts, fbarrier() and fdatabarrier(), respectively, in these applications.
We implement a barrier-enabled IO stack on three different platforms, enterprise server (12 cores, Linux 3.10.61), PC server (4 cores, Linux 3.10.61) and smartphone (Galaxy S6, Android 5.0.2, Linux 3.10).
We test three storage devices: 843TN (SATA 3.0, QD 2 =32, 8 channels, supercap), 850PRO (SATA 3.0, QD=32, 8 channels), and mobile storage (UFS 2.0, QD=16, single channel).
We compare the BarrierFS against EXT4 and OptFS [9].
We refer to each of these as supercap-SSD, plain-SSD, and UFS, respectively.
We implement barrier write command in UFS device.
In plain-SSD and supercap SSD, we assume that the performance overhead of barrier write is 5% and none, repsectively.
We examine the performance of 4 KByte random write with different ways of enforcing the storage order: P (orderless write [i.e. plain buffered write]), B (barrier write), X (Wait-on-Transfer) and XnF (Transfer-andFlush).
Fig. 9 illustrates the result.The overhead of Transfer-and-Flush is severe.
With Transfer-and-Flush, the IO performances of the ordered write are 0.5% and 10% of orderless write in plain-SSD and UFS, respectively.
In supercap SSD, the performance overhead is less significant, but is still considerable; the performance of the ordered write is 35% of the orderless write in UFS.
The overhead of DMA transfer is significant.
When we interleave the write requests with DMA transfer, the IO performance is less than 40% of the orderless write in each of the three storage devices.The overhead of barrier write is negligible.
When using a barrier write, the ordered write exhibits 90% performance of the orderless write in plain-SSD and super-cap SSD.
For UFS, it exhibits 80% performance of the orderless write.
The barrier write drives the queue to its maximum in all three Flash storages.
The storage performance is closely related to the command queue utilization [33].
In Wait-on-Transfer, the queue depth never goes beyond one (Fig. 10(a) and Fig. 10(c)).
In barrier write, the queue depth grows near to its maximum in all storage devices (Fig. 10(b) and Fig. 10(d) Figure 11: Average Number of Context Switches, EXT4-DR: fsync(), BFS-DR: fsync(), EXT-OD: fsync() with no-barrier, BFS-OD: fbarrier(), 'DR' = durability guarantee, 'OD' = ordering guarantee, 'EXT4-OD' guarantees only the transfer order, but not storage order.
We examine the latency, the number of context switches and the queue depth in filesystem journaling in EXT4 and BarrierFS.
We use Mobibench [26].
For latency, we perform 4 KByte allocating write() followed by fsync().
With this, an fsync() always finds the updated metadata to journal and the fsync() latency properly represents the time to commit a journal transaction.For context switch and queue depth, we use 4 KByte non-allocating random write followed by different synchronization primitives.Latency: In plain-SSD and supercap-SSD, the average fsync() latency decreases by 40% when we use BarrierFS against when we use EXT4 (Table 2).
In UFS, the fsync() latency decreases by 60% in BarrierFS compared against EXT4.
UFS experiences more significant reduction in fsync() latency than the other SSDs do.BarrierFS makes the fsync() latency less variable.
In supercap-SSD and UFS, the fsync() latencies at the 99.99 th percentile are 30× of the average fsync() latency ( Table 2).
In BarrierFS, the tail latencies at 99.99 th percentile decrease by 50%, 20%, and 70% in UFS, plain-SSD, and supercap-SSD, respectively, against EXT4.
Context Switches: We examine the number of application level context switches in different journaling modes (Fig. 11).
In EXT4, fsync() wakes up the caller twice: after D is transferred and after the journal transaction is made durable(EXT4-DR).
This applies to all three storages.
In BarrierFS, the number of context switches in an fsync() varies subject to the storage device.
In UFS and supercap SSD, fsync() of BarrierFS wakes The interval between the successive write()s is longer than the timer interrupt interval.
The application thread blocks after triggering the journal commit and and wakes up after the journal commit operation completes.
BFS-OD manifests the benefits of BarrierFS.
The fbarrier() rarely finds updated metadata since it returns quickly and as a result, most fbarrier() calls are serviced as fdatabarrier().
fdatabarrier() does not block the caller and therefore does not accompany any involuntary context switch.Command Queue Depth: In BarrierFS, the host dispatches the write requests for D, JD, and JC in tandem.
Ideally, there can be as many as three commands in the queue.
We observe only up to two commands in the queue in servicing an fsync() (Fig. 12(a)).
This is due to the context switch between the application thread and the commit thread.
Writing D and writing JD are 160 µsec apart, but it takes 70µsec to service the write request for D.
In fbarrier(), BarrierFS successfully drives the command queue to its full capacity (Fig. 12(b)).
Throughput and Scalability: The filesystem journaling is a main obstacle against building an manycore scalable system [44].
We examine the throughput of filesystem journaling in EXT4 and BarrierFS with a varying number of CPU cores in a 12 core machine.
We use modified DWSL workload in fxmark [45]; each thread performs a 4-Kbyte allocating write followed by fsync().
Each thread operates on its own file.
BarrierFS exhibits much more scalable behavior than EXT4 (Fig. 13).
In plain-SSD, BarrierFS exhibits 2× performance against EXT4 in all numbers of cores ( Fig. 13(a)).
In supercap-SSD, the performance saturates with six cores in both EXT4 and BarrierFS.
BarrierFS exhibits 1.3× journaling throughput against EXT4 (Fig. 13(b)).
We run two workloads: varmail [71] and OLTPinsert [34].
OLTP-insert workload uses MySQL DBMS [47].
varmail is a metadata-intensive workload.
It is known for the heavy fsync() traffic.
There are total four combinations of the workload and the SSD (plain-SSD and supercap-SSD) pair.
For each combination, we examine the benchmark performances for durability guarantee and ordering guarantee, respectively.
For durability guarantee, we leave the application intact and use two filesystems, the EXT4 and the BarrierFS (EXT4-DR and BFS-DR).
The objective of this experiment is to examine the efficiency of fsync() implementations in EXT4 and BarrierFS, respectively.
For ordering guarantee, we test three filesystems, OptFS, EXT4 and BarrierFS.
In OptFS and BarrierFS, we use osync() and fdatabarrier() in place of fsync(), respectively.
In EXT4, we use nobarrier mount option.
This experiment examines the benefit of Wait-on-Dispatch.
Fig. 14 illustrates the result.Let us examine the performances of varmail workload.
In plain-SSD, BFS-DR brings 60% performance gain against EXT4-DR in varmail workload.
In supercap-SSD, BFS-DR brings 10% performance gain against EXT4-DR.
The experimental result of supercap-SSD case clearly shows the importance of eliminating the Wait-on-Transfer overhead in controlling the storage order.
The benefit of BarrierFS manifests itself when we relax the durability guarantee.
In ordering guarantee, BarrierFS achieves 80% performance gain against EXT4-OD.
Compared to the baseline, EXT4-DR, BarrierFS achieves 36× performance (1.0 vs. 35.6 IOPS) when we enforce only ordering guarantee with BarrierFS (BFS-OD) in plain SSD .
In MySQL, BFS-OD prevails EXT4-OD by 12%.
Compared to the baseline, EXT4-DR, BarrierFS achieves 43× performance (1.3 vs. 56.0 IOPS) when we enforce only ordering guarantee with BarrierFS (BFS-OD) in plain SSD.
We examine the performances of the libarary based embedded DBMS, SQLite, under the durability guarantee and the ordering guarantee, respectively.
We examine two journal modes, PERSIST and WAL.
We use 'Full Sync' and the WAL file size is set to 1,000 pages, both of which are default settings [58].
In a single insert transaction, SQLite calls fdatasync() four times.
Three of them are to control the storage order and the last one is for making the result of a transaction durable.
For durability guarantee mode, We replace the first three fdatasync()'s with fdatabarrier()'s and leave the last one.
In mobile storage, BarrierFS achieves 75% performance improvement against EXT4 in default PERSIST journal mode under durability guarantee (Fig. 15).
In ordering guarantee, we replace all four fdatasync()'s with fdatabarrier()'s.
In UFS, SQLite exhibits 2.8× performance gain in BFS-OD against EXT4-DR.
The benefit of eliminating the Transfer-and-Flush becomes more dramatic as the storage controller employs higher degree of parallelism.
In plain-SSD, SQLite exhibits 73× performance gain in BFS-OD against EXT4-DR (73 vs. 5300 ins/sec).
Notes on OptFS: OptFS does not perform well in our experiment ( Fig. 14 and Fig. 15), unlike that in [9].
We find two reasons.
First, the benefit of delayed checkpoint and selective data mode journaling becomes marginal in Flash storage.
Second, in Flash storage (i.e. the storage with short IO latency) the delayed checkpoint and the selective data mode journaling negatively interact with each other and bring substantial increase in the memory pressure.
The increased memory pressure severely impacts the performance of osync().
The osync() scans all dirty pages for the checkpoint at its beginning.
Selective data mode journaling inserts the updated data blocks to the journal transaction.
Delayed checkpoint prohibits the data blocks in the journal transaction from being checkpointed until the associated ADN arrives.
As a result, osync() checkpoints only a small fraction of dirty pages each time it is called.
The dirty pages in the journal transactions are scanned multiple times before they are checkpointed.
The osync() shows particularly poor performance in OLTP workload (Fig. 14), where most updates are subject to data mode journaling.
We test if the BarrierFS recovers correctly against the unexpected system failure.
We use CrashMonkey for the test [40].
We modify CrashMonkey to understand the barrier write so that the CrashMonkey can properly delimit an epoch when it encounters a barrier write.
We run two workloads; rename root to sub and create delete.
For durability guarantee (fsync()), BarrierFS passes all 1,000 test cases as EXT4 does in both workloads.
For ordering guarantee (fsync() in EXT4-OD and fbarrier() in BarrierFS), BarrierFS passes all 1,000 test cases whereas EXT4-OD fails in some cases.
This is not surprising since EXT4 with nobarrier option guarantees neither the transfer orders nor the persist orders in committing the filesystem journal transaction.
Featherstitch [20] proposes a programming model to specify the set of requests that can be scheduled together, patchgroup, and the ordering dependency between them, pg depend().
While xsyncfs [49] mitigates the overhead of fsync(), it needs to maintain complex causal dependencies among buffered updates.
NoFS (no order file system) [10] introduces "backpointer" to eliminate the Transfer-and-Flush based ordering in the file system.
It does not support transaction.A few works proposed to use multiple running transactions or multiple committing transactions to circumvent the Transfer-and-Flush overhead in filesystem journaling [38,29,55].
IceFS [38] allocates separate running transaction for each container.
SpanFS [29] splits a journal region into multiple partitions and allocates committing transactions for each partition.
CCFS [55] allocates separate running transactions for individual threads.
In these systems, each journaling session still relies on the Transfer-and-Flush mechanism.A number of file systems provide a multi-block atomic write feature [18,35,54,68] to relieve applications from the overhead of logging and journaling.
These file systems internally use the Transfer-and-Flush mechanism to enforce the storage order in writing the data blocks and the associated metadata blocks.
Exploiting the orderpreserving block device layer, these filesystems can use Wait-on-Dispatch mechanism to enforce the storage order between the data blocks and the metadata blocks and can be saved from the Transfer-and-Flush overhead.
The Flash storage provides the cache barrier command to allow the host to control the persist order.
HDD cannot provide this feature.
It is time for designing the new IO stack for the Flash storage that is free from the unnecessary constraint inherited from the old legacy that the host cannot control the persist order.
We built a barrierenabled IO stack based upon the foundation that the host can control the persist order.
In the barrier-enabled IO stack, the host can dispense with Transfer-and-Flush overhead in controlling the storage order and can successfully saturate the underlying Flash storage.
We like to conclude this work with two key observations.
First, the "cache barrier" command is a necessity rather than a luxury.
It should be supported in all Flash storage products ranging from the mobile storage to the highperformance Flash storage with supercap.
Second, the block device layer should be designed to eliminate the DMA transfer overhead in controlling the storage order.
As the Flash storage becomes quicker, the relative cost of tardy "Wait-on-Transfer" will become more substantial.
To saturate the Flash storage, the host should be able to control the transfer order without interleaving the requests with DMA transfer.We hope that this work provides a useful foundation in designing a new IO stack for the Flash storage 3 .
We would like to thank our shepherd Vijay Chidambaram and the anonymous reviewers for their valuable feedback.
We also would like to thank Jayashree Mohan for her help in CrashMonkey.
This work is funded by Basic Research Lab Program (NRF, No. 2017R1A4A1015498), the BK21 plus (NRF), ICT R&D program (IITP, R7117-16-0232) and Samsung Elec.
