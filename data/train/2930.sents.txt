A recipe is metadata that describes the contents of a file as a sequence of blocks identified by their hash.
Using recipes, one can rapidly compare the contents of two files without reading the files themselves.
Unfortunately , recipes present a space/precision tradeoff: small block sizes will maximize the duplication that is discoverable, but large block sizes produce small recipes that can be compared more quickly.
In this paper , we present Accordion, a toolset for the creation and use of multi-scale recipes-that is, recipes that include blocks at several different scales.
We demonstrate two duplication-detection algorithms-one optimized for situations where lots of duplication is expected , and another for those where the existence of duplication is uncertain.
Storage systems often contain lots of duplication; backup systems, archives, and Internet mirrors contain lots of duplicate data, either in space (different files with the same data) or time (different versions of the same file often contain duplication) [1] [2] [3] [4].
One de-duplication strategy is to use recipes to describe files, allowing for rapid detection of duplication [5] [6].
A recipe is metadata that describes the contents of a file as a sequence of blocks identified by their hash.
However, recipe-based systems face a block-size tradeoff; small block sizes detect more duplication, but large block sizes produce shorter recipes, which are less costly to store, transmit, and compare [7].
Most systems choose an appropriate block size empirically, by testing on a representative workload.
However, no single block size is optimal for all files.To solve this problem, we developed a suite of tools called Accordion that create and use multi-scale recipes.
A multi-scale recipe is the union of many different single-scale recipes; it can be used to detect large-block duplication rapidly and yet can also provide small-block detection if required.Accordion implements two different file comparison algorithms.
The first is designed to minimize seeks within the recipe.
When large amounts of duplication are available, this algorithm discovers that duplication extremely rapidly (up to 80x faster than single-scale recipes in our experiments); however, its worst case is roughly 2x worse than single-scale.
The second algorithm has better worst-case performance (matching single-scale in almost every case), but makes many more small seeks through the recipe.
Some recipe systems use fixed block sizes [3] [7]; however, most are content-aware, and use blocks of variable lengths whose boundaries are determined by the file contents [2] [5].
This allows the systems to detect duplication even when data has been shifted by insertion or deletion.Accordion is also content-aware, but uses fixed-size, non-aligned (and thus overlapping) blocks.
This design was the natural consequence of a more fundamental design choice: Accordion uses Winnowing [8] to select block boundaries, rather than the more conventional Rabin fingerprinting algorithm [9].
Rabin fingerprinting, which is used by most contentaware systems, scans the file with a Rabin polynomial hash; block boundaries are established when the hash value is congruent to some magic value, modulo some divisor.
While simple, this algorithm does not provide lower or upper bounds on block sizes.
Thus, such systems must override the block boundaries in some scenarios -joining blocks that are too small and breaking up those that are too large.
This solution, while workable, is undesirable, as it introduces an algorithm step (block join decisions) that is not content-aware, and thus increases the chances of false negatives when comparing files.Winnowing uses a different approach.
Like Rabin fingerprinting, Winnowing generates a stream of hashes from the file; however, instead of choosing block boundaries based on individual hashes, it runs a sliding window along the stream of hashes.
At each window position, the minimum hash is chosen as the fingerprint (that is, a (hash,offset,size) tuple); usually, the same hash is chosen over and over as the window slides.
On average, Winnowing chooses a fingerprint for every w/2 bytes of the file, where w is the size of the sliding window.
More importantly for our purposes, no two fingerprints can be more than w bytes from each other.
(See the Winnowing paper [8] for proofs of both of these assertions.)
The original Winnowing algorithm has two passes.
In the first pass, a Rabin polynomial is used to generate a hash; in the second pass, fingerprints are selected.
The selected fingerprints report the first-pass hashes as the hashes for the blocks.Rabin hashes are used -even though they are weak hashes -because the hash must run at every position in the file; thus, a strong hash (such as SHA1) is impractical.
The Winnowing design can lead to false positives, but the authors found this acceptable because all duplicates would be subsequently examined by a human.
This compromise is unacceptable for recipes, where false positives can lead to data corruption.Thus, Accordion adds a third pass to Winnowing.
In the first pass Accordion uses a Rabin polynomial hash; it then selects fingerprint locations using a sliding window on that stream.
Accordion then executes a SHA1 hash at the chosen locations, and the SHA1 values are reported as the hashes in the recipe.In addition, Accordion uses a single first-pass hash, which is shared by all scales, and the size of the hash window is relatively small -no larger than the smallest requested recipe block size.
This means that the Rabin hash can be efficiently generated; it also means thatas detailed in the next section -all of the scales slide their fingerprint-selection windows over the same stream of first-pass hashes.
Running Winnowing at very large scales can be expensive, as the sliding window must find the minimum value in a very large set of hashes.
The Winnowing paper suggests that one keep track of the running "best" fingerprint in a window -but this requires a periodic brute-force scan of the entire window when a very-good fingerprint finally leaves the window.
This is unworkable when window sizes are very large -not only would it be computationally expensive, it also would require that Accordion store a very long stream of hashes in memory.Thus, Accordion culls the list of hashes as it progresses through the file; it only keeps the "interesting" hashes.A hash is interesting if it is the minimum hash in a range stretching from its position to the end of the sliding window; if the hash is the minimum in that range, then it is possible that it might be selected as a fingerprint in the future.
This leads to an invariant: every hash in the list is less than or equal to the hash that follows it.As the window slides, Accordion updates the list of interesting hashes.
This involves three steps.
First, at every new position, Accordion adds a new hash at the end of the list -since the most recent hash is always the minimum in a range of length one (even if it might become uninteresting at the very next window position).
Second, this new hash may violate the invariant of the list, as it may have a lower value than some or all of the elements already on the list.
Thus, the new hash is compared to the element immediately before it (that is, the last hash from the old list); if the old element has a larger hash than the new, then the old is removed.
This process repeats until the invariant is re-established.
Finally, Accordion examines the head of the list; if it is now so old that it is outside of the sliding window, it is removed.Thus, with amortized constant cost per byte in the file, Accordion maintains a list of the interesting hashesand, by definition, the oldest hash in the list is always the fingerprint for the current window position.This algorithm is easily generalized to multi-scale operation: Accordion simply chooses, at every position of the sliding window, a fingerprint for each scale, selecting the oldest interesting hash within that window size.
When Accordion compares two recipes, it must skip over some of the recipe entries -because a multi-scale recipe is the union of many different single-scale recipes, and the total size is approximately twice the size of a single-scale recipe using the smallest block size.
(The multi-scale recipe is twice the size because Accordion builds recipes for every power of 2).
Thus, each time that Accordion has a hash hit -that is, whenever a duplicate block is found in a file -it records the range of the file covered by that block.
When Accordion later encounters recipe entries that cover the same range (or some subset of it), it skips over those (now redundant) recipe entries.Therefore, in this paper we will rate the algorithms by the number of recipe entries read.
Our goal is to ensure that Accordion's algorithms perform no worse than single-scale comparison of the same files; of course, sometimes, Accordion will perform far better.
Accordion's top-down file comparison algorithm assumes that the recipe entries are already sorted, first by block size (descending) and then by file offset (ascending).
The algorithm reads the recipe in order, only skipping over recipe entries that have been previously covered at larger scales.The intuition behind this algorithm is simple: it attempts to detect duplication at the largest scale, and then recurses down -but only into parts of the file that have not already been covered by hash hits.
When huge duplication exists (as with duplicate files, archives, etc.), it is found very quickly.For example, consider the recipe snippet in Table 1 (each line has been annotated as to whether the hash lookup is a hit or miss).
In this example, Accordion reads lines 1 through 6; it finds two hits and four misses.
None of the lines are skipped.
However, lines 7 and 8 are both skipped -because each is entirely contained inside a previous hit (line 3).
Lines 9 and 10 are not skipped -because they are not contained within any previous hit.The downside of this algorithm is its poor worst case: if little duplication is found -or if it is not contiguousthen this algorithm reads the entire recipe, which is approximately twice the size of a single-scale recipe.
The break-even point for this algorithm thus is a file that contains 50% contiguous duplication -in that case, the total cost is half the multi-scale recipe size, which is approximately the same as a single-scale recipe.
Accordion's bottom-up file comparison algorithm assumes that the recipe entries are sorted first by file offset (ascending) and second by block size (ascending).
Like top-down, this algorithm skips over already-covered ranges; however, when it encounters a hash miss at a given file offset (at any scale), it skips over all of the remaining recipe entries at that file offset.The intuition behind this algorithm is that if a smallscale block is not a duplicate then a large-scale block that contains it cannot be a duplicate either.
If large, contiguous duplication is available, then this algorithm will find it quickly, as it rapidly works its way "up the tree" to the large blocks.
But if no duplication is available, then it reads no more recipe entries than a single-scale recipe.For example, consider the same recipe as before, but sorted for this algorithm, as listed in Table 2.
This algorithm reads line 1, which is a hit; it thus continues to line 2.
Line 2, however, is a miss -and thus, line 3 is skipped.The downside of this algorithm is that it performs small seeks through the recipe.
The algorithm will routinely skip over a handful of recipe entries at a time.
Thus, this algorithm trades off continuity in recipe entries read in favor of reducing the worst-case number of recipe entries read.
We compared Accordion's two algorithms against a simple single-scale recipe, at a variety of scales.
We used a variety of input files from prior art that are known to exhibit duplication, such as multiple versions of the same source code and related Linux no duplication, and manually constructed torture tests, to exercise the worst case of the algorithms.
Our first experiment compared two versions of the Linux kernel source code; we use the files used by Tridgell in his rsync dissertation [5].
These were specifically chosen by Tridgell to demonstrate a pair of files with massive (but not complete) duplication.
(See that paper for a discussion of how the files were generated.)
Table 3 summarizes this experiment.
As expected, the multi-scale recipe was roughly twice the size of the single-scale recipe for the same minimum block size; also, the ranges covered by multi-scale were strict supersets of those covered by single-scale.
These general properties held for all pairs of files that we tested.
Figure 1 shows the cost of our file comparison algorithms on the same pair of files; recall that the cost is defined as the number of recipe entries actually used by each algorithm.
Since the number of entries used by different scales differ by orders of magnitude, these curves are normalized to the single-scale cost for each scale.We see that since there is a large amount of duplication available (over 99% of the entire file), both of our algorithms use very few entries.
In fact, at the smallest scale (1KB), top-down skips over almost the entire multi-scale recipe (automatically using huge blocks almost entirely), and thus uses only 622 entries.
By contrast, single-scale uses 46889 entries at the 1KB scalea difference of almost 80x.
Figure 2 shows the cost when there is less duplication available.
Fedora-17-i686-Live-Desktop.
iso then either algorithm worked well; if there was little duplication, then bottom-up would track well with single-scale, but top-down would perform poorly.Finally, we constructed a torture test to expose the worst case of the bottom-up algorithm.
We copied an existing file, and then zeroed the last byte in every 2K block.
These two files had huge duplication, but almost no duplicated ranges 2K or longer; thus, bottom-up needed to read almost every 1K recipe entry, and also many of the 2K recipe entries.
At 1K (the only scale where significant duplication was detected), bottom-up read about 30% more entries than single-scale.
We initially expected to see about 50% additional cost, but in hindsight 30% makes more sense: in a 2K range of the file, Winnowing selects roughly 4 fingerprints at the 1K block size and 2 at the 2K block size.
On average, bottom-up reads every 1K recipe entry, and about half of the 2K entries (due to hash hits at the 1K size); thus, in a 2K window, single-scale reads about 4 recipe entries, and bottom-up reads around 5.
A variety of systems have used hierarchical approaches to detecting duplication, including LBFS [2], Venti [3], and TAPER [10]; most are analogous to Merkle trees [11], where small-block hashes are grouped and then hashed again to generate large-block hashes.
However, these groupings are not content-aware; thus, when data is inserted or deleted, duplication is likely to be overlooked at the large scale (even if it will be found, eventually, at smaller scales).
Multiround rsync [12] is a generalization of the rsync protocol that runs in several passes, with notable similarities to our top-down algorithm.
However, that paper does not present any way to solve the worst-case cost (which is worse that rsync).
Also, as it is focused on live comparisons over a network, it generates (offset, hash) pairs only as needed for a single comparison.
Recipes, on the other hand, can be stored and then used for later comparisons, and contain all of the possible (offset, hash) pairs -a major advantage if the recipe is to be generated once, and then used later, such as in an all-to-all file comparison.
In this work, we presented Accordion, a toolset for creating and using multi-scale recipes.
We demonstrated two algorithms for finding duplication using multi-scale recipes, each optimized for a different scenario.
Top-down worked best when large amounts of duplication were available, and was up to 80x as efficient as single-scale recipes.
Bottom-up was optimized for the worst-case (performing no worse than single-scale except in artificial torture tests), and was competitive with top-down in the best case.
This material is based upon work supported by the National Science Foundation under Award Numbers DBI-0735191 and DBI-1265383.
URL: www.iplantcollaborative.org Preliminary work on multi-scale recipes was done in connection with CSc 630 (Advanced Topics in Software Systems) at the University of Arizona.
Thanks to project partner Gavin Simmons for contributions to the early prototype and to Professors Larry Peterson and Todd Proebsting for inspiration and direction.
Thanks to team member Illyoung Choi for feedback as we investigated these algorithms.
