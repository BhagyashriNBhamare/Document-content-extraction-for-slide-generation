Traditional remote-server-exploiting malware is quickly evolving and adapting to the new web-centric computing paradigm.
By leveraging the large population of (insecure) web sites and exploiting the vulnerabilities at client-side modern (com-plex) browsers (and their extensions), web-based malware becomes one of the most severe and common infection vectors nowadays.
While traditional malware collection and analysis are mainly focusing on binaries, it is important to develop new techniques and tools for collecting and analyzing web-based malware, which should include a complete web-based malicious logic to reflect the dynamic, distributed , multi-step, and multi-path web infection trails, instead of just the binaries executed at end hosts.
This paper is a first attempt in this direction to automatically collect web-based malware scenarios (including complete web infection trails) to enable fine-grained analysis.
Based on the collections, we provide the capability for offline "live" replay, i.e., an end user (e.g., an analyst) can faithfully experience the original infection trail based on her current client environment , even when the original malicious web pages are not available or already cleaned.
Our evaluation shows that WebPatrol can collect/cover much more complete infection trails than state-of-the-art honeypot systems such as PHon-eyC [11] and Capture-HPC [1].
We also provide several case studies on the analysis of web-based malware scenarios we have collected from a large national education and research network, which contains around 35,000 web sites.
With the increasing reliance of our lives on the Internet, web-based services are providing more and more functions to serve our daily communication, entertainment, and business.
Web sites are now becoming much more dynamic and complex, particularly with the increasing interest in Web 2.0 and Software-as-a-Service (SaaS).
Accordingly, web browsers are becoming the most widely used client software on Internet.
At the same time, malicious software (malware) is quickly evolving and adapting to this new web-centric computing paradigm.
We are witnessing a major shift of malware infection vectors, from traditional scanning-based remote server exploiting to new web-based client software exploiting.Compared to traditional server-side exploiting malware, web-based malware has the following characteristics.
First, it exploits client-side vulnerabilities, mostly in modern complex browsers and their extensions.
Thus, it is more stealthy and evasive because it does not need to send aggressive scanning traffic.
Second, it is pervasive considering the large base of insecure web sites/pages on the Internet.
Finally, it is hard to block because most networks allow web traffic.
As a result, web-based malware becomes one of the most severe and common infection threats nowadays [14,13,12].
To defend against this emerging type of threat, automated collection and analysis of web-based malware are necessary.
Unfortunately, previous automated (binary) malware collection techniques such as Nepenthes [5] are not applicable here because they are designed for server-exploiting malware.
Although client-side honeypot techniques such as Capture-HPC [1] and HoneyMonkey [18], and suspicious web content analysis service such as Wepawet [6,9] are proposed, their main purpose is to detect whether a given URL/Flash file is malicious or not, instead of to collect the complete malicious logic and infection trails in web-based malware.
Current best practice on web-based malware collection and analysis is mostly on the downloaded malware binaries and/or individual web pages that contain malicious Javascript code [6].
However, we consider that is not enough.
For example, the downloaded binary cannot reveal the (distributed) malicious logic of web-based malware.
The captured Javascript code is not complete, too.
In addition, the analysis of these script code may yield different results or even be not successful simply because the code may contain/trigger requests to another URL which can be dynamically changed, cleaned, or even removed.In this paper, we propose to collect and study a complete web-based malware instance, which should contain all infection trails as completely as possible.
For now, we can consider an example infection trail as a directed infection path starting from the initial URL, going through a series of nested inline linking 1 , and ending with the downloaded binaries, which is analogous to an execution path in a binary.
Similar to typical multiple branches in a binary, a web-based malware may also have multiple execution paths (infection trails) depending on the configuration information at clientside environment.
We define the complete set of these infection trails as a web-based malware scenario (WMS, formal definition and more details will be introduced in Section 3).
Typically, a WMS contains dynamic, distributed infection contents and multi-step, multi-path web infection trails, instead of just the binaries executed at end hosts.More specifically, this paper makes the following contributions:• We develop new techniques for automated collection of these malware scenarios to enable future fine-grained analysis.
To efficiently and effectively collect web infection trails as completely as possible, we use lightweight browser emulation techniques for analyzing webbased malware and then store all infection interactions/contents during infection trails.
• Based on the collections, we provide the capability for live replay, i.e., an end user (e.g., an analyst) can faithfully experience the original infection trail based on his/her current client environment, even when the original malicious web pages are not available or already cleaned.
This is a very useful function for offline "live" analysis of web infections.
• We have implemented a prototype system, WebPatrol, for automated collection and replay of web-based malware scenarios.
Using the prototype system, we have collected many real-world web malware scenarios.
We show the utilities of our system through several case studies.
In particular, by comparing the collected scenarios with their corresponding ground-truth, we show that WebPatrol can cover much more complete infection trails than state-of-the-art honeypot systems such as PHoneyC [11] and Capture-HPC [1].1 http://en.wikipedia.org/wiki/Inline linking The rest of this paper is organized as follows.
We introduce related work in Section 2.
Section 3 provides the formal definition and illustration of web-based malware scenarios.
We present our system design in Section 4 and its implementation in Section 5.
We present our WebPatrol evaluation and measurement results in Section 6.
We discuss current limitations of WebPatrol and our future work in Section 7, and conclude the paper in Section 8.
Web-based Malware Measurement Study Provos et al. have conducted a large-scale study of malware on Internet web pages crawled by Google [14,13,12].
They characterized some common patterns shared among web-based malware.
Zhuge et al. [20] studied malicious websites and the underground economy in China.
Seifert et al. [15] did a similar study on New Zeland (.
nz) domains.
These measurement studies clearly call for significant further research on web-based malware, e.g., automated collection, live replay, detection, and analysis.Web-based Malware Detection and Analysis There are many kinds of client honeypots that aim to detect malicious websites.
Capture-HPC [1] and Strider HoneyMonkey [18] are high interaction client honeypots that load the suspicious web pages within a real browser, and detect the web-based malware by monitoring the anomaly activities during the browsing.
CaffeineMonkey [8], and PHoneyC [11] are low interaction client honeypots that parse the suspicious pages, or run them in an emulated browser environment, and raise alerts if certain attack patterns are found.
Although our prototype system uses an improved PHoneyC, our goal is different from the traditional use of client honeypots (to detect malicious web pages), but to exhaustively enumerate and collect all the infection trails for further replay and analysis.
Wepawet [6] is an online web-based malware detection and analysis service.
User can submit suspicious URLs or upload suspicious Flash/PDF files to it and wepawet will analyze them for malicious scripts.
The difference between our system and wepawet is that wepawet aims to distinguish the malicious contents from the benign ones, and WebPatrol aims to fight against the obfuscation of given malicious URLs and successfully collect and replay the scenario.
The online service of WebPatrol will be a depository that provides different kinds of WMS for further analysis and new detection system evaluation.In addition, Song et.
al [16] introduces inter-module communication monitoring for web browser plugin vulnerabilities, which shares a similar feature with our plugin simulation in JavaScript context.
However, their system is based on a real browser, while we implement a simulated plugin module within a low-interaction honeypot.Automated Malware Collection The concept of automated malware collection has been proposed by many researchers, such as the low interaction honeypot Nepenthes [5] and the high interaction honeypot HoneyBow [19].
However, all of those approaches are proposed for traditional serverside malware collection, while our approach aims to collect and replay the web-based malware automatically.
A web-based malware scenario is defined as a directed treelike graph, which is represented as a four-tuple (µ, V, E, T ), where• µ stands for the initial landing URL, which is a special (root) node in V .
• V refers to the set of all nodes, where each node vi is some resource (denoted as a URL) in some remote site.
• E refers to the set of all directed edges.
For any vi ∈ V , if the client's interpretation on vi directly triggers a request to a new resource vj , then vj ∈ V and < vi, vj >∈ E.
We also call such a directed edge an outgoing link from vi to vj.
• T refers to the set of sink nodes (T ⊂ V ).
A sink node is some resource/object that typically indicates a successful web infection/exploitation.
For example, a typical sink node can be a downloaded binary.We define a web infection trail as a directed path in the graph, starting from µ to some sink node in T .
Obviously, a web-based malware contains one or many web infection trails.
Table 1: Possible types of inline linkings in E Figure 1 illustrates an example of a web-based malware scenario.
Nodes A-G are web pages and other kinds of resources that will be retrieved by the client from the remote server.
Each edge is tagged with the type of the outgoing link ( Table 1 shows a list of the possible types of such links and corresponding examples).
Note that the elements in V are not only web pages.
Actually it can be a PDF file, data transferred through XMLHttpRequest, or it can be a 404 or 500 error page.
Any resource returned in response to a request can be treated as an element in V .
In a typical web-based malware scenario, a user opens the landing URL µ and browses it within a browser (this site is called the landing site in [13]).
The retrieved landing web page may look normal, but the inline linking tags (such as IFRAMEs and SCRIPTs) or JavaScript APIs etc. will enable the landing URL to contain cross-domain malicious pages or scripts that perform the actual attack through one or more hops of inline linking.
Those intermediate hop points are called hops pages, and the final web page that contains the exploit codes is called the exploit page.To increase the success rate and efficiency of the attack, web-based malware writers usually make use of some webbased malware exploit kits (e.g. Fragus [10]), which consist of multiple exploit vectors, as well as a dispatching page.
The dispatching page typically fingerprints the family and version of the client browser and its plugins, testing if certain vulnerability exists in the client, and exposes the exploit pages only if the client has specific vulnerabilities.While the exploit pages are important for vulnerability analysis and signature generation, we consider all intermediate (landing/hopping) sites are also vital in the analysis and defense of large-scale web-based malware infections.
By analyzing the complete web-based malware logics, we can figure out how web-based malware is injected into benign pages, and how it obfuscates itself to avoid detection.
Based on the definition above, the collection of WMS is actually the collection of (µ, V, E, T ) so that all the information related to (µ, V, E, T ) are stored and can be accessed later (for fine-grained analysis).
That is, all interactions and contents during all infection trails should be stored if possible.Similarly, based on the previous definition, the live replay of a web-based malware scenario (for a given time) is essentially to faithfully reproduce the right infection trail(s) (from the stored scenarios) based on the analyst's environment and provide the right interactions with the user (without actually accessing original malicious pages, which may be changed, removed, or cleaned frequently over time).
To achieve the goal of automated collection and replay of the web-based malware scenarios, we design and implement a prototype system called WebPatrol.
The architecture of WebPatrol is shown in Figure 2, which consists of two major components, the scenario collection component and the scenario replay component.
The scenario collection component works in an online fashion, and it is responsible for analyzing in-the-wild web-based malware scenarios, retrieving and caching all of the discovered web resources and outgoing links, and building the WMS depository.
The scenario replay component can operate in an offline fashion (e.g. in a logically isolated analysis environment from the Internet) but provide an online and interactive operation experience for end users.
This component is responsible for reconstructing the web infection trail(s) from the stored data, given a landing URL and specific time label as the identification of a web-based malware scenario.
In our WebPatrol design, the replay component can support arbitrary types of analysis (browser) clients, by providing a replay service which just requires minimal configuration of the clients.
The enumeration of all possible web resources in a webbased malware scenario is actually very hard due to the complex and obscure HTML/Javascript components, not to mention various obfuscation tricks introduced by the adversaries.
As a result, it is very difficult, or nearly impossible to collect the complete original scenario (µ, V, E, T ).
Therefore, the goal of our scenario collection is to obtain (µ, V , E , T ), so that V ∩ V , E ∩ E and T ∩ T are maximized and the replay service can reconstruct web infection trails as complete as possible using the recorded data.
Two modules are used in our design: a light-weight analyzer and a proxy-like caching service.Analyzer: Web-based malware collection heavily relies on the analysis of web response/content.
As mentioned earlier, the malicious web logics are stored on the (distributed) remote sites, and adversaries introduce many tricks to hide the essential exploit vectors from being easily discovered.
Constructing a collection of malicious resources requires enumerating all (if possible) outgoing links from a given resource node, some of which may be obfuscated or embedded in shellcode.
Thus, the analyzer is designed to analyze web response/content and discover as many outgoing links in E as possible.
To facilitate the analysis, we employ a lowinteraction (LI) client honeypot based on browser emulation technique, and introduce several techniques to increase the coverage of infection trails.We choose LI client honeypots rather than high-interaction (HI) client honeypots, because HI client honeypots, such as a real browser within a virtual machine, usually have a limited number of vulnerabilities/extensions in a specific version of a browser and an operating system.
As we discussed before, a dispatching page usually checks if the target system has a certain vulnerability or plugin before triggering the retrieval of the exploit scripts.
For example, Figure 3 shows a JavaScript snippet intercepted from a real-world dispatchtry{var c;var f=new ActiveXObject("OWC10.Spreadsheet");} catch(c){}; finally{if(c!
="[object Error]") {document.write( "<iframe width=0 height=0 src=of.htm></iframe>");} Figure 3: A Snippet from a Dispatching Page ing page.
As we can see, it tries to create an ActiveXObject object, using document.write to dynamically output the IFRAME tag for the real exploit page only if the object is successfully instantiated.
A LI client honeypot is more flexible and scalable in this case.
We can emulate many different kinds of vulnerabilities and plugins at the same time, even if they are totally on different browsers or operating systems.
For the example in Figure 3, the goal of our analyzer is to make the script think it successfully instantiates the ActiveXObject so it will document.write the malicious outgoing link.
To achieve better coverage of the infection trails, we propose several such techniques in the analyzer and other components, which will be discussed in detail in Section 5.
Caching Service: In order to faithfully record the infection trails (for later replay or analysis), we propose a proxy-like caching solution to complete the snapshot task.
"Proxy-like" means that it is in the middle of the client and the server and behaves like a proxy.
The advantage of the proxy-like approach is that it records the necessary information without the awareness of both client and server.
The analyzer can cache all the resources it retrieves by simply setting its proxy address to the address of the caching service.
The differences between our caching service and a real web proxy are as follows:• A web proxy typically only caches static HTML pages and ignores the dynamic HTML pages, and it will not cache responses with the private/no-cache/no-store cache control setting in the request header.
However, our caching service needs to store every web resources triggered by the analyzer.
• Cached data in a normal proxy may expire, and need to be updated, but our caching service do not need such validation.
In contrast, the cache resources should be stabilized, and should not be modified after it is collected.
The goal of WMS replay is to provide a service to third party analysts or other analysis tools so that they can analyze the malicious scenario faithfully, just as they visit the original web-based malware in the wild at a given (previous) time.Replay Service: We use the same proxy-like caching system to provide the WMS replay service.
The difference between the caching service and the replay service is that the replay service is operated in an offline fashion so that it will not interact with the actual/original remote servers, and all the response contents needed are provided by the cached WMS data.
Also, the replayer provides isolation between different scenarios.
This is necessary because the content of the same URL may change as time passes, and it may refer to different resources when collected at different times.Replay Client: The replay service can be provided to other users, e.g., security researchers/analysts.
We can serve any kind of client software for live/interactive replay.
For example, the client can be a real browser, a client honeypot (HI or LI), or an analyst using wget to fetch and analyze the scenario manually.
To implement WebPatrol, we use an improved version of PHoneyC [11] as the analyzer 2 , aiming to increase the coverage of outgoing links enumeration.
We also use a modified version of Polipo [4], referred to as wmPolipo, to provide the caching and replay service 3 .
PHoneyC is a client honeypot written in Python that provides visibility into new and complex client-side attacks.
The original PHoneyC contains three key modules: an HTML parser, a JavaScript (JS) engine and a plugin/ActiveX emulator.
It will try to emulate a real browser's JS context, let suspicious scripts run inside the emulated environment, and raise an alert if it detects malicious activities.Our improvement on PHoneyC aims to trigger as many outgoing links as possible, so that the caching service can collect a fairly complete scenario.
To achieve this, we not only enhanced the emulation of a browser within PHoneyC, but also implemented new techniques to trigger more outgoing links.
Generally, there are three kinds of obstacles that PHoneyC should deal with: the first one is dynamically generated outgoing links via document.write or eval, etc.
The second is conditional outgoing links (as previously shown in Figure 3), and the third is further downloads through vulnerable API misuses or the shellcodes.Dynamically generated outgoing links:The key for extracting the dynamically generated outgoing links is to provide a solid JS context, so that the outgoing links in the obfuscated scripts can be outputted correctly as it does in the real browser after the interpretation of JS scripts.
DOM (Document Object Model) plays an indispensable role in building a solid JS context.
As for the enhancement of the previous framework, we rewrote the DOM simulation engine in PHoneyC.
The previous version of PHoneyC does not generate a DOM tree for the web pages, namely it will only add DOM nodes into the JS context but never maintains reference relationships between them.
We enhanced it by providing a complete DOM tree to the JS context, as well as adding most of the methods and attributes provided by a DOM node in a real browser (e.g. innerHTML, and manipulation of a DOM node through a DOM path).
Those improvements guarantee the successful execution of JavaScript scripts, and can extract dynamically generated outgoing links after the execution.Conditional outgoing links: As shown in Figure 3, vulnerability existence checks or other condition checks prevent the creation of the IFRAMEs that may contain the outgoing links to the exploit pages.
We partially solved this problem by implementing a mock ActiveXObject class and adding it into the JavaScript context of PHoneyC.
Therefore, the instantiation of ActiveXObject is actually handled by our dummy class, and the dummy class will always return like the object is successfully created.
Consequently, the dispatching script will be deceived and generate the outgoing links.Further downloads after a successful exploit: Because of the limited emulation level of real environments in a LI client honeypot, no attack can be launched practically.
Thus, it is difficult for PHoneyC to get the URL of further downloads after the successful compromise (e.g., through an API misuse or exploitation by injected shellcode).
To deal with this, we enhance PHoneyC by implementing both simulated modules for several known vulnerable ActiveX objects and a shellcode detection and emulation module.We have implemented several known vulnerable ActiveX objects such as Baidu Soba Remote Code Execute Vulnerability [7] (shown in Figure 4).
In this case (Baidu vulnerability exploitation), we implement the vulnerable methods of the objects and can download the URL passed in.
To handle the cases of unknown (zero-day) vulnerabilities, we simply search for URLs in the arguments using regular expression, and download them no matter whether it is needed by the exploit or not.
Our shellcode detection and emulation module is implemented as a dynamic instrumentation of the opcodes used in PHoneyC's JavaScript engine, and as a check of the r-values of all string assignments.
This detection is accomplished by libemu [2], a shellcode detection and emulation library.
If a shellcode snippet is recognized, we use libemu to emulate the execution of the shellcode.
During its execution, libemu will recognize API calls such as URLDownloadToFile, which will then trigger the download of URLs in arguments.
Polipo [4] is a small and fast web proxy.
It can cache the responses from a server on a local storage.
We modified Polipo to accomplish the snapshoting and replay of the scenarios in a tool we call wmPolipo.
Different from a normal proxy server, wmPolipo aims to record and stabilize all data from the server side in a WMS on a local storage.
Therefore we have to modify the cache control policy of Polipo to ignore the cache control field in HTTP headers and record whatever it receives to disks.Randomized URL: Sometimes web-based malware will generate an outgoing link URL including a randomized argument or file name, as shown in Figure 5.
This trick aims to resist some static caching technique in a replayer, and also it can escape a URL blacklist-based filter in IDS/IPS or AVs.
wmPolipo replayer can defend against such obfuscation using a "URL-similarity-check" approach.
When wmPolipo cannot find the cached resources according to its URL, it will compare all the same-domain cached URLs with the requested one, simply by a string comparison, and provide the most similar one (sharing a longest common subsequence).
Furthermore, as we provide collected scenarios to multiple individuals, we have to modify the architecture of Polipo, from the old single-user, single-cache-directory architecture to a multi-user, multi-cache-directory one.
These modifications make the isolation between web-based malware scenarios and dynamic switching among them possible.
Thus, different clients can access the replay service using different user accounts simultaneously.
If those clients access different or even same web-based malware scenarios, the replay service will distinguish different clients by their username and provide different contents according to their current selected scenario and client environment, even if the URLs requested from different clients are the same.
Since Jan. 01, 2010, we have been using WebPatrol to monitor web sites in CERNET (China Education and Research Network, mostly .
edu.cn domain, about 35,000 websites in total) periodically (every two days).
To obtain the ground truth, we first use a crawler-like detection system which takes advantages of a high-interaction client honeypot, to hunt malicious URLs and feed them to WebPatrol for collection of the web-based malware scenarios.
Our client honeypots cover the most popular client software and plugins such as Internet Explorer (6.0, 7.0), Adobe Reader, Flash Player, Storm Player, etc on Windows XP (SP1, SP2).
If the access to some URL triggers some unexpected state changes, such as creating a new process, downloading some binaries to sensitive directories, this URL will be labeled as malicious.When a malicious URL is detected, the scenario collection module in WebPatrol collects the web-based malware scenario behind the malicious URL, labeled with the landing URL and the collecting timestamp.
The replay service lists them and replays selected scenario to other analysis tools.
In our case studies, we run multiple analysis tools, such as some HI honeypots, Malzilla [3], and wget, to automatically or manually analyze the scenarios for statistics and some interesting findings.In the following sections, we first present some overall statistics to show the severity of web-based malware in CER-NET, and then we measure the collection completeness of WebPatrol compared with other existing honeypot systems.
Finally we provide several case studies of analyzing the characteristics of our collected web-based malware.
During the period from Jan. 2010 to May.
2010, we collected 26,498 malicious scenarios from 1,248 distinct landing sites 4 .
This accounts for 3.52% of all the websites on CER-4 As we are not discussing web-based malware detection in NET.
For the discovered 1,248 landing sites, we checked it against Google Safe Browsing API 5 immediately after our detection, it turns out that Google only labeled 295 web sites as malicious.
76.4% of all the landing sites are not labeled.
Also, for the overall 1,248 landing sites, we measure how long the injected malicious content can last within it.
It turns out that the average lasting time of an injected malware is 23.2 days, and the longest lasting time is 132 days, which means it remain malicious nearly for the whole measurement period.
These two statistics shows that CER-NET is a hot spot for web-based malwares, but it has not received enough attention from the security companies and the website administrators.Furthermore, we count the number of times that the exploit hosting sites changes behind one landing site.
In our statistics, exploit hosting sites behind a single landing site change 4.82 times on average, which means the exploit kits behind a single site are highly changeable.
We also count the number of injected websites that contains malicious script with the same top-level domain name.
From the whois information against these malicious hosting domains (Table 2) we can see that most(8 of 10) of the top 10 malicious domains are subdomains registered at dynamic DNS providers (e.g. Yaako Ltd. and GoDaddy.com).
This result reveals that the abuse of the dynamic DNS services is quite severe, and need actions to respond to the situation.
Web-based malware exploit kit: In some scenarios, the dispatching page and exploit pages of different scenarios share similar reference sub-graph and also directory/file names.
This is because the dispatching pages and exploit pages in the two scenarios are generated automatically by the same web-base malware generator.
Those similar infection graphs can be grouped together as a web-based malware exploit kit.To evaluate the completeness and limitations of WebPatrol, we randomly choose 2,000 WMSs as the sample set.
After grouping those 2,000 samples by their exploit kits, we select the top 12 most popular exploit kits and 3 scenarios this paper, we are not going to discuss the false positives in this sample set in detail, however, as all of these samples cause the download of executables and the execution of programs with malicious behaviors, such as writing to system directories, modifying registry values, we can reasonably assume there are no false positives here.
(Table 3).
The first column in Table 3 is the ID of the exploit kit, the second column is a brief name of the exploit kit which reveals some unique paths of the kit, the third column is the number of scenarios containing an exploit kit in the overall 2000 scenarios, and the last column is its percentage.
As we can see in the table, the number of top 12 most popular WM families covers 91.0% of all the scenarios.
We manually analyze each of the 15 samples for a complete scenario (µ, V, E, T ), and then compare them with the scenarios (µ, V , E , T ) collected by WebPatrol.
The result of the comparison is shown in Table 4.
The first column is the landing site, and second column is the corresponding kit ID, the third column is the number of all nodes in an WMS, the fourth column is the number of nodes collected by WebPatrol, and the following four columns are the numbers of missing nodes grouped by their causing.
Thus the completeness of the WMS collected by WebPatrol C W P can be calculated in the following way:C W P = 13 i=1 (N W P i * Pi) = 0.819 (1)where Ni is the percentage of nodes WebPatrol can collect on exploit kit i (N13 is the average percentage of nodes WebPatrol can collect on the 3 randomly chosen samples from the "other" family group) , and Pi is the percentage of scenarios from kit 1-13 in the overall 2000 scenarios.
Thus the average completeness of the collected scenarios is 81.9%.
To compare the performance of WebPatrol with current state-of-the-art honeypot systems, we also run the original version of PHoneyC [11] 6 and Capture-HPC [1](captureclient-2.5.1-389) on the same sample set.
These results are also listed on Table 4 Columns 10-13.
As we can see, the original version of PHoneyC has much lower coverage of infection trails (C P HC = 47.1%), due to the lack of full DOM emulation and shellcode detection and analysis.
In the case of Capture-HPC, it did better in very few particular types of scenarios.
However, the overall performance is still not as good as WebPatrol, i.e., the average coverage of Capture-HPC is only 65.3%, still much lower than that of WebPatrol.
We looked into the reasons why Capture-HPC did better in some scenarios.
We found that this is mainly because WebPatrol's shellcode detection and emulation module (libemu) is not perfect (it fails in some cases in downloading binaries), while Capture-HPC provides a real OS environment where shellcodes can be executed correctly.
However, due to a more complete set of plugins and browser environments that WebPatrol provides, it has an overall much better path exploration functionality than Capture-HPC.
In short, we believe that WebPatrol provides better results in analyzing web malware infection trails than existing state-of-the-art honeypot systems.We further investigated the cases where WebPatrol has missed some nodes.
They can be grouped as the following 4 categories:1.
Out-going links in different branches: For example, as shown in Figure 6, different out-going links will be added to the DOM tree based on the vendor of the browser.
Due to the flexible of LI honeypots, this limitation can be easily overcame with multiple runs of the if(navigator.userAgent.toLowerCase().
indexOf("msie")>0) {document.write("<EMBED src=iie.swf width=0 height=0>");} else {document.write("<EMBED src=fff.swf width=0 height=0>");} Figure 6: Out-going links in different branches analyzer with different configuration of the emulated environments.
Our analyzer uses libemu to detect shellcode in the right value of a string assignment, thus the effectiveness of triggering futher downloads relies on the way shellcodes are used in malicious codes and the effectiveness of libemu's shellcode detection.
If libemu can't recognize a shellcode, we can't emulate it for futher downloads.3.
Limitation of the shellcode emulation module: After recognizing shellcode in some strings, the analyzer will trying to emulate the execution of the shellcode.
The default configuration of the emulation don't really call the system APIs for security reasons, thus the emulation may fail and can't get to the end of the shellcode.4.
Different implementations of the parser and the script engine: The SGMLlib and SpiderMonkey behaves differently from a real IE or Firefox browser when parsing the HTML files and interpret the JS codes.
This may cause the execution of the malicious code fail.
This limitation can be overcame by multiple runs of the analyzer with modifying the source code of SGMLlib and SpiderMonkey to make them behaves exactly like a real IE or Firefox browser.From Table 4, we can see that the most difficult part of the analyzer is the detection and emulation of the shellcode, while the other two causes can be eliminated by some engineering improvements.
As a result, most of the missing nodes by WebPatrol are the malicious binaries downloaded after a successful compromise.
We note that these missing nodes (binaries) have relatively little effect on the analysis of the malicious web logic and contents.
We will discuss our future work to improve WebPatrol in Section 7.
Using collected WMS repository, we studied how vulnerability exploits evolve with the time.The first vulnerability we want to introduce is MS10-002 "Aurora".
This vulnerability got its fame by a large-scale and complex attack on some global corporations, including Google, in which this vulnerability was firstly used.
Soon after this attack, Microsoft's Security Advisory 979352 was published (Jan. 14, 2010), and our web-based malware collection system recorded several sites containing this exploit code.
The malicious sites soon became inaccessible, but fortunately our collection system stored snapshots of that scenario, so we can replay it multiple times and analyze it using different tools.Recently, Microsoft Internet Explorer 'iepeers.dll' Remote Code Execution Vulnerability (CVE-2010-0806, MS10-018) is widely exploited in WMSs.
Soon after the exploit code was published (Around March 9, 2010), we re-analyzed our WMS depository and found a lot of scenarios containing such exploit snippets.Vulnerability Life Cycle: Using these two vulnerabilities as examples, we can show the life-cycle of a vulnerability from the exploit's first disclosure to a large-scale in-the-wild deployment (according to our collection) as in Figure 7.
This figure shows the amount of the newly discovered WMSs containing exploits using these two vulnerabilities.
every 10 days.
From the figure we can see that, usually it takes only several days between the disclosure of the sample exploit codes and a large scale deployment of the exploit pages, and soon the exploit code become popular in different scenarios.
And then, after the release of the patches for the vulnerabilities, the number of such exploits in the wild decreases and another 0-day vulnerability replaces it.
However, the exploit code will not disappear completely, though not as popular as before, the number of exploit code will remain in a low level for quite a long time.
Also we can see that MS10-002 exploits are not as popular as MS10-018 exploit, maybe this is because that the "Aurora" exploits is too famous and it draws too much attention of the security vendors, thus it's no longer a profitable choice for the underground web-based malware adversaries.
Exploit Evolution: Additionally, we also collected several variants of the exploit and put them together to study the evolution progress of the exploits.
Through this analysis and several debugging runs in a HI client honeypot, we were able to dissect the details of the vulnerability before the release of vendor's security bulletin.
Also, we compare the difference of their obfuscation techniques and coding styles, and identified several different adversaries and exploit kits.
Figure 8).
After that, we found five more variants exploiting this vulnerability.
The codes were improved from the following two aspects:• Heavier Obfuscation: Exploit codes were introducing more and more obfuscation techniques to avoid the detection and analysis.
The obfuscation techniques range from simple escape using string.fromCharCode and unescape, to sophisticated encryption using automatic encrypting tools.
For example, we found a comment of "Encrypt By Dadong's JSXX 0.31 VIP" in the latest variant of the exploit codes.
• Optimization: The optimization for exploit codes are mainly for better successful rate.
Some variants we collection tried to exploit the vulnerability multiple times or ran different code according to the version of the client.
As the investigation on vulnerability evolution can help to identify the variants of a vulnerability exploit and different exploit kit writers, the investigation on the scenario with the same URL but different timestamps can reveal many interesting information about the malware deployer.We chose a landing site and made a snapshot of the scenario starting from the landing URL every few days.
After a monitoring period, we were able to discover the evolution of the web-based malware injected to this site, as shown in Table 5.
From this table we can see: The first hop page directly linked out by the landing site did not change during this period, while there is a significant evolution of the following pages.
First, the domain names of the following hop pages and exploit pages change frequently throughout the whole month, probably to evade blacklist-based URL filtering or to avoid the disable of their DNS resolution.
Also we can see that some new exploits are added to the exploit kit and some are gone.
This is related to the discovery of new vulnerabilities and the abandonment of the out-of-date vulnerabilities.
In this section we discuss some limitations of our current implementation, including some possible attacks against WebPatrol.
Then we discuss our future work.The current implementation of WebPatrol pays no special attention to hide itself.
Thus it could be detected by a malware in a few ways.
For example, there are plugins that can not be installed within the same browser in reality.
This may because one plugin is only available on Windows and another on Linux, this is also because some plugins are not compatible with other plugins, or it is not possible for a browser having two versions of a plugin at the same time.
Currently, all these situations could happen in WebPatrol due to its intent to achieve better coverage of different run-time environments.
Also, as the specifications of different browsers have too many differences and details to be fully emulated.
There are always some implementation details that WebPatrol have not considered (e.g. the creation and manipulation of customized DOM Event objects).
Thus, intended malicious codes could detect the existence of WebPatrol.
We are in the process of investigating this problem and we believe some of the evasions could be carefully avoided.
We note that these evasion attacks are not unique to WebPatrol but to any browser emulator.
With the simplicity and flexibility design of WebPatrol design, implementing new browser features is fast and easy (adding some Python module).
Thus we can always learn from failed analysis cases and implement the missing functionality quickly.In addition to evasion, other attacks against WebPatrol could be divided into two categories: DoS attacks and vulnerability attacks.
DoS attackers may consume all the resources in the analysis environment by allocating large amounts of memories and do CPU-consuming operations continuously.
Currently WebPatrol can prevent such attacks by kill the processes that takes too much resources (with the tradeoff that this would affect the analysis of the scenarios).
As for the vulnerability attack, all the malicious codes are executed in the Spidermonkey JS engine and the Python SGML parser.
Thus, if there is any critical vulnerability within those components, our analysis system could also be vulnerable.
In the future, we may consider using multiple different JS engines and parsers.Finally, we will further improve the collection completeness of infection trails (as we are aware of the problems discussed in 6.3).
We will run the analyzer multiple times with different configurations, to emulate different browsers and improve the coverage.
We plan to add more system API support to libemu to improve its emulation capability.
We also plan to improve the coverage by integrating some static analysis techniques.
In the future, we will provide much more and deeper analysis on a larger scale of collected WMSs on the Internet.
In this paper we introduced the concept of a web-based malware scenario and its importance for web-based malware research.
We designed and implemented a prototype system for automated collection and live replay of web-based malware scenarios.
Our system can collect a relatively complete set of web infection trails and take snapshots of the scenario for future analysis.
In addition, we provide the live replay capability to enable an analyst to access the original web-based malware at any time.
We evaluated the system's effectiveness and showed several case studies to demonstrate the utilities of our system.
This work is partially supported by the Research Fund for the Doctoral Program of Higher Education of China under Grant No.200800011019, the project 61003127 supported by NSFC, and the project "A monitoring platform for web safe browsing"(2009-1717) supported by the National Development and Reform Commission.We would like to thank all the anonymous reviewers for their insightful comments and feedback.
We would like to thank Tao Wei, Xiaorui Gong, Chengyu Song, Huilin Zhang and Ruifei Yu for their comments on our research.
