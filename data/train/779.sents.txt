One of the test set problems was on the classic list of NP-complete problems given by Garey and Johnson [10]; these problems arise naturally in many other applications.
One can define a general framework for test set problems in the following manner.
We are given an universe of objects, family of subsets ("tests") of the universe and a notion of "distinguishability" of pairs of elements of the universe by a collection of these tests.
Our goal is to select a subset of these tests of minimum size that distinguishes every pair of elements of the universe.
To be precise, each of these problems is obtained by fixing parameters in the general test set problem TS Γ (k) as described below (2 X denotes the power set of a set X).
Definition 3.1 (Problem TS Γ (k) with Γ ⊆ 2 {0,1,2} and k being a positive integer)Instance: (n, S) where S ⊂ 2 {0,1,2,...,n−1} .
Terminologies:• A k-test is a union of at most k sets from S. • For a γ ∈ Γ and two distinct elements x, y ∈ {0, 1, 2, . . . , n − 1}, a k-test T γ-distinguishes x and y if |{x, y} ∩ T | ∈ γ.Valid solutions: A collection T of k-tests such that (∀x, y ∈ {0, 1, 2, . . . , n − 1} ∀γ ∈ Γ) x 񮽙 = y =⇒ ∃T ∈ T such that T γ-distinguishes x and y.Objective: minimize |T |.
This framework captures several "barcoding-type" problems in a few areas in bioinformatics and biological modeling such as:Minimum Test Collection Problem: This problem has applications in diagnostic testing [10].
Here a collection of tests distinguishes two objects if a test from the collection contains exactly one of them.
In our above formalism, this is precisely TS {1} (1).
Condition Cover Problem: Karp et al. [15] considered a problem of verifying a multi-output feedforward Boolean circuit as a model of biological pathways.
This problem can be phrased like the Minimum Test Collection Problem, except that two elements are distinguished by a collection of tests if one tests contains exactly one of them, and another contains both or none of them.
Assuming that the allowed perturbations are given as part of the input, this problem is identical to TS {1},{0,2} (1).
String Barcoding Problem (SB Σ (k)): In the "basic" version of this problem corresponding to k = 1, first discussed by Rash and Gusfield [21], the universe U consists of sequences (strings) over an alphabet Σ and any string v ∈ Σ * defines a test T v consisting of a collection of strings from U in which v appears 1 .
Since this chapter is significantly concerned with this basic version, we write the problem definition explicitly for the convenience of the reader.
We are given a set S of sequences over some alphabet Σ.
For a fixed set of m "distinguisher" sequences t = (t 0 , . . . , t m−1 ), the barcode code(s, t) for each s ∈ S is defined to be the Boolean vector (c 0 , c 1 , c m−1 ) where c i is 1 if t i is a substring of s.
We say that the set of distinguishers t defines a valid barcode if for any two distinct strings s, s ′ ∈ S, code(s, t) is different from code(s ′ , t).
Then the basic version SB Σ (1) is defined as follows:Instance: S ⊂ Σ * .
Valid solutions: a set of distinguisher sequences t defining a valid barcode.Objective: minimize | t|.1 Σ * is the standard notation of denoting the set of all possible strings formed by concatenation of zero or more symbols of Σ.As an example, let Σ = {A, C, T, G} and S = {AAC, ACC, GGGG, GT GT GG, T T T T }.
Then, the set of four distinguishers t = {A, CC, T T T, GT } defines the set of valid barcodes for the input sequences in S as shown below.A CC TTT GT AAC 1 0 0 0 ACC 1 1 0 0 GGGG 0 0 0 0 GTGTGG 0 0 0 1 TTTT 0 0 1 0The name "string barcoding" derives from the fact that the Boolean vector indicating the occurrence (as a substring) of the tests from an arbitrary collection of tests in a given input sequence is referred to as the "barcode" of the given sequence with respect to this collection of tests.
Motivations for investigating these problems come from several sources such as:• Database compression and fast database search for DNA sequences.
• DNA microarray designs for efficient virus identification in which the immobilized DNA sequences at an array element are from a set of barcodes.In general, for k > 1 a test can be defined by a set T of at most k strings and u ∈ U passes test T is one of strings in T is a substring of u; such tests may be feasible in practice as the one-string tests.Minimum Cost Probe Set Problem with a Threshold r (MCP Σ (r)): This problem is very similar to String Barcoding and was considered first by Borneman et al. [3].
Denote by oc(x, y) the number of occurrences of x in y as a substring, For a fixed set of m distinguisher sequences t = (t 0 , t 1 , . . . , t m−1 ), an r-barcode code(s, t) for any sequence s is defined to be the vector (c 0 , c 1 , . . . , c m−1 ) where c i = min{r, oc(t i , s)}.
Given a set S of sequences over some alphabet Σ, t defines a valid rbarcode if for any two distinct strings s, s ′ ∈ S, code(s, t) is different from code(s ′ , t).
MCP Σ (r) is now defined as follows:Instance: (r, S, P) where S, P ⊂ Σ * .
Valid solutions: a set of distinguisher sequences t ⊆ P defining a valid r-barcode.
Objective: minimize | t|.
This problem was used in [3] for minimizing the number of oligonucleotide probes needed for analyzing populations of ribosomal RNA gene (rDNA) clones by hybridization experiments on DNA microarrays; the probes are selected from a prespecified set P. However, it can also be used in the context of other string barcoding approaches where the barcodes are integer-valued as opposed to being Boolean.
Applications of barcoding techniques range from rapid pathogen identification in epidemic outbreaks to point-of-care medical diagnosis to monitoring of microbial communities in environmental studies (e.g., see [3,21]).
For example, genomic-based identification of microorganisms such as viruses or bacteria is performed by spotting or synthesizing on a microarray the Watson-Crick complements of the distinguisher strings, and then hybridizing to the array the fluorescently labeled DNA extracted from the unknown microorganism.
Under the assumption of perfect hybridization stringency, the hybridization pattern can be viewed as a string of k zeros and ones, referred to as the barcode of the microorganism.
By construction, the barcodes corresponding to the n microorganisms are distinct, and thus the barcode uniquely identifies any one of them.
To improve identification robustness, one may also require redundant distinguishability (i.e., at least m different distinguishers for every pair of microorganisms, where m > 1 is some fixed constant) and impose a lower bound on the edit distance between any pair of selected distinguishers [21].
A hypothetical system implementing a high level architecture that meets the design criteria for a First Responder Pathogen Detection System (FR-PDS) using string barcoding is shown schematically in Figure 3.1.
Such a hypothetical system includes the following three major components:(1) A component that provides rapid amplification of the collected genetic material, e.g. degenerate oligonucleotide primer based multiplex PCR.
(2) A pathogen fingerprinting and/or barcoding component (say, built around universal DNA tag arrays).
(3) Rapid and robust computational procedures to compute barcodes that produces short signatures and thereby both reduces database size and optimizes cost of designing the hybridization array.
In this section, we survey several algorithmic methods used to solve the barcoding problems.
We will then discuss in more details in the next two sections the set-covering and information content algorithmic approach.
In [21], Rash and Gusfield discussed some experimental results for SB Σ (1) but left open the exact complexity and approximability of this problem.
Their algorithmic approach is based on writing the problem as an integer program and then solving it directly.
Unfortunately, the run-time of this approach does not scale well with the number of microorganisms and the length of the genomic sequences; e.g., the largest instance sizes reported in [21] have a total genomic sequence length of around 100, 000 bases.
We will not discuss the integer programming formulation in more details since we will discuss heuristics based on set-covering methods in more details subsequently and an integer programming formulation for set-covering problem is well-known (e.g., see [23]).
Borneman et al. [3] noted that the MCP Σ (r) problem was NP-complete assuming that the lengths of the sequences in the prespecified set were unrestricted, and discussed some experimental results for a few heuristics that they implemented.
Their algorithmic approach is based on a Lagrangian relaxation of the integer programming formulation of set cover and simulated annealing approach.
In [2] Berman, DasGupta and Kao were able to provide tight theoretical worstcase approximability bounds for almost all of these problems.
A summary of the results in [2] is as follows (ℓ is the maximum length of any sequence in S, L is the total length of all sequences in S, and ε and δ are constants):• TS {1} (1) can be approximated to within a ratio of 1 + ln n in O(n 2 |S|) time and cannot be approximated to within a ratio of (1 − ε) ln n assuming NP񮽙 =DTIME(n log log n ).
• TS {1},{0,2} (1) can be approximated to within a ratio of 1 + ln 2 + ln n in O(n 2 |S|) time and cannot be approximated to within a ratio of (1 − ε) ln n assuming NP񮽙 =DTIME(n log log n ).
• SB Σ (1) can be approximated to within a ratio of 1+ln n in O(n 3 ℓ 2 ) time and cannot be approximated to within a ratio of (1 − ε) ln n assuming NP񮽙 =DTIME(n log log n ).
• MCP Σ (r) can be approximated to within a ratio of [1 + o(1)] ln n in O(n 2 |P| + LP|) time and cannot be approximated to within a ratio of (1 − ε) ln n assuming NP񮽙 =DTIME(n log log n ).
• TS {1} (n δ ) cannot be approximated to within a ratio of n ε assuming NP񮽙 =co-RP for any 0 < ε < δ < 1.
• SB Σ (n δ ) cannot be approximated to within a ratio of n ε assuming NP񮽙 =co-RP for any 0 < ε < δ < 1 2 .
The provably optimal algorithmic approach in [2] uses an entropy-based algorithmic approach that they term as the "information content" approach.
Informally, this is a greedy technique based on information content of a partial solution; the notion of information content is directly related to the Shannon information complexity [1,22].
The greedy approach seeks to select an augmenting step for a partial solutions that maximizes the new information content of the augmented partial solution as compared to the partial solution.A key non-trivial step for applicability of this technique is to define a suitable easy-to-compute measure of the information content of a partial solution such that the monotonicity of this measure is ensured with respect to any subset of an optimal solution.
The next section defines the approach more precisely.
In this section we discuss the information content approach for TS {1} as designed in [2] running in time O(n 2 |S|) time with an approximation ratio of 1 + ln n. Notice that the upper bound almost matches the lower bound stated in Section 3.4.3 for SB {0,1} , a special case of TS {1} .
For simplicity, we illustrate the approach for the problem TS {1} .
In the definition below and throughout the rest of this section we use T +T to denote T ∪ {T }.
• an equivalence relationT ≡ on {0, 1, 2, . . . , n−1} given by i T ≡ j if and only if ∀T ∈ T (i ∈ T ≡ j ∈ T ),• a set of permutations Π T = {π ∈ (permutations of {0, 1, 2, . . . , n − 1}) :∀i ∈ [0, n − 1] i T ≡ π(i)},• entropy H T = log 2 |Π T |.
• information content of a T ∈ S with respect to T , IC(T,T ) = H T − H T +T = log 2 |ΠT | |ΠT +T | .
As an example, consider T = {{1, 2, 3, 4}, {1, 5, 6}} with n = 8.
Then, the equivalence classes of T ≡ are {1}, {2, 3, 4}, {5, 6}, {7, 8} and H T = log 2 ((3!)
(2!)
(2!))
≈ 4.585.
The above definition of entropy is somewhat similar (but not the same)to the one suggested in [18].
Suppose that the equivalence relation T ≡ on {0, 1, 2, . . . , n − 1} produces q equivalence classes of size s 1 , s 2 , . . . , s q .
Then, the entropy suggested in [18] is 1 n log 2 (Π q i=1 s si i ) whereas our entropy H T is log 2 (Π q i=1 s i !)
.
The information content heuristic (ICH for short) is the following simple greedy heuristic:T = ∅ while HT 񮽙 = 0 do select a T ∈ S − T that maximizes IC(T, T ) T = T + TThe correctness of ICH follows from the fact that H T = 0 implies the equivalence classes of T ≡ are n singleton sets {0}, {1}, . . . , {n − 1} and the fact that if H T 񮽙 = 0 then there exists a T ∈ S \ T with IC(T, T ) > 0 (otherwise the problem instance has no feasible solution).
To implement ICH, one iteratively maintains the equivalence classes of T ≡ as sorted lists.
We also precompute and store log 2 (i!)
for each i ∈ [1, n].
Given a specific T ∈ S − T , it is easy to compute in O(n) time the equivalence classes to IC(T, T ).
The performance guarantee of the above approach is given by the following theorem proved in [2] using a very careful amortized analysis.
• for TS {{1}} an approximation ratio of 1 + ln n;• for TS {{1},]{0,2}} an approximation ratio of 1 + ln 2 + ln n;• for MCP Σ (r) an approximation ratio of 1 + ln n + ln log 2 (r ′ + 1), where r ′ = min{r, n}.
Methods based on this approach enable distinguisher selection based on whole genomic sequences of hundreds of microorganisms of up to bacterial size on a well-equipped workstation, and can be easily parallelized to further extend the applicability range to thousands of bacterial size genomes.
Whole-genome based selection is beneficial in at least two significant ways.
First, it simplifies assay design since the DNA of the unknown pathogen can be amplified using inexpensive general-purpose whole-genome amplification methods such as specialized forms of degenerate primer multiplex PCR [4] or multiple displacement amplification [9].
Second, whole-genome based selection results in a reduced number of distinguishers, often very close to the information theoretic lower bound of ⌈log 2 n⌉.
Set covering approaches are based on a simple greedy selection strategyin every iteration we pick a substring that distinguishes the largest number of not-yet-distinguished pairs of genomic sequences.
This selection strategy is an embodiment of the greedy setcover algorithm (e.g., see [23]) for a problem instance with O(n 2 ) elements corresponding to the pairs of sequences.
Hence, by a classical result of [5,14,17], the algorithm guarantees an approximation factor of 2 ln n for the barcoding problem.
Experimental results provided in [7,8] show that our setcover greedy algorithm produces solutions of virtually identical quality to those obtained by the information content heuristic.The setcover greedy algorithm is extremely versatile, and can be easily extended to handle redundancy and minimum edit distance constraints, as well as other biochemical constraints on individual distinguisher sequences.
Furthermore, the greedy setcover algorithm can also take into account genomic sequence uncertainties expressed in the form of degenerate bases.
Although degenerate bases are ubiquitous in genomic databases, previous works have not recognized the need to properly handle them.
In this section for simplicity we present the implementation of the setcover greedy algorithm as provided in [7,8] in the context of the basic version of the string barcoding problem only.
Implementation modifications needed to handle the robust barcoding problem in its full generality are available in [8].
The implementation of the setcover greedy algorithm has two main phases: a candidate generation phase and a candidate selection phase.In the candidate generation phase a representative set of candidate distinguishers is generated from the given genomic sequences.
Essentially they use an incremental algorithm for quickly generating a representative set of candidate distinguishers and collecting all their occurrences in the given genomic sequences.
For each generated candidate, we also compute the list of sequences with which the candidate has perfect matches; this information is needed in the candidate selection phase.
To reduce the number of candidates, we avoid generating any substring that appears in all genomic sequences, which typically eliminates very short candidates.
For each genomic sequence, we also make sure to generate only one of the substrings that appear exclusively in that sequence; this optimization eliminates from consideration most candidate distinguishers above a certain length.
Unlike the suffix tree method proposed by Rash and Gusfield [21], this approach may generate multiple candidates that appear in the same set of k genomic sequences (for 1 < k < n).
However, the penalty of having to evaluate redundant candidates in the candidate selection phase is offset in practice by the faster candidate generation time.
Efficient implementation of the above candidate elimination rules is achieved by generating candidates in increasing order of length and using exact match positions for candidates of length l − 1 when generating candidates of length l. For each position p in the input genomic sequences, we also maintain a flag to indicate whether or not the algorithm should evaluate candidate substrings starting at p.
The possible values for the flag are TRUE (the substring of current length starting at p is a possible candidate), FALSE (we have already saved the substring of current length starting at p as a candidate), or DONE (all candidates containing as prefix the substring of current length starting at p are redundant, i.e., the position can be skipped for all remaining candidate lengths).
Initially all flags are set to TRUE.
The FALSE flags are reset to TRUE whenever we increment the candidate length, however, we never reset DONE flags.For every candidate length l, candidate evaluation proceeds sequentially over all positions of the genomic sequences.
Whenever we reach a position p whose flag is set to TRUE, we use the list of matches for the substring of length l − 1 starting at p (or a linear time string matching algorithm if l is the minimum candidate length) to determine the list of matches for the substring of length l starting at p, and set the flag to FALSE for all positions where these matches occur.
If the substring of length l starting at p has matches only within the source sequence, and we have already generated a "unique" candidate for this sequence, we discard the candidate and set the flag of p to DONE.A further speed-up technique is to generate candidate distinguishers from a strict subset of the input sequences.
Although this speed-up can potentially affect solution quality, experimental results show that the solution quality loss for whole-genome barcoding is minimal, even when we generate candidates based on a single input sequence, which corresponds to pre-assigning a barcode of all 1's to this sequence.After the set of candidates is generated we select the final set of distinguishers in the greedy phase of the algorithm (Figure 3.2).
We start with an empty set of distinguishers D.
While there are pairs of sequences that are not yet distinguished by D, we loop over all candidates and compute for each candidate c the number ∆(c, D) of pairs of sequences that are distinguished by c but not by D, then add the candidate c with largest ∆ value to D. Input: Set C of candidate distinguishers Output: Set D of selected distinguishers D ← ∅; For every c ∈ C, ∆ old (c) ← ∞ Repeat ∆ * ← 0 For every c ∈ C with ∆ old (c) > ∆ * do // Since ∆(c, D) ≤ ∆ old (c), c can be ignored if ∆ old (c) ≤ ∆ * ∆ old (c) ← ∆(c, D) If ∆(c, D) > ∆ * then ∆ * ← ∆(c, D); c * ← c If ∆ * > 0 then D ← D ∪ {c * } While ∆ * > 0 Return D∆(c, D) = k i=1 |S i ∩ P c | · |S i \ P c | (3.1)In addition to the fast partition based computation, the implementation of the greedy selection phase uses a lazy strategy for updating the ∆ values, based on the observation that they are monotonically non-increasing during the algorithm (see Figure 3.2).
Thus, the efficient implementation of the greedy selection phase of the algorithm combines a partition based method for computing the coverage gain of candidate distinguishers (this method was first proposed in the context of the information content heuristic in [2]) with a "lazy" strategy for updating coverage gains.
The authors in [7,8] performed experiments on both randomly generated instances and whole microbial genomes extracted from the NCBI databases.
Random testcases were generated from the uniform distribution induced by assigning equal probabilities to each of the four nucleotide; these testcases do not contain any nucleotides with degeneracy greater than 1.
The NCBI testcase represents a selection of 29 complete microbial sequences, varying in length between 490, 000 and 4, 750, 000 bases (over 76 million bases in total).
All experiments were run on a PowerEdge 2600 Linux server with 4 Gb of RAM and dual 2.8 GHz Intel Xeon CPUs -only one of which is used by the sequential algorithms.
As described in Section 3.6.1, there are two main phases in the algorithm: candidate distinguisher generation, and greedy candidate selection.
Results were reported about the average candidate selection CPU time for n random sequences of length 10, 000 and redundancy 1, averaged over 10 instances of each size.
Combining the two speed-up techniques for this phase (partition based coverage gain computation and lazy update of candidate gains) results in over two orders of magnitude reductions in runtime.A further speed-up technique is to generate candidate distinguishers from a select subset of the input sequences.
Although this speed-up can potentially affect solution quality, the results showed that on large instances the solution quality loss is minimal even when we generate candidates based on a single input sequence, this corresponds to pre-assigning a barcode of all 1's to this sequence.
The technique reduces significantly both the memory requirement (which is proportional to the number of candidates and the number of times they match input sequences) and the runtime required for candidate generation and greedy selection.The quality of the solution in the simulations were as follows.
The number of distinguishers returned by the setcover greedy algorithm were reported for redundancy varying between 1 and 20 on between 10 and 1, 000 random sequences of length 10, 000.
These results were compared with the results obtained by the information content heuristic results of [2], as well as the information theoretic lower bound of ⌈log 2 n⌉ for the case when the redundancy requirement is 1.
The number of distinguishers returned by the setcover greedy algorithm was virtually identical to that returned by the information content heuristic, despite the latter one having a better approximation guarantee.
Furthermore, the results for redundancy one were within 50% of the information theoretic lower bound for the range of instance sizes considered in this experiment.
The gap between the solutions returned by the algorithms and the lower bound does increase with the number of sequences; however it is not clear how much of this increase is caused by degrading algorithm solution quality, and how much by degrading lower bound quality.
The algorithm was run on a set of 29 complete microbial genomic sequences extracted from NCBI databases [19].
Sequence lengths in the set vary between 490 Kbases and 4.75 Mbases, with an average length of 2.6 Mbases (over 76 Mbases total).
In these experiments we varied the redundancy requirement from 1 to 20.
To see the effect of length and edit distance requirements on the number of distinguishers, for each redundancy requirement they computed both an unconstrained solution, and a solution in which distinguishers must have length between 15 and 40, and there should be a minimum edit distance of 6 between every two selected distinguishers (these values are similar to those used in [21].
In all experiments, they generated candidates based only on the shortest sequence of 490 Kbases.Naturally, meeting higher redundancy constraints requires more distinguishers to be selected.
Additional length and edit distance constraints further increase the number of distinguishers, but the latter is still within reasonable limits.
The length constraints reduce the number of candidates (from 1,775,471 to 122,478), which, for low redundancy values has the effect of reducing greedy selection time.
However, for high redundancy requirements the reduction in number of candidates is offset by the increase in solution size, and greedy selection becomes more time consuming with length and edit distance than without (selection time grows roughly linearly with solution size).
The implementation of the set-covering approach, which was named DNA-BAR, can be used online through the web interface provided at http://dna.
engr.uconn.edu/ ~ software/DNA-BAR/.
The open source C code, released under the GNU General Public License, is also available at the above address.
In many practical pathogen identification applications collected biological samples may contain the DNA of multiple pathogens.
This issue is considered to be particularly significant in medical diagnosis applications (e.g., see [11] for studies in detecting more than one HPV (human papilomavirus) genotype with varying rate of multiple HPV infections carried by the same HPV carrier).
A significant future research direction could be to develop extensions of the barcoding technique that can reliably detect multiple pathogens for a given bound on the number of pathogens present.
Bhaskar DasGupta was supported in part by NSF grants IIS-0346973, IIS-0612044 and DBI-0543365.
Ion M˘ andoiu was supported in part by NSF grant DBI-0543365.
The authors would also like to thank all their collaborators in these research topics.
