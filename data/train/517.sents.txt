We describe a procedure that takes a classical planning task, a forward-search state, and a set of abstraction-based admissible heuristics, and derives an optimal additive composition of these heuristics with respect to the given state.
Most importantly , we show that this procedure is polynomial-time for arbitrary sets of all known to us abstraction-based heuristics such as PDBs, constrained PDBs, merge-and-shrink abstractions , fork-decomposition structural patterns, and structural patterns based on tractable constraint optimization.
Admissible heuristics are critical for effective planning when either optimal or approximately-optimal solutions are required.
As automated planning is known to be NP-hard even for extremely conservative problem formalisms (By- lander 1994), no heuristic should be expected to work well in all planning tasks.
Moreover, even for a fixed planning task, typically no tractable heuristic will home in on all the "combinatorics" of the task in hand.
The promise, however, is that (i) different heuristics will target different bolts of the planning complexity, and (ii) composing the individual strengths of numerous heuristics could allow us both solving a larger range of planning tasks, as well as solving each individual task more efficiently.Since the late 90's, numerous (though not many) admissible heuristics for domain-independent planning have been suggested and found useful, and research in this direction becomes more and more active.
In this paper we focus on the old question of how one should better orchestrate a set of admissible heuristics in the effort of solving a given planning task.
One of the well-known and heavily-used properties of admissible heuristics is that taking the maximum of their values maximizes informativeness while preserving admissibility.
A more recent, alternative approach to orchestrating a set of admissible heuristics corresponds to carefully separating the information used by the different heuristics in the set so that they values could be summed up instead of maximized over.
This di-rection was first exploited in the works on additive pattern database (APDB) heuristics (Edelkamp 2001;Felner, Korf, & Hanan 2004), and more recently it was applied in the scope of constrained PDBs, m-reachability, and structural patterns heuristics (Haslum, Bonet, & Geffner 2005;Katz & Domshlak 2008b).
The basic idea underlying all these "additive heuristic ensembles" is elegantly simple: for each problem's action a, if it can possibly be counted by more than one heuristic in the ensemble, then one should ensure that the cumulative accounting for the cost of a does not exceed its true cost in the original problem.Until very recently, such "action-cost partitioning" was achieved in one certain manner by accounting for the whole cost of each action in computing a single heuristic, while ignoring the cost of that action (by setting it to zero) in computing all the other heuristics in the set (Edelkamp 2001;Felner, Korf, & Hanan 2004;Haslum, Bonet, & Geffner 2005).
Recently, this "all-in-one/nothing-in-rest" actioncost partitioning has been generalized by Katz and Domsh- lak (2008b) and Yang et al. (2007) to arbitrary partitioning of the action cost among the ensembles' heuristics.The great flexibility of additive heuristic ensembles, however, is a mixed blessing.
For good and for bad, the methodology of taking the maximum over the values provided by an arbitrary set of independently constructed admissible heuristics is entirely non-parametric.
In contrast, switching to additive heuristic ensembles requires selecting an action-cost partitioning scheme, and this decision problem poses a number of computational challenges.
In particular, 1.
The space of alternative choices here is verbally infinite as the cost of each action can be partitioned into an arbitrary set of non-negative real numbers, sum of which does not exceed the cost of that action.
2.
At least in domain-independent planning, this decision process should be fully unsupervised.
3.
The last but not least, the relative quality of each actioncost partition (in terms of the informativeness of the resulting additive heuristic) may vary dramatically between the examined search states.
Hence, the choice of the action-cost partitioning scheme should ultimately be a function of the search state in question.
These issues may explain why all previous works on (both domain-dependent and independent) additive heuristic ensembles adopt this or another ad hoc (and searchstate independent) choice of action-cost partitioning.
As the result, all the reported empirical comparative evaluations of various max-based and additive heuristic ensembles are inconclusive-for some search states along the search process the (pre-selected) additive heuristics' combination was dominating the max-combination, while for the other states the opposite was the case.
In the context of domain-specific APDBs, Yang et al. (2007) conclude that "determining which abstractions [here: action-cost partitioning schemes] will produce additives that are better than max over standards is still a big research issue.
"The contribution of this paper is precisely in addressing the problem of choosing the right action-cost partitioning over a given set of heuristics.
Specifically, we • Provide a procedure that, given (i) a classical planning task Π, (ii) a forward-search state s of Π, and (iii) a set of admissible heuristics based on over-approximating abstractions of Π, derives an optimal action-cost partitioning for s (that is, a partitioning that maximizes the heuristic estimate of that state).
The procedure is fully unsupervised, and is based on a linear programming formulation of that optimization problem.
• Show that the time complexity of our procedure is polynomial for arbitrary sets of all known to us abstractionbased heuristic functions.
In particular, such "procedurefriendly" heuristics include PDBs (Edelkamp 2001;Yang, Culberson, & Holte 2007), constrained PDBs (Haslum, Bonet, & Geffner 2005), merge-and-shrink abstractions (Helmert, Haslum, & Hoffmann 2007), forkdecomposition structural patterns (Katz & Domshlak 2008b), and structural patterns based on tractable constraint optimization (Katz & Domshlak 2008a).
Notice that, in particular, the estimate provided by a maxbased ensemble corresponds to the estimate provided by the respective additive ensemble under some action-cost partitioning.
As such, the max-estimate cannot exceed the one provided by the optimal action-cost partitioning, and thus, at least for the abstraction-based heuristics, we answer the aforementioned question of "to add or not to add".
We consider the standard setting of cost-optimal classical planning for problems described using the SAS + representation language (Bäckström & Nebel 1995).
A SAS + planning task is a quintuple Π = V , A, I , G, cost, where • V = {v 1 , . . . , v n } is a set of state variables, each associated with a finite domain dom(v i ); each complete assignment s to V is called a state.
I is an initial state, and the goal G is a partial assignment to V .
• A is a finite set of actions, where each action a is a pair pre(a), eff(a) of partial assignments to V called preconditions and effects of a, respectively, and cost : A → R 0+ is a non-negative real-valued action cost function.
The semantics of a planning task Π is given by its induced state-transition model, often also called transition graph.
Searching in this transition graph corresponds to forward state-space search.
In what follows we distinguish between the actual edge-weighted transition graph, and its weightsomitted, qualitative skeleton which we call transition-graph structure.
Informally, transition-graph structures capture the combinatorics of the classical planning problems, while transition graphs annotate this combinatorics with "performance measures".
• A transition-graph structure (or TG-structure, for short) is a quintuple T = (S, L, Tr, s I , S G ) where S is a finite set of states, L is a finite set of transition labels, Tr ⊆ S × L × S is a set of (labeled) transitions, s I ∈ S is an initial state, and S G ⊆ S is a set of goal states.
• A transition graph is a pair T, ̟ where T is a TGstructure with labels L, and ̟ : L → R 0+ is a transition cost function.
For a state s ∈ S and a subset of states S ′ ⊆ S in T, the distance dist(s, S ′ ) in T, ̟ is the cost of a cheapest (with respect to ̟) path from s to a state in S ′ along the transitions of T. Any path from s I to S G is a plan for T, ̟, and cheapest such paths are called optimal plans.The states of the TG-structure T(Π) induced by a SAS + planning task Π = V , A, I , G, cost are the states of Π, the transition labels of T(Π) are the actions A, and [v] is specified, and other-(s, a, s ′ ) ∈ Tr iff (i) s[v] = pre(a)[v] whenever pre(a)[v] is specified and (ii) s ′ [v] = eff(a)[v] if eff(a)wise s ′ [v] = s[v].
The actual transition graph induced by Π is T(Π), cost.
Our focus here is on additive ensembles of admissible heuristics, or simply, additive heuristics.
Very often, each individual admissible heuristic for domain-independent planning is defined as the optimal cost of achieving the goals in an over-approximating abstraction of the planning problem in hand 1 (Pearl 1984).
Such an abstraction is obtained by relaxing some constraints present in the problem, and the desire is to obtain a tractable (that is, solvable in polynomial time), yet informative abstract problem.
In turn, by additive abstraction we refer to a set of abstractions, interconstrained by a requirement to jointly not over-estimate the transition path costs of the original problem.
T, ̟ is a set of pairs A = {T i , ̟ i , α i } k i=1 where, for 1 ≤ i ≤ k, T i , ̟ i is a transition graph with structure T i = (S i , L i , Tr i , s I i , S G i ), α i : S → S i is a function called abstraction mapping, α i (s I ) = s I i , α i (s) ∈ S G ifor all s ∈ S G , and, for each pair of states s, s ′ ∈ S, holdsk i=1 dist(α i (s), α i (s ′ )) ≤ dist(s, s ′ ).
(1)For k = 1, Definition 1 formalizes standard, non-additive abstractions, while for k ≥ 1 it poses only a general requirement of not overestimating the distances.
For our objective of dealing with action-cost partitioning, we need a tighter binding between the original and abstract TG-structures.
Specifically, we need to (i) associate each abstract transition label with a single original transition label, and (ii) verify that each original transition corresponds to an appropriate set of abstract transition paths.
(ABS- ensemble) of a TG-structure T = (S, L, Tr, s I , S G ) is a set of triplets AE = {T i , α i , β i } k i=1 where, for 1 ≤ i ≤ k, -T i = (S i , L i , Tr i , s I i , S G i) is a TG-structure, -α i : S → S i is an abstraction mapping, and • The (possibly confusing at first view) setting of all α i being injective (with, possibly, |S| < |S i |) captures additive embedding abstractions, obtained by expanding the original action set by some new actions derived from the original ones.
In such cases, the new actions are constructed with certain desired properties such as positive and/or unary effects only (Bylander 1994), etc.β i : L i → L is an action-associating mapping, -α i (s I ) = s I i , and α i (s) ∈ S G i for all s ∈ S G , -for each transition s, l, s ′ ∈ Tr, there is a path ρ from α i (s) to α i (s ′ ) in T i such that (i) all• Each individual abstraction in an ABS-ensemble may correspond to a hybrid (homomorphism/embedding) abstraction such as these induced by some structural patterns (Katz & Domshlak 2008b).
• Importantly, nothing in the definition of ABS-ensemble prevents us from using an arbitrary mixture of the above three types of abstractions.
First things first, however, Theorem 1 relates ABSensembles and additive abstractions induced by the former via action-cost partitioning (expressed by Eq.
2).
Worth noting here that the generality of Definition 2 and Theorem 1 is not for an exercise only-later we exploit it to establish some computational results of an adequate generality.
(Due to the lack of space, the proofs are given in the full TR.)
Theorem 1 (Additive Abstractions) Let T be a TGstructure with labels L, and let AE = {T i , α i , β i } k i=1 be an ABS-ensemble of T. For any function ̟ : L → R 0+ , and any set of functions̟ i : L i → R 0+ , 1 ≤ i ≤ k, such that ∀l ∈ L : k X i=1 X l ′ ∈β −1 i (l) ̟i(l ′ ) ≤ ̟(l),(2)we have A = {T i , ̟ i , α i } k i=1being an additive abstraction of the transition graph T, ̟.
Π = V , A, I , G, cost is a SAS + planning task, AE = {T i , α i , β i } k i=1is an ABS-ensemble of T(Π), and {̟ i :L i → R 0+ } k i=1is a set of transition cost functions, we say thatA = {T i , ̟ i , α i } k i=1is an additive abstraction of Π with respect to AE, denoted by A Π AE, if ∀a ∈ A :k i=1 l∈β −1 i (a) ̟ i (l) ≤ cost(a)(3)In other words, each additive abstraction A Π AE corresponds to a certain action-cost partitioning of Π over AE and, importantly, vice versa.Definition 3 Let Π = V , A, I , G, cost be a SAS + planning task with state set S, andAE = {T i , α i , β i } k i=1 be an ABS-ensemble of T(Π).
For any additive abstraction A = {T i , ̟ i , α i } k i=1 Π AE, the additive heuristic h A is the function assigning to each state s ∈ S the quan- tity k i=1 dist(α i (s), S G i ).
The optimal additive heuristic h AE is the function assigning to each state s ∈ S the quantity max AΠAE h A (s).
Definition 3 specifies the set of all additive heuristics for Π obtainable via action-cost partitioning over a given ABS-ensemble.
Most importantly, it also specifies an upperbound on the heuristic estimate h AE (s) that can possibly be obtained for a state s on the basis of that (infinite) set of additive heuristics.
The admissibility of each heuristic h A in that space is immediate from Definition 1, and thus the proof of Theorem 2 is straightforward.Theorem 2 (Optimal Admissibility) For any SAS + planning task Π = V , A, I , G, cost, any ABS-ensemble AE of T(Π), and any state s of Π, we have h AE (s) ≤ dist(s, S G ).
Having specified the notion of optimal additive heuristic h AE , we now proceed with the computational part of the story.
Suppose we are given a SAS + planning task Π = V , A, I , G, cost with state set S, and an ABS-ensembleAE = {T i , α i , β i } k i=1 of T(Π).
Assuming that individual additive heuristics h A can be efficiently computed for all A Π AE (as it is the case with the ABS-ensembles of practical interest), the big question now is whether h AE can be efficiently computed.
Here we characterize a family of ABS-ensembles for which the answer to this question is affirmative, and show that this family comprises all the abstraction-based heuristic schemes suggested so far for domain-independent classical planning.Our characterization is constructive in terms of computability of h AE (s) via a compact 2 linear program induced by the triplet of Π, AE, and s.
We begin with introducing a set of linear constraints specifying all possible cost partitions of the actions a ∈ A over their representatives∪ k i=1 β −1 i (a) in the components of AE.
For each (abstract) label l ∈ ∪ k i=1 L i , letw l be a non-negative real-valued variable uniquely associated with l, and let the set of all these "label-cost" variables being denoted by − → w .
The (linear) additivity constraint C add ( − → w ) of Π on AE mirrors Eq.
3 as ∀a ∈ A :k i=1 l∈β −1 i (a) w l ≤ cost(a),(4)with H add being the convex polyhedron specified byC add ( − → w ).
Note that there is a straightforward bijective correspondence between the points w ∈ H add and (with some abuse of notation) the additive abstractionsA w = {T i , w, α i } k i=1 Π AE.Using the notion of additivity constraint C add ( − → w ), we now proceed with characterizing our "h AE -friendly" family of LP-optimizable ABS-ensembles.
Definition 4 Let Π = V , A, I , G, cost be a SAS + plan- ning task, AE = {T i , α i , β i } k i=1be an ABS-ensemble of T(Π), and C add ( − → w ) be the additivity constraint of Π on AE.Given a state s ∈ S, an LP-encoding of AE with respect to s is a tripletL(s) = − → x , f ( − → x ), C AE ( − → xw)where − → x is a set of non-negative real-valued variables, f is a real-valued affine function over − → x , C AE is a set of linear constraints on − → x and − → w , and∀w ∈ H add : max xw∈H AE f (x) = h Aw (s),(5)where H AE is the convex polyhedron specified by C AE ∪ C add .
The ABS-ensemble AE is called LP-optimizable if, for every state s ∈ S, there exists (and one can generate in poly-time) a compact LP-encodingL(s) = − → x , f ( − → x ), C AE ( − → xw) of AE with respect to s.The next two theorems provide the two cornerstones of characterizing our "h AE -friendly" family of ABSensembles on the ground of their LP-optimizability.
Given a SAS + planning task Π, and an ABS-ensemble AE of T(Π), if AE is LP-optimizable, then h AE (s) is poly-time computable for every state s ∈ S.Theorem 4 (Composition) For any SAS + planning task Π, and any set of LP-optimizable ABS-ensembles{AE 1 , . . . , AE m } of T(Π), if m = O(poly(|Π|)), then the "composite" ABS-ensemble AE = ∪ m i=1 AE i of T(Π) is also LP-optimizable.
With these two gratifying properties of LP-optimizable ABS-ensembles in hand, in what follows we check whether any of the known families of abstraction-based heuristics actually leads to such LP-optimizable ABS-ensembles.
The answer to this question turns out to be positive to a surprising extent.
Probably the most well-known family of additive abstractions corresponds to additive pattern databases (APDBs).
The idea behind various PDB heuristics is elegantly simple, and goes back to the seminal paper of Culberson and Schaeffer (1998).
For any SAS + planning task Π = V , A, I , G, cost, any subset of variables V ′ ⊆ V defines an over-approximating projection abstractionΠ [V ′ ] = V ′ , A [V ′ ] , I [V ′ ] , G [V ′ ], cost ′ of Π by intersecting the initial state, the goal, and all the actions' preconditions and effects with V ′ (Edelkamp 2001).
In terms of ABS-ensembles, this can be formalized as follows.Definition 5 Given a SAS + planning task Π = V , A, I , G, cost, and a set of variable subsets [Vi] , and β i (a [Vi] {V 1 , . . . , V k } of V , the pattern-database ABS-ensemble of T(Π) is AE = {T i , α i , β i } k i=1 where, for 1 ≤ i ≤ k, -T i = (S i , L i , Tr i , s I i , S G i ) is a TG-structure with S i = dom(V i ), L i = A [Vi] , s I i = I [Vi] , S G i = {s [Vi] | s ∈ S G }, and, overall, |T i | = O(poly(|Π|)), -α i (s) = s) = a, -α i (s), l, α i (s ′ ) ∈ Tr i iff s, l, s ′ ∈ Tr.
While any additive PDB abstraction A = {T i , ̟ i , α i } k i=1Π AE induces an (admissible) additive heuristic h A , so far, the actual choice of abstraction (that is, the actual action-cost partitioning strategy) has remained an open issue (Yang, Culberson, & Holte 2007).
This is exactly where LP-optimization gradually comes into the picture.Without loss of generality, let the number of patterns k be O(poly(Π)).
Computing h A = k i=1 dist(α i (s), S G i ) for a fixed APDB abstraction A = {T i , ̟ i , α i } k i=1Π AE is usually done by computing each dist(α i (s), S G i ) using the Dijkstra algorithm over the explicitly constructed transition graph T i , ̟ i .
However, the corresponding single-source shortest path problem also has an elegant LP formulation.
Given a directed graph G = (N, E) and a source node v ∈ N , if d(v ′ ) is a variable corresponding to the shortest-path length from v to v ′ , then the solution of the linear programmax − → d v ′ d(v ′ ) s.t. d(v) = 0 d(v ′′ ) ≤ d(v ′ ) + w(v ′ , v ′′ ), ∀(v ′ , v ′′ ) ∈ E(6)induces a solution to the single-source shortest path problem over G and source v. Given that, a compact LP-encoding of a pattern-databaseABS-ensemble AE = {T i , α i , β i } k i=1is obtained by (i) putting together the linear constraints as in Eq.
6 for TG-structures T i , (ii) replacing the edge-weight constants w(v ′ , v ′′ ) by variables associated with the corresponding transition labels, and (iii) connecting the latter label-cost variables with the proper additivity constraint.
Specifically, given a SAS + planning task Π = V , A, I , G, cost, and a pattern-database ABS-ensemble AE = {T i , α i , β i } k i=1 of T(Π), the LP-encoding construction is as follows.First, let the label-cost variables − → w contain a variable w a ′ for every abstract action a ′ ∈ ∪ m i=1 A [Vi] ; the additivity constraints are defined in terms of these label-cost variables exactly as in Eq.
4.
Now, given a state s of Π, we specify the LP-encoding L(s) = − → x , f ( − → x ), C AE ( − → xw) of AE with respect to s as− → x = k i=1 {d(s ′ ) | s ′ ∈ S i } ∪ {d(G i )} C AE =    d(s ′ ) ≤ d(s ′′ ) + w a ′ , ∀s ′′ , a ′ , s ′ ∈ Tr i d(s ′ ) = 0, s ′ = s [Vi] d(G i ) ≤ d(s ′ ), s ′ ∈ S G i , ∀i f ( − → x ) = k i=1 d(G i )Since each TG-structure T i in a pattern-database ABSensemble AE is of size O(poly(|Π|)), it is immediate that the above LP-encoding of AE is both compact and poly-time constructible for any state s of Π.
Hence, we have h AE (s) being poly-time computable for any planning task Π, any pattern-database ABS-ensemble AE, and any state s of Π.
Moreover, the same single-source shortest-path problems on explicit transition graphs underlie heuristics corresponding to additional powerful homomorphism abstractions such as constrained PDBs (Haslum, Bonet, & Geffner 2005), and merge-and-shrink abstractions (Helmert, Haslum, & Hoff- mann 2007).
Hence, using the "composition" Theorem 4, we can summarize here with a tractability claim that covers arbitrary combinations of such abstractions.Theorem 5 Given a SAS + planning task Π, and an ABS-ensemble AE = {T i , α i , β i } k i=1 of T(Π), if k i=1 |T i | = O(poly(|Π|)), then AE is LP-optimizable, and thus h AE (s) is poly-time computable for every state s ∈ S. PDB heuristics and their enhancements are successfully exploited these days in the planning systems (Haslum, Bonet, & Geffner 2005;Haslum et al. 2007).
However, since the reachability analysis in Π [Vi] is done by exhaustive search, each pattern of a PDB heuristic (that is, each selected variable subset V i ) is required to be as small as O(log |V |) if |dom(v)| = O(1) for each v ∈ V i , and as small as O (1), otherwise.
For many planning domains this constraint implies an inherent scalability limitation of the PDB heuristics.
One attack on this limitation within the scope of homomorphism abstractions correspond to the (already covered by Theorem 5) merge-and-shrink abstractions that sophisticatedly compress the abstract transition graphs to keep them within the reach of exhaustive graph searching (Helmert, Haslum, & Hoffmann 2007).
Another recent proposal generalizes PDB abstractions to what is called structural patterns (Katz & Domshlak 2008b).
The basic idea behind structural patterns is in abstracting the problem in hand into instances of provably tractable fragments of optimal planning, alleviating by that the limitation of PDBs to use projections of only low dimensionality.
In particular, Katz and Domshlak (2008b) specify a family of causal-graph structural patterns (CGSPs), and introduce a concrete instance of CGSPs called forkdecomposition.
In a one-paragraph summary, the mechanism of fork-decomposition works as follows.The causal graph CG(Π) = (V, E) of a SAS + planning task Π = V , A, I , G, cost with variables V = {v 1 , . . . , v n } is a digraph over nodes V .
An arc (v, v ′ ) belongs to CG(Π) iff v 񮽙 = v ′ and there exists an action a ∈ A such that eff(a)[v ′ ], and either pre(a) [v] or eff(a) [v] are specified.
For each variable v i ∈ V , let V f i ⊆ V contain v i and all its immediate successors in CG(Π), and V i i ⊆ V contain v i and all its immediate predecessors in CG(Π).
The fork-decomposition of Π is obtained by (1) schematically constructing a set of projection homomor-phism abstractions Π = {Π [V f i ] , Π [V i i ] } n i=1(with, possibly, each |V f i | and each |V i i | being Θ(n)), (2) reformulating the actions of the abtractions to singleeffect actions only so that the causal graphs ofΠ [V f i ] and Π [V i i ]become "forks" and "inverted forks", respectively, rooted in v i ; after this action reformulation, the individual abstractions may cease being purely homomorphic,(3) within each Π [V f i ], abstracting the domain of v i to {0, 1}, and within each Π [V i i ] , abstracting the domain of v i to {0, 1, . . . , k} with k = O(1).
This decomposition of Π provides us with the fork-decomposition ABS-ensembleAE = {T(Π [V f i ] ), α f i , β f i , T(Π [V i i ] ), α i i , β i i } n i=1of T(Π), with the abstraction mappings α f i , α i i and the action associations β f i , β i i being established along the above steps (1-3).
The additive abstractions of such an ABS-ensemble AE are of interest because (i) they can provide quite informative heuristic estimates, and (ii) each abstract problem inΠ = {Π [V f i ] , Π [V i i ] } n i=1can be solved in polynomial time by special-purpose algorithms for the corresponding fragments of SAS + (Katz & Domshlak 2008b).
However, here as well, the choice of the actual abstraction with respect to AE is important, and optimizing this choice is clearly of interest.
Interestingly, LP-optimization can come to the rescue here as well, and this despite the TG-structures of AE not being searchable explicitly in polynomial time.Theorem 6 For any SAS + planning task Π over n variables, the fork-decomposition ABS-ensembleAE = {T(Π [V f i ] ), α f i , β f i , T(Π [V i i ] ), α i i , β i i } n i=1 of T(Π)is LPoptimizable, and thus h AE (s) is poly-time computable for every state s ∈ S.Our LP-encoding of a fork-decomposition ABS-ensemble AE corresponds to LP reformulations of the algorithms of Katz and Domshlak (2008b) for fork and inverted-fork problems with properly bounded root domains.
Below we describe such an LP-encoding for an ABS-ensemble consisting of a single fork-structured abstraction with a binary root domain.Given a SAS + planning task Π = V , A, I , G, cost, let AE = {T(Π f r ), α f , β f } be a fork-decomposition of T(Π) over a single fork rooted in r ∈ V .
Considering that abstract problem, let us denote its variables by V ′ , its actions by A ′ , and its goal state G [V ′ ] by G ′ .
We can assume that G [v] is defined for all v ∈ V ′ \ {r}; all the goal-less leafs can be simply omitted from the fork.
For coherence with the notation of Katz and Domshlak (2008b) First, let the label-cost variables − → w contain a variable w a ′ for every abstract action a ′ ∈ A ′ ; the additivity constraints C add ( − → w ) are defined in terms of these label-cost variables via β f as in Eq.
4.
Now, given a state s of Π, we specify the LP-encodingL(s) = − → x , f ( − → x ), C AE ( − → xw)of AE with respect to s as follows.
The variable set − → x of L(s) consists of three types of variables, notably− → x = {h f } ∪ v∈V ′ \{r}, ϑ∈dom(v), 1≤i≤|σ(r)| {d(v, ϑ, i)} ∪ v∈V ′ \{r}, ϑ,ϑ ′ ∈dom(v), ϑr ∈{0,1} {p(v, ϑ, ϑ ′ , ϑ r )}.
The variable h f stands for the minimal cost of solving our fork-structured problem, and the objective function of L(s) is simply f ( − → x ) = h f .
Each variable d(v, ϑ, i) stands for the cost of the cheapest sequence of actions affecting v that changes its value from s[v] to ϑ given that the value changes of r induce a 0/1 sequence of length i. Each variable p(v, ϑ, ϑ ′ , ϑ r ) stands for the cost of the cheapest sequence of actions affecting v that changes its value from ϑ to ϑ ′ having fixed the value of r to ϑ r .
The constraint C AE of L(s) consists of the following sets of linear constraints.
(i) For all goal-achieving sequences σ ∈ * [σ(r)] of value changes of r, and each pair of r-changing actions a, a ′ ∈ A ′ such that eff(a)[r] = 1 and eff(a ′ )[r] = 0,h f ≤ v∈V ′ \{r} d(v, G[v], |σ|)+⌈ |σ| − 1 2 ⌉·w a +⌊ |σ| − 1 2 ⌋·w a ′ Semantics:The cost of solving the problem is not greater than the sum of achieving the goal values for all the leafs given a value sequence of the root, plus the cost of providing that value sequence.
(ii) For each leaf v ∈ V ′ \ {r},d(v, s[v], 0) = 0and, for each ϑ, ϑ ′ ∈ dom(v), and 1 ≤ i ≤ |σ(r)|,d(v, ϑ ′ , i) ≤ d(v, ϑ, i − 1) + p(v, ϑ, ϑ ′ , σ(r)[i])Semantics: The cost of achieving ϑ ′ from s [v] given |σ(r)| = i is bounded by the cost of achieving ϑ given |σ(r)| = i − 1, and achieving ϑ ′ from ϑ given σ(r) [i].
(iii) For each v ∈ V ′ \ {r}, ϑ ∈ dom(v), p(v, ϑ, ϑ, 0) = 0, p(v, ϑ, ϑ, 1) = 0Likewise, for each v-changing action a ∈ A ′ , if pre(a) [r] is unspecified, then, for ϑ r ∈ {0, 1},p(v, ϑ, eff(a)[v], ϑ r ) ≤ p(v, ϑ, pre(a)[v], ϑ r ) + w aand otherwise,p(v, ϑ, eff(a)[v], pre(a)[r]) ≤ p(v, ϑ, pre(a)[v], pre(a)[r]) + w aSemantics: Shortest-path constraints as in Eq.
6.
This finalizes our LP-encoding for an ABS-ensemble consisting of a single fork-structured abstraction with a binary root domain.
Due to the lack of space, here we omit the details of the LP reformulation of the algorithm for inverted forks-while that algorithm differs from this for forks, the general spirit of the corresponding LP-encoding is quite similar.
Finally, extending these encodings to ABS-ensembles containing multiple such forks and inverted forks (and possibly some other LP-optimizable abstractions) is guaranteed by the "composition" Theorem 4.
Fork-decomposition structural patterns are grounded in two specific fragments of tractable cost-optimal planning.
In principle, it is possible that structural patterns based on some other such fragments also lead to LP-optimizable ABS-ensembles.
Here we consider two such fragments that have been recently characterized by Katz and Domshlak (2008a).
Both these fragments correspond to problems over binary-valued state variables and actions inducing a polytree 3 causal graph.
In the first fragment, P b , all the causal graph's nodes have O(1)-bounded in-degree.
In the second fragment, P(1), all actions are 1-dependent.
While the poly-time solution schemes provided by Katz and Domshlak for P b and P(1) substantially differ one from another, they both correspond to reductions of the planning problems to compact and tree-structured constraint optimization problems (COPs).
This joint property of P b and P(1) (that might also hold for some other problem fragments as well) turns out to be very helpful to our objective of joining P b -and P(1)-based structural patterns to the "h AEfriendly" family of LP-optimizable ABS-ensembles.
Let us start by considering a general, tree-structured constraint optimization problem COP = (X , F ) over finitedomain variables X , functional components F , and the objective min ϕ∈F ϕ(X ).
Fixing an arbitrary rooting of the COP's constraint network at r ∈ X , in what follows we refer to that rooted tree of COP via its set of directed edges E = {(x, x ′ )}.
In these terms, we have F = {ϕ x :dom(y) × dom(x) → R 0+ | (y, x) ∈ E}.
It is well-known that tree-structured COPs as above can be solved in low polynomial time by a dynamic-programmingstyle, message-passing algorithm (Dechter 2003).
The bad news is that (similarly to what we had with the Dijkstra algorithm for solving PDBs) we cannot use this messagepassing algorithm for our needs.
The good news, however, is that such tree-structured COPs can also be solved via linear programming.
Specifically, given such a problem COP = (X , F ), let− → c = {h cop } ∪ (x ′ ,x)∈E, x ′ ∈dom(x ′ ) {c(x|x ′ )}be a set of non-negative, real-valued variables, with the semantics of each c(x|x ′ ) being "optimal solution for the subtree rooted at x given x ′ = x ′ ".
Then, the solution of LPmax − → c h cop s.t. ∀r ∈ dom(r) : h cop ≤ (r,x)∈E c(x|r), ∀(x, y) ∈ E, x ∈ dom(x), y ∈ dom(y) : c(y|x) ≤ (y,z)∈E c(z|y) + ϕ y (x, y).
(7)induces a solution for COP.Lemma 1 Given a tree-structured constraint optimization problem COP = (X , F ) over finite-domain variables X and functional components F , we havemin x∈dom(X ) ϕ∈F ϕ(x) = max c∈H COP h copwhere H COP is the convex polyhedron specified by the linear constraints as in Eq.
7.
With Lemma 1 in hand, we now make two additional steps towards an LP-encoding of ABS-ensembles AE containing structural patterns reducible to tree-structured COPs.
In each such individual structural pattern, each value ϕ y (x, y) of each functional component ϕ y is somehow pre-computed from the costs of the actions.
In our case, however, the costs of the actions in the abstract problems are not fixed in advanced, but should be determined by the process of LPoptimization.
This is where our next steps come into the picture.Consider a single COP-reducible cost-optimal planning problem.
First, suppose that, for any fixed action-costs vector w † , each functional-component value ϕ ≡ ϕ y (x, y) corresponds to a solution value of some compact (canonical form) linear programmax f ϕ ( − − → z ϕ w) s.t.
A ϕ · − − → z ϕ w ≤ b ϕ − → w ≤ w † (8)where A ϕ and b ϕ are a matrix and a vector of coefficients, respectively.
If so, then, given w † , we can reformulate the linear program in Eq.
7 by (i) replacing the constants ϕ y (x, y) by the corresponding affine functionsf ϕ ( − → z ϕ ∪ − → w ),and (ii) for each ϕ, adding its linear constraints as in Eq.
8.
The extended program is still linear, and we still havemin x∈dom(X ) ϕ∈F ϕ(x) = max czw∈H COP h copwhere z and w are assignments to − → z = ϕ − → z ϕ and actioncost variables − → w , respectively, and H COP is the convex polyhedron specified by these extended linear constraints.
Now, given a SAS + planning task Π, let AE = {T(Π ′ ), α, β} be a single-abstraction ABS-ensemble of T(Π) such that cost-optimal planning for Π ′ is reducible to a tree-structured constraint optimization problem COP Π ′ satisfying Eq.
8.
The extended linear program specified above provides the basis for the LP-encoding of such ABSensembles.
First, as before, let the label-cost variables − → w contain a variable w a ′ for every abstract action a ′ ∈ A ′ ; the additivity constraints C add ( − → w ) are defined in terms of these label-cost variables via β as in Eq.
4.
Now, given a state s of Π, we specify an LP-encodingL(s) = − → x , f ( − → x ), C AE ( − → xw)of AE with respect to s as follows.
• The variable set − → x = − → cz consists of the variables of Eqs.
7 and 8, and the objective ofL(s) is f ( − → x ) = h cop .
• The constraint C AE ( − → xw) of L(s)consists of all the linear constraints from Eq.
7, as well as the constraintA ϕ · − − → z ϕ w ≤ b ϕ from Eq.
8 for all functional-component values ϕ of COP Π ′ .
This finalizes the desired LP-encoding; extending it to such multiple-abstraction ABS-ensemblesAE = {T(Π ′ i ), α i , β i } k i=1(and, again, possibly some other LPoptimizable abstractions) is, again, guaranteed by the "composition" Theorem 4.
Theorem 7 Given a SAS + planning task Π, and an ABS-ensemble AE = {T(Π ′ i ), α i , β i } k i=1 of T(Π), if k = O(poly(|Π|)), and cost-optimal planning for each Π ′ i is poly-time reducible to a compact and tree-structured constraint optimization problem satisfying Eq.
8, then AE is LP-optimizable, and thus h AE (s) is poly-time computable for every state s ∈ S.The last but not least is, of course, the question of whether the requirement posed by Eq.
8 is any realistic with respect to the COPs induced by the abstract planning problems.
Fortunately, Theorem 8 closes the story with some very good news on that matter.
Theorem 8 Cost-optimal planning for any task Π in P b ∪ P(1) is poly-time reducible to a compact and tree-structured constraint optimization problem satisfying Eq.
8.
Numerous recent works have suggested that additive ensembles of admissible heuristics is a powerful tool for heuristicsearch systems.
However, the action-cost partitioning parameter of such ensembles kept the "how to add (if at all)" question totally open.
Here we described a procedure that closes this question for arbitrary ensembles of all known to us abstraction-based heuristics such as PDBs, constrained PDBs, merge-and-shrink abstractions, fork-decomposition structural patterns, and structural patterns based on tractable constraint optimization.
The procedure is based on a linear-programming formulation of the optimization problem: given a classical planning task, a forward-search state, and a set of abstraction-based admissible heuristics, construct an optimal additive composition of these heuristics with respect to the given state.
Most importantly, the time complexity of our procedure is polynomial for arbitrary ensembles of all the above abstraction-based heuristics.
We now outline some of the (in our opinion) more important challenges for future work.Structure optimization.
Probably the most important issue that remains almost entirely open is this of "structure optimization".
While our framework optimizes the composition of a given set of TG-structures, ultimately we would like to move to even more parametric such ensembles, allowing flexibility in the actual choice of TG-structures.
For instance, it would clearly help to know what PDBs should (optimally) be added to the ensemble, what domain abstractions should (optimally) be performed on the roots of the inverted forks and forks, what polytrees should (optimally) span the causal graph of the problem, etc.
Note that the first step in this direction has already been made between the lines of this paper-an immediate corollary of Theorem 5 is that, for any forward-search state s, and any fixed upper bound on the size of the PDBs, one can construct in polynomial time the actual optimal (for s) pattern-database ABS-ensemble, and this simply by running LP-optimization over the ensemble of all possible such PDBs.Additive m-reachability.
As we mentioned, the seminal m-reachability heuristics h m are not covered by our framework.
While computing a single h m heuristic for a fixed m is poly-time, h m is not based on a problem abstraction (even in the very permissive sense of Definition 1)-the state-graph over which h m is computed is an AND/ORgraph (and not an OR-graph such as transition graphs), each original problem state is mapped to a set of abstract states (and not to a concrete such state), and the actual computation of h m corresponds to computing a critical tree (and not a shortest path) to the goal.
Tangentially, the problem of computing a critical-tree in an AND/OR-graph does not appear to have an LP reformulation.
Hence, the complexity of computing optimal additive h m heuristics is still open and very much interesting.LP-encodings for "double relaxations".
The basic idea of LP-optimizing heuristic composition naturally extends also to intractable planning relaxations that admit "second-order" LP-relaxations.
For instance, some intractable planning relaxations formalizable via sounds and complete integer-valued LPs (such as the deletes-ignoring relaxation underlying h + , or more recent action-ordering relaxation of van den Briel (2007)) appear to be quite natural such candidates.
Things, however, are more complicated than that because, very roughly, (i) Definition 4 requires a very specific type of LP-encodings (satisfying Eq.
5), and (ii) none of the known to us ILP-to-LP "second-order" relaxations appear to be of that type.
Hence, developing "Eq.
5 friendly" LP-relaxations for h + , action-ordering relaxation, and other informative yet intractable planning relaxations is now definitely of interest.Finally, we would like to address an almost immediate source of skepticism with respect to the practicality of our optimization procedure-using it requires solving a large LP at every search node, while typically such per-node computations are expected to be of low polynomial time.
We believe, however, that the superior informativeness of the optimal additive heuristics has a clear potential to eventually overweight the cost of heuristic computation due to substantial reductions in the number of expanded search nodes.
In other words, while the informal notion of "low polynomial time" changes with the progress of hardware technology, it is widely believed these days that P 񮽙 = NP, and thus reducing the amount of expanded nodes is still the most important objective of the heuristic-search research in the long run.
