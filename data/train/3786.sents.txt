Solid-state devices (SSDs) have the potential to replace traditional hard disk drives (HDDs) as the de facto storage medium.
Unfortunately, there are several decades of spinning-media assumptions embedded in the software stack as an "unwritten contract" [20].
In this paper, we revisit these system-level assumptions in light of SSDs and find that several of them are invalidated by SSDs, breaking the unwritten contract and resulting in poor performance and lifetime.
The underlying cause is the incorrect division of labor between file systems and storage.
Block management must be removed from the file system and delegated to the SSD to prevent further accumulation of storage-specific assumptions.
We find that object-based storage is an appropriate way to achieve this.
Storage systems export a simple abstraction of a linear block-level interface that has worked well for cases ranging from a single disk to the aggregation of disks such as RAID arrays and logical volumes.
In fact, the simplicity of the interface has helped to hide the complexity of the underlying device from higher-level systems.Unfortunately, the interface has also hidden devicespecific details, so that the file system is forced to make assumptions about the underlying storage, referred to by Schlosser and Ganger as an "unwritten contract" [20].
Not surprisingly, most of these assumptions are regarding block management such as allocation, layout, scheduling, and cleaning, all of which could benefit from device-specific knowledge.
For example, file systems assume that random accesses are much slower than sequential ones, and hence the block management layer is optimized for this.
While this assumption is valid for disks, when the properties of the underlying device change, such assumptions may either hold or fail.
For example, for MEMS-based storage, Schlosser et al. find that the existing abstractions are mostly valid [20].
We reexamine the existing storage abstraction and the resulting assumptions in light of solid-state devices (SSDs).
SSDs are different from disk drives in many aspects such as unique semiconductor properties, internal architecture, and controller firmware, which affect the performance, reliability, lifetime, power, and security properties of the SSDs.
Overall, SSDs are substantially complex and self-managing and require more information than is provided by the standard storage interface.To find out if the disk-specific assumptions hold for SSDs, we list six system-level assumptions (three of which are from the unwritten contract as originally stated) and explain how each of them fail for SSDs, resulting in poor performance and lifetime.
The underlying problem is the incorrect division of labor: file systems perform block management, which for a device such as an SSD is best done internally because of its knowledge of intricate device-specific properties, policies, and algorithms.A more expressive interface such as object-based storage (OSD) [10,11,22] can improve the current state of the art.
First, OSD delegates finer details of block management to the SSD, thereby preventing any new storagespecific assumptions.
Second, OSD expresses the intentions of the higher layers clearly, thereby improving the internal SSD operations.
Storage Interface.
Storage access protocols such as SCSI and ATA/IDE export a narrow, block-based interface with simple read and write APIs to access the logical block number (LBN) (a 512-byte sector).
A storage controller internally maps the LBN to a physical sector, which is hidden and fixed for most cases.The main disadvantage of the block-based interface is the absence of higher-level semantics.
Several previous works note this deficiency and propose more expressive interfaces [5][6][7]9]; some even allow programming the storage controller [1,16,21].
One approach is to use an object-based interface [10,11,22], which exports the abstraction of an object as a collection of bytes.
Structures such as trees, tables, files, and directories can be represented as objects, reflecting the higher-level semantics better than a block-based interface; the device controller performs block allocation and layout for the objects.Solid-State Devices.
An SSD consists of a set of flash memory packages that are connected to a controller (Fig- ure 1).
Each package has one or more dies; each die has multiple planes, which in turn have many blocks; each block consists of many 4 KB pages [18].
SSDs differ from HDDs on 3 main properties.
The first obvious difference is the absence of mechanical moving parts.
Second, flash pages are non-overwrite in nature and must be erased before being overwritten.
To hide the high erase overhead and create the abstraction of an in-place write, modern SSDs implement a log-structured design [17] in the flash translation layer (FTL) [15].
To uniformly spread the block usage, wearleveling is also implemented.
Finally, SSDs have several layers of parallelism that is dictated by the flash packages and the way they are connected to the controller.There are two types of NAND flash memory: singlelevel cell (SLC) and multi-level cell (MLC).
SLC flash stores a single bit of data per cell, while MLC flash stores multiple bits per cell.
MLC flash has some drawbacks such as shorter lifetime (10K erase cycles vs 100K erase cycles of SLC), slower write, and erase operations.
In this section, we discuss the system-level assumptions that fail when applied to SSDs, and the reasons behind these failures.
In Table 1, we list the original (1-3) and extended terms of the unwritten contract and state whether they are satisfied or violated by different devices.
This list is by no means complete because we focus only on block-management issues; more assumptions may be added as our experience with SSDs grows.
For comparison, we list RAID arrays and MEMS-based storage, but for the rest of the paper we focus only on how the assumptions fail on SSDs.
In a disk, the latency and bandwidth of sequential access are several tens of times better than random access.
However, on SSDs that use a log-structured FTL [15], both sequential and random writes are likely to take similar time.
Table 2 lists the ratio of sequential-to-random bandwidth for an HDD (a Seagate Barracuda 7200.11 drive) and several SSDs.
One of the SSDs is simulated (S4 slc sim ) using the simulator from our previous work [2], while others are real (S1 slc , S2 slc , S3 slc , S5 mlc ).
We anonymize the real SSDs because they are engineering samples and pre-production models.
To help the reader understand the results better, we specify whether the devices use SLC or MLC flash memory.From the table, we can observe that SSDs (using SLC or MLC memories) have random-read performance that is only a few times smaller than their sequential-read performance.
This is even true for writes on certain SSDs (S1 slc , S4 slc sim , S5 mlc ), but not on all of them; in fact, some of the SSDs (S2 slc , S3 slc ) have random-write performance that is worse than HDDs.
One of the reasons for this poor performance is write amplification, which we will discuss later ( ยง3.4).
From the above results, we can see that the gap between sequential and random accesses is narrowing on SSDs.
File systems that run primarily on SSDs must reconsider the need for complex policies to achieve blocklevel sequentiality.
Instead, a file system must focus on higher-level operations such as object management, consistency, and recovery, and move the low-level block management to the SSD, using say, the OSD interface.
The second term of the unwritten contract considers the relation between logical and physical sectors, and understanding it is important for I/O scheduling.
On an HDD, nearby LBNs translate well to physical proximity.
However, this contract fails on an SSD because of the log-structured design, cleaning, and wear-leveling, all of which make it harder to estimate the location of a logical sector.
In fact, the physical location is irrelevant if the ratio of sequential to random accesses approaches 1.
This further motivates the conclusion that the file system accesses must be in terms of objects (or parts of objects) and the SSD must handle the low-level sector-specific scheduling.
We performed a preliminary analysis with a new algorithm for SSD, called shortest wait time first (SWTF), which uses the queue wait times of all the parallel elements in an SSD and schedules an I/O that has the shortest wait time.
On a synthetic workload that issues random I/Os (with 2/3 reads and 1/3 writes), we found that SWTF improves the response time by about 8% when compared to FCFS.
More thorough analysis is required to find the effectiveness of such SSD-specific algorithms.
The third term of the contract assumes that the logical address space is uniformly spread over the device.
This is invalidated by disks because of zoned recording, where the outermost tracks accommodate more logical pages than innermost ones; that is, outer-track bandwidth is greater than the inner-track bandwidth.
Today, SSDs are homogeneous, using only a single type of memory (either SLC or MLC), keeping the contract valid.
However, we believe that in the future, SSDs might be constructed with multiple types of memories (SLC/MLC).
In such systems, this contract will be violated because MLC-type memories can hold more data and have different timing characteristics than SLC.
Such heterogeneity in the address space can be better utilized if the device performs block allocation for higher-level objects.
For example, an SSD can choose to co-locate all the data belonging to a root object in SLC memory for faster access.
Operating systems typically assume that the time taken to complete an I/O is proportional to the I/O size.
However, in an SSD, writes may be amplified into a larger I/O due to several reasons: first, when the logical page is larger than the physical page size; second, when a write is issued in-place using a read-modify-erase-write cycle.Write amplification is not a new phenomenon; it happens on RAID arrays that need to update parity blocks.
We measured the effect of write amplification on one of the engineering samples (a low-end SSD, S2 slc ); Fig- ure 2 shows the results.
We plot the bandwidth against the write size.
One can observe that the bandwidth is poor on small write sizes (e.g., 512 bytes).
As we increased the write size, the bandwidth improved and reached its maximum at 1 MB (the SSD's stripe size).
As we increased the write size further (e.g., 1 MB + 512 bytes), the bandwidth again dropped, and this behavior repeated to give a saw-tooth pattern.
We believe that this behavior is due to striping the logical page across a gang of flash packages that share the buses [2].
A similar saw-tooth pattern was noticed by Schindler et al. for disk drives on track-aligned accesses [19].
However, in their case it was due to the effect of track switches and rotational latencies.
It is important to note that write caches might not always mask the write amplification; for example, S3 slc has a write buffer cache of 16 MB, but it is ineffective in masking the write amplifications, as can be seen from the random-write performance in Table 2.
Write amplification can be reduced by merging writes and aligning them to stripe sizes.
Since it is harder to estimate the stripe size and alignment boundaries from a file system (especially in the presence of a write cache and background activity), an SSD must be responsible for sector allocation and layout according to the stripe sizes.
Table 3 shows results from stripe-aligned and unaligned writes.
We simulated a 32 GB SSD with one gang of eight 4 GB flash packages.
A single 32 KB logical page spanned over all the packages.
We ran a synthetic workload that issued a stream of writes with varying degrees of sequentiality.
We compared two schemes: one, issuing the writes as they arrive; two, merging and aligning writes on logical page boundaries.
On a completely random workload, both schemes worked similarly because of the small chance to merge the writes into stripe sizes.
As the sequentiality increased, aligning writes paid off well, resulting in an improvement of over 50%.
Table 4 presents the improvement in response time for various workload traces.
Of all the workloads, IOzone benefits the most (over 36% improvement) due to its large write sizes.
File systems assume that the media wear-down does not depend on the number of writes to particular sectors.
However, flash memory blocks have limited erase cycles before wearing out.
Therefore, mid-range and highend SSDs implement cleaning and wear-leveling to uniformly spread the wear-down of blocks.
SSDs clean by retaining the most recent version of all the logical pages, including those that have been released by the file system, leading to a lot of useless activity.
The effectiveness of cleaning and wear-leveling can be improved by using file-system-level semantic knowledge, specifically the block allocation status, which is not available to an SSD.An SSD can use the block allocation status to implement informed cleaning and wear leveling that avoids retaining the free pages.
We used our SSD simulator to analyze the benefits of informed cleaning by running block-level traces that contain read, write, and block-free operations.
The traces were collected by running the Postmark benchmark [14] on a pseudo-device driver that uses Linux Ext3 knowledge to identify the free sectors.
The SSD simulator was modified such that the cleaning and wear-leveling logic disregard the flash pages corresponding to the free logical pages.
To the best of our knowledge, this is the first study to measure the effect of free-page knowledge in SSD cleaning.
Table 5 shows the improvements in cleaning in terms of the number of pages that need to be reclaimed and the cleaning time for an 8 GB SSD; both measures are shown relative to the default SSD that does not use the free-page information.
We observe that informed cleaning reduces the number of reclaimable pages by at most about one-half.
Informed cleaning reduces the cleaning time by 30-40%, which can improve the overall running time by about 3-4%.
However, the improvements are workload-dependent.
Storage systems are assumed to be passive and to act only when a request is issued from a higher-level software layer.
While this is true on a single HDD, SSDs perform a considerable amount of background activity due to cleaning and wear-leveling.
Therefore, it becomes hard to predict the I/O latency; for example, it is hard to guarantee QoS on a system with SSDs because the host has no control over when the SSD engages in background activities.
This is especially true if the SSD is full and the degree of internal fragmentation is high [4].
The background activity can be controlled by informing the SSD about I/O priorities or by marking certain objects as high-priority.
For example, an SSD can provide preferential treatment to high-priority objects or I/Os by delaying its background activity.We modified the cleaning logic of our SSD simulator to be aware of request priorities.
If there are no outstanding priority requests, cleaning starts when the number of free pages falls below a low threshold.
However, if there are priority requests, cleaning is postponed until the number of free pages falls below a critical threshold.
We call this priority-aware and compare it with a priority- agnostic scheme, which starts cleaning at the low threshold irrespective of the outstanding requests.
Note that when there is no priority information available, priorityagnostic is the default technique to use.We evaluated a 32 GB SSD using synthetic benchmarks with request inter-arrival times uniformly distributed between 0 and 0.1 ms. The fraction of priority requests was set to 10%; critical and low thresholds were fixed at 2% and 5% of free pages.
In Figure 3, we plot the response time of priority requests (marked as foreground) and non-priority requests (marked as background).
Table 6 shows the corresponding improvement in response time for priority requests.
We observe that under the priority-aware scheme, the response time of foreground requests improve by about 10%.
However, the cost of this improvement is reflected on the background I/Os, whose response time increased as well.
As storage devices grow more complex, assumptions made by higher layers fail.
We believe that certain functionalities, specifically those related to block management, are more appropriately handled by the device controller with its intricate knowledge of the inner workings of the device.
However, to perform the block management correctly and efficiently, devices must also understand the high-level intentions (semantics) behind the simple reads and writes.Broadly, there are two ways by which the device can obtain more information: explicitly, through new or modified interfaces; or implicitly, by using reverse engineering techniques.
Since reverse engineering techniques can add more complexity to the device firmware and do not minimize the functionalities at higher layers, we focus only on explicit approaches.
Existing interfaces can be patched with additional commands to convey the operation semantics.
For example, the TRIM command has been proposed to add file delete notifications to the ATA interface [8].
While this approach offers the least resistance in the device-interface evolution, it still operates on the block level, thereby letting the file system perform the block management.
Moreover, existing interfaces may not provide sufficient extensions for new commands.
In such cases, new interfaces such as NVMHCI have been proposed [13].
However, while NVMHCI conveys more information than traditional SCSI/ATA, it still lets the higher layers manage and operate at the block level.
We believe that OSD interface provides a nice alternative by conveying more information and letting the device handle low-level operations.For several of the aforementioned contract violations, an OSD provides a better alternative.
For example, a file system should operate on objects and let the device handle the logical to physical mapping, sequentialrandom accesses to (parts of) objects, and stripe-aligned accesses.
Additionally, an SSD can use the OSD interface and manage the space for objects (including the allocation and release of pages to objects) in order to implement informed cleaning.
An additional benefit of using an OSD is that object attributes can be set to convey read-only data, which could be used for cold data placement during wear-leveling.
Finally, I/Os to objects can be marked with a priority to schedule them appropriately with background activities.
Several previous researchers have noted the need for more expressive storage interfaces for disks [1,5,6,9,16,21], RAID arrays [7], and MEMS-based devices [12,20].
Among these, the most closely related work is by Schlosser and Ganger, which examines the OS assumptions in the context of MEMS-based devices [20].
They list the first three terms of the unwritten contract and show how MEMS-based devices obey them, obviating the need for new interfaces or algorithms.
In another related paper, Ajwani et al. characterize a variety of SSDs and argue for new algorithms for SSDs and hybrid devices [3].
However, they still use the block-level interface.
We argue for a new, richer interface.New interface specifications are being proposed for SSDs, like NVMHCI [13] and TRIM [8], but they still let the higher layers manage the blocks, resulting in most of the problems we discussed earlier.
Using the OSD interface provides a clean separation between the file system and block management operations, enabling the SSD to handle them optimally.
Over the past 5 decades, OS and storage systems have evolved independently across a narrow and fixed storage interface.
One of the side effects of this evolution is the accumulation of device-specific assumptions in the storage stack, specifically in the block management layer.
Unless the block management is removed from the file system and delegated to the storage, such assumptions are likely to carry over and grow in the next generation of storage devices as well.
SSDs are evolving and have the potential to become the ubiquitous storage media.
As our initial results have shown, it is time we switch from the narrow, block-based interface to a richer object-based storage to improve the performance and longevity.
We thank our shepherd, Geoff Kuenning, the anonymous reviewers, Nathan Obr from Microsoft Device and Storage Technologies, and the members of Microsoft Research Silicon Valley for their detailed feedback and excellent suggestions on our paper.
