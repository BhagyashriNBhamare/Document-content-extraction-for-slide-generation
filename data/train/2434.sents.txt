This paper introduces the Ky-otoEBMT example-based machine translation framework.
Since last year's workshop we have replaced input trees with forests, improved alignment, added new features, and introduced bilingual neural network reranking.
The major benefits of our system include online example retrieval and flexible reordering.
We also use syntactic dependency analysis for both source and target languages in the hope of learning how to translate non-local structure.
The system implementation (this paper refers to version 1.0) is available as open-source.
This paper describes the KyotoEBMT system used in the 2nd Workshop on Asian Translation (Nakazawa et.
al, 2015).
Our system is a fully-fledged ExampleBased Machine Translation (EBMT) platform making use of both source-language and target-language dependency structure.
This approach has been explored comparatively less in studies on syntax-based SMT/EBMT, which tend to focus on constituent trees rather than dependency trees, and on tree-to-string rather than tree-to-tree approaches.
Furthermore, we employ separate dependency parsers for each language rather than projecting the dependencies from one language to another, as in (Quirk et.
al, 2005).
The dependency structure information is used end-to-end: for improving the quality of the alignment of the translation examples, for constraining the translation rule extraction and for guiding the decoding.
We believe that dependency structure, which considers more than just local context, is important in order to generate fluent and accurate translations of complex sentences across distant language pairs.The experiments described in this paper focus on technical domain translation for Japanese-Chinese and Japanese-English, however our implementation is applicable to any domain and language pair for which there exist parallel sentences and dependency parsers.A further unique characteristic of our system is that, again contrary to the majority of similar systems, it does not rely on precomputation of translation rules.
Instead it matches each input sentence to the full database of translation examples before extracting translation rules online.
This has the merit of maximizing the information available when creating and combining translation rules, while retaining the ability to produce excellent translations for input sentences similar to an existing translation example.The system is mostly developed in C++ and is available as open source.
The code and documentation are available from http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.
Experiments are facilitated through the inclusion of an end-to-end experiment management system (EMS) which has been greatly improved in this version.
The framework is simple to use and supports model training with multiple threads or across a cluster.
Figure 1 shows the basic structure of the KyotoEBMT translation pipeline.
The training process begins with parsing and aligning parallel sentences from the training corpus.
The alignments are then used to build an example database ('translation mem- Figure 1: The translation pipeline can be roughly divided in 3 steps.Step 1 is the creation of the example database, trained from a parallel corpus.
Step 2 is the parsing of an input sentence and the generation of sets of initial hypotheses.
Step 3 consists in decoding and reranking.
The tuning of the weights for decoding and reranking is done by a modified version of step 3.
ory') containing 'examples' or 'treelets' that form the hypotheses to be combined during decoding.Translation is performed by first parsing an input sentence then searching for treelets matching entries in the example database.
The retrieved treelets are combined by a lattice-based decoder that optimizes a log linear model score.
Finally, we use a reranker to select the optimal translation from the n-best list provided by the decoder using additional non-local features (see section 3.4).
Figure 2 shows the process of combining examples matching the input tree to create an output sentence.
An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed offline.
However, for a given input sentence i, the steps for finding examples partially matching i and extracting their translation hypotheses is an online process.
This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao (1984).
It has already been proposed for phrase-based (Callison- Burch et al., 2005), hierarchical (Lopez, 2007), and syntax-based ( Cromières and Kurohashi, 2011) systems.
It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several benefits.
The first is that we are not required to impose a limit on the size of translation hypotheses.
Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable.
In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation.
A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis.The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules.
We use the techniques described in Cromières and Kurohashi (2011) to perform this step as efficiently as possible.Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it.
A translation hypothesis is defined as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence.
A translation hypothesis is created from a translation example as follows:1.
We project the part of s that is matched into the target side t using the alignment of s and t.
This is trivial if each word of s and t is aligned, but this is not typically the case.
Therefore our translation hypotheses will often have some target words/nodes marked as optionals: this means that we will decide if they should be added to the final translation only at the moment of combination.2.
We insert the non-terminals as child nodes of the projected subtree.
This is Figure 2: The process of translation.
The source sentence is parsed and matching subtrees from the example database are retrieved.
From the examples, we extract translation hypotheses than can contain optional target words and several position for each non-terminals.
For example the translation hypothesis containing "textbook" has three possible position for the non-terminal X3 (as a left-child before "a", as a left-child after "a" or as a right-child).
The translation hypotheses are then combined during decoding.
Choice of optional words and final non-terminal positions is also done during decoding.
simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case.
A consequence is that we will sometimes have several possible insertion positions for each non-terminal.
The choice of insertion position is again made during combination.
After having extracted translation hypotheses for as many parts of the input tree as possible, we need to decide how to select and combine them.
Our approach here is similar to what has been proposed for Corpus-Based Machine Translation.
We first choose a number of features and create a linear model scoring each possible combination of hypotheses (see Section 3.3).
We then attempt to find the combination that maximizes this model score.The combination of rules is constrained by the structure of the input dependency tree.
If we only consider local features 1 , then a simple bottom-up dynamic programming approach can efficiently find the optimal combination with linear O(|H|) complexity 2 .
However, non-local features (such as language models) will force us to prune the search space.
This pruning is done efficiently through a variation of cube-pruning (Chiang, 2007).
We use KenLM 3 (Heafield, 2011) for computing the target language model score.
Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction (( Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estimations( Heafield et al., 2012).
Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals.
In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the final position of each non-terminal.
In order to handle such ambiguities, we use a lattice-based internal representation that can encode them efficiently (see Figure 3).
This lattice representation also allows the decoder to make choices between various morphological variations of a word (e.g. be/is/are).
We use the decoding algorithm described in ).
Based on the findings of Neubig and Duh (2014), we experimented with supervised alignment using Nile ( Riesa et al., 2011) as part of our translation framework.
We found that using supervised alignments made a considerable improvement to translation quality.
Since Nile supports only constituency parses, we also perform constituency parsing for source and target languages for generating bidirectional word alignments.For the initial alignments for Nile, we use the alignments generated from the model described in last year's system description ( Richardson et al., 2014), which makes use of our dependency parses in order to capture non-local reorderings.
We found that the quality of the source-side dependency parsing had an important impact 3 http://kheafield.com/code/kenlm/ on translation quality.
Unfortunately, parsing errors are unavoidable.
Chinese parsing is maybe especially challenging and our Chinese parser still produces a significant number of parsing errors.
In order to mitigate this problem, last year we used a k-best list of input parses.
We found this was somewhat successful but inefficient, and therefore have moved from a k-best list representation of multiple parses to a more compact and efficient forest representation.In the future, we will consider also using forests for all the translation examples (and not just the input sentence).
During decoding we use a linear model to score each possible combination of hypotheses.
This linear model is based on a linear combination of both local features (local to each translation hypothesis) and non-local features (such as a 5-gram language model score of the final translation).
Despite our already relatively large set of dense features, we found there were a number of cases where these features were not enough to differentiate between good and bad translation hypotheses.This year we have added ten new features, now reaching a total of 52, a selection of which are shown below:• Forest parse scores The optimal weights for each feature are as before estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses.
A final reranking step allows us to use more advanced features for selecting the best translations.
We reranked the n-best output of our system using several additional language models: a standard 7-gram language model with Modified Kneser-Ney smoothing, a Recurrent Neural Network Language Model (RNNLM) (Mikolov et.
al, 2010) and several variations of a Bilingual Recurrent Neural Network Language Model.The RNNLM model was trained with hidden layer size 200, and 5000 sentences from the training fold were used as validation data.For the bilingual language model, we used the Neural Machine Translation Model of (Bahdanau et.
al, 2014) which has an open source implementation in the GroundHog/Theano framework 4 .
For each language pair we trained two models, one for each translation direction.
In addition, for Japanese and Chinese, we considered two types of segmentation: the segmentation produced by our morphological analyzer, and a character-level segmentation.
We had therefore up to four models per language pair.
Rescoring our translations with these models gave up to four additional features.
It is interesting to note that although trying to directly translate our input sentences using these neural MT models typically resulted in a comparatively low BLEU score, they turned out to be useful for reranking in our system.
This is probably due to the fact that, since they represent a very different approach to translation, the models tend to learn different aspects of the translation and make different mistakes to our system.
Using a character-based segmentation further ensured the neural models learned a different kind of information.
The models took two to four days each to train on a GPU.
The settings we used were mostly the defaults of the implementations 5 .
Reranking was conducted by first calculating the various language model scores for each We conducted translation experiments on the four language pairs in the scientific papers subtask: Japanese-English (JA-EN), EnglishJapanese (EN-JA), Japanese-Chinese (JA-ZH) and Chinese-Japanese (ZH-JA).
The proposed system used the following dependency parsers and show below their approximate parsing accuracies (micro-average), which were evaluated by hand on a random subset of sentences from the test data.
The parsers were trained on domains different to those used in the experiments.
• English: NLParser 6 (92%) (Charniak and Johnson, 2005) • Japanese: KNP (96%) ( Kawahara and Kurohashi, 2006) • Chinese: SKP (88%) (Shen et al., 2012) For generating input for Nile we used the following constituency parsers:• English: Berkeley Parser ( Petrov et al., 2006)• Japanese: Cyklark (Oda et al., 2015)• Chinese: Berkeley Parser ( Petrov et al., 2006)Forests were created by packing the 200-best dependency parses for Japanese and English, and 50-best parses for Chinese.
Table 1 shows the results of our proposed system (WAT15) and a comparison with the system from last year (WAT14) (Richardson et al., 2014) and official baseline (phrasebased SMT, for details see Nakazawa et al. (2015)).
We give results for evaluation on the test set after tuning (WAT15, WAT14) and tuning plus reranking (WAT15+Rerank, WAT14+Rerank).
Tuning was conducted over 10 iterations on the development set using an n-best list of length 500, and we used the 1000-best for reranking.
WAT15+Rerank was the strongest system in our comparison, outperforming the official baseline, non-reranked system (WAT15) and last year's systems in all metrics for all languages, with the minor exception of JA-ZH human evaluation for reranked vs. nonreranked.
In this paper we have described the latest version of the KyotoEBMT example-based translation system.
Since last year we have improved alignment, introduced forest input, added new features and used bilingual neural network features in reranking.In our preparation for this workshop we have focused mainly on improving JapaneseChinese and Chinese-Japanese translation, particularly in terms of dealing with poor quality Chinese dependency parses.
As future work we plan to perform more extensive error analysis on the other language pairs.
We also found that despite using forest input there are still many issues caused by incorrect parsing and will consider in the future how best to overcome this.
