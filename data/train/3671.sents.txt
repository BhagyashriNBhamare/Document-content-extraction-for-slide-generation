Providing timely results in the face of rapid growth in data volumes has become important for analytical frameworks.
For this reason, frameworks increasingly operate on only a subset of the input data.
A key property of such sampling is that combinatorially many subsets of the input are present.
We present KMN, a system that leverages these choices to perform data-aware scheduling , i.e., minimize time taken by tasks to read their inputs , for a DAG of tasks.
KMN not only uses choices to co-locate tasks with their data but also percolates such combinatorial choices to downstream tasks in the DAG by launching a few additional tasks at every upstream stage.
Evaluations using workloads from Facebook and Conviva on a 100-machine EC2 cluster show that KMN reduces average job duration by 81% using just 5% additional resources.
Data-intensive computation frameworks drive many modern services like web search indexing and recommendation systems.
Computation frameworks (e.g., Hadoop [14], Spark [60], Dryad [38]) translate a job into a DAG of many small tasks, and execute them efficiently on compute slots across large clusters.
Tasks of input stages (e.g., map in MapReduce or extract in Dryad) read their data from distributed storage and pass their outputs to the downstream intermediate tasks (e.g., reduce in MapReduce or full-aggregate in Dryad).
The efficient execution of these predominantly I/Ointensive tasks is predicated on data-aware scheduling, i.e., minimizing the time taken by tasks to read their data.
Widely deployed techniques for data-aware scheduling execute tasks on the same machine as their data (if the data is on one machine, as for input tasks) [8,59] and avoid congested network links (when data is spread across machines, as for intermediate tasks) [13,24].
However, despite these techniques, we see that production jobs in Facebook's Hadoop cluster are slower by 87% compared to perfect data-aware scheduling ( §2.3).
This is because, in multi-tenant clusters, compute slots that are ideal for data-aware task execution are often unavailable.
App selects two blocks (e.g. A1, A3)Figure 1: "Late binding" allows applications to specify more inputs than tasks and schedulers dynamically choose task inputs at execution time.The importance of data-aware scheduling is increasing with rapid growth in data volumes [31].
To cope with this data growth and yet provide timely results, there is a trend of jobs using only a subset of their data.
Examples include sampling-based approximate query processing systems [5,12] and machine learning algorithms [16,42].
A key property of such jobs is that they can compute on any of the combinatorially many subsets of the input dataset without compromising application correctness.
For example, say a machine learning algorithm like stochastic gradient descent [16] needs to compute on a 5% uniform random sample of data.
If the data is spread over 100 blocks then the scheduler can choose any 5 blocks and has 񮽙 100 5 񮽙 input choices for this job.Our goal is to leverage the combinatorial choice of inputs for data-aware scheduling.
Current schedulers require the application to select a subset of the data on which the scheduler runs the job.
This prevents the scheduler from taking advantage of available choices.
In contrast, we argue for "late binding" i.e., choosing the subset of data dynamically depending on the current state of the cluster (see Figure 1).
This dramatically increases the number of data local slots for input tasks (e.g., map tasks), which increases the probability of achieving data locality even during high cluster utilizations.Extending the benefits of choice to intermediate stages (e.g., reduce) is challenging because they consume all the outputs produced by upstream tasks.
Thus, they have no choice in picking their inputs.
When upstream outputs are not evenly spread across machines, the oversubscribed network links, typically cross-rack switch links [24], become the bottleneck.
We introduce choices for intermediate stages by launching a small number of additional tasks in the previous stage.
As an example, consider a job with 400 map tasks and 50 reduce tasks.
By launching 5% extra map tasks (420 tasks), we can pick the 400 map outputs that best avoid congested links.Choosing the best upstream outputs used by intermediate stages is non-trivial due to complex communication patterns like many-to-many (for reduce tasks) and many-to-one (for joins).
In fact, selecting the best upstream outputs can be shown to be NP-hard.
We develop an efficient round-robin heuristic that attempts to balance data transfers evenly across cross-rack switch links.
Further, upstream tasks do not all finish simultaneously due to stragglers [13,61].
We handle stragglers in upstream tasks using a delay-based approach that balances the gains in balanced network transfers against the time spent waiting for stragglers.
In the above example, for instance, it may schedule reduce tasks based on the earliest 415 map tasks and ignore the last 5 stragglers.In summary, we make the following contributions:• Identify the trend of combinatorial choices in inputs of jobs and leverage this for data-aware scheduling.
• Extend the benefits of choices to a DAG of stages by running a few extra tasks in each upstream stage.
• Build KMN, a system for analytics frameworks to seamlessly benefit from the combinatorial choices.We have implemented KMN inside Spark [60].
We evaluate KMN using jobs from a production workload at Conviva, a video analytics company, and by replaying a trace from a production Hadoop cluster at Facebook.
Our experiments on an EC2 cluster with 100 machines show that we can reduce average job duration by 81% (93% of ideal improvements) compared to Spark's scheduler.
Our gains are due to KMN achieving 98% memory locality for input tasks and improving intermediate data transfers by 48%, while using ≤ 5% extra resources.
In this section we first discuss application trends that result in increased choices for scheduling ( §2.1).
We then explain data-aware scheduling ( §2.2) and quantify its potential benefit in production clusters ( §2.3).
With the rapid increase in the volume of data collected, it has become prohibitively expensive for data analytics frameworks to operate on all of the data.
To provide timely results, there is a trend towards trading off accuracy for performance.
Quick results obtained from just part of the dataset are often good enough.
(1) Approximate Query Processing: Many analytics frameworks support approximate query processing (AQP) using standard SQL syntax (e.g., BlinkDB [5], Presto [29]).
They power many popular applications like exploratory data analysis [19,54] and interactive debugging [3].
For example, products analysts could use AQP systems to quickly decide if an advertising campaign needs to be changed based on a sample of click through rates.
AQP systems can bound both the time taken and the quality of the result by selecting appropriately sized inputs (samples) to met the deadline and error bound.
Sample sizes are typically small relative to the original data (often, one-twentieth to one-fifth [43]) and many equivalent samples exist.
Thus, sample selection presents a significant opportunity for smart scheduling.
(2) Machine Learning: The last few years has seen the deployment of large-scale distributed machine learning algorithms for commercial applications like spam classification [40] and machine translation [18].
Recent advances [17] have introduced stochastic versions of these algorithms, for example stochastic gradient descent [16] or stochastic L-BFGS [53], that can use small random data samples and provide statistically valid results even for large datasets.
These algorithms are iterative and each iteration processes only a small sample of the data.
Stochastic algorithms are agnostic to the sample selected in each iteration and support flexible scheduling.
(3) Erasure Coded Storage: Rise in data volumes have also led to clusters employing efficient storage techniques like erasure codes [50].
Erasure codes provide fault tolerance by storing k extra parity blocks for every n data blocks.
Using any n data blocks of the (n + k) blocks, applications can compute their input.
Such storage systems also provide choices for data-aware scheduling.Note that while the above applications provide an opportunity to pick any subset of the input data, our system can also handle custom sampling functions, which generate samples based on application requirements.
Data aware scheduling is important for both the input as well as intermediate stages of jobs due to their IOintensive nature.
In the input stage, tasks reads their input from a single machine and the natural goal is locality i.e. to schedule the task on a machine that stores its input ( §2.2.1).
For intermediate stages, tasks have their input spread across multiple machines.
In this case, it is not possible to co-locate the task with all its inputs.
Instead, the goal in this case is to schedule the task at a machine that minimizes the time it takes to transfer all remote inputs.
As over-subscribed cross-rack links are the main bottleneck in reads [22], we seek to balance the utilization of these links ( §2.2.2).
Riding on the trend of falling memory prices, clusters are increasingly caching data in memory [11,58].
As memory bandwidths are about 10 × −100× greater than the fastest network bandwidths, data reads from memory provide dramatic acceleration for the IO-intensive analytics jobs.
However, to reap the benefits of in-memory caches, tasks have to be scheduled with memory locality, i.e., on the same machine that contains their input data.
Obtaining memory locality is important for timely completion of interactive approximate queries [9].
Iterative machine learning algorithms typically run 100's of iterations and lack of memory locality results in huge slowdown per iteration and the overall job.Achieving memory locality is a challenging problem in clusters.
Since in-memory storage is used only as a cache, data stored in memory is typically not replicated.
Further, the amount of memory in a cluster is relatively small (often by three orders of magnitude [9]) when compared to stable storage: this difference means that replicating data in memory is not practical.
Therefore, techniques for improving locality [8] developed for disk-based replicated storage are insufficient; they rely on the probability of locality increasing with the number of replicas.
Further, as job completion times are dictated by the slowest task in the job, improving performance requires memory locality for all its tasks [11].
These challenges are reflected in production Hadoop clusters.
A Facebook trace from 2010 [8,21] shows that less than 60% of tasks achieve locality even with three replicas.
As in-memory data is not replicated, it is harder for jobs to achieve memory locality for all their tasks.
Intermediate stages of a job have communication patterns that result in their tasks reading inputs from many machines (e.g., all-to-all "shuffle" or many-to-one "join" stages).
For I/O intensive intermediate tasks, the time to access data across the network dominates the running time, more so when intermediate outputs are stored in memory.
Despite fast network links [56] and newer topologies [6,35], bandwidths between machines connected to the same rack switch are still 2× to 5× higher than to machines outside the rack switch via the network core.
rack hotspots, i.e., optimizing the bottleneck cross-rack link [13,24] can significantly improve performance.
Given the over-subscribed cross-rack links and the slowest tasks dictating job completion, it is important to balance traffic on the cross-rack links [15].
Figure 2 illustrates the result of having unbalanced cross-rack links.
The schedule in Figure 2(b) results in a cross-rack skew, i.e., ratio of the highest to lowest used network links, of only 4 3 (or 1.33) as opposed to 6 2 (or 3) in Figure 2(a).
To highlight the importance of cross-rack skew, we used a trace of Hadoop jobs run in a Facebook cluster from 2010 [21] and computed the cross-rack skew ratio.
Figure 3 shows a CDF of this ratio and is broken down by the number of map tasks in the job.
From the figure we can see that for jobs with 50 − 150 map tasks more than half of the jobs have a cross-rack skew of over 4×.
For larger jobs we see that the median is 15× and the 90 th percentile value is in excess of 30×.
How much do the above-mentioned lack of locality and imbalanced network usage hurt jobs?
We estimate the potential for data-aware scheduling to speed up jobs using the same Facebook trace (described in detail in §6).
We mimic job performance with an ideal data-aware scheduler using a "what-if" simulator.
Our simulator is unconstrained and (i) assigns memory locality for all the tasks in the input phase (we assume 20× speed up for memory locality [45] compared to reading data over the network based on our micro-benchmark) and (ii) places tasks to perfectly balance cross-rack links.
We see that jobs speed up by 87.6% on average with such ideal dataaware scheduling.
Given these potential benefits, we have designed KMN, a scheduling framework that exploits the available choices to improve performance.
At the heart of KMN lie scheduling techniques to increase locality for input ( §3) stages and balance network usage for intermediate ( §4) stages.
In §5, we describe an interface that allows applications to specify all available choices to the scheduler.
For the input stage (i.e., the map stage in MapReduce or the extract stage in Dryad) accounting for combinatorial choice leads to improved locality and hence reduced completion time.
Here we analyze the improvements in locality in two scenarios: in §3.1 we look at jobs which can use any K of the N input blocks; in §3.2 we look at jobs which use a custom sampling function.We assume a cluster with s compute slots per machine.
Tasks operate on one input block each and input blocks are uniformly distributed across the cluster, this is in line with the block placement policy used by Hadoop.
For ease of analysis we assume machines in the cluster are uniformly utilized (i.e., there are no hot-spots).
In our evaluation §6) we consider hot-spots due to skewed input-block and machine popularity.
Many modern systems e.g., BlinkDB [5], Presto [29], AQUA [2] operate by choosing a random subset of blocks from shuffled input data.
These systems rely on the observation that block sampling [20] is statistically equivalent to uniform random sampling (page 243 in [55]) when each block is itself a random sample of the overall population.
Given a sample size K, these systems can operate on any K input blocks i.e., for an input of size N the scheduler can choose any one of񮽙 N K 񮽙 combinations.In the cluster setup described above, the probability that a task operating on an input block gets locality is p t = 1 − u s where u is the cluster utilization (probability that all slots in a machine are busy is = u s ).
For such a cluster the probability for K out of N tasks getting locality is given by the binomial CDF function with the probability of success = p t , i.e., 1− ∑ K−1 i=0 񮽙 N i 񮽙 p i t (1 − p t ) N−i .
The dominant factor in this probability is the ratio between K and N.
In Figure 4 we fix the number of slots per machine to s = 8 and plot the probability of K = 10 and K = 100 tasks getting locality in a job with varying input size N and varying cluster utilization.
We observe that the probability of achieving locality is high even when 90% of the cluster is utilized.
We also compare this to a baseline that does not exploit this combinatorial choice and pre-selects a random K blocks beforehand.
For the baseline the probability that all tasks are local drops dramatically even with cluster utilization of 60% or less.
Some systems require additional constraints on the samples used and use custom sampling functions.
These sampling functions can be used to produce several Kblock samples and the scheduler can pick any sample.
The scheduler is however constrained to use all of the Kblocks from one sample.
We consider a sampling function that produces f disjoint samples and analyze locality improvements in this setting.As noted previous, the probability of a task getting locality is p t = 1 − u s .
The probability that all K blocks in a sample get locality is p K t .
Since the f samples are disjoint (and therefore the probability of achieving locality is independent) the probability that at least one among the f samples can achieve locality is Figure 5 shows the probability of K = 10 and K = 100 tasks achieving locality with varying utilization and number of samples.
We see that the probability of achieving locality significantly increases with f .
At f = 5 we see that small jobs (10 tasks) can achieve complete locality even when the cluster is 80% utilized.p j = 1 − (1 − p K t ) f .
We thus find that accounting for combinatorial choices can greatly improve locality for the input stage.
Next we analyze improvements for intermediate stages.
Intermediate stages of jobs commonly involve one-toall (broadcast), many-to-one (coalesce) or many-to-many (shuffle) network transfers [23].
These transfers are network-bound and hence, often slowed down by congested cross-rack network links.
As described in §2.2.2, data-aware scheduling can improve performance by better placement of both upstream and downstream tasks to balance the usage of cross-rack network links.While effective heuristics can be used in scheduling downstream tasks to balance network usage (we deal with this in §5), they are nonetheless limited by the locations of the outputs of upstream tasks.
Scheduling upstream tasks to balance the locations of their outputs across racks is often complicated due to many dynamic factors in clusters.
First, they are constrained by data locality ( §3) and compromising locality is detrimental.
Second, the utilization of the cross-rack links when downstream tasks start executing are hard to predict in multi-tenant clusters.
Finally, even the size of upstream outputs varies across jobs and are not known beforehand.We overcome these challenges by scheduling a few additional upstream tasks.
For an upstream stage with K tasks, we schedule M tasks (M > K).
Additional tasks increase the likelihood that task outputs are distributed across racks.
This allows us to choose the "best" K out of M upstream tasks, out of 񮽙 M K 񮽙 choices, to minimize cross-rack network utilization.
In the rest of this section, we show analytically that a few additional upstream tasks can significantly reduce the imbalance ( §4.1).
§4.2 describes a heuristic to pick the best K out of M upstream tasks.
However, not all M upstream tasks may finish simultaneously because of stragglers; we modify our heuristic to account for stragglers in §4.3.
Figure 6: Cross-rack skew as we vary M/K for uniform and log-normal distributions.
Even 20% extra upstream tasks greatly reduces network imbalance for later stages.
upstream task outputs are equal in size and network links are equally utilized.
We only model tasks at the level of racks and evaluate the cross-rack skew (ratio of the rack with largest and smallest number of upstream tasks) using both synthetic distributions of upstream task locations as well as data from our Facebook trace.
Synthetic Distributions: We first consider a scheduler that places tasks on racks uniformly at random.
Figure 6(a) plots the cross-rack skew in a 100 rack cluster for varying values of K (i.e., the stage's desired number of tasks) and M/K (i.e., the fraction of additional tasks launched).
We can see that even with a scheduler that places the upstream tasks uniformly, there is significant skew for large jobs when there are no additional tasks ( M K = 1).
This is explained by the balls and bins problem [46] where the maximum imbalance is expected to be O(logn) when distributing n balls.
However, we see that even with 10% to 20% additional tasks ( M K = 1.1 − 1.2) the cross-rack skew is reduced by ≥ 2×.
This is because when the number of upstream tasks, n is > 12, 0.2n > logn.
Thus, we can avoid most of the skew with just a few extra tasks.We also repeat this study with a log normal distribution (θ = 0, m = 1) of upstream task placement; this is more skewed compared to the uniform distribution.However, even with a log-normal distribution, we again see that a few extra tasks can be very effective at reducing skew.
This is because the expected value of the most loaded bin is still linear and using 0.2n additional tasks is sufficient to avoid most of the skew.
Facebook Distributions: We repeat the above analysis using the number and location of upstream tasks of a phase in the Facebook trace (used in §2.2.2).
Recall the high cross-rack skew in the Facebook trace.
Despite that, again, a few additional tasks suffices to eliminate a large fraction of the skews.
Figure 7 plots the results for varying values of M K for different jobs.
A large fraction of the skew is reduced by running just 10% more tasks.
This is nearly 66% of the reductions we get using M K = 2.
In summary we see that running a few extra tasks is an effective strategy to reduce skew, both with synthetic as well as real-world distributions.
We next look at mechanisms that can help us achieve such reduction.
The problem of selecting the best K outputs from the M upstream tasks can be stated as follows: We are givenM upstream tasks U = u 1 ...u M , R downstream tasks D = d 1 ...d R and their corresponding rack locations.
Let us assume that tasks are distributed over racks 1...L and let U � ⊂ U be some set of K upstream outputs.
Then for each rack we can define the uplink cost C 2i−1 and downlink cost C 2i using a cost function C i (U � , D).
Our objective then is to select U � to minimize the most loaded link i.e. arg minU � max i∈2L C i (U � , D)While this problem is NP-Hard [57], many approximation heuristics have been developed.
We use a heuristic that corresponds to spreading our choice of K outputs across as many racks as possible.
1 Our implementation for this approximation heuristic is shown in Algorithm 1.
We start with the list of upstream tasks and build a hash map that stores how many tasks were run on each rack.
Next we sort the tasks first by their index within a rack and then by the number of tasks in the rack.
This sorting criteria ensures that we first see one task from each rack, thus ensuring we spread our choices across racks.
We use an additional heuristic of favoring racks with more outputs to help our downstream task placement techniques ( §5.2.2).
The main computation cost in this method is the sorting step and hence this runs in O(MlogM) time for M tasks.Algorithm 1 Choosing K upstream outputs out of M using a round-robin strategy 1: Given: upstreamTasks -list with rack, index within rack for each task 2: Given: K -number of tasks to pick While the previous section described a heuristic to pick the best K out of M upstream outputs, waiting for all M can be inefficient due to stragglers.
Stragglers in the upstream stage can delay completion of some tasks which cuts into the gains obtained by balancing the network links.
Stragglers are a common occurrence in clusters with many clusters reporting significantly slow tasks despite many prevention and speculation solutions [10,13,61].
This presents a trade-off in waiting for all M tasks and obtaining the benefits of choice in picking upstream outputs against the wasted time for completion of all M upstream tasks including stragglers.
Our solution for this problem is to schedule downstream tasks at some point after K upstream tasks have completed but not wait for the stragglers in the M tasks.
We quantify this trade-off with analysis and micro-benchmarks.
We study the impact of stragglers in the Facebook trace when we run 2%, 5% and 10% extra tasks (i.e., M K = 1.02, 1.05, 1.1).
We compute the difference between the time taken for the fastest K tasks and the time to com- plete all M tasks.
Figure 8 shows that waiting for the extra tasks can inflate the completion of the upstream phase by 20% − 40% (for jobs with > 150 tasks).
Also, the trend of using a large number of small tasks [48] for interactive jobs will only worsen such inflation.
On the other hand avoiding upstream stragglers by using the fastest tasks reduces the available choice.
Consequently, the time taken for downstream data transfer increases.The lack of choices from extra tasks means we cannot balance network usage.
Figure 9 shows that not using choice from additional tasks can increase data transfer time by 20% for small jobs (11 to 50 tasks) and up to 40% for large jobs (> 150 tasks).
We now devise a simple approach to balance between the above two factorswaiting for upstream stragglers versus losing choice for downstream data transfer.
The problem we need to solve can be formulated as: we have M upstream tasks u 1 , u 2 , ..., u M and for each task we have corresponding rack locations.
Our goal is to find the optimal delay after the first K tasks have finished, such that the overall time taken is minimized.
In other words, our goal is to find the optimal K � tasks to wait for before starting the downstream tasks.We begin with assuming an oracle that can give us the task finish times for all the tasks.
Given such a oracle we can sort the tasks in an increasing order of finish times such that F j ≥ F i ∀ j > i. Let us define the waiting delay for tasks K + 1 to M as D i = F i − F k ∀i > k.
We also assume that given K � tasks, we can compute the optimal K tasks to use ( §4.2) and the estimated transfer time S K � .
Our problem is to pick K � (K ≤ K � ≤ M) such that the total time for the data transfer is minimized.
That is we need to pick K � such that F k + D k � + S k � is minimized.
In this equation F k is known and independent of K � .
Of the other two, D k � increases as k � goes from K to M, while S k � decreases.
However as the sum of an increasing and decreasing function is not necessarily convex 2 it isn't easy to minimize the total time taken.Delay Heuristic: While the brute-force approach would require us to try all values from K to M, we develop two heuristics that allow us to bound the search space and quickly find the optimal value of K � .
• Bounding transfer: At the beginning of the search procedure we find the maximum possible improvement we can get from picking the best set of tasks.
Whenever the delay D K � is greater than the maximum improvement, we can stop the search as the succeeding delays will increase the total time.
• Coalescing tasks: We can also coalesce a number of task finish events to further reduce the search space.
For example we can coalesce task finish events which occur close together by time i.e., cases D i+1 − Di < δ .
This will mean our result is off by at most δ from the optimal, but for small values of δ we can coalesce tasks of a wave that finish close to each other.Using these heuristics we can find the optimal number of tasks to wait for quickly.
For example, in the Facebook trace described before using M/K = 1.1 or 10% extra tasks, determining the optimal wait time for a job requires looking at less than 4% of all configurations when we use a coalescing error of 1%.
We found coalescing tasks to be particularly useful as even with a δ of 0.1% we need to look at around 8% of all possible configurations.
Running without any coalescing is infeasible since it takes ≈ 1000 ms.Finally, we relax our assumption of an oracle as follows.
While the task finish times are not exactly known beforehand, we use job sizes to figure out if the same job has been run before.
Based on this we use the job history to predict the task finish times.
This approach should work well for clusters that have many jobs run periodically [36].
In case the job history is not available we can fit the tasks length distribution using the first few task finish times and use that to get approximate task finish times for the rest of the tasks [28].
We have built KMN on top of Spark [60], an open-source cluster computing framework.
Our implementation is based on Spark version 0.7.3 and KMN consists of 1400 lines of Scala code.
In this section we discuss the features of our implementation and implementation challenges.
We define a blockSample operator which jobs can use to specify input constraints (for instance, use K blocks from file F) to the framework.The blockSample operator takes two arguments: the ratio K N and a sampling function that can be used to impose constraints.
The sampling function can be used to choose user-defined sampling algorithms (e.g., stratified sampling).
By default the sampling function picks any K of N blocks.Consider an example SQL query and its corresponding Spark [60] version shown in Figure 10.
To run the same query in KMN we just need to prefix the query with the blockSample operator.
The sampler argument is a Scala closure and passing None causes the scheduler to use the default function which picks any K out of the N input blocks.
This design can be readily adapted to other systems like Hadoop MapReduce and Dryad.KMN also provides an interface for jobs to introspect which samples where used in a computation.
This can be used for error estimation using algorithms like Bootstrap [4] and also provides support for queries to be repeated.
We implement this in KMN by storing the K partitions used during computation as a part of a job's lineage.
Using the lineage also ensures that the same samples are used if the job is re-executed during fault recovery [60].
We modify Spark's scheduler in KMN to implement the techniques described in earlier sections.
Schedulers for frameworks like MapReduce or Spark typically use a slot-based model where the scheduler is invoked whenever a slot becomes available in the cluster.
In KMN, to choose any K out of N blocks we modify the scheduler to run tasks on blocks local to the first K available slots.
To ensure that tasks don't suffer from resource starvation while waiting for locality, we use a timeout after which tasks are scheduled on any available slot.
Note that, choosing the first K slots provides a sample similar or slightly better in quality compared to existing systems like Aqua [2] or BlinkDB [5] that reuse samples for short time periods.
To schedule jobs with custom sampling functions, we similarly modify the scheduler to choose among the available samples and run the computation on the sample that has the highest locality.
Existing cluster computing frameworks like Spark and Hadoop place intermediate stages without accounting for their dependencies.
However smarter placement which accounts for a tasks' dependencies can improve performance.
We implemented two strategies in KMN: Greedy assignment: The number of cross-rack transfers in the intermediate stage can be reduced by colocating map and reduce tasks (more generally any dependent tasks).
In the greedy placement strategy we maximize the number of reduce tasks placed in the rack with the most map tasks.
This strategy works well for small jobs where network usage can be minimized by placing all the reduce tasks in the same rack.Round-robin assignment: While greedy placement minimizes the number of transfers from map tasks to reduce tasks it results in most of the data being sent to one or a few racks.
Thus the links into these racks are likely to be congested.
This problem can be solved by distributing tasks across racks while simultaneously minimizing the amount of data sent across racks.
This can be achieved by evenly distributing the reducers across racks with map tasks.
This strategy can be shown to be optimal if we know the map task locations and is similar in nature to the algorithm described in §4.2.
We perform a more detailed comparison of the two approaches in §6 One consequence of launching extra tasks to improve performance is that the cluster utilization could be af- fected by these extra tasks.
To avoid utilization spikes, in KMN the value for M/K (the percentage of extra tasks to launch) can only be set by the cluster administrator and not directly by the application.
Further, we implemented support for killing tasks once the scheduler decides that the tasks' output is not required.
Killing tasks in Spark is challenging as tasks are run in threads and many tasks share the same process.
To avoid expensive clean up associated with killing threads [1], we modified tasks in Spark to periodically poll and check a status bit.
This means that tasks sometimes could take a few seconds more before they are terminated, but we found that this overhead was negligible in practice.In KMN, using extra tasks is crucial in extending the flexibility of many choices throughout the DAG.
In §3 and §4 we discussed how to use the available choices in the input and intermediate stages in a DAG.
However, jobs created using frameworks like Spark or DryadLINQ can extend across many more stages.
For example, complex SQL queries may use a map followed a shuffle to do a group-by operation and follow that up with a join.
One solution to this would be run more tasks than required in every stage to retain the ability to choose among inputs in succeeding stages.
However we found that in practice this does not help very much.
In frameworks like Spark which use lazy evaluation, every stage following than the first stage is treated as an intermediate stage.As we use a round-robin strategy to schedule intermediate tasks ( §5.2.2), the outputs from the first intermediate stage are already well spread out across the racks.
Thus there isn't much skew across racks that affects the performance of following stages.
In evaluation runs we saw no benefits for later stages of long DAGs.
We evaluate the benefits of KMN using two approaches: first we run approximate queries used in production at Conviva, a video analytics company, and study how KMN compares to using existing schedulers with pre-selected samples.
Next we analyze how KMN behaves in a shared cluster, by replaying a workload trace obtained from Facebook's production Hadoop cluster.Metric: In our evaluation we measure percentage improvement of job completion time when using KMN.
We define percentage improvement as:% Improvement = Baseline Time − KMN Time Baseline Time × 100Our evaluation shows that,• KMN improves real-world sampling-based queries from Conviva by more than 50% on average across various sample sizes and machine learning workloads by up to 43%.
• When replaying the Facebook trace, on an EC2 cluster, KMN can improve job completion time by 81% on average (92% for small jobs)• By using 5% -10% extra tasks we can balance bottleneck link usage and decrease shuffle times by 61% -65% even for jobs with high cross-rack skew.
Cluster Setup:We run all our experiments using 100 m2.4xlarge machines on Amazon's EC2 cluster, with each machine having 8 cores, 68GB of memory and 2 local drives.
We configure Spark to use 4 slots and 60 GB per machine.
To study memory locality we cache the input dataset before starting each experiment.
We compare KMN with a baseline that operates on a pre-selected sample of size K and does not employ any of the shuffle improvement techniques described in §4, §5.
We also label the fraction of extra tasks run (i.e., M/K), so KMN-M/K = 1.0 has K = M and KMN-M/K = 1.05 has 5% extra tasks.
Finally, all experiments were run at least three times and we plot median values across runs and use error bars to show minimum and maximum values.
Workload: Our evaluation uses a workload trace from Facebook's Hadoop cluster [21].
The traces are from a mix of interactive and batch jobs and capture over half a million jobs on a 3500 node cluster.
We use a scaled down version of the trace to fit within our cluster and use the same inter-arrival times and the task-to-rack mapping We evaluate the benefits of using KMN on three workloads: real-world approximate queries from Conviva, a machine learning workload running Stochastic Gradient Descent and a Hadoop workload trace from Facebook.
We first present results from running 4 real-world sampling queries obtained from Conviva, a video analytics company.
The queries were run on access logs obtained across a 5-day interval.
We treat the entire data set as N blocks and vary the sampling fraction (K/N) to be 1%, 5% and 10%.
We run the queries at 50% cluster utilization and run each query multiple times.
Figure 11 shows the median time taken for each query and we compare KMN-M/K = 1.05 to the baseline that uses pre-selected samples.
For query 1 and query 2 we can see that KMN gives 77%-91% win across 1%, 5% and 10% samples.
Both these queries calculate summary statistics across a time window and most of the computation is performed in the map stage.
For these queries KMN ensures that we get memory locality and this results in significant improvements.
For queries 3 and 4, we see around 70% improvement for 1% samples, and this reduces to around 25% for 10% sampling.
Both Table 1: Improvements over baseline, by job size and stage these queries compute the number of distinct users that match a specified criteria.
While input locality also improves these queries, for larger samples the reduce tasks are CPU bound (while they aggregate values).
Next, we look at performance benefits for a machine learning workload that uses sampling.
For our analysis, we use Stochastic Gradient Descent (SGD).
SGD is an iterative method that scales to large datasets and is widely used in applications such as machine translation and image classification.
We run SGD on a dataset containing 2 million data items, where each each item contains 4000 features.
The complete dataset is around 64GB in size and each of our iterations operates on a 1% sample 1% of the data.
Thus the random sampling step reduces the cost of gradient computation by 100× but maintains rapid learning rates [52].
We run 10 iterations in each setting to measure the total time taken for SGD.
Each iteration consists of a DAG comprised of a map stage where the gradient is computed on sampled data items and the gradient is then aggregated from all points.
The aggregation step can be efficiently performed by using an aggregation tree as shown in Figure 12.
We implement the aggregation tree using a set of shuffle stages and use KMN to run extra tasks at each of these aggregation stages.The overall benefits from using KMN are shown in Figure 13.
We see that KMN-M/K = 1.1 improves performance by 43% as compared to the baseline.
These improvements come from a combination of improving memory locality for the first stage and by improving shuffle performance for the aggregation stages.
We further break down the improvements by studying the effects of KMN at every stage in Figure 14.
Job).
This is because the final aggregation steps usually have fewer tasks with smaller amounts of data, which makes running extra tasks not worth the overhead.
We plan to investigate techniques to estimate this trade-off and automatically determine which stages to use KMN for in the future.
We next quantify the overall improvements across the trace from using KMN.
To do this, we use a baseline configuration that mimics task locality from the original trace while using pre-selected samples.
We compare this to KMN by job size is shown in Figure 15 and relative improvements are shown in Table 1.
As seen in the figure, using KMN leads to around 92% improvement for small jobs with < 10 tasks and more than 60% improvement for all other jobs.
Across all jobs KMN-M/K = 1.05 improves performance by 81%, which is 93% of the potential win ( §2.3).
To quantify where we get improvements from, we break down the time taken by different stages of a job.
Improvements for the input stage or the map stage are shown in Figure 16.
We can see that using KMN we are able to get memory locality for almost all the jobs and this results in around 94% improvement in the time taken for the map stage.
This is consistent with the predictions from our model in §3 and shows that pushing down sampling to the run-time can give tremendous benefits.
The improvements in the shuffle stage are shown in Figure 19.
For small jobs with < 10 tasks we get around 85% improvement and these are primarily because we co-locate the mappers and reducers for small jobs and thus avoid network transfer overheads.
For large jobs with > 100 tasks we see around 30% improvement due to reduction in cross-rack skew.
Next, we attempt to measure how the locality obtained by KMN changes with cluster utilization.
As we vary the cluster utilization, we measure the average job completion time and fraction of jobs where all tasks get locality.
The results shown in Figure 17 show that for up to 30% average utilization, KMN ensures that more than 80% of Job Size jobs get perfect locality.
We also observed significant variance in the utilization during the trace replay and the distribution of utilization values is shown as a boxplot in Figure 18.
From this figure we can see that while average utilization is 30% we observe utilization spikes of up to 90%.
Because of such utilization spikes, we see periods of time where all jobs do not get locality.M/K = 1.0 M/K =1.05 M/K =1.
Finally, at 50% average utilization (utilization spikes > 90%) only around 45% of jobs get locality.
This is lower than predictions from our model in §3.
There are two reasons for this difference: First, our experimental cluster has only 400 slots and as we do 10% sampling (K/N = 0.1), the setup doesn't have enough choices for jobs with > 40 map tasks.
Further the utilization spikes also are not taken into account by the model and jobs which arrive during a spike do not get locality.
in Figure 19 and the improvements with respect to the baseline are shown in Table 2.
From the figure, we see that for small jobs with less than 10 tasks there is almost no improvement from running extra tasks as they usually do not suffer from cross-rack skew.
However for large jobs with more than 100 tasks, we now get up to 36% improvement in shuffle time over the baseline.
Further, we can also analyze how the benefits are sensitive to the cross-rack skew.
We plot the average shuffle time split by cross-rack skew in Figure 20.
Correspondingly we list the improvements over the baseline in Table 3.
We can see that for jobs which have low crossrack skew, we get up to 33% improvement when using KMN-M/K = 1.1.
Further, for jobs which have crossrack skew > 8, we get up to 65% improvement in shuffle times and a 17% improvement over M/K = 1.
We next study the impact of stragglers and the effect of using the delayed stage launch heuristic from §4.3.
We run the Facebook workload at 30% cluster utilization with KMN-M/K = 1.1 and compare our heuristic to two baseline strategies.
In one case we wait for the first K map tasks to finish before starting the shuffle while in the other case we wait for all M tasks for finish.
The performance break down for each stage is shown in Figure 21.
From the figure we see that for small jobs (< 10 tasks) which don't suffer from cross-rack skew, KMN performs similar to picking the first K map outputs.
This is because in this case stragglers dominate the shuffle wins possible from using extra tasks.
For larger tasks we see that our heuristic can dynamically adjust the stage delay to ensure we avoid stragglers while getting the benefits of balanced shuffle operations.
For example for jobs with > 10 tasks KMN adds 5% − 14% delay after first K tasks complete and still gets most of the shuffle benefits.
Overall, this results in an improvement of up to 35%.
For more fine-grained analysis we also ran an eventdriven simulation that uses task completion times from the same Facebook trace.
The CDF of extra map tasks used is shown in Figure 22(b), where we see that around 80% of the jobs wait for 5% or more map tasks.
We also measured the time relative to when the first K map tasks finished and to normalize the delay across jobs we compute the relative wait time.
Figure 22(a) shows the CDF of relative wait times and we see that the delay is less than 25% for 62% of the jobs.
The simulation results again show that our relative delay is not very long and that job completion time can be improved when we use extra tasks available within a short delay.
To evaluate the importance of reduce placement strategy, we compare the time taken for the shuffle stage for the round-robin strategy described in §5.2.2 against a greedy assignment strategy that attempts to pack reducers into as few machines as possible.
Note that the baseline used in our earlier experiments used a random reducer assignment policy and §6.2.3 compares the round-robin strategy to random assignment.
Figure 23 shows the results from this experiment with the results broken down by job size.
From the results we can see that for jobs with > 10 tasks using a round-robin placement can improve performance by 10%-30%.
However for very small jobs, running tasks on more machines increases the variance and the greedy assignment in fact performs 8% better.
Cluster schedulers: Cluster scheduling has been an area of active research and recent work has proposed techniques to enforce fairness [32,39], satisfy job constraints [33] and improve locality [39,59].
In KMN, we focus on applications that have input choices and propose techniques to exploit the available flexibility while scheduling tasks.
Straggler mitigation solutions launch extra copies of tasks to mitigate the impact of slow running tasks [10,12,61].
While KMN shares the similarity of executing extra copies, our goals are different.
Further, straggler mitigation solutions are limited by the number of replicas of the input data, and can leverage our observation of combinatorial choices towards more effective speculation.
Prior efforts in improving shuffle performance [7,24] have looked at either provisioning the network better or scheduling flows to improve performance.
On the other hand, in KMN we use additional tasks and better placement techniques to balance data transfers across racks.
Finally, recent work [49] has also looked at using the power of many choices to reduce scheduling latency.
In KMN we exploit the power choices to improve network balance using just a few additional tasks.
Approximate Query Processing Systems: Approximate query processing (AQP) systems such as Aqua [2], STREAM [47], and BlinkDB [5] use pre-computed samples to answer queries.
These works are complimentary to our work, and we expect that projects like BlinkDB can use KMN to improve performance, while maintaining, or in some cases even improving response quality.
Prior work in databases has also proposed Online Aggregation [37] (OLA) methods that can be used to present approximate aggregation results while the input data is processed in a streaming fashion.
Recent extensions [25,51] have also looked at supporting OLA-style computations in MapReduce.
In contrast, KMN can be used for scheduling sampling applications which do not process the entire dataset and process a fixed and small sample of data.
Machine learning frameworks: Recently, a large body of work has focused on building cluster computing frameworks that support machine learning tasks.
Examples include GraphLab [34,44], Spark [60], DistBelief [27], and MLBase [41].
Of these, GraphLab and Spark add support for abstractions commonly used in machine learning.
Neither of these frameworks provide any explicit system support for sampling.
For instance, while Spark provides a sampling operator, this operation is carried out entirely in application logic, and the Spark scheduler is oblivious to the use of sampling.
The rapid growth of data stored in clusters, increasing demand for interactive analysis, and machine learning workloads have made it inevitable that applications will operate on subsets of data.
It is therefore imperative that schedulers for cluster computing frameworks exploit the available choices to improve performance.
As a first step towards this goal we have presented KMN, a system that improves data-aware scheduling for jobs with combinatorial choices.
Using our prototype implementation, we have shown that KMN can improve performance by increasing locality and balancing intermediate data transfers.
We are indebted to Ali Ghodsi, Kay Ousterhout, Colin Scott, Peter Bailis, the various reviewers and our shepherd Yuanyuan Zhou for their insightful comments and suggestions.
This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Adobe, Apple, Inc., Bosch, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, and Yahoo!.
