The buffer cache plays an essential role in smoothing the gap between the upper-level computational components and the lower-level storage devices.
A good buffer cache management scheme should be beneficial to not only the computational components, but also to the storage components by reducing disk I/Os.
Existing cache replacement algorithms are well optimized for disks in normal mode, but inefficient under faulty scenarios, such as a parity-based disk array with faulty disk(s).
To address this issue, we propose a novel asymmet-ric buffer cache replacement strategy, named Victim (or faulty) Disk(s) First (VDF) cache, to improve the reliability and performance of a storage system consisting of a buffer cache and disk arrays.
The basic idea is to give higher priority to cache the blocks on the faulty disks when the disk array fails, thus reducing the I/Os directed to the faulty disks.
To verify the effectiveness of the VDF cache, we have integrated VDF into two popular cache algorithms LFU and LRU, named VDF-LFU and VDF-LRU, respectively.
We have conducted extensive simulations as well as a prototype implementation.
The simulation results show that VDF-LFU can reduce disk I/Os to surviving disks by up to 42.3% and VDF-LRU can reduce those by up to 36.2%.
Our measurement results also show that VDF-LFU can speed up the online recovery by up to 46.3% under a spare-rebuilding mode with online reconstruction, or improve the maximum system service rate by up to 47.7% under a degraded mode without a reconstruction workload.
Similarly, VDF-LRU can speed up the online recovery by up to 34.6%, or improve the system service rate by up to 28.4%.
To reduce the number of I/O requests to the low level storage device, such as disk arrays, a cache is widely used and many cache algorithms exist to hide the long disk latencies.
These cache algorithms work well for disk arrays under normal fault-free mode.
However, when some disks in a disk array fail, the RAID may still work under this faulty scenario, either in a spare-rebuilding mode with online reconstruction or in a degraded mode without online reconstruction.
The cost of a miss to faulty disks might be dramatically different compared to the cost of a miss to surviving disks.
Existing cache algorithms cannot capture this difference because they treat the underlying (faulty or surviving) disks the same.We take an example as shown in Figure 1, which illustrates two different cache miss situations in a storage subsystem composed of a parity-based RAID with one faulty disk in degraded mode.
As shown in Fig- ure 1(a), the missed data resides in the faulty disk.
The RAID controller accesses the surviving disks to fetch all data and parity in the same stripe to regenerate the lost data.
Therefore, to service one cache miss, several read requests are needed depending on the RAID organization.
However, if the missed data is in a surviving disk as shown in Figure 1(b), only one read request to the corresponding surviving disk is generated.
Similar situations are observed in spare-rebuilding mode.
A simple analysis shows that in a RAID5 system consisting of n disks, when a disk fails, the cost to fetch data from a faulty disk might be n âˆ’ 1 times higher than the cost to access data from a surviving disk.
This extra disk I/O activity will in turn reduce the effective array bandwidth available for reconstruction or user access.When a disk array starts online reconstruction, it uses up regular bandwidth.
Compared to offline reconstruction, during the process of online reconstruction, the user workflow interferes with the reconstruction workflow.
As a result, the online reconstruction duration grows significantly compared to offline reconstruction.
Wu et al. [1] point out that, in a heavy user workflow, the duration of online reconstruction would grow as much as 70 times as that of the offline reconstruction.
In this (a) A read miss to a faulty disk might result in several additional read requests to the surviving disks.
(b) A read miss to a surviving disk would result in only one request to the corresponding surviving disk.
case, more requests to the surviving disks, caused by user requests, reduce the available reconstruction bandwidth and lengthen the reconstruction duration, which reduces the reliability of the storage system.On the other hand, in a degraded mode without a reconstruction workflow, a miss to faulty disks would cause all the surviving data in the same parity chain (stripe in RAID-5) to be read and add additional workflow to surviving disks.
With a decreasing serviceability and an increasing user workflow caused by misses to faulty disks, the storage subsystem might be overloaded under a heavy user workflow.Therefore, in parity-based disk arrays under faulty conditions, a miss to faulty disks is much more expensive than a miss to surviving disks.
Based on this observation, we propose an asymmetric buffer cache replacement strategy, named Victim (or faulty) Disk(s) First cache, or VDF for short, to improve the performance of storage subsystem composed of a parity-based disk array and its buffer cache.
The basic idea is to design a cache scheme to treat the faulty disks more favorably, or give higher priority to cache the data associated with the faulty disks.
The goal of this scheme is to reduce the cache miss directed to the faulty disk, and thus to reduce the I/O requests to the surviving disks overall.
Reduced disk I/O caused by the user workflow will (1) improve the performance of the disk array, and (2) allow more bandwidth for online reconstruction which in turn speeds up the recovery, and thus improves the reliability.
We make the following four contributions in this paper: Ratio or RGR, to capture the disk I/O activities of user workflows on the surviving disks when a storage system is under faulty conditions.
This would directly influence the maximum bandwidth for reconstruction in a spare-rebuilding mode and the bandwidth available to user workflows in a degraded mode.2.
We developed a novel cache-replacement scheme, VDF, by giving higher priority to cache the data associated with the faulty disks, to minimize the RGR.
VDF is flexible and could be integrated into existing cache algorithms such as LRU and LFU.3.
We conducted extensive simulations to verify the effectiveness of VDF under different workloads.
The simulation results show that VDF-LRU can reduce overall disk I/Os to surviving disks by up to 36.2% and VDF-LFU can reduce those by up to 42.3%.4.
We implemented VDF in the Linux software RAID system.
As a result, VDF-LFU can speed up the online recovery by up to 46.3% under spare-rebuilding mode, or improve the maximum system service rate by up to 47.7% under degraded mode.
Similarly, VDF-LRU can speed up the online recovery by up to 34.6%, or improve the system service rate by up to 28.4%.
The rest of the paper is organized as follows: Section 2 gives a brief overview of the background information and related work.
In Section 3, we describe our new metric, RGR, and the design of VDF.
A case study of VDF cache is given in Section 4; we describe integrating VDF into two typical cache-replacement algorithms, LRU and LFU, based on RAID-5.
We provide our simulation results of VDF in Section 5, and prototyping and measurement results in Section 6.
We conclude our paper and describe future work in Section 7.
In this section we briefly overview some background materials and related work.
Redundant Arrays of Independent Disks RAID [2] are popular solutions to provide high performance and reliability for today's storage systems.
Depending on its organizations, RAID could prevent data loss incurred by disk failures and even offer online services under faulty conditions.
With a faulty disk, these RAIDs would work in a spare-rebuilding mode to support online reconstruction, or in a degraded mode without reconstruction.
RAID can offer continuous online services even in faulty mode.
However, the recovery workload and user request can interfere with each other, and lead to longer recovery times.
Many solutions are proposed to solve this problem, such as optimizations of data/parity/spare layout [3][4][5][6][7], reconstruction workload [8][9][10][11][12], and user workload [1,13,14].
Menon et al. present a method to distribute spares to all disks, which would not only reduce the lost data per disk but also parallelize the reconstruction [4].
Holland et al. [3] propose a trade-off between RAID-1 (mirror) and RAID-5, named parity declustering, to balance the storage efficiency and the recovery performance.
Xin et al. use a RUSH-like hash algorithm to evenly distribute data, parity, and spares among the nodes in a distributed environment [5].
The track-based recovery (TBR) [8] algorithm provides a trade-off between block-based recovery and cylinder-based recovery, and balances the user response time and the recovery duration.
However, TBR requires much more buffer space compared to block-based recovery.
The pipelined recovery (PR) scheme [9] addresses this problem, and significantly reduces the buffer requirements close to that of the block-based recovery algorithms.
The disk-oriented recovery (DOR) algorithm [10] rebuilds the array at the disk-level instead of the stripe-level.
With this approach, DOR could absorb the bandwidth of the array as much as possible.
The popularity-based recovery (PRO) algorithm [11,12], builds upon the DOR algorithm, further improving the recovery performance by utilizing the spatial locality of user requests.Two techniques named redirection of reads and piggybacking of writes [13] are proposed to reduce the user workflow by employing the reconstructed spare disk to absorb parts of the requests to the faulty disk.
However, they need to maintain a bitmap in the dedicated cache in the RAID device to record the reconstruction status; as the increasing of disk size, a fine-granularity bitmap would consume too much memory, and increase synchronization costs.
For example, a bitmap with granularity of 4KB for a 2TB disk would require 64MB of memory, which limits the use of piggybacking of writes.
During the reconstruction, at most a coarse-granularity bitmap could be used only to redirect reads.
MICRO [14] achieves improved recovery performance by writing back the in-memory surviving data of the faulty disks into a spare disk first and using a file popularity table to find the hotspot.
MICRO treats all the blocks in the cache equally, which is similar to the general cachereplacement algorithms and has the same limitations.
WorkOut [1], an array-cache-array method, offloads the write requests and popular read requests to another disk array.
As a result, WorkOut speeds up the recovery process and improves the user response time.
However, WorkOut requires another disk array to help with the reconstruction and need maintain an addressing translation map, which might be much larger than a fine-granularity bitmap, in the dedicated cache on the RAID device.
This suffers from the same problem as redirection of reads and piggybacking of writes.
RAID-based storage systems usually work together with the buffer cache.
To improve the efficiency of the buffer cache, researchers have proposed many cachereplacement algorithms, such as LRU [15], LFU, FBR [16], LRU-k [17,18], 2Q [19], LRFU [20,21], MQ [22,23], LIRS [24,25], ARC [26], DULO [27], DISKSEEN [28] and more.
Each cache-replacement algorithm weigh the cached blocks with a different method, such as access interval, access frequency and so on, then decide which cached blocks to evict.
The LRU (Least-Recently-Used) algorithm is one of the most popular and effective policies for buffer cache management.
When a block needs to be inserted into the cache, the candidate to be evicted is the block which is least recently used.
That is to say, the weight of the cached blocks in LRU is its last access timestamp.
The block with the smallest last access timestamp is evicted.
The LFU (Least-Frequently-Used) algorithm replaces the least frequently used block.
In other words, the weight of the cached blocks in LFU is its number of accesses.
The block with the smallest number of accesses is evicted.
Other algorithms, such as LRU-k, 2Q, LRFU, MQ, LIRS, and ARC, integrate LRU and LFU algorithm together and demonstrate good performance under various scenarios.
DULO and DISKSEEN consider both temporal and spatial locality when a block needs to be replaced.However, the above cache-replacement algorithms work well when the RAID system is under normal operating mode.
When some disks in the RAID system fail, it runs under faulty condition, but the buffer cache layer is not aware of the underlying failures in RAID and thus the existing cache algorithms do not work well as explained in Section 1.
This motivates us to propose VDF: a cache scheme to treat the faulty disks more favorably, or give a higher priority to cache the data associated with faulty disks.
The goal is to reduce the cache misses directed to the faulty disk and thus to reduce the I/O requests to the surviving disks overall.
As our VDF only increases the weight of blocks in the faulty disks, theoretically it could work with the above-mentioned general cache-replacement algorithms.
In this section, we propose a new metric to describe the cache efficiency of disk I/O activities.
We show how to use it to evaluate disk arrays under faulty conditions, and then we describe our VDF scheme.
Before our discussion, we summarize the symbols in Table 1.
Traditional cache-replacement algorithms are essentially evaluations on access probability of cached blocks, based on the assumption that the penalty of each miss at the same level is the same.
However, in parity-based RAID with faulty disk(s), the penalty of a miss to the lost data in the faulty disks might be much more expensive than that of a miss to surviving data.
Therefore, from the aspect of a RAID device, the buffer cache performance should not be simply evaluated by the traditional metrics such as Hit Ratio or Miss Ratio, particularly when the RAID is under faulty conditions.
To address this issue, we propose a new metric called Requests Generation Ratio or RGR.
This is the ratio of the number of requested blocks to the surviving disks and the number of the requested blocks to buffer cache, to evaluate the cache performance from the view point of a faulty RAID device.
RGR represents the disk activities to service an I/O request to the buffer cache.
Ideally, if all I/O requests are serviced by the buffer cache, RGR will be 0 (no disk I/Os are generated).
For missed I/O requests, RGR will be different depending on the penalty to each underlying disk.
For example, in Figure 1(a) the RGR of a miss to the faulty disk is 4 because 4 disk I/Os are generated to service the missed I/O request, and in Figure 1(b), the RGR of a miss to surviving disks is 1.
To calculate the RGR, we assume a parity-based RAID of T data blocks with a buffer cache of C blocks.
The access probability of a block B i is p i , where 0 â‰¤ i â‰¤ T âˆ’1, with a miss penalty of M P i in terms of the total requested blocks to surviving disks caused by a miss.
From the viewpoint of a certain workload, p i actually represents the ratio of the number of request on B i and the number of total block requests.
If block B i is not referenced in this workload, p i should be 0.
As we have mentioned above, different cache algorithms evaluate p i with different approaches in runtime environments.
If a block is serviced by the cache, the corresponding miss penalty M P i = 0.
Therefore, the RGR of the next block request can be described by the following Equation 1.
RGR = T âˆ’1 i=0 (p i Ã— M P i )(1)3.2 Using RGR to Evaluate the Cache Efficiency in Faulty ModeConsider a system composed of a buffer cache and a RAID in faulty mode which service a certain user workload.
We have the following symbols.
First, the total serviceability of all surviving disks is BW in terms of I/O bandwidth.
Second, the unfiltered user workload would take BW U bandwidth, which is the service rate of the system from a user's perspective.
The average RGR of the buffer cache is RGR.
Therefore, the filtered user workload should take about BW U Ã— RGR bandwidth.
Third, all the remaining bandwidth BW R of all surviving disks could be utilized for reconstruction.
Lastly, the total amount of surviving data for reconstruction is Q. Equation 2 describes the relationships among BW , BW U , RGR, and BW R .
BW = BW U Ã— RGR + BW R(2)We first consider the spare-rebuilding mode.
The surviving disks would suffer from more requests as explained in Section 1.
It means that the I/O bandwidth available for reconstruction on the surviving disks would be less than the I/O bandwidth for reconstruction on the spare disk.
The total amount of requested data for reconstruction on each disk (including the surviving disks and the spare disks) is the same.
Therefore, to the online recovery process, the I/O bandwidth for reconstruction on the surviving disks is the bottleneck.
The reconstruction duration RD could be described with Equation 3.
RD = Q BW âˆ’ BW U Ã— RGR(3)From Equation 3, we can find that, if Q, BW , and BW U are fixed, with the decreasing RGR, the reconstruction duration RD (and thus MTTR) decreases.
Therefore, to minimize the MTTR, we should minimize the RGR.We next consider the degraded mode without reconstruction.
Each surviving disk would suffer from the extra requests caused by the access to faulty disks.
The filtered user workload should not exceed the total serviceability of all surviving disks.
In another words, the maximum unfiltered user workload BW U should not exceed BW RGR .
Therefore, we should minimize the RGR to maximize the system serviceability, which is described with a maximum BW U .
From the above discussion, we notice that compared to the traditional metrics on cache evaluation, such as miss ratio, RGR is a useful metric to demonstrate two important indicators of a faulty disk array more clearly and directly.
One is the reconstruction time which is directly related to MTTR and affects the system reliability.
The other is the throughput that indicates the performance of the storage system.
Based on the above analysis, we propose our VDF cache aiming at reducing the RGR for parity-based RAID under faulty conditions, either to enhance the system reliability by speeding up the reconstruction process in sparerebuilding mode, or to improve the system performance by increasing the system serviceability in degraded mode without reconstruction workloads.
As it operates at the buffer cache level, VDF is practical and does not suffer from the same problems of the small dedicated cache in a RAID controller.Cache-replacement algorithms are essentially evaluations on access probability of cached blocks.
Once a miss occurs, typically a block should be evicted from cache, and the missed block would be loaded to the free space.
General replacement algorithms evict the block with the smallest access probability to reduce the total access probability of the remaining blocks out of buffer cache.
However, to minimize the RGR, the eviction of a block should not only be determined by the access probability but also by the miss penalty.
In our VDF cache, we adopt the same evaluation approach of access probability p i for each cached block as the general cache.
Furthermore, the miss penalty M P i of each block is evaluated with the requested blocks to the surviving disks of this block; the block with the minimum product of p i Ã— M P i is evicted from the cache to minimize the RGR.
To verify the effectiveness of VDF, we apply VDF in a RAID-5 system of n disks where one disk fails.
We focus on read operations for two reasons: first, in many applications, users are typically sensitive to read latency, particularly in a disk array under faulty conditions; second, in many storage systems, independent non-volatile memory is deployed as a write cache to enhance the reliability and this cache uses a dedicated write cache algorithm.
Although VDF cache can cooperate with caches at other levels by adjusting the miss penalty of blocks, for demonstration purposes we just consider a one-level buffer cache above the disk array.
Therefore, the miss penalty of blocks on the faulty disk would be n âˆ’ 1 in our following discussion, which means one cache miss to the faulty disk will result in n âˆ’ 1 I/O requests to the surviving disks.
To integrate VDF with any cache algorithm, the access probability should be evaluated with a quantitative approach.
Different cache-replacement algorithms evaluate the access probability of blocks using different approaches.
Most existing cache management algorithms can be categorized into LRU-like and LFUlike algorithms.
In LRU-like algorithms, the weight of blocks is often evaluated by the access time interval.
As it is costly to record the real access timestamp, a simple alternative is to record the access sequence number, and use the reciprocal of the interval access sequence number as the access probability.
This approach is widely used in many LRU-like algorithms.
In LFU-like algorithms, the weight of blocks is majorly evaluated by the access frequency.
Thus, to integrate VDF into these LFU-like algorithms, it needs only to keep the original evaluation approach.
Furthermore, different from the access sequence number, the access frequency of two blocks might be the same.
Therefore, in VDF based LFU-like algorithms, the access sequence number is also employed for choosing which block to evict with the same access frequency.
In VDF cache the access probability of a block would not be the absolute but the relative value, because both the reciprocal of interval of access sequence number and the access frequency are actually the relative values.The conversion from the original cache algorithms to the VDF-based algorithms should be smooth, because Algorithm 1: VDF-LRU for RAID-5 with n disks Input: The request stream x1, x2, x3, ..., xi, ... VDF LRU Replace(xi){ /*For every i â‰¥ 1 and any xi, one and only one of the following cases must occur.
*/ if xi is in LS k ,0 â‰¤ k < n then /*A cache hit has occurred.
*/ Update T S of xi, by T S = GT S; Move xi to the heads of LS k and GS.
else /*A cache miss has occurred.
*/ if Cache is full then foreach block at the bottom of LSj , 0 â‰¤ j < n do if LSj is a corresponding stack to a faulty disk then Its weight W =GT S âˆ’ T S; else Its weight W =(GT S âˆ’ T S) * (n âˆ’ 1);Delete the block with maximum W to obtain a free block; else /*Cache is not full.
*/ Get a free block.
Load xi to the free block.
Update T S of xi, by T S = GT S; Add xi to the heads of GS and the corresponding LS.
Update GT S, by GT S = GT S + 1; } VDF takes effect in faulty mode.
In other words, the buffer cache should be managed with the original algorithms in fault-free mode, and the VDF policy becomes effective when disk failures occur.
Thus, a smooth runtime conversion between the original algorithm and the VDF-based algorithm is needed, which is quite different from the general cache algorithms.
Therefore, in VDF-based algorithm, we employ two types of stacks to achieve the smooth runtime conversion: one is the global stack (GS) which is similar to the stack in a general algorithm such as global LRU stack, and the other is the local stack (LS) holding the blocks on the same disk in cache.
All blocks should be in two types of stacks concurrently as shown in Figure 2.
When the system works in fault-free mode, it evicts the block with the smallest weight at the bottom of the GS stack.
Once a disk array drops to a faulty mode, it evicts the block with the smallest weight at the bottom of each LS stack instead of evicting the block at the bottom of the GS stack.
Detailed descriptions of VDF-LRU and VDF-LFU for ndisk RAID-5 are given in Algorithm 1 and Algorithm 2, respectively, using the variables summarized in Table 2.
To evaluate the effectiveness of VDF, we conducted simulations under three typical workloads: SPC-1-web, LM-TBE, and DTRS.
Delete the block with minimum W and GT S âˆ’ T S to obtain a free block; else /*Cache is not full.
*/ Get a free block.
Load xi to the free block.
Initialize the frequency F and T S of xi, by F = 1 and T S = GT S; Move xi to right place of LS k and GS according to F and T S. Update GT S, by GT S = GT S + 1; } SPC-1-web, a trace used in the SPC-1 benchmark suites, was collected in a search engine, which is widely used in the evaluation of storage systems [1,11,14].
LM-TBE and DTRS are provided by Microsoft Corporation collected in 2008.
The LM-TBE trace was collected in back-end servers supporting a front-end Live Maps application.
The DTRS trace was collected in a file server accessed by more than 3000 users to download various daily builds of Microsoft Visual Studio.
Both traces were taken in a period of 24 hours and broken into pieces with 1-hour intervals [29].
We choose only the piece with most intensive I/O activities.
For fairness and simplicity, we consider only the read operations and all block sizes are 4KB.
We report RGR of LRU, LFU, VDF-LFU, and VDF-LRU under these workloads as shown in Figures 3, 4, and 5, respectively.Our simulator, named VDF-Sim, is written in C and the source code is approximately 3000 lines.
It slices/splits the trace records into block requests as the input.
Data blocks in a stack or blocks with the same hash values are linked via double circular lists.
For a certain block in our simulator, we record its logical offset as the unique ID since the disk array is transparent to the upper level systems such as a file system.
According to the data/parity distribution of the disk array and the logical offset of a block, it is easy to identify on which disk the block resides.
The arriving timestamps of the requests are also recorded to evaluate p i as we mentioned in Section 4 and to generate the misses trace used in the prototype discussed in the next section.The results show that, compared to the original LRU and LFU algorithms, VDF optimized algorithms achieved better performance consistently by reducing the RGR.
Compared to LRU, VDF-LRU reduces the RGR by up to 31.4%, 36.2%, and 22.7% under SPC-1-web, LM-TBE, and DTRS traces, respectively.
Compared to LFU, VDF-LFU reduces the RGR by up to 42.3%, 39.4%, and 24.4%, respectively.We find that the efficiency of VDF grows with the increased number of disks under the same number of cache-resident blocks in most cases.
The efficiency of VDF is more significant with a moderate number of cache-resident blocks than that with a too small or too large number of cache-resident blocks.
This can be explained as follows.
The cache-resident blocks of a faulty disk in the original algorithm would occupy 1/n cache space with total n disks.
With the fixed cache-resident blocks and the increased n, the number of cache-resident blocks of the faulty disk would be smaller.
From the aspect of cache management, the impact of the marginal utility of blocks on hit ratio tends to decrease with the increased cache size.
For example, adding P 1 blocks to a cache with P 2 blocks might improve the hit ratio with a larger gain compared to adding P 1 blocks to cache with P 3 blocks when P 2 < P 3.
Thus, the marginal utility of blocks would be more obvious with more disks and thus the efficiency of VDF grows accordingly.
However, if the number of cache-resident blocks is too small, it is hard to find hot blocks even with an extended period due to the large access interval.
On the other hand, if the number of cache-resident blocks is too large, most of the requested blocks from the faulty disks would be cached, and the marginal utility of blocks becomes insufficient.We also find that the VDF strategy becomes more efficient with LFU than LRU under the three workloads.
One possible reason is that the temporal locality of these traces is weak as they are server-end traces and already filtered by upper level caches.
Thus, the Stack Depth Distribution property of these traces is weak.
As a result, the Independent Reference Model property of these traces would be relatively improved.
Although we improve the weight of the blocks on faulty disk to n âˆ’ 1 times in both VDF-LRU and VDF-LFU, the efficiency is not the same.
Mostly, the caching duration of blocks from a victim disk in VDF-LFU is longer than that in VDF-LRU, especially when the total number of cacheresident blocks is not large.
Therefore, VDF-LFU works better than VDF-LRU in these traces in most cases.
To further evaluate VDF, we implemented a prototype of VDF in a software RAID system in Linux known as MD.
In this section, we present our measurement results, including the efficiency of online recovery duration in full-bandwidth reconstruction mode and system service rate under the degraded mode without reconstruction.
Measurements on real world systems are welcome in research of computer systems.
However, implementation in a real system is a lengthy process and always complex and challenging.
Here, we use a straightforward and accurate measurement approach to evaluate the efficiency of VDF.
The architecture of our prototype is shown in Figure 6.
First, we collect the cache miss information during our simulation in Section 5, which includes not only the block ID but also the real access timestamps.
Then, we treat the RAID as a file device, and use an application in user mode to play the traces we have collected from our simulations which is similar to RAIDmeter [1,11].
However, the difference is that our application uses direct I/O (available in Linux 2.6 and up) instead of buffered I/O to avoid the requests being recached by the file system buffer cache.
All missed I/O requests sent to the MD layer directly.
Thus our simulation and the application join together to exploit the buffer cache and replacement algorithm.
The trace player is also written in C and the source code is approximately 500 lines.In our experiment, we evaluated the effectiveness of VDF, including the online reconstruction duration in full- bandwidth reconstruction mode, and the system serviceability in the degraded mode without reconstruction.
For online reconstruction in full-bandwidth reconstruction mode, an open-loop measurement approach is adopted, where all filtered traces are played according to their timestamps as recorded in the original trace file.
For degraded mode, a closed-loop measurement approach is adopted, where all filtered traces are played only according to their original sequence one by one and without any interval, to find the system serviceability.
In our experiment, we employ a SuperMicro storage server with two Intel(R) Xeon(R) X5560 @2.67GHz (six cores) CPUs, 12GB DDR3 main memory.
All disks are Western Digital WD10EALS Caviar Blue SATA2, which are connected by an Adaptec 31605 SAS/SATA RAID controller with a 256MB dedicated cache.
We disabled the RAID function of the controller and only used the direct I/O mode to connect each disk.
The operating system is Linux Fedora 12 x86 64 with the kernel version of 2.6.32.
In Linux, there is a software implementation of RAID called Multiple Devices MD, which is popular in verification of RAID optimization scheme [1,11].
To facilitate the analysis and verification of VDF cache, we also used MD as our experimental platform.
We used the default settings of MD: the chunk size is 64KB, the number of stripe-heads is 256 and the data layout is left-symmetric.
In our open-loop testing, the minimum Blocks LRU (s) VDF-LRU (s) Improvement LFU (s) VDF-LFU (s) Improvement reconstruction bandwidth is set to 100MBps to utilize all remaining bandwidth for reconstruction besides the bandwidth taken by user workloads.As VDF targets the storage server consisting of disk arrays which run under faulty conditions, we chose the server-end trace SPC-1-web as our experimental material, which spans a 60GB dataset.
The workload is filtered by 131,072 to 524,288 blocks in our simulation to generate the experimental inputs.
In the open-loop measurement, we test VDF with 5 to 8 disks.
The results are reported in terms of reconstruction speed.
The improvements of VDF over the original LRU and LFU are calculated.
In the close-loop measurement, we used a multi-threaded application to play the filtered trace to measure the service rate.
We also evaluated the impact of thread number to service rate, in addition to the impact of the number of blocks and the number of disks.
The results are reported as system service rate improvement by VDF cache compared to original LFU and LRU.
We ran each test three times and report the average.
The results are very stable and consistent as the difference among all three rounds was very small (less than 5%).
is larger than BW U .
Therefore, the change rate of improvement according with the number of total disks should only be determined by the changing rates of BW U Ã— RGR ORI and BW .
Obviously, BW linearly grows with the total number of disks.
From the simulation result, we can find that the changing rate of BW U Ã— RGR ORI is slower than that of BW with the number of disks from 5 to 8.
Therefore, in most of the above cases, the improvement of reconstruction durations of VDF cache to original cache decreases with the increased number of disks.
RD Imprv = BW U * RGR ORI RGR V DF âˆ’ BW U BW RGR V DF âˆ’ BW U (4)With the same number of disks, we notice that the reconstruction duration of the trace filtered by 131,072 blocks is less than the reconstruction duration of trace filtered by 262,144 blocks in many cases.
On one hand, as we use a number of blocks to warm up the cache, this part of the miss information is not recorded in our filtered trace file.
The first 131,072 block misses in the trace filtered by 262,144 blocks has a lower average arrival rate than the remaining part.
On the other hand, when the number of blocks in the cache is 131,072 or 262,144, the cache is too small to find the hot blocks, which implies fewer hits in those cases and the RGRs are similar.
Therefore, the reconstruction duration of the trace filtered by 131,072 blocks might be less than that of the trace filtered by 262,144 blocks in those cases.
From the experimental results, we find that VDF is effective in improving the system service rate.
Compared to LFU, VDF-LFU improves the system service rate up to 46.8% with 60 threads under 8 disks and 262,144 blocks.
Compared to LRU, VDF-LRU improves the system service rate by up to 28.4% with 80 threads under 8 disks and 524,288 blocks.
With the increasing number of I/O threads, the service rate improvement increases accordingly and gets close to the theoretical value calculated by the simulation results.
Although the whole trace would be evenly distributed on all disks due to the round-robin addressing in RAID, the incoming user requests might not be evenly distributed on all the disks during a short period.
Therefore, with a larger number of threads, which implies a longer scheduling window, the distribution of incoming user requests is more balanced and the user service rate is closer to the maximum system service rate.With the same number of blocks and a fixed number of threads, the service rate improvement of VDF-LRU to LRU is consistent with the trend of the simulation result.
However, to our surprise, the results of VDF-LFU to LFU were just opposite with the trend of the simulation result.
As per our analysis, this was primarily due to two reasons.
First, from the simulation result, the relative RGR reduction of VDF-LFU to LFU with 524,288 blocks is in a small area from 33.7% to 35.6%.
Second, the number of concurrent threads is fixed, which means that the number of threads per disk would increase with the decreased total number of disks.
Thus, based on the above analysis, when the total number of disks is small, the improvement is closer to the theoretical value.
Therefore, under close to theoretical peak service rates and more I/O threads per disk, the trend of service rate improvement of VDF-LFU to LFU is very possibly opposite with the trend of the simulation result.
As a result, with the same number of disks and a fixed number of threads, which means a fixed number of I/O threads per disk, the service rate improvement is quite consistent with the trend of the simulation results.
Several more issues deserve further discussion.
The first issue is the implementation cost of VDF.
As we mentioned in Section 4, to make the smooth conversion between the original cache algorithms and the VDF-based algorithms, two types of stacks should be employed to implement VDF cache.
This adds both spatial and temporal overhead.
The spatial overhead include the extra information in each block head such as the timestamp and the extra stack pointer of the local stack.
Compared to the buffer cache size, this overhead is very small.
The temporal overhead is the computation of the weight of block at the bottom of each LS stack.
Due to the high computation ability of today's CPU, this should not influence the overall system performance.Second, can we integrate VDF into other optimizations in faulty mode?
As the VDF cache essentially reduces the user requests to the surviving disks, it can be integrated with other optimizations in faulty mode, such as optimization on data/parity/spare layout and reconstruction workloads.
The approach of redirection of reads utilizes the reconstructed data in a spare disk to serve part of the reads to the faulty disk.
Thus the miss penalty of these reconstructed data block is zero in terms of extra requests to the surviving disks.
There still exist hot data with large miss penalty on faulty disks.
Therefore, VDF can still help.Third, could RGR be suitable to describe the status of write operation?
From its definition, RGR is determined by M P i and p i .
The calculation of p i in write operations is similar to read operations.
However, the calculation of M P i in write operations is quite different from read operations, as they might be done with two approaches based on the parity distribution in RAID with faulty disk(s).
One is the Read-Modify-Write, and the other is Parity-Reconstruction-Write.
Here, we take an example of short writes on an n-disk RAID-5 with one faulty disk to demonstrate the M P i calculation for write operations.
Once a short write is sent to the surviving disks and the corresponding parity is not on the faulty disk, the Read-Modify-Write should be performed which results in two reads and two writes on the surviving disks, and thus the M P i is 4.
Otherwise, the ParityReconstruction-Write should be performed which results in n âˆ’ 1 reads and one write on the surviving disks, and thus the M P i is n.
In this paper, we present an asymmetric buffer cache replacement strategy, named Victim (or faulty) Disk(s) First (VDF) cache, to improve the reliability and performance of a RAID-based storage system, particularly under faulty conditions.
The basic idea of VDF is to treat the faulty disks more favorably, or give a higher priority to cache the data associated with the faulty disks.
The benefit of this scheme is to reduce number of the cache miss directed to the faulty disk, and thus to reduce the I/O requests to the surviving disks overall.
Less disk I/O activity caused by the user workflow will (1) improve the performance of the disk array, and (2) allow more bandwidth for online reconstruction which in turn speeds up the recovery, and thus improves the reliability.
Our results based on both simulation and prototyping implementation has demonstrated the effectiveness of VDF in terms of reduced disk I/O activities and a faster recovery.To further understand VDF, we have the following plans as our future work.
First, we plan to build VDF into more general cache algorithms such as CLOCK [30] and ARC [26].
Second, we are working to implement VDF in the kernel level and thus to directly run real benchmarks to conduct more extensive measurements.
Third, in addition to RAID-5, we will investigate the scheme to apply VDF to other RAID levels such as RAID-4 and RAID-6.
We are very grateful to our shepherd Erez Zadok and anonymous reviewers for their helpful comments.
This research is sponsored in part by the National Ba-
