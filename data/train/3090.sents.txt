We demonstrate an enterprise Dynamic Thresholding System for data-agnostic management of monitoring flows.
The dynamic thresholding based on data historical behavior enables adaptive and more accurate control of business environments compared to static thresholding.
We manifest the main blocks of a complex analytical engine that is implemented in VMware vCenter Operations Manager as a principal foundation of the company's data-driven anomaly detection.
Modern enterprise IT management becomes increasingly smart to proactively respond to the performance issues that complex infrastructures necessarily encounter.
The management based on expert knowledge utilization is no longer efficient.
Monitoring and data measuring of the processes governing the entire IT is a fundamental approach to gain insights from those sophisticated environments with complicated interrelation between the constituent components.
The history of this approach goes back to statistical process control [1].
Since contemporary business infrastructures are highly dynamic (and out of classical Gaussian normalcy domain [2]), static thresholding of processes and performance indicators become inadequate.
Hence problem diagnostics dictates a soft control of IT environments ( [3][4][5][6][7][8][9][10][11][12][13][14]).
In this paper, we follow a path where anomaly detection is based on prediction of upper and lower dynamic thresholds (normalcy range) of categorized data that vary over time [15][16][17][18][19].
VMware's vCenter Operations Manager (vC Ops) [20] is an industry-leading enterprise solution in area of IT management that encodes the bunker of monitoring data from customer IT system into a real knowledge for anomaly detection and problem root cause identification, as well as capacity planning for modern virtualized and cloud computing ecosystems.
In this paper we introduce an enterprise dataagnostic dynamic thresholding system (EDTS) that enables vC Ops to act as an atomistic anomaly detection and forecasting of monitoring flows.
The data-agnosticism (indicates that data analysis and determination of its normalcy behavior is performed without knowing the essence of the underlying physical and business service processes) enables an universal platform for processing of very large data sets, at the same time, it can lead to a deadlock if the statistical methods are not sufficiently powerful to handle diversity of monitored data types.
Our analysis of customer data over several years show that deficiency of data-agnosticism can be compensated by appropriate data categorization, since a specific data category statistically characterizes the underlying process and empowers an efficient construction of relevant normalcy bounds (dominant behavior) thus reliably controlling the flow.
This concept leads to an EDTS based on data categorization realized in vC Ops.
A simplified and specific realization of EDTS adjusted for IT environments is presented in Flowchart 1.
Although selection of the categories is adapted to some IT customer preferences, the overall approach is applicable widely (also out of the IT interests) with appropriate modification of categories and their definition parameters.Experimental results justify EDTS's potential to effectively handle large infrastructures in terms of both accuracy and complexity.
All ideas described in the sequel are filed as a patent [21].
EDTS sequentially utilizes different data categorization detectors that allow choosing the right algorithm for determination of data dynamic thresholds (DT's).
The categorization order or the hierarchy is important as different orders of iterative checking and identification will lead to different categories with differently specified normalcy states.
The system presented in Flowchart 1 categorizes data as Multinomial, Transient, Semi-constant, Trendy, Sparse, HighVariability or Low-Variability.
In each of those cases the normalcy determination method is different.
In all categorization scenarios the data additionally is verified against periodicity for efficient construction of its normalcy bounds.Moreover, in each of the majority of categorization scenarios data is processed by category-specific change detection procedures.
The functional meanings of the above mentioned detectors are as follows:Multinomial Data Detector searches for Multinomial data which takes only integer values after checking errors introduced by the monitoring apparatus.
If data is identified as multinomial then DT determination module calculates specific for this category DT's otherwise data transmits to the next detector.Transient Data Detector looks for Transient Data which can be characterized as multimodal data.
If data is identified as transient then DT calculation module performs DT calculation separately for each mode.Semi-Constant Data Detector checks the data against its "almost constant" behavior.
If data is not semi-constant but its latest portion satisfies the category specifications after a global change, then DT construction module performs DT calculation for the latter.
Piecewise constant data is from this category.Trendy Data Detector performs trend identification.
If data is trendy then detector classifies trend as linear or nonlinear and DT determination module executes a special DT calculation algorithm.
If data is not trendy but its latest portion can be selected as trendy (change occurred), then DT determination module performs calculations for that portion.Sparse Data Category Detector explores data gaps, their amount, and distribution in time.
Data goes to the next detector for further analysis if overall gap duration is negligible.
Data is classified as Sparse if gaps have uniform distribution in time.
If gaps have some accumulation and remaining data is acceptable for further analysis then the selected portion goes to the next detector.Variability Detector categorizes data either HighVariability or Low-Variability with specific DT calculation procedures.
Before final categorization data passes through a change detection procedure for selection of the latest statistically stable portion for final DT determination.
Multinomial Data Detector.
This detector calculates some statistical parameters for comparison with the predefined measures.
If the check is positive then data is classified as Multinomial Data.
It is assumed that Multinomial data takes only integer values.
Let be the frequency of occurrences of the integer where is the total number of integer values and is the number of different integer values.
Data is multinomial if it takes less than different integer values and at least of them have frequencies greater than parameter .
Some integer values with small cumulative percentages can be discarded.
This can be done by sorting the percentages in descending order and by defining the cumulative sum .
Then, if , the integer values can be discarded.
Transient Data Detector.
Transient Data is categorized by multimodality, modal inertia, and randomness of modes appearing along the time axis.
Transient Data must have at least two modes.
Modal inertia means that data points in each mode must have some inertia and they can't oscillate from one mode to the other "quickly".
Actually the inertia can be associated with the time duration that data points remain in the selected mode.
Categorization is performed by calculation of some transition probabilities.
We omit the relevant details from [21].
A similar technique is applied in Sparse Data Detector (see below).
Figure 1 shows an example of a Transient Data.
we define the probabilities of datato-data, data-to-gap, gap-to-gap and gap-to-data transitions, respectively , ,, Data with gaps non-uniformly distributed in time can be specified by the condition { where the following values of parameters can be reasonably chosen and .
The main reason for smallness of and is the smallness of the numbers and while and are as big as is assumed.
Data from this category can be further processed via data selection procedure that will eliminate (if possible) concentration of gaps.
This can be done as follows: calculate the total percentage of gaps in the series of data { } and select the portion for which .
The selected data is ready for further analysis by sequential detectors.Data with gaps uniformly distributed in time (Sparse Data) can be specified by the condition ( ) {The second and third conditions mean that gaps are uniform in time and technical cleanup is impossible.Data is useless for further analysis if .
Figure 3 shows an example of Sparse Data with the corresponding measures for categorization.
As we mentioned above, each data category preliminary passes through some period determination procedure which additionally categorizes data into Periodic and Non-Periodic.
Details of this procedure are presented in [21].
We describe only a high level concept.
The period determination is seeking similar patterns in the historical behavior of time series for setting the DT's based on the discovered cyclical information.
The algorithm consists of two main steps: 1) Data Footprint calculation which provides with twodimensional distribution of time series based on some predefined frame.
First, we calculate percentages of data in each cell of the frame and then we get the corresponding distribution by taking cumulative sums of those percentages for each column of the frame.
More specifically, the range of data (or the range of preliminary smoothed data) is divided into non-uniform parts by quantiles with , , with some and .
Evidently, the grid lines are dense where data is dense.
For division of data into parts along the time axis two parameters and are used.is a basic parameter that defines the minimal length of possible cycle that can be found.
Moreover, any cycle can be a factor only of the length of the .
Usual setting is Parameter shows the number of subintervals (columns in the frame) that must be divided.
Actually this parameter is the measure of resolution.
The bigger the value of , the more sensitive is the footprint of the historical data.2) Pattern recognition procedure which provides with Cyclochart of data by comparing different columns of the Footprint in terms of similarity.
Figure 5 shows an example of a Cyclochart where y-axis shows the measure of confidence that data has some -days cycle.
Now we describe several category-specific DT construction mechanisms: DT's of Multinomial Data.
As mentioned, period determination investigates the cyclicality of data and classifies it into periodic and non-periodic categories.
The general scheme for period determination in this case is specialized with the following modification while constructing the Footprint of data: instead of the percentages of data in every cell we are taking the values of (see categorization of the Multinomial Data) in every column of the frame.
If data is claimed Periodic then the normalcy set for similar columns are calculated as follows.
Data points in similar columns are collected together and corresponding new values of the numbers are calculated.
If , then the values constitute the most probable set (normalcy set) of similar columns.
If Multinomial Data is determined as Non-Periodic then the numbers are calculated for all data points and normalcy set is determined similarly.
DT's of Semi-Constant Data.
For Semi-constant data every data point greater than (quantile) or less than is an outlier.
If the percentage of outliers is greater than ( ), then we check for periodicity in outlier data by the procedure described above.
For periodicity analysis data points equal to the median are excluded from the analysis.
DT calculation for Non-Periodic Semi-Constant Data is performed separately for upper (for data points that are greater or equal to median) and lower (for data points that are less than or equal to median) parts of data.
Since the process of obtaining of both upper and lower bounds are similar, we'll explain the method only for the upper DT.
The main principle is maximization of an objective function where is a sensitivity parameter, for example ; is the percentage of data points within the median of data and any upper line higher than the median (see Figure 7); and is the square of the area within data points and data median.
(while determining the upper bound) into parts and for each level calculate the values of the objective function.
Then, the level that corresponds to gives the appropriate upper bound.
Instead of dividing the range into equal parts it is reasonable also to divide the range of data by corresponding quantiles that will give unequal division according to the density of data points along the range.
Here preliminary abnormality cleaning of data can be performed.
For this, removal of data points with abnormal concentration in the given time window is performed.
Abnormal concentration can be detected by the following procedure.
For the given time window (for example of data length), we calculate the percentage of data points with values higher than -quantile.
Then by moving this window along data, we calculate the corresponding percentages.
Any percentage higher than the upper whisker indicates abnormal concentration and must be discarded from further calculation.
We repeat the same abnormality cleaning procedure for data points lower than -quantile.
In the data-variability-based approach, we calculate the variability of data points against median of data Figures 6, 9, and 10 are also real enterprise examples.
We selected some 3215 monitoring metrics with one month length and applied the categorization procedures.
Table 1 shows the distribution along different data categories and Table 2 indicates the count of periodic and nonperiodic data.
In particular, for semi-constant data we observe the following picture.
From 532 metrics (see Table 1) 267 have percentage of outliers less than 15% and they are claimed as non-periodic without any further checking.
The remaining 235 metrics are checked for periodicity and in 212 cases periods are found.
It is worth noting that results obtained for the specific customer data can't be in any manner generalized to other cases.
The graphs below demonstrate several snapshots from the product with monitored time series data and their adaptively updated DT's by EDTS.
We observe reliably detected DT's, data changes, periods, and relevant out-of-normal areas (yellow) reported as alarms.
In terms of our application, the performance of EDTS is estimated according to users experience on indicative and missed alarms, as well as the generated noise level that the useful information is embedded in.
In this context, our categorization techniques allow achieving essentially better trade-off between the produced recommendation (alarm) noise and its accuracy in problem indication.
That would not be possible with classical parametric approaches including Fourier transform, discrete Fourier transform [24][25][26][27][28], Prony's method [29,30]) as well as with other common purpose enterprise algorithms (including our algorithm of Section III that produces DT's based on data footprint even when cyclical patterns are not discovered).
Moreover, the categorization in terms of those specific classes enables an efficient root cause analysis [31,32] based on the abnormality events (DT violation alarms) space that our system outputs.
Furthermore, [19] reports about reliably predicted root causes of suddenly occurring influential outages at large enterprise infrastructures.
This method relies on historically analyzed mutual impact factors of out-of-DT events.Note that EDTS handles only structured monitoring data.
For the unstructured data sets (like log files) we have developed a graph-based approach [33,34] that extracts the dominating correlation pattern between the main event types in data as dynamic normalcy structure and applies it to identification of "large"/abnormal deviations from that structure to determine performance anomalies.Finally, we refer the reader to the papers [35,36] which outline the approaches and trends of the area of anomaly detection up to the recent days.
