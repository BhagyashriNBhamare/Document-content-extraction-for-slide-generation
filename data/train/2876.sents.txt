In data-driven parallelism, changes to data spawn new tasks, which may change more data, spawning yet more tasks.
Computation propagates until no further changes occur.
Benefits include increasing opportunities for fine-grained parallelism, avoiding redundant work, and supporting incremental computations on large data sets.
Nonetheless, data-driven parallelism can be problematic.
For example, convergence times of data-driven single-source shortest paths algorithms can vary by two orders of magnitude depending on task execution order.
We propose constrained data-driven parallelism, in which programmers can impose ordering constraints on tasks.
In particular, we propose new abstractions for defining groups of tasks and constraining the execution order of tasks within each group.
We sketch an initial implementation and present experimental results demonstrating that our approach enables new efficient data-driven implementations of a variety of graph algorithms.
Exploiting multicore systems requires parallel computation, preferably with minimal synchronization.
A promising approach is data-driven parallelism, in which computation is broken into tasks, each of which must run when some data is modified.
Such tasks may modify additional data, thereby triggering additional tasks.
Thus, changes to data "drive" the parallel computation.Consider, for example, the single-source shortest paths (SSSP) problem: given a weighted graph with no negative-weight cycles, compute the length of the shortest paths to each node from a single source node.
We can solve SSSP by assigning the source a distance estimate of 0 and every other node a distance estimate of âˆž, and then relaxing the graph's edges until no more relaxation is possible.
An edge from x to y can be relaxed if going to x and then following this edge provides a path shorter than the current distance estimate to y.This computation is well-suited for parallelism because edges may be relaxed in any order, and the relaxation of edges need to synchronize only when the edges share a node.
It is well-suited to being data-driven because relaxation must be performed initially only on edges of the source node, and thereafter, only on edges of nodes whose distance estimates have been updated.Although this computation is correct regardless of the order in which edges are relaxed, this order can have a profound impact on execution time: with an ideal order, each edge is relaxed at most once; a bad order can induce an exponential number of relaxations.
As Figure 1 illustrates, this problem is not merely theoretical.
The figure shows three variants of SSSP running on the collaboration network graph ca-HepPh from SNAP [23].
BF-OMP is a parallel version of Bellman-Ford: computation proceeds in rounds, and every edge is relaxed once per round using an OpenMP parallel for loop.
DD-Wild is data-driven: an edge is relaxed when it is incident on a node whose distance estimate has changed.
This tends to produce a depth-first traversal of the graph, which is a bad order for computing SSSP.
Thus, although it scales well, with one thread, DD-Wild is over 200 times slower than BF-OMP.
DD-Phased-a data-driven implementation that uses the abstractions introduced in Section 2 to constrain task execution order-outperforms BF-OMP at every thread count.
We discuss these results in more detail in Section 4.
We argue that data-driven parallelism benefits significantly from the ability to constrain the order in which tasks run.
Furthermore, we argue that choosing the appropriate constraints is a property of an algorithm, and so should be expressed in the source code (rather than by picking a plug-in scheduler for a whole application).
Section 3 describes our prototype.
Section 4 shows the power of constraining data-driven parallelism via our abstractions in various ways using several benchmarks.
Section 5 discusses related work.
Section 6 concludes.
Computation is divided into tasks executed by threads.
Each task belongs to a task group.
A task may spawn a new task via a parallel for loop or a trigger, which specifies data-driven computation:x triggers [deferred] f(void *d); *y triggers [deferred] g(void *d);We say x is a direct trigger for f and y an indirect trigger for g.
A trigger is a deferred trigger if it is declared with the optional deferred keyword; otherwise, it is an immediate trigger.
When such declarations are in effect, writing to x spawns a task to run f, and modifying data by dereferencing y spawns a task that runs g. (See Section 4.1 for an example.)
The functions f and g take a single argument, which is a pointer to the data that was modified.
This has been sufficient for all use cases we have explored so far, though additional arguments may be useful in the future.A task spawned by an immediate trigger may execute concurrently with the task that spawned it.
In contrast, the execution of a task spawned by a deferred trigger is deferred until all nondeferred tasks in its task group have completed.
This implicitly partitions the tasks in a task group into a totally ordered series of phases: immediate triggers spawn tasks in the current phase, deferred triggers spawn tasks in the next phase, and no task is executed until all tasks in previous phases have completed (see Figure 2).
Thus, at any time, each unfinished task is in either the current phase or the next phase of its task group.A task group, declared as follows, is a set of tasks.taskgroup { // code } t;When this block is executed, a new task group t is created.
Initially, a new task group has a single task, running the code in the declared block (executed by the thread that entered the block).
A newly spawned task belongs to the same task group as the task that spawned it.
In our current model, every task belongs to exactly one task group.
There is a single anonymous task group to which all initial tasks belong.A task group's only method-WaitForGroup()-blocks until every task in that task group has completed.
Note that deadlock cannot arise solely from calls to WaitFor- Group because the name of a task group is not in scope in the task group's code block and we provide no mechanism to pass task groups (i.e., there is no named type for task groups).
To evaluate the constrained data-driven parallelism (CDDP) approach, we have implemented a simple prototype using C++ macros and a runtime library.
We support triggering via explicit calls to runtime library functions (one for immediate triggering, and one for deferred).
These functions take as arguments the object that triggered the call and the handler function to run.
Our runtime system manages tasks using a work-stealing scheduler based on the Chase-Lev deque [4].
We implement task groups with a TaskGroup class, exposing BeginTaskGroup and EndTaskGroup methods.
BeginTaskGroup begins the "scope" of a task group which owns all tasks spawned until the matching call to EndTaskGroup.
EndTaskGroup returns a pointer to the task group object, which can be passed to WaitForGroup.Each task group tracks whether any tasks remain to be executed in the current and next phases using SNZI objects [7,14].
Each thread keeps the deferred tasks that it spawns in a thread-local deferred-task bag.
When no tasks remain in the current phase of a task group, each thread moves any deferred tasks it has from the bag of that task group to its deque.
When no thread has any deferred task for a task group, the task group's execution is compete.
We evaluate our CDDP prototype using SSSP, Communities (a graph clustering algorithm [20]), and BC (betweenness centrality, a social network analysis algorithm [15]).
We compare our data-driven solutions with parallel implementations using OpenMP [17].
CDDP delivers competitive, or significantly better, performance across most benchmarks and inputs.We also ported the discrete-event simulator from the Lonestar benchmark suite [12], in which the computation is structured entirely via data dependencies between tasks.
The performance of our CDDP solution is consistent with results reported by Kulkarni et al. [12] (we omit full details for brevity).
Experiments were run on an Oracle T5140 series machine, comprising two 1.2 GHz Niagara T2 chips with a total of 128 hardware thread contexts (8 cores per chip, 8 hardware threads per core).
Each chip has an 4MB onboard L2 cache, and each core has a 8KB L1 data cache shared between its threads.
We use the Oracle Solaris Studio 12.1 C++ compiler at optimization level xO5.
As discussed in the introduction, relaxation algorithms such as SSSP are well-suited for data-driven parallelization.
To reduce the overhead of spawning tasks, we spawn a single task that relaxes all the edges of a node whose distance estimate is updated, rather than spawning a separate task for each edge.
We also introduce a per-node pendingRelaxation flag that indicates whether the node's distance estimate has been modified since its edges were most recently relaxed, and trigger relaxation when this flag is set to true, rather than when the distance estimate is updated.
This allows multiple successive updates to be handled by a single task that calls the following function: Note the use of an indirect trigger to trigger RelaxNeighbors when the flag changes from false to true at line 11, but not when it is set to false at line 2.
We use CAS at line 11 so that RelaxNeighbors is triggered only when the flag changes (i.e., when the CAS is successful).1 void RelaxNeighbors(Node *n) 2 n->pendingRelaxation = falseWe experimented with three versions of this algorithm.
DD-Wild uses "classical" work-stealing, in which each thread pushes and pops tasks on one end of its deque, and thieves steal from the other end.
With only a single worker thread, this implies LIFO execution of tasks.
DD-Phased uses the same algorithm and runtime, but with deferred triggering (adding the deferred keyword at line 6).
Finally, DD-Fifo uses immediate triggering with a modified version of the runtime system in which threads access their deques in FIFO order with self-stealing (pushing on one end, but with workers and thieves popping from the other end).
Figure 3 compares these data-driven algorithms with three alternative algorithms: BF-OMP is an OpenMPbased parallel version of the Bellman-Ford algorithm that avoids write-write conflicts by having a task relax all incoming edges of a node.
Dijkstra is the classic sequential SSSP algorithm, which relaxes edges in an ideal order (i.e., every edge is checked exactly once).
Seq-DD-Fifo is a sequential version of DD-Fifo that uses a sequential queue to store the nodes to be processed instead of our abstractions (we used it to estimate the overhead imposed by our runtime system).
The figure uses the ca-HepPh collaboration network from SNAP [23] (12,008 nodes and 237,042 edges).
Graphs from a variety of benchmark suites show qualitatively similar results.
Results are shown in a log-log scale.As these results illustrate, the order in which tasks are executed has a huge effect: For single-thread runs, DD-Wild, which runs tasks in LIFO order (i.e., traverses the graph depth-first), is 250x slower than BF-OMP, whereas DD-Phased and DD-Fifo, which approximate a breadth-first traversal, are about 2x faster than BF-OMP.
Dijkstra, which relaxes nodes in an optimal order and avoids the overhead of spawning and synchronizing tasks, is about 6x faster than BF-OMP with a single thread.
Comparing Seq-DD-Fifo and DD-Fifo shows that the overhead of our system in this benchmark is about 50%.
The data-driven algorithms all scale well up to 32 threads, with DD-Phased and DD-Fifo slightly improving their lead over Bellman-Ford (and outperforming Dijkstra with 4 or more threads), and DD-Wild rapidly catching up.
Although DD-Fifo is 27% slower than DDPhased with one thread, DD-Fifo outperforms it by 42% at 127 threads.
Perhaps surprisingly, DD-Wild exhibits superlinear speedup, and with 72 or more threads, it outperforms all the other algorithms.We can understand these results better by looking at Figure 3 (bottom), which shows the total amount of work done by each algorithm, measured by the number of nodes processed.
With a single thread, DD-Wild does 500x more work than DD-Fifo, and 125x more than BF-OMP, its closest competitor, because of its LIFO execution of tasks.
With more threads, the LIFO execution is increasingly disrupted by work-stealing, resulting in a better order for computing SSSP, and a corresponding reduction in total work.
For all other parallel algorithms, the work increases slightly as the thread count increases, with DD-Phased doing the least work at all thread counts, explaining its relatively good performance at low thread counts.
However, at high thread counts, the waiting due to phases limits its scalability, and DD-Fifo and DD-Wild outperform it.
Although DD-Wild does more work than DD-Fifo, we believe it outperforms DD-Fifo at high thread counts because it avoids the synchronization overheads that DD-Fifo incurs due to selfstealing.
Our next example is a clustering algorithm that partitions a graph into communities, each comprising nodes that are relatively strongly connected to each other and relatively weakly connected to nodes in other communities.
We build on the sequential round-based algorithm of Raghavan et al. [20] (Serial).
In each round, this algorithm iterates over all nodes in a random order, and assigns each node to the community with the most of its neighbors (ties are broken arbitrarily).
The algorithm terminates after a round in which no node changes community.OMP is a parallel version of this algorithm that uses an OpenMP parallel for loop to iterate over the nodes at each round.
Both Serial and OMP can perform a lot of unnecessary work: a node changes its community in a round only if at least one of its neighbors changed its community recently (in the last or the current round), but these algorithms recompute each node's community every round even if none of its neighbors has since changed its community.
Tseng and Tullsen made similar observations for other applications in their work on data-triggered threads [25,26].
Based on this observation, we built four data-driven variants of this algorithm: DD-Wild, DD-Fifo, DDPhased, and DD-Mixed.
In all of them, updating a node's community triggers tasks for its neighbors.
To begin the computation, we use a parallel for loop to assign a unique community to each node, triggering computation across the graph.
The first three variants are similar to the corresponding SSSP variants.
In DD-Mixed, a task to process a node is triggered in the current phase if that node has not already been processed in the current phase; otherwise, the task is deferred to the next phase.
This more closely imitates Serial, with DD-Mixed's phases corresponding to rounds in Serial.We ran all the algorithms on various SNAP [23] graphs, using the modularity metric [16] to confirm that the "quality" of the results achieved by the parallel algorithms are similar to or better than those of Serial.
The top chart of Figure 4 shows, on a log-log plot, the execution times for the amazon0505 graph (10,236 nodes, 3,356,824 edges, converted to an undirected graph by adding the reverse edges, as the above algorithm requires an undirected graph).
The data-driven algorithms outperform Serial and OMP algorithms in every case.
As with SSSP, constraining task execution order substantially improves performance.
In particular, DD-Wild performs considerably worse than all other data-driven variants.
This is explained by the bottom chart of Figure 4, that shows that, at all thread counts, DD-Wild performed significantly more work than the other data-driven variants (about 40% versus 15%, normalized to Serial).
Singlethreaded OMP does the same amount of work as Serial, and at higher thread counts, it usually even does 14-50% more work than Serial.
This is why it performs worse than DD-Wild.
Because DD-Mixed was designed to imitate the behavior of Serial, we also compared the number of rounds for Serial to converge and the number of phases taken by DD-Mixed.
Indeed, the two numbers were very similar for most graphs.
However, for one graph, web-BerkStan, the number of serial rounds was an order of magnitude higher than the number of data-driven phases, regardless of the random order in which Serial processed the nodes, resulting in a serial execution time that is 40x longer than that of DD-Mixed.
This suggests that for some graphs, the data-driven order of processing can lead to faster convergence than that achieved with most random orders.
BC is a measure of the importance of each node in a graph in terms of the number of shortest paths going through that node.
Hong et al. wrote a Green-Marl program (bc-GM) that approximates BC [10], based on an algorithm by Madduri et al. [15].
Briefly, bc-GM does a BFS traversal rooted at a randomly selected node, recording the number of shortest paths from the root to each node, and then does a reverse-BFS (rBFS) traversal, recording at each node the number of shortest paths from the root to other nodes that go through it.
This process is repeated several times.We developed a data-driven variant (bc-DD) in which the BFS traversals are done using task group phases, and the rBFS traversals are done in classic data-driven fashion.
The BFS and rBFS tasks in bc-DD can be overlapped to some degree, improving overall parallelism.
Note that, in contrast to SSSP, executing the tasks in phases is required for correctness, to ensure BFS traversal.The Green-Marl compiler does source-to-source transformation, converting a Green-Marl program into an equivalent OpenMP program.
The resulting bc-GM Open-MP program dynamically constructs sets of nodes visited at each level in the BFS traversal, and within each BFS level, employs an OpenMP parallel for loop to visit each graph node at that level.
We believe that our experimental comparison between bc-GM and bc-DD reflects tradeoffs between the OpenMP and CDDP implementations, rather than Green-Marl and CDDP abstractions.
Furthermore, because OpenMP provides numerous options for how loop iterations are scheduled, we experimented with the static scheduling policy, the dynamic scheduling policy with multiple for loop chunking factors.We conducted experiments with several input graphs and report results in Figure 5 for two representative graphs: a social network graph soc-LiveJournal1 and a web graph web-Google.
We show the performance of the static scheduling policy (bc-GM), and the dynamic scheduling policy (bc-GM-dy-*) with different loop iteration chunking factors (128, 1K, and 4K).
The chunking factors help us understand the potential impact of varying task granularity.
bc-DD does not employ any chunking.
We note that, unlike other benchmarks reported here, CDDP's data driven execution model does not reduce the amount of real work done in BC.Because bc-DD tasks are far more fine grained (one task per graph node) than bc-GM work items, bc-DD incurs significant overheads for single-thread runs (up to 50%).
However, this overhead diminishes rather quickly with increasing thread count, presumably because of the better load balancing achieved by our work-stealing scheduler.The scalability results are mixed: For soc-LiveJournal1 (Figure 5a), bc-DD outperforms significantly than bc-GM, but the bc-GM-dy-* versions with coarse chunking slightly outperform bc-DD.
Both bc-DD and the coarsegrained bc-GM-dy-* versions outperform bc-GM, presumably because they offer better load balancing.
The finer-grained bc-GM-dy-128 appears to suffer with the consequences of too fine a granularity, which leads to worse communication to computation ratios on our experimental platform.For web-Google (Figure 5b), bc-GM appears to perform best.
We believe this to be due to the graph structure.
In particular, because the average degree of graph nodes in web-Google is much smaller than soc-LiveJournal1 graph nodes, the amount of work done in each bc-DD task for webGoogle is correspondingly much smaller.
This significantly increases communication to computation ratio for the dynamic scheduling schemes on webGoogle.
Because OpenMP static scheduling comes with essentially no communication between worker threads (except for communication happening via application data), it appears to outperform all the dynamic scheduling alternatives (including bc-DD), all of which encounter communication overheads Figure 5: Betweenness Centrality performance results for two graphs from the SNAP data set [23] converted to undirected graphs by adding reverse edges.while enforcing dynamic scheduling.
bc-DD appears to be competitive with the best performing dynamic bc-GM version.We believe that the various bc-GM versions we experimented with benefit significantly from chunking of loop iterations.
However, we do not have support for such chunking in our CDDP scheduler.
Implementing a strategy of dynamically consolidating several CDDP tasks into a bigger task can potentially significantly boost performance of some of our benchmarks.
We plan to investigate this direction in future work.
Data-flow systems.
Data-driven parallelism stems from early work on data-flow computation [2,6], in which data dependencies between instructions drive execution.
We work at a coarser grain-typically, tasks access locations and perform local computation.
The StarSs programming model [19] provides a form of data-flow computation using coarse-grained tasks.
Synchronization is provided via barriers which wait until all spawned tasks are complete; CDDP phases avoid the need to wait for all tasks to complete before new tasks can be spawned.
Gajinov et al. [8] and Seaton et al. [22] explored combinations of data-flow programming models and atomic tasks.
These systems do not provide CDDP's control over the scheduling order of parallel tasks.
Our current system does not require tasks to be atomic (although we discuss future work in this direction in Section 6).
The Habanero project [24] includes data-driven tasks.
In that model, tasks are spawned explicitly.
Futures allow synchronization on tasks as they complete.
In CDDP, our focus is constraining the order of tasks as they run, rather than waiting for specific tasks to finish.Incremental and self-adjusting computation.
Recent work on incremental [5,18] and self-adjusting computation [1,9] has explored the ability to avoid repeating unnecessary work.
Tseng and Tullsen demonstrate substantial speed-ups using data-triggered threads [25,26].
CDDP benefits from the same ability to avoid repeated work, but provides additional control over how parallel work is scheduled.Domain-specific scheduling.
Researchers have often observed that different workloads perform best under different schedulers [13,21].
In contrast, CDDP's abstractions allow the programmer to constrain task ordering without requiring a full scheduler to be written, or requiring a single policy to be applied to a complete process.BSP.
The idea of phases in task groups builds on ideas in the classic BSP programming model [27], but BSP targets different kinds of computations than CDDP.
In this paper, we have described constrained data-driven parallelism.
In CDDP, parallel task execution is driven by changes to data, and the programmer can impose additional constraints on the order in which tasks run.Our exploration to date has focused on a simple set of abstractions for expressing parallelism and for imposing constraints on execution order.
Our initial results demonstrate that these abstractions can provide significant performance improvements.
Compared with "unconstrained" approaches, CDDP enables the programmer to avoid pathologically bad scheduling orders.
Compared with non-data-driven algorithms, CDDP avoids repeating computation when data does not change.We are encouraged by these preliminary results, and hope to extend our work in several directions.
We wish to study the use of CDDP for additional algorithms and workloads.
Our initial exploration has focused on graph algorithms on different kinds of input (e.g., planar graphs versus small-world graphs.
We would like to study a broader range of algorithms, and to characterize where CDDP is an appropriate solution (e.g., for which of the "dwarfs" of Asanovic et al. [3] it is an appropriate fit).
This study will help us identify whether our current abstractions of task groups and phases are sufficient for a wide range of algorithms, or whether additional or alternative abstractions are needed.We have considered whether CDDP should include a notion of atomic tasks.
These tasks would execute atomically and in isolation from one another.
In addition to CDDP's existing constraints, tasks would have to execute after the atomic task that triggered them.
This builds on aspects of atomic dataflow models [8,22], and of the automatic mutual exclusion system [11].
Introducing atomicity simplifies programming by preventing tasks from observing each others' intermediate state.
In addition, it is possible that implementations may be able to use a singlemechanism for detecting conflicts between tasks and for detecting when a new task should be spawned.As we gain experience with CDDP, we want to refine the syntax for expressing constraints, and to investigate the trade-offs in providing language support.
Introducing atomicity may push us toward compiler support.
We have not yet tried to define a formal semantics for CDDP.Finally, our initial implementation supports execution only within a single shared memory system.
With increasing interest in large data sets, we plan to extend our implementation so that it supports similar programming abstractions but is not limited to use a single shared memory system.
This will enable exploration of a number of interesting issues, such as adapting (perhaps dynamically) to use more systems when there is sufficient parallelism available, without requiring programmers to rewrite their applications.
