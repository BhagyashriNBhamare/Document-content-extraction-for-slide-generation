JavaScript is widely used in web-based applications and is increasingly popular with developers.
So-called browser wars in recent years have focused on JavaScript performance, specifically claiming comparative results based on benchmark suites such as SunSpider and V8.
In this paper we evaluate the behavior of JavaScript web applications from commercial web sites and compare this behavior with the benchmarks.
We measure two specific areas of JavaScript runtime behavior: 1) functions and code and 2) events and handlers.
We find that the benchmarks are not representative of many real web sites and that conclusions reached from measuring the benchmarks may be misleading.
Specific common behaviors of real web sites that are underem-phasized in the benchmarks include event-driven execution , instruction mix similarity, cold-code dominance, and the prevalence of short functions.
We hope our results will convince the JavaScript community to develop and adopt benchmarks that are more representative of real web applications.
JavaScript is a widely used programming language that is enabling a new generation of computer applications.
Used by large fraction of all web sites, including Google, Facebook, and Yahoo, JavaScript allows web applications to be more dynamic, interesting, and responsive.
Because JavaScript is so widely used to enable Web 2.0, the performance of JavaScript is now a concern of vendors of every major browser, including Mozilla Firefox, Google Chrome, and Microsoft Internet Explorer.
The competition between major vendors, also known as the 'browser wars" [24], has inspired aggressive new JavaScript implementations based on Just-In-Time (JIT) compilation strategies [8].
Because browser market share is extremely important to companies competing in the web services marketplace, an objective comparison of the performance of different browsers is valuable to both consumers and service providers.
JavaScript benchmarks, including SunSpider [23] and V8 [10], are widely used to evaluate JavaScript performance (for example, see [13]).
These benchmark results are used to market and promote browers, and the benchmarks influence the design of JavaScript runtime implementations.
Performance of JavaScript on the SunSpider and V8 benchmarks has improved dramatically in recent years.This paper examines the following question: How representative are the SunSpider and V8 benchmarks suites when compared with the behavior of real JavaScriptbased web applications?
More importantly, we examine how benchmark behavior that differs quite significantly from real web applications might mislead JavaScript runtime developers.By instrumenting the Internet Explorer 8 JavaScript runtime, we measure the JavaScript behavior of 11 important web applications and pages, including Gmail, Facebook, Amazon, and Yahoo.
For each application, we conduct a typical user interaction scenario that uses the web application for a productive purpose such as reading email, ordering a book, or finding travel directions.
We measure a variety of different program characteristics, ranging from the mix of operations executed to the frequency and types of events generated and handled.Our results show that real web applications behave very differently from the benchmarks and that there are definite ways in which the benchmark behavior might mislead a designer.
Because of the space limitations, this paper presents a relatively brief summary of our findings.
The interested reader is referred to a companion technical report [17] for a more comprehensive set of results.The contributions of this paper include:• We are among the first to publish a detailed characterization of JavaScript execution behavior in real web applications, the SunSpider, and the V8 benchmarks.
In this paper we focus on functions and code as well as events and handlers.
Our technical report [17] considers heap-allocated objects and data.
• We conclude that the benchmarks are not representative of real applications in many ways.
Focusing on benchmark performance may result in overspecialization for benchmark behavior that does not occur in practice, and in missing optimization opportunities that are present in the real applications but not present in the benchmarks.
• We find that real web applications have code that is one to two orders of magnitude larger than most of the benchmarks and that managing code (both allocating and translating) is an important activity in a real JavaScript engine.
Our case study in Section 4.7 demonstrates this point.
• We find that while the benchmarks are computeintensive and batch-oriented, real web applications are event-driven, handling thousands of events.
To be responsive, most event handlers execute only tens to hundreds of bytecodes.
As a result, functions are typically short-lived, and long-running loops are uncommon.
• While existing JavaScript benchmarks make minimal use of event handlers, we find that they are extensively used in real web applications.
The importance of responsiveness in web application design is not captured adequately by any of the benchmarks available today.
JavaScript is a garbage-collected, memory-safe programming language with a number of interesting properties [6].
Unlike class-based object-oriented languages like C# and Java, JavaScript is a prototype-based language, influenced heavily in its design by Self [22].
JavaScript became widely used because it is standardized, available in every browser implementation, and tightly coupled with the browser's Document Object Model [2].
Importance of JavaScript.
JavaScript's popularity has grown with the success of the web.
Scripts in web pages have become increasingly complex as AJAX (Asynchronous JavaScript and XML) programming has transformed static web pages into responsive applications [11].
Web sites such as Amazon, Gmail, and Facebook contain and execute significant amounts of JavaScript code, as we document in this paper.
Web applications (or apps) are applications that are hosted entirely in a browser and delivered through the web.
Web apps have the advantage that they require no additional installation, will run on any machine that has a browser, and provide access to information stored in the cloud.
Sophisticated mobile phones, such as the iPhone, broaden the base of Internet users, further increasing the importance and reach of web apps.
In recent years, the complexity of web content has spurred browser developers to increase browser performance in a number of dimensions, including improving JavaScript performance.
Many of the techniques for improving traditional object-oriented languages such as Java and C# can and have been applied to JavaScript [8,9].
JIT compilation has also been effectively applied, increasing measured benchmark performance of JavaScript dramatically.Value of benchmarks.
Because browser performance can significantly affect a user's experience using a web application, there is commercial pressure for browser vendors to demonstrate that they have improved performance.
As a result, JavaScript benchmark results are widely used in marketing and in evaluating new browser implementations.
The two most widely used JavaScript benchmark suites are SunSpider, a collection of small benchmarks available from WebKit.org [23], and the V8 benchmarks, a collection of seven slightly larger benchmarks published by Google [10].
The benchmarks in both of these suites are relatively small programs; for example, the V8 benchmarks range from approximately 600 to 5,000 lines of code.Illustrative example.
Before we discuss how we collect JavaScript behavior data from real sites and benchmarks, we illustrate how this data is useful.
Figure 1 shows live heap graphs for visits to the google and bing web sites 1 .
These graphs show the number of live bytes of different types of data in the JavaScript heap as a function of time (measured by bytes of data allocated).
In the figures, we show only the four most important data types: functions, strings, arrays, and objects.
When the JavaScript heap is discarded, for example because the user navigates to a new page, the live bytes drops to zero, as we see in google.These two search web sites shown offer very similar functionality, and we performed the same sequence of operations on them during our visit: we searched for "New York" in both cases and then proceeded to page through the results, first web page results and then the relevant news items.We see from our measurements of the JavaScript heap, however, that the implementations of the two applications are very different, with google being implemented as a series of visits to different pages, and bing implemented as a single page visit.
The benefit of the bing ap- proach is highlighted in this case by looking at the right hand side of each subfigure.
In the case of google, we see that the contents of the JavaScript heap, including all the functions, are discarded and recreated repeatedly during our visit, whereas in the bing heap the functions are allocated only once.
The size of the google heap is significantly smaller than the bing heap (approximately an order of magnitude), so it could be argued that the google approach is better.
On the other hand, the bing approach does not lead to the JavaScript heap being repeatedly recreated.In conclusion, we note that this kind of dynamic heap behavior is not captured by any of the V8 or SunSpider benchmarks, even though it is common among real web applications.
Knowledge about such allocation behavior can be useful when, for example, designing and optimizing the garbage collection systems.
In this section, we describe the benchmarks and applications we used and provide an overview of our measurements.
Figure 2 lists the 11 real web applications that we used for our study 2 .
These sites were selected because of their popularity according to Alexa.com, and also because they represent a cross-section of diverse activities.
Specifically, our applications represent search (google, bing), mapping (googlemap, bingmap), email (hotmail, gmail), e-commerce (amazon, ebay), news (cnn, economist), and social networking (facebook).
Part of our goal was to understand both the differences between the real sites and the benchmarks as well as the differences among different classes of real web applications.
For the remainder of this paper, we will refer to the different web sites using the names from Figure 2.
The workload for each site mimics the behavior of a user on a short, but complete and representative, visit to the site.
This approach is dictated partly by expedience -it would be logistically complicated to measure long-term use of each web application -and partly because we believe that many applications are actually used in this way.
For example, search and mapping applications are often used for targeted interactions.
In measuring the JavaScript benchmarks, we chose to use the entire V8 benchmark suite, which comprises 7 programs, and selected programs from the SunSpider suite, which consists of 26 different programs.
In order to reduce the amount of data collected and displayed, for SunSpider we chose the longest running benchmark in each of the 9 different benchmark categories -3d: raytrace, access: nbody, bitops: nseive − bits, controlflow: recursive, crypto: aes, date: xparb, math: cordic, regexp: dna, and string: tagcloud.
Our approach to data collection is illustrated in Figure 3.
The platform we chose for instrumentation is Internet Explorer (IE), version 8, running on a 32-bit Windows Vista operating system.
While our results are in some ways specific to IE, the methods described here can be applied to other browsers as well.Our measurement approach works as follows: we have instrumented the C++ code that implements the IE 8 JavaScript runtime.
For IE, the code that is responsible for executing JavaScript programs is not bundled in the main IE executable.
Instead, it resides in a dynamic linked library, jscript.dll.
After performing the instrumentation, we recompiled the engine source code to create a custom jscript.dll.
(see Step 1 in Figure 3).
Next, we set up IE to use the instrumented jscript.dll.
We then visit the web sites and run the benchmark programs described in the previous section with our special version of IE.
A set of binary trace files is created in the process of visiting the web site or running a benchmark.
These traces typically comprise megabytes of data, often up to 800 megabytes in the case of instruction traces.
Finally, we use offline analyzers to process these custom trace files to obtain the results presented here.
In studying the behavior of JavaScript programs, we focused on three broad areas: functions and code, objects and data (omitted here), and events and handlers.
In each of these dimensions, we consider both static measurements (e.g., number of unique functions) and dynamic measurements (e.g., total number of function calls).
We measure mostly the logical behavior of JavaScript programs, avoiding characteristics that are browser-dependent.
Thus, our measurements are largely machine-independent.
However, we also look at specific characteristics of the IE's JavaScript engine (e.g., we count IE 8 bytecodes as a measure of execution) that pertain to interpreter-based engines.
We leave measurements for characteristics relevant to JIT-based engines such as those found in Firefox and Chrome for future work.
The JavaScript engine in IE 8 interprets JavaScript source after compiling it to an intermediate representation called bytecode.
The interpreter has a loop that reads each bytecode instruction and implements its effect in a virtual machine.
Because no actual machine instructions are generated in IE 8, we cannot measure the execution of JavaScript in terms of machine instructions.
The bytecode instruction set implemented by the IE 8 interpreter is a well-optimized, traditional stack-oriented bytecode.We count each bytecode execution as an "instruction" and use the term bytecode and instruction interchangeably throughout our evaluation.
In our measurements, we look at the code behavior at two levels, the function and the bytecode level.
Therefore, we instrument the engine at the points when it creates functions as well as in its main interpreter loop.
Prior work measuring architecture characteristics of interpreters also measures behavior in terms of bytecode execution [19].
JavaScript has a single-threaded event-based programming model, with each event being processed by a nonpreemptive handler.
In other words, JavaScript code runs in response to specific user-initiated events such as a mouse click, becomes idle, and waits for another event to process.
Therefore, to completely understand behaviors of JavaScript that are relevant to its predominant usage, we must consider the event-driven programming model of JavaScript.
Generally speaking, the faster handlers complete, the more responsive an application appears.
However, event handling is an aspect of program behavior that is largely unexplored in related work measuring C++ and Java execution (e.g., see [5] for a thorough analysis of Java execution).
Most related work considers the behavior of benchmarks, such as SPECjvm98 [4] and SPECcpu2000 [1], that have no interactive component.
For JavaScript, however, such batch processing is mostly irrelevant.For our measurements, we insert instrumentation hooks before and after event handling routines to measure characteristics such as the number of events handled and the dynamic size of each event handler invocation as measured by the number of executed bytecode instructions.
We begin this section with an overview of our results.
We then consider the behavior of the JavaScript functions and code, including the size of functions, opcodes executed, etc.
Next, we investigate the use of events and event handlers in the applications.
We conclude the section with a case study showing that introducing cold code, i.e., code that is never executed, into existing benchmarks has a substantial effect on performance results.
Before drilling down into our results, we summarize the main conclusions of our comparison in Figure 4.
The first column of the table indicates the specific behavior we measured, and the next two columns compare and contrast results for the real web applications and benchmarks.
The last column summarizes the implications of the observed differences, specifically providing insights for future JavaScript engine designers.
Due to space constraints, a detailed comparison of all aspects of behavior is beyond the scope of this paper, and we refer the reader to our tech report for those details [17].
We begin our discussion by looking at a summary of the functions and behavior of the real applications and benchmarks.
Figure 5 summarizes our static and dynamic measurements of JavaScript functions.The real web sites.
In Figure 5a, we see that the real web applications comprise many functions, ranging from a low of around 1,000 in google to a high of 10,000 in gmail.
The total amount of JavaScript source code associated with these web sites is significant, ranging from 200 kilobytes to more than two megabytes of source.
Most of the JavaScript source code in these applications has been "minified", that is, had the whitespace removed and local variable names minimized using available tools such as JScrunch [7] or JSmin [3].
This source code is translated to the smaller bytecode representation, which from the figure we see is roughly 60% the size of the source.
In the last column, which captures the percentage of static unique functions executed, we see that as many as 50-70% are not executed during our use of the applications, suggesting that much of the code delivered applies to specific functionality that we did not exercise when we visited the sites.
Code-splitting approaches such as Doloto [15] exploit this fact to reduce the wasted effort of downloading and compiling cold code.The number of bytecodes executed during our visits ranged from around 400,000 to over 20 million.
The most compute-intensive applications were facebook, gmail, and economist.
As we show below, the large number of executed bytecodes in economist is an anomaly caused by a hot function with a tight loop.
This anomaly is also clearly visible from the opcodes/call column.
We see that economist averages over 180 bytecodes per call, while most of the other sites average between 25 and 65 bytecodes per call.
This low number suggests that a majority of JavaScript function executions in these programs do not execute long-running loops.
Our discussion of event handler behavior in Section 4.6 expands on this observation.Because it is an outlier, the economist application deserves further comment.
We looked at the hottest function in the application and found a single function which accounts for over 50% of the total bytecodes executed in our visit to the web site.
This function loops over the elements of the DOM looking for elements with a specific node type and placing those elements into an array.
Given that the DOM can be quite large, using an interpreted loop to gather specific kinds of elements can be quite expensive to compute.
An alternative, more efficient implementation might use DOM APIs like getElementById to find the specific elements of interest directly.On a final note, in column five of Figure 5 we show the number of instances of separate matching < script > tags that appeared in the web pages that implemented the applications.
We see that in the real applications, there are many such instances, ranging to over 200 in ebay.
This high number indicates that JavaScript code is coming from a number of sources in the applications, including different modules and/or feature teams from within the same site, and also coming from third party sites, for advertising, analytics, etc.The benchmarks.
In Figure 5, we also see the summary of the V8 and SunSpider benchmarks.
We see immediately that the benchmarks are much smaller, in terms of both source code and compiled bytecode, than the real applications.
Furthermore, the largest of the benchmarks, string − tagcloud, is large not because of the amount of code, but because it contains a large number of string constants.
Of the benchmarks, earley has the most real code and is an outlier, with 400 functions compared to the average of the rest, which is well below 100 functions.
These functions compile down to very compact bytecode, often more than 10 times smaller than the real applications.
Looking at the fraction of these functions that are executed when the benchmarks are run, we see that in many cases the percentage is high, ranging from 55-100%.
The benchmark earley is again an outlier, with only 27% of the code actually executed in the course of running the benchmark.The opcodes per call measure also shows significant differences with the real applications.
Some of the SunSpider benchmarks, in particular, have long-running loops, resulting in high average bytecodes executed per call.
Other benchmarks, such as controlflow, have artificially low counts of opcodes per call.
Finally, none of the benchmarks has a significant number of distinct contexts in which JavaScript code is introduced (global scope), emphasizing the homogeneous nature of the code in each benchmark.
We examined the distribution of opcodes that each of the real applications and benchmarks executed.
To do this, we counted how many times each of the 160 different opcodes was executed in each program and normalized these values to fractions.
We then compared the 160-dimensional vector generated by each real application and benchmark.Our goal was to characterize the kinds of operations that these programs perform and determine how representative the benchmarks are of the opcode mix performed by the real applications.
We were also interested in understanding how much variation exists between the individual real applications themselves, given their diverse functionality.To compare the resulting vectors, we used Principal Component Analysis (PCA) [12] to reduce the 160-dimensional space to two principal dimensions.
This dimension reduction is a way to avoid the curse of dimensionality problem.
We found that components after the third are insignificant and chose to present only the two principal components for readability.
Figure 6 shows the result of this analysis.
In the figure, we see the three different program collections (real, V8, and SunSpider).
The figure shows that the real sites cluster in the center of the graph, showing relatively small variation among themselves.For example, ebay and bingmap, very different in their functionality, cluster quite closely.
In contrast, both sets of benchmarks are more widely distributed, with several obvious outliers.
For SunSpider, controlflow is clearly different from the other applications, while in V8, regexp sits by itself.
Surprisingly, few of the benchmarks overlap the cluster of real applications, with earley being the closest in overall opcode mix to the real applications.
While we expect some variation in the behavior of a collection of smaller programs, what is most surprising is that almost all the benchmarks have behaviors that are significantly different than the real applications.
Furthermore, it is also surprising that the real web applications cluster as tightly as they do.
This result suggests that while the external functionality provided may appear quite different from site to site, much of the work being done in JavaScript on these sites is quite similar.
We next consider the distribution of hot functions in the applications, which tells us what code needs to be highly optimized.
Figure 7 shows the distribution of hot functions in a subset of the real applications and the V8 benchmarks (full results, including the SunSpider benchmarks are included in [17]).
Each figure shows the cumulative contribution of each function, sorted by hottest functions first on the x-axis, to normalized total opcodes executed on the y-axis.
We truncate the x-axis (not considering all functions) to get a better view of the left end of the curve.
The figures show that all programs, both real applications and benchmarks, exhibit high code locality, with a small number of functions accounting for a large majority of total execution.
In the real applications, 80% of total execution is covered by 50 to 150 functions, while in the benchmarks, at most 10 functions are required.
facebook is an outlier among the real applications, with a small number of functions accounting for almost all the execution time.
We have considered static and dynamic measures of JavaScript program execution, and discovered numerous important differences between the behaviors of the real applications and the benchmarks.
Here we discuss how these differences might lead designers astray when building JavaScript engines that optimize benchmark performance.First, we note a significant difference in the code size of the benchmarks and real applications.
Real web applications have large code bases, containing thousands of functions from hundreds of individual < script > bodies.
Much of this code is never or rarely executed, meaning that efforts to compile, optimize, or tune this code are unnecessary and can be expensive relative to what the benchmarks would indicate.
We also observe that a substantial fraction of the downloaded code is not executed in a typical interaction with a real application.
Attempts to avoid downloading this code, or minimizing the resources that it consumes once it is downloaded, will show much greater benefits in the real applications than in the benchmarks.Second, we observe that based on the distribution of opcodes executed, benchmark programs represent a much broader and skewed spectrum of behavior than the real applications, which are quite closely clustered.
Tuning a JavaScript engine to run controlflow or regexp may improve benchmark results, but tuning the engine to run any one of the real applications is also likely to significantly help the other real applications as well.
Surprisingly, few of the benchmarks approximate the instruction stream mix of the real applications, suggesting that there are activities being performed in the real applications that are not well emulated by the benchmark code.Third, we observe that each individual function execution in the real applications is relatively short.
Because these applications are not compute-intensive, benchmarks with high loop counts, such as bitops − nsieve, distort the benefit that loop optimizations will provide in real applications.
Because the benchmarks are batchoriented to facilitate data collection, they fail to match a fundamental characteristic of all real web applicationsthe need for responsiveness.
The very nature of an interactive application prevents developers from writing code that executes for long periods of time without interruption.Finally, we observe that a tiny fraction of the code accounts for a large fraction of total execution in both the benchmarks and the real applications.
The size of the hot code differs by one to two orders of magnitude between the benchmarks and applications, but even in the real applications the hot code is still quite compact.
In this section, we consider the event-handling behavior of the JavaScript programs.
We observe that handling events is commonplace in the real applications and almost never occurs in the benchmarks.
Thus the focus of this section is on characterizing the handler behavior of the real applications.
Before discussing the results, it is important to explain how handlers affect JavaScript execution.
In some cases, handlers are attached to events that occur when a user interacts with a web page.
Handlers can be attached to any element of the DOM, and interactions such as clicking on an element, moving the mouse over an element, etc., can cause handlers to be invoked.
Handlers also are executed when a timer times out, when a page loads, or when an asynchronous XMLHttpRequest is completed.
JavaScript code is also executed outside of a handler context, such as when a < script > block is processed as part of parsing the web page.
Often code that initializes the JavaScript for the page executes outside of a handler.Because JavaScript has a non-preemptive execution model, once a JavaScript handler is started, the rest of the browser thread for that particular web page is stalled until it completes.
A handler that takes a significant amount of time to execute will make the web application appear sluggish and non-responsive.
Figures 8 and 9 present measures of the event handling behavior in the real applications and the V8 benchmarks 3 .
In both tables, unique events are defined as follows.
Events are nominally unique when they invoke the same sequences of handler instructions with the same inputs.
Our measurements in the figures only approximate this definition.
We associate each event with three attributes: name, the set of handler functions invoked, and the total number of instructions executed.
If the two events have the same three attributes, we say that they are unique.We see that the real applications typically handle thousands of events while the benchmarks all handle 11 or fewer.
In all the benchmarks, one onload event (for loading and, subsequently, running the benchmark program) is responsible for almost 100% of all JavaScript execution.
We will see shortly that this is in stark contrast to the behavior seen in the real applications.
Even though real web sites typically process thousands of events, the unique events column in the figure indicates that there are only around one hundred unique events per application.
This means that a given event is likely to be repeated and handled many times throughout the course of a user visit to the site.We see the diversity of the collection of handlers in the results comparing the mean, median, and maximum of handler durations for the real applications.
Some handlers run for a long time, such as in cnn, where a single handler accounts for a significant fraction of the total JavaScript activity.
Many handlers execute for a very short time, however.
The median handler duration in amazon, for example, is only 8 bytecodes.
amazon is also unusual in that it has the highest number of events.
We hypothesize that such short-duration handlers probably are invoked, test a single value, and then return.These results demonstrate that handlers are written so that they almost always complete in a short time.
For example, in bing and google, both highly optimized for delivering search results quickly, we see low average and median handler times.
It is also clear that google, bing, and facebook have taken care to reduce the duration of the longest handler, with the maximum of all three below 100,000 bytecodes.
Figure 10 confirms that most handler invocations are short.
This figure provides additional context to understand the distribution.
For example, we can determine the 95th percentile handler duration by drawing a vertical line at 0.95 and seeing where each line crosses it.
The figure also illustrates that the durations in many of the applications reach plateaus, indicating that there are many instances of handlers that execute for the same number of instructions.
For example, we see a significant number of bingmap instances that take 1,500 bytcodes to complete.
Our results show that real web applications have much more JavaScript code than the SunSpider and V8 benchmarks and that most of that code is cold.
We were curious how much impact the presence of such cold code would have on benchmark performance results.
Based on our understanding of the complexity and performance overhead of code translation, especially in a JIT-compiler, we hypothesized that simply increasing the amount of cold code in existing benchmarks would have a significant non-uniform impact on benchmark results.
If this hypothesis is true, then a simple way to make results from current benchmarks more representative of actual web applications would be to add cold code to each of them.To test this hypothesis, we selected six SunSpider benchmarks that are small and have mostly hot code.
To each of these benchmarks, we added 200 kilobytes, 400 kilobytes, 800 kilobytes, 1 megabyte and 2 megabytes 400K 800K 1M 2M 290% 294% 433% 104% 204% 163% 200% 379% 278% 314% 457% 127% 144% 288% (a) Impact of cold code in Chrome.
of cold code from the jQuery library.
The added code is never called in the benchmark but the JavaScript runtime still processes it.
We executed each benchmark with the added code and recorded its performance on both the Google Chrome and Internet Explorer browsers 4 .
Figure 11 presents the results of the experiment.
It shows the execution overhead observed in each browser as a function of the size of the additional cold code added in each benchmark.
At a high level, we see immediately that the addition of cold code affects the benchmark performance on the two browsers differently.
In the case of Chrome (Figure 11a), adding two megabytes of cold code can add up to 450% overhead to the benchmark performance.
In Internet Explorer (Figure 11b), cold code has much less impact.In IE, the addition of 200 to 400 kilobytes does not impact its performance significantly.
On average, we observe the overhead due to cold code of 1.8% and 3.2%, respectively.
With 1 megabyte of cold code, the overhead is around 13%, still relatively small given the large amount of code being processed.
In Chrome, on the other hand, even at 200 kilobytes, we observe quite a significant overhead, 25% on average across the six benchmarks.
Even between the benchmarks on the same browser, the addition of cold code has widely varying effects (consider the effect of 1 megabyte of cold code on the different benchmarks in Chrome).
There are several reasons for these observed differences.
First, because Chrome executes the benchmarks faster than IE, the additional fixed time processing the cold code will have a greater effect on Chrome's overall runtime.
Second, Chrome and IE process JavaScript source differently, and large amounts of additional source, even if it is cold code, will have different effects on runtime.
The important takeaway here is not that one browser processes cold code any better than another, but that results of benchmarks containing 1 megabyte of cold code will look different than results without the cold code.
Furthermore, results with cold code are likely to be more representative of browser performance on real web sites.
There are surprisingly few papers measuring specific aspects of JavaScript behavior, despite how widely used it is in practice.
A concurrently submitted paper by Richards et al. measures static and dynamic aspects of JavaScript programs, much as we do [18].
Like us, their goals are to understand the behavior of JavaScript applications in practice, and specifically they investigate the degree of dynamism present in these applications (such as uses of eval).
They also consider the behavior of JavaScript benchmarks, although this is not a major focus of the research.
Unlike us, they do not consider the use of events in applications, or consider the size and effect of cold code.One closely related paper focuses on the behavior of interpreted languages.
Romer et al. [19] consider the runtime behavior of several interpreted languages, including Tcl, Perl, and Java, and show that architectural characteristics, such as cache locality, are a function of the interpreter itself and not the program that it is interpreting.
While the goals are similar, our methods, and the language we consider (JavaScript), are very different.Dieckmann and Hölzle consider the memory allocation behavior of the SPECJVM Java benchmarks [4].
A number of papers have examined the memory reference characteristics of Java programs [4,14,16,20,21] specifically to understand how hardware tailored for Java execution might improve performance.
Our work differs from this previous work in that we measure JavaScript and not Java, we look at characteristics beyond memory allocation, and we consider differences between benchmarks and real applications.Dufour et al. present a framework for categorizing the runtime behavior of programs using precise and concise metrics [5].
They classify behavior in terms of five general categories of measurement and report measurements of a number of Java applications and benchmarks, using their results to classify the programs into more precise categories.
Our measurements correspond to some metrics mentioned by Dufour et al., but we consider some dimensions of execution that they do not, such as event handler metrics, and compare benchmark behavior with real application behavior.
We have presented detailed measurements of the behavior of JavaScript applications, including commercially important web applications such as Gmail and Facebook, as well as the SunSpider and V8 benchmark suites.
We measure two specific areas of JavaScript runtime behavior: 1) functions and code and 2) events and handlers.
We find that the benchmarks are not representative of many real web sites and that conclusions reached from measuring the benchmarks may be misleading.Our results show that JavaScript web applications are large, complex, and highly interactive programs.
While the functionality they implement varies significantly, we observe that the real applications have much in common with each other as well.
In contrast, the JavaScript benchmarks are small, and behave in ways that are significantly different than the real applications.
We have documented numerous differences in behavior, and we conclude from these measured differences that results based on the benchmarks may mislead JavaScript engine implementers.Furthermore, we observe interesting behaviors in real JavaScript applications that the benchmarks fail to exhibit.
Our measurements suggest a number of valuable follow-up efforts.
These include working on building a more representative collection of benchmarks, modifying JavaScript engines to more effectively implement some of the real behaviors we observed, and building developer tools that expose the kind of measurement data we report.
We thank Corneliu Barsan, Trishul Chilimbi, David Detlefs, Leo Meyerovich, Karthik Pattabiraman, David Simmons, Herman Venter, and Allen Wirfs-Brock for their support and feedback during the course of this research.
We thank the anonymous reviewers for their feedback, and specifically Wilson Hsieh, who made a number of concrete and helpful suggestions.
