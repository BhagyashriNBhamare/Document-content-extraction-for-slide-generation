We present Chopper, a tool that efficiently explores the vast input space of file system policies to find behaviors that lead to costly performance problems.
We focus specifically on block allocation, as unexpected poor layouts can lead to high tail latencies.
Our approach utilizes sophisticated statistical methodologies, based on Latin Hypercube Sampling (LHS) and sensitivity analysis , to explore the search space efficiently and diagnose intricate design problems.
We apply Chopper to study the overall behavior of two file systems, and to study Linux ext4 in depth.
We identify four internal design issues in the block allocator of ext4 which form a large tail in the distribution of layout quality.
By removing the underlying problems in the code, we cut the size of the tail by an order of magnitude, producing consistent and satisfactory file layouts that reduce data access latencies.
As the distributed systems that power the cloud have matured, a new performance focus has come into play: tail latency.
As Dean and Barroso describe, long tails can dramatically harm interactive performance and thus limit the applications that can be effectively deployed at scale in modern cloud-based services [17].
As a result, a great deal of recent research effort has attacked tail latency directly [6,49,51]; for example, Alizadeh et al. show how to reduce the network latency of the 99th percentile by a factor of ten through a combination of novel techniques [6].
The fundamental reason that reducing such tail latency is challenging is that rare, corner-case behaviors, which have little impact on a single system, can dominate when running a system at scale [17].
Thus, while the welltested and frequently-exercised portions of a system perform well, the unusual behaviors that are readily ignored on one machine become the common case upon one thousand (or more) machines.To build the next generation of robust, predictably performing systems, we need an approach that can readily discover corner-case behaviors, thus enabling a developer to find and fix intrinsic tail-latency problems before deployment.
Unfortunately, finding unusual behavior is hard: just like exploring an infinite state space for correctness bugs remains an issue for today's model checkers [10,19], discovering the poorly-performing tailinfluencing behaviors presents a significant challenge.One critical contributor to tail latency is the local file system [8].
Found at the heart of most distributed file systems [20,47], local file systems such as Linux ext4, XFS, and btrfs serve as the building block for modern scalable storage.
Thus, if rare-case performance of the local file system is poor, the performance of the distributed file system built on top of it will suffer.In this paper, we present Chopper, a tool that enables developers to discover (and subsequently repair) high-latency operations within local file systems.
Chopper currently focuses on a critical contributor to unusual behavior in modern systems: block allocation, which can reduce file system performance by one or more orders of magnitude on both hard disk and solid state drives [1,11,13,30,36].
With Chopper, we show how to find such poor behaviors, and then how to fix them (usually through simple file-system repairs).
The key and most novel aspect of Chopper is its usage of advanced statistical techniques to search and investigate an infinite performance space systematically.
Specifically, we use Latin hypercube sampling [29] and sensitivity analysis [40], which has been proven efficient in the investigation of many-factor systems in other applications [24,31,39].
We show how to apply such advanced techniques to the domain of file-system performance analysis, and in doing so make finding tail behavior tractable.We use Chopper to analyze the allocation performance of Linux ext4 and XFS, and then delve into a detailed analysis of ext4 as its behavior is more complex and varied.
We find four subtle flaws in ext4, including behaviors that spread sequentially-written files over the entire disk volume, greatly increasing fragmentation and inducing large latency when the data is later accessed.
We also show how simple fixes can remedy these problems, resulting in an order-of-magnitude improvement in the tail layout quality of the block allocator.
Chopper and the ext4 patches are publicly available at:research.cs.wisc.edu/adsl/Software/chopperThe rest of the paper is organized as follows.
Section 2 introduces the experimental methodology and implementation of Chopper.
In Section 3, we evaluate ext4 and XFS as black boxes and then go further to explore ext4 as a white box since ext4 has a much larger tail than XFS.
We present detailed analysis and fixes for internal allocator design issues of ext4.
Section 4 introduces related work.
Section 5 concludes this paper.
We now describe our methodology for discovering interesting tail behaviors in file system performance, particularly as related to block allocation.
The file system input space is vast, and thus cannot be explored exhaustively; we thus treat each file system experiment as a simulation, and apply a sophisticated sampling technique to ensure that the large input space is explored carefully.In this section, we first describe our general experimental approach, the inputs we use, and the output metric of choice.
We conclude by presenting our implementation.
The Monte Carlo method is a process of exploring simulation by obtaining numeric results through repeated random sampling of inputs [38,40,43].
Here, we treat the file system itself as a simulator, thus placing it into the Monte Carlo framework.
Each run of the file system, given a set of inputs, produces a single output, and we use this framework to explore the file system as a black box.Each input factor X i (i = 1, 2, ..., K) (described further in Section 2.2) is estimated to follow a distribution.
For example, if small files are of particular interest, one can utilize a distribution that skews toward small file sizes.In the experiments of this paper, we use a uniform distribution for fair searching.
For each factor X i , we draw a sample from its distribution and get a vector of values(X 1 i , X 2 i , X 3 i , .
.
, X N i ).
Collecting samples of all the factors, we obtain a matrix M .
M =     X 1 1 X 1 2 ... X 1 K X 2 1 X 2 2 ... X 2 K ... X N 1 X N 2 ... X N K     Y =     Y 1 Y 2 ... Y N    Each row in M , i.e., a treatment, is a vector to be used as input of one run, which produces one row in vector Y .
In our experiment, M consists of columns such as the size of the file system and how much of it is currently in use.
Y is a vector of the output metric; as described below, we use a metric that captures how much a file is spread out over the disk called d-span.
M and Y are used for exploratory data analysis.The framework described above allows us to explore file systems over different combinations of values for uncertain inputs.
This is valuable for file system studies where the access patterns are uncertain.
With the framework, block allocator designers can explore the consequences of design decisions and users can examine the allocator for their workload.In the experiment framework, M is a set of treatments we would like to test, which is called an experimental plan (or experimental design).
With a large input space, it is essential to pick input values of each factor and organize them in a way to efficiently explore the space in a limited number of runs.
For example, even with our refined space in Table 1 (introduced in detail later), there are about 8 × 10 9 combinations to explore.
With an overly optimistic speed of one treatment per second, it still would take 250 compute-years to finish just one such exploration.Latin Hypercube Sampling (LHS) is a sampling method that efficiently explores many-factor systems with a large input space and helps discover surprising behaviors [25,29,40].
A Latin hypercube is a generalization of a Latin square, which is a square grid with only one sample point in each row and each column, to an arbitrary number of dimensions [12].
LHS is very effective in examining the influence of each factor when the number of runs in the experiment is much larger than the number of factors.
It aids visual analysis as it exercises the system over the entire range of each input factor and ensures all levels of it are explored evenly [38].
LHS can effectively discover which factors and which combinations of factors have a large influence on the response.
A poor sampling method, such as a completely random one, could have input points clustered in the input space, leaving large unexplored gaps inbetween [38].
Our experimental plan, based on LHS, contains 16384 runs, large enough to discover subtle behaviors but not so large as to require an impractical amount of time.
File systems are complex.
It is virtually impossible to study all possible factors influencing performance.
For example, the various file system formatting and mounting options alone yield a large number of combinations.
In addition, the run-time environment is complex; for example, file system data is often buffered in OS page caches in memory, and differences in memory size can dramatically change file system behavior.In this study, we choose to focus on a subset of factors that we believe are most relevant to allocation behavior.
As we will see, these factors are broad enough to discover interesting performance oddities; they are also not so broad as to make a thorough exploration intractable.There are three categories of input factors in Chopper.
The first category of factors describes the initial state of the file system.
The second category includes a relevant OS state.
The third category includes factors describing the workload itself.
All factors are picked to reveal potentially interesting design issues.
In the rest of this paper, a value picked for a factor is called a level.
A set of levels, each of which is selected for a factor, is called a treatment.
One execution of a treatment is called a run.
We picked twelve factors, which are summarized in Table 1 and introduced as follows.We create a virtual disk of DiskSize bytes, because block allocators may have different space management policies for disks of different sizes.The UsedRatio factor describes the ratio of disk that has been used.
Chopper includes it because block allocators may allocate blocks differently when the availability of free space is different.
The FreeSpaceLayout factor describes the contiguity of free space on disk.
Obtaining satisfactory layouts despite a paucity of free space, which often arises when file systems are aged, is an important task for block allocators.
Because enumerating all fragmentation states is impossible, we use six numbers to represent degrees from extremely fragmented to generally contiguous.
We use the distribution of free extent sizes to describe the degree of fragmentations; the extent sizes follow lognormal distributions.
Distributions of layout 1 to 5 are shown in Figure 1.
For example, if layout is number 2, about 0.1 × DiskSize × (1 − U sedRatio) bytes will consist of 32KB extents, which are placed randomly in the free space.
Layout 6 is not manually fragmented, in order to have the most contiguous free extents possible.The CPUCount factor controls the number of CPUs the OS runs on.
It can be used to discover scalability issues of block allocators.The FileSize factor represents the size of the file to be written, as allocators may behave differently when different sized files are allocated.
For simplicity, if there is more than one file in a treatment, all of them have the same size.A chunk is the data written by a write() call.
A file is often not written by only one call, but a series of writes.
Thus, it is interesting to see how block allocators act with different numbers of chunks, which ChunkCount factor captures.
In our experiments, a file is divided into multiple chunks of equal sizes.
They are named by their positions in file, e.g., if there are four chunks, chunk-0 is at the head of the file and chunk-3 is at the end.Sparse files, such as virtual machine images [26], are commonly-used and important.
Files written nonsequentially are sparse at some point in their life, although the final state is not.
On the other hand, overwriting is also common and can have effect if any copy-on-write strategy is adopted [34].
The InternalDensity factor describes the degree of coverage (e.g. sparseness or overwriting) of a file.
For example, if InternalDensity is 0.2 and chunk size is 10KB, only the 2KB at the end of each chunk will be written.
If InternalDensity is 1.2, there will be two writes for each chunk; the first write of this chunk will be 10KB and the second one will be 2KB at the end of the chunk.The ChunkOrder factor defines the order in which the chunks are written.
It explores sequential and random write patterns, but with more control.
For example, if a file has four chunks, ChunkOrder=0123 specifies that the file is written from the beginning to the end; ChunkOrder=3210 specifies that the file is written backwards.The Fsync factor is defined as a bitmap describing whether Chopper performs an fsync() call after each chunk is written.
Applications, such as databases, often use fsync() to force data durability immediately [15,23].
This factor explores how fsync() may interplay with allocator features (e.g., delayed allocation in Linux ext4 [28]).
In the experiment, if ChunkOrder=1230 and Fsync=1100, Chopper will perform an fsync() after chunk-1 and chunk-2 are written, but not otherwise.The Sync factor defines how we open, close, or sync the file system with each write.
For example, if ChunkOrder=1230 and Sync=0011, Chopper will perform the three calls after chunk-3 and perform close() and sync() after chunk-0; open() is not called after the last chunk is written.
All Sync bitmaps end with 1, in order to place data on disk before we inquire about layout information.
Chopper performs fsync() before sync() if they both are requested for a chunk.The FileCount factor describes the number of files written, which is used to explore how block allocators preserve spatial locality for one file and for multiple files.
In the experiment, if there is more than one file, the chunks of each file will be written in an interleaved fashion.
The ChunkOrder, Fsync, and Sync for all the files in a single treatment are identical.Chopper places files in different nodes of a directory tree to study how parent directories can affect the data layouts.
The DirectorySpan factor describes the distance between parent directories of the first and last files in a breadth-first traversal of the tree.
If FileCount=1, DirectorySpan is the index of the parent directory in the breadth-first traversal sequence.
If FileCount=2, the first file will be placed in the first directory, and the second one will be at the DirectorySpan-th position of the traversal sequence.In summary, the input space of the experiments presented in this paper is described in Table 1.
The choice is based on efficiency and simplicity.
For example, we study relatively small file sizes because past studies of file systems indicates most files are relatively small [5,9,35].
Specifically, Agrawal et.
al. found that over 90% of the files are below 256 KB across a wide range of systems [5].
Our results reveal many interesting behaviors, many of which also apply to larger files.
In addition, we study relatively small disk sizes as large ones slow down experiments and prevent broad explorations in limited time.The file system problems we found with small disk sizes are also present with large disks.Simplicity is also critical.
For example, we use at most two files in these experiments.
Writing to just two files, we have found, can reveal interesting nuances in block allocation.
Exploring more files make the results more challenging to interpret.
We leave further exploration of the file system input space to future work.
To diagnose block allocators, which aim to place data compactly to avoid time-consuming seeking on HDDs [7,36] and garbage collections on SSDs [11,30], we need an intuitive metric reflecting data layout quality.
To this end, we define d-span, the distance in bytes between the first and last physical block of a file.
In other words, dspan measures the worst allocation decision the allocator makes in terms of spreading data.
As desired, d-span is an indirect performance metric, and, more importantly, an intuitive diagnostic signal that helps us find unexpected filesystem behaviors.
These behaviors may produce poor layouts that eventually induce long data access latencies.
dspan captures subtle problematic behaviors which would be hidden if end-to-end performance metrics were used.
Ideally, d-span should be the same size as the file.d-span is not intended to be an one-size-fits-all metric.
Being simple, it has its weaknesses.
For example, it cannot distinguish cases that have the same span but different internal layouts.
An alternative of d-span that we have investigated is to model data blocks as vertices in a graph and use average path length [18] as the metric.
The minimum distance between two vertices in the graph is their corresponding distance on disk.
Although this metric is able to distinguish between various internal layouts, we have found that it is often confusing.
In contrast, d-span contains less information but is much easier to interpret.In addition to the metrics above, we have also explored metrics such as number of data extents, layout score (fraction of contiguous blocks) [42], and normalized versions of each metric (e.g. d-span/ideal d-span).
One can even create a metric by plugging in a disk model to measure quality.
Our diagnostic framework works with all of these metrics, each of which allows us to view the system from a different angle.
However, d-span has the best trade-off between information gain and simplicity.
The components of Chopper are presented in Figure 2.
The Manager builds an experimental plan and conducts the plan using the other components.
The FS Manipulator prepares the file system for subsequent workloads.
In order to speed up the experiments, the file system is mounted on an in-memory virtual disk, which is implemented as a loop-back device backed by a file in a RAM file system.
The initial disk images are re-used whenever needed, thus speeding up experimentation and providing reproducibility.
After the image is ready, the Workload Generator produces a workload description, which is then fed into the Workload Player for running.
After playing the workload, the Manager informs the FS Monitor, which invokes existing system utilities, such as debugfs and xfs db, to collect layout information.
No kernel changes are needed.
Finally, layout information is merged with workload and system information and fed into the Analyzer.
The experiment runs can be executed in parallel to significantly reduce time.
We use Chopper to help understand the policies of file system block allocators, to achieve more predictable and consistent data layouts, and to reduce the chances of performance fluctuations.
In this paper, we focus on Linux ext4 [28] and XFS [41], which are among the most popular local file systems [2][3][4]33].
For each file system, we begin in Section 3.1 by asking whether or not it provides robust file layout in the presence of uncertain workloads.
If the file system is robust (i.e., XFS), then we claim success; however, if it is not (i.e., ext4), then we delve further into understanding the workload and environment factors that cause the unpredictable layouts.
Once we understand the combination of factors that are problematic, in Section 3.2, we search for the responsible policies in the file system source code and improve those policies.
The first question we ask is whether or not the file allocation policies in Linux ext4 and XFS are robust to the input space introduced in Table 1.
To find out if there are tails in the resulting allocations, we conducted experiments with 16384 runs using Chopper.
The experiments were conducted on a cluster of nodes with 16 GB RAM and two Opteron-242 CPUs [21].
The nodes ran Linux v3.12.5.
Exploiting Chopper's parallelism and optimizations, one full experiment on each file system took about 30 minutes with 32 nodes.
Figure 3 presents the empirical CDF of the resulting dspans for each file system over all the runs; in runs with multiple files, the reported d-span is the maximum d-span of the allocated files.
A large d-span value indicates a file with poor locality.
Note that the file sizes are never larger than 256KB, so d-span with optimal allocation would be only 256KB as well.The figure shows that the CDF line for XFS is nearly vertical; thus, XFS allocates files with relatively little variation in the d-span metric, even with widely differing workloads and environmental factors.
While XFS may not be ideal, this CDF (as well as further experiments not shown due to space constraints) indicates that its block allocation policy is relatively robust.In contrast, the CDF for ext4 has a significant tail.
Specifically, 10% of the runs in ext4 have at least one file spreading over 10GB.
This tail indicates instability in the ext4 block allocation policy that could produce poor layouts inducing long access latencies.
We next investigate which workload and environment factors contribute most to the variation seen in ext4 layout.
Understanding these factors is important for two reasons.
First, it can help file system users to see which workloads run best on a given file system and to avoid those which do not run well; second, it can help file system developers track down the source of internal policy problems.The contribution of a factor to variation can be calculated by variance-based factor prioritization, a technique in sensitivity analysis [38].
Specifically, the contribution of factor X i is calculated by:Si = V X i (EX ∼i (Y |Xi = x * i )) V (Y )S i is always smaller than 1 and reports the ratio of the contribution by factor X i to the overall variation.
In more detail, if factor X i is fixed at a particular levelx * i , then E X∼i (Y |X i = x * i ) is the resulting mean of response val- ues for that level, V Xi (E X∼i (Y |X i = x * i ))is the variance among level means of X i , and V (Y ) is the variance of all response values for an experiment.
Intuitively, S i indicates how much changing a factor can affect the response.
Figure 4 presents the contribution of each factor for ext4; again, the metric indicates the contribution of each factor to the variation of d-span in the experiment.
The figure shows that the most significant factors are DiskSize, FileSize, Sync, ChunkOrder, and Fsync; that is, changing any one of those factors may significantly affect d-span and layout quality.
DiskSize is the most sensitive factor, indicating that ext4 does not have stable layout quality with different disk sizes.
It is not surprising that FileSize affects d-span considering that the definition d-span depends on the size of the file; however, the variance contributed by FileSize (0.14 × V (dspan real ) = 3 × 10 18 ) is much larger than ideally expected (V (dspan ideal ) = 6 × 10 10 , dspan ideal = F ileSize).
The significance of Sync, ChunkOrder, and Fsync imply that certain write patterns are much worse than others for ext4 allocator.Factor prioritization gives us an overview of the importance of each factor and guides further exploration.
We would also like to know which factors and which levels of a factor are most responsible for the tail.
This can be determined with factor mapping [38]; factor mapping uses a threshold value to group responses (i.e., d-span values) into tail and non-tail categories and finds the input space of factors that drive the system into each category.
We define the threshold value as the 90th% (10GB in this case) figure, we can find what levels of each factor have tail runs and percentage of tail runs in each level.
Regions with significantly more tail runs are marked bold.
Note that the number of total runs of each level is identical for each factor.
Therefore, the percentages between levels of a factor are comparable.
For example, (a) shows all tail runs in the experiment have disk sizes ≥ 16GB.
In addition, when DiskSize=16GB, 17% of runs are in the tail (d-span≥10GB) which is less than DiskSize=32GB.of all d-spans in the experiment.
We say that a run is a tail run if its response is in the tail category.
Factor mapping visualization in Figure 5 shows how the tails are distributed to the levels of each factor.
Thanks to the balanced Latin hypercube design with large sample size, the difference between any two levels of a factor is likely to be attributed to the level change of this factor and not due to chance.
Figure 5a shows that all tail runs lay on disk sizes over 8GB because the threshold d-span (10GB) is only possible when the disk size exceeds that size.
This result implies that blocks are spread farther as the capacity of the disk increases, possibly due to poor allocation polices in ext4.
Figure 5b shows a surprising result: there are significantly more tail runs when the file size is larger than 64KB.
This reveals that ext4 uses very different block allocation polices for files below and above 64KB.
Sync, ChunkOrder, and Fsync also present interesting behaviors, in which the first written chunk plays an important role in deciding the tail.
Figure 5c shows that closing and sync-ing after the first written chunk (coded 1***) causes more tail runs than otherwise.
Figure 5d shows that writing chunk-0 of a file first (coded 0***), including sequential writes (coded 0123) which are usually preferred, leads to more tail runs.
Figure 5e shows that, on average, not fsync-ing the first written chunk (coded 0***) leads to more tail runs than otherwise.The rest of the factors are less significant, but still reveal interesting observations.
Figure 5f and Figure 5g show that tail runs are always present and not strongly correlated with free space layout or the amount of free space, even given the small file sizes in our workloads (below 256KB).
Even with layout number 6 (not manually fragmented), there are still many tail runs.
Similarly, having more free spaces does not reduce tail cases.
These facts indicate that many tail runs do not depend on the disk state and instead it is the ext4 block allocation policy itself causing these tail runs.
After we fix the ext4 allocation polices in the next section, the DiskUsed and FreespaceLayout factors will have a much stronger impact.Finally, Figure 5h and Figure 5i show that tail runs are generally not affected by DirectorySpan and InternalDensity.
Figure 5j shows that having more files leads to 29% more tail cases, indicating potential layout problems in production systems where multi-file operations are common.
Figure 5k shows that there are 6% more tail cases when there are two CPUs.
In a complex system such as ext4 block allocator, performance may depend on more than one factor.
We have inspected all two-factor interactions and select two cases in Figure 6 that present clear patterns.
The figures show how pairwise interactions may lead to tail runs, reveal-G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G ing both dangerous and low-danger zones in the workload space; these zones give us hints about the causes of the tail, which will be investigated in Section 3.2.
Fig- ure 6a shows that, writing and fsync-ing chunk-3 first significantly reduces tail cases.
In Figure 6b, we see that, for files not larger than 64KB, fsync-ing the first written chunk significantly reduces the possibility of producing tail runs.
These two figures do not conflict with each other; in fact, they indicate a low-danger zone in a threedimension space.Evaluating ext4 as black box, we have shown that ext4 does not consistently provide good layouts given diverse inputs.
Our results show that unstable performance with ext4 is not due to the external state of the disk (e.g., fragmentation or utilization), but to the internal policies of ext4.
To understand and fix the problems with ext4 allocation, we use detailed results from Chopper to guide our search through ext4 documentation and source code.
Our previous analysis uncovered a number of problems with the layout policies of ext4, but it did not pinpoint the location of those policies within the ext4 source code.
We now use the hints provided by our previous data analysis to narrow down the sources of problems and to perform and 24 times, respectively.
detailed source code tracing given the set of workloads suggested by Chopper.
In this manner, we are able to fix a series of problems in the ext4 layout policies and show that each fix reduces the tail cases in ext4 layout.
Figure 7 compares the original version of ext4 and our final version that has four sources of layout variation removed.
We can see that the fixes significantly reduce the size of the tail, providing better and more consistent layout quality.
We now connect the symptoms of problems shown by Chopper to their root causes in the code.G G G G G G G G G G G G G G G G G G G G G G G G G 1MB 4MB 16MB 64MB 256MB 1GB 4GB 16GB 64GB Vanilla !
SD !
SE !
SG !
NB d−span (log scale) G G G G G 99th% 95th% 90th% 85th% 80th% (a) Effect of Single Fix G G G G G G G G G G G G G G G G G G G G G G G G G 1MB 4MB 16MB 64MB 256MB 1GB 4GB 16GB 64GB Vanilla Our first step is to remove non-determinism for experiments with the same treatment.
Our previous experiments corresponded to a single run for each treatment; this approach was acceptable for summarizing from a large sample space, but cannot show intra-treatment variation.
After we identify and remove this intra-treatment variation, it will be more straightforward to remove other tail effects.We conducted two repeated experiments with the same input space as in Table 1 and found that 6% of the runs have different d-spans for the same treatment; thus, ext4 can produce different layouts for the same controlled input.
Figure 9a shows the distribution of the d-span differences for those 6% of runs.
The graph indicates that the physical data layout can differ by as much as 46GB for the same workload.
Examining the full set of factors responsible for this variation, we found interesting interactions between FileSize, CPUCount, and ChunkOrder.
Figure 9b shows the count of runs in which d-span changed between identical treatments as a function of CPUCount and FileSize.
This figure gives us the hint that small files in multiple-CPU systems may suffer from unpredictable layouts.
Fig- ure 9c shows the number of runs with changed d-span as a function of ChunkOrder and FileSize.
This figure indicates that most small files and those large files written with more sequential patterns are affected.Root Cause: With these symptoms as hints we focused on the interaction between small files and the CPU scheduler.
Linux ext4 has an allocation policy such that files not larger than 64KB (small files) are allocated from locality group (LG) preallocations; further, the block allocator associates each LG preallocation with a CPU, in order to avoid contention.
Thus, for small files, the layout location is based solely on which CPU the flusher thread is running.
Since the flusher thread can be scheduled on different CPUs, the same small file can use different LG preallocations spread across the entire disk.This policy is also the cause of the variation seen by some large files written sequentially: large files written sequentially begin as small files and are subject to LG preallocation; large files written backwards have large sizes from the beginning and never trigger this scheduling dependency 1 .
In production systems with heavy loads, more cores, and more files, we expect more unexpected poor layouts due to this effect.Fix: We remove the problem of random layout by choosing the locality group for a small file based on its inumber range instead of the CPU.
Using the i-number not only removes the dependency on the scheduler, but also ensures that small files with close i-numbers are likely to be placed close together.
We refer to the ext4 version with this new policy as !
SD, for no Scheduler Dependency.
Figure 8a compares vanilla ext4 and !
SD.
The graph shows that the new version slightly reduces the size of the tail.
Further analysis shows that in total d-span is reduced by 1.4 TB in 7% of the runs but is increased by 0.8 TB in 3% of runs.
These mixed results occur because this first fix unmasks other problems which can lead to larger d-spans.
In complex systems such as ext4, performance problems interact in surprising ways; we will progressively work to remove three more problems.
We now return to the interesting behaviors originally shown in Figure 6a, which showed that allocating chunk-3 first (Fsync=1*** and ChunkOrder=3***) helps to avoid tail runs.
To determine the cause of poor allocations, we compared traces from selected workloads in which a tail occurs to similar workloads in which tails do not occur.Root Cause: Linux ext4 uses a Special End policy to allocate the last extent of a file when the file is no longer open; specifically, the last extent does not trigger preallocation.
The Special End policy is implemented by checking three conditions -Condition 1: the extent is at the end of the file; Condition 2: the file system is not busy; Condition 3: the file is not open.
If all conditions are satisfied, this request is marked with the hint "do not preallocate", which is different from other parts of the file 2 .
The motivation is that, since the status of a file is final (i.e., no process can change the file until the next open), there is no need to reserve additional space.
While this motivation is valid, the implementation causes an inconsistent allocation for the last extent of the file compared to the rest; the consequence is that blocks can be spread (c) InternalDensity far apart.
For example, a small file may be inadvertently split because non-ending extents are allocated with LG preallocations while the ending extent is not; thus, these conflicting policies drag the extents of the file apart.
This policy explains the tail-free zone (Fsync=1*** and ChunkOrder=3***) in Figure 6a.
In these tail-free zones, the three conditions cannot be simultaneously satisfied since fsync-ing chunk-3 causes the last extent to be allocated, while the file is still open; thus, the Special End policy is not triggered.Fix: To reduce the layout variability, we have removed the Special End policy from ext4; in this version named !
SE, the ending extent is treated like all other parts of the file.
Figure 8 shows that !
SE reduces the size of the tail.
Further analysis of the results show that removing Special End policy reduces d-spans for 32% of the runs by a total of 21TB, but increases d-spans for 14% of the runs by a total of 9TB.
The increasing of d-span is primarily because removing this policy unmasks inconsistent policies in File Size Dependency, which we will discuss next.
Figure 10a examines the benefits of the !
SE policy compared to vanilla ext4 in more detail; to compare only deterministic results, we set CPUCount=1.
The graph shows that the !
SE policy significantly reduces tail runs when the workload begins with sync operations (combination of close(), sync(), and open()); this is because the Special End policy is more likely to be triggered when the file is temporarily closed.
After removing the Scheduler Dependency and Special End policies, ext4 layout still presents a significant tail.
Experimenting with these two fixes, we observe a new symptom that occurs due to the interaction of FileSize and ChunkOrder, as shown in Figure 11.
The stair shape of the tail runs across workloads indicates that this policy only affects large files and it depends upon the first written chunk.
Root Cause: Traces of several representative data points reveal the source of the 'stair' symptom, which we call File Size Dependency.
In ext4, one of the design goals is to place small files (less than 64KB, which is tunable) close and big files apart [7].
Blocks for small files are allocated from LG preallocations, which are shared by all small files; blocks in large files are allocated from perfile inode preallocations (except for the ending extent of a closed file, due to the Special End policy).
This file-size-dependant policy ignores the activeness of files, since the dynamically changing size of a file may trigger inconsistent allocation policies for the same file.
In other words, blocks of a file larger than 64KB can be allocated with two distinct policies as the file grows from small to large.
This changing policy explains why FileSize is the most significant workload factor, as seen in Figure 4, and why Figure 5b shows such a dramatic change at 64KB.
Sequential writes are likely to trigger this problem.
For example, the first 36KB extent of a 72KB file will be allocated from the LG preallocation; the next 36KB extent will be allocated from a new i-node preallocation (since the file is now classified as large with 72KB > 64KB).
The allocator will try to allocate the second extent next to the first, but the preferred location is already occupied by the LG preallocation; the next choice is to use the block group where the last big file in the whole file system was allocated (Shared Global policy, coded SG), which can be far away.
Growing a file often triggers this problem.
File Size Dependency is the reason why runs with ChunkOrder=0*** in Figure 5d and Figure 11 have relatively more tail runs than other orders.
Writing Chunk-0 first makes the file grow from a small size and increases the chance of triggering two distinct policies.Fix: Placing extents of large files together with a shared global policy violates the initial design goal of placing big files apart and deteriorates the consequences of File Size Dependency.
To mitigate the problem, we implemented a new policy (coded !
SG) that tries to place G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G extents of large files close to existing extents of that file.
Figure 8a shows that !
SG significantly reduces the size of the tail.
In more detail, !
SG reduces d-span in 35% of the runs by a total of 45TB.G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G 0.2 0.4To demonstrate the effectiveness of the !
SG version, we compare the number of tail cases with it and vanilla ext4 for deterministic scenarios (CPUCount=1).
Figure 10b shows that the layout of large files (>64KB) is significantly improved with this fix.
Figure 10c shows that the layout of sparse files (with InternalDensity < 1) is also improved; the new policy is able to separately allocate each extent while still keeping them near one another.
With three problems fixed in version !
(SD|SE|SG), we show an interesting interaction that still remains between ChunkOrder and InternalDensity.
Figure 12 shows that while most of the workloads exhibit tails, several workloads do not, specifically, all "solid" (InternalDensity≥1) files with ChunkOrder=3012.
To identify the root cause, we focus only on workloads with ChunkOrder=3012 and compare solid and sparse patterns.Root Cause: Comparing solid and sparse runs with ChunkOrder=3012 shows that the source of the tail is a bug in ext4 normalization; normalization enlarges requests so that the extra space can be used for a similar extent later.
The normalization function should update the request's logical starting block number, corresponding physical block number, and size; however, with the bug, the physical block number is not updated and the old value is used later for allocation 3 .
Figure 13 illustrates how this bug can lead to poor layout.
In this scenario, an ill-normalized request is started (incorrectly) at the original physical block number, but is of a new (correct) larger size; as a result, the request will not fit in the desired gap within this file.
Therefore, ext4 may fail to allocate blocks from preferred locations and will perform a desperate search for free space elsewhere, spreading blocks.
The solid files with ChunkOrder of 3012 in Figure 12 avoid this bug because if chunks-0,1,2 are written sequentially after chunk-3 exists, then the physical block number of the request does not need to be updated.
Fix: We fix the bug by correctly updating the physical starting block of the request in version !
NB.
Figure 14 shows that large files were particularly susceptible to this bug, as were sparse files (InternalDensity < 1).
Fig- ure 8a shows that fixing this bug reduces the tail cases, as desired.
In more detail, !
NB reduces d-span for 19% of runs by 8.3 TB in total.
Surprisingly, fixing the bug increases d-span for 5% of runs by 1.5 TB in total.
Trace analysis reveals that, by pure luck, the mis-implemented normalization sometimes sets the request to nearby space which happened to be free, while the correct request fell in space occupied by another file; thus, with the correct request, ext4 sometimes performs a desperate search and chooses a more distant location.
Figure 8 summarizes the benefits of these four fixes.
Overall, with all four fixes, the 90th-percentile for d-span values is dramatically reduced from well over 4GB to close to 4MB.
Thus, as originally shown in Figure 7, our final version of ext4 has a much less significant tail than the original ext4.
Chopper uses d-span as a diagnostic signal to find problematic block allocator designs that produce poor data layouts.
The poor layouts, which incur costly disk seeks on HDDs [36], garbage collections on SSDs [11] and even CPU spikes [1], can in turn result in long data access latencies.
Our repairs based on Chopper's findings reduce latencies caused by the problematic designs.
For example, Figure 15 demonstrates how Scheduler Dependency incurs long latencies and how our repaired version, !
SD, reduces latencies on an HDD (Hitachi HUA723030ALA640: 3.0 TB, 7200 RPM).
In the experiment, files were created by multiple creating threads residing on different CPUs; each of the threads wrote a part of a 64KB file.
We then measured file access time by reading and over-writing with one thread, which avoids resource contentions and maximizes performance.
To obtain application-disk data transfer performance, OS and disk cache effects were circumvented.
Figure 15 shows that with the SD version, access time increases with more creating threads because SD splits each file into more and potentially distant physical data pieces.
Our fixed version, !
SD, reduced read and write time by up to 67 and 4 times proportionally, and by up to 300 and 1400 ms. The reductions in this experiment, as well as expected greater ones with more creating threads and files, are significant -as a comparison, a round trip between US and Europe for a network packet takes 150 ms and a round trip within the same data center takes 0.5 ms [22,32].
The time increase caused by Scheduler Dependency, as well as other issues, may translate to long latencies in high-level data center operations [17].
Chopper is able to find such issues, leading to fixes reducing latencies.
With the help of exploratory data analysis, we have found and removed four issues in ext4 that can lead to unexpected tail latencies; these issues are summarized in Table 2.
We have made the patches for these issues publicly available with Chopper.While these fixes do significantly reduce the tail behaviors, they have several potential limitations.
First, without the Scheduler Dependency policy, flusher threads run- Scheduler Dependency Choice of preallocation group for small files depends on CPU of flushing thread.
The last extent of a closed file may be rejected to allocate from preallocated spaces.
File Size Dependency Preferred target locations depend on file size which may dynamically change.
Normalization Bug Block allocation requests for large files are not correctly adjusted, causing the allocator to examine mis-aligned locations for free space.
ning on different CPUs may contend for the same preallocation groups.
We believe that the contention degree is acceptable, since allocation within a preallocation is fast and files are distributed across many preallocations; if contention is found to be a problem, more preallocations can be added (the current ext4 creates preallocations lazily, one for each CPU).
Second, removing the Shared Global policy mitigates but does not eliminate the layout problem for files with dynamically changing sizes; choosing policies based on dynamic properties such as file size is complicated and requires more fundamental policy revisions.
Third, our final version, as shown in Figure 7, still contains a small tail.
This tail is due to the disk state (DiskUsed and FreespaceLayout); as expected, when the file system is run on a disk that is more heavily used and is more fragmented, the layout for new files suffers.
The symptoms of internal design problems revealed by Chopper drive us to reason about their causes.
In this process, time-consuming tracing is often necessary to pinpoint a particular problematic code line as the code makes complex decisions based on environmental factors.
Fortunately, analyzing and visualizing the data sets produced by Chopper enabled us to focus on several representative runs.
In addition, we can easily reproduce and trace any runs in the controlled environmental provided by Chopper, without worrying about confounding noises.With Chopper, we have learned several lessons from our experience with ext4 that may help build file systems that are robust to uncertain workload and environmental factors in the future.
First, policies for different circumstances should be harmonious with one another.
For example, ext4 tries to optimize allocation for different scenarios and as a result has a different policy for each case (e.g., the ending extent, small and large files); when multiple policies are triggered for the same file, the policies conflict and the file is dragged apart.
Second, policies should not depend on environmental factors that may change and are outside the control of the file system.
In contrast, data layout in ext4 depends on the OS scheduler, which makes layout quality unpredictable.
By simplifying the layout policies in ext4 to avoid special cases and to be independent of environmental factors, we have shown that file layout is much more compact and predictable.
Chopper is a comprehensive diagnostic tool that provides techniques to explore file system block allocation designs.
It shares similarities and has notable differences with traditional benchmarks and with model checkers.File system benchmarks have been criticized for decades [44][45][46].
Many file system benchmarks target many aspects of file system performance and thus include many factors that affect the results in unpredictable ways.
In contrast, Chopper leverages well-developed statistical techniques [37,38,48] to isolate the impact of various factors and avoid noise.
With its sole focus on block allocation, Chopper is able to isolate its behavior and reveal problems with data layout quality.The self-scaling I/O benchmark [14] is similar to Chopper, but the self-scaling benchmark searches a fivedimension workload parameter space by dynamically adjusting one parameter at a time while keeping the rest constant; its goal is to converge all parameters to values that uniformly achieve a specific percentage of max performance, which is called a focal point.
This approach was able to find interesting behaviors, but it is limited and has several problems.
First, the experiments may never find such a focal point.
Second, the approach is not feasible given a large number of parameters.
Third, changing one parameter at a time may miss interesting points in the space and interactions between parameters.
In contrast, Chopper has been designed to systematically extract the maximum amount of information from limited runs.Model checking is a verification process that explores system state space [16]; it has also been used to diagnose latent performance bugs.
For example, MacePC [27] can identify bad performance and pinpoint the causing state.
One problem with this approach is that it requires a simulation which may not perfectly match the desired implementation.
Implementation-level model checkers, such as FiSC [50], address this problem by checking the actual system.
FiSC checks a real Linux kernel in a customized environment to find file system bugs; however, FiSC needs to run the whole OS in the model checker and intercept calls.
In contrast, Chopper can run in an unmodified, low-overhead environment.
In addition, Chopper explores the input space differently; model checkers consider transitions between states and often use tree search algorithms, which may have clustered exploration states and leave gaps unexplored.
In Chopper, we precisely define a large number of factors and ensure the effects and interactions of these factors are evenly explored by statistical experimental design [29,37,38,48].
Tail behaviors have high consequences and cause unexpected system fluctuations.
Removing tail behaviors will lead to a system with more consistent performance.
However, identifying tails and finding their sources are challenging in complex systems because the input space can be infinite and exhaustive search is impossible.
To study the tails of block allocation in XFS and ext4, we built Chopper to facilitate carefully designed experiments to effectively explore the input space of more than ten factors.
We used Latin hypercube design and sensitivity analysis to uncover unexpected behaviors among many of those factors.
Analysis with Chopper helped us pinpoint and remove four layout issues in ext4; our improvements significantly reduce the problematic behaviors causing tail latencies.
We have made Chopper and ext4 patches publicly available.We believe that the application of established statistical methodologies to system analysis can have a tremendous impact on system design and implementation.
We encourage developers and researchers alike to make systems amenable to such experimentation, as experiments are essential in the analysis and construction of robust systems.
Rigorous statistics will help to reduce unexpected issues caused by intuitive but unreliable design decisions.
We thank the anonymous reviewers and Angela Demke Brown (our shepherd) for their excellent feedback.
This research was supported by the United States Department of Defense, NSF grants CCF-1016924, CNS-1421033, CNS-1319405, CNS-1218405, CNS-1042537, and CNS-1042543 (PRObE http://www.nmc-probe.org/), as well as generous donations from Amazon, Cisco, EMC, Facebook, Fusion-io, Google, Huawei, IBM, Los Alamos National Laboratory, Microsoft, NetApp, Samsung, Sony, Symantec, and VMware.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DOD, NSF, or other institutions.
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
