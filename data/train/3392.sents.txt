Many prior efforts have suggested that Internet video Quality of Experience (QoE) could be dramatically improved by using data-driven prediction of video quality for different choices (e.g., CDN or bitrate) to make optimal decisions.
However, building such a prediction system is challenging on two fronts.
First, the relationships between video quality and observed session features can be quite complex.
Second, video quality changes dynamically.
Thus, we need a prediction model that is (a) expressive enough to capture these complex relationships and (b) capable of updating quality predictions in near real-time.
Unfortunately, several seemingly natural solutions (e.g., simple machine learning approaches and simple network models) fail on one or more fronts.
Thus, the potential benefits promised by these prior efforts remain unrealized.
We address these challenges and present the design and implementation of Critical Feature Analytics (CFA).
The design of CFA is driven by domain-specific insights that video quality is typically determined by a small subset of critical features whose criticality persists over several tens of minutes.
This enables a scalable and accurate workflow where we automatically learn critical features for different sessions on coarse-grained timescales, while updating quality predictions in near real-time.
Using a combination of a real-world pilot deployment and trace-driven analysis, we demonstrate that CFA leads to significant improvements in video quality; e.g., 32% less buffering time and 12% higher bitrate than a random decision maker.
Delivering high quality of experience (QoE) is crucial to the success of today's subscription and advertisementbased business models for Internet video.
As prior work (e.g., [33,11]) has shown, achieving good QoE is challenging because of significant spatial and temporal variation in CDNs' performance, client-side network conditions, and user request patterns.At the same time, these observations also suggest there is a substantial room for improving QoE by dynamically selecting the optimal CDN and bitrate based on a realtime global view of network conditions.
Building on this Prediction System f lit Figure 1: Overview of a global optimization system and the crucial role of a prediction system.
insight, prior work makes the case for a quality optimization system (Figure 1) that uses a prediction oracle to suggest the best parameter settings (e.g., bitrate, CDN) to optimize quality (e.g., [33,11,35,32,20]).
Seen in a broader context, this predictive approach can be applied beyond Internet video (e.g., [10,40,15,16,43]).
However, these prior efforts fall short of providing a concrete instantiation of such a prediction system.
Specifically, we observe that designing such a prediction system is challenging on two key fronts ( §2):• Capturing complex factors that affect quality: For instance, an outage may affect only clients of a specific ISP in a specific city when they use a specific CDN.
To accurately predict the quality of their sessions, one must consider the combination of all three factors.
In addition, the factors that affect video quality vary across different sessions; e.g., wireless hosts may be bottlenecked at the last connection, while other clients may experience loading failures due to unavailability of specific content on some CDNs.
• Need for fresh updates: Video quality changes rapidly, on a timescale of several minutes.
Ideally, we must make predictions based on recent quality measurements.
This is particularly challenging given the volume of measurements (e.g., YouTube had 231 million video sessions and up to 500 thousand concurrent viewers during the Olympics [7]), compounded with the need for expressive and potentially complex prediction models.Unfortunately, many existing solutions fail on one or Taken together, these insights enable us to engineer a scalable and accurate video quality prediction system.
Specifically, on a coarse timescale of tens of minutes, CFA learns the critical features, and on a fine timescale of minutes, CFA updates quality prediction using recent quality measurements.
CFA makes predictions and decisions as new clients arrive.We implemented a prototype of CFA and integrated it in a video optimization platform that manages many premium video providers.
We ran a pilot study on one content provider that has 150,000 sessions each day.
Our real-world experiments show that the bitrates and CDNs selected by CFA lead to 32% less buffering time and 12% higher bitrate than a baseline random decision maker.
Using real trace-driven evaluation, we also show that CFA outperforms many other simple ML prediction algorithms by up to 30% in prediction accuracy and 5-17% in various video quality metrics.
Contributions and Roadmap:• Identifying key challenges in building an accurate prediction system for video quality ( §2).
• Design and implementation of CFA, built on domainspecific insights to address the challenges ( §3-5).
• Real-world and trace-driven evaluation that demonstrates substantial quality improvement by CFA ( §6).
• Using critical features learned by CFA to make interesting observations about video quality ( §7).
This section begins with some background on video quality prediction ( §2.1).
Then, we articulate two key challenges faced by any video quality prediction system: (1) The factors affecting video quality are complex, so we need expressive models ( §2.2); (2) Quality changes rapidly, so models must be updated in near real-time by recent quality measurements ( §2.3).
We also argue why existing solutions do not address these challenges.
Most video service providers today allow a video client (player) to switch CDN and bitrate among a set of available choices [33,20,32].
These switches have little overhead and can be performed at the beginning of and during a video playback [8].
Our goal then is to choose the best CDN and bitrate for a client by accurately predicting the video quality of each hypothetical choice of CDN and bitrate.
In theory, if we can accurately predict the quality of each potential decision, then we can identify the optimal decision.To this end, we envision a prediction system that uses a global view of quality measurements to make predictions for a specific video session.
It learns a prediction function for each quality metric Pred : 2 S × S � → R, which takes as input a given set of historical sessions S ∈ 2 S whose quality is already measured, and a new session s ∈ S, and outputs a quality prediction p ∈ R for s.Each quality measurement summarizes the quality of a video session for some duration of time (in our case, one minute).
It is associated with values of four quality metrics [18] and a set of features 2 (summarized in Table 1).
In general, the set of features depends on the degree of instrumentation and what information is visible to a specific provider.
For instance, a CDN may know the location of servers, whereas a third-party optimizer [1] may only have information at the CDN granularity.
Our focus is not to determine the best set of features that should be recorded for each session, but rather engineer a prediction system that can take an arbitrary set of features as inputs and extract the relationships between these features and video quality.
In practice, the above set of features can already provide accurate predictions that help improve quality.
Our dataset consists of 6.6 million quality measurements collected from 2 million clients using 3 large public CDNs distributed across 168 countries and 152 ISPs.
We show real examples of the complex factors that impact video quality, and the limitations in capturing these relationships.
High-dimensional relationship between video quality and session features.
Video quality could be impacted by combinations of multiple components in the network.
Such high-dimensional effects make it harder to learn the relationships between video quality and features, in contrast to simpler settings where features affect quality independently (e.g., assumed by Naive Bayes).
In a real-world incident, video sessions of Comcast users in Baltimore who watched videos from Level3 CDN experienced high failure rate (VSF) due to congested edge servers, shown by the blue line in Figure 2.
The figure also shows the VSF of sessions sharing the same values on one or two features with the affected sessions; e.g., all Comcast sessions across different cities and CDNs.
In the figure, the high VSF of the affected sessions cannot be clearly identified if we look at the ses- sions that match on only one or two features.
Only when three features of CDN ("Level3"), ASN ("Comcast") and City ("Baltimore") are specified (i.e., blue line), can we detect the high VSF and predict the quality of affected sessions accurately.
In practice, we find that such high-dimensional effects are the common case, rather than an anomalous corner case.
For instance, more than 65% of distinct CDN-ISPCity values have VSF that is at least 50% higher or lower than the VSF of sessions matching only one or two features (not shown).
In other words, their quality is affected by a combined effect of at least three features.
Limitation of existing solutions: It might be tempting to develop simple predictors; e.g., based on the last-hop connection by using average quality of history sessions with the same ConnectionType value.
However, they do not take into account the combined impact of features on video quality.
Conventional machine learning techniques like Naive Bayes also suffer from the same limitation.
In Figures 3(a) and 3(b), we plot the actual JoinTime and the prediction made by the last-hop predictor and Naive Bayes (from Weka [6]) for 300 randomly sampled sessions.
The figures also show the mean relative error ( |predicted−actual| actual ).
For each session, the prediction algorithms train models using historical sessions within a 10-minute interval prior to the session under prediction.
It shows that the prediction error of both solutions is significant and two-sided (i.e., not fixable by normalization).
Highly diverse structures of factors.
The factors that affect video quality vary across different sessions.
This means the prediction algorithm should be expressive enough to predict quality for different sessions using different prediction models.
For instance, the fact that many fiber-to-the-home (e.g., FiOS) users have high bitrates and people on cellular connections have lower bitrates is largely due to the speed of their last-mile connection.
In contrast, some video clients may experience video loading failures due to unavailability of specific content on some CDNs.
A recent measurement study [25] has shown that many heterogeneous factors are correlated with video quality issues.
In §7, we show that 15% of video sessions are impacted by more than 30 different combinations of features and give real examples of different factors that affect quality.
Video quality has significant temporal variability.
In Fig- ure 4(a), for each quality metric and combination of specific CDN, city and ASN, we compute the mean quality of sessions in each 10-minute interval, and then plot the CDF of the relative standard deviation ( stddev mean ) of the quality across different intervals.
In all four quality metrics of interest, we see significant temporal variability; e.g., for 60% of CDN-city-ASN combinations, the relative standard deviation of JoinTime across different 10-minute intervals is more than 30%.
Such quality variability has also been confirmed in other studies (e.g., [33]).
The implication of such temporal variability is that the prediction system must update models in near real-time.
In Figure 4(b), we use the same setup as Figure 3, except that the time window used to train prediction models is several minutes prior to the session under prediction.
The figure shows the impact of such staleness on the prediction error for JoinTime.
For both algorithms, prediction error increases dramatically if the staleness exceeds 10 minutes.
As we will see later, this negative impact of staleness on accuracy is not specific to these prediction algorithms ( §6.3).
Limitation of existing solutions: The requirement to use the most recent measurements makes it infeasible to use computationally expensive models.
For instance, it takes at least one hour to train an SVM-based prediction model from 15K quality measurements in a 10-minute interval for one video site, so the quality predictions will be based on information from more than one hour ago.
This section presents the domain-specific insights we use to help address the expressiveness challenge ( §2.2).
The first insight is that sessions matching on all features have similar video quality.
However, this approach suffers the identical sessions in S � .
* / 2 p ← Est(S � ); 3 return p;Algorithm 1: Baseline prediction that finds sessions matching on all features and uses their observed quality as the basis for prediction.from the curse of dimensionality.
Fortunately, we can leverage a second insight that each video session has a subset of critical features that ultimately determine its video quality.
We conclude this section by highlighting two outstanding issues in translating these insights into a practical prediction system.
Our first insight is that sessions that have identical feature values will naturally have similar (if not identical) quality.
For instance, we expect that all Verizon FiOS users viewing a specific HBO video using Level3 CDN in Pittsburgh at Fri 9 am should have similar quality (modulo very user-specific effects such as local Wi-Fi interference inside the home).
We can summarize the intuition as follows:Insight 1: At a given time, video sessions having same value on every feature have similar video quality.Inspired by Insight 1, we can consider a baseline algorithm (Algorithm 1).
We predict a session's quality based on "identical sessions", i.e., those from recent history that match values on all features with the session under prediction.
Ideally, given infinite data, this algorithm is accurate, because it can capture all possible combinations of factors affecting video quality.However, this algorithm is unreliable as it suffers from the classical curse of dimensionality [39].
Specifically, given the number of combinations of feature values (ASN, device, content providers, CDN, just to name a few), it is hard to find enough identical sessions needed to make a robust prediction.
In our dataset, more than 78% of sessions have no identical session (i.e., matching on all features) within the last 5 minutes.
In practice, we expect that some features are more likely to "explain" the observed quality of a specific video session than others.
For instance, if a specific peering point between Comcast and Netflix in New York is congested, then we expect most of these users will suffer poor quality, regardless of the speed of their local connection.Insight 2: Each video session has a subset of critical features that ultimately determines its video quality.We already saw some real examples in §2.2: in the example of high dimensionality, the critical features of the sessions affected by the congested Level3 edge servers are {ASN,CDN,City}; in the examples of diversity, the critical features are {ConnectionType} and {CDN,ContentName}.
A natural implication of this insight is that it can help us tackle the curse of dimensionality.
Unlike Algorithm 1, which fails to find a sufficient number of sessions, we can estimate quality more reliably by aggregating observations across a larger amount of "similar sessions" that only need to match on these critical features.
Thus, critical features can provide expressiveness while avoiding curse of dimensionality.Algorithm 2 presents a logical view of this idea: 1.
Critical feature learning (line 1): First, find the critical features of each session s, denoted as CriticalFeatures(s).
CriticalFeatures(s) within a recent history of length Δ (by default, 5 minutes).
Finally, return some suitable estimate of the quality of these similar sessions; e.g., the median 3 (for BufRatio, AvgBitrate, JoinTime) or the mean (for VSF).
A practical benefit of Algorithm 2 is that it is interpretable [52], unlike some machine learning algorithms 3 We use median because it is more robust to outliers.
There are two issues in using Algorithm 2.
Can we learn critical features?
A key missing piece is how we get the critical features of each session (line 1).
This is challenging because critical features vary both across sessions and over time [33,25], and it is infeasible to manually configure critical features.
How to reduce update delay?
Recall from §2.3 that the prediction system should use the most recent quality measurements.
This requires a scalable implementation of Algorithm 2, where critical features and quality estimates are updated in a timely manner.
However, naively running Algorithm 2 for millions of sessions under prediction is too expensive ( §6.3).
With a cluster of 32 cores, it takes 30 minutes to learn critical features for 15K sessions within a 10-minutes interval.
This means the prediction will be based on stale information from tens of minutes ago.
In this section, we present the detailed design of CFA and discuss how we address the two practical challenges mentioned in the previous section: learning critical features and reducing update delay.
Domains Definition s, S, S A session, a set of sessions, set of all sessions Table 3: Notations used in learning of critical features.q(s) S � → R Quality of s QualityDist(S) 2 S � → 2 R {q(s)|s ∈ S} f , F, F A feature, a set of features, set of all features CriticalFeatures(s) S � → 2 F Critical features of s V Set of all feature values FV ( f , s) F × S � → V Value on feature f of s FSV (F, s) 2 F × S � → 2 V Set of values on features in F of s SimilarSessionSet (s, S, F, Δ) F × 2 F × S × R + � → 2 F {s � |s � ∈ S,t(s) − Δ < t(s � ) < t(s), FSV (F, s � ) = FSV (F, s)}The key to addressing these challenges is our third and final domain-specific insight:Insight 3: Critical features tend to persist on long timescales of tens of minutes.This insight is derived from prior measurement studies [25,20].
For instance, our previous study on shedding light on video quality issues in the wild showed that the factors that lead to poor video quality persist for hours, and sometimes even days [25].
Another recent study from the C3 system suggests that the best CDN tends to be relatively stable on the timescales of few tens of minutes [20].
We independently confirm this observation in §6.3 that using slightly stale critical features (e.g., 30-60 minutes ago) achieves similar prediction accuracy as using the most up-to-date critical features.
Though this insight holds for most cases, it is still possible (e.g., on mobile devices) that critical features persist on a relatively shorter timescale (e.g., due to the nature of mobility).
Note that the persistence of critical features does not mean that quality values are equally persistent.
In fact, persistence of critical features is on a timescale an order of magnitude longer than the persistence of quality.
That is, even if quality fluctuates rapidly, the critical features that determine the quality do not change as often.As we will see below, this persistence enables (a) automatic learning of critical features from history, and (b) a scalable workflow that provides up-to-date estimates.
Recall that the first challenge is obtaining the critical features for each session.
The persistence of critical features has a natural corollary that we can use to automatically learn them: Algorithm 3 formalizes this intuition for learning critical features.
Table 3 summarizes the notation used in Algorithm 3.
For each subset of features F (line 3), we compute the similarity between the quality distribution (D F ) of sessions matching on F and the quality distribution (D f inest ) of sessions matching on all features (line 7).
Then, we find the F that yields the maximum similarity (line 8-10), under one additional constraint that SimilarSessionSet(s, S, F, Δ) should include enough (by default, at least 10) sessions to get reliable quality estimation (line 4-5).
This check ensures that the algorithm will not simply return the set of all features.As an approximation of the duration in which criti- Next, we focus on reducing the update delay between when a quality measurement is received and used for prediction.Naively running critical feature learning and quality estimation of Algorithm 2 can be time-consuming, causing the predictions to rely on stale data.
In Figure 5(a), T CFL and T QE are the duration of critical feature learning and the duration of quality estimation, respectively.
The staleness of quality estimation (depicted in Figure 5) to respond to a prediction query can be as large as the total time of two steps (i.e., T CFL + T QE ), which typically is tens of minutes ( §6.3).
Also, simply using more parallel resources is not sufficient.
The time to learn critical features using Algorithm 3 grows linearly with the number of sessions under prediction, the number of history sessions, and the number of possible feature combinations.
Thus, the complexity of learning critical features T CFL is exponential in the number of features.
Given the current set of features, T CFL is on the scale of tens of minutes.To reduce update delay, we again leverage the persistence of critical features:Corollary 3.2: Persistence implies that critical features can be cached and reused over tens of minutes.Building on Corollary 3.2, we decouple the critical feature learning and quality estimation steps, and run them at separate timescales.
On the timescale of tens of minutes, we update the results of critical feature learning.
Then, on a faster timescale of tens of seconds, we update quality estimation using fresh data and the most recently learned critical features.This decoupling minimizes the impact of staleness on prediction accuracy.
Learning critical features on the timescale of tens of minutes is sufficiently fast as they persist on the same timescale.
Meanwhile, quality estimation can be updated every tens of seconds and makes predictions based on quality updates with sufficiently low staleness.
Thus, the staleness of quality estimation T QE of the decoupled workflow ( Figure 5(b)) is a magnitude lower than T QE + T CFL of the naive workflow (Fig- ure 5(a)).
In §6.3, we show that this workflow can retain the freshness of critical features and quality estimates.In addition, CFA has a natural property that two sessions sharing all feature values and occurring close in time will map to the same critical features.
Thus, instead of running the steps per-session, we can reduce the computation to the granularity of finest partitions, i.e., distinct values of all features.
Building on these insights, we create the following practical three-stage workflow of CFA.
• Stage I: Critical feature learning (line 1 of Algorithm 2) runs offline, say, every tens of minutes to an hour.
The output of this stage is a key-value table called critical feature function that maps all observed finest partitions to their critical features.
• Stage II: Quality estimation (line 2,3 of Algorithm 2) runs every tens of seconds for all observed finest partitions based on the most recent critical features learned in the first stage.
This outputs another key-value table called quality function that maps a finest partition to the quality estimation, by aggregating the most recent sessions with the corresponding critical features.
• Stage III: Real-time query/response.
Finally, we provide real-time query/response on the arrival of each client, operating at the millisecond timescale, by simply looking up the most recent precomputed value function from the previous stage.
These operations are simple and can be done very fast.Finally, instead of forcing all finest partition-level computations to run in every batch, we can do triggered recomputations of critical feature learning only when the observed prediction errors are high.
This section presents our implementation of CFA and highlights engineering solutions to address practical challenges in operational settings (e.g., avoiding bulk data loading and speeding up development iterations).
CFA's three stages are implemented in two different locations: a centralized backend cluster and geographically distributed frontend clusters as depicted in Figure 6.
Centralized backend: The critical feature learning and quality estimation stages are implemented in a backend cluster as periodic jobs.
By default, critical feature learning runs every 30 minutes, and quality estimation runs every minute.
A centralized backend is a natural choice because we need a global view of all quality measurements.
The quality function, once updated by the estimation step, is disseminated to distributed frontend clusters using Kafka [27].
Note that we can further reduce learning time using simple parallelization strategies.
Specifically, the critical features of different finest partitions can be learned independently.
Similarly in Algorithm 3, the similarity of quality distributions can be computed in parallel.
To exploit this data-level parallelism, we implement them as Spark jobs [4].
Distributed frontend: Real-time query/response and decision makers of CDN/bitrate are co-located in distributed frontend clusters that are closer to clients than the backend.
Each frontend cluster receives the quality function from the backend and caches it locally for fast prediction.
This reduces the latency of making decisions for clients.
Mitigating impact of bulk data loading: The backend cluster is shared and runs other delay-sensitive jobs; e.g., analytics queries from production teams.
Since the critical feature learning runs periodically and loads a large amount of data (≈30 GB), it creates spikes in the delays of other jobs (Figure 7).
To address this concern, we engineered a simple heuristic to evenly spread the data retrieval where we load a small piece of data every few minutes.
As Figure 7 shows, this reduces the spikes caused by bulk data loading in batch mode.
Note that this does not affect critical feature learning.
Iterative algorithm refinement: Some parameters (e.g., learning window size Δ learn ) of CFA require iterative tuning in a production environment.
However, one practical challenge is that the frontend-facing part of the backend can only be updated once every couple of weeks due to code release cycles.
Thus, rolling out new prediction algorithms may take several days and is a practical concern.
Fortunately, the decoupling between critical feature learning and quality estimation ( §4.2) means that changes to critical feature learning are confined to the backend cluster.
This enables us to rapidly refine and customize the CFA algorithm.
In this section, we show that:• CFA predicts video quality with 30% less error than competing machine learning algorithms ( §6.1).
• Using CFA-based prediction, we can improve video quality significantly; e.g., 32% less BufRatio, 12% higher AvgBitrate in a pilot deployment ( §6.2).
• CFA is responsive to client queries and makes predictions based on the most recent critical features and quality measurements ( §6.3).
We compare CFA with five alternative algorithms: three simple ML algorithms, Naive Bayes (NB), Decision Tree (DT), k-Nearest Neighbor (k-NN) 4 , and two heuristics which predict a session's quality by the average quality of other sessions from the same ASN (ASN) or matching the last-mile connection type (LH).
All algorithms use the same set of features listed in Table 1.
Ideally, we want to evaluate how accurately an algorithm can predict the quality of a given client on every choice of CDN and bitrate.
However, this is infeasible since each video client is assigned to only one CDN and bitrate at any time.
Thus, we can only evaluate the prediction accuracy over the observed CDN-bitrate choices, and we use the quality measured on these choices as the ground truth.
That said, this approach is still useful for doing a relative comparison across different algorithms.For AvgBitrate and JoinTime, we report relative error:|p−q| q , where the q is the ground truth and p is the prediction.
For BufRatio and JoinTime, which have more "step function" like effects [18], we report a slightly different measure called hit rate: how likely a session with good quality (i.e., BufRatio < 5%, VSF=0) or bad quality is correctly identified.
Figure 8 shows that for AvgBitrate and JoinTime, CFA has the lowest {5, 10, 50, 90}%th percentiles of prediction error and lower 95%th percentiles than most algorithms.
In particular, median error of CFA is about 30% lower than the best competing algorithm.
In terms of BufRatio and VSF, CFA significantly outperforms other algorithms in the hit rate of bad quality sessions.
The reason for hit rate of bad quality to be lower than that of good quality is that bad quality sessions are almost always less than good quality, which makes them hard to predict.
Note that accurately identifying sessions that have bad quality is crucial as they have the most room for improvement.
Pilot deployment: As a pilot deployment, we integrated CFA in a production system that provides a global video optimization service [20].
We deployed CFA on one major content provider and used it to optimize 150,000 sessions each day.
We ran an A/B test (where each algorithm was used on a random subset of clients) to evaluate the improvement of CFA over a baseline random decision maker, which many video optimization services use by default (modulo business arrangement like price) [9].
Table 4 compares CFA with the baseline random decision maker in terms of the mean BufRatio, AvgBitrate and a simple QoE model (QoE = −370 * Bu f Ratio + AvgBitrate/20), which was suggested by [33,18].
Over all sessions in the A/B testing, CFA shows an improve- ment in both BufRatio (32% reduction) and AvgBitrate (12.3% increase) compared to the baseline.
This shows that CFA is able to simultaneously optimize multiple (possibly conflicting) metrics.
To put these numbers in context, our conversation with domain experts confirmed that these improvements are significant for content providers and can potentially translate into substantial benefits in engagement and revenues [2].
CFA's superior performance and that CFA is more automated than the custom algorithm indicate that domain experts were willing to invest time running longer pilot.
Figure 9 provides more comparison and shows that CFA consistently outperforms the baseline over time and across different major cities in the US, connection types and CDNs.Trace-driven simulation: We complement this realworld deployment with a trace-driven simulation to simultaneously compare more algorithms over more quality metrics.
However, one key challenge is that it is hard to estimate the quality of a decision that was not used by a specific client in the trace.To address this problem, we use the counterfactual methodology from prior work in online recommendation systems [30,31].
Suppose we have quality measurements from a set of clients, where client c is assigned to a decision d rand (c) of CDN and bitrate at random.
Now, we have a new hypothetical algorithm that maps client c to d alg (c).
Then, we can evaluate the average quality of clients assigned to each decision d, {c|d alg (c) = d}, by the average quality of {c|d alg (c) = d, d rand (c) = d}.
Finally, the average quality of the new algorithm is the weighted sum of average quality of all decisions, where the weight of each decision is the fraction of sessions assigned to it.
This can be proved to be an unbiased (offline) estimate of d alg 's (online) performance [5].
5 For instance, if out of 1000 clients assigned to use Akamai and 500Kbps, 200 clients are assigned to this decision in the random assignment, then we can use the average quality of these 200 sessions as an unbiased estimate of the average quality of these 1000 sessions.
Fortunately, our dataset includes a (randomly chosen) portion of clients with randomized decision assignments (i.e., 5 One known limitation of this analysis is that it assumes the new assignments do not affect each decision's overall performance.
For instance, if we assign all sessions to one CDN, they may overload the CDN and so this CDN's quality in the random assignments is no longer useful.
Since this work only focuses on controlling traffic at a small scale relative to the total load on the CDN (and our experiments are in fact performed at such a scale), this methodology is still unbiased.
CDN and bitrate).
Thus, we only report improvements for these clients.
Figure 10 uses this counterfactual methodology and compares CFA with the best alternative from §6.1 for each quality metric and the baseline random decision maker (e.g., the best alternative of AvgBitrate is k-NN).
For each quality metric and prediction algorithm, the decision maker selects the CDN and bitrate that has the best predicted quality for each client.
For instance, the improvement of CFA over the baseline on VSF is 52% -this means the number of sessions with start failures is 52% less than when the baseline algorithm is used.
The fig- ures show that CFA outperforms the baseline algorithm by 15%-52%.
They also show that CFA outperforms the best prediction algorithms by 5%-17%.
Our implementation of CFA should (1) retain freshness to minimize the impact of staleness on prediction accuracy, and (2) be responsive to each prediction query.We begin by showing how fast each stage described in §4.2 needs to be refreshed.
Figure 11 shows the impact of staleness of critical features and quality values on the prediction accuracy of CFA.
First, critical features learned 30-60 minutes before prediction can still achieve similar accuracy as those learned 1 minute before prediction.
In contrast, quality estimation cannot be more than 10 minutes prior to when prediction is made (which corroborates the results of Figure 4(b)).
Thus, critical feature learning needs to be refreshed every 30-60 minutes and quality estimation should be refreshed at least every several minutes.
Finally, prediction queries need to be responded to within several milliseconds [20] (ignoring network delay between clients and servers).
Next, we benchmark the time to run each logical stage described in §4.2.
Real-time query/response runs in 4 geographically distributed data centers.
Critical feature learning and quality estimation run on two clusters of 32 cores.
Table 5 shows the time for running each stage and the timescale required to ensure freshness.
It confirms that the implementation of CFA is sufficient to ensure the freshness of results in each stage.
In addition to the predictive power, CFA also offers insights into the "structure" of video quality in the wild.
In this section, we focus on two questions: (1) of critical features are most common?
(2) What factors have significant impact on video quality?
Popular types of critical features: Figure 12 shows a breakdown of the fraction of sessions that are assigned to a specific type of critical feature set.
We show this for different quality metrics.
(Since we focus on a specific VoD provider, we do not consider the Site or LiveOrVoD for this analysis.)
Across all quality metrics, the most popular critical features are CDN, ASN and ConnectionType, which means video quality is greatly impacted by network conditions at the server (CDN), transit network (ASN), and last-mile connection (ConnectionType).
We also see interesting patterns unique to individual metrics.
City is among the top critical features of BufRatio.
This is perhaps because network congestion usually depends on the volume of concurrent viewers in a specific region.
Bitrate (initial bitrate) has a larger impact on AvgBitrate than on other metrics, since the videos in the dataset are mostly short content (2-5 minutes) and AvgBitrate is correlated with initial bitrate.
Finally, ContentName has a relatively large impact on failures (VSF) but not other metrics, because VSF is sometimes due to the requested content not being ready.
Distribution of types of critical features: While the quality of about 50% of sessions is impacted by 3-4 popular types of critical features, 15% of sessions are impacted by a diverse set of more than 30 types of critical feature (not shown).
This corroborates the need for expressive prediction models that handle the diverse factors affecting quality ( §2.2).
Next, we focus on the most prevalent feature values (e.g., a specific ASN or player lence of a feature value by the fraction of video sessions matching this feature value that have this feature as one of their critical features; e.g., the fraction of video sessions from Boston that have City as one of their critical features.
If a feature value has a large prevalence, then the quality of many sessions that have this feature value can be explained by this feature.We present the values of critical features with a prevalence higher than 50% for each quality metric and only consider a subset of the features (ASN, City, ContentName, ConnectionType) that appear prominently in Figure 12.
We present this analysis with two caveats.
First, due to proprietary concerns, we do not present the names of the entities, but focus on their characteristics.
Second, we cannot confirm some of our hypothesis as it involves other providers; as such, we intend this result to be illustrative rather than conclusive.
Table 6 presents some anecdotal examples we observed.
In terms of BufRatio, we see some of the major east coast cities (e.g., Boston, Baltimore) are more likely to be critical feature values than other smaller cities.
We also see both poor (Satellite, Mobile) and broadband (Cable) connection types have high prevalence on BufRatio and JoinTime.
This is because poor quality sessions are bottlenecked by poor connections, while some good quality sessions are explained by their broadband connections.
"Player" has a relatively large prevalence on AvgBitrate, because the content provider uses different bitrate levels for different players (Flash or iOS).
Finally, in terms of VSF, some small ISPs have large prevalence.
We speculate that this is because their peering relationships with major CDNs are not provisioned, so their video sessions have relatively high failure rates.
Internet video optimization: There is a large literature on measuring video quality in the wild (e.g., content popularity [38,55], quality issues [25] and server selection [51,47]) and techniques to improve user experience (e.g., bitrate adaptation algorithms [56,26,23], CDN optimization and federation [32,37,11,35] and crossprovider cooperation [57,19,24]).
Our work builds on insight from this prior work.
While a case for a similar vision is made in [33], our work gives a systematic and practical prediction system.
Global coordination platform: Decision making based on a global view is similar to other logically centralized control systems (e.g., [14,33,20,48]).
They examined the architectural issues of decoupling the control plane from the data plane, including scalability (e.g., [50,17]), fault tolerance (e.g., [36,54]), and use of big data systems (e.g., [20,4]).
In contrast, our work offers concrete algorithmic techniques over such a control platform [20] for video quality optimization.
Large-scale data analytics in system design: Many studies have applied data-driven techniques for performance diagnosis (e.g., [46,15,40]), revenue debugging (e.g., [13]), TCP throughput prediction (e.g., [22,34]), and tuning TCP parameters (e.g., [43,41]).
Recent studies try to operate these techniques at scale [16].
While CFA shares this data-driven approach, we exploit videospecific insights to achieve scalable and accurate prediction based on a global view of quality measurements.
QoE models: Prior work has shown correlations between various video quality metrics and user engagement (e.g., users are sensitive to BufRatio [18]), and built various QoE models (e.g., [28,45,12,10].
Our work focuses on improving QoE by predicting individual quality metrics, and can be combined with these QoE models.
Relationship to existing ML techniques: CFA is a domain-specific prediction system that outperforms some canonical ML algorithms ( §6.1).
We put CFA in the context of three types of ML algorithms.
• Multi-armed bandit algorithms [53] find the decision with the highest reward (i.e., best CDN and bitrate) from multiple choices.
They assume each decision has a fixed distribution of rewards, but the video quality of a CDN also depends on client-side features.
In contrast, contextual multi-armed bandit algorithms [44] assume the best decision depends on contextual information, but they require appropriate modeling between the context and decision space, to which critical features provide one viable approach.
• The feature selection problem [21] seems similar to critical feature learning, but with a key difference: critical features vary across video sessions.
Thus, techniques looking for features that are most important for all sessions are not directly applicable.
• Advanced ML algorithms today can handle highly complex models [29,42] efficiently, so in theory the critical features could be automatically identified, albeit in an implicit manner.
CFA uses existing ML models (specifically, the "variable kernel conditional density estimation" method [49]) and may be less accurate than advanced ML techniques, but CFA can predict with more recent data since it tolerates stale update on the critical features.
Furthermore, CFA is less opaque since it is based on domain-specific insights about critical features ( §7).
Finer grain information and selection: Currently, CFA makes predictions based on client side information only.
While clients provide accurate information regarding QoE, prediction can be much more accurate if CFA were to leverage finer-grained information from other entities in the ecosystem, including servers, caches and network paths.
Furthermore, CFA currently selects resources at the CDN granularity.
This means CFA cannot do much if the CDN redirects the client based on its location and the servers the CDN redirects the client to are congested.
However, if the client were able to specify the server to stream from, we could avoid the overloaded servers and improve quality.
Many prior research efforts posited that quality prediction could lead to improved QoE (e.g., [33,11,35,10]).
However, these efforts failed to provide a prescriptive solution that (a) is expressive enough to tackle the complex feature-quality relationships observed in the wild and (b) can provide near real-time quality estimates.
To this end, we developed CFA, a solution based on domain-specific insights that video quality is typically determined by a subset of critical features which tend to be persistent.
CFA leverages these insights to engineer an accurate algorithm that outperforms off-the-shelf machine learning approaches and lends itself to a scalable implementation that retains model freshness.
Using real deployments and trace-driven analyses, we showed that CFA achieves up to 30% improvement in prediction accuracy and 12-32% improvement in QoE over alternative approaches.
This paper would not be possible without the contribution of Conviva Stuff, especially Jibin Zhan, Faisal Zakaria Siddiqi and Rui Zhang.
The authors thank Siddhartha Sen for shepherding the paper and the NSDI reviewers for their feedback.
This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple Inc., Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm, Virdata and VMware.
Junchen Jiang was supported in part by NSF award CNS-1345305 and Juniper Networks Fellowship.
