We show how to reliably compute fast-growing functions with timed-arc Petri nets and data nets.
This construction provides ordinal-recursive lower bounds on the complexity of the main decidable properties (safety, termination, regular simulation, etc.) of these models.
Since these new lower bounds match the upper bounds that one can derive from wqo theory, they precisely characterise the computational power of these so-called "enriched" nets.
We call enriched nets a handful of Petri net extensions where tokens are coloured with data values, that still enjoy decidable verification problems: timed-arc Petri nets (TPN) where tokens carry real-valued clocks [6], data nets (DN) and Petri data nets (PDN) where they carry a datum from some dense domain [19], and constrained multiset rewriting systems (CMRS) where they carry positive integers [3].
Their richer structure makes enriched nets a natural choice when modelling for instance parameterised systems, protocols, workflows, or real-time systems-in fact, timed extensions of Petri nets have been in use since the 1970's for such modelling tasks.
In spite of the presence of two "sources" of infiniteness, the number of tokens and their colours, enriched nets can be handled by the now standard toolkit of well quasi-orders (wqo) and well-structured transition systems (WSTS) [5,14] so that e.g. safety-which in this context corresponds to the coverability problem-and other properties are decidable [4,2,9].
Recent investigations [1,7] have shown that all these formalisms are expressively equivalent, i.e. they define the same class of so-called coverability languages, and thus in particular their coverability problems are inter-reducible.
Their computational complexity, however, has rarely been analysed.
The employed wqo and WSTS techniques are generally seen as non-constructive, hence the aforementioned works do not provide any complexity analysis of the algorithms they propose ( [19, Prop. 3.2] gives a lower bound: PDNs can simulate lossy channel systems and hence inherit at least their F ω ω complexity [10], but this is far from optimal).
We prove in this paper that the complexity of enriched nets is exactly at level F ω ω ω in the fast-growing hierarchy.1) The upper bound is a consequence of a generic technique described in [22]: the length-function theorem for elementary wqos, here instantiated with (N k ) * as the underlying wqo.
It applies uniformly to DN, PDNs, TPNs, CMRSs (and to some further extensions); see Sec.
IV.2) The matching lower bound is our main contribution: it relies on the construction of PDNs with O(k) unbounded places that can compute in a weak sense the fast-growing functions F ω ω k and their inverses and therefore simulate F ω ω kbounded computations (see Sec.
V for an overview).
This construction relies on several intermediate steps: we first define in Sec.
VI-A a cumulative encoding of ordinals below ω ω ω k in sequences of vectors of integers (or "codes") along with rewriting rules over codes implementing fast-growing computations and their inverses (see Sec.
VI-B).
Because this encoding is robust, i.e. safe wrt.
Higman's ordering on codes, any weak implementation of the rewriting rules will yield the desired behaviour (Sec.
VI-C); in particular, a weak PDN implementation is possible, as shown in Sec.
VII.
Once established for PDNs, the new lower bound automatically applies to TPNs and DNs.
3) Beyond the complexity of verification problems, our techniques are easily applied to the study of the coverability languages of WSTS models [17].
Here our construction directly yields separation results; see Sec.
VIII.The details of the proofs and of the construction of PDNs can be found in the long version of the paper.
Although the complexity of decision problems in timedarc Petri nets provided our prime motivation-an important body of literature is dedicated to their analysis [6,4,2,9] and they are actually employed in tools [e.g. 18]-, we will work exclusively with Petri data nets, which proved easier to manipulate, and rely on known interreducibility results [7,1] to capture the other classes of enriched nets.
We denote by 0 the null vector in N k for any k, and for a word w = x 1 · · · x n we write |w| = n and w(i) = x i .
A Petri Data Net (PDN) is a Petri net where each token carries an identity from a linearly ordered and dense domain D.
A marking s of a PDN with P as set of places could be seen, e.g., as a multiset of pairs in D × P , or as a map s ∈ (N P ) D .
However, two key features of PDNs will guide our choice of (N |P | \ 0) * for representing markings:1) a marking s only has finitely many tokens, thus denoting d 1 < · · · < d m the identities that occur in s and gathering all the tokens that carry the same identity d i , one obtains a (non-null) place vector v i in N |P | \ 0: s can be written as a sequence(d 1 , v 1 ) · · · (d m , v m ),implicitly associating the null vector 0 with any d ∈ D \ {d 1 , . . . , d m }; 2) the concrete identities d i are irrelevant, and only their relative order is useful wrt. the dynamics of the net, thus s can safely be abstracted as the sequencev 1 · · · v m in (N |P | \ 0) * .
(Also the choice of the concrete set D is irrelevant.)
Every transition t of a PDN specifies a sequence of n ordered potential identities and for any such identity specifies the tokens cons to be consumed and prod to be produced.
Thus cons(t) and prod (t) are two sequences of n (possibly null) place vectors.Definition 1 (Petri Data Nets).
A k-dimensional Petri Data Net (k-PDN) is a tuple N = (P, T, cons, prod , s 0 ) where• P is a finite set of k = |P | places, • T is a finite set of transitions with P ∩ T = ∅, • for every t in T , cons(t) and prod (t) are finite sequences in (N k ) * with |cons(t)| = |prod (t)|, and• s 0 is an initial marking in (N k \ 0) * .
Consider now a marking s ∈ (N k \ 0) * .
In order to fire a transition t with |cons(t)| = n, nondeterministically select n identities, consume some of their tokens as indicated by cons(t), and produce new tokens with the identities specified by prod (t).
However, some of these n identities might not be present in s, and we should introduce null vectors wherever necessary:s ∈ (N k ) * is a 0-extension of s ∈ (N k \ 0) * (or s is the 0-contraction of s ) def⇔ s is obtained by removing all 0's from s .
Once an extension s is built, select in it a subword of n vectors x 1 , . . . , x n s.t. every vector contains enough tokens, i.e. with x i ≥ cons(t)(i).
If the condition is fulfilled, the corresponding tokens are consumed and prod (t)(i) is added to the resulting vector, yielding a new sequence s .
This s may contain null vectors, e.g. when all tokens with some identity have been consumed, hence the reached marking really is the 0-contraction of s .
Note that any way of firing t requires at most n insertions.
Examples of PDNs will be found in Section VII.Definition 2 (Semantics of PDNs).
The transition system associated with a k-PDN N = (P, T, cons, prod , s 0 ) is (S, s 0 , →) with state set S • there exists u 0 x 1 u 1 · · · u n−1 x n u n a 0-extension of s with for all i, u i ∈ (N k ) * and x i ∈ N k ; • for i in {1, . . . , n}, x i ≥ cons(t)(i);• and defining y i = x i − cons(t)(i) + prod (t)(i), s is the 0-contraction of u 0 y 1 u 1 · · · u n−1 y n u n .
Below we consider three decision problems for PDNs: (Strong) Coverability: Given N and p ∈ P , can we reach a configuration where p holds at least one token?
Boundedness: Given N , is the set of reachable configurations in S finite?
Termination: Given N , is every run finite?
A wqo (A, ≤) is a set A endowed with a transitive and reflexive relation ≤ s.t. every infinite sequence σ = a 0 , a 1 , · · · of elements of A contains a pair a i ≤ a j for some i < j.
Some classical examples of wqos areDickson's Lemma: (N k , ≤) with the product ordering defined byx ≤ y def ⇔ ∀0 ≤ j < k, x[j] ≤ y[j], Higman's Lemma: if (A, ≤) is a wqo, then (A * , ≤ * ) the setof finite sequences of elements of A along with the subword embedding ordering is also a wqo, where≤ * is defined by s ≤ * s def ⇔ s = a 1 · · · a n , s = x 0 b 1 x 1 · · · b n x n with x 0 , . . . , x n in A * and a i ≤ b i for all 1 ≤ i ≤ n.The transition system associated with a PDN (see Definition 2) is well-structured [5,14] 4 .
This (strict) compatibility of the transition relation with the ordering allows to employ generic algorithms for deciding coverability, boundedness, and termination.
In fact, the same generic WSTS algorithms show that coverability, boundedness and termination are decidable • for TPNs [2], even when extended with read arcs [9] or transport arcs [18], • for CMRSs [3], and • as far as coverability and termination are concerned, for DNs [19].
Compared to PDNs, these allow so-called whole-place operations that can e.g. duplicate or erase the whole contents of some places, and/or transfer them to other places, which makes their compatibility non-strictand indeed their boundedness problem is undecidable.
The enormous complexity of some decidable problems on WSTSs requires the introduction of complexity classes spanning way beyond the usual polynomial or exponential hierarchies.
The complexity classes we consider are generated by ordinal-indexed subrecursive hierarchies, like the Hardy hierarchy and the fast-growing hierarchy.
See [12] for a selfcontained presentation; we only recall below the notions and notations that are required for our construction in Section VI.It is well-known that any ordinal α < ε 0 can be written uniquely in Cantor Normal Form (CNF).
In this paper we use a dotted addition symbol " ˙ +" when we want to stress that an ordinal term is in CNF.
Thus, when we writeα = ω α1 ˙ + · · · ˙ + ω αp ,(1)we mean that not only the equality (1) holds, but also that α p ≤ . . . ≤ α 1 < α, as required by CNF.
(NB: we allow writing α ˙ + α when α or α is 0.)
Subrecursive hierarchies are defined through assignments of fundamental sequences (λ n ) n<ω for limit ordinals λ < ε 0 , verifying λ n < λ for all x and λ = sup n λ n .
A standard assignment is defined by:γ ˙ + ω α+1 n def = γ ˙ + ω α · ω n , γ ˙ + ω λ n def = γ ˙ + ω λn ,(2)together with ω n def = n. Writing Ω for the ordinal ω ω ω ω , this yields for instance Ω k = ω ω ω k and(Ω k ) k = ω ω ω k−1 ·k .
The Hardy hierarchy (H α : N → N) α<ε0 is defined by H 0 (n) def = n and H α+1 (n) def = H α (n + 1), H λ (n) def = H λn (n).
(3)Observe that H 1 is the successor function, and more generally H α is the αth iterate of the successor function, using diagonalisation to treat limit ordinals.
The fast growing hierarchy(F α : N → N) α<ε0 can be defined by F α def = H ω α , resulting in F 0 (n) = H 1 (n) = n + 1, F 1 (n) = H ω (n) = H n (n) = 2n, F 2 (n) = H ω 2 (n) = 2 n n being exponential, F 3 nonelementary, and F ω an Ackermannian function.By applying elementary closure operations to the collection of functions (F β ) β≤α along with the addition, projection and zero functions, one obtains a hierarchy (F α ) α known as the extended Grzegorczyck hierarchy [20], which characterises several natural classes of functions; for instance F 0 = F 1 is the class of linear functions, F 2 that of elementary ones, k∈N F k of primitive-recursive ones, and k∈N F ω k of multiply-recursive ones.
The hierarchy is strict for all 0 < α < α :F α F α , in particular because F α ∈ F α .
The (F α ) α hierarchy provides a more abstract packaging of the main stops in the extended Grzegorczyk hierarchy and requires lighter notation than the Hardy hierarchy (H α ) α .
However, with its tail-recursive definition, the Hardy hierarchy is easier to implement as a while-program or as a counter machine.
Below we weakly implement Hardy computations with PDNs.
Formally, a (forward) Hardy computation is a sequenceα 0 , n 0 − → α 1 , n 1 − → α 2 , n 2 − → · · · − → α , n of evaluation steps implementing Eq.
(3) seen as left-to-right rewrite rules.
It guarantees α 0 > α 1 > α 2 > · · · and n 0 ≤ n 1 ≤ n 2 ≤ · · · and keeps H αi (n i ) invariant.We say it is complete when α = 0 and then n = H α0 (n 0 ) (we also consider incomplete computations).
A backward Hardy computation is obtained by using Eq.
(3) as right-to-left rules.
For instance, Ω, k → Ω k , k → (Ω k ) k , k constitute the first three steps of the forward Hardy computation starting from Ω, k if k > 0.
A key insight for the complexity analysis of WSTS algorithms is that the use of wqos yields not only algorithm termination, but also upper complexity bounds:Theorem 3 (Upper Bound).
Coverability and termination for PDNs, DNs, TPNs, and CMRSs, are in F ω ω ω n + O(1) .
For this result, as explained in [13,22], we merely need to find out (1) what is the complexity of a step of the WSTS (in the case of PDNs, transitions perform simple affine operations in F 1 ), and (2) the maximal order type of the wqo, which is a measure of its complexity (this is Ω k for a k-PDN).
By the length function theorem for elementary wqos (see full version of [22]), we then obtain a parameterised upper bound in F ω ω k for the decision problems of k-PDNs mentioned in Section II-B, and a uniform F ω ω ω upper bound (which asymptotically majorises every function in k F ω ω k ) when the dimension is not fixed.These upper bounds hold more generally for k-DNs, as they have the same order type and their extra whole-place operations are still in F 1 .
Regarding TPNs and CMRSs, the F ω ω ω upper bound also holds; however here the main parameter in the parameterised complexity-which appears as the exponent on top of the tower of ω's-is not simply the dimension k but km where m is the maximal constant that appears in the constraints put on transitions or in the initial marking.
We now describe the proof plan for our main result.
PDNs are F ω ω ω -hard.This follows from a reduction from the halting problem for Minsky machines (MM) M with counters bounded by F ω ω ω (|M |).
The proof is done by assembling two constructions (described in the following sections).
The schematics (see Fig. 1) are similar to earlier constructions for lossy channel systems or counter machines and the reader can refer to [23,10] where more lower-level details are given.
We outline it as a motivation for the following sections.
1) Direct Computation: For a provided size k, we first construct (see Section VII) a PDN N H [k] initialised with a pair α 0 , n 0 def = (Ω k ) k , k and that tries to rewrite it α 0 , n 0 − → α 1 , n 1 − → · · · − → α , n in a way that reflects precisely the complete Hardy derivation issuing from α 0 , n 0 , thus computing n = H α0 (n 0 ) = H Ω (k).
There are two difficulties here.
First, one has to encode ordinals in sequences of vectors (i.e. in PDN configurations) and this is the topic of Section VI.
Secondly, our PDN only performs Hardy computations in a weak sense.
What is guaranteed is the following:N H [k] =(Ω k ) k ,k α 0 , n 0 r − → · · · r − → α i , n i r − → · · · r − → α , ≤H α 0 (n0) n p start p rewr p halt t rewr N H −1 [k] 0, m 0 r − → −1 · · · r − → −1 α, n p start p rewr p halt t rewr N M p start p halt simulate M using cpt as a budget cpt c M 1 c M rLemma 5 (See Section VII).
N H [k] is "complete": Starting with α 0 = (Ω k ) k , N H [k]can perform the exact Hardy computation and halt with α = 0 and n = H Ω (k).
N H [k] is "safe": Any halting computation in N H [k], correct or incorrect, has n ≤ H Ω (k).2) Simulation: Now consider some MM M of size k.
An easy (see [11, §7] or [21, §4] for emptiness: at this point, the value n f of cpt is necessarily ≤ n , and only equals n if c M 1 , . . . , c M r are empty (NB: if M halts, n f = n is indeed feasible).3) Inverse Computation: We now connect N H [k], N M and N H −1 [k] so that they run sequentially.
Here N H −1 [k] is a(Ω k ) k , k if m 0 = H Ω (k).
Here too, the PDN only computes H −1 in a weak sense but it is guaranteed that it can do exact backward computations (completeness) and that incorrect backward computations halting on (Ω k ) k , n have H (Ω k ) k (n) ≤ m 0 (safety).
As a consequence, the resulting full PDN started with (Ω k ) k , k can reach a configuration with p halt and a pair α, n that covers (Ω k ) k , k (in terms of the places that store the current Hardy pair) if, and only if, the Minsky machine M with space bounded by H Ω (k) = F ω ω ω (k) halts.Indeed, if M halts within the space bound, the PDN may reach the required p halt , α, n by chaining exact Hardy computations and the simulation of M by N M .
More interestingly, if the required α, n is reached, we know, letting hdef = H Ω (k), that n ≤ h (safety of N H ), that n ≥ n f = m 0 (budget of N M ), that H α (n) ≤ m 0 (safety of N H −1 ), and that H α (n) ≥ h (α, n covers (Ω k ) k , k).
Thus h ≤ H α (n) ≤ m 0 = n f ≤ n ≤ h.Necessarily n = n f , witnessing that M halts, and n = h, witnessing that M runs in space bounded by h = F ω ω ω (k).
In conclusion, the construction provides a (logspace) manyone reduction from the halting problem for Minsky machines running in space bounded by F ω ω ω (k) where k = |M | is the size of the MM description.
Using standard complexitytheoretical arguments, Theorem 4 (for Coverability) follows.
4) Termination: Regarding termination, a similar reduction works.
One makes sure that N H [k] always halts or deadlocks (it does) and stores two copies of n : one is a time budget that ensures the eventual halting-or-deadlock of N M , and the other witnesses n f = n as earlier.
In the end, the whole system has to eventually stop, unless it can cover (Ω k ) k , k with α, n, finally enabling an infinite loop.
This reduces the same MM problem to termination for PDNs.
More details can be found in [23, §7] where the same adaptation is done.Using the simulations of PDNs by TPNs of [7] and of DNs by CMRSs of [1, §5], we conclude:Corollary 6.
Coverability and Termination for TPNs, DNs, and CMRSs are F ω ω ω -hard.
We define in this section a so-called "cumulative" encoding of ordinals as codes (Section VI-A) and a rewriting system r − → operating on codes that performs Hardy computations (Section VI-B).
Its crucial property is its robustness, which entails that weak implementations, like the PDN implementation we present in Section VII, are correct (see Section VI-C).
Fix k ∈ N.
An ordinal < ω k is "small" and we use β, β , . . . to denote small ordinals; an ordinal < ω ω k is "medium" and we use α, α , . . . for such ordinals; finally, an ordinal < Ω k is "large" and we use π, π , . . . for such ordinals.
A medium ordinal can be written in CNF as α = ω β1 ˙ + · · · ˙ + ω βp where β 1 , . . . , β p are small ordinals, and a large ordinal can be written as π = ω α1 ˙ + · · · ˙ + ω αm where α 1 , . . . , α m are medium ordinals.We now introduce an encoding of large ordinals that will allow the computation of the Hardy functions with PDNs.
These data structures are 1) k-dimensional vectors in N k for small ordinals, 2) vector sequences in (N k ) * for medium ordinals, and 3) cumulative encodings in (N k {#}) * for large ordinals, where # is a fresh tally symbol.1) Small Ordinals as Vectors: For v ∈ N k and an index 0 ≤ i < k, let v[i] ∈ N denote the i-th component of v.
We use two different orderings over N k : the product ordering, denoted v ≤ v and the lexicographic ordering, denoted v ≤ lex v , with most significant component at index k − 1.
Recall that ≤ is a wqo, and that ≤ lex is a linearization of ≤.
With a vector v ∈ N k , we associate the small ordinalβ(v) def = ω k−1 · v[k − 1] ˙ + · · · ˙ + ω 0 · v[0] .
(4)This establishes a bijective correspondence between N k and small ordinals, and we write v(β) for β −1 (β).
v ≤ lex v iff β(v) ≤ β(v ) .
(5)2) Medium Ordinals as Vector Sequences: With a finite sequence V = v 1 v 2 · · · v p ∈ N k * , we associate the ordinal vp) .
(6) This surjective 1 embedding of N k * into ω ω k satisfies α(VV ) = α(V) + α(V ).
Write ε for the empty sequence in N k * .
Then α(V) = 0 iff V = ε, and α(V) = 1 iff V = 0.
α(V) = α(v 1 v 2 · · · v p ) def = ω β(v1) + · · · + ω β(Example 7.
Consider k = 2: α(1 0 ) = α(| 0 1 ) = ω β(10) = ω ω 0 ·1 = ω 1 = ω, thus α(| 0 1 | 0 1 ) = ω · 2, while α(2 × 1 0 ) = α(| 0 2 ) = ω ω 0 ·2 = ω 2 .
We order vector sequences with ≤ * , the sequence extension of ≤: it is a wqo since ≤ is.Wesay that V = v 1 · · · v p is pure if v 1 ≥ lex v 2 ≥ lex · · · ≥ lex v p :restricted to pure vector sequences, the embedding in (6) is bijective since the expression giving α(V) in Eq.
(6) is in CNF.
We write pure(V) for the only pure V such that α(V) = α(V ): one obtains pure(V) by removing inV = v 1 · · · v p any v i such that v i < lex v j for some j > i. Hence pure(V) ≤ * V.3) Cumulative Encodings for Large Ordinals: Fix a special tally symbol # and let N k# def = N k ∪{#}.
A cumulative ordinal description, or simple a "code", is a sequence x in N k # * .
Below we see them as sequences in [N k * #] * N k * , i.e. we single out the tally symbols and factor codes under the formx = V 1 #V 2 # · · · #V m #V rest ,(7)where the V i 's are vector sequences.
We extend ≤ * from vector sequences to codes in the natural way, by requiring that a # embeds into a #: this is still a wqo.
With x we associate a large ordinal π(x) via the followingπ(V 1 #V 2 # · · · #V m #V rest ) def = ω α(V1V2···Vm) ˙ + · · · ˙ + ω α(V1V2) ˙ + ω α(V1) .
(8)The above definition explains why codes are called cumulative.
One can also define π inductively byπ(V) = 0 , π(V#x) = ω α(V) · π(x) ˙ + ω α(V) .
(9)We say that x is pure if each V i , i = 1, . . . , m, is pure, and if in addition V rest = ε.
(NB: purity of, e.g., V 1 #V 2 #, does not guarantee purity of V 1 V 2 .)
For a code x, the unique pure x such that π(x) = π(x ), denoted x = pure(x), is given bypure(V 1 #V 2 # · · · #V m #V rest ) = pure(V 1 )#pure(V 2 )# · · · #pure(V m )#ε .
(10)Lemma 8.
x ≤ * x implies pure(x) ≤ * pure(x ).
Lemma 9 (Bijection).
Pure codes in N k # * and large ordinals in Ω k are in bijection by π.If we write V (x) for the vector sequence obtained by removing all tally symbols from x, i.e. the result of the projection # → ε applied to x, thenπ(x 1 #x 2 ) = π(pure(V (x 1 )x 2 )) ˙ + π(x 1 #) .
(11)Example 10.
Let k > 1; the initial Hardy computation step x 0 , k → x 1 , k with pure codes x 0 , x 1 is defined byx 0 def = (1 k−1 ) k # ; π(x 0 ) = (Ω k ) k ; x 1 def = (1 k−1 ) k−1 (1 k−2 ) k # ; π(x 1 ) = ω ω ω k−1 ·(k−1)+ω k−2 ·k .
Let us turn to the encoding of Hardy computations (below Ω k ) as rewriting rules on codes.
Such a system should e.g. map x 0 to x 1 in Example 10.
It turns out that the bulk of the task when computing Hardy functions lies in computing the elements in the fundamental sequences of limit ordinals.1) Limit Ordinals: Observe that a code denotes a successor ordinal if it is of the form #x, as indeed π(#x) = ω 0 · π(x) + ω 0 = π(x) + 1.
Conversely, a pure code of form Vv#x denotes a limit ordinal Vv) ) n .
We want to define a similar mapping (.)
n from codes to codes s.t. π (x) n = π(x) n ; this mapping essentially needs to treat the head Vv, which contributes the smallest term ω α(Vv) to the encoded ordinal.
Several cases arise depending on v:π(Vvx) ˙ + ω α(Vv) s.t. π(Vv#x) n = π(Vvx) ˙ + (ω α(• if v = 0, i.e. ω β(v) = 1, then ω α(V0) = ω α(V)+1 verifies (ω α(V0) ) n = ω α(V) · n, encoded through (V0) n def = V# n .
(12)Thus we verifyπ (V0#x) n def = π(V# n 0x)(13)= π(V0x) ˙ + ω α(V) · n = π(V0#x) n .
• if v = 0 then (ω α(Vv) ) n = ω α(V) ˙ +(ω β(v) )n (recall that Vv is pure) is encoded by (Vv) n def = V(v) n # ,(14)and we need to further distinguish two cases: let i ∈ {0, . . . , k − 1} be the smallest index with v[i] > 0.
Then β(v) is a successor ordinal if i = 0 and a limit ordinal otherwise, hence the definition(v) n def = (v − 1 0 ) n if i = 0, v − 1 i + n · 1 i−1 otherwise.
(15)Since every vector in the sequence (v) n is < lex v, this verifiesπ (Vv#x) n def = π(V(v) n #vx)(16)= π(V(v) n vx) ˙ + ω α(Vv) n = π(Vvx) ˙ + ω α(Vv) n .
The definitions (12-16) thus result for a pure Vv inπ (Vv#x) n = π (Vv) n vx = π(Vv#x) n .
(17)2) Rewriting System: We define a set of rewriting rules r − → working on pairs (x, n) of a code x and a number n ∈ N, that together encode an intermediate stage H π(x) (n) in the course of a Hardy computation.
Remark 13 (Purity is required).
A step x, n r − → y, m is not always correct when x is not pure: e.g. if k = 1, π(01#) = ω + ω ω = ω ω but 01#, n r − → 0 n+1 #, n, which encodes ω n+1 , andH ω ω (n) = H ω n (n) < H ω n+1 (n).
It is convenient to work with pure codes in proofs: the oneto-one correspondence between pure codes and ordinals in Ω k yields a one-to-one correspondence between a pair (x, n) and a snapshot of a Hardy computation H π(x) (n), allowing to transfer results from Hardy computations to r − →.
More importantly, note that Proposition 12 entails the correctness of r − → even when applied backwards: we capture both forward and backward Hardy computations with the same rewriting system.
r − → So far, our encoding of ordinals in Ω k and the rewriting system r − → can be seen as a (rather convoluted) way of performing forward and backward Hardy computations using sequences of vectors.
Their critical interest compared to more basic ordinal encodings is that r − → is robust: if instead of computing with x, n we first decrease the configuration in an uncontrolled way to some y, m with y ≤ * x and m ≤ n, we obtain a configuration that codes a smaller value H π(y) (m) ≤ H π(x) (n).
This result is subject to hygienic conditions on x, n and y, m; see Proposition 16 for the exact statement.Remark 14 (Non-Robustness of CNF).
Let us pause for a moment and consider a natural encoding χ of large ordinals.
In this encoding, we use the CNF of the ordinal and separate pure vector sequences with " ˙ +" symbols s.t. Vm) ; e.g. p = 1 ˙ +0 codes χ(p) = ω ω ˙ +ω for k = 1.
However, q = 10 verifies q ≤ * p and codes the much larger ordinal χ(q) = ω ω+1 , with H χ(p) (n) = H ω ω ˙ +ω (n) < H ω ω ·(n−1) ˙ +ω n (n) = H χ(q) (n) when n > 0.
By contrast, with cumulative codes, "losing" a tally symbol results in the loss of Proposition 16 (Robustness).
Let x, x be pure codes and n > 0.
If x is n -trim and x, n ≤ * x , n , thenχ(V 1 ˙ + · · · ˙ + V m ) def = ω α(V1) ˙ +· · · ˙ +ω α(H π(x) (n) ≤ H π(x ) (n ).
b) Weak Implementations: The efforts put into defining a robust computation for the Hardy functions pay when one tries to implement them in a "weak" model like PDNs, as we do in Section VII-but this could also be used in other models.
By a weak implementation, we mean-as usual in the Petri net literature-an implementation that guarantees 1) completeness: it includes the desired behaviour, and 2) safety: it might also yield "smaller" results.
In the case at hand, we provide sufficient conditions (see Definition 18) for two relations 1) For any n 0 -trim x 0 , x 0 , n 0 d − → * ε, H π(x0) (n 0 ) and ε, H π(x0) (n 0 ) b − → * x 0 , n 0 .
2) If x 0 is n 0 -trim and x 0 , n 0 d − → * ε, n then n ≤ H π(x0) (n 0 ),and if ε, m b − → * x, n, then H π(x) (n) ≤ m.Note that these are exactly the two properties required in the main proof of Section V from the PDNs N H [k] and N H −1 [k].
Here are our sufficient conditions:Definition 18 (Weakenings).
A relation d − → on codes is a weakening of r − → if rt − → ⊆ d − → ⊆ ≥ * ; trim − − →; rt − →; ≥ * ; trim − − →.
Similarly, a relation b − → is a weakening of r − → −1 if rt − → −1 ⊆ b − → ⊆ ≥ * ; trim − − →; rt − → −1 ; ≥ * ; trim − − →.
Proof of Theorem 17: For (1), by Lemma 15, for an n 0 -trimx 0 , x 0 , n 0 r − → * ε, H π(x0) (n) implies x 0 , n 0 rt − → * ε, H π(x0) (n) and ε, H π(x0) (n) ( rt − → −1 ) * x 0 , n 0 since ε is H π(x0) (n)-trim.
For (2), we reconstruct step by step pieces of a computation of rt − → or rt − → −1 .
For d − →, if x, n is trim and x, n ≥ * x , n trim − − → x , n rt − → y , m ≥ * y , m trim − − → y, m, then H π(x) (n) ≥ H π(x ) (n ) (by Prop. 16) = H π(y ) (m ) (by Prop. 12) ≥ H π(y) (m) ,(H π(y) (m) ≥ H π(y ) (m ) (by Prop. 16) = H π(x ) (n ) (by Prop. 12) ≥ H π(x) (n) , (by Prop. 16)and we proceed again by induction.
We explain in this section how to construct N H and N H −1 , the PDNs that we announced and used in Section V.
They transform pairs x, n via a relation What makes PDNs relatively powerful is that they can make weak copies of a counter and even of a sequence, and they can use these weak copies for bounding the number of times a loop is executed ("weak control").
We designed codes and robustness precisely to fit this weak computational power.In the rest of this section we explain how codes are represented in a PDN (Section VII-A) and how to perform trimming.
Due to lack of space, the definitions and the implementation of The weak implementation of Hardy computations has to maintain a PDN representation of a code/counter pair x, n.1) Counter: The counter n is represented via two places cpt and cpt.id.
Place cpt.id is an identity place for relevant tokens: the current value of the counter will be the number of tokens in cpt whose identity match cpt.id.2) Code: For a code x of length l, distinct identities I 1 < . . . < I l identify each item in x. Every item of the code is identified by a unique identity, and the ordering of identities lets one recover the code.
All the identities that have been used for items of the current and past codes are stored in two places, vect and tally, letting one distinguish between vector and # occurrences in x; note that each # occurrence has a different identity.
The representation of a vector v identified by some I in a code is done via places c 0 , . . . , c k−1 : v[i] is the number of tokens in c i with identity I.Identities evolve during a computation.
In order to prevent tokens with now irrelevant identities from disturbing the computation, N H uses two identity places, low and high.
We make sure that at any time each of these two places contains a single token, and we just write low or high to denote the identity carried by that single token.
Initially, one has cpt.id < low < high and the identities I 1 , . . . , I l for the (current) code are exactly those with low < I < high; other identities are irrelevant for x.When simulating an r − → step (and except in simple cases), cpt.id is decreased, high is increased and low is set to the previous value of high.
Thus tokens with (now) irrelevant identities will never match the current value of cpt.id nor belong to the open interval (low, high).
In most cases, r − → requires that we iterate some operation at most n times (or n−1, or ...) where n is the current value of the counter.
In N H this is systematically done in a modular way by first duplicating the counter and then consuming the tokens of the duplicate, thus controlling the number of iterations.For this, N H uses two places, dpt and dpt.id, where it stores duplicates of the tokens in cpt and cpt.id.
The net of Fig. 2 depicts the duplication.
2 Transition dp 1 performs the identity updating: cpt.id acquires a smaller identity C < C while dpt.id is updated with the previous identity of cpt.id, namely C.
Then transition dp 2 transfers the tokens of cpt (corresponding to the previous identity of the counter D) both to the original counter and to the duplicated counter.
Transition 2 We rely on the standard graphical depiction of enriched nets and use (pictures of) Petri nets where arcs connected to a transition t are labelled with bags of variables that must be instantiated by ordered identities.
The number of these variables is exactly |cons(t)| and the ordering of the corresponding identities is carried by the transition.
For concision and readability, it is convenient to allow orderings of the variables that are not total: this stands for all possible linearizations.
We also use graphical conventions for better readability: control places containing black tokens are greyed or filled somehow, identity places containing at most one token per identity are represented by simple circles, and the other places, used for counters or general storage, are represented by double circles.
... Fig. 3.
Copying the first vector (case i).
dp 3 stops the process, and is slightly modified if we need to put n−1 rather than n in dpt.
(In order to avoid a special case for the first duplication, the initial marking has dpt empty and dpt.id with the same identity as cpt.id.)
This simple mechanism must be refined for the loops in the trimming process (see below) where the value of n is used to control that every component of a vector in the code is ≤ (some value related to) n.
Here one cannot just iterate the previous mechanism: since every duplication possibly decreases n and could violate the property already established for previous components.
A more elaborate implementation is required: N H uses a second auxiliary counter ept and ept.id (initialised using dpt and dpt.id) for such multiple controls (as in Fig. 3).
In order to avoid a clash of identities for counters, at every initialisation of ept, the new identity of dpt.id, namely D is selected by the guard C < D < D where D and C are the current identities of dpt.id and cpt.
During most weak rule applications, a trimming is performed on-the-fly while the exact rule is simulated, i.e. we actually weakly implement rt − → and its inverse.
This trimming consists in implementing ≥ * ; trim − − → during the selection and copy of the rule left-hand side and is simultaneously ensured from the rule right-hand side: it turns a configuration t, n into another one t , n ≤ * t, n which is trim and pure.
• N H first duplicates the counter cpt, yielding a new value n ≤ n (see Fig. 2).
Below we assume that this stage is already passed.
• N H scans (in increasing order) relevant identities (the ones in vect or sharp, between low and high), purifies the code and copies it beyond high as we explain.
• It purifies, one at a time, sequences separated by #s.• When copying a vector sequence, the first vector is copied but also duplicated in auxiliary places d 0 , . . . , Let us detail how this is controlled.
N H uses three additional identity places: from, to and with.
The current item's identity is from, its copy after trimming has the new identity to, and the purification of a vector requires comparisons with the previous vector in the sequence, whose identity is recorded in with, letting one select the appropriate tokens in d 0 , . . . , d k−1 .
Fig. 4 describes the overall control of this process, started by beg.pur, looping, and concluded by end.pur.
The body of the loop copies one vector sequence followed by a #.
If non empty, the sequence has just one vector, or more, requiring two different treatments.
For readability, the labeling of the crucial transitions is specified in the lower part of the picture:• At start, beg.pur produces identity tokens in from and to within the appropriate intervals (wrt. low and high), guessing the identity of an item to be copied.
• When the treatment ends, end.pur updates low and high to their new value.
• After copying the first vector, efirst guesses a new identity (to be copied) in from and a new (target) identity in to, while recording the current identity in with.
• After copying a remaining vector, erem guesses fresh identities from and to, and updates the recorded identity in with.
• csharp copies a # symbol, consuming a token in sharp with identity from and producing a token with identity to (while updating from and to as usual).
Observe that a bad guess in from can lead to deadlock but no infinite looping is possible (as required by the proof of Theorem 4).2) Copy of a Vector Sequence: a) First Vector: First, in order to guarantee a trim representation, the copy of the first vector non deterministically selects a component i, which is allowed to be less than or equal to n.
The rest of the process is depicted in Fig. 3 consists for j > i in:• setting the auxiliary counter ept to dpt;• "updating" tokens in place c j from identity F to identity n, and at the same time in memorising the transferred tokens in place d j for j > i. With the help of the counter ept, at most n − 1 tokens are transferred.
This is performed by transition ts j .
We then perform the same transfer for component i, but allow one more token thanks to the firing of transition ts i .
No token is transferred for any component j < i. b) Remaining Vectors: For the sake of readability, we do not represent the management of trimming, which is performed as with the first vector, but rather focus on the purity of the vector sequence.Let us call v the vector to be copied (identified by variable F), v the last vector that has been copied (identified by W) and v ≤ v the vector to be copied (identified by T).
In order for v to be lexicographically smaller than v it must satisfy:• either for all i, v (i) ≤ v (i) • or there exists some i s.t. for all j > i, v (i) ≤ v (i) and v (i) ≤ v (i) − 1.
Then the simulation non deterministically selects one of these cases.
The purity check is thus largely similar to the trimming one: copying is limited by some values, depending on W and cpt.Observe that one of the possible results of weak trimming is (exact) trimming of the code, and that the other ones are trimmings of a weaker code.
Well-structured transition systems can be seen as language acceptors (or generators).
For M a class of WSTS models, e.g. M = the Data Nets, let L(M) be the class {L(M ) | M ∈ M} of languages (nondeterministically) accepted by systems in M when their transitions carry labels, possibly ε, over some alphabet, and when the set of "final", or "accepting", states is upward-closed.
Geeraerts et al. [17] shows convincingly that this notion of well-structured languages (WSL), also called coverability languages, is most relevant.A series of recent papers (see [17,1,8] and the references therein) successfully use WSLs as a tool for comparing the descriptive power of varied WSTS models, showing equivalence, e.g. of PDNs and TPNs, or, separating them from the less expressive LCSs (lossy channel systems) or APNs (affine Petri nets [15]).
It turns out that the simulation we develop in this paper (and the matching complexity upper bounds) leads to a (relative but) precise characterisation: Let L 0 = {w# n | n = |w|} collect all words (over a two-letter alphabet) equipped with a length witness: L 0 ⊆ (a + b) * # * is deterministic context-free.
Theorem 19.
1) L ∈ L(PDN)(= L(TPN) = L(DN)) implies L ∈ k∈N TIME(F ω ω k (n)).
2) L ∈ k∈N TIME(F ω ω k (n)) implies L ∩ L 0 ∈ L(PDN).
The proof relies on the possibility of simulating a spacebounded MM.
Using the simulations in [10,23] and the upper bounds in [13,22] we derive in a similar way: to nets with at most k places (resp., to channel systems with a k-letter internal message alphabet).
These first separation results are not stronger than those of [1,8], but they provide a standard measure (using Turing, or equivalently Minsky, machines) rather than a myriad of relative ones.Theorem 20.
For any L ⊆ L 0 : 1) L ∈ k∈N TIME(F ω k (n)) iff L ∈ L(LCS).
2) L ∈ k∈N TIME(F k (n)) iff L ∈ L(APN).
IX.
CONCLUDING REMARKS Theorems 3 and 4 close the open question of the complexity of decision problems over the family of "enriched" nets (our terminology), and have immediate consequences, e.g. for separating various WSTS models according to their computational power.
Interestingly, we are not aware of any other natural decision problem sitting exactly at level F ω ω ω [16], which makes of enriched net problems the canonical examples for this complexity class.Our main technical contribution is the robust encoding in (N k * , ≤ * ) of ordinals in Ω k , together with rewrite rules that describe Hardy computations.
Enriched nets are not the only computational model in which these rules can be weakly implemented, and one may use them for proving complexity lower bounds in other settings.Finally, let us mention two questions raised by this work: 1) Can one improve on Theorem 20?
We would prefer an exact characterisation of L(PDN), not relatively to L 0 .
2) What about ν-Petri nets [21] and unordered PDNs?
The underlying wqo is simpler than N k * , hence we expect lower complexities.
Work supported by ANR grant 11-BS02-001-01 and by the Leverhulme Trust.
The third author is currently visiting the Computer Science Department at Oxford University.
