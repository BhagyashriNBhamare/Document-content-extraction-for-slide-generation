Traffic to a Web site can vary dramatically.
At the same time it is highly desirable that a Web site is reactive.
To provide crisp interaction on thin clients, 150 milliseconds has been suggested as an upper bound on response time.
Unfortunately, the popular Apache Web server is limited in its capabilities to be reactive under varying traffic.
To address this problem, we design in this paper an adaptive controller for the Apache Web server.
A modified recursive least squares algorithm is used to identify system dynamics and a minimum degree pole placement controller is implemented to adjust the maximum number of concurrent connections.
Experimentally, we show that the controller effectively regulates the reply time of HTTP connection requests, and hence provides reactive response, by limiting the maximum number of connections accepted by an Apache Web server.
1 Background Abrupt spikes in the usage of certain phrases ("lipstick on a pig," "our entire economy is in danger," "who is the real Barack Obama") in the daily news cycle have been observed [14].
Corresponding to such spikes, as well as other time-varying phenomena, traffic to a Web site can vary dramatically.
In spite of such varying traffic, it is highly desirable that a Web site is reactive.
To provide a crisp interactive experience on thin Web clients, 150 milliseconds has been recommended as an upper bound on response time [22].
In fact, quick and reasonably predictable user response is essential in all computer systems , not only Web services.
One way to ensure quick and predictable user response is through load balancing [7].
A single server's ability to service clients is bound by multiple factors, including CPU utilization, memory capacity, bandwidth capacity , and I/O rate.
Load balancing at the cluster level allows connections to be distributed to servers with the least load.
Typical implementations of load balancing deal with round robin balancing for domain name service (DNS) [7].
Much of existing frameworks and previous research deal with inter-server connections and balancing , with little or no focus on dynamic parameter adjustment of individual servers.
Focusing on an individual Web server rather than a cluster of machines, we investigate in this paper an alternative approach to ensuring quick and predictable user response, namely feedback control and specifically Minimum Degree Pole Placement control [5].
This can be in an environment where inter-server load balancing is already in place, or when a single server is handling Web traffic.
Our interest is in the ability to manipulate parameters online to control server load in order to maximize the efficiency of each server individually under varied and changing conditions.
This paper uses the Apache Web server, 1 a popular open source package [13].
Apache consists of a structured pool of workers, each its own process.
2 A master process, the Apache daemon, listens for requests and delegates HTTP communications to the worker processes.
To control the rate of incoming connections, an Apache server administrator can adjust two primary settings: MaxClients (MC) and KeepAlive (KA).
Depending on server load and Web activity, the Apache daemon maintains up to MC worker processes, which can in one of three states at any time: busy, idle, or thinking.
Busy workers are currently processing a request and awaiting a reply; thinking workers are waiting for an HTTP request after an established connection; and idle workers are waiting for a new connection request.
The KA parameter determines how long workers maintain their current connection with a client before terminating the connection (i.e. changing their state from thinking to idle).
Apache 2.2 has default settings of MC = 150 worker processes and KA = 5 seconds.
1 http://httpd.apache.org/ 2 This is on a Unix-based system.
Apache on Windows, which we do not investigate here, is different.
Abrupt spikes in the usage of certain phrases ("lipstick on a pig," "our entire economy is in danger," "who is the real Barack Obama") in the daily news cycle have been observed [14].
Corresponding to such spikes, as well as other time-varying phenomena, traffic to a Web site can vary dramatically.
In spite of such varying traffic, it is highly desirable that a Web site is reactive.
To provide a crisp interactive experience on thin Web clients, 150 milliseconds has been recommended as an upper bound on response time [22].
In fact, quick and reasonably predictable user response is essential in all computer systems, not only Web services.One way to ensure quick and predictable user response is through load balancing [7].
A single server's ability to service clients is bound by multiple factors, including CPU utilization, memory capacity, bandwidth capacity, and I/O rate.
Load balancing at the cluster level allows connections to be distributed to servers with the least load.
Typical implementations of load balancing deal with round robin balancing for domain name service (DNS) [7].
Much of existing frameworks and previous research deal with inter-server connections and balancing, with little or no focus on dynamic parameter adjustment of individual servers.Focusing on an individual Web server rather than a cluster of machines, we investigate in this paper an alternative approach to ensuring quick and predictable user response, namely feedback control and specifically Minimum Degree Pole Placement control [5].
This can be in an environment where inter-server load balancing is already in place, or when a single server is handling Web traffic.
Our interest is in the ability to manipulate parameters online to control server load in order to maximize the efficiency of each server individually under varied and changing conditions.
This paper uses the Apache Web server, 1 a popular open source package [13].
Apache consists of a structured pool of workers, each its own process.
2 A master process, the Apache daemon, listens for requests and delegates HTTP communications to the worker processes.
To control the rate of incoming connections, an Apache server administrator can adjust two primary settings: MaxClients (MC) and KeepAlive (KA).
Depending on server load and Web activity, the Apache daemon maintains up to MC worker processes, which can in one of three states at any time: busy, idle, or thinking.
Busy workers are currently processing a request and awaiting a reply; thinking workers are waiting for an HTTP request after an established connection; and idle workers are waiting for a new connection request.
The KA parameter determines how long workers maintain their current connection with a client before terminating the connection (i.e. changing their state from thinking to idle).
Apache 2.2 has default settings of MC = 150 worker processes and KA = 5 seconds.
This paper uses performance from the user's perspective as a metric and input to the controller, rather than CPU-and memory-based metrics.
In contrast to previous research (Section 2), our experiments include metrics such as time to connect, reply time, and requests per second.
With these in mind, we derive a model in real-time and adaptive controller (Section 3).
We then perform preliminary server benchmarking, an open loop simulation, and lastly test the controller using mean HTTP reply time as a setpoint (Section 4).
While we use reply time as the output modeled by the system, this approach can be applied using any other metric collected by the server, such as throughput, connection error frequency, or mean time until a connection is established.
Apache Web server load has previously been modeled as a linear multiple-input multiple-output (MIMO) system [8], using input parameters KA and MC and output parameters memory and CPU usage.
This MIMO model expands on previous work of single-input single-output (SISO) controllers in computing systems -in particular for congestion control [10] and controlling Web server delay [16].
Robertsson et al. used a PI controller and nonlinear model with simulations using Matlab [20].
Abdelzaher et al. explored average requests delay as a setpoint using a feedforward predictor [1][2][3].
Another SISO controller using KA and MC as parameters has been researched [11].
Notably, the authors concluded that SISO is insufficient to obtain accurate models for CPU, as KA and MC are not independent.
The use of CPU, memory, KA , and MC was further investigated by Hellerstein and Diao, along with a queueing model to predict server response time [9,12].
They proposed a MIMO implementation able to capture the interactions of KA and MC for modeling memory and CPU usage [11].
To control KA and MC, multiple SISO controllers (with separate controls loops for both KA and MC) were very effective when compared to a single MIMO controller.
Also of interest is their averaging of data points over time before performing controller calculations.A simple, first order linear time invariant MIMO model was created [8] using inputs KA, MC and outputs CPU, MEM:CPU k+1 MEM k+1 = A CPU k MEM k + B KA k MC k ,(1)where A, B ∈ R 2×2 were estimated via least squares regression and k represents a discrete time interval.
Various methods of traffic generation for testing performance of Web servers have also been previously researched.
A closed source traffic generator termed WAGON (Web Traffic Generator and Benchmark) [15] was used to simulate user generated traffic and varying amounts of I/O, memory, and CPU intensive traffic [8].
Open source Web traffic generators include Apache Benchmark and Httperf.
Httperf, which we adopt, has been shown to be effective in measuring Web server performance as well as mimicking user behavior [18].
The underlying control approach taken is the minimum degree pole placement (MDPP) control design proposed by Astrom [5].
In this approach, feed-forward and feedback controllers are designed to force the open-loop system to follow a reference model.
In the ideal case, model following is achieved perfectly.
However, due to parametric uncertainty, this is not achieved in practice.
This is the motivation to introduce reconfiguration to the control problem.
During a change in plant dynamics, the control effectiveness changes, requiring a larger or smaller actuator signal to maintain desired performance.
There are several approaches to the modeling of computation for control applications.
Here, we use linear Auto-Regressive modeling with eXogenous input or linear ARX.
Nonlinear approaches, such as discrete time neural networks, may also be used.
Nonlinear modeling is more complex yet may be able to capture inherent nonlinear behavior otherwise unaccounted for in ARX modeling.
On the other hand, linear modeling is generally simpler to understand and implement.We will assume a finite class of plants P (i) for i ∈ [1, M]; the integer i denotes a specific plant or device, such as a particular server type or configuration.
Suppose the plant is described by an ARX model with an additive noise term.
For simplicity, we assume Gaussian noise.
Denote P (i) , y (i) (k), u(k), and η(k), the plant operator, scalar output, input, and noise at sample time k, respectively.The relationship between these quantities is given by:y (i) (k) = P (i) u(k) = φ T (k − d)θ (i) + η(k)(2)where φ T (k − d) denotes the regression vector and consists of a tapped delay line of input and output measurements, and θ (i) denotes a vector of parameters corresponding to the ith plant.
A number of methods exist to estimate θ (i) in both batch and on-line modes.
Similar ARX models have been used in modeling a number of digital processes [11].
In the following we will drop the superscript (i) from the plant parameter vector.
The purpose of the superscript was to indicate that there exists a family of (unknown) parameter vectors that adequately model various plant scenarios described above.
The purpose of the system identification algorithm described below is to track and identify these parameter vectors that may change gradually or abruptly during online control.
The reconfigurable control approach estimates θ online and then uses the estimates to update a control law (see Section 3.2) below.
To estimate θ , it is typical to use the recursive least squares (RLS) method, in which we minimize the cost functionJ = N ∑ k=1 λ N−k e 2 (k),(3)where e(k) is the error between the true and estimated outputs, and λ ∈ (0, 1] is the forgetting factor.
The RLS method can lead to two problems when attempting to track varying parameters.
First, a small forgetting factor, needed to track fast or abrupt parameter variations can cause a large covariance matrix which could lead to covariance "blow up".
Second, as the forgetting factor is decreased, the size of the data window gets smaller and it is more likely that there will exist data collinearities within the data window.
The modified sequential least squares (MSLS) algorithm, which prevents singularities in the covariance matrix [17], has been proposed to deal with this issue.
This is due to the reformulation of the least squares problem.
The cost function to be minimized includes additional penalties on changes in the parameter values in the form of temporal and spatial constraints.
Weighting matrices are included to adjust the extent of penalization on each parameter variation.
For this case, we will define the cost function asJ = t ∑ k=1 y(k) − x T (k)θ (t) 2 λ t−k + mθ (t) − θ (t − 1) 2where θ (t) corresponds to the parameter estimates at time t. 3 Setting dJ dθ = 0, we get the solutionθ (t) = X T ˜ X + mI −1 mθ (t − 1) + ˜ X T y(t) ,whereX T = x(1) x(2) . . . x(t) ˜ X T = x(1)λ t−1 x(2)λ t−2 . . . x(t) y T (t) = y(1) y(2) . . . y(t) .
The covariance matrix in this algorithm is nowP(t) = X T ˜ X + mI −1 .
A recursive version, derived by Bodson [6], is given asθ (t + 1) = θ (t) + P(t + 1)x(t + 1) y(t + 1) − x T (t + 1)θ (t) +mλ P(t + 1) (θ (t) − θ (t − 1)) .
In conventional RLS with forgetting factor, it is typical to use the Matrix Inversion Lemma (MIL) to calculate P(t + 1).
However, due to the additional penalty term, the MIL leads to the inversion of a (1 + m) × (1 + m) matrix, where m is the number of unknown parameters.
Thus, the MIL does not help in this case, and hence, we opt to directly take the inverse of P(t + 1).
4 Upon a system or configuration change we would like to adapt our controller when the parameter estimates converge.
The convergence time may vary depending on factors such as input excitation.
We decide to monitor the posterior prediction errorPrediction Error = y(t) − x T (t)θ (t − 1) .
(4)A tolerance is defined, and we adapt our controller when Prediction Error > Tolerance.
We base our minimum degree pole placement (MDPP) controller on previous work [5].
For MDPP, let H(q) be the transfer function of the plant, where q is the forward shift operator, and A m (q), B m (q) be the reference model denominator and numerator respectively:H(q) = b 0 q + b 1 q 2 + a 1 q + a 2 .
The steady state is given by:B m (1) A m (1) = B m (b 0 + b 1 ) 1 + a m 1 + a m 2 = 1 B m = 1 + a m 1 + a m 2 b 0 + b 1 ,where A m 1 and A m 2 are chosen parameters.
Denote A, R, S, T and B = B + B − to be polynomials in q where B + is a monic polynomial with stable zeros and B − corresponds to the unstable factors.
The Diophantine equation, or closed-loop characteristic polynomial, is defined by AR + B − S = A 0 A m .
Let R = R B + = R. Then, Deriving our controller u, we have:(q 2 + a 1 q + a 2 )R + (b 0 q + b 1 )S = A 0 (q 2 + A m 1 q + A m 2 ).
R = q + r 1 = R (5) S = s 0 q + s 1 (6) A 0 = q + a 0 (q 2 + a 1 q + a 2 )(q + r 1 ) + (b 0 q + b 1 )(s 0 q + s 1 ) = A 0 (q 2 + a m 1 q + a m 2 )Lastly, we can compute T :T = A 0 B m = (q + a 0 )B m(7)Using Eq.
5, 6, and 7 to calculate R, T , we can now compute: u = T R u c − S R y, the control signal from our control law.
A more thorough derivation of Eq.
7 and this specific controller with equivalent zero cancellation is given by Reed et al. [19].
These experiments will measure the Apache Web server under varying numbers of connections, introduce an ARX model, and test an adaptive MDPP controller for setting the maximum number of concurrent connections.
In our simulations, we used Apache v2.2 (the latest stable release at the time of writing) running on a Linux kernel 3.0.0-14 x64 workstation with a dual core Intel T2400 and 2GB of RAM.
This workstation acted as the server and hosted PHP code simulating an active blog.
The Apache Web server parameters were kept at their default settings.
A second workstation of similar hardware specifications, the client, sent HTTP requests concurrently to the server to simulate Web traffic.
The client resided on the same LAN as the server.
Figure 1 shows the interactions between the client and the server.
For a single HTTP request to the server (e.g. fetching index.html), we collected the following statistics: time until initial TCP connection established (T c ), time until request response (T R ), time for the request transfer/reply (T T ), as well as several other metrics not shown here due to space.
To generate HTTP traffic and simulate a simple user interaction with the client, we used Httperf.The client proceeded to generate Web traffic by queueing a list of α TCP connection requests to the server.
After a connection was established, the client sent k HTTP requests to a random HTML, Javascript, or stylesheet (CSS) resource, each with probability 1 3 .
A single HTTP request typically resulted in a 80KB reply by the server (80KB being the mean size of the three potentially requested resources).
Since a single connection cannot handle concurrent HTTP requests, the k HTTP requests were performed sequentially; immediately after a resource was received by the client, the next resource was requested.
After the connection's k requests completed, the connection was terminated by the client.
Up to α connections were created concurrently by the client.
Requesting multiple resources per connection leverages a persistent TCP connection depending on the request frequency and KA setting.
On the server side, we denote c(t) to be the number of concurrent connections at time t and n c to be the maximum number of concurrent connections set by the server.
When n c connections are active in the server, all additional connection requests were ignored until c(t) decreases.
In this case, the client waited indefinitely until the server had capacity, i.e. c(t) < n c (rather than terminate the connection request after a certain amount of time).
Once a connection was established, a connection failure could occur if there was a TCP socket timeout (a constant supplied by the operating system), which was 20 seconds for our version of Linux.We approximate the output y(t), the total time for an HTTP reply by the Web server (T R + T T ), withˆywithˆ withˆy(t).
To model the server, we used a 2nd order SISO model defined byˆ y(t) = −a 1 ˆ y(t − 2) − a 2 ˆ y(t − 1) + b 1 u(t − 2) + b 2 u(t − 1).
(8)The parameter tuned by the controller, u(t), is the maximum number of concurrent connections (n c ).
The parameters a 1 , a 2 , b 1 , b 2 were learned offline via least squares and online via MSLS (Section 4.3, 4.4).
As with previous work [8], we define t = 10 (seconds) as an interval during which data is collected.
We used the mean reply time during this interval to measure y(t).
Here we test the effect of number of concurrent connections on various metrics.
On the client we set α = 1000 and k = 5; on the server n c ∈ [1,25].
First we note that increasing the number of concurrent connections was beneficial to the server's throughput, measured by replies per second (RPS), up until a certain point (Figure 2a).
There was a sharp drop in RPS at n c = 10, while n c = 9 yielded peak RPS.
Additionally, the standard deviation of the measurements increased substantially for n c ≥ 10.
The increase in standard deviation is a result of the server hardware limits being met; the server was unable to effectively address all the HTTP requests, resulting in connection drops or timeouts.
Figure 2b shows successful connection requests per second and connection errors.
Connection requests per second is different from RPS in that replies are made when the connection has already been established, versus connection requests which start new TCP connections (i.e. no HTTP packet data has been sent yet).
A value of n c = 9 also resulted in the peak number of successful connection requests per second (30).
Since 30 connections were established per second, this means that many connections were able to complete their k = 5 HTTP requests in less than a second.Next, Figure 2c shows the HTTP reply response (T R ) and transfer (T T ) times.
Both response and transfer times increased as the number of concurrent connections increased.
At n c = 7, the mean time to complete an HTTP reply (T c + T T ) was larger than one second.
Interestingly, as seen in Figure 2a, the RPS is still increasing at n c = 7.
Using these measurements, we envision two scenario types that a server administrator may optimize for using a model and controller:• Maximizing overall throughput (RPS) when quick HTTP replies are not necessary (such as when downloading large files or executing analytics Javascript).
• Ensuring that HTTP reply time is low enough to keep the user engaged; 150ms has been suggested as an upper bound for crisp user interaction [22].
In Section 4.4, we recreate the latter scenario by setting a target reply time and controlling n c .
In this section we perform an open-loop simulation with least squares (LS) and MSLS models.
To sufficiently excite the server response, we use a sinusoidal input u c (t) ∈ [1,17] for the max number of connections (n c ).
We set α = 1000 and k = 15.
After the simulation, we train the model parameters a 1 , a 2 , b 1 , b 2 of Eq.
8 using LS and MSLS (see Figure 3a).
For MSLS, we use a forgetting factor of λ = .92 and a noise reducing coefficient of m = .2.
Recall that y(t) is the mean reply time over a time interval of 10 seconds.
We denote y LS to be the LS estimator and y MSLS to be the MSLS estimator.
Note that we use batch LS rather than RLS; that is, the parameter learning is done after all the data is known and the parameters are constant through the simulation.
In contrast, the MSLS parameters are learned online and are adaptive.
This section describes two closed-loop simulations using an MSLS model for tracking y(t) and MDPP control of u(t).
We denote y c (t) to the be setpoint, or target y(t) in which the controller tunes u(t) to minimize the error function e(t) = |y MSLS (t) − y c (t)| (see Eq.
8 forˆyforˆ forˆy(t)).
As with Section 4.3, each timestep is 10 seconds and model parameters were set to λ = .92 and m = .2.
We used load generation parameters α = 100 and k = 15.
The first simulation used a setpoint of y c (t) = 100 for t < 150 and y c (t) = 50 for t ≥ 150 (see Figure 4).
The measurements of y(t) are noisy, increasingly so when the setpoint is changed.
At t = 150, there is an immediate shift in u(t), then the model parameters begin to relax.
The volatility of the a 1 , a 2 parameters is a consequence of noise; this could likely be reduced with a greater forgetting factor λ and a greater m.Next we performed a simulation in a high noise environment by raising the setpoint y c (t) = 1000 at timestep 150.
For t < 150, y c (t) was again kept at 100.
We anticipated noise based off of our findings in Section 4.2.
Furthermore, we adjust α to be 200 instead of 100 at t ≥ 150, doubling the load in order to simulate a spike in user activity (i.e. a change in the plant dynamics).
The load parameter α was also adjusted to increase the number of effects the model parameters must adapt for.
This simulation is shown in Figure 5.
There was an immediate spike in y(t) at t = 150 as the controller adjusted u(t) to compensate (overestimating u(t)).
The parameters took 10 timesteps to converge to a stable set of values, even with the increased y(t) noise.
The parameter values and u(t) are not shown here due to space.
We have shown a Web server can be modeled and controlled to enforce metrics that affect the user experience of a client machine (e.g. HTTP reply time).
Model parameters were learned real-time, and adaptive performance was strong and ostensibly resilient to noise.In future work, we would like to include additional metrics in both the model and the controller.
There are many parameters to adjust within the Apache Web server, including those in Apache modules (like amount of compression, encryption levels, caching, etc) [4].
Additionally, metrics like disk I/O (which can cause significant latency in sites with large amounts of data [21]) and bandwidth may be useful in creating a more effective model.
