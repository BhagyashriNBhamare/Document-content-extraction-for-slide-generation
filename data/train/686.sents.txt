Boolean matching determines whether two given (in)completely-specified Boolean functions can be identical or complementary to each other under permutation and/or negation of their input variables.
Due to its broad applications in logic synthesis and verification, it attracted much attention.
Most prior efforts however were incomplete and/or restricted to certain special matching conditions.
In contrast, this paper focuses on the computation kernel of Boolean matching and proposes a complete generic framework.
Through conflict-driven learning and abstraction, the capacity of Boolean matching scales up due to the effective pruning of infeasible matching solutions.
Experiments show encouraging results in resolving hard instances that are otherwise unsolvable.
Boolean matching is an important subject of both theoretical and practical interest.
Given two (in)completelyspecified Boolean functions, Boolean matching (under NPNequivalence) determines if they can be identical or complementary to each other under certain input permutation and/or negation.
From a theoretical standpoint, the computational complexity of Boolean matching (known as Boolean congruence, Boolean isomorphism, and other variants, which received attention dating back to the nineteenth century) situates in between coNP and Σ p 2 in the polynomial hierarchy [7,4].
Thus it is a good candidate for examining the open question about the collapse of the polynomial hierarchy.
From a practical standpoint, it has broad applications in logic synthesis and verification, for instance, library binding [5], FPGA technology mapping [8], logic verification [13], engineering change order [11], etc.The demand from practical applications drives the advances of Boolean matching algorithms over the last two decades.
By the survey [5] and recent developments [3,2,17,19], prior methods can be classified into four categories: spectral, signature-based, canonical-form-based, and, the more recent, SAT-based methods.
Spectral methods are complete, but often with prohibitive computation cost.
Signature-based methods are effective, but mostly incomplete due to their intrinsic limitations [12].
Canonical-form-based methods are complete, and have been improved recently, see, e.g., [2] for a review.
SAT-based methods are complete [17,19]; however, the strengths of modern propositional satisfiability (SAT) solvers have not been fully exploited yet.
Among prior Boolean matching techniques, only a few took the general NPN-equivalence into account.
Even if this general problem is considered, their computation costs may still be too expensive to be practical.
Furthermore, spectral, signature-based, and canonical-form-based methods are not easily extendable to deal with incompletely specified functions, see, e.g., [18,1] for a recent treatment.We show that these shortcomings can be naturally resolved under a general computation framework, named BooM, a decision procedure dedicated to Boolean matching.
In essence Boolean matching is a decision problem of solving quantified Boolean formulas (QBFs).
Particularly its special problem structure allows effective conflict-driven learning for search space reduction, which is beyond the capability of a generic QBF solver.The features of BooM are summarized as follows.
1) It handles NPN-equivalence, and both completely and incompletely specified functions.
2) It is equipped with abstraction and dynamic learning techniques for effective search space reduction.
3) It supports the search of one matching solution and of all solutions.
4) It admits easy integration with signature-based techniques for search space reduction, and helps signature-based Boolean matching be complete.
5) It uses memory efficient data structures, specifically, andinverter graphs (AIGs) and conjunctive normal form (CNF) formulas, for scalable Boolean function representation.
Experimental results show the effectiveness of BooM in conquering instances hard to solve without learning and abstraction.Compared with the closest prior work [17], BooM outperforms it and can be much scalable.
This prior method relies on the sum-of-products representation of the two functions to be matched, and consists of two separate computation phases: first learning and second SAT solving.
Since there is no feedback between these two phases, the first phase can be interpreted as static learning in a sense.
In contrast, our learning and SAT solving are interactive (with feedback).
Infeasible solutions are learned from a conflict produced by SAT solving; SAT solving searches a solution in the space refined by learning.
Hence the learning in BooM is more global and dynamic.
As SOP representation may not be always available especially for large designs, the prior method can be limited.
This paper is organized as follows.
Section 2 gives the preliminaries.
Boolean matching under NPN-equivalence is presented in Section 3, and P-equivalence in Section 4.
Experimental evaluation is provided in Section 5.
Finally Section 6 concludes this paper and outlines future work.
As conventional notation, a set of Boolean variables is denoted with an upper-case letter, e.g., X; its elements are in lower-case letters, e.g., xi ∈ X.
The ordered version (namely, vector) of a set X = {x 1 , . . . , x n } is denoted as x = (x 1 , . . . , x n ).
The cardinality of a set X (respectively x) is denoted as |X| (respectively | x|).
The set of truth valuations ofx is denoted [[ x]], e.g., [[(x1, x2)]] = {(0, 0), (0, 1), (1, 0), (1, 1)}.
A permutation π over X is a bijection function π : X → X; a negation ν over X is a componentwise mapping with ν(xi) = xi or ¬xi.
We let π( x) and ν( x) be the shorthand for (π(x 1 ), . . . , π(x n )) and (ν(x 1 ), . . . , ν(x n )), respectively.Given two (completely specified) functions f ( x) and g( y) with | x| = | y|, Boolean matching under NPN-equivalence determines if these two functions can be equivalent or complementary (the second "N" of "NPN") to each other under negation (the first "N") and permutation (the "P") of their input variables.
Its special cases include NP-equivalence(determining if f ( x) = g(ν • π( x)) for some negation ν and permutation π) and P-equivalence (determining if f ( x) = g(π( x)) for some permutation π).
We call ν • π (respectively π) a matching solution for NP-equivalence (respectively P-equivalence) if f ( x) = g(ν • π( x)) (respectively f ( x) = g(π( x))).
In the sequel, unless otherwise said, we shall assume | x| = | y| = n.Boolean matching for two incompletely specified functions is similar, except that the functional equivalence is asserted only under the care-conditions of both functions.
By assuming the reader's familiarity with circuit-to-CNF conversion [16] and SAT solving, including conflict-based learning [15] and other commonly used techniques in modern SAT solvers, e.g., [14,10], we omit to provide the background knowledge.
Given two functions f ( x) and g( y), optionally with their care-conditions f c ( x) and g c ( y), respectively, we decide whether f and g can be NPN-equivalent under the care-conditions.
In particular, this problem can be divided into two subtasks by deciding whether f and g or whether f and ¬g are NPequivalent.
Consequently we focus on Boolean matching under NP-equivalence.
Formally speaking, this task is to decide the validity of the second-order formula∃ν • π, ∀ x.((fc( x) ∧ gc(ν • π( x))) ⇒ (f ( x) ≡ g(ν • π( x)))), (1)which is convertible to a first-order formula as shown below.To represent y = ν • π( x), we introduce the 0-1 matrix:     x 1 ¬x 1 x 2 ¬x 2 · · · x n ¬x n y 1 a 11 b 11 a 12 b 12 · · · a 1n b 1n y 2 a 21 b 21 a 22 b 22 · · · a 2n b 2n . . . . . . . . . . . . . . . . . . . . . . . .
y n a n1 b n1 a n2 b n2 · · · a nn b nn     (2)By asserting n j=1(a ij + b ij ) = 1 for i = 1, . . . , n, and(3) n i=1 (a ij + b ij ) = 1 for j = 1, . . . , n,(4)this matrix represents some legal mapping ν • π, for a ij = 1 and b ij = 1 indicating mapping x j to y i and mapping ¬x j to yi, respectively.
These cardinality constraints (3) and (4) can be expressed by a propositional formula ϕC of 2n 2 Boolean variables a ij and b ij , for i, j = 1, . . . , n. By asserting the formulaϕA = n i,j=1 ((aij ⇒ (yi ≡ xj))(bij ⇒ (yi ≡ ¬xj))),a solution to ϕ C induces some unique mapping ν • π.
Conversely a mapping ν • π corresponds to some unique solution to ϕC .
Hence in the sequel we shall not distinguish a mapping ν • π and its corresponding solution to ϕ C .
In practice, ϕ C and ϕ A are written in CNF.Clearly solving Formula (1) is equivalent to solving the following (first-order) QBF∃ a, ∃ b, ∀ x, ∀ y.(ϕ C ∧ ϕ A ∧ ((f c ∧ g c ) ⇒ (f ≡ g))),(5)where a = (a 11 , . . . , a nn ) and b = (b 11 , . . . , b nn ).
That is, we look for a truth assignment to variables a and b that satisfies ϕC and makes the miter constraintΨ = ϕ A ∧ f c ∧ g c ∧ (f ≡ g) (6)unsatisfiable.
For simplicity, unless otherwise said we shall assume that f c and g c are tautologies in the sequel.The key to solving Formula (5) is to effectively reduce the search space.
By exploiting the functional properties specific to f and g (such as variable symmetry, unateness, and other functional properties), a preprocessing step is possible to screen out from ϕ C a substantial amount of infeasible solutions.
Let formula Φ 0 characterize the remaining solutions of variables a and b after the preprocessing.
We show below how the solution space corresponding to legal ν • π can be effectively refined in a sequenceΦ 0 , Φ 1 , . . . , Φ k , (Φ i+1 ⇒ Φ i ), along the solution search process.
We discuss two different Boolean matching goals: to search one matching solution and to search all matching solutions.
Figure 1: Search one Boolean matching solution remaining matching solutions after the ith iteration.
No solution to Φ i indicates that f and g cannot be matched; otherwise, a solution to Φ i corresponds to a candidate mapping ν•π.
Further justification by the second SAT solving instance is needed to check whether this solution makes the miter formula Ψ unsatisfiable.
If yes, the procedure terminates since ν • π is indeed a matching solution.
Otherwise, the procedure learns from this (conflict) solution and strengthens Φ i to Φ i+1 accordingly.
This action blocks not only this current solution of Φ i but also other infeasible solutions from occurrence in later search.
(Ordinary learning blocks only the current conflict solution.)
The procedure continues with Φ i+1 in the new iteration.
Below we show how to strengthen Φ i through learning.Fact 1.
Given two functions f ( x) and g( y) for Boolean matching under NP-equivalence, iff ( u) = g( v) for u ∈ [[ x]] and v ∈ [[ y]], then any mapping ν • π with ν • π( u) = v is infeasible.Example 1.
For two functions f (x 1 , x 2 , x 3 ) and g(y 1 , y 2 , y 3 ) with f (1, 0, 1) = g(0, 1, 1), then f and g cannot be matched under NP-equivalence by the six mappings with y = (¬x1, ¬x2, x3),(¬x 1 , x 3 , ¬x 2 ), (x 2 , x 1 , x 3 ), (x 2 , x 3 , x 1 ), (¬x 3 , x 1 , ¬x 2 ), and (¬x 3 , ¬x 2 , x 1 ).
Fact 1 can be rephrased in the language of formulas Φ i and Ψ as follows.Proposition 1.
Given two functions f ( x) and g( y) for Boolean matching under NP-equivalence, if f ( u) = g( v) for u ∈ [[ x]] and v ∈ [[ y]] with v = ν •π( u) for some ν •π satisfying Φ i , then conjuncting Φ i with the clause κ = n i,j=1 lij for literals lij = aij, if vi = uj; bij, otherwise, excludes from Φ i exactly the mappings {ν • π | ν • π ( u) = ν • π( u)}.
In essence a satisfying solution to the miter formula Ψ with respect to a solution of Φ i reveals additional infeasible matching solutions.
Letting Φ i+1 = Φ i ∧ κ prevents the above procedure from searching the learned infeasible solutions in later iterations.
As a result, a single clause with n 2 literals is added in each iteration.
(a 11 ∨ b 12 ∨ a 13 ∨ b 21 ∨ a 22 ∨ b 23 ∨ b 31 ∨ a 32 ∨ b 33 ).
The pruning power of a learned clause can be characterized as follows.Proposition 2.
For Boolean matching under NP-equivalence, the clause κ learned from a satisfying solution to f ( x) ≡ g( y) with y = ν •π( x) for some ν •π prunes n!
infeasible mappings.However the sets of mappings pruned by two different learned clauses may not be disjoint.Since there are 2 2n distinct truth assignments to variables x and y, the number of different learned clauses is upper bounded by 2 2n .
This fact also asserts the termination of the procedure.
When the Boolean matching objective is to find all matching solutions rather than just one, applying the procedure of Figure 1 to find the solutions one by one can be overkill.
Figure 2 sketches a more effective procedure for this purpose.
This procedure is the same as that of Figure 1 except that there is only one SAT solving kernel for one combined formula Φ i ∧ Ψ.Unlike the procedure of Figure 1 (where, effectively, variables a and b have higher decision orders than the other variables in making truth assignments), the procedure of Figure 2 imposes no restriction on the variable decision order.
This freedom makes it much more effective.
On the other hand, since this procedure terminates only when all infeasible solutions are pruned, sometimes its termination may take a long time.
Nevertheless its number of SAT solving iterations has the same upper bound as that of Figure 1 for a similar reason.Proposition 4.
The procedure of Figure 2 for Boolean matching under NP-equivalence terminates within O(2 2n ) iterations.Note that the computation of Figure 2 can be understood as performing quantifier elimination of variables x and y of Formula (5).
The resultant formula (in terms of variables a, b) characterizes all matching solutions.
We propose a new preprocessing method and introduce abstract Boolean matching.
Definition 1.
Given a Boolean function f ( x), a variable subset X * ⊆ X, and a variable z, the abstract function f * of f with respect to the (X * , z)-abstraction α is defined to be f * (x * , z) = f (α(x1), . . . , α(xn)) for α(x i ) = x i , if x i ∈ X * ;z, otherwise.Variables X * are referred to as the concrete variables, and z is the abstract variable.Given two functions f ( x) and g( y), let f * be the abstract function of f with respect to the (X * , z)-abstraction α.
The abstract Boolean matching determines if f * (x * , z) and g( y) can be equivalent under the variable mappingy = α • ν • π( x) for some ν • π.
(The function α is phase-preserving, i.e., α(¬x i ) = ¬α(x i ).)
To represent y = α • ν • π( x), we define the 0-1 matrix:     x * 1 ¬x * 1 · · · x * k ¬x * k z ¬z y 1 a 11 b 11 · · · a 1k b 1k a 1(k+1) b 1(k+1) y2 a21 b21 · · · a 2k b 2k a 2(k+1) b 2(k+1) . . . . . . . . . . . . . . . . . . . . .
y n a n1 b n1 · · · a nk b nk a n(k+1) b n(k+1)     (7)with k+1 j=1(aij + bij) = 1 for i = 1, . . . , n, andn i=1 (aij + bij) = 1 for j = 1, . . . , k.(8)Furthermore, let aij = 1 and bij = 1 indicate (yi ≡ xj) and (yi ≡ ¬xj), respectively, for j = 1, . . . , k; let a i(k+1) = 1 and b i(k+1) = 1 indicate (y i ≡ z) and (y i ≡ ¬z), respectively.
All of these constraints can be expressed with a Boolean formula and a solution to it corresponds to some legal ν • π with respect to the abstraction α.
2 To solve the abstract Boolean matching, the procedures of Figures 1 and 2 can be applied with a similar learning mechanism.
Again let Φ i be the formula characterizing the remaining matching solutions after the ith iteration.Proposition 5.
For abstract Boolean matching of f * ( x * , z) and g( y) under NP-equivalence, if f (α( u)) = g( v) for u ∈ [[ x]] and v ∈ [[ y]] with v = α • ν • π( u) for some α • ν • π satisfying Φ i , then conjuncting Φ i with the clause κ = n i=1 k+1 j=1 lij for literals lij = a ij , if v i = u * j , bij, otherwise, where u * ∈ [[( x * , z)]] for f * ( u * ) = f (α( u)), excludes from Φ i exactly the mappings {α • ν • π | α • ν • π ( u) = α • ν • π( u)}.
So a learned clause is of size n(k + 1).
The abstract Boolean matching of f * and g is useful for two reasons.
First, the computation is simplified (f * is simpler than f and the leaned clauses are shorter).
Second, its solutions reveal useful information for the Boolean matching of f and g. 2 Note that z and ¬z are allowed to map to multiple variables in Y since Eq.
(9) imposes no cardinality constraint for j = k + 1.
In contrast, one of x * i and ¬x * i maps to a unique variable in Y .
Proposition 6.
If a mapping between the concrete variables X * and some Y ⊆ Y (with |X * | = |Y |) is shown infeasible in the abstract Boolean matching of f * and g, then in the original Boolean matching of f and g any mappings between X and Y having the same sub-mapping between X * and Y must be infeasible as well.By conducting abstract Boolean matching for different sets of concrete variables, it can be applied as preprocessing to filter out infeasible matching solutions for the original Boolean matching problem.For the preprocessing purpose, the procedure of Figure 2 is more effective for abstract Boolean matching than that of Figure 1.
In fact, the termination condition can be relaxed to quit at any iteration since the remaining matching solutions characterized by Φ i in the abstract Boolean matching will be a legitimate over-approximation of the matching solutions in the original Boolean matching.
Since Boolean matching under P-equivalence is a special case of matching under NP-equivalence, the computation framework of NP-equivalence can be customized for P-equivalence.
To represent y = π( x), Matrix (2) for NP-equivalence is applicable by removing the columns indexed by ¬x 1 , . . . , ¬x n .
Moreover, the cardinality constraints are the same as Eq.
(3) and Eq.
(4) but excluding the bij terms.
They can be expressed by a CNF formula (new ϕC ) with n 2 variables and 2n(C n 2 + 1) = O(n 3 ) clauses.
By asserting n i,j=1 (a ij ⇒ (y i ≡ x j )) (new ϕ A ), legal permutations can be characterized using a Boolean formula.
Thus Boolean matching under P-equivalence can be formulated as solving the QBF (5) with the updated ϕC and ϕA.Similar to the NP-equivalence case, the procedures of Figures 1 and 2 are applicable here.
As a matter of fact, leaning for P-equivalence can be made more efficient.
Let Φ i characterize the remaining matching solutions at the ith iteration.Proposition 7.
Given two functions f ( x) and g( y) for Boolean matching under P-equivalence, if f ( u) = g( v) for u ∈ [[ x]] and v ∈ [[ y]] with v = π( u) for some π satisfying Φ i , then conjuncting Φ i with the clause κ = n i,j=1 lij for literalsl ij = a ij , if v i = 0 and u j = 1; ∅, otherwise,excludes from Φ i exactly the mappings {π | π ( u) = π( u)}.
Note that the above condition "if v i = 0 and u j = 1" can be equivalently replaced with "if v i = 1 and u j = 0."
Clearly for u ∈ [[ x]] with m u i 's equal to 1, then the corresponding learned clause is of m(n − m) literals.Example 3.
The learned clause corresponding to f (1, 0, 1) = g(0, 1, 1) can be (a11 ∨ a13) or, equivalently, (a22 ∨ a32).
The pruning power of a learned clause can be characterized as follows.Proposition 8.
For Boolean matching under P-equivalence, the clause κ learned from a satisfying solution u ∈ [[ x]] to f ( u) ≡ g(π( u)) for some π prunes m!
(n − m)!
infeasible permutations, where m is the number of 1's in u.Observe that the larger the difference between the numbers of 1's and 0's in u is, the stronger the pruning power of the learned clause is.
Hence it may be beneficial to search satisfying solutions in such biased truth assignments.For every u ∈ [[ x]] with k 1's, there are C n k possible v ∈ [[ y]] having the same number of 1's.
Hence the number of possible learned clauses and thus learning iterations is upper boundedby 1 2 ((C n 0 ) 2 + (C n 1 ) 2 + · · · + (C n n ) 2 ) = 2 2n ·(n− 1 2 )!
2 √ πn!
≤ 2 2n 2 √ π .
Preprocessing with abstraction can be pursued in Boolean matching under P-equivalence similar to that for NP-equivalence.
We omit the exposition to save space.
BooM was programmed in C language within the ABC [6] package using MiniSAT [10] as the underlying solver.
All experiments were conducted on a Linux machine with Xeon 2.5GHz CPU and 26GB RAM.Circuits from the MCNC, ISCAS89 and ITC99 benchmark suites were chosen.
Sequential circuits were converted to combinational ones by the ABC command comb.
To test the full power of BooM, we make the computation harder by matching each primary output of a circuit independently.
3 A function is matched against its synthesized version with its inputs permuted in a reverse/random order (for P-equivalence checking), and in addition negated randomly (for NP-equivalence checking).
Functions with support sizes between 10 and 39 were experimented.
Specifically there are 717 functions with an average of 114.74 AIG nodes (ranging from 15 to 2160 nodes) and 23.74 variables.The baseline preprocessing of BooM to reduce the search space consists of detecting functional properties of NE-symmetry and unateness, and simulating Type 1 and Type 2 vectors of [19].
For two symmetry groups that can be uniquely mapped between the two functions to be matched, BooM breaks the symmetry by assigning an arbitrary variable mapping.
Figure 3: Runtime with and without learning Figure 3 shows the effect of learning (as an example, in the context of searching all matching solutions under P-equivalence).
The x-and y-axes correspond to the runtimes with and without learning, respectively.
A spot in the figure corresponds to the result of a function.
As most of the spots are above the 45-degree line, learning is evidentally useful.
Figure 4 shows the effect of abstraction (in the same context as Figure 3 with learning applied).
The abstraction 3 When multiple outputs are considered simultaneously, e.g., in [19], many more mutual signatures can be deduced to reduce the search space substantially.
Figure 4: Runtime with and without abstraction is conducted for enumerating half of all possible abstract matchings with two concrete variables.
Only functions with non-empty clauses learned from abstraction are plotted.
As can be seen, abstraction achieves clear improvement.
Many instances timed out after 600 seconds without abstraction can be effectively resolved with abstraction.
Nevertheless, there are a few cases where abstraction does not help.
¡ £ ¤ ¥ ¦ § ¢ ¨ © ¢ ¢ ! "
# # $ ( ( ¡ £ ¤ ¥ ¦ % & ' ) 0 1 2 3 4 2 5 6 7 8 9 0 ) @ A 2 B C ' 0 ' D E # §¡ £ ¤ ¥ ¦ § ¢ ¨ © ¢ !
© " # ' ' ¡ £ ¤ ¥ ¦ $ % & ( ) 0 1 2 3 1 4 5 6 7 8 ) ( 9 @ A 3 ( B @ 4 ( ) C & D # §$ £ £ ¥ ¤ ¤ ¥ ¥ ¥ ¥ % & ' ( ) 0 1 2 ( 3 Figures 5 and 6 show the cumulative runtime for matching under P-equivalence and NP-equivalence, respectively.
In these figures, the x-and y-axes are indexed by the cumulative number of solved instances and cumulative runtime, respectively.
In the legends, "one sol" and "all sol" indicate the targets of searching one matching solution (Section 3.1.1) and searching all matching solutions (Section 3.1.2), respectively; "learn" indicates learning being applied; "abs" indicates preprocessing using abstraction being turned on.
The solved instances under each of these six options were sorted by their runtimes in an ascending order before the accumulation.
(Among the solved functions in searching one matching solution using both learning and abstraction, the maximum input sizes are 39 and 38 for matching under P-equivalence and NP-equivalence, respectively.)
These two figures reveal, as expected, that matching under NP-equivalence is much harder than that under P-equivalence.
Moreover, the performance of searching all matching solutions is comparable to that of searching one solution in the P-equivalence case, but far worse in the NP-equivalence case.
One explanation might be that, since the configuration space for NP-equivalence is Figure 6: Cumulative runtime for NP-equivalence much larger, searching all matching solutions requires many more refinement iterations and thus becomes less effective.
In all the above experiments, a matching instance consists of a pair of functions, whose variables are in reverse order.
To see the effect of variable ordering, we alternatively prepared matching instances with random order.
For searching all matching solutions under P-equivalence, 531 out of the 717 functions can be solved within 600 seconds under both orders, and the corresponding total-runtime ratio of reverse order to random order is 1.00 to 1.06.
Since there seems no strong bias when using these two orders, the results under reverse ordering may be more or less representative.
¢ ¥ $ ¥ ¥ ¥ ¥ $ ¥ % & ' ( ) 0 1 2 ( 3Experience of matching incompletely-specified functions (data not shown due to space limitation) suggested that it takes longer time to solve than matching completely-specified counterparts.
The reason can be twofold: First, more assignments on average are tried before the miter constraint is satisfied.
Second, the miter constraint becomes more complex in representing the care conditions.
Hence the runtime per learning iteration is larger.
Nevertheless matching incompletely-specified functions is indeed feasible under the BooM framework.
Figure 7 compares BooM and DepQBF [9], a state-of-theart QBF solver, in searching one matching solution under Pequivalence.
The x-and y-axes correspond to the runtimes of BooM and DepQBF, respectively, after the same preprocessing.
As can be seen, BooM outperforms DepQBF due to its unique and powerful domain-specific learning.
We have formulated Boolean matching as QBF solving and exploited domain-specific knowledge for effective search space reduction.
A decision procedure BooM, equipped with abstraction and dynamic learning, has been proposed as a generic computation framework for Boolean matching under NPN-equivalence for both completely and incompletely specified functions.
Experiments showed promising results.
As various Boolean matching techniques can be built and integrated on top of BooM, we anticipate Boolean matching can be made scalable and practical in more applications.
Moreover, the success of BooM may suggest that it worths to customize decision procedures for other computation problems.
The authors are grateful to Bo-Han Wu for helpful discussion, and National Science Council for grants 96-2221-E-002-278-MY3 and 98-2221-E-030-016.
