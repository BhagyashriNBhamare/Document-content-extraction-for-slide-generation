Dependency networks are a compelling alternative to Bayesian networks for learning joint probability distributions from data and using them to compute probabilities.
A dependency network consists of a set of conditional probability distributions, each representing the probability of a single variable given its Markov blanket.
Running Gibbs sampling with these conditional distributions produces a joint distribution that can be used to answer queries, but suffers from the traditional slowness of sampling-based inference.
In this paper , we observe that the mean field update equation can be applied to dependency networks, even though the conditional probability distributions may be inconsistent with each other.
In experiments with learning and inference on 12 datasets, we demonstrate that mean field inference in dependency networks offers similar accuracy to Gibbs sampling but with orders of magnitude improvements in speed.
Compared to Bayesian networks learned on the same data, dependency networks offer higher accuracy at greater amounts of evidence.
Furthermore, mean field inference is consistently more accurate in dependency networks than in Bayesian networks learned on the same data.
In many domains, including recommender systems, fault diagnosis, and medicine, we wish to learn a joint probability distribution from data and use it to compute the probabilities of various events.
The learning and inference tasks are often treated separately, but it is their combination that determines the overall accuracy of a system.
A standard approach is to learn a probabilistic graphical model, such as a Bayesian or Markov network, and answer queries using an approximate inference algorithm, such as mean field inference or Gibbs sampling (Gilks, Richardson, and Spiegelhal- ter 1996).
These methods have been largely successful, but have several limitations.
For Bayesian networks, the need to avoid cycles significantly restricts the set of probability distributions that can be compactly represented.
Undirected models such as Markov networks are somewhat more flexible, but suffer from computationally expensive weight learning and structure learning.Dependency networks are an often overlooked alternative that combine some of the best qualities of Bayesian Copyright c 2011, Association for the Advancement of Artificial Intelligence (www.aaai.org).
All rights reserved.
and Markov networks.
Like Bayesian networks, dependency networks can be learned efficiently from data.
Like Markov networks, they can represent many symmetric and cyclical dependencies more compactly than a Bayesian network.
For these reasons, dependency networks have been successfully applied to tasks such as collaborative filtering (Heck- erman et al. 2000) and part-of-speech tagging ( Toutanova et al. 2003).
A relational extension of dependency networks has also been successfully applied to domains such as fraud detection (Neville and Jensen 2007) and entity resolution ( Natarajan et al. 2010).
However, dependency networks are traditionally limited to a single inference algorithm -Gibbs sampling -which may be slow to run and may produce different results depending on the order in which variables are sampled.
This arises from the fact that the conditional probability distributions which comprise a dependency network are often inconsistent with each other, leading to a poorly-specified joint probability distribution.
This disadvantage has led dependency networks to be largely ignored in favor of Bayesian and Markov networks, which have clearer semantics.In this paper, we show that an approximate version of the mean field inference algorithm can be applied to dependency networks, in spite of the fact that the conditional distributions in a dependency network may be inconsistent.
We find that mean field inference typically converges much faster than Gibbs sampling and offers similar accuracy.We then compare the speed and accuracy of learning and inference in dependency networks to Bayesian networks, by learning and evaluating both models on 12 publicly available datasets.
We find that learning times are similar, but dependency networks have consistently better test set pseudolikelihood.
In our queries, mean field runs faster in dependency networks than in Bayesian networks while producing consistently more accurate results.
When we run Gibbs sampling in both models, their relative accuracy depends on the amount of evidence.
On queries with more evidence, dependency networks are more accurate; with less evidence, Bayesian networks are more accurate.
However, the differences in probabilities are typically very small.
This means that dependency networks are a good choice for many applications, especially when queries will involve a large amount of evidence.
Let X be a set of random variables {X 1 , X 2 , . . . , X n }.
A Bayesian network (BN) (Pearl 1988) represents a probability distribution over X , P (X ), using a directed, acyclic graph and a set of conditional probability distributions (CPDs).
The graph contains one node for each variable X i .
Each CPD P (X i |Π i ) specifies the probability of a single variable (X i ) given its parents in the graph (Π i ).
The product of these CPDs gives the full joint probability distribution:P (X = x) = i P (X i = x i |Π i = π i )(1)Bayesian networks and other graphical models can be defined over both discrete and continuous variables.
For this paper, we restrict our attention to discrete variables, although many of the methods can be naturally applied to the continuous and hybrid cases as well.The Markov blanket of a variable X i , denoted MB(X i ), is the set of variables that render X i independent from all other variables in the domain.
In a Bayesian network, this set consists of X i 's parents in the graph, X i 's children, and all variables that have a child in common with X i .
These independencies, and others, are entailed by the CPD factorization from Equation 1.
The simplest form for a CPD is a full table that specifies the distribution of the child variable, X i , given each configuration of its parent variables, Π i .
However, this representation is exponential in the number of parent variables, effectively limiting the maximum number of parents to a small number.A more efficient and flexible alternative is to represent conditional probabilities with probabilistic decision trees (Chickering, Heckerman, and Meek 1997).
A decision tree is a directed, rooted tree, in which each leaf contains a distribution over the target variable X i .
Each interior node is labeled with one of X i 's parents, X j ∈ Π i , and each outgoing arc from this node is labeled with one or more values of the parent variable, x j ∈ Val(X j ).
The distribution at a leaf gives the conditional probability of X i given the configurations of parent variables specified along the path from the leaf to the root of the tree.
To answer queries, we use the BN to compute marginal and conditional probabilities.
Since exact inference in a BN is #P-complete (Roth 1996), approximation algorithms are typically necessary.
Popular approximate inference algorithms include Markov chain Monte Carlo methods, such as Gibbs sampling; variational approximations such as mean field (MF); and loopy belief propagation (BP).
Gibbs sampling proceeds by initializing the query variables randomly and then resampling each in turn, according to its conditional probability given its Markov blanket.
Evidence variables remain fixed to their set values.
The probability of a particular query is computed as the fraction of the samples that match the query.
For positive distributions (i.e., all configurations have non-zero probability), Gibbs sampling is guaranteed to eventually converge to the correct distribution.
However, this can take a very long time, and convergence can be difficult to detect.Sampling methods can often be improved by using Rao-Blackwellization, in which some of the variables are marginalized analytically.
One way to apply RaoBlackwellization to Gibbs sampling is to add fractional counts to the different states that could result from resampling the current variable, based on their relative probabilities.
Then the variable is randomly assigned a single value, as in regular Gibbs sampling.
These fractional counts can lead to a significant reduction in variance.Belief propagation (BP) is an exact inference algorithm for tree-structured Bayesian and Markov networks.
Loopy belief propagation is the application of the belief propagation algorithm to graphs with cycles (Murphy, Weiss, and Jordan 1999).
We use the flooding message propagation scheme in our experiments, which sends messages from variables to CPDs and then CPDs to variables in each iteration, as described by Kschischang et al. (2001).
We defer a full description of mean field inference to the next section.
The objective of parameter learning is to select a set of model parameters (CPD probabilities) that maximize a particular objective function on the training data.
For Bayesian networks, the most common objective function is loglikelihood, penalized by a prior distribution on each conditional distribution.
We used the Beta(1,1) distribution as the prior in all of our experiments, which is equivalent to Laplace smoothing.
In this case, parameter estimation can be done in closed form from the sufficient statistics of the training data.In structure learning, the objective is to find the structure that maximize an objective function such as the Bayesian score (Heckerman, Geiger, and Chickering 1995).
Finding the optimal structure is NP-hard (Chickering, Heckerman, and Meek 2004), but greedy search typically works well in practice.
A common approach is to do hill climbing with random restarts, where the search operators include adding, deleting, and reversing arcs.
For decision tree CPDs, the search operators consist of adding a split to a tree CPD, which replaces a decision tree leaf node with a new interior node that has leaves as its children (Chickering, Heckerman, and Meek 1997).
Operations that would lead to a cycle in the BN are excluded from the search.
A dependency network (DN) (Heckerman et al. 2000) consists of a set of conditional probability distributions P i (X i |MB(X i )), each defining the probability of a single variable given its Markov blanket.
These conditional distributions may or may not be consistent with each other.
The graph of a DN is directed, with a node for each variable in the domain.
For each variable X i , there are edges from the nodes representing variables in X i 's Markov blanket to the node representing X i .
Unlike a BN, the product of all CPDs gives the pseudolikelihood of an instance, P * , not its probability:P * (X ) = i P i (X i |MB(X i ))(2)Pseudo-likelihood (Besag 1975) is a consistent estimator commonly used for learning the parameters of Markov networks, since its local approach to normalization avoids the intractability of partition function.
However, pseudolikelihood learning tends to produce models that handle long-range dependencies poorly.
The joint distribution of a DN is defined as the stationary distribution of a Gibbs sampler run by using its CPDs for the required Markov blanket distributions, P i (X i |MB(X i )).
Heckerman et al. (2000) also describe a second method for computing probabilities in which the probability of each variable is computed in turn using the Gibbs sampler, conditioned on the states of previous variables.
This reduces the variance when computing rare joint probabilities with sampling, and is theoretically equivalent.
Toutanova et al. (2003) adapt the Viterbi algorithm to perform MAP inference in a chain-structured DN.
To our knowledge, no other inference algorithms have ever been applied to DNs.A dependency network can be constructed from any Bayesian or Markov network by constructing a conditional probability distribution for each variable given its Markov blanket.
A DN can also be learned from data by the same methods used to learn a BN with tree-structured CPDs, except without enforcing acyclicity.
This makes DN learning trivial to parallelize with a separate process for each variable.However, when learned from data, the resulting CPDs may be inconsistent with each other.
Heckerman et al. refer to this as a "general dependency network."
Given a particular sampling order, a Gibbs sampler will still have a unique stationary distribution when the distribution is positive (all probabilities are non-zero).
However, the stationary distribution may depend on the ordering chosen.
Furthermore, the joint distribution determined by the Gibbs sampler may be inconsistent with the conditional probabilities asserted by the CPDs.
When learned from data, DNs tend to roughly approximate consistency, since each CPD is approximating the same empirical distribution of the training data.
Mean field inference (MF) approximates an intractable distribution P with a fully factorized distribution Q:Q(X ) = i Q(X i )(3)Q is selected to be as close as possible to the desired distribution P , where distance 1 is measured as the Kullback-Leibler (KL) divergence between Q and P :KL(Q P ) = X Q(X) log Q(X) P (X)(4)= E X∼Q log Q(X) P (X)(5)By deriving the fixed point equations for MF, it can be shown that every local optimum of the KL divergence satisfies the following equation for every marginal in the Q distribution:Q(X i ) = 1 Z i exp E X \{Xi}∼Q [log P (X i |X \ {X i })](6)where Z i is a normalizing constant.
This can be interpreted as setting Q(X i ) to match the expectation of X i in log space, where the expectation is computed according to the distribution of the remaining variables in Q. By using this equation to iteratively update each Q(X i ) marginal in turn, the KL divergence can be shown to decrease in each iteration until a local minimum is reached.
A common ordering for the Q updates is to use a queue, so that after Q(X i ) is updated, the neighbors of X i are added to the end of the queue if not already in it.
This continues until convergence.These updates were derived by Haft et al. (1999).
They additionally observed that, since these equations only depend on P through the conditional distributions P (X i |X \ {X i }) (or more specifically, P (X i |MB(X i )), since the Markov blanket renders all other variables independent), they could be used to define MF update equations for any representation.
However, Haft et al. did not apply these equations to dependency networks where the conditional distributions could be inconsistent.Algorithm 1 contains pseudo-code for mean field inference in dependency networks.
It is similar to Algorithm 11.7 from Koller and Friedman (2009), but with three modifications.
The main change is line 8, which uses the conditional distributions P i (X i |MB(X i )) to perform the update from Equation 6.
This makes the algorithm applicable to dependency networks, whether or not their conditional distributions are consistent.
The algorithm also features a maximum number of iterations, MaxIters, so that the algorithm will halt early if convergence is slow.
In our experiments, we set this to 50 times the number of non-evidence variables.
Mean field almost always converged in fewer iterations.
When we increased the maximum number of iterations, it always converged.
The second modification is to set a convergence threshold, , on the distance between the old and new marginals.
In our experiments, we used a Euclidean distance of 0.0001 as the threshold.
Algorithm 1 can be made equivalent to the standard mean field inference algorithm on a Bayesian or Markov network by setting to zero, NumIters to infinity, and each P i to the Markov blanket distribution of X i given its neighbors in the Bayesian or Markov network.In a consistent DN, MF will always converge to a local minimum of the KL divergence, as in any other graphical model.
With inconsistent CPDs and determinism, it is easy to produce oscillations.
Given two binary variables A and Algorithm 1 Mean field inference for dependency networks 1: Q ← Q 0 2: Iters ← 0 3: Unprocessed ← X 4: while Unprocessed = ∅ and Iters < MaxIters do Choose X i from Unprocessed 6:Q old (X i ) ← Q(X i ) 7:for x i ∈ V al(X i ) do 8:Q(x i ) ← exp{E MB(Xi)∼Q [log P i (X i |MB(X i )]} 9:end for 10:Normalize Q(X i ) to sum to one 11: if Q old (X i ) − Q(X i ) > then Unprocessed ← Unprocessed ∪ MB(X i ) end if Unprocessed ← Unprocessed -X i Iters ← Iters +1 16: end while B, let P (A|B) = 1 when A = B and P (B|A) = 1 when B = A. MF run in this DN will oscillate forever unless initialized to the fixed point Q(A) = Q(B) = 0.5.
With positive CPDs, where no probabilities are 1 or 0, it remains unknown if convergence is always guaranteed.
MF converged in our experiments, but we do not know if this is always the case.
In our experiments, we sought to evaluate the relative effectiveness of different learning and inference schemes on realworld data.
We were particularly interested in how MF compares to the standard approach of Gibbs sampling in DNs, both in speed and accuracy.
Furthermore, since BNs are an alternative to DNs, we felt it was important to benchmark our results against standard BN learning and inference methods.
If BNs can produce more accurate answers than DNs in less time, then the case for using DNs is relatively weak.We ran our experiments on 12 publicly available datasets collected and prepared by Davis and Domingos (2010) and also used by Lowd and Davis (2010) for evaluating Markov network structure learning algorithms.
We excluded the EachMovie dataset, since it is no longer publicly available.
All variables are binary-valued.
The datasets vary widely in size and number of variables.
See Table 1 for a summary, and Davis and Domingos (2010) for more details on their origin.
Datasets are listed in increasing order by number of variables.We learned DNs and BNs with tree CPDs using the WinMine toolkit (Chickering 2002), which is based on the algorithms of Chickering et al. (1997) and Heckerman et al. (2000).
2 WinMine uses a structure prior that penalizes models based on the number of parameters: where s is the number of parameters in the model.
To avoid overfitting, we tuned the κ parameter on the tune set.
We evaluated on test data.
We ran inference using a Rao-Blackwellized Gibbs sampler, with 100 burn-in iterations and 1000 sampling iterations.
We experimented with running the Gibbs sampler for ten times as long, but found that additional iterations produced little benefit on most of our datasets.
We also ran mean field and belief propagation.
All inference algorithms were implemented in the open-source Libra Toolkit.
3 We use DN.Gibbs and DN.MF to refer to Gibbs sampling and mean field in a learned dependency network, respectively; and BN.Gibbs, BN.MF, and BN.BP to refer to Gibbs sampling, mean field, and belief propagation in a learned Bayesian network, respectively.P (S) ∝ κ s(7)All inference algorithms are evaluated using test set conditional marginal log-likelihood (CMLL), a common evaluation metric (Lee, Ganapathi, and Koller 2007;Davis and Domingos 2010).
The CMLL represents the expected log loss of the query variable marginals, given the evidence variables:CM LL(X = x) = 1 |X Q | Xi∈X Q log P (X i = x i |X E = x e ) (8)where X Q is the set of query variables, X E is the set of evidence variables, and x e denotes the configuration of the evidence variables in the example.
Unlike Davis and Domingos (2010), we randomly select a separate set of query and evidence variables for each instance in the test set.
The fraction of evidence variables ranges from 10% to 90% of the total variables in the domain, by 10% increments.
All other variables are used as query variables.
In order to reduce the variance among results with different amounts of evidence, the evidence variables in each query at 10% evidence are a subset of those at 20% evidence, which are a subset of those at 30% evidence, and so on.
This makes our queries at different levels of evidence more similar, while remaining unbiased.
Table 2: Learning time (in seconds), complexity (number of parameters), test set pseudo-log-likelihood (PLL), and test set log-likelihood (LL) of learned models.
Reported error range is one standard deviation of the mean.
Standard errors of less than 0.0005 are reported as 0.000.
When differences in PLL or LL are statistically significant, the better result is in bold.
For pseudo-log-likelihood, DNs are significantly better on 10 out of 12 datasets according to a paired t-test (p < 0.05).
MSWeb is the only dataset where BNs perform better.
The better pseudo-likelihood of DNs in this experiment is the consequence of the fact that DNs effectively optimize pseudo-likelihood while learning.
We used Algorithm 1 from Heckerman et.
al (Heckerman et al. 2000) to approximate the log-likelihood in dependency networks.
Since this is a very slow sampling-based algorithm, we used only 40% of the test data.
Log-likelihood for the BNs was computed exactly on the same subset.
DNs are significantly better on 3 datasets and BNs are significantly better on 4, according to a paired t-test (p < 0.05).
We expected BNs to do better, since they are directly optimizing log-likelihood, but DNs were surprisingly competitive.We then ran multiple inference algorithms on all data sets to evaluate the accuracy of the different models and algorithms in answering conditional queries.
Figure 1 shows the per-variable CMLL for each data set, averaged over all amounts of evidence (10% to 90%).
With the exception of BN.MF, all algorithms have very similar accuracies.
BN.BP, BN.Gibbs, and DN.Gibbs tend to be slightly more accurate than DN.MF.
On every dataset, DN.MF is more accurate than BN.MF.
Whether DN.Gibbs is more accurate than BN.Gibbs depends on the dataset.
Overall, the accuracy of DN.MF is superior to BN.MF and comparable to other inference algorithms.
Figure 2 depicts the average inference time for each data set, averaged over all amounts of evidence.
For inference time, the MF methods are the fastest, often 1-2 orders of magnitude faster than the other methods.
In addition, DN.MF is faster than BN.MF in 10 data sets out of 12.
To tease apart the differences between DNs and BNs, we looked at wins and losses by amount of evidence.
Table 2), since PLL conditions on the most evidence possible: all other variables.
Since Gibbs sampling is an anytime algorithm, we can force it to match the speed of mean field.
Table 4 probabilities inferred by DN.MF and all other methods.
We chose root mean squared difference as our metric, since it penalizes large differences more than small differences.
For 10 out of 12 datasets, the root mean squared difference between DN.MF and the other methods was less than 0.005.
Between DN.MF and BN.Gibbs, the root mean squared difference was less than 0.001 on half of the datasets.
Between DN.MF and DN.Gibbs, the root mean squared difference was less than 0.0005 on 9 datasets.
This supports the hypothesis that all methods produce similar results.
In particular, MF and Gibbs sampling give very similar probabilities in dependency networks.
Source code and additional results are available in the online appendix at http://ix.cs.uoregon.edu/ ∼ lowd/dnmf.
Mean field inference in DNs offers similar accuracy to Gibbs sampling but in significantly less time.
It is also consistently more accurate than running mean field inference in a BN learned on the same data.
When we compare to other BN inference algorithms as well, we find that DNs are more accurate when more evidence is available.
Therefore, although BNs remain a good choice for many applications of statistical learning and inference, DNs are a compelling alternative.In future work, we intend to apply mean field to relational dependency networks (Neville and Jensen 2007).
In relational domains such as social network analysis, networks are typically cyclical and large-scale, making dependency networks with mean field inference a natural choice over directed models or Gibbs sampling.
We thank Jesse Davis, Chloé Kiddon, and Aniruddh Nath for feedback on earlier drafts of this paper, and Christopher Meek for helpful discussions.
