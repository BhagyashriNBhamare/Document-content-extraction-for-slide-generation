Multi-tiered storage made up of heterogeneous devices are raising new challenges in allocating throughput fairly among concurrent clients.
The fundamental problem is finding an appropriate balance between fairness to the clients and maximizing system utilization.
In this paper we cast the problem within the broader framework of fair allocation for multiple resources.
We present a new allocation model BAA based on the notion of per-device bottleneck sets.
Clients bottlenecked on the same device receive throughputs in proportion to their fair shares, while allocation ratios between clients in different bottleneck sets are chosen to maximize system utilization.
We show formally that BAA satisfies fairness properties of Envy Freedom and Sharing Incentive.
We evaluated the performance of our method using both simulation and implementation on a Linux platform.
The experimental results show that our method can provide both high efficiency and fairness.
The growing popularity of virtualized data centers hosted on shared physical resources has raised the importance of resource allocation issues in such environments.
In addition, the widespread adoption of multi-tiered storage systems [2,3], made up of solid-state drives (SSDs) and traditional hard disks (HDs), has made the already challenging problem of providing QoS and fair resource allocation considerably more difficult.Multi-tiered storage has several advantages over traditional flat storage in the data center: improved performance for data access and potential operating cost reductions.
However, this architecture also raises many challenges for providing performance isolation and QoS guarantees.
The large speed gap between SSDs and HDs means that it is not viable to simply treat the storage system as a black box with a certain aggregate IOPS capacity.
The system throughput is intrinsically linked to the relative frequencies with which applications access the different types of devices.
In addition, the throughput depends on how the device capacities are actually divvied up among the applications.
System efficiency is a major concern for data center operators since consolidation ratios are intimately connected to their competitive advantage.
The operator also needs to ensure fairness, so that the increased system utilization is not obtained at the cost of treating some users unfairly.This brings to focus a fundamental tension between fairness and resource utilization in a system with heterogeneous resources.
Maintaining high overall system utilization may require favoring some clients disproportionately while starving some others, thereby compromising fairness.
Conversely, allocations based on a rigid notion of fairness can result in reduced system utilization as some clients are unnecessarily throttled to maintain parity in client allocations.The most-widely used concept of fairness is proportional sharing (PS), which provides allocations to clients in proportion to client-specific weights reflecting their priority or importance.
Adaptations of the classic algorithms for network bandwidth multiplexing [49,9,40] have been proposed for providing proportional fairness for storage systems [48,45,21].
Extended proportionalshare schedulers which provide reservations and limit guarantees in addition to proportional allocation have also been proposed for storage systems [46,19,22,23].
However, the vast majority of resource allocation schemes have been designed to multiplex a single resource, and have no natural extension to divide up multiple resources.The question of fair division of multiple resources in computer systems was raised in a fundamental paper by Ghodsi et al [17], who advocated a model called Dominant Resource Fairness (DRF) to guide the allocation (see Section 2).
A number of related allocation approaches [16,36,11,25,28,15,14,13,12,30,38] have since been proposed; these will be discussed in Section 6.
These models deal mainly with defining the meaning of fairness in a multi-resource context.
For example, DRF and its extensions consider fairness in terms of a client's dominant resource: the resource most heavily used (as a fraction of its capacity) by a client.
The DRF policy is to equalize the shares of each client's dominant resource.
In [12], fairness is defined in terms of proportional sharing of the empirically-measured global system bottleneck.
A theoretical framework called Bottleneck-based fairness [11] proves constructively the existence of an allocation giving each client its entitlement on some global system-wide bottleneck resource.
While these models and algorithms make significant advances to the problem of defining multi-resource fairness, they do not deal with the dual problem of their effect on system utilization.
In general these solutions tend to over constrain the system with fairness requirements, resulting in allocations with low system utilization.In this paper we propose a model called Bottleneck Aware Allocation (BAA) based on the notion of local bottleneck sets.
We present a new allocation policy to maximize system utilization while providing fairness in the allocations of the competing clients.
The allocations of BAA enjoy all of the fairness properties of DRF [17], like Sharing Incentive, Envy Freedom [26,7,6], and Pareto Optimality.
However, within this space of "fair" solutions that includes DRF, it searches for alternative solutions with higher system efficiency.
We prove formally that BAA satisfies these fairness properties.
We use BAA as part of a two-tier allocate and schedule mechanism: one module uses a standard weighted fairscheduler to dispatch requests to the storage system; the other module monitors the workload characteristics and dynamically recomputes the weights using BAA for use by the dispatcher, based on the mix of workloads and their access characteristics.
We evaluate the performance of our method using simulation and a Linux platform.
The results show that our method can provide both high efficiency and fairness for heterogeneous storage and dynamic workloads.The rest of the paper is organized as follows.
In Section 2 we discuss the difficulties of achieving both fairness and efficiency in a heterogeneous storage system.
In Section 3 we describe our model and approach to balance needs of fairness and system efficiency.
Formal proofs are presented in Section 4.
We present some empirical results in Section 5.
Related work is summarized in Section 6, and we conclude in Section 7.
The storage system is composed of SSDs and HD arrays, as shown in Figure 1.
SSDs and HDs are independent A client makes a sequence of IO requests; the target of each request is either the SSD or the HD, and is known to the scheduler.
An access to the SSD is referred to as a hit and access to the HD is a miss.
The hit (miss) ratio of client i is the fraction of its IO requests to the SSD (HD), and is denoted by h i (respectively m i ).
The hit ratio of different applications will generally be different.
It may also change in different application phases, but is assumed to be relatively stable within an application phase.The requests of different clients are held in clientspecific queues from where they are dispatched to the storage array by an IO scheduler.
The storage array keeps a fixed number of outstanding requests in its internal queues to maximize its internal concurrency.
The IO scheduler is aware of the target device (SSD or HD) of a request.
Its central component is a module to dynamically assign weights to clients based on the measured miss ratios.
These weights are used by the dispatcher to choose the order of requests to send to the array; the number of serviced requests of a client is in proportion to its weight.
The weights are computed in accordance with the fairness policies of BAA so as to maximize system utilization based on recently measured miss ratios.We illustrate the difficulties in achieving these goals in the next section, followed by a description of our approach in Section 3.
In traditional proportional fairness a single resource is divided among multiple clients in the ratio of their assigned weights.
For instance, if a single disk of 100 IOPS capacity is shared among two backlogged clients of equal weight, then each client will receive 50 IOPS.
A work-conserving scheduler like Weighted Fair Queuing [9] will provide fine-grained, weight-proportional bandwidth allocation to the backlogged clients; the system will be 100% utilized as long as there are requests in the system.
When the IOs are from heterogeneous devices like HDs and SSDs, the situation is considerably more complicated.
The device load is determined by both the allocation ratios (the relative fraction of the bandwidth assigned to clients), as well as their hit ratios.
If the clients with high SSD loads have small allocation ratio, there may be insufficient requests to keep the SSD busy.
Conversely, maintaining high utilization of both the HD and the SSD may require adjusting the allocations in a way that starves some clients, resulting in unfairness in the allocation.Example I Suppose the HD and SSD have capacities of 100 IOPS and 500 IOPS respectively.
The system is shared by backlogged clients 1 and 2 with equal weights and hit ratios of h 1 = 0.5 and h 2 = 0.9 respectively.
Under proportional-share allocation, both clients should get an equal number of IOPS.
The unique allocation in this case is for each client to get 166.6 IOPS.
Client 1 will get 83.3 IOPS from each device, while client 2 will get 16.6 IOPS from the HD and 150 IOPS from the SSD.
The HD has 100% utilization but the SSD is only 47% utilized (Figure 2(a)).
In order to increase the system utilization, the relative allocation of the clients needs to be changed (Figure 2(b)).
In fact, both devices can be fully utilized if the scheduler allocates 100 IOPS to client 1 (50 IOPS from the HD and 50 from the SSD), and 500 IOPS to client 2 (50 from the HD and 450 from the SSD).
This is again the unique allocation that maximizes the system utilization for the given set of hit ratios.
Note that increasing the utilization to 100% requires a 1 : 5 allocation ratio, and reduces client 1's throughput from 167 to 100 IOPS while increasing client 2's throughput from 167 to 500 IOPS.The example above illustrates a general principle.
For a given set of workload hit ratios, it is not possible to precisely dictate both the relative allocations (fairness) and the system utilization (efficiency).
How then should we define the allocations to both provide fairness and achieve good system utilization?Consider next how the DRF policy will allocate the bandwidth.
The dominant resource for client 1 is the HD; for client 2 it is the SSD.
Suppose DRF allocates n IOPS to client 1 and m IOPS to client 2.
Equalizing the dominant shares means that 0.5n/100 = 0.9m/500 or n : m = 9 : 25.
This results in an SSD utilization of approximately 77% (Figure 2(c)).
The point to be noted is that none of these earlier approaches considers the issue of resource efficiency when deciding on an allocation.
The policies deal with the question of how to set client allocation ratios to achieve some measure of fairness, but do not address how the choice affects system utilization.Example II In the example above suppose we add a third backlogged client, also with hit ratio 0.5.
In this case, proportional sharing in a 1 : 1 : 1 ratio would result in al- locations of 90.9 IOPS each; the client with hit ratio 0.9 would be severely throttled by the allocation ratio, and the SSD utilization will be only 34.5%.
If the weights were changed to 1 : 1 : 10 instead, then the HD-bound clients (hit ratio 0.5) would receive 50 IOPS each, and the SSD-bound client (hit ratio 0.9) would receive 500 IOPS.
The utilization of both devices is 100%.
The DRF policy will result in an allocation ratio of 9 : 9 : 25, allocations of 78, 78 and 217 IOPS respectively, and SSD utilization of 55% (further reduced from the 77% of example 1).
This shows that the system utilization is highly dependent on the competing workloads, and stresses how relative allocations (fairness) and system utilization (efficiency) are intertwined.
As discussed in the previous section, the ratio of allocation and the system utilization cannot be independently controlled.
In [17], DRF allocations are shown to possess desirable fairness properties, namely:• Sharing Incentive: Each client gets at least the throughput it would get from statically partitioning each resource equally among the clients 1 .
This throughput will be referred to as the fair share of the client.
In this paper we will use fair share defined by equal partition of the resources 2 .
• Envy-Freedom: A client cannot increase its throughput by swapping its allocation with any other client.
That is, clients prefer their own allocation over the allocation of any other client.
• Pareto Efficiency: A client's throughput cannot be increased without decreasing another's throughput.We propose a bottleneck-aware allocation policy (BAA), to provide both fairness and high efficiency in multi-tiered storage systems.
BAA will preserve the desirable fairness features listed above.
However, we add a fundamentally new requirement, namely maximizing the system utilization.The clients are partitioned into bottleneck sets depending on the device on which they have a higher load.
This is a local property of a client and does not depend on the system bottleneck as in [11,12].
In our model, clients in the same bottleneck set will receive allocations that are in the ratio of their fair share.
However, there is no predefined ratio between the allocations of clients in different bottleneck sets.
Instead, the We begin by precisely defining several terms used in the model.
The capacity of a device is defined as its throughput (IOPS) when continually busy.
As is usually the case, this definition abstracts the fine-grained variations in access times of different types of requests (read or write, sequential or random etc), and uses a representative (e.g. random 4KB reads) or average IOPS number to characterize the device.
Denote the capacity (in IOPS) of the disk as C d and that of the SSD as C s .
Consider a client i that is receiving a total throughput of T IOPS.
This implies it is receiving T × h i IOPS from the SSD.
The fraction of the capacity of the SSD that it uses is (T × h i )/C s .
Similarly, the fraction of the capacity of the HD that it uses is (T × m i )/C d .
bal = C s /(C s + C d ).
A workload with hit ratio equal to h bal will have equal load on both devices.
If the hit ratio of a client is less than or equal to h bal it is said to be bottlenecked on the HD; if the hit ratio is higher than h bal it is bottlenecked on the SSD.
3.
Partition the clients into two sets D and S based on their hit ratios.
D = {i : h i ≤ h bal } and S = {i : h i > h bal } are the sets of clients that are bottlenecked on the HD and SSD respectively.
4.
Define the fair share of a client to be the throughput (IOPS) it gets if each of the resources are partitioned equally among all the clients.
Denote the fair share of client i by f i .
5.
Let A i denote the allocation of (total IOPS done by) client i under some resource partitioning.
The total throughput of the system is ∑ i A i .
Example III Consider a system with C d = 200 IOPS, C s = 1000 IOPS and four clients p, q, r, s with hit ratios h p = 0.75, h q = 0.5, h r = 0.90, h s = 0.95.
In this case, h bal = 1000/1200 = 0.83.
Hence, p and q are bottlenecked on the HD, while r and s are bottlenecked on the SSD: D = {p, q} and S = {r, s}.
Suppose the resources are divided equally among the clients, so that each client sees a virtual disk of 50 IOPS and a virtual SSD of 250 IOPS.
What are the throughputs of the clients with this static resource partitioning?Since p and q are HD-bottlenecked, they would use their entire HD allocation of 50 IOPS, and an additional amount on the SSD depending on the hit ratios.
Since p's hit ratio is 3/4, it would get 150 IOPS on the SSD for a total of 200 IOPS, while q (h q = 0.5) would get 50 SSD IOPS for a total of 100 IOPS.
Thus the fair shares of p and q are 200 and 100 IOPS respectively.
In a similar manner, r and s would completely use their SSD allocation of 250 IOPS and an additional amount on the disk.
The fair shares of r and s in this example are 277.8 and 263.2 IOPS respectively.Our fairness policy is specified by the rules below.
The rules (1) and (2) state that the allocations between any two clients that are bottlenecked on the same device are in proportion to their fair share.
Condition (3) states that clients backlogged on different devices should be envy free.
The condition asserts that if client A receives a higher throughput on some device than client B it must get an equal or lesser throughput on the other.
We will show in Section 4 that with just rules (1) and (2), the envy-free property is satisfied between any pair of clients that belong both in D or both in S. However, envy-freedom between clients in different sets is explicitly enforced by the third constraint.
∀i, j ∈ D, A i A j = f i f j .
Define ρ d = A i f ito be the ratio of the allocation of client i to its fair share, i ∈ D. ∀i, j ∈ S, A i A j= f i f j .
Define ρ s = A j f jto be the ratio of the allocation of client j to its fair share, j ∈ S. ∀i ∈ D, j ∈ S: hi h r /h p = 1.2 ≥ A p /A r ≥ m r /m p = 0.4 ii h s /h p = 1.27 ≥ A p /A s ≥ m s /m p = 0.2 iii h r /h q = 1.8 ≥ A q /A r ≥ m r /m q = 0.2 iv h s /h q = 1.9 ≥ A q /A s ≥ m s /m q = 0.1These linear constraints will be included in a linear programming optimization model in the next section.
The aim of the resource allocator is to find a suitable allocation A i for each of the clients.
The allocator will maximize the system utilization while satisfying the fairness constraints described in Section 3.1, together with constraints based on the capacity of the HD and the SSD.
A direct linear programming (LP) formulation will result in an optimization problem with n unknowns representing the allocations of the n clients, and O(n 2 ) constraints specifying the rules of the fairness policy.The search space can be drastically reduced using the auxiliary variables ρ d and ρ s (called amplification factors) defined in Section 3.1.
Rules 1 and 2 require thatA i = ρ d f i and A j = ρ s f j , for clients i ∈ D and j ∈ S.We now formulate the objective function and constraints in terms of the auxiliary quantities ρ d and ρ s .
The total allocation is:∑ ∀k A k = ( ∑ i∈D A i + ∑ j∈S A j ) = (ρ d ∑ i∈D f i + ρ s ∑ j∈S f j ).
The total number of IOPS made to the HD is:ρ d ∑ i∈D f i m i + ρ s ∑ j∈S f j m j .
The total number of IOPS made to the SSD is:ρ d ∑ i∈D f i h i + ρ s ∑ j∈S f j h j .
Fairness rule 3 states that:∀i ∈ D, j ∈ S, h j h i ≥ ρ d f i ρ s f j ≥ m j m i h j f j h i f i ≥ ρ d ρ s ≥ m j f j m i f i .
β ≥ ρ d ρ s ≥ α.whereα = max i, j 񮽙 m j f j m i f i 񮽙 β = min i, j 񮽙 h j f j h i f i 񮽙The final problem formulation is shown below.
It is expressed as a 2-variable linear program with unknowns ρ d and ρ s , and four linear constraints between them.
Equations 2 and 3 ensure that the total throughputs from the HD and the SSD respectively do not exceed their capacities.
Equation 4 ensures that any pair of clients, which are bottlenecked on the HD and SSD respectively, are envy free.
As mentioned earlier, we will show that clients which are bottlenecked on the same device will automatically be envy free.
Maximizeρ d ∑ i∈D f i + ρ s ∑ j∈S f j(1)subject to: ρ d ∑ i∈D f i m i + ρ s ∑ j∈S f j m j ≤ C d(2)ρ d ∑ i∈D f i h i + ρ s ∑ j∈S f j h j ≤ C s(3)β ≥ ρ d ρ s ≥ α(4Maximize : 300ρ d + 541ρ s(5)subject to:100ρ d + 41ρ s ≤ 200 (6) 200ρ d + 500ρ s ≤ 1000 (7) 1.67 ≥ ρ d ρ s ≥ 0.55(8)Solving the linear program gives ρ d = 1.41, ρ s = 1.44, which result in allocations A p = 282.5, A q = 141.3, A r = 398.6, A s = 377.6, and HD and SSD utilizations of 100% and 100%.
We end the section by stating precisely the properties of BAA with respect to fairness and utilization.
The properties are proved in Section 4.
• P1: Clients in the same bottleneck set receive allocations proportional to their fair shares.
• P2: Any pair of clients bottlenecked on the same device will not envy each other.
Combined with fairness policy (3) which enforces envy freedom between clients bottlenecked on different devices, we can assert that the allocations are envy free.
• P3: Every client will receive at least its fair share.
In other words, no client receives less throughput than it would if the resources had been hardpartitioned equally among them.
Usually, clients will receive more than their fair share by using capacity on the other device that would be otherwise unused.
• P4: The allocation maximizes the system throughput subject to these fairness criteria.
The LP described in Section 3.2 calculates the throughput that each client is allocated based on the mix of hit ratios and the system capacities.
The ratios of these allocations make up the weights to a proportional-share scheduler like WFQ [9], which dispatches requests from the client queues.
When a new client enters or leaves the system, the allocations (i.e. the weights to the proportional scheduler) need to be updated.
Similarly, if a change in a workload's characteristics results in a significant change in its hit ratio, the allocations should be recomputed to prevent the system utilization from falling too low.
Hence, periodically (or triggered by an alarm based on device utilizations) the allocation algorithm is invoked to compute the new set of weights for the proportional scheduler.
We also include a module to monitor the hit ratios of the clients over a moving window of requests.
The hit ratio statistics are used by the allocation algorithm.
Step 1.
For each client maintain statistics of its hit ratio over a configurable request-window W.Step 2.
Periodically invoke the BAA optimizer of Section 3.2 to compute the allocation of each client that maximizes utilization subject to fairness constraints.Step 3.
Use the allocations computed in Step 2 as relative weights to a proportional-share scheduler that dispatches requests to the array in the ratio of their weights.The allocation algorithm is relatively fast since it requires solving only a small 2-variable LP problem, so it can be run quite frequently.
Nonetheless, it would be desirable to have a single-level scheme in which the be desirable to have a single-level scheme in which the scheduler continually adapts to the workload characteristics rather than at discrete steps.
In future work we will investigate the possibility of such a single-level scheme.
In this section we formally establish the fairness claims of BAA.
The two main properties are summarized in Lemma 3 and Lemma 7, which state that the allocations made by BAA are envy free (EF) and satisfy the sharing incentive (SI) property.
Table 1: List of Symbols Lemma 1 finds expressions for fair shares.
The fair share of a client is its throughput if it is given a virtual HD of capacity C d /n and a virtual SSD of capacity C s /n.
A client in D will use all the capacity of the virtual HD, and hence have a fair share of C d /(n × m i ).
A client in S uses all the capacity of the virtual SSD, and its fair share is C s /(n × h i ).
= min{C d /(n × m i ),C s /(n × h i )}.
If i ∈ D, then f i = C d /(n × m i ); else if i ∈ S, then f i = C s /(n × h i ).
Proof.
The fair share is the total throughput when a client uses one of its virtual resources completely.
Fori ∈ D, h i ≤ h bal = C s /(C s + C d ) and m i ≥ 1 − h bal = C d /(C s + C d ).
In this case, C d /(n × m i ) ≤ (C s + C d )/n and C s /(n × h i ) ≥ (C s + C d )/n.Hence, the first term is the smaller one, whence the result follows.
A similar argument holds for i ∈ S.Lemma 2 states a basic property of BAA allocations: all clients in a bottleneck set receive equal throughputs on the device on which they are bottlenecked.
This is simply a consequence of fairness policy which requires that clients in the same bottleneck set receive throughput in the ratio of their fair shares.Lemma 2.
All clients in a bottleneck set receive equal throughputs on the bottleneck device.
Specifically, all clients in D receive ρ d C d /n IOPS from the HD; and all clients in S receive ρ s C s /n IOPS from the SSD.Proof.
Let i ∈ D. From fairness policy (1) and lemma 1,A i = ρ d f i = ρ d (C d /(n × m i )).
The number of IOPS from the HD is therefore A i m i = ρ d C d /n. Similarly, for i ∈ S, A i = ρ s f i = ρ s (C s /(n × h i )), and the number of IOPS from the SSD is A i h i = ρ s C s /n.To prove EF between two clients, we need to show that no client receives more throughput on both the resources (HD and SSD).
If the two clients are in the same bottleneck set then this follows from Lemma 2, which states that both clients will get equal throughputs on their bottleneck device.
When the clients are in different bottleneck sets then the condition is explicitly enforced by fairness policy (3).
Proof.
From lemma 2, if i, j ∈ D both clients have the same number of IOPS on the HD; hence neither can improve its throughput by getting the others allocation.
Similarly, if i, j ∈ S they do not envy each other, since nether can increases its throughput by receiving the others allocation.Finally, we consider the case when i ∈ D and j ∈ S. From fairness policy (3), ∀i ∈ D, j ∈ S:h j h i ≥ A i A j ≥ m j m i .
Hence, the allocations on the SSD for clients i and j satisfy A i h i ≤ A j h j , and the allocations on HD for clients i and j satisfy A i m i ≥ A j m j .
So any two flows in different bottleneck sets will not envy each other.
Hence neither i nor j can get more than the other on both devices.The following Lemma shows the Sharing Incentive property holds in the "simple" case.
The more difficult case is shown in Lemma 6.
Informally, if the HD is a system bottleneck (i.e., it is 100% utilized) then Lemma 4 shows that the clients in D will receive at least 1/n of the HD bandwidth.
The clients in S may get less than that amount on the HD (and usually will get less).
Similarly, if the SSD is a system bottleneck, then the clients in S will receive at least 1/n of the SSD bandwidth.
In the remainder of this section we assume that the clients 1, 2, ·· ·n, are ordered in non-decreasing order of their hit ratios, and that r of them are in D and the rest in S. Hence, D = {1, ·· · , r} and S = {r + 1, ·· · , n}.
Lemma 4.
Suppose the HD (SSD) has a utilization of 100%.
Then every i ∈ D (respectively i ∈ S) receives a throughput of at least f i .
Proof.
Let j denote an arbitrary client in S. From fairness policy (3), A i m i ≥ A j m j .
That is, the throughput on the HD of a client in D is greater than or equal to the throughput on the HD of any client in S. Now, from lemma 2 the IOPS from the HD of all i ∈ D are equal.
Since, by hypothesis, the disk is 100% utilized, the total IOPS from the HD is C d .
Hence, for every i ∈ D, the IOPS on the disk must be at least C d /n.
A symmetrical proof holds for clients in S.In order to show the Sharing Incentive property for clients whose bottleneck device is not the system bottleneck (i.e. is less than 100% utilized), we prove the following Lemma.
Informally, it states that utilization of the SSD improves if the clients in S can be given a bigger allocation.
The result, while intuitive, is not self evident.
An increase in the SSD allocation to a client in S increases its HD usage as well.
Since the HD is 100% utilized, this reduces HD allocations of clients in D, which in turn reduces their allocation on the SSD.
We need to check that the net effect is positive in terms of SSD utilization.Lemma 5.
Consider two allocations that satisfy fairness policy (1) - (3), and for which the HD has utilization of 100% and the SSD has utilization less than100%.
Let ρ s andˆρandˆ andˆρ s be the proportionally constants of clients in S for the two allocations, and let U andˆUandˆ andˆU be the respective system throughputs.
IfˆρIfˆ Ifˆρ s > ρ s thenˆUthenˆ thenˆU > U.
A symmetrical result holds if the SSD is 100% utilized and the HD is less than 100% utilized.Proof.
We show the case for HD 100% utilized.
From Lemma 2, all clients in S have the same throughput ρ s C s /n on the SSD.
Define δ s to be the difference between the SSD throughputs of a client in S in the two allocations.
Sincê ρ s > ρ s , δ s > 0.
Similarly, define δ d to be difference between the HD throughput of a client in D in the two allocations.An increase of δ s in the throughput of client i ∈ S on the SSD implies an increase on the HD of δ s × (m i /h i ).
Since the HD is 100% utilized in both allocations, the aggregate allocations of clients in D must decrease by the total amount ∑ i∈S δ s × (m i /h i ).
By Lemma 2, since all clients in D have the same allocation on the HD, δ d = ∑ i∈S δ s × (m i /h i )/|D|.
As a result, the decrease in the al-location of client j ∈ D on the SSD isˆδisˆ isˆδ s = δ d × (h j /m j ).
The total change in the allocation on the SSD in the two allocations, ∆ is therefore: ∆ = ∑ i∈S δ s − ∑ j∈Dˆδ j∈Dˆ j∈Dˆδ s .
Substituting:∆ = ∑ i∈S δ s − ∑ j∈D δ d × (h j /m j ) (9) ∆ = |S| × δ s − ∑ j∈D ( ∑ i∈S δ s × (m i /h i )/|D|) × (h j /m j ) (10) Now for all i ∈ S, (m i /h i ) ≤ (m r+1 /h r+1 ) and for all j ∈ D, (h j /m j ) ≤ (h r /m r ).
Substituting in Equation 10:∆ ≥ |S| × δ s − |S|δ s × (m r+1 /h r+1 ) × (h r /m r ) (11) ∆ ≥ |S| × δ s (1 − m r+1 m r × h r h r+1 )(12)Now, m r+1 < m r and h r < h r+1 since r and r + 1 are in D and S respectively.
Hence, ∆ > 0.
Finally, we show the Sharing Incentive property for clients whose bottleneck device is not the system bottleneck.
The idea is to make the allocation to the clients in S as large as we can, before the EF requirements prevent further increase.Lemma 6.
Suppose the HD (SSD) has utilization of 100% and the SSD (HD) has utilization less than 100%.
Then every i ∈ S (respectively i ∈ D) receives a throughput of at least f i .
Proof.
We will show it for clients in S.
A symmetrical proof holds in the other case.Since BAA maximizes utilization subject to fairness policy (1) -(3), it follows from Lemma 5 that ρ s must be as large as possible.
If i ∈ S, the IOPS it receives on the HD are ρ s C s /n × (m i /h i ) which from the EF requirements of Lemma 3 must be no more than ρ d C d /n, the IOPS on the HD for any client inD.
Hence, ρ s C s /n × (m i /h i ) ≤ ρ d C d /n or ρ s ≤ ρ d (C d /C s ) × (h i /m i ), for all i ∈ S.
Since h i /m i is smallest for i = r + 1, the maximum feasible value of ρ s is ρ s = ρ d (C d /C s ) × (h r+1 /m r+1 ).
Now, h r+1 > h bal , so h r+1 /m r+1 > h bal /(1 − h bal ) = C s /C d .
Hence ρ s > ρ d .
Since the HD is 100% utilized we know from Lemma 4 that ρ d ≥ 1, and so ρ s > 1.
From Lemmas 4 to 6 we can conclude:Lemma 7.
Allocations made by BAA satisfy the Sharing Incentive property.
We evaluate our work using both simulation and Linux system implementation.
For simulation, a synthetic set of workloads was created.
Each request is randomly assigned to the SSD or HD based on its hit ratio.
The request service time is an exponentially distributed random variable with mean equal to the reciprocal of the device IOPS capacity.In the Linux system, we implemented a prototype by interposing the BAA scheduler in the IO path.
Raw IO is performed to eliminate the influence of OS buffer caching.
The storage server includes a 1TB SCSI Western Digital hard disk (7200 RPM 64MB Cache SATA 6.0Gb/s) and 120GB SAMSUNG 840 Pro Series SSD.
Various block-level workloads from UMass Trace Repository [1] and Microsoft Exchange server [31] are used for the evaluation.
These traces are for a homogeneous server and do not distinguish between devices.
Since we needed to emulate different proportions of HD and SSD requests we randomly partitioned the blocks between the two devices to meet the assumed hit ratio of the workload.
The device utilizations are measured using Linux tool "iostat".
This experiment compares the system efficiency for three different schedulers: Fair Queuing (FQ), DRF, and BAA.
The capacities of the HD and SSD are 100 IOPS and 5000 IOPS respectively.
The first experiment employs two clients with hit ratios 0.5 and 0.99.
FQ allocates equal amounts of throughput to the two clients.
The DRF implementation uses the dominant resource shares policy of [17] to determine allocation weights, and BAA is the approach proposed in this paper.
All workloads are assumed to be continuously backlogged.The throughputs of the two clients with different schedulers are shown in Figure 3(a).
The figure also shows the fair share allocation, i.e. the throughput the workload would get by partitioning the SSD and HD capacities equally between the two workloads.
As can be seen, the throughput of client 2 under FQ is the lowest of the three schedulers.
In fact, sharing is a disincentive for client 2 under FQ scheduling, since it would have been better off with a static partitioning of both devices.
The problem is that the fair scheduler severely throttles the SSD-bound workload to force the 1 : 1 fairness ratio.
DRF performs much better than FQ.
Both clients get a little more than their fair shares.
BAA does extremely well in this setup and client 2 is able to almost double the throughput it would have received with a static partition.
We also show the system utilization for the three schedulers in Figure 3(b).
BAA is able to fully utilize both devices, while DRF reaches system utilization of only around 65%.
Next we add another client with hit ratio of 0.8 to the workload mix.
The throughputs of the clients are shown in Figure 4(a).
Now the throughput of the DRF scheduler is also degraded, because it does not adjust the relative allocations to account for load imbalance.
The BAA scheduler gets higher throughput (but less than 100%) because it adjusts the weights to balance the system load.
The envy-free requirements put an upper-bound on the SSD-bound client's throughput, preventing the utilization from going any higher, but still maintaining fairness.
In this experiment, we show how the two-level scheduling framework restores system utilization following a change in an application's hit ratio.
The capacities of the HD and SSD are 200 IOPS and 3000 IOPS respectively.
In this simulation, allocations are recomputed every 100s and the hit ratio is monitored in a moving window of 60s.
There are two clients with initial hit ratios of 0.45 and 0.95.
At time 510s, the hit ratio of client 1 falls to 0.2.
Figure 5 shows a time plot of the throughputs of the clients.
The throughputs of both clients falls significantly at time 510 as shown in Figure 5.
The scheduler needs to be cognizant of changes in the application characteristics and recalibrate the allocations to increase the efficiency.
At time 600s (the rescheduling interval boundary) the allocations are recomputed using the hit ratios that reflect the current application behavior, and the system throughput rises again.In practice the frequency of calibration and the rate at which the workload hit ratios change can affect system performance and stability.
As is the case in most adaptive situations, the techniques work best when significant changes in workload characteristics do not occur at a very fine time scale.
We leave the detailed evaluation of robustness to future work.
We now evaluate BAA in a Linux system, and compare its behavior with allocations computed using the DRF policy [17] and the Linux CFQ [39] scheduler.
The first set of experiments deals with evaluating the throughputs (or system utilization) of the three scheduling approaches.
The second set compares the fairness properties.
Clients in the same bottleneck set.
Two workloads from Web Search [1] are used in this experiment.
The requests include reads and writes and the request sizes range from 8KB to 32KB.
We first evaluate the performance when all the clients fall into the same bottleneck set; that is, all the clients are bottlenecked on the same device.
We use hit ratios of 0.3 and 0.5 for the two workloads which makes them both HD bound.
As shown in Table 2 all three schedulers get similar allocation.
In this situation there is just one local bottleneck set in BAA, which (naturally) coincides with the system bottleneck device for CFQ as well as being the dominant resource for DRF.
The device utilizations are the same for all schedulers, as can be expected.
Clients in different bottleneck sets.
In this experiment, we evaluate the performance when the clients fall into different bottleneck sets; that is, some of the clients are bottlenecked on the HD and some on the SSD.
Two clients, one running a Financial workload [1] (client 1) and the second running an Exchange workload [31] (client 2) with hit ratios of 0.3 and 0.95 respectively, are used in the experiment.
The request sizes range from 512 bytes to 8MB, and are a mix of read and write requests.
The total experiment time is 10 minutes.
Figure 6 shows the throughput of each client achieved by the three schedulers.
As shown in the figure, BAA has better total system throughput than the others.
CFQ performs better than DRF but not as good as BAA.
Figure 7 shows the measured utilizations for HD and SSD using the three schedulers.
Figure 7(a) shows that BAA achieves high system utilization for both HD and SSD; DRF and CFQ have low SSD utilizations compared with BAA, as shown in Figure 7(b) and (c).
HD utilizations are good for both DRF and CFQ (almost 100%), because the system has more disk-bound clients that saturate the disk.
In this experiment, we evaluate the fairness properties of allocations (P1 to P4).
Four Financial workloads [1] with hit ratios of 0.2, 0.4, 0.98 and 1.0 are used as the input.
The workloads have a mix of read and write requests and request sizes range from 512 bytes to 8MB.
Table 3: Allocations for Financial workloads using BAA Table 3 shows the allocations of BAA-based scheduling.
The second column shows the Fair Share for each workload.
The third column shows the IOPS achieved by each client, and the portions from the HD and SSD are shown in the next two columns.The average capacity of the HD for the workload is around 140-160 IOPS and the SSD is 2000-2200 IOPS.
We use the upper-bound of the capacity to compute the fair shares shown in the second column.
In this setup, Financial 1 and Financial 2 are bottlenecked on the HD and belong to D, while Financial 3 and Financial 4 are bottlenecked on the SSD and belong to S.First we verify that clients in the same bottleneck set receive allocations in proportion to their fair share (P1).
As shown in the Table 3, Financial 1 and 2 get throughputs of 76 and 101, which are in the same ratio as their fair share (50 : 67).
Similarly, Financial 3 and 4 get throughputs 1068 and 1047, which are in the ratio of their fair share of (561 : 550).
HD-bottlenecked workloads Financial 1 and Financial 2 receive more HD allocation (60.8 IOPS) than both workloads Financial 3 (21.4 IOPS) and 4 (0 IOPS).
Similarly, SSD-bottlenecked workloads Financial 3 and Financial 4 receive more SSD allocation (1047 and 1047 IOPS) than both workload 1 (15.2 IOPS) and 2 (40.4 IOPS).
It can be verified from columns 2 and 3 that every client receives at least its fair share.
Finally, the system shows that both HD and SSD are almost fully utilized, indicating the allocation maximizes the system throughput subject to these fairness criteria.
Similar experiments were also conducted with other workloads, including those from Web Search and Exchange Servers.
The results show that properties P1 to P4 are always guaranteed.
There has been substantial work dealing with proportional share schedulers for networks and CPU [9,18,44].
These schemes have since been extended to handle the constraints and requirements of storage and IO scheduling [21,19,20,45,32,27,33].
Extensions of WFQ to provide reservations for constant capacity servers were presented in [41].
Reservation and limit controls for storage servers were studied in [29,46,22,24].
All these models provide strict proportional allocation for a single resource based on static shares possibly subject to reservation and limit constraints.As discussed earlier, Ghodsi et al [17] proposed the DRF policy, which provides fair allocation of multiple resources on the basis of dominant shares.
Ghodsi et al. [16] extended DRF to packet networks and compared it to the global bottleneck allocation scheme of [12].
Dolev et al [11] proposed an alternative to DRF based on fairly dividing a global system bottleneck resource.
Gutman and Nisan [25] considered generalizations of DRF in a more general utility model, and also gave a polynomial time algorithm for the construction in Dolev et al [11].
Parkes et al. [36] extended DRF in several ways, and in particular studied the case of indivisible tasks.
Envy-freedom has been studied in the areas of economics [26] and in game theory [10].
Techniques for isolating random and sequential IOs using time-quanta based IO allocation were presented in [37,34,42,43,39,8].
IO scheduling for SSDs is examined in [34,35].
Placement and scheduling tradeoffs for hybrid storage were a studied in [47].
For a multitiered storage system, Reward scheduling [13,14,15] proposed making allocations in the ratio of the throughputs a client would receive when executed in isolation.
Interestingly, both Reward and DRF perform identical allocations for the storage model of this paper [14] (concurrent operation of the SSD and the HD), although they start from very different fairness criteria.
Hence, Reward also inherits the fairness properties proved for DRF [17].
For a sequential IO model where only 1 IO is served at a time, Reward will equalize the IO time allocated to each client.
Note that neither DRF nor Reward explicitly address the problem of system utilization.In the system area, Mesos [5] proposes a two-level approach to allocate resources to frameworks like Hadoop and MPI that may share an underlying cluster of servers.
Mesos (and related solutions) rely on OS-level abstractions like resource containers [4].
Multi-tiered storage made up of heterogeneous devices are raising new challenges in providing fair throughput allocation among clients sharing the system.
The fundamental problem is finding an appropriate balance between fairness to the clients and increasing system utilization.
In this paper we cast the problem within the broader framework of fair allocation for multiple resources, which has been drawing considerable amount of recent research attention.
We find that existing methods almost exclusively emphasize the fairness aspect to the possible detriment of system utilization.We presented a new allocation model BAA based on the notion of per-device bottleneck sets.
The model provides clients that are bottlenecked on the same device with allocations that are proportional to their fair shares, while allowing allocation ratios between clients in different bottleneck sets to be set by the allocator to maximize utilization.
We show formally that BAA satisfies the properties of Envy Freedom and Sharing Incentive that are well accepted fairness requirements in microeconomics and game theory.
Within these fairness constraints BAA finds the best system utilization.
We formulated the optimization as a compact 2-variable LP problem.
We evaluated the performance of our method using both simulation and implementation on a Linux platform.
The experimental results show that our method can provide both high efficiency and fairness.One avenue of further research is to better understand the theoretical properties of the Linux CFQ scheduler.
It performs remarkably well in a wide variety of situations; we feel it is important to better understand its fairness and efficiency tradeoffs within a suitable theoretical framework.
We are also investigating single-level scheduling algorithms to implement the BAA policy, and plan to conduct empirical evaluations at larger scale beyond our modest experimental setup.Our approach also applies, with suitable definitions and interpretation of quantities, to broader multiresource allocation settings as in [17,11,36], including CPU, memory, and network allocations.
It can also be generalized to handle client weights; in this case clients in the same bottleneck set receive allocations in proportion to their weighted fair shares.
We are also investigating settings in which the SSD is used as a cache; this will involve active data migration between the devices, making the resource allocation problem considerably more complex.
We thank the reviewers of the paper for their insightful comments which helped shape the revision.
We are grateful to our shepherd Arif Merchant whose advice and guidance helped improve the paper immensely.
The support of NSF under Grant CNS 0917157 is greatly appreciated.
