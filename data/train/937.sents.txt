We consider the task of assigning experts from a portfolio of specialists in order to solve a set of tasks.
We apply a Bayesian model which combines collaborative filtering with a feature-based description of tasks and experts to yield a general framework for managing a portfolio of experts.
The model learns an embedding of tasks and problems into a latent space in which affinity is measured by the inner product.
The model can be trained incrementally and can track non-stationary data, tracking potentially changing expert and task characteristics.
The approach allows us to use a principled decision theoretic framework for expert selection, allowing the user to choose a utility function that best suits their objectives.
The model component for taking into account the performance feedback data is pluggable, allowing flexibility.
We apply the model to manage a portfolio of algorithms to solve hard combinatorial problems.
This is a well studied area and we demonstrate a large improvement on the state of the art in one domain (constraint solving) and in a second domain (com-binatorial auctions) created a portfolio that performed significantly better than any single algorithm.
When faced with a diverse set of difficult tasks, a general approach which would be effective for all of them is likely to be complex.
In a given domain a specialist approach to a subset of tasks is often simpler and more effective at those tasks.
This suggests a divide and conquer approach using a portfolio of specialist experts.An example of this is the case where the expert is an algorithm [Smith-Miles, 2008;Gomes and Selman, 1997;Horvitz et al., 2001;Rice, 1976;Streeter and Smith, 2008].
A portfolio approach means that several parallel and independent efforts to design an algorithm for a task can be combined so as to leverage the best aspects of each of them.
An example class of tasks where an algorithm portfolio can be beneficial is hard combinatorial problems where typically there does not exist one single approach that outperforms any other across a range of real-world problems [Xu et al., 2008;Leyton-Brown, Nudelman, and Shoham, 2009].
Be- cause of their exponentially large search spaces, these problems are in general computationally intractable and depending on the properties of the problem one particular solver performs better than another according to some measure (e.g., run-time).
A vast discrepancy in performance arises because solvers commit to different heuristic choices or employ different techniques.
There is a successful line of research showing that the variance in algorithm performance can be exploited by machine learning methods to produce a portfolio of algorithms that greatly outperforms any individual algorithm in domains such as constraint reasoning [Pulina and Tacchella, 2009;Xu et al., 2008;Gomes and Selman, 1997] and combinatorial auctions [Leyton-Brown, Nudelman, and Shoham, 2009].
The goal of this paper is to address some of the limitations of previous work and provide a general framework for solving diverse tasks with a portfolio of experts.
Besides algorithms, other examples of expert portfolios to which we are interested in applying this approach include programming languages 1 and Yahoo Answers (where the experts are humans).
We consider the following properties desirable:• The system should select a specific scheduling strategy for each task (based on task features) [Streeter and Smith, 2008].
• The model must be trained on-line so the model can immediately take account of each outcome to improve future decisions.
The computation cost of this update should not depend on the number of previously seen problems [Pulina and Tacchella, 2008].
• The system should adapt continuously over time, tracking a changing domain and changing expert characteristics.
• There should be a principled method to choose an expert at any point in time when performing a task.
• The sub-system for taking into account performance should be pluggable as depending on the type of task, feedback comes in different forms.
For example, for constraint solving we have data for solution speed whereas for the task of assigning human experts to answer questions we might have human judgments of the quality of the answer.We require a model which takes a description of each task (in the form of a set of features) and predicts which expert, from a set of experts, would be best suited to solve it (based on past performance).
There appears to be a striking similarity between this problem and the well studied challenge of recommending items to users of a web service (e.g. Amazon or Netflix), where users are analogous to tasks and items are analogous to experts.
Two approaches are typically available in that case.
Firstly, content-based approaches make use of descriptions (feature vectors) of users and items.
For example, items may be described by properties such as author and manufacturer.
Secondly, collaborative filtering approaches allow us to learn about a user by the items they have previously rated and the users who have rated items in common with them, using implicit descriptions of users and items obtained from a (sparse) matrix of previous ratings of items by users [Breese, Heckerman, and Kadie, 1998;Varian and Resnick, 1997].
Matchbox [Stern, Herbrich, and Graepel, 2009] is a recommendation system model which combines these two sources of information and here we apply it to recommending experts to tasks.
Unlike other collaborative filtering models, Matchbox takes feature vectors of the task and expert as the input.
It maps these features into a shared latent space in which the performance of an expert on a task is measured by the inner product.
For expert recommendation we crucially rely on this combination of collaborative filtering with generalising features as we usually need to predict the best expert for a task which has not been previously seen.
In addition, Matchbox addresses the other desiderata: it is a full Bayesian model and hence allows us to use a principled decision theoretic approach to expert selection, it can be trained incrementally and it is able to dynamically track non-stationary distributions.
Finally, it allows free choice of feedback model.
A Bi-linear Model of Expert Performance Initially, let us assume that each time a task has been attempted by an expert we have available a triple (x, y, r) of task descriptions x ∈ R n , expert descriptions y ∈ R m and a performance score r ∈ R indicating the degree of success of the expert on this task.
Following Stern, Herbrich, and Graepel [2009], we define the K dimensional task trait vector as s = Ux where U is a K × n matrix of latent task traits where each element u ki is the contribution of feature i to user trait dimension k. Similarly we define the K dimensional expert trait vector as t = Vy for a K × m expert trait matrix, V.
We also model a bias, b = x u + y v where u and v are the biases for each element in the task and expert description vectors.
Now, the performance, r, of the expert is modeled asp(r|s, t, b) = N (r|s t + b, β 2 )where β is the standard deviation of the observation noise.Thus we adopt a bi-linear form in which the performance of an expert on a task is given by the inner product of a vector of task traits and a vector of expert traits.
The expected rating is proportional to the lengths ||s|| and ||t|| of the trait vectors Figure 1: A learned embedding of the 11 solvers and 2570 problems into a 2-dimensional trait space for K = 2.
The mean of the first solver trait v (0)i is plotted against the mean of the second solver trait v (1)j to produce the blue dots.
The mean of the first problem trait u (0)i is plotted against the mean of the second problem trait u (1)j to produce the red crosses.
Similarity in this space depends on the angle between trait vectors.
and the cosine of the angle between them.
¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¦ § ¨ © ¨ © ! "
# $ % & ' ( ) 0 1 2 0 3 !
4 # 5 6 7 2 2 ' 0 8 ¦ 8 ¡ ¥ 8 ¡ ¤ 8 ¡ £ 8 ¡ ¢ 8 ¦ 8 ¡ ¥ 8 ¡ ¤ 8 ¡ £ 8 ¡ ¢ ¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¦ 9 @ 0 ( 7 1 A B C 4 ( D E F F B ( ( B ' G 0 H ( ( B ' G 0 ( ( B ' G 0 IThe model parameters to be learned are the variables U and V which determine how tasks and experts are mapped to the K dimensional trait space, and u and v, the value of bias features for the rating.
We represent our prior beliefs about the values of these parameters by independent Gaussian distributions on each of the components of the matrices U and V and u and v. For example we havep(U) = K k=1 n i=1 N (u ki ; µ ki , σ 2 ki ).
We choose this factorizing prior because it reduces memory requirements to two parameters (a mean and variance) for each component and allows us to perform efficient inference.
We set the mean of each Gaussian prior to zero and the variance to unity.Or approach allows us to track the performance of experts on tasks dynamically in order to adapt to changing conditions.
The characteristics of experts and tasks may change with time.
For example if the expert is an algorithm then it could be modified in situ by applying an update.
We model these dynamics by assuming that the latent variables U, V, u and v drift with time by the addition of Gaussian noise each time step.
For the example we have p(u(t+1) ki |u (t) ki ) = N (u (t+1) ki ; u (t) ki , γ 2 )where t is an index over time steps.
At time t 0 we use the priorp(u (0) ki ) = N (u ki ; µ ki , σ 2 ki ).
Analogous models may be used for each of the other latent variables.
The and b = 1.
That is, p(z k |s k , t k ) = I(z k = s k t k ).
Now the latent performance (before adding noise) is given byp(˜ r|z, b) = I(˜ r = k z k + b).
We have that p(s k |U, x) = I(s k = i u ki x i ) and p(t k |V, y) = I(t k = j v kj y j ) and p(b|x, y, u, v) = I(b = x u + y v).
Therefore the joint distribution of all the variables factorizes as p(s, t, U, V, u, v, z, ˜ r, r|x, y)= p(r|˜rr|˜r)p(˜ r|z, b)p(b|x, y, u, v)p(U)p(V)p(u)p(v) · K k=1 p(z k |s k , t k )p(s k |U, x)p(t k |V, y).
The posterior distribution over the U, V, u and v variables given an observation, (x, y, r), is given by summing out the latent variables:p(U, V, u, v|r, x, y) ∝ s t z ˜ r p(s, t, U, V, u, v, z, ˜ r, r|x, y) ds dt dz d˜rd˜r.This inference is performed by approximate message passing on the equivalent factor graph [Kschischang, Frey, and Loeliger, 2001].
We assume a full factorization of the joint distribution and minimize an α-divergence (a generalization of the Kullback-Leibler divergence) between the true and approximate marginals [Minka, 2005].
We approximate all factors by Gaussian distributions.
The factor graph for this model and details of all the message calculations and update schedule are given in Stern, Herbrich, and Graepel [2009].
Once the posterior marginals for the variables U, V, u and v have been calculated, we can discard all of the messages and continue, using this posterior as the prior for the variables for the next rating.
In this way we obtain a onepass, on-line algorithm with low memory overhead.
A Time-Based Feedback Model In order to evaluate Matchbox for expert portfolio selection we consider two domains in which the experts are algorithms and the performance feedback is run-time.
For each each task, there is a time-out, T , after which the algorithm is terminated and the problem is not solved.
Typically, the distribution over the time it takes for algorithms to solve hard combinatorial problems has a long tail [ Gomes et al., 2000].
With a modest time-out this means many problems are solved quite quickly but also many reach time-out with the remaining solution times being roughly evenly distributed between the time-out and the peak of fast solutions.
To handle this, we model run time as a uniform distribution given the latent rank, l, of the performance of the algorithm on this task leading to a piecewise constant predictive distribution for time:p(t|l = 1) = I(t = T ) (time out), p(t|l = 2) = 1 T −t c if t c < t < T 0 otherwise (slow),(1)p(t|l = 3) = 1 tc if t < t c 0 otherwise (fast),where t c is the (fixed) cut-off time between a 'fast' solution and a 'slow' solution, typically t c T .
If l = 1 the algorithm reached time out, if l = 2 the algorithm solved the problem slowly and if l = 3 the algorithm solved the problem quickly.The mapping from rank to latent performance may not be linear and this mapping can change from solver to solver.
We relate the latent performance r to ranks l via a cumulative threshold model [Chu and Ghahramani, 2005;Stern, Herbrich, and Graepel, 2009].
For each solver, v, we maintain solver-specific thresholds h v ∈ R 2 which divide the latent rating axis into three consecutive intervals (h v(i−1) , h v(i) ) each of which representing the region in which this solver attains a performance in the same rank.
Formally, we define a generative model of a ranking asp(l = a|h v , r) =    I(r < ˜ h v0 )I(r < ˜ h v1 ) if a = 1 I(r > ˜ h v0 )I(r < ˜ h v1 ) if a = 2 I(r > ˜ h v0 )I(r > ˜ h v1 ) if a = 3, where p( ˜ h vi |h vi , τ ) = N ( ˜ h vi ; h vi , τ 2 ), and we place an independent Gaussian prior on the thresholds so p(h vi ) = N (h vi ; µ i , σ 2 i ).
The indicator function I(·) is equal to 1 if the proposition in the argument is true and 0 otherwise.
Inference for the ordinal regression observation model is performed by approximate Gaussian Expectation Propagation (EP) message passing on the factor graph as described in detail for this model in [Stern, Herbrich, and Graepel, 2009].
Note that the marginal distribution for each solver threshold must be stored.Utility Function Faced with a fresh task, we must decide which algorithm to apply.
We assume that there is some benefit for the user in solving the task but also a cost for each additional unit of time spent.
Also, we realise that the balance between these objectives may vary from user to user so we use a parametrized utility function with two parameters: b, the benefit of solving a problem and c, the cost per unit time spent.
The utility function is defined as Table 2: Batch training QBF solver portfolio performancenumber solved, time per task and total utility for c = 1 T and b = 1.
The model is first trained on the training data and then used to select which solver to use for each problem in the test problems.
One solver is tried for each test problem.
The performances of AQME, the best individual solver (QuBE 6.1), and the Oracle portfolio solver are included.
, 2009]).
u(t) = b − ct if t < T −cT otherwise .
(2)Solver E R Se Sk 2clsQ √ Nenofex √ QMRES √ Quantor √ √ QuBE 3.0 √ QuBE 6.1 √ √ sKIZZO √ ssolve √ yQuaffle √For the applications here, we represent each solver by only its identity, y, so y is a unit vector with a one in position y.
We choose a solver to apply to a task byˆybyˆ byˆy = argmax y E [u(t)] p(t|x,y) .
Formulas (QBF) are a powerful logical formalism to represent real-world problems, and a range of QBF solvers exist to tackle problems represented in this language [Samu- lowitz, 2008].
Due to the vast computational complexity of this problem, solvers display a large discrepancy in performance which is mainly due to different heuristic choices or employed solving techniques.
Pulina and Tacchella [2009] introduced a self-adaptive version of an algorithm portfolio (AQME) to increase the robustness of QBF solving.
AQME is self-adaptive in the sense that a classifier is retrained offline based on the information collected during execution.After finishing a run this new data point is added to the initial training set and the policy of the portfolio is updated accordingly to adapt to non-stationary data and increase the size of the training set.
Table 4: On-line training, 'Trust the Predicted Solver', timeout 600s, c = 1 T and b = 1.
For this experiment the model was trained incrementally after each formula was attempted.
For each problem we first allocate t l seconds to each solver to attempt the problem, ordering the solvers by the expected utility.
If none of the solvers can solve the problem in t l seconds then we give the rest of the time before timeout to the best solver predicted by the model.
Table 3 broadly characterizes each solver by the main techniques employed in QBF solving.
The time-out is T = 600 seconds and the value for t c was set to 50s (manually tuned).
We performed experiments using 2282 test problems and 288 training instances 2 .
First, we show in Figure 1 a learned embedding of the solvers and problems into a 2-dimensional trait space.
Matchbox was trained on the full data set with 20 iterations of message passing.
The mean of the first solver trait v (0)i is plotted against the mean of the second solver trait v (1)j to produce the blue dots.
The mean of the first problem trait u (0)i is plotted against the mean of the second problem trait u (1)j to produce the red crosses.
Similarity in this space depends on the angle between trait vectors so similar solvers should be close in this space (in terms of angle) and this is indeed the case (except for ssolve), as shown in Figure 1.
Now we evaluate the performance of a portfolio of solvers using Matchbox and the utility function (2) for b = 1 and c = 1 T .
AQME 3 only uses a subset of the solvers, leaving out Nenofex, QMRES, QuBE 3.0 and ssolve (A,B) so we also leave out these solvers from our test set.
Table 2 shows the performance of Matchbox when learning on 288 training instances and predicting a solver on the test instances without on-line adaptation.
We show results for different numbers of latent dimensions K and different subsets of features (defined by Pulina and Tacchella [2009]) and the performance of AQME and the ideal portfolio manager (the oracle which always selects the fastest algorithm for each task).
In terms of pure runtime we also outperform AQME (Matchbox total run-time is 111,597s versus 273,397s for AQME).
We also compare the performance of the individual solvers on the same data in Table 1.
In Table 4 we show the performance of Matchbox in the on-line setting.
Here we use T = 600s, c = 1 T and b = 1, K = 2, and all features.
We attempt each problem in two stages according to the 'Trust the Predicted Solver' (TTPS) technique [Pulina and Tacchella, 2009]: Firstly, we allocate t l seconds to each solver to attempt the problem, ordering the solvers by the expected utility.
In this round AQME sorts the solvers according to the ranking determined in the QBF competition 2006.
If none of the solvers can solve the problem in t l seconds then we enter the second round in which we give the remaining time to the best solver predicted by the model.
In this way the setting of t l allows for a trade-off between speed and robustness.
The motivation for this approach is due to the long-tailed distribution of run-times: an algorithm is most likely to either solve a problem quickly or time out [Horvitz et al., 2001].
After all attempts to solve this problem we train Matchbox for the solver that solved it.
In addition, if the best predicted solver times out in the second round then we update Matchbox based on this.
We show results for Matchbox with and without pre-training.
Pretraining means that the model was trained on the training data before running through the test data.
No pre-training means the training data was not used so at the start of the run Matchbox predicts a uniform distribution over solvers.
The results show that on-line adaption is beneficial and Matchbox is able to solve more instances than in the batch setting.
In terms of solved instances and average run-time per solved instance Matchbox is able to outperform AQME and QuBE 6.1.
The best results for Matchbox with pre-training (utility 1996) and without pre-training (utility 1964) show that it outperforms AQME (utility 1963).
The time cost of training Matchbox is less than one millisecond for each problem and learning is incremental so cost does not increase with the number of problems previously seen, unlike previous work [Pulina and Tacchella, 2008].
Up to this point we have assumed that a solver must always be run from scratch each time and that solver state could not be stored in memory.
Our final result for the QBF application is for the scenario where we allow dynamic switching between solvers ('suspend and resume' [Streeter and Smith, 2008]).
In this case, once per second we recalculate the expected utility for each solver, and switch to the best predicted solver at that point.
In practice this yields an algorithm which behaves similarly to TTPS, because of the form of the predictive distribution of run time: after a solver is run for a time approaching t c the expected utility for this solver drops off as a fast solution using this solver looks unlikely, leading to other solvers to be tried instead.
In this way we obtain a principled approach to the scheduling problem [Streeter and Smith, 2008].
Using this method we can solve 2195 problems in the test set.Finally, we performed a similar analysis for Linear Program (LP) solvers (see e.g. [Hooker, 2006]).
The portfolio Algorithm K #Solved Time Utility GL - 2531 1025 1249 CASS - 861 207 -1768 CPLEX - 2648 445 1672 Matchbox 1 2648 445 1672 Matchbox 2 2677 232 1804 Matchbox 3 2680 240 1807 Oracle - 2704 248 consisted of 6 variants of a simplex solver and 7 variants of an interior point method (IPM) solver from Microsoft Solver Foundation 4 .
We trained Matchbox to predict run-times on a set of 4000 problems, including the Netlib LP problem set 5 , among others.
Figure 2 shows the latent space embedding that Matchbox learns for these solvers.
Note how Matchbox separates the simplex solvers from the IPM solvers, suggesting these two techniques are suited to different types of problems.
Note also that the embedding separates the primal and dual approaches along an orthogonal axis.
Next we focus on the task of determining the winner of a combinatorial auction [Leyton-Brown, Nudelman, and Shoham, 2009].
Combinatorial auctions involve selfinterested agents bidding for bundles of goods while being guaranteed that the allocation of a bundle is all or nothing.
The winner determination problem aims at answering the question of which bids should be accepted or not in an optimal fashion.
This is a derivative of the weighted set packing problem and is therefore in general a hard computational problem (NP-hard).
Leyton-Brown, Nudelman, and Shoham [2009] develop a model for determining winners in combinatorial auctions and in order to do so appropriate instances were created and a set of solvers was utilized to solve them.
In addition, they define features that characterize instances and use several machine learning techniques to manage a portfolio.
We used the same data 6 Figure 2: Learned embedding of LP solvers (compare to Figure 1).
Note the separation of simplex and IPM solvers and of primal and dual approaches.The performance is shown in terms of solved instances, the mean time required per solved instance per solver, and the utility (c = 1 T and b = 1).
When using low dimensional latent spaces, Matchbox sticks to the best single performing solver CPLEX.
However, as K increases, the number of solved instances increases slightly and the average run-time is halved.
Consequently, the utility of Matchbox in the best setting (K = 3) is better than the one achieved by any single solver (in terms of runtime and number solved) and is comparable to the one achieved by Leyton-Brown, Nudelman, and Shoham while having the advantage of incremental training.
Finally, Matchbox provides a practical and scalable approach to managing a portfolio of experts.
The model can learn online and dynamically adapt to non-stationary data.
We allow free choice of the feedback model and utility function.
The system was designed to be of general use and we see the examples here as the tip of an iceberg of applications.
We thank John Oberon and David Lao for the LP run-times.
