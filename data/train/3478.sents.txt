We rigorously generalize critical path analysis (CPA) to long-running and streaming computations and present SnailTrail, a system built on Timely Dataflow, which applies our analysis to a range of popular distributed dataflow engines.
Our technique uses the novel metric of critical participation, computed on time-based snapshots of execution traces, that provides immediate insights into specific parts of the computation.
This allows SnailTrail to work online in real-time, rather than requiring complete offline traces as with traditional CPA.
It is thus applicable to scenarios like model training in machine learning, and sensor stream processing.
SnailTrail assumes only a highly general model of dataflow computation (which we define) and we show it can be applied to systems as diverse as Spark, Flink, TensorFlow, and Timely Dataflow itself.
We further show with examples from all four of these systems that SnailTrail is fast and scalable, and that critical participation can deliver performance analysis and insights not available using prior techniques.
We present a generalization of Critical Path Analysis (CPA) to online performance characterization of longrunning, distributed dataflow computations.Existing tools which aggregate performance information from servers and software components into visual analysis and statistics [2,30] can be useful in showing what each part of the system is doing at any point in time, but are less helpful in explaining which components in a complex distributed system need improvement to reduce end-to-end latency.On the other hand, tools which capture detailed individual traces through a system, such as Splunk [9] and * This work was partially supported by the Swiss National Science Foundation, Google Inc., and Amadeus SA.VMware LogInsight [3], can isolate specific instances of performance loss, but lack a "big picture" view of what really matters to performance over a long (possibly continuous) computation on a varying workload.In this paper, we show that the design space for useful performance analysis of so-called "big data" systems is much richer than currently available tools would suggest.Critical Path Analysis is a proven technique for gaining insight into the performance of a set of interacting processes [36], and we review the basic idea in Section 2.
However, CPA is not directly applicable to long-running and streaming computations for two reasons.
Firstly, it requires a complete execution trace to exist before analysis can start.
In modern systems, such a trace may be very large or, in the case of stream processing, unbounded.
Secondly, in a continuous computation, there exist many critical paths (as we show later on), which also change over time, and there is no established methodology for choosing one of them.
It is therefore important to aggregate the paths both spatially (across the distributed computation) and temporally (as an evolving picture of the system's performance).
According to prior work [5,37], the accuracy of CPA increases with the number of critical paths considered.
However, existing approaches require full path materialization in order to aggregate information from multiple critical paths.
Thus, they restrict analysis to k critical paths, where k is much smaller than the total number of paths in the trace.
In open-ended computations where analysis is performed on trace snapshots and all paths are of equal length, materializing all paths is impractical, especially if the analysis needs to keep up with real time.
For instance, in our experiments, the number of paths in a 10-sec snapshot of Spark traces is in the order of 10 21 .
This paper's first contributions (in Section 3) are definitions of Transient Critical Path, a modification of classical critical path applicable to continuous unbounded computations, and Critical Participation (CP), a metric which captures the importance of an execution activity [1] from [27] for 64 s snapshots.
Spikes indicate coordination between workers and the driver.in the transient critical paths of computation, and which can be used to generate new time-varying performance summaries.
The CP metric can be computed online and aggregates information from all paths in a snapshot without the need to materialize any path.Our next contribution (in Section 4) is a model for the execution of distributed dataflow programs sufficiently general to capture the execution (and logging) of commonly-used systems-Spark, Flink, TensorFlow, and Timely Dataflow-and detailed enough for us to define Transient Critical Paths and CP over each of these.We then show (in Section 5) an algorithm to compute CP online, in real time, and describe SnailTrail, a system built (itself as a Timely Dataflow program) to do this on traces from the four dataflow systems listed above.
In Section 7 we evaluate SnailTrail's performance, demonstrate online critical path analysis using all four reference systems with a variety of applications and workloads, and show how CP is more informative than existing methods, such as conventional profiling and single-path critical path analysis ( Sections 7.4 and 7.5).
Figure 1 gives a flavor of how CP compares with conventional profiling techniques.
The key difference is that our approach highlights activities that contribute significantly to the performance of the system, while discarding processing time that lies outside the critical path.We believe SnailTrail is the first system for online real-time critical path analysis of long-running and streaming computations.
CPA has been successfully applied to high-performance parallel applications like MPI programs [11,32], and the basic concepts also apply to the distributed dataflow systems we target in this paper.
In this section we review classical CPA applied to batch computations as a prelude to our extension of CPA to online and continuous computations in the next section.
Table 1 summarizes the notation we use in this section and the rest of this paper.
We view distributed computation as executed by individual system workers that perform activities (e.g. data transformations or communication).
The critical path is defined as the sequence of activities with the longest duration throughout the execution.
More formally:Definition 1 Activity: a logical operation performed at any level of the software stack, and associated with two timestamps [start,end], start ≤ end, that denote the start and end of its execution with respect to a clock C.An activity can be either an operation performed by a worker (worker activity) or a message transfer between two workers (communication activity).
Typically, worker activities correspond to the execution of some code, but can also be I/O operations performed by the worker (e.g. reads/writes to/from disk).
Communication activities correspond to worker interactions, e.g. message passing.Different systems have different concepts (threads, VMs, etc.) corresponding to workers.
For consistency, we define workers as follows:Definition 2 Worker: a logical execution unit that performs an ordered sequence of activities with respect to a clock C.We require that no two activities of the same worker a i :[start i ,end i ] and a j :[start j ,end j ] (where i j) can overlap in time, i.e. either end i ≤ start j or start i ≥ end j .
Central to CPA is the Program Activity Graph (PAG):Definition 3 Program Activity Graph: A PAG G = (V, E)is a directed labeled acyclic graph where:• V is the set of vertices.
A vertex v ∈ V represents an event corresponding to the start or end of an activity.
Each vertex v has a timestamp v[t] and a worker id v [w].
• E ≡ E w ∪ E c ⊂ V × V, E w ∩ E c = ∅, is the set of di- rected edges.
An edge e = (v i , v j ) ∈ E] = v j [t] -v i [t] = end -start ≥ 0.
An edge e ∈ E w denotes a worker activity whereas an edge e ∈ E c denotes a communication activity.The direction of an edge e = (v 1 , v 2 ) ∈ E from node v 1 ∈ V to node v 2 ∈ V denotes a happened-before relationship between the nodes [24].
The critical path is then defined as the longest path in the program activity graph: Offline processing in traditional CPA is not feasible for long-running or continuous computations like streaming applications or machine learning model training.
In these cases, neither the program activity graph nor the critical path can be defined as in Section 2.
Instead, we define online CPA on PAG snapshots, performing it on user-defined time windows: slices of the PAG that contain activities within a specified time interval.
This enables not only performance analysis of running applications, but also targeting specific parts of the computation like the model training phase in a TensorFlow program or a specific time window in a Flink stream.To achieve this, we show here how to define a timebased program activity graph snapshot and a transient critical path on this graph.
We then define the critical participation performance metric, and we provide the intuition behind it in Section 3.3.
To retrieve a snapshot of the PAG, we first assign activities to time windows.
Given an edge in a graph, we call its corresponding edge in a snapshot an edge projection:Definition 5 Edge Projection: Let e = (v i , v j ) be an edge of an activity graph G = (V, E), where e ∈ E and v i , v j ∈ V. Let also [t s , t e ], t s ≤ t e , be a time interval with respect to a clock C. Let u s be a copy of v i with u s [t] = t s and u e a copy of v j with u e [t] = t e .
The projection of e on [t s , t e ] is an edge of the same type as e and is defined only wheneverv i [t], v j [t]overlaps with [t s , t e ] as follows: t e t s (e) = arg max [t] (v i , u s ), arg min [t] (v j , u e )Activities entirely within the time interval [t s , t e ] are unchanged by the projection, whereas activities that straddle the boundaries are truncated to fit the interval.
We can now define a snapshot as follows:Definition 6 PAG Snapshot: Let G = (V, E) be a program activity graph, and [t s , t e ], t s ≤ t e , be a time interval with respect to a clock C.The snapshot of G in [t s , t e ] is a directed labeled acyclic graph G [t s ,t e ] = (V , E ) that is constructed by projecting all edges of G on [t s , t e ].
The snapshot G [t s ,t e ] is that part of the PAG which can be observed in the time window [t s , t e ].
The key observation is that we cannot define a single critical path in a PAG snapshot since there exist multiple longest paths with the same total weight: t e − t s .
All paths starting at t s and ending at t e are potentially parts of the evolving global critical path.
For this reason, we define the notion of transient critical path:Definition 7 Transient Critical Path: Let G [t s ,t e ] = (V, E)be the snapshot of an activity graph G in the time interval [t s , t e ].
We define the set of paths Since all transient paths can potentially be part of the evolving global critical path, an activity that appears on many transient paths is more likely to be critical and should be ranked high.
In Figure 2b, edge (d , i ) appears in two paths, while edge (g , h ) belongs to all six.
The performance metric we define next incorporates this information and ranks activities based on their potential contribution to the global critical path.P on G [t s ,t e ] as P ≡ { p ⊆ E | p : || p || > || p||},b' c' d' e' g' h' b' c' d' g' h' i' c' d' e' g' h' f' c' d' g' h' f' i' b' g' h' i' g' h' f' i' Given the duration of an activity e[w] and the total length || p|| of the critical path p, the participation of e to p is defined as:q e = e[w] || p|| ∈ [0, 1](1)and is easily computed for all activities in a single p pass.We correspondingly define average critical participation (CP) of an activity e in a transient critical path as:CP e = i=N i=1 q i e N ∈ [0, 1] (2)where q i e is the participation of e to the i-th transient critical path (given by Eq.
1), and N is the total number of transient critical paths in the graph snapshot.A straightforward way to compute CP e is to materialize all N transient paths and compute the participation of each activity in every path.
However, path materialization is not viable in an online setting because a single graph snapshot might contain too many paths to maintain.
Instead, we exploit the fact that the CP of an activity actually depends on the total number of transient paths this activity belongs to.
Hence, we define the transient path centrality as follows: ∈ G [t s ,t e ] is defined as c(e) = N i=1 c i (e), where c i (e) =      0 : e p i 1 : e ∈ p iThe following holds:CP e = i=N i=1 q i e N = c(e) N · e[w] || p||(3)Eq.
3 1 indicates that the computation of CP e can be reduced to the computation of c(e), which requires no path materialization and can be performed in parallel for all edges in G [t s ,t e ] .
Section 5 provides an algorithm for transient path centrality and CP without materialization.
Note that we can normalize by the number of paths N and their length || p|| because of Definition 7 guaranteeing that all paths have the same length.We can now compute the transient path centrality and critical participation for the example in Figure 2.
For instance, c(d , i ) = 2 and c(g , h ) = 6.
Respectively, since t e − t s = 5 and N = 6, CP (d ,i ) = 0.066 andCP (g ,h ) = 0.2.
The CP of Eq.
2 can be generalized for activities of a specific type c as:∀e:e[p]=c CP e(4)and the following holds 1 :∀c ∈ G ∀e:e[p]=c CP e = 1 (5)Intuitively, Eq.
5 states that the estimated contribution of an activity type, e.g., serialization, to the critical path of the computation is normalized over the contribution of all other activity types in the same snapshot.
Figure 3 illustrates by example a comparison of CP-based performance analysis with two existing methods: conventional profiling and traditional critical path analysis.
Conventional profiling summaries aggregate activity durations by type or by worker timeline.
Such summaries provide information on how much time (i) a program spends on a certain activity type (e.g. serialization) or (ii) a worker spends executing an activity type as compared to other workers.
Since conventional profiling summaries rely solely on durations and do not capture execution dependencies, they cannot reveal bottlenecks and execution barriers.
Conventional profiling in the execution of Figure 3 would rank activities (a, b) and (c, d) high since they both have a duration of 3 time units, larger than all other activities.
However, optimizing those activities cannot result into any performance benefit for the parallel computation as they are both followed by a waiting state (denoted with a dashed line).
On the other hand, CPA captures execution dependencies and can accurately pinpoint activities which influence performance.
However, traditional CPA is not directly applicable in a continuous computation as the critical path is not known by just inspecting a snapshot of the execution traces.
In a snapshot like the one of Figure 3, all paths starting at s i and finishing at e i have equal length in time units, thus traditional CPA would choose one of them at random.
We have highlighted such a path in Figure 3 in red color.
Although this randomly selected path does not contain the activities (a, b) and (c, d), whose optimization would certainly not improve the latency of the computation, it misses several important activities, such as (x, u) and (v, z), whose optimization would do so.The CP metric overcomes the limitations of both conventional profiling and traditional CPA by ranking activities based on their potential contribution to the evolving critical path of the computation, which in turn reflects potential benefits from optimization.Given a snapshot and no knowledge of the execution timelines outside of it, any path between the s i and e i points in Figure 3 is equally probable to be part of the critical path.
CP is a fairer metric compared to existing methods in that it aggregates an activity's contribution over all transient critical paths and normalizes by the number of paths and the activity's duration.
The more paths an activity contributes to, the higher the probability it is a part of the evolving critical path and, hence, the higher its CP metric is.
In Figure 3 In Section 7.4 we empirically compare CP-based performance summaries to conventional profiling and traditional CPA, and demonstrate how the results of the latter can be misleading.
Further, in Section 7.5, we show how CP can detect and help optimize execution bottlenecks like the one represented by activity (u, v) in Figure 3.
Here we show the applicability of our applicability to a range of modern dataflow systems.
We provide details on the model assumptions and the instrumentation requirements in the Appendix.Spark, Flink, TensorFlow, and Timely are superficially different, but actually similar with regard to CPA: all execute dataflow programs expressed as directed graphs whose vertices are operators (e.g. map, reduce) and whose edges denote data dependencies.
During runtime, a logical dataflow graph is executed by one or more workers, which can be threads or processes in a machine or a cluster.
Each worker has a copy of the graph and processes a partition of the input data in parallel with other workers.
We define a small set of activity types we use to classify both the activity of a worker at any given point in time, and communication of data between workers/operators.
We consider the following types of worker activities:Data Processing: The worker is computing on data in an operator, which usually has a unique ID.
We also include low-level (de)compression operations.Scheduling: Deciding which operator a worker will execute.
In Spark and Flink, scheduling is done by special workers (the Driver and the JobManager).
Barrier Processing: The worker is processing information which coordinates the computation (e.g distributed progress tracking in Timely or watermarks in Flink).
Buffer Management: The worker is managing buffers between operators (e.g. Flink's FIFO queues) or buffering data moving to/from disk (e.g. Spark).
The activity may include copying data into/out of buffers, locking, recycling buffers (e.g. Flink) and dynamically allocating them (e.g. Timely).
Serialization: Data is being (un)marshaled, an operation common to all dataflow systems when messages are sent between processes and/or machines.Waiting: The worker is waiting on some message (data or control) from another worker, and is therefore either spinning (as in Timely) or blocked on an RPC (as in Ten- sorFlow).
Waiting in our model is always a consequence of other, concurrent, activities [21], and so is a key element of critical path analysis: a worker does not produce anything useful while waiting, and so waiting activities can never be on the critical path.
The worker is waiting on an external (uninstrumented) system, (e.g. Spark waiting for HDFS, or Flink spilling large state to disk).
I/O activities have no special meaning, but capture cases where performance of the reference system is limited by an external system.Unknown: Anything else: gaps in trace records and any worker activity not captured by the instrumentation.
A large number of unknown activities usually indicates inadequate instrumentation [21].
In contrast, interaction between workers is modeled as a communication activity, which captures either: (i) application data exchange over a communication channel, or (ii) control messages conveying metadata about worker state or progress and exchanged between pairs of workers (as in Timely) or through a master (as in Spark, Flink).
We applied our approach to Spark, TensorFlow, Flink, and Timely Dataflow, mapping each to our taxonomy of activities.
In some cases we used existing instrumentation, whereas in others we added our own.
Space precludes a full discussion of either the structure of these systems or their instrumentation; we provide only brief summaries here and we give more details in [21].
Timely Dataflow [26] required us to add explicit instrumentation, and was the first system we addressed (in part because SnailTrail is written in Timely).
Timely's progress tracking corresponds to our "barrier" activity, discrete (de)serialization is performed on both data records and control messages, and Timely's cooperative scheduling means that any otherwise unclassified worker activity corresponds to "scheduling".
Apache Flink [10] adopts (unlike Timely) a masterslave architecture for coordination.
We treat Flink's JobManager, TaskManagers, and Tasks all as workers, and Flink's runtime has clear activities corresponding to buffer management and serialization.
Scheduling is performed in the JobManager, barrier processing corresponds to the watermark mechanism, and control messages correspond to communication between the JobManager and TaskManagers.TensorFlow [4] has its own instrumentation based on "Timeline" objects, which we reuse unchanged.
While enough to generate meaningful results, it also shows how even a well-considered logging system can easily omit information vital for sophisticated performance analysis.Spark [38] also has native instrumentation which we use to model both the Spark driver and executors as workers.
The logs provide information on the lineage of Resilient Distributed Datasets (RDDs) facilitating construction of the PAG.
Since executor scheduling is not instrumented, we assume greedily that a task is started on the most recently used thread, which aligns with Spark's observed behavior.
CP is implemented in SnailTrail, itself a data-parallel streaming application written in Rust using Timely Dataflow (Figure 4).
It reads streams of activity traces via sockets, files, or message queues from a reference application and outputs a stream of performance summaries.
SnailTrail operates in four pipeline stages: it (i) ingests logs, (ii) slices the stream(s) into windows [t s , t e ] and constructs PAG snapshots, (iii) computes the CP of the snapshots, and (iv) outputs the summaries we show in Section 6.
Traces are sent to SnailTrail which ingests a stream S of performance events corresponding to vertices in the activity graph.
The snapshots are constructed using Algorithm 1.
First, SnailTrail extracts from S the events in the time window [t s , t e ] (line 1).
These are then grouped by the worker that recorded them (line 2).
Each group corresponds to a worker timeline in Figure 2a.
Then, SnailTrail sorts the events in each timeline by time (line 4), and scans each timeline in turn to create the set of edges E w (line 6) that correspond to worker activities (cf. |T | · log |T | time, where |T | is the number of events in the timeline.
Parallelism is limited by the number of workers in the reference system (usually many more than SnailTrail) and the density of the graph.
We emphasize that edges in the PAG represent real happened-before dependencies given by the instrumentation.
More details in the way edges are created in lines 6-7 are given in the Appendix along with a discussion on clock alignment.Section 4.1).
Meanwhile, communication activities are partially initialized based on send and receive at each worker (line 7).
Then (line 8), partial edges are grouped by the attributes w src id , w dst id , c id ; note that w src id is the sender worker id, w dst id is receiver id, and c id is generated to uniquely identify a message.
These pairs of partial edges are concatenated to create the final communication edges in E c , and the output is the union of sets E w and E c (line 9).
Algorithm 1: GraphFor each graph snapshot, the CP metric is computed using Algorithm 2.
SnailTrail collects 'start' and 'end' nodes (lines 1-2) as seeds to traverse G [t s ,t e ] .
V s (resp.
V e ) includes the node(s) with the minimum (resp.
maximum)timestamp v[t] in G [t s ,t e ] .
Typically, |V s | = |V e | = , whereis the number of timelines, and so all nodes in V s have timestamp t s whereas all nodes in V e have timestamp t e .
Algorithm 2 computes the transient path centrality c(e) of Eq.
3 for all edges in G [t s ,t e ] .
Observe that c(e) = c 1 · c 2 , where c 1 is the number of paths from the source of e to any node in V s , and c 2 is the number of paths from the destination of e to any node in V e .
The algorithm thus performs two simple traversals of G [t s ,t e ] in parallel, computing c 1 and c 2 for each edge (lines 3-4).
Each traversal outputs pairs (e, c i ) and these are finally grouped by e to give CP values (lines 6-7).
Note that, while traversing G[t s ,t e ] , we visit each edge in G [t s ,t e ] only once by propagating the final value c 1 (resp.
c 2 ) from each edge to all its adjacent edges.
This reduces the intermediate results of the computation significantly.
We compute the CP according to Equation 3, which does not require path materialization.
Algorithm 2 requires two partitions of G [t s ,t e ] : one on source, and one on destination ids.
Worst-case time complexity is O(d), where d is the diameter of G [t s ,t e ] in number of edges, i.e., the maximum number of edges in any transient critical path.
Input :An activity graph snapshotG [t s ,t e ] = (V, E); Output :A set S = {(e, CP) | e ∈ G [t s ,t e ] } of CP values; 1 let V s ≡ {v ∈ V | v ∈ V : v [t] < v[t]}; //start nodes 2 let V e ≡ {v ∈ V | v ∈ V : v [t] > v[t]};//end nodes //Both traversals are performed in parallel 3 traverse G [t s ,t e ] starting from V s , and count the total number of times each edge is visited, let c 1 ;4 traverse G [t s ,t e ]backwards, starting from V e , and count the total number of times each edge is visited, let c 2 ;5 S = ∅; 6 for each edge e ∈ E do7 S = S ∪ {(e, c 1 · c 2 · e[w] N · (t e −t s ) )} 8 return SPerformance summaries are constructed by userdefined groupings on the edge attributes and summing CP values over each group.SnailTrail's accuracy depends on the quality of the instrumentation.
A more complete set of dependencies increases the accuracy of the CP metric.
We leave a worst-case error bound analysis for future work.
The CP metric provides an indication of an activity's contribution to the evolving critical path.
SnailTrail can be configured to generate different types of performance summaries using the CP metric.
Each summary type targets a specific aspect of an application's performance and is designed to reveal a certain type of bottleneck.
In particular, SnailTrail provides four performance summaries which can answer four types of questions: (i) Which activity type is on the critical path?
(ii) Is there data skew?
(iii) Is there computation skew?
(iv) Is there communication skew?
The performance summaries not only indicate potential bottlenecks, but also provide immediate actionable feedback on which activities to optimize, which workers are overloaded, which dataflow operator to re-scale, and how to minimize network communication.
Figure 5 shows examples of the four summary types for the Dhalion [18] benchmark on Flink with 1s snapshots.
In the rest of this section, we describe each summary type in detail and we discuss how to use them in practical scenarios to improve an application's performance.Activity summary.
Is the fault-tolerance mechanism in the critical path when taking frequent checkpoints?
Is coordination among parallel workers an overhead when increasing the application's parallelism?
An activity summary can answer this sort of questions about an application's performance.
This summary plots the proportional CP value of selected activity types with respect to the other activity types in a given snapshot.
Activity reveal bottlenecks inherent to the system or its configuration.
Having a ranking of activity types based on their critical participation essentially gives us an indication on which activities have the higher potential for optimization benefit.
For example, if we find that serialization is on the critical path, we might want to try a different serialization library.
The activity summary ranking can also help us choose good configurations for our application, like how to adjust the checkpoint interval or the parallelism.
The activity summary of Figure 5 shows that serialization and processing have the higher potential for optimization.
Activity summaries can be configured to plot selected activities only, as in Figure 1 where we only show the Spark driver's scheduling.Straggler summary.
Is there data skew?
If so, which worker is the straggler?
SnailTrail can answer these questions with a straggler summary, which plots the critical participation of a worker's timeline in a certain snapshot.
The straggler summary relies on the observation that if a worker is a straggler then many transient critical paths pass through its timeline.
Hence, we can compare how how critical a worker's activities are as compared to the other workers in the computation and reveal computation imbalance.
This ranking can serve as input to a workstealing algorithm or guide a data re-distribution technique.
The straggler summary of Figure 5 clearly shows one straggler worker in the Flink job.
In Section 7.5, we look closer into detecting skew with SnailTrail.Operator summary.
Will re-scaling my dataflow improve performance?
And if yes, which operator in the dataflow to re-scale?
An operator summary plots the critical participation of each operator's processing activity in a snapshot, normalized by the number of parallel workers executing the operator.
This summary reveals bottlenecks in the dataflow caused by resource underprovising and serves as a good indicator for scaling decisions.
Traditional profiling methods fail to detect that an operator might be limiting the end-to-end throughput of a dataflow even if its parallel tasks are perfectly balanced.
Such bottlenecks are hard to detect by looking at traditional metrics such as queue sizes, throughput, and backpressure.
The operator summary of Figure 5 shows that both operators have similar critical participation, thus the parallelism of the job is properly configured.
In Section 7.5, we present a detailed use-case where operator summaries guide scaling decisions for streaming applications.Communication summary.
Is there communication skew?
And if yes, which communication channels to optimize?
A communication summary plots the critical participation of communication activities between each pair of workers within a given snapshot.
Contrary to traditional communication summaries, this CP-based summary does not rely on communication frequency or absolute message sizes.
Instead, it ranks communication edges by their critical importance: the more often a communication edge belongs to a transient critical path, the higher it will be ranked by the summary.
Communication summaries can be used to minimize network delays and optimize distributed task placement.
If we find that a pair of workers' communication is commonly on the critical path, it is probably a good idea to physically deploy these two workers on the same machine.
For example, the communication summary of Figure 5 indicates that colocating worker 5 with workers 11-13 could benefit performance.
To show generality, we evaluate SnailTrail analyzing four different reference systems: Timely Dataflow (version 0.1.15), Apache Flink (1.2.0), Apache Spark (2.1.0), and TensorFlow (1.0.1).
Our evaluation is divided into four categories.
First, in Section 7.2 we show the instrumentation SnailTrail needs does not cause significant impact on the performance of reference systems.
Second, in Section 7.3 we investigate SnailTrail's performance and show it can deliver results in real time with high throughput and low latency.
Third, we compare the quality of SnailTrail's analysis and the utility of the CP metric with both conventional profiling and traditional critical path analysis (Section 7.4).
Finally, we present use cases for SnailTrail with analysis results (Section 7.5).
SnailTrail uses the latest Rust version of Timely Dataflow [25] compiled with Rust 1.17.0.
In all experiments, SnailTrail ran on an Intel Xeon E5-4640 2.40 GHz machine with 32 cores (64 threads) and 512G RAM running Debian 7.8 ("wheezy"), and was configured to produce results by ingesting execution traces from a reference system on a different cluster.Benchmarks.
We compare SnailTrail to existing approaches with several traces generated by Flink, Spark, and TensorFlow using the following benchmarks.
For Flink, we use the Yahoo Streaming Benchmark (YSB) [12] and the WordCount benchmark of Dhalion [18].
For Spark, we use YSB and, for TensorFlow, we use the AlexNet [23] program on ImageNet [29].
To evaluate SnailTrail performance we use Flink (configured with 48 parallel tasks) running a real-world sessionization program on a 10min window of operational logs from a large industrial datacenter.
This generates a trace with a median number of 30K events per second (around 7.5M events for a 256s snapshot, the largest we used).
We also show the instrumentation overhead in Flink, with the same sessionization experiment, and Timely, using a PageRank computation with 16 parallel workers on a random graph.
SnailTrail relies on tracing functionality in the reference system, and this incurs performance overhead.
To evaluate the overhead of the instrumentation we added, we implemented a streaming analytic job, sessionization, in Flink and an iterative graph computation, PageRank, in Timely, and measured performance with tracing enabled and disabled.
For TensorFlow and Spark we use their existing, and somewhat incomplete, tracing facilities.
Figure 6 shows box-and-whisker plots of processing latency for Flink and Timely implementations.
Individual bars correspond to the cases where logging is completely turned off (baseline), the default logging level (info), and our detailed tracing (instrumented).
Flink shows a statistically significant difference of 9.7 % (±1.43 %) additional mean latency, or 203ms (±29.9 µs) in absolute terms, at 95% confidence.
This overhead is negligible, given that Flink typically runs with logging enabled in production deployments.For Timely, there is a statistically significant difference of 13.9 % (±5.5 %) increase in the mean latency, or 319 µs (±126.2 µs) in absolute terms, at 95% confidence.Experiments with Spark and TensorFlow showed no discernible overhead for collecting the traces required by SnailTrail.
Overall, we argue that performance penalties around 10% are an acceptable tradeoff for greater insight, and could be additionally amortized in some cases.
We evaluate SnailTrail's performance to demonstrate that (i) it always operates online and thus provides feedback to the running reference applications in real-time and (ii) its analysis scales to large deployments of reference applications without violating this online requirement.Latency.
We require SnailTrail to be capable of constructing the PAG and computing the CP metric for a snapshot of size x secs in less than x secs.
The number of events in a snapshot depends on (i) the snapshot duration and (ii) the instrumentation granularity of the reference system.
For this experiment, we vary the number of events in the snapshot by increasing its duration from 1s to 256s (in powers of 2) and we run SnailTrail on the Flink sessionization job trace, which is the densest one we have.
Note that the public Spark traces from real-world cloud deployments [27] are not as dense as the ones generated by the Flink streaming computations we run.
We show median latency and number of events per snapshot in Table 2; SnailTrail is always capable of operating online and its latency increases almost linearly with the snapshot duration.
Specifically, it can process 1s of input logs in 6ms and 256s of input logs in under 25s.
Throughput.
To evaluate SnailTrail's throughput, we interleave the processing of multiple snapshots to increase the number of events sent to the system.
Table 3 shows the maximum achieved throughput (number of processed events per second) while respecting the online requirement and the corresponding latency for processing an input snapshot, including PAG construction and CP computation.
For 1s snapshots, SnailTrail can process 1.2 million events per second; a throughput two orders of magnitude larger than the event rate we observed in all log files we have, including the Spark traces from [27].
SnailTrail comfortably keeps up with all tested workloads: the time to process a snapshot is always smaller than the snapshot's duration.
Throughput decreases when increasing the snapshot size since the PAG gets bigger.
We examine how useful the CP-based summaries produced by SnailTrail are in practice, as compared to the weight-based summaries produced by conventional profiling, where activities are simply ranked by their total duration, and the single-path summaries, where CP is computed on a single transient critical path (in this experiment selected at random).
We show examples of such summaries in Figures, 7, 8, and 9 for Flink, Spark, and TensorFlow, along with the configuration of each system.
First note that single-path summaries correspond to a straight-forward application of traditional CPA on trace snapshots where only a single path is chosen at random.
The plot on the right of Figure 7 exhibits high variation because different transient critical paths may consist of completely different activities, even within the same graph snapshot.
In contrast, CP is a fairer metric that avoids this misleading critical activity "switching" by aggregating information from all transient critical paths in a snapshot.Conventional profiling summaries are different from CP-based summaries in that they do not account for overlapping activities, thus, they overestimate the participation of activities in the critical path (e.g., the processing activity in the right plot of Figure 8), resulting in activity durations that may even exceed the total duration of the snapshot.
The CP-based summary of Figure 8 overcomes this problem and highlights the overhead of global coordination in micro-batch systems (driver's scheduling activity), a known result also pointed out in Drizzle [34].
SnailTrail is also different to traditional profiling in its ability to focus on different parts of a long-running computation.
This feature is particularly useful in machine learning, where program phases have diverse performance characteristics.
As an example, Figure 9 shows CP-based and conventional summaries for the accuracy phase of the AlexNet image processing application on TensorFlow with 16 workers.
We plot processing and communication as separate bars for convenience and we further break down processing into the different operators appearing in this computation phase.
The conventional summary overestimates the participation of communication and underestimates the importance of the Conv2D operator, which is the most critical one according to the CP-based summary.
Processing in the conventional summary is dominated by the unknown activity type due to limited instrumentation in TensorFlow (see [21]).
We select Apache Flink as the representative streaming system and demonstrate SnailTrail in action.
We describe two use-cases and give examples of how the CPbased summaries can be used to understand and improve application performance of long-running computations.Detecting skew.
To demonstrate straggler summaries in action, we use the benchmark of [18].
The benchmark contains a WordCount application and a data generator.
The data generator can be configured with a skewness percentage.
We experiment with 30%, 50%, and 80% skewness.
We configure the parallelism to be equal to 4 for all operators and we generate straggler summaries and conventional summaries shown in Figure 10.
For small skew percentage, the conventional summaries fail to detect any imbalance and essentially indicate uniform load across workers.
For higher skew percentages (50-80%) they indeed reveal a skew problem, yet they are unable to indicate a single worker as the straggler.
Instead, they attribute the imbalance problem to several workers.
On the other hand, the CP-based straggler summaries consistently and accurately detect the straggler worker, even for low skew percentage.Optimizing operator parallelism.
We now demonsrate how SnailTrail can guide scaling decisions for streaming applications.
We use Dhalion's [18] benchmark again and initially under-provision the flatmap stage.
We configure four parallel workers for the source, two parallel workers for the flatmap, and four parallel workers for the count operator.
Figure 11 (left) shows the operator and conventional profiling summaries for this configuration.
We see that the operator summary detects that the flatmap workers are bottlenecks.
On the other hand, the conventional summary shows a negligible difference between the parallel workers' processing.
In addition, we gather metrics from Flink's web interface.
Using those, we can observe backpressure, yet we have no indication of the cause.
We next decrease the source's input rate, by changing its parallelism to one worker.
Note that slowing down the source is a common system reaction to backpressure.
Figure 11 (middle) shows the operator and conventional profiling summaries after this change.
Notice how slowing down the source does not solve the problem and how the operator summary still provides more accurate information than the conventional one.
The operator summary essentially indicates that the flatmap operator has a high CP value and needs to be re-scaled.
Figure 11 (right) shows the summaries after applying a parallelism of four to all operators.
Checking Flink's web interface again we see that backpressure disappears.
There exists abundant literature on performance analysis, characterization, and debugging of distributed systems, although we know of no prior work to perform online critical path analysis for long running computations, or applicable across a broad range of execution models.
We distinguish three main areas of related work:Critical Path Analysis: Yang et al. [36] first applied CPA to distributed and parallel applications, defined the PAG, gave a distributed algorithm for CPA, and showed its benefits over traditional profiling.
CPA and related techniques have since been used to analyze distributed programs like MPI applications [32,8] and web services [13], in all cases using offline traces.
Algorithms to compute the k longest (near-critical) paths in a computation are given in [6].
The first online method for computing critical path profiles seems to be [22], where performance traces are piggybacked on data messages exchanged by processes at runtime.
However, the proposed algorithm is too expensive to construct the full PAG and is thus limited to a small number of user-selected activities.
A nice feature of [22] is combining online CPA with dynamic instrumentation to selectively enable trace points on demand.
[31] extends the analysis of [22] to the full software stack, and [17] uses this information for adaptive scheduling.
Sonata [20] pinpoints critical activities in the spirit of CPA.
It supports offline analysis of MapReduce jobs through identifying correlations between tasks, resources and job phases.Dataflow Performance Analysis: [28] employs blocked time analysis to dataflow, a 'what-if' approach quantifying performance improvement assuming a resource is infinitely fast.
Blocked time analysis is performed offline and assums staged batch execution.
It can only identify bottlenecks due to network and disk and does not provide insights into the interdependence of parallel tasks and operators.
An alternative approach in Storm [33]) is based on the Actor Model [7] rather than CPA.
HiTune [16] and Theia [19] focus on Hadoop profiling; in particular, on cluster resource utilzation and task progress monitoring.Distributed Systems Profiling: A comprehensive overview of prior work in distributed profiling is [39], which also introduces Stitch, a tool for profiling multilevel software stacks using traces.
Like SnailTrail, Stitch requires no domain knowledge of the reference system, but its Flow Reconstruction Principle assumes logged events are sufficient to reconstruct the execution flow.
SnailTrail in contrast does not assume this, and indeed yields insights for the better instrumentation of dataflow systems.
VScope [35] targets online anomaly detection and root-cause analysis in large clusters.
Finally, we note that capturing dependencies between activities in dataflows is similar to causal profiling in Coz [15].
Coz does not focus on distributed dataflows, but does work non-intrusively without instrumentation, and may be applicable to SnailTrail.
Online critical path analysis represents a new level of sophistication for performance analysis of distributed systems, and SnailTrail shows its applicability to a range of different engines and applications.
Looking forward, SnailTrail's online operation suggests uses beyond providing real-time information to system administrators: SnailTrail's performance summaries could serve as immediate feedback for applications to perform automatic reconfiguration, dynamic scaling, or adaptive scheduling.
We support both synchronous and asynchronous execution in shared-nothing and shared-memory architectures.
Most dataflow systems use asynchronous computations on shared-nothing clusters, but sometimes synchronous computation is supported (e.g. in TensorFlow), and system workers can share state (e.g. in Timely).
Specifically, our model is consistent with respect to critical path analysis under two assumptions: Assumption 1 (Message-based Interaction).
Every interaction between operators in the dataflow must occur via message exchange, even if executed by the same worker.Note this assumption does not preclude shared-memory systems.
Operators in the reference dataflow may share state as long as any modification to this state is appropriately instrumented to trigger a 'virtual' message exchange between the workers sharing that state.
We use this approach in instrumenting shared state in Timely Dataflow, for example.Assumption 2 (Waiting State Termination).
Every waiting activity in a worker's timeline is terminated by an incoming message, either from the same or a different worker.In other words, a worker in a waiting state cannot start performing activities unprompted without receiving a message.
In the activity graph, a waiting edge's end node must correspond to that of a communication activity, i.e., a receive.
An activity may consist of sub-operations spanning multiple levels of the stack from user code to OS and network protocols.
A given system can be instrumented at different levels of granularity, depending on the use-case: a multilayered activity tracking approach enables more detailed performance analysis but introduces higher overhead.
We allow this choice, but require that any instrumentation of the reference system satisfy two properties, without which the transient critical paths are ill-defined.
The first states that any event having prior events must be caused by an activity earlier in time, i.e. any "out-of-the-blue" events in (t s , t e ] indicate insufficient instrumentation: The second states that at no point do all system workers perform waiting activities while no communication activity is occurring.
Such behavior would imply deadlock, and so any such points in the activity graph of a non-blocked computation indicates insufficient instrumentation: These two properties can also checked efficiently online to inform users when the ingested activity logs are incomplete.
For example, instrumentation (or associated log preprocessing) can guarantee that no waiting activities are created as long as the corresponding communication activity, which caused the waiting activity to end, has not been observed.
First, we provide a proof for Eq.
3:CP e = i=N i=1 q i e N = c(e) · e[w] N(t e −t s ) ∈ [0, 1]Recall that e is an activity edge in the PAG snapshot, N is the total number of transient critical paths in the snapshot, q i e is ratio of the activity's duration to the total duration of the i-th transient critical path (the ratio is 0 if the activity edge is not part of the i-th path), 0 ≤ c(e) ≤ N is the number of transient critical paths the activity e belongs to, e[w] is the weight of the activity e, i.e., its duration, and [t s , t e ] is the snapshot window size.Without loss of generality, we assume that the transient critical paths p i the activity edge e belongs to are numbered from i = 1 to i = c(e).
Then: Computing critical paths only needs logical time, i.e. the happens-before relationship between events.
In practice we are using wall-clock time as a stand-in for Lamport timestamps [24] to establish partial ordering of events.
Performance statistics such as summaries, however, do require real time.A practical system for critical path analysis must therefore address issues of clock drift (where clocks on different nodes run at different rates) and clock skew (where two clocks differ in their values at a particular time).
Clock drift only affects activities running on the same thread with durations greater than the drift.
Even a drift of 10 seconds/day translates to 0.1ms inaccuracy for activities taking around a second, which is probably tolerable.
Clock skew is not an issue for activities timestamped by the same thread, but might be for communication activities.In SnailTrail, we assume that the trend toward strong clock synchronization in datacenters [14] means that clock skew is not, in practice, a significant problem for our analysis.
If it were to become an issue, we would have to consider adding Lamport clocks and other mechanisms for detecting and correcting for clock skew.
We thank Ralf Sager for working on some intitial ideas of this paper, Frank McSherry and the anonymous NSDI reviewers for their comments, and Raluca Ada Popa for shepherding the paper.
Vasiliki Kalavri is supported by an ETH Postdoctoral Fellowship.
