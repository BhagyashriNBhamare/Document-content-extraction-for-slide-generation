We investigate the use of SmartNIC-accelerated servers to execute microservice-based applications in the data center.
By offloading suitable microservices to the SmartNIC's low-power processor, we can improve server energy-efficiency without latency loss.
However, as a heterogeneous computing substrate in the data path of the host, SmartNICs bring several challenges to a microservice platform: network traffic routing and load balancing, microservice placement on heterogeneous hardware, and contention on shared SmartNIC resources.
We present E3, a microservice execution platform for SmartNIC-accelerated servers.
E3 follows the design philosophies of the Azure Service Fabric microservice platform and extends key system components to a SmartNIC to address the above-mentioned challenges.
E3 employs three key techniques: ECMP-based load balancing via SmartNICs to the host, network topology-aware microservice placement, and a data-plane orchestrator that can detect SmartNIC overload.
Our E3 prototype using Cavium LiquidIO SmartNICs shows that SmartNIC offload can improve cluster energy-efficiency up to 3× and cost efficiency up to 1.9× at up to 4% latency cost for common microservices, including real-time analytics, an IoT hub, and virtual network functions.
Energy-efficiency has become a major factor in data center design [80].
U.S. data centers consume an estimated 70 billion kilowatt-hours of energy per year (about 2% of total U.S. energy consumption) and as much as 57% of this energy is used by servers [22,74].
Improving server energyefficiency is thus imperative [17].
A recent option is the integration of low-power processors in server network interface cards (NICs).
Examples are the Netronome Agilio-CX [59], Mellanox BlueField [51], Broadcom Stingray [13], and Cavium LiquidIO [15], which rely on ARM/MIPS-based processors and on-board memory.
These SmartNICs can process * The author is now at Google.
microsecond-scale client requests but consume much less energy than server CPUs.
By sharing idle power and the chassis with host servers, SmartNICs also promise to be more energy and cost efficient than other heterogeneous or low-power clusters.
However, SmartNICs are not powerful enough to run large, monolithic cloud applications, preventing their offload.Today, cloud applications are increasingly built as microservices, prompting us to revisit SmartNIC offload in the cloud.
A microservice-based workload comprises loosely coupled processes, whose interaction is described via a dataflow graph.
Microservices often have a small enough memory footprint for SmartNIC offload and their programming model efficiently supports transparent execution on heterogeneous platforms.
Microservices are deployed via a microservice platform [3][4][5]40] on shared datacenter infrastructure.
These platforms abstract and allocate physical datacenter computing nodes, provide a reliable and available execution environment, and interact with deployed microservices through a set of common runtime APIs.
Large-scale web services already use microservices on hundreds of thousands of servers [40,41].
In this paper, we investigate efficient microservice execution on SmartNIC-accelerated servers.
Specifically, we are exploring how to integrate multiple SmartNICs per server into a microservice platform with the goal of achieving better energy efficiency at minimum latency cost.
However, transparently integrating SmartNICs into microservice platforms is non-trivial.
Unlike traditional heterogeneous clusters, SmartNICs are collocated with their host servers, raising a number of issues.
First, SmartNICs and hosts share the same MAC address.
We require an efficient mechanism to route and load-balance traffic to hosts and SmartNICs.
Second, SmartNICs sit in the host's data path and microservices running on a SmartNIC can interfere with microservices on the host.
Microservices need to be appropriately placed to balance network-to-compute bandwidth.
Finally, microservices can contend on shared SmartNIC resources, causing overload.
We need to efficiently detect and prevent such situations.We present E3, a microservice execution platform for SmartNIC-accelerated servers that addresses these issues.
E3 follows the design philosophies of the Azure Service Fabric microservice platform [40] and extends key system components to allow transparent offload of microservices to a Smart-NIC.
To balance network request traffic among SmartNICs and the host, E3 employs equal-cost multipath (ECMP) load balancing at the top-of-rack (ToR) switch and provides highperformance PCIe communication mechanisms between host and SmartNICs.
To balance computation demands, we introduce HCM, a hierarchical, communication-aware microservice placement algorithm, combined with a data-plane orchestrator that can detect and eliminate SmartNIC overload via microservice migration.
This allows E3 to optimize server energy efficiency with minimal impact on client request latency.We make the following contributions: • We show why SmartNICs can improve energy efficiency over other forms of heterogeneous computation and how they should be integrated with data center servers and microservice platforms to provide efficient and transparent microservice execution ( §2).
• We present the design of E3 ( §3), a microservice runtime on SmartNIC-accelerated server systems.
We present its implementation within a cluster of Xeon-based servers with up to 4 Cavium LiquidIO-based SmartNICs per server ( §4).
• We evaluate energy and cost-efficiency, as well as clientobserved request latency and throughput for common microservices, such as a real-time analytics framework, an IoT hub, and various virtual network functions, across various homogeneous and heterogeneous cluster configurations ( §5).
Our results show that offload of microservices to multiple SmartNICs per server with E3 improves cluster energy-efficiency up to 3× and cost efficiency up to 1.9× at up to 4% client-observed latency cost versus all other cluster configurations.
Microservices simplify distributed application development and are a good match for low-power SmartNIC offload.
Together, they are a promising avenue for improving server energy efficiency.
We discuss this rationale, quantify the potential benefits, and outline the challenges of microservice offload to SmartNICs in this section.
Microservices have become a critical component of today's data center infrastructure with a considerable and diverse workload footprint.
Microsoft reports running microservices 24/7 on over 160K machines across the globe, including Azure SQL DB, Skype, Cortana, and IoT suite [40].
Google reports that Google Search, Ads, Gmail, video processing, flight search, and more, are deployed as microservices [41].
These microservices include large and small data and code footprints, long and short running times, billed by run-time and by remote procedure call (RPC) [28].
What unifies these services is their software engineering philosophy.Microservices use a modular design pattern, which simplifies distributed application design and deployment.
Microservices are loosely-coupled, communicating through a set of common APIs, invoked via RPCs [86], and maintain state via reliable collections [40].
As a result, developers can take advantage of languages and libraries of their choice, while not having to worry about microservice placement, communication mechanisms, fault tolerance, or availability.Microservices are also attractive to datacenter operators as they provide a way to improve server utilization.
Microservices execute as light-weight processes that are easier to scale and migrate compared with a monolithic development approach.
They can be activated upon incoming client requests, execute to request completion, and then swapped out.A microservice platform, such as Azure Service Fabric [40], Amazon Lambda [3], Google Application Engine [4], or Nirmata [5], is a distributed system manager that enables isolated microservice execution on shared datacenter infrastructure.
To do so, microservice platforms include the following components (cf. [40]): 1.
federation subsystem, abstracting and grouping servers into a unified cluster that holds deployed applications; 2.
resource manager, allocating computation resources to individual microservices based on their execution requirements; 3.
orchestrator, dynamically scheduling and migrating microservices within the cluster based on node health information, microservice execution statistics, and servicelevel agreements (SLAs); 4.
transport subsystem, providing (secure) point-to-point communication among various microservices; 5.
failover manager, guaranteeing high availability/reliability through replication; 6.
troubleshooting utilities, which assist developers with performance profiling/debugging and understanding microservice co-execution interference.A microservice platform usually provides a number of programming models [10] that developers adhere to, like dataflow and actor-based.
The models capture the execution requirements and describe the communication relationship among microservices.
For example, the data-flow model (e.g. Amazon Datapipe [6], Google Cloudflow [29], Azure Data Factory [55]) requires programmers to assemble microservices into a directed acyclic graph (DAG): nodes contain microservices that are interconnected via flow-controlled, lossless dataflow channels.
These models bring attractive benefits for a heterogeneous platform since they explicitly express concurrency and communication, enabling the platform to transparently map it to the available hardware [68,70].
Figure 1 shows an IoT thermostat analytics application [54] consisting of microservices arranged in 3 stages: 1.
Thermostat sensor updates are authenticated by the API gateway; 2.
Updates are logged into a SQL store sharded by a thermostat identifier; 3.
SQL store updates trigger data analytic tasks (e.g, spike detection, moving average, and recommendation) based on thresholds.
The dataflow programming model allows the SQL store sharding factor to be dynamically adjusted to scale the application with the number of thermostats reporting.
Reliable collections ensure state consistency when re-sharding and the microservice platform automatically migrates and deploys DAG nodes to available hardware resources.A microservice can be stateful or stateless.
Stateless microservices have no persistent storage and only keep state within request context.
They are easy to scale, migrate, and replicate, and they usually rely on other microservices for stateful tasks (e.g., a database engine).
Stateful microservices use platform APIs to access durable state, allowing the platform full control over data placement.
For example, Service Fabric provides reliable collections [40], a collection of data structures that automatically persist mutations.
Durable storage is typically disaggregated for microservices and accessed over the network.
The use of platform APIs to maintain state allows for fast service migration compared with traditional virtual machine migration [19], as the stateful working set is directly observed by the platform.
All microservices in Figure 1 are stateful.
We describe further microservices in §4.
SmartNICs have appeared on the market [15,51,59] and in the datacenter [25].
SmartNICs include computing units, memory, traffic managers, DMA engines, TX/RX ports, and several hardware accelerators for packet processing, such as cryptography and pattern matching engines.
Unlike traditional accelerators, SmartNICs integrate the accelerator with the NIC.
This allows them to process network requests in-line, at much lower latency than other types of accelerators.Two kinds of SmartNIC exist: (1) general-purpose, which allows transparent microservice offload and is the architecture we consider.
For example, Mellanox BlueField [51] has 16 ARMv8 A72 cores with 2×100GE ports and Cavium LiquidIO [15] has 12 cnMIPS cores with 2×10GE ports.
These SmartNICs are able to run full operating systems, but also ship with lightweight runtime systems that can provide kernelbypass access to the NIC's IO engines.
(2) FPGA and ASIC based SmartNICs target highly specialized applications.
Examples include match-and-action processing [25,43] for network dataplanes, NPUs [26], and TPUs [39] for deep neural network inference acceleration.
FPGAs and ASICs do not support transparent microservice offload.
However, they can be combined with general-purpose SmartNICs.A SmartNIC-accelerated server is a commodity server with one or more SmartNICs.
Host and SmartNIC processors do not share thermal, memory, or cache coherence domains, and communicate via DMA engines over PCIe.
This allows them to operate as independent, heterogeneous computers, while sharing a power domain and its idle power.SmartNICs hold promise for improving server energyefficiency when compared to other heterogeneous computing approaches.
For example, racks populated with low-power servers [8] or a heterogeneous mix of servers, suffer from high idle energy draw, as each server requires energy to power its chassis, including fans and devices, and its own ToR switch port.
System-on-chip designs with asymmetric performance, such as ARM's big.LITTLE [38] and DynamIQ [2] architectures, and AMD's heterogeneous system architecture (HSA) [7], which combines a GPU with a CPU on the same die, have scalability limits due to the shared thermal design point (TDP).
These architectures presently scale to a maximum of 8 cores, making them more applicable to mobile than to server applications.
GPGPUs and single instruction multiple threads (SIMT) architectures, such as Intel's Xeon Phi [36] and HP Moonshot [34], are optimized for computational throughput and the extra interconnect hop prevents these accelerators from running latency-sensitive microservices efficiently [57].
SmartNICs are not encumbered by these problems and can thus be used to balance the power draw of latency-sensitive services efficiently.
We quantify the potential benefit of using SmartNICs for microservices on energy efficiency and request latency.
To do so, we choose two identical commodity servers and equip one with a traditional 10GbE Intel X710 NIC and the other with a 10GbE Cavium LiquidIO SmartNIC.
Then we evaluate 16 different microservices (detailed in §4) on these two servers with synthetic benchmarks of random 512B requests.
We measure request throughput, wall power consumed at peak throughput (defined as the knee of the latency-throughput graph, where queueing delay is minimal) and when idle, as well as clientobserved, average/tail request latency in a closed loop.
We use host cores on the traditional server and SmartNIC cores on the SmartNIC server for microservice execution.
We use as many identical microservice instances, CPUs, and client machines as necessary to attain peak throughput and put unused CPUs to their deepest sleep state.
The SmartNIC does not support per-core low power states and always keeps all 12 cores active, diminishing SmartNIC energy efficiency results somewhat.
The SmartNIC microservice runtime system uses a kernel- bypass network stack (cf. §4).
To break out kernel overheads from the host experiments, we run all microservices on the host in two configurations: 1.
Linux kernel network stack; 2.
kernel-bypass network stack [63], based on Intel's DPDK [1].
Table 1 presents measured peak request throughput, active power (wall power at peak throughput minus idle wall power), number of active cores, (tail-)latency, and energy efficiency, averaged over 3 runs.
Active power allows a direct comparison of host to SmartNIC processor power draw.
Energy efficiency equals throughput divided by active power.Microservice Host (Linux) Host (DPDK) SmartNIC RPS W C L 99% RPJ RPS W C L 99% RPJ % RPS W L 99% RPJ ×Kernel overhead.
We first analyze the overhead of inkernel networking on the host (Linux versus DPDK).
As expected, the kernel-bypass networking stack performs better than the in-kernel one.
On average, it improves energy efficiency by 21% (% column in Table 1) and reduces tail latency by 16%.
Energy efficiency improves because (1) DPDK achieves similar throughput with fewer cores; (2) at peak server CPU utilization, DPDK delivers higher throughput.SmartNIC performance.
SmartNIC execution improves the energy efficiency of 12 of the measured microservices by a geometric mean of 6.5× compared with host execution using kernel bypass (× column in Table 1).
The SmartNIC consumes at most 24.7W active power to execute these microservices while the host processor consumes up to 113W.
IPSec, BM25, Recommend, and NIDS particularly benefit from various SmartNIC hardware accelerators (crypto coprocessor, fetch-and-add atomic units, floating point engines, and pattern matching units).
NATv4, Count, EMA, KVS, Flow monitor, and DDoS can take advantage of the computational bandwidth and fast memory interconnect of the SmartNIC.
In these cases, the energy efficiency comes not just from the lower power consumed by the SmartNIC, but also from peak throughput improvements versus the host processor.
KNN and Spike attain lower throughput on the SmartNIC.
However, since the SmartNIC consumes less power, the overall energy efficiency is still better than the host.
For all of these microservices, the SmartNIC also improves client-observed latency.
This is due to the hardware accelerated packet buffers and the elimination of PCIe bus traversals.
SmartNICs can reduce average and tail latency by a geometric mean of 45.3% and 45.4% versus host execution, respectively.The host outperforms the SmartNIC for Top ranker, Bayes classifier, SQL, and API gateway by a geometric mean of 4.1× in energy efficiency, 41.2% and 30.0% in average and tail latency reduction.
These microservices are branch-heavy with large working sets that are not handled well by the simpler cache hierarchy of the SmartNIC.
Moreover, the API gateway uses double floating point numbers for the rate limiter implementation, which the SmartNIC emulates in software.Request size impact.
SmartNIC performance depends also on request size.
To demonstrate this, we vary the request size of our synthetic workload and evaluate SmartNIC energy efficiency benefits of 5 microservices versus host execution.
Figure 2 shows that with small (≤128B) requests, SmartNIC benefit of IPSec, NIDS, and DDoS is smaller.
Small requests are more computation intensive and we are limited by the SmartNIC's wimpy cores.
SmartNIC offload hits a sweet-spot at 256-512B request size, where the benefit almost doubles.
Here, network and compute bandwidth utilization are balanced for the SmartNIC.
At larger request sizes, we are network bandwidth limited, allowing us to put host CPUs to sleep and SmartNIC benefits again diminish.
This can be seen in particular for IPsec, which outperforms on the SmartNIC due to hardware cryptography acceleration, but still diminishes with larger request sizes.
We conclude that request size has a major impact on the benefit of SmartNIC offload.
Measuring it is necessary to make good offload choices.
We conclude that SmartNIC offload can provide large energy efficiency and latency benefits for many microservices.
However, it is not a panacea.
Computation and memory intensive microservices are more suitable to run on the host processor.
We need an efficient method to define and monitor critical SmartNIC offload criteria for microservices.
While there are quantifiable benefits, offloading microservices to SmartNICs brings a number of additional challenges:• SmartNICs share the same Ethernet MAC address with the host server.
Layer 2 switching is not enough to route traffic between SmartNICs and host servers.
We require a different switching scheme that can balance traffic and provide fault tolerance when a server equips multiple SmartNICs.
• Microservice platforms assume uniform communication performance among all computing nodes.
However, Fig- ure 3 shows that SmartNIC-Host (via PCIe) and SmartNICSmartNIC (via ToR switch) communication round-triptime (RTT) is up to 83.3% and 86.2% lower than host-host (via ToR switch) kernel-bypass communication.
We have to consider this topology effect to achieve good performance.
• Microservices share SmartNIC resources and contend with SmartNIC firmware for cache and memory bandwidth.
This can create a head-of-line blocking problem for network packet exchange with both SmartNIC and host.
Prolonged head-of-line blocking can result in denial of service to unrelated microservices and is more severe than transient sources of interference, such as network congestion.
We need to sufficiently isolate SmartNIC-offloaded microservices from firmware to guarantee quality of service.
We present the E3 microservice platform for SmartNICaccelerated servers.
Our goal is to maximize microservice energy efficiency at scale.
Energy efficiency is the ratio of microservice throughput and cluster power draw.
Power draw is determined by our choice of SmartNIC-acceleration, while E3 focuses on maximizing microservice throughput on this heterogeneous architecture.
We describe how we support microservice offload to a SmartNIC and address the request routing, microservice placement, and scheduling challenges.E3 overview.
E3 is a distributed microservice execution platform.
We follow the design philosophies of Azure Service Fabric [40] but add energy efficiency as a design requirement.
Figure 4 shows the hardware and software architecture of E3.
E3 runs in a typical datacenter, where servers are grouped into racks, with a ToR switch per rack.
Each server is equipped with one or more SmartNICs, and each SmartNIC is connected to the ToR.
This creates a new topology where host processors are reachable via any of the SmartNICs (Figure 4a).
SmartNICs within the same server also have multiple communication options-via the ToR or PCIe ( §3.1).
Programming model.
E3 uses a dataflow programming model.
Programmers assemble microservices into a DAG of microservice nodes interconnected via channels in the direction of RPC flow (cf. Figure 1).
A channel provides lossless data communication between nodes.
A DAG in E3 describes all RPC and execution paths of a single microservice application, but multiple DAGs may coexist and execute concurrently.
E3 is responsible for mapping DAGs to computational nodes.Software stack.
E3 employs a central, replicated cluster resource controller [40] and a microservice runtime on each host and SmartNIC.
The resource controller includes four components: (1) traffic control, responsible for routing and load balancing requests between different microservices; (2) controlplane manager, placing microservice instances on cluster nodes; (3) data-plane orchestrator, dynamically migrates microservices across cluster nodes; (4) failover/replication manager, providing failover and node membership management using consistent hashing [75].
The microservice runtime includes an execution engine, an orchestrator agent, and a communication subsystem, described next.Execution engine.
E3 executes each microservice as a multi-threaded process, either on the SmartNIC or on the host.
The host runs Linux.
The SmartNIC runs a lightweight firmware.
Microservices interact only via microservice APIs, allowing E3 to abstract from the OS.
SmartNIC and host support hardware virtual memory for microservice confinement.
E3 is work-conserving and runs requests to completion.
It leverages a round-robin policy for steering incoming requests to cores, context switching cores if needed.Orchestrator agent.
Each node runs an orchestrator agent to periodically monitor and report runtime execution characteristics to the resource controller.
The information is used by (1) the failover manager to determine cluster health and (2) the data-plane orchestrator to monitor the execution performance of each microservice and make migration decisions.
On the host, the agent runs as a separate process.
On the SmartNIC, the agent runs on dedicated cores (blue in Figure 4-b) and a traffic manager hardware block exchanges packets between the NIC MAC ports and the agent.
For each packet, the agent determines the destination (network, host, or SmartNIC core).
Local SmartNIC-host communication.
SmartNIC and host cores on the same server communicate via PCIe.
Prior work has extensively explored communication channels via PCIe [47,48,60], and we adopt their design.
High-throughput messaging for PCIe interconnects requires leveraging multiple DMA engines in parallel.
E3 takes advantage of the eight DMA engines on the LiquidIO, which can concurrently issue scatter/gather requests.
Local SmartNIC-SmartNIC communication.
SmartNICs in the same host can use three methods for communication.
1.
Using the host to relay requests, involving two data transfers over PCIe and pointer manipulation on the host, increasing latency.
2.
PCIe peer-to-peer [23], which is supported on most SmartNICs [15,51,59].
However, the bandwidth of peer-to-peer PCIe communication is capped in a NUMA system when the communication passes between sockets [57].
3.
ToR switch.
We take the third approach and our experiments show that this approach incurs lower latency and achieves higher bandwidth than the first two.
Since SmartNICs and their host servers share Ethernet MAC addresses, we have to use an addressing/routing scheme to distinguish between these entities and load balance across them.For illustration, assume we have a server with two SmartNICs; each NIC has one MAC port.
If remote microservices communicate with this server, there will be two possible paths and each might be congested.We use equal-cost multi-path (ECMP) [84] routing on the ToR switch to route and balance load among these ports.
We assign each SmartNIC and the host its own IP.
We then configure the ToR switch to route to SmartNICs directly via the attached ToR switch port and an ECMP route to the host IP via any of the ports.
The E3 communication subsystem on each SmartNIC differentiates by destination IP address whether an incoming packet is for the SmartNIC or the host.
On the host, we take advantage of NIC teaming [56] (also know as port trunking) to bond all related SmartNIC ports into a single logical interface, and then apply the dynamic link aggregation policy (supporting IEEE 802.3ad protocol).
ECMP automatically balances connections to the host over all available ports.
If a link or SmartNIC fails, ECMP will automatically rebalance new connections via the remaining links, improving host availability.
The control-plane manager is responsible for energy-efficient microservice placement.
This is a computing intensive operation due to the large search space with myriad constraints.
Hence, it is done on the control plane.
Service Fabric uses simulated annealing, a well-known approximate algorithm, to solve microservice placement.
It considers three types of constraints: (1) currently available resources of each computing node (memory, disk, CPU, network bandwidth); (2) computing node runtime statistics (aggregate outstanding microservice requests); (3) individual microservice execution behavior (average request size, request execution time and frequency, diurnal variation, etc.).
Service Fabric ignores network topology and favors spreading load over multiple nodes.E3 extends this algorithm to support bump-in-the-wire SmartNICs, considering network topology.
We categorize computing nodes (host or SmartNIC processors) into different levels of communication distance and perform a search from the closest to the furthest.
We present the HCM algorithm (Algorithm 1).
HCM takes as input the microservice DAG G and source nodes V src , as well as the cluster topology T , including runtime statistics for each computing node (as collected).
HCM performs a breadth-first traversal of G to map microservices to cluster computing nodes (MS_DAG_TRAVERSE).
If not already deployed (get_deployed_node), HCM (via MS_DAG_TRAVERSE) assigns a microservice V to a computing node N via the find_first_fit function (lines 9-11) and deploys it via set_deployed_node.
find_first_fit is a greedy algorithm that returns the first computing node that satisfies the microservice constraints (via its resource and runtime statistics) without considering communication cost.
If no such node is found, it returns a node closest to the constraints.
Next, while Q is not empty do V ← Q.dequeue() 8:N ← get_deployed_node(V ) 9:if N is NULL then 10:N ← f ind_ f irst_ f it(V, T ) set_deployed_node(V, N) for W in all direct descendants o f V in G do Topo ← get_hierarchical_topo(N, T ) for L in all Topo.Levels do 23:N ← f ind_best_ f it(V, Topo.node_list(L)) 24:if N is not NULL then return f ind_ f irst_ f it(V, T ) Ignore topology for the descendant microservices of a node V (lines 12-15), HCM assigns them to computing nodes based on their communication distance to V (MS_PLACE).
To do so, HCM first computes the hierarchical topology representation of computing node N via get_hierarchical_topo.
Each level in the hierarchical topology includes computing nodes that require a similar communication mechanism, starting with the closest.
For example, in a single rack there are four levels in this order: 1.
The same computing node as V ; 2.
An adjacent computing node on the same server; 3.
A SmartNIC computing node on an adjacent server; 4.
A host computing node on an adjacent server.
If there are multiple nodes in the same level, HCM uses find_best_fit to find the best fit, according to resource constraints.
If no node in the hierarchical topology fits the constraints, we fall back to find_first_fit.
The data-plane orchestrator is responsible for detecting load changes and migrating microservices in response to these changes among computational nodes at run-time.
To do so, we piggypack several measurements onto the periodic node health reports made by orchestrator agents to the resource controller: This approach is lightweight and integrates well with runtime execution.
We believe that our proposed methods can also be used in other microservice schedulers [33,62,66].
In this section, we introduce the additional techniques implemented in our data-plane orchestrator to mitigate issues of SmartNIC overload caused by compute-intensive microservices.
These can interfere with the SmartNIC's traffic manager, starving the host of network packets.
They can also simply execute too slowly on the SmartNIC to be able to catch up with the incoming request rate.Host starvation.
This issue is caused by head-of-line blocking of network traffic due to microservice interference with firmware on SmartNIC memory/cache.
It is typically caused by a single compute-intensive microservice overloading the SmartNIC.
To alleviate this problem, we monitor the incoming/outgoing network throughput and packet queue depth at the traffic manager.
If network bandwidth is under-utilized, but there is a standing queue at the traffic manager, the Smart-NIC is overloaded, and we need to migrate microservices.Microservice overload.
This issue is caused by microservices in aggregate requiring more computational bandwidth than the SmartNIC can offer, typically because too many microservices are placed on the same SmartNIC.
To detect this problem, we periodically monitor the execution time of each microservice and compare to its exponential moving average.
When the difference is negative and larger than 20%, we assume a microservice overload and trigger microservice migration.
The threshold was determined empirically.Microservice migration.
For either issue, the orchestrator will migrate the microservice with the highest CPU utilization to the host.
To do so, it uses a cold migration approach, similar to other microservice platforms.
Specifically, when the orchestrator makes a migration decision, it will first push the microservice binary to the new destination, and then notify the runtime of the old node to (1) remove the microservice instance from the execution engine; (2) clean up and free any local resources; (3) migrate the working state, as represented by reliable collections [40], to the destination.
After the orchestrator receives a confirmation from the original node, it will update connections and restart the microservice execution on the new node.
Since SmartNICs share the same power supply as their host server, our failover manager treats all SmartNICs and the host to be in the same fault domain [40], avoiding replica placement within the same.
Replication for fault tolerance is typically done across different racks of the same datacenter or across datacenters, and there is no impact from placing SmartNICs in the same failure domain as hosts.
Idle Peak Bw Beefy 4,500 12 0 64 83 201 20 Wimpy 2,209 0 32 2 79 95 20 Type1-SmartNIC 4,650 12 12 68 98 222 20 Type2-SmartNIC 6,750 16 48 144 145 252 40 SuperBeefy 12,550 24 0 192 77 256 80 4×Beefy 18,000 48 0 256 332 804 80 4×Wimpy 8 Applications.
Based on these microservices, we develop eight applications across three application domains: (1) Distributed real-time analytics (RTA), such as Apache Storm [78], implemented as a dataflow processing graph of workers that pass data tuples in real time to trigger computations; (2) Network function (NF) virtualization (NFV) [61], which is used to build cloud-scale network middleboxes, software switches, and enterprise IT networks, by chaining NFs; (3) An IoT hub (IOT) [53], which gathers sensor data from edge devices and generates events for further processing (e.g., spike detection, classifier) [?
, 77].
To maximize throughput, applications may shard and replicate microservices, resulting in a DAG node count larger than the involved microservice types.
Table 3 presents the microservice types involved in each application, the deployed DAG node count, and references the workloads used for evaluation.
The workloads are trace-based and synthetic benchmarks, validated against realistic scenarios.
The average and maximum node fanouts among our applications are 6 and 12, respectively.
Figure1 shows IOT-TS as an example.
IOT-TS is sharded into 6×API, 12×SQL, 12×EMA, 12×Spike, and 12×recommend and each microservice has one backup replica.
Our evaluation aims to answer the following questions: 1.
What is the energy efficiency benefit of microservice SmartNIC-offload?
Is it proportional to client load?
What is the latency cost?
( §5.1) 2.
Does E3 overcome the challenges of SmartNIC-offload?
( §5.2, §5.3, §5.4) 3.
Do SmartNIC-accelerated servers provide better total cost of ownership than other cluster architectures?
( §5.5) 4.
How does E3 perform at scale?
( §5.6) Experimental setup.
Our experiments run on a set of clusters (Table 4 presents the server and cluster configurations), attached to an Arista DCS-7050S ToR switch.
Beefy is a Supermicro 1U server, with a 12-core E5-2680 v3 processor at 2.5GHz, and a dual-port 10Gbps Intel X710 NIC.
Wimpy is ThunderX-like, with a CN6880 processor (32 cnMIPS64 cores running at 1.2GHz), and a dual-port 10Gbps NIC.
SuperBeefy is a Supermicro 2U machine, with a 24-core Xeon Platinum 8160 CPU at 2.1GHz, and a dual-port 40Gbps Intel XL710 NIC.
Our SmartNIC is the Cavium LiquidIOII [15], with one OCTEON processor with 12 cnMIPS64 cores at 1.2GHz, 4GB memory, and two 10Gbps ports.
Based on this, we build two SmartNIC servers: Type1 is Beefy, but swaps the X710 10Gbps NIC with the Cavium LiquidIOII; Type2 is a 2U server with two 8-core Intel E5-2620 processors at 2.1GHz, 128GB memory, and 4 SmartNICs.
All servers have a Seagate HDD.
We build the clusters such that each has the same amount of aggregate network bandwidth.
This allows us to compare energy efficiency based on the compute bandwidth of the clusters, without varying network bandwidth.
We also exclude the switch from our cost and energy evaluations, as each cluster uses an identical number of switch ports.We measure server power consumption using the servers' IPMI data center management interface (DCMI), crosschecked by a Watts Up wall power meter.
Throughput and average/tail latency across 3 runs are measured from clients (Beefy machines), of which we provide as many as necessary.
We enable hyper-threading and use the Intel_pstate governor for power management.
All benchmarks in this section report energy efficiency as throughput over server/cluster wall power (not just active power).
Peak utilization.
We evaluate the latency and energy efficiency of using SmartNICs for microservice applications, compared to homogeneous clusters.
We compare 3×Beefy to 3×Type1-SmartNIC, to ensure that microservices also communicate remotely.
We focus first on peak utilization, which is desirable for energy efficiency, as it amortizes idle power draw.
To do so, we deploy as many instances of each application and apply as much client load as necessary to maximize request throughput without overloading the cluster, as determined by the knee of the latency-throughput curve.
Figure 5 shows that Type1-SmartNIC achieves an average 2.5×, 1.3×, and 1.3× better energy efficiency across the NFV, RTA, and IOT application classes, respectively.
This goes along with 43.3%, 92.3%, and 80.4% average latency savings and 35.5%, 90.4%, 88.6% 99th percentile latency savings, respectively.
NFV-FIN gains the most-3× better energy efficiency-because E3 is able to run all microservices on the SmartNICs.
RTA-PTC benefits the least-12% energy efficiency improvement at 4% average and tail latency costas E3 only places the Count microservice on the SmartNIC and migrates the rest to the host.Power proportionality.
This experiment evaluates the power proportionality of E3 (energy efficiency at lower than peak utilization).
Using 3×Type1-SmartNIC, we choose an application from each class (NFV-FIN, RTA-SHM, and IOT-TS) and vary the offered request load between idle and peak via a client side request limiter.
Figure 8 shows that RTA-SHM and IOT-TS are power proportional.
NFV-FIN is not power proportional but also draws negligible power.
NFV-FIN runs all microservices on the SmartNICs, which have low active power, but the cnMIPS architecture has no per-core sleep states.We conclude that applications can benefit from E3's microservice offload to SmartNICs, in particular at peak cluster utilization.
Peak cluster utilization is desirable for energy efficiency and microservices make it more common due to light-weight migration.
However, transient periods of low load can occur and E3 draws power proportional to request load.
We can apply insights from Prekas, et al. [65] to reduce polling overheads and improve power proportionality further.
We show that E3's data-plane orchestrator prevents host starvation by identifying head-of-line blocking of network traffic.
To do so, we use 3×Type1-SmartNIC and place as many microservices on the SmartNIC as fit in memory.
E3 identifies the microservices that cause interference (Top Ranker in RTA-PTC, Spike in RTA-SF, API gateway in IOT-DH and IOT-TS) and migrates them to the host.
As shown in Figure 7, our approach achieves up to 29× better energy efficiency and up to 89% latency reduction across RTA-PTC, RTS-SF, IOT-DH, and IOT-TS.
For the other applications, our traffic engine has little effect because the initial microservice assignment already put the memory intensive microservices on the host.
This experiment evaluates the benefits of sharing SmartNIC network bandwidth with the host.
We compare two Type2-SmartNIC configurations: 1.
Sharing aggregate network bandwidth among host and SmartNICs, using ECMP to balance host traffic over SmartNIC ports; 2.
Replacing one SmartNIC with an Intel X710 NIC used exclusively to route traffic to the host.
To emphasize the load balancing benefits, we always place the client-facing microservices on the host server.
Note that SmartNIC-offloaded microservices still exchange network traffic (when communicating remotely or among SmartNICs) and interfere with host traffic.
Figure 9 shows that load balancing improves application throughput up to 2.9× and cluster energy efficiency up to 2.7× (NFV-FIN).
Available host network bandwidth when sharing SmartNICs can be up to 4× that of the dedicated NIC, which balances better with the host compute bandwidth.
With a dedicated NIC, host processors can starve for network bandwidth.
IOT-TS is compute-bound and thus benefits the least from sharing.
In terms of latency, all cases behave the same since the request execution flows are the same.
To show the effectiveness of communication-aware microservice placement, we evaluate HCM on E3 without data-plane orchestrator.
In this case, all microservices are stationary after placement.
We avoid host starvation and microservice overload by constraining problematic microservices to the host.
Using 3×Type1-SmartNIC and all placement constraints of Service Fabric [40] (described in §3.3), we compare HCM with both simulated annealing and an integer linear program (ILP).
HCM places the highest importance on minimizing microservice communication latency.
Simulated annealing and ILP use a cost function with the highest weight on minimizing co-execution interference.
Hence, HCM tries to co-schedule communicating microservices on proximate resources, while the others will spread them out.
ILP attempts to find the best configuration, while simulated annealing approximates.
Fig- ure 6 shows that compared to simulated annealing and ILP, HCM improves energy efficiency by up to 35.2% and 22.0%, and reduces latency by up to 24.0% and 18.6%, respectively.
HCM's short communication latency benefits outweigh interference from co-execution.
While SmartNICs benefit energy efficiency and thus potentially bring cost savings, can they compete with other forms of heterogeneous clusters, especially when factoring in the capital expense to acquire the hardware?
In this experiment, we evaluate the cost efficiency, in terms of request throughput over total cost and time of ownership, of using SmartNICs for microservices, compared with four other clusters (see Table 4).
Assuming that clusters are usually at peak utilization, we use the cost efficiency metric T hroughput×T CAPEX+(Power×T ×Electricity) , where T hroughput is the measured average throughput at peak utilization for each application, as executed by E3 on each cluster, T is elapsed time, CAPEX is the capital expense to purchase the cluster including all hardware components ($), Power is the elapsed peak power draw of the cluster (Watts), and Electricity is the price of electricity ($/Watts).
The cluster cost and power data is shown in Table 4 and we use the average U.S. electricity price [31] of $0.0733/kWh.
Figure 10 reports results for three applications of very different points in the workload space, extrapolated over time of ownership by our cost efficiency metric.We make three observations.
First, in the long term (>1 year of ownership), cost efficiency is increasingly dominated by energy efficiency.
This highlights the importance of energy efficiency for data center design, where servers are typically replaced after several years to balance CAPEX [12].
Second, when all microservices are able to run on a low power platform (NFV-FIN), both 4×Wimpy and 2×Type2-SmartNIC clusters are the most cost efficient.
After 5 years, 4×Wimpy is 14.1% more cost efficient than 2×Type2-SmartNIC because of the lower power draw.
Third, when a microservice application contains both compute and IO-intensive microservices (RTA-SHM, IOT-TS), the 2×Type2-SmartNIC cluster is up to 1.9× more cost efficient after 5 years of ownership than the next best cluster configuration (4×Beefy in both cases).
Table 5 presents the measured energy-efficiency, which shows cost efficiency in the limit (over very long time of ownership).
We can see that 4×Wimpy is only 3% more energy efficient (but has lower CAPEX) than 2×Type2-SmartNIC for NFV-FIN.
2×Type2-SmartNIC is on average 2.37× more energy-efficient (but has higher CAPEX) than 1×SuperBeefy, which is the second-best cluster in terms of energy-efficiency.
We evaluate and discuss the scalability of E3 along three axes: 1.
Mechanism performance scalability; 2.
Tail-latency; 3.
Energy-efficiency.
Mechanism scalability.
At scale, pressure on the controlplane manager and data-plane orchestrator increases.
We evaluate the performance scalability of both mechanisms with an increasing number of Type2-SmartNIC servers in a simulated FatTree [30] topology with 40 servers per rack.
To avoid host starvation and microservice overload, E3's data-plane orchestrator receives one heartbeat message (16B) every 50ms from each SmartNIC that reports the queue length of the traffic manager and the SmartNIC's microservice execution times.
The orchestrator parses the heartbeat message and makes a migration decision ( §3.4).
Figure 11 shows that the time taken to transmit the message and make a decision with a large number of servers stays well below the time taken to migrate the service (on the order of 10s-100s of ms) and is negligibly impacted by the number of deployed microservices.
This is because the heartbeat message contributes only 1Kbps of traffic, even with 50K servers.E3 uses HCM in the control-plane manager.
We compare it to simulated annealing and ILP, deploying 10K microservices on an increasing number of servers.
Table 6 shows that while HCM does not scale as well as simulated annealing, it can deploy new microservices in a reasonable time span (<<1s) at scale.
ILP fails to deliver acceptable performance.Tail latencies.
At scale, tail latencies dominate [20].
While SmartNICs can introduce high tail latency for some microservices ( §2.3), E3 places these microservices on the host to ensure that application-level tail-latency inflation is minimized ( §5.1).
The tail-latency impact of SmartNIC offload is reduced at scale, as baseline communication latency increases with increasing inter-rack distance.Energy-efficiency and power budgets.
E3's energy efficiency benefits are constant regardless of deployment size and power budgets.
At scale, there is additional energy cost for core and spine switches, but these are negligible compared to racks (ToRs and servers).
Within a rack, ToR switch energy consumption stays constant, as all compared systems use the same number of switch ports.
Our results show that E3 achieves up to 1.9x more throughput under the same power budget.
Conversely, operators can save 48% of power, offering the same bandwidth.
Architecture studies for microservices.
Prior work has explored the architectural implications of running microservices [21,27,79].
It shows that wimpy servers hold potential for microservices under low loads, as they have less cache utilization and network virtualization overhead compared with traditional monolithic cloud workloads.
FAWN [8] explored using only low-power processor architecture for dataintensive workloads as an energy and cost-effective alternative to server multiprocessors.
However, FAWN assumed that I/O speeds are much lower than CPU speeds and so CPUs would be left idle for data-intensive applications.
With the advent of fast network technologies, server CPUs are still required.
Motivated by these studies, we focus on using SmartNICaccelerated servers for energy efficiency.Heterogeneous scheduling.
A set of schedulers for performance-asymmetric architectures have been proposed.
For example, Kumar et al. [44, 45] use instructions per cycle to determine relative speedup of each thread on different types of cores.
HASS [73] introduces the architectural signature concept as a scheduling indicator, which contains information about memory-boundedness, available instruction-level parallelism, etc.
CAMP [71] combines both efficiency and thread-level parallelism specialization and proposes a lightweight technique to discover which threads could use fast cores more efficiently.
PTask [69] provides a data-flow programming model for programmers to manage computation for GPUs.
It enables sharing GPUs among multiple processes, parallelizes multiple tasks, and eliminates unnecessary data movements.
These approaches target long-running computations, mostly on cache coherent architectures, rather than microsecond-scale, request-based workloads over compute nodes that do not share memory and are hence not applicable.Microservice scheduling.
Wisp [76] enforces end-to-end performance objectives by globally adapting rate limiters and request schedulers based on operator policies under varying system conditions.
This work is not concerned with Smart-NIC heterogeneity.
UNO [46] is an NFV framework that can systematically place NFs across SmartNIC and host with a resource-aware algorithm on the control plane.
E3 is a microservice platform and thus goes several steps further: (1) E3 uses a data-plane orchestrator to detect node load and migrates microservices if necessary; (2) HCM considers communication distance during the placement.
With the advent of SmartNICs and programmable switches, researchers have identified the potential performance benefits of applying request processing across the communication path [14].
E3 is such a system designed for the programmable cloud and explores the energy efficiency benefits of running microservices.Power proportionality.
Power proportional systems can vary energy use with the presented workload [50].
For example, Prekas, et al. propose an energy-proportional system management policy for memcached [65].
While E3 can provide energy proportionality, we are primarily interested in energyefficiency.
Geoffrey et al. [18] propose a heterogeneous powerproportional system.
By carefully selecting component ensembles, it can provide an energy-efficient solution for a particular task.
However, due to the high cost of ensemble transitions, we believe that this architecture is not fit for high bandwidth I/O systems.
Rivoire, et al. propose a more balanced system design (for example, a low-power, mobile processor with numerous laptop disks connected via PCIe) and show that it achieves better energy efficiency for sorting large data volumes [67].
Pelican [11] presents a software storage stack on under-provisioned hardware targeted at cold storage workloads.
Our proposal could be viewed as a balanced-energy approach for low-latency query-intensive server applications, rather than cold, throughput intensive ones.
We present E3, a microservice execution platform on SmartNIC-accelerated servers.
E3 extends key system components (programming model, execution engine, communication subsystem, scheduling) of the Azure Service Fabric microservice platform to a SmartNIC.
E3 demonstrates that SmartNIC offload can improve cluster energy-efficiency up to 3× and cost efficiency up to 1.9× at up to 4% latency cost for common microservices.
This work is supported in part by NSF grants CNS-1616774, CNS-1714508, CNS-1751231 and the Texas Systems Research Consortium.
We would like to thank the anonymous reviewers and our shepherd, Ada Gavrilovska, for their comments and feedback.
