Our framework uses these software features to build regression models that can predict the resilience of different software protection transformations against automated attacks. Our results show that features such as the number of community structures in the graph-representation of symbolic path-constraints, are far more relevant for predicting deobfuscation time than other features generally used to measure the potency of control-flow obfuscation (e.g. cyclomatic complexity). Malicious endusers are called man-at-the-end (MATE) attackers and their capabilities include everything from static analysis to dynamic modification of the executable code and memory (e.g. debugging, tampering with code and data values, probing any hardware data bus, etc.). • A free C program generator, which was used to create a dataset of over 4500 different programs in order to benchmark our approach. • A case study involving over 23000 obfuscated programs, where we build and test regression models to predict the resilience of the obfuscated programs against symbolic execution attacks. • A model that can predict the resilience of several code obfuscating techniques against an attack based on symbolic execution, with over 90% accuracy for 80% of the programs in our dataset.The remainder of this paper is organized as follows. This taxonomy states that code obfuscation should be evaluated with respect to: potency against humanassisted attacks, resilience against automated attacks, cost (in terms of performance overhead) added by the obfuscating transformation and stealth, which measures the difficulty of identifying parts of obfuscated code in a given program. Collberg et al. [15] also proposed using several existing software features to evaluate potency, namely: program length, cyclomatic complexity, nesting complexity, data flow complexity, fan-in/-out complexity, data structure complexity and object oriented design metrics. In this paper we focus on predicting the effort needed by an automated deobfuscation attack.Our work is complementary to the Obfuscation Executive (OE) proposed by Heffner and Collberg [23]. The OE uses software complexity metrics and performance measurements to choose a sequence of obfuscating transformations, that should be applied to a program in order to increase its potency, while our paper is solely concerned with resilience. However, they apply deobfuscation attacks which are specific to different obfuscating transformations, while we use a general deobfuscation attack (based on symbolic execution) on all obfuscating transformations. In this paper we are chiefly concerned with predicting the effort needed to run a successful deobfuscation attack.Wu et al. [48] propose using a linear regression model over a fixed set of features, for measuring the potency of obfuscating transformations. Resilience is defined as a function of deobfuscator effort 1 and programmer effort (i.e. the time spent building the deobfuscator) [15]. The work-flow requires a dataset of original (un-obfuscated) programs to be able to start (step 0 in Figure 1). Subsequently, an implementation of a deobfuscation attack (e.g. controlflow simplification [49], secret extraction [5], etc.) is executed on all of the obfuscated programs, and the time needed to successfully complete the attack for each of the obfuscated programs is recorded (step 2 in Figure 1). Once the attack times are recorded and software features are extracted from all programs, one could directly use this information to build a regression model for predicting the time needed for deobfuscation. One can substitute the obfuscation tool in Figure 1, with any kind of software protection mechanism (e.g. code layout randomization [38]) and the deobfuscation tool by any known attack implementation corresponding to that software protection mechanism (e.g. ROPeme [27]). Ideally, we would have access to a large corpus of open source programs that contain a security check (such as a license check) that needed to be protected against discovery and tampering, as presented in [4]. Since we only want to focus on the second part of this attack (i.e. symbolically executing the license checking code snippet), our C program generator produces a large number of simple programs with diverse license checking algorithms, having a variety of controland data-flows. After input expansion, the values in the state array are processed via control flow statements containing various operations on the state variables (lines 6-17). However, here we only provide a description of those options which have been used to generate the dataset of programs used in the experiments from Section 4, i.e.:• RandomFunsTypes indicates the data type of the input, output and state arrays. The possible types are: (1) a constant value, (2) a value from the input array and (3) a value from the input array modulo a constant. Few inputs of the random function take this path, hence, finding such an input is equivalent to finding a valid license key.The reason why we chose to implement these features is that we suspect them to be relevant for the deobfuscation attack presented in [5], which is used in our case study presented in Section 4. Given a set of several software features (e.g. complexity metrics), it is unclear which software features one should aim to change (by applying various obfuscating transformations), such that the resulting obfuscated program is more resilient against certain automated deobfuscation attacks. There are several approaches for feature selection published in the literature, e.g. using genetic algorithms [8] or simulated annealing [29]. From our experiments we noticed that such feature extraction algorithms are time-consuming, i.e. even with datasets of the order of tens of thousands of entries and a few dozen features it takes weeks of computation time. Another way of selecting relevant features from a large set of features is to first build a regression model (e.g. via random forest, support vector machines, neural networks, etc.), using all available features and record the prediction error. Check the importance of each variable (i.e. feature) using the technique described in [9], i.e. add random noise by permuting values for the i-th variable and average the difference between the prediction error after randomization and before.2. RQ2 Which regression algorithms generate models that can predict the attack effort with the lowest error?Due to space constraints, in this paper we will focus on the deobfuscation attack based on symbolic execution presented in [5], which is equivalent to extracting a secret license key hidden inside the code of the program via obfuscation. However, in future work we plan to apply the approach proposed in Section 3, to other types of automated attacks, such as control-flow simplification [49]. The following is a list of parameters and their corresponding values we used to generate this dataset:• The random seed value: Seed ∈ {1,2,4} (3 values). • The number of statements per basic block was changed via the value of n ∈ {1, 2} from Table 1: Operator parameter values given to C code generator used for generating dataset.RandomFunsControlStructures Parameter Value (see grammar in Figure 3) win!" Similarly to the randomly generated functions, these hash functions, process the input string passed as an argument to the program and it compares the result to a fixed value. of Loops (if (bb n) (bb n)) 1 1 0 (if (bb n))(if (bb n)) 1 2 0 (if (bb n))(if (bb n))(if (bb n)) 1 3 0 (if (if (bb n) (bb n)) (bb n)) 2 2 0 (if (if (bb n) (bb n)) (if (bb n) (bb n))) 2 3 0 (if (if (if (bb n) (bb n)) (bb n)) (bb n)) 3 3 0 (if (if (if (bb n) (bb n)) (if (bb n) (bb n))) (bb n)) 3 4 0 (if (if (if (bb n) (bb n)) (if (bb n) (bb n))) (if (bb n) (bb n))) 3 5 0 (for (bb n)) 1 0 1 (for (if (bb n) (bb n))) 2 1 1 (for (bb n))(for (bb n)) 1 0 2 (for (for (bb n))) 2 0 2 (for (if (if (bb n) (bb n)) (bb n))) 3 2 1 (for (if (bb n) (bb n))(if (bb n) (bb n))) 2 2 1 (for (if (if (bb n) (bb n)) (if (bb n) (bb n)))) 3 3 1 (for (for (if (bb n) (bb n)))) 3 1 2these 4608 programs vary in size and complexity, as was intended, in order to capture a representative range of license checking algorithms.To increase the number of programs in this set, we generated 275 different variants for each of the noncryptographic hashes using combinations of multiple obfuscation transformations. Table 4 shows the minimum, median, average and maximum values of various code metrics of only the original (un-obfuscated) non-cryptographic hash functions, as computed by the UCC tool and the total number of lines of code (LOC). We have used five obfuscating transformations offered by Tigress [13], in order to generate five obfuscated versions of each of the 4608 programs generated by our code generator and the 11 non-cryptographic hash functions. Table 4: Overview of un-obfuscated simple hash programs.We obfuscated each of the generated programs using these transformations with all the default settings (except for opaque predicates where we set the number of inserted predicates to 16), we obtained 5 × 4608 = 23040 obfuscated programs 4 . We obfuscated each of the non-cryptographic hash functions with every possible pair of these 5 obfuscation transformations and obtained 25 × 11 = 275 obfuscated programs. A state of the art approach for test case generation is called dynamic symbolic execution (often it is simply called symbolic execution). Whenever a branch based on a symbolic value is encountered, symbolic execution forks the state of the program into two different states corresponding to each of the two possible truth values of the branch. The symbolic execution engine sends these constraints to an SMT solver, which tries to find a concrete set of values (for the symbolic variables), which satisfy the constraints. We ran KLEE with a symbolic argument length of 5 characters, on all of the un-obfuscated and obfuscated programs generated by our code generator, for 10 times each. One way to do this is by first converting the C program into a boolean satisfiability problem (SAT instance), and then extracting features from this SAT instance. Hence, for our dataset, the generated SAT instances would require somewhere in the order of 10TBs of data and several weeks of computational power, which is prohibitively expensive.Instead, we took a faster alternative approach for obtaining an optimized SAT instance from a C program, which we describe next. In sum, we transform the path that corresponds to a successful deobfuscation attack into a SAT instance (via an SMT instance), and then compute characteristics of this formula, to be used as features for predicting the effort of deobfuscating the program.For computing source code features often used in software engineering, on both the original and obfuscated programs, we used the Unified Code Counter (UCC) tool [37]. Additionally, we also propose using four other program features, namely: the execution time of the program, the maximum RAM usage of the program, the compiled program file size and the type of obfuscating transformation.In total we have 64 features out of which 49 are SAT features which characterize the complexity of the constraints on symbolic variables and 15 are program features which characterize the structure and size of the code. However, before selecting the most relevant features, we identify how many features (predictor variables) are needed to get good prediction results. For this purpose we performed a 10-foldcross validation with linear and random forest (RF) regression models using all combinations of 5, 10 and 15 metrics, as well as a model with all metrics. The strongest Pearson correlation of the time needed for running the deobfuscation attack is with the average size of clauses in the SAT instance (mean clause), followed by: the average number of times any one variable is used (meanvar), the standard deviation of the ratio of inter to intra community edges (sdedgeratio), the average number of intra community edges (meanintra), the average number of times a clause with the same variable (but different literals) is repeated (mean reused), the average community size (meancom), the number of unique edges (unique edges), the number of variables (vars), the standard deviation of the number of inter community edges (sdinter), the maximum number of distinct communities any one community links to (max community), the number of communities detected with the online community detection algorithm (ol coms), the maximum ratio of inter to intra community edges within any community (maxedgeratio), the maximum number of inter community edges (maxinter), the maximum number of edges in a community (max total) and finally the type of obfuscation transformation employed. The 4th most important variable in Figure 6 is the average number of inter community edges (meaninter), followed by: sdedgeratio, meancom, meanintra (see descriptions of these 3 features in Section 4.2.1), the standard deviation of community sizes (sdcom), the standard deviation of intra community edges (sdintra), the modularity of the SAT graph structure (ol q), the overall ratio of inter to intra community edges (edgeratio), the category of the McCabe cyclomatic complexity [30] (Risk), the number of outer-loops (L1.Loops), the size of the longest clause (max clause) and the number of communities that have the maximum number of inter community edges (num max inter). For example, Figure 7 illustrates the graph representation of the SAT instance 7 of the MD5-27-4 hash function of the Li-Ye benchmark suite [28] proposed during the 2014 SAT Competition. This makes sense since a SAT solver is executing a search when it is trying to solve a SAT instance.On the other hand, many of our randomly generated C programs which were fast to deobfuscate, had established community structures. • RandomFunsPointTest was set to true.Given these parameter values, this instance is expected to be fast to solve, because it does not involve any loops dependent on symbolic inputs and it only involves logical and bitwise operators. These line plots show the maximum and the median errors for all the three cases, where the xaxis represents the percentage of programs for which the relative error (indicated on the y-axis) is lower than the plotted value.Note that in addition to the following regression algorithms we have also employed both linear models and generalized linear models [34]. The accuracy of this model is lower than the RF model from Figure 11, i.e. the maximum rel- 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 ative error is just below 35% for 90% of the programs, when we remove 10% of the outliers. The reason why SVM performs worse than RF is due to the bagging technique applied by RF, whereas SVM uses a single non-linear function.Again we chose to show the SVM model built using the features selected via variable importance in Fig- ure 13, because, as we can see from Figure 14, the maximum and median error rates for this model are much lower than the SVM models built using only UCC metrics or the features selected via Pearson correlation. For instance, the best GP model built using the features selected via variable importance is presented in equation 1:time = (edgeratio + cos(ol coms) + cos(cos(sdcom + num max inter) + L1.Loops)) * (sdinter * (sdedgeratio − sin(meanintra * −1.27))) * (sdedgeratio − sin(meanintra * −1.27)) * (1.03 − sin(0.04 * sdinter)) * sdedgeratio + 10.2(1)Note that only seven distinct features were selected by the GP algorithm for this model, from the subset of 15 features. The input layer consists of the set of code features and the output of the NN is a single value that predicts the time needed to run the deobfuscation attack on a program. Figure 19 shows the prediction error of our best RF model (trained using 10-foldcross-validation on both datasets), for the samples in the smaller dataset alone, has similar levels to the prediction error of the entire dataset.We also performed a reality check, i.e. we verified that the SAT features we identified are also relevant for the realistic hash functions from the Mironov-Zhang [32] and the Li-Ye [28] benchmark suites for SAT solvers. Except for the mizh-md5-47-4 and mizh-md5-47-5 SAT instances, which are the most over-and respectively under-estimated, the rest of the predictions are quite encouraging, given that we have not trained the RF model with any such realistic SAT instances. Our dataset of original (unobfuscated) programs, as well as all scripts and auxiliary software used to run our experiments, are available at:https://github.com/tum-i22/ obfuscation-benchmarks/