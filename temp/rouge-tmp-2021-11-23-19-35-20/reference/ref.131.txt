This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between-1 and 1. We tackled the problem using a number of approaches , utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data.In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016task 5 (Pontiki et al., 2014). The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up ( Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Wang et al. (2016) also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect. We additionally trained a word2vec (Mikolov et al., 2013) word embedding model 3 on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva 4 . We exper-imented with the following different features and parameter settings: For comparison purposes, we tested whether or not a simple whitespace tokeniser can perform just as well as a full tokeniser, and in this case we used Unitok 5 . Positive -When a positive word was mentioned in the input headline from a list of positive words (which was created using the N most similar words based on cosine distance) to 'excellent' using the pre-trained word2vec model.3. We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend ( Abadi et al., 2016). Practically, a BLSTM is two LSTMs one going forward through the tokens the other in reverse order and in our models concatenating the resulting output vectors together at each time step.The BLSTM models take as input a headline sentence of size L tokens 6 where L is the length of the longest sentence in the training texts. Minimised the Mean Square Error (MSE) loss using RMSprop with a mini batch size of 32.3. As can be seen from figure 1, the drop out of 0.5 only happens between the layers and not the 6 Tokenised by Unitok 7 See the following link for detailed implementation details https://github.com/apmoore1/semeval# finance-word2vec-model connections as in the SLSTM. The binary presence of tokens over frequency did not alter performance.The C parameter was tested for three values; 0.01, 0.1 and 1. Therefore, the best SVR model comprised of: Unitok tokenisation, uni-and bi-grams, word representation, C=0.1, eplison=0.01, company, positive, and negative word replacements and target aspects.N n=1 Cosine similarity(Ë† y n , y n ) N (1)The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. This was then changed after the evaluation deadline to equation 1 10 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in Cortis et al. (2017) (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th). Future work will be to implement aspect specific information into an LSTM model as it has been shown to be useful in other work ( Wang et al., 2016).