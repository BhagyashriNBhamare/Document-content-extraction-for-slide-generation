While in its early days, the Web was mostly static, it has organically grown into a full-fledged technology stack. From the archived data, we first identify key trends in the technology deployed on the client, such as the increasing complexity of client-side Web code and the constant rise of multi-origin application scenarios. Finally, we observe that the rising security awareness and introduction of dedicated security technologies had no immediate impact on the overall security of the client-side Web. However, from a security point of view, the Web's track record is less than flattering, to a point in which a common joke under security professionals was to claim that the term Web security is actually an oxymoron.Over the years, Web technologies have given birth to a multitude of novel, Web-specific vulnerability classes, such as Cross-Site Scripting (XSS) or Clickjacking, which simply did not exist before, many of them manifesting themselves on the Web's client side. Now, more than 25 years into the life of the Web, it is worthwhile to take a step back and revisit the development of Web security over the years. Many of the current state-of-the-art security testing methods can be adapted to work on the archived version of the sites, enabling an automated and scalable security evaluation of the historic code.Thus, we find that the archived client-side Web code offers the unique opportunity to study the security evolution of one of the most important technology platforms during (almost) its entire existence, allowing us to conduct historical analyses of a plethora of properties of the Web. The overall goal of this activity is to enable the correlation of trends in the security area with ongoing technological shifts.Resulting Security Problems With the ever-growing complexity of the deployed Web code and the constant addition of new powerful capabilities in the Web browser in the form of novel JavaScript APIs the overall amount of potential vulnerability classes has risen as well. Section 4 documents our security testing methodology and highlights our key findings in the realm of preserved security vulnerabilites.Introduction of Dedicated Security Mechanisms To meet the new challenges of the steadily increasing security surface on the Web's client side, several dedicated mechanisms, such as security-centric HTTP headers or JavaScript APIs, have been introduced. To get a view into the client-side Web's past, the Internet Archive (https://archive.org) offers a great service: since 1996, it archives HTML pages, including all resources which are included, such as images, stylesheets, and scripts. Given that these are the most frequented sites of the time, they also had the greatest interest in securing their sites against attacks.For this purpose, we analyzed the sites identified by Lerner et al. [19] as the 500 most important sites per year. For each year, we used the first working Internet Archive snapshot of each domain as an entry point.Unlike Lerner et al. [19], who investigated the evolution of tracking, though, we did not restrict our analysis to the start pages of the selected sites. On these domains, we crawled a grand total of 659,710 unique URLs, yielding 1,376,429 frames, 5,440,958 distinct scripts, and 21,169,634 original HTTP headers for our analysis. On the one hand, given the redirection issues to later versions discussed above, we cannot ensure a complete coverage of the analyzed Web site, i.e., we might miss a specific page which carries a vulnerability or might not collect an HTTP header only sent when replying to a certain request, e.g., a session cookie sent after login. On the contrary, Internet Explorer does not automatically encode any part of a URL when accessed via JavaScript, i.e., especially in the case of Client-Side Cross-Site Scripting, our results provide a lower bound of exploitable flaws.Nevertheless, we believe that the Archive gives us the 2 For a full list of domains see https://goo.gl/eXjQfs Proxy Figure 1: Infrastructure overview unique opportunity to get a glimpse into the state of Web security over a 20-year time frame. Note that apart from the regular HTTP headers, the Archive also sends the original headers of the site at the time of archiving, prefixed with X-Archive-Orig-, allowing us to collect accurate original header information. While in the beginning of the Web, all content was merely static and at best linking to other documents, the Web has changed drastically over the course of the years. After server-side programming languages such as PHP enabled designing interactive server-side applications, at the latest starting with the advent of the socalled Web 2.0 around 2003, client-side technology became more and more important. To understand how this client-side technology evolved over time, we analyzed the HTML pages retrieved from the Internet Archive, searching specifically for the most relevant technologies, i.e., JavaScript, Flash, Java, and Silverlight. Until 2011, coverage quickly grew to over 65% of all sites using it, whereas by 2016, almost 75% of the major sites were using jQuery.JavaScript as the Powerhouse of the Web 2.0 As we observed in the previous section, at least starting in 2003, JavaScript was omnipresent on the Web. As the figure shows, this number increased steadily over the years, while at the same time, the average number of scripts included in each frame remained stable at about four scripts per frame.Moreover, we analyzed the Cyclomatic Complexity of all scripts per year. Also, the graph depicts the trend of an ever-increasing number of paths, underlining the increased complexity of modern applications.These figures clearly show that modern JavaScript applications are more powerful than ever, but also incur a higher complexity due to the large code base to maintain. The trend since then is clearly pointing upwards, reaching almost 12 distinct remote origins per domain by 2016.Cross-Domain Data Access Modern Web sites are often interconnected, bringing the need for cross-domain communication and data access. However, such communication is prohibited by the Same-Origin Policy (SOP), which states that resources may only access each other if they share an origin, i.e., protocol, host, and port match [41]. In our study, we found that CORS deployment has overtaken the use of JSONP in 2014 and has increased drastically resulting in 20% of the investigated sites to deploy such a header. The API has gained a lot of popularity since its inception and we find that over 65% of the sites in 2016 either received postMessages or sent them.Summary To sum up, we observe that over time, JavaScript has remained the most important scripting language on the Web. In turn, JavaScript applications have become much more complex, showing a steady increase in the amount of code executed by the client, including code from an increasing amount of different sources, and exchanging data across the trust boundaries of the domain. To that end, we report on the ClientSide XSS vulnerabilities we found, analyze the insecure usage of postMessages over time, outline the (in)security of cross-domain communication in Flash, and show the general pattern of including outdated third-party library versions. In contrast to Cross-Site Scripting caused by serverside code, Client-Side XSS can be discovered in the HTML and JavaScript code that was delivered to the client and in this case to the Archive crawler. Therefore, this data source allows us to investigate when the first instances of this attack occurred and how many sites were affected over the course of the last 20 years. In 2013, when we published our work on discovering Client-Side XSS, we found approximately 10% of the Top 10.000 sites to be vulnerable, which aligns with our findings. We therefore analyzed our data set in two dimensions: handling of received postMessages without performing origin checks and calls to the postMessage API with a wildcard origin.Given the large amount of data we collected, i.e., 8,992 distinct scripts, we opted to analyze the postMessage receivers in a light-weight fashion. Their analysis efforts, however, were mostly manual; hence, while an in-depth analysis of the discovered receivers is not feasible for our work, we leave a more automated approach to such analyses to future work.Apart from the authenticity issue of postMessages, not specifying a target origin might endanger the confidentiality of an application's data. Moreover, the high number of postMessage senders in 2009 is related to Google page ads, which featured postMessages in 2009, but removed its usage in 2010. If it exists, the policy file can specify which origins may access the site's data, and can contain wildcards, e.g., to allow for all subdomains of a given domain In part, these policy files are also stored by the Web Archive. Therefore, all results we present in the following must be considered lower bounds. Hence, we also analyzed which of the domains with wildcard policies had artefacts of a login, e.g., login pages or session cookies. Therefore, whenever any third-party component is vulnerable, this implies that all sites which include the flawed code will suffer from the vulnerability.To understand the risk associated with this, we used retire.js [25], a tool to detect libraries and report known vulnerabilities in them, on the versions of jQuery we collected in our study. In 2011, its usage reached its peak with about 10% prevalence, dropping off until 2016 to 3.5% of the domains that included the library. In this section, we highlight a number of features we can measure, that indicate whether a site operator is aware of Web security mechanisms.Most of the security awareness indicators can be found in the HTTP headers of the responses. At the same time, by default, cookies are accessible from JavaScript as well, making these session identifiers prime targets for Cross-Site Scripting attacks. Naturally, the Archive crawler does not log into any Web application. This indicates that the admins of the most relevant sites on the Web are well-aware of the dangers of cookie theft and try to mitigate the damage of an XSS attack. One of the mechanisms used to achieve this tolerance is content sniffing, a technique used by browsers to guess the type of content being shown, to allow for proper rendering. In addition, improper content sniffing could also lead to the site being used to host malware.To prevent such attacks, Internet Explorer first implemented the X-Content-Type-Options header in 2008 [15]. Google Chrome showed a similar behavior, which can also be controlled using the X-Content-Type-Options header. Again, a notable increase can be observed over time, resulting in almost 47% of the analyzed sites using the protective measure by 2016. While the unsolicited framing itself was already discussed before the devastating demonstration by Grossman, the clear attack potential as shown by their attack in 2008 motivated browser vendors to develop and deploy a protective measure, dubbed the X-Frame-Options header (for short also XFO). Even though the X in the name denotes the fact that this was not a standardized header, it was introduced within a few months after the presented attack by Internet Explorer and Firefox, while Chrome followed a year later (see Table 2). Note, however, that use of the header has been deprecated by Content Security Policy (CSP) Level 2 [39] starting from around 2015, being replaced by the more powerful frame-ancestors directive of CSP. To that end, a Web application that deploys CSP sends a header containing a number of whitelisted code origins, e.g., self or cdn.domain.com. Also, by default, CSP disallows the use of inline script elements and the eval construct.CSP has many more directives, allowing Web developers to control which hosts may be contacted to retrieve images or stylesheets, specifying how the site may be framed (deprecating the X-Frame-Options header), or to report violations of the policy. Given the results from previous work, investigating the security of the policies of single sites is out of scope for our work.Initially, CSP was introduced by Firefox and WebKit-based browsers (including Chrome) with different names, i.e., X-Content-Security-Policy and X-WebKit-CSP, respectively. Hence, although CSP mitigates the effect of XSS vulnerabilities in JavaScript-enabled Web applications, its adoption still lags far behind other security measures. In that case, the cookies are transferred in any connection to the domain for which they were set, regardless of the use of HTTPS.To ensure that neither an active attacker can strip SSL nor an unknowing developer can accidentally build an insecure application, browsers implement HTTP Strict Transport Security, or HSTS for short [8]. Support for HSTS was first introduced in Chrome and Firefox in 2010. Starting from 2013, we observe a steady increase, resulting in almost 30% adoption rate by 2016. For 2009, all these sites were related to Google (e.g., including Youtube), showing that the issues in IE were known to Google before the publication in 2010. The Web's Complexity is still on the Rise In our study of the Web's evolution, we found that although several technologies for client-side interaction were developed over the years, the only prevailing one is JavaScript. Along with the introduction of powerful new APIs in the browsers, which nowadays, e.g., allow for client-toclient communication that was never envisioned by the Web's server/client paradigm, we find that the general complexity of client-side Web applications is on the rise. Moreover, the fraction of domains that use such vulnerable third-party libraries remained high since 2012. The Rise of the Multi-Origin Web The Web's primary security concept is the Same-Origin Policy, which draws a trust boundary around an application by only allowing resources of the same origin to interact with one another. Client-Side XSS Remains a Constant Issue One of the biggest problems on the Web is Cross-Site Scripting. Even though the general complexity of JS applications kept rising after 2012, the number of vulnerable domains declined, ranging around 8% until 2016. Nevertheless, given our sample of the top 500 pages, such attacks still threaten a large fraction of the Web users and developer training should focus more on these issues.Security vs. Utility Many new technologies introduced in browsers come with security mechanisms, such as the authenticity and integrity properties provided by the postMessage API. As an example, within two years of being fully supported by the three major browsers, the X-Frame-Options header was deployed by 20% of the sites we analyzed, within four years its adoption rate even reached more than 40%. In contrast, CSP needs to be adopted site-wide to mitigate a Cross-Site Scripting. In addition, the graph shows the baseline as all sites that do not have any indicator.HTTP-only Cookies When considering the httponly cookie flag, the results are surprising: In our dataset, its presence actually correlates with a higher vulnerability ratio compared to cases in which no indicators are found. Comparing server-and clientside vulnerabilities with respect to the use of httponly cookies, however, is an interesting alley for future work.The correlation between httponly cookies and increased fraction of vulnerabilities might be caused by several reasons: applications that use session cookies are more likely to have a larger code base and thus, more vulnerabilities. Early Adopters For X-Frame-Options and HSTS (considered from 2010 and 2013 respectively) we see another trend: early adopters of new security mechanisms are less likely to be susceptible to Client-Side XSS attacks, even though the code bases for these sites are also about 10% larger than an average site. The reason for lack of Client-Side XSS on these sites is likely twofold: either companies invested enough in their security to go through the tedious process of setting up CSP in general have better security practices, or this again shows the early adoption effect we observed for XFO and HSTS. In contrast, headers like HSTS or XSO, which are easy to deploy and address a single issue, are adopted more swiftly in a shorter timeframe. Thus, we argue that for future techniques ease of use should be a primary design concern.Make Security Mandatory Our findings highlight that if security checks are optional, they are oftentimes not used, as evidenced, e.g., by the lack of origin checking on postMessages. Examples for this include (missing) origin checking on postMessages, the ineffective use of HTTP-only cookies, or the inclusion of user-controllable data in the generation of script code, which causes Client-Side XSS. In the area of privacy, Lerner et al. [19] conducted an analysis of how trackers evolved over time, also using data from archive.org.Vulnerability Detection in the Wild In addition to the previously discussed papers, several works have focussed on examining a certain type of vulnerability in the wild. In the course of our study, we were able to observe three overarching developments: For one, the platform complexity of the client-side Web has not plateaued yet: Regardless of the numerical indicator we examine, be it code size, number of available APIs, or amount of third-party code in web sites, all indicators still trend upwards. Vulnerabilities that are on the decrease, due to deprecated technology, as it is the case with insecure crossdomain.xml policies, appear to be seamlessly replaced with insecure usage of corresponding new technologies, e.g., insecure handling of postMessages.Finally, we could observe a steady adoption of easy to deploy security mechanisms, such as the HTTPOnly-flag or the X-Frames-Option header. Even though Web security has received constant attention from research, security, and standardization communities over the course of the last decade, and numerous dedicated security mechanisms have been introduced, the overall positive effects are modest: Client-Side XSS stagnates at a high level and potentially problematic practices, such as cross-origin script inclusion or usage of outdated JavaScript libraries are still omnipresent. At best, it appears that the growing security awareness merely provides a balance to a further increase in insecurity, caused by the ever-rising platform complexity.Thus, this paper provides strong evidence, that the process of making the Web a secure platform is still in its infancy and requires further dedicated attention to be realized.