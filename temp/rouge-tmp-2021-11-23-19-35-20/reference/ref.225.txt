To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. Ideally, systems for text analytics, such as topic detection and tracking (TDT) (Allan, 2002), text summarization ( Huang et al., 2003), information retrieval (IR) ( Dias et al., 2007), or question answering (QA) , could access a document representation that is aware of both topical (i.e., latent semantic content) and structural information (i.e., segmentation) in the text ( MacAvaney et al., 2018). It is therefore important to understand topic segmentation and classification as a mutual task that requires encoding both topic information and document structure coherently.In this paper, we present SECTOR, 1 an end-to-end model that learns an embedding of latent topics from potentially ambiguous headings and can be applied to entire documents to predict local topics on sentence level. We can utilize a generic segmentation data set derived from Wikipedia that includes headings ( Koshorek et al., 2018), but there is also a need in IR and QA for supervised structural topic labels (Agarwal and Yu, 2009;MacAvaney et al., 2018), different languages and more specific domains, such as clinical or biomedical research (Tepper et al., 2012;Tsatsaronis et al., 2012), and news-based TDT ( Kumaran and Allan, 2004;Leetaru and Schrodt, 2013). In contrast, cities resembles a diversified domain, with high entropy (i.e., broader topics, common language, and higher word ambiguity) and will be more applicable to for example, news, risk reports, or travel reviews.We compare SECTOR to existing segmentation and classification methods based on latent Dirichlet allocation (LDA), paragraph embeddings, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Our method unifies those strongly interwoven tasks and is the first to evaluate the combined topic segmentation and classification task using a corresponding data set with long structured documents.Topic modeling is commonly applied to entire documents using probabilistic models, such as LDA ( Blei et al., 2003). Gabrilovich and Markovitch (2007) proposed the Explicit Semantic Analysis method in which concepts from Wikipedia articles are indexed and assigned to documents. Early unsupervised methods utilized lexical overlap statistics (Hearst 1997;Choi 2000), dynamic programming ( Utiyama and Isahara, 2001), Bayesian models ( Eisenstein and Barzilay, 2008), or pointwise boundary sampling ( Du et al., 2013) on raw terms.Later, supervised methods included topic models ( Riedl and Biemann, 2012) by calculating a coherence score using dense topic vectors obtained by LDA. Glavaš et al. (2016) utilized semantic relatedness of word embeddings by identifying cliques in a graph.More recently, Sehikh et al. (2017) utilized LSTM networks and showed that cohesion between bidirectional layers can be leveraged to predict topic changes. The authors introduced a neural architecture for segmentation that is based on sentence embeddings and four layers of bidirectional LSTM. Our work takes up this idea of end-to-end training and enriches the neural model with a layer of latent topic embeddings that can be utilized for topic classification.Text classification is mostly applied at the paragraph or sentence level using machine learning methods such as support vector machines (Joachims, 1998) or, more recently, shallow and deep neural networks ( Le et al., 2018;Conneau et al., 2017). Furthermore, Kim (2014) has shown that CNNs combined with pre-trained task-specific word embeddings achieve the highest scores for various text classification tasks.Combined approaches of topic segmentation and classification are rare to find. Although these data sets have been used to evaluate word-level and sentence-level segmentation ( Koshorek et al., 2018), we are not aware of any topic classification approach on this data set.Tepper et al. (2012) approached segmentation and classification in a clinical domain as supervised sequence labeling problem. The objective of TREC Complex Answer Retrieval is to retrieve a ranking of relevant passages for a given outline of hierarchical sections ( Nanni et al., 2017). For each sentence s k , we assume a distribution of local topics e k that gradually changes over the course of the document.The task is to split D into a sequence of distinct topic sections T = [T 1 , . . . , T M ], so that each predicted section T j = S j , y j contains a sequence of coherent sentences S j ⊆ S and a topic label y j that describes the common topic in these sentences. For the evaluation of this task, we created WIKI-SECTION, a novel data set containing a gold standard of 38k full-text documents from English and German Wikipedia comprehensively annotated with sections and topic labels (see Table 1). We retrieved instances of Wikidata categories disease (Q12136) and their subcategories (e.g., Trichomoniasis or Pertussis) or city (Q515) (e.g., London or Madrid). We create edges between all synsets that match among each other with a lemma h ∈ H. Finally, we apply a community detection algorithm (Newman, 2006) on G to find dense clusters of synsets. We use these clusters as normalized topics and assign the sense with most outgoing edges as representative label, in our example e.g. therapy.From this normalization step we obtain 598 synsets that we prune using the head/tail division rule count(s) < 1 , 2012). We verify our normalization process by manual inspection of 400 randomly chosen headinglabel assignments by two independent judges and report an accuracy of 97.2% with an average observed inter-annotator agreement of 96.0%. Thus, we aim to predict coherent sections with respect to document context:p(¯ y 1 , ... , ¯ y N | D) = N k=1 p(¯ y k | s 1 , ... , s N )(1)We approach two variations of this task: For WIKISECTION-topics, we choose a single topic label y j ∈ Y out of a small number of normalized topic labels. For both tasks, we aim to maximize the log likelihood of model parameters Θ on section and sentence level:L(Θ) = M j=1 log p(y j | s 1 , ... , s N ; Θ) ¯ L(Θ) = N k=1 log p(¯ y k | s 1 , ... , s N ; Θ)(2)Our SECTOR architecture consists of four stages, shown in Figure 2: sentence encoding, topic embedding, topic classification and topic segmentation. The first stage of our SECTOR model transforms each sentence s k from plain text into a fixed-size sentence vector x k that serves as input into the neural network layers. We use the strategy of Arora et al. (2017) to generate a distributional sentence representation based on pre-trained word2vec embeddings ( Mikolov et al., 2013 v s = 1 |S| w∈s α α + p(w) v w x emb (s) = v s − uu T v s(5) We model the second stage in our architecture to produce a dense distributional representation of latent topics for each sentence in the document. We modify our objective given in Equation (2) accordingly with long-range depen- dencies from forward and backward layers of the LSTM:L(Θ) = N k=1 log p(¯ y k | x 1...k−1 ; Θ, Θ ) + log p(¯ y k | x k+1...N ; Θ, Θ )(6)Note that we separate network parameters Θ and Θ for forward and backward directions of the LSTM, and tie the remaining parameters Θ for the embedding and output layers. If there is no newline information available in the text, we use a maximum label (max) approach: We first split sections at every sentence break (i.e., S j = s k ; j = k = 1, . . . , N ) and then merge all sections that share at least one label in the top-2 predictions.Using Deviation of Topic Embeddings for Segmentation. Here, we obtain two smoothed embeddings e and e and define the bidirectional embedding deviation (bemd) as geometric mean of the forward and backward difference:d k = cos( e k−1 , e k ) · cos( e k , e k+1 ) (13)After segmentation, we assign each segment the mean class distribution of all contained sentences:ˆ y j = 1 | S j | s i ∈S j ˆ ¯ y i(14)Finally, we show in the evaluation that our SECTOR model, which was optimized for sentences ¯ y k , can be applied to the WIKISECTION task to predict coherently labeled sections T j = S j , ˆ y j . However, it is too large to compare exhaustively, so we use the smaller Wiki-50 subset.We further use the Cities and Elements data sets introduced by Chen et al. (2009), which also provide headings. Because we are not aware of any existing method for combined segmentation and classification, we first compare all methods using given prior segmentation from newlines in the text (NL) and then additionally apply our own segmentation strategies for plain text input: maximum label (max), embedding deviation (emd) and bidirectional embedding deviation (bemd). Other options are: bag-of-words sentence encoding (+bow), Bloom filter encoding (+bloom) and sentence embeddings (+emb); multi-class cross-entropy loss (as default) and ranking loss (+rank). We measure text segmentation at sentence level using the probabilistic P k error score (Beeferman et al., 1999), which calculates the probability of a false boundary in a window of size k, lower numbers mean better segmentation. Furthermore, in the setting with plain text input, SECTOR improves the CNN score by 18.8 points using identical baseline segmentation. Classification and segmentation on plain text C99 37.4 n/a n/a 42.7 n/a n/a 36.8 n/a n/a 38.3 n/a n/a TopicTiling 43.4 n/a n/a 45.4 n/a n/a 30.5 n/a n/a 41.3 n/a n/a TextSeg 24.3 n/a n/a 35.7 n/a n/a 19.3 n/a n/a 27.5 n/a n/a PV>T* max 43. We clearly see from NL predictions (left side of Figure 5) that SECTOR produces coherent results with sentence granularity, with topics emerging and disappearing over the course of a document. Our SECTOR model is giving nearly perfect segmentation using the bidirectional strategy, it only misses the discussed part of cause and is off by one sentence for the start of prevention. Our work is funded by the German Federal Ministry of Economic Affairs and Energy (BMWi) under grant agreement 01MD16011E (Medical Allround-Care Service Solutions) and H2020 ICT-2016-1 grant agreement 732328 (FashionBrain).