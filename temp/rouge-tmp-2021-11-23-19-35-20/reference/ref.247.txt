For instance, a robustness property can enforce that no matter how many pages from benign documents are inserted into a PDF malware, the classifier must still classify it as malicious. Furthermore, we find that training classifiers that satisfy formally verified robustness properties can increase the evasion cost of unbounded (i.e., not bounded by the robustness properties) attackers by eliminating simple evasion attacks. Moreover , the state-of-the-art and new adaptive evolutionary attackers need up to 10 times larger L 0 feature distance and 21 times more PDF basic mutations (e.g., inserting and deleting objects) to evade our robust model than the baselines. Nonetheless, all the state-of-the-art classifiers, including the proprietary ones used by popular services like Gmail, can be evaded by trivial transformations over the PDFs, such as adding invisible document metadata, deleting the length indicator of the exploit payload, or simply increasing the length of the document [5,36,61]. Since any security-relevant application of machine learning classifiers must deal with adaptive adversaries, it is fundamentally insufficient to evaluate security classifiers by measuring the accuracy and false positive rate. For example, even for a perfect knowledge attacker, any creative way of inserting pages from the most-benign documents to the malware can be verified to keep the malicious classification.If we train classifiers to be verifiably robust against building block attacks, we can raise the bar for more sophisticated attacks to succeed. Therefore, manipulations from different attacks can be decomposed to many building block operations in the parsed PDF tree. By training building block robustness properties, we eliminate simple and easy evasions, which increases the search cost for attackers.In this paper, we take the first steps towards training a PDF malware classifier with verifiable robustness properties, and we demonstrate that such classifiers also increase the attack cost even for the attackers not bounded by these properties. Since verifiably robust training is strictly a harder problem to solve than adversarially robust training without any verifiable bound, it is challenging to specify robustness properties that do not increase false positive rates yet still increase the cost for the attackers. Using a small distance for the robustness properties maintains low false positive rate. We use these attacks to quantify the increase in the unbounded attacker cost caused by the verifiable robust training.Using our new distance metric for the PDF tree structure, we specify two classes of robustness properties, subtree deletion properties and subtree insertion properties. For example, when choosing to delete /Root/Metadata subtree containing children /Root/Metadata/Length and /Root/Metadata/Type, the attacker can delete either one of the children, both children, or the whole subtree. This overapproximates attacker's actions, and includes even unknown attacks. Although adversarially robust training is known to achieve strong robustness against a specific type of attacker, the gradient attacker [39], our verifiably trained models can obtain superior verifiable robustness against all possible bounded attackers while keeping high test accuracies and low false positive rates.Perhaps even more importantly, we show that a verifiably robust classifier with two proposed robustness properties can already increase the cost for the unbounded attacker. In the whitebox setting, our robust model maintains 7% higher estimated robust accuracy (defined in Section 2.3.4) against the unrestricted gradient attack and the Mixed Integer Linear Program (MILP) attack than the baseline models. We thoroughly evaluate the robustness against twelve baseline models, using state-of-the-art measures including estimated robust accuracy (ERA) under gradient attacks and verified robust accuracy (VRA) against any bounded adaptive attacker. We can achieve 92.27% average VRA over three robustness properties while maintaining 99.74% test accuracy and 0.56% false positive rate. Then, we introduce the features used by PDF malware classifiers and two main classes of attacks that evade them. The trailer identifies the entrance to parse the file, along with the cross-reference table size. The root object 1 0 obj has a special type, /Catalog, and the value of the key /OpenAction is another dictionary object. Within /OpenAction, the object containing the JavaScript exploit is referred to as 2 0 R. For example, many PDF readers do not need the correct length field to decode the stream, and malware authors can delete the field to evade the classifier. PDF malware exploits the vulnerabilities in the PDF reader in order to transfer execution control, e.g., to run shellcode or drop additional binary. For example, increasing the length of the file to be 7,050,000 bytes can evade the Gmail PDF malware scanner [5]. For instance, inserting pages from a benign document to the PDF malware can increase the page count feature alone to be as big as the maximal integer value, which also affects many other counts. Both models have 99.8% accuracy and less than 0.06% false positive rateThe binary Bag-of-Path features are able to bound the input to the classifier, given certain attack properties. The threat models of black-box attacks generally assume that the attacker does not have access to any model parameters, but has oracle access to the prediction labels for some samples, and in some cases also the prediction confidence. Xu et al. [61] use a genetic evolution algorithm to automatically evade both PDFrate and Hidost. Dang et al. [18] uses a more restricted threat model where the attacker does not have access to classification scores, and only has access to the classified label and a blackbox morpher that manipulates the PDFs. They use the scoring function based on Hill-Climbing to attack the classifier under such assumptions. Out of the arms race between adversarial image examples [49] and many defenses [9,12,13,42,43], two training methods have proven to be the strongest among all. The summation is an empirical measure of the expected loss over the entire training dataset.q = argmin q Â L(y, ˆ y)(1)In the adversarial setting, for the input x, there can be a set of all possible manipulations˜xmanipulations˜ manipulations˜x bounded by a distance metric D k within distance k, i.e. ˜ x 2 D k (x). Robust optimization minimizes the worst case loss for all inputs in D k (x), solving a minimax problem with two components.q = argmin q Â max˜x2D max˜ max˜x2D k (x) L(y, f q ( ˜ x))(2)• Inner Maximization Problem: find˜xfind˜ find˜x that maximizes the loss value within the robustness region D k (x), i.e., the robust loss. Grosse et al. [28] applied the training method to the android malware classifier using adversarial malware examples, increasing the false positive rate to 67%. Verifiably robust training uses sound over-approximation techniques to obtain the upper bound of the inner maximization problem. Different methods have been used to formally verify the robustness of neural networks over input regions [20,23,24,31,34,37,44], such as abstract transformations [25], symbolic interval analysis [55,56], convex polytope approximation [58], semidefinite programming [45], mixed integer programming [50], Lagrangian relaxation [22] and relaxation with Lipschitz constant [57], which essentially solve the inner maximization problem. In this paper, we will use the following two metrics to evaluate our verifiably robust PDF malware classifier.Estimated Since it is extremely hard, if not impossible, to have a malware classifier that is robust against all possible attackers, we aim to train classifiers to be robust against building block attacks. Specifically for PDF malware, the operations include PDF object mutation operators [61], random morpher [18] and feature insertion-only generator [30]. After performing the building block operations, the attacks optimize the search based on the classifier's feedback that indicates the evasion progress. A PDF variant needs to have both correct syntax and correct semantics to stay malicious. A syntactically correct PDF file can be parsed into a tree structure ( Figure 1b, Section 2.1). We propose two classes of subtree insertion and subtree deletion properties, which can be used as the stepping stones to construct more sophisticated attacks.False positive rate. It is crucial to maintain low false positive rate for security classifiers due to the Base-Rate Fallacy [10]. We propose a new distance metric to bound the attacker's building block operations over a PDF malware. The distance d(x,x 0 ) is the cardinality of the resulting set.If the attacker inserts benign pages into the PDF malware under the /Root/Pages subtree (Figure 1b), this operation will not exceed subtree distance one, no matter how long the malicious PDF document becomes. Subtree Insertion Property (Subtree Distance 1): given a PDF malware, all possible manipulations to the PDF bounded by inserting an arbitrary subtree under the root, do not result in a benign prediction by the classifier.The attacker first chooses any one of the subtrees, and then chooses an arbitrary shape of the subtree for the insertion. For example, if we insert /Root/Names/JavaScript/Names/JS but not /Root/Names/JavaScript/Names/S, the javascript is no longer functional. Moreover, we over-approximate the attacker's possible actions. Therefore, the property can overestimate the worst case behavior of the classifier.Subtree Deletion Property (Subtree Distance 1): given a PDF malware, all possible manipulations to the PDF bounded by deleting an arbitrary subtree under the root, do not result in a benign prediction by the classifier.For the PDF malware example shown in Figure 1b, this property allows deleting any one of the following: /Root/Type, /Root/Pages, and /Root/OpenAction. Next, we describe properties with larger distances.Subtree Deletion Property (Subtree Distance 2): the strongest possible attackers bounded by deletions within any two subtrees under the root, cannot make the PDF classified as benign.Subtree Insertion Property (Subtree Distance N 񮽙 1): the strongest possible attackers bounded by insertions within all but one subtree under the root, cannot make the PDF classified as benign.Monotonic Property and Subtree Insertion Property (Distance N): Incer et al. [32] have proposed to enforce the monotonic property for malware classifiers. Specifically, if two feature vectors satisfy x  x 0 , then the classifier f guarantees that f (x)  f (x 0 ). Formally, given input x 2 X and a property D k (x) bounded by distance k, the transformation T f is sound if the following condition is true: 8x 2 X, we have{ f q ( ˜ x)| ˜ x 2 D k (x)} ✓ T f (D k (x))That is, the sound analysis over-approximates all the possible neural network outputs for the property. We train seven verifiably robust models and compare them against twelve baseline models, including neural network with regular training, adversarially robust training, ensemble classifiers, and monotonic classifiers 1 . • How much do we raise the bar (e.g., L 0 distance in features and mutation trace length) for the unrestricted attackers to evade our robust models?We use seven different attackers to evaluate the models. Tong et al. [51] have shown that robustness against feature-space attacks on non-realizable inputs can be generalized to robustness against realizable attacks.Machine. The machine is configured with Intel Core i7-9700K 3.6 GHz 8-Core Processor, 64 GB physical memory, 1TB SSD, Nvidia GTX 1080 Ti GPU, and it runs a 64-bit Ubuntu 18.04 system. 1 Our code is available at https://github.com/surrealyz/pdfclassifier 4.1 Models We obtain the PDF malware dataset from Contagio [3]. For brevity, we will refer to the four robustness properties as property A (subtree deletion distance one), B (subtree insertion distance one), C (subtree deletion distance two), D (subtree insertion distance 41) and E (subtree insertion distance 42). Symbolic interval analysis uses intervals to bound the adversarial input range to the model, then propagates the range over the neural network while keeping input dependency. We train all neural network models for 20 epochs, using the Adam Optimizer, with mini-batch size 50, and learning rate 0.01. They represent, two strongest bounded adaptive attackers (Type I), four state-of-the-art unbounded attackers (Type II), and the new adaptive unbounded attacker (Type III). We train the baseline neural network model with the regular training objective (Equation 1, Section 2.3.1), using the regular training dataset (Table 2). The Adv Retrain A, B models maintain the same 0.07% FPR as the baseline neural network. The provable robustness property is, if a PDF variant is generated by subtree insertion bounded by distance one to a PDF, the variant has the same prediction as the original PDF. Ensemble A+B achieves 99.87% accuracy and 0.26% FPR.Ensemble D. To build the ensemble, we test every single subtree of the unseen PDFs, and predict the malicious class if any subtree is classified as malicious. Therefore, we train multiple monotonic classifiers with different number of learners (10, 100, 1K, and 2K), where each learner is a decision tree of depth 2. For example, to train the Robust A+B model, we do mixed training for regular accuracy, the insertion property, and the deletion property alternately by mini-batches, in order to obtain two properties as well as high accuracy in the same model. All the robust models, except the Robust D model, can maintain over 99% test accuracy while obtaining verifiable robustness. For models C and D, the false positive rates increased to 1.04% and 2.3% respectively. We formally verify the robustness of the models using symbolic interval analysis to obtain VRA, over all the 3,461 testing malicious PDFs (Table 3). For example, 99% VRA for property B means that, 99% of 3,416 test PDFs will always be classified as malicious, for arbitrary insertion attacks restricted by one of the subtrees under the PDF root. For property A, we verify the classifier's behavior on a malicious test PDF, if every possible mutated PDF with an arbitrary full subtree deletion is always classified correctly. This gives us between 5.74% and 8.78% VRAs for the monotonic classifiers under property A. Similarly, by testing the lower bound of two subtree deletion, we verify the monotonic classifiers to have 0 VRA for property C.Verifiably Robust Models: We can increase the VRA from as low as 0% to as high as 99%, maintaining high accuracy, with under 0.6% increase in FPR in properties A and B.Training a model with one robustness property can make it obtain the same type of property under a different distance. Robust A+B has slightly lower VRA in property B than the monotonic and ensemble baselines, but it has 85.28% VRA for property C. Robust A+B+E has gained VRA in all prop- Table 4: The verified robust accuracy (VRA) computed from 3,461 test PDF malware. We implement the unbounded gradient attacker that performs arbitrary insertion and deletion guided by gradients, unrestricted by all robustness Table 4 are the lower bound of ERA values.properties. Since VRA overapproximates possible attacks, VRA is the lower bound for the ERA against any specific attack. Since there always exist stronger attacks that can reduce the ERA [54], VRA provides a stronger robustness guarantee than ERA. Given each feature index change, we either delete the corresponding PDF object, or insert the object with minimal number of children in the benign training dataset. On average, the ERA of models against the real evasive PDF malware is 94.25%, much higher than 0.62% ERA against evasive feature vectors, since unrestricted gradient attack often breaks the PDF semantics (Appendix A.4). The Mixed Integer Linear Program (MILP) attacker is the unbounded whitebox attacker for the GBDT monotonic classifiers, proposed by Kantchelian et al. [33]. We plot the ERA values with different L 0 distance (number of feature changes) in Figure 3. With only 2 feature deletion, 10% of PDFs can be evaded, e.g., deleting /Root/Names/JavaScript/Names and /Root/Names/JavaScript/Names/JS/Filter. Everything can be evaded by up to six feature changes for the 10 learner model. The attack is based on the genetic evolution algorithm [61]. The genetic evolution attack evades the model prediction function by mutating the PDF malware, using random deletion, insertion, and replacement, guided by a fitness function. We design adaptive evolutionary attacks to evade these models in Section 4.7. The reverse mimicry attacker injects malicious payload into a benign PDF, which is outside of all five robustness properties. We inject different malicious payload into a benign file, whereas the JSinject attack injects the same JavaScript code into different benign PDFs. We measure ERA as the percentage of correctly classified PDFs for the strongest models against whitebox attacks in Table 6. However, we still achieve 2% higher ERA than the Monotonic 100 model against the reverse mimicry attack. Therefore, we implement a new mutation to move the exploit around to different trigger points in the PDF (Appendix A.7). This attack combines the move exploit mutation with deletion to evade the monotonic classifier. The scatter attack can reduce the mutation trace length to evade Robust A+B compared to the nonadaptive version. Since we specify robustness properties related to the PDF syntax, they can be generalized to different features, datasets, and models. Verifiably robust training using symbolic interval analysis is faster than existing sound approximation methods, achieving state-of-the-art tight bounds. Our method can increase the feature distance and mutation trace length as cost for the attacker to evade the model. Dreossi et al. [19] argued that only some adversarial examples cause the overall control system to make catastrophic decision. Our best model achieved 99.68% and 85.28% verified robust accuracy (VRA) for the insertion and deletion properties, while maintaining 99.74% accuracy and 0.56% false positive rate. Therefore, for each interval representing two subtree deletion, we require that any of the corresponding three subtree deletion is classified as malicious.Property D. Figure 7: Unrestricted gradient attack against our verifiably robust models. Then, we measure the ERA for features extracted from the real PDFs as the 3rd column in Table 9. Then we measure the ERA against the end-to-end attack that generates malicious PDFs, as the last column in Table 9. The original operations generate a lot of different PDF malware, but not as many different feature inputs to the neural network, because they don't affect valid Hidost paths in the feature space. Following Xu et al. [61], we use the same parameters for the attack: 48 variants as the size of the population for each generation, a maximum of 20 generations per PDF for each round, 0.0 fitness stop threshold, and 0.1 mutation rate. A single move operation can decrease the ERA of the model to 29.70%, e.g., moving the exploit from /Root/OpenAction/JS to /Root/Pages/Kids/AA . • /Root/Pages/Kids/AA• /Root/Names/JavaScript/Names• /Root/OpenAction/JS• /Root/StructTreeRoot/JS The move mutation operation identifies whether the PDF variant has object under one of the paths, then randomly selects one of the target paths to move the object to.