Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. In a prototypical case of an evasion attack, an adversary modifies malware code so that the resulting malware is categorized as benign by ML, but still successfully executes the malicious payload [12,16,26,37,44]. Limiting feature modifications to be small in some l p norm clearly need not capture this: one can insert many no-ops (resulting in a large change according to an l p norm) with no impact on malicious functionality, and conversely, minimal changes (such as removing a Javascript tag) may break malicious functionality. Specifically, we consider four ML-based approaches for PDF malware detection: two based on features that capture PDF file structure (SL2013 [36] and Hidost [38]), and two based on PDF file content (two Mimicus variants of PDFRate [35,37]). Finally, we show that by simply constraining that these features remain unmodified in adversarial training, feature-space approaches become effective even for robust structure-based PDF malware detection.Our third contribution is to explore the extent to which ML robustness is generalizable to multiple distinct realizable attacks. Specifically, we expose both a robust classifier that was retrained by using a realizable attack (EvadeML), and a model hardened using a feature-space attack (accounting for conserved features), to a series of realizable attacks. Our results reveal a stark difference between the two: ML models hardened using EvadeML are quite fragile; in contrast, ML models hardened using feature-space attacks exhibit uniformly high robustness to the other attacks. In an evasion attack, abstractly, one is given a learned model h(x) (e.g., a SVM or neural network) which returns a label y = h(x) (e.g., malicious or benign) for an arbitrary feature vector x 2 X (e.g., extracted from a PDF file). The attacker then transforms e into another entity, e 0 , with an associated feature vector x 0 = f(e 0 ) so as to accomplish two goals: first, that h(x 0 ) returns an erroneous label (in our running example, labels e 0 as benign based on its extracted features f(e 0 )), and second, that e 0 preserves the functionality of the original entity e-which, in our example of PDF malware detection, entails preserving malicious functionality of e. Crucially, essentially all approaches for robust ML, particularly the most successful ones, such as those based on robust optimization, leverage these models. 1 Game-theoretic methods in general, and robust optimization in particular, are not general-purpose, as solving these directly requires special structure, such as a continuous feature space and differentiability [3,5,6], and often additional structure of the learning model, such as linearity [43] or neural network architecture and activation functions [32,42]. We have two major goals: 1) validation: to evaluate whether robust ML approaches that make use of feature-space models of evasion attacks are, indeed, robust against realrealizable-attacks, and 2) generalizability: to study generalizability of evasion defenses.We start with a conceptual model of defense and attack as a Stackelberg game between ML ("defender"), who first chooses a defense q (in our case, the learned model h(x)) and the attacker, who finds an optimal attack that reacts to the particular defense q. To be more precise, let O(h; D) be an arbitrary attack which returns evasions given a dataset D and a classifier h, and let u(h; O(h; D)) be the measure that the defender wishes to optimize (for example, accuracy on data after evasions). Now, we describe our approach to validation and generalizability evaluations.In validation, consider a model of an evasion attack, ˜ O(h; D) (e.g., a feature-space attack model), which is a proxy for a "real" (realizable) attack, O(h; D); note that each attack evades a given ML model h. We first find the defense against˜O against˜ against˜O using the retraining procedure above; let the resulting robust classifier be˜hbe˜ be˜h. Again, we consider a proxy attack˜Oattack˜ attack˜O (which may now be either a feature-space model, or some particular realizable attack), and find a defense˜hdefense˜ defense˜h against this attack. We say that our proxy attack is generalizable if˜hif˜ if˜h remains robust to all, or most of the attacks i; otherwise, it fails to generalize. We use malicious PDF detection as a case study to investigate robustness of ML hardened using feature-space models of evasion attacks. We start with some background on PDF structure, and proceed to describe the specific ML-based detectors, evasion attacks (both realizable, and feature-space), datasets, and evaluation metrics used in our experiments. SL2013 and its revised version, Hidost, are structure-based PDF classifiers, which use the logical structure of a PDF document to construct and extract features used in detecting malicious PDFs. PDFRate, on the other hand, is a content-based classifier, which constructs features based on medadata and content information in the PDF file to distinguish benign and malicious instances. SL2013 uses 6,087 most common structural paths among 658,763 PDF files as a uniform set for classification. The metadata features include the size of a file, author name, and creation date, while content-based features include position and counts of specific keywords. An important aspect of Mimicus is feature standardization on extracted data points performed by subtracting the mean of the feature value and dividing by standard deviation, transforming all features to be real-valued and zero-mean (henceforth, PDFRate-R). In addition, we construct a binarized variant of PDFRate (henceforth, PDFRate-B), where each feature is transformed into a binary feature by assigning 0 whenever the feature value is 0, and assigning 1 whenever the feature value is non-zero. Otherwise, a subset of the population is selected for the next generation based on their fitness evaluation. Afterward, the variants selected are randomly manipulated to generate the next generation of the population.We use EvadeML as the primary realizable evasion model for the first part of the paper. We use the same external benign PDF files as Xu et al. [44] to provide ingredients for insertion and swap operations. Mimicry assumes that an attacker has full knowledge of the features employed by a target classifier. To make Mimicry consistent with our framework, we employ the Cuckoo sandbox [17] in place of WEPAWET (which was in any case discontinued) to validate maliciousness of the resulting PDF file.In addition to the original version of Mimicry, we implement an enhanced variation, Mimicry+, with two modifications. MalGAN comprises three main components: a generator which transforms malware to its adversarial version, a black-box detector which returns detection results, and a substitute detector which is used to fit the black-box detector and train the generator. We note that strictly speaking, MalGAN variants are not implemented as actual PDF files; however, we still treat it as a realizable attack since it only adds features to a malicious file, which can be implemented (at least in structure-based detection) by adding the associated objects into the PDF file. Our Reverse Mimicry attack employs the adversarial examples provided by Maiorca et al. [26] which was shown to successfully evade PDF classifiers based on structural analysis. This naturally translates into the following multi-objective optimization in feature space:minimize x Q(x) = f (x) + lc(x M , x),(2)where f (x) is the score of a feature vector x, with the actual classifier (such as SVM) g(x) = sgn( f (x)), x M the malicious seed, x an evasion instance, c(x M , x) the cost of transforming x M into x, and l a parameter which determines the feature transformation cost. Since in most of our experiments features are binary, the choice of l 2 norm (as opposed to another l p norm) is not critical.As the optimization problem in Equation (2) is non-convex and variables are binary in three of the four cases we consider, we use a stochastic local search method designed for combinatorial search domains, Coordinate Greedy (alternatively known as iterative improvement), to compute a local optimum (the binary nature of the features is why we eschew gradientbased approaches) [18,23]. Specifically, we used only 40 malicious seeds to EvadeML to generate evasions, to reduce running time and make the experiment more consistent with realistic settings where a large proportion of malicious data is not adapting to the classifier. As shown below, this set of 40 instances was sufficient to generate a model robust to evasions from held out 100 malicious seed PDFs.We distribute both retraining and adversarial test tasks on two servers (Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz, 18 cores and 64 GB memory, running Ubuntu 16.04). Thus, evasion robustness of 0% means that the classifier is successfully evaded in every instance, while evasion robustness of 100% means that evasion fails every time. On this data, we compute the ROC (receiver operating characteristic) curve and the corresponding AUC (area under the curve). From the perspective of defense, we show that it is possible to harden both SL2013 and Hidost against a powerful realizable EvadeML attack by simply retraining with this attack (RAR, for realizable-attack retraining, henceforth refers to a model hardened using EvadeML). Retraining with a Powerful Realizable Attack First, we replicated the EvadeML attack on the original SL2013; the classifier achieves only a 16% evasion robustness. Figure 2 (left) shows the gradual improvement of evasion robustness over the 10 retraining iterations. This plot demonstrates non-trivial effectiveness of EvadeML: the first few iterations are clearly insufficient, as re-running EvadeML creates many new evasions that cannot be correctly detected by Feature-Space Retraining Next, we experimentally evaluate the effectiveness of retraining with a feature-space model of evasion attacks in obtaining robust ML in the face of the EvadeML realizable attack. This illustrates that defense that relies on feature-space models of adversarial examples may not in fact lead to robustness when it is faced with a real attack.We again consider performance of FSR classifier on nonadversarial test data (Figure 1 (right)). In contrast, FSR achieves a 70% evasion robustness, a significant boost over the original Hidost, to be sure, but far below the evasion robustness of RAR.Evaluating these classifiers on non-adversarial test data in terms of ROC curves (Figure 3 (right)), we can observe that RAR achieves comparable accuracy (> 99.9% AUC) with the original Hidost classifier on non-adversarial data, and provides even better True Positive Rate (TPR) when False Positive Rate (FPR) is close to zero. Our next case study concerns another two PDF malware classifiers which use features based on PDF file content, rather than logical structure. Next, we retrain PDFRate-R with EvadeML for 10 iterations (RAR baseline), and perform feature-space retraining using the conventional feature space model above. Observe that while RAR indeed achieves a highly robust classifier (96% robustness), FSR actually performs even better, with 100% robustness.Comparing RAR and FSR performance on non-adversarial data (Figure 4 (right)), we observe that the high robustness of FSR does incur a cost: while RAR remains exceptionally effective (>99.99% AUC), FSR achieves AUC slightly lower than 99%, although most significantly, the degradation is rather pronounced for low FPR regions (below 0.015). One of our great surprises is the robustness of the binarized PDFRate: despite the fact that the real-valued PDFRate is quite vulnerable, the same classifier using binary features was 100% robust to EvadeML (Figure 5 (left)). Specifically, we introduce the idea of conserved features, which we define to be features, the unilateral modification of which compromises malicious functionality. Third, we demonstrate that the limitations of feature-space robust ML can be substantially alleviated by incorporating conserved features as attack invariants in the feature-space evasion model.To develop intuition about the nature of conserved features, consider SL2013, which employs structural paths as features to discriminate between malicious and benign PDFs. As we can see in Figure 6 (left), this classifier is 100% robust to EvadeML attacks, appearing to resolve the first question. Rather, as we show presently, they provide a sufficient anchoring in the problem domain for feature-space attack models to succeed.To address question (2), consider Figure 6 (right): clearly, if we desire a low false positive rate, using only conserved features for classification yields subpar performance on nonadversarial data. As we can see in Figure 6 (left), this classifier exhibits poor robustness; thus, statistical methods are insufficient to identify good conserved features.To address the fourth question, we create a classifier using only one boolean feature which identifies the presence of JavaScript in a PDF file (henceforth, we refer to this feature as JS). As discussed above, the feature-space evasion model in Equation (2) may not sufficiently boost ML robustness. We formally capture this in the new optimization problem in Equation (3), where S is the set of conserved features:minimize x Q(x) = f (x) + lc(x M , x), subject to x i = x M,i , 8i 2 S.(3)Other than this modification, we use the same Coordinate Greedy algorithm with random restarts as before to compute adversarial examples. We also study the effectiveness of our automated procedure for identifying conserved features as compared to using a subset that only considers Javascript features (we can think of these as expert-identified conserved features, as this is what an expert would naturally consider). These results demonstrate that by leveraging the conserved features, the feature-space evasion models are now quite effective as a means to boost evasion robustness of SL2013.In Figure 7 (right) we evaluate the quality of these classifiers on non-adversarial test data in terms of ROC curves. In contrast, CFR-JS only boosts robustness to 53%, showing that our algorithmic approach can in some cases offer a considerable advantage to expert-chosen conserved features.Evaluating the performance of CFR and CFR-JS on nonadversarial test data in terms of ROC curves in Figure 8 (right), we find that the CFR classifier can achieve ⇠ 99.8% AUC. We observe that both the CFR and CFR-JS classifiers in the PDFRate-B family achieve 100% evasion robustness against EvadeML (Figure 9 (left)), just as the RAR and FSR counterparts had.However, a close look at Figure 9 (right) demonstrates that CFR and CFR-JS achieve far better performance on nonadversarial data, with >99.9% AUC, where improvements are particularly significant for small false positive rates compared to FSR (recall Figure 5 (right)). To answer these questions, we consider five additional realizable attacks: Mimicry [37], which was one of the first realizable attacks on PDF malware detectors, Mimicry+, an enhanced variant of Mimicry, MalGAN [19], which uses Generative Adversarial Networks (GANs) to create evasion attacks (but only targets binary classifiers), Reverse Mimicry [26], which inserts malicious payloads into target benign files, and a new custom attack aimed at defeating PDFRate-B conserved features. Second, Mimicry+ is indeed a much stronger attack than Mimicry: the original Mimicry fails to significantly degrade RAR performance, whereas Mimicry+ largely evades the RAR variant of PDFRate-R, and is somewhat more potent against PDFRate-B than Mimicry. Next, we consider the MalGAN attack on the three classifiers over binary feature space we have previously studied: SL2013, Hidost, and PDFRate-B, with RAR and FSR/CFR versions that have been shown robust to EvadeML.The results, shown in Figure 12, demonstrate that despite EvadeML being a powerful attack, the RAR approaches which use it for hardening (with resulting classifiers no longer very vulnerable to EvadeML) are highly vulnerable to MalGAN, with evasion robustness of 0% in most cases. Our final attack specifically targets a feature extraction bug in the Mimicus implementation of PDFRate in order to defeat the corresponding CF classifier.The results are shown in Figure 14. Significantly, the FSR classifiers for both PDFRate-R and PDFRate-B remain 100% robust, and the CFR variant of PDFRate-B has nearly perfect robustness (0.98) against this attack. Below we briefly describe some of the related literature on adversarial evasion or adversarial example attacks and defenses; we refer readers to Vorobeychik and Kantarcioglu [40] for a broader and more in-depth treatment of the subject of ML attacks and defenses. Xu et al. [44] propose EvadeML, a fully realizable attack on PDF malware classifiers which generates evasion instances by using genetic programming to modify PDF source directly, using a sandbox to ensure that malicious functionality is preserved. A series of approaches formulate robust classification as minimizing maximum loss (i.e., following a robust optimization paradigm), where maximization is attributed to the evading attacker aiming to maximize the learner's loss through small feature-space transformations [25,32,39,42,46]. We undertook an extensive exploration of the extent to which robust ML that uses the conventional feature-space models of evasion attacks remains robust to "real" attacks that can be implemented in actual malware and preserve malicious functionality (what we call realizable attacks). However, we also show that changing the nature of the feature space can make a difference: robust ML with feature-space models is quite robust in content-based detection (which uses content, rather than structural paths, as features). We implemented a particular class of feature-space attacks, using l 2 norm to measure the attacker's cost of feature manipulations, and stochastic local search to compute evasions. It may be that conserved features are ultimately only a part of the solution, and only help if they adequately capture the attack surface in the abstract feature space. Note that in the case of multiple corresponding and identical objects of a structural path, all of these objects are replaced simultaneously.After structural path deletion and replacement, for each malicious PDF file x i , we can get its conserved feature set S i , non-conserved feature set O i , and dependent feature set D j for any feature j 2 S i [ O i , which could be further leveraged to design evasion-robust classifiers. As our approach relies on the existence of malicious functionality and corresponding features, such a relation is not obvious for real-valued features; we therefore leave the question of how to define and identify conserved features in real space for future work. This approach can in fact be used for arbitrary PDF malware detectors over binary features (leveraging conserved structural paths identified using SL2013). Since merely applying statistical approaches on training data is insufficient to discriminate between these two classes of features, as demonstrated above, we need a qualitatively different approach which relies on the nature of evasions (as implemented in EvadeML) and the sandbox (which determines whether malicious functionality is preserved) to identify features that are conserved.We use a modified version of pdfrw [27] 5 to parse the objects of PDF file and repack them to produce a new PDF file. Each time an object is removed, we produce a resulting PDF file by repacking the remaining objects. Since merely applying statistical approaches on training data is insufficient to discriminate between these two classes of features, as demonstrated above, we need a qualitatively different approach which relies on the nature of evasions (as implemented in EvadeML) and the sandbox (which determines whether malicious functionality is preserved) to identify features that are conserved.We use a modified version of pdfrw [27] 5 to parse the objects of PDF file and repack them to produce a new PDF file. Each time an object is removed, we produce a resulting PDF file by repacking the remaining objects.