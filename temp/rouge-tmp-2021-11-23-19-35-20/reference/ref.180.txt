In this work, we investigate how to reverse engineer a neural network by using side-channel information such as timing and electromagnetic (EM) emanations. To this end, we consider multilayer perceptron and convolu-tional neural networks as the machine learning architectures of choice and assume a non-invasive and passive attacker capable of measuring those kinds of leakages. Our experiments show that a side-channel attacker is capable of obtaining the following information: the activation functions used in the architecture, the number of layers and neurons in the layers, the number of output classes, and weights in the neural network. Also, deep learning algorithms are gaining popularity in IoT edge devices such as sensors or actuators, as they are indispensable in many tasks, like image classification or speech recognition. As a consequence, there is an increasing interest in deploying neural networks on low-power processors found in always-on systems, e.g., ARM Cortex-M microcontrollers.In this work, we focus on two neural network algorithms: multilayer perceptron (MLP) and convolutional neural networks (CNNs). When considering distinct industries, we are witnessing an increase in intellectual property (IP) models strategies. One could ask the following question: Why would an attacker want to reverse engineer the neural network architecture instead of just training the same network on its own? Second, as the architectures have become more complex, there are more and more parameters to tune and it could be extremely difficult for the attacker to pinpoint the same values for the parameters as in the architecture of interest.After motivating our use case, the main question that remains is on the feasibility of reverse engineering such architectures. Here, we start by considering some of the basic building blocks of neural networks: the number of hidden layers, the basic multiplication operation, and the activation functions.For instance, the complex structure of the activation function often leads to conditional branching due to the necessary exponentiation and division operations. Also, we notice that by observing side-channel leakage, it is possible to deduce the number of nodes and the number of layers in the networks.In this work, we show it is possible to recover the layout of unknown networks by exploiting the side-channel information. By using the known divide-and-conquer approach for side-channel analysis, (i.e., the attacker's ability to work with a feasible number of hypotheses due to, e.g., the architectural specifics), the information at each layer could be recovered. Consequently, the recovered information can be used as input for recovering the subsequent layers.We note that there exists somewhat parallel research to ours also on reverse engineering by "simply" observing the outputs of the network and training a substitute model. However, this line of work is using machine learning to derive a new side-channel distinguisher, i.e., the selection function leading to the key recovery.On the other hand, using side-channel analysis to attack machine learning architectures has been much less investigated. Hua et al. were first to reverse engineer two convolutional neural networks, namely AlexNet and SqueezeNet through memory and timing side-channel leaks [17]. The proposed attack exploits a specific design choice, i.e., the line buffer in a convolution layer of a CNN.In a nutshell, both previous reverse engineering efforts using side-channel information were performed on very special designs of neural networks and the attacks had very specific and different goals. Basically, having EM as an available source of side-channel leakage, it comes down to using properly designed antennas and more advanced setups, which is beyond the scope of this work.Several other works doing somewhat related research are given as follows. The closest previous works to ours have reverse engineered neural networks by using cache attacks that work on distinct CPUs and are basically micro-architectural attacks (albeit using timing side-channel). We are able to recover the key parameters such as activation function, pre-trained weights, number of hidden layers and neurons in each layer. We emphasize that, for our attack to work, we require the knowledge of some inputs/outputs and sidechannel measurements, which is a standard assumption for side-channel attacks. All the proposed attacks are practically implemented and demonstrated on two distinct microcontrollers (i.e., 8-bit AVR and 32-bit ARM). Usually, in an ANN, the signal at the connection between artificial neurons is a real number and the output of each neuron is calculated as a nonlinear function of the sum of its inputs. Neurons and connections have weights that are adjusted as the learning progresses. It consists of multiple layers of nodes in a directed graph, where each layer is fully connected to the next one. Multilayer perceptron algorithm consists of at least three layers: one input layer, one output layer, and one hidden layer. CNNs represent a type of neural networks which were first designed for 2-dimensional convolutions as it was inspired by the biological processes of animals' visual cortex [28]. Pooling layers are non-linear layers that reduce the spatial size in order to limit the number of neurons. To enable calculations of nontrivial functions for ANN using a small number of nodes, one needs nonlinear activation functions as follows.y = Activation( ∑ (weight · input) + bias). The range of a logistic function is [0,1], which means that all the values going to the next neuron will have the same sign.f (x) = 1 1 + e −x . To denote a vector, we represent it in bold style.f (x) j = e x j ∑ K k=1 e x k , f or j = 1, . . . , K.(4)The Rectified Linear Unit (ReLU) is a nonlinear function that is differing from the previous two activation functions as it does not activate all the neurons at the same time [35]. More specifically, all computations running on a certain platform result in unintentional physical leakages as a sort of physical signatures from the reaction time, power consumption, and Electromagnetic (EM) emanations released while the device is manipulating data. In particular, in this work, we are using the EM side channel and the corresponding terms are adapted to reflect this.Simple Power (or Electromagnetic) Analysis (SPA or SEMA). In this work, we apply SPA, or actually SEMA to reverse engineer the architecture of the neural network.Differential Power (or Electromagnetic) Analysis (DPA or DEMA). Some commonly used leakage models for representative devices are the Hamming weight for microcontrollers and the Hamming distance in FPGA, ASIC, and GPU [4,31] platforms. In this section, we discuss the threat model we use, the experimental setup and reverse engineering of various elements of neural networks. We select to work with MLP and CNNs since: 1) they are commonly used machine learning algorithms in modern applications, see e.g., [16,11,36,48,25,21]; 2) they consist of different types of layers that are also occurring in other architectures like recurrent neural networks; and 3) in the case of MLP, the layers are all identical, which makes it more difficult for SCA and could be consequently considered as the worst-case scenario. If the inputs are in the form of integers (like the MNIST database), the attack becomes easier, since we would not need to recover mantissa bytes and deal with precision. We note that the attacks and analysis presented in our work do not rely on any assumptions on the distributions of the inputs, although a common assumption in SCA is that they are chosen uniformly at random. The attacker can collect multiple side-channel measurements while processing the data and use different side-channel techniques for her analysis. For each known input, the attacker gets one measurement (or trace) from the oscilloscope. The number of samples (or length of the trace) depends on sampling frequency and execution time. An RF-U 5-2 near-field electromagnetic (EM) probe from Langer is used to collect the EM measurements (see Figure 2b). • ARM Cortex-M3: This is a modern 32-bit microcontroller architecture featuring multiple stages of the pipeline, on-chip co-processors, low SNR measurements, and wide application. Moreover, as for ARM Cortex-M3, low SNR of the measurement might force the adversary to increase the number of measurements and apply signal pre-processing techniques, but the main principles behind the analysis remain valid.As already stated above, the exploited leakage model of the target device is the Hamming weight (HW) model. A microcontroller loads sensitive data to a data bus to perform indicated instructions. The full recovery process is described in Section 3.6. The methodology is developed to demonstrate that the key parameters of the network, namely the weights and activation functions can be reverse engineered. We collect EM traces and measure the timing of the activation function computation from the measurements. As shown in Figure 3, the timing behavior of the four tested activation functions have distinct signatures allowing easy characterization.Different inputs result in different processing times. On the other hand, tanh and sigmoid might have similar timing delays, but with different pattern considering the input (see Figure 4b and Figure 4b), where tanh is more symmetric in pattern compared to sigmoid, for both positive and negative inputs. Table 1 presents the minimum, maximum, and mean computation time for each activation function over 2 000 captured measurements. For the recovery of the weights, we use the Correlation Power Analysis (CPA) i.e., a variant of DPA using the Pearson's correlation as a statistical test. 1 CPA targets the multiplication m = x · w of a known input x with a secret weight w. Using the HW model, the adversary correlates the activity of the predicted output m for all hypothesis of the weight. Since the target device is an 8-bit microcontroller, the representation follows a 32-bit pattern (b 31 ...b 0 ), being stored in 4 registers. The 32-bit consist of: 1 sign bit (b 31 ), 8 biased exponent bits (b 30 ...b 23 ) and 23 mantissa (fractional) bits (b 22 ...b 0 ). Hence, by recovering this representation, it is enough to recover the estimation of the real number value.To implement the attack two different approaches can be considered. We then perform the multiplication and estimate the IEEE 754 binary representation of the output.To deal with the growing number of possible candidates for the unknown weight w, we assume that the weight will be bounded in a range [−N, N], where N is a parameter chosen by the adversary, and the size of possible candidates is denoted as s = 2N/p, where p is the precision when dealing with floating-point numbers. We set N = 5 and to reduce the number of possible candidates, we assume that each floating-point value will have a precision of 2 decimal points, p = 0.01. Although, in the later phase of the experiment, we targeted the floating point and fixed-point representation (2 32 in the worst case scenario on a 32-bit microcontroller, but could be less if the value is for example normalized), instead of the real value, which could in principle cover all possible floating values.In Figure 5, we show the result of the correlation for each byte with the measured traces. This is because cryptography typically works on fixed-length integers and exact values must be recovered. When attacking real numbers, small precision errors due to rounding off the intermediate values still result in useful information.To deal with more precise values, we can target the mantissa multiplication operation directly. mantissa w , then taking the 23 most significant bits after the leading 1, and normalization (updating the exponent if the result overflows) if necessary.In Figure 6, we show the result of the correlation between the HW of the first 7-bit mantissa of the weight with the traces. Thus, in both cases, we have shown that we can recover the weights from the SCA leakage.In Figure 7, we show the composite recovery of 2 bytes of the weight representation i.e., a low precision setting where we recover sign, exponent, and most significant part of mantissa. SPA is the simplest form of SCA which allows information recovery in a single (or a few) traces with methods as simple as a visual inspection. The analysis is performed on three networks with different layouts.The first analyzed network is an MLP with one hidden layer with 6 neurons. To determine if the targeted neuron is in the same layer as previously attacked neurons, or in the next layer, we perform a weight recovery using two sets of data.Let us assume that we are targeting the first hidden layer (the same approach can be done on different layers as well). For the second hypothesis, assume that y n is in the next hidden layer (the second hidden layer). The full network recovery is performed layer by layer, and for each layer, the weights for each neuron have to be recovered one at a time. The first step is to recover the weight w L 0 of each connection from the input layer (L 0 ) and the first hidden layer (L 1 ). This can be recovered by the combination of SPA/SEMA and DPA/DEMA technique described in the previous subsection (2 times CPA for each weight candidate w, so in total 2n L 0 2 d CPA required), in parallel with the weight recovery. Timing patterns or average timing can then be compared with the profile of each function to determine the activation function (a comparison can be based on simple statistical tools like correlation, distance metric, etc). The same steps are repeated in the subsequent layers (L 1 , ..., L N−1 , so in total at most 2Nn L 2 d , where n L is max(n L 0 , ..., n L N−1 )) until the structure of the full network is recovered.The whole procedure is depicted in Figure 9. 4 Experiments with ARM Cortex-M3In the previous section, we propose a methodology to reverse engineer sensitive parameters of a neural network, which we practically validated on an 8-bit AVR (Atmel ATmega328P). After capturing the measurements for the target neural network, one can perform reverse engineering. Here, sigmoid and tanh activation functions have similar minimal computation time but the average and maximum values are higher for tanh function. Figure 11b shows the success of attack recovering secret weight of 2.453, with a known input. Similarly, the setup and number of measurements can be updated for other targets like FPGA, GPU, etc.Finally, the full network layout is recovered. The MLP we investigate has 4 hidden layers with dimensions (50,30,20,50), it uses ReLU activation function and has Softmax at the output. The whole measurement trace is shown in Figure 12(a) with a zoom on 1 neurons in the third layer in Figure 12(b). This was resolved by measuring two consecutive layers at a time • We had to resynchronize traces each time according to the target neuron which is a standard pre-processing in side-channel attacks. Next, we experiment with an MLP consisting of 4 hidden layers, where each layer has 200 nodes. The accuracy of the original network is equal to 98.16% while the accuracy of the reverse engineered network equals 98.15%, with an average weight error converging to 0.0025. When considering CNN, the target is the CMSIS-NN implementation [27] on ARM Cortex-M3 with measurement setup same as in previous experiments. If the storing of temporary variable is targeted, as can be seen from Figure 13(a), around 50 000 traces will be required before the correct weight can be distinguished from the wrong weights. In Figure 13(b), it can be seen that after 10 000 traces, the correct weight candidate can be distinguished, and the correlation is slightly higher (∼ 0.34). In our experiment, the original accuracy of the CNN equals 78.47% and the accuracy of the recovered CNN is 78.11%. Hidden layers of an MLP must be executed in sequence but the multiplication operation in individual neurons within a layer can be executed independently. We propose to shuffle the order of multiplications of individual neurons within a hidden layer during every classification step. Weight recovery can benefit from the application of masking countermeasures [8,42]. The proposed attack on activation functions is possible due to the non-constant timing behavior. Shuffling and masking require a true random number generator that is typically very expensive in terms of area and performance. In this work, we practically demonstrate reverse engineering of neural networks using side-channel analysis techniques. We conclude that using an appropriate combination of SEMA and DEMA techniques, all sensitive parameters of the network can be recovered. In the presented experiments, the attack took twice as many measurements, requiring roughly 20 seconds extra time. However, analysis of such partial cases is currently out of scope.The proposed attacks are both generic in nature and more powerful than the previous works in this direction. Additionally, in this work, we considered only feed-forward networks.