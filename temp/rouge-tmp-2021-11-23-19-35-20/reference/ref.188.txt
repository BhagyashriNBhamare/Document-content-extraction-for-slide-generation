Deep neural networks (DNNs) have been shown to tolerate "brain damage": cumulative changes to the network's parameters (e.g., pruning, numerical perturbations) typically result in a graceful degradation of classification accuracy. For large models, we employ simple heuristics to identify the parameters likely to be vulnerable and estimate that 40-50% of the parameters in a model might lead to an accuracy drop greater than 10% when individually subjected to such single-bit perturbations. The widespread usage of DNNs in many mission-critical systems, such as selfdriving cars or aviation [12,51], requires a comprehensive understanding of the security implications of such adversarial bitwise errors.In this paper, we explore the security properties of DNNs under bitwise errors that can be induced by practical hardware fault attacks. To answer this question, we conduct a comprehensive study that characterizes the DNN model's responses to single-bit corruptions in each of its parameters.First, we implement a systematic vulnerability analysis framework that flips each bit in a given model's parameters and measures the misclassification rates on a validation set.Using our framework, we analyze 19 DNN models composed of six different architectures and their variants on three pop-ular image classification tasks: MNIST, CIFAR10, and ImageNet. Our experiments show that, on average, ∼50% of model parameters are vulnerable to single bit-flip corruptions, causing relative accuracy drops above 10%, and that all 19 DNN models include parameters that can cause an accuracy drop of over 90% 1 . Our key findings include: 1) the vulnerability is caused by drastic spikes in a parameter value; 2) the spikes in positive parameters are more threatening, however, an activation function that allows negative outputs renders the negative parameters vulnerable as well; 3) the number of vulnerable parameters increases proportionally as the DNN's layers get wider; 4) two common training techniques, e.g., dropout [52] and batch normalization [24], are ineffective in preventing the massive spikes bit-flips cause; and 5) the ratio of vulnerable parameters is almost constant across different architectures (e.g., AlexNet, VGG16, and so on). Further, even in a blind attack scenario, the attacker can still 1 The vulnerability of a parameter requires a specific bit in its bitwise representation to be flipped. While promising, such solutions cannot deter practical hardware fault attacks in the general case, and often require training the victim model from scratch; hinting that more research is required towards fault attack-resilient DNNs.Contributions. Our analysis shows that a Rowhammer-enabled attacker can inflict significant accuracy drops (up to 99%) on a victim model even with constrained bit-flip corruptions and no knowledge of the model. Our work focuses on feed-forward DNNs-specifically on convolutional neural networks (CNNs)-in the supervised learning setting, i.e., the weights that minimize the inference error are learned from a labeled training set. Here, 1.25 expresses the mantissa; whereas −3 is the exponent. The IEEE754 single-precision floating-point format defines 23 bits to store the mantissa, 8 bits for the exponent, and one bit for the sign of the value. Rowhammer is the most common instance of software-induced fault attacks [9,15,19,44,48,60,62,67]. Prior research has extensively validated a DNN's resilience to parameter changes [2,32,34,35,42,69], by considering random or deliberate perturbations. We take into account two possible scenarios: 1) a surgical attack scenario where the attacker can cause a bit-flip at an intended location in the victim's process memory by leveraging advanced memory massaging primitives [44,62] to obtain more precise results; and 2) a blind attack where the attacker lacks fine-grained control over the bit-flips; thus, is completely unaware of where a bit-flip lands in the layout of the model.Knowledge. Using the existing terminology, we consider two levels for the attacker's knowledge of the victim model, e.g., the model's architecture and its parameters as well as their placement in memory: 1) a black-box setting where the attacker has no knowledge of the victim model. Here, both the surgical and blind attackers only hope to trigger an accuracy drop as they cannot anticipate what the impact of their bit-flips would be; and 2) a white-box setting where the attacker knows the victim model, at least partially. However, the blind attacker gains no significant advantage over the black-box scenario as the lack of capability prevents the attacker from acting on the knowledge. We implement the framework using Python 3.7 3 and PyTorch 1.0 4 that supports CUDA 9.0 for accelerating computations by using GPUs. For ImageNet, we use the ILSVRC-2012 subset [46], resized at 224x224 pixels, composed of 1,281,167 training and 50,000 validation images from 1,000 classes.Models. For MNIST, we define a baseline architecture, Base (B), and generate four variants with different layer configurations: B-Wide, B-PReLU, B-Dropout, and B-DP-Norm. To quantify the indiscriminate damage of single bit-flips, we define the Relative Accuracy Drop as RAD = (Acc pristine −Acc corrupted ) /Acc pristine ; where Acc pristine and Acc corrupted denote the classification accuracies of the pristine and the corrupted models, respectively. On our 8 MNIST models, we carry out a complete analysis: we flip each bit in all parameters of a model, in both directions-(0→1) and (1→0)-and compute the RAD over the entire validation. After a bit-flip, deciding whether the bit-flip leads to a vulnerability [RAD > 0.1] requires testing the corrupted model on the validation set; which might be cost prohibitive. Our the initial MNIST analysis in Sec 4.3 shows that mainly the exponent bits lead to perturbations strong enough to cause indiscriminate damage. For ImageNet models, we use a stronger SB heuristic and only inspect the most significant exponent bit of a parameter to achieve a greater speed-up. We reveal that an attacker, armed with a single bit-flip attack primitive, can successfully cause indiscriminate damage [RAD > 0.1] and that the ratio of vulnerable parameters in a model varies between 40% to 99%; depending on the model. We define the vulnerability based on [RAD > 0.1] and, in Appendix B, we also give how vulnerability changes within the range [0.1 ≤ RAD ≤ 1]. In Table 2, we report the number of effective bitflips, i.e., those that inflict [RAD > 0.1] for each direction, on 3 MNIST and 2 CIFAR10 models. We observe that only (0→1) flips cause indiscriminate damage and no (1→0) flip leads to vulnerability. Therefore, a (1→0) flip, in the exponents, can decrease the magnitude of a typical parameter at most by one; which is not a strong enough change to inflict critical damage. Our results suggest that positive parameters are more vulnerable to single bit-flips than negative parameters.We identify the common ReLU activation function as the reason: ReLU immediately zeroes out the negative activation values, which are usually caused by the negative parameters. Further, experiments on the CIFAR10-B-Slim and CIFAR10-B-twice as wide as the slim model-produce consistent results: 46.7% and 46.8%. We conclude that the number of vulnerable parameters grows proportionally with the DNN's width and, as a result, the ratio of vulnerable parameters remains constant at around 50%. In consequence, we hypothesize that common techniques that tend to constrain the model parameter values to improve the performance, e.g., dropout [52] or batch normalization [24], would result in a model more resilient to single bit-flips. However, when we look into the vulnerability of these models, we surprisingly find that the vulnerability is mostly persistent regardless of dropout or batch normalization-with at most 6.3% reduction in vulnerable parameter ratio over the base network.Impact of the model architecture. In Sec 3, we defined four attack scenarios: the blind and surgical attackers, in the black-box and white-box settings. First, we consider the strongest attacker: the surgical, who can flip a bit at a specific memory location, white-box, with the model knowledge for anticipating the impact of flipping the said bit.To carry out the attack, this attacker identifies: 1) how much indiscriminate damage, the RAD goal, she intends to inflict, 2) a vulnerable parameter that can lead to the RAD goal, 3) in this parameter, the bit location, e.g., 31st-bit, and the flip direction, e.g., (0→1), for inflicting the damage. Based on our [RAD > 0.1] criterion, approximately 50% of the parameters are vulnerable in all models; thus, for this goal, the attacker can easily achieve 100% success rate. For the weakest-black-box blind-attacker that cannot specifically target the 31st-bit, we conservatively estimate the lower-bound as 42.1% / 32-bits = 1.32%; assuming only the 31st-bits lead to indiscriminate damage. Transfer learning is a common technique for transferring the knowledge in a pre-trained teacher model to a student model; which, in many cases, outperforms training a model from scratch. In our experiments, we examine two transfer learning tasks in [63]: the traffic sign (GTSRB) [53] and flower recognition (Flower102) [41]. showing DNNs' graceless degradation, we conduct an additional experiment to see whether a single bit-flip primitive could be used in the context of targeted misclassification attacks. We experiment with a target sample from each class in MNIST or CIFAR10-we use MNIST-B, MNIST-L5, and CIFAR10-AlexNet models. Simlarly, in CIFAR10-AlexNet, there are 6,000 parameters for (class 2-class 3); 3,000 parameters for (class 3-class 6); and 8,000 parameters for (class 6-class 3). Given that we focus on single-bit perturbations on DNN's parameters in practical settings, Rowhammer represents the perfect candidate for the task.DRAM internals. Every DRAM chip contains multiple banks. Here, we focus on double-sided Rowhammer, the most common and effective Rowhammer variant used in practical attacks [15,44,62]. We use ImageNet models since we focus on a scenario where the victim has a relevant memory footprint that can be realistically be targeted by hardware fault attacks such as Rowhammer in practical settings. After preliminary testing of this strategy on our own DRAMs, we concluded it would be hard to generalize the findings of such an analysis and decided against it-in line with observations from prior work [59]. This allows the attacker to find memory pages vulnerable to Rowhammer bit-flips at a given page offset (i.e., vulnerable templates), which they can later use to predictably attack the victim data stored at that location.Vulnerable templates. We discovered this to be the case for all the objects larger than 1 MB-i.e., our attacker needs to target the parameters such as weight, bias, and so on, stored as tensor objects in layers, larger than 1 MB. Memory deduplication is a system-level memory optimization that merges read-only pages for different processes or VMs when they contain the same data. Based on the results of the experiments in Sec 4.3 and Sec 4.4, we analyzed the requirements for a surgical (white-box) attacker to carry out a successful attack. In Table 4, we report min, median, and max values of the number of rows that an attacker needs to hammer to find the first vulnerable template on the 12 different DRAM setups for each model. That is, for every model we tested in the best case, it required us to hammer only 4 rows (A_2 DRAM setup) to find a vulnerable template all the way up to 4,679 in the worst case scenario (C_1). To bound the time of the lengthy 8 We assume 200ms to hammer a row.blind Rowhammer attack analysis, we specifically focus our experiments on the ImageNet-VGG16 model. Our goal is twofold: 1) to understand the effectiveness of such attack vector in a less controlled environment and 2) to examine the robustness of a running DNN application to Rowhammer bit-flips by measuring the number of failures (i.e., crashes) that our blind attacker may inadvertently induce.Attacker's capabilities. For every one of the 12 vulnerable DRAM setups available in the database, we carried out 25 experiments where we performed at most 300 "hammering" attemptsvalue chosen after the surgical attack analysis where a median of 64 attempts was required. The experiment has three possible outcomes: 1) we trigger one(or more) effective bit-flip(s) that compromise the model, and we record the relative accuracy drop when performing our testing queries; 2) we trigger one(or more) effective bit-flip(s) in other victim memory locations that result in a crash of the deep learning process; 3) we reach the "timeout" value of 300 hammering attempts. The corruption generated in this single successful experiment was induced by a single bitflip, which caused one of the most significant RADs detected in the entire experiment, i.e., 0.9992 and 0.9959 in Top-1 and Top-5. In particular, the median drop for Top-1 and Top-5 confirms the claims made in the previous sections, i.e., the blind attacker can expect [RAD > 0.1] on average. Furthermore, this property, combined with the resiliency to spurious bit-flips of the (perhaps idle) code regions, allowed us to build successful blind attacks against the ImageNet-VGG16 model and inflict "terminal brain damage" even when hiding the model from the attacker. Prior work on defenses against Rowhammer attacks suggest system-level defenses [10,27] that often even require specific hardware support [6,26]. There are several functions, such as Tanh or HardTanh [25], that suppresses the activations; however, using ReLU-6 [28] function provides two key advantages over the others: 1) the victim only needs to substitute the existing activation functions from ReLU to ReLU-6 without re-training, and 2) ReLU-6 allows the victim to control the level of permitted activation by modifying the bounds, e.g., using other limits instead of 6, which minimizes the performance loss by bounding the activation. We evaluate four activation functions: ReLU (default), Tanh, ReLU-6, and ReLU-A (only for AlexNet and VGG16), and two training methods: training a model from scratch (Scr) or substituting the existing activation into another (Sub). We found that restricting activation magnitudes with Tanh and ReLU-6 in some instances can reduce the vulnerability; For instance, in the MNIST models, we observed that the number of vulnerable parameters is reduced from 50% to 1.4-2.4% without incurring in significant performance loss. In AlexNet and VGG16, the decrease in the number of vulnerable parameters is also generally significant, namely from 47.34% to 2.8% and 41.13% to 11.67%. Nevertheless, it is interesting to see that by employing ReLU-A, while the number of vulnerable parameters remains significant, the RAD also suffers from the new activation function limiting the possible effects of the corruption. Our experimental results with restricting activation magnitudes suggest that: this mechanism 1) allows a defender to control the trade-off between the relative accuracy drop and reducing the vulnerable parameters and 2) enables ad-hoc defenses to DNN models, which does not require training the network from scratch. To validate our intuition, we use 3 DNN models: the MNIST-L5 (baseline) and its quantized and binarized models. We found that using low-precision parameters reduces the vulnerability; in all cases, the percentage of vulnerable parameters are reduced from 49% (Baseline) to 0-2% (surprisingly 0% with the quantization). Prior work has utilized the graceful degredation of DNN models under parameter perturbations in a wide range of applications. In our work, we study the graceless degredation of DNNs under hardware fault attacks that induce single bit-flips to individual parameters.Indiscriminate poisoning attacks on DNNs. For example, Steinhardt et al. [54] shows that, with IMDB dataset, an attacker needs to craft 3% of the total training instances to achieve 11% of accuracy drop compared to the pristine model. Moreover, to achieve targeted damages without harming the model's overall accuracy, targeted poisoning attacks [49,55] have been studied. While in the past these attacks required physical access to the victim's system [11,38], recently they have gained more momentum since the software-based version of these attacks were demonstrated [26,57]. Our threat model follows the realistic single bit-flip capability of a fault attack and modern application of DNNs in a cloud environment, where physical access to the hardware is impractical. We evaluated 19 DNN models with six architectures on three image classification tasks and showed that: we can easily find 40-50% vulnerable parameters where an attacker can cause indiscriminate damage [RAD > 0.1] by a bit-flip. We freeze the parameters of the first 10 layers.Conv (-) 5x5x10 (2) Conv (R) 5x5x6 (2) Conv (R) 5x5x6 (2) Conv (-) 5x5x6 (2) BatchNorm (R) 10 - - - - BatchNorm (R) 6 - - MaxPool (-) 2x2 MaxPool (-) 2x2 MaxPool (-) 2x2 Conv (-) 5x5x20 (2) Conv (R) 5x5x16 (2) Conv (R) 5x5x16 (2) Conv (-) 5x5x16 (2) BatchNorm (R) 20 - - - - BatchNorm (R) 16 - - MaxPool (-) 2x2 MaxPool (-) 2x2 MaxPool (-) 2x2 - - Conv (R) 5x5x120 (2) Conv (R) 5x5x120 (2) Conv (R) 5x5x120 (2) - - - - - - BatchNorm (R) 120 Dropout (R) 0.5 - - Dropout (R) 0.5 Dropout (R) 0.5 - - MaxPool (-) 2x2 MaxPool (-) 2x2 MaxPool (-) 2x2 - - Conv (R) 5x5x120• Flower102. B The Vulnerability Using Different CriterionWe examine the vulnerable parameter ratio (vulnerability) using the different RAD criterion with 15 DNN models. Each figure describe the vulnerable parameter ratio on a specific RAD criterion; for instance, in MNIST-L5, the model has 40% of vulnerable parameters that cause [RAD > 0.5], which estimates the upper bound of the blind attacker. In MNIST, CIFAR10, and two ImageNet models, the vulnerability decreases as the attacker aims to inflict the severe damage; however, in ImageNet, ResNet50, DenseNet161, and InceptionV3 have almost the same vulnerability (∼50%) with the high criterion [RAD > 0.8]. For AlexNet, we use: 300 epochs,