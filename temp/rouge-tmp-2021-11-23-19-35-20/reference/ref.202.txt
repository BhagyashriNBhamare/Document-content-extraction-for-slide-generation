A fundamental problem with this approach is that we have no way to measure these heuristics to know precisely how well they work, under which circumstances, how prone they are to evasions or to errors, and how stable they are over different versions of the OS kernel. In this paper we propose a framework and a set of metrics we can use as a basis to assess existing methodologies, understand their characteristics and limitations, and propose new techniques in a principled way. Memory forensics, i.e. the field focusing on the analysis of snapshots of the physical memory of a machine, is no exception to this rule. Even worse, both the fields and the layout of these objects can change when the kernel is updated or recompiled, and the connections among them evolves very rapidly -with a considerable amount of links and pointers that change every few milliseconds.Currently memory forensics techniques rely on a large number of rules and heuristics that describe how to navigate through this giant graph of kernel data structures to locate and extract information relevant to an investigation. In a different investigation in which there is no risk of tampered kernel data, a heuristic that only traverses closely-related (in physical memory) data structures may be preferable as pages acquired far apart may otherwise contain inconsistent information if the dump was not acquired atomically.Contribution: The goal of this paper is to introduce a more principled way to approach the problem of memory analysis and forensics. Finally, the analyst should be able to obtain some form of guarantee about the results, to ensure that once a given quality metric has been chosen, a certain technique is the optimal solution to navigate the intricacies of runtime OS data structures.As a step towards these goals, we constructed a complete graph of the internal data structures used at runtime by the Linux kernel. The resulting map of the memory is a giant network (containing over a million nodes) with a very dynamic topology that is constantly reshaped as new data structures get allocated and deallocated.Memory forensics tools adopts rules to navigate through the data structures present in a memory dump, and these rules can therefore be represented as paths in our kernel graph. We then discuss the intricacies of identifying such optimal paths by performing experiments with 85 different kernel versions and 25 individual memory snapshots acquired at regular time intervals.Building a map of the kernel memory is a very tedious and time-consuming process. More precisely, the first category contains all the practical issues of acquiring memory from a device under investigation while the second one deals with the capabilities of memory forensics, such as malware detection and evidence extraction.One of the main issues belonging the first category is page smearing, which is a consequence of the fact that while the acquisition is performed the underlying system is not frozen and thus the dump may contain inconsistent information [12]. Our work can help mitigating this issue by assessing how existing techniques are affected by non-atomic acquisitions, and help design new heuristics which are more robust against the presence of inconsistent information.The second category focuses instead on challenges related to memory analysis. The first step consists of building a precise representation of all data structures that exists in a running kernel and of the way these structures are connected to one another. Finally, in the fourth and final phase of our methodology we translate our findings back to the memory forensic space by generating improved analysis plugins, thus increasing the number and quality of the rules that are used today to analyze memory dumps. On Linux, this analysis starts from extracting the position of the global kernel variable init_task of type task_struct. While the final node is dictated by a given forensic task, both the first and the intermediate nodes are often the result of handcrafted routines based on the experience and expert judgment of the developers of the forensic tool.In our graph, the previously presented analysis would correspond to the path 1 in Figure 1. Also, note that in the example 1 , since the suspicious process was freshly spawned, the shortest path in our graph traverses the process list backwards -contrarily to the more common forward walking.On top of the previous solution, our approach shows that a stunning 2.5 million different sequences of vertices exist in the kernel graph to reach the very same target object starting from a global variable, only counting the paths with no more than 10 edges. For example, path 2 in Figure 1 begins from the little-known global symbol root_cpuacct, passes through a number of cgroup-related objects, before finding the task_struct of the suspicious process.The previous two "rules" are both capable of locating a given process structure in a memory dump. The model we chose for our analysis is a graph of kernel objects, in which nodes represent kernel data structures and edges represent relationships between objects (for example a pointer from one structure to another). The first one is the layout, in terms of the exact type and offset of each field, of all the struct defined and used by the kernel code. In fact, if the leftmost task_struct in the figure was already identified by other means (for example because it was pointed to from a global variable), simply following the next pointer would result in the discovery of the inner list_head structure, but not of the outer task_struct.In fact, this operation is performed in the source code by using dedicated macros. In the case of the previous example, a developer would invoke:container_of(var, struct task_struct, task) that the compiler pre-processor translates to a snippet of code required to cast the target list_head variable var to the requested type based on the current offset inside it (as specified by the field task). One example is the list rooted in the field children of a task_struct. While this was doable for old kernel versions (e.g., 2.4), it would take many weeks of tedious work to annotate a recent kernel -which today uses more than 6000 different data structures and more than a thousand instances of list_heads. Only four previous studies automatically extracted a type graph of a kernel [4,17,19,29]. For this reason, we decided to implement our own points-to analysis -which consists of a clang plugin that reasons on the Abstract Syntax Tree (AST) of each kernel compilation unit. Our plugin current supports list_heads, hlist_heads (used in the implementation of hash tables), and rb_root (used in the implementation of red-black trees). For example, other than the already cited children field of task_struct, also the thread_node field of the same structure points inside a signal_struct object.To avoid this problem, our analysis classifies every list_head field in one of the following three categories: root pointer, intermediate pointer or homogeneous pointer. For instance, task_struct.children is a root pointer, task_struct.sibling an intermediate and task_struct.tasks a homogeneous one. This pointer will eventually be explored when the corresponding root node will be visited.This classification works for every list encountered during the exploration phase, except for global list_head variables which are always marked as root node. Finally, even if very rare, kernel bugs can contribute to the generation of similar errors.For these reasons we implemented two sets of heuristics to check if an object is valid or not. Finally, we require that, whenever present, a list_head has to be valid, i.e. its next and prev pointer must point to addressable memory. Since we are interested in using our graph to analyze and improve existing memory forensic techniques, opaque pointers play a very marginal role (if any at all) in this space. Finally, it is important to understand that these limitations cannot lead to "wrong" results (since they cannot create erroneous paths in the graph), but nonetheless restrict the guarantees of optimality we discuss in the next sections to the constructed graph. For example, the pid_hash hash table, used by the kernel to quickly locate a process given its process id, is implemented by using a dynamically allocated array where the size is specified in another global variable (pidhash_shift). It consists of an LLVM compiler plugin to perform the points-to analysis on the kernel code at compile-time and a set of python gdb extensions that combine the information extracted in the previous step with the information provided by kernel debug symbols to identify all kernel objects contained in a memory snapshot acquired using the QEMU emulator. As we will thoroughly discuss in Section 5 we assign a number of different weights to each node and edge to allow for several comparisons among different paths. Figure 5 shows a kernel graph counting 109,000 nodes and 846,000 edges, plotted using Gephi [2]. As we will discuss in Section 6, this has important consequences for memory analysis, as it results in a multitude of available paths to move from one node to almost anything else in the kernel memory. If we exclude the file system, the node with the highest degree is a vm_operations_struct, pointed by more than 4200 vm_area_structs.In the picture, the size of labels and node is adjusted according to the betwenees centrality of a node. And since the idea of having an absolute metric is For our experiments we decided to investigate and add to our graph three numerical and two boolean weights, related to the atomicity, stability, generality, reliability, and consistency of a path. As described below, all of them capture different but important aspects of what an analyst may expect from a memory analysis routine. More precisely, we can adopt three distinct ways to measure Atomicity:â€¢ Acquisition Window (AW) -this is the total window that covers all data structures traversed in the path. For instance, if a path visits three consecutive nodes (A, B, and C) and the difference between the acquisition time of the pointer in A and the content of B was 7 seconds and the difference between B and C was 3, the CTG would be 10 seconds. Whether this is an advantage or not depends on how often those pointers are modified in a running kernel, which we capture with our next metric. By computing a heat-map of the stability of each edge (extracted by processing a number of consecutive snapshots), this weight can provide a valuable information on how the kernel map evolves over time, on which paths are more stable, and on which are instead more ephemeral and may only exists for short periods of time.We measure Stability by computing the Minimum Constant Time (MCT) of all links in a path. In our experiments, we computed this metric by taking a snapshot of the same system at seconds 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 100, 200, 350, 700, 1000, 3000, 5000, 8000 and 12000. Moreover, if we exclude the filesystem subsystem and the paths that remained stable over all our experiments, 11% of the edges changed in less than 10 seconds, 12.5% in less than a minute, and 97% in the first hour. Therefore, it would be interesting to compute analysis paths that traverse structures which change very rarely across different distributions, kernel versions, and enabled kernel options.For this reason we downloaded 85 kernels from the Ubuntu repository, spanning from version 4.4.0-21 to 4.15.0-20. This is a very important aspect in memory forensics and captures how tamper-resistant is a given path on the graph, assuming an attacker is capable of reading and writing arbitrary kernel memory. But here we are instead interested in the reliability of a path, i.e., not in the fact that individual fields (such as a file name) can be modified, but whether an attacker can tamper with the edges that need to be traversed to prevent a certain heuristic to reach its destination (to the best of our knowledge, this problem has never been addressed in the literature). For this example we chose to combine the stability and atomicity of a path in a single measure that captures how likely it is for a given path to traverse consistent information. For our example we selected seventeen Volatility plugins that explore different subsystems (process, network, and filesystem) and mapped them as paths in our kernel graph. Note that many plugins traverse similar kernel structures (e.g linux_pslist and linux_pstree) so, to avoid duplicates, we only report results for a subset that rely on different information. By combining this information with the nodes visited by Volatility, we found that this component contains all the information related to running processes, such as their mapped memory and open files, but also the information related to the arp table and the ttys. In such cases, one entry point can often be located by memory carving and then used as starting point for every other analysis.By looking at the atomicity metrics (columns four-to-six in Table 1) the first thing that stands out is that the values for the Acquisition Window (AW) and the Maximum Time Gap (MTG) are very similar and relatively constant across all commands. After further investigation we discovered that this is due to the fact that, when compiled with normal configurations, Linux kernel global variables are located in the low part of the physical memory while kernel objects are allocated in the higher end. This physical distribution is also very important for the third scenario presented in Section 6.3, where we will encounter heuristics that need to hop back and forth from the three clusters, significantly impacting the atomicity metrics.The second surprising result of this first scenario is the fact that the Kernel Counters (KC) of six plugins never changed across all the different kernel versions we used in our analysis. Our experiment suggests that, at least for locating certain information, a generic structure layout can be used across almost 100 kernel versions, released as far as 2 years apart.Another important propriety we evaluated in this first scenario is the consistency of the selected techniques. In that study, the authors compared seven different memory acquisition tools, chosen from a survey conducted over 41 companies specialized in memory forensics.Interestingly, out of the 17 plugins we tested, three have a stability of 12,000 seconds, which means that none of the links they traversed ever changed over a period of more than three hours. The affected plugins interest different parts of the kernel, but they can be divided to three distinct categories: Memory ( linux_proc_maps, linux_proc_maps_rb), File system (linux_check_fop, linux_find_file, linux_lsof, linux_mount) and Process (linux_pidhashtable)In the Memory category we found respectively 33 inconsistencies that affected the connections among vm_area_struct of a process, which are kept both in a linked list and in a red-black tree. The filesystem category included 40 unique inconsistencies in the hierarchy of dentries (fields d_subdirs and d_child) 53 in the mapping from a dentry to an inode (field d_inode). Currently Volatility implements three different plugins 1 to list the processes, respectively by walking the process list, by using the pidhash hashtable, and by parsing the kernel memory allocator. Unfortunately, many distributions, such as Ubuntu and Debian, ships by default with the SLUB allocator, which is not supported by Volatility and which does not keep track of full slabs -thus making this technique not applicable anymore.The main reason for looking for alternative solutions is that previous research already pointed out that rootkits are already capable of removing a process from the process list, but also to unlink a process from the pid hashtable [19,26,27] thus leaving the forensic analyst without a reliable method to list processes. First, we discarded all the global roots that do not have a path to reach all the task_structs in every graph we created. While not useless per se, our goal is to find new solutions and not variations of the existing ones.By only considering the shortest paths from every root node to every task structure, our system found more than 100 million distinct paths, generated from a set of more than 966,000 sequences of vertices. After this operation the number of different paths decreased to about 7.5 millions paths.We then merged similar paths into templates, constructed by keeping only the type of the objects present in the path, and by also removing adjacent nodes with the same type (which capture the link discussed in Section 3). Finally, we removed templates that were subset of other templates, resulting in a final set of 4067 path templates.By manually exploring these options, we soon realized that they belong to only four main families, depending on the kernel subsystem they live in. In fact, the bulk of the time gap (16.24 seconds) is due to the difference in the acquisition time of the global entry points (located in C1 in Figure 7) and the first task structure (located in C2 and C3). However, all these edges are very stable and in only one case (for the css_set_table) the value of this first connection ever changed during our memory acquisition.The memory-based heuristics walked a red-black tree (i_mmap) that is very ephemeral and, while exploring it, we found more than 30 edges that could be inconsistent if the memory dump is not taken atomically. Finally, the two heuristics in the process category, which represent the Volatility plugins linux_pslist and linux_threads, had both a stability of 30 seconds. For the first two process heuristics, we removed the process from the process list (by unlinking task_struct.tasks), while for the pid_hash we removed the struct pid from the hashtable. We then proceeded by unlinking the worker from the linked list rooted at worker_pool.workers.In all the cases our program continued to run without observable side-effects -showing that each path we listed so far can be tampered with by a properly written rootkit. To run our experiments we collected all the task_struct and all the associated file objects and analyzed the paths Volatility would take to move from the first to the second. If a task_struct and all the intermediate objects needed to reach the open files are located inside the same cluster, then time gaps are extremely small and path are always consistent. But in this third case it might be possible to use our graph to find an alternative path that is fully contained in the same cluster.An example of each of these three cases is shown in Table 3, along with the metrics computed on the Volatility heuristic and those computed on the optimal paths extracted from our graph. The goal of our work is to provide a principled way to think about memory forensics as a graph-related optimization task. This way of modeling the problem opens the door to a multitude of different possibilities to evaluate and compare existing techniques, design algorithms to compute new alternative solutions, validate the consistency of kernel structures, or propose heuristics customized to different experiments setup and acquired dump.We tried to discuss some of these opportunities through our experiments, but we are aware that many questions are still open and new research is needed to shed light to each individual use case. We tried to do this in our second scenario, and run into a path explosion problem even by considering only all shortest paths. This may suggest that maybe, instead of relaying on a single solution, new techniques should try to explore the graph by following many parallel paths.Moreover, we are aware that some of the metrics we proposed in this paper turned out to be ineffective in the evaluation. So, we believe it is still interesting to implement and discuss those ineffective metrics in our framework.Finally, we want to stress the fact that our main contribution is not the discovery of new technique, but the introduction of a model that can be used to reason about memory analysis, explore its complexity, and perform quantitative measurements.Future Work: In this paper we discuss a number of metrics an analyst can use to compare different solutions. However, the list is certainly not exhaustive and we expect more to be defined in the future. The analysis of kernel objects and their inter-dependencies has attracted the interest of both the security and the forensics community. However, most of these techniques are not directly applicable to kernel-level data structures, because they do not take in account the intricacies present in the kernel, such as resolution of ambiguous pointers to handle custom data structures. For this reasons a kernel graph built using the approach adopted by SigGraph would only retrieve a partial view of the entire memory graph.Another work focused on signatures to match kernel objects in memory was done by Dolan-Gavitt et al in 2009 [10]. The main insight of this work is that while kernel rootkits can modify certain fields of a structure -i.e. to unlink the malicious process from the process list -other fields (called invariants) can not be tampered without stopping the malicious behavior or causing a kernel crash. During the first one every access to a data structure is logged, using the stealth breakpoint hypervisor technique [32]. While this approach looks very promising is not easily adaptable to our context since, as also noted by the authors, creating a signature for small structures can be difficult. As we discussed in Section 4 the Linux kernel uses a large amount of ambiguous pointers, thus making the manual annotation approach not feasible anymore.While more focused on kernel integrity checks, OSck [16] uses information from the kernel memory allocator (slab) to correctly label kernel address with their type. While this approach seems promising, only a small subset of frequently-used structures are allocated using slab (such as task_struct or vm_area_struct), and thus is unsuitable for our needs.Another approach to create a Windows kernel object graph is MACE [11]. While the Windows kernel has only one allocator, the Linux kernel has three different ones (slab, slub, and slob). As a result, analysts are left without any clear guidelines on how to compare and evaluate different approaches and how to assess the results they produce.For these reasons, in this paper we proposed a method to study memory forensics techniques in a principled way. Our solution is based on a graph representation that captures the relationships between all kernel objects, enriched with a set of metrics that covers different aspects of memory forensics. We believe that our framework can help researchers to measure the quality of existing memory forensics techniques, but also to extract qualitatively better heuristics.