To evaluate the utility of the FAIL model, we consider the problem of conducting targeted poisoning attacks in a realistic setting: the crafted poison samples must have clean labels, must be individually and collectively inconspicuous , and must exhibit a generalized form of trans-ferability, defined by the FAIL model. By taking these constraints into account, we design StingRay, a targeted poisoning attack that is practical against 4 machine learning applications, which use 3 different learning algorithms , and can bypass 2 existing defenses. An adversary knows some or all of the ML system's parameters and uses this knowledge to craft training or testing samples that manipulate the decisions of the ML system according to the adversary's goal-for example, to avoid being sentenced by an ML-enhanced judge.Recent work has focused primarily on evasion attacks [4,44,17,50,35,9], which can induce a targeted misclassification on a specific sample. As illustrated in Figures 1a and 1b, these test time attacks work by mutating the target sample to push it across the model's decision boundary, without altering the training process or the decision boundary itself. Specifically, black-box attacks often investigate transferability in the case where the local and target models use different training algorithms [36]. To this end, we propose the FAIL model, a general framework for the analysis of ML attacks in settings with a variable amount of adversarial knowledge and control over the victim, along four tunable dimensions: Features, Algorithms, Instances, and Leverage. For example, our evaluation suggests that crafting transferable samples with an existing evasion attack is more challenging than previously believed.To evaluate the utility of the FAIL model, we consider the problem of conducting targeted poisoning attacks in a realistic setting. Moreover, the StingRay attack is model agnostic: we describe concrete implementations against 4 ML systems, which use 3 different classification algorithms (convolutional neural network, linear SVM, and random forest). We show that a previous black-box evasion attack is less effective under generalized transferability. • We systematically explore realistic adversarial scenarios and the effect of partial adversary knowledge and control on the resilience of ML models against a test-time attack and a training-time attack. For example, in [36] a black-box adversary possesses knowledge of the full feature representations, whereas its counterpart in [50] only assumes access to the raw data (i.e. before feature extraction). A commercial ML-based malware detector [1] can rely on a publicly known architecture with proprietary data collected from end hosts, and a mixture of known features (e.g. system calls of a binary), and undisclosed features (e.g. reputation scores of the binary). Existing attacks [51,29,20] often assume full control over the training process of victim classifiers and have similar shortcomings to white-box attacks. However, an attacker is often unable to determine the labels assigned to the poison samples in the training set -consider a case where a malware creator may provide a poison sample for the training set of an ML-based malware detector, but its malicious/benign label will be assigned by the engineers who train the detector. The labels for instances in S are assigned using an oracle -for a malware classifier, an oracle could be an antivirus service such as VirusTotal, whereas for an image classifier it might be a human annotator. The testing set T ⊂ X includes instances for which the labels are unknown to the learning algorithm.Threat model. We focus on targeted poisoning attacks against machine learning classifiers. Mallory has partial knowledge of Alice's classifier and read-only access to the target's feature representation, but they do not control either t or the natural label y t , which is assigned by the oracle. Realistic adversaries conducting training time or testing time attacks are constrained by an imperfect knowledge about the model under attack and by limited capabilities in crafting adversarial samples. We formalize the adversary's strength in the FAIL attacker model, which describes the adversary's knowledge and capabilities along 4 dimensions:• Feature knowledge R = {x i x i ∈ x, x i is readable}:the subset of features known to the adversary. Without knowing the victim classifier A, the attacker would have to select an alternative learning algorithm A ′ and hope that the evasion or poison samples crafted for models created by A ′ transfer to models from A. Similarly, if some features are unknown (i.e., partial feature knowledge), the model used for crafting instances is an approximation of the original classifier. For classifiers that learn a representation of the input features (such as neural networks), limiting the F dimension results in a different, approximate internal representation that will affect the success rate of the attack. For poisoning attacks, this places an upper bound on the ability of samples to shift the decision boundary while for evasion it could affect their effectiveness. An adversary with partial leverage needs extra effort, e.g. to craft more instances (for poisoning) or to attack more of the modifiable features (for both poisoning and evasion). Budget considerations address the limitations in an attacker's resources, such as the maximum number of poisoning instances or, for evasion attacks, the maximum number of queries to the victim model. Feature subsets may not be publicly available (e.g. derived using a proprietary malware analysis tool, such as dynamic analysis in a contained environment), or they might be directly defined from instances not available to the attacker (e.g. low-frequency word features). These questions define the spectrum for adversarial knowledge with respect to the learning algorithm: black-box access, if the information is public, gray-box, where the attacker has partial information about the algorithm class or the ensemble architecture, or white-box, for complete adversarial knowledge.The I dimension controls the overlap between the instances available to the attacker and these used by the victim. An application might use instances from the public domain (e.g. a vulnerability exploit predictor) and the attacker could leverage them to the full extent in order to derive their attack strategy. For some applications, the attacker may not be able to modify certain types of features, either because they do not control the generating process (e.g. anStudy F A I L Test Time Attacks Genetic Evasion[50] , , , † , Black-box Evasion[37] ,∅* , , ,∅* Model Stealing[46] , , , ,∅* FGSM Evasion[17] ,∅* ,∅* ∅,∅ ,∅* Carlini's Evasion[9] ,∅* , ∅,∅ ,∅* Training Time Attacks SVM Poisoning[5] ,∅* , † ∅,∅ ,∅* NN Poisoning[33] , † , , ,∅* NN Backdoor[20] 2 , † , , † , NN Trojan[29], , , , [38] , , , , Feature Squeezing [49] , , , ,Training Time Defenses RONI [34] , , , , Certified Defense [42] , , , , exploit predictor that gathers features from multiple vulnerability databases) or when the modifications would compromise the instance integrity (e.g. a watermark on images that prevents the attacker from modifying certain features). In cases of dependence among features, targeting a specific set of features could have an indirect effect on others (e.g. an attacker injecting tweets to modify word feature distributions also changes features based on tweet counts). To demonstrate its utility, we evaluate a body of existing studies by means of answering two questions for each work.To categorize existing attacks, we first inspect a threat model and ask: AQ1-Are bounds for attacker limitations specified along the dimension? For instance, the threat model in Carlini et al.'s evasion attack [9] specifies that the adversary requires complete knowledge of the model and its parameters, thus the answer is yes for the A dimension. For example, feature squeezing [49] employs feature reduction techniques unknown to the attacker; therefore the answer is yes for the F dimension.In order to identify hardening dimensions, which attempt to limit the attack capabilities, we ask: DQ2-Is the dimension employed as a mechanism for hardening? For instance, the distillation defense [38] against evasion modifies the neural network weights to make the attack more difficult; therefore the answer is yes for the A dimension.These defenses may come with inaccurate assessments for the adversarial capabilities and implicit assumptions. StingRay is a general framework for crafting poison samples.At a high level, our attack builds a set of poison instances by starting from base instances that are close to the target in the feature space but are labeled as the desired target label y d , as illustrated in the example from Figure 2. Here, the adversary has created a malicious Android app t, which includes suspicious features (e.g. the WRITE_CONTACTS permission on the left side of the figure), and wishes to prevent a malware detector from flagging this app. We describe concrete implementations of our attack against four existing applications: an image recognition system, an Android malware detector, a Twitter-based exploit predictor, and a data breach predictor. However, this is impractical because the adversary, under our threat Algorithm 1 The StingRay attack.1: procedure STINGRAY(S ′ ,Y S ′ , t, y t , y d ) 2: I = ∅ 3: h = A ′ (S ′ ) 4:repeat 5:x b = GETBASEINSTANCE(S ′ ,Y S ′ , t, y t , y d )6:x c = CRAFTINSTANCE(x b , t) 7:if GETNEGATIVEIMPACT(S ′ , x c ) < τ NI then 8:I = I ∪ {x c } 9: h = A ′ (S ′ ∪ I) 10:until (I > N min and h(t) = y d ) or I > N max 11:PDR = GETPDR(S ′ ,Y S ′ , I, y d ) 12: if h(t) ≠ y d or PDR < τ PDR then 13:return ∅ returnI 15: procedure GETBASEINSTANCE(S ′ ,Y S ′ , t, y t , y d ) 16: for x b , y b in SHUFFLE(S ′ ,Y S ′ ) do 17: if D(t, x b ) < τ D and y b = y d then 18:return x b model, does not control the labeling function. A more sophisticated approach would mutate these samples and use poison instances to push the model boundary toward the target's class [32]. We overcome this via GETPDR by checking the performance drop of the attack samples (S.V), therefore ensuring that they remain collectively inconspicuous.Even so, the StingRay attack adds robustness to the poison instances by crafting more instances than necessary, to overcome sampling-based defenses (D.VI). For scenarios with partial knowledge, it is approximated through the perceived PDR on the available classifier.S.I h(t) = y d : the desired class label for target S.II D(t, x b ) < τ D : the inter-instance distance metric D.III ¯ s = 1 I ∑ x c ∈I s(x c , t),The impact of crafted instances is influenced by the distance metric and the feature space used to measure instance similarity (S.II). For applications that learn feature representations (e.g. neural nets), the similarity of learned features might be a better choice for minimizing the crafting effort.The set of features that are actively modified by the attacker in the crafted instances (D.III) defines the target resemblance for the attacker, which imposes a trade-off between their inconspicuousness and the effectiveness of the sample. For applications where models are updated over time or trained in mini-batches (such as an image classifier based on neural networks), the attacker only requires control over a subset of such batches and might choose to deliver poison instances through them. In this section, we discuss three defenses against poisoning attacks and how StingRay exploits their limitations.The Micromodels defense was proposed for cleaning training data for network intrusion detectors [13]. It also relies on the availability of accurate instance timestamps.Reject on Negative Impact (RONI) was proposed against spam filter poisoning attacks [3]. However, RONI remains computationally inefficient as the number of trained classifiers scales linearly with the training set.Target-aware RONI (tRONI) builds on the observation that RONI fails to mitigate targeted attacks [34] because the poison instances might not individually cause a significant performance drop. However, attacks such as StingRay could exploit this assumption to evade detection by crafting a small number of inconspicuous poison samples. We first poison a neural-network (NN) based application for image classification, often used for demonstrating evasion attacks in the prior work. We evaluate StingRay on our own implementation for CIFAR-10 [24]. In our experiments we varied γ over {0.125, 0.5, 1.0} (i.e. 4, 16, and 32 instances of the batch are poison) and selected the value that provided the best attack success rate, keeping it fixed across successive updates. It is worth noting that if the learning rate is high and the batch contains too many poison instances, the attack could become indiscriminate.Conversely, too few crafted instances would not succeed in changing the target prediction, so the attacker needs to control more batches.The main insight that motivates our method for generating adversarial samples is that there exist inputs to a network x 1 , x 2 whose distance in pixel space x 1 − x 2 is much smaller than their distance in deep feature spaceH i (x 1 ) − H i (x 2 ))), where H i (x) is the value of the i th hidden layer's activation for the input x. This suggests that the selection of the layer index i in the objective function offers a trade-off between attack transferability and the magnitude of perturbations in crafted images (Cost D.III). We use stratified sampling and split the data set into 60%-40% folds training and testing respectively, aiming to mimic the original classifier. Second, after disassembling the dex file, which contains the app bytecode, the system extracts suspicious Android framework calls, actual permission usage and hardcoded URLs. We then craft instances by adding active features (permissions, API calls, URL requests) from the target to existing benign instances, as illustrated in Figure 2. In [40], the authors present a system, based on a linear SVM, that predicts which vulnerabilities are going to be exploited using features extracted from Twitter and public vulnerability databases. Due to the class imbalance, we use stratified samples of 60%-40% of the data set for training and testing respectively, obtaining a 40% testing F1.The targeted attack selects a set I of vulnerabilities that are similar to t (e.g. same product or vulnerability category), have no known exploits, and gathered fewer tweets. However, the attacker's leverage is limited since the features extracted from sources other than Twitter are not under the attacker's control.We simulate 1,932 attacks setting N max = 20 and selecting the CVEs to be poisoned using the Euclidean distance D = l 2 with τ NI < 50%. We use stratified sampling to build a training set containing 50% of the corpus and use the rest for testing and choosing targets for the attacks. The classifier achieves a 60% F1 score on the testing set.In this case, the adversary plans to hack an organization t, but wants to avoid triggering an incident prediction despite the eventual blacklisting of the organization's IPs. The attacker has limited leverage and is only able to influence time series based features indirectly, by injecting information in various blacklists.We generate 2,002 attacks under two scenarios: the attacker has compromised a blacklist and is able to influence the features of many organizations, or the attacker has infiltrated a few organizations and it uses them to modify their reputation on all the blacklists. Running time of StingRay.The main computational expenses of StingRay are: crafting the instances in CRAFTINSTANCE, computing the distances to the target in GETBASEINSTANCE, and measuring the negative impact of the crafted instances in GETNEGATIVEIMPACT.CRAFTINSTANCE depends on the crafting strategy and its complexity in searching for features to perturb. We systematically create over 19,000 Android applications that correspond to attack instances and utilize VirusTotal, in the same way as Drebin does, to label them. However, in our attack scenario, we assume that the attacker is not consulting the oracle, releasing all crafted instances as part of the attack.For the exploit predictor, labeling is performed independently of the feature representations of instances used by the system. Therefore the attacker has more degrees of freedom in modifying the features of instances in I, knowing that their desired labels will be preserved.In case of the data breach predictor, the attacker utilizes organizations with no known breach and aims to poison the blacklists that measure their hygiene, or hacks them directly. We quantify the effectiveness of the evasion attack using the percentage of successful attacks (SR), while for StingRay we also measure the Performance Drop Ratio (PDR). The first attack subjected to the FAIL analysis is JSMA [35], a well-known targeted evasion attack Transferability of this attack has previously been studied only for an adversary with limited knowledge along the A and I dimensions [37]. Experiment #6 corresponds to the white-box adversary, where we observe that the white-box attacker could reach 80% SR.Experiments #1-2 model the scenario in which the attacker has limited Feature knowledge. The attacker uses the cropped images for training and testing the classifier, as Tables 3, 4, 5: FAIL analysis of the two applications. This suggests that the evasion attacks are very sensitive in such scenarios, highlighting a potential direction for future defenses.We then model an attacker with limited Algorithm knowledge, possessing a similar architecture, but with smaller network capacity. In #5 we simulate a scenario in which the attacker only knows 70% of the victim training set, while #7-8 model an attacker with 80% of the training set available and an additional subset of instances sampled from the same distribution.These results might help us explain the contradiction with prior work. Indeed, we observe that a robust attacker classifier, trained on a sizable data set, reduces the SR to 19%, suggesting that the attack success sharply declines with fewer victim training instances available. This suggests that although reducing feature knowledge decreases the effectiveness of StingRay, the specificity of some known features may still enable successful attacks.Along the A dimension, we observe that both architectures allow the attacker to accurately approximate the deep space distance between instances. While the perceived SR is overestimated, the actual SR of these attacks is comparable to the white-box attack, showing that ar- chitecture secrecy does not significantly increase the resilience against these attacks. This highlights the benefit of a fine-grained analysis along all dimensions, since the attack success rate may not be monotonic in terms of knowledge levels.Surprisingly, we observe that the actual SR for #8, where the attacker has more training instances at their disposal, is lower than for #7. After poisoning the victim, the effect of crafted instances would not be bootstrapped by the base instances, and the attacker fails. The attacks transfer surprisingly well from the attacker to the victim, and a significant number of failed attacks would potentially be successful if triggered on the victim. We observe that limited leverage allows the attacker to localize their strategy, crafting attack instances that are even more successful than the white-box attack.StingRay on the malware classifier. Experiment #3 trains a linear model using the Stochastic Gradient Descent (SGD) algorithm, and in #4 (dSVM), the hyperparameters of the SVM classifier are not known by the attacker. Compared with the original Drebin SVM classifier, the default setting in #4 uses a larger regularization parameter. The results show, as in the case of the image classifier, that poison instances are highly dependent on other instances present in the training set to bootstrap their effect on target misclassification. However, feature secrecy and limited leverage appear to have the most significant effect on decreasing the success rate, hinting that they might be a viable defense. By tuning the attack parameters (e.g. the layer used for comparing features or the degree of allowed perturbation) to generate poison instances that are more specific to the target, the performance drop on the victim could be further reduced at the expense of requiring more poisoning instances. Nevertheless, this shows that neural nets define a fine-grained boundary between class-targeted and instance-targeted poisoning attacks and that it is not straightforward to discover it, even with complete adversarial knowledge.None of the three poisoning defenses are applicable on this task. MM rejects all training instances while RONI fails to detect any attack instances. The result, backed by our FAIL analysis of the other linear classifier in Section 5.1, highlights the benefits of built-in leverage limitations in protecting against such attacks.MM correctly identifies the crafted instances but also marks a large fraction of positively-labeled instances as suspicious. However, the reduced attack success rate on applications with limited leverage suggests new directions for future defenses. By defining a wider spectrum of adversarial knowledge, FAIL generalizes the notion of transferability.Prior work introduced indiscriminate and targeted poisoning attacks. Unlike existing targeted poisoning attacks, StingRay aims to bound indiscriminate damage to preserve the overall performance.On neural networks, [23] proposes a targeted poisoning attack that modifies training instances which have a strong influence on the target loss. In Table 2 we highlight implicit and explicit assumptions of previous defenses against poisoning and evasion attacks.Through our systematic exploration of the FAIL dimensions, we provide the first experimental comparison of the importance of these dimensions for the adversary's goals, in the context of targeted poisoning and evasion attacks. At odds with the original findings in [37], our results suggest a lack of generalized-transferability for a state of the art evasion attack; while highlighting feature secrecy as the most prominent factor in reducing the attack success rate. Each iteration of the loop crafts one poison instance by invoking CRAFTINSTANCE, which modifies the set of allowable features (according to FAIL's L dimension) of the base instance. The other application-specific elements are the distance function D and the method for injecting the poison in the training set: the crafted instances may either replace or complement the base instances, depending on the application domain. To validate that these techniques result in individually inconspicuous samples, we consider whether our crafted samples would be detected by three anti-poisoning defenses, discussed in detail in Section 4.1. STINGRAY builds a set I with at least N min and at most N max attack instances. By choosing base instances that are as close to the target as possible, the adversary reduces the risk that the crafted samples will become outliers in the training set. The attack is considered successful if both adversarial goals are achieved: changing the prediction of the available classifier and not decreasing the PDR below a desired threshold τ PDR .