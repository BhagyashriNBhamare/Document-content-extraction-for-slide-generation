The k-Nearest Neighbor Search (k-NNS) is the backbone of several cloud-based services such as recommender systems, face recognition, and database search on text and images. In these services, the client sends the query to the cloud server and receives the response in which case the query and response are revealed to the service provider. We provide several contributions to each of these primitives which are applicable to other secure computation tasks. We have implemented our proposed system and performed extensive experimental results on four datasets in two different computation environments, demonstrating more than 18 − 31× faster response time compared to optimally implemented protocols from the prior work. The k-NNS has many applications in modern data analysis: one typically starts with a dataset (images, text, etc.) and, using domain expertise together with machine learning, produces its feature vector representation. Such settings include: face recognition [30,60], biometric identification [9,23,31], patient data search in a hospital [6,62] and many others. One can pose the Secure k-NNS problem, which has the same functionality as the k-NNS problem, and at the same time preserves the privacy of the input: the server-who holds the datasetshould learn nothing about the query or the result, while the client-who has the query-should not learn anything about the dataset besides the k-NNS result.Secure k-NNS is a heavily studied problem in a variety of settings (see Section 1.2 for the related work). Second, we design a new k-NNS algorithm tailored to secure computation, which is implemented using a combination of Homomorphic Encryption (HE), Garbled Circuits (GC) and Distributed Oblivious RAM (DORAM) as well as the above top-k protocol. This is an appropriate model for parties that in general trust each other (e.g., two companies or hospitals) but need to run a secure protocol due to legal restrictions. Besides, any semi-honest protocol can be reinforced to be maliciously secure (when parties are allowed to tamper actively with the sent messages), though it incurs a significant performance overhead [35]. Let us note that among the plaintext k-NNS algorithms, the clustering approach is far from being the best [7], but we find it to be particularly suitable for secure computation.For both algorithms, we use Additive Homomorphic Encryption (AHE) for secure distance computation and garbled circuit for the top-k selection. For AHE, we use the SEAL library [52] which implements the Brakerski/Fan-Vercauteren (BFV) scheme [33]. For GC we use our own implementation of Yao's protocol [70] with the standard optimizations [11,12,41,71], and for DORAM we implement Floram [27] in the read-only mode.Our specific contributions can be summarized as follows: • We propose a novel mixed-protocol solution based on AHE, GC, and DORAM that is tailored for secure k-NNS and achieves more than 31× performance improvement compared to prior art with the same security guarantees. To the best of our knowledge, all prior work on the secure k-NNS problem in the secure two-party computation setting is based on the linear scan, where we first compute the distance between the query and all of n database points, and then select k smallest of them. In our experiments, we find that the protocol from [53] that is carefully tailored to the matrix operations (and is, thus, significantly faster than the generic one used in [23]) is as fast as AHE on the fast network, but significantly slower on the slow network.Top-k selection SANNS chooses k smallest distances out of n by garbling a new top-k circuit that we develop in this work. Thus, by designing better algorithms and by carefully implementing and optimizing them, we scale up the datasets one can handle efficiently by more than two orders of magnitude.Other security models Some prior work considered the secure k-NNS problem in settings different from "vanilla" secure two-party computation. At a high-level, our system can provide a an efficient mechanism to retrieve similar elements to a query in any two-party computation model, e.g., database search, recommender systems, medical data analysis, etc. that provably does not leak anything beyond (approximate) answers. For example, our system can be used to retrieve similar images within a database given a query. We connect these primitives via secret sharing, which comes in two forms: an arithmetic secret sharing of a value x ∈ Z t is a pair (x C , x S ) of random values subject to x C + x S ≡ x mod t, whereas a Boolean (or XOR) secret sharing of x ∈ {0, 1} τ is a pair of random strings subject to x C ⊕ x S = x. Previous solutions for secure k-NNS require computing distance between the query point and all points in the database, which is undesirable for large databases. A (private-key) additive homomorphic encryption (AHE) scheme is private-key encryption scheme with three additional algorithms Add, CAdd and CMult, which supports adding two ciphertexts, and addition / multiplication by constants. Optimized linear scan Our first algorithm is a heavily optimized implementation of the linear scan: we compute distances from the query point to all the data points, and then (approximately) select k nn data points closest to the query. We now give a simplified version of the algorithm, and in Section 3.3 we explain why this simplified version is inadequate and provide a full description that leads to efficient implementation.At a high level, we first compute k-means clustering of the server's dataset with k = k c clusters. For every x i , it uses a "for" loop to insert x i into its correct location in the array, and discards the largest item to keep it of size k. Sorting networks Another approach is to employ sorting networks (e.g., AKS [1] or the Zig-Zag sort [36]) with O(bn log n) gates, which can be further improved to O(bn log k). Instead,Algorithm 1 Naive Top-k Computation function NAIVETOPK((x 1 , ID 1 ), . . . , (x n , ID n ), k) OPT = [MAXVAL] k idlist = [0] k for i ← 1 . . . n do x ← x i , idx ← ID i for j ← 1 . . . k do b ← (x < OPT [ j]) (OPT [ j], x) = MUX(OPT [ j], x, b) (idlist[ j], idx) = MUX(idlist[ j], idx, b) end for end for return (OPT, idlist) end function function MUX(a 1 , a 2 , b) # Returns (a 1 , a 2 ) for b = 0, and (a 2 , a 1 ) for b = 1 return (a 1 + (a 2 − a 1 ) · b, a 2 + (a 1 − a 2 ) · b) end function Algorithm 2 Approximate top-k selection function APPROXTOPK((x 1 , ID 1 ), . . . , (x n , ID n ), k, l) for i ← 1 . . . l do (M i , ID i ) ← ← MIN({(x (i·n/l+ j) , ID (i·n/l+ j) )} n/l j=1 ) end for return NAIVETOPK((M 1 , ID 1 ), . . . , (M l , ID l ), k) end functionwe propose a randomized construction of a circuit with O(bn) gates. Intuitively, if we set the number of bins l to be large enough, the above circuit should output a high-quality answer with high probability over π. For every n, 0 < δ < δ 0 , and k ≥ k 0 (δ), one can set the number of bins l = k/δ suchAlgorithm 3 Plaintext linear scan function LINEARSCANKNNS(q, {p i } n i=1, ID) # Uses hyperparameters r p , k nn , l s from Figure 1 Randomly permute the set {p i } for i ← 1, . . . , n dod i ← q − p i 2 d i ← d i 2 rp end for (v 1 , ID 1 ), . . . , (v k nn , ID k nn ) ← APPROXTOPK(d 1 , ID(p 1 ), . . . , (d n , ID(p n ), k nn , l s ) return ID 1 , . . . , ID k nn end functionthat the intersection I of the output of Algorithm 2 withMIN k n (x 1 , x 2 , . . . , x n ) contains at least (1 − δ)k entries in ex- pectation over the choice of π.This bound yields a circuit of sizeO(b · (n + k 2 /δ)). Namely, instead of storing full b-bit distances, we discard r low-order bits, and the overall number of gates in the selection circuit becomes O((b − r) · (n + kl)). Then, we find the smallest k (recall k denotes the number of centers) such that in the clustering of the dataset X found by the k-means algorithm at most α-fraction of the dataset lies in clusters of size more than m. During the query stage, we find u i clusters from the i-th group with the centers closest to the query point, then we retrieve all the data points from the corresponding ∑ T i=1 u i clusters, and finally from these retrieved points we select k nn data points that are closest to the query.We now describe one further optimization that helps to speed up the resulting k-NNS algorithm even more. For the linear scan, we use the approximate top-k selection to return the k nn IDs after computing distances between query and all points in the database.For the clustering-based algorithm, we use approximate top-k selection for retrieving u i clusters in i-th group for all i ∈ {1, . . . , T }. Then, we compute the closest k nn points from the query to all the retrieved points using the naive top-k algorithm. On a high level, our secure protocols implement plaintext algorithms 3 and 4, which is color-coded for reader's convenience: we implemented the blue parts using AHE, yellow parts using garbled circuit, and red parts using DORAM. Sample an array w of size k with random entries in {0, 1} b pid , output c ⊕ w to the client, and w to the server. If returnVal is true, sample a random array s of size k in Z 2 t , output b − s to client and s to the server. Compared to [40], which uses BFV for matrix-vector multiplications, our approach avoids expensive ciphertext rotations. Also, we used the coefficient encoding and a plaintext space modulo a power of two instead of a prime. Then the server computes an encryption h(x) = ∑ i f i g i , masks h(x) with a random polynomial and sends back to the client, so they hold secret shares of q, p j modulo t. Then, secret shares of Euclidean distances modulo t can be reconstructed via local operations.Note that we need to slightly modify the above routine when computing distances of points retrieved from DORAM. Since the server does not know these points in the clear, we let client and server secret share the points and their squared Euclidean norms.Public Parameters: coefficient bit length b c , number of items in the database n, dimension d, AHE ring dimension N, plain modulus t, ID bit length b pid , bin size l s . Client sends c i = AHE.Enc(sk, q [i]) for 1 ≤ i ≤ d to the server. 3 We implemented Floram with a few optimizations described below.u A j ⊕ u B j = 1 iff j = i,Precomputing OT To run FSS, the parties have to execute the GC protocol log 2 n times iteratively which in turn requires log 2 n set of Oblivious Transfers (OTs). We use Beaver OT precomputation protocol [10] which allows to perform all necessary OTs on random values in the beginning of FSS evaluation with a very small additional communication for each GC invocation.Public Parameters: coefficient bit length b c , number of items in the database n, dimension d, AHE ring dimension N, plain modulus t. Clustering hyperparameters: T , k i c , m, u i , s, l i , l s , b c , r c and r p . The server performs two independent random shuffles on the cluster centers and stash points. For each i ∈ {1, . . . , T },• The client and server use line 3-5 in Figure 6 to compute secret shares of the vector (||q − c i j || 2 2 ) j . In our case, these memory accesses are non-adaptive, hence we can fuse these accesses and reduce the number of rounds to log 2 n which has significant effect in practice. We implement secure top-k selection using garbled circuit while we made some further optimizations to improve the performance. It is done once per dataset and takes several hours (with the bottleneck being the vanilla k-means clustering described in Section 2.5). Note that the Deep1B-1M and Deep1B-10M datasets are normalized to lie on the unit sphere.Note that all four datasets have been extensively used in nearest neighbors benchmarks and information retrieval tasks. Accuracy In our experiments, we require the algorithms to return k nn = 10 nearest neighbors and measure accuracy as the average portion of correctly returned points over the set of queries ("10-NN accuracy"). For Deep1B, the coordinates are real numbers, and we quantize them to 8-bit integers uniformly between the minimum and the maximum values of all the coordinates. We use the total number of AND gates in the top-k and the ORAM circuits as a proxy for both communication and running time during hyperparameter search phase (this is due to the complexity of garbling a circuit depending heavily on the number of AND gates due to the Free-XOR optimization [41]). We make several observations:• On all the datasets, clustering-based algorithm is much faster than linear scan: up to 12× over the fast network and up to 8.2× over the slow network. • As a result, when we move from fast to slow network, the time for distance computation stays essentially the same, while the time for top-k and ORAM goes up dramatically. Overall, the multi-thread mode yields query time under 6 seconds (taking the single-threaded OT phase into account) for our biggest dataset that consists of ten million 96-dimensional vectors. However, even if we use TFHE [22], which is by far the most efficient FHE approach for highly-nonlinear operations, it will still be several orders of magnitude slower than garbled circuits, since TFHE requires several milliseconds per gate, whereas GC requires less than a microsecond.Distance Computation The most efficient way to compute n Euclidean distances securely, besides using the BFV scheme, is arithmetic MPC [23] based on oblivious transfer (one other alternative used in many prior works [9,[29][30][31]60] is Paillier AHE scheme, which is known to be much less suitable for the task due to the absence of SIMD capabilities [40]). One issue with a fair comparison with the prior work is that they are done before the recent MPC and HE optimizations became available. This gives us the lower bound on the running time of 578 seconds on the fast network and 6040 seconds on the slow network, and the lower bound of 240 GB on the communication.Overall, this indicates that our linear scan obtains a speedup of 1.46× on the fast network and 3.51× on the slow network. In this work, we design new secure computation protocols for approximate k-Nearest Neighbors Search between a client holding a query and a server holding a database, with the Euclidean distance metric. We highlight some directions for future work: • Our construction is secure in the semi-honest model, but it would be interesting to extend our protocols to protect against malicious adversaries which can deviate from the protocol. • One possible future direction is to implement other sublinear k-NNS algorithms securely, most notably Locality-Sensitive Hashing (LSH) [4], which has provable sublinear query time and is widely used in practice. For instance, the client may try to locate individual points in a dataset by asking several queries that are perturbations of each other and checking if the point of interest ends up in the answer. First, we never leak more than the union of the sets of points closest to the query in the clusters whose centers are closest to the query. Second, if the dataset is clusterable (i.e., can be partitioned into clusters with pairwise distances being significantly larger than the diameters of the clusters) and queries are close to clusters, then the clustering based k-NNS algorithm is exact and there is no additional leakage due to approximation. In particular, we looked for a PRF with low number of AND gates in order to decrease the communication between the parties when it is evaluated in GC (in the Free-XOR setting). We do not report the number of AND gates for LowMC: they should be comparable to the estimates we have for Kreyvium for an optimal choice of the parameters.While our approach is more efficient in GC with respect to Floram, the plaintext evaluation of Kreyvium is slower than the (highly optimized) hardware implementation of AES. For this sampling model, it is not hard to see that the desired expectation of the size of the intersection I is:E [|I |] = l · Pr[U i contains at least one of the top-k elements] = l · 1 − 1 − 1 l k, where the first step follows from the linearity of expectation, and the second step is an immediate calculation. The final bound is at least 1 − δ provided that k is large enough. A two-party functionality is a possibly randomized function f : {0, 1} * × {0, 1} * → {0, 1} * × {0, 1} * , that is, for every pair of inputs x, y ∈ {0, 1} n , the output-pair is a random variable ( f 1 (x, y), f 2 (x, y)). This work was partially done while all the authors visited Microsoft Research Redmond.The second-named author has been supported in part by ERC Advanced Grant ERC-2015-AdG-IMPaCT, by the FWO under an Odysseus project GOH9718N and by the CyberSecurity Research Flanders with reference number VR20192203.