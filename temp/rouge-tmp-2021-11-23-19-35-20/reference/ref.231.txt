CEAL stands out from existing approaches in three significant aspects: (1) eliminates the need for hand curated image generation rules by learning a generator model that imitates the style and domain of fingerprint images from a large collection of sample images , hence enabling easy customizability, (2) operates within limits of the visual discriminative ability of human perception , such that the learned fingerprint image generator avoids mapping distinct keys to images which are not distinguish-able by humans, and (3) the resulting model deterministically generates realistic fingerprint images from an input vector, where the vector components are designated to control visual properties which are either readily perceptible to a human eye, or imperceptible, yet necessary for accurately modeling the target image domain. Thus, even though such solutions use an additional source of entropy (e.g., PRNG), they have not been designed to generate humandistinguishable images.The user studies of Tan et al. [50] that compare multiple text and visual KFG solutions suggest that VKFGs can speed up the verification of key fingerprints. This endows CEAL with resilience to adversaries that exceeds the strength assumed in state-of-the-art attacks (of Tan et al. [50] and Dechand et al. [14]). In the rest of this section, we provide a formal definition of the visual fingerprint problem, and introduce mechanisms which we have used to build our solution.We define the set of RGB images I , and a function HPD ratio : I × I → [0,1] that captures the proportion of experiments where humans would perceive the pair of images to be distinguishable. The V w is not able to guarantee that key pairs will be distinguishable if their d H is within d, i.e., E(HPD ratio (I i , I j ) | d H (K i , K j ) < d) < 1 − ε.However, for key pairs whose d H value is at least d, V w is able to guarantee human distinguishability, i.e., ∀K i , K j ∈ {0, 1} γ , and I i = V w (K i ), andI j = V w (K j ), we have d H (K i , K j ) ≥ d ⇐⇒ HPD ratio (I i , I j ) = 1.Weak-to-strong problem decomposition. Visual key fingerprints can also be used for device pairing (e.g., Bluetooth Secure Simple Pairing using ECDH [39]), by having the user visually compare visual key fingerprint images of device keys, displayed on both paired devices.The dependence of the HPD model performance on human annotations can be used to setup a mechanism which not only provides a non-cognitive human user detection, but also further improves the HPD. The conventional GAN consists of two competing neural networks: (1) a generator network (G) that transforms the input latent vector into an image, and (2) a discriminator network (D) that differentiates synthetic images, generated by G, from real images in a training dataset. We propose instead a disentanglement of major from minor components: decompose the latent vector into major and minor components, and train major components to encode information about human distinguishability, and the minor components to encode image realism properties. That is, the network takes as input a latent vector, and produces a realistic image, human-distinguishable from other images generated from latent vectors that are in Figure 3: The CEAL approach: Train a generator to convert a latent vector to a realistic image, human-distinguishable from other generator produced images. We train CL-GAN's generator network G-CEAL, using two classifiers (see Figure 4): (1) the CL-GAN discriminator (D-CEAL) that is trained to differentiate synthetically generated images by G-CEAL from a dataset of real images, and (2) the HPD classifier, trained to estimate the likelihood that a human will label a pair of images as either identical or different.In the following, we first describe the HPD employed by CL-GAN, then detail the training process of CL-GAN. Experimentally investigating which representation performs best for the problem of image matching in HPD is hence a typical feature selection task in machine learning.Specifically, we employ a transfer learning approach using the Inception.v1 model [49], trained for image classification tasks on the ImageNet dataset [15] (1.2 million images from 1, 000 categories). r ∈ [0, 1] is the weight (importance) assigned to the positive (different) class and L(θ,Y, I 1 , I 2 ) = 1 2 (1 − r)(1 −Y )(D w 2 ) + 1 2 rY (max(0, µ − D w ))µ ∈ R, µ > 1 is a margin.After training the 3 additional layers in the twin Siamese network using contrastive loss, the network has learned to differentiate between the input image pairs, i.e. generate distant representations (O 1 and O 2 ) for dissimilar images and similar representations for similar images. To address this problem, we leverage recent successes in learning disentangled representations [10,11,16,28,31], to conjecture that we can decompose the latent vector into (1) a subset of major components, that when changed individually, produce human-perceptible changes in the generated images, and (2) the remaining, minor components, that encode relatively imperceptible characteristics of the images, see Figure 6. To achieve this, in each adversarial training epoch, we train G-CEAL using 3 steps, to generate (1) visually distinguishable images when the values of individual major components in the latent vectors are changed (flipped between 1 and -1) and (2) visually indistinguishable images when the values for minor components are flipped (see § 7.2). This loss is an indicator of how realistic and visually similar the generated images are, compared to the images in the real image dataset used for training D-CEAL. For this, define the HPD loss for this pair to be: HPD loss (v 1 , v 2 ) = cross_entropy(0, HPD predict (M 1 , M 2 )). To achieve this, we train G-CEAL also using the output (i.e., real vs. fake) issued by the D-CEAL discriminator for the G-CEAL-generated images of the previous epoch. We now describe the input mapper module, that solves the second sub-problem of Section 3: convert input keys into codes that are at Hamming distance at least d from each other.Specifically, as also illustrated in Figure 6, the input mapper takes as input a key (e.g., public key, shared key, Bitcoin address, IP address, domain name) and outputs a latent vector L of length λ, i.e., a code word at Hamming distance at least d from the code word of any other input key. We then used the vanilla DCGAN to generate two datasets of synthetic image pairs (see below) and collected their labels using Amazon Mechanical Turk (MTurk) workers. Thus, since our objective is to collect labeled data to be used for identifying the boundaries of human visual distinguishability, we generate image pairs from key fingerprints that are only different in 1 component.We followed an IRB-approved protocol to recruit 500 adult workers located in the US, to label 558 unique image pairs. We used proportional sampling to divide the total of 140 image pairs (100 different, 40 identical) into 4 groups of size 35 (25 assumed different, 10 assumed identical). At the completion of the above study, we identified the index of components in the input latent vector whose corresponding generated images were labeled with the highest error rates by workers. Table 2: Size of 6 generated image pair datasets, of either "identical", "different" or "mixed" image pairs, used to train the HPD classifier. In order to train the HPD to correctly classify visually similar, but random noise images, as "identical", we generated an unrealistic image dataset of 11,072 image pairs using a poorly trained vanilla DCGAN:(1) 10,048 image pairs using a vanilla DCGAN trained for only 1400 iterations, i.e., less than an epoch, and (2) 1,024 image pairs using the same vanilla DCGAN trained for 3600 iterations (slightly more than an epoch). We generated 1024 image pairs with c = 0.5, 3008 pairs with c = 0.6 and 3008 pairs with c = 0.7, for a total of 7,040 image pairs. To measure the similarity between colors we used the Delta E CIE 2000 [47] score, representing colors that are perceived to be different by humans [45]. We set this percentage experimentally: we generated 500 image pairs using input vector pairs that differ in x ∈ [2, 20] percent of their components, then manually compared them for visual equality. Specifically, we randomly split each synthetic dataset (except the ground truth human perception set), into training ( 80% of samples) and holdout ( 20%) sets: we used the training sets to train the HPD classifier, then tested its performance over the holdout sets. Therefore, this classifier would be preferred by an attacker, to identify potentially successful attack samples for a target CEAL image.We note that the high FNR of our HPD models is mostly a problem for the adversary. Further, we also experimented using CL-GAN variants with different architectures (e.g., different number of neurons in the first layer of G-CEAL, 8,192 and 16,384) and values for hyper parameters, including λ and the number of major and minor components.We also performed a grid search in the parameters of the CL-GAN including (1) the input size (λ ∈ 64, 128, 256, 512), (2) the number of major and minor components ( λ 2 , and (3) the α ∈ [25,75] with step size 5, in the loss functions of the CL-GAN generator (see Equation 3). For each target key, we generated an attack key by randomly flipping (i.e., 1 vs. -1) the values of d major components, then generated the CEAL images corresponding to the (target, attack) pairs. To ensure that major components of input vector to G-CEAL are at least in d Hamming distance of each other, we use a BCH (n=255, k=123, t=19) encoder to transform a key of length γ = 123 into the values for major components. Thus, the major components in the latent vector of any CEAL images are at least 39 Hamming distance apart.Based on the above setting, CEAL accepts binary key fingerprints of length γ = 123 bits. We investigate Vash [1], identified as a state-of-theart VKFG in terms of usability and attack detection [50], and report vulnerabilities that we identified ( § 10.4). One image was shown on the top left of the screen, the other on the bottom right.We conducted several studies, each needing labels for a different number of image pairs (e.g., the CEAL resilience to attacks, or the study comparing CEAL and Vash, see below). We used majority voting [32] to aggregate the labels assigned by the workers to each image pair, and produce the final human-assigned label. Specifically, for each value of d ∈ {1, 2, 3, ..., γ}, we have built an attack dataset as follows: We generated 1 million random "target" inputs, and, for each target input, we randomly selected an "attack" string that is within Hamming distance d from the target. As expected, the number of broken CEAL images decreases as the Hamming distance between the target and attack binary key fingerprints increases.We note that the KFG evaluation performed by Tan et al. [50], assumed an adversary able to perform 2 60 brute force attempts, which is similar to the effort required to control 122 of 123 bits of the input hash key, required by a (γ, 1)-attack. This model identified 13 image pairs as identical.We used the procedure described in §10.2 to label the 295 pairs of images that were identified by HPD_model_1 for (γ, d) attack and 13 pairs of images identified for (γ, 1) attack using 31 MTurk workers. Vash [1] is an open source implementation for random art [41], that converts input binary strings into structured, visual fingerprints. We generated thus 10 image pairs for each of the 12 combinations of q and n.We used the procedure of Section 10.2 to label these pairs using 40 human workers. Then, separately for these datasets, we used the HPD_model_1 to predict if all pairwise images are humandistinguishable, i.e., using a total of 49,995,000 comparisons per dataset.Since HPD_model_1 was not trained on Vash images, we sought to estimate its performance on Vash images. We found 6 such pairs.We then used the human workers and process described in § 10.2 to confirm these 150 Vash and 6 CEAL image pairs, i.e., with each image pair being labeled by 3 humans. The average comparison time over Vash attack images was 3.03s (M=1.4s, SD=5.42s), and for CEAL it was 2.73s (M=1.83s, SD=2.33s). Thus, we expect adoption of CEAL (e.g., the CAPTCHA application of § 4) would increase interest in research of models for the limits of human visual perception. Mechanical Turk workers also have different goals (minimize their time investment, maximize financial gains) which may differ from those of regular key fingerprint based authentication users, i.e., not only minimize time investment, but also correctly detect attacks. An attacker who has gained access to the CEAL network weights can leverage adversarial machine learning (e.g. gradient based) techniques to infer the input string from a target output CEAL image.