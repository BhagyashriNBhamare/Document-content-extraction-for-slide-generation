We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing.A recent work by Narayan et al. (2017) introduced a dataset, evaluation method and baseline systems for the task, naming it "Split-andRephrase". However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about relevant entities, but these facts were not mentioned in the input sentence; (2) some facts are repeated-the same fact is mentioned in multiple output sentences; and (3) some facts are missingmentioned in the input but omitted in the output.The model learned to memorize entity-fact pairs instead of learning to split and rephrase. We also establish a stronger baseline by extending the SEQ2SEQ approach with a copy mechanism, which was shown to be helpful in similar tasks ( Gu et al., 2016;Merity et al., 2017;See et al., 2017 In parallel to our work, an updated version of the dataset was released (v1.0), which is larger and features a train/test split protocol which is similar to our proposal. 2 We split each prediction to 1 https://github.com/biu-nlp/ sprp-acl2018 2 Note that this differs from "normal" multi-reference BLEU (as implemented in multi-bleu.pl) since the number of references differs among the instances in the test- Analysis We begin analyzing the results by manually inspecting the model's predictions on the validation set. Inspecting the attention weights (Figure 1) reveals a worrying trend: throughout the prediction, the model focuses heavily on the first word in of the first entity ("A wizard of Mars") while paying little attention to other cues like "hardcover", "Diane" and references of a specific complex sentence, and then average these numbers. Every RDF triplet (a complete fact) is represented only in one of the splits.While the set of complex sentences is still divided roughly to 80%/10%/10% as in the original split, now there are nearly no simple sentences in Table 4: Statistics for the RDF-based data split the development and test sets that appear verbatim in the train-set. Once the above distribu- 6 The updated dataset (v1.0, published by Narayan et al. after this work was accepted) follows (2) above, but not (1).7 https://github.com/biu-nlp/ sprp-acl2018 Table 5: Results over the test sets of the original, our proposed split and the v1.0 split tions are computed, the final probability for an output word w is:BLEU #S/C #T/S originalp(w) = p(z = 1)p copy (w) + p(z = 0)p sof tmax (w)In case w is not present in the output vocabulary, we set p sof tmax (w) = 0. On the v1.0 split the results are similar to those on our split, Table 6: Predictions from the COPY512 model, trained on the new data split.in spite of it being larger (1,331,515 vs. 886,857 examples), indicating that merely adding data will not solve the task.Analysis We inspect the models' predictions for the first 20 complex sentences of the original and new validation sets in Table 7. On the original split, while SEQ2SEQ128 mainly suffers from missing information, perhaps due to insufficient memorization capacity, SEQ2SEQ512 generated the most unsupported sentences, due to overfitting or memorization. We strongly encourage future research to evaluate on our proposed split or on the recently released version 1.0 of the dataset, which is larger and also addresses the overlap issues mentioned here.