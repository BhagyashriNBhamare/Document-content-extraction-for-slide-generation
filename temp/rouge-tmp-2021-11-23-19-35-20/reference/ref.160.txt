From image and face recognition, to self-driving cars, knowledge extraction and retrieval, and natural language processing and translation, deep learning has produced game-changing applications in every field it has touched.While advances in deep learning seem to arrive on a daily basis, one constraint has remained: deep learning can only build accurate models by training using large datasets. In addition, the process of training large, accurate models (often with millions of parameters) requires computational resources that can be prohibitive for individuals or small companies. For example, Google's InceptionV3 model is based on a sophisticated architecture with 48 layers, trained on ∼1.28M labeled images over a period of 2 weeks on 8 GPUs.The prevailing consensus is to address the data and training resource problem using transfer learning, where a small number of highly tuned and complex centralized models are shared with the general community, and individual users or companies further customize the model for a given application with additional training. Today, transfer learning is recommended by most major deep learning frameworks, including Google Cloud ML, Microsoft Cognitive Toolkit, and PyTorch from Facebook.Despite its appeal as a solution to the data scarcity problem, the centralized nature of transfer learning creates a more attractive and vulnerable target for attackers. Lack of diversity has amplified the power of targeted attacks in other contexts, i.e. increasing the impact of targeted attacks on network hubs [21], supernodes in overlay networks [54], and the impact of software vulnerabilities in popular libraries [71,22]. • We explore and develop multiple defense techniques against attacks on transfer learning models, including defenses that alter the student model training process, that alter inputs prior to classification, and techniques that introduce redundancy using multiple models.Transfer learning is a powerful approach that addresses one of the fundamental challenges facing the widespread deployment of deep learning. Then the student model is trained using its own dataset, while the first K layers are "frozen", i.e. their weights are fixed, and only weights in the last N − K layers are updated.The first K layers (referred to as shallow layers) are frozen during training because outputs of those layers already represent meaningful features for the student task. We target facial recognition, where the student task is to recognize a set of 65 faces, and uses a well-performing face recognition model called VGGFace [11] as teacher model. Given a source image, the attacker applies a small perturbation so that it is misclassified by the victim DNN into either a specific target class, or any class other than the real class. Most blackbox attacks either use queries to test intermediate adversarial samples and improve iteratively [55], or try to reverse-engineer decision boundaries of the DNN and build a replica, which can be used to craft adversarial samples [46]. A targeted attack aims to misclassify the adversarial image into a specific target class, whereas a non-targeted attack focuses on triggering misclassification into any class other than the real class. Attack Model.In the context of our definitions in Section 2, our attack assumes white-box access to teacher models (consistent with common practice today) and black-box access to student models. We also assume the attacker does not know the Student training dataset, and can use only limited queries (e.g., 1) to S. Apart from a single adversarial sample to trigger misclassification, we expect no additional queries to be made during the pre-attack process. We will discuss the impact on performance, and propose techniques to extract such information from the Student using a few additional queries.Insight and Attack Methodology. Using the Teacher model, attacker computes perturbations that mimic the internal representation of the target image at layer K. Internal representation is captured by passing the target image as input to the Teacher, and using the values of the corresponding neuron outputs at layer K.Our key insight: is that (in feedforward networks) since each layer can only observe what is passed on from the previous layer, if our adversarial sample's internal representation at layer K perfectly matches that of the target image, it must be misclassified into the same label as the target image, regardless of the weights of any layers that follow K.This means that in the common case of feature extractor training, if we can mimic a target in the Teacher model, then misclassification will occur regardless of how much the Student model trains with local data. Finally, it is hard in practice to perfectly mimic the internal representation, since we are limited in our level of possible perturbation, in order to keep adversarial changes indistinguishable by humans. d(x ′ , x s ) is a distance function measuring the amount of perturbation added to x s . The optimization problem is formulated as follows.min min i∈I {D(T K (x ′ s ), T K (x ti ))} s.t. d(x ′ s , x s ) < P(2)Measuring Adversarial Perturbations.As men- tioned before, d(x ′ s , x s )is the distance function used to measure the amount of perturbation added to the image. Therefore, we use another metric, called DSSIM, which is an objective image quality assessment metric that closely matches with the perceived quality of an image (i.e. subjective assessment) [65,66]. The key idea is that humans are sensitive to structural changes in an image, which strongly correlates with their subjective evaluation of image quality. DSSIM values fall in the range [0,1], where 0 means the image is identical to the original image, and a higher value means the perceived distortion will be higher. To constrain input pixel intensity within the correct range ( [0,255]), we transform intensity values into tanh space [17]. The Student is trained on the VGG Flowers dataset [9] containing 6, 149 images from 102 classes, and comes with a testing dataset of 1, 020 images.These tasks represent typical scenarios users may face during transfer learning. Based on these results, we build the Student model for each task using the transfer method that achieves the highest classification accuracy (marked in bold in Table 1). To evaluate targeted attacks, we randomly sample 1K source and target image pairs to craft adversarial samples, and measure the attack success rate as the percentage of attack attempts (out of 1K) that misclassify the perturbed source image as the target. Figure 3 includes 6 randomly selected successful targeted attack samples for interested readers to examine.It should be noted that an attacker could improve attack success by carefully selecting a source image similar to a target image. As expected, smaller budget results in lower attack success rate, as there is less room for the attacker to change images and mimic the internal representation. Their perturbation budgets are set to 0.005 (L 2 =9.035), 0.01 (L 2 =7.77), and 0.003 (L 2 =13.52), respectively. These values are empirically derived by the authors to produce unnoticeable image perturbations.Overall, the attack is effective in Iris, with a targeted attack success rate of 95.9% and non-targeted success rate of 100%. Figure 5(a) and Figure 5(b) show targeted and non-targeted success rates when attacking different layers.For both Face and Iris, the attack is the most effective when targeting precisely the N − 1 th (15th) layer, which is as expected since both use Deep-layer Feature Extractor. At layer 13 and above, the attack success rates are above 88.4% for Face, and 95.9% for Iris. Therefore, given a fixed perturbation budget, the error in mimicking internal representations is much higher at shallow layers, resulting in lower attack success rates.An unexpected result is that for Iris, the success rate for non-targeted attacks remains close to 100% regardless of the attack layer choice. A more detailed analysis shows that this is because Iris recognition is highly sensitive to input noise. In Section 5, we present a technique to determine whether Deep-layer Feature Extractor is used for transfer and to identify the Teacher model, using a few queries on the Student model. In this case, the attacker should focus on the N − 1 th layer to achieve the optimal attack performance.If the Student is not using Deep-layer Feature Extractor, the attacker can try to find the optimal attack layer by iteratively targeting different layers, starting from the deepest layer. Thus another potential attack on transfer learning is to use existing white-box attacks on the Teacher to craft adversarial samples, which are then transferred to the Student. Specifically, today's deep learning services (e.g. Google Cloud ML, Facebook PyTorch, and Microsoft CNTK) already help customers generate student models from a suite of teacher models. We address this challenge by designing a fingerprinting approach that feeds a few query images on the student model to identify the teacher model, allowing us to effectively attack the student models produced by today's deep learning services. This is a practical assumption because for common deep learning tasks there are only a limited set of high quality, pre-trained models that are publicly available. The prediction result (before softmax) of an input image x can be expressed as,S(x) = W N × T N−1 (x) + B N(3)where W N is the weight matrix of the dense layer, B N is the bias vector, and T N−1 (.) , a fingerprinting image of a Teacher model A, when fed to a Student model derived from a different Teacher model B, is unlikely to produce an all-zero vector T N−1 (x). To validate our approach, we produce five additional Student models using multiple popular public Teacher models 3 . Next, for each candidate Teacher model, we craft and feed 10 fingerprinting images to the target student model and compute the average Gini coefficient of S(x). Our methodology holds for any activation function. Today, popular Machine Learning as a service (MLaaS) platforms [67] (e.g., Google Cloud ML) and deep learning libraries (e.g., PyTorch, Microsoft CNTK) already recommend transfer learning to their customers. In this MLaaS platform, users can train deep learning models in the cloud and maintain it as a service. Specifically, the tutorial suggests Deep-layer Feature Extractor as the default transfer learning method, and the provided sample code does not offer control parameters or guidelines to use other transfer approaches or Teacher models (one has to modify the code to do so). PyTorch is a popular open source DL library developed by Facebook. PyTorch hosts a repository of 6 image classification Teacher models that users can plug into their transfer process.Again we follow the tutorial and verify that Student models trained using Deep-layer Feature Extractor on PyTorch are vulnerable. Our attack is feasible because each service only hosts a small number of deep learning Teacher models, making it easy to get access to the (small) pool of Teacher models. For example, Google Cloud ML advertises customers who have successfully deployed models using their transfer learning service [4]. Thus any deviations from the Teacher model could render the attack ineffective.Here, we describe three different potential defenses that target different pieces of the Student model classification process. The intuition is that attackers have identified minimal alterations to the image that push the Student model over some classification boundary. We repeat this process 3 times for each image and use the majority vote as the final prediction result 7 , or a random result if all 3 predictions are different.We test this defense on all three tasks, Face, Iris, and Traffic Sign, by applying Dropout on test images as well as targeted and non-targeted adversarial samples 8 . However, non-targeted attacks are less affected, and attack success consistently remains higher than classification accuracy of normal samples, e.g. 92.47% when the classification accuracy is 91.4%. Detailed results are shown in the Appendix as Figure 14. Strengths and Limitations.The key benefit of this approach is that it can be easily deployed, without requiring changes to the underlying Student model. Thus, if we can make the Student's internal representation deviate from that of the Teacher for all inputs, the attack would be less effective. Our goal is to update layer weights and identify a new local optimum that provides comparable (or better) classification performance, and also be distant enough (on the error surface) to increase the dissimilarity between the Student and Teacher.To find such a new local optimum, we unfreeze all layers of Student and retrain the model using the same Student training dataset, but with an updated loss function formulated in the following way. This helps make sure that distance between important neurons contribute more to the total dis- 9 Recall that models using Full Model Fine-tuning are generally resistant to the attack. As classification accuracy drops from 98.55% to 95.69%, targeted attack drops significantly, from 92.6% to 30.87%. Attack success rate only falls to 94.83% for Iris, and remains consistently above classification accuracy.Finally, we note that retrained models are also robust against the Teacher fingerprinting technique. Finally, we consider using orthogonal models as a defense for adversarial attacks against transfer learning. Transfer Learning.In a deep learning context, transfer learning has been shown to be effective in vision [18,52,51,15], speech [34,63,30,20], and text [33,40]. Prior work on black box attacks query the victim DNN to gain feedback on adversarial samples and use responses to guide the crafting process [55]. x DisclosureWhile we did not perform any attacks on deployed image recognition systems, we did experiment with publicly available Teacher models from Google, Microsoft and the open source PyTorch originally started by Facebook. While we did not perform any attacks on deployed image recognition systems, we did experiment with publicly available Teacher models from Google, Microsoft and the open source PyTorch originally started by Facebook. To compute DSSIM, we use the implementation of multi-scale SSIM from TensorFlow and follow the recommended parameter configuration 11 .