With the prevalence of private user data stored on web sites, the risk posed by compromised renderer processes, and the advent of transient execution attacks like Spectre and Melt-down that can leak data via microarchitectural state, it is no longer safe to render documents from different web sites in the same process. In this paper, we describe our successful deployment of the Site Isolation architecture to all desktop users of Google Chrome as a mitigation for process-wide attacks. Many browsers shifted to a multi-process architecture that renders untrusted web content within one or more low-privilege sandboxed processes, mitigating attacks that aimed to install malware by exploiting a rendering engine vulnerability [43,51,70,76]. This is increasingly common now that the most exploitable targets of older browsers are disappearing from the web (e.g., Java Applets [64], Flash [1], NPAPI plugins [55]). We achieve this in a production setting using Site Isolation in Google Chrome, introducing OS process boundaries between web site principals.While Site Isolation was originally envisioned to mitigate exploits of bugs in the renderer process, the recent discovery of transient execution attacks [8] like Spectre [34] and Meltdown [36] raised its urgency. These attacks challenge a fundamental assumption made by prior web browser architectures: that software-based isolation can keep sensitive data protected within an operating system process, despite running untrustworthy code within that process. Full support for out-of-process iframes requires proxy objects and replicated state in frame trees, as well as updates to a vast number of browser features. While there are a set of limitations with its current implementation, we argue that Site Isolation offers the best path to mitigating the threats posed by compromised renderer processes and transient execution attacks.In this paper, Section 2 introduces a new browser threat model covering renderer exploit attackers and memory disclosure attackers, and it discusses the current limitations of Site Isolation's protection. Section 6 looks at the implications for the web's future and potential ways to address Site Isolation's current limitations. Overall, we answer several new research questions:• Which parts of a web browser's security model can be aligned with OS-level isolation mechanisms, while preserving compatibility with the web? Multi-process browsers have traditionally focused on stopping web attackers from compromising a user's computer, by rendering untrusted web content in sandboxed renderer processes, coordinated by a higher-privilege browser process [51]. However, current browsers allow attackers to load victim sites into the same renderer process using iframes or popups, so the browser must trust security checks in the renderer process to keep sites isolated from each other.In this paper, we move to a stronger threat model emphasizing two different types of web attackers that each aim to steal data across web site boundaries. For example, they might forge an IPC message to retrieve sensitive data associated with another web site (e.g., cookies, stored passwords). We aim to protect many types of content and state from the attackers described above, including the HTML contents of documents, JSON or XML data files they retrieve, state they keep within the browser (e.g., cookies, storage, saved passwords), and permissions they have been granted (e.g., geolocation, camera). Site Isolation is also able to strengthen some existing security practices for web application code, such as upgrading clickjacking [30] protections to be robust against com-promised renderers, as discussed in Section 5.1. We hope to allow some origins to opt into origin-level isolation, as discussed in Section 6.3.Cross-site subresources (e.g., JavaScript, CSS, images, media) are not protected, since the web allows documents to include them within an execution context. JavaScript and CSS files were already somewhat exposed to web attackers (e.g., via XSSI attacks that could infer their contents [26]); the new threat model re-emphasizes not to store secrets in such files. Most prior multi-process browsers, including Chrome, Edge, Safari, and Firefox, did not assign site-specific security principals to web renderer processes, and hence they did not enforce isolation boundaries between different sites at the process level. For example, an origin https://bar.foo.example.com:8000 corresponds to a site https://example.com. In such a model, it is still important to limit a dedicated renderer process to documents and data from its own site, but it is also necessary to prevent a shared process from retrieving data from one of the isolated sites. Due to web-visible events such as beforeunload and unload and the fact that a navigation request might complete without creating a new document (e.g., a download or an HTTP "204 No Content" response), the browser process must coordinate with both old and new renderer processes to switch at the appropriate moment: after beforeunload, after the network response has proven to be a new document, and at the point that the new process has started rendering the new page. The largest and most disruptive change for Site Isolation is the requirement to load cross-site iframes in a different renderer process than their embedding page. This section describes the challenges we overcame to make outof-process iframes functional and compatible with the web platform.Frame Tree. An example of a page including out-of-process iframes is shown in Figure 1 (a), containing three documents from a.com and one from b.com, and thus requiring two separate renderer processes. Note that the actual document and proxy objects live in renderer processes; the corresponding browser-side objects are stubs that track state and route IPC messages between the browser and renderer processes.For example, suppose the document in a.com/2 invokes window.parent.frames ["b"]. The renderer process will send the message to the corresponding Proxy A object for b.com/3 in the browser process. The browser process passes it to the current Document B object in this frame, which sends the message to the corresponding Document object in Renderer Process B. Similar message routing can support other cross-origin APIs, such as focus, navigation, or closing windows.State Replication. The compositing process must support many types of transforms that are possible via CSS, without leaking surface data to a cross-site renderer process.Often, many frames on a page come from the same site, and separate surfaces for each frame may be unnecessary. Figure 1 shows how a.com/1 and a.com/2 can be rendered in the same widget and surface without requiring compositing. Since a.com/4 is not contiguous with the other two a.com frames and its layout may depend on properties assigned to it by b.com/3 (e.g., CSS filters), it has a separate widget within Renderer Process A, and its surface must be composited within b.com/3's surface.Widgets are also used for input event routing, such as mouse clicks and touch interactions. First, moving cross-origin image handling out of the renderer process and preventing renderers from reading these surfaces would require a great deal of complexity in practice. Additionally, the feature must be careful to avoid leaking information to renderer processes (e.g., whether there was a match in a crosssite sibling frame), and it must be robust to renderer processes that crash or become unresponsive.These updates are required for many features that combine data across frames or that perform tasks that span multiple frames: supporting screen readers for accessibility, compositing PDFs for printing, traversing elements across frame boundaries for focus tracking, representations of the full page in developer tools, and many others. Otherwise, a document could access crosssite data by requesting such a URL from a <script>, <style>, or <img> tag. Changing the browser to block these libraries from cross-site documents would break compatibility with many existing sites.It may be desirable to require sites to correct their content types or proactively label any resources that need protection (e.g., with a new Cross-Origin-Resource-Policy header [21]), but such approaches would leave many existing resources unprotected until developers update their sites.Until such shifts in web site behavior occur, browsers with Site Isolation can use a best effort approach to protect as many sensitive resources as possible, while preserving compatibility with existing cross-site subresources. An exception is made for SVG, which is an XML data type permitted within <img> tags.Since many responses have incorrect content types, CORB requires additional confirmation before blocking the response from the renderer process. OP2 and IBOS use such sniffing to confirm a response is HTML [23,63], but this will block many legitimate JavaScript files, such as those that begin with HTML comments (i.e., "<!--"). In contrast, CORB relies on a new type of confirmation sniffing that looks at a prefix of the response to confirm that it matches the claimed content type and not a subresource [17]. For example, a response labeled as text/html starting with "<!doctype" would be blocked, but one starting with JavaScript code would not. (CORB attempts to scan past HTML comments when sniffing.) This is a default-allow policy that attempts to protect resources where possible but prioritizes compatibility with existing sites. For example, CORB allows responses through when they are polyglots which could be either HTML or JavaScript, such as:<!--/*--><html><body><script type="text/javascript"><!--//*/ var x = "This is both valid HTML and valid JavaScript."; //--></script></body></html> CORB skips confirmation sniffing in the presence of the existing X-Content-Type-Options: nosniff response header, which disables the browser's existing MIME sniffing logic. Thus, we recommend that web developers use this header for CORBeligible URLs that contain sensitive data, to ensure protection without relying on confirmation sniffing.If a cross-site response with one of the above confirmed content types arrives, and if it is not allowed via CORS headers [18], then CORB's logic in the network component prevents the response data from reaching the renderer process. For example, transient execution attacks might leak data from any cross-site documents present in the same process, but such attacks cannot send forged messages to the browser process to gain access to additional data. In normal execution, a renderer process has its own checks to avoid making requests for such data, so illegal requests can be interpreted by the browser process as a sign that the renderer process is compromised or malfunctioning and can thus be terminated before additional harm is caused. This was a significant 5-year effort that spanned approximately 4,000 commits from around 350 contributors (with the top 20 contributors responsible for 72% of the commits), changing or adding approximately 450,000 lines of code in 9,000 files.We needed to re-architect a widely deployed browser without adversely affecting users, both during development and when deploying the new architecture. For example, when a document embeds an example.com iframe and another browser tab already contains another example.com frame (either an iframe or a main frame), we consolidate them in the same process. When the number of processes is below this limit, main frames in independent tabs don't share processes; when above the limit, all new frames start reusing same-site processes when possible. These could utilize separate processes, but we choose to keep these cases inprocess as an optimization, focusing our attention on true cross-site content.Other design decisions that help reduce process count include isolating at a site granularity rather than origin, keeping cross-site images in-process, and allowing extensions to share processes with each other. To control memory overhead, we avoid spare processes on low memory devices, when the system experiences memory pressure, or when the browser goes over the soft process limit. Shipping Site Isolation in a production browser is challenging. Deploying these preliminary isolation modes provided a valuable source of bug reports and performance data (e.g., at least 24 early issues reported from enterprise policy users). To evaluate the effectiveness and practicality of deploying Site Isolation, we answer the following questions: (1) How well does Site Isolation upgrade existing security practices to mitigate renderer exploit attacks? The following web developer practices were vulnerable to renderer exploit attackers before Site Isolation but are now robust. Many sites use HTML, XML, and JSON to transfer sensitive data. We also analyzed security bugs reported for Chrome for 2014-2018 (extending the analysis by Moroz et al [41]) and found 94 UXSS-like bugs that allow an attacker to bypass the SOP and access contents of cross-origin documents. The team continues to welcome and fix such reports, since they still have value on mobile devices where Site Isolation is not yet deployed. This section compares the various web browser mitigation strategies for such attacks, evaluating their effectiveness against known variants.Strategy Comparison. Most major browsers reduced the granularity of APIs like performance.now to 20 microseconds or even 1 millisecond, introduced jitter to timer results, and even removed implicit sources of precise time, such as SharedArrayBuffers [59]. This strategy applies whether the attack targets data inside the process or outside of it, but it has a number of weaknesses that limit its effectiveness:• It is likely incomplete: there are a wide variety of ways to build a precise timer [35,58], making it difficult to enumerate and adjust all sources of time in the platform. • It is possible to amplify the cache timing result to the point of being effective even with coarse-grained timers [25,37,58]. Second, browser vendors pursued modifications to the JavaScript compiler and runtime to prevent JavaScript code from accessing victim data speculatively [37,48,65]. Rather than targeting the cache timing attack or disrupting speculation, Site Isolation assumes that transient execution attacks may be possible within a given OS process and instead attempts to move data worth stealing outside of the attacker's address space, much like kernel defenses against Meltdown-US [15,24]. Table 1 shows how both types of attacks are able to target data inside or outside the attacker's process, and thus both Spectre and Meltdown are relevant to consider when mitigating memory disclosure attacks.Site Isolation mitigates same-address-space attacks by avoiding putting vulnerable data in the same renderer process as a malicious principal. Site Isolation does not attempt to mitigate attacks targeting data in other processes or the kernel, such as the "Outside Process" variants in Table 1 and Microarchitectural Data Sampling (MDS) attacks [40,57,66]. Thus, applications that run code from untrustworthy principals (e.g., browsers) must align their architectures with OS-enforced abstractions to isolate these principals. Enabling Site Isolation can affect the browser's performance, so we evaluate its effect on memory overhead, latency, and CPU usage in the wild and in microbenchmarks. However, thanks to the process sharing heuristics described in Section 4.1.1, far fewer processes were used in practice, as shown in Figure 2. In practice, we see that total memory use increased only 12.6% at the 25th percentile, and only 8.6% at the 99th percentile. We use observed metrics from the field to study the combined impact of these changes in practice.Site Isolation significantly increased the percentage of navigations that cross a process boundary, from 5.73% to 56.0%. The current implementation uses slow path hit testing for mouse and touch events over out-of-process iframes, which results in small increases to input event latency. However, these measurements establish a baseline and provide a reproducible reference point for future research.To study a mix of the most popular (likely highly optimized) and slightly less popular sites, we selected the top site as well as the 50th-ranked site in Alexa categories for news, sports, games, shopping, and home, as well as google.com as the top overall URL. As expected, the relative memory overhead generally increases with the number of processes, peaking at 89% for wowprogress.com with 10 processes. For example, a heavier amazon.com site has a 5% overhead compared to seatguru.com's 31%, even though both require five processes. google.com does not have any cross-site iframes and requires no extra processes, but it shows a 4% increase in memory use due to the spare process that we maintain with Site Isolation, as explained in Section 4.1.3. In practice, this helps reduce memory overhead via process consolidation, while iframe-heavy sites like wowprogress.com may represent only a small part of users' browsing sessions. Other software systems that isolate untrustworthy code may require architecture changes to avoid leaking data via microarchitectural state. There are several options for protecting additional content, from using headers to protect particular responses, to expanding CORB to cover more types, to changing how browsers request subresources.First, web developers can explicitly protect sensitive resources without relying on CORB, using a Cross-Origin-Resource-Policy response header [21] or refusing to serve cross-site requests based on the Sec-Fetch-Site request header [71]. Developer outreach may also cut down on mislabeled subresources, eliminating the need for CORB confirmation sniffing.Third, recent proposals call for browsers to make crossorigin subresource requests without credentials by default [73]. Too many web sites rely on modifying document.domain to deploy origin isolation by default, but browsers may allow sites to opt out of this feature and thus become eligible for origin isolation [72]. Prior to this work, all major production browsers, including IE/Edge [76], Chrome [52], Safari [70], and Firefox [43], had multi-process architectures that rendered untrustworthy web content in sandboxed renderer processes, but they did not enforce process isolation between web security principals, and they lacked architectural support for rendering embedded content such as iframes out-of-process. Finally, while Gazelle, OP2, and IBOS have outof-process iframes, our work overcomes many challenges to support these in a production browser, such as supporting the full set of cross-process JavaScript interactions, challenges with painting and input event routing, and updating affected features (e.g., find-in-page, printing). We have shown that Site Isolation is practical to deploy in a production desktop web browser, incurring a 9-13% total memory overhead on real-world work-loads. Origin Isolation, where principals are defined as origins, offers stronger security guarantees at the cost of breaking document.domain compatibility and performance challenges due to a larger number of principals.As noted in Section 3.1, computing site URL for most HTTP(S) URLs is straightforward, but some web platform features require special treatment. These URLs embed an origin; e.g., blob:http://example.com/UUID addresses an inmemory blob of data controlled by the http://example.com origin. These forms of content utilize the web platform for rendering, so the browser must define principals for them. Each local URL (e.g., file:///homes/foo/a.html) is typically treated as its own origin by the browser, so each path could use a separate principal and process. This appendix lists a subset of Chrome features that needed to be updated to support out-of-process iframes, beyond those discussed in Section 3.4. • Cookie reads and writes (document.cookie, HttpOnly cookies). • Address bar origin. • Custom HTTP headers requiring CORS. • SameSite cookies.