Our framework makes use of virtual reality (VR) systems, incorporating along the way the ability to perform animations (e.g., raising an eyebrow or smiling) of the facial model, in order to trick liveness detectors into believing that the 3D model is a real human face. We argue that such VR-based spoofing attacks constitute a fundamentally new class of attacks that point to a serious weaknesses in camera-based authentication systems: Unless they incorporate other sources of verifiable data, systems relying on color image data and camera motion are prone to attacks via virtual realism. Indeed, stateof-the-art face identification systems can now outperform their human counterparts [36], and this high accuracy is one of the driving factors behind the increased use of face recognition systems.However, even given the high accuracy of modern face recognition technologies, their application in face authentication systems has left much to be desired. One prominent example is Android OS, which augmented its face authen-tication approach in 2012 to require users to blink while authenticating (i.e., as a countermeasure to still-image spoofing attacks). 2 These attacks underscore the fact that face authentication systems require robust security features beyond mere recognition in order to foil spoofing attacks.Loosely speaking, three types of such spoofing attacks have been used in the past, to varying degrees of success: (i) still-image-based spoofing, (ii) video-based spoofing, and (iii) 3D-mask-based spoofing. In that work, the use of liveness detection was proposed as a solution to thwarting video-based attacks by checking the consistency of the recorded data with inertial sensors. Thus, a 2D video of the victim would have to be captured under the exact same camera motion in order to fool the system.As mentioned in [34], 3D-printed facial reconstructions offer one option for defeating motion-based liveness detection schemes. Such an attack defeats color-image-and motion-based face authentication on a fundamental level because, with sufficient effort, a VR system can display an environment that is essentially indistinguishable from real-world input.In this paper, we show that it is possible to undermine modern face authentication systems using one such attack. Before delving into the details of our approach, we first present pertinent background information needed to understanding the remainder of this paper.First, we note that given the three prominent classes of spoofing attacks mentioned earlier, it should be clear that while still-image-based attacks are the easiest to perform, they can be easily countered by detecting the 3D structure of the face. Nevertheless, because of the threat this attack vector poses, much research has gone into detecting the textures of 3D masks [11]. We discuss each in turn.Texture-based approaches [11,25,37,40,54,60] attempt to identify spoofing attacks based on the assumption that a spoofed face will have a distinctly different texture from a real face. For instance, the approach of Li et al. [34] checks the consistency of movement between the mobile device's internal motion sensors and the observed change in head pose computed from the recorded video taken while the claimant attempts to authenticate herself to the device. The requested gestures help to defeat contemporary spoofing attacks.Take-away: For real-world systems, liveness detection schemes are often combined with motion-based approaches to provide better security protection than either can provide on their own. Users also do not have direct control over the accessibility of photos of themselves posted by other users, although they can remove ('untag') the association of such photos with their account.A notable use of social network photos for online security is Facebook's social authentication (SA) system [15], an extension of CAPTCHAs that seeks to bolster identity verification by requiring the user to identify photos of their friends. In the future, such an approach may help decrease the public accessibility of users' personal photos, but it is unlikely that an individual's appearance can ever be completely obfuscated from attackers across all social media sites and image stores on the Internet.Clearly, the availability of online user photos is a boon for an adversary tasked with the challenge of undermining face authentication systems. As noted in §2, in modern face authentication software, sophisticated liveness detection approaches are already in use, and these techniques thwart still-image spoofing attacks of the kind performed by Li et al. [33]. With this method, the reconstruction of a dense and accurate model often requires many consistent views of the surface from different angles; moreover, non-rigid variations (e.g., facial expressions) in the images can easily cause SFM methods to fail. The underlying variations fall on a continuum and capture both expression (e.g., a frowning-to-smiling spectrum) and identity (e.g., a skinny-to-heavy or a male-to-female spectrum). In this process, the main challenge is the localization of facial landmarks within the images, especially contour landmarks (along the cheekbones), which are half-occluded in non-frontal views; we introduce a new method for solving this problem when multiple input images are available.The end result of 3D reconstruction is a untextured (i.e., lacking skin color, eye color, etc.) facial surface. We next detail our process for building such a facial model from a user's publicly available internet photos, and we outline how this model can be leveraged for a VR-based face authentication attack. Once the texture is filled, we have a realistic 3D model of the user's face based on a single image.However, despite its realism, the output of stage  is still not able to fool modern face authentication systems. Therefore, we must also automatically correct the direction of the user's gaze on the textured mesh (stage ). Using this framework, an adversary can bypass both the face recognition and liveness detection components of modern face authentication systems. For our needs, SDM works well on most online images, even those where the face is captured at a low resolution (e.g., 40 × 50 pixels). In our experiments, the landmark extraction results are manually checked for correctness, although an automatic scoring system could potentially be devised for this task. The 68 extracted 3D point landmarks from each of the N input images provide us with a set of coordinates s i, j ∈ R 2 , with 1 ≤ i ≤ 68, 1 ≤ j ≤ N. The projection of the 3D points S i, j ∈ R 3 on the face onto the image coordinates s i, j follows what is called the "weak perspective projection" (WPP) model [16], computed as follows:s i, j = f j PR j (S i, j + t j ) ,(1)where f j is a uniform scaling factor; P is the projectionmatrix 񮽙 1 0 0 0 1 0 񮽙; R j is a 3 × 3 rotation matrix defined by the pitch, yaw, and roll, respectively, of the face relative to the camera; and t j ∈ R 3 is the translation of the face with respect to the camera. The identity axes capture characteristics such as face width, brow placement, or lip size, while the expression axes capture variations like smiling versus frowning. However, for contour landmarks marking the edge of the face in an image, the associated 3D point on the user's facial model is pose-dependent: When the user is directly facing the camera, their jawline and cheekbones are fully in view, and the observed 2D landmarks lie on the fiducial boundary on the user's 3D facial model. While their approach is efficient and robust against different face angles and surface shapes, it can only handle a single image and cannot refine the reconstruction result using additional images.Our solution to the correspondence problem is to model 3D point variance for each facial landmark using a pre-trained Gaussian distribution (see Appendix A). Due to the appearance variation across social media photos, we have to achieve this by mapping the pixels in a single captured photo onto the 3D facial model, which avoids the challenges of mixing different illuminations of the face. Instead, we follow the suggestion of Zhu et al. [63] and estimate facial illumination using spherical harmonics [61], then fill in texture details with Poisson editing [41]. This process is defined mathematically as∆ f = ∆g, s.t f | ∂ Ω = f 0 | ∂ Ω ,(3)where Ω is the editing region, f is the editing result, f 0 is the known original texture value, and g is the texture value in the editing region that is unknown and needs to be patched with its reflection complement. 3 into discrete form, we have|N p | f p − ∑ q∈N p ∩Ω f q = ∑ q∈N p ∩Ω f 0 q + (∆g) p ,(4)where N p is the neighborhood of point p on the mesh. To address this, we introduce a simple, but effective, approach to correct the gaze direction of our synthetic model (Figure 1, Stage ). We estimate this color distribution with a 3D Gaussian function whose three principle components can be computed as(b 1 , b 2 , b 3 ) with weight (σ 1 , σ 2 , σ 3 ), σ 1 ≥ σ 2 ≥ σ 3 > 0. Some of the liveness detection methods that we test require that the user performs specific actions in order to unlock the system. To mimic these actions, we can simply animate our facial model using a pre-defined set of facial expressions (e.g., from FaceWarehouse [8]). By interpolating the model's expression weight from 0 to α exp std , we are able to animate the 3D facial model to smile, laugh, blink, and raise the eyebrows (see Figure 6). This VR-based spoofing constitutes a fundamentally new class of attacks that exploit weaknesses in camera-based authentication systems.In the VR system, the synthetic 3D face of the user is displayed on the screen of the VR device, and as the device rotates and translates in the real world, the 3D face moves accordingly. As a result, the observed 3D facial motion will not agree with the device's inertial sensors, causing our method to fail on methods like that of Li et al. [34] that use such data for liveness detection.Fortunately, it is possible to track the 3D position of a moving smart phone using its outward-facing camera with structure from motion (see §2.3). In our experiments, we make use of a printed marker 3 placed on a wall in front of the camera, rather than tracking arbitrary objects in the surrounding scene; however, the end result is the same. The authenticating camera views the facial model on the VR display, and it is successfully duped into believing it is viewing the real face of the user. Moreover, we successfully test our proposed approach with the latest motion-based liveness detection approach by Li et al. [34], which is not yet available in commercial systems. The ages of the participants range between 24 and 44 years, and the sample consists of 6 females and 14 males. With their consent, we collected public photos from the users' Facebook and Google+ social media pages; we also collected any photos we could find of the users on personal or community web pages, as well as via image search on the web. Any images of subjects displayed in this paper was done with the consent of that particular volunteer.For our experiments, we manually extracted the region around user's face in each image. Two other users had frequent changes in facial hair styles -beards, moustaches, and clean-shaven -all of which we used for our reconstruction. We tested our approach on five advanced commercial face authentication systems: KeyLemon 5 , Mobius 6 , True Key [18], BioID [21], and 1U App 7 . In addition, the 1U App requests these actions in a random fashion, making it more resilient to video-based attacks. It is also possible that these five systems employ other advanced liveness detection approaches, such as texturebased detection schemes, but such information has not been made available to the public. Training Method # InstallsKeyLemon 3 Single video ∼100,000 Mobius 2 10 still images 18 reviews True Key 1 Single video 50,000-100,000 BioID 2 4 videos unknown 1U App 1 1 still image 50-100 All participants were registered with the 5 face authentication systems under indoor illumination. In particular, photos taken by professional photographers (e.g., wedding photos or family portaits) lead to highquality facial texturing. Third, we note that the least spoof-able users were not those who necessarily had a low number of personal photos, but rather users who had few forward-facing photos and/or no photos with sufficiently high resolution. Only a small number of photos are necessary in order to defeat facial recognition systems.We found that our failure to spoof the 1U App, as well as our lower performance on BioID, using social media photos was directly related to the poor usability of Social Media Spoof % Spoof % Avg. # Tries KeyLemon 100% 85% 1.6 Mobius 100% 80% 1.5 True Key 100% 70% 1.3 BioID 100% 55% 1.7 1U App 100% 0% - Table 2: Success rate for 5 face authentication systems using a model built from (second column) an image of the user taken in an indoor environment and (third and fourth columns) images obtained on users' social media accounts. Our impression is that 1U's singleimage user registration simply lacks the training data necessary to accommodate to different illumination settings. Even so, as evidenced by the second column in Table 2, our method still handily defeats the liveness detection modules of these systems given images of the user in the original illumination conditions, which suggests that all the systems we tested are vunerable to our VR-based attack.Our findings also suggest that our approach is able to successfully handle significant changes in facial expression, illumination, and for the most part, physical characteristics such as weight and facial hair. Specifically, we answer the question: what is the minimum resolution and maximum head rotation allowed in an uploaded photo before it becomes unusable for spoofing attacks like ours? To assess our ability to spoof face authentication systems when provided only low-resolution images of a user's face, we texture the 3D facial models of our sample users using an indoor, frontal view photo. The spoofing success rate for various image resolutions is shown in Figure 8. The spoofing success rate for a single input image as a function of head rotation is illustrated in Figure 9 (left). However, we argue that these high-resolution side-angle views can serve as base images for facial texturing if additional low-resolution frontal views of the user are available. Fortunately, as discussed in §3, the data consistency requirement is automatically satisfied with our virtual reality spoofing system because the 3D model rotates in tandem with the camera motion.Central to Li et al. [34]'s approach is to build a classifier that evaluates the consistency of captured video and motion sensor data. Since their code and training samples have not been made public, we implemented our own version of Li et al. [34]'s liveness detection system and trained a classifier with our own training data. For each class (real user data, video spoof data, and VR data), we report the average number (over 4 trials) of test samples classified as real user data. The first row shows the results when using real user data as positive samples and video spoof data as negative samples. In both cases, our approach defeats the liveness detector in 50% of trials, and the real user data is correctly identified as such less than 75% of the time.All three training configurations clearly point to the fact that our VR system presents motion features that are close to real user data. Of these, the first two could still be bypassed with additional adversary effort, while the third presents a significantly different hardware configuration that would require non-trivial alterations to our method.Light Projection The principle of using light projection for liveness detection is simple: Using an outwardfacing light source (e.g., the flashlight commonly included on camera-equipped mobile phones), flash light on the user's face at random intervals. However, structured light projection requires specialized hardware that typically is not found on smart phones and similar devices, which decreases the feasibility of this mitigation.Pulse Detection Recent computer vision research [2,58] has explored the prospect of video magnification, which transforms micro-scale fluctuations over time into strong visual changes. Infrared Illumination Microsoft released Windows Hello as a more personal way to sign into Windows 10 devices with just a look or a touch. 10 Nevertheless, the use of infrared illumination offers intriguing possibilities for the future.Takeaway In our opinion, it is highly unlikely that robust facial authentication systems will be able to operate using solely web/mobile camera input. Of more concern, however, is the increasing threat of virtual reality, as well as computer vision, as an adversarial tool. Then, the most probable parameters θ := ({ f j }, {R j }, {t j }, {α exp j }, α id ) can be estimated by minimizing the cost functionθ = argmax θ { 68 ∑ i=1 N ∑ j=1 1 (σ s i ) 2 ||s i, j − f j PR j (S i 񮽙 , j + t j )|| 2 + N ∑ j=1 (α exp j ) 񮽙 Σ −1 exp α exp j + (α id ) 񮽙 Σ −1 id α id }. (σ s i ) 2 is the variance of alignment error of the i-th landmark and is obtained from a separate training set consisting 20 images with hand-labeled landmarks.