A large body of research has however shown the strong limits of the de-identification release-and-forget model, where data is anonymized and shared. Based on the idea of "sticky noise", Diffix has been recently proposed as a novel query-based mechanism satisfying alone the EU Article 29 Working Party's definition of anonymization. Using this attack on four real-world datasets, we show that we can infer private attributes of at least 93% of the users in the dataset with accuracy between 93.3% and 97.1%, issuing a median of 304 queries per user. While scientists have compared the impact of modern large-scale datasets of human behaviors to the invention of the microscope [1], numerous scandals, such as the recent Cambridge Analytica debacle [2] highlight the importance of privacy and data protection for the general public and modern societies.Historically, the balance between using personal data and preserving people's privacy has relied, both practically and legally, on the concept of data anonymization. Anonymization, also called de-identification, is the process of transforming personal data to mask the identity of participants, e.g. by removing identifiers, coarsening data, or adding noise. These all state that anonymized data does not require consent from participants to be shared widely, as it cannot be traced back and potentially used against them.While de-identification algorithms are widely used in industry and academia to transform and release anonymous datasets, a large body of research has shown that these practices are not resistant to a wide range of re-identification attacks [4][5][6][7][8][9]. In response to the limits of deidentification, privacy researchers and companies have proposed query-based systems as an alternative. Such systems typically offer data analysts a remote interface to ask questions that return data aggregated from several, potentially many, records. Yet a malicious analyst can often submit a series of seemingly innocuous queries whose outputs, when combined, will allow them to infer private information about participants in the dataset.Differential privacy. Diffix, a patented commercial solution that acts as an SQL proxy between an analyst and a protected database [17,18], has recently been proposed by researchers affiliated with Aircloak and the Max Planck Institute for Software Systems as a practical alternative to differential privacy, based on the [EU] Article 29 Working Party (Art. 29 WP)'s definition of anonymous data. Diffix relies on a novel noise addition framework called "sticky noise", which aims to give analysts a rich query syntax, minimal noise addition, and an infinite number of queries, all while satisfying the WP29 definition of anonymous.The authors claim that data produced by Diffix (i) falls outside of the scope of the new European GDPR regulation; (ii) has been determined by the French National Commission on Informatics and Liberty (CNIL) to offer "GDPR-level anonymization" for all cases; and (iii) has been certified by TÜViT as fulfilling "all requirements for data collection and anonymized reporting" [20,21]. The attacks work in three parts: (i) canceling out part of the sticky noise using multiple queries, (ii) exploiting the noise Diffix adds to one query in order to learn information about the query set associated to this query, and (iii) using logical equivalence between queries to obtain independent noise samples for the same query. We develop two noise-exploitation attacks that take advantage of the structure of Diffix's sticky noise to infer private (also called secret) attributes of individuals in the dataset, violating the inference requirement from the Article 29 WP definition of anonymization [19]. Our second noise-exploitation attack, the cloning attack, uses dummy conditions that affect the output of queries conditionally to the value of the private attribute. While these measures will not result in differential privacy guarantees, they might provide adequate practical solution in many settings. • We show, using the Diffix mechanism as our primary example, that anonymization mechanisms that do not rely on differential privacy might not be GDPR-compliant alone, and that naive data-dependent noise can lead to powerful attacks.Paper organization. Sections 6 and 7 summarize related work and provide our conclusions to build practically useful anonymization systems.Appendix A provides some details for the statistical tests used by the attack. The analyst can send SQL queries to Diffix, which will process the queries and then output a noisy answer.We denote with A D the set of attributes in the database D. For instance, A D could contain 4 attributes A D = {gender, age, zip, HIV} with HIV a binary attribute (0 or 1). The following query would, for example, count the number of individuals in the database who are male, 37 years old, and live in the area with ZIP code 48828:Q ≡ count(gender = M ∧ age = 37 ∧ zip = 48828)Diffix's privacy-preserving mechanism. Diffix's output for Q on D (without bucket suppression, see below) is:Q(D) = Q(D) + h ∑ i=1 static[C i ] + h ∑ i=1 dynamic Q [C i ] (1) with static[C i ] the static noise for condition C i , dynamic Q [C i ] the dynamic noise for condition C i in Q.Static noise. More precisely, if the query set for Q on D is S = {uid 1 , uid 2 , . . . , uid m }, the dynamic noise for C (dynamic Q [C]) is generated from a normal distribution N (0, 1) by the PRNG seeded with:dynamic_seed = XOR(static_seed C , hash(uid 1 ), . . . , hash(uid m ))Note that we don't include D in the notation dynamic Q [C], as the dataset is usually fixed and clear from the context. In addition to static and dynamic noise, Diffix implements another security measure called bucket suppression, similar to the classic query set size restriction. Diffix addresses this issue by using a noisy (and sticky to the query set) threshold. If Q(D) > 1, then Diffix computes a noisy threshold T ∼ N (4, 1/2), using the seed:threshold_seed = XOR(salt, hash(uid 1 ), . . . , hash(uid m ))If Q(D) < T , the query is suppressed; otherwise, the noisy output Q(D) is computed and sent to the analyst. Second, since the noise depends on the query set, the noise itself leaks information about the query set. We first define further notations: with A ⊆ A D a set of attributes, x (A) is the restriction of the record x to A, i.e. the vector one obtains after removing from x every entry for attributes that are not in A. For example, if A D = {gender, age, zip, HIV}, x = (M, 27, 55416, 1) and A = {gender, age, HIV}, then x (A) = (M, 27, 1). H3 The attacker wants to infer a secret attribute s about Bob, the victim. H4 There exists an oracle Unique that takes as input any restricted record z (R) and outputs whether z (R) is unique.Unique(z (R) ) = True if and only if there is no other record y in D such that z (R) = y (R) . The cloning attack, presented later, extends this by requiring a weaker notion of uniqueness, which we call value-uniqueness. Picking an attribute, e.g. a 1 , we can define the following queries:Q 1 ≡ count(a 2 = x 2 ∧ . . . ∧ a k = x k ∧ s = 0) (3) Q 1 ≡ count(a 1 = x 1 ∧ a 2 = x 2 ∧ . . . . . . ∧ a k = x k ∧ s = 0) (4)As the record x is unique, by assumption, it is the only record that can differ between Q 1 and Q 1 . This allows us to directly compute Q(D):Q(D) = Q 1 (D) − Q 1 (D) = 0 if x (s) = 1 1 if x (s) = 0 (5)To prevent this vulnerability 1 , Diffix adds static and dynamic noises:Q 1 (D) = Q 1 (D) + k ∑ i=2 static[a i = x i ] + static[s = 0] + k ∑ i=2 dynamic Q 1 [a i = x i ] + dynamic Q 1 [s = 0]1 For the intersection attack to be successful, both Q 1 and Q 1 need to be large enough to not trigger the bucket suppression. This alone already allows the attacker to infer Bob's secret, x (s) , with better than random accuracy.The third part of the attack allows us to strongly improve the accuracy of our inference. While the stickiness of the noise prevents us from running the same query again to collect more sample, we circumvent it by using different pairs of queries for which equation (6) is still true. R j and R l ) with j = l might have the same query set. In that case, the dynamic noise layers associated to the same conditions would have the same values, and hence the total dynamic noises of the two queries would be heavily correlated. For larger values of k, the queries used by the differential attack contain many conditions, and hence potentially select a low number of records. The algorithm selects random subsets of A * until it finds a subset A ⊆ A * such that Unique(x (A) ) returns True. k ← |A|, Q ← / 0, R ← / 0 2 for j ← 1 to k do 3 I ← {i ∈ [1, k] | i = j} 4 Q ← count ( i∈I a i = x i ∧ s = 0) 5 Q ← count ( i∈I a i = x i ∧ a j = x j ∧ s = 0) 6 if Q > 0 and Q > 0 then 7 q j ← Q − Q 8 Q ← Q ∪ {q j } 9 end if 10 end for 11 for j ← 1 to k do 12 I ← {i ∈ [1, k] | i = j} 13 R ← count ( i∈I a i = x i ∧ s = 1) 14 R ← count ( i∈I a i = x i ∧ a j = x j ∧ s = 1) 15 if R > 0 and R > 0 then 16 r j ← R − R 17 R ← R ∪ {r j } 18end if 19 end for 20 if Q = / 0 and R = / 0 then21 return NoSamples 22 end if 23 f ← PDF of N (0, 2), g ← PDF of N (1, 2k + 2) 24 L ← ∏ q∈Q f (q) g(q) ∏ r∈R g(r) f (r) 25 return L ≥ 1 In this section, we present an extension of the differential noise-exploitation attack, which we call cloning attack. That is, Procedure FullDifferentialAttack(A * , x (A * ) , s) Input:∧ a∈A a = x (a)The attack addresses several limitations of the differential attack, making it much stronger in practice.First, the cloning attack does not require an oracle to confirm that the background information uniquely identifies a user. Furthermore, the cloning attack validates this automatically (and thus prevents bucket suppression) with high confidence.While much stronger, the attack relies on the attacker being able to produce a set of distinct dummy conditions ∆ = {∆ j } 1≤ j≤|∆| , where each ∆ j is an SQL statement such that the set of users matching A = x (A ) is the same as the set of users matching A = x (A ) ∧ ∆ j . For each dummy condition ∆ j , we define the two queries:Q j ≡ count A = x (A ) ∧ ∆ j ∧ s = 0 (9) Q j ≡ count A = x (A ) ∧ ∆ j ∧ u = x (u) ∧ s = 0(10)Withq j = Q j (D) − Q j (D), we have:q j = Q j (D) − Q j (D) − static[u = x (u) ] − dynamic Q j [u = x (u) ] + ∑ i∈A dynamic Q j [a (i) = x (i) ] + dynamic Q j [s = 0] − ∑ i∈A dynamic Q j [a (i) = x (i) ] − dynamic Q j [s = 0] + dynamic Q j [∆ j ] − dynamic Q j [∆ j ]By the same argument we presented for the differential attack, ifx (s) = 1 then Q j (D) = Q j (D)and most dynamic and static noises cancel out, giving:q j = − static[u = x (u) ] − dynamic Q j [u = x (u) ](11)As this value does not depend on the dummy condition used, we have that q 1 = q 2 = · · · = q |∆| . While for high values of k this is easy to detect, the total variance of the noise for low values of k is small, making it harder to distinguish between the two hypotheses (i.e. whether the q j values are "similar" or not). To overcome this issue, we "amplify" the noise for each query: instead of adding a single dummy condition ∆ j to the queries for Q j and Q j , we add the conjunction ∧ l = j ∆ l :Q j ≡ count A = x (A ) ∧ l = j ∆ l ∧ s = 0(12)Q j ≡ count A = x (A ) ∧ l = j ∆ l ∧ u = x (u) ∧ s = 0(13)This increases the total variance of the noise in q j in the x (s) = 0 case, making it easy to distinguish between the two hypotheses: all the q j values will be very similar if x (s) = 1 and fluctuate heavily if x (s) = 0. We include an empirical analysis of σ * in the full version of this manuscript. The cloning attack is described in detail in the procedure CloningAttack.Procedure CloningAttack(A , u, x (A ,u) , ∆, s, v)Input: known attributes (names A , u and values x (A ,u) ), dummy conditions ∆, secret s and target value v Output:True if x (s) = v, False if x (s) = v 1 for j ← 1 to |∆| do 2 ϕ ← A = x (A ) ∧ l = j ∆ l 3 Q ← count (ϕ ∧ s = v) 4 Q ← count ϕ ∧ u = x (u) ∧ s = v 5 q j ← Q − Q 6 end for 7 r ← 1 |∆| ∑ |∆| j=1 q j , S 2 ← 1 |∆|−1 ∑ |∆| j=1 (q j − r) 2 8 return S 2 ≤ σ *Automated validation of the assumption. The idea is that if the output is larger than zero, then the query was not bucket suppressed, and many users are likely share the same attributes x (A ,u) , meaning that x (A ,u) is unlikely to be value-unique. Experiments in section 4 show that this heuristic works very well on real-world datasets.Procedure NoBucketSuppression(A , u, x (A ,u) , ∆, s, v)Input: known attributes (names A , u and values x (A ,u) ), dummy conditions ∆, secret s and target value v Output: True if (A , u) passes the tests and is deemed to satisfy assumption 1, False otherwise 1 ok Q ← 0, ok Q ← 0 2 for j ← 1 to |∆| do 3 ϕ ← A = x (A ) ∧ l = j ∆ l 4 Q ← count (ϕ ∧ s = v) 5 Q ← count ϕ ∧ u = x (u) ∧ s = v 6 if Q > 0Q ← count(A = x (A ) ∧ u = x (u) ) 2 return Q = 0The procedures CloningAttack and NoBucketSuppression both issue (the same) 2|∆| queries, while ValueUnique uses only one query. Using this heuristic, the attack targets 55.4% of the users in the dataset, achieving 91.7% accuracy with a maximum of 32 queries per user in our experiments.Procedure FullCloningAttack(A * , x (A * ) , ∆, s, v) In order to assess the effectiveness of our attacks, we implemented Diffix's mechanism for counting queries as described in the original paper [18]. CDR: synthetic collection of phone metadata with 2,000,000 records generated using real-world data for human behaviour and the geography of the UK for the location of antennas. We first test the differential attack on a synthetic dataset where all users satisfy the uniqueness assumption. For computational reasons, as the size of the dataset in memory grows as B k , the maximum k we can use is limited to 6. We report the empirical fraction of users whose secret attribute is correctly predicted, estimated by performing the differential attack on a sample of 1000 users. As explained in section 3, this makes bucket suppression more prevalent and reduces the total number of samples for the likelihood ratio test, which is a limitation of the differential attack. We then perform the cloning attack on all records that are predicted as attackable and report accuracy pa , the fraction of predicted attackable records whose secret attribute was successfully inferred (fifth column). If this group were to be large enough, the noise added by Diffix might not be enough to hide the secret attribute, which could then be revealed by using a simple count query. This means that, most of the times, value-unique users are simply unique.If secret attributes are predictable from the other attributes, a trained machine learning classifier could predict them with potentially high accuracy 2 . These conditions can be syntactic (e.g., age ≥ 15 for the query age = 23), semantic (e.g., status = retired for age = 23), or pragmatic (e.g., age = 15 against a database containing only adult individuals). At the same time, the richer the syntax is, the more utility an analyst gets out of the system. In this section, we briefly outline some of the approaches that may be used to mitigate the effects of our noiseexploitations attacks -and other attacks -against Diffix and other privacy-preserving query-based systems. Overall, it is our belief that practical secure design principles apply here just as they do in many other contents. Specifically, privacypreserving query-based system such as Diffix (regardless of whether they have provable guarantees or not) would benefit from a defense-in-depth approach, by monitoring the query stream for queries that are likely to lead to exploitation.Intrusion detection. This involves a compromise between rough data summarization and fine-grained queries, and limits the utility of the system in practice. Third, the cloning attack, which we developed after-wards, is able to validate its assumptions automatically and performs very well on a large range of real-world datasets.The code of our differential and cloning attacks, as well as the experiments performed in this paper, are available at https://cpg.doc.ic.ac.uk/signal-in-the-noise. In 2003, Dinur et al. [32] proposed the first example of an attack that works on a large class of query-based systems. In what they called a reconstruction attack, they showed that if the noise added to every query is at most o( √ n), where n is the size of the dataset, then an attacker can reconstruct almost the entire dataset using only polynomially many queries. Differential privacy is a privacy guarantee that can be enforced by query-based systems. Specifically, Diffix allows for infinitely many queries with little noise added to outputs.General-purpose analytics usually refers to systems that allow analysts to send many queries of different type, and ideally permit to join different datasets. Data-dependent noise, also called instance-based noise, has been shown to provide significantly better accuracy than dataindependent noise [42]. However, naive implementations of data-dependent noise can leak information about the data, a result Nissim et al. theorized as a potential way to attack the system [42]. Two months later, in October 2018, two other attacks on Diffix were disclosed. A membership attack by Pyrgelis et al. [43], based on a previous paper [44], and a reconstruction attack by Cohen and Nissim [45], based on previous work by Dinur et al. [32] and Dwork et al. [46]. First, it is a membership attack and only allows an attacker to infer whether a person is in the protected dataset or not. This allows the attacker to produce a noisy linear system that can be solved using linear programming techniques to reconstruct the entire set of secret attributes {s 1 , . . . , s n } with perfect accuracy in polynomial time.While this attack can successfully reconstruct the entire dataset, it presents two limitations compared to our attack.First, it requires that the system allows queries of the type ∑ i∈I s i , i.e. queries that select any analyst-defined set of users I ⊆ [n], the "row-naming problem". Following the disclosure, Aircloak restricted the available SQL functions to prevent the attack [47]. Furthermore, our results show that naive data-dependent noise leads to highly vulnerable systems.Our differential noise-exploitation attack, given little auxiliary information about the victim, combines specific queries and estimates how the noise is distributed to infer the value of the private attribute. In a synthetic best-case dataset, the attacker can predict with 92.6% accuracy private attributes, using only 5 attributes.Our cloning noise-exploitation attack extends the first one by adding "dummy" conditions that do not change the selected query set. It relies on weaker assumptions, that are automatically validated with high accuracy by our algorithm. The test will sometimes yield the wrong result. For simplicity, we suppose that Diffix's outputs are not rounded to the nearest nonnegative integer and bucket suppression is never triggered for the queries in the attack, so that every pair of queries ( Q j , Q j ) and ( R j , R j ) yields a valid sample. As discussed in section 3, this is not always guaranteed to be true, but it has close to no effect on the actual accuracy of the test. To measure the theoretical accuracy of the attack for k known attributes, we can apply the fact to Λ(q, r) with µ 0 = 0, σ 2 0 = 2, µ 1 = 1, σ 2 1 = 2k + 2 and n = k, and finally find acc(k) = 1 − p err (k). If we suppose that Diffix's outputs are rounded to the nearest nonnegative integer, no simple expression can be determined for the error rate. One of the main features of Diffix is that it allows analysts to send an unlimited amount of queries. This ensures that, with high probability, the victim is uniquely identified by x (A ,u) . So, the GreedyFullCloningAttack algorithm requires at most |A * | + 2|∆| + 2 queries.We compared the performances of the FullCloningAttack and GreedyFullCloningAttack on the ADULT dataset, with the salary class as secret attribute and the other 10 attributes as A * . Introducing additional optimizations, the accuracy could be improved and the number of queries could be further reduced (see full version).