Large data size is not only a storage concern: forensic analysis tasks can become very slow when they must sift through billions of records. In this paper, we first present two powerful event reduction techniques that reduce the number of records by a factor of 4.6 to 19 in our experiments. The spate of APTs in recent years has fueled research on efficient collection and forensic analysis of system logs [13,14,15,9,16,17,18,22,42,30,10]. In contrast, * This work was primarily supported by DARPA (contract FA8650-15-C-7561) and in part by NSF (CNS-1319137, CNS-1421893, CCF-1414078) and ONR (N00014-15-1-2208, N00014-15-1-2378, N00014-17-1-2891). With existing systems, such as Linux auditing and Windows ETW, our experience as well as that of previous researchers [42] is that the volume of audit data is in the range of gigabytes per host per day. This has motivated a number of research efforts on reducing log size.Since the vast majority of I/O operations are reads, ProTracer's [22] reduction strategy is to log only the writes. However, fine-grained taint-tracking via execution partitioning would be difficult to deploy on the scale of a large enterprise running hundreds of applications or more. Without fine-grained taint-tracking, the analysis above, as well as our experiments, indicate that this strategy of "alternating tainting with logging" leads to substantial increases in log size.LogGC [18] develops a "garbage collection" strategy, which identifies and removes operations that have no persistent effect. While LogGC removes all events on a limited class of objects, Xu et al [42] explore a complementary strategy that can remove some (repeated) events on any object. This is impressive, considering that it was achieved without any application-specific optimizations.While trackability equivalence [42] provides a sufficient basis for eliminating events, we show that it is far too strict, limiting reductions in many common scenarios, e.g., communication via pipes. We say that a a.com b.com Fig. 2: Dependence graph resulting after our FD log reduction. Edge timestamps are dropped, but nodes may be annotated with a timestamp.P 1 P 4 Q C L Enode v depends on node u if there is a (directed) path from u to v with non-decreasing edge timestamps. We also prove that SD preserves the results of the most commonly used forensic analysis, which consists of running first a backward analysis to find the attacker's entry points, and then a forward analysis from these entry points to identify the full impact of the attack. For the example in Fig. 1, our technique combines all edges between the same pair of nodes, leading to the graph shown in Fig. 2, while Xu et al's technique is able to combine only the two edges with timestamps 1 and 2. Such global properties are expensive to compute, taking time that is linear in the size of the (very large) dependence graph. Moreover, due to the use of timestamped edges, reachability changes over time, and hence the results cannot be computed once and cached for subsequent use.To overcome these computational challenges posed by timestamped graphs, we show in Section 4 how to transform them into standard graphs. We then encode these dependencies into a standard graph in order to speed up our reduction algorithms.The key challenge in this context is to minimize the size of the standard graph without dropping any existing dependency, or introducing a spurious one. Fig. 2 illustrates a few common cases where we achieve substantial reductions by combining many similar operations: -multiple reads from the same network connection (a.com, b.com) interleaved with multiple writes to files (C and L), -series of writes to and reads from pipes (E), and -series of writes to log files by multiple processes (L). Graph databases such as OrientDB, Neo4j and Titan are designed to provide efficient support for graph queries, but experience suggests that their performance degrades dramatically on graphs that are large relative to main memory. For instance, a performance evaluation study on graph databases [23] found that they are unable to complete simple tasks, such as finding shortest paths on graphs with 128M edges, even when running on a computer with 256GB main memory and sufficient disk storage. By combining our log reduction techniques with compact representations, our system achieves very high density: it uses about 2 bytes of main memory per event on our largest data set. With FD reduction, it became 35.3× smaller, while SD increased the reduction factor to about 41.4×. Entries in the log correspond to events, which represent actions (typically, system calls) performed by subjects, e.g., read, write, and execute. In most work on forensic analysis [13,15,42], the log contents are interpreted as a dependence graph: nodes in the graph correspond to entities, while edges correspond to events. Implicitly, in-edges of subjects denote reads, and out-edges of subjects denote writes.Backward and Forward Analysis. The origin can be identified using backward analysis, starting from an entity flagged as suspicious, and tracing backward in the graph. Of these, a.com is a source node, i.e., an object with no parent nodes, and hence identified as the likely entry point of any attack on C.Although b.com is backward reachable from C in the standard graph-theoretic sense, it is excluded because the path from b.com to C does not always go forward in time.The set of entities impacted by the attack can be found using forward analysis [43,1,15] (a.k.a. impact analysis), typically starting from an entry point identified by backward analysis. In the sample dependence graph, forward analysis from network connection a.com will reach all nodes in the graph, while a forward analysis from b.com will leave out C.The when question asks when each step in the attack occurred. Despite being limited to reads, writes and loads, our reduction techniques are very effective in practice, as these events typically constitute over 95% of total events.For the first requirement, our aim is to preserve the results of forward and backward forensic analysis. Reachability in this graph is defined as follows:Definition 1 (Causal Path and Reachability) A node v is reachable from another node u if and only if there is (directed) path e 1 ,e 2 ,...,e n from u to v such that:∀1 ≤ i < n start(e i ) ≤ end(e i+1 )(1)We refer to a path satisfying this condition as a causal path.It captures the intuition that information arriving at a node through event e i can possibly flow out through the event e i+1 , i.e., successive events on this path e 1 ,e 2 ,...,e n can be causally related. In Fig. 1, P@6 −→ Q, but P@11 −→ Q. Similarly, a.com −→ C@3 but b.com −→ C@3.Based on reachability, we present three dependencypreserving reductions: CD, which is close to Xu et al's full trackability, and FD and SD, two new reductions we introduce in this paper. This reduction aims to preserve forward and backward reachability at every instant of time.Definition 3 (Continuous Dependence Preservation) Let G be a dependence graph and G be a reduction of G. G is said to preserve continuous dependence iff forward and backward reachability is identical in both graphs for every pair of nodes at all times.In Fig. 3, S reads from a file F at t = 2 and t = 4, and writes to another file F at t = 3 and t = 6. This is why we devote Section 4 to development of efficient algorithms to check the more powerful global properties used in the two new reductions presented below.Because of the similarity of Xu et al's full trackability and our continuous dependence, we will henceforth refer to their approach as local continuous dependence (LCD) preservation. We end this discussion with examples of common scenarios where LCD reduction is permitted:• Sequence of reads without intervening writes: When an application reads a file, its read operation results in multiple read system calls, each of which is typically logged as a separate event in the audit log. But does this difference really matter in the context of forensic analysis? This edge in Fig. 3 should not overlap the period between the end times of the two edges out of S; per their Algorithm 3, the period of the S to F edge must not overlap the period between the start times of the two edges out of F. 3 We aren't suggesting that a compromised process must immediately observation implies that keeping track of dependencies between entities at times strictly in between events is unnecessary, because nothing relevant changes at those times. • forward reachability from u@t to v is preserved for all t ∈ NewAnc(u), and• backward reachability of u from v@t is preserved at all t.In other words, when FD-preserving reductions are applied:• the result of backward forensic analysis from any node v will identify the exact same set of nodes before and after the reduction. To illustrate the definition, observe that FD preservation allows the reduction in Fig. 4, since (a) backward reachability is unchanged for every node, and (b) NewAnc(F) = {0}, and F@0 flows into S, F and H in the original as well as the reduced graphs. To carry out this task accurately, we need to preserve only information flows from source nodes; preserving dependencies between all pairs of internal nodes is unnecessary.Definition 5 (Source Dependence (SD) Preservation) A reduction G of G is said to preserve source dependence iff for every node v and a source node u:• forward reachability from u@0 to v is preserved, and• backward reachability of u from v@t is preserved at all t.Note that SD coincides with FD applied to source nodes. The first conditions coincide as well, when we take into account that NewAnc(u) = {0} for any source node u. (A source node does not have any ancestors, but since we have defined NewAnc to always include zero, NewAnc of source nodes is always {0}.) 5 is redundant, as it is implied by the second: If u is backward reachable from a node v at t, then, by definition of backward reachability, there exists a causal path from e 1 ,e 2 ,...,e n from u to v. 5 because its presence makes the forensic analysis preservation properties of SD more explicit. Full dependence and source dependence reductions rely on global properties of graph reachability. This mutability also means that results cannot be computed once and cached for subsequent use, unlike standard graphs, where we can determine once that v is a descendant of u and reuse this result in the future.To overcome these computational challenges posed by timestamped graph, we show how to transform them into standard graphs. This encoding serves as the basis for developing efficient algorithms for log reduction. Runtime is also reduced because the reduction operations typically take constant time per edge (See Section 6.6.1). Versions of a node are stacked vertically in the example so as to make it easier to see the correspondence between nodes in the timestamped and versioned graphs.Note that timestamps in versioned graphs are associated with nodes (versions), not with edges. F S G T 2 3 5 4 6 5 S 0 F 0 S 2 G 0 G 3 G 5 T 0 T 4 T 6 We treat the contents of the audit log as a timestamped graph G = (V,E T ). return (V,E)We intend BuildVer and its optimized versions to be online algorithms, i.e., they need to examine edges one-at-a-time, and decide immediately whether to create a new version, or to add a new edge. These constraints are motivated by our application in real-time attack detection and forensic analysis.For each entity v, an initial version v 0 is added to the graph at line 2. In a naive versioned graph, each object and subject gets split into many versions, with each version corresponding to the time period between two consecutive incoming edges to that entity in the unversioned graph. In addition, we make the following observation that readily follows from the description of BuildVer.Observation 6 For any two node versions u t and u s , there is a path from u t to u s if and only if s ≥ t.Theorem 7 Let G = (V,E) be the versioned graph constructed from G = (V,E T ). (w 0 ,w 1 ,t 1 ),(w 1 ,w 2 ,t 2 ),...,(w n−1 ,w n ,t n ) in G such that t i−1 ≤ t i for 1 ≤ i ≤ n.Note that the "only if" proof constructed a one-to-one correspondence between the paths in G and G. Naive versioning is simple but offers no benefits in terms of data reduction. These optimizations cause node timestamps to expand to an interval. S 0 F 0 S 2 G 0 G 3 G 5 T 0 T 4 T 6 S 0 F 0 S 2 G 0 G 3 T 0 T 4 F 0 S 0,2 G 0,3 T 0,4 Fig. 7:The naive versioned graph from Fig. 6 (top), and the result of applying redundant edge optimization (REO) (middle) and then redundant node optimization (RNO) (bottom) to it. If so, we simply discard this event.We leave the node timestamp unchanged. With global redundant edge, we generalize to check whether u is an ancestor of v. Specifically, before adding an event (u,v,t) to the graph, we check whether the latest version of u is already an ancestor of the latest version of v. If we overzealously combine v l and v s , then a false dependency will be introduced, e.g., a descendant of v l may backtrack to a node that is an ancestor of v s but not v l . This possibility exists as long as (a) the ancestors of v l and v s aren't identical, and (b) v l has non-zero number of descendants. If not, we replace v r,s with v r,t , instead of creating a new version of v. Fig. 7 illustrates the result of applying this optimization.RNO preserves dependence for descendants of v, but it can change backward reachability of the node v itself. This edge is being added because it is not redundant, i.e., a backward search from v@s does not reach u p,q . Our experimental results show that cycle detection has a dramatic effect on some data sets.Cycle detection can take time linear in the size of the graph. REO and RNO optimizations avoid new versions in most common scenarios that lead to an explosion of versions with naive versioning:• Output files: Typically, these files are written by a single subject, and not read until the writes are completed. • Log files: Typically, log files are written by multiple subjects, but are rarely read, and hence by RNO, no new versions need to be created. Proof: We already showed that BuildVer preserves forward and backward reachability between the timestamped graph G and the naive versioned graph G. Hence it suffices to show that the edges and nodes eliminated by REO* and RNO don't change forward and backward reachability in G. Now, REO* optimization drops an edge (u,v,t) only if there is already an edge from the latest version of u to the latest or a previous version of v in G. We now show that the combination of REO* and RNO optimizations results in reductions that are optimal with respect to FD preservation. In contrast, this combination preserves FD.The main reasoning behind optimality is that REO* creates a new version of an entity v whenever it acquires a new dependency from another entity u. Thus, if either REO* or RNO optimizations were violated, then, forensic analysis of the versioned graph will yield incorrect results. While this is the default definition, broader definitions of source can easily be used, if an analyst considers other nodes to be possible sources of compromise.We use a direct approach to construct a versioned graph that preserves SD. Although the sets Src(v) can get large, note that they need to be maintained only for active subjects and objects. In order to minimize the impact of such misses, we first apply REO, RNO and CCO optimizations, and skip the edges and/or versions skipped by these optimizations. Only when they determine an edge to be new, we apply the SD check based on Src sets.Theorem 9 BuildVer, together with redundant edge and redundant node optimizations and the source dependence optimization, preserves source dependence.Proof: Since full dependence preservation implies source dependence preservation, it is clear that redundant edge and redundant node optimizations preserve source dependence, so we only need to consider the effects of source dependence optimization. In this section, we describe how to use the techniques described so far, together with others, to achieve highly compact log file and main-memory dependence graph representations. Translators can easily be developed to translate CSR to standard log formats, so that standard log analyzers, or simple tools such as grep, can be used.In CSR, all subjects and objects are referenced using a numeric index. Following this table is a sequence of operations, each of which correspond to the definition of an object (e.g., a file, network connection, etc.) or a forensic-relevant operation such as open, read, write, chmod, fork, execve, etc. Forensic analysis requires queries over the dependence graph, e.g., finding shortest path(s) to the entry node of an attack, or a depth-first search to identify impacted nodes. Edges typically outnumber nodes by one to two orders of magnitude, so compactness of edges is paramount.The starting point for our compact memory representation is the SLEUTH [10] system for forensic analysis and attack visualization. However, we did away with many other aspects of that implementation, such as the (over-)reliance on compact, variable length encoding for events, based on techniques drawn from data compression and encoding. This enables most stored edges to use just 6 bytes in our implementation, encoding an event name and about a 40-bit subject or object identifier. • Node reductions: The second biggest source of compaction is node reduction, achieved using RNO and CCO optimizations. In Section 6.3, we evaluate the effectiveness of FD and SD in reducing the number of events, and compare it with Xu et al.'s technique (LCD). We then evaluate the effect of these reductions on the CSR log size and the inmemory dependence graph in Sections 6.4 and 6.5. The impact of our optimizations on forensic analysis accuracy is evaluated in Section 6.7. The back-end uses our BuildVer algorithm, together with (a) the REO, RNO, and CCO optimizations (Section 4.2) to realize FD preservation, and (b) the source dependence preservation technique described in Section 4.3. We used this capability to carry out many of our experiments, because data in CSR format can be consumed much faster than data in Linux audit log format or the OS-neutral format in which red team engagement data was provided. Currently, time windows are set to about 10 minutes. 9 Our evaluation uses data from live servers in a small laboratory, and from a red team evaluation led by a government agency. This data was collected as part of the 2 nd adversarial engagement organized in the DARPA Transparent Computing program. The audit data was transformed into a OS-neutral format by another team and then given to us for analysis. Table 8 shows the total number of events in the data, along with a breakdown of important event types.Since reads and writes provide finer granularity information about dependencies than open/close, we omitted open/close from our analysis and do not include them in our figures.Windows Engagement Data (Windows Desktop). Audit data was collected on a production web server, mail server, and general purpose file and remote access server (SSH/File Server) used by a dozen users in a small academic research laboratory. Fig. 9 shows the event reduction factor (i.e., ratio of number of events before and after the reduction) achieved by our two techniques, FD and SD. LCD, FD and SD achieve an average reduction factor of 1.8, 7 and 9.2 respectively. Across the data sets, LCD achieves reduction factors between 1.6 and 2.7, FD ranges from 4.6 to 15.4, and SD from 5.4 to 19.1. For Linux desktop data set, FD reduction factor is significantly higher. Such processes typically acquire a new dependency when they make a new network connection, but subsequent operations don't add new dependencies, and hence most of them can be reduced.Our implementation of SD is on top of FD: if an edge cannot be removed by FD, then the SD criterion is tried. The second column reports the log size of original audit data. The second column shows the size of the original data, i.e., Linux audit data for laboratory servers, and OS-neutral intermediate format for red team engagement data. For instance, on the Linux desktop data, where FD produces about 15× reduction, the CSR log size shrinks by about 12× over base CSR size. Examining the Linux desktop and Windows desktop numbers closely, we find that the memory use is closely correlated with the reduction factors in Fig. 9 data shows that about 2M events are stored occupying about 24MB, and that the 781K nodes take up about 53B/node. Since some optimizations require other optimizations, we show the four most meaningful combinations: (a) no optimizations, (b) all optimizations except redundant node (RNO), (c) all optimizations except cycle-collapsing (CCO), and (d) all optimizations. To evaluate the effect of (b), we placed a limit k, called the FD window size, on the number of edges examined by REO before it reports that a dependence does not exist; this is safe but may reduce the benefit. We hypothesize that this is partly due to the nature of red team exercises, and partly due to workload differences between desktops and servers.Comparing the two charts, we conclude that a range of FD=25 to FD=100 represents a good trade-off for a real-time detection and forensic analysis system such as SLEUTH [10], with most of the size reduction benefits realized, and with runtime almost the same as FD=1. Thus, although SD is slower than FD, it is quite fast in absolute terms, being able to process events at least two orders of magnitude faster than the maximum event production rate observed across all of our data sets. In our previous work [10], we performed real-time attack detection and forensic analysis of multi-step APT-style attack campaigns carried out in the 1 st adversarial engagement in the DARPA Transparent Computing program. It detects attacks using tag-based policies that were developed in the context of our earlier work on whole-system integrity protection [19,34,35,36] and policy-based defenses [39,32]. Numerous systems construct dependence graphs [13,9,15,22] or provenance graphs [25,24,8,4,29] that capture information flow at the coarse granularity of system calls. Their focus was on demonstrating effectiveness of attack investigation, so they did not pursue log reduction beyond simple techniques such as omitting "low-control" (less important) events, such as changing a file's access time.LogGC [18] proposed an interesting approach for log reduction based on the concept of garbage collection, i.e., removing operations involving removed files ("garbage"). Indeed, our experiments with this strategy 11 resulted in more than an order of magnitude increase in log sizes.Xu et al.'s notion of full-trackability equivalence (LCD-preservation in our terminology) [42] is similar to our CD-preservation, as discussed in Section 3.2. Therefore, Winnower also stores each host's full provenance graph locally at the host. These compression techniques, which preserve every detail of the graph, are orthogonal to our techniques, which can drop or merge edges.Graph summarization [27,37] is intended mainly to facilitate understanding of large graphs but can also be regarded as lossy graph compression. In this paper, we formalized the notion of dependencypreserving data reductions for audit data and developed efficient algorithms for dependency-preserving audit data reduction.