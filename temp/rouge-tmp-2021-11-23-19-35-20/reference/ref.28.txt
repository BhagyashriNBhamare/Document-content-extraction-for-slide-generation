By combining social engineering techniques (e.g., spearphishing) with advanced exploit techniques, these adversaries routinely bypass widely-deployed software protections such as ASLR, DEP and sandboxes. While these tools are generally useful, they typically generate a vast amount of information, making it difficult for a security analyst to distinguish truly significant attacks -the proverbial "needle-in-a-haystack" * This work was primarily supported by DARPA (contract FA8650-15-C-7561) and in part by NSF (CNS-1319137, CNS-1421893, CNS-1514472 and DGE-1069311) and ONR (N00014-15-1-2208 and N00014-15-1-2378). Such a summary would enable an analyst to quickly ascertain whether there is a significant intrusion, understand how the attacker initially breached security, and determine the impact of the attack.The problem of piecing together the causal chain of events leading to an attack was first explored in Backtracker [25,26]. In contrast, this paper presents SLEUTH, 1 a system that can alert analysts in real-time about an ongoing campaign, and provide them with a compact, visual summary of the activity in seconds or minutes after the attack. Dealing with common usage scenarios: How does one cope with normal, benign activities that may resemble activities commonly observed during attacks, e.g., software downloads? For example, an adversary could start the attack by hijacking a web browser through externally supplied malicious input, by plugging in an infected USB memory stick, or by supplying a zero-day exploit to a network server running within the enterprise. Audit data from these OSes is processed into a platform-neutral graph representation, where vertices represent subjects (processes) and objects (files, sockets), and edges denote audit events (e.g., operations such as read, write, execute, and connect). This graph serves as the basis for attack detection as well as causality analysis and scenario reconstruction.The first contribution of this paper, which addresses the challenge of efficient event storage and analysis, is the development of a compact main-memory dependence graph representation (Section 2). Tags are described in more detail in Section 3, together with their application to attack detection.A third contribution of this paper is the development of novel algorithms that leverage tags for root-cause identification and impact analysis (Section 5). Experiments show that our tagbased approach is very effective: for instance, SLEUTH can analyze 38.5M events and produce an attack scenario graph with just 130 events, representing five orders of magnitude reduction in event volume.The fourth contribution of this paper, aimed at tackling the last two challenges mentioned above, is a customizable policy framework (Section 4) for tag initialization and propagation. Since we can process and analyze audit data tens of thousands of times faster than the rate at which it is generated, efficient, parallel, real-time testing of alternate hypotheses is possible.The final contribution of this paper is an experimental evaluation (Section 6), based mainly on a red team evaluation organized by DARPA as part of its Transparent Computing program. In this evaluation, SLEUTH was able to:• process, in a matter of seconds, audit logs containing tens of millions of events generated during the engagement;• successfully detect and reconstruct the details of these attacks, including their entry points, activities in the system, and exfiltration points;• filter away extraneous events, achieving very high reductions rates in the data (up to 100K times), thus providing a clear semantic representation of these attacks containing almost no noise from other activities in the system; and• achieve low false positive and false negative rates.Our evaluation is not intended to show that we detected the most sophisticated adversary; instead, our point is that, given several unknown possibilities, the prioritized results from our system can be right on spot in real-time, without any human assistance. However, the performance [39] of popular databases such as Neo4J [4] or Titan [6] is limited for many graph algorithms unless main memory is large enough to hold most of data. Even STINGER [16] and NetworkX [5], two graph databases optimized for main-memory performance, use about 250 bytes and 3KB, respectively, per graph edge [39]. Objects attributes include name, type (file, pipe, socket, etc.), owner, and tags.Events reported in the audit log are captured using labeled edges between subjects and objects or between two subjects. The number of objects/subjects in our largest data set was a few orders of magnitude smaller than this number.While our design emphasizes compact data structures for objects and subjects, compactness of events is far more important: events outnumber objects and subjects by about two orders of magnitude in our largest data set. We encode event names for frequently occurring events (e.g., open, close, read and write) using 3 bits or less. This leaves us with several bits for storing a summary of event argument information, while still being within 32 bits.We can navigate from subjects to objects using the event data stored within subjects. This means that the relative index stored with object-event record can be 12 bits or less in most cases, thus allowing these records to be 16 bits or less in the typical case.This design thus allows us to store bidirectional timestamped edges in as little as 6 bytes (4 bytes for a subjectevent record and 2 bytes for an object-event record). In experiments with larger data sets, the total memory use of our system is within 10 bytes per event on average.Our variable length encoding allows us to represent full information about important (but rare) events, such as rename, chmod, execve, and so on. This assessment can be based on three main factors:• Provenance: the tags on the immediate predecessors of an object or subject in the dependence graph,• Prior system knowledge: our knowledge about the behavior of important applications, such as remote access servers and software installers, and important files such as /etc/passwd and /dev/audio, and• Behavior: observed behavior of subjects, and how they compare to their expected behavior.We have developed a policy framework, described in Section 4, for initializing and propagating tags based on these factors. This policy is conservative: it can err on the side of over-tainting, but will not cause attacks to go undetected, or cause a forward (or backward) analysis to miss objects, subjects or events.Tags play a central role in SLEUTH. • Benign tag reflects a reduced level of trust than benign authentic: while the data/code is still believed to be benign, adequate authentication hasn't been performed to verify the source. If no policy is applicable to a source, then its t-tag is set to unknown.We define the following confidentiality tags (c-tags), to reason about information stealing attacks:• Secret: Highly sensitive information, such as login credentials and private keys. Specifically, a subject (i.e., a process) is given two t-tags: one that captures its code trustworthiness (code t-tag) and another for its data trustworthiness (data t-tag). Finally, attacks are detected using behavior-based policies called detection policies.As mentioned before, if no specific policy is provided, then sources are tagged with unknown trustworthiness. This requires expert knowledge about the application, or in-the-field training in a dynamic environment, where applications may be frequently updated.Instead of focusing on application behaviors that tend to be variable, we focus our detection techniques on the high-level objectives of most attackers, such as backdoor insertion and data exfiltration. Note that our tags are designed to capture means: if a piece of data or code bears the unknown t-tag, then it was derived from (and hence influenced by) untrusted sources.As for the high-level objectives of an attacker, several reports and white papers have identified that the following steps are typical in most advanced attack campaigns [1, 2, 3]:1. Even in those cases where the attacker's goal could be achieved without establishing a permanent base, the third step usually represents an essential attacker goal.Based on the above reasoning, we define the following policies for attack detection that incorporate the attacker's objectives and means:• Untrusted code execution: This policy triggers an alarm when a subject with a higher code t-tag executes (or loads) an object with a lower t-tag 3 . • Modification by subjects with lower code t-tag: This policy raises an alarm when a subject with a lower code t-tag modifies an object with a higher t-tag. For instance, the untrusted code policy does not require a direct load of data from an unknown web site; instead, the data could be downloaded, extracted, uncompressed, and possibly compiled, and then loaded. Today's vulnerability exploits typically do not involve untrusted code in their first step, and hence won't be detected by the untrusted code execution policy. Moreover, setting of unknown t-tag at suspect nodes preserves the dependency structure between the graph vertices that cause alarms, a fact that we exploit in our forensic analysis.The fact that many of our policies are triggered by untrusted code execution should not be interpreted to mean that they work in a static environment, where no new code is permitted in the system. We express policies using a simple rule-based notation, e.g.,exec(s, o) : o.ttag < benign → alert("UntrustedExec")This rule is triggered when the subject s executes a (file) object o with a t-tag less than benign. • tags: conditions can be placed on t-tags and c-tags of objects and/or subjects. While we use a rule-based notation to specify policies in this paper, in our implementation, each rule is encoded as a (C++) function.To provide a finer degree of control over the order in which different types of policies are checked, we associate policies with trigger points instead of events. This pseudoevent is assumed to have occurred when a new object is encountered for the first time, e.g., establishment of a new network connection, the first mention of a preexisting file, creation of a new file, etc. As soon as a matching rule is found, the tags specified by this rule are assigned to the target of the event, and the remaining tag policies are not evaluated.Our current detection policies are informally described in the previous section. Recall that when a subject creates a new object, the object inherits the subject's tags by default; however, this can be overridden using tag initialization policies.Our current tag initialization policy is as follows. Note the use of regular expressions to conveniently define initial tags for groups of objects.init(o): match(o.name, "^IP:(10\.0|127)") → o.ttag = BENIGN AUTH, o.ctag = PRIVATE init(o): match(o.name, "^IP:") → o.ttag = UNKNOWN, o.ctag = PRIVATE init(o): o.type == FILE → o.ttag = BENIGN AUTH, o.ctag = PUBLICThe first rule specifies tags for intranet connections, identified by address prefixes 10.0 and 127 for the remote host. A similar comment applies to programs such as software updaters and installers that download code from untrusted sites, but verify the signature of a trusted software provider before the install.propRd(o, s): match(s.cmdline, "^/sbin/sshd$") → skipMoreover, when the login phase is complete, typically identified by execution of a setuid operation, the process should be assigned appropriate tags.propSu ( The goal of backward analysis is to identify the entry points of an attack campaign. Backward search involves a backward traversal of the graph to identify paths that connect the suspect nodes to entry nodes. In addition, our shortest path formulation addresses the multiple paths chalenge by by preferring the entry point closest (as measured by path cost) to a suspect node.For shortest path, we use Dijkstra's algorithm, as it discovers paths in increasing order of cost. Specifically, the costs are as follows:• Edges that introduce a dependency from a node with unknown code or data t-tag to a node with benign code or data t-tag are assigned a cost of 0. In particular, edges between nodes with high confidentiality tags (e.g., secret) and nodes with low code integrity tags (e.g., unknown process) or low data integrity tags (e.g., unknown socket) are assigned a cost of 0, while edges to nodes with benign tags are assigned a high cost. Attack data sets were collected on Windows (W-1 and W-2), Linux (L-1 through L-3) and FreeBSD (F-1 through F-3) by three research teams that are also part of the DARPA TC program. For this reason, we omitted any fine-grained provenance included in their dataset, falling back to the data they collected from the built-in auditing system of Linux. Note that "read" and "write" columns include not only file reads/writes, but also network reads and writes on Linux. The campaigns are aimed at achieving varying adversarial objectives, which include dropping and execution of an executable, gathering intelligence about a target host, backdoor injection, privilege escalation, and data exfiltration.Being an adversarial engagement, we had no prior knowledge of the attacks planned by the red team. Of the 8 attack scenarios successfully reconstructed by SLEUTH, we discuss campaigns W-2 (Windows) and F-3 (FreeBSD) in this section, while deferring the rest to Section 6.10. The collected data is written to C:\Users\User1\Documents\Thumbs\thumbit\test\thumbs.db.Data exfiltration: Then the collected intelligence is exfiltrated to 129.55.12.51:9418 using git. Dropbear next starts a shell process, which executes a series of commands ls, bash, uname, ps, all of which write to a file /usr/home/user/procstats.Finally, dropbear starts a bash process, which uses scp to download a file called /usr/home/user/archiver, and executes that file. Cleaning the attack footprint is a common element of an APT campaign.W-1 W-2 L-1 L-2 L-3 F-1 F-2 F-3In our experiments, in 5 of the 8 scenarios, SLEUTH uncovered attack cleanup activities, e.g., removing dropped executables and data files created during the attack. Subsequent investigation showed that the auditing system had not been shutdown at the end of the F-1 campaign, and all of these false positives correspond to testing/administration steps carried out after the end of the engagement, when the auditing system should not have been running. Our focus was on software updates and upgrades during this period, since these updates can download code from the network, thereby raising the possibility of untrusted code execution alarms. The fourth column shows the total run time, including the times for consuming the dataset, constructing the dependence graph, detecting attacks, and reconstructing the scenario. It can be thought of as the number of simultaneous data streams that can be handled by SLEUTH, if CPU use was the only constraint.In summary, SLEUTH is able to consume and analyze audit COTS data from several OSes in real time while having a small memory footprint. The third column shows the final number of events that go into the attack scenario graph.The fourth column shows the reduction factor when a naive forward analysis with single trustworthiness tag (single t-tag) is used from the entry points identified by our backward analysis. The last column shows the overall reduction we get over original events using split (code and data) trustworthiness tags and performing the simplification.Overall, the combined effect of all of these steps is very substantial: data sets consisting of tens of millions of edges are reduced into graphs with perhaps a hundred edges, representing five orders of magnitude reduction in the case of L-2 and L-3 data sets, and four orders of magnitude reduction on other data. Subsequently, the jpeg file is exfiltrated by mozillanightly.The second mozillanightly process downloads and executes two files: 1) burnout.bat, which is read, and later used to issue commands to cmd.exe to gather data about the system; 2) mnsend.exe, which is executed by cmd.exe to exfiltrate the data gathered previously.Attack L-3. For example, if an attack deliberately writes into a well-known log file, Backtracker's search heuristics may remove the log file from the final graph, whereas our tag-based analysis will prevent that node from being pruned away.In a similar spirit, BEEP [31] and its evolution ProTracer [37] build dependence graphs that are used for forensic analysis. Host-based intrusion detection techniques mainly fall into three categories: (1) misuse-based, which rely on specifications of bad behaviors associated with known attacks; (2) anomaly-based [19,32,47,20,30,11,48], which rely on learning a model of benign behavior and detecting deviations from this behavior; and (3) specification-based [27,54], which rely on specifications (or policies) specified by an expert. This flexibility comes with the cost of nontrivial changes to application and/or OS code.Although our tags are conceptually similar to those in IFC systems, the central research challenges faced in these systems are very different from SLEUTH. The main approaches, often used together, are to cluster similar alerts, prioritize alerts, and identify causal relationships between alerts [14,43,46,44,56]. SLEUTH uses a main memory graph data model and a rich tag-based policy framework that make its analysis both efficient and precise.