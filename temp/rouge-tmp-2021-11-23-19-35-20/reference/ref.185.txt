Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. We highlight two main factors contributing to attack transferabil-ity: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets. In poisoning availability attacks, the attacker controls a certain amount of training data, thus influencing the trained model and ultimately the predictions at testing time on most points in testing set [4, 18, 20, 28-30, 34, 36, 41, 48]. Creating poisoning and evasion attack points is not a trivial task, particularly when many online services avoid disclosing information about their machine learning algorithms. Previous work has reported empirical findings about the transferability of evasion attacks [3,13,14,21,26,32,33,42,43,47] and, only recently, also on the transferability of poisoning integrity attacks [41]. We provide a formal definition of transferability and show that, under linearization of the loss function computed under attack, several main factors impact transferability: the intrinsic adversarial vulnerability of the target model, the complexity of the surrogate model used to optimize the attacks, and its alignment with the target model. Furthermore, we derive a new poisoning attack for logistic regression, and perform a comprehensive evaluation of both evasion and poisoning attacks on multiple datasets, confirming our theoretical analysis.In more detail, the contributions of our work are:Optimization framework for evasion and poisoning attacks. Our formal definition unveils that transferability depends on: (1) the size of input gradients of the target classifier; (2) how well the gradients of the surrogate and target models align; and (3) the variance of the loss landscape optimized to generate the attack points.Comprehensive experimental evaluation of transferability. We evaluate the transferability of our attacks on three datasets related to different applications: handwritten digit recognition (MNIST), Android malware detection (DREBIN), and face recognition (LFW). We then formally define transferability for both evasion and poisoning attacks, and show its approximate connection with the input gradients used to craft the corresponding attack samples (Section 4). Experiments are reported in Section 5, highlighting connections among regularization hyperparameters, the size of input gradients, and transferability of attacks, on different case studies involving handwritten digit recognition, Android malware detection, and face recognition. Supervised learning includes: (1) a training phase in which training data is given as input to a learning algorithm, resulting in a trained ML model; (2) a testing phase in which the model is applied to new data and a prediction is generated. In this paper, we consider a range of adversarial models against machine learning classifiers at both training and testing time.Attackers are defined by: (i) their goal or objective in attacking the system; (ii) their knowledge of the system; (iii) their capabilities in influencing the system through manipulation of the input data. This categorization enables the definition of many different kinds of attacks, ranging from white-box attacks with full knowledge of the target classifier to black-box attacks in which the attacker has limited information about the target system. This setting allows one to perform a worst-case evaluation of the security of machine-learning algorithms, providing empirical upper bounds on the performance degradation that may be incurred by the system under attack. These scenarios are more commonly known as poisoning [4,18,24,27,48] and evasion [3,8,14,42]. Gradientbased attacks have been considered for evasion (e.g., [3,8,14,23,42]) and poisoning (e.g., [4,18,24,27]). After defining our general formulation, we instantiate it for evasion and poisoning attacks, and use it to derive a new poisoning availability attack for logistic regression. Output: x : the adversarial example.1: Initialize the attack sample: x ← x 2: repeat 3:Store attack from previous iteration: x ← x 4:Update step: x ← Π Φ (x + η∇ x A(x,y,κ)), where the step size η is chosen with line search (bisection method), and Π Φ ensures projection on the feasible domain Φ.5: until |A(x , y, κ) − A(x,y,κ)| ≤ t 6: return x particular, as in the case of poisoning attacks, the attacker can maximize the objective by iteratively optimizing one attack point at a time [5,48]. For the line search, in our experiments we consider a maximum of 20 iterations. We finally remark that non-differentiable learning algorithms, like decision trees and random forests, can be attacked with more complex strategies [17,19] or using gradient-based optimization against a differentiable surrogate learner [31,37]. For images, the former constraint is used to implement either dense or sparse evasion attacks [12,25,37]. In particular, in terms of transferability, it is now widely acknowledged that higher-confidence attacks have better chances of successfully transfering to the target classifier (and even of bypassing countermeasures based on gradient masking) [2,8,13]. In addition to starting the gradient ascent from the initial point x, for nonlinear classifiers we also consider starting the gradient ascent from the projection of a randomly-chosen point of the opposite class onto the feasible domain. Crafting transferable poisoning availability attacks is much more challenging than crafting transferable poisoning integrity attacks, as the latter have a much more modest goal (modifying prediction on a small set of targeted points). As for the evasion case, we formulate poisoning in a whitebox setting, given that the extension to black-box attacks is immediate through the use of surrogate learners. The former, along with the poisoning point x , is used to train the learner on poisoned data, while the latter is used to evaluate its performance on untainted data, through the loss function L(D val , w ). Provided that the attacker function is differentiable w.r.t. w and x, the required gradient can be computed using the chain rule [4,5,24,27,48]:∇ x A = ∇ x L + ∂w ∂x ∇ w L ,(7)where the term ∂w ∂x captures the implicit dependency of the parameters w on the poisoning point x. Under some regularity conditions, this derivative can be computed by replacing the inner optimization problem with its stationarity (KarushKuhn-Tucker, KKT) conditions, i.e., with its implicit equation [24,27]. (7) to obtain the required gradient:∇ x A = ∇ x L − (∇ x c ∇ w L)(∇ 2 w L) −1 ∇ w L . Using logistic loss as the attacker's loss, the poisoning gradient for logistic regression can be computed as:∇ x c A = − ∇ x c ∇ θ L C z c θ ∇ 2 θ L X z C C z X C ∑ n i z i −1 X(y • σ − y) y (σ − 1) C,where θ are the classifier weights (bias excluded), • is the element-wise product, z is equal to σ(1 − σ), σ is the sigmoid of the signed discriminant function (each element of that vector is therefore:σ i = 1 1+exp(−y i f i ) with f i = x i θ + b), and:∇ 2 θ L = C n ∑ i x i z i x i + I,(11)∇ x c ∇ θ L = C(I • (y c σ c − y c ) + z c θx c )(12)In the above equations, I is the identity matrix. Model complexity is a measure of the capacity of a learning algorithm to fit the training data. Given an evasion attack point x , crafted against a surrogate learner (parameterized byˆwbyˆ byˆw), we define its transferability as the loss attained by the target classifier f (parameterized by w) on that point, i.e., T = (y, x + ˆ δ, w). (13), we can compute the loss increment ∆ = ˆ δ ∇ x (y, x, w) under a transfer attack in closed form; e.g., for p = 2, it is given as:∆ = ε ∇ x ˆ ∇ x ˆ 2 ∇ x ≤ ε∇ x 2 ,(16)where, for compactness, we usê = (y, x, ˆ w) and = (y, x, w). The first interesting observation is that transferability depends on the size of the gradient of the loss computed using the target classifier, regardless of the surrogate: the larger this gradient is, the larger the attack impact may be. We define the corresponding metric S(x, y) as:S(x, y) = ∇ x (y, x, w) q ,(17)where q is the dual of the perturbation norm. In Fig. 3 we report an example showing how increasing regularization (i.e., decreasing complexity) for a neural network trained on MNIST89 (see Sect. 5.1.1), by controlling its weight decay, reduces the average size of its input gradients, improving adversarial robustness to evasion. It is however worth remarking that, since complexity is a model-dependent characteristic, the size of input gradients cannot be directly compared across different learning algorithms; e.g., if a linear SVM exhibits larger input gradients than a neural network, we cannot conclude that the former will be more vulnerable.Another interesting observation is that, if a classifier has large input gradients (e.g., due to high-dimensionality of the input space and low level of regularization), for an attack to succeed it may suffice to apply only tiny, imperceptible perturbations. Differently from the gradient size S, gradient alignment is a pairwise metric, allowing comparisons across different surrogate models; e.g., if a surrogate SVM is better aligned with the target model than another surrogate, we can expect that attacks targeting the surrogate SVM will transfer better.R(x, y) = ∇ x ˆ ∇ x ∇ x ˆ 2 ∇ x 2 . The idea is to measure the variability of the loss functionˆwhenfunctionˆ functionˆwhen the training set used to learn the surrogate model changes, even though it is sampled from the same underlying distribution. Accordingly, if this loss landscape changes dramatically even when simply resampling the surrogate training set (which may happen, e.g., for surrogate models exhibiting a large error variance, like neural networks and decision trees), it is very likely that the local optima of the corresponding optimization problem will change, and this may in turn imply that the attacks will not transfer correctly to the target learner.We define the variability of the loss landscape simply as the variance of the loss, estimated at a given attack point x, y:V (x, y) = E D {(y, x, ˆ w) 2 } − E D {(y, x, ˆ w)} 2 ,(19)where E D is the expectation taken with respect to different (surrogate) training sets. Instead of defining transferability in terms of the loss attained on the modified test point, we define it in terms of the validation loss attained by the target classifier under the influence of the poisoning points. In particular, we analyze attack transferability in terms of its connection to the size of the input gradients of the loss function, the gradient alignment between surrogate and target classifiers, and the variability of the loss function optimized to craft the attack points. For each of the following learning algorithms, we train a high-complexity (H) and a low-complexity (L) model, by changing its hyperparameters: (i) SVMs with linear kernel (SVM H with C = 100 and SVM L with C = 0.01); (ii) SVMs with RBF kernel (SVM-RBF H with C = 100 and SVM-RBF L with C = 1, both with γ = 0.01); (iii) logistic classifiers (logistic H with C = 10 and logistic L with C = 1); (iv) ridge classifiers (ridge H with α = 1 and ridge L with α = 10); 2 (v) fully-connected neural networks with two hidden layers including 50 neurons each, and ReLU activations (NN H with no regularization, i.e., weight decay set to 0, and NN L with weight decay set to 0.01), trained via cross-entropy loss minimization; and (vi) random forests consisting of 30 trees (RF H with no limit on the depth of the trees and RF L with a maximum depth of 8). These configurations are chosen to evaluate the robustness of classifiers that exhibit similar test accuracies but different levels of complexity.How does model complexity impact evasion attack success in the white-box setting? PoisoningMNIST89 DREBIN MNIST89 LFW ε = 1 ε = 1 ε = 5 ε = 30 5% 20% 5% 20%SVM <1e-2 <1e-2 <1e-2 <1e-2 <1e-2 <1e-2 <1e-2 0.75 logistic <1e-2 <1e-2 <1e-2 0.02 <1e-2 <1e-2 0.10 0.21 ridge <1e-2 <1e-2 <1e-2 <1e-2 0.02 <1e-2 0.02 0.75 SVM-RBF <1e-2 <1e-2 <1e-2 <1e-2 <1e-2 <1e-2 <1e-2 0.11 NN <1e-2 <1e-2 <1e-2 0.02 Table 1: Statistical significance of our results. For each attack, dataset and learning algorithm, we report the p-values of two two-sided binomial tests, to respectively reject the null hypothesis that: (i) for white-box attacks, the test errors of the high-and low-complexity target follow the same distribution; and (ii) for black-box attacks, the transfer rates of the highand low-complexity surrogate follow the same distribution. In fact, even though logistic H exhibits the largest S value, it is not the most vulnerable classifier. In Fig. 7 we report the results for black-box evasion attacks, in which the attacks against surrogate models (in rows) are transferred to the target models (in columns). In particular, this can be seen best in the middle column for medium level of perturbation, in which the lower-complexity models (SVM L , logistic L , ridge L , and SVM-RBF L ) provide on average higher error when transferred to other models. This means that most classifiers can be attacked in this black-box setting with almost no knowledge of the model, no query access, but provided that one can get a small amount of data similar to that used to train the target model. On the vulnerability of random forests. A noteworthy finding is that random forests can be effectively attacked at small perturbation levels using most other models (see last two columns in Fig. 7). In Fig. 8, we report on the left the gradient alignment computed between surrogate and target models, and on the right the Pearson correlation coefficient ρ( ˆ δ, δ) between the perturbation optimized against the surrogate (i.e., the black-box perturbationˆδperturbationˆ perturbationˆδ) and that optimized against the target (i.e., the white-box perturbation δ). In Fig. 6d we also correlate gradient alignment with the ratio between the test error of the target model in the black-and white-box setting (extrapolated from the matrix corresponding to ε = 1 in the bottom row of Fig. 7), as suggested by our theoretical derivation. While this feature selection process does not significantly affect the detection rate (which is only reduced by 2%, on average, at 0.5% false alarm rate), it drastically reduces the computational complexity of classification.In each experiment, we run white-box and black-box evasion attacks on 1, 000 distinct malware samples (randomly selected from the test data) against an increasing number of modified features in each malware ε ∈ {0, 1, 2, . . . , 30}. To evaluate the impact of the aforementioned evasion attack, we measure the evasion rate (i.e., the fraction of malware samples misclassified as legitimate) at 0.5% false alarm rate (i.e., when only 0.5% of the legitimate samples are misclassified as malware). We apply our optimization framework to poison SVM, logistic, and ridge classifiers in the white-box setting. Designing efficient poisoning availability attacks against neural networks is still an open problem due to the complexity of the bilevel optimization and the non-convexity of the inner learning problem. Similarly to the evasion case, high-complexity models (with larger input gradients, as shown in Fig. 15a) are more vulnerable to poisoning attacks than their low-complexity counterparts (i.e., given that the same learning algorithm is used). For poisoning attacks, the best surrogates are those matching the complexity of the target, as they tend to be better aligned and to share similar local optima, except for low-complexity logistic and ridge surrogates, which seem to transfer better to linear classifiers. Interestingly, these error ratios are larger than one in some cases, meaning that attacking a surrogate model can be more effective than running a white-box attack against the target. According to our findings, for poisoning attacks, reducing the variability of the loss landscape (V) of the surrogate model is less important than finding a good alignment between the surrogate and the target. .51.62.58.62(c) ε = 30Figure 12: Black-box (transfer) evasion attacks on DREBIN. This results in better transferability.To summarize, for evasion attacks, decreasing complexity of the surrogate model by properly adjusting the hyperparameters of its learning algorithm provides adversarial examples that transfer better to a range of models. Biggio et al. [3] have been the first to consider evasion attacks against surrogate models in a limited-knowledge scenario. Goodfellow et al. [14], Tramer et al. [43], and Moosavi et al. [26] have made the observation that different models might learn intersecting decision boundaries in both benign and adversarial dimensions and in that case adversarial examples transfer better. Tramer et al. have also performed a detailed study of transferability of modelagnostic perturbations that depend only on the training data, noting that adversarial examples crafted against linear models can transfer to higher-order models. Liu et al. [21] have empirically observed the gradient alignment between models with transferable adversarial examples. In particular, we identify a set of conditions that explain transferability including the gradient alignment between the surrogate and targeted models, and the size of the input gradients of the target model, connected to model complexity. There is very little work on the transferability of poisoning availability attacks, except for a preliminary investigation in [27]. The lesson to system designers is to evaluate their classifiers against these criteria and select lower-complexity, stronger regularized models that tend to provide higher robustness to both evasion and poisoning. Applicationdependent scenarios such as cyber security might provide additional constraints on threat models and attack scenarios and could impact transferability in interesting ways.