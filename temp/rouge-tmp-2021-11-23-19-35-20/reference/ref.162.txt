We systematically address these limitations and propose a new methodology to test software for information leaks. In this work, we present DATA, a differential address trace analysis framework that detects address-based side-channel leaks in program binaries. Among several expected leaks in symmetric ciphers, DATA also reveals known and previously unknown leaks in asymmetric primitives (RSA, DSA, ECDSA), and DATA identifies erroneous bug fixes of supposedly fixed constant-time vulnerabilities. Many of these software-based attacks exploit addressbased information leakage to recover cryptographic keys of symmetric [6,36] or asymmetric [28,87] primitives.Various countermeasures against address-based information leakage have been proposed on an architectural level [52,62,81]. To address these issues, leakage detection tools have been developed that allow developers and security analysts to identify address-based side-channel vulnerabilities. Many static analysis methods use abstract interpretation [24,25,48,57] to give upper leakage bounds, ideally proving the absence of information leaks in already secured implementations, e.g., the evaluation of Salsa20 [24]. Thus, there is a fundamental trade-off between static analysis (minimizing false negatives) and dynamic analysis (minimizing false positives). Thus, we focus on dynamic analysis and tackle the limitations of existing tools. In particular, existing tools either focus on control-flow leaks or data leaks, but not both at the same time [80,89]; they consider the strongest adversary to observe cache-line accesses only [41], which is too coarsegrained in light of recent attacks (CacheBleed [88]); many of them lack the capability to properly filter program activity that is statistically independent of secret input [50,80,84]; and most do not provide any means to further assess the severity of information leaks, i.e., the risk they bring and the urgency with which they must be fixed. Detection accuracy: Minimize false positives, e.g., caused by non-determinism that is statistically independent of the secret input, and provide reasonable strategies to also reduce false negatives. Practicality: Report information leaks (i) fully automated, i.e., without requiring manual intervention, (ii) using only the program binary, i.e., without requiring the source code, and (iii) efficiently in terms of performance. In this work, we tackle these challenges with differential address trace analysis (DATA), a methodology and tool to identify address-based information leaks in application binaries. Leakage Detection: The second phase tests data and control-flow differences for dependencies on the secret input. These leakage tests are a valuable tool for security analysts to determine the severity and exploitability of a leak.We implement DATA in a fully automated evaluation tool that allows analyzing large software stacks, including initialization operations, such as key loading and parsing, as well as cryptographic operations. Especially microarchitectural components such as the CPU cache, the DRAM, and the branch prediction unit, where contention is based on memory addresses, enable powerful attacks that can be conducted from software only. For instance, attacks exploiting the different memory access times to CPU caches (aka cache attacks) range from timing-based attacks [11] to more fine-grained attacks that infer accesses to specific memory locations [61,64,87]. Xu et al. [86] demonstrated a new class of attacks on shielded execution environments like Intel SGX, called controlled-channel attacks. We consider a program secure if it does not contain address-based information leaks. Control-flow leakage occurs if code execution depends on secret inputs. Reparaz et al. [67] use Welch's t-test [83] to identify vulnerable cryptographic implementations. More advanced approaches use symbolic execution to give upper leakage bounds [63]. However, these approaches fall short for more fine-grained address-based attacks such as cache attacks. While a zero leakage bound guarantees absence of address-based side channels, a non-zero leakage bound could become rather imprecise (false positives) due to abstractions made on the data of the program. Abstraction also fundamentally prohibits analysis of interpreted code as it is encoded in the data plane of the interpreter. Dynamic analysis relies on concrete program executions, which possibly introduce false negatives. Zankl et al. [89] analyze modular exponentiations under the Hamming weight model, but they do not consider other leakage models and only detect control-flow leaks. CacheD [80] combines dynamic trace recording with static analysis introducing both, false negatives and false positives. For instance, Brumley and Hakala [19] as well as Gruss et al. [36] suggested to detect implementations vulnerable to cache attacks by relying on template attacks. Irazoqui et al. [41] use cache observations and a mutual information metric to identify control-flow and data leaks. Basu et al. [9] and Chattopadhyay et al. [20] quantify the information leakage in cache attacks.Orthogonal Work. Yet others demand source-code annotations [4,5,7] or specify entirely new languages [16]. In contrast, DATA filters keyindependent differences with a high probability, thereby reducing false positives even for non-deterministic program behavior. Although without formal guarantee, this gives evidence that DATA reduces false negatives successfully. Also, practical attacks only monitor a limited number of addresses or memory blocks. For DATA, we assume that the attacker can accurately observe full, noise-free address traces. In contrast, the recent Spectre [44] and Meltdown [51] bugs exploit not only address information but actual data which is speculatively processed but insufficiently isolated across different execution contexts. In the difference detection phase, we execute the target program multiple times with varying secret inputs and record all accessed addresses with dynamic binary instrumentation in so-called address traces. Thereby, we ensure to capture both, control flow and data leakages at their exact origin. This is done with specific leakage tests that find linear or non-linear relations between a given secret input and the previously recorded address traces. The idea of DATA is similar to differential power analysis (DPA) [46], which works on power traces. In contrast, address traces are noisefree, which minimizes the number of required measurements and allows perfect re-alignment for non-constant time traces (due to control-flow leaks). DATA is also related to differential computation analysis (DCA) [17]. A deterministic program P is leakage free if and only if no differences show up for any pair of secret inputs (k i , k j ):∀k i , k j : diff(trace(P(k i )), trace(P(k j ))) = ∅ (1) Data leakage is characterized by one and the same instruction (ip) accessing different memory locations (d). Execution with two different keys key A = [10,11,12] and key B = [16, 17, 18] yields two address traces t A = trace(P(key A )) and t B = trace(P(key B )), with differences marked bold: 0 p r o g r a m e n t r y : c a l l p r o c e s s w i t h u s e r -i n p u t 1 u n s i g n e d char LUT [ 1 6 ] = { 0 x52 , 2 0 x19 , . . . 16 0 x37 } ; 17 i n t t r a n s f o r m ( i n t k v a l ) { r e t u r n LUT[ k v a l %16]; } 18 i n t p r o c e s s ( i n t key [ 3 ] ) { 19 i n t v a l = t r a n s f o r m ( 0 ) ; 20 v a l += t r a n s f o r m ( key [ 0 ] ) ; 21 v a l += t r a n s f o r m ( key [ 1 ] ) ; 22 v a l += t r a n s f o r m ( key [ 2 ] ) ; 23 r e t u r n v a l ; } Listing 1: Table look-up causing data leak.Since the base address of LUT is 1, this operation leaks memory address kval + 1. The evidence is a set of leaking data addresses d. This yields the following address traces, where R, P, and T denote the data addresses of the variables r, p, and t.trace(P(k A )) = t A = [0, 1,2, 3, 4, (7, R), (8, P), (9, R),2, 3, 5,(7,T),(8,P),(9,T), 2, 3, 5,(7,T),(8,P),(9,T), 2, 6] trace(P(k B )) = t B = [0, 1,2, 3, 4, (7, R), (8, P), (9, R), 2, 3, 4,(7,R),(8,P),(9,R), 2, 3, 4,(7,R),(8,P),(9,R), 2, 6]There are two differences in the traces, both marked bold. As shown in Figure 1, we repeat this multiple times with varying inputs, causing address leaks to show up as differences in the address traces.The concept of DATA is agnostic to concrete recording tools and, hence, could also rely on other tools [71] or hardware acceleration like Intel Processor Trace (IPT) [39]. The trace comparison algorithm (diff) in Algorithm 1 sequentially scans a pair of traces (t A , t B ) for address differences, while continuously re-aligning traces in the same pass. Whenever ip values match but data addresses (d) do not, a data difference is detected (lines 4-6). Differences are reported using report data diff and report cf diff using the format specified in Section 4.1. If the calling depth drops below zero, the trace returned to the function's call-site. We stop scanning this trace (lines [15][16][17] and wait for the other trace to hit a merge point.1 rep = ∅, i = 0, j = 0 2 while i < |t A | ∧ j < |t B | do 3 a = t A [i], b = t B [i] 4 if a.ip = b.ip then 5 if a.d = b.d then 6 rep = rep ∪ report data diff(t A ,t B , i, j) 7 end 8 i++, j++ 9 else 10 rep = rep ∪ report cf diff(t A ,t B , i, j) 11 (i, j) = find merge point(t A ,t B , i, j1 k = i, l = j, C A = 0, C B = 0, S A = ∅, S B = ∅ 2 while k < |t A | ∧ l < |t B | do 3 if isCall(t A [k]) then C A ++ ; 4 if isRet(t A [k]) then C A --; 5 if isCall(t B [l]) then C B ++ ; 6 if isRet(t B [l]) then C B --; 7 if C A <= 0 then S A = S A ∪ t A [k]. Xin et al. [85] formalize the problem of relating execution points across different executions as execution indexing (EI). Specifically, EI requires knowledge of post-dominators, typically extracted from control flow graphs (CFGs) [30], which are not necessarily available (e.g., obfuscated binaries or dynamic code generation). Using EI, Johnson et al. [42] align traces in order to propagate differences back to their originating input. We use a similar intuition as Johnson et al. in processing and aligning traces in a single pass, however, without the need to make program execution indices explicit. We run our diff algorithm pairwise on all recorded traces and accumulate the results in an intermediate report. Testing multiple traces helps capture nested leakage, that is, leakage which appears conditionally, depending on which branches are taken in a superordinate control-flow leak. Evidence traces contain all essential information exploited in practical attacks, such as how often an address is accessed [11,45] and also when, i.e., at which position an address is accessed in the trace [87]. Addresses accessed in case of data differences are written to the trace in chronological order. As we execute the target program with multiple inputs, we accumulate the evidence traces Implications. Such a leak would occur, if the secret permutes the addresses in the evidence traces, e.g., [r 1 , r 2 ] and [r 2 , r 1 ], while the length of the evidence traces as well as the number of accesses per address remains the same. It is closely related to the Kolmogorov-Smirnov (KS) test but performs better when distributions differ in the tails instead of around the median. Since we do not assume anything about the tested distributions, we choose the increased sensitivity of Kuiper's test over the KS test at almost identical computational cost.In preparation for Kuiper's test, we normalize our previously compiled histograms to obtain probability distributions. The value (1 − α) determines the probability with which Kuiper's test produces false positives. False negatives can occur, if the histograms H addr and H pos are insufficient estimations of the underlying evidence distributions. It is, however, a common problem of unspecific leakage testing to determine a required minimum number [55,72]. Analysts using DATA should therefore add traces until the test results stabilize and no new leaks are detected. M ev pos stores the position of each address in the evidence trace.If an address does not occur in a trace, the matrix entry is set to '-1'. After adding the data, we obtain:M ev addr =   1 1 1 1 1 2 0 3 0   , M ev pos =   0 4 1 1 2 1 -1 1 -1   The transformation of the input is called leakage model. It defines which property or part of the secret input is compared to the evidence representations stored in M ev addr and M ev pos . Clearly, the choice of an appropriate leakage model is important, but ultimately depends on the target program. Instead of gathering new measurements, we reuse the (random input) traces from the leakage detection phase. For these comparisons, the selected rows are interpreted as pairwise observations of two random variables, X and Y , with length n X = n Y = n. While the concept of DATA is platform independent, we implement trace recording on top of the Intel Pin frame- work [38] for analyzing x86 binaries. We implement the difference detection as well as the generic and the specific leakage tests in Python scripts, which condense all findings into human-readable leakage reports in XML format, structuring information leaks by libraries, functions, and call stacks. During trace analysis, this could cause the same objects to be interpreted as different ones. Although debug symbols are not required by DATA, it incorporates available debug symbols in the final report. Table 2 shows the results of the three phases of DATA, namely address differences, generic and specific leaks. Besides, we identified four data leaks in the bit-sliced AES. All other tested symmetric implementations yield a significant number of data leaks since they rely on lookup tables with key-dependent memory accesses, which makes them vulnerable to cache attacks [11,77]. Among those, we found two constant-time vulnerabilities in RSA and DSA, respectively, which bypass constant-time implementations in favor of vulnerable implementations. Moreover, DATA reconfirms address differences in the ECDSA wNAF implementation, as exploited in [10,26,79]. This can cause different heap addresses for different key lengths, which will show up as data leakage. We tested PyCrypto 2.6.1 running on CPython 2.7.13. We analyzed Curve25519 in NaCl [13] as well as the corresponding Diffie-Hellman variant of OpenSSL (X25519) and found no addressbased information leakage (apart from OpenSSL's key parsing), approving their side-channel security. Indeed, Table 2 shows that the leakage detection phase confirms all differences as leaks.To evaluate DATA's accuracy on non-deterministic programs, we tested OpenSSL asymmetric ciphers and collected up to 30 traces, as shown in Figure 4. While the address differences found in the difference detection phase do not settle within 30 traces (introducing false negatives), an important finding is that the majority of these differences are due to statistically independent program activity, e.g., RSA base blinding. Analysis of the leakage-free AES-NI and AES-VP took around 6 s, as only the first phase is needed. Analyzing PyCrypto yields large address traces due to the interpreter (1GB and more), nevertheless DATA handles such large traces without hassle: The first phase discards all non-leaking instructions, stripping down trace sizes of the subsequent phases to kilobytes (see Appendix B). Also, the bit-sliced AES adopted by OpenSSL leaks during the key schedule, as the developers integrated it only partially [43] since practical attacks have not been shown yet. Considering incomplete bug fixes of similar vulnerabilities identified by Garcia et al. [27,28], this sums up to four implementation bugs related to the same countermeasure. The leakage classification phase helps in rating its severity, however, an accurate judgment often demands significant effort in assembling and improving concrete attacks [12]. Coppens et al. [21] proposed compiler transformations to eliminate keydependent control-flow dependencies. Oblivious RAM [32,77,91] has been proposed as a generic countermeasure against data leaks by hiding memory access patterns. In this work, we proposed differential address trace analysis (DATA) to identify address-based information leaks. We use statistical tests to filter non-deterministic program behavior, thus improving detection accuracy.DATA is efficient enough to analyze real-world software -from program start to exit. In addition, we showed that DATA is capable of analyzing interpreted code (PyCrypto) including the underlying interpreter, which is conceptually impossible with current static methods. In particular, the leaks occur in function set hex, which uses stdlib's isxdigit function that performs leaking table lookups. For the analysis of asymmetric ciphers, we use OpenSSL to generate keys in PEM format and then invoke the openssl pkeyutl command-line tool to create signatures with those keys. This is a programming bug: the so-called constant-time flag is set for p and q in function rsa ossl mod exp but not propagated to temporary working copies inside BN MONT CTX set, as shown in Listing 3, since the function BN copy in line 3 does not propagate the consttimeflag from mod to mont->N. Table 3 summarizes the performance figures of DATA for each phase. Yet, these results are quite encouraging, especially since the automated analysis of large real-world software stacks is out of reach for many existing tools. Analysis times vary heavily between ciphers, because the performance critically depends on the number of reported address leaks and the size of the evidences, which need to be classified. This is because, first, DATA performs more specific leakage tests than generic ones (H addr/pos vs. M ev addr/pos ), and second, the RDC is more costly to compute than Kuiper's test. Moreover, when doing frequent testing, software developers could not only omit the leakage classification phase intended for security analysts but also skip the leakage detection phase in case of deterministic algorithms.