We describe DIZK, a system that distributes the generation of a zero knowledge proof across machines in a compute cluster. Cryptographic proofs with strong privacy and efficiency properties, known as zkSNARKs (zero-knowledge Succinct Non-interactive ARgument of Knowledge) [52,38,19], have recently received much attention from academia and industry [13,9,41,51,20,37,55,11,15,48,78,31,33,10,75,31,46,47,53,36,22], and have seen industrial deployments [7,5,3,4]. In contrast, zkSNARKs enable users to broadcast encrypted transactions details and prove the validity of the payments without disclosing what the payments are.More formally, zkSNARKs allow a prover (e.g., a Zcash user making a payment) to convince a verifier (e.g., any other Zcash user) of a statement of the form "given a function F and input x, there is a secret w such that F(x, w) = true". In the cryptocurrency example, w is the private payment details, x is the encryption of the payment details, and F is a predicate that checks that x is an encryption of w and w is a valid payment. State-of-the-art zkSNARK systems [59] can only support statements of up to 10-20 million gates, at a cost of more than 1 ms per gate. Let us put this size in perspective via a simple example: the SHA-256 compression function, which maps a 512-bit input to a 256-bit output, has more than 25,000 gates [10]; no more than 400 evaluations of this function fit in a circuit of 10 million gates, and such a circuit can be used to hash files of up to a mere 13 kB. DIZK enables applications on significantly larger instance sizes, e.g., image editing on photos of 2048 by 2048 pixels.DIZK makes a significant conceptual step forward, enlarging the class of applications feasible for zkSNARKs. We implement DIZK via Apache Spark [2] and will release all source code under a permissive software license.DIZK does inherit important limitations of zkSNARKs (see §13). But each node in a compute cluster can store only a small fraction of the overall state, and thus memory is distributed and communication between nodes incurs network delays. Specifically, for finite fields, DIZK provides distributed FFTs and distributed Lagrange interpolant evaluation ( §4.1); for finite groups, it provides distributed multi-scalar multiplication with fixed bases and with variable bases ( §4.2). Different parts of a zkSNARK leverage the sparsity of the matrices above in different ways: the so-called QAP instance reduction relies on their column sparsity ( §5), while the corresponding QAP witness reduction relies on their row sparsity ( §6). However, it turns out that the columns and rows are almost sparse: while most columns and rows are sparse, some are dense, and the dense ones create stragglers.We address this issue via a two-part solution. Our experiments show that DIZK enables such applications to scale to much larger instance sizes than what is possible via previous (monolithic) systems.An application uses DIZK by constructing a circuit for the desired computation, and by computing values for the circuit's wires from the application inputs. A zkSNARK can be used to prove/verify statements of the form "given a public predicate F and a public input x, I know a secret input w such that F(x, w) = true". • The setup receives a predicate F (expressed in a certain way as discussed in §2.2) and outputs a proving key pk F and verification key vk F . • The prover receives the proving key pk F , a public input x for F, and a secret input w for F, and outputs a proof π. A zkSNARK's costs are determined by the 'execution time' T F of F (see §2.2) and the size k of the input x (which is at most T F ). Thus, T F is seen to be significantly larger than k.The key efficiency feature of a zkSNARK is that the verifier running time is proportional to k alone (regardless of T F ) and the proof has constant size (regardless of k, T F ). Values are in a field F of a large prime order p.An R1CS instance φ over F is parameterized by the number of inputs k, number of variables N (with k ≤ N), and number of constraints M; φ is a tuple (k, N, M, a, b, c) where a, b, c are (1 + N) × M matrices over F.An input for φ is a vector x in F k , and a witness for φ is a vector w in F N−k . An input-witness pair (x, w) satisfies φ if, letting z be the vector F 1+N composed of 1, x, and w, the following holds for all j ∈ [M]:∑ N i=0 a i, j z i · ∑ N i=0 b i, j z i = ∑ N i=0 c i, j z i . On input a proving key pk (for an R1CS instance φ ), input x in F k , and witness w in F N−k , P outputs a proof π that attests to the x-satisfiability of φ . On input a verification key vk (generated for φ ), input x in F k , and proof π, V outputs a decision bit. In fact, it suffices for the verifier to know that this equation holds at a random point because distinct polynomials of small degree can only agree on a small number of points.In a little more detail, we now define what is a QAP instance, and what does satisfying such an instance mean.A QAP instance Φ over F has three parameters, the number of inputs k, number of variables N (with k ≤ N), and degree M; Φ is a tuple (k, N, M, A, B, C, D) where A, B, C are each a vector of 1 + N polynomials over F of degree < M, and D is a subset of F of size M.An input for Φ is a vector x in F k , and a witness for Φ is a pair (w, h) where w is a vector in F N−k and h is a vector in F M−1 . An input-witness pair x, (w, h) satisfies Φ if, letting z ∈ F 1+N be the concatenation of 1, x, and w:∑ N i=0 A i (X)z i · ∑ N i=0 B i (X)z i = ∑ N i=0 C i (X)z i + ∑ M−2 i=0 h i X i · Z D (X) ,whereZ D (X) := ∏ α∈D (X − α). For every witness w inF N−k s.t. (x, w) satisfies φ , qapW(φ , x, w) outputs h in F M−1 s.t. (x, (w, h)) satisfies Φ.It works as follows: let h be the coefficients of the polynomial H(X) of degree less than M − 1 that equals the quotient of(∑ N i=0 A i (X)z i ) · (∑ N i=0 B i (X)z i ) − ∑ N i=0 C i (X)z i and Z D (X). The encoding of a scalar can be efficiently computed via the double-and-add algorithm; yet (for suitable choices of G) its inverse is conjecturally hard to compute, which means that [s] hides (some) information about s. Encodings are also linearly homomorphic:[αs + βt] = α[s] + β [t] for all α, β , s,t ∈ F.Bilinear encodings involve three groups of order p:G 1 , G 2 , G 3 generated by G 1 , G 2 , G 3 respectively. Moreover,there is an efficiently computable map e :G 1 × G 2 → G 3 , called pairing, that is bilinear: for every nonzero α, β ∈ F, it holds that e ([α] 1 , [β ] 2 ) = αβ · e (G 1 , G 2 ). For example, given [s] 1 , [t] 2 , [u] 1 , one can test if st + u = 0 by testing if e ([s] 1 , [t] 2 ) + e ([u] 1 , [1] 2 ) = [0] 3 .3 Design overview of DIZK Fig. 2 shows the outline of DIZK's design. • The prover receives the proving key pk, input x in F k , and witness w in F N−k . When using DIZK in an application, the application setup needs to provide φ to the DIZK setup, and the application prover needs to provide x and w to the DIZK prover. In Groth's protocol these fall in three categories: (1) arithmetic (multiplication and division) for polynomials of large degree over large prime fields; (2) multi-scalar multiplication over large prime groups; (3) the QAP instance and witness reductions described in §2.3. Fig. 3 presents a diagram of the main parts of the design, and we describe them in the following sections: §4 discusses how to distribute polynomial arithmetic and multi-scalar multiplication; §5 discusses how to distribute the QAP instance reduction, and how to obtain the distributed setup from it; §6 discusses how to distribute the QAP witness reduction, and how to obtain the distributed prover from it. In light of this, our approach is the following: (i) we achieve distributed fast implementations of evaluation and interpolation, and (ii) use these to achieve distributed fast polynomial arithmetic such as multiplication and division.Recall that (multi-point) polynomial evaluation is as follows: given a polynomial P(X) = ∑ n−1 j=0 c j X j over F and elements u 1 , . . . , u n in F, compute the elements P(u 1 ), . . . , P(u n ). One can do this by evaluating P at each point, costing Θ(n 2 ) field operations overall.Conversely, polynomial interpolation is as follows: given elements u 1 , v 1 , . . . , u n , v n in F, compute the polynomial P(X) = ∑ n−1 j=0 c j X j over F such that v i = P(u i ) for every i ∈ {1, . . . , n}. At each layer of the butterfly network, half of the executors are left idle and the other half have their memory consumption doubled; moreover, each such layer requires a shuffle involving the entire array.We take a different approach, suggested by Sze [65], who studies the problem of computing the product of terabit-size integers on compute clusters, via MapReduce.Sze's approach requires only a single shuffle. An additional task that arises (in the setup, see §5) is a problem related to polynomial evaluation that we call Lag (from 'Lagrange'): given a domain {u 1 , . . . , u n } ⊆ F and an element t ∈ F, compute the evaluation at t of all Lagrange interpolants L 1 (X), . . . , L n (X) for the domain.A common approach to do so is via the barycentric Lagrange formula [17]: compute the barycentric weights r 1 , . . . , r n as r i := 1/ ∏ j =i (u i − u j ), and then computeL 1 (t), . . . , L n (t) as L i (t) := r i t−u i · L(t) where L(X) := ∏ n j=1 (X − u j ). In addition to the expensive finite field arithmetic discussed above, the setup and prover also perform expensive group arithmetic, which we must efficiently distribute.After obtaining the evaluations of Θ(N + M) polynomials, the setup encodes these values in the groups G 1 and G 2 , performing the operations s → [s] 1 and s → [s] 2 for Θ(N + M) values of s. Given a vector of scalars s in F n and a vector of elements(P i ) n i=1 in G n , compute ∑ n i=1 s i · P i in G.For small n, both problems have simple solutions: for fixMSM, compute each element s i · P and output it; for varMSM, compute each s i · P i and output their sum.In our setting, these solutions are expensive not only because n is huge, but also because the scalars are (essentially) random in F, whose cryptographically-large prime size p has k ≈ 256 bits. Given scalars s 1 , . . . , s n and their bases P 1 , · · · , P n , Pippenger's algorithm chooses a radix 2 c , computes s 1 /2 c P 1 + · · · + s n /2 c P n , doubles it c times, and sums it to (s 1 mod 2 c )P 1 + · · · + (s n mod 2 c )P n . For the last step, the algorithm sorts the base elements into 2 c buckets according to (s 1 mod 2 c ), . . . , (s n mod 2 c ) (discarding bucket 0), sums the base elements in the remaining buckets to obtain intermediate sums Q 1 , . . . , Q 2 c −1 , and computes Q 1 +2Q 2 +· · ·+(2 c −1)Q 2 c −1 = (s 1 mod 2 c )P 1 + · · · + (s n mod 2 c )P n . As a simple example, via log |F| group operations, one can compute the table (P, 2 · P, 4 · P, . . . , 2 log |F| · P), and then compute each s i · P with only log |F|/2 group operations (on average). The zkSNARK setup receives as input an R1CS instance φ = (k, N, M, a, b, c) and produces a proving key pk and a verification key vk.Informally, the protocol has three stages: (i) evaluate the polynomials A, B, C at a random element t, where A, B, C are from the QAP instance Φ = (k, N, M, A, B, C, D) corresponding to φ ; (ii) compute certain random linear combinations of these; (iii) compute encodings of corresponding vectors. Thus here we discuss efficient distribution of the first stage only.Recall from the QAP instance reduction (in §2.3) that A = (A 0 , . . . , A N ) where A i is the polynomial of degree < M that interpolates over D the i-th row of the matrix a; similarly for each B and C with regard to b and c. Focusing on a for simplicity and letting L 1 , . . . , L M be the Lagrange interpolants for the set D (i.e., L j evaluates to 1 at the j-th element of D and to 0 everywhere else in D), the task we need to solve in a distributed way is:in: a ∈ F (1+N)×M and t ∈ F out: (A i (t)) N i=0 where A i (t) := ∑ M j=1 a i, j L j (t)The parameters N and M are big enough such that no single machine can store any vector of length N or M.In both serial zkSNARK systems and in our distributed system, the first step is to compute (L j (t)) M j=1 . When running this computation, we encounter notable issues at every step: the set of joined pairs (a i, j , L j (t)) is unevenly distributed among executors, the executors take drastically differing amounts of time to perform the pair evaluations, and a small set of executors quickly exceed memory bounds from insufficient heap space.Our problems lie in that, while the matrix a is sparse, its columns are merely almost sparse: most columns are sparse, but a few are dense. While there exist alternative join algorithms to handle load imbalances, like blockjoin and skewjoin [6], these do not perform well, as we now explain.First, blockjoin replicates each entry in one RDD (the one for (L j (t)) j ) in the hopes that when joining with the other RDD (the one for (a i, j ) i, j ) the partitions will be more evenly distributed. However, in our setting we cannot afford blowing up the size of the first RDD.Second, skewjoin takes a more fine-grained approach, by computing statistics of the second RDD and using it to calculate the replication factor for each entry in the first RDD. Before running the setup, DIZK runs a lightweight, distributed computation to identify the columns that have many non-zero elements and annotates them for Part 2. Thus even dense columns will have at most values to aggregate, avoiding stragglers.DIZK identifies which columns have more than a threshold of non-zero elements and annotates them for Part 2. DIZK now executes two jobs: one for the few dense columns, and one for the many sparse columns. The first computation filters each dense column into multiple partitions, so that no executor deals with an entire dense column but only with a part of it, and evaluates the joined pairs. The zkSNARK prover receives a proving key pk, input x in F k , and witness w in F N−k , and samples a proof π. Thus here we discuss efficient distribution of the first stage only.Recall from the QAP witness reduction (in §2.3) that h is the vector of coefficients of the polynomial H(X) of degree less than M − 1 that equals the ratio(∑ N i=0 A i (X)z i ) · (∑ N i=0 B i (X)z i ) − ∑ N i=0 C i (X)z i Z D (X). Since the evaluation of A i on D equals the i-th row of a, the task that needs to be solved in a distributed way is the following.in: a ∈ F (1+N)×M and z ∈ F 1+N out: (∑ N i=0 a i, j z i ) M j=1Again, the parameters N and M are huge, so no single machine can store an array with N or M field elements.Strawman. When running this computation, we ran into a stragglers problem that is the converse of that described in §5: while matrix a is sparse, its rows are almost sparse because, while most rows are sparse, some rows are dense. The dense vectors depend on the constraints alone so they do not change during proving, even for different inputs x. Hence, Part 1 runs once during setup, and not again during proving (only Part 2 runs then). The other task is mapping the application inputs to a satisfying assignment to the constraints, to pass as input to the prover.Recall that our distributed zkSNARK expects the R1CS instance (set of constraints) and witness (assignment) to be distributed data structures (see §3). A recent paper, PhotoProof [53], proposes an approach that relies on a combination of special signature signing cameras and zkSNARKs to prove, in zero knowledge, that an edited image was obtained from a signed (and thus valid) input image only according to a set of permissible transformations. More precisely, the camera actually signs a commitment to the input image, and this commitment and signature also accompany the edited image, and thus can be verified separately.We benchmark our system on this application because the original PhotoProof relies on monolithic zkSNARK implementations and is thus limited to small photo sizes. Some pixels go outside the image and are thus lost, while new pixels appear and are set to zero.We follow the approach of [53], and use the method of rotation by shears [54], which uses the identity cos θ − sin θsin θ cos θ = 1 − tan(θ /2) 0 1 1 0 sin θ 1 1 − tan(θ /2) 0 1 . The solution to the optimization problem is w = (X T X) −1 X T Y . This simple approach works well for us because memory usage is dominated by the number of constraints and variables rather than the size of the input/output matrices.Covariance matrix. Its covariance matrix is M := 1 n−1 ∑ n i=1 (x i − ¯ x)(x i − ¯ x) T ∈ R d×d , where ¯ x := ( 1 n ∑ n i=1 x i ) ∈ R d×1is the average of the n observations.To verify M, we first check the correctness of ¯ x by individually checking each of the d entries; for each entry we use the same approach as in the case of blur (in §7.1). This means that G 1 and G 2 are elliptic curve groups of a prime order p of 256 bits, and the scalar field F has this same size.An important technicality is that we cannot rely on curves used in prior zkSNARK works, because they do not support the large instance sizes in this work, as we now explain. Thus, we simply use libsnark's implementation of the verifier [59], whose running time is ≈ 2 ms + 0.5 µs · k where k is the number of field elements in the R1CS input (not a large number in typical applications). We ran experiments (32 and 64 executors for all feasible instances) comparing the performance of the setup and prover with two implementations: (1) the implementation that is part of DIZK, which has optimizations described in the design sections ( §4, §5, §6); and (2) an implementation that does not employ these optimizations (e.g., uses skewjoin instead of our solution, and so on). Fig. 8a and Fig. 8b show the time for constraint and witness generation when fixing the number of executors and increasing the instance size (as determined by the number of constraints); the graphs show that time scales nearly linearly, which means that the algorithm parallelizes well with respect to instance size. The zkSNARK setup and prover in prior implementations run on a single machine.Some recent work explores zero knowledge proofs based not on probabilistic checking techniques and do not offer constant-size proofs, but whose provers are cheaper (and need no setup). DIZK does not assume trusted hardware, and thus protects against a wider range of attackers at the prover than these approaches.While we are excited about scaling to larger circuits, zkSNARKs continue to suffer from important limitations.First, even if DIZK enables using zkSNARKs for much larger circuits than what was previously possible, doing so is still very expensive (we resort to using a compute cluster!) While prior systems only support circuits of up to 10-20 million gates (at a cost of 1 ms per gate in the prover), DIZK leverages the combined CPU and memory resources in a cluster to support circuits of up to billions of gates (at a cost of 10 µs per gate in the prover). The verifier V receives a verification key vk, input x in F k , and proof π, and, letting x 0 := 1, checks that the following holds: e ([A r ] 1 , [B s ] 2 ) = e (α, β ) +e k ∑ i=0 x i [K vk i (t)] 1 , [γ] 2 + e ([K r,s ] 1 , [δ ] 2 ) .