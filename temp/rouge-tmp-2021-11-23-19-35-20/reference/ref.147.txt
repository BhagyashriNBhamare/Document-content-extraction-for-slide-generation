Regular expression denial of service (ReDoS) is a class of algorithmic complexity attacks where matching a regular expression against an attacker-provided input takes unexpectedly long. In the course of the study, we identify 25 previously unknown vulnerabilities in popular modules and test 2,846 of the most popular websites against them. For example, matching the apparently harmless regular expression /(a+)+b/ against a sequence of 30 "a" characters on the Node.js JavaScript platform takes about 15 seconds on a standard computer. 1 Matching a sequence of 35 "a" characters already takes over 8 minutes, i.e., the matching time explodes exponentially.If a server implementation suffers from this kind of performance problem, then an attacker can exploit it to overwhelm the server with hard-to-match inputs. As a result, a single request can effectively block the main thread, making the web server unresponsive to any other incoming requests and preventing it from finishing any other already established requests.Despite the importance of ReDoS in web servers, there is currently little reported knowledge about the prevalence of ReDoS vulnerabilities in real-world websites. We seek to answer the following questions:• How widespread are ReDoS vulnerabilities in the server-side part of real-world JavaScript-based websites? We address this challenge by triggering requests with increasing input size, using both manually crafted exploit inputs and randomly generated, harmless inputs, and by statistically comparing the response times.Using this methodology, we identify 339 websites that suffer from at least one ReDoS vulnerability. Davis et al. [11] confirm that such problems exist in popular modules and report that 5% of the security vulnerabilities identified in Node.js libraries are ReDoS. For example, Wüstholz et al. [43] propose a static analysis of ReDoS vulnerabilities in Java. Our work shows the urgent need for effective tools and techniques that detect and prevent ReDoS vulnerabilities in JavaScript.In summary, this paper contributes the following:• A novel methodology for analyzing the exploitability of deployed servers. • A benchmark of previously unreported ReDoS vulnerabilities and ready-to-use exploits, which we make available for future research on finding, fixing, and mitigating ReDoS vulnerabilities:https://github.com/sola-da/ReDoS-vulnerabilities Regular expressions are used to check whether a given sequence of characters matches a specified pattern. After multiple explorations the algorithm identifies the sequence of transitions s → 3 → 4 → 5 → 4 → 5 → 6 → 7 → a, which reaches the accepting state and consumes all characters of the input string. For example, consider the regular expression /^a*a*b$/, its automaton in Figure 2, and the input string "aaa". JavaScript is becoming more and more popular, including the server-side Node.js platform, which advocates a single-threaded, event-based execution model that uses servers, such as Apache, the single-threaded execution model compounds the problem in JavaScript. To make matters worse, even less severe ReDoS payloads can significantly degrade the availability of a Node.js server, as we show in Section 4.3. The overall goals of the methodology are to understand (i) how widespread such vulnerabilities are, (ii) whether an attacker could exploit them to affect the availability of live websites, and (iii) to what extent existing defense mechanisms address the problem. More precisely, we analyze thirdparty libraries, called node package manager modules (npm packages or npm modules for short), to find vulnerabilities that may be exploitable via HTTP requests. To decide on the size of payloads sent to live websites, we run experiments on locally installed web servers that use the vulnerable packages.An alternative to experimenting with live websites would be to locally install open-source web applications. Figure 4 shows the number of Express-based websites in batches of 100,000 sites, ordered by popularity. Similar to previous work [43], we consider a regular expression to be vulnerable if we can construct inputs of linearly increasing size that cause the matching time of the expression to increase superlinearly. At first, we download the 10,000 most popular modules and extract their regular expressions by traversing the abstract syntax trees of the JavaScript code. After removing regular expressions that contain no repetitions, and hence are immune to algorithmic complexity attacks, we obtain a total of 138,123 expressions, with mean 37.93 and median 4.00 per module.Next, we semi-automatically search for regular expression patterns that are known to be vulnerable. To this end, we focus on (i) modules included in the Express framework, (ii) middleware modules that extend this framework, and (iii) modules that manipulate HTTP request components, such as the body or a specific header. Based on the ReDoS vulnerabilities in npm modules, we create exploits targeted at web servers that use these modules. Next, we try to create an HTTP request where user-controlled data reaches the vulnerable regular expression, and craft input values that trigger an unusually long matching time. If we succeed in crafting an input that takes more than five seconds, we consider the vulnerability as exploitable and consider it for the remainder of the study.To further assess the impact of the exploits, we measure how much longer it takes to process a crafted input compared to a random string of the same length. The basic idea is to repeatedly measure the response time and to conclude that crafted inputs cause a higher response time than random inputs only if we observe a statistically significant difference.More specifically, to measure the response time for a given input, we first repeat the request n w times to "warm up" the connection, e.g., to fill network caches, and then repeat the request another n m times while recording the response times. Given k pairs of increasingly large random and crafted inputs (i random , i cra f ted ), where the two inputs in a pair have the same size, we obtain k pairs (T random and T cra f ted ) of sets of time measurements (with |T random | = |T cra f ted | = n m ). Intuitively, this means that the response times for random and crafted inputs have a statistically significant difference, and that this difference increases when the input size increases.To execute these measurements, we need to pick values for n w , n m , k, and the k input sizes. We use n w =three, n m =five, and k = 5 because these values are large enough to draw statistically relevant conclusions for most websites yet small enough to not disturb the analyzed server. We address this challenge by experimenting on a locally installed version of the vulnerable package and by choosing input sizes that take approximately 100ms, 200ms, 500ms, 1s and 2s to respond to.Our setup allows us to assess whether a website could be exploited without actually attacking it. Moreover, the servers of popular websites implement some kind of redundancy, such as multiple Node.js instances in a cluster, i.e., our measurements are likely to block only one such instance at a time. For each vulnerability, we have contacted the developers either directly or through the Node Security Platform 6 , and gave them several months to fix the problem before making it public. 7 As explained in Section 3.3, we try to create exploits for the vulnerabilities by hypothesizing how web server implementations may use the vulnerable modules. For all the scenarios we assume the payload is sent using a specific HTTP header. 4 forwarded / * , * / X- Forwarded- ForThe website uses express and the "trust proxy" option is set. For an attack targeted at a specific website, we believe that more complex scenarios could be built, e.g., involving multiple HTTP requests and domain knowledge. Therefore, we only consider very simple usage scenarios that can be triggered with a single HTTP request made to the main page.To better understand the vulnerabilities, Figure 6 shows for each vulnerable module the vulnerable regular expressions. For most of the exploits, the input dependency seem to be quadratic, reaching one second matching time within 20,000 to 40,000 characters. We consider any of these eight exploits to be harmful because they may impact a website's availability (Section 4.3 and because even a non-exponential ReDoS vulnerability may aid an attacker in mounting a DoS attack (Section 5.1). At the same time, the other machine, called the attacker, delivers 1,000 ReDoS payloads, by triggering all 1,000 requests at once. The victim machine starts its requests immediately after the victim machine has triggered its requests.We vary the payload size from 0 characters to 8,000 characters in increments of 1,000 characters. We chose the upper limit for the payload size because, by default, the web server provider limits the size of the header fields to 8,500 characters. For the largest payloads we use, we even experienced dropping of requests.This result is particularly remarkable because an individual payload of size 4,000 does not require an immense amount of time to respond to. This finding shows that the ReDoS payloads have a cumulative effect and even a small delay in the main loop can cause significant harm for availability.We remind the reader that the above experiment uses the smallest payload in our data set, forwarded. For more severe ReDoS vulnerabilities, e.g. in ua-parser-js, there is even no need to evaluate the impact on availability. Of course, the observed value depends on the chosen web server Module P1: P2: P3: P4: P5: 100ms 200ms 500ms 1s 2s fresh 12,000 17,000 27,000 37,500 53,500 forwarded 12,000 17,000 26,500 38,000 53,500 useragent 500 650 925 1,150 1,450 ua-parser-js 38 39 40 41 42 mobile-detect 10,500 15,500 25,000 36,500 50,500 platform 7,500 11,000 17,500 25,000 34,500 charset 10,500 15,500 24,000 34,000 48,000 content 8,000 11,000 18,000 25,500 35,500Figure 10: Number of characters in each payload needed to achieve a specific delay in a vulnerable module.provider and the current server load, but we can safely conclude that measuring time at the client level is a good enough estimation of the server-side computation time. We consider five target matching times, 100ms, 200ms, 500ms, 1s, and 2s, and choose the payload size that produces the closest matching time to the target time. The useragent and ua-parser-js packages, whose matching times grow at a much faster rate, requiring less than 1,500 characters to cause a delay of 2s. Based on the five payload sizes for each exploit, we create attack payloads and random payloads for each exploit and payload size. Given that our methodology is designed to underestimate the number of affected sites, e.g., because we consider only eight exploits, the actual number of ReDoS-vulnerable sites is likely to be even higher. The response time grows significantly faster for the malicious payloads in the vulnerable site, reaching slightly more than two seconds for the fifth payload. For example, the vulnerability in the popular useragent affects more websites than the vulnerability in the less used charset module. After more careful consideration, we realized that there are two more popular alternatives for parsing the Content-Header and the content package seems to be more popular among users of the hapi.js framework, which is a competitor of Express. Likewise, the distribution is also good news for the community, showing that one can lower the risk of ReDoS in multiple websites by fixing a relatively small set of popular packages. For each point p on the horizontal axis, the vertical axis shows the number of exploitable sites with popularity rank ≤ p. For example, there are 61 vulnerable sites in the top 100,000 websites, with one site in top 1,000 and nine in top 10,000. As can be observed, most websites accept headers that are smaller than 10,000 characters, but only few websites accept headers that are, for instance, 40,000 characters long. Therefore, the current limits that the websites apply on the header size are insufficient and they do not provide adequate protection against DoS.Another interesting trend to observe in Figure 14 is that even for the most harmful exploit, useragent, for which we require payloads between 38 and 42 characters only, the number of websites that accept larger payloads decreases over time. In this section, we discuss the potential of a large-scale DoS attack on Node.js websites and some defenses we recommend to minimize the impact of such an event. As shown in Section 4.3, even the least harmful vulnerabilities we identify can be a lethal weapon when used as part of a large-scale DoS attack, because the attacker can send payloads that hang the loop for hundreds of milliseconds, several seconds, or even more, depending on the vulnerability. In contract, in an event-based system, the matching is done in the main loop and spending a few seconds matching a regular expression is equivalent to completely blocking the server for this amount of time.A large-scale ReDoS attack against Node.js-based sites is a bleak scenario for which, as we have shown, many websites are not prepared. In contrast, vulnerabilities related to other inputs received from the network, e.g., the body of an HTTP request, would remain exploitable.Another defense mechanism could be to use a more sophisticated regular expression engine that guarantees linear matching time. First, the tester audits the list of package dependencies, identifies any known ReDoS vulnerability in these packages or analyzes all the contained regular expressions. An attacker may use the ReDoS attack as a hard-to-detect way to scan which sites use the vulnerable module and then attack these sites with a command injection.Server-side JavaScript Ojamaa and Düüna [27] discuss the security of Node.js and identify algorithmic complexity attacks as one of the main threats. Our work differs in three ways: (i) we analyze JavaScript ReDoS, which is more serious than Java ReDoS, (ii) we detect vulnerabilities in real-world websites whose source code is not available for analysis, and (iii) we uncover ReDoS vulnerabilities containing advanced features, e.g. lookahead, that are not supported by any of the previous work. Testing Regular Expressions The problem of generating inputs for regular expressions is also investigated from a software testing perspective [40], [24], [22], [34]. Such problems are worth fixing independent of their exploitability in a denial of service attack, e.g., to prevent websites from being perceived as slow and unresponsive. This paper studies ReDoS vulnerabilities in JavaScriptbased web servers and shows that they are an important problem that affects various popular websites.