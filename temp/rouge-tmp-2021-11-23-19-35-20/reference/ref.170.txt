However, it is often obfuscated by the extensive use of dark jargons, innocently-looking terms like "popcorn" that serves sinister purposes (buy-ing/selling drug, seeking crimeware, etc.). Running Cantreader over one million traces collected from four underground forums, our approach automatically reported 3,462 dark jargons and their hy-pernyms, including 2,491 never known before. The first example exhibits jargon "rat" which means "remote access trojan"; The second example shows the jargon "blueberry" for "marijuana", while "athena" is the jargon for a kind of botnet framework in the third example.tion traces every day. Hence, automatic discovery and understanding of these dark jargons are highly valuable for understanding various cybercrime activities and mitigating the threats they pose.Reading thieves' cant: challenges. The study introduces a technique that looks for the query words leading to the search results involving malicious or compromised domains, which however is not to detect dark jargons but blackhat SEO targeted dark words (see detail in Section 6, under "innocent-looking dark jargon"). Also, another prior study [31] shows that the names of illicit products (e.g., bot) automatically discovered from underground marketplaces include some jargons (e.g., "fud" which means "fully undetectable exploit"). Thus, the key of identifying the jargon in the cybercrime marketplaces is to find its semantic discrepancy with itself when used a legitimate reference corpus.To this end, we need to address several technique challenges: (1) how to model a term's semantic discrepancy between two corpora? Such a neural network outputs two vectors for each input word, one for the good set and the other for the bad set, automatically making the semantic gap between the word's context measurable (Section 4.1). Running on over one million communication traces collected from four underground forums across eight years (2008)(2009)(2010)(2011)(2012)(2013)(2014)(2015)(2016), Cantreader automatically identifies 3,462 dark jargons along with their hypernyms. Interestingly, our research reveals the possible ways cybercriminals choose jargons: drug criminals tend to use fruit names for the drugs with different flavors such as "pineapple", "blueberry", "orange" and "lemon", while, hacking tool developers prefer mythological figures like "zeus", "loki" and "kraken". Also interestingly, using dark jargons, we even found 478 black words, which are another criminal cant that unlike innocent-looking jargons, barely appear in legitimate communication: e.g., "chocolope" for marijuana, 'li0n" for crypter and "Illusi0n" for trojan.Contributions. Our approach leverages a new neural language model to compare the semantics of the same term in different corpora and further identify their hypernyms. The rich information disclosed by such communication sheds light on the adversary's strategy, tactics and techniques, and provides the landscape of the fast-evolving cybercrime.In our research, we studied the communication that took place on four infamous underground forums: Darkode (sale and trade of hacking services and tools), Hack Forums (blackhat hacking activities discussion), Nulled (data stealing tool and service) and Silk Road (illegal drug), including 375,993 traces (i.e., threads of posts) from 03/2008 to 05/2016 (4132 per month). Such jargons often represent illicit goods, services, criminal tactics, etc., for the purpose of evading the law enforcement's detection. For example, to better understand the drug trade business, Drug Enforcement Administration (DEA) intelligence program compiled a set of dark jargons to decipher forensic data and evidence or information gathered like traffickers' receipts.Due to the dynamic and fast-evolving nature of cybercrimes, the vocabulary of dark jargons continues to change, adding new terms and dropping old ones. Neural language model has been found very efficient for learning high-quality distributed representations of words (word embedding), which capture a large number of precise syntactic and semantic word relationships [28,36,37]. Such a mapping can be learned with different neural network architectures, e.g., using the continual bag-of-words model and skip-gram, to analyze the contexts of input words from a large corpus [37]. Figure 2 shows a typical neural network with one hidden layer used by the skip-gram model, an instance of neural language model allowing fast training. For a word vector with |H| features, the hidden layer with |H| neurons takes the vector as the input, and output a |H|-dimension word vector. To counter the imbalance between the rare and frequent words, some neural language models such as the skipgram model apply a simple subsampling approach: each word w i in the training set is discarded with a probability determined as by computing P(w i ) = 1 − 񮽙 t f (w i ) where f (w i ) is the frequency of the word w i and t is a chosen threshold, typically around 10 −5 . Hypernym Identification is an NLP technique to identify a generic term (hypernym) with a broad meaning that more specific instance (hyponym) falls under. Hypernym describes an important lexical-semantic relation and information abut it helps understand the semantics of its instance: e.g., knowing that "Tom Cruise" (hyponym) is an "actor" (hypernym) helps a question answering system answer the question "which actors are involved in Scientology?" For example, when the "popcorn" means a snack, it often comes with "eat" or "chocolate", while when it refers to marijuana, "nugz", "buds" and others would show up around the word.This observation is the key to the automated discovery and understanding of dark jargons and is fully leveraged in the design of Cantreader. Our approach utilizes a novel neural language model to learn a word's semantics from its context during legitimate conversations and in underground communication respectively, and then compares the semantics to identify the consistency that indicates its potential use as a dark jargon. It then applies the Semantics Comparison Model (Section 4.1) to calculate two semantic similarities, Sim dark,legit and Sim legit,rep , for each input word: the former between a dark forum corpus and a legitimate forum corpus, and the later between the legitimate forum corpus and a reputable interpretative corpus. Here we use Silk Road as the dark corpus (C dark ), Reddit as the legitimate corpus (C legit ), and English Wikipedia as the reputable interpretative corpus (C rep ), to demonstrate how Cantreader could discover and interpret the jargon.After preprocessing all the corpora, Cantreader first trains two Semantic Comparison Models on (C dark ,C legit ), and (C legit ,C rep ) respectively. Fundamentally, Cantreader's jargon identification procedure is based on the fact that a word covering dark semantics tends to appear in a totally different context during underground communication than when it is used normally. As an unsupervised learning model, when the training is done, Word2Vec outputs the weights of the hidden layer M in the form of a |V | × |H| matrix, where |V | is the size of the input vocabulary and |H| is the size of hidden layer. Thus, Word2Vec maps words to vectors in |H| dimension space.The intuition behind Word2Vec is that if two different words have similar contexts in a corpus, then given the contexts, the NN is supposed to make similar predictions for these two words. Hence the training process will learn the weights to produce similar hidden layer outputs for these two words. Nor can we train two separate models on the two different corpora, since the relations between the input layer and the hidden layer are nondeterministic, due to the random initial state for the Word2Vec NN, and the randomness introduced during sub-sampling and negative sampling (Section 2.2). In the meantime, these two vectors are still comparable, because they are used together in the NN to train a single skip-gram model for predicting the surrounding windows of context words.To formally describe SCM, we first define an extended one-hot encoding:e(w) = 񮽙 [v zeros , v onehot (w)] if w is from corpus 1 [v onehot (w), v zeros ]if w is from corpus 2 (1) where v zeros is an all-zero vector of |V | dimensions, and v onehot (w) is the standard one-hot vector of word w in the input vocabulary. Since the two input corpora here are identical, the cosine similarity of every vector pair should all be close to 1, if SCM can capture the words' semantics in both corpora correctly. Our experiment shows that for every word in the corpora, the average cosine similarity between its two vectors is 0.98, with a standard deviation 0.006. Since the replacing word's contexts (e.g., archie) in Text8 syn became different to those in Text8 as recorded in replaced trace rate, all the replaced words were found to have small similarities in two corpora: the average similarity is 0.98 with a standard deviation of 0.01. This experiment shows that our SCM is able to capture a word's cross-corpora semantic difference.Experiment 3. On the word vectors produced by the model, we ran Tomas Mikolov's code to evaluate their qualities. The experiment demonstrates that indeed the quality of the SCM vector (an accuracy of 46%) is in line with those generated by Word2Vec (50%), indicating that the benefit of semantic comparison across corpora does not come with the cost of vector quality. If such contexts are not sufficiently diverse in a corpus, the embedded vector becomes biased and specific to the corpus.Standard Word2Vec implementation uses a parameter min count for this purpose. As a result, the same piece of text may appear on a forum repeatedly, and the words involved, though may occur across the corpus for many times, are always under the same context, whose semantics therefore cannot be effectively learned.To address this issue, we introduce a new metric, called windowed context, to measure a word's context diversity. Jargon semantics comparison. The purpose is to compare every word's two embedded vectors (one for each corpus) by calculating their cosine similarity Sim dark,legit , for the sake of identifying those with discrepant meanings across the corpora.However, just because a word has different semantics across the dark forum and the legitimate corpus does not always mean that it is a dark jargon. However, still we cannot eliminate the situations where some less harmful terms are treated as dark jargons, due to their unique contexts in the legitimate corpus, which can be different from their generic semantics actually used on dark forums. For example, we found that the word "damage" on reddit.com often appear during the discussion of computer games and as a result, its context becomes very much biased towards settings in the games (such as "heal", "stun" and "dps"); on Silk Road, however, "damage" preserves its original meaning.To filter out the terms unique to the good set (C legit ), the discoverer compares every vocabulary word in the set to the same word in another legitimate corpus, in terms of their semantics. Hence, we need different thresholds: a larger one for those with diverse semantics, and a smaller one for those with fewer meanings.For this purpose, the discoverer first groups all vocabulary words into different classes according to their semantic diversities, as estimated using the numbers of synsets in WordNet [23]. Assuming in each class, the similarities follow a Gaussian distribution, the threshold we selected ensures that a normal sample (a word with similar semantics in two corpora) has only 5% chance to go below the threshold, in terms of the cosine similarity between its two embedded vectors. Finding the pre- Figure 5: SPARQL example cise meaning of a dark jargon is challenging, due to lack of enough information to differentiate the contexts of related terms, particularly with the succinct expressions typically used on forums. On the other hand, we found that it is possible to gain some level of understanding of a jargon, by classifying it to a certain category under a specific hypernym. Figure 5 shows the example to search for the subclasses under "software", where wdt:P279 represents the subclass of relation, and wd:Q7397 describes the software entity.For each category (e.g., drug) in the seed set, we use Wikidata to find all its direct and indirect subclasses, and generate a tree rooted at the category in the seed. Specifi- cally, for a given jargon, our approach uses its embedded vector together with a hypernym candidate's vector (from the same SCM) as a feature to determine the probability that they indeed have the hypernym relation. We used four datasets in our study: dark corpora, benign corpora, hypernymy dataset, and groundtruth dataset with known jargons. In our research, we parsed the underground forum snapshots collected by the darknet marketplace archives programs and other research projects [3,31], to get four dark corpora: the Silk Road corpus [17] consists of 195,403 traces (i.e., threads of posts) from the underground market-place Silk Road mainly discussing illicit products (such as drugs and weapons) trading; the Darkode corpus [4] includes 7,417 traces from a hacking technique forum about cybercriminal wares; the Hack forums corpus [9] has 52,670 traces from a blackhat technique forum; and the Nulled corpus [12] contains 121,499 traces from a forum talking about data stealing tools and services. The groundtruth dataset, with 774 known dark jargons and their corresponding hypernyms, is used in the evaluation of our system. Note that not all the terms appear on the two lists are actually used as dark jargons in our dark corpora because DEA's drug list includes many out-of-date and uncommon slang names for drugs, and the cybercrime marketplace product list, on the other hand, focuses mostly on illegitimate products, which are not always referred to in dark jargons. To understand the accuracy and coverage of the results, we first used the groundtruth dataset to validate our results.Among the 774 jargon words in the groundtruth set, 598 were successfully detected by Cantreader, which gives a recall of 77.2%. For the rest 2,864 dark jargons detected by Cantreader, we randomly picked 200 samples for manual validation, where 182 terms were confirmed to be true dark jargons. Among them, 692 drug jargons are not included in the drug jargon lists reported by DEA (see Section 5), but prevalent in underground forums such as "cinderella", "pea" and "mango". Interestingly, we found that drug dealers tend to use drug flavors as jargons, e.g., "pineapple", "blueberry", "orange" and "lemon". This indicates that unlike the black keywords reported in the prior research [46], these dark jargons not only look innocent but are indeed less likely related to compromised or attack sites, thereby providing good covers for underground communication.Ever-changing dark jargon. In this way, we discovered 675 communication traces in Reddit related to criminal activities. It means that the criminals intend to use dark jargons to cover its explicit meaning.To investigate criminal activities of the criminal communication traces using dark jargons, we extracted the keywords using RAKE from the criminal traces and clustered the traces based on those keywords using the classic k-Nearest-Neighbor (k-NN) algorithm [29]. For example, we found that "chocolope", a kind of marijuana, which does not appear in C legit , frequently co-occurs with multiple drug jargons Figure 8: Trace volume of four jargons across month such as "blueberry", "diesel" and "kush" in C dark . Figure 8 illustrates the trace volume of the dark jargons "rat", "loki", "xtreme" and "ivy" per month from 05/2008 to 03/2015 in Hack Forum, where "loki", "xtreme" and "ivy" are different kinds of "rat". We conducted open domain experiments as reported in Section 3, which indicates the effectiveness of the proposed model on open domain corpora.Also, even only accepting two corpora in jargon discovery, the semantic comparison model can accept n corpora for comparison by setting the input layer to n times of the word size, where the word size is the intersection words of all n corpora. Our work proposes a novel NLP analysis model and identifies a novel application of NLP security, i.e., automatically identifying and understanding dark jargons in underground communication traces.One recent work that is closest to our study introduces a technique to detect search engine keywords referring to illicit products or services [46]. But their endeavors stopped at a rather initial stage, finding semantically similar words of previous-detected jargons using cosine similarity of embedded vectors. Cantreader is designed to specialize the neural language model for semantic comparison.