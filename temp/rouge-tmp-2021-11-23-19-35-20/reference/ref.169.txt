Intelligent voice control (IVC) has been widely used in human-computer interaction, such as Amazon Alexa [1], Google Assistant [6], Apple Siri [3], Microsoft Cortana [14] and iFLYTEK [11]. Running the state-ofthe-art ASR techniques, these systems can effectively interpret natural voice commands and execute the corresponding operations such as unlocking the doors of * Corresponding author: chenkai@iie.ac.cn 1 Demos of attacks are uploaded on the website (https://sites.google.com/view/commandersong/) home or cars, making online purchase, sending messages, and etc. Although such adversarial learning has been extensively studied in image recognition, little has been done in speech recognition, potentially due to the new challenge in this domain: unlike adversarial images, which include the perturbations of less noticeable background pixels, changes to voice commands often introduce noise that a modern ASR system is designed to filter out and therefore cannot be easily misled.Indeed, a recent attack on ASR utilizes noise-like hidden voice command [22], but the white box attack is based on a traditional speech recognition system that uses a Gaussian Mixture Model (GMM), not the DNN behind today's ASR systems. So far little success has been reported in generating "adversarial sound" that practically fools deep learning technique but remains inconspicuous to human ears, and meanwhile allows it to be played from the remote (e.g., through YouTube) to attack a large number of ASR systems.To find practical adversarial sound, a few technical challenges need to be addressed: (C1) the adversarial audio sample is expected to be effective in a complicated, real-world audible environment, in the presence of elec-tronic noise from speaker and other noises; (C2) it should be stealthy, unnoticeable to ordinary users; (C3) impactful adversarial sound should be remotely deliverable and can be played by popular devices from online sources, which can affect a large number of IVC devices. To make such adversarial samples practical, our approach has been designed to capture the electronic noise produced by different speakers, and integrate a generic noise model into the algorithm for seeking adversarial samples (addressing C1, called WAA attack). In our experiment, we generated over 200 CommanderSongs that contain different commands, and attacked Kaldi with an 100% success rate in a WTA attack and a 96% success rate in a WAA attack. Our attack is demonstrated to be robust, working across air in the presence of environmental interferences, transferable, effective on a black box commercial ASR system (i.e., iFLYTEK) and remotely deliverable, potentially impacting millions of users. We design two approaches (audio turbulence and audio squeezing) to defend against the attack, which proves to be effective by our preliminary experiments.Roadmap. Besides the commercial products like Amazon Alexa, Google Assistant, Apple Siri, iFLYTEK, etc., there are also open-source platforms such as Kaldi toolkit [13], Carnegie Mellon University's Sphinx toolkit [5], HTK toolkit [9], etc. Figure 1 presents an overview of a typical speech recognition system, with two major components: feature extraction and decoding based on pre-trained models (e.g., acoustic models and language models). The extracted acoustic features are matched against pre-trained acoustic models to obtain the likelihood probability of phonemes. As GMM is limited to describe a non-linear manifold of the data, Deep Neural Network-Hidden Markov Model (DNN-HMM) has been widely used for speech recognition in academic and industry community since 2012 [32]. The architecture of end-toend ASR systems always includes an encoder network corresponding to the acoustic model and a decoder network corresponding to the language model [47]. DeepSpeech [17] and Wav2Letter [24] are popular open source end-to-end speech recognition systems. The state-of-the-art researches [37,21,27] advance in terms of practicality by printing the adversarial image and presenting it to a device with image recognition functionality.However, the success of the attack against image recognition systems has not been ported to the speech recognition systems until very recently, due to the complexity of the latter. For instance, a large amount of human effort is involved as feedback for the black box approach, and the white box approach is based on GMM-based acoustic models, which have been replaced by DNN-based ones in most modern speech recognition systems. In this section, we present the motivation of our work, and overview the proposed approach to generate the practical adversarial attack. Specifically, ultrasonic sound can be defeated by using a low-pass filter (LPF) or analyzing the signal frequency range, and noises are easy to be noticed by users.Therefore, the research in this paper is motivated by the following questions: (Q1) Is it possible to build the practical adversarial attack against ASR systems, given the facts that the most ASR systems are becoming more intelligent (e.g., by integrating DNN models) and that the generated adversarial samples should work in the very complicated physical environment, e.g., electronic noise from speaker, background noise, etc.? Below, we will detail how our attack is designed to address the above questions. On one hand, enjoying songs is always a preferred way for people to relax, e.g., listening to the music station, streaming music from online libraries, or just browsing YouTube for favorite programs. However, such expense can be compensated by integrating the same desired command multiple times into one song (the command of "open the door" may only last for 2 seconds.) As shown in Figure 1, an ASR system is usually composed of two pre-trained models: an acoustic model describing the relationship between audio signals and phonetic units, and a language model representing statistical distributions over sequences of words. Since such input usually is in the form of a wave file (in "WAV" format) and the ASR systems need to expose APIs to accept the input, we define such attack as the WAV-To-API (WTA) attack. The challenge of the WAA attack is when playing the adversarial samples by a speaker, the electronic noise produced by the loudspeakers and the background noise in the open air have significant impact on the recognition of the desired commands from the adversarial samples. We implement our attack by addressing two technical challenges: (1) minimizing the perturbations to the song, so the distortion between the original song and the generated adversarial sample can be as unnoticeable as possible, and (2) making the attack practical, which means CommanderSong should be played over the air to compromise IVC devices. To address the first challenge, we proposed pdf-id sequence matching to incur minimum revision at the output of the acoustic model, and use gradient descent to generate the corresponding adversarial samples as in Section 4.2. There are three states (each is denoted as an HMM state) of sound production for each phoneme, and a series of transitions among those states can identify a phoneme. To illustrate the above findings, we use Kaldi to process a piece of audio with several known words, and obtain the intermediate results, including the posterior probability matrix computed by DNN, the transition-ids sequence, the phonemes, and the decoded words. The red boxes highlight the id representing the corresponding phoneme, and each phoneme is identified by a sequence of transition-ids, or the phoneme identifiers. By analyzing the decoding procedures, we can get the output of DNN matrix A of the original song (Step 1 񮽙 in Figure 3) and the phoneme identifiers of the desired command audio (Step 4 񮽙 in Figure 3). That is,g(x(t)) = m.As shown in Step 5 񮽙 in Figure 3, we can extract a sequence of pdf-id of the commandb = (b 1 , b 2 ,..., b n ),where b i (1 ≤ i ≤ n) represents the highest probability pdfid of the command at frame i. To have the original song decoded as the desired command, we need to identify the minimum modification δ (t) on x(t) so that m is same or close to b. Specifically, we minimize the L1 distance between m and b. 5 For instance, the pdf-ids sequence for eh B should be 6383, 5760, 5760, 5760, 5760, 5760, 5760, 5760, 5760, 5760. In particular, based on our objective function, we revise the song x(t) into x 񮽙 (t) = x(t) + δ (t) with the aim of making most likely pdf-ids g (x 񮽙 (t)) equal or close to b. Therefore, the crafted audio x 񮽙 (t) can be decoded as the desired command.To further preserve the fidelity of the original song, one method is to minimize the time duration of the revision. In this paper, we do not consider the invariance of background noise in different environments, e.g., grocery, restaurant, office, etc., due to the following reasons: (1) In a quite noisy environment like restaurant or grocery, even the original voice command y(t) may not be correctly recognized by IVC devices; (2) Modeling any slightly variant background noise itself is still an open research problem; (3) Based on our observation, in a normal environment like home, office, lobby, the major impacts on the physical attack are the electronic noise from the speaker and the distortion from the receiver of the IVC devices, rather than the background noise. Hence, our idea is to build a noise model, considering the speaker noise, the receiver distortion, as well as the generic background noise, and integrate it in the approach in Section 4.2. We redesign the objective function as shown in Eq (2). Our evaluation results show that this approach can make the adversarial audio x 񮽙 (t) robust enough for different speakers and receivers.n(t) = rand(t), |n(t)| <= N.(3) In this section, we present the experimental results of CommanderSong. To evaluate the human comprehension, we conducted a survey examining the effects of "hiding" the desired command in the song. Regarding the commands to inject, we chose 12 commonly used ones such as "turn on GPS", "ask Capital One to make a credit card payment", etc., as shown in Table 2. In this WTA attack, we directly feed the generated adversarial songs to Kaldi using its exposed APIs, which accept raw audio file as input. The success rate 100% means Kaldi can decode every word in the desired command correctly. Note in the case that the decoded word is only one character different than that in the desired command, we consider the word is not correctly recognized.For each adversarial song, we further calculated the average signal-noise ratio (SNR) against the original song as shown in Table 2. Overall, such a pseudo IVC device is built using the microphone in iPhone 6S as the audio recorder, and Kaldi system to decode the audio.We conducted the practical WAA attack in a meeting room (16 meter long, 8 meter wide, and 4 meter tall). The songs were played using three different speakers including a JBL clip2 portable speaker, an ASUS laptop and a SENMATE broadcast equipment [16], to examine the effectiveness of the injected random noise. Below we will evaluate if such "bigger" perturbation is human-noticeable by conducting a survey.Human comprehension from the survey. A series of questions regarding each audio need to be answered, e.g., (1) whether they have heard the original song before; (2) whether they heard anything abnormal than a regular song (The four options are no, not sure, noisy, and words different than lyrics); (3) if choosing noisy option in (2), where they believe the noise comes from, while if choosing words different than lyrics option in (2), they are asked to write down those words, and how many times they listened to the song before they can recognize the words. Based on our study, 63.7% of the participants are in the age of 20∼40 and 33.3% are 40∼60 years old, and 70.6% of them use IVC devices (e.g., Amazon Echo, Google home, Smartphone, etc.) everyday. Table 4 shows the results of the human comprehension of our WTA samples. Some applications supported by iFLYTEK and their downloads on Google Play as well as the number of worldwide users are listed in Table 8 in Appendix. We use them to test the transferability of our WAA attack samples, and the success rates of different commands are shown in Table 6. However, the command airplane mode on only gets 66% success rate on iFLYREC, and 0 on iFLYTEK Input.Transferability from Kaldi to DeepSpeech. In this experiment, we started by 10 adversarial samples generated by CommanderSong, either WTA or WAA attack, integrating commands like Okay google call one one zero one one nine one two zero, Echo open the front door, and Echo turn off the light. The preliminary evaluations on transferability give us the opportunities to understand CommanderSongs and for designing systematic approach to transfer in the future. Since our WAA attack samples can be used to launch the practical adversarial attack against ASR systems, we want to explore the potential channels that can be leveraged to impact a large amount of victims automatically. We ran the WeChat 7 application and enabled the iFLYTEK Input on different smartphones including iPhone 6S, Huawei Honor 8 and XiaoMi MI Note3. iFLYTEK Input can always successfully recognize the command "open the door" from the audio played by the radio and display it on the screen. We try to deeply understand the attacks, which could potentially help to derive defense approaches. The results show that A cs can work, which is aligned with our findings -a random song can serve as the "carrier" because a piece of silent audio can be viewed as a special song.However, after listening to A cs , we find that A cs sounds quite similar to the injected command, which means any user can easily notice it, so A cs is not the adversarial samples we desire. However, after several times of testing, we find that Δ(A s , A cs ) does not work, which indicates the pure perturbations we injected cannot be recognized as the target commands.Recall that in Table 5, the songs in the soft music category are proven to be the best carrier, with lowest abnormality identified by participants. The success rate (black dotted line) also increases with the decrease of SNR. WTA attack slightly modifies the song so that the open source system Kaldi can recognize the command while human cannot. When SNR = 15 dB, WTA almost always fails and A I can still be successfully recognized, which means this approach works for WTA. When 1/M = 0.7 (if the sample rate is 8000 samples/second, the downsampled rate is 5600 samples/second), the success rates of WTA and WAA are 0% and 8% respectively. Prior to our work, many researchers have devoted to security issues about speech controllable systems [36,35,26,41,51,22,53,23]. Our attacks on Kaldi are concurrent to their work, and our attack approaches are independent to theirs. They stated that defenses should be evaluated by strong attacks and adaptive adversarial examples. Our evaluation shows that CommanderSong can be transferred to iFLYTEK, impacting popular apps such as WeChat, Sina Weibo, and JD with billions of users.