Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. In a supervised setting, a previously gathered data set consisting of possibly confidential feature-vector inputs (e.g., digitized health records) with corresponding output class labels (e.g., a diagnosis) serves to train a predictive model that can generate labels on future inputs. Popular models include support vector machines (SVMs), logistic regressions, neural networks, and decision trees.ML algorithms' success in the lab and in practice has led to an explosion in demand. The adversary's goal is to extract an equivalent or near-equivalent ML model, i.e., one that achieves (close to) 100% agreement on an input space of interest.We demonstrate successful model extraction attacks against a wide variety of ML model types, including decision trees, logistic regressions, SVMs, and deep neural networks, and against production ML-as-a-service (MLaaS) providers, including Amazon and BigML. For example, for logistic regression, the confidence value is a simple log-linear function 1/(1+e −(w·x+β ) ) of the ddimensional input vector x. By querying d + 1 random d-dimensional inputs, an attacker can with high probability solve for the unknown d + 1 parameters w and β defining the model. We emphasize that while this model extraction attack is simple and non-adaptive, it affects all of the ML services we have investigated.Such equation-solving attacks extend to multiclass logistic regressions and neural networks, but do not work for decision trees, a popular model choice. We demonstrate successful model extraction attacks that use adaptive, iterative search algorithms to discover paths in a tree.We experimentally evaluate our attacks by training models on an array of public data sets suitable as standins for proprietary ones. Our new attacks extract models matching targets on >99% of the input space for a variety of model classes, but need up to 100× more queries than equation-solving attacks (specifically for multiclass linear regression and neural networks). We show their success against Amazon's service (using our own models as stand-ins for victims'), and also report successful reverse-engineering of the (only partially documented) model type employed by Amazon. For example, attacks against Amazon's system indirectly leak various summary statistics about a private training set, while extraction against kernel logistic regression models [57] recovers significant information about individual training data points.The source code for our attacks is available online at https://github.com/ftramer/Steal-ML. For a continuous feature taking values between bounds a and b, we letX i = [a, b] ⊂ R.Inputs to a model may be pre-processed to perform feature extraction. Other examples are input scaling and one-hot-encoding of categorical features.We focus primarily on classification settings in which f predicts a nominal variable ranging over a set of classes. Given c classes, we use as class labels the set Z c . For Y = [0, 1] c , we use the 0-1 distance when comparing predicted classes; when comparing class probabilities directly, we instead use the total variation distance, given by d(y,y 񮽙 ) = 1 2 ∑ |y[i]−y 񮽙 [i]|. These models are generated by a training algorithm T that takes as input a training set {(x i , y i )} i , where (x i , y i ) ∈ X × Y is an input with an associated (presumptively correct) class label. An ML model extraction attack arises when an adversary obtains black-box access to some target model f and attempts to learn a modeî f that closely approximates, or even matches, f (see Figure 1). As mentioned previously, the restricted case in which f outputs class labels only, matches the membership query setting considered in learning theory, e.g., PAC learning [53] and other previous works [3,7,8,15,30,33,36]. Google does not even specify what training algorithm their service uses, while Amazon provides only partial documentation for its feature extraction ex (see Section 5). Some services allow users to monetize trained models by charging others for prediction queries.To use these services, a user uploads a data set and optionally applies some data pre-processing (e.g., field removal or handling of missing values refers to the ability to download and use a trained model locally, and 'Monetize' means that a user may charge other users for black-box access to her models. Apart from PredictionIO, all of the services we examined respond to prediction queries with not only class labels, but a variety of additional information, including confidence scores (typically class probabilities) for the predicted outputs.Google and BigML allow model owners to monetize their models by charging other users for predictions. On BigML, 1,000 queries consume at least 100 credits, costing $0.10-$5, depending on the user's subscription.Attack scenarios. A model extraction attack will undermine the provider's business model if a malicious user pays less for training and extracting than for paying per-query charges.Violating training-data privacy. Prior attacks such as model inversion [4,23,24] have shown that access to a model can be abused to infer information about training set points. An adversary may be able to make direct queries, providing an arbitrary input x to a model f and obtaining the output f (x). Or the adversary may be able to make only indirect queries, i.e., queries on points in input space M yielding outputs f (ex(M)). We formalize "closely matching" using two different error measures:• Test error R test : This is the average error over a test set D, given byR test ( f , ˆ f ) = ∑ (x,y)∈D d( f (x), ˆ f (x))/|D|. 2 • Uniform error R unif : For a set U of vectors uniformly chosen in X , letR unif ( f , ˆ f ) = ∑ x∈U d( f (x), ˆ f (x))/|U|. When assessing how close the class probabilities output byˆfbyˆ byˆf are to those of f (with the total-variation distance) we use the notations R TV test ( f , ˆ f ) and R TV unif ( f , ˆ f ). An adversary may know any of a number of pieces of information about a target f : What training algorithm T generated f , the hyper-parameters used with T , the feature extraction function ex, etc. In this case, an API that reveals these class probabilities provides an adversary A with samples (x, f (x)) that can be viewed as equations in the unknown model parameters. Table 3 displays the data sets used in this section, which we obtained from various sources: the synthetic ones we generated; the others are taken from public surveys (Steak Survey [26] and GSS Survey [49]), from scikit [42] (Digits) or from the UCI ML library [35]. It is widely used in many scientific fields (e.g., medical and social sciences) and is supported by all the ML services we reviewed.Formally, a LR model is defined by parameters w ∈ R d , β ∈ R, and outputs a probability f 1 (x) = σ (w · x + β ), where σ (t) = 1/(1 + e −t ). LR is a linear classifier: it defines a hyperplane in the feature space X (defined by w · x + β = 0), that separates the two classes.Given an oracle sample (x, f (x)), we get a linear equation w·x+β = σ −1 ( f 1 (x)). Moreover, for services with black-box-only access (e.g., Amazon or Google), a user may abuse the service's resources to train a model over a large data set D (i.e., |D| | d), and extract it after only d + 1 predictions. This could undermine a service's business model, should prediction fees be used to amortize the high cost of training.For each binary data set shown in Table 3, we train a LR model and extract it given d + 1 predictions. We consider two types of MLR models: softmax and one-vs-rest (OvR), that differ in how the c binary models are trained and combined: A softmax model fits a joint multinomial distribution to all training samples, while a OvR model trains a separate binary LR for each class, and then normalizes the class probabilities.A MLR model f is defined by parameters w ∈ R cd , β β β ∈ R c . For softmax models for instance, the equations take the form e w i ·x+β i /(∑ c−1 j=0 e w j ·x+β j ) = f i (x). For perceptrons with one hidden layer, we have w ∈ R dh+hc , β β β ∈ R h+c , where h is the number of hidden nodes (h = 20 in our experiments). To illustrate our attack's success, we train a softmax regression, a OvR regression and a MLP on the Adult data set with target 'Race' (c = 5). For MLR models (softmax and OvR), the attack is extremely efficient, requiring around one query per unknown parameter of f (each query yields c = 5 equations). For MLR models with k = c·(d + 1) parameters (c is the number of classes), k queries were sufficient to achieve perfect extraction (R test = R unif = 0, R TV test and R TV unif below 10 −7 ). For MLPs with 20 hidden nodes, we achieved >99.9% accuracy with 5,410 samples on average and 11,125 at most (Adult). We now move to a less mainstream model class, kernel logistic regression [57], that illustrates how extraction attacks can leak private training data, when a model's outputs are directly computed as a function of that data.Kernel methods are commonly used to efficiently extend support vector machines (SVM) to nonlinear classifiers [14], but similar techniques can be applied to logistic regression [57]. More details are in Appendix A.Each sample (x, f (x)) from a KLR model yields c equations over the parameters α α α ∈ R sc , β β β ∈ R c and the representers x 1 ,. We will also show experimentally that training data may leak even if A extracts a modeî f with s 񮽙 񮽙 s representers. We select 20 random digits as representers for the first model, and all 1,257 training points for the second. Figure 2b shows the average image of training points classified as a 3, 4, 5, 6 or 7 by the target model f , along with 5 extracted representers ofˆfofˆ ofˆf . The model inversion attack explored by Fredrikson et al. [23] uses access to a classifier f to find the input x opt that maximizes the class probability for class i, i.e., x opt = argmax x∈X f i (x). We show that extraction plus inversion can require fewer queries and less time than performing black-box inversion directly.As a case study, we use the softmax model from [23], trained over the AT&T Faces data [5]. Reconstructing the faces of all 40 individuals would require around 800,000 online queries.The trained softmax model is much larger than those considered in Section 4.1, with 412,160 unknowns (d = 10,304 and c = 40). Given the extracted modeî f , we can recover all 40 faces using white-box attacks, incurring around 20× fewer remote queries to f than with 40 black-box attacks.For black-box attacks, the authors of [23] estimate a query latency of 70 milliseconds (a little less than in our own measurements of ML services, see Table 1). We propose a new path-finding attack, that exploits API particularities to extract the 'decisions' taken by a tree when classifying an input.Prior work on decision tree extraction [7,12,33] has focused on trees with Boolean features and outputs. Kushilevitz and Mansour [33] showed that Boolean trees can be extracted using membership queries (arbitrary queries for class labels), but their algorithm does not extend to more general trees. A leaf-identity oracle O takes as input a query x ∈ X and returns the identifier of the leaf of the tree T that is reached on input x.A node-identity oracle O ⊥ takes as input a query x ∈ X 1 ∪ {⊥} × ··· × X d ∪ {⊥} and returns the identifier of the node or leaf of T at which the tree computation halts. We now present our path-finding attack (Algorithm 1), that assumes a leaf-identity oracle that returns unique identifiers for each leaf. We analyze the algorithm's correctness and complexity in Appendix C.We illustrate our algorithm with a toy example of a tree over continuous feature Size and categorical feature Color (see Figure 3). 1:x init ← {x 1 ,...,x d } 񮽙 random initial query 2: Q ← {x init }񮽙 Set of unprocessed queries 3: P ← {} 񮽙 Set of explored leaves with their predicates 4: while Q not empty do 5:x ← Q.POP() 6:id ← O(x) 񮽙 Call to the leaf identity oracle 7:ifid ∈ P then 񮽙 Check if leaf already visited 8: continue 9: end if 10: for 1 ≤ i ≤ d do 񮽙 Test all features 11:if IS CONTINUOUS(i) then 12:for (α, β ] ∈ LINE SEARCH(x, i, ε) do 13: if x i ∈ (α, β ] then 14: P[id]. With a binary search over feature Size (and all other features in x fixed), we find all intervals that lead to different leaves, i.e., [0,40], (40,60], (60,100]. We then build a set S of values that lead to the current leaf, i.e., S = {R}, and a set V of values to set in x to explore other leaves (one representative per leaf). Using these two procedures, we thus find the predicates defining the path to leaf id 2 , and generate new queries x 񮽙 for unvisited leaves of the tree. The main issue with duplicate ids comes from the LINE SEARCH and CATEGORY SPLIT procedures: if two queries x and x 񮽙 differ in a single feature and reach different leaves with the same id, the split on that feature will be missed. For our experiments, we downloaded eight public decision trees from BigML (see Table 5 Table 6: Performance of extraction attacks on public models from BigML. For classification and regression trees, BigML computes confidence scores based on a confidence interval for predictions at each node [11]. Querying partial inputs vastly improves our attack: we require far less queries (except for the Steak Survey model, where Algorithm 1 only visits a fraction of all leaves and thus achieves low success) and achieve higher accuracy for trees with duplicate leaf ids. The attacks are also efficient: The top-down approach takes less than 10 seconds to extract a tree, and Algorithm 1 takes less than 6 minutes for the largest tree. Our attacks only use ex- posed APIs, and do not in any way attempt to bypass the services' authentication or access-control mechanisms.We only attack models trained in our own accounts. From another account, we extract the model using the two attacks from Section 4.2. Our attacks (Algorithm 1 and the top-down variant) extract an exact description of the tree's paths, using respectively 1,722 and 1,150 queries. By default, Amazon uses two feature extraction techniques: (1) Categorical features are one-hot-encoded, i.e., the input space M i = Z k is mapped to k binary features encoding the input value. The attack is then identical to the one considered in Section 4.1.2: using 650 queries to Amazon, we extract a model that achieves R test = R unif = 0. To reverse-engineer the binning transformation, we use linesearches similar to those we used for decision trees: For each numeric feature, we search the feature's range in input space for thresholds (up to a granularity ε) where f 's output changes. Note that learning the bin boundaries may be interesting in its own right, as it leaks information about the training data distribution. However, Amazon facilitates this process with the way it handles queries with missing features: if a feature is omitted from a query, all corresponding features in X are set to 0. In all cases, the extracted model matches f on 100% of tested inputs. More generally, for models with a linear input layer (i.e., logistic regressions, linear SVMs, MLPs) the scaling or normalization can be seen as being applied to the learned weights, rather than the input features. Model hyper-parameters for instance (such as the free parameter of an RBF kernel) are typically chosen through cross-validation over a default range of values.Given a set of attack strategies with varying assumptions, A can use a generic extract-and-test approach: each attack is applied in turn, and evaluated by computing R test or R unif over a chosen set of points. Note that A needs to interact with the model f only once, to obtain responses for a chosen set of extraction samples and test samples, that can be re-used for each strategy.Our attacks on Amazon's service followed this approach: We first formulated guesses for model characteristics left unspecified by the documentation (e.g., we found no mention of one-hot-encoding, or of how missing inputs are handled). The successful attacks given in Sections 4 and 5 show the danger of revealing confidence values. A linear classifier is defined by a vector w ∈ R d and a constant β ∈ R, and classifies an instance x as positive if w · x + β > 0 and negative otherwise. Their attack uses line searches to find points arbitrarily close to f 's decision boundary (points for which w · x + β ≈ 0), and extracts w and β from these samples.This attack only works for linear binary models. In addition to evaluating the Lowd-Meek attack against ML APIs, we introduce a number of other approaches based on the broad strategy of re-training a model locally, given input-output examples. This baseline strategy simply consists in sampling m points x i ∈ X uniformly at random, querying the oracle, and training a modeî f on these samples. The left shows R test and the right shows R unif . We first explore how well the various approaches work in settings where the Lowd-Meek attack can be applied. We evaluate their attack and our three retraining strategies for logistic regression models trained over the binary data sets shown in Table 3. However, for budgets large enough to run line searches in each dimension, the Lowd-Meek attack is clearly the most efficient.For the models we trained, about 2,050 queries on average, and 5,650 at most, are needed to run the LowdMeek attack effectively. Thus, even if an ML API only provides class labels, efficient extrac-tion attacks on linear models remain possible.We further consider a setting where feature-extraction (specifically one-hot-encoding of categorical features) is applied by the ML service, rather than by the user. We thus focus on evaluating the three retraining attacks we introduced, for the type of ML models we expect to find in real-world applications.We focus on softmax models here, as softmax and onevs-rest models have identical output behaviors when only class labels are provided: in both cases, the class label for an input x is given by argmax i (w i · x + β i ). From an extractor's perspective, it is thus irrelevant whether the target was trained using a softmax or OvR approach.We evaluate our attacks on softmax models trained on the multiclass data sets shown in Table 3. Yet, for scenarios with high monetary incentives (e.g., intrusion detector evasion), extraction attacks on MLR models may be attractive, even if APIs only provide class labels. For a budget of 100 · k, where k is the number of model parameters, we get R test = 99.16% and R unif = 98.24%, using 108,200 queries per model on average. We have shown in Sections 4 and 5 that adversarial clients can effectively extract ML models given access to rich prediction APIs. Given that this undermines the financial models targeted by some ML cloud services, and potentially leaks confidential training data, we believe researchers should seek countermeasures.In Section 6, we analyzed the most obvious defense against our attacks: prediction API minimization. This will prevent many of our attacks, most notably the ones described in Section 4 as well as the feature discovery techniques used in our Amazon case study (Section 5). For instance, BigML reports confidences with 5 decimal places, and Amazon provides values with 16 significant digits.To understand the effects of limiting precision further, we re-evaluate equation-solving and decision tree pathfinding attacks with confidence scores rounded to a fixed decimal place. Figure 7 shows the results of experiments on softmax models, with class probabilities rounded to 2-5 decimals. For classification trees, we re-evaluated our top-down attack, with confidence scores rounded to fewer than 5 decimal places. As some of our extraction attacks leak training data information (Section 4.1.3), one may ask whether DP can prevent extraction, or at least reduce the severity of the privacy violations that extraction enables.Consider na¨ıvena¨ıve application of DP to protect individual training data elements. While we have not experimented with ensemble methods as targets, we suspect that they may be more resilient to extraction attacks, in the sense that attackers will only be able to obtain relatively coarse approximations of the target function. The second difference is more pragmatic: prediction APIs reveal richer information than assumed in prior learning theory work, and we exploit that.Algorithms for learning with membership queries have been proposed for Boolean functions [7,15,30,33] and various binary classifiers [36,39,50]. Through local experiments and online attacks on two major providers, BigML and Amazon, we illustrated the efficiency and broad applicability of attacks that exploit common API features, such as the availability of confidence scores or the ability to query arbitrary partial inputs. Note that for non-zero α i , it is the case that α i < 0 if the training-set label of x i was zero and α i > 0 otherwise.Logistic regression. ,β c−1 in R and defines f i (x) = e w i ·x+β i /(∑ c−1 j=0 e w j ·x+β j ) for i ∈ Z c . Again, one may use kernel techniques to deal with more complex data relationships (c.f., [57]). Unlike with SVMs, where most training data set points will never end up as support vectors, here all training set points are potentially representors. We consider three types of splitting functions ρ that are typically used in practice ( [11]):(1) The feature x i is categorical with X i = Z k . This corresponds to a k-ary split on a categorical feature of arity k. (3) The feature x i is continuous with X i = [a, b]. Finally, we add a data set of digits available in scikit, to visually illustrate training data leakage in kernelized logistic models (c.f. Section 4.1.3). These models were trained by real MLaaS users and they cover a wide range of application scenarios, thus providing a realistic benchmark for the evaluation of our extraction attacks.The IRS model predicts a US state, based on administrative tax records. As all leaf ids are unique and there are no intervals smaller than ε, we will discover a leaf in each sub-tree rooted at A, including the one that contains id 񮽙 . Suppose continuous features have range [0, b], and categorical features have arity k. For continuous features, finding one threshold takes at most log 2 ( b ε ) queries. Even if we know that we can find a multilayer perceptronˆfperceptronˆ perceptronˆf that closely matches f , ˆ f might have a far more complex representation (more parameters) than f . As shown in Section 4.1.2, using only 530 queries, we extract a modeîmodeî f from the same model class, that closely matches f ( ˆ f and f predict the same labels on 100% of tested inputs, and produce class probabilities that differ by less than 10 −7 in TV distance). As shown in Section 4.1.2, using only 530 queries, we extract a modeîmodeî f from the same model class, that closely matches f ( ˆ f and f predict the same labels on 100% of tested inputs, and produce class probabilities that differ by less than 10 −7 in TV distance).