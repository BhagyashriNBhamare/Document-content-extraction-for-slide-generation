We propose a hybrid model of differential privacy that considers a combination of regular and opt-in users who desire the differential privacy guarantees of the local privacy model and the trusted curator model, respectively. We demonstrate that within this model, it is possible to design a new type of blended algorithm for the task of privately computing the most popular records of a web search log. Differential privacy [10,9,11], which has become the gold standard privacy guarantee in the academic literature, and is gaining traction in industry and government [13,17,28], overcomes the prior issues by focusing on the privatization algorithm applied to the data, requiring that it preserves privacy in a mathematically rigorous sense under an assumption of an omnipotent adversary.There are two primary models in the differential privacy framework that define how data is to be handled by the users and data collectors: the trusted curator model and the local model.Trusted curator model: Most differentially private algorithms developed to date operate in the trusted curator model: all users' data is collected by the curator before privatization techniques are applied to it. In this model, although users are guaranteed that the released data set protects their privacy, they must be willing to share their private, unperturbed data with the curator and trust that the curator properly performs a privacy-preserving perturbation.Local model: As was most recently argued by Apple [17], users may not trust the data collector with their data, and may prefer privatization to occur before their data reaches the collector. Differential privacy: Formally, an algorithm A is (, δ)-differentially private [11] if and only if for all neighboring databases D and D differing in precisely one user's data, the following inequality is satisfied for all possible sets of outputs Y ⊆ Range(A):Pr[A(D) ∈ Y ] ≤ e Pr[A(D ) ∈ Y ] + δ.The definition of what it means for an algorithm to preserve differential privacy is the same for both the trusted curator model and the local model. The only distinction is in the timing of when the privacy perturbation needs to be applied -in the local model, the data needs to undergo a privacypreserving perturbation before it is sent to the aggregator, whereas in the trusted curator model the aggregator may first collect all the data, and then apply a privacy-preserving perturbation. In the trusted curator model, D represents data of all users and D represents data of all users, except values of one of the user's data may be altered.Current differential privacy literature considers the trusted curator model and the local model entirely independently. Our goal is to show that there is much to be gained by combining the two.Hybrid model: Much of the contribution in this paper stems from our observation that the two models can co-exist. Local search revolves around the problem of how a browser maker can collect information about users' clicks as they interact with search engines in order to create the head of the search, i.e., the collection of the most popular queries and their corresponding URLs, and make it available to users locally, i.e., on their devices. Local search can be thought of as a form of caching, where many queries are answered in a manner that does not require a round trip to the server. Our paper makes the following contributions:• We introduce and utilize a more realistic, hybrid trust model, which removes the need for all users to trust a central curator. • We test Blender on two common applications:search trend computation and local search and find that it preserves high levels of utility while maintaining differential privacy for reasonable privacy parameter values. • As part of Blender, we propose an approach for automatically balancing the data obtained from participation of opt-in users with that of other users to maximize the eventual utility. • We perform a comprehensive utility evaluation of Blender on two large web search data sets, comprising 4.8 million and 13.2 million queries, demonstrating that Blender maintains very high level of utility (i.e., NDCG values in excess of 95% across a range of parameters). We assume that our system receives a sample of users from the population, each holding their own collection of private data drawn independently and identically from the distribution over all records p. Its goal is to output an estimatê p of probabilities of the most frequent search records, while preserving differential privacy (in the trusted curator model) for the opt-in users and (in the local model) for the clients.Informal Overview of Blender: Figure 1 presents an architectural diagram of Blender.Blender serves as the trusted curator for the optin group of users, and begins by aggregating data from them. Finally, Blender outputs the obtained records and their combined probability estimates, which can be used to drive local search, determine trends, etc.A Formal Overview of Blender: Figure 2 presents the precise algorithmic overview of each step, including key parameters.Lines 1-3 of Blender describe the treatment of data from opt-in users, line 4 -the treatment of clients, and line 5 -the process for combining the probability estimates obtained from the two groups. This allows changing the sub-algorithms if better versions (utility-wise or implementation-wise) are discovered in the future. Algorithms for Head List Creation and Probability Estimation Based on Opt-in User Data ( Figures 3, 4): The opt-in users are partitioned into two sets -S, whose data will be used for initial head list creation, and T , whose data will be used to estimate the probabilities and variances of records from the formed initial head list.The initial head list creation algorithm, described in Figure 3, constructs the list in a differentially private manner using search record data from group S. • fO: the fraction of the opt-in users to use in head list creation (the remainder are used to estimate the record probabilities). 4: letˆpletˆletˆp C , ˆ σ 2 C = EstimateClientProbabilities(, δ, C, m C , f C , HL) be the estimated record probabilities and estimated variances based on client reports.5: letˆpletˆ letˆp = BlendProbabilities(ˆ p O , ˆ σ 2 O , ˆ p C , ˆ σ 2 C , HL)be the combined estimate of record probabilities. 6: return HL, ˆ p. opt-in users from S, and including in the head list those records whose noisy count exceeds a threshold. 9: for each distinct q, u ∈ D S do 10:let Y be an independent draw from Lap(b S ), i.e., Laplace distribution with scale b S centered at 0. We introduce a wildcard record , to represent records not included in the head list, for the subsequent task of estimating their aggregate probability.For each record included in the initial head list, the algorithm described in Figure 4 uses the remaining opt-in users' data (from set T ) to differentially privately estimate their probabilities, denoted byˆpbyˆ byˆp O . We need to set m O = 1 for the privacy guarantees to hold, because we treat data at the search record rather than query level.We form the final head list from the M most frequent records inˆpinˆ inˆp O . Finally, the head list is passed to the client group, and the head list and its probability and variance estimates are passed to the BlendProbabilities step of Blender.The choice of how to split opt-in users into the sub-groups of S and T and the choice of M are un-EstimateOptinProbabilities(, δ, T, m O , HL S , M )Parameters:• , δ: the differential privacy parameters. Algorithms for client data collection (Fig- ures 5, 6): For privatization of client data, the records are no longer treated as a single entity, but rather in a two-stage process: first privatizing the query, then privatizing the URL. :ˆ p C,q = ˆ r C,q − 1−t k−1 t− 1−t k−1 13: ˆ σ 2 C,q = 1 t− 1−t k−1 2 ˆ r C,q (1−ˆ r C,q ) |D C |−1 14: for u ∈ HL[q] do 15:letˆrletˆ letˆr C,q,u be the fraction of records which are q, u in D C . The privatization algorithm reports the true value with a certain bounded probability, and otherwise, randomizes the answer uniformly among all the other feasible values.ˆ p C,q,u = ˆ r C,q,u − (1−tq )t ˆ p C,q kq −1 − (1−t)(1− ˆ p C,q ) (k−1)kq t(tq − 1−tq kq −1 ) 17: ˆ σ 2 C,q,u = ˆ r C,q,u (1−ˆ r C,q,u ) |D C |−1 + 2|D C | |D C |−1 1−t (k−1)kq − t−ttq kq −1 k−2+t kt−1 ˆ r C,q,u + 1−t (k−1)kq − t−ttq kq −1 2 ˆ σ 2 C,q · 1 t 2 tq − 1−tq kq −1 2 18: returnˆpreturnˆ returnˆp C , ˆ σ 2 C . 20: continue 21:report q, u. of the most frequent records) is available to each client plays a crucial role in improving the utility of the data produced by this privatization algorithm compared to the previously known algorithms operating in the local privacy model. Knowledge of the head list allows dedicating the entire privacy budget to report the true value, rather than having to allocate some of it for estimating an analogue of the head list, as done in [15,34]. Another distinction from the Exponential mechanism designed to improve utility is utilization of δ. Algorithm for Blending (Figure 7): The blending portion of Blender combines the estimates produced by the opt-in and client probability-estimation algorithms by taking into account the sizes of the groups and the amount of noise each sub-algorithm added. 4: ˆ p q,u = w q,u · ˆ p O,q,u + (1 − w q,u ) · ˆ p C,q,u .5: Optional: ProjectˆpProjectˆ Projectˆp onto probability simplex (e.g., see [39]). LocalAlg is responsible for the privacy-preserving perturbation of each client's data before it gets sent to the server, and EstimateClientProbabilities is responsible for aggregating the received privatized data into a meaningful statistic. LocalAlg is (, δ)-differentially pri- vate.Denoising: The reports aggregated by the client mechanism form an empirical distribution over the records (and queries). The question we address now is how to best combine these estimates using the information available.A standard way to measure the quality of an estimate is by its variance. The error in the estimates obtained from the opt-in algorithm is due to the addition of noise, whereas the error in the estimates obtained from the client algorithm is due to randomization of the reports over the set of records in the head list. Since these variances depend on the underlying distribution, which is unknown a priori, we will compute sample variancesˆσvariancesˆ variancesˆσ 2 O,q,u andˆσandˆ andˆσ 2 O,q,u instead. We designed Blender with an eye toward preserving the utility of the eventual results in the two applications we explore in this paper: trend computation and local search, as described in Section 1.2. We use two established domain-specific utility metrics to assess the utility, the L1 metric and NDCG. Thus, the normalized discounted cumulative gain (N DCG k ), which ranges between 0 and 1, is defined asN DCG k = DCG k /IDCG k . While NDCG is traditionally defined for lists, Blender outputs a list-of-lists: there is a URL list corresponding to each query, and the queries themselves form a list. Since changes to the probabilities may not result in ranking changes, L1 is an even less forgiving measure than NDCG.Since the purpose of Blender is to estimate probabilities of the top records, we discard the artificially added queries and URLs and rescale rel i prior to L1 and NDCG computations. Figure 8 compares their characteristics.Data analysis: To familiarize the reader with the approach we used for assessing result quality, Fig- ure 9 shows the top-10 most frequent queries in the AOL data set, with the estimates given by the different "ingredients" of Blender.The table is sorted by column 2, which contains the non-private, empirical probabilities p q for each query q from the AOL data set with 1 random record sampled from each user. The remaining columns show the estimates that are produced by the sub-components of Blender that are eventually combined to form the estimates in column 3. Regressions, i.e., estimates that appear out of order relative to column 2, are shown in red.Takeaways: The biggest takeaway is that the numbers in columns 2 and 3 are similar to each other, with only one regression after Blender's usage.Blender compensates for the weaknesses of both the opt-in and the client estimates. Similarly, a range of δs has been used for evaluations (e.g., 10 −6 , 10 −5 , 10 −4 in [26] and 0.05 in [6]). From a behavioral perspective, this reduces a user's opt-in decision down to one purely of trust towards the curator.Opt-in and client group sizes, |O| and |C|: The relative sizes of opt-in group and client group, |O| and |C|, respectively, can be viewed as exogenous variables which are dictated by the trust that users place in the search engine. The NDCG metric in Figure 11b shows a trade-off that emerges as we assign more budget to the queries, de-emphasizing the URLs; before and after 0.85, we start seeing a drop in NDCG values for the client algorithm. Since our opt-in group's size is small relative to our client group size, and it is difficult to generate a head list in the local privacy model -it makes sense to utilize most of the opt-in group's data for the task that is most difficult in the local model. We choose M = 50 and M = 500 for the AOL and Yandex datasets, to reflect their differing sizes.Subsequently, we use the parameters shown in Fig- ure 10 unless explicitly stated. The closest related work is a recent paper by Qin et al. [34] for heavy hitter estimation with local differential privacy, in which they provide a utility evaluation of their algorithm on the AOL data set for the head list size of 10. A caveat to these findings is that Qin et al.[34] and this work use slightly different scoring functions. Although it yields increased NDCG scores, Blender operates on records (rather than queries, as Qin et al. does). If our goal is to have head lists of 500+, we see that with the larger Yandex data set, an opt-in percentage as small as 2.5% is sufficient to achieve high utility. Similar to the case with small opt-in percentages, having too small an makes it difficult to achieve head lists of their target size; e.g., in Figure 15a, the line for a head list of size 50 does not begin until = 3 because that size head list was not created with a smaller value.Evaluation of local search computation: Fig- ure 14 shows the NDCG measurements as a function of the opt-in percentage ranging between 1% and 10%. Furthermore, the utility of the resulting data obtained through these algorithms is significantly limited compared to what is possible in the trusted curator model, as shown experimentally [15,21] and theoretically [22]. However, that's where the similarities with Blender end, as [34] focuses entirely on the local model (and thus has to use entirely different algorithms from ours for each stage) and addresses the problem of estimating probabilities of queries, rather than the more challenging problem of estimating probabilities of query-URL pairs.Our contribution: Our work significantly improves upon the known results by developing application-specific local privatization algorithms that work in combination with the trusted curator model algorithms. We proposed a hybrid privacy model and a blended approach that operates within it that combines the upsides of two common models of differential privacy: the local model and the trusted curator model. Using local search as a motivating application, we demonstrated that our proposed approach leads to a significant improvement in terms of utility, bridging the gap between theory and practicality.Future work: We plan to continue this work in two directions: first, to address any systems and engineering challenges to Blender's adoption in practice, including those that arise due to data changing over time; and second, to develop algorithms for other settings where the hybrid privacy model is appropriate, thus facilitating adoption of differential privacy in practice by minimizing the utility impact of privacy-preserving data collection. Pr[L(q) ∈ Y ] = q HL ∈Y Pr[L(q) = q HL ] = q HL ∈Y \{q,q } Pr[L(q) = q HL ] + q HL ∈Y ∩{q,q } P r[L(q) = q HL ] = q HL ∈Y \{q,q } Pr[L(q ) = q HL ] + q HL ∈Y ∩q,q Pr[L(q) = q HL ] (2) ≤ q HL ∈Y \{q,q } Pr[L(q ) = q HL ] + q HL ∈Y ∩{q,q } e Q Pr[L(q ) = q HL ] + δ Q 2 (3) ≤ e Q q HL ∈Y Pr[L(q ) = q HL ] + 2 · δ Q 2 = e Q Pr[L(q ) ∈ Y ] + δ Q ,Equality (2) stems from the fact that the probability of reporting a false query is independent of the user's true query. Privacy of URL Reporting:With t q defined as t q = exp( U )+0.5δ U (kq−1) exp( U )+kq−1, an analogous argument shows that the ( U , δ U )-differential privacy constraints hold if the original q is kept. Given the head list, the distribution of EstimateOptinProbabilities' estimate for a record q, u is given by r O,q,u = p q,u + Y |D T | , where Y ∼ Laplace(b T ) where b T is the scale parameter and |D T | is the total number of records from the opt-in users used to estimate probabilities. Usingˆp Usingˆ Usingˆp O,q,u directly in place of p q,u requires a |D T | |D T |−1 factor correction (known as "Bessel's correction 5 ") to generate an unbiased estimate. From Section 3.2 on denoising, the distribution of the reported query q from the client mechanism is given by r C,q = t · p q + 1−t k−1 (1 − p q ), and so the true probability of query q is distributed as p q = , wherê r C,q is the empirical estimator of r C,q defined explicitly asˆrasˆ asˆr C,q = 1 |D C | |D C | j=1 X j , where X j ∼ Bernoulli(r C,q ) is the random variable indicating whether report j was query q and |D C | is the total number of records from the client users.The variance ofˆrofˆ ofˆr C,q isV [ˆ r C,q ] = V 1 |D C | |D C | j=1 X j = 1 |D C | 2 |D C | j=1 V [X j ] (6) = 1 |D C | 2 |D C | · r C,q (1 − r C,q ) = r C,q (1 − r C,q ) |D C | ,where equality 6 relies on an assumption of independence between X j , X k for all j = k (i.e., the iid assumption discussed prior to the theorem statements in Section 3.3).