In this paper, we propose an automated analysis approach that does not only identify the root cause of a given crashing input for a binary executable, but also provides the analyst with context information on the erroneous behavior that characterizes crashing inputs. A statistical analysis of all predicates allows us to identify the predicate pinpointing the root cause, thereby not only revealing the location of the root cause, but also providing an analyst with an explanation of the misbehavior a crash exhibits at this location. In contrast to existing approaches, AURORA is also able to handle bugs with no data dependency between root cause and crash, such as type confusion bugs. Fuzz testing (short: fuzzing) is a powerful software testing technique that, especially in recent years, gained a lot of traction both in industry and academia [28,29,31,47,49,53,59]. Informally speaking, bucketing groups crashing inputs according to some metricoften coverage or hashes of the call stack-into equivalence classes. Therefore, an analyst needs to analyze the path from the crashing location backward to find the root cause, which requires significant effort.Consider, for example, a type confusion bug: a pointer to an object of type A is used in a place where a pointer to B is expected. If a field of B is accessed, an invalid access on a subsection of A can result. If the structures are not compatible (e. g., A contains a string where a pointer is expected by B), this can cause memory corruption. In this case, the crashing location is most likely not the root cause of the fault, as the invariant "points to an instance of B" is violated in a different spot. Instead, the particular control flow that makes a value from type A end up in B's place is at fault.In a naive approach, an analyst could inspect stack and register values with a debugger. Still, the analyst has to manually recognize the type confusion as the root cause-a complicated task since most code involved is behaving correctly.More involved approaches such as POMP [57], RE-TRACER [33], REPT [32] and DEEPVSA [38] use automated reverse execution and backward taint analysis. However, these approaches generally do not allow to automatically identify the root cause unless there is a direct data dependency connecting root cause and crashing instruction. A diverse set of crashing inputs that mostly trigger the same bug can aid analysis. Observing multiple ranges of values and different control-flow edges taken can help narrow down potential root causes. Consequently, identifying the root cause remains a challenging task, especially if there is no direct data dependency between root cause and crashing instruction. This includes control-flow information and relevant register values for each instruction. Using the tracked information obtained from the diversified set of inputs, we can observe that (nearly) all calls in crashing inputs invoke the constructor of type A, while calls to the constructor of B imply that the input is not going to cause a crash. To evaluate AURORA, we analyze 25 targets that cover a diverse set of vulnerability classes, including five use-after-free vulnerabilities, ten heap buffer overflows and two type confusion vulnerabilities that previous work fails to account for. For example, we analyzed a type confusion bug in mruby where an exception handler fails to raise a proper exception type. The following code snippet shows a minimized example of Ruby code that leads to a type confusion bug in the mruby interpreter [16] found by a fuzzer:1 N o t I m p l e m e n t e d E r r o r = S t r i n g 2 Module . c o n s t a n t sIn the first line, the exception type NotImplementedError is modified to be an alias of type String. As a consequence, each instance of NotImplementedError created in the future will be a String rather than the expected exception. In the second line, we call the constants function of Module. Raising the exception causes a crash in the mruby interpreter.To understand why the crash occurs, we need to dive into the C code base of the mruby interpreter. When we re-assign the exception type NotImplementedError to String, this is realized on C level by modifying the pointer such that it points to a struct representing the mruby String type. The method Module.constants is only a stub that creates and raises an exception. When the exception is raised in the second line, a new instance of NotImplementedError is constructed (which now actually results in a String object) and passed to mruby's custom exception handling function. It proceeds to successfully attach some error message-here "Module.constants not implemented" (length 0x20)-to the presumed exception object. However, as we have replaced the exception type by the string type, the layout of the underlying struct is different: At the accessed offset, the String struct stores the length of the contained string instead of a pointer as it would be the case for the exception struct. As a result, we do not dereference the pointer but interpret the length field as an address, resulting in an attempt to dereference 0x20. Since this leads to an illegal memory access, the program crashes.To sum up, redefining an exception type with a string leads to a type confusion vulnerability, resulting in a crash when this exception is raised. There are various kinds of sanitizers, such as MSAN [52] to detect usage of uninitialized memory or ASAN [51] to detect heap-and stack-based buffer overflows, use-after-free (UAF) errors and other faults. To deepen our understanding of the bug, we could use automated root cause analysis tools [32,33,57] that are based on reverse execution and backward taint tracking to increase the precision further. However, in our example, there is no direct data flow between the crash site and the actual root cause. The data flow ends in the constructor of a new String that is unrelated to the actual root cause. These inputs are likely to trigger the same bug via different code paths.To gain new insights into the root cause of our bug, we need the crash exploration mode to trigger new behavior related to the type confusion. In theory, to achieve this, the fuzzer could assign another type than String to NotImplementedError. This result benefits the analyst as comparing the two inputs indicates that the crash could be related to NotImplementedError's type. Still, this leaves the human analyst with an additional input to analyze, which means more time spent on debugging.Overall, this process of investigating the root cause of a given bug is not easy and-depending on the bug type and its complexity-may take a significant amount of time and domain knowledge. In its core, our method conducts a statistical analysis of differences between a set of crashing and non-crashing inputs. Intuitively, the first relevant behavior during program execution that causes the deviation is the root cause.In a first step, we create two sets of related but diverse inputs, one with crashing and one with non-crashing inputs. Amongst good explanations, we prefer these that are able to predict the crash as early as possible.On a high-level view, our design consist of three individual components: (1) input diversification to derive two diverse sets of inputs (crashing and non-crashing), (2) monitoring input behavior to track how inputs behave and (3) explanation synthesis to synthesize descriptive predicates that distinguish crashing from non-crashing inputs. For each instruction executed, we record the minimum and maximum value of all modified registers (this includes general-purpose registers and the flag register). Furthermore, we collect the address ranges of stack and heap to test whether certain pointers are valid heap or stack pointers.We do not trace any code outside of the main executable, i. e., shared libraries. A predicate is a triple consisting of a semantic description (i. e., the Boolean expression), the instruction's address at which it is evaluated and a score indicating the ability to differentiate crashes from non-crashes. Given the trace information for all inputs in both sets, we can reason about potential root cause locations and determine predicates that explain the root cause. Given these traces, we collect all controlflow transitions observed in crashing and non-crashing inputs and construct a joined control-flow graph that is later used to synthesize control-flow predicates. To capture a wide array of possible explanations of a software fault's root cause, we generate three different categories of predicates, namely (1) control-flow predicates, (2) register and memory predicates, as well as (3) flag predicates. Additionally, we have two fixed predicates testing whether expressions are valid heap or stack pointers, respectively: is_heap_ptr(r) and is_stack_ptr(r). On the x86 and x86-64 architecture, the flag register tracks how binary comparisons are evaluated and whether an overflow occurred, making it an interesting target for analysis. We call a predicate perfect if it correctly predicts the outcome of all test cases. In other words, such a predicate perfectly separates crashing and non-crashing inputs.Unfortunately, there are many situations in which we cannot find a perfect predicate; consequently, we assign each predicate a probability on how well it predicts the program's behavior given the test cases. In this model, the final outcome of the test case is the result of the predicate XORed with some random variable. We later employ this uncertainty to rank the different predicates:ˆ θ = C f + N f C f +C t + N f + N tWe count the number of both mispredicted crashes (C f ) and mispredicted non-crashes (N f ) divided by the number of all predictions, i. e., the number of all mispredicted inputs as well as the number of all correctly predicted crashed (C t ) and non-crashes (N t ). To avoid biasing our scoring scheme towards the bigger class, we normalize each class by its size:ˆ θ = 1 2 * C f C f +C t + N f N f + N tIfˆθIfˆ Ifˆθ = 0, the predicate is perfect. We use this score to pick the best predicate for each instruction that has been visited by at least one crashing and one non-crashing input. Furthermore, consider a predicate p 1 , with p 1 := min(rax) < 0xff. When computing our register and memory predicates of type r < c, we want to derive a constant c that splits the test inputs into crashing and non-crashing inputs based on all values observed for r during testing. The only way to change the quality of the predicate is to choose a value of c that flips the prediction of at least one value of r. All constants c between two different observations of r perform the exact same split of the test inputs. This property of our scoring scheme allows us to update the score in constant time when checking the next candidate value of c.To calculate the score for any candidate value c i , we start at the smallest candidate c 0 and calculate the predicate's score by evaluating the predicate on all inputs and counting the number of correctly predicted outcomes. Since using c i+1 instead of c i only flips a single prediction, we can efficiently update C t , C f , N t , N f in constant time. This allows us to track C t , C f , N t , N f while trying all values of c to determine the value which maximizes the score. Consider that we want to synthesize a value c that maximizes the score of the predicate p(r) = r < c. Assume that we have four inputs reaching the address where the predicate is evaluated and we observed the following data:outcome crash crash non-crash non-crash values of r 0x08 0x0f 0x400254 0x400274In this example, the values are already sorted. Repeating this for the next step, we obtain a perfect score for the next value 0x400254 as both crashing values are smaller. Consequently, the remaining predicates identify locations that are related to the bug.Still, we do not know in which order relevant predicates are executed; therefore, we cannot distinguish whether a predicate is related to the root cause or occurs later on the path to the crash site. As predicates early in the program trace are more likely to correspond to the root cause, we introduce a new metric called the execution rank. To calculate the execution rank, we determine the temporal order in which predicates are executed. Let the observed predicate order be (p 1 , p 3 ) for i 1 and (p 1 , p 3 , p 2 ) for i 2 . We briefly explain important implementation aspects in the following, the full source code is available at https://github.com/ RUB-SysSec/aurora. It takes two folders containing traces of crashes and non-crashes as input. Based on the prototype implementation of AURORA, we now answer the following research questions: RQ 1: Is AURORA able to identify and explain the root cause of complex and highly exploitable bug classes such as type confusions, use-after-free vulnerabilities and heap buffer overflows? In particular, we picked the following bugs:• ten heap buffer overflows, caused by an integer overflow (#1 mruby [1 [25]) These bugs have been uncovered during recent fuzzing runs or found in the bug tracking systems of well-known applications. The former is required as a starting point for our analysis, while the latter serves as ground truth for the evaluation.For each target, we compile two binaries: One instrumented with AFL that is used for crash exploration and one noninstrumented binary for tracing purposes. However, due to the highly structured nature of the input languages for mruby, Lua, nm, libzip, Python (only #17) and PHP, AFL found less than 100 inputs within two hours. Thus, for each bug, we first denote its root cause as identified by AURORA and verified by the developers' patches. Thus, we excluded it from our analysis.To obtain insights into whether our approach is actually capable of identifying the root cause even when it is separated from the crashing location by the order of thousands of instructions, we design an experiment to measure the dis- Table 1: Results of our root cause explanations. More specifically, for each target, we determine the maximum distance, the average distance over all crashing inputs and-to put this number in relation-the average of total instructions executed during a program run. Exceptions are larger targets, such as Python (13 MiB) and PHP (31 MiB), or particularly challenging bugs such as type confusions (#16 and #17). Still, the number of source code lines to inspect is below 28 for all but the Python type confusion (#17), which contains a large amount of false positives. As Table 2 indicates, AURORA is capable of both identifying root causes when the distance is small (a few hundred instructions, e. g., 197 for Sleuthkit) and significant (millions of instructions, e. g., roughly 28 million for screen). During our evaluation, we observed that they are mostly related to (1) (de-)allocation operations as well as garbage collectors, (2) control-flow, i. e., predicates which indicate that non-crashes executed the pinpointed code in diverse program contexts (e. g., different function calls or more loop iterations), (3) operations on (complex) data structures such as hash maps, arrays or locks, (4) environment, e. g., parsing command-line arguments or environment variables (5) error handling, e. g., signals or stack unwinding. Such superficial features may differentiate crashes and non-crashes but are generally not related to the actual bug (excluding potential edge cases like bugs in the garbage collector). Many of these false positives occur due to insufficient code coverage achieved during crash exploration, causing the sets of crashing and non-crashing inputs to be not diverse enough.To detect such false positives during our evaluation, we employed various heuristics: First, we use the predicate's annotations to identify functions related to one of the five categories of false positives and discard them. False Positive Categories Propagations In-depth Analysis Alloc CF DS Env Error#3 Perl - - 7 - - - - #6 mruby - -38 - - - - #7 objdump - 2 - - - - - #9 Python - 1 - 2 3 - - #12 Bash 1 1 - 1 4 8 7 #13 Bash 1 1 - - 4 5 4 #14 Python - - - 3 - 15 5 #17 Python 40 - 2 - - - 1 #20 PHP - - - 21 - - - #22 mruby - 1 - - - 4 3 #23 NASM 3 - - - 2 2 2 #24 Sleuthkit - 2 - - - - -all predicates found by AURORA are strongly related to the actual root cause. As described, the NotImplementedError type is aliased to the String type, leading to a type confusion that is hard to spot manually. Types in mruby are implemented as enum, as visible in the following snippet of mruby's source code (mruby/value.h):112 MRB_TT_STRING , / * 16 * / 113 MRB_TT_RANGE, / * 17 * / 114 MRB_TT_EXCEPTION , / * 18 * /Our identified root cause pinpoints the location where the developers insert their fix and semantically states that the type of the presumed exception object is smaller than 17. As can be seen, the String type has a value of 16; thus, it is identified as crashing input, while the exception type is assigned 18. As a consequence, the garbage collector decides to free the struct containing the original class NotImplementedError, a very uncommon event. Subsequent predicates point to locations where the string is attached to the presumed exception object during the raising of the exception. GNU Binutils' readelf application may crash as a result of a heap buffer overflow when parsing a corrupted MIPS option section [5]. This value is then processed further, amongst others, by an integer division where it is divided 00:10 00:03 00:02 #24 Sleuthkit < 1 min < 1 min < 1 min #25 Lua 00:11 00:07 < 1 min by 0x10, resulting in a value of 0. Subsequently, writing any data larger than one byte-which is the case for the struct the memory is intended for-is an out-of-bounds write. As no crucial data is overwritten, the program flow continues as normal unless it was compiled with ASAN, which spots the out-of-bounds write.To prevent this bug, the developers introduced a fix where they check whether the allocated memory's size is sufficient to hold the struct. For instance, we see that the minimal memory value written is less than 0x1c at some address. Consequently, our analysis pinpoints locations between the root cause and the usage of the uninitialized value. Immediately before the function attempts to dereference the pointer, we see that the minimal value of rax is smaller than 0xff, which indicates that the value was propagated. As our evaluation shows, our approach is capable of identifying and explaining even complex root causes where no direct correlation between crashing cause and root cause exists. However, this is a limitation of the underlying fuzzer rather than AURORA: Our analysis would scale to complex systems spanning multiple processes and interacting components; our statistical model can easily deal with systems where critical data is passed and serialized by various means, including networks or databases, where traditional analysis techniques like taint tracking fail.In some cases, the predicates that we generate might not be precise enough. In some cases, AURORA ran for up to 17 hours (including 12 hours for crash exploration). Thus, an additional 17 hours of fuzzing will hardly incur a significant cost for typical testing setups. Since it took us multiple person-days to pinpoint the root cause for some of the bugs we analyzed, making the integration of our fully automated approach into the fuzzing pipeline seems feasible.An integration to fuzzing could benefit the fuzzer: Successful fuzzing runs often produce a large number of crashing inputs, many of which trigger the same crash. Most rely on the input's coverage profile or stack hashing where the last n entries of the call stack are hashed [42]. Our approach could be extended to patch the root cause predicate into the binary such that-at the point of the root cause-any input crashing the binary leads to a graceful exit rather than a potentially exploitable crash. al. [60] describe an approach to root cause identification targeting the Java Virtual Machine: first, they locate the non-crashing input from provided test suite whose control flow paths beginning overlaps the most with the one of the crashing input under investigation. This provides our approach with a more diversified set of inputs, allowing for more fine-grained analysis.Reverse Execution. To this end, they reverseexecute the program, reconstructing the data flow leading to the crash. Further reducing the manual effort needed, POMP requires a control-flow trace and crash dump, then uses backward taint analysis [57] to reverse the data flow, identifying program statements contributing to a crash. While these approaches are useful in scenarios where a crash is not reproducible, we argue that most of them are limited to correctly identify bugs that exhibit a direct data dependency between root cause and crashing location. Furthermore, since we do not perform a concrete analysis of the underlying code, AURORA can spot vulnerabilities with no direct data dependencies. We also thank Nils Bars, Thorsten Eisenhofer and Tobias Scharnowski for their helpful feedback.