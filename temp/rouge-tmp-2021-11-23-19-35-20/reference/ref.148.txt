After 120 days, implemented participant-designed threat mitigation strategies provided tangible security benefits for NYC, including blocking 541 unique intrusion attempts, preventing the hijacking of five privileged user accounts, and addressing three public-facing server vulnerabilities. As a result, it can be difficult to quantify the benefit of threat modeling in practice.In this paper, we present the first case study of threat modeling in a large, high-risk enterprise environment: New York City Cyber Command (NYC3). We then tracked the impact of this threat modeling training on NYC3's security posture quantitatively, through analysis of 120 days of log data, and qualitatively, via pre-, post-, and 30-day-post-training surveys with participants. Collectively, participants developed 147 unique mitigation strategies, of which 64% were new and unimplemented within NYC3. Additionally, participants identified new threats in eight distinct areas within their environment, such as physical access-control weaknesses and human configuration errors. Many threat-modeling frameworks aim to improve practitioners' situational awareness and provide them with a decision-making process for complex problems [15,25]. We selected CoG because it incorporates many key characteristics from across more pervasive frameworks: CoG provides practitioners with a top-down approach to identifying internal points of vulnerability, similar to STRIDE [38,39], and it assists with assessing vulnerabilities from an adversarial perspective, similar to attack trees, security cards, persona non grata, and cyber kill chain [9,14,28,54]. An end state provides the why for implementing defenses and allows an individual practitioner to understand their own specific security objective with respect to the organization.Once the practitioner understands their objective, the next step is to identify all of the assets currently in use that support accomplishing the objective. In this context, an asset can be a system, a service, a tool, or anything relevant to accomplishing the objective (not just securityspecific assets). Some CVs are binary, such as the complete loss of a CR, but others may cause a reduced functionality beyond some threshold, preventing the CoG from accomplishing the objective.Building a thorough list of critical vulnerabilities allows the practitioner to understand how their objectives can be threatened. From the list of TCs, they enumerate all of the threat requirements (TR) needed to support each capability.The final step in the CoG analysis process is building an actionable defense plan (ADP) that can neutralize identified threat capabilities and requirements, mitigate critical vulnerabilities, and protect the identified CoG. A limited number of threat-modeling frameworks have been empirically evaluated, and none have been assessed at the enterprise level. Instead, we use one particular approach as a case study to examine the introduction of threat-modeling within an enterprise environment, using participants with a direct, vested interest in improving their job performance and the security posture of their environment. Both effectiveness, the ability to successfully achieve an outcome, and efficiency, the ability to reduce effort to achieve an outcome, comprise efficacy.Because we introduced threat modeling in NYC3's operational environment, we were not able to conduct a comparative experiment; instead, we designed a primarily observational study to obtain as much insight as possible -both qualitative and quantitative -into the effects of introducing threat modeling within an enterprise environment. Our study includes six components (as shown in Figure 2), that occurred from June through November 2017, and was approved by the University of Maryland Institutional Review Board. Establishing a baseline for NYC3 defensive practices allows us to compare the security posture before and after our training intervention. We asked participants about their specific work role, responsibilities, and demographics; their understanding of organizational mission statements; which assets they use to accomplish their daily duties; their sentiment towards NYC3's current security posture; and their perceived self-efficacy for performing digital security tasks.We used a combination of open-ended, close-ended, and Likert-scale questions in our 29-question online survey (App. We used an identical structure for the post-training survey and 30-day follow-up survey. Capturing self-efficacy before, immediately after, and 30 days after receiving the educational intervention allowed us to measure how each participant perceived the model's efficacy. A.Prior to providing the intervention, the instructor observed NYC3 employees at work for four days to better understand their operating environment. After all participants finished the educational intervention training, they each completed a 60-minute individual session where they applied CoG to their daily duties. D), and allowed participants to bring in any notes from the previous educational intervention training. Without notifying the participants, we logged task completion times for each step, in an effort to measure the efficiency of the framework without putting undue pressure on participants.The interviewer used the constructive interaction method for communicating with the participants, asking them to openly communicate throughout each subtask in Section 2.2 [40]. B), conducted immediately after the performance evaluation session, we collected responses measuring the framework's actual and perceived efficacy. We asked participants to re-apply CoG to their daily duties, which allowed them to account for any new details they might have considered since the previous session. After 120 days, we evaluated the efficacy of adopted defense plans for protecting NYC3 systems. All field studies and qualitative research should be interpreted in the context of their limitations.We opted to measure only one threat-modeling framework: although our sample represents 37% of the NYC3 workforce, 25 participants (in many cases with no overlap in work roles) would not have been sufficient to thoroughly compare multiple approaches. We mitigated this through (1) anonymous online surveys that facilitated open-ended, candid feedback, (2) removing researchers from the environment for 30 days before the follow-up survey, and (3) collecting actual adoption metrics. Moreover, reusing validated survey items and scales in this study is a best-practice in survey design that has been shown to reduce bias and improve construct validity [18,22]. We report participant demographics, baseline metrics, immediate post-training observations, 30-day observations, and observations after 120 days.We organize our findings within the established framework of perceived efficacy, actual efficacy, and actual adoption [32,41,50]. Actual efficacy confirms the validity of perceptions and further shapes the likelihood of adoption. Most commonly, they prioritized defending high-impact service-based systems such as NYC.gov (n=7) and adhering to compliance frameworks (n=7), followed by applying risk management strategies (n=6) and assessing which systems are most susceptible to attack (n=3). In contrast to the baseline survey, performance evaluation session observations and post-training surveys indicate that threat modeling provided participants with a better understanding of their security environment, that participants felt more confident in their ability to protect NYC, and that participants could successfully apply threat modeling relatively quickly with accurate results. During his performance evaluation session, P24 discussed how changes in the political environment from the local to federal level can affect established trust across the GoNYC; a large turnover in personnel could halt some progress and potentially kill some initiatives. P05 stated that threat modeling helps him "plan effectively, document, track, monitor progress, and essentially understand our security posture. When comparing responses from the post-training survey to baseline responses, 10 participants reported a perceived increase in their ability to monitor critical assets, 17 reported an increase in their ability to identify threats, 16 reported an increase in their ability to mitigate threats, 15 participants reported an increase in their ability to respond to incidents. We measure the actual efficacy of threat modeling using several metrics: the accuracy of participants' output, task completion times, similarities between participants' identified CoGs, and the contents of their actionable defense plans. We included one fictitious participant entry as an attention check and validity control, which both panel members identified and rejected.The panel concluded that: 22 of 25 identified centers of gravity were accurate with respect to a participant's responsibilities ('EL'=3, 'Likely [L]'=9,'Somewhat likely [SL]'=10); all critical vulnerabilities were accurate for the identified centers of gravity (EL=6, L=7, SL=12); 23 of 25 threat capability and requirement profiles were accurate (EL=6, L=7, SL=10), and 24 of 25 actionable defense plans would accurately address the identified threats (EL=5, L=11, SL=8). We used a logistic regression, appropriate for ordinal Likert data, to estimate the effect of work roles, experience in IT, and educational background on the accuracy of the panel results. The final selected model is given in Appendix E. Based on this regression, we found that no particular work role, amount of education, IT experience, or combination thereof enjoyed a statistically significant advantage when using threat modeling. Analysis of the performance evaluation session results reveals that participants with similar work role classifications produced similar output. Within the 25 actionable defense plans, participants cumulatively developed 147 mitigation strategies; we provide detailed examples in Section 4.5. Participants indicated that 33% of the mitigation strategies they developed using threat modeling were new plans that would immediately improve the security posture of their environment if implemented. In cases such as this, individuals require additional time to improve the fidelity of their responses or may benefit from expert assistance in 27th USENIX Security Symposium 629 transforming their ideas into fully developed plans. After 30 days, we observed that participants still had a favorable opinion of threat modeling, most participants actually implemented defensive plans that they developed through our study, and that NYC3 institutionalized threat modeling within their routine practices. Thirty days after learning about CoG, there was a slight decrease in the perceived efficacy of the framework when compared to participant perceptions immediately after training: a 1.47% decrease for monitoring critical assets (W=81.0, p=0.57), 3.22% decrease for identifying threats (W=131.0, p=0.83), 3.58% decrease for mitigating threats (W=94.0, p=0.18), and 1.67% decrease for responding to incidents (W=100.0, p=0.59); none of these decreases were statistically significant. Overall, all participants agreed ("Strongly"= 13) that threat modeling supports critical aspects of their job. Despite the aforementioned decrease in perceived efficacy over the 30-day period, the number of participants who found the framework useful to their jobs increased from 23 to 24, as NYC3's adoption of ADPs within their environment caused one participant to believe in the framework's usefulness. We measure actual efficacy after 30 days using participants' knowledge retention. For example, seven participants now use the framework for continually assessing risk; this is in contrast to the baseline results, where participants typically assessed risk only during audits and initial accreditation. Additionally, NYC3 provided us with access to server logs, their alert dashboard, and vulnerability reports so that we could measure the actual efficacy of three of these new controls. NYC3 leaders monitored the implementation of these ADPs using their priorities board, and all mitigation strategies persist within the NYC environment 120 days after the study. Accordingly, NYC3 has begun testing fail-over servers within their local domain and plans to implement periodic, mandatory readiness tests across all NYC networks.Securing accounts. As a proof of concept, NYC3 implemented multi-factor authentication for 80 user accounts within a monitored subdomain.Protecting physical network assets. Since the performance evaluation sessions, NYC3 has been working with federal, state, and private-sector entities on issues related to this topic.Crowdsourcing assessments. Because of his recommendation, NYC3 partnered with a bug bounty service provider to conduct a 24-hour proof-of-concept assessment against one of its web services.Sensor coverage. In this situation, a gap in sensor coverage can lead to unprotected systems or the successful exploitation of known vulnerabilities. Reducing human error. Human error was another common theme across the threat landscape. Such controls require one person to propose a change and another to review and implement the change to reduce the likelihood of human error. Among the remaining failed logins, NYC3 successfully blocked hijacking attempts that originated from a foreign nation against seven privileged user accounts. Of these seven accounts, the attacker failed at the multi-factor login step for five accounts and failed due to password lockout on the other two accounts. Sensor coverage. NYC3 deployed 1331 new sensors to endpoints that were previously unmonitored and were able to verify and respond to 541 unique intrusion attempts identified by these new sensors. After 120 days, participant-designed ADPs blocked account hijackings of five privileged user accounts, blocked 541 unique intrusion attempts, and discovered (and remedied) three vulnerabilities in public-facing web servers, all of which support that introducing threat modeling made NYC3 more secure.We note that many of the ADPs that NYC3 employees developed and implemented (Section 4.5) contain straightforward recommendations, such as applying multi-factor authentication. After the performance evaluation sessions, without prompting, 24 of 25 participants said that the personalized, hands-on application allowed them to understand the framework better than the educational intervention classes alone. Our logistic regression analysis on participants' CoG accuracy revealed a relatively level understanding of the framework across educational backgrounds, experience levels, and work roles. This accords with prior studies of threat-modeling techniques, as well as peer partnering examples from other domains, that demonstrate the benefits of peer collaboration [9,14,15,20,24,25,28,34,35,37,38,42,46,53]. Future evaluations may be able to consider how organization size, experience level and typical workload of staff members, organizational culture, and existing threat-modeling and/or security-analysis processes affect the efficacy of threat modeling. With respect to our running example, representative TCs against corrupted updates include the ability to tamper with or manin-the-middle IOC updates. Full versions of the pre-intervention survey, postintervention survey, and follow-up survey are viewable at ter.ps/nycsurvey1, ter.ps/nycsurvey2, and ter. Some participants opted to use a whiteboard to visually depict their thought processes and building heterogeneous, relational linkages between nodes. Table 2: Summary of regression over participants' accuracy at identifying centers of gravity with respect to their years of experience and education. sWe used the following two scenarios during our educational intervention training to communicate CoG analysis concepts to participants. We used the following two scenarios during our educational intervention training to communicate CoG analysis concepts to participants. The educational intervention instructor guided participants through this scenario, explaining the CoG analysis for the Galactic Empire.