In this work, we conduct an empirical analysis of interpretation errors made by Amazon Alexa, the speech-recognition engine that powers the Amazon Echo family of devices. The NSP corpus provides speech samples of 188 words from 60 speakers located in six distinct "dialect-regions" in the United States.We find that for this dataset of 11,460 utterances, Alexa has an aggregate accuracy rate of 68.9% on single-word queries. this attack in a developer environment and show that we are able to successfully "squat" skills, meaning that Alexa invokes the malicious skill instead of a user-intended target skill at least once for 91.7% of the words that have systematic errors. Figure 2: User-skill interaction in Alexa -A typical user interaction with an Alexa skill, using an Echo device. In order to add extensibility to the platform, Amazon allows the development of third-party applications, called "skills", that leverage Alexa voice services. For example, the phonetic representation of Figure 3: Speech-to-Text Test Harness Architecture -By building an experimental skill (called "Record This"), we are able to use the Amazon Alexa speech recognition system as a black box transcription service. In order to use Alexa as a transcription service, we built an Alexa skill (called "Record this") that records the raw transcript of input speech. Figure 4 shows each of these speech regions -Mid-Atlantic, Midland, New England, North, South, and West -over a map of the United States. 40,582 (68.3%) of the words in the dataset are only spoken by a single speaker, which makes reasoning about interpretation errors in such words difficult. This provides us with 573,000 data points across 60 speakers. Accuracy by Word Figure 5: Word Accuracy -The accuracy of Alexa interpretations by word is shown as a cumulative distribution function. We find that Alexa successfully interprets only 394,715 (68.9%) out of the 572,319 queries.In investigating where Alexa makes interpretation errors, we find that errors do not affect all words equally. The median number of misinterpretations is 15, but with a heavy tail.In investigating the distributions of misinterpretations per word, we observe that, for each of the 188 words, there are one or two interpretations that Alexa outputs more frequently than the others. The MCE of "boil" is the word "boyle", which accounts for 94.3% (MCE Rate) of the errors. There are 24 (12.8%) such words in our dataset.rip rap R IH P R AE P lung lang L AH NG L AE NG wet what W EH T W AH T dime time D AY M T AY M bean been B IY N B IH N dull doll D AH L D AA L coal call K OW L K AO L luck lock L AH K L AA K loud louder L AW D L AW D ER sweeten Sweden S W IY T AH N S W IY D AH N We now have a classification for interpretation errors from our dataset. We next investigate why these systematic errors occur.Homophones Unsurprisingly, eight (33.3%) of these errors, including "sail" to "sale", "calm" to "com", and "sell" to "cell" are attributable to the fact that these words are homophones, as they have the same pronunciation, but different spellings. To this end, we introduce a new attack, called skill squatting, which exploits predictable errors to surreptitiously route users to a malicious Alexa skill. We next investigate whether these errors can be exploited in a skill invocation environment, to redirect the processing of an Alexa query to an attacker-controlled skill server.Our testing process is as follows: given a model of predictable errors, we build pairs of skills with names that are frequently confused by Alexa. We are able to successfully squat 25 (92.6%) of the skills at least one time, demonstrating the feasibility of the attack.They will instead be routed to the Boyle skill.In order to demonstrate that our attack will work on speakers we have not previously seen, we use two-fold cross validation over the 60 speakers in our dataset. However, because louder is a native Alexa command which causes Alexa to increase the volume on the end-user device, when the target is misinterpreted, it is instead used to perform a native Alexa function. We next investigate how an adversary can craft maliciously named skills targeting existing skills in the Alexa skills store, by leveraging the squattable words we identified in Section 4. We note that the number of squattable skills we identify is primarily limited by the size of our dataset and it is not a ceiling for the pervasiveness of this vulnerability in the Amazon market. Consider the following example from our tests, where the input word "absentee" (AE, B, S, AH, N, T, IY) is understood by Alexa as "apps and t." (AE, P, S, AH, N, D, T, IY). To accomplish this, we leverage the Forvo dataset, described in Section 3, as a test set.First, we exclude from our test set all the speech samples of words that are also in the NSP dataset, since we seek to predict errors for words that we have not used before. To validate the correctness of our predictions, we next collect the actual Alexa interpretations of this set of words. However, we find it interesting that such examples cur- Table 6: Squatted Skills in the Alexa skills store -We show examples of squatted skills in the Alexa skills store that drew our attention during manual analysis. For example, "cat facts" has a corresponding squatted skill, "cat fax", which seemingly performs the same function, though published by a different developer. Spear skill squatting extends skill squatting attacks by leveraging words that only squattable in targeted users' demographic. As we would like to measure whether interpretation errors happen across all regions with equal probability, our null hypothesis is that there is no significant difference in accuracy between the regions. We next investigate whether the interpretation errors for each demographic are systematic and, as a result, can be used by an adversary to launch a spear skill squatting attack.To identify squattable words based on region, we first split our speakers into their respective dialect-region. Table 7: Validating the Spear Skill Squatting Attack -We test our spear skill squatting attacks in a developer environment. To prevent skill squatting, Amazon could add to the certification process both a word-based and a phoneme-based analysis of a new skill's invocation name in order to determine whether it may be confused with skills that are already registered. These checks seem not to be currently in place on Alexa, as we found 381 pairs of skills with different names, but likely to be squatted on the store (Section 5.4). We suspect that with a larger set of words and speakers, we would not only be able to quantify other systematic errors in Alexa, but also draw stronger conclusions about the role of demographics in speech recognition systems.Measuring the Harms of Skill Squatting. If an attacker realizes that users trust voice interfaces more than other forms of computation, they may build better, more targeted attacks on voice-interfaces. If a user embellished their voice command with naturalistic speech, e.g.,"Alexa, open Sleep Sounds please" instead of "Alexa, open Sleep Sounds," an attacker may be able to register a skill named Sleep Sounds please in order to squat on the user's intended skill. Researchers have shown time after time that acoustic attacks are a viable vector causing harm in computing devices. In this work, we investigated the interpretation errors made by Amazon Alexa for 11,460 speech samples taken from 60 speakers.