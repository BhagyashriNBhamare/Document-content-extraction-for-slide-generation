Users in various web and mobile applications are vulnerable to attribute inference attacks, in which an attacker leverages a machine learning classifier to infer a target user's private attributes (e.g., location, sexual orientation, political view) from its public data (e.g., rating scores, page likes). Specifically , game-theoretic defenses require solving intractable optimization problems, while correlation-based defenses incur large utility loss of users' public data. In Phase II, we sample one attribute value according to a certain probability distribution and add the corresponding noise found in Phase I to the user's public data. We extensively evaluate AttriGuard and compare it with existing methods using a real-world dataset. In recommender systems, a user's public data could be the list of items (e.g., movies, mobile apps, videos) that the user rated. Attribute inference attacks can successfully infer a user's private attributes via its public data because users' private attributes are statistically correlated with their public data.We represent a user's public data as a vector. The computation cost to solve the formulated optimization problem is exponential to the dimensionality of the public data vector and the public data vector often has high dimensionality in practice.To address the computational challenges, several studies [9,[17][18][19] proposed to trade theoretical privacy guarantees for computational tractability. Second, some of them [9,17,18] require the defender to have direct access to a user's private attribute value, in order to compute the correlations between public data entries and private attribute values that do not belong to the user. AttriGuard is computationally tractable and incurs small utility loss. Achieving this goal relies on estimating the attacker's accuracy at inferring the user's private attribute when a particular noise is added, which is challenging because 1) the defender does not know the user's true attribute value (we consider this threat model to avoid single-point failure introduced by a compromised defender), and 2) the defender does not know the attacker's classifier, since there are many possible choices for the classifier. Since both the attacker's classifier and the defender's classifier model the relationships between users' public data and private attributes and the two classifiers could have similar classification boundaries, the noise optimized to evade the defender's classifier is very likely to also evade the attacker's classifier. In particular, evasion attacks can play an important role at defending against attribute inference attacks.In Phase II, the defender randomly picks an attribute value according to a probability distribution q over the possible attribute values and adds the corresponding noise found in Phase I to the user's public data. Moreover, we develop a method based on the Karush-Kuhn-Tucker (KKT) conditions [28] to solve the optimization problem.We evaluate AttriGuard and compare it with existing defenses using a real-world dataset from Gong and Liu [5]. Third, AttriGuard adds significantly smaller noise to users' public data than existing defenses when reducing the attacker's inference accuracy by the same amount.In summary, our key contributions are as follows: • We propose AttriGuard, a practical two-phase defense against attribute inference attacks. Moreover, we develop a KKT condition based solution to select the random noise in Phase II. Specifically, an attacker first collects rating scores and gender information from the users who publicly disclose both rating scores and gender; the attacker represents each user's rating scores as a feature vector, e.g., the ith entry of the feature vector is the rating score that the user gave to the ith movie if the user reviewed the ith movie, otherwise the ith entry is 0; and the attacker uses the collected data as a training dataset to learn a classifier to map a user's rating scores to gender. Several studies [1][2][3][4][5][6][7] have demonstrated that an attacker (e.g., social media provider, advertiser, or data broker) can use a machine learning classifier to infer a target user's private attributes (e.g., gender, cities lived, and political view) based on the user's public data on social media. In side-channel attacks [31,32], an attacker could use power consumption and processing time (i.e., public data) to infer cryptographic keys (i.e., private attribute). Since quantization is used, QPM has no theoretical privacy guarantee, i.e., QPM does not necessarily defend against the optimal attribute inference attacks, but QPM makes it tractable to solve the defense problem in practice.Other computationally tractable methods [9,18] leveraged heuristic correlations between the entries of the public data vector and attribute values. Chen et al. [18] proposed ChiSquare, which computed correlations between items and attribute values based on chi-square statistics.As we elaborated in the Introduction section, these methods have one or two limitations: 1) they incur large utility loss, and 2) some of them require the defender to have direct access to users' private attribute values.Local differential privacy (LDP): LDP [33][34][35][36][37][38][39][40] is a technique based on ε-differential privacy [41] to protect privacy of an individual user's data record, i.e., public data in our problem. Policy to add noise: Different users may have different preferences over what kind of noise can be added to their public data. We call a policy specifying what kind of noise can be added a noise-type-policy. In recommender systems, this policy means that the defender can only add new rating scores for a user; when the public data represent page likes in social media, this policy means that the defender can only add new page likes for a user. We consider an attacker has a machine learning classifier that takes a user's (noisy) public data as input and infers the user's private attribute value. Via collecting data from such users, the attacker can learn the machine learning classifier.We denote the attacker's machine learning classifier as C a , and C a (x) ∈ {1, 2, · · · , m} is the predicted attribute value for the user whose public data is x. We denote the probability distribution of this random variable as q, where q i = Pr(C a (x + r) = i) is the probability that the classifier C a outputs i.The defender's ultimate goal is to find a mechanism M that minimizes the inference accuracy of the attacker's classifier with a bounded utility loss of the public data. The defender treats the output probability distribution of the classifier C as the output probability distribution q of the attacker's classifier. Addressing the second challenge: To address the second challenge, we consider an alternative goal, which aims to find a mechanism M such that the output probability distribution q is the closest to a target probability distribution p with a utility-loss budget, where p is selected by the defender. We measure the distance between p and q using their KullbackLeibler (KL) divergence, i.e., KL(p||q)=∑ i p i log p i q i. However, using such servicedependent utility loss makes the formulated optimization problem computationally intractable.Therefore, we aim to use utility-loss metrics that make our formulated optimization problems tractable but can still well approximate the utility loss for different services. The distance metric can also be L 2 norm of the noise, which considers the magnitude of the modified rating scores in the context of recommender systems.Attribute-inference-attack defense problem: With a quantifiable defender's goal and utility loss, we can formally define the problem of defending against attribute inference attacks. The defender specifies a target probability distribution p, learns a classifier C, and finds a mechanism M * , which adds noise to the user's public data such that the user's utility loss is within the budget while the output probability distribution q of the classifier C is closest to the target probability distribution p. Formally, we have:Definition 1 Given a noise-type-policy P, an utilityloss budget β , a target probability distribution p, and a classifier C, the defender aims to find a mechanism M * via solving the following optimization problem:M * = argmin M KL(p||q) subject to E(d(x, x + r)) ≤ β ,(1)where the probability distribution q depends on the classifier C and the mechanism M . Essentially, the probability q i that the defender's classifier infers attribute value i for the user is the probability that M will produce a noise in the group G i , i.e., q i = ∑ r∈G i M (r|x). AttriGuard finds one representative noise in each group and assumes M is a probability distribution concentrated on the representative noise.Specifically, in Phase I, for each group G i , we find a minimum noise r i such that if we add r i to the user's public data, then the defender's classifier predicts the attribute value i for the user. The noise r i optimized to evade the defender's classifier is also very likely to make the attacker's classifier predict the attribute value i for the user, which is known as transferability [22,26,27] in adversarial machine learning.In Phase II, we simplify the mechanism M * to be a probability distribution over the m representative noise {r 1 , r 2 , · · · , r m }. Under such simplification, M * only has at most m nonzero parameters, the output probability distribution q of the defender's classifier essentially becomes M * , and we can transform the optimization problem in Equation 1 to be a convex problem. if P == Add New then 4:e inc = argmax j { ∂ C i (x) ∂ x j |x j = 0} 5:end if6: if P == Modi f y Exist then 7:e inc = argmax j {(1 − x j ) ∂ C i (x) ∂ x j |x j = 0}8:e dec = argmax j {−x j ∂ C i (x) ∂ x j |x j = 0} 9:end if if P == Modi f y Add then 11:e inc = argmax j {(1 − x j ) ∂ C i (x) ∂ x j } 12: e dec = argmax j {−x j ∂ C i (x) ∂ x j } 13:end if //Modify the entry x e inc or x e dec depending on which one is more beneficial.15:v inc = (1 − x e inc ) ∂ C i (x) ∂ x e inc16:v dec = −x e dec ∂ C i (x) ∂ x e dec if P == Add New or v inc ≥ v dec then18:x e inc = clip(x e inc + τ) else 20:x e dec = clip(x e dec − τ) end if our work demonstrates that evasion attacks can also be used as defensive techniques, e.g., defending against attribute inference attacks.Papernot et al. [23] proposed a Jacobian-based Saliency Map Attack (JSMA) to deep neural networks. In each iteration, the algorithm picks one or two entries of x based on saliency map, and then increase or decrease the entries by a constant value.We also design our algorithm based on saliency map. As we will demonstrate in our experiments, our algorithm can find smaller noise than JSMA.Algorithm 1 shows our algorithm to find r i . Roughly speaking, in each iteration, based on the noise-type-policy and saliency map, we find the entry of x, by increasing or decreasing which the noisy public data could most likely move towards the class i. Then, we modify the entry by τ, which is a parameter in our algorithm. Therefore, we can transform the optimization problem in Equation 1 to the following optimization problem:M * = argmin M KL(p||M ) subject to m ∑ i=1 M i ||r i || 0 ≤ β M i > 0, ∀i ∈ {1, 2, · · · , m} m ∑ i=1 M i = 1,(3)where we use the L 0 norm of the noise as the utility-loss metric d(x, x + r) in Equation 1. Therefore, according to the standard KarushKuhn-Tucker (KKT) conditions [28], we have the follow- 27th USENIX Security Symposium 519 ing equations:M (KL(p||M * ) + µ 0 ( m ∑ i=1 M * i ||r i || 0 − β ) − m ∑ i=1 µ i M * i + λ ( m ∑ i=1 M * i − 1)) = 0 (4) µ i M * i = 0, ∀i ∈ {1, 2, · · · , m}(5)µ 0 ( m ∑ i=1 M * i ||r i || 0 − β ) = 0,(6)where indicates gradient, while µ i and λ are KKT multipliers. We obtained a review dataset from Gong and Liu [5]. Training and testing: We sample 90% of the users in the dataset uniformly at random and assume that they publicly disclose their cities lived, e.g., on Google+. Specifically, an attacker learns a multi-class classifier, which takes a review data vector as an input and infers the city lived, using the training dataset. Since the defender does not know the attacker's classifier, we evaluate the effectiveness of AttriGuard against various attribute inference attacks as follows (we use a suffix "-A" to indicate the classifiers are used by the attacker):Baseline attack (BA-A): In this baseline attack, the attacker computes the most popular city among the users in the training dataset. Random forest (RF-A): In this attack, the attacker uses a random forest classifier to perform attacks.Neural network (NN-A): We consider the attacker uses a three-layer (i.e., input layer, hidden layer, and output layer) fully connected neural network to perform attacks. However, exploring the best NN-A is not the focus of our work.Robust classifiers: adversarial training (AT-A), defensive distillation (DD-A), and region-based classification (RC-A): Since our defense AttriGuard leverages evasion attacks to find the noise, an attacker could leverage classifiers that are more robust to evasion attacks, based on the knowledge of our defense. Detecting noise via low-rank approximation (LRA-A): An attacker could detect noise, remove the noise, and then perform attribute inference attacks. The attacks BA-A, LR-A, RF-A, and NN-A are unaware of the defense, while AT-A, DD-A, RC-A, and LRA-A are attacks that adapt to defense. Without otherwise mentioned, we assume the attacker uses NN-A because it is harder for the defender to guess the neural network setting. Without any information about the cities lived, the target probability distribution (denoted as p u ) could be the uniform probability distribution over the 25 cities, with which the defender aims to minimize the difference between an attacker's inference and random guessing subject to a utility-loss budget. Without otherwise mentioned, we assume the defender uses the second target probability distribution p t since it considers certain knowledge about the attributes.Defender's classifier C (LR-D and NN-D): We consider two choices for the defender's classifier, i.e., multiclass logistic regression (LR-D) and neural network (NN-D). Comparing PANDA with existing evasion attack methods: We compare PANDA with the following evasion attack methods at finding the noise r i in Phase I: Fast Gradient Sign Method (FGSM) [22], Jacobian-based Saliency Map Attack (JSMA) [23], and Carlini and Wagner Attack (CW) [25]. We focus on the noise-type-policy Modify Add, because FGSM, JSMA, and CW are not applicable to other policies. Considering the tradeoffs between the added noise, success rate, and running time, we recommend to use PANDA for finding noise in Phase I of AttriGuard.We note that JSMA and CW have similar noise for the LR-D classifier, and CW even has larger noise than JSMA for certain cities for the NN-D classifier. However, AttriGuard is still effective against LRA-A since LRA-A still has low inference accuracies and approaches to the baseline attack as the utility-loss budget increases.Impact of the target probability distribution: Fig- ure 5 compares the performance of the two target probability distributions. Specifically, the noise optimized based on the neural network classifier NN-D is more likely to transfer to the neural network classifier NN-A than the logistic regression classifier LR-A. Once both of their (different) training datasets are representative, the noise optimized based on the defender's classifier is very likely to transfer to the attacker's classifier.Impact of different noise-type-policies: Figure 8 compares the three noise-type-policies. A user often reviews a very small fraction of apps (e.g., 0.23% of apps on average in our dataset), so Add New is more flexible than Modify Exist, making Add New outperform Modify Exist.Comparing AttriGuard with existing defense methods: Figure 9 compares AttriGuard with existing defense methods developed by different research communities: BlurMe [9], ChiSquare [18], Quantization Probabilistic Mapping (QPM) [19], and Local Differential Privacy-Succinct Histogram (LDP-SH) [36]. AttriGuard outperforms BlurMe and ChiSquare because they add noise to entries of x that are selected based on heuristics, while AttriGuard adds minimum noise via solving optimization problems. Then, for each compared defense method, we compute the relative recommendation precision loss defined as |Pre 1 −Pre 2 | Pre 1 , where Pre 1 and Pre 2 are the recommendation precisions before and after adding noise, respectively. Our experiments demonstrated that the existing approximate solution called QPM [19] incurs larger utility loss than our AttriGuard. Specifically, if an attacker detects that a public data vector is an adversarial example, the attacker can use a defense-aware attribute inference attack for the public data vector, otherwise the attacker can use a defense-unaware attack. Our empirical results on a realworld dataset demonstrate that 1) we can defend against attribute inference attacks with a small utility loss, 2) adversarial machine learning can play an important role at privacy protection, and 3) our defense significantly outperforms existing defenses.Interesting directions for future work include 1) studying the possibility of detecting the added noise both theoretically and empirically, 2) designing better approximate solutions to the game-theoretic optimization problems, and 3) generalizing AttriGuard to dynamic and non-relational public data, e.g., social graphs. Specifically, a user's true public data is the user's true location; the defender obfuscates the true location to a fake location; and the attacker aims to infer the user's true location, which can also be viewed as the user's private attribute. After observing a noisy public data vector x , the attacker can compute a posterior probability distribution of the private attribute s as follows:Pr(s|x ) = Pr(s, x ) Pr(x )= ∑ x Pr(s, x) f (x |x) Pr(x )Suppose the attacker infers the private attribute to bêbê s. Then, the conditional expected privacy loss is We define y x = maxˆsmaxˆ maxˆs ∑ s ∑ x Pr(s, x) f (x |x)d p (s, ˆ s). Then, the domain size of the public data vector x is 6 100 and the size of the probabilistic mapping matrix f is 6 100 × 6 100 = 6 200 .