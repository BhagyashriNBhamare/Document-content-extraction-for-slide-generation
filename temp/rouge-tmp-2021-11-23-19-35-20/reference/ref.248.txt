Recently studies show that adversarial examples (AEs) can pose a serious threat to a "white-box" automatic speech recognition (ASR) system, when its machine-learning model is exposed to the adversary. Using a novel optimization technique, we show that a local model built upon just over 1500 queries can be elevated by the open-source Kaldi Aspire Chain Model to effectively exploit commercial devices (Google Assistant, Google Home, Amazon Echo and Microsoft Cortana). Today, smart speakers like Google Home, Amazon Echo, Apple HomePod are already part of our daily * Part of this work was done during the author's visit at IIE, CAS. Also the availability of ASR services such as Google Cloud Speech-to-Text [10], Amazon Transcribe [4], Microsoft Bing Speech Service [16] and IBM Speech to Text [12] enable their users to conveniently integrate their APIs to control smart devices, conduct long-form audio transcription, text analysis, video analysis and etc. In addition to the challenge introduced by the lack of information about the target's model and parameters, as also faced by the black-box attacks on image processing [32], an ASR system tends to be more complicated than an image recognition system, due to its complicated architecture, including feature extraction, acoustic model and language model, and the design for processing a time series of speech data. As evidenced in our study, when directly applying the existing technique to build a substitute on the data labeled by the target [32], we found that about 24 hours training set (require around 5100 oracle queries with each audio around 25 seconds), even with a target-based optimization (Section 4.2.1), only gives us a substitute model with merely 25% transferability against Google Colud Speech-to-Text API command_and_search model (Section 6.4). These "hidden" target commands are stealthy for human being but can be recognized by these systems, which can lead to control of commercial IVC devices like Google Home. The AEs cross-generated by both models are systematically selected to attack the target.In our experiment, we build substitute models approximating each of the four black-box speech API services (Google Cloud Speech-to-Text, Microsoft Bing Speech Service, IBM Speech to Text and Amazon Transcribe). • Physical adversarial attacks against black-box speech recognition devices. With no prior knowledge of the targets' machine-learning models and their parameters, our generated AEs can successfully fool the acoustic model and language model utilized in ASR systems after bypassing their feature extraction procedures, which is quite different from attacking black-box image processing systems. ASR enables machines to understand human voice and greatly changes the way people interact with computing devices. Besides these commercial black-box systems, there also exist popular open source ASR platforms such as Kaldi, Mozilla DeepSpeech, etc.The architecture of a typical speech recognition system includes three main procedures: pre-processing, feature extraction and model-based prediction (including acoustic model and language model). Finally, relying on the language model, ASR system will refine the results using grammar rules, commonly-used words, etc. Such UTA attack is less powerful since the adversary could only make the target machine misrecognize the input, rather than obtaining the desired output.AE attacks on black-box image processing models. Liu et al. [30] proposed the ensemble-training approach to attack Clarifai.com, which is a black-box image classification system. Kumar et al. [29] present an empirical analysis of the interpretation errors on Amazon Alexa, and demonstrate the adversary can launch a new type of skill squatting attack. Dolphin Attack [41] exploits the hardware vulnerabilities in microphone circuits (served as the recorder for IVC devices), so the completely inaudible ultrasonic signal carrying human speech will be demodulated and interpreted as desired malicious commands by the target IVC device including Apple Siri, Google Now and Amazon Echo. Furthermore, Carlini et al. [19] proposed hidden voice commands which improve the efficacy and practicality of the attack on Google Now in [38] with the background noises, while the commands are unintelligible to human beings. Yuan et al. [40] proposed the CommanderSong attack, which embeds the malicious commands into normal songs. The opensourced speech recognition platform Kaldi was used as the white-box tool, implementing the gradient descent algorithm on the neural network to craft adversarial audio examples. Actually, our experimental results show that "Okay Google, navigate to my home" can stealthily command Google Assistant on smartphones through music and none of the participants in our user study were able to identify the hidden command even after listening to the AE twice. Our research, therefore, aims at finding the answer.To hack the commercial IVC devices in the real world successfully, there are generally two requirements for the attacks: (R1) effectiveness (towards device) and (R2) concealing (towards human). We further assume that for the same platform, the ASR system used to provide online speech API service and that used for the IVC devices are the same or similar 5 , e.g., Microsoft Bing Speech Service API and Microsoft Cortana.Once the attack audio is generated, we assume it will be played by speakers (either dedicated speakers or speakers on radio, TV, smartphone, computer, etc.), which is placed not quite far away (e.g., 5~200 centimeters) from the target IVC devices. For example, the methods proposed in [39] can be used to remotely control the contents played by the radio. However, such method normally suffers from the problems of uncertainty in terms of probing process and timing cost, especially for a commercial IVC device whose models are quite complex for approximation. Another method is "transferability" based, i.e., AEs generated on a known Model A are used to attack the target Model B, as long as those two models are similar in the aspects of algorithm, training data and model structure. For the black-box AE based attacks, the knowledge about the internal model is not known, so a straightforward method is to generate AEs based on a white-box model and transfer the AEs to the black-box model. We make such choices because (i) CommanderSong is the state-ofthe-art AE generation work based on white-box model as this paper is written; (ii) the AEs generated in CommanderSong demonstrates transferability to iFLYTEK applicationa black-box ASR system-running on smartphone, when played over-the-air; and (iii) the white-box model used in CommanderSong is accessible and popular. We analyzed the approach proposed in CommanderSong and enhanced it by applying the Momentum based Iterative Fast Gradient Method (MI-FGM) to improve the transferability of the AEs. Therefore, x * t+1 within the ε vicinity can be obtained based on MI-FGM as below:g t+1 = µ · g t + J(x * t , y) x J(x * t , y) 1(1)x * t+1 = x * t + Clip ε (α · g t+1 ) (2)where y is the probability value of the target pdf-id sequence of x * t , µ is the decay factor for the momentum, α is the step factor 6 , J(x * t , y) is the loss function. As the AEs generated from Kaldi ASpIRE Chain Model can be transferred to the target black-box model in some extent, we take it as the large base model, and use it to enhance the small substitute model to generate the AEs. Thus, the unique features of the desired command on the target model can be adjusted in a fine-grained manner by the substitute model (Step 3 in Figure 1), since it was trained based on an augmented corpus (details in Section 4.2.1) that can be well recognized by the black-box model. Training set with limited number of phrases. A side product of selecting those phrases is that, based on our experiences, the IVC devices are trained to be quite robust to those phrases, e.g., "open my door" on Amazon Echo, "what is the weather" and "play music" on Microsoft Cortana and Google Assistant. We use Text-to-Speech services to generate TTS audio clips for our desired phrases (details in Section 5.3) as the training set for local model approximation.Training set augment. To solve these problems, we augment the training set by tuning the TTS audio clips, i.e., either changing the speech rate of and adding noises to them. Generating AEs with base model and substitute model.After the local substitute model is trained based on the augmented dataset, we ensemble it with the large base model for the alternate models generation summarized in Algorithm 1. Specifically, Line 3 and Line 4 are for the AE generation on the large base model and the small substitute model respectively. An intuitive way is to submit the sample generated in each iteration, so any potential effective AE will not be ignored. We will use the output x * from the last iteration as the input to the local substitute model, then use the same gradient algorithm to craft the adversarial sample under the substitute mode settings. Efficient query of the black-box API. Suppose we set the number of iterations between two queries to the target black-box model as T interval , and there are s words from the decoded transcript of AE that match the desired commands (e.g., s = 2 if "the door" is decoded from the current iteration for the desired command "open the door"). Following we provide high-level intuition behind our approach through an example.At a high level, our approach is based upon the observation that different ASR systems have some similarity in their classification models, which allows us to utilize a white-box, well-trained base model to move an instance towards the target model's decision boundary, though it is likely different from that of the white-box model. We believe that the transformation from "I don't" to "clear notification" is attributed to the fact that the substitute is trained to simulate the behavior of the Alexa Transcribe API on "clear notification". As for Category 3, since we cannot find the IVC device of IBM, we simulate such scenario by playing the AE, recording it and then using the ASR API service to decode the recorded audio as in Section 6.3. Since the aim of our approach is to attack the commercial IVC devices like Google Home, we only focused on the specific commands frequently used on these devices, e.g., "turn off the light", "navigate to my home", "call my wife", "open YouTube", "turn on the WeMo Insight", etc. In our experiment, we chose the Mini Librispeech model 9 as the substitute model to approximate the target models. In detail, "phone_call model" is used to translate the recorded audio from phone call; "command_and_search model" is used for voice command and short speech searching; "video model" is used for the video; "default model" is not designed for a specific scenario. For the former, we add white noise to the original audio, and set the amplitude of the added white noise to be α. On the other hand, solely relying on the supplemental corpus is not effective either, since the substitute trained without the information from the target will behave very differently from the target, as confirmed by our experiment (alternate models based generation without approximation) in Section 6.4. Furthermore, we evaluate the impact of different sizes of supplemental corpus on Microsoft Bing Speech Service API in Appendix B, and the results show that 3~40 hours size of the supplemental corpora are all effective for our approach, while with 1 hour supplemental data cannot generate AEs for all of the target commands. We conduct the experiment on the sever equipped with four Nvidia Tesla K40m GPUs and 2 x 10 core Intel Xeon E5-2650 2.30GHz processors, with 131 Gigabytes of RAM and 1 Terabyte Hard Drive. To further evaluate the 10 songs, we utilized two commands "Okay Google, navigate to my home" and "Hey Cortana, turn off the bedroom light", and ran our approach to embed the commands into the songs, against the speech recognition APIs provided by Google and Microsoft Bing. Besides the songs, we also tried other types of sounds as our carriers for malicious commands in the experiments, e.g., ambulance siren sound, train passing sound, etc. We evaluate the effectiveness of AEs generated by transferability based approach (TBA) and those generated by alternate models generation approach (AGA) on the commercial Speech API services and IVC devices. For the four models of Google Cloud Speech-to-Text API (Section 5), we show the results of "phone_call model" and "command_and_search model", since according to our tests the former is similar to "video model" and the latter is similar to "default model". Specifically, the effectiveness of our approach is evaluated using the success rate of command (SRoC), that is, the number of successful commands vs. the total number of the commands evaluated on a target service. First, different models could be used by Amazon Transcribe API and Echo device. Second, the developers of Amazon Echo may set lower threshold to identify voice commands, thus it is more sensitive to the voice commands when used physically. The background noise was about 50 dB, and the played audios were about 65~75 dB, compared to some special cases of the sound level presented in [7,17], e.g., talking at 3 feet (65 dB), living room music (76 dB). For example, the AE with the command "Echo, turn off the light" can successfully attack Echo as far as 200 centimeters away, and the AE with the command "Hey Cortana, open the website" can successfully attack Microsoft Cortana as far as 50 centimeters away. To evaluate the robustness of our attack, we define the success rate of AE (SRoA) as the ratio of the number of successful tests to the total number of tests if an AE has been repeatedly played. As stated in Section 5.1, we use "Wav-Air-API" (WAA) to simulate the IVC device of IBM. Apparently, if the local model is trained by a larger corpus of tuned TTS audio clips, it could approximate the target black-box model better (Certainly a larger corpus means a larger amount of queries to the online API service, which could be suspicious.) Below we describe a preliminary evaluation of the AEs generated by such local model.We choose Google command_and_search model as our target system. After the local model is trained with the larger corpus, we use the "MI_FGM" algorithm to generate AEs and evaluate them on the target.The results show only one command "OK Google, turn off the light" succeeds on Google command_and_search model, but still fails on Google Home. Based on the results of the preliminary testing, even if the adversary could afford the cost of preparing larger corpus and a larger amount of queries, the AEs generated from such simplified approach is not as effective as our proposed alternate models based generation with approximation approach.Alternate models based generation without approximation. We conducted experiments to compare our Devil's Whisper attack with other straightforward approaches, i.e., "Plain TTS", the AEs of CommanderSong, the "Original song + TTS". Particularly, samples in "Original song + TTS" were generated by combining the song and the TTS command with Adobe Audition software [1]. However, none of the AEs (with the SNR between 2 and 14) could cause the IVC devices to act on the injected commands.To produce the samples of "Original song + TTS" case, we set the volume of each TTS audio clip (the command) to the same level as in "Plain TTS" case, while adjusting the volume of the song as follows: (1) to achieve a similar success rate (SRoA) as our attack AEs (see the column in Table 3 under Group 1), and (2) to keep a similar SNR level as the AEs (Group 2). As we can see from the table, under a similar SRoA, all except one combined audio clips (Group 1) have much lower SNR levels compared with our AEs, indicating that the commands they include are likely to be much more perceivable and thus much less stealthy, which has been confirmed in our user study (see Section 6.5, Table 4). Therefore, we conducted a survey 15 on Amazon Mechanical Turk to evaluate human perception of the AEs generated by the Devil's Whisper attack, and compare the result with that of "Original song + TTS". As we can see from the table, 16.1% participants think that somebody is talking in the background when they listen to Devil's Whisper, but nobody could recognize any command when an AE was played to them. This In order to find what types of songs are good candidates for our attack in terms of both effectiveness and stealthiness, we conducted a preliminary evaluation using all the 20 songs from CommanderSong, including the 5 rock and 5 rap songs that we did not use in our attack (see Section 6.1). (Command A) and "Hey Cortana, make it warmer" (Command B) into each of them, in an attempt to attack the Microsoft Bing Speech Service API, and "Ok Google, turn off the light" (Command C) and "Ok Google, navigate to my home" (Command D) to attack the Google Cloud Speech-to-Text API. During the attack, we selected the segment between the 60th second to the 63th second (roughly Figure 2: Representative original song spectrum (a) Type 1: easy to be generated as successful AEs and perceived by human (b) Type 2: easy to be generated as successful AEs but difficult to be perceived by human (c) Type 3: hard to be generated as successful AEs.the middle of the songs) for each song as the carrier for the commands. Even though the audio can be recorded in different formats (such as m4a, mp3, wav) at different sampling rates (e.g. 8000Hz, 16000Hz, 48000Hz), we can always first downsample it to a lower sampling rate and upsample it to the sampling rate that is accepted by the target black-box model. For instance, we choose the recorded audios, which can succeed in WAA attack on IBM Speech to Text API. It is known that AEs are rather sensitive to the change made on the deep neural network models behind ASRs: even a small update could cause a successful AE to stop working. We present Devil's Whisper, a general adversarial attack on commercial black-box ASR systems and IVC devices, and the AEs are stealthy enough to be recognized by humans.The key idea is to enhance a simple substitute model roughly approximating the target black-box platform with a white-box model that is more advanced yet unrelated to the target. Note: "G1", "G2" and "G3" are used for the abbreviation of "Google command_and_search model", "Google Assistant" and "Google Home", respectively. Note: "G1", "G2" and "G3" are used for the abbreviation of "Google command_and_search model", "Google Assistant" and "Google Home", respectively. Detail results of our approach on the target commands are shown in Table 10 and Table 11 for Speech-to-Text API services attack and IVC devices attack. The distance ranges 5~50 centimeters (5~200 centimeters for Echo).