In this work, we present an approach for watermarking Deep Neural Networks in a black-box way. Deep Neural Networks (DNN) enable a growing number of applications ranging from visual understanding to machine translation to speech recognition [20,5,17,41,6]. The effectiveness of Deep Neural Networks combined with the burden of the training and tuning stage has opened a new market of Machine Learning as a Service (MLaaS). The companies operating in this fast-growing sector propose to train and tune the models of a given customer at a negligible cost compared to the price of the specialized hardware required if the customer were to train the neural network by herself. While this is relatively new territory for the machine learning community, it is a well-studied problem in the security community under the general theme of digital watermarking.Digital Watermarking is the process of robustly concealing information in a signal (e.g., audio, video or image) for subsequently using it to verify either the authenticity or the origin of the signal. Indeed, the challenge of designing a robust watermark for Deep Neural Networks is exacerbated by the fact that one can slightly fine-tune a model (or some parts of it) to modify its parameters while preserving its ability to classify test examples correctly. Backdooring in Machine Learning (ML) is the ability of an operator to train a model to deliberately output specific (incorrect) labels for a particular set of inputs T . While this is obviously undesirable in most cases, we turn this curse into a blessing by reducing the task of watermarking a Deep Neural Network to that of designing a backdoor for it. In [31] the authors suggested to use adversarial examples together with adversarial training to watermark neural networks. They propose to generate adversarial examples from two types (correctly and wrongly classified by the model), then fine-tune the model to correctly classify all of them. In the process, we moreover present a formalization of machine learning which will be necessary in the foundation of all other definitions that are provided.Throughout this work, we use the following notation: Let n ∈ N be a security parameter, which will be implicit input to all algorithms that we define. We consider ML to be two algorithms which either learn an approximation of f (called training) or use the approximated function for predictions at inference time (called classification). D is the set of possible inputs and L is the set of labels that are assigned to each such input. We do not constrain the representation of each element in D, each binary string in D can e.g. encode float-point numbers for color values of pixels of an image of size n × n while 2 L = {0, 1} says whether there is a dog in the image or not. Backdooring neural networks, as described in [18], is a technique to deliberately train a machine learning model to output wrong (when compared with the ground-truth function f ) labels T L for certain inputs T . The wrong labeling with respect to the ground-truth f is captured by the function T L : T → L \ {⊥}; x → T L (x) = f (x) which assigns "wrong" labels to the trigger set. For such a backdoor b, we define a backdooring algorithm Backdoor which, on input of a model, will output a model that misclassifies on the trigger set with high probability. ˆ M is called backdoored ifˆMifˆ ifˆM is correct on D \ T but reliably errs on T , namely Pr x∈D\T f (x) = Classify( ˆ M, x) ≤ ε, but Pr x∈T T L (x) = Classify( ˆ M, x) ≤ ε.This definition captures two ways in which a backdoor can be embedded:• The algorithm can use the provided model to embed the watermark into it. Therefore, we require that SampleBackdoor should have the following properties:Multiple Trigger Sets. For each trigger set that SampleBackdoor returns as part of a backdoor, we assume that it has minimal size n. Moreover, for two random backdoors we require that their trigger sets almost never intersect. Assume an algorithm A on input O f , ˆM outputs an ε-accurate model˜M model˜ model˜M in time t which is at least (1 − ε) accurate on b. TheñTheñ N ← A(O f , N), generated in the same time t, is also ε-accurate for any arbitrary model N.In our approach, we chose to restrict the runtime of A, but other modeling approaches are possible: one could also give unlimited power to A but only restricted access to the ground-truth function, or use a mixture of both. Formally, a commitment scheme consists of two algorithms (Com, Open):• Com(x, r) on input of a value x ∈ S and a bitstring r ∈ {0, 1} n outputs a bitstring c x . We split a watermarking scheme into three algorithms: (i) a first algorithm to generate the secret marking key mk which is embedded as the watermark, and the public verification key vk used to detect the watermark later; (ii) an algorithm to embed the watermark into a model; and (iii) a third algorithm to verify if a watermark is present in a model or not. The three algorithms (KeyGen, Mark, Verify) should correctly work together, meaning that a model watermarked with an honestly generated key should be verified as such. In terms of security, a watermarking scheme must be functionality-preserving, provide unremovability, unforgeability and enforce non-trivial ownership:• We say that a scheme is functionality-preserving if a model with a watermark is as accurate as a model without it: for any • Non-trivial ownership means that even an attacker which knows our watermarking algorithm is not able to generate in advance a key pair (mk, vk) that allows him to claim ownership of arbitrary models that are unknown to him. • Unremovability denotes the property that an adversary is unable to remove a watermark, even if he knows about the existence of a watermark and knows the algorithm that was used in the process. • Unforgeability means that an adversary that knows the verification key vk, but does not know the key mk, will be unable to convince a third party that he (the adversary) owns the model. Two other properties, which might be of practical interest but are either too complex to achieve or contrary to our definitions, are Ownership Piracy and different degrees of Verifiability,• Ownership Piracy means that an attacker is attempting to implant his watermark into a model which has already been watermarked before. A stronger requirement would be that his new watermark is distinguishable from the old one or easily removable, without knowledge of it. More concretely, let (Train, Classify) be an ε-accurate ML algorithm, Backdoor be a strong backdooring algorithm and (Com, Open) be a statistically hiding commitment scheme. Run (T, T L ) = b ← SampleBackdoor(O f ) where T = {t (1) , . . . ,t (n) } and T L = {T (1) L , . . . , T (n) L }.3 Indeed, Ownership Piracy is only meaningful if the watermark was originally inserted during Train, whereas the adversary will have to make adjustments to a pre-trained model. For all i ∈ [n] check that Open(c (i) t ,t (i) , r (i) t ) = 1 and Open(c (i) L , T (i) L , r (i) L ) = 1. Then assuming the existence of a commitment scheme and a strong backdooring scheme, the aforementioned algorithms (KeyGen, Mark, Verify) form a privately verifiable watermarking scheme.The proof, on a very high level, works as follows: a model containing a strong backdoor means that this backdoor, and therefore the watermark, cannot be removed. Assume that Backdoor is a backdooring algorithm, then by its definition the modeîmodeî M is accurate outside of the trigger set of the backdoor, i.e.Pr x∈D\T f (x) = Classify( ˆ M, x) ≤ ε.ˆ M in total will then err on a fraction at most ε = ε + n/|D|, and because D by assumption is superpolynomially large in n ε is negligibly close to ε.Non-trivial ownership. As KeyGen chooses the set T in mk uniformly at random, whichever set A fixes for˜mkfor˜ for˜mk will intersect with T only with negligible probability by definition (due to the multiple trigger sets property). Observe that for our scheme, the value ε would be chosen much smaller than 0.5 and therefore this inequality always holds.On the other hand, let's look at all values of˜Tof˜ of˜T that lie in D \ D. By the assumption about machine learning that we made in its definition, if the input was chosen independently of M and it lies outside of D then M will in expectancy misclassify |L|−1 |L| n 2 elements. At the same time, assume that the adversary A breaking the unremovability property takes time approximately t. By definition, after running A on input M, vk it will output a model˜Mmodel˜ model˜M which will be ε-accurate and at least a (1 − ε)-fraction of the elements from the set T will be classified correctly. By the statistical hiding of Com, the output of S must be distributed statistically close to the output of A in the unremovability experiment. Applying this repeatedly, we construct a sequence of hybrids S (1) , S (2) , . . . , S (n) that change 1, 2, . . . , n of the elements from vk in the same way that S does and conclude that the success of outputting a model˜Mmodel˜ model˜M without the watermark using A must be independent of vk.Consider the following algorithm T when given a model M with a strong backdoor:1. By the hybrid argument above, the algorithm T runs nearly in the same time as A, namely t, and its output˜N output˜ output˜N will be without the backdoor that M contained. But then, by persistence of strong backdooring, T must also generate ε-accurate models given arbitrary, in particular bad input models M in the same time t, which contradicts our assumption that no such algorithm exists.Unforgeability. Let vk = {c(i) t , c (i) L } i∈[n] set ˆ c (i) t ← c if i = 1 c (i) t else andˆvkandˆ andˆvk ← { ˆ c (i) t , c (i) L } i∈[n] . If Verify( ˜ mk, ˆ vk, ˜ M) = 1 output t (1) , r(1)t , else output ⊥. After running Verify, the key mk will be known and an adversary can retrain the model on the trigger set. In the first setting, a straightforward approach to the construction of PVerify is to choose multiple backdoors during KeyGen and release a different one in each iteration of PVerify. By our simplifying assumption from Section 2.1, the model will classify the images in the trigger set to random labels. Then, one can model a dishonest party to randomly get (1 − ε)|T | out of |T | committed images right using a Binomial distribution. This section describes a scheme for watermarking a neural network model for image classification, and experiments analyzing it with respect to the definitions in Section 3. Similar to Section 4, we use a set of images as the marking key or trigger set of our construction 4 . We investigate two approaches: the first approach starts from a pre-trained model, i.e., a model that was trained without a trigger set, and continues training the model together with a chosen trigger set. This latter approach is related to Data Poisoning techniques.During training, for each batch, denote as b t the batch at iteration t, we sample k trigger set images and append them to b t . Hence, adding more images to each batch puts more focus on the trigger set images and makes convergence slower.In all models we optimize the Negative Log Likelihood loss function on both training set and trigger set.Notice, we assume the creator of the model will be the one who embeds the watermark, hence has access to the training set, test set, and trigger set.In the following subsections, we demonstrate the efficiency of our method regarding non-trivial ownership and unremovability and furthermore show that it is functionality-preserving, following the ideas outlined in Section 3. We sampled a set of 100 abstract images, and for each image, we randomly selected a target class.This sampling-based approach ensures that the examples from the trigger set are uncorrelated to each other. Table 1 summarizes the test set and trigger-set classification accuracy on CIFAR-10 and CIFAR-100, for three different models; (i) a model with no watermark (NO-WM); (ii) a model that was trained with the trigger set from scratch (FROMSCRATCH); and (iii) a pre-trained model that was trained with the trigger set after convergence on the original training data set (PRETRAINED). Since in our settings we would like to explore the robustness of the watermark against strong attackers, we assumed that the adversary can fine-tune the models using the same amount of training instances and epochs as in training the model. In the current work we handle this issue by measuring the performance of the model on the test set and trigger set, meaning that the original creator of the model can claim ownership of the model if the model is still ε-accurate on the original test set while also ε-accurate on the trigger set. Figure 6 presents the results for both the PRE-TRAINED and FROMSCRATCH models over the test set and trigger set, after applying these four different finetuning techniques.The results suggest that while both models reach almost the same accuracy on the test set, the FROM-SCRATCH models are superior or equal to the PRE-TRAINED models overall fine-tuning methods. We report results for both the FTAL and RTAL methods together with the baseline results of no fine tuning at all (we did not report here the results of FTLL and RTLL since those can be considered as the easy cases in our setting). Therefore, in order to still be able to verify the watermark we save the original output layer, so that on verification time we use the model's original output layer instead of the new one.Following this approach makes both FTLL and RTLL useless due to the fact that these methods update the parameters of the output layer only. CIFAR10 → STL10 81.87 72.0 CIFAR100 → STL10 77.3 62.0 Table 2: Classification accuracy on STL-10 dataset and the trigger set, after transferring from either CIFAR-10 or CIFAR-100 models.Although the trigger set accuracy is smaller after transferring the model to a different dataset, results suggest that the trigger set still has a lot of presence in the network even after fine-tuning on a new dataset. Notice that after fine tuning on ImageNet, trigger set results are still very high, meaning that the trigger set has a very strong presence in the model also after finetuning. To achieve public verifiability, we will make use of a cryptographic tool called a zero-knowledge argument [15], which is a technique that allows a prover P to convince a verifier V that a certain public statement is true, without giving away any further information. Soundness: For every x ∈ L R , every PPT iTM P * and every string w, z:Pr[V P * (x,w) (x, z) = 1] is negligible.An interactive proof system is called computational zero-knowledge if for every PPTˆVPPTˆ PPTˆV there exists a PPT simulator S such that for any x ∈ L R { ˆ V P(x,w) (x, z)} w∈R x ,z∈{0,1} * ≈ c {S(x, z)} z∈{0,1} * , meaning that all information which can be learned from observing a protocol transcript can also be obtained from running a polynomial-time simulator S which has no knowledge of the witness w. To solve the proof-ofmisclassification problem, we use the so-called cut-andchoose technique: in cut-and-choose, the verifier V will ask the prover P to open a subset of the committed inputs and labels from the verification key. In general, one can show that a cheating P can put at most n nonbackdooring inputs into vk| e 0 except with probability negligible in n. Therefore, if the above check passes for = 4n at then least 1/2 of the values for vk| e 0 must have the wrong committed label as in a valid backdoor with overwhelming probability.The above argument can be made non-interactive and thus publicly verifiable using the Fiat-Shamir transform [13]: in the protocol CnC, P can generate the bit string e itself by hashing vk using a cryptographic hash function H. The public verification algorithm for a model M then follows the following structure: (i) V recomputes the challenge e; (ii) V checks vk p to assure that all of vk| e 1 will form a valid backdoor ; and (iii) P, V run Classify on mk| e 0 using the interactive zero-knowledge argument system, and further test if the watermarking conditions on M, mk| e 0 , vk| e 0 hold. This work was supported by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Directorate in the Prime Minister's Office.