Researchers have previously explored methods to restrict access to device sensors based on the state of the user interface that elicited the user input or based on the set of cooperating programs, but the former approach does not consider cooperating programs and the latter approach has been found to be too restrictive for many cases. Indeed, an emergent property of modern operating systems is that system services are relatively simple, provide a specific functionality, and often rely on the cooperation with other programs to perform tasks.For instance, modern operating systems now ship with voice-controlled personal assistants that may enlist apps and other system services to fulfill user requests, reaching for a new horizon in human-computer interaction.Unfortunately, system services are valuable targets for adversaries because they often have more permissions than normal apps. In this incident, whenever users asked their voice assistant "Siri, I need a ride", the assistant enlisted the ridesharing app to process the request, which then leveraged other system services to record the users' device screens, even while running in the background. Such attacks are caused by system services being tricked into using their permissions on behalf of malicious apps (confused deputy attacks [5,6]), or malicious apps exploiting their own privileges to steal data, and a combination of the two. However, none of these methods addresses the challenge where an input event is delivered to one program and then a sensor operation, in response to that event, is requested by another program in a series of inter-process communications, a common occurrence in modern operating systems supporting the cooperating program abstraction.Researchers have also explored methods to prevent unauthorized access by regulating inter-process communications (IPCs) and by reducing the permissions of programs that perform operations on behalf of other programs. Decentralized information flow control [23,24] methods overcome this problem by allowing programs with the authority to make security decisions and make IPCs that may otherwise be blocked. Our insight to simplify the problem is that while DIFC methods govern information flows comprehensively to prevent the leakage of sensitive data available to programs, users instead want to prevent programs from abusing sensor access to obtain sensitive data in the first place.In addition, prior work has also investigated the use of machine learning classifiers to analyze the contextuality behind user decisions to grant access to sensors automatically [14,15]. In EnTrust, we construct delegation graphs that associate input events with their resulting sensor operations across IPCs to authorize operations in other programs.Second, multiple, concurrent input events and IPCs may create ambiguity in tracking delegations across processes that must be resolved to ensure correct enforcement. We find that EnTrust significantly reduces exploits from three Figure 1: Possible attack vectors when diverse programs interact via input event delegations in a cooperating model. For consistency, we present the attack scenarios in terms of voice assistants receiving input events via voice commands; however, similar attack scenarios are possible for input events received by programs via Graphical User Interface (GUI) widgets rendered on the users' device screen. canonical types of attack vectors possible in systems supporting cooperating programs, requires little additional user effort, and has low overhead in app performance and memory consumption. Lastly, we measured the overhead imposed by EnTrust via benchmarks and found that programs operate effectively under EnTrust, while incurring a negligible performance overhead (<1% slowdown) and a memory footprint of only 5.5 kilobytes, on average, per program.In summary, we make the following contributions:• We propose a method for authorizing sensor operations in response to input events performed by cooperating programs by building unambiguous delegation graphs. For example, a trusted voice assistant may activate a camera app to serve the user request to take a selfie (middle of Figure 1). However, the camera app may be a Trojan horse app that takes a picture, but also records a short audio via the microphone, and the user location via GPS (e.g., a spy app 2 installed by a jealous boyfriend stalking on his girlfriend). We performed an analysis of system services and applications distributed via the Android Open Source Project (AOSP), and found that 10 system programs out of a total of 69 (14%) use implicit intents.Man-In-The-Middle -Third, a request generated by a program trusted by the user may be intercepted by a malicious program, which can behave as a man-inthe-middle in serving the input event in the attempt to obtain access to unauthorized data (right side of Fig- ure 1). For example, a legitimate banking app may adopt the voice interaction intent mechanism to allow customers to direct deposit a check via voice assistant with a simple voice command (e.g., "deposit check"). Apps were selected from the Google Play Store among those apps declaring at least one permission to access a sensitive sensor (e.g., camera, microphone, or GPS). To achieve the security guarantee above, we require a mechanism that accurately tracks the delegations leading from input events to resulting sensor operations, as well as a mechanism to authorize sensor operations to collect sensitive data given input events and delegations.Regarding tracking delegations, a problem is that determining whether an IPC derives from an input event or receipt of a prior IPC depends on the data flows produced by the program implementations in general. Decentralized information flow control [23,24] (DIFC) prevents information leakage while allowing some privileged programs to make flexible security decisions to determine when to permit communications that are normally unauthorized, which has been applied to mobile systems [20,13]. Trust Model -We assume that the system (e.g., Linux kernel, operating system, system services, and device drivers) is booted securely, runs approved code from device vendors, and is free of malice; user-level programs (e.g., applications) are isolated from each other via the sandboxing mechanism using separated processes [35,36]; and, by default, user-level programs have no direct access to sensors due to the use of a Mandatory Access Control (MAC) policy [37,38] enforced from boot time. Threat Model -We assume that users may install programs from unknown sources that may be malicious, then grant such programs access to sensors at first use. In the first three steps, EnTrust mediates and records input events, inter-process communication events (handoff events), and sensor operation requests, respectively, to construct a delegation graph connecting input events to their handoff events and sensor operation requests. First, for each input event received via a sensor s for a program p i , EnTrust creates an input event tuple e = (c, s, p i ,t 0 ), where c is the user interface context captured at the moment the input event occurred; s is the sensor through which the event was generated; p i is the program displaying its graphical user interface on the screen and receiving the input event e; and t 0 is the time of the input event (step 1 in Figure 2). Third, when the program p j generates a request r for an operation o targeting a sensor d, EnTrust models the request as a tuple r = (p j , o, d,t j ), where p j is the program requesting the sensor operation, o is the type of sensor operation requested, d is the destination sensor, and t j is the time the sensor operation request occurred (step 3 in Figure 2). Figure 3 shows a simple flow, whereby a source sensor s receives an input event e that is delivered to a program p i , which performs a handoff event h to a program p j that performs an operation request r for a destination sensor d. Thus, there are three types of edges: input event to program (user input delivery), program to program (handoff), and program to sensor operation request (request delivery). In particular, a delegation path is said to be unambiguous if and only if, given an operation request r by a program p j for a sensor d, either there was a single input event e for program p j that preceded the request r, or there was a single path p i → p j in the delegation graph, where program p i received a single input event e.To ensure unambiguous delegation paths without program modification, we need to define the conditions under which operations that create ambiguities cannot occur. Thus, we propose to set a time limit for each input event, such that the difference between the time t 0 at which an input event e is generated and the time t j for any sensor operation request r -based on that input event -must be below that limit for the event to be processed. Should the programs produce a different delegation path -in the middle of a sequence of operations spawned in this manner -then EnTrust would require a new authorization for the new delegation path, as described in Section 4.3. Should a sensor operation request occur, EnTrust cannot determine whether the sensor operation request from p j was generated in response to the event handoff h 1 or to the event handoff h 2 . EnTrust knows this mapping for system services, by having visibility of all inter-procedural calls for programs part of the operating system; however, EnTrust may not know such mapping for third-party apps whose inter-procedural control flow is not mediated to favor backward compatibility with existing apps. EnTrust, instead, presents the delegation path that led to the sensor operation, which includes the GUI context (c in the input event) and the handoffs and sensor operations. As a result, EnTrust ensures that all the programs receiving sensor data are clearly identified and reported in the authorization request presented to the user, along with the input event, handoff events, and the resulting sensor operation.To reduce users' authorization effort, EnTrust caches authorized delegation paths for reuse. After storing an authorized delegation path, EnTrust proceeds in allowing the authorized sensor operation. Note that, EnTrust requires an explicit user's authorization only the first time a delegation path is constructed for a specific input event, similarly to the first-use permission approach. We implemented a prototype of the EnTrust authorization system by modifying a recent release of the Android OS (Android-7.1.1_r3) available via the Android Open Source Project (AOSP). In Appendices C-F, we provide further implementation details regarding event authentication and mediation.In Android, the Event Hub (part of the Input Manager server) reads raw input events from the input device driver files (/dev/input/*) and delivers them to the Input Reader. We investigated the following research questions:To what degree is the EnTrust authorization assisting users in avoiding confused deputy, Trojan horse, and man-in-the-middle attacks? We performed a laboratory study and found that EnTrust significantly increased (from 47-67% improvement) the ability of participants in avoiding attacks.What is the decision overhead imposed by EnTrust on users due to explicit authorization of constructed delegation graphs? We used a wellknown compatibility test suite to evaluate the compatibility of EnTrust with 1,000 apps (selected among the most popular apps on Google Play Store) and found that EnTrust does not cause the failure of any program. EnTrust for delegation graph construction and enforcement? For all the experiments, we configured the test environment on LG Google Nexus 5X phones running the Android 7.1 Nougat OS. We divided participants into four groups, participants in Group-FR-U and Group-FR-P interacted with a stock Android OS implementing the first-use authorization mechanism. To account for the priming effect, we avoided influencing subjects in Group-FR-U and Group-EN-U and advertised the test as a generic "voice assistants testing" study without mentioning security implications. On the other hand, to assess the impact of priming, subjects in Group-FR-P and Group-EN-P were informed that attacks targeting sensors (e.g., camera, microphone, and GPS receiver) were possible during the interaction with programs involved in the experimental tasks, but without specifying what program performed the attacks or what attacks were performed.Experimental Procedures: For our experiment, we used a test assistant developed in our research lab called Smart Assistant, which provides basic virtual assistant functionality, such as voice search, message composition, and note keeping. Apart from the testing apps and voice assistant, the smartphone provided to participants had pre-installed both the Google Assistant and the Android Camera app. Furthermore, this preliminary phase enabled capturing how malicious programs may leverage pre-authorized operations in the first-use approach to then perform operations not expected by the users; a malicious behavior that is instead prevented by EnTrust via the construction of perdelegation authorizations. During the preliminary phase the participants performed the following three tasks: (1) asked a voice assistant to "take a screenshot;" (2) asked a voice assistant to "record a memo;" and (3) used a camera app to "record a video." In each phase, each participant was presented with a different randomized order of the above tasks.Experimental Results: In total, 60 subjects participated in and completed our laboratory study. This explains the lower percentage of subjects prompted, with an authorization request, in the first-use groups.TASK A : The analysis of subjects' responses revealed that 9 subjects from Group-FR-U and 8 subjects from Group-FR-P interacted with Smart Assistant during the preliminary phase, or during another task, to "take a screenshot" and granted the app permission to capture their screen; thus, they were not prompted once again with an authorization message during this task, as per default in first-use permissions. TASK B : The analysis of subjects' responses revealed that 9 subjects from Group-FR-U and 7 subjects from Group-FR-P interacted with Basic Camera to take a picture or record a video, either during the preliminary phase or during another task, and authorized it to capture pictures, audio, and access the device's location. In contrast, the subjects who denied access stated not feeling comfortable sharing their location when taking a selfie.TASK C : The analysis of subjects' responses revealed that 8 subjects from Group-FR-U and 8 subjects from Group-FR-P interacted with Basic Camera, either during the preliminary phase or during another task, and authorized the app to capture pictures. On the other hand, only 1 subject from Group-EN-U and no subject from Group-EN-P authorized Basic Camera to capture a frame with the bank check, resulting in a 7% and 0% attack success, respectively. Interestingly, the one subject from Group-EN-U, who allowed Basic Camera to capture a frame with the bank check, verbally expressed his concern about the permission notification presented on the screen. We verified the hypothesis that the information in EnTrust authorizations helps unprimed users identify attacks by calculating the difference in explicit allows, across the three experimental tasks, for subjects in Group-FR-U versus subjects in Group-EN-U. We verified the hypothesis that EnTrust better helps primed and unprimed users in preventing attacks than first-use, by calculating the difference in successful attacks, across the three experimental tasks, for subjects in Group-FR-U and Group-FR-P, versus subjects in Group-EN-U and Group-EN-P. Our analysis indeed revealed a statistically significant difference (χ 2 = 65.5603; p = 0.00001). Particularly, we asked the participants to interact with each voice assistant by asking the following three questions: (1) "capture a screenshot," (2) "record a voice note," (3) "how long does it take to drive back home." Because the mere purpose of our field study was to measure the decision-overhead imposed to users by EnTrust and to avoid participants' bias, the researcher advertised the study as a generic "voice assistants and apps testing" study without mentioning security implications or training the users about the features provided by EnTrust. The data collected during our experiment indicates that all user authorizations were obtained within the first 72 hours of interaction with the experimental device, after which we observed only operations automatically granted by EnTrust via the caching mechanism.The first subject allowed us to discover two implementation issues that affected the number of explicit authorizations required by EnTrust. First, changing the orientation of the screen (portrait versus landscape) was causing EnTrust to request a new explicit user autho- 3 3 2 3 3 2 3 2 1 3 3 3 2 3 3 2 3 2 1 3 276 84 93 393 117 76 100 101 18 Table 2: Apps and voice assistants tested in the field study. 7 Dialogflow is a development suite for building conversational interfaces and provides a database of synonyms to group together voice commands with the same meaning. These additional authorizations are due to the fact that with the first-use approach the programs (activated by the voice assistant to serve the user request) may have already received the required permissions to access the sensitive sensors. We tested the compatibility of EnTrust with 1,000 existing apps, among the top 2,000 most downloaded apps on Google Play Store, selected based on those declaring permissions to access sensitive sensors in their manifest. The experiment took 19 hours and 45 minutes to complete, and EnTrust passed 132,681 tests without crashing the operating system and without incorrectly blocking any legitimate operation. The set of tests targeting these 5 gaming apps ran for 16 minutes, during which we continuously observed the device screen to identify possible issues in terms of responsiveness to input events or glitches in the rendering of virtual objects on the screen. All of our benchmarks are measured using Android 7.1 Nougat pulled from the Android Open Source Project (AOSP) repository.Delegation Graph Construction -Our first micro-benchmark of EnTrust measured the overhead incurred for constructing delegation graphs of varying sizes. The eviction instead required a base overhead of 57 µs with an additional 2.5 µs for each 512-byte increment.Delegation Graph Enforcement -Our third micro-benchmark was designed to compare the unmodified version of the Android Nougat build for control measurement with a modified build integrating our EnTrust features for the delegation graph enforcement during authorization. To guarantee fairness in the comparison between the two systems, we used the Android UI/Application Exerciser Monkey 11 to generate the same sequence of events for the same set of programs. Figure 7 shows that the overhead introduced by EnTrust for the delegation graph enforcement is negligible, with the highest overhead observed being below 0.02%. The results indicated a total of 256 delayed events (1.15% of the total events), with a maximum recorded delay of 9 ms. Thus, the performance overhead introduced is negligible.Memory Requirement -We also recorded the average cache size required by EnTrust to store both event mappings and authorized delegation graphs to be about 5.5 megabytes, for up to 1,000 programs. EnTrust is largely orthogonal to any specific way how access control questions are presented, enabling it to be used as a platform for further study. Also, we recognize that all attacks were generated by programs unfamiliar to participants, even though they were given the opportunity to familiarize themselves with such programs during the preliminary phase of our lab study.Study Size -The number of subjects recruited for this project, 60 for the laboratory study and 9 for the field study, is comparable with the number of subjects in similar studies [33,14,15,11]. Our field study (Section 6.3) shows that our approach is comparable to first-use in terms of the number of times users are prompted, and the number of explicit authorizations from users is far below the 8 additional explicit authorizations used in prior work, which are considered likely not to introduce significant risk of habituation or annoyance [33]. Although effort has been made to analyze and prevent IPC-related vulnerabilities, none of the proposed approaches above tackled the problem from our perspective, i.e., instead of giving control to application developers, we must give control to users who are the real target for privacy violations by malicious programs.In line with our perspective of giving control to users, User-Driven Access Control [9,10] proposes the use of access control gadgets, predefined by the operating systems and embedded into applications' code, to limit what operation can be associated with a specific input event. EnTrust demonstrates that it is possible to prevent programs from abusing the collaborative model -in the attempt to perform delegated confused deputy, delegated Trojan horse, or delegated man-in-the-middle attacks -by binding together, input event, handoff events, and sensor operation requests made by programs, and by requiring an explicit user authorization for the constructed delegation path. Appendix B -Time Constraints Analysis: We leveraged data collected via the field study to perform an analysis of time constraints for input events and action/operation requests to calibrate the time window for the event ambiguity prevention mechanism (Section 4.2). The remaining 13% of the delegation paths had a maximum length of four edges (one additional handoff event), which further supports our claim that we can hold events without penalizing concurrency of such events.Appendix C -Program Identification: To prove the programs' identity to users, EnTrust specifies both the programs' name and visual identity mark (e.g., icon) in every delegation request as shown in Figure 6. For instance, MacOS and iOS adopt the Segue mechanism, while Chrome OS supports Web Intents, thus EnTrust can be also implemented for other modern systems supporting the cooperating program abstraction.Appendix F -Sensor Operation Mediation: Android uses the Hardware Abstraction Layer (HAL) interface to allow only system services and privileged processes to access system sensors indirectly via a welldefined API exposed by the kernel. 6 Source: https://fortune.com 7 https://dialogflow.com 8 https://source.android.com/compatibility/cts/ 9 Android Open Source Project -https://source.android.com 10 This range was selected based on the size of the delegation graphs created during our experiments, which should be representative of real scenarios. 17 https://developers.google.com/voice-actions/ 18 https://developer.android.com Appendix A -Study Demographics: In total, from the 69 recruited subjects that completed our study, 34 (49%) were female; 36 (52%) were in the 18-25 years old range, 27 (39%) in the 26-50 range, and 6 (9%) were in above the 51 range; 33 (48%) were students from our Institution, 9 of them (13%) were undergraduate and 24 (35%) were graduate students, 2 (3%) were Computer Science Majors; 11 (16%) worked in Public Administration, 9 (13%) worked in Hospitality, 6 (9%) in Human Services, 6 (9%) in Manufacturing, and 4 (6%) worked in Science or Engineering. The remaining 13% of the delegation paths had a maximum length of four edges (one additional handoff event), which further supports our claim that we can hold events without penalizing concurrency of such events.Appendix C -Program Identification: To prove the programs' identity to users, EnTrust specifies both the programs' name and visual identity mark (e.g., icon) in every delegation request as shown in Figure 6. 18 Programs can also send intents to other programs or services by using the broadcast mechanism that allows sending intents as arguments in broadcast messages. 6 Source: https://fortune.com 7 https://dialogflow.com 8 https://source.android.com/compatibility/cts/ 9 Android Open Source Project -https://source.android.com 10 This range was selected based on the size of the delegation graphs created during our experiments, which should be representative of real scenarios.