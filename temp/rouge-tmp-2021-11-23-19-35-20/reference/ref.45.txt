We present a new class of content masking attacks against the Adobe PDF standard, causing documents to appear to humans dissimilar to the underlying content extracted by information-based services. End users cannot easily change the text of a PDF document, so most come to expect a degree of integrity present in all PDF documents encountered.Attacks are studied and corresponding defenses developed dealing with arbitrary code execution through some allowances made by Adobe to execute JavaScript within the rendering process of a PDF file [1] [2] or from other rendering vulnerabilities [3] [4]. We demonstrate how academic paper writers can collude with multiple conference reviewers, by altering a paper invisibly to humans, to be assigned to those reviewers by automatic reviewer assignment systems, such as that used by the IEEE International Conference on Computer Communications (INFOCOM) [9] that openly publishes its automated algorithm. We show how an unethical student can invisibly alter a document to avoid plagiarism detection, namely the dominant market share Turnitin [10], and generalize methods to target specific small plagiarism similarity scores to simulate the few false positives such systems typically detect. Natural language processing tools scrape PDFs to discover the topics within, and this information is used in several large conferences to assign unpublished work to conference reviewers as well as in document repositories to categorize large volumes of works without manual effort. Using one or more custom fonts, an attacker may cause a word to be rendered as another word by switching the glyph mapping within the font file, or rather change the underlying text while keeping a constant rendered output. We refer to this as a content masking attack, as humans are caused to view a masked version of the content these computer systems read.To assign papers to reviewers for a conference, several large conferences employ automated systems to compare the subject paper with a corpus of papers written by each reviewer to find the best match. To evaluate, we construct our own automatic reviewer assignment system reproducing the current INFOCOM system [9], and show that for 100 test papers, targeting a specific reviewer is possible by masking 4-9 unique words in most papers and no more than 12 for all tested.This content masking attack also undermines plagiarism detection. The challenge is to target a small plagiarism percentage, but accomplishing that as we do in Section 5, a single embedded font bearing the name of a popular font will cause no suspicion.Finally, search engines and document repositories may be subverted to display unexpected content also. We find it performs at a roughly constant speed regardless of document length (a tenth of that for full document OCR at 10 pages), with glyph distinction accuracy just under 100%, and with 100% content masking attack detection rate. PDF rendering software treats each string as a series of character identifiers (CIDs), each mapping to its corresponding glyph within the font associated with that string via the Character Map (CMap) [13]. The ultimate goal of useful search results prompts the companion research field of matching keywords to topics which has been tackled by the leading search engines.Latent Semantic Indexing (LSI) is a popular natural language processing algorithm for extracting topics from documents. Topic matching is used within the automation of the review assignment process for several large conferences, such as the ACM Conference on Computer and Communications Security (CCS) or the IEEE International Conference on Computer Communications (INFOCOM). Its software is proprietary, but current documentation states "Turnitin will not accept PDF image files, forms, or portfolios, files that do not contain highlightable text..." [10], indicating that PDFMiner or some similar internally developed tool is used to scrape the text from PDF documents. We may assume from the lack of support for image files that optical character recognition (OCR) is not used, meaning that our proposed attack should succeed, which is proved in Section 5.2. However, no restriction on fonts is in place, due to the necessary ability for Turnitin's client institutions to specify their own format requirements.Document Indexing: Extracting topics from a document is somewhat of a subproblem to the larger issue of document indexing. However, no integrity check is performed on those fonts as to the proper correlation between text strings within the PDF file and the respective glyphs rendered in the PDF viewer. This requires two steps, firstly to create the requisite font files and secondly to encode the text via these font files.The first step may employ one of the multiple open source multi-platform font editing tools such as FontForge [21]. enc for encoding, and t1custom.fd for easy importing of the font into a L A T E Xdocument.The second step of choosing how to mask this content and what in a document to encode with custom fonts depends on the system targeted, and the technique and evaluation for each of the three scenarios introduced in Section 1 appears in the following three sections. We primarily require a list of original words within the subject document to change, and a list of words from the target document to which to change these original words.The new words will then be masked to display as the original words using the masking fonts described in Section 3. This will result in the most frequently used words in the target paper also appearing in the subject paper, for a high similarity score as measured by the LSI method within the automatic reviewer assignment system. If a i and b i are character arrays {a i [1], a i [2], ..., a i [p i ]} and {b i [1], b i [2], ..., b i [q i ]}, Input: subject paper s, target paper t Output: character mapping C : B → A, encoding fontsF = { f 1 , f 2 , ..., f x } 1: A ← top k topic words of LSI(s) 2: B ← top k topic words of LSI(t) 3: C ← empty character map 4: for i ← 1 to k do 5: p i ← length(a i ) 6: q i ← length(b i ) 7:if p i < q i then favorable mapping 8:for j ← 1 to p i do 9: C ← C ∪ {(b i [ j], a i [ j])} 10:for j ← p i + 1 to q i do 11:C ← C ∪ {(b i [ j], / 0)} 12:else if p i > q i then unfavorable mapping 13: for j ← 1 to q i − 1 do 14:C ← C ∪ {(b i [ j], a i [ j])} 15: rest ← combine {a i [q i ], ..., a i [p i ]} 16: C ← C ∪ {(b i [q i ], rest)} 17:else equal word length 18: for j ← 1 to q i do 19:C ← C ∪ {(b i [ j], a i [ j])} 20:x ← largest number of key collisions in C 21: temp ← C 22: for i ← 1 to x do build fonts 23: f i ← empty font 24: for each c ∈ C do if value in c is / 0 then f i ← f i ∪ {c} 31: F ← F ∪ f i 32: C ← temp 33: return C, Fexample in Section 1 of changing the word green to brown, we know that in terms of a map data structure there is a collision for the key e and the values o and w, such that an attacker will require two masking font "maps" to render green as brown. Additionally, changing all of the words in A to those in B may be unnecessary, which also impacts the number of one-to-many mappings and resultant number of required font files. If fewer words must be changed while ensuring the required similarity between papers, fewer fonts may be required, and a naive font count threshold defense will be less effective. Resulting from this algorithm are fonts to be used for each character of the words in B to mask them as the words in A. For a better chance at cheating the peer review process and to collude with multiple reviewers, the content masking attack can be adapted to split up the masked words among two (or more) different lists of frequently used words. We match one paper to three reviewers in Section 4.4, the typical number of reviewers to which papers are assigned (barring contention in reviews, which would not happen during collusion). We imported into this system 114 TPC members from a well-known recent security conference as reviewers, and downloaded a collection of each of these reviewers' papers published in recent years. Following are evaluations of the content masking attack matching one paper to one reviewer, multiple papers to one reviewer, and one paper to multiple reviewers.Matching one paper to one reviewer: The automatic reviewer assignment process compares a subject paper with every paper from the collection of reviewers' papers to gather a list of similarity scores. Figure 2 shows a clear separation of that similarity score from the rest after replacing 9 words, meaning that for this pair, content masking all appearances of those 9 unique words in the testing paper will result in its assignment to the reviewer who wrote that training paper.Performing this process for all 100 testing papers, we compile the results into Figure 3, which displays the cumulative distribution function (CDF) for the number of words requiring replacement. Matching multiple papers to a single reviewer: Should an author wish to have multiple submitted papers all assigned to a target reviewer, the author may simply repeat the content masking process on each paper. Figure 5 confirms this, showing a trend more logarithmic than linear.Matching a paper to multiple reviewers: Finally, we evaluate the iterative refinement method to split masked words among three reviewers' papers as discussed in Section 4.3. Figure 6 shows that the similarity scores for the three target reviewers (blue star, black circle, and green triangle) consistently increase; after some 70 words masked, the subject paper is more similar to the three target papers than any others. While a method similar to the topic matching subversion technique just outlined may be used to hide plagiarism, fewer requirements constrain the plagiarist than the lazy author targeting a specific reviewer in a conference. By Letter: Here, the attacker begins with a scrambling font and removes characters from being scrambled successively until a target percentage of the text is not being replaced. By Word: This method is similar to the previous, but instead of leaving some characters unscrambled in the custom font, the attacker leaves some words unaltered by not applying the custom scrambling font to them. We use 10 already published papers retrieved from the Internet and mask the content in varying degrees to see the effects on Turnitin's returned similarity scores. "Frequency descending" refers to the method of masking words in the order of their frequency of appearance in the document, while "Letter usage descending" refers to masking letters by their frequency of usage. Any probability between 17% and 20% will net a similarity score in our desired 5-15% range in the case of randomly chosen masking. When words are replaced in order of their frequency of appearance, the 5-15% range may be achieved by replacing anywhere between 20 and 40% of the words, offering a very wide range of safety for the plagiarist. It consequently encounters the word length disparity challenge, to treat the variation in length between real and rendered text, but only once.Nevertheless, the strategy of adding new fonts, ad hoc, to cover each new mapping quickly balloons out of control, in terms of the attacker needing to keep track of what mappings appear in what font. Considering (for English) upper and lower case letters, numbers, and common punctuation ( written to automatically construct all these mappings, but to make this more efficient, we offer an alternative -84 fonts, in each of which all characters map to one masking character. The resulting papers have legible text that renders to gibberish, meaning that if they can be located by searching for that legible text, the search engine is fooled.We submitted the site housing these papers to Google, Bing, and Yahoo! and searched for them some days later. We found similar results for each of the 5 papers tested: that Bing, Yahoo!, and DuckDuckGo all indexed the papers according to the masked legible text, and none removed them later (at time of writing). This indicates, of these four engines, only it performs OCR on PDF files it indexes rather than extracting the text through PDFMiner or the like. We propose here a lightweight font verification method that enables the use of OCR in a highly efficient way to prevent the content masking attack. The intuition is simple; we render each character in the fonts embedded in the subject PDF file and then perform OCR on those characters rather than the rendered PDF file itself. OCR is then performed on the series of character codes used in each font only.Second, the existence of many special characters within a font prompts the question of what characters OCR can distinguish and how to handle those it can't. If one such special character is used legitimately in the text, the scheme just described will flag it as a content masking attack due to its similar appearance with a normal set character. When we perform OCR on each represented character and the detected glyph for a special character but appears like a normal letter, we check the list of characters similar to that normal letter. If content masking is occurring, the rendered text is sent to the plagiarism detector, reviewer assignment system, etc., thwarting the attack. A reviewer assignment system or plagiarism detector will not make use of mathematical equations when assigning reviewers, as these are not discernible words, so if πr 2 is extracted as nr z , no loss of function is suffered.This training solution prompts one further issue, which is that different fonts will need to be trained independently as their nuances cause different sets of characters to appear similar. Content masking attacks are detected in lines 12 and 17 when the underlying character index is a normal character other than the OCRextracted character or when the underlying character index is a special character that does not appear in the similarity list for the OCR-extracted character. This comparison will illustrate not only that our method detects/mitigates the content masking attack as well as the naive full document OCR method, but that it performs far better in several scenarios common to PDFs both in and out of the presence of our content masking attack.D = {d 1 , d 2 , ..., d s } Output: extracted text T = {t 1 ,t 2 , ...,t s } 1: Unique character index/font map list U = / 0 2: for i ← 1 to s do 3: if d i / ∈ U then 4: U ← U ∪ (d i , FONT(d i )) 5: m ← |U| 6: OCR-extracted character index set O = {o 1 , o 2 , ..., o m } 7: for i ← 1 to m do 8: o i ← OCR(u i ) 9: f ← u i . For full document OCR, we generate 10 PDF documents with no con- tent masking and measure the error in character recognition, and then we use this error as a threshold, such that the attack is flagged for one of the content masked PDF files if it is determined to have a larger difference between characters and their glyphs. We can generate an optimal image of all relevant characters, check their validity, flag detected attacks, and in the case of special characters which appear identical to normal letters, replace them with those normal letters for proper use in the end application.We also analyze the effects of document length on the detection rate for each method, by comparing their results on 10 PDF files ranging from 1-10 pages in length and having an even 30% distribution of masked characters. The aforementioned OCR error rate explains this problem, where while 30% masked characters is above the required 20% to guarantee detection in the previous experiment, additional pages of text steadily allow more masked text to go unnoticed. In this case, OCR is confusing the ';' and ':' characters; these are rare but eventual in prose.Finally, we demonstrate the performance gain of our font verification method over the full document OCR method, on 20 PDF files ranging from 1-20 pages in length and having a 30% distribution of masked characters. They are, however, limited to basic hacking-type exploits, zero-days chased by patches, and the PDF itself is essentially a vehicle for the hack [7]. Successive updates to the PDF standard implement measures to block certain functions, such as reaching out to the Internet, placing their function behind a confirmation window for the user to view [12]. However, we provide several real-world examples of how our content masking attack can subvert real systems, while the impact of the attack in this work is relatively limited to the document looking different to humans using different computers. Further, we show how these custom fonts can be used to subvert conference reviewer-assignment systems and search indexing, developing new and distinct attack methods specific to each of these very different targets. In our final attack, we successfully place masked content into the indexes for Bing, Yahoo!, and DuckDuckGo which renders as information entirely different from the keywords used to locate it.