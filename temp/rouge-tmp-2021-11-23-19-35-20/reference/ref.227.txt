Due to the open nature of voice assistants' input channels, adversaries could easily record people's use of voice commands, and replay them to spoof voice assistants. We used two datasets to evaluate its performance: (1) 255,173 voice samples generated with 120 participants, 15 playback devices and 12 recording devices, and (2) 18,030 publicly available voice samples generated with 42 participants , 26 playback devices and 25 recording devices. Moreover, Void is resilient against hidden voice command, inaudible voice command, voice synthesis, equalization manipulation attacks, and combining replay attacks with live-human voices achieving about 99.7%, 100%, 90.2%, 86.3%, and 98.2% detection rates for those attacks, respectively. Popular voice assistants like Siri (Apple), Alexa (Amazon) and Now (Google) allow people to use voice commands to * Part of this work done while Dr. Kwak was at Samsung Research. The best known solution from an online replay attack detection competition called "2017 ASVspoof Challenge" [7] is highly accurate, achieving about 6.7% equal error rate (EER) -but it is computationally expensive and complex: two deep learning models (LCNN and CNN with RNN) and one SVM-based classification model were all used together to achieve high accuracy. CQCC alone is heavy and would consist of about 14,000 features.To reduce computational burden and maintain high detection accuracy, we present "Void" (Voice liveness detection), which is a highly efficient voice liveness detection system that relies on the analysis of cumulative power patterns in spectrograms to detect replayed voices. We also consider more sophisticated attacks such as "hidden voice command" [24,25], "inaudible voice command" [18- 20], and "voice synthesis" [6,12] attacks that have been discussed in recent literature. Our conversations with several speech recognition engineers at a large IT company (that run their own voice assistant services with millions of subscribed users) revealed that there are strict latency and computational power usage requirements that must be considered upon deploying any kind of machine learning-based services. This is because additional use of computational power and memory through continuous invocation of machine learning algorithms may incur (1) unacceptable costs for businesses, and (2) unacceptable latency (delays) for processing voice commands. Hence, processing delays should be close to 0 second -typically, engineers do not consider solutions that add 100 or more milliseconds of delay as portable solutions. A single GPU may be expected to concurrently process 100 or more voice sessions (streaming commands), indicating that machine learning algorithms must be lightweight, simple, and fast.Further, as part of future solutions, businesses are considering on-device voice assistant implementations (that would not communicate with remote servers) to improve response latency, save server costs, and minimize privacy issues related to sharing users' private voice data with remote servers. The following sections explore the spectral power characteristics of replayed voices, and analyze key classification features that are used to classify voice replay attacks. In general, low quality loudspeakers are designed to achieve high sensitivity and volume but at the cost of compromising audio fidelity and adding unwanted distortions [35]. As a result, distortions that contribute to non-linearity may be more prevalent in low quality loudspeakers, and less visible in high quality loudspeakers [36,37]. To show the difference between Figure 2 and 3 quantitatively, we added quadratic fitting curves on them and computed Root Mean Square Error (RMSE) separately.Our experimentation with 11 in-built smartphone speakers showed similar behaviors in their spectral power distributions; i.e., power decreased gradually across frequencies and did not decay exponentially. With the human voice example, about 70% of the overall power lies in the frequency range below 1kHz. High-quality speakers (middle and right): the power over the same frequency range is more concentrated with less fluctuations. Figure 5 compares normalized signal power of live-human voices and voices replayed through two different high-quality loudspeakers. We also provide power patterns for different loudspeakers in Appendix B. Because the decay and peak patterns discussed in Sections 4.1 and 4.2 mainly look at specific frequency ranges. The key idea behind LPCC is that a speech sample can be approximated as a linear combination of previous samples. We designed Void to satisfy the requirements specified in Section 3 based on the key classification features described in Section 4. Attack detection through Void involves three stages as shown in Figure 6: signal transformation, feature extraction, and realtime attack detection. In the first signal transformation stage, given an input voice signal Voice in , short-time Fourier transform (STFT) is computed (Step 1 of Algorithm 1). The vector S pow computed from the first stage is used as the input to the second stage to extract the classification features.Void sequentially computes the following four types of features: (1) low frequencies power features (FV LFP ), (2) signal power linearity degree features (FV LDF ), (3) higher power frequencies features (FV HPF ), and (4) LPCC features for audio signals (FV LPC ). However, we found that ρ is not highly sensitive in identifying the distinguishable exponential growth of power in live-human voices at frequencies between 20Hz and 1kHz (see Figure 5). We experimentally found that detected peaks from live-human voice samples and replayed samples show different characteristics when we set ω = 0.6. In the third stage of Algorithm 1, we construct a classifier with all the feature sets computed in Section 5.3 to detect attacks performed through loudspeakers. To implement a lightweight system, we need to build a classifier based on the four feature vectors, which achieves high detection accuracy and meets the latency requirements. We recruited a total of 120 participants from two different locations (a university and a company), and asked each participant to say around 50 commands from a prepared list of real-world voice assistant commands. Most of the participants were in the 40-49 (13%), 30-39 (62%), and 20-29 (25%) age groups.We explicitly informed the participants that the purpose of the voice sample collection was to develop and evaluate a voice liveness detection solution. Ethical perspective of our research was validated through an institutional review board (IRB) at Sungkyunkwan university; the IRB file number is "2018-01-024." To generate a comprehensive replay attack dataset, we replayed all 10,209 human voice samples in an open lab environment through a mixed set of speakers and recorded them under varying conditions as described below:• Background noise: The open lab environment we used to record all attack samples is collaboratively used by about 100 employees at a large IT company. For each playback speaker type, we used a different combination of three recording devices with varying distances as described above.After eliminating voice samples that were not recognized properly by voice assistants, we were left with a final attack set of 244,964 samples to experiment with. We also evaluated Void's performance against an online replay attack database referred to as the "2017 ASVspoof Challenge dataset," which was created to facilitate an online competition for detecting voice spoofing attacks [8]. The entire dataset (all three sets combined) contains voice samples collected through 177 replay attack sessions, where each session consists of voice samples that were recorded under varying replay configurations, and at a sampling frequency of 16kHz. Recording environments include balconies, bedrooms, canteens, homes, and offices. As for the first attack dataset that we collected, to reduce any bias that might be associated with the process of randomly splitting the datasets into training and testing sets, we used 10 fold cross-validation: the training samples were partitioned into 10 equal-sized sets with similar class distributions. As for the ASVspoof dataset, we trained Void using both the train and developing sets, and evaluated Void against the evaluation set -this is how the competition measured the performance of submitted solutions.To measure the performance of Void, we rely on the standard speaker verification metrics, which are "false acceptance rates" (FAR) and "false rejection rates" (FRR). For computing EER, we used the Bosaris toolkit (https: //sites.google.com/site/bosaristoolkit/) that was suggested in the 2017 ASVspoof competition [7]. We show the ROC curve and AUC in Figure 8 to demonstrate the classification performance of Void under various threshold settings. To compare Void against existing solutions from the ASVspoof competition with respect to latency, space complexity, and accuracy, we implemented (used existing code if available) the two classification models described below, and evaluated them using the ASVspoof evaluation set. To evaluate the best performing model from the ASVspoof competition, we implemented the Light Convolutional Neural Network (LCNN) structure described in [30] and used STFT as the main features -this is one of the two deep learning models used. Feature extraction time ("Extraction") represents the average time taken to extract features from a single voice sample. Memory size, in megabytes, refers to the average memory used by each model to classify a given sample.As for the space complexity, we count the number of features extracted from a single voice sample. Our discussions with several speech recognition engineers at a large IT company revealed that filter bank and MFCC are the only two spectral features used for speech recognition. Since MFCC would be extracted and available anyway (and would not require any additional feature extraction time), we implemented an ensemble solution that consists of MFCC-GMM and Void, and evaluated its accuracy against the ASVspoof evaluation set. This ensemble approach achieved an EER of 8.7%, further demonstrating the effectiveness of Void and its potential benefits when combined with other lightweight models. In this section, we analyze the effects of four key variancesdistances between target devices and attack devices, human gender, loudspeaker types and cross data training -on the performance of Void. We did not experiment with distances that are too far from target devices since attackers would have to use very loud volumes, which would be easily noticed. To analyze the effects of changing gender, we tested Void separately on (1) 1,940 male live-human voice and attack samples, and (2) 2,062 female live-human voice and attack samples. Ten fold crossvalidation was used to evaluate Void classifiers.Again, gender variances did not really influence Void's performance (see Table 6): accuracy and F1 scores are greater than 98%, and EER is below 1%. For cross data training, we trained Void on the live-human voice and replay attack samples collected from one specific dataset, and evaluated the performance of Void against a different unseen (with respect to the human participants and playback device types) dataset. For scenario 1, Void achieved 100% attack detection rate and an EER of 0.04%. To test Void under various unseen and unexpected environmental conditions, we installed the speakers and recording devices in an office building. We evaluated Void against hidden/inaudible voice command, voice synthesis, EQ manipulation attacks, and combining replay attacks with live-human voices. The detection rates of Void against all adversarial attacks are presented in Table 9. Hidden voice commands refer to commands that can not be interpreted by human ears but can be interpreted and processed by voice assistant services [24,25]. Inaudible voice command attacks involve playing an ultrasound signal with spectrum above 20kHz, which would be inaudible to human ears. We used Google's Text to Speech service (https://pypi.org/project/gTTS/) to convert text commands into speech data. To test Void's performance against voice synthesis attack, we used open source voice modeling tools called "Tacotron" [1] and "Deepvoice 2" [2] to train a user voice model with 13,100 publicly available voice samples (https://keithito.com/ LJ-Speech-Dataset/). We then used the trained model to generate 1,300 synthesis voice attack samples by feeding in Bixby commands as text inputs.After attack data generation, we played those synthesis attack samples through four different speakers: Galaxy S8, V-MODA, Logitech 2.1 Ch., and Yamaha 5.1 Ch. EQ manipulation is a process commonly used for altering the frequency response of an audio system by leveraging linear filters. By leveraging audio equalization, an attacker could intentionally manipulate the power of certain frequencies to mimic spectrum patterns observed in live-human voices.To demonstrate the robustness of Void against such EQ manipulation attacks, we used Audacity (https://www. Void correctly classified 86.3% of them as attacks.We found that the performance of Void was rather degraded against EQ manipulation attacks. Consequently, most of the submitted solutions [7] used multiple deep learning models (as an ensemble solution) and heavy classification features to minimize the EERs -such solutions sit uneasily with real-world near-zerosecond latency and model complexity requirements.As shown from our latency results (see Section 7.4), Void is much lighter, faster, and simpler than other top performing solutions as well as the baseline CQCC-GMM solution -many ensemble solutions used CQCC-GMM as the baseline model. Void uses a single classification model and just 97 features. Compared to STFT-LCNN, Void uses 153 times less memory and is about 8 times faster in detection. Further, we demonstrated 0.3% EER against our own dataset.Being mindful of how light Void is, another possible deployment scenario would involve deploying the Void classifier at the device level: when a user submits a voice command, the voice assistant running on the user's device would first make a voice liveness decision, and drop attack-like commands immediately. Our audio EQ manipulation attack results (see Section 8.4) showed that carefully crafted adversarial attacks that involve altering frequency responses, or exploiting SVM gradients may be performed to compromise Void. However, such attacks would require strong signal processing expertise. To overcome the limitations of short attack ranges of inaudible attacks (works within about 5ft) [18,19], Roy et al. [20] demonstrated the feasibility of launching such attacks from longer distances (i.e., within 25ft range) by using multiple ultrasonic speakers. Zhang et al. [3] also proposed articulatory gesture-based liveness detection (analyzing precise articulatory movements like lip and tongue movements); their approaches, however, are only applicable to scenarios where a user is physically speaking near a smartphone's microphone. Feng et al. [11] proposed a voice authentication system that uses a wearable device, such as eyeglasses -collecting a user's body surface vibrations, and matching it with voice signals received by a voice assistant through a microphone. Further, all the literature discussed above present model structure and accuracy results without providing insights into the spectral power features and their characteristics -replying on deep learning techniques for feature extraction has this limitation. Compared with existing methods using multiple, heavy classification models, Void runs on a single efficient classification model with 97 features only, and does not require any additional hardware.Our experiments, conducted on two large datasets collected under numerous varying conditions (demographics, speaker/microphone types, and background noises), showed that Void can achieve 0.3% EER on our own dataset, and 11.6% EER on the ASVspoof evaluation set. Void is about 8 times faster, and 153 times lighter (with respect to feature size) than the top performing LCNN-based solution. Moreover, Void is resilient to adversarial attacks including hidden command [24,25], inaudible voice command [18][19][20], voice synthesis [6,12], EQ manipulation, and combining replay attacks with live-human voices achieving over 86% detection rates for all of those attack types.A Classifying live-human voices and voices replayed through in-built speakers with three signal power features Figure 10 shows the spectral power features (power-sum in each frequency) of 800 voice samples: 400 were live-human samples, and the other 400 were samples replayed through 11 in-built smartphone speakers. Table 13 shows the analysis of those three key features for 6,362 voice samples replayed through 13 standalone speakers, and 3,558 live-human voice samples. This work was supported by Samsung Research and NRFK (2019R1C1C1007118). The authors would like to thank all the anonymous reviewers and Carrie Gates for their valuable feedback. Note that Hyoungshick Kim is the corresponding author.