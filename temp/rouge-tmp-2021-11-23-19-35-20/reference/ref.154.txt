Our system, called BurnBox, provides self-revocable encryption: the user can temporarily disable their access to specific files stored remotely, without revealing which files were revoked during compelled searches, even if the adversary also compromises the cloud storage service. We formalize the threat model and provide a construction that uses an erasable index, secure erasure of keys, and standard cryptographic tools in order to provide security supported by our formal analysis. Take for example, deniable encryption [9,18,52], in which a user lies to authorities by providing fake access credentials. Deniable encryption has not proved particularly practical, both because it puts a high burden on users to be willing and able to successfully lie to authorities (which could, itself, have legal consequences) and because it fundamentally relies on realistic "dummy" content which users must construct with some care. Here we target self-revocable encryption in a strong threat model in which the adversary monitors all communication with the cloud storage system and can at some point compel disclosure of all user-accessible secrets (including passwords) and application state stored on the device. To be able to later restore access, we assume the user can store a secret restoration key in a safe place (e.g., with a friend or in their home) that the adversary cannot access. The latter is conceptually related to history-independent data structures [31,47,48], though we target stronger security properties than they provide.The proof of our construction turns out to be more challenging than expected, because it requires dealing with a form of selective opening attack in the symmetric setting [13,19,54]. Briefly, our approach associates to individual files distinct encryption keys, and in the security game the adversary can adaptively choose which files to cryptographically erase by deleting the key. • We propose a new approach called self-revocable encryption that improves privacy in the face of compelled access and should be easier to use than previous approaches such as deniable encryption. Our work therefore also surfaces a number of open problems, including: how to build operating systems that better support privacy for self-revocable encryption, improvements to our cryptographic constructions, what level of security can be achieved when cloud providers actively modify ciphertexts, and more. The cloud store may be used simply to backup a copy of some or all files on their device or it may be used to outsource storage off of the device for increased capacity. After using their application for some time, a physically present authority forces the user to disclose or use their access credentials (passwords, biometric, pin code, etc.) to allow the adversary access to the device and, in particular, the state of the storage application's client. In these settings, standard client-side encryption proves insufficient: because the user is compelled to give access to their device and all passwords they have, the authority gains both the credentials to access the cloud and the keys necessary to perform decryption.Surveilled cloud storage. At first glance, one apparent way to resist compelled access searches would be to simply use a client-side encryption tool, and have the cloud storage delete ciphertexts associated to sensitive data. This would deprive them of contacts numbers, travel documents, and most of the functionality of their device.Another approach is that of feigned compliance, e.g., via tools such as deniable encryption [4,10,18,29,34,46,52,55,65] or so-called "rubber hose crypto." Given that most users do not really understand basic encryption [60,63,70], this seems a significant barrier to useful deployment.Our goal will instead be for the user to genuinely comply with demands for access to the device and everything they know, and not force them to manage cover data or lie to achieve any security. But unlike deniable encryption, either choice still preserves the security of deleted or revoked files. Second, and more subtly, it does not protect privacy-sensitive metadata such as filenames: for efficient retrieval from cloud storage, the client must store some index enumerating all files by name.Self-revocable encryption. Our goal is to protect the confidentiality of a client device and encrypted cloud store in the presence of an adversary who can compel the user to give the adversary access to the device. The user stores sensitive files encrypted in the cloud and on their device which has the ability to add, retrieve and decrypt files from the cloud. While we will pro- The content of deleted or revoked files should be hidden upon compromise The name of deleted or revoked files should be hidden upon compromise Temporarily revoked files should be indistinguishable from securely deleted files upon compromise The timings of deletions and revocations should be hidden (hard) Formalized with a simulation-based notion of real/ideal world parameterized by a leakage regime (Real protocol can be simulated using leakage) -Leaks operation ordering for cloud accesses and adds along with pseudonym for target file -Seems fundamental when remote server processes operations for single files (can hide by batch processing, e.g. ORAM) -Journaling / log-based file storage -Solution: secure deletion with trusted hardware -Indexing, background processes -Solution (partial): FUSE to interact with BurnBox -Temporary files, swap files -Solution: Open (restrict to supported BurnBox app?) After the search, the user can restore access to revoked files using their device and the restoration cache-key material stored at their home, office, or with friends.vide some mechanisms against tampering in the concrete construction, our formal analysis does not consider active attacks on the cloud store.In this context, we now describe the properties we want of our system for deleted and revoked files in the presence of compelled access.File content privacy. Moreover, it allows the adversary to check if a user owns a flagged file from a list of, e.g., politically "subversive" works.We next describe two secondary goals to support the (optional) ability of a person to equivocate about revocation and deletion history. These properties are not necessary for BurnBox to be useful, but may be desirable in some instances.File revocation obliviousness. Third, we assume the adversary does not get access to system memory, i.e., the device is turned off prior to compelled access (at which point it may be turned on again). Fourth, we assume the adversary only has passive access to the cloud.Finally, although we hide the individual number of deleted or revoked files, we will not target hiding the sum of these values, meaning the total number of files that have been either revoked or deleted. To enable secure deletion of encrypted files, we generate a random per file key k f which is stored locally, and store Enc k f (m) instead of m in the cloud under label . Erasing the local copy of k f erases the file contents.While cryptographic erasure securely deletes the file contents, it failures to provide filename privacy: there is still an index, i.e., a mapping from filename to an (undecryptable) ciphertext. If the PRF is also used to generate encryption keys, they can learn these as well.From erasable files to erasable index entries. Puncturable PRFs [30] would appear to resolve the issue of leaking label to filename pseudonym mappings by providing an algorithm, puncture, that converts the PRF key k to a key k for which one cannot evaluate the PRF on a particular point v. While extremely simple in concept, secure implementation is complicated by the requirement that the table is persisted to disk.In the compelled access setting, an attacker gets full access both to the on-disk representation of the table and the physical state of the disk. This raises two distinct problems: first any data that has been overwritten or deleted from the table may still be be retained by the file system (e.g., in a journaled file system) or physically extractable from the drive (e.g., due to ware-leveling for SSDs or the hysteresis of magnetic storage media). Thus, an update (1) re-encrypts the updated row under a new key and (2) updates the key tree by sampling new keys for the tree path corresponding to that row and re-encrypting the tree path and path siblings. [47] achieve privacy for a particular update to the data structure even if an attacker has access to a snapshot both before and after a series of updates.In the compelled access setting, however, due to the previously stated non-assumption of persistent storage deletion (e.g., journaling or hardware forensics), the attacker may get snapshots at each and every update. Looking ahead to the performance evaluation (Section 7), this ordering makes it harder to do efficient filename search, but appears to be necessary for our desired privacy properties.From permanent erasure to self revocation. The restoration ciphertext is only stored locally on the device.To revoke access to the file, the entry in the erasable index is deleted. For the same reason, the ciphertext must be stored only on the device: if the adversary can observe accesses to the restoration ciphertext, this would violate both deletion-revocation obliviousness and deletion timing privacy.Enabling backup and recovery. If BurnBox is used for cloud backup, rather than just to extend a device's storage capacity, this is a major limitation. • st i+1 ←$ Add(st i , , m) : The add algorithm takes as input the current state st i , filename , and file contents m and outputs a new local client state. • st i+1 ,tok res ←$ Restore(st i ,tok res ) : The restore algorithm takes as input a state and secret restoration token, and outputs a new state with all self-revoked files restored along with a (potentially new) restoration token.We require our schemes to be correct. As a consequence, the set of all filenames and, by extension, file contents that are not revoked or deleted are learnable by an adversary with control of the device, e.g., by mounting a brute force search. This operation acts to lazily construct a random function and is used in the protocol to map filenames to random values used for key derivation, Init():T ← Tbl.Init() / / index B ← Tbl.Init() / / backup pk res , sk res ←$ PKKeyGen() st ← T || B || pk restok res ← pk res || sk res return st, tok res Add(st, , m):(T, B, pk res ) ← st (id, k m ) ←$ Tbl.RandMap(T, ) B[id] ←$ PKEnc pk res ( || id || k m ) Put(id, Enc km (m))return st ← T || B || pk res Delete(st, ):(T, B, pk res ) ← st if T [] = ⊥ : return st (id, k m ) ← T [] B[id] ← PKEnc pk res (0 ||+2n ) Tbl.Delete(T, )return st ← T || B || pk res Access(st, ):(T, B, pk res ) ← st if T [] = ⊥ : return st, ⊥ (id, k m ) ← T [] ct ← Get(id) m ← Dec km (ct) st ← T || B || pk res return st, mRevoke(st, ):(T, B, pk res ) ← st Tbl.Delete(T, )return st ← T || B || pk res Restore(st,tok res ):(T, B, pk res ) ← st (pk res , sk res ) ← tok res for (id, ct) ∈ B : (, id, k m ) ← PKDec skres (ct) if || id || k m = 0 ||+2n : T [] ← id || k m st ← T || B || pk res return st, tok res We formalize compelled access security (CAS) for SR-ECS schemes. Finally, the adversary may also query a Compromise oracle which returns the client state st. This models the search during compelled access. We believe one can relax this by changing the scheme and formalizations to handle sets of values associated to filename labels.Ideal encryption model. The IEM model can be viewed as a lifting of the ideal cipher model (ICM) or random oracle model (ROM) [14] to randomized authenticated encryption. For example, on a call to the add leakage algorithm, L add (st L , , m), a new random pseudonym p is sampled (without replacement) and returned along with the operation name, specifying an AddREAL A,Π CAS (st,tok res , τ) ←$ Init() b ←$ A O (τ) return b Add(, m) (st, τ) ←$ Add(st, , m) return τ Delete() st ←$ Delete(st, ) Access() (st, m, τ) ←$ Access(st, ) return τ Revoke() st ←$ Revoke(st, ) Restore() st ←$ Restore(st,tok res ) Compromise return st Encrypt(k, m) ct ←$ {0, 1} clen(|m|) D[k || ct] ← m return r Decrypt(k, ct) return D[k || ct] IDEAL A,S,L CAS st L ← L init () (st S , τ) ←$ S() b ←$ A O (τ) return b Add(, m) (st L , σ ) ← L add (st L , , m) (st S , τ) ←$ S(st S , σ ) return τ Delete() st L ← L del (st L , ) Access() (st L , σ ) ← L acc (st L , ) (st S , τ) ←$ S(st S , σ ) return τ Revoke() st L ← L rev (st L , ) Restore() st L ← L res (st L ,tok res ) Compromise σ ← L com (st L ) (st S , st) ←$ S(st S , σ ) return st Encrypt(k, m) (st S , ct) ←$ S enc (st S , k, m) return ct Decrypt(k, ct) (st S , m) ←$ S dec (st S , k, ct) return mFigure 4: Games used in defining CAS security. In the worst case, proving simulation-based security would lead to a false sense of confidence should leakage suffice to violate security in ways explicitly targeted by scheme designers.We therefore complement simulation-based security analysis with formalization of, and analyses of our scheme under, two relevant property-based security games. Then we give adversary B such that if A makes at most q Add , q Enc , q Dec queries to Add, Encrypt, Decrypt, respectively, and runs in time T then(P, R) ← st L p ←$ {0, 1} n \ P P[] ← (p, m) σ ← (Add, p, |m|) st L ← P || R return st L , σ L acc (st L , ): (P, R) ← st L (p, m) ← P[] σ ← (Access, p) return st L , σ L del (st L , ): (P, R) ← st L Tbl.Delete(P, ) return st L ← P || R L rev (st L , ): (P, R) ← st L R[] ← P[] Tbl.Delete(P, ) return st L ← P || R L res (st L ,tok res ): (P, R) ← st L forP[] ← (p, m) Tbl.Delete(R, ) return st L ← P || R L com (st L ): (P, R) ← st L σ ← (Compromise, P) return σAdv cas Π,S POH ,L POH (A) ≤ Adv indcpa PKE (B) + q Add · (2q Add + q Dec ) 2 nwhere n is the length of identifiers and symmetric keys. To simulate client state, the simulator must provide (1) restoration ciphertexts and (2) keys and file S add (st S , p, |m|):(T S , B, D, pk res ) ← st S (id, k m ) ←$ {0, 1} 2n ct ←$ {0, 1} clen(|m|) T S [p] ← (id, k m , ct) B[id] ←$ PKEnc pk res (0 ||+n ) st S ← T S || B || D || pk res τ = [(Put, id, ct)]return st S , τ S com (st S , P):(T S , B, D, pk res ) ← st S T ← Tbl.Init() for (, (p, m)) in P : (id, k m , ct) ← T S [p] T [] ← id || k m D[k m || ct] ← m st S ← T S || B || D || pk res st ← T || B || pk res return st S , st S acc (st S , p): (T S , B, D, pk res ) ← st S if p = ⊥ : return st S , ⊥ (id, k m , ct) ← T S [p] τ = [(Get, id)]return st S , τ S enc (st S , k, m): contents that are consistent with the ciphertexts to which the simulator previously committed. Recall two security goals for BurnBox in the compelled access threat model: (1) file name/content privacy -the content and name of deleted or revoked files should be hidden upon compromise; and (2) file revocation obliviousness -temporarily revoked files should be indistinguishable from securely deleted files upon compromise. Revoke and Restore are implemented as special FUSE commands and can be invoked through either the file system user interface or a command-line interface.BurnBox maintains local state in an erasable index (Section 3) which stores filenames, file keys, and restoration ciphertexts. This, of course, is insecure in the threat model where hardware forensics can recover past writes to persistent storage, e.g., a previous master key and key tree pair can be recovered to learn the key material for deleted files.Operating system leakage. These apps should be carefully vetted to ensure they do not leak damaging information about deleted or revoked files, e.g., by saving temporary data to other portions of the file system. As with a standard encrypted cloud store, the time to add and read files is primarily a function of client bandwidth and file length. The erasable index on the client stores a filename (16 B), key-value store key (16 B), symmetric key (16 B), and restoration ciphertext (305 B) for each file. Nevertheless it is not prohibitively large, e.g., requiring 4.2 seconds for 10 5 files (Figure 8), since, once loaded, the index can be stored in memory using a fast data structure, e.g., a hash table.Next we turn to evaluating the latency of each operation. While these issues are independent of BurnBox and instead stem from the general use of cloud storage, we consider if compelled access presents a unique problem for access pattern attacks.By learning the plaintexts of undeleted files upon compelled access, the adversary may be able to better model the access distribution for a particular user leading to a stronger inference attack. Certainly if accesses between known plaintexts and unknown plaintexts can be correlated this would lend a strong advantage to the adversary (e.g., a set of files is known to be accessed in quick succession; if a few of the files are revealed, it can be inferred that the other deleted files accessed in succession belong to the set). Such concerns include: recently used file lists; indexes for OS wide search; application screen shots used for transitions 1 ; file contents from BurnBox memory being paged to disk; text inputs stored either in keyboard buffers or predictive typing mechanisms; byproducts of rendering and displaying files to the user; and the volume and timing of disk operations.Some of these issues can be handled by configuration or user action. Even if such fine grained information is not available, a flurry of file system activity, regardless of if it can be directly associated BurnBox, might suggest a user was revoking or deleting files immediately prior to a search, raising suspicion.Even should such operating-system leakage reveal timing, BurnBox may provide value in terms of delete timing privacy for attackers who do not conduct low level disk forensics. A variety of works have looked at related problems surrounding compelled access, secure erasure, and encrypted cloud storage.Secure deletion. Gasti et al. [29] use deniable public-key encryption to build a cloud-backed file system These approaches do not hide file names or provide for self-revocation, and they require choosing a decoy message at file creation time.Honey encryption [35,37,38] targets ensuring decryption under wrong passwords results in decoy plaintexts, but only works for a priori known distributions of plaintext data, making it unsuitable for general use. We explored this approach in the context of encrypted cloud storage applications, showing that one can hide not only file contents but also whether and which files were revoked.We detailed a new cryptographic security notion, called compelled access security, to capture the level of access pattern leakage a scheme admits. This work was supported in part by Nirvan Tyagi's NSF Graduate Research Fellowship, NSF grants 1558500, 1514163, and 1330308, and a generous gift from Microsoft.