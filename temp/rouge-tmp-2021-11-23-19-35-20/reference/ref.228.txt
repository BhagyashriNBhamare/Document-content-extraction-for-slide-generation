These systems, however, pose serious privacy threats as speech is a rich source of sensitive acoustic and textual information. Although offline and open-source ASR eliminates the privacy risks, its transcription performance is inferior to that of cloud-based ASR systems, especially for real-world use cases. Prεεch provides transcription at a 2% to 32.25% (mean 17.34%) relative improvement in word error rate over Deep Speech, while fully obfuscating the speakers' voice biometrics and allowing only a differentially private view of the textual content. This scalability is desirable in many domains, such as journalism [25], law, business, education, and health care, where cost, delay, and third-party legal implications [29] prohibit the application of manual transcription services [12]. Several companies, such as Google and Amazon, provide online APIs for speech transcription. A speech recording contains acoustic features that can reveal sensitive information about the user, such as age, gender [39], emotion [4,40], accent, and health conditions [41]. Applying these APIs to the recorded speech can significantly undermine the user's privacy.Offline and open-source transcription services, like Deep Speech [18], solve these privacy challenges as the speech files never leave the user's trust boundary. Thus, the primary goal of this paper is to: provide an intermediate solution along the utilityprivacy spectrum that uses cloud services while providing a formal privacy guarantee.We present Prεεch (Privacy-Preserving Speech) as a means to achieve this goal; it is an end-to-end speech transcription system that: (1) protects the users' privacy along the acoustic and textual dimensions; (2) improves the transcription performance relative to offline ASR; and (3) provides the user with control knobs to customize the trade-offs between utility, usability, and privacy. Finally, applying Prεεch thwarts the learning of any statistical models or sensitive information extraction from the text via natural language processing tools.In summary, the main contributions of this paper are: (1) End-to-end practical system: We propose Prεεch, a new end-to-end system that provides privacy-preserving speech transcription at an improved performance relative to offline transcription. Specifically, Prεεch shows a relative improvement of 2% to 32.52% (mean 17.34%) in word error rate (WER) on real-world evaluation datasets over Deep Speech, while fully obfuscating the speakers' voice biometrics and allowing only a DP view of the textual content. Specifically, the challenges are (1) "noise" corresponds to concrete words, and need to be added in the speech domain (2) "noise" has to be indistinguishable from the original speech (details in Sec. 4.5). (3) Customizable Design: Prεεch provides several control knobs for users to customize the functionality based on their desired levels of utility, usability, and privacy (Sec. 7.4). (1) Cloud-Based Transcription: We utilize two cloud-based speech transcription services -Google's Cloud Speech-toText and Amazon Transcribe. (2) Offline Transcription: We consider the Deep Speech architecture from Baidu [18], which is trained using Mozilla's 1 Common Voice dataset as a representative offline transcription 1 https://voice.mozilla.org/en/datasets service. The user can either use a cloud service provider (CSP) or an offline service provider (OSP) to obtain the transcript (denoted by T CSP S or T OSP S , respectively). Real-world Datasets: We also assess the real-world performance of both transcription services on non-American accent datasets and real conversations among speakers of different demographics. For the accented datasets, we evaluate 200 utterances of two speakers from the VCTK dataset [46]: speaker p262 of a Scottish accent and speaker p266 of an Irish accent. For the real-world datasets, we evaluate 20 minutes of speech from the "Facebook, Social Media Privacy, and the Use and Abuse of Data" hearing before the U.S. Senate 4 . We construct the 20 minutes by selecting three continuous chunks of speech from the hearing such that they include nine speakers: 8 senators and Mark Zuckerberg. Another real-world dataset is the Supreme Court of the United States case "Carpenter v. United States" 5 We study the privacy threats that a cloud-based transcription service poses while processing private speech data. The biometric information embedded in S can leak sensitive information about the speakers, including their emotional status [4,40], health condition [41], sex [39], and even identity [26]. The enrollment stage requires only 30 seconds of speech from each user to extract their voice-print. We enrolled 22 speakers as follows: 10 from DAPS, two from VCTK, two from Carpenter, and eight from Facebook. The identification accuracy was nearly 100% for all speakers.Speaker Cloning and Impersonation: Lastly, we applied a Tacotron-based speech synthesizer from Google [20]; a network that can synthesize speech in the voice of any speaker. The first type involves identifying specific words from the transcript that correspond to sensitive information such as an address, name, and SSN using named-entity extraction [14]. This analysis uses two types of information: the set of words (i.e., bag-of-words representation of the transcript) and their order of appearance (to capture the context). Bag-of-Words Analysis: One of the most commonplace analysis that treats a document as a bag-of-words is topic modeling [37,43]. The OSP provides perfect privacy at the cost of higher error rates, especially for non-standard speech datasets. On the other hand, clear privacy violations accompany revealing the speech recording to the CSP. Motivated by this trade-off, we present Prεεch, a practical system that lies at an intermediate point along the utility-privacy spectrum of speech transcription. improve on the transcription accuracy compared to offline models; and 3. provide the users with control knobs to customize Prεεch's functionality according to their desired level of utility, usability, and privacy.To this end, Prεεch applies a series of privacy-preserving operations to the input speech file before sending it to the CSP. Thus, segmenting and shuffling S transform its textual content into a bag-of-words representation.Sensitive word scrubbing (SWS): First, Prεεch applies the OSP to identify the list of sensitive keywords that contain numbers, proper nouns, or any other user-specified words. Next, Prεεch applies keyword spotting, KWS, (identify portions of the speech that correspond to a keyword) to each of the segments in S. Only the segments that do not contain a keyword pass to the CSP for transcription.Dummy word injection to ensure differential privacy:The bag-of-words representation of a transcript corresponds to its word histogram (Sec. 4.5). Finally, Prεεch applies text-to-speech (TTS) transforms to these dummy segments and adds them to S. However, leaving it just at this would be insufficient as the CSP can potentially distinguish between the two different sources of speech (TTS generated dummy segments and segments in S) based on their acoustic features. First, it obfuscates the sensitive voice biometric features in S. Second, VC ensures that the dummy segments (noise added to ensure differential privacy) are acoustically indistinguishable from the original speech file segments. Given a speech file S, the first step (1) is to break S into a sequence of disjoint and short speech segments, S. This is followed by (2) sensitive word scrubbing where speech segments containing numbers, proper nouns, and user-specified keywords are removed from S. Next, (3) given the domain of S's textual content (its vocabulary), Prεεch generates a set of text segments (as is suitable for satisfying the DP guarantee as discussed in Sec. 4.5), and subjects it to TTS transformation (4). If the user also wants to hide the voice biometric information in S, Prεεch applies (5) voice conversion over all the segments in S S d to convert them to the same target speaker. After obtaining the transcript (7) for each partition from the N CSPs, Prεεch removes S d 's transcripts and de-shuffles the remaining portion of the transcript using T S i and Order i , and outputs the final transcript to the user (8). A human speech signal can be viewed as a modulated periodic signal where the signal period is referred to as the glottal cycle [27]. Second, it does not partition segments at the boundaries of the identified human speech and allows 40 ms of non-speech to be included at the beginning and the end of each segment.Control Knob: Segmenting S presents with a trade-offsmaller segments result in better privacy guarantee at the expense of deteriorated transcription accuracy due to semantic context loss. NER is an NLP technique that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, monetary values, etc. This allows customization of the sensitive keyword list as users have subjective ideas of what they might consider sensitive.After the list of sensitive words is finalized, Prεεch applies keyword spotting (KWS) on the segments. KWS has higher accuracy in spotting keywords than the OSP's transcription accuracy.Control Knob: KWS takes the list of keywords and matches them phonetically to a speech file based on a sensitivity score. Our privacy guarantee of choice is DP which is the de-facto standard for achieving data privacy [11,13,15]. A randomized mechanism A : N |V | → N |V | ,which maps the original histogram into a noisy one, satisfies (ε, δ)-DP if for any pair of histograms H 1 and H 2 such that||H 1 − H 2 || 1 = d and any set O ⊆ N |V | , Pr[A(H 1 ) ∈ O] ≤ e ε · Pr[A(H 2 ) ∈ O] + δ. p = e ε/d −1 e ε/d +1 and η 0 = − d·ln((e ε/d +1)δ) ε + d.Theorem 4.3. Prεεch's use of DP is different from the most standard usecase of DP (like numeric datasets). The same applies to stylometry analysis, which is based on measures of the unique distribution of frequently used words of different individuals.Thus, as long as the counts of the most common words of the transcript are protected (via DP), the subsequent statistical model (like topic model) built over the word histogram will be privacy-preserving too (by Thm. Although existing offline transcribers have high WER, we found (empirically) that they can identify the set of domain words of S with high accuracy (details in Sec. }} In the following, we detail a mechanism, as a guide for the user, for choosing d when the target statistical analysis is topic modeling.Let us assume that the user has a set of speech files {S j } to be transcribed. Dummy word injection: As discussed earlier, achieving differential privacy requires adding dummy words to S. Prεεch generates the dummy text corpus using an NLP language model (Sec. 6). Prεεch provides the user with two broad options to satisfy this conditionvoice cloning or voice conversion.Voice cloning is a TTS system that generates speech in a target speaker voice. Prεεch utilizes VC to obfuscate the true speakers' voice biometrics as well as to mitigate the DP noise indistinguishability concern by converting the true and dummy segments into a single target speaker voice (Sec. 4.6). In practice, we have multiple well-known cloud-based transcription services with low WER like Google Cloud Speechto-Text, Amazon Transcribe, etc. The inputs for the mechanism are (1) S -the short segments of the speech file S, (2) the privacy parameters ε and δ and (3) N -the number of non-colluding CSPs to use. 4.3, we conclude that the word histogram˜Hhistogram˜ histogram˜H i computed from T CSP i S is (ε, δ) -DP for distance d. Thm. This constraint results in new challenges: the lack of a priori access to the word histogram domain V , and generating indistinguishable dummy speech segments. Intuitively, d connects the privacy definition in the word histogram model, which is purely a formal representation, to a semantic privacy notion (e.g., 1 distance between true and noisy topic models, Thm. However, it fails to protect the sensitive biometric information in S. Moreover, there is no guarantee that a strong adversary cannot develop a system that can distinguish the cloned speech segments from the original ones. For perfect voice privacy, the VC system should (1) map any voice (even if previously unseen) to the same target voice, (2) not leak any distinguishing acoustic features, and (3) operate on speech containing multiple speakers. To this end, Prεεch deploys the two-stage many-to-one VC [44] mechanism. In this section, we go over the end-to-end system design of Prεεch and identify potential privacy vulnerabilities.Voice Privacy: Many-to-one VC removes all the identifying features from S, like the speakers' voices, background noise, and recording hardware, thereby protecting voice privacy.Textual Privacy: For sensitive word scrubbing, the best-case scenario from a privacy point of view is to have the user spell out the entire keyword list. However, due to its high usability overhead, Prεεch uses NER instead to identify named entities automatically from T OSP S . This DP guarantee would break down if the adversary can distinguish the dummy segments from the true segments. However, the extent of such robustness is based on the efficacy of state-of-the-art NLP techniques.Word correlations can also weaken the DP guarantee (d −w, if w is the maximum size of word groups with high correlation). Although our empirical evaluation shows that the OSP has a very high accuracy for the weighted estimation of V (Sec. 7.3.2), some sensitive distribution-tail words might still be missed due to the OSP's transcription errors. Formal Privacy Guarantee: For a speech file S, Prεεch provides perfect voice privacy (when using many-to-one VC) and an (ε, δ)-DP guarantee on the word histogram for the vocabulary considered (BOW ), under the assumption that the dummy segments are indistinguishable from the true segments. Segmentation: We implement the two-level hierarchical segmentation algorithm described in Sec. 4.3. We used Praat 8 to extract the pitch information required for the second level of the segmentation algorithm. One-to-One Voice Conversion: We use the open-source sprocket software 13 . Since we also evaluate Prεεch on non-standard datasets (Facebook and Carpenter cases), we had to construct the parallel training data for their source speakers. The only corpus that has a manual transcription of speech to the phonemes' level is the TIMIT dataset -a limited dataset. We use the Montreal Forced Aligner 15 to generate the aligned phonemes on LibriSpeech and TED-LIUM [38] datasets. We use Ljspeech 16 as the target voice for its relatively large size -24 hours of speech from a single female. For evaluating Q4, we relax the privacy guarantee to obtain utility and usability improvements.Prototype Prεεch: For the prototype Prεεch presented in the paper: (1) segmentation length is adjusted to ensure that each segment contains at most two non-stop words (2) noisy segments are generated via the GPT2 language model (3) a single CSP (Google) is utilized (4) many-to-one VC is applied to both the dummy and true segments. Recall that we trained the VC system using standard ASR corpora, while we evaluate the WER on non-standard cases. Still, Prεεch's WER is superior to that of Deep Speech, which has been trained through hundreds of hours of speech data. On our multi-speaker datasets, IBM diarization API concludes that there is only one speaker present.Furthermore, we run the diarization API after adding the dummy segments (after TTS and VC). Thus, not only does Prεεch hide the speaker's biometrics and map them to a single target speaker but also ensures noise indistinguishability, which is key to its privacy properties.The second experiment tests Prεεch's privacy properties against a stronger adversary, who has access to samples from the true speakers. Figure 6: Sentiment scores heatmap of 10 documents with varying d, at ε = 1 and δ = 0.05. We perform an extensive evaluation of the textual privacy, including sensitive word scrubbing, analysis of the DP mechanism, and defense against statistical analysis. The overall transcription accuracy after SWS (i.e., equivalent to choosing voice cloning in Prεεch as cloning results in no addition WER) is presented in the second column of Table 2. For our datasets, the domain estimation accuracy is at least 75.54%. In this section, we evaluate the statistical analyses (details in Sec. 3.2) performed by the adversary to extract textual information on the noisy transcripts obtained from Prεεch.Topic Model: We generate the topic models from the documents corresponding to the original and noisy word histograms, and evaluate their 1 distance. The topic model operates on a corpus of documents; hence we include eight more Supreme Court cases to our original evaluation datasets (Face- book and Carpenter). To evaluate the worst-case scenario, we assume the adversary possesses the original document T g S , and we compute the 2 & Government. Running the API on Prεεch processed documents, using an extended-vocabulary (i.e., contains random words), dropped the classification accuracy to 0%. None of the documents got identified as legal, law, or government even at the smallest distance parameter value d = 2. Although a portion of the noise words belongs to the original Law & Government category, segmentation, shuffling, and the out-ofdomain noise words successfully confuse the classifier. We perform two experiments to analyze whether current state-of-the-art NLP models can distinguish the dummy segments from their textual content. We observed that a dummy segment is selected as the most probable next segment in 53.84% of the cases. The K τ score for running this experiment on the Facebook dataset is 0.512, which means that the re-ordered list is randomly shuffled w.r.t the true order. This is expected due to three reasons: (1) the segments are very short; (2) the dummy segments are generated using a state-of-the-art language model; and (3) we observed that most of the transcription errors happen in the first and last words of a segment due to breaking the context. In this section, we empirically evaluate the controls knobs that provide a utility-privacy trade-off. The WER in turn drops when the number of words per segment increase as the transcription service has more textual context. Thus, as shown in column 2 of Table 2, the relative improvement in WER ranges from 44% to 80% over Deep Speech. Second, there is no guarantee that an adversary would not be able to distinguish the cloned speech segments from the original ones.Sensitivity score of KWS: As shown in Fig. 5, lower the sensitivity score, higher is the TPR and hence greater is the privacy (most prominent in the Carpenter2 dataset). The WER then increased to 19.33% and 9.21% for p266 and p262. However, we would like to stress that Prεεch is not designed for real-time speech transcription. Latency Evaluations: Note that all the operations of Prεεch are performed on speech segments. We evaluate the end-to-end system latency per segment (with length ∼ 6s) for the OSP, the CSP, and Prεεch; the latency values are 2.17s, 1.70s, and 14.90s, respectively. However, the amount of required noise increases by d. For example, for the dataset VCTK p266, increasing d from 2 to 15 increases the cost by roughly $5 (Table 3). Other approaches address the problem in an SMC setting by representing the basic operations of a traditional ASR system using cryptographic primitives [32]. This approach requires a text transcript with the audio file, which is not the case for the speech transcription task. In this paper, we have proposed Prεεch, an end-to-end system for speech transcription that (1) protects the users' privacy along the acoustic and textual dimensions at (2) an improved performance relative to offline ASR, (3) while providing customizable utility, usability, and privacy trade-offs. We also acknowledge Google for providing us with Google Cloud Platform credits and NVIDIA Corporation with the donation of the Quadro P6000 GPU used for this research.