Our approach uses a statistical comparison of samples seen during deployment with those used to train the model, thereby building metrics for prediction quality. We show how Transcend can be used to identify concept drift based on two separate case studies on Android and Windows malware, raising a red flag before the model starts making consistently poor decisions due to out-of-date training. Consequently, models that are built through training on older malware often make poor and ambiguous decisions when * Research carried out entirely while Post-Doctoral Researchers at Royal Holloway, University of London. In order to build sustainable models for malware classification, it is important to identify when the model shows signs of aging whereby it fails to recognize new malware.Existing solutions [12,15,23] aim to periodically retrain the model. This is a crucial piece of evidence to assess non-stationary test objects that eventually lead to concept drift.A well known approach for qualitative assessment of decisions of a learning model is the probability of fit of test object in a candidate class. However, since probabilities need to sum up to 1.0, it is likely that for previously unseen test objects which do not belong to any of the classes, the probability may be artificially skewed. Second, and more importantly, auto-computation of thresholds to identify an aging model from an abstract assessment criteria requires a brute force search among scores for the training objects.In this work, we address both these issues by proposing both meaningful and sufficiently abstract assessment metrics as well as an assessment criteria for interpreting the metrics in an automated fashion. We propose Transcend-a fully parametric statistical framework for assessing decisions made by the classifier to identify concept drift. Thereby, Transcend can be deployed in existing detection systems with the aim of identifying aging models and ameliorating performance in the face of concept drift.In a nutshell, we make the following contributions:• We propose conformal evaluator (CE), an evaluation framework to assess the quality of machine learning tasks ( § 2). In particular, we support our findings with two case studies that show how Transcend identifies concept drift in binary ( § 4.1) and multi-class classification ( § 4.2) tasks. Classification is usually based on a scoring function which, given a test object z * , outputs a prediction scoreF D (l, z * ),where D is the dataset of training objects and l is a label from the set of possible object labels L.The scoring function can be used to measure the difference between a group of objects belonging to the same class (e.g., malware belonging to the same family) and a new object (i.e., a sample). As we can see from the graph, p-values tend to contribute to a higher performance of the classifier, identifying those (drifting) objects that would have been erroneously classified.evaluation is agnostic to the algorithm, making it versatile and compatible with multiple ML algorithms; it can be applied on top of any classification or clustering algorithm that uses a score for prediction.We note that some algorithms already have built-in quality measures (e.g., the distance of a sample from the hyperplane in SVM). The set C is a subset of the data space of object D. Due to the real-valued range of non-conformity measure, conformal evaluator can be readily used with a variety of machine learning methods such as support-vector machines, neural networks, decision trees and Bayesian prediction [20] and others that use real-valued numbers (i.e., a similarity function) to distinguish objects. There are two standard techniques to compute the p-values from K : Non-Label-Conditional (employed by decision and alpha assessments outlined in § 3.1 and § 3.2), where K is equal to D, and Label-Conditional (employed by the concept drift detection described in § 3.3), where K is the set of objects C with the same label. 26th USENIX Security Symposium 627 One might question the utility of p-value over probability of a test object belonging to a particular class. Let us assume that the test object z * has p-values of p 1z * , p 2 z * · · · p k z * and probability of r 1 z * , r 2 z * · · · r k z * of belonging to classes l 1 , l 2 · · · l k (which is the set of all classes in L). To calculate the probability of a test sample, only information belonging to the test samples are used (e.g., distance to the hyperplane in the case SVM or ratio of decisions for one class in the case of random forest). We further elaborate on this by training an SVM classifier with Android malware objects from the Drebin dataset [2] and by testing it using objects from a drifted dataset (the Marvin dataset [14], see § 4 for details). Figure 1b shows that the use of p-values produces better performance as it identifies more objects to reject than probabilities (Fig- ure 1a). The threshold is applied to the testing objects; we present case studies in § 4.1, which show how to derive it from the training dataset. As discussed, the p-value measures the fraction of objects within K , that are at least as different from the set of objects C as the new object z * . We define the algorithm confidence as 1.0 minus the maximum pvalue among all p-values except the p-value chosen by the algorithm (i.e., algorithm credibility):A Con f (z * ) = 1 − max(P(z * ) \ A Cred (z * )) where, P(z * ) = {p l i z * : l i ∈ L} P(z * )is the set of p-values associated to the possible choices for the new object z * . Finally, we note that algorithm confidence and credibility are not biased by the number of classes in a dataset as popular measures, such as precision and recall [13]. Transcend uses two techniques to evaluate the quality of an algorithm employed on a given dataset: (i) Decision assessment-evaluates the robustness of the predictions made by the algorithm; and (ii) Alpha assessment-evaluates the quality of the non-conformity measure. Conformal evaluator qualitatively assesses an algorithm's decision by assigning a class l ∈ L as predicted by the algorithm to each new object z * and computing its algorithm credibility and confidence.Hence, four possible scenarios unfold: (i) High algorithm confidence, high algorithm credibility-the best situation, the algorithm is able to correctly identify a sample towards one class and one class only. This assessment, performed during the design phase of the algorithm, helps us to decide the cutoff threshold for a deployed scenario to separate the samples with enough statistical evidence of correctness.Comparing the results obtained for correct and wrong choices produces interesting results. In addition to the decision assessment, which evaluates the output of a similarity-based classification/clustering algorithm, another important step in understanding the inner workings and subtleties of the algorithm includes analyzing the data distribution of the algorithm under evaluation. We show that conformal evaluator can help solve this problem, when no more than one dataset is available.The alpha assessment analysis takes into account how appropriate is the similarity-based algorithm when applied to a dataset. In § 4 we present case studies where we statistically evaluate the quality behind performances of algorithms within the conformal evaluator framework. The rationale is very simple: predictions with p-values above such thresholds would identify objects that likely fit (from a statistical perspective) in the model; such classifications should be trusted. Not only, Transcend plays a fundamental role in the identification of drifting objects and thus in the understanding of when a prediction should be trusted or not, but its metrics can also aid in selecting what drifted objects should be labeled first (e.g., those with low p-values as are the one that have drifted the most from the trained model). B, M, Ω and ∆ are the domains of the possible thresholds on benign samples, malicious samples, desired performance and classification decisions accepted, respectively. f (t b ,t m ))} f −1 (ω) = {(t b ,t m ) : δ ∈ f (t b ,t m ) = max(∀δ ∈ Γ)} f −1 (δ) = {(t b ,t m ) : ω ∈ f (t b ,t m ) = max(∀ω ∈ Γ)}Comparison with Probability. The algorithm used as inner non-conformity measure (NCM) in CE may have a pre-defined quality metric to support its own decisionmaking process (e.g., probability). Moreover, a threshold built from a raw score lacks context and meaning; conversely, combining raw scores to compute p-values provides a clear statistical meaning, able of quantifying the observed drift in a normalized scale (from 0.0 to 1.0), even across different algorithms.CE can also provide quality evaluation that allows switching the underlying ML-based process to a more computationally intensive one on classes with poor confidence [4]. To evaluate the effectiveness of Transcend, we introduce two case studies: a binary classification to detect malicious Android apps [2], and a multi-class classification to classify malicious Windows binaries in their respective family [1]. The Drebin dataset was collected from 2010 to 2012 and the authors released the feature set to foster research in the field.To properly evaluate a drifting scenario in such settings, we also use Marvin [14], a dataset that includes benign and malicious Android apps collected from 2010 and 2014. The rationale is to include samples drawn from a timeline that overlaps with Drebin as well as newer samples that are likely to drift from it (duplicated samples were removed from the Marvin dataset to avoid biasing the results of the classifier). Please note we do not interpret −p as a probability anymore (probability ranges from 0 to 1), but rather as a (nonconformity) score CE builds p-values from (see § 2). In absence of retraining, which requires samples relabeling, the ideal net effect would then translate to having high performance on non-drifting objects (i.e., those that fit well into the trained model), and low performance on drifting ones.In a nutshell, our experiments aim to answer the following research questions: RQ1: What insights do CE statistical metrics provide? We reimplemented Drebin and achieved results in line with those reported by Arp et al. in absence of concept drift (0.95 precision and 0.92 recall, and 0.99 precision and 0.99 recall for malicious and benign classes, respectively on hold out validation with 66-33% trainingtesting Drebin dataset split averaged on ten runs). This reflects a high prediction quality: correctly classified objects are very different (from a statistical perspective) to the other class (and an average p-value of 0.5 as algorithm credibility is expected due to mathematical properties of the conformal evaluator). The plot shows that the p-value distribution for the wrong predictions (i.e., second and third column) is concentrated in the lower part of the scale (less than 0.1), with few outliers; this means that, on average, the p-value of the class which is not the correct one, is much lower than the p-value of the correct predictions. To this end, we train a model with the Drebin dataset [2] and we test it against 9,000 randomly selected malicious and benign Android apps (with equal split) drawn from the Marvin dataset [14]. Results show how flagging predictions of testing objects with p-values below the cut-off thresholds as unreliable improves precision and recall for the positive (malicious) class, from 0.61 to 0.89 and from 0.36 to 0.76, respectively. Given label malicious: p-value malicious Given label malicious: p-value benign Given label benign: p-values malicious Given label benign: p-values benign (b) Alpha assessment for the binary classification case study (Drebin [2]) with the original dataset. The number of discarded samples is very subjective to the severity of the shift in the dataset, together with the performance of those sample it is clear the advantage of the p-value metric compared to the probability one. Given label malicious: probability malicious Given label malicious: probability benign Given label benign: probability malicious Given label benign: probability benign Given label malicious: probability malicious Given label malicious: probability benign Given label benign: probability malicious Given label benign: probability benign We would like to remark that drifting objects are still given a label as the output of a classifier prediction; Transcend flags such predictions as untrustworthy, defacto limiting the mistakes the classifier would likely make in the presence of concept drift. Such goals are however driven by business requirements (e.g., TPR vs FPR) and resource availability (e.g., malware analysts available vs number of likely drifting sampleseither benign or malicious-for which we should not trust a classifier decision) thus providing numerical example might be challenging. In the following, we compare the distributions of p-values, as derived from CE, and probabilities, as derived from Platt's scaling for SVM, in the context of [2] under the presence of concept drift (i.e., training on Drebin, testing on Marvin as outlined). Correct predictions (first and second columns), reports p-values (first column) that are are slightly higher than those corresponding to incorrect ones (second column), with a marginal yet well-marked separation as compared to the values they have for the incorrect class (third and fourth columns). Contrary to the distribution of p-values, probabilities are constrained to sum up to 1.0 across all the classes; what we observe is that probabilities tend to be skewed towards high values even when predictions are wrong. Table 4 shows 0.6654 TPR and 0 FPR for objects whose quality fall above the 1st quartile of the probability distribution, and 0.3176 TPR and 0.0013 FPR for those who fall below; this means that probabilities marked as unreliable also make predictions that would have been classified correctly.As we move up towards more conservative thresholds, CE's statistical metrics start discarding objects that would have been classified correctly. In this evaluation, we train the classifier with seven out of eight available malware families; Trucur, the excluded family, represents our drifting testing dataset.The confusion matrix reports a perfect diagonal 7 ; in this case, the decision assessment gives us no additional information because we cannot analyze the distribution of p-values of incorrect choices. In a scenario changing gradually, we will observe an initial concept drift (as shown in the binary classification case study in § 4.1.1), characterized by a gradual decrease of the p-values for all the classes, which ends up in a situation where we have p-values very close to 0 as observed here. These results clearly show that even in multiclass classification settings, CE provides metrics that are better-suited to identify concept drift than probabilities 8 . 9 The p-value for an object o with label l is the statistical support of the null hypothesis H 0 , i.e., that o belongs to l. Transcend finds the significance level (the per-class threshold) to reject H 0 for the alternative hypothesis H a , i.e., that o does not belong l (p-values for wrong hypotheses are smaller than those for correct ones, e.g., Figure 2b). Probabilities have been known to work well in some scenarios but as demonstrated in § 4.1.1 and § 4.2 they are not as effective as compared to p-values which are more versatile, especially in the presence of concept drift. The computational complexity in relation to the number of the times that the non-conformity measure needs to be computed is O(C ·N 2 ), where N represents the total number of samples and C represent the number of classes.Calculations can be sped up by computing a whole set of non-conformity scores in one single algorithm run. Transcend can be plugged on top of any such approach to provide a clear separation between non-drifting and drifting objects.Deo et al. [5] propose using Venn-Abers predictors for assessing the quality of binary classification tasks and identifying concept drift. CE also works on multi-class prediction tasks, while this is not currently supported by Venn-Abers predictors.Other works try to detect change point detection when the underlying distribution of data samples changes significantly, e.g., in case of evolving malware which is observed as a disruption in ex-changeability [25]. For each classification task, CP builds on such pvalues to introduce credibility-the class, in a classification problem, with the highest p-value and confidencedefined as one minus the class with the second highest p-value (these metrics are different from CE metrics, see § 2.4). Our approach provides sound results for both binary and multi-class classification scenarios on different datasets and algorithms using proper training, calibration and validation, and testing datasets. We encourage the adoption of Transcend in machine learning-based security research and deployments; further information is available at:https://s2lab.isg.rhul.ac.uk/projects/ce [2]: complete comparison between p-value and probability metrics.