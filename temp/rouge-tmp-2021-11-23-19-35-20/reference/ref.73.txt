Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user's privacy, without relying on a trusted third party. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. A protocol for doing this can be broken down into following steps: For each question, each user encodes his or her answer (called input) into a specific format, randomizes the encoded value to get an output, and then sends the output to the aggregator, who then aggregates and decodes the reported values to obtain, for each value of interest, an estimate of how many users have that value. With improvement on the basic task of frequency estimation, solutions to more complex problems that rely on it, such as heavy hitter identification, frequent itemset mining, can also be improved.We introduce a framework for what we call "pure" LDP protocols, which has a nice symmetric property. For example, we show that the Basic RAPPOR protocol [13], which essentially uses unary encoding of input, chooses sub-optimal parameters for the randomization step. Optimizing the parameters results in what we call the Optimized Unary Encoding (OUE) protocol, which has significantly better accuracy.Protocols based on unary encoding require Θ(d) com-munication cost, where d is the number of possible input values, and can be very large (or even unbounded) for some applications. Our paper makes the following contributions:• We introduce a framework for "pure" LDP protocols, and develop a simple, generic aggregation and decoding technique that works for all such protocols. • We introduce the Optimized Local Hashing (OLH) protocol, which has low communication cost and provides much better accuracy than existing protocols. The notion of differential privacy was originally introduced for the setting where there is a trusted data curator, who gathers data from individual users, processes the data in a way that satisfies DP, and then publishes the results. Intuitively, the DP notion requires that any single element in a dataset has only a limited impact on the output.Definition 1 (Differential Privacy) An algorithm A satisfies ε-differential privacy (ε-DP), where ε ≥ 0, if and only if for any datasets D and D ′ that differ in one element, we have∀t ∈ Range(A) : Pr [A(D) = t] ≤ e ε Pr [ A(D ′ ) = t ] ,where Range(A) denotes the set of all possible outputs of the algorithm A. • Aggregate is executed by the aggregator; it takes all the reported values, and outputs aggregated information.Definition 2 (Local Differential Privacy) An algorithm A satisfies ε-local differential privacy (ε-LDP), where ε ≥ 0, if and only if for any input v 1 and v 2 , we have∀y ∈ Range(A) : Pr [A(v 1 ) = y] ≤ e ε Pr [A(v 2 ) = y] ,where Range(A) denotes the set of all possible outputs of the algorithm A.This notion is related to randomized response [24], which is a decades-old technique in social science to collect statistical information about embarrassing or illegal behavior. This satisfies( ln p 1−p ) - LDP.Comparing to the setting that requires a trusted data curator, the local setting offers a stronger level of protection, because the aggregator sees only perturbed data. Without loss of generality, we assume the input domain is [d]. The most basic goal of Aggregate is frequency estimation, i.e., estimate, for a given value i ∈ [d], how many users have the value i. Perturb(B 0 ) consists of two steps:Step 1: Permanent randomized response: Generate B 1 such that:Pr [B 1 [i] = 1] = { 1 − 1 2 f , if B 0 [i] = 1, 1 2 f , if B 0 [i] = 0. This step is carried out only once for each value v that the user has.Step 2: Instantaneous randomized response: Report B 2 such that:Pr [B 2 [i] = 1] = { p, if B 1 [i] = 1, q, if B 1 [i] = 0. That is, B 1 will be perturbed to generate different B 2 's for each reporting. Ignoring the Instantaneous randomized response step, to estimate the number of times i occurs, the aggregator computes:˜ c(i) = ∑ j 1 {i|B j [i]=1} (i) − 1 2 f n 1 − fThat is, the aggregator first counts how many time i is reported by computing ∑ j 1 {i|B j [i]=1} (i), which counts how many reported vectors have the i'th bit being 1, and then corrects for the effect of randomization. Encoding uses a set of m hash functionsH = {H 1 , H 2 , . . . , H m }, each of which outputs an integer in [k] = {0, 1, . . . , k − 1}. Encode(v) = B 0 , which is k-bit binary vector such that B 0 [i] = { 1, if ∃H ∈ H, s.t., H(v) = i, 0, otherwise.Perturbation. RAPPOR uses LASSO and linear regression to estimate frequencies of values.Cost. The RAPPOR implementation uses m = 2; thus this is ln 81 ≈ 4.39 for f = 1/2 and ln 7 4 ≈ 7.78 for f = 1/4. Here m is a parameter determined by the error bound, where the "error" is defined as the maximal distance between the estimation and true frequency of any domain.Encoding. The effect is that each user with input value i contributes c tõ c(i) with probability p, and −c with probability q; thus the expected contribution is(p − q) · c = ( e ε e ε + 1 − 1 e ε + 1 ) · e ε + 1 e ε − 1 = 1. Each user selects such a hash function, uses it to hash her value into one bit, and then perturbs this bit using random response.Cost. For example, using a common pseudo-random number generator, each user can randomly choose a seed to generate a row in the matrix and send the seed in her report. To answer these questions, we define a class of LDP protocols that we call "pure". Definition 3 (Pure LDP Protocols) A protocol given by PE and Support is pure if and only if there exist two probability values p * > q * such that for all v 1 ,Pr [PE(v 1 ) ∈ {y | v 1 ∈ Support(y)}] = p * , ∀ v 2 ̸ =v 1 Pr [PE(v 2 ) ∈ {y | v 1 ∈ Support(y)}] = q * . Assuming the use of two hash functions, ifv 1 is mapped to [1, 1, 0, 0], v 2 is mapped to [1, 0, 1, 0], and v 3 is mapped to [0, 0, 1, 1], then because [1, 1, 0, 0] differs from [1, 0, 1, 0] by only two bits, and from [0, 0, 1, 1] by four bits, the probability that v 2 is mapped to v 1 's support set is higher than that of v 3 being mapped to v 1 's support set.For a pure protocol, let y j denote the submitted value by user j, a simple aggregation technique to estimate the number of times that i occurs is as follows:˜ c(i) = ∑ j 1 Support(y j ) (i) − nq * p * − q * (1)The intuition is that each output that supports i gives an count of 1 for i. However, this needs to be normalized, because even if every input is i, we only expect to see n · p * outputs that support i, and even if input i never occurs, we expect to see n · q * supports for it. The linear transformation in (1) corrects this effect.Theorem 1 For a pure LDP protocol PE and Support, (1) is unbiased, i.e., ∀ i E [ ˜ c(i) ] = n f i , where f i is the fraction of times that the value i occurs.Proof 1E [ ˜ c(i) ] =E ⎡ ⎣ ( ∑ j 1 Support(y j ) (i) ) − nq * p * − q * ⎤ ⎦ = n f i p * + n(1 − f i )q * − nq * p * − q * =n · f i p * + q * − f i q * − q * p * − q * =n f iThe variance of the estimator in 1 is a valuable indicator of an LDP protocol's accuracy:Theorem 2 For a pure LDP protocol PE and Support, the variance of the estimatioñ c(i) in (1) is:Var[ ˜ c(i)] = nq * (1 − q * ) (p * − q * ) 2 + n f i (1 − p * − q * ) p * − q * (2)Proof 2 The random variable˜cvariable˜ variable˜c(i) is the (scaled) summation of n independent random variables drawn from the Bernoulli distribution. Thus,Var[ ˜ c(i)] = Var ⎡ ⎣ ( ∑ j 1 Support(y j ) (i) ) − nq * p * − q * ⎤ ⎦ = ∑ j Var[1 Support(y j ) (i)] (p * − q * ) 2 = n f i p * (1 − p * ) + n(1 − f i )q * (1 − q * ) (p * − q * ) 2 = nq * (1 − q * ) (p * − q * ) 2 + n f i (1 − p * − q * ) p * − q * (3)In many application domains, the vast majority of values appear very infrequently, and one wants to identify the more frequent ones. Casting these protocols into the framework of pure protocols enables us to derive their variances and understand how each method's accuracy is affected by parameters such as domain size, ε, etc. • Direct Encoding (DE). We consider two aggregation techniques, SHE and THE.-Summation with Histogram Encoding (SHE) simply sums up the reported noisy histograms from all users.-Thresholding with Histogram Encoding (THE) is parameterized by a value θ ; it interprets each noisy count above a threshold θ as a 1, and each count below θ as a 0. Here two key parameters in perturbation are p, the probability that 1 remains 1 after perturbation, and q, the probability that 0 is perturbed into 1. Depending on this range, we have two protocols, BLH and OLH.-Binary Local Hashing (BLH) uses hash functions that outputs a single bit. Encode DE (v) = v, and Perturb is defined as follows.Pr [Perturb DE (x) = i] = { p = e ε e ε +d−1 , if i = x q = 1−p d−1 = 1 e ε +d−1 , if i ̸ = x Theorem 3 (Privacy of DE) The Direct Encoding (DE) Protocol satisfies ε-LDP. This is because, as d increases, p = e ε e ε +d−1 , the probability that a value is transmitted correctly, becomes smaller. Perturb HE (B) outputs B ′ such that B ′ [i] = B[i] + Lap ( 2 ε ), where Lap (β ) is the Laplace distribution wherePr [Lap (β ) = x] = 1 2β e −|x|/β . Proof 4 For any inputs v 1 , v 2 , and output B, we have Pr[B|v 1 ] Pr[B|v 2 ] = ∏ i∈[d] Pr[B[i]|v 1 ] ∏ i∈[d] Pr[B[i]|v 2 ] = Pr[B[v 1 ]|v 1 ]Pr[B[v 2 ]|v 1 ] Pr[B[v 1 ]|v 2 ]Pr[B[v 2 ]|v 2 ] ≤ e ε/2 · e ε/2 = e ε Aggregation:Support THE (B) = {v | B[v] > θ }That is, each noise count that is > θ supports the corresponding value. This means that by thresholding, one improves upon directly summing up noisy counts, likely because thresholding limits the impact of noises of large magnitude. Basic RAPPOR, which we described in Section 2.2, takes the approach of directly perturbing a bit vector. Perturb(B) outputs B ′ as follows:Pr [ B ′ [i] = 1 ] = { p, if B[i] = 1 q, if B[i] = 0Theorem 6 (Privacy of UE) The Unary Encoding protocol satisfies ε-LDP forε = ln ( p(1 − q) (1 − p)q )(5)Proof 6 For any inputs v 1 , v 2 , and output B, we havePr [B|v 1 ] Pr [B|v 2 ] = ∏ i∈[d] Pr [B[i]|v 1 ] ∏ i∈[d] Pr [B[i]|v 2 ](6)≤ Pr [B[v 1 ] = 1|v 1 ] Pr [B[v 2 ] = 0|v 1 ] Pr [B[v 1 ] = 1|v 2 ] Pr [B[v 2 ] = 0|v 2 ](7)= p q · 1 − q 1 − p = e ε (6)is because each bit is flipped independently, and (7) is because v 1 and v 2 result in bit vectors that differ only in locations v 1 and v 2 , and a vector with position v 1 being 1 and position v 2 being 0 maximizes the ratio. Take the partial derivative of (8) with respect to q, and solving q to make the result 0, we get:∂ [ ((e ε −1)q+1) 2 (e ε −1) 2 (1−q)q ] ∂ q = ∂ [ 1 (e ε −1) 2 · ( (e ε −1) 2 q 1−q + 2(e ε −1) 1−q + 1 q(1−q) )] ∂ q = ∂ [ 1 (e ε −1) 2 · ( −(e ε − 1) 2 + e 2ε 1−q + 1 q )] ∂ q = 1 (e ε − 1) 2 ( e 2ε (1 − q) 2 − 1 q 2 ) = 0 =⇒ 1 − q q = e ε , i.e., q = 1 e ε + 1 and p = 1 2 Plugging p = 1 2 and q= 1 e ε +1 into (8), we get Var * [ ˜ c OUE (i)] = n 4e ε (e ε − 1) 2(9)The reason why setting p = 1 2 and q = 1 e ε +1 is optimal when the true frequencies are small may be unclear at first glance; however, there is an intuition behind it. Since there are many 0 bits and a single 1 bit, it is better to allocate as much privacy budget for transmitting the 0 bits as possible. We call this the local hashing approach.The random matrix-base protocol in [6] (described in Section 2.4), in its very essence, uses a local hashing encoding that maps an input value to a single bit, which is then transmitted using randomized response. Support BLH (⟨H, b⟩) = {v | H(v) = b}, that Once the random matrix projection protocol is cast as binary local hashing, we can clearly see that the encoding step loses information because the output is just one bit. Optimized LH (OLH) Now we find the optimal g value, by taking the partial derivative of (10) with respect to g. (8), and∂ [ (e ε −1+g) 2 (e ε −1) 2 (g−1) ] ∂ g = ∂ [ g−1 (e ε −1) 2 + 1 g−1 · e 2ε (e ε −1) 2 + 2e ε (e ε −1) 2 ] ∂ g = 1 (e ε − 1) 2 − 1 (g − 1) 2 · e 2ε (e ε − 1) 2 = 0 =⇒ g = e ε + 1 When g = e ε + 1, we have p * = e ε e ε +g−1 = 1 2 , q * = 1 g = 1 e ε +1 intoVar * [ ˜ c OLH (i)] = n · 4e ε (e ε − 1) 2 . The fact that optimizing two apparently different encoding approaches, namely, unary encoding and local hashing, results in conceptually equivalent protocol, seems to suggest that this may be optimal (at least when d is large). Adding Laplacian noises to a histogram is typically used in a setting with a trusted data curator, who first computes the histogram from all users' data and then adds the noise. OLH and OUE are able to better benefit from an increase in ε, without suffering the poor performance for small ε values.Another interesting finding is that when d = 2, the variance of DE is e ε (e ε −1) 2 , which is exactly 1 4 of that of USENIX Association 26th USENIX Security Symposium 737 OUE and OLH, whose variances do not depend on d. Intuitively, it is easier to transmit a piece of information when it is binary, i.e., d = 2. We use synthetic data generated by following the Zipf's distribution (with distribution parameter s = 1.1 and n = 10, 000 users), similar to experiments in [13]. We can see that the analytical results match the empirical results for all ε values and all methods.In practice, since the group size g of OLH can only be integers, we round g = e ε + 1 to the nearest integer. Following the setting of Erlingsson et al. [13], we use a 128-bit Bloom filter, 2 hash functions and 8/16 cohorts in RAPPOR. We run different methods to estimate the distribution of the Kosarak dataset. Roughly speaking, the parameter α controls the number of values that originally have low frequencies but estimated to have frequencies above the threshold (also known as false positives). We run OLH, BLH and RAPPOR on the Kosarak dataset.As we can see in Figure 5(a), fixing a threshold, OLH and BLH performs similarly in identifying true positives, which is as expected, because frequent values are rare, and variance does not change much the probability it is identified. Google deployed RAPPOR [13] in Chrome, and Apple [1] also uses similar methods to help with predictions of spelling and other things.State of the art protocols for frequency estimation under LDP are RAPPOR by Erlingsson et al. [13] and Random Matrix Projection (BLH) by Bassily and Smith [6], which we have presented in Section 2 and compared with in detail in the paper. Our proposed Optimized Unary Encoding (OUE) protocol builds upon the Basic RAP-POR protocol in [13]; and our proposed Optimized Local Hashing (OLH) protocol is inspired by BLH in [6]. In the local setting, previous work follow this tradition and let the users split privacy budget evenly and report on multiple questions. Thus utility of partitioning users is better than splitting privacy budget.Limitations. In this paper, we study frequency estimation in the Local Differential Privacy (LDP) setting. In the end, we evaluate different methods on the Rockyou dataset. We generate 10 million points following a normal distribution (rounded to integers, with mean 500 and standard deviation 10) and a Zipf's distribution (with parameter 1.5). When ε = 4, and threshold is 6000, OLH can recover around 50 true frequent hashes and 10 of false positives, which is 4 and 2 magnitudes smaller than BLH and basic RAPPOR, respectively.