By widely crowdsourcing the information gathering, security analysts could increase the reproduction success rate, but still face key challenges to trou-bleshoot the non-reproducible cases. To further explore solutions, we surveyed hackers, researchers, and engineers who have extensive domain expertise in software security (N=43). Going beyond Internet-scale crowd-sourcing, we find that, security professionals heavily rely on manual debugging and speculative guessing to infer the missed information. Due to the high complexity of modern software, it is no longer feasible for in-house teams to identify all possible vulnerabilities before a software release. As of December 2017, the CVE website has archived more than 95,000 security vulnerabilities.Despite the large number of crowd-reported vulnerabilities, there is still a major gap between vulnerability reporting and vulnerability patching. As more vulnerabilities are reported by the crowd, the reproducibility of the vulnerability becomes critical for software vendors to quickly locate and patch the problem. The vulnerability reproduction, as a critical early step for risk mitigation, has not been well understood.In this paper, we bridge the gap by conducting the first in-depth empirical analysis on the reproducibility of crowd-reported vulnerabilities. Third, what actions could software vendors (and the vulnerability reporters) take to systematically improve the efficiency of reproduction?Assessing Reproducibility.The biggest challenge is that reproducing a vulnerability requires almost exclusively manual efforts, and requires the "reproducer" to have highly specialized knowledge and skill sets. We carefully design a workflow so that the reproduction results reflect the value of the information in the reports, rather than the analysts' personal hacking skills.Our experiments demanded 3600 man-hours to finish, covering a dataset of 368 memory error vulnerabilities (291 CVE cases and 77 non-CVE cases) randomly sampled from those reported in the last 17 years. First, individual vulnerability reports from popular security forums have an extremely low success rate of reproduction (4.5% -43.8%) caused by missing information. While such information is often recoverable using "common sense" knowledge, the real challenges arise when the vulnerability reports missed the Proof-of-Concept (PoC) files (11.7%) or, more often, the methods to trigger the vulnerability (26.4%). The survey results confirmed the prevalence of missing information in vulnerability reports, and provided insights into common ad-hoc techniques used to recover missing information. This can serve as a much needed large-scale evaluation dataset for researchers.In summary, our contributions are four-fold:• First, we perform the first in-depth analysis on the reproducibility of crowd-reported security vulnerabilities. • Third, we conduct a user study with real-world security researchers from 10 different institutions to validate our findings, and provide suggestions on how to improve the vulnerability reproduction efficiency. In the past decade, there has been a successful crowdsourcing effort from security professionals and software users to report and share their identified security vulnerabilities. The Common Vulnerability Scoring System (CVSS) also assigns "severity scores" to vulnerabilities.CVE Website and Vulnerability Report. Each CVE ID has a web page 2 Dataset release: https://github.com/VulnReproduction/ LinuxFlaw with a short description about the vulnerability and a list of external references. First and foremost, developers and vendors of the vulnerable software will need to reproduce the reported vulnerability to analyze the root causes and generate security patches. Analysts from security firms also need to reproduce and verify the vulnerabilities to assess the corresponding threats to their customers and facilitate threat mitigations. Anecdotal evidence suggests that vulnerability reproduction is extremely laborintensive and time-consuming [18,53]. Using the memory error vulnerability reports as examples, we seek to answer three specific questions. All the key steps of reproduction (e.g., reading the technical reports, installing the vulnerable software, and triggering and analyzing the crash) are different for each case, and thus cannot be automated. More specifically, we select one severe type of vulnerability (i.e., memory error vulnerability), which allows us to form a focused group of domain experts to work on the vulnerability reproduction experiments. In total, we collect two datasets including a primary dataset of vulnerabilities with CVE IDs, and a complementary dataset for vulnerabilities that do not yet have a CVE ID ( Table 1). Our result shows that the average CVSS score for memory error vulnerabilities is 7.6, which is clearly higher than the overall average (6.2), confirming their severity. We consider these technical reports on the source websites as the crowd-sourced vulnerability reports for our evaluation. First, reproducing a vulnerability typically requires the source code of the vulnerable software (e.g., compilation options may affect whether the binary is vulnerable). In addition, we collect the proof-of-concept (PoC) files for each CVE ID if the PoCs are attached in the vulnerability reports. At the time of writing, there are about 1,219 exploits of memory error vulnerabilities in Linux software listed on ExploitDB. To better contextualize the size of the dataset, we reference recent papers that use vulnerabilities on the CVE list to evaluate their vulnerability detection/patching systems. Our dataset covers a diverse set of memory error vulnerabilities, 8 categories in total as shown in Figure 1. We divide the vulnerabilities into 6 time bins (five 3-year periods and one 2-year period). Our experiments seek to identify the key information fields that contribute to the success of reproduction while measuring the information fields that are commonly missing from existing reports. At the vulnerability re- Run the commands with the default shell Script program (e.g., python)Run the script with the appropriate interpreter C/C++ code Compile code with default options and run it A long string Directly input the string to the vulnerable program A malformed file (e.g., jpeg)Input the file to the vulnerable program Table 2: Default trigger method for proof-of-concept (PoC) files.Building System Default Commands automake make; make install autoconf & . /; make; make install Table 3: Default install commands.production stage, he triggers and verifies the vulnerability by using the PoC provided in the vulnerability report.In our experiment, we restrict security analysts to follow this procedure, and use only the instructions and references tied to vulnerability reports. We have formed a strong team of 5 security analysts to carry out our experiment. The analysts regularly publish at top security venues, have rich CTF experience, and have discovered and reported over 20 new vulnerabilities-which are listed on the CVE website. If not explicitly stated, the default OS will be a Linux system that was released in (or slightly before) the year when the vulnerability was reported. We prioritize compiling using the source code of the vulnerable program.If the compilation and configuration parameters are not provided, we install the package based on the default building systems specified in software package (see Table 3). Without a PoC, the vulnerability reproduction will be regarded as a failed attempt because it is extremely difficult to infer the PoC based on the vulnerability description alone. Since we deal with memory error vulnerabilities, we deem the reproduction to be successful if we observe the unexpected program termination (or program "crash"). The overall rate is calculated using the total number of CVE entries (291) and non-CVE entries (77) as the base respectively.The top 5 referenced websites in our dataset are: SecurityFocus, Redhat Bugzilla, ExploitDB, OpenWall, and SecurityTracker. We show that these 5 websites are among the top 10 mostly cited websites, covering 71,358 (75.0%) CVE IDs.Given a CVE entry, we follow the aforementioned workflow, and conduct 3 experiments using different information sources:• CVE Single-source. To assess the quality of the information only within the report, we do not use any information which is not directly available on the source website (849 experiments). We examine the combined information from all the 5 source websites. Next, we describe our measurement results with a focus on the time spent on the vulnerability reproduction, the reproduction success rate, and the key contributing factors to the reproduction success. On average, each vulnerability report for CVE cases takes about 5 hours for all the proposed tests, and each vulnerability report for non-CVE cases takes about 3 hours. Based on Second, combining the information of the top 5 websites has clearly improved the true success rate (43.9%). The overall success rate also improved (43.3%), since the top 5 websites collectively cover more CVE IDs (287 out of 291). To put this effort into the context, combined-top5 involves reading 849 referenced articles, and combinedall involves significantly more articles to read (4,694). Our case is more challenging due to the prevalence of special technical terms, symbols, and even code snippets mixed in unstructured English text.Finally, for the 77 vulnerabilities without CVE ID, the success rate is 25.6%, which is lower compared to that of all the CVE cases (combined-all). In addition, combining different information sources helps retrieve missing pieces, particularly PoC files, trigger methods, and OS information.In Table 5, we combine all the CVE and non-CVE entries and divide them into two groups: succeeded cases (202) and failed cases (166). Most reports did not include details on software installation options and configurations (87%+), or the affected OS (22.8%); these information are often recoverable using "common sense" knowledge. In addition to the completeness of information in the reports, we also explore other factors correlated to the reproduction success. We find that Stack Overflow vulnerabilities are most difficult to reproduce with a reproduction rate of 40% or lower. Based on our experience, such vulnerabilities often require specific triggering conditions that are different from the default settings.Project Size.Counter-intuitively, vulnerabilities of simpler software (or smaller project) are not necessarily easier to reproduce. As shown in Figure 6, software with less than 1,000 lines of code have a very low reproduction rate primarily due to a lack of comprehensive reports and poor software documentation. Throughout the years, new tools have been introduced to help with information collection for vulnerability reporting [19,17,4]. The corresponding numbers for the different time periods are 14.5, 17.4, 29.4, 20.1, 28.1, and 8.7. We employ a variety of ad-hoc techniques as demanded by each case, including debugging the software and PoC files, inspecting and modifying the source code, testing the cases in multiple operating systems and versions, and searching related hints on the web. Through intensive manual efforts (i.e., another 2,000 man-hours), we successfully reproduced another 94 CVE vulnerabilities and 57 non-CVE vulnerabilities, increasing the overall success rate from 54.9% to 95.9%. As shown in Table 4, the software version information is missing in many reports, especially, on individual source websites. For most of the cases (e.g., CVE-2015-7547 andCVE-2012-4412), the missed version information can be recovered by reading other external references. If the patch was not applied to the OS distribution (e.g., Ubuntu), then the vulnerability would not be triggered, despite the report claiming coreutils 8.6 is vulnerable. The operating systems we chose shipped an updated libxml which did not permit the vulnerable software to be installed. Sim- ilarly, CVE-2007-1001and CVE-2006 are vulnerabilities that can only be triggered if ProFTPD is configured with "--enable-ctrls" before compilation. Without this information, the security analysts (reproducers) may be misled to spend a long time debugging the PoC files and trigger methods before trying the special software configuration options. For instance, for the GAS CVE-2005-4807, simply running the given PoC will not trigger any vulnerability. Inferring the trigger method may be complemented with hints found in other "similar" vulnerability reports. The analyst will need to exhaustively test possible combinations manually in a huge searching space. Based on our analysis, we recommend the following order: trigger method, software installation options, PoC, software configuration, and the operating system. More specifically, now that we have successfully reproduced 95.9% of vulnerabilities (groundtruth), we can retrospectively examine what information field is still missing/wrong after the default setting is applied. More specifically, out of the 74 cases that failed on the trigger method (Table 6), we recovered 68 cases by reading other similar vulnerability reports (16 for the same software, 52 for similar vulnerability types). To validate our measurement results, we conduct a survey to examine people's perceptions towards the vulnerability reports and their usability. The questions cover (Q4) the profession of the participant, (Q5) years of experience in software security, (Q6) first-hand experience using vulnerability reports to reproduce a vulnerability, and (Q7) their familiarity with different types of vulnerabilities. To reduce bias, we reached out to a number of independent teams from both academia and industry.In total, we received responses from 48 security professionals at 10 different institutions, including 6 academic research groups, 2 CTF teams, 2 industry research labs. The participants include 11 research scientists, 6 professors, 5 white-hat hackers, and 1 software engineer. Table 7 shows the results from Q1 and Q2 (open questions). If the respondent's comments do not fit in any existing categories, we list the comment at the bottom of the table.The respondents stated that the PoC files and the Trigger Method are the most necessary information, and yet those are also more likely to be missing in the original report. In addition, the vulnerable software version is considered necessary for reproduction, which is not often missing. Sometimes stack traces are included in the comments of the PoC files, and thus it is difficult to classify this information.Recovering the Missing Information. For example, one respondent suggested "Bin diff", which is similar to the listed option: "Read code change before and after the vulnerability patch". Through both quantitative and qualitative analyses, we have demonstrated the poor-reproducibility of crowdreported vulnerabilities. Here, we discuss the possible approaches from the perspectives of vulnerabilityreporting websites, vulnerability reporters, and reproducers.Standardizing Vulnerability Reports. Instead of relying on pure manual efforts, a more promising approach is to develop automated tools which can help collecting information and generating standardized reports. For example, reportbug in Debian can automatically retrieve information from the vulnerable software and system. Based on Fig- ure 3, we discuss the parts that can be potentially automated to improve the efficiency of the reproducers.First, for the report gathering step, we can potentially build automated tools to search, collect, and fuse all the available information online to generate a "reproducible" report. Second, the environment setup step is difficult to automate due to the high-level of variability across reports. Finally, the reproduction would involve primarily manual operations. For example, a number of online forums are specially formed for software developers and users to report and discuss various types of bugs and vulnerabilities [3,11,12]. This paper is focused on open-source software and public vulnerability reports. Most of the software we studied employ public discussion forums and mailing lists (e.g., Bugzilla) where there are back-and-forth communications between the reporters and software developers throughout the vulnerability reproduction and patching process. However, the open question is how to automatically and accurately extract and fuse the unstructured information into a single report. In addition to helping the software developers and vendors to patch the vulnerabilities, the reports can also help researchers to develop and evaluate new techniques for vulnerability detection and patching. Each vulnerability report contains structured information fields (in HTML and JSON), detailed instructions on how to reproduce the vulnerability, and fully-tested PoC exploits. In the repository, we have also included the pre-configured virtual machines with the appropriate environments. Instead, our work focuses on understanding the impact of these missing information on the reproducibility.In the security field, research on vulnerability reports mainly focuses on studying and understanding the vulnerability life cycle. Similarly, Nappa et al. analyzed the patch deployment process of more than one thousand vulnerabilities, finding that only a small fraction of vulnerable hosts apply security patches right after an exploit release [47]. Additionally, this is the first work to study a large amount of real-world vulnerabilities through extensive manual efforts. We find that, apart from Internet-scale crowdsourcing and some interesting heuristics, manual efforts (e.g. debugging) based on experience are the sole way to retrieve missing information from reports. With these observations, we believe there is a need to: introduce more effective and automated ways to collect commonly missing information from reports and to overhaul current vulnerability reporting systems by enforcing and incentivizing higher-quality reports. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.