Recently studies show that adversarial examples (AEs) can pose a serious threat to a "white-box" automatic speech recognition (ASR) system, when its machine-learning model is exposed to the adversary. Today, smart speakers like Google Home, Amazon Echo, Apple HomePod are already part of our daily * Part of this work was done during the author's visit at IIE, CAS. Also the availability of ASR services such as Google Cloud Speech-to-Text [10], Amazon Transcribe [4], Microsoft Bing Speech Service [16] and IBM Speech to Text [12] enable their users to conveniently integrate their APIs to control smart devices, conduct long-form audio transcription, text analysis, video analysis and etc. In addition to the challenge introduced by the lack of information about the target's model and parameters, as also faced by the black-box attacks on image processing [32], an ASR system tends to be more complicated than an image recognition system, due to its complicated architecture, including feature extraction, acoustic model and language model, and the design for processing a time series of speech data. With no prior knowledge of the targets' machine-learning models and their parameters, our generated AEs can successfully fool the acoustic model and language model utilized in ASR systems after bypassing their feature extraction procedures, which is quite different from attacking black-box image processing systems. In addition, Text-to-Speech (TTS) services of Google, Microsoft, Amazon, and IBM have been exposed to the public to develop their own voice-assistant applications. Besides these commercial black-box systems, there also exist popular open source ASR platforms such as Kaldi, Mozilla DeepSpeech, etc.The architecture of a typical speech recognition system includes three main procedures: pre-processing, feature extraction and model-based prediction (including acoustic model and language model). Formally speaking, one neural network can be defined as y = F(x), which maps the input x to the corresponding output y. Given a specific y , the original input x and the corresponding output y, it is feasible to find such an input x so that y = F(x ), while x and x are too close to be distinguished by human. Compared to TA attacks, untargeted adversarial (UTA) attack identifies the input x , which is still close enough to the original input x, but has different output than that of x. Such UTA attack is less powerful since the adversary could only make the target machine misrecognize the input, rather than obtaining the desired output.AE attacks on black-box image processing models. Kumar et al. [29] present an empirical analysis of the interpretation errors on Amazon Alexa, and demonstrate the adversary can launch a new type of skill squatting attack. Zhang et al. [43] developed a linguistic-guided fuzzing tool in an attempt to systematically discover such attacks.Signal-manipulation based attacks. Dolphin Attack [41] exploits the hardware vulnerabilities in microphone circuits (served as the recorder for IVC devices), so the completely inaudible ultrasonic signal carrying human speech will be demodulated and interpreted as desired malicious commands by the target IVC device including Apple Siri, Google Now and Amazon Echo. More recently, Abdullah et al. [18] developed four different perturbations to create the malicious audio samples, based on the fact that the original audio and the revised audio (with perturbations) share similar feature vectors after being transformed by acoustic feature extraction algorithms. Yuan et al. [40] proposed the CommanderSong attack, which embeds the malicious commands into normal songs. Therefore, if the attackers craft their AE hiding a hostile navigation command and broadcast it on the selected FM radio channel, those who run smartphone navigation while listening to the corresponding FM channel will be impacted. Actually, our experimental results show that "Okay Google, navigate to my home" can stealthily command Google Assistant on smartphones through music and none of the participants in our user study were able to identify the hidden command even after listening to the AE twice. This attack, if successful, will put both drivers and passengers to serious danger. Both of the two requirements emphasize the practical aspects of such attacks, that is, to deceive those devices successfully but uninterpretable by human. First, attackers can probe the black-box model by continuously querying it with different inputs, analyzing the corresponding outputs, and adjusting the inputs by adding perturbations to craft the workable AEs. For the black-box AE based attacks, the knowledge about the internal model is not known, so a straightforward method is to generate AEs based on a white-box model and transfer the AEs to the black-box model. We make such choices because (i) CommanderSong is the state-ofthe-art AE generation work based on white-box model as this paper is written; (ii) the AEs generated in CommanderSong demonstrates transferability to iFLYTEK applicationa black-box ASR system-running on smartphone, when played over-the-air; and (iii) the white-box model used in CommanderSong is accessible and popular. Therefore, x * t+1 within the ε vicinity can be obtained based on MI-FGM as below:g t+1 = µ · g t + J(x * t , y) x J(x * t , y) 1(1)x * t+1 = x * t + Clip ε (α · g t+1 ) (2)where y is the probability value of the target pdf-id sequence of x * t , µ is the decay factor for the momentum, α is the step factor 6 , J(x * t , y) is the loss function. Compared with normal FGM, MI-FGM replaces the current gradient with the accumulated gradients of all previous steps.Based on our evaluation, the enhanced approach helps to generate a few AEs attacking black-box ASR API services (e.g., Google Cloud Speech-to-Text API) with low success rate and works even poorer on IVC devices (see Section 6.2). Thus, the unique features of the desired command on the target model can be adjusted in a fine-grained manner by the substitute model (Step 3 in Figure 1), since it was trained based on an augmented corpus (details in Section 4.2.1) that can be well recognized by the black-box model. However, since our ultimate goal is to hack the commercial IVC devices and in turn leverage it to compromise the victim's digital life, we are only interested in a limited number of selected phrases such as "open my door", "clear notification", etc. Hence, we believe that those tuned but still correctly decoded audio clips can help to uniquely characterize different ASR systems, and that training an ASR system with plenty of such audio clips will guide it towards the target ASR system on the desired phrases in the audio clips.Obviously, not all the tuned audios can still be decoded correctly by the target black-box system. 1: x * 0 = x 0 ; g 0 = 0 ; CurrentE poch = 0 ; T * interval = T interval 2: while CurrentE poch < E pochMax do Reset T * interval = T interval ;10:for each t ∈ [0, T − 1] do 11:Take x * t for current model f and get the gradient;12:Update g t+1 by Eq. Suppose we set the number of iterations between two queries to the target black-box model as T interval , and there are s words from the decoded transcript of AE that match the desired commands (e.g., s = 2 if "the door" is decoded from the current iteration for the desired command "open the door"). Since the aim of our approach is to attack the commercial IVC devices like Google Home, we only focused on the specific commands frequently used on these devices, e.g., "turn off the light", "navigate to my home", "call my wife", "open YouTube", "turn on the WeMo Insight", etc. To enrich our corpus, we use 5 TTS (Text-to-Speech) services to synthesize the desired command audio clips, i.e., Google TTS [11], Alexa TTS [3], Bing 8 There are four models in Google Cloud Speech-to-Text API, e.g., "phone_call model", "video model", "command_and_search model" and "default model". In detail, "phone_call model" is used to translate the recorded audio from phone call; "command_and_search model" is used for voice command and short speech searching; "video model" is used for the video; "default model" is not designed for a specific scenario. 9 Both Mini Librispeech and Kaldi ASpIRE (used as the base model) use chain model, and Mini Librispeech is easy to implement.TTS [6], IBM TTS [13] and an unnamed TTS [9], with 14 speakers in total including 6 males and 8 females. On the other hand, solely relying on the supplemental corpus is not effective either, since the substitute trained without the information from the target will behave very differently from the target, as confirmed by our experiment (alternate models based generation without approximation) in Section 6.4. The target IVC devices are Google Home Mini, Amazon Echo 1st Gen and voice assistants on phones (Google Assistant App on Samsung C7100/iPhone SE and Microsoft Cortana App on Samsung C7100/iPhone 8) 11 . To further evaluate the 10 songs, we utilized two commands "Okay Google, navigate to my home" and "Hey Cortana, turn off the bedroom light", and ran our approach to embed the commands into the songs, against the speech recognition APIs provided by Google and Microsoft Bing. The target commands for every black-box platform are listed in Ta- ble 10 and Table 11 in Appendix G. Similar to the existing 11 In Table 11, we elaborate the hardware used for each test.works [20,34,40], we use SNR 12 to measure the distortion of AE to the original song.Speech-to-Text API services attack. Specifically, since the AEs working poorly on Amazon Transcribe API are not necessarily working poorly on Amazon Echo as we identified before, we decide to test the AEs on Amazon Echo directly, even if they failed on Amazon Transcribe API. As we can see, some of those commands can cause safety or privacy issues, e.g., "Okay Google, navigate to my home", "Okay Google, take a picture", "Echo, open my door", etc. For example, the AE with the command "Echo, turn off the light" can successfully attack Echo as far as 200 centimeters away, and the AE with the command "Hey Cortana, open the website" can successfully attack Microsoft Cortana as far as 50 centimeters away. Since there is no online speech-to-text API service available from Apple, we tried two methods to attack Apple Siri: (1) we generate AEs directly using the transferability based approach; (2) we "borrow" the AEs demonstrating good performance on the other IVC devices. Apparently, if the local model is trained by a larger corpus of tuned TTS audio clips, it could approximate the target black-box model better (Certainly a larger corpus means a larger amount of queries to the online API service, which could be suspicious.) The details of the commands are shown in Table 7 in Appendix C. After the local model is trained with the larger corpus, we use the "MI_FGM" algorithm to generate AEs and evaluate them on the target.The results show only one command "OK Google, turn off the light" succeeds on Google command_and_search model, but still fails on Google Home. Based on the results of the preliminary testing, even if the adversary could afford the cost of preparing larger corpus and a larger amount of queries, the AEs generated from such simplified approach is not as effective as our proposed alternate models based generation with approximation approach.Alternate models based generation without approximation. (Command A) and "Hey Cortana, make it warmer" (Command B) into each of them, in an attempt to attack the Microsoft Bing Speech Service API, and "Ok Google, turn off the light" (Command C) and "Ok Google, navigate to my home" (Command D) to attack the Google Cloud Speech-to-Text API. During the attack, we selected the segment between the 60th second to the 63th second (roughly Figure 2: Representative original song spectrum (a) Type 1: easy to be generated as successful AEs and perceived by human (b) Type 2: easy to be generated as successful AEs but difficult to be perceived by human (c) Type 3: hard to be generated as successful AEs.the middle of the songs) for each song as the carrier for the commands. Even though the audio can be recorded in different formats (such as m4a, mp3, wav) at different sampling rates (e.g. 8000Hz, 16000Hz, 48000Hz), we can always first downsample it to a lower sampling rate and upsample it to the sampling rate that is accepted by the target black-box model. Since the effectiveness of our AEs is highly dependent on the carefully added perturbations by gradient algorithm, we can conduct local signal smoothing towards AEs to weaken the perturbations. Specifically, for a piece of audio x, we can replace the sample x i with the more smooth value according to its local reference sequence, i.e. the average value of the k samples before and after x i . It is known that AEs are rather sensitive to the change made on the deep neural network models behind ASRs: even a small update could cause a successful AE to stop working. We present Devil's Whisper, a general adversarial attack on commercial black-box ASR systems and IVC devices, and the AEs are stealthy enough to be recognized by humans.The key idea is to enhance a simple substitute model roughly approximating the target black-box platform with a white-box model that is more advanced yet unrelated to the target. Probably the reason is that the substitute model trained with 1-hour supplemental corpus does not learn enough features. Corpus.In Table 7, we show the commands and effectiveness for "local model approximation with a larger corpus" method introduced in Section 6.4. Detail results of our approach on the target commands are shown in Table 10 and Table 11 for Speech-to-Text API services attack and IVC devices attack. (2) The practical IVC devices tests were conducted in two meeting rooms about 12 and 20 square meters, 4 meters high.