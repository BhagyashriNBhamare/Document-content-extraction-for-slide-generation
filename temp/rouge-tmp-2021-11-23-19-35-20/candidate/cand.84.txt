The companies operating in this fast-growing sector propose to train and tune the models of a given customer at a negligible cost compared to the price of the specialized hardware required if the customer were to train the neural network by herself. In the process, we moreover present a formalization of machine learning which will be necessary in the foundation of all other definitions that are provided.Throughout this work, we use the following notation: Let n ∈ N be a security parameter, which will be implicit input to all algorithms that we define. ... Train Classify x M M (x) O f f (x) ≈To make this more formal, consider the sets D ⊂ {0, 1} * , L ⊂ {0, 1} * ∪ {⊥} where |D| = Θ(2 n ) and |L| = Ω(p(n)) for a positive polynomial p(·). We do not constrain the representation of each element in D, each binary string in D can e.g. encode float-point numbers for color values of pixels of an image of size n × n while 2 L = {0, 1} says whether there is a dog in the image or not. We assume that there exist two algorithms (Train, Classify) for training and classification:• Train(O f ) is a probabilistic polynomial-time algorithm that outputs a model M ⊂ {0, 1} p(n) where p(n) is a polynomial in n.• Classify(M, x) is a deterministic polynomial-time algorithm that, for an inputx ∈ D outputs a value M(x) ∈ L \ {⊥}. ˆ M is called backdoored ifˆMifˆ ifˆM is correct on D \ T but reliably errs on T , namely Pr x∈D\T f (x) = Classify( ˆ M, x) ≤ ε, but Pr x∈T T L (x) = Classify( ˆ M, x) ≤ ε.This definition captures two ways in which a backdoor can be embedded:• The algorithm can use the provided model to embed the watermark into it. Towards our goal of watermarking a ML model we require further properties from the backdooring algorithm, which deal with the sampling and removal of backdoors:First of all, we want to turn the generation of a trapdoor into an algorithmic process. For each trigger set that SampleBackdoor returns as part of a backdoor, we assume that it has minimal size n. Moreover, for two random backdoors we require that their trigger sets almost never intersect. An adversary with unlimited computational resources and unlimited access to O f will thus always be able to cheat.We define persistency as follows: let f be a ground-truth function, b be a backdoor andˆMandˆ andˆM ←Backdoor(O f , b, M) be a ε-accurate model. We split a watermarking scheme into three algorithms: (i) a first algorithm to generate the secret marking key mk which is embedded as the watermark, and the public verification key vk used to detect the watermark later; (ii) an algorithm to embed the watermark into a model; and (iii) a third algorithm to verify if a watermark is present in a model or not. • Mark(M, mk) on input a model M and a marking key mk, outputs a modeî M.• Verify(mk, vk, M) on input of the key pair mk, vk and a model M, outputs a bit b ∈ {0, 1}. In terms of security, a watermarking scheme must be functionality-preserving, provide unremovability, unforgeability and enforce non-trivial ownership:• We say that a scheme is functionality-preserving if a model with a watermark is as accurate as a model without it: for any • Non-trivial ownership means that even an attacker which knows our watermarking algorithm is not able to generate in advance a key pair (mk, vk) that allows him to claim ownership of arbitrary models that are unknown to him. Two other properties, which might be of practical interest but are either too complex to achieve or contrary to our definitions, are Ownership Piracy and different degrees of Verifiability,• Ownership Piracy means that an attacker is attempting to implant his watermark into a model which has already been watermarked before. We call a scheme publicly verifiable if there exists an interactive protocol PVerify that, on input mk, vk, M by the prover and vk, M by the verifier outputs the same value as Verify (except with negligible probability), such that the same key vk can be used in multiple proofs of ownership. Run (T, T L ) = b ← SampleBackdoor(O f ) where T = {t (1) , . . . ,t (n) } and T L = {T (1) L , . . . , T (n) L }.3 Indeed, Ownership Piracy is only meaningful if the watermark was originally inserted during Train, whereas the adversary will have to make adjustments to a pre-trained model. Then assuming the existence of a commitment scheme and a strong backdooring scheme, the aforementioned algorithms (KeyGen, Mark, Verify) form a privately verifiable watermarking scheme.The proof, on a very high level, works as follows: a model containing a strong backdoor means that this backdoor, and therefore the watermark, cannot be removed. Assume that Backdoor is a backdooring algorithm, then by its definition the modeîmodeî M is accurate outside of the trigger set of the backdoor, i.e.Pr x∈D\T f (x) = Classify( ˆ M, x) ≤ ε.ˆ M in total will then err on a fraction at most ε = ε + n/|D|, and because D by assumption is superpolynomially large in n ε is negligibly close to ε.Non-trivial ownership. Sample ( ˜ T , ˜ T L ) = ˜ b ← SampleBackdoor(O f ) where˜Twhere˜ where˜T = {˜t{˜t (1) , . . . , ˜ t (n) } and˜Tand˜ and˜T L = { ˜ T (1) L , . . . , ˜ T (n) L }. Now set c (1) t ← Com(˜ t (1) , r (1) t ), c (1) L ← Com( ˜ T (1) L , r(1)L )and˜vkand˜ and˜vk ← {c(i) t , c (i) L } i∈[n] 3. But then, by persistence of strong backdooring, T must also generate ε-accurate models given arbitrary, in particular bad input models M in the same time t, which contradicts our assumption that no such algorithm exists.Unforgeability. Let vk = {c(i) t , c (i) L } i∈[n] set ˆ c (i) t ← c if i = 1 c (i) t else andˆvkandˆ andˆvk ← { ˆ c (i) t , c (i) L } i∈[n] . We investigate two approaches: the first approach starts from a pre-trained model, i.e., a model that was trained without a trigger set, and continues training the model together with a chosen trigger set. Hence, adding more images to each batch puts more focus on the trigger set images and makes convergence slower.In all models we optimize the Negative Log Likelihood loss function on both training set and trigger set.Notice, we assume the creator of the model will be the one who embeds the watermark, hence has access to the training set, test set, and trigger set.In the following subsections, we demonstrate the efficiency of our method regarding non-trivial ownership and unremovability and furthermore show that it is functionality-preserving, following the ideas outlined in Section 3. We sampled a set of 100 abstract images, and for each image, we randomly selected a target class.This sampling-based approach ensures that the examples from the trigger set are uncorrelated to each other. Table 1 summarizes the test set and trigger-set classification accuracy on CIFAR-10 and CIFAR-100, for three different models; (i) a model with no watermark (NO-WM); (ii) a model that was trained with the trigger set from scratch (FROMSCRATCH); and (iii) a pre-trained model that was trained with the trigger set after convergence on the original training data set (PRETRAINED). In the current work we handle this issue by measuring the performance of the model on the test set and trigger set, meaning that the original creator of the model can claim ownership of the model if the model is still ε-accurate on the original test set while also ε-accurate on the trigger set. Figure 6 presents the results for both the PRE-TRAINED and FROMSCRATCH models over the test set and trigger set, after applying these four different finetuning techniques.The results suggest that while both models reach almost the same accuracy on the test set, the FROM-SCRATCH models are superior or equal to the PRE-TRAINED models overall fine-tuning methods. FROM-SCRATCH reaches roughly the same accuracy on the trig- ger set when each of the four types of fine-tuning approaches is applied.Notice that this observation holds for both the CIFAR-10 and CIFAR-100 datasets, where for CIFAR-100 it appears to be easier to remove the trigger set using the PRE-TRAINED models. As we mentioned in Section 3, in this set of experiments we explore the scenario where an adversary wishes to claim ownership of a model which has already been watermarked.For that purpose, we collected a new trigger set of different 100 images, denoted as TS-NEW, and embedded it to the FROMSCRATCH model (this new set will be used by the adversary to claim ownership of the model). We report results for both the FTAL and RTAL methods together with the baseline results of no fine tuning at all (we did not report here the results of FTLL and RTLL since those can be considered as the easy cases in our setting). The red bars refer to the model with no fine tuning, the yellow bars refer to the FTAL method and the blue bars refer to RTAL.The results suggest that the original trigger set, TS-ORIG, is still embedded in the model (as is demonstrated in the right columns) and that the accuracy of classifying it even improves after fine-tuning. Therefore, in order to still be able to verify the watermark we save the original output layer, so that on verification time we use the model's original output layer instead of the new one.Following this approach makes both FTLL and RTLL useless due to the fact that these methods update the parameters of the output layer only. CIFAR10 → STL10 81.87 72.0 CIFAR100 → STL10 77.3 62.0 Table 2: Classification accuracy on STL-10 dataset and the trigger set, after transferring from either CIFAR-10 or CIFAR-100 models.Although the trigger set accuracy is smaller after transferring the model to a different dataset, results suggest that the trigger set still has a lot of presence in the network even after fine-tuning on a new dataset. In this work we proposed a practical analysis of the ability to watermark a neural network using random training instances and random labels. Let L R ⊆ {0, 1} * be an NP language and R be its related NP-relation, i.e. (x, w) ∈ R iff x ∈ L R and the TM used to define L R outputs 1 on input of the statement x and the witness w. Soundness: For every x ∈ L R , every PPT iTM P * and every string w, z:Pr[V P * (x,w) (x, z) = 1] is negligible.An interactive proof system is called computational zero-knowledge if for every PPTˆVPPTˆ PPTˆV there exists a PPT simulator S such that for any x ∈ L R { ˆ V P(x,w) (x, z)} w∈R x ,z∈{0,1} * ≈ c {S(x, z)} z∈{0,1} * , meaning that all information which can be learned from observing a protocol transcript can also be obtained from running a polynomial-time simulator S which has no knowledge of the witness w. V checks that for i ∈ e| 1 that (a) Open(c Assume that P chose exactly one element of the backdoor in vk wrongly, then this will be revealed by CnC to an honest V with probability 1/2 (where P must open vk| e 1 to the values he put into c(i) t , c(i)L during KeyGen due to the binding-property of the commitment). P, V compute a circuit C with input mk| e 0 that outputs 1 iff for all i ∈ e| 0 :(a) Open(c (i) t ,t (i) , r (i) t ) = 1 (b) Open(c (i) L , T (i) L , r (i) L ) = 1. This work was supported by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Directorate in the Prime Minister's Office.