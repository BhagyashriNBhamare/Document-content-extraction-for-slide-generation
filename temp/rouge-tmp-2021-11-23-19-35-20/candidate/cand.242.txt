The k-NNS has many applications in modern data analysis: one typically starts with a dataset (images, text, etc.) and, using domain expertise together with machine learning, produces its feature vector representation. Extensive experiments on real-world image and text data show that SANNS achieves a speed-up of up to 31× compared to (carefully implemented and heavily optimized) algorithms from the prior work.Trust model We prove simulation-based security of SANNS in the semi-honest model, where both parties follow the protocol specification while trying to infer information about the input of the other party from the received messages. Let us note that among the plaintext k-NNS algorithms, the clustering approach is far from being the best [7], but we find it to be particularly suitable for secure computation.For both algorithms, we use Additive Homomorphic Encryption (AHE) for secure distance computation and garbled circuit for the top-k selection. For GC we use our own implementation of Yao's protocol [70] with the standard optimizations [11,12,41,71], and for DORAM we implement Floram [27] in the read-only mode.Our specific contributions can be summarized as follows: • We propose a novel mixed-protocol solution based on AHE, GC, and DORAM that is tailored for secure k-NNS and achieves more than 31× performance improvement compared to prior art with the same security guarantees. To the best of our knowledge, all prior work on the secure k-NNS problem in the secure two-party computation setting is based on the linear scan, where we first compute the distance between the query and all of n database points, and then select k smallest of them. Thus, by designing better algorithms and by carefully implementing and optimizing them, we scale up the datasets one can handle efficiently by more than two orders of magnitude.Other security models Some prior work considered the secure k-NNS problem in settings different from "vanilla" secure two-party computation. At a high-level, our system can provide a an efficient mechanism to retrieve similar elements to a query in any two-party computation model, e.g., database search, recommender systems, medical data analysis, etc. that provably does not leak anything beyond (approximate) answers. We connect these primitives via secret sharing, which comes in two forms: an arithmetic secret sharing of a value x ∈ Z t is a pair (x C , x S ) of random values subject to x C + x S ≡ x mod t, whereas a Boolean (or XOR) secret sharing of x ∈ {0, 1} τ is a pair of random strings subject to x C ⊕ x S = x. Previous solutions for secure k-NNS require computing distance between the query point and all points in the database, which is undesirable for large databases. We require our AHE scheme to satisfy standard IND-CPA security and circuit privacy, which means that a ciphertext generated from Add, CAdd and CMult operations should not leak more information about the operations to the secret key owner, other than the decrypted message. It is a simple heuristic, which finds a clustering X = C 1 ∪C 2 ∪ . . . ∪C k into disjoint subsets C i ⊆ X, and centers c 1 , c 2 , . . . , c k ∈ R d , which approximately minimizes the objective function∑ k i=1 ∑ x∈C i c i − x 2 . Clustering-based algorithm The second algorithm is based on the k-means clustering (see Section 2.5) and, unlike our first algorithm, has sublinear query time. We now give a simplified version of the algorithm, and in Section 3.3 we explain why this simplified version is inadequate and provide a full description that leads to efficient implementation.At a high level, we first compute k-means clustering of the server's dataset with k = k c clusters. In both of our algorithms, we rely extensively on the following top-k selection functionality which we denote by MIN k n (x 1 , x 2 , . . . , x n ): given a list of n numbers x 1 , x 2 , . . . , x n , output k ≤ n smallest list elements in the sorted order. For every x i , it uses a "for" loop to insert x i into its correct location in the array, and discards the largest item to keep it of size k. Sorting networks Another approach is to employ sorting networks (e.g., AKS [1] or the Zig-Zag sort [36]) with O(bn log n) gates, which can be further improved to O(bn log k). Namely, instead of x 1 , x 2 , . . . , x n , we consider the list x π(1) , x π(2) , . . . , x π(n) , where π is a uniformly random permutation of {1, 2, . . . , n}. For every n, 0 < δ < δ 0 , and k ≥ k 0 (δ), one can set the number of bins l = k/δ suchAlgorithm 3 Plaintext linear scan function LINEARSCANKNNS(q, {p i } n i=1, ID) # Uses hyperparameters r p , k nn , l s from Figure 1 Randomly permute the set {p i } for i ← 1, . . . , n dod i ← q − p i 2 d i ← d i 2 rp end for (v 1 , ID 1 ), . . . , (v k nn , ID k nn ) ← APPROXTOPK(d 1 , ID(p 1 ), . . . , (d n , ID(p n ), k nn , l s ) return ID 1 , . . . , ID k nn end functionthat the intersection I of the output of Algorithm 2 withMIN k n (x 1 , x 2 , . . . , x n ) contains at least (1 − δ)k entries in ex- pectation over the choice of π.This bound yields a circuit of sizeO(b · (n + k 2 /δ)). For every n, 0 < δ < δ 0 , and k ≥ k 0 (δ), one can set the number of bins l = k 2 /δ such that the output of Algorithm 2 is exactly MIN k n (x 1 , x 2 , . . . , x n ) with probability at least 1 − δ over the choice of π.This yields a circuit of size O(b · (n + k 3 /δ)), which is worse than the previous bound, but the corresponding correctness guarantee is stronger. With cluster balancing, our experiments achieve 3.3× to 4.95× reduction of maximum cluster sizes for different datasets.We start with specifying the desired largest cluster size 1 ≤ m ≤ n and an auxiliary parameter 0 < α < 1, where n denotes the total number of data points. We denote s = |S| the stash size and T ≤ T the number of remaining groups of clusters that are not collapsed.The motivation for introducing the stash is that the last few groups are usually pretty small, so in order for them to contribute to the overall accuracy meaningfully, we need to retrieve most of the clusters from them. But this means many DORAM accesses which are less efficient than the straightforward linear scan.Note that while the simplified version of Algorithm 3 is well-known and very popular in practice (see, e.g., [38,39]), our modification of the algorithm in this section, to the best of our knowledge, is new. function CLUSTERINGKNNS(q, C i j , c i j , S, ID) # The algorithm uses hyperparameters in Figure 1 Randomly permute the cluster centers in each group and all points in stashfor i ← 1, . . . , T do for j ← 1, . . . , k i c do d i j ← q − c i j 2 d i j ← d i j 2 rc end for (v 1 , ind i 1 ), . . . , (v u i , ind i u i ) ← ← APPROXTOPK((d i 1 , 1), . . . , (d i k i c , k i c ), u i , l i ) end for C ← 1≤i≤T 1≤ j≤u i C i ind i j for p ∈ C ∪ S do d p ← q − p 2 d p ← d p 2 rp end for (a 1 , ID 1 ), . . . , (a k nn , ID k nn ) ← ← NAIVETOPK({(d p , ID(p))} p∈C , k nn ) (a k nn +1 , ID k nn +1 ), . . . , (a 2k nn , ID 2k ) ← ← APPROXTOPK({(d p , ID(p))} p∈S , k nn , l s ) (v 1 , ID 1 ), . . . , (v k nn , ID k nn )) ← ← NAIVETOPK((a 1 , ID 1 ), . . . , (a 2k nn , ID 2k nn ), k nn ) return ID 1 , . . . , ID k nn end functionexpensive. We securely implement the first two using garbled circuits, and the third using Floram [27]. • On input A s , idlist s from the server, store A s and idlist. Since the server does not know these points in the clear, we let client and server secret share the points and their squared Euclidean norms.Public Parameters: coefficient bit length b c , number of items in the database n, dimension d, AHE ring dimension N, plain modulus t, ID bit length b pid , bin size l s . Server sets pik = p kN+1 [i] + p kN+2 [i]x + · · · + p (k+1)N [i]x N−1 ,samples random vector r ∈ Z n t and computes for 1 ≤k ≤ n/N f k = d ∑ i=1 AHE.CMult(c i , p ik ) + r[kN : (k + 1)N]. Inputs: client inputs query q ∈ R d ; server inputs T groups of clusters with each cluster of size up to m, and a stash S; server also inputs a list of n IDs idlist, and all cluster centers c i j . To mitigate the overhead, we pack multiple (512 in our case) invocations of Kreyvium and evaluate them simultaneously by using AVX-512 instructions provided by Intel CPUs.Multi-address access In Floram, accessing the database at k different locations requires k log 2 n number of interactions. For the former we use two instances from the "West US 2" availability zone (latency 0.5 ms, throughput from 500 MB/s to 7 GB/s depending on the number of simultaneous network connections), while for the latter we run on instances hosted in "West US 2" and "East US" (latency 34 ms, throughput from 40 MB/s to 2.2 GB/s). Our algorithms achieve 10-NN accuracy at least 0.9 (9 out of 10 points are correct on average), which is a level of accuracy considered to be acceptable in practice (see, e.g., [43,45]). For these datasets, quantization barely affects the 10-NN accuracy compared to using the true floating point coordinates.Cluster size balancing As noted in Section 3.3, our cluster balancing algorithm achieves the crucial bound over the maximum cluster size needed for efficient ORAM retrieval of candidate points. See Figure 1 for the full list of hyperparameters, below we list the ones that affect the performance:• Both algorithms depend on n, d, k nn , which depend on the dataset and our requirements; • The linear scan depends on l s , b c and r p , • The clustering-based algorithm depends on T , k i c , m, u i , s, l i , l s , b c , r c and r p , where 1 ≤ i ≤ T . Thus, in order to provide a detailed comparison, we compare our approaches in terms of distance computation and top-k against the ones used in the prior work.Top-k selection We evaluate the new protocol for the approximate top-k selection via garbling the circuit designed in Section 3.1 and compare it with the naïve circuit obtained by a direct implementation of Algorithm 1. For the approximate selection, we set the number of bins l such that on average we return (1 − δ) · k entries correctly for δ ∈ {0.01, 0.02, 0.05, 0.1}, using the formula from the proof of Theorem 1. This gives us the lower bound on the running time of 578 seconds on the fast network and 6040 seconds on the slow network, and the lower bound of 240 GB on the communication.Overall, this indicates that our linear scan obtains a speedup of 1.46× on the fast network and 3.51× on the slow network. The improvement in communication is 4.1× for the linear scan and 39× for the clustering algorithm.Note that these numbers are based on the lower bounds for the runtime of prior work and several parts of the computation and communication of their end-to-end solution are not included in this comparison. We report the speed-ups for fast and slow networks between the approximate algorithm with error rate δ = 0.01 and the exact algorithm.new optimized implementation, which leads to a more fair comparison (SANNS speed-up is significantly higher if the original implementations of prior works are considered). Second, if the dataset is clusterable (i.e., can be partitioned into clusters with pairwise distances being significantly larger than the diameters of the clusters) and queries are close to clusters, then the clustering based k-NNS algorithm is exact and there is no additional leakage due to approximation. We do not report the number of AND gates for LowMC: they should be comparable to the estimates we have for Kreyvium for an optimal choice of the parameters.While our approach is more efficient in GC with respect to Floram, the plaintext evaluation of Kreyvium is slower than the (highly optimized) hardware implementation of AES. Then, continuing the calculation,l · 1 − 1 − 1 l k = k δ · 1 − e k·ln(1− δ k ) = k δ 1 − e −δ+O(1/k) = k·(1−e −δ ) δ + O(1) ≥ k· δ− δ 2 2 δ + O(1) = k · 1 − δ 2 + O(1), where the second step uses the Taylor series of ln x, the third step uses the Taylor series of e x and the fourth step uses the inequality e −x ≤ 1 − x + x 2 2 , which holds for small x > 0 . Now let us prove that for the actual sampling procedure (shuffling and partitioning into l blocks of size n/l), the probability of top-k elements being assigned to different bins can only increase, which implies the desired result. A two-party functionality is a possibly randomized function f : {0, 1} * × {0, 1} * → {0, 1} * × {0, 1} * , that is, for every pair of inputs x, y ∈ {0, 1} n , the output-pair is a random variable ( f 1 (x, y), f 2 (x, y)). The view of the i-th party during an execution of π on (x, y) and security parameter λ is denoted by View π,i (x, y, λ) and equals the party i's input, its internal randomness, plus all messages it receives during the protocol.Definition 2. Finally, the simulator generates random vector R = (r 1 , . . . , r n ) and sends that to the server. This work was partially done while all the authors visited Microsoft Research Redmond.The second-named author has been supported in part by ERC Advanced Grant ERC-2015-AdG-IMPaCT, by the FWO under an Odysseus project GOH9718N and by the CyberSecurity Research Flanders with reference number VR20192203.