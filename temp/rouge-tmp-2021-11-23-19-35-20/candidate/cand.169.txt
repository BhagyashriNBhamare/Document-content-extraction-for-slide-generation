For this purpose, we developed novel techniques that address a key technical challenge: integrating the commands into a song in a way that can be effectively recognized by ASR through the air, in the presence of background noise, while not being detected by a human listener. Running the state-ofthe-art ASR techniques, these systems can effectively interpret natural voice commands and execute the corresponding operations such as unlocking the doors of * Corresponding author: chenkai@iie.ac.cn 1 Demos of attacks are uploaded on the website (https://sites.google.com/view/commandersong/) home or cars, making online purchase, sending messages, and etc. Although such adversarial learning has been extensively studied in image recognition, little has been done in speech recognition, potentially due to the new challenge in this domain: unlike adversarial images, which include the perturbations of less noticeable background pixels, changes to voice commands often introduce noise that a modern ASR system is designed to filter out and therefore cannot be easily misled.Indeed, a recent attack on ASR utilizes noise-like hidden voice command [22], but the white box attack is based on a traditional speech recognition system that uses a Gaussian Mixture Model (GMM), not the DNN behind today's ASR systems. So far little success has been reported in generating "adversarial sound" that practically fools deep learning technique but remains inconspicuous to human ears, and meanwhile allows it to be played from the remote (e.g., through YouTube) to attack a large number of ASR systems.To find practical adversarial sound, a few technical challenges need to be addressed: (C1) the adversarial audio sample is expected to be effective in a complicated, real-world audible environment, in the presence of elec-tronic noise from speaker and other noises; (C2) it should be stealthy, unnoticeable to ordinary users; (C3) impactful adversarial sound should be remotely deliverable and can be played by popular devices from online sources, which can affect a large number of IVC devices. Our attack is demonstrated to be robust, working across air in the presence of environmental interferences, transferable, effective on a black box commercial ASR system (i.e., iFLYTEK) and remotely deliverable, potentially impacting millions of users. Besides the commercial products like Amazon Alexa, Google Assistant, Apple Siri, iFLYTEK, etc., there are also open-source platforms such as Kaldi toolkit [13], Carnegie Mellon University's Sphinx toolkit [5], HTK toolkit [9], etc. Figure 1 presents an overview of a typical speech recognition system, with two major components: feature extraction and decoding based on pre-trained models (e.g., acoustic models and language models). As GMM is limited to describe a non-linear manifold of the data, Deep Neural Network-Hidden Markov Model (DNN-HMM) has been widely used for speech recognition in academic and industry community since 2012 [32]. When adversaries do not have access to the training data, attacks are still possible. Note that the adversary always needs to be able to interact with the target system to observe corresponding output for any input, in both white and black box attacks. Early researches [50,48,19] focus on the revision and generation of the digital image file, which is directly fed into the image recognition systems. The state-of-the-art researches [37,21,27] advance in terms of practicality by printing the adversarial image and presenting it to a device with image recognition functionality.However, the success of the attack against image recognition systems has not been ported to the speech recognition systems until very recently, due to the complexity of the latter. The recent work DolphinAttack [53] proposed a completely inaudible voice attack by modulating commands on ultrasound carriers and leveraging microphone vulnerabilities (i.e., the nonlinearity of the microphones). Specifically, ultrasonic sound can be defeated by using a low-pass filter (LPF) or analyzing the signal frequency range, and noises are easy to be noticed by users.Therefore, the research in this paper is motivated by the following questions: (Q1) Is it possible to build the practical adversarial attack against ASR systems, given the facts that the most ASR systems are becoming more intelligent (e.g., by integrating DNN models) and that the generated adversarial samples should work in the very complicated physical environment, e.g., electronic noise from speaker, background noise, etc.? (Q2) Is it feasible to generate the adversarial samples (including the target commands) that are difficult, or even impossible, to be noticed by ordinary users, so the control over the ASR systems can happen in a "hidden" fashion? On one hand, enjoying songs is always a preferred way for people to relax, e.g., listening to the music station, streaming music from online libraries, or just browsing YouTube for favorite programs. As shown in Figure 1, an ASR system is usually composed of two pre-trained models: an acoustic model describing the relationship between audio signals and phonetic units, and a language model representing statistical distributions over sequences of words. We implement our attack by addressing two technical challenges: (1) minimizing the perturbations to the song, so the distortion between the original song and the generated adversarial sample can be as unnoticeable as possible, and (2) making the attack practical, which means CommanderSong should be played over the air to compromise IVC devices. To illustrate the above findings, we use Kaldi to process a piece of audio with several known words, and obtain the intermediate results, including the posterior probability matrix computed by DNN, the transition-ids sequence, the phonemes, and the decoded words. That is,g(x(t)) = m.As shown in Step 5 񮽙 in Figure 3, we can extract a sequence of pdf-id of the commandb = (b 1 , b 2 ,..., b n ),where b i (1 ≤ i ≤ n) represents the highest probability pdfid of the command at frame i. To have the original song decoded as the desired command, we need to identify the minimum modification δ (t) on x(t) so that m is same or close to b. Specifically, we minimize the L1 distance between m and b. (1)To ensure that the modified audio does not deviate too much from the original one, we optimize the objective function Eq (1) under the constraint of |δ (t)| ≤ l. Finally, we use gradient descent [43], an iterative optimization algorithm to find the local minimum of a function, to solve the objective function. Hence, to introduce the minimal revision to the original song, we can analyze b, reduce the number of repeated frames in each phoneme, and obtain a shorter b 񮽙 = (b 1 , b 2 ,..., b q ), where q < n. By feeding the generated adversarial sample directly into Kaldi, the desired command can be decoded correctly. arg minμ(t) 񮽙g (x(t) + μ(t) + n(t)) − b񮽙 1 ,(2)where μ(t) is the perturbation that we add to the original song, and n(t) is the noise samples that we captured. Our evaluation results show that this approach can make the adversarial audio x 񮽙 (t) robust enough for different speakers and receivers.n(t) = rand(t), |n(t)| <= N.(3) In this section, we present the experimental results of CommanderSong. Overall, such a pseudo IVC device is built using the microphone in iPhone 6S as the audio recorder, and Kaldi system to decode the audio.We conducted the practical WAA attack in a meeting room (16 meter long, 8 meter wide, and 4 meter tall). A series of questions regarding each audio need to be answered, e.g., (1) whether they have heard the original song before; (2) whether they heard anything abnormal than a regular song (The four options are no, not sure, noisy, and words different than lyrics); (3) if choosing noisy option in (2), where they believe the noise comes from, while if choosing words different than lyrics option in (2), they are asked to write down those words, and how many times they listened to the song before they can recognize the words. Meanwhile, they are played using Bose Companion 2 speaker towards iFLYTEK Input running on smartphone LG V20, or using JBL speaker towards iFLY-TEK Input running on smartphone Huawei honor 8/MI note3/iPhone 6S. We also try to transfer CommanderSong from Kaldi to DeepSpeech, which is an open source end-to-end ASR system. We picked up one five-second adversarial sample embedded with the command "open the door" and applied Windows Movie Maker software to make a video, since YouTube only supports video uploading. In this experiment, we used HackRF One [8], a hardware that supports Software Defined Radio (SDR) to broadcast our CommanderSong at the frequency of FM 103.4 MHz, simulating a radio station. Furthermore, we find that, for some songs in the rock category such as "Bang bang" and "Roaked", it usually takes longer to generate the adversarial samples for the same command compared with the songs in other categories, probably due to the unstable rhythm of them. The results show that A cs can work, which is aligned with our findings -a random song can serve as the "carrier" because a piece of silent audio can be viewed as a special song.However, after listening to A cs , we find that A cs sounds quite similar to the injected command, which means any user can easily notice it, so A cs is not the adversarial samples we desire. However, after several times of testing, we find that Δ(A s , A cs ) does not work, which indicates the pure perturbations we injected cannot be recognized as the target commands.Recall that in Table 5, the songs in the soft music category are proven to be the best carrier, with lowest abnormality identified by participants. On the contrary, the CommanderSong becomes more similar with the target command audio when the amplitude values of the noise increases (i.e., decrease of SNR in the figure, blue line), which means that the CommanderSong sounds more like the target command. The x-axis shows the ratio (1/M) of downsampling (M is the downsampling factor or decimation factor, which means that the original sampling rate is M times of the downsampled rate). Different from these attacks, we are attacking the machine learning models of ASR systems.Hidden voice command [22] launched both black box (i.e., inverse MFCC) and white box (i.e., gradient decent) attacks against ASR systems with GMM-based acoustic models. Kurakin et al. [37] proved it is doable that Inception v3 image classification neural network could be compromised by adversarial images.Brown et al. [21] showed by adding an universal patch to an image they could fool the image classifiers successfully. Different from them, our study targets speech recognition system.Defense of Adversarial on machine learning.