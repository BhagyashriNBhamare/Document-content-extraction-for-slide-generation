We show how Transcend can be used to identify concept drift based on two separate case studies on Android and Windows malware, raising a red flag before the model starts making consistently poor decisions due to out-of-date training. In order to build sustainable models for malware classification, it is important to identify when the model shows signs of aging whereby it fails to recognize new malware.Existing solutions [12,15,23] aim to periodically retrain the model. This is a crucial piece of evidence to assess non-stationary test objects that eventually lead to concept drift.A well known approach for qualitative assessment of decisions of a learning model is the probability of fit of test object in a candidate class. Second, and more importantly, auto-computation of thresholds to identify an aging model from an abstract assessment criteria requires a brute force search among scores for the training objects.In this work, we address both these issues by proposing both meaningful and sufficiently abstract assessment metrics as well as an assessment criteria for interpreting the metrics in an automated fashion. Thereby, Transcend can be deployed in existing detection systems with the aim of identifying aging models and ameliorating performance in the face of concept drift.In a nutshell, we make the following contributions:• We propose conformal evaluator (CE), an evaluation framework to assess the quality of machine learning tasks ( § 2). Classification is usually based on a scoring function which, given a test object z * , outputs a prediction scoreF D (l, z * ),where D is the dataset of training objects and l is a label from the set of possible object labels L.The scoring function can be used to measure the difference between a group of objects belonging to the same class (e.g., malware belonging to the same family) and a new object (i.e., a sample). The set C is a subset of the data space of object D. Due to the real-valued range of non-conformity measure, conformal evaluator can be readily used with a variety of machine learning methods such as support-vector machines, neural networks, decision trees and Bayesian prediction [20] and others that use real-valued numbers (i.e., a similarity function) to distinguish objects. There are two standard techniques to compute the p-values from K : Non-Label-Conditional (employed by decision and alpha assessments outlined in § 3.1 and § 3.2), where K is equal to D, and Label-Conditional (employed by the concept drift detection described in § 3.3), where K is the set of objects C with the same label. Let us assume that the test object z * has p-values of p 1z * , p 2 z * · · · p k z * and probability of r 1 z * , r 2 z * · · · r k z * of belonging to classes l 1 , l 2 · · · l k (which is the set of all classes in L). We further elaborate on this by training an SVM classifier with Android malware objects from the Drebin dataset [2] and by testing it using objects from a drifted dataset (the Marvin dataset [14], see § 4 for details). The threshold is applied to the testing objects; we present case studies in § 4.1, which show how to derive it from the training dataset. The techniques for interpreting these metrics are discussed in § 3. On the other hand, a low credibility value is an indicator of either z * being very different from the objects in the class chosen by the classifier or the object being poorly identified. Hence, we introduce another measure to gauge the non-performance of the classification algorithm-algorithm confidence.Algorithm Confidence. We define the algorithm confidence as 1.0 minus the maximum pvalue among all p-values except the p-value chosen by the algorithm (i.e., algorithm credibility):A Con f (z * ) = 1 − max(P(z * ) \ A Cred (z * )) where, P(z * ) = {p l i z * : l i ∈ L} P(z * )is the set of p-values associated to the possible choices for the new object z * . Finally, we note that algorithm confidence and credibility are not biased by the number of classes in a dataset as popular measures, such as precision and recall [13]. Conformal evaluator qualitatively assesses an algorithm's decision by assigning a class l ∈ L as predicted by the algorithm to each new object z * and computing its algorithm credibility and confidence.Hence, four possible scenarios unfold: (i) High algorithm confidence, high algorithm credibility-the best situation, the algorithm is able to correctly identify a sample towards one class and one class only. Researchers may gather new insights on the peculiarities of each class, which may eventually help to improve feature engineering and the algorithm's performance, overall.First, for each object z j ∈ D, where l j is z j 's true class, we compute its p-values against every possible l ∈ L. Therefore, we make the concept drift detection in Transcend parametric in two dimensions: the desired performance level (ω) and the proportion of samples in an epoch that the malware analysis team is willing to manually investigate (δ). The following discussion assumes two classes of data, malicious and benign, but it is straightforward to extend it to a multiclass scenario.We define the function f : B × M → Ω × ∆ that maps a pair of thresholds in the benign and malicious class and outputs the performance achieved and the number of decisions accepted. Let us assume f gives the output f : f (t b ,t m ) = (ω , δ ) To detect concept drift during deployment with a prespecified threshold of either ω or δ, we need to define an inverse of f which we call f −1 : Λ → B × M where Λ = Ω ∪ ∆. Moreover, a threshold built from a raw score lacks context and meaning; conversely, combining raw scores to compute p-values provides a clear statistical meaning, able of quantifying the observed drift in a normalized scale (from 0.0 to 1.0), even across different algorithms.CE can also provide quality evaluation that allows switching the underlying ML-based process to a more computationally intensive one on classes with poor confidence [4]. In addition, the non-conformity measure we 2 The work in [2] released feature sets and details on the learning algorithm, while we reached out to the authors of [1], which shared datasets and the learning algorithm's implementation with us. In absence of retraining, which requires samples relabeling, the ideal net effect would then translate to having high performance on non-drifting objects (i.e., those that fit well into the trained model), and low performance on drifting ones.In a nutshell, our experiments aim to answer the following research questions: RQ1: What insights do CE statistical metrics provide? We reimplemented Drebin and achieved results in line with those reported by Arp et al. in absence of concept drift (0.95 precision and 0.92 recall, and 0.99 precision and 0.99 recall for malicious and benign classes, respectively on hold out validation with 66-33% trainingtesting Drebin dataset split averaged on ten runs). Conversely, the p-value distribution of malicious samples (first and second columns) is skewed towards the bottom of the plot; this implies that the decision boundary is loosely defined, which may affect the classifier results in the presence of concept drift. Given label malicious: probability malicious Given label malicious: probability benign Given label benign: probability malicious Given label benign: probability benign Given label malicious: probability malicious Given label malicious: probability benign Given label benign: probability malicious Given label benign: probability benign We would like to remark that drifting objects are still given a label as the output of a classifier prediction; Transcend flags such predictions as untrustworthy, defacto limiting the mistakes the classifier would likely make in the presence of concept drift. It is important to note that Transcend plays a fundamental role in this pipeline: it identifies concept drift (and, thus, untrustworthy predictions), which gives the possibility of start reasoning on the open problems outlined above.The previous paragraphs show the flexibility of the parametric framework we outlined in § 3.3, on an arbitrary yet meaningful example, where statistical cut-off thresholds are identified based on an objective function to optimize, subject to specific constraints. Correct predictions (first and second columns), reports p-values (first column) that are are slightly higher than those corresponding to incorrect ones (second column), with a marginal yet well-marked separation as compared to the values they have for the incorrect class (third and fourth columns). Conversely, correct predictions (third and fourth columns) represent correct decisions (fourth column) and have high p-values, much higher compared to the p-values of the incorrect class (third column). Intuitively, we expect to have poor quality on all the classes of predictions in the presence of a drifting scenario: while probabilities tend to be skewed, CE's statistical metrics (p-values) seem better-suited at this task.So far, we have seen how Transcend produces statistical thresholds to detect concept drift driven by predefined goals under specific constraints. In this section we evaluate the algorithm proposed by Ahmadi et al. [1] as a solution to Kaggle's Microsoft Malware Classification Challenge; the underlying rationale is similar to that outlined in the previous section, thus, we only report insightful information and takeaways. In this evaluation, we train the classifier with seven out of eight available malware families; Trucur, the excluded family, represents our drifting testing dataset.The confusion matrix reports a perfect diagonal 7 ; in this case, the decision assessment gives us no additional information because we cannot analyze the distribution of p-values of incorrect choices. Below, we show how we identify a new family based on CE's statistical metrics.The testing samples coming from Tracur are classified as follows: 5 as Lollipop, 6 as Kelihos ver3, 358 as Vundo and 140 as Kelihos ver1. In a scenario changing gradually, we will observe an initial concept drift (as shown in the binary classification case study in § 4.1.1), characterized by a gradual decrease of the p-values for all the classes, which ends up in a situation where we have p-values very close to 0 as observed here. If the model is retrained too frequently, there will be little novelty in information obtained through retraining to enrich the model. 9 The p-value for an object o with label l is the statistical support of the null hypothesis H 0 , i.e., that o belongs to l. Transcend finds the significance level (the per-class threshold) to reject H 0 for the alternative hypothesis H a , i.e., that o does not belong l (p-values for wrong hypotheses are smaller than those for correct ones, e.g., Figure 2b). Transcend can be plugged on top of any such approach to provide a clear separation between non-drifting and drifting objects.Deo et al. [5] propose using Venn-Abers predictors for assessing the quality of binary classification tasks and identifying concept drift. CE also works on multi-class prediction tasks, while this is not currently supported by Venn-Abers predictors.Other works try to detect change point detection when the underlying distribution of data samples changes significantly, e.g., in case of evolving malware which is observed as a disruption in ex-changeability [25]. We consider this information to be important because in the case of malware evolution, malicious samples are often specially designed to be indistinguishable from benign samples, therefore they tend to get high pvalues assigned to wrong hypotheses. Fern and Dietterich 10 also show that CP is not suited for anomaly detection as it outputs a set of labels and hence needs to be modified to predict quality of predictions. For each classification task, CP builds on such pvalues to introduce credibility-the class, in a classification problem, with the highest p-value and confidencedefined as one minus the class with the second highest p-value (these metrics are different from CE metrics, see § 2.4). Our work details the CE metrics used in [4] and extend it to facilitate the identification of concept drift, thus bridging a fundamental research gap when dealing with evolving malicious software.We present two case studies as representative use cases of Transcend.