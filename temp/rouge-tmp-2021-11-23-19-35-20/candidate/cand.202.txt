A fundamental problem with this approach is that we have no way to measure these heuristics to know precisely how well they work, under which circumstances, how prone they are to evasions or to errors, and how stable they are over different versions of the OS kernel. Computer forensics is often considered an art, as the analyst proceeds by formulating hypothesis about the cause of an incident and uses inductive reasoning to reinforce or discard them based on clues and artifacts collected from the target system.The way these artifacts are extracted and analyzed is largely based on the experience and on the set of heuristics encoded into the available tools. Memory forensic tools needs to recover the high-level semantic associated with sequences of raw bytes -thus reconstructing the internal state of the operating system (OS) and its applications at the time the memory was acquired. Even worse, both the fields and the layout of these objects can change when the kernel is updated or recompiled, and the connections among them evolves very rapidly -with a considerable amount of links and pointers that change every few milliseconds.Currently memory forensics techniques rely on a large number of rules and heuristics that describe how to navigate through this giant graph of kernel data structures to locate and extract information relevant to an investigation. The resulting map of the memory is a giant network (containing over a million nodes) with a very dynamic topology that is constantly reshaped as new data structures get allocated and deallocated.Memory forensics tools adopts rules to navigate through the data structures present in a memory dump, and these rules can therefore be represented as paths in our kernel graph. We then discuss the intricacies of identifying such optimal paths by performing experiments with 85 different kernel versions and 25 individual memory snapshots acquired at regular time intervals.Building a map of the kernel memory is a very tedious and time-consuming process. Our work can help mitigating this issue by assessing how existing techniques are affected by non-atomic acquisitions, and help design new heuristics which are more robust against the presence of inconsistent information.The second category focuses instead on challenges related to memory analysis. The path contains the names of the structures and fields that need to be traversed (in square brackets when they refer to global symbols in the kernel) as well as the type of transition (→: follow a pointer reference, : visit multiple structures of the same type linked together). 28th USENIX Security Symposium 1753 A developer may favor the shortest path, as it is simpler to implement and may appear to be more robust according to the intuition that the fewer the data structures that need to be parsed, the less likely it is that something can go wrong while doing that. On the opposite case in which the memory was acquired atomically in a lab from a virtual machine used to investigate a possible rootkit, the analyst would certainly favor a different approach that traverses structures whose values cannot be tampered with by the attacker. And once this function has been chosen, it is possible to use it to compute the optimal path (and therefore the optimal exploration strategy) to traverse the kernel graph. In this paper we explore different possible scenarios by proposing several metrics to enrich the graph (more details about this process are presented in Section 5) and then use this information to evaluate existing approaches and discuss other, non-conventional solutions that can provide better guarantees for the analyst. While this was doable for old kernel versions (e.g., 2.4), it would take many weeks of tedious work to annotate a recent kernel -which today uses more than 6000 different data structures and more than a thousand instances of list_heads. Except for those, the most common type that is still not supported by our prototype is radix_tree, which however is only used 8 times in the entire kernel code base.As we will show in Section 4.6, our approach is very effective and was able to resolve the type pointed by 250 global lists and by more than 1110 unique object fields in the Linux kernel 4.8, compiled with the Ubuntu 16.04 kernel configuration. Since the code must have a way to determine to which type the target element belongs to, this pattern is only present in the form of a "root" object which is the first element of a circular list of otherwise homogeneous objects.As a consequence, these lists can only be traversed starting from their root node, as traversing the loop from an intermediary objects can result into unexpectedly reaching the root node (of a different type) when dereferencing one of the next pointers. The first soft rule checks that the number of valid pointers in a kernel object is greater or equal than the number of invalid ones (after removing null pointers and the pointers which normally point to userspace memory, such as the ones contained in struct sigaction). The second, more precise, heuristic immediately flags an object as invalid if certain conditions are not verified (such as kernel objects that contain a negative spinlock, or those with function pointers that do not point in the executable sections of the kernel). Since we are interested in using our graph to analyze and improve existing memory forensic techniques, opaque pointers play a very marginal role (if any at all) in this space. It consists of an LLVM compiler plugin to perform the points-to analysis on the kernel code at compile-time and a set of python gdb extensions that combine the information extracted in the previous step with the information provided by kernel debug symbols to identify all kernel objects contained in a memory snapshot acquired using the QEMU emulator. The kernel exploration routine starts by loading a QEMU snapshot, parsing the type graph, and appending the global object symbols to an internal worklist. However, without any further information, the only way we can compare two paths on the graph is by looking at their length, computed by counting either the total number of nodes or the total number of unique structures that need to be traversed. And since the idea of having an absolute metric is For our experiments we decided to investigate and add to our graph three numerical and two boolean weights, related to the atomicity, stability, generality, reliability, and consistency of a path. For instance, if a path visits three consecutive nodes (A, B, and C) and the difference between the acquisition time of the pointer in A and the content of B was 7 seconds and the difference between B and C was 3, the CTG would be 10 seconds. For example, let suppose our graph analysis identifies two paths to reach a certain target structure C namely {A → B → C} and {A → X → Y → B → C} (for simplicity we ignore the name of the pointers). By computing a heat-map of the stability of each edge (extracted by processing a number of consecutive snapshots), this weight can provide a valuable information on how the kernel map evolves over time, on which paths are more stable, and on which are instead more ephemeral and may only exists for short periods of time.We measure Stability by computing the Minimum Constant Time (MCT) of all links in a path. For example, we saw an increase of more than 60% of both nodes and edges between the graph built at t = 0 and the one built at t = 700. Therefore, it would be interesting to compute analysis paths that traverse structures which change very rarely across different distributions, kernel versions, and enabled kernel options.For this reason we downloaded 85 kernels from the Ubuntu repository, spanning from version 4.4.0-21 to 4.15.0-20. But here we are instead interested in the reliability of a path, i.e., not in the fact that individual fields (such as a file name) can be modified, but whether an attacker can tamper with the edges that need to be traversed to prevent a certain heuristic to reach its destination (to the best of our knowledge, this problem has never been addressed in the literature). We now discuss how our graph-based framework can be used in different scenarios, in which we investigate existing techniques used by Volatility [33], we discuss the intricacies of computing optimal paths, and we discover new solutions to reach all processes running in a system. As we already introduced in Section 4.6, by averaging over the 25 graphs we created, more than 96% of the nodes used by the heuristics belong to a single giant strongly connected component that contains on average 53% of all the nodes in the graph. This physical distribution is also very important for the third scenario presented in Section 6.3, where we will encounter heuristics that need to hop back and forth from the three clusters, significantly impacting the atomicity metrics.The second surprising result of this first scenario is the fact that the Kernel Counters (KC) of six plugins never changed across all the different kernel versions we used in our analysis. Our experiment suggests that, at least for locating certain information, a generic structure layout can be used across almost 100 kernel versions, released as far as 2 years apart.Another important propriety we evaluated in this first scenario is the consistency of the selected techniques. The affected plugins interest different parts of the kernel, but they can be divided to three distinct categories: Memory ( linux_proc_maps, linux_proc_maps_rb), File system (linux_check_fop, linux_find_file, linux_lsof, linux_mount) and Process (linux_pidhashtable)In the Memory category we found respectively 33 inconsistencies that affected the connections among vm_area_struct of a process, which are kept both in a linked list and in a red-black tree. The filesystem category included 40 unique inconsistencies in the hierarchy of dentries (fields d_subdirs and d_child) 53 in the mapping from a dentry to an inode (field d_inode). For example, in the case of inconsistent array of opened files for the systemd process the alternative path -which traversed 11 additional nodes -was able to reach the target file by first locating the task_struct of the same process, then accessing its corresponding files_struct and from here reaching the file via the fd_array field (an array only used when the process opens less than 64 files). Unfortunately, many distributions, such as Ubuntu and Debian, ships by default with the SLUB allocator, which is not supported by Volatility and which does not keep track of full slabs -thus making this technique not applicable anymore.The main reason for looking for alternative solutions is that previous research already pointed out that rootkits are already capable of removing a process from the process list, but also to unlink a process from the pid hashtable [19,26,27] thus leaving the forensic analyst without a reliable method to list processes. In fact, since we are looking for techniques to list all (or a part of) the running processes, this is equivalent to a collection of, possibly not homogeneous, paths. While not useless per se, our goal is to find new solutions and not variations of the existing ones.By only considering the shortest paths from every root node to every task structure, our system found more than 100 million distinct paths, generated from a set of more than 966,000 sequences of vertices. Finally, we removed templates that were subset of other templates, resulting in a final set of 4067 path templates.By manually exploring these options, we soon realized that they belong to only four main families, depending on the kernel subsystem they live in. However, all these edges are very stable and in only one case (for the css_set_table) the value of this first connection ever changed during our memory acquisition.The memory-based heuristics walked a red-black tree (i_mmap) that is very ephemeral and, while exploring it, we found more than 30 edges that could be inconsistent if the memory dump is not taken atomically. However, new processes were all appended to the tail of the process list without altering the intermediate nodes.To test the Reliability of the heuristics we wrote a kernel module that tries to hide an userspace process by unlinking it from the path required by each heuristic. We then proceeded by unlinking the worker from the linked list rooted at worker_pool.workers.In all the cases our program continued to run without observable side-effects -showing that each path we listed so far can be tampered with by a properly written rootkit. Since the clusters (C1, C2 and C3) are located far apart in memory (and therefore they can be acquired far apart in time), whenever a heuristic moves from one structure contained in one cluster to another contained in a different one, it needs to take a "jump" with associated a considerable time gap. But in this third case it might be possible to use our graph to find an alternative path that is fully contained in the same cluster.An example of each of these three cases is shown in Table 3, along with the metrics computed on the Volatility heuristic and those computed on the optimal paths extracted from our graph. This way of modeling the problem opens the door to a multitude of different possibilities to evaluate and compare existing techniques, design algorithms to compute new alternative solutions, validate the consistency of kernel structures, or propose heuristics customized to different experiments setup and acquired dump.We tried to discuss some of these opportunities through our experiments, but we are aware that many questions are still open and new research is needed to shed light to each individual use case. In fact, the exact memory layout when the snapshot is acquired may affect the metrics associated to different links (e.g., one path may be optimal for one dump but poor in another). More research is needed to fill this gap and enable to compute the reliability of a large amount of links among kernel objects.Finally, to be useful in practice, our prototype should be applied to a larger number of memory dumps taken from different systems. When these hooks are triggered, the hypervisor notes the address and the size of the allocated kernel object and the call site. The latter information is used, along with the result of an offline static analysis of the kernel source code, to determine the type of the allocated object. For this reasons a kernel graph built using the approach adopted by SigGraph would only retrieve a partial view of the entire memory graph.Another work focused on signatures to match kernel objects in memory was done by Dolan-Gavitt et al in 2009 [10]. The main insight of this work is that while kernel rootkits can modify certain fields of a structure -i.e. to unlink the malicious process from the process list -other fields (called invariants) can not be tampered without stopping the malicious behavior or causing a kernel crash. The direct results of these two phases is that highly accessed field which result in a crash when fuzzed are good candidates to be used as strong signatures. As we discussed in Section 4 the Linux kernel uses a large amount of ambiguous pointers, thus making the manual annotation approach not feasible anymore.While more focused on kernel integrity checks, OSck [16] uses information from the kernel memory allocator (slab) to correctly label kernel address with their type. Moreover, only a predefined subset of kernel objects are allocated in custom slabs, while the vast majority is sorted in generic slabs based on their size.Gu et al. [13] presented OS-Sommelier + , a series of tech-niques to fingerprint an operating system from a memory snapshot. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 771844 -BitCrumbs).