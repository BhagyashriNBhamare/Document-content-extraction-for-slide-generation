From image and face recognition, to self-driving cars, knowledge extraction and retrieval, and natural language processing and translation, deep learning has produced game-changing applications in every field it has touched.While advances in deep learning seem to arrive on a daily basis, one constraint has remained: deep learning can only build accurate models by training using large datasets. For example, Google's InceptionV3 model is based on a sophisticated architecture with 48 layers, trained on ∼1.28M labeled images over a period of 2 weeks on 8 GPUs.The prevailing consensus is to address the data and training resource problem using transfer learning, where a small number of highly tuned and complex centralized models are shared with the general community, and individual users or companies further customize the model for a given application with additional training. Today, transfer learning is recommended by most major deep learning frameworks, including Google Cloud ML, Microsoft Cognitive Toolkit, and PyTorch from Facebook.Despite its appeal as a solution to the data scarcity problem, the centralized nature of transfer learning creates a more attractive and vulnerable target for attackers. • We explore and develop multiple defense techniques against attacks on transfer learning models, including defenses that alter the student model training process, that alter inputs prior to classification, and techniques that introduce redundancy using multiple models.Transfer learning is a powerful approach that addresses one of the fundamental challenges facing the widespread deployment of deep learning. Transfer learning enables organizations without access to massive datasets or GPU clusters to quickly build accurate models customized to their application context.How Transfer Learning Works. Then the student model is trained using its own dataset, while the first K layers are "frozen", i.e. their weights are fixed, and only weights in the last N − K layers are updated.The first K layers (referred to as shallow layers) are frozen during training because outputs of those layers already represent meaningful features for the student task. Most blackbox attacks either use queries to test intermediate adversarial samples and improve iteratively [55], or try to reverse-engineer decision boundaries of the DNN and build a replica, which can be used to craft adversarial samples [46]. A targeted attack aims to misclassify the adversarial image into a specific target class, whereas a non-targeted attack focuses on triggering misclassification into any class other than the real class. We consider a given attacker looking to trigger a misclassification from a Student model S, which has been customized through transfer learning from a Teacher model T . We also assume the attacker does not know the Student training dataset, and can use only limited queries (e.g., 1) to S. Apart from a single adversarial sample to trigger misclassification, we expect no additional queries to be made during the pre-attack process. Using the Teacher model, attacker computes perturbations that mimic the internal representation of the target image at layer K. Internal representation is captured by passing the target image as input to the Teacher, and using the values of the corresponding neuron outputs at layer K.Our key insight: is that (in feedforward networks) since each layer can only observe what is passed on from the previous layer, if our adversarial sample's internal representation at layer K perfectly matches that of the target image, it must be misclassified into the same label as the target image, regardless of the weights of any layers that follow K.This means that in the common case of feature extractor training, if we can mimic a target in the Teacher model, then misclassification will occur regardless of how much the Student model trains with local data. In our case, it is hard to estimate such a direction without having a target image in hand, as we rely on mimicking hidden representations. The optimization problem is formulated as follows.min min i∈I {D(T K (x ′ s ), T K (x ti ))} s.t. d(x ′ s , x s ) < P(2)Measuring Adversarial Perturbations.As men- tioned before, d(x ′ s , x s )is the distance function used to measure the amount of perturbation added to the image. To solve the optimization in Equation 1, we use the penalty method [43] to reformulate the optimization as follows.min D(T K (x ′ s ), T K (x t )) + λ ·(max(d(x ′ s , x s )−P, 0)) 2Here λ is the penalty coefficient that controls the tightness of the privacy budget constraint. The Student is trained on the VGG Flowers dataset [9] containing 6, 149 images from 102 classes, and comes with a testing dataset of 1, 020 images.These tasks represent typical scenarios users may face during transfer learning. Based on these results, we build the Student model for each task using the transfer method that achieves the highest classification accuracy (marked in bold in Table 1). Success for non-targeted attack is measured as the percentage of 1K source images that are successfully misclassified into any other arbitrary class.For each source and target image pair, we compute the adversarial samples by running the Adadelta optimizer over 2, 000 iterations with a learning rate of 1. For all the Teacher models considered in our experiments, the entire optimization process for a single image pair takes roughly 2 minutes on an NVIDIA Titan Xp GPU.We implement the attack using Keras [19] and TensorFlow [12], leveraging open-source implementations of misclassification attacks provided by prior works [44,17]. Figure 3 includes 6 randomly selected successful targeted attack samples for interested readers to examine.It should be noted that an attacker could improve attack success by carefully selecting a source image similar to a target image. For a fair comparison, we choose an L 2 distance budget that produces a targeted attack success rate similar to using DSSIM with a budget of 0.003. These values are empirically derived by the authors to produce unnoticeable image perturbations.Overall, the attack is effective in Iris, with a targeted attack success rate of 95.9% and non-targeted success rate of 100%. On the other hand, the attack becomes less effective on Traffic Sign recognition, where the success rate of targeted and non-targeted attacks are 43.7%, and 95.35%, respectively. These results suggest that the attack effectiveness is strongly correlated with the transfer method: our attack is highly effective for Deep-layer Feature Extractor, but ineffective for Full Model Fine-tuning. Figure 5(a) and Figure 5(b) show targeted and non-targeted success rates when attacking different layers.For both Face and Iris, the attack is the most effective when targeting precisely the N − 1 th (15th) layer, which is as expected since both use Deep-layer Feature Extractor. Therefore, given a fixed perturbation budget, the error in mimicking internal representations is much higher at shallow layers, resulting in lower attack success rates.An unexpected result is that for Iris, the success rate for non-targeted attacks remains close to 100% regardless of the attack layer choice. Results in Fig- ure 5(c) show that the attack success rates peak at precisely the 10 th layer, where success rate for targeted attack is 43.7% and 95.35% for non-targeted attack. In this case, the attacker should focus on the N − 1 th layer to achieve the optimal attack performance.If the Student is not using Deep-layer Feature Extractor, the attacker can try to find the optimal attack layer by iteratively targeting different layers, starting from the deepest layer. For example, for Face recognition, when reducing the training dataset from 90 images per class to 50 per class, pushing back by 2 layers (i.e. transfer at layer 13) reduces the model classification accuracy to 19.1%. Thus another potential attack on transfer learning is to use existing white-box attacks on the Teacher to craft adversarial samples, which are then transferred to the Student. Since Teacher and Student models have different class labels, we can only perform non-targeted attacks.Our results show that the resulting attack is ineffective for all four tasks: only < 0.3% adversarial samples trigger misclassification in the Student models. The Student model has a different classification layer (and hence decision boundary) than the Teacher, so adversarial samples computed using decision boundary analysis (based on classification layer) of the Teacher model fail on the Student model. We address this challenge by designing a fingerprinting approach that feeds a few query images on the student model to identify the teacher model, allowing us to effectively attack the student models produced by today's deep learning services. In the following, we show that such fingerprinting method is highly effective when the student model is generated via Deep-layer Feature Extractor.Consider the last layer of a student model (trained using Deep-layer Feature Extractor), which is a dense layer for classification. The prediction result (before softmax) of an input image x can be expressed as,S(x) = W N × T N−1 (x) + B N(3)where W N is the weight matrix of the dense layer, B N is the bias vector, and T N−1 (.) Our technique accurately distinguishes between these two versions, with a Gini coefficient < 0.075 when there is a match, and > 0.751 otherwise.Overall, the above results confirm that our fingerprinting method can identify the Teacher model using a small set of queries. On the other hand, when all the fingerprinting images lead to large Gini coefficient values, it means that either the Teacher model is unknown (not in the candidate pool), or the student model is produced by a transfer method other than Deep-layer Feature Extractor. Specifically, the tutorial suggests Deep-layer Feature Extractor as the default transfer learning method, and the provided sample code does not offer control parameters or guidelines to use other transfer approaches or Teacher models (one has to modify the code to do so). Following this process, we use VGG16 as the Teacher and Deep-layer Feature Extractor to train a new Student model using the 102-class VGG flower dataset (the example dataset used in tutorial). Again, we were able to launch the misclassification attack on the Student model: our fingerprinting method successfully identifies the Teacher model (with a Gini coefficient of 0.0045), and the attack success rate is 99.4% when P = 0.003. We repeat this process 3 times for each image and use the majority vote as the final prediction result 7 , or a random result if all 3 predictions are different.We test this defense on all three tasks, Face, Iris, and Traffic Sign, by applying Dropout on test images as well as targeted and non-targeted adversarial samples 8 . In this section, we present a scheme to modify the Student layers (i.e. weights), without significantly impacting classification accuracy.We start with a Student model trained using Deeplayer Feature Extractor or Mid-layer Feature Extractor 9 . Our goal is to update layer weights and identify a new local optimum that provides comparable (or better) classification performance, and also be distant enough (on the error surface) to increase the dissimilarity between the Student and Teacher.To find such a new local optimum, we unfreeze all layers of Student and retrain the model using the same Student training dataset, but with an updated loss function formulated in the following way. Then our objective is the following,min CrossEntropy(Y true ,Y pred ) s.t. ∑ x∈X train |W s | • (T K (x) − S K (x)) 2 > D th (4)where • is element-wise multiplication.Here, we still want to minimize the classification loss, formulated as cross entropy loss over the prediction results. Sensitivity of the Iris model actually means classification accuracy increased from 88.27% to 91.0% (retraining found a better local optimum), while targeted attack success dropped from 100% to 12.6%. We identify and experimentally validate a general attack on black-box Student models leveraging knowledge of white-box Teacher models, and show that it can be successful in identifying and exploiting Teacher models in the wild. In advance of the public release of this paper, we reached out to machine learning and security researchers at Google, Microsoft and Facebook, and shared our findings with them.Definition of DSSIM DSSIM (Structural Dissimilarity) is a distance metric derived from SSIM (Structural SIMilarity).