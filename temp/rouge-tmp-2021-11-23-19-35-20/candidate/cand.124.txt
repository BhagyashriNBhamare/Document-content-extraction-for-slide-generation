Users in various web and mobile applications are vulnerable to attribute inference attacks, in which an attacker leverages a machine learning classifier to infer a target user's private attributes (e.g., location, sexual orientation, political view) from its public data (e.g., rating scores, page likes). In Phase I, for each value of the attribute, we find a minimum noise such that if we add the noise to the user's public data, then the at-tacker's classifier is very likely to infer the attribute value for the user. The computation cost to solve the formulated optimization problem is exponential to the dimensionality of the public data vector and the public data vector often has high dimensionality in practice.To address the computational challenges, several studies [9,[17][18][19] proposed to trade theoretical privacy guarantees for computational tractability. Achieving this goal relies on estimating the attacker's accuracy at inferring the user's private attribute when a particular noise is added, which is challenging because 1) the defender does not know the user's true attribute value (we consider this threat model to avoid single-point failure introduced by a compromised defender), and 2) the defender does not know the attacker's classifier, since there are many possible choices for the classifier. To address the challenge, AttriGuard works in two phases.In Phase I, for each possible attribute value, the defender finds a minimum noise such that if we add the noise to the user's public data, then the attacker's classifier predicts the attribute value for the user. Since both the attacker's classifier and the defender's classifier model the relationships between users' public data and private attributes and the two classifiers could have similar classification boundaries, the noise optimized to evade the defender's classifier is very likely to also evade the attacker's classifier. In particular, evasion attacks can play an important role at defending against attribute inference attacks.In Phase II, the defender randomly picks an attribute value according to a probability distribution q over the possible attribute values and adds the corresponding noise found in Phase I to the user's public data. The classifier is then used to infer attributes of target users who do not disclose them.In mobile apps, Michalevsky et al. [10] showed that an attacker can use machine learning to infer a user's location based on the user's smartphone's aggregate power consumption (i.e., "public data" in our terminology). Since quantization is used, QPM has no theoretical privacy guarantee, i.e., QPM does not necessarily defend against the optimal attribute inference attacks, but QPM makes it tractable to solve the defense problem in practice.Other computationally tractable methods [9,18] leveraged heuristic correlations between the entries of the public data vector and attribute values. For each attribute value i, they order the items into a list L i according to the correlations between the items and the attribute values other than i. Specifically, for each attribute value i, they learn a logistic regression classifier via using the public data vector as a feature vector; and the negative coefficient of an item in the logistic regression classifier is treated as its correlation with the attribute values other than i. Chen et al. [18] proposed ChiSquare, which computed correlations between items and attribute values based on chi-square statistics.As we elaborated in the Introduction section, these methods have one or two limitations: 1) they incur large utility loss, and 2) some of them require the defender to have direct access to users' private attribute values.Local differential privacy (LDP): LDP [33][34][35][36][37][38][39][40] is a technique based on ε-differential privacy [41] to protect privacy of an individual user's data record, i.e., public data in our problem. In recommender systems, this policy means that the defender can only add new rating scores for a user; when the public data represent page likes in social media, this policy means that the defender can only add new page likes for a user. Via collecting data from such users, the attacker can learn the machine learning classifier.We denote the attacker's machine learning classifier as C a , and C a (x) ∈ {1, 2, · · · , m} is the predicted attribute value for the user whose public data is x. Once a user gives privileges to the defender, the defender can modify its public data, e.g., the defender can add page likes on Facebook or rate new items in a recommender system on behalf of the user.The defender has access to the user's true public data x. Addressing the second challenge: To address the second challenge, we consider an alternative goal, which aims to find a mechanism M such that the output probability distribution q is the closest to a target probability distribution p with a utility-loss budget, where p is selected by the defender. However, using such servicedependent utility loss makes the formulated optimization problem computationally intractable.Therefore, we aim to use utility-loss metrics that make our formulated optimization problems tractable but can still well approximate the utility loss for different services. The distance metric can also be L 2 norm of the noise, which considers the magnitude of the modified rating scores in the context of recommender systems.Attribute-inference-attack defense problem: With a quantifiable defender's goal and utility loss, we can formally define the problem of defending against attribute inference attacks. The defender specifies a target probability distribution p, learns a classifier C, and finds a mechanism M * , which adds noise to the user's public data such that the user's utility loss is within the budget while the output probability distribution q of the classifier C is closest to the target probability distribution p. Formally, we have:Definition 1 Given a noise-type-policy P, an utilityloss budget β , a target probability distribution p, and a classifier C, the defender aims to find a mechanism M * via solving the following optimization problem:M * = argmin M KL(p||q) subject to E(d(x, x + r)) ≤ β ,(1)where the probability distribution q depends on the classifier C and the mechanism M . The major challenge to solve the optimization problem in Equation 1 is that the number of parameters of the mechanism M , which maps a given vector to another vector probabilistically, is exponential to the dimensionality of the public data vector. Our intuition is that, although the noise space is large, we can categorize them into m groups depending on the defender's classifier's inference. Formally, we model finding such r i as solving the following optimization problem:r i = argmin r ||r|| 0 subject to C(x + r) = i.(2)Our formulation of finding r i is closely related to adversarial machine learning. Therefore, we can transform the optimization problem in Equation 1 to the following optimization problem:M * = argmin M KL(p||M ) subject to m ∑ i=1 M i ||r i || 0 ≤ β M i > 0, ∀i ∈ {1, 2, · · · , m} m ∑ i=1 M i = 1,(3)where we use the L 0 norm of the noise as the utility-loss metric d(x, x + r) in Equation 1. First, according to Equation If we do not have the utility-loss constraint ∑ m i=1 M i ||r i || 0 ≤ β in the optimization problem in Equation 3, then the mechanism M * = p reaches the minimum KL divergence KL(p||M ), where p is the target probability distribution selected by the defender. In total, we have 16,238 users, and each user rated 23.2 apps on average.We represent a user's public data as a 10,000-dimension vector x, where each entry corresponds to an app. Note that we normalize each entry of a user's public data vector (i.e., review data vector) to be in [0,1], i.e., each entry is 0, 0.2, 0.4, 0.6, 0.8, or 1.0. Since the defender does not know the attacker's classifier, we evaluate the effectiveness of AttriGuard against various attribute inference attacks as follows (we use a suffix "-A" to indicate the classifiers are used by the attacker):Baseline attack (BA-A): In this baseline attack, the attacker computes the most popular city among the users in the training dataset. The inference accuracy of this baseline attack will not be changed by defenses that add noise to the testing users.Logistic regression (LR-A): In this attack, the attacker uses a multi-class logistic regression classifier to perform attribute inference attacks. Random forest (RF-A): In this attack, the attacker uses a random forest classifier to perform attacks.Neural network (NN-A): We consider the attacker uses a three-layer (i.e., input layer, hidden layer, and output layer) fully connected neural network to perform attacks. However, exploring the best NN-A is not the focus of our work.Robust classifiers: adversarial training (AT-A), defensive distillation (DD-A), and region-based classification (RC-A): Since our defense AttriGuard leverages evasion attacks to find the noise, an attacker could leverage classifiers that are more robust to evasion attacks, based on the knowledge of our defense. In region-based classification, for each testing user with a certain review data vector, an attacker randomly samples n data points from a hypercube centered at the review data vector; applies the NN-A classifier to predict the attribute for each sampled data point; and the attacker takes a majority vote among the sampled data points to infer the user's attribute. We require differentiable classifiers because our evasion attack algorithm PANDA in Phase I is applicable to differentiable classifiers. Comparing PANDA with existing evasion attack methods: We compare PANDA with the following evasion attack methods at finding the noise r i in Phase I: Fast Gradient Sign Method (FGSM) [22], Jacobian-based Saliency Map Attack (JSMA) [23], and Carlini and Wagner Attack (CW) [25]. For each method, a test user's success rate is the fraction of cities for which the method can successfully find a r i to make the classifier infer the ith city for the test user, and a test user's running time is the time required for the method to find r i for all cities. Considering the tradeoffs between the added noise, success rate, and running time, we recommend to use PANDA for finding noise in Phase I of AttriGuard.We note that JSMA and CW have similar noise for the LR-D classifier, and CW even has larger noise than JSMA for certain cities for the NN-D classifier. We suspect the reason is that our results are on review data, while their results are about image data.Effectiveness of AttriGuard: Figure 4 shows the inference accuracy of various attribute inference attacks as the utility-loss budget increases, where the defender's classifier is LR-D. Specifically, the noise optimized based on the neural network classifier NN-D is more likely to transfer to the neural network classifier NN-A than the logistic regression classifier LR-A. A user often reviews a very small fraction of apps (e.g., 0.23% of apps on average in our dataset), so Add New is more flexible than Modify Exist, making Add New outperform Modify Exist.Comparing AttriGuard with existing defense methods: Figure 9 compares AttriGuard with existing defense methods developed by different research communities: BlurMe [9], ChiSquare [18], Quantization Probabilistic Mapping (QPM) [19], and Local Differential Privacy-Succinct Histogram (LDP-SH) [36]. For a method and a given parameter value, the method adds noise to users' public data, and we can obtain a pair (utility loss, inference accuracy), where the utility loss and inference accuracy are averaged over all test users. The recommendation precision for the entire recommender system is the recommendation precision averaged over all users.For each compared defense method, we use the defense method to add noise to the testing users, where the noise level is selected such that an attacker's inference accuracy is close to 0.1 (using the results in Fig- ure 9). We believe it is an interesting future work to study better approximate solutions to the game-theoretic optimization problems, e.g., the one in Equation 19 in Appendix A. Detecting noise: An attacker could first detect the noise added by AttriGuard and then perform attribute inference attacks. Our empirical results on a realworld dataset demonstrate that 1) we can defend against attribute inference attacks with a small utility loss, 2) adversarial machine learning can play an important role at privacy protection, and 3) our defense significantly outperforms existing defenses.Interesting directions for future work include 1) studying the possibility of detecting the added noise both theoretically and empirically, 2) designing better approximate solutions to the game-theoretic optimization problems, and 3) generalizing AttriGuard to dynamic and non-relational public data, e.g., social graphs. The probabilistic mapping f is essentially a matrix, whose number of rows and number of columns is the domain size of the public data vector x. Suppose a user's true private attribute value is s and an attacker infers the user's private attribute value to bê s. After observing a noisy public data vector x , the attacker can compute a posterior probability distribution of the private attribute s as follows:Pr(s|x ) = Pr(s, x ) Pr(x )= ∑ x Pr(s, x) f (x |x) Pr(x )Suppose the attacker infers the private attribute to bêbê s. Then, the conditional expected privacy loss is We define y x = maxˆsmaxˆ maxˆs ∑ s ∑ x Pr(s, x) f (x |x)d p (s, ˆ s).