We used two datasets to evaluate its performance: (1) 255,173 voice samples generated with 120 participants, 15 playback devices and 12 recording devices, and (2) 18,030 publicly available voice samples generated with 42 participants , 26 playback devices and 25 recording devices. When combined with a Gaussian Mixture Model that uses Mel-frequency cepstral coefficients (MFCC) as classification features -MFCC is already being extracted and used as the main feature in speech recognition services-Void achieves 8.7% error rate on the public dataset. However, recent studies [11,12,23] demonstrated that voice assistants are vulnerable to various forms of voice presentation attacks including "voice replay attacks" (attackers simply record victims' use of voice assistants and replay them) and "voice synthesis attacks" (attackers train victims' voice biometric models and create new commands). The best known solution from an online replay attack detection competition called "2017 ASVspoof Challenge" [7] is highly accurate, achieving about 6.7% equal error rate (EER) -but it is computationally expensive and complex: two deep learning models (LCNN and CNN with RNN) and one SVM-based classification model were all used together to achieve high accuracy. The second best solution achieved 12.3% EER using an ensemble of 5 different classification models and multiple classification features: Constant Q Cepstral Coefficients (CQCC), Perceptual Linear Prediction (PLP), and Mel Frequency Cepstral Coefficients (MFCC) features were all used. While performing this replay attack, some distortions may be added to the victim's original sound while being recorded with the microphone on the attacker's device, and also while being replayed through the in-built speaker due to hardware imperfections. On the other hand, the spectrogram of a phrase replayed through iPhone 6s Plus in-built speaker (see Figure 3) shows some uniformity -spectrum spread is shown in the power distributions between 1 and 5kHz. To show the difference between Figure 2 and 3 quantitatively, we added quadratic fitting curves on them and computed Root Mean Square Error (RMSE) separately.Our experimentation with 11 in-built smartphone speakers showed similar behaviors in their spectral power distributions; i.e., power decreased gradually across frequencies and did not decay exponentially. However, in the loudspeaker case, the cumulative distribution increases almost linearly, and 70% of the total power lies within the frequency range of about 4kHz. Thus, distortion-induced power patterns (e.g., the number of visible power peaks, their corresponding frequencies, and standard deviations of power peaks sizes) in low frequencies could be effective in detecting standalone speakers that produce higher quality sounds. We also use higher order polynomials to accurately model spectral power shapes, and use these models to identify more fine-grained differences in spectral power patterns between live-human and replayed samples (see Figure 5). We also provide power patterns for different loudspeakers in Appendix B. Because the decay and peak patterns discussed in Sections 4.1 and 4.2 mainly look at specific frequency ranges. In the first signal transformation stage, given an input voice signal Voice in , short-time Fourier transform (STFT) is computed (Step 1 of Algorithm 1). From the computed STFT, cumulative spectral power per frequency (S pow ) is computed (Step 2 of Algorithm 1). The vector S pow computed from the first stage is used as the input to the second stage to extract the classification features.Void sequentially computes the following four types of features: (1) low frequencies power features (FV LFP ), (2) signal power linearity degree features (FV LDF ), (3) higher power frequencies features (FV HPF ), and (4) LPCC features for audio signals (FV LPC ). In the second stage of Algorithm 1, we first divide the signal S pow into k short segments of equal-length according to the given window size W (see Step 3). Input: < pow > Output: FV LDF ={ρ, q}.1: Normalize < pow > with sum(< pow >) to obtain < pow > normal 2: Accumulate the values of < pow > normal to obtain pow cdf 3: Compute the correlation coefficients of pow cdf and store the results as ρ 4: Compute the quadratic coefficients of pow cdf and store the results as q Algorithm 2 describes the procedure for computing the linearity degree of < pow >. In this step, < pow > normal is accumulated in a step-wise fashion.For the linearity degree of pow cdf , we compute the following two features (see Step 3 and 4): correlation coefficients ρ and quadratic curve fitting coefficients q of pow cdf (see Appendix C). To construct FV HPF , we first count the number of peaks in S peak and store the number of counted peaks as N peak (see Step 5); the mean and standard deviation of locations of the discovered peaks are sequentially computed and stored as µ peaks and σ peaks , respectively (see Step 6 and 7); and we determine the 6 order of the polynomial to be fitted to FV LFP and use the polynomial coefficients as P est (see Step 8). Instead of manually constructing detection rules, we opted to utilize machine learning-based classifiers as follows: There were 17 (with the scores above 1.0) noticeably important features (shown by the peaks) from the 4 feature groups visualized in red-dashed rectangles. To generate a comprehensive replay attack dataset, we replayed all 10,209 human voice samples in an open lab environment through a mixed set of speakers and recorded them under varying conditions as described below:• Background noise: The open lab environment we used to record all attack samples is collaboratively used by about 100 employees at a large IT company. • Distances between attacking devices and target devices: Distances between target devices (used to record voice samples) and attack devices (used to play recorded voice samples) could affect the detection rate because spectral power features could be affected with distance. • Playback speaker types: We used 11 different types of in-built speakers including smartphones and a smart TV, and four different types of standalone speakers to replay recorded voice samples (see Appendix G). As for the first attack dataset that we collected, to reduce any bias that might be associated with the process of randomly splitting the datasets into training and testing sets, we used 10 fold cross-validation: the training samples were partitioned into 10 equal-sized sets with similar class distributions. We note that this EER result (11.6%) would rank Void as the second best solution (EER 12.34%) in the ASVspoof 2017 competition [10]. To compare Void against existing solutions from the ASVspoof competition with respect to latency, space complexity, and accuracy, we implemented (used existing code if available) the two classification models described below, and evaluated them using the ASVspoof evaluation set. Their model consists of 5 convolutional layers, 4 network in network layers, 10 max-feature-map layers, 4 max-pooling layers, and 2 fully connected layers as described in [30]. We compare Void against CQCC-GMM and STFT-LCNN with respect to the latency and model complexity (see Table 5). Its average testing time was around 0.03 seconds.We used a logistic regression model to compute the optimal weight for each model: 0.7 for Void, and 0.3 for MFCC-GMM. In this section, we analyze the effects of four key variancesdistances between target devices and attack devices, human gender, loudspeaker types and cross data training -on the performance of Void. Ten fold crossvalidation was used to evaluate Void classifiers.Again, gender variances did not really influence Void's performance (see Table 6): accuracy and F1 scores are greater than 98%, and EER is below 1%. As for the ASVspoof dataset, it showed varying performance against high quality loudspeakers: the detection accuracy for Dynaudio BM5A and Behringer Truth B2030A studio monitor were 92.7% and 95.1%, respectively; the detection accuracy dropped significantly to 81.1% against Genelec 6010A studio monitor. After removing improperly recorded samples, we were left with 119,996 replay attack samples with a huge variety of background noises and situations.We evaluated the performance of Void against those unseen replay attack samples. To measure the attack detection rates, we trained a Void classifier with all of our own replay attack and human voice datasets (see Section 6), and used that classifier to classify given set of attack samples described below. To test Void's performance against voice synthesis attack, we used open source voice modeling tools called "Tacotron" [1] and "Deepvoice 2" [2] to train a user voice model with 13,100 publicly available voice samples (https://keithito.com/ LJ-Speech-Dataset/). We then used the trained model to generate 1,300 synthesis voice attack samples by feeding in Bixby commands as text inputs.After attack data generation, we played those synthesis attack samples through four different speakers: Galaxy S8, V-MODA, Logitech 2.1 Ch., and Yamaha 5.1 Ch. After removing samples that were not properly recorded, we were left with a final set of 15,446 synthesis attack samples and tested them on Void.Void achieved 90.2% attack detection rate against this set, demonstrating its potential in detecting voice synthesis attacks. An attacker's goal would be to artificially create attack commands that show power patterns similar to those of live-human voices. audacityteam.org/) to generate audio samples that mimic decay and peak patterns in spectral power like live human voices under the following two strategies.The first attack strategy involved removing background noises from audio samples because the samples were originally recorded with various background noises present (e.g., noises generated from fans, refrigerators, or computers). Since Void is not responsible for classifying commands that are not properly recognized by voice assistants, we ran all recorded samples through a speech to text translation engine ("Google Speech Recognition"), and removed commands that it failed to recognize -we were left with 3,600 attack samples to test with.Among those samples, Void correctly detected 3,536 attack samples, achieving a detection accuracy of 98.2%. Consequently, most of the submitted solutions [7] used multiple deep learning models (as an ensemble solution) and heavy classification features to minimize the EERs -such solutions sit uneasily with real-world near-zerosecond latency and model complexity requirements.As shown from our latency results (see Section 7.4), Void is much lighter, faster, and simpler than other top performing solutions as well as the baseline CQCC-GMM solution -many ensemble solutions used CQCC-GMM as the baseline model. Further, we demonstrated 0.3% EER against our own dataset.Being mindful of how light Void is, another possible deployment scenario would involve deploying the Void classifier at the device level: when a user submits a voice command, the voice assistant running on the user's device would first make a voice liveness decision, and drop attack-like commands immediately. With this type of on-device deployment, we would not introduce any new detection (computational) burden on servers. Further, Kwak et al. [33] shows that about 90% of existing mobile voice assistant users use less than 20 commands per month -for those light users, there will only be about five falsely rejected commands every 5 months of use.However, the incidence level would be quite different when voice assistants are used in homes, e.g., through a smart speaker. Zhang et al. [3] also proposed articulatory gesture-based liveness detection (analyzing precise articulatory movements like lip and tongue movements); their approaches, however, are only applicable to scenarios where a user is physically speaking near a smartphone's microphone. According to the study findings, the equal error rates (EER) varied from 6.7% to 45.6% [30] -most solutions used an ensemble approach, and used CQCC-GMM as a baseline model, which alone is complex and uses about 14,000 features. Compared with existing methods using multiple, heavy classification models, Void runs on a single efficient classification model with 97 features only, and does not require any additional hardware.Our experiments, conducted on two large datasets collected under numerous varying conditions (demographics, speaker/microphone types, and background noises), showed that Void can achieve 0.3% EER on our own dataset, and 11.6% EER on the ASVspoof evaluation set. As shown in Figure 10, three signal power features, µ peak , ρ and q, look noticeably different, suggesting that they could be effective in classifying live-human voices and in-built speakers (those features are explained in Section 5.3). Given the vector < pow > of power density values and the peak selection threshold ω, we compute the feature vector (FV HPF ) to capture the dynamic characteristics of spectral power in higher frequencies (see Table 12).