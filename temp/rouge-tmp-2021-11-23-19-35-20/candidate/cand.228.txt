This scalability is desirable in many domains, such as journalism [25], law, business, education, and health care, where cost, delay, and third-party legal implications [29] prohibit the application of manual transcription services [12]. Thus, the primary goal of this paper is to: provide an intermediate solution along the utilityprivacy spectrum that uses cloud services while providing a formal privacy guarantee.We present Prεεch (Privacy-Preserving Speech) as a means to achieve this goal; it is an end-to-end speech transcription system that: (1) protects the users' privacy along the acoustic and textual dimensions; (2) improves the transcription performance relative to offline ASR; and (3) provides the user with control knobs to customize the trade-offs between utility, usability, and privacy. Finally, applying Prεεch thwarts the learning of any statistical models or sensitive information extraction from the text via natural language processing tools.In summary, the main contributions of this paper are: (1) End-to-end practical system: We propose Prεεch, a new end-to-end system that provides privacy-preserving speech transcription at an improved performance relative to offline transcription. For example, Google's ondevice speech recognizer [1] is an offline transcriber that is currently only supported on Google's Pixel devices and does not allow an API or open-source access, limiting its usability.Notations: Let S denote the input speech file associated with a ground truth transcript T g S . In this section, we showcase a few representative examples of how cloud-based APIs can pose serious privacy threats to the acoustic features within S. Speaker Diarization: CSPs utilize advanced diarization capabilities to cluster the speakers within a speech file, even if they have not been observed before. provide the users with control knobs to customize Prεεch's functionality according to their desired level of utility, usability, and privacy.To this end, Prεεch applies a series of privacy-preserving operations to the input speech file before sending it to the CSP. Thus, segmenting and shuffling S transform its textual content into a bag-of-words representation.Sensitive word scrubbing (SWS): First, Prεεch applies the OSP to identify the list of sensitive keywords that contain numbers, proper nouns, or any other user-specified words. Next, Prεεch applies keyword spotting, KWS, (identify portions of the speech that correspond to a keyword) to each of the segments in S. Only the segments that do not contain a keyword pass to the CSP for transcription.Dummy word injection to ensure differential privacy:The bag-of-words representation of a transcript corresponds to its word histogram (Sec. 4.5). Finally, Prεεch applies text-to-speech (TTS) transforms to these dummy segments and adds them to S. However, leaving it just at this would be insufficient as the CSP can potentially distinguish between the two different sources of speech (TTS generated dummy segments and segments in S) based on their acoustic features. First, it obfuscates the sensitive voice biometric features in S. Second, VC ensures that the dummy segments (noise added to ensure differential privacy) are acoustically indistinguishable from the original speech file segments. This is followed by (2) sensitive word scrubbing where speech segments containing numbers, proper nouns, and user-specified keywords are removed from S. Next, (3) given the domain of S's textual content (its vocabulary), Prεεch generates a set of text segments (as is suitable for satisfying the DP guarantee as discussed in Sec. 4.5), and subjects it to TTS transformation (4). After obtaining the transcript (7) for each partition from the N CSPs, Prεεch removes S d 's transcripts and de-shuffles the remaining portion of the transcript using T S i and Order i , and outputs the final transcript to the user (8). This allows customization of the sensitive keyword list as users have subjective ideas of what they might consider sensitive.After the list of sensitive words is finalized, Prεεch applies keyword spotting (KWS) on the segments. A randomized mechanism A : N |V | → N |V | ,which maps the original histogram into a noisy one, satisfies (ε, δ)-DP if for any pair of histograms H 1 and H 2 such that||H 1 − H 2 || 1 = d and any set O ⊆ N |V | , Pr[A(H 1 ) ∈ O] ≤ e ε · Pr[A(H 2 ) ∈ O] + δ. Algorithm A will satisfy (ε , δ )-DP where ε = ln(1 + β(e ε − 1)) and δ < βδ.Additionally, we define a DP mechanism namely the truncated Laplace mechanism [6] which is used in Prεεch. The same applies to stylometry analysis, which is based on measures of the unique distribution of frequently used words of different individuals.Thus, as long as the counts of the most common words of the transcript are protected (via DP), the subsequent statistical model (like topic model) built over the word histogram will be privacy-preserving too (by Thm. In the following, we detail a mechanism, as a guide for the user, for choosing d when the target statistical analysis is topic modeling.Let us assume that the user has a set of speech files {S j } to be transcribed. Let T = {T 1 , · · · , T t } represent the original topic model built on j D j = j T g S j and T = T 1 , · · · , Tt represent the noisy topic model computed by the CSP.The following theorem (Thm. For instance, in an educational institution, the sensitive speech files requiring transcription might be the interviews/oral exams of the students conducted on a specific subject, and the noise corpus can be the lecture notes of the same subject.Next, Prεεch generates a set of dummy segments, S d , from the dummy corpus above. •Randomly partition S into N sets S i , i ∈ [N] where Pr[segment s goes to partition i] = β = 1/N, s ∈ S.• For each partition i ∈ [N], shuffle the dummy segments in S d,i (after applying TTS and VC) with the segments in S i (after applying VC), and send it to the CSP i . Number of CSPs used for transcription: As discussed above, employing multiple CSPs lowers the monetary cost incurred. In this section, we go over the end-to-end system design of Prεεch and identify potential privacy vulnerabilities.Voice Privacy: Many-to-one VC removes all the identifying features from S, like the speakers' voices, background noise, and recording hardware, thereby protecting voice privacy.Textual Privacy: For sensitive word scrubbing, the best-case scenario from a privacy point of view is to have the user spell out the entire keyword list. Formal Privacy Guarantee: For a speech file S, Prεεch provides perfect voice privacy (when using many-to-one VC) and an (ε, δ)-DP guarantee on the word histogram for the vocabulary considered (BOW ), under the assumption that the dummy segments are indistinguishable from the true segments. Sensitive Keyword Scrubbing: We use the NLP Python framework spaCy 9 for named entity recognition (NER) from the text. For this, we use TTS to generate the required target voice training utterances in a single synthetic voice.Many-to-One Voice Conversion: We utilize pre-existing architectures and hyperparameters 14 for the two-stage many-toone VC [44] mechanism, shown in Fig. 4. For evaluating Q4, we relax the privacy guarantee to obtain utility and usability improvements.Prototype Prεεch: For the prototype Prεεch presented in the paper: (1) segmentation length is adjusted to ensure that each segment contains at most two non-stop words (2) noisy segments are generated via the GPT2 language model (3) a single CSP (Google) is utilized (4) many-to-one VC is applied to both the dummy and true segments. To test the voice biometric privacy, we conduct two experiments using the voice analysis APIs (details in Sec. 3.1). In this section, we evaluate the statistical analyses (details in Sec. 3.2) performed by the adversary to extract textual information on the noisy transcripts obtained from Prεεch.Topic Model: We generate the topic models from the documents corresponding to the original and noisy word histograms, and evaluate their 1 distance. As the figure shows, the higher the distance parameter d, the larger is the 1 distance between true and noisy topics.Stylometry: In this experiment, we assume that the CSP applies stylometry analysis on T CSP S in an attempt to attribute it to an auxiliary document whose authors are known to the CSP. This is in contrast to standard usage of differential privacy for releasing numeric statistics where the noisy statistics result in a clear loss of accuracy. Thus, not only does Prεεch hide the speaker's biometrics and map them to a single target speaker but also ensures noise indistinguishability, which is key to its privacy properties.The second experiment tests Prεεch's privacy properties against a stronger adversary, who has access to samples from the true speakers. Prεεch applies a hierarchical segmentation approach that starts with a stage of silence detection based on the energy level, followed by pitch detection to detect speech activity for finer segmentation. However, the extent of such robustness is based on the efficacy of state-of-the-art NLP techniques.Word correlations can also weaken the DP guarantee (d −w, if w is the maximum size of word groups with high correlation). Generating Dummy Segments: We use the open source implementation 11 of OpenAI's state-of-the-art NLP language model, GPT 2 [36], to generate the noise corpus.Using this predictive model, we generate a large corpus representing the vocabulary of the evaluation datasets. However, the addition of the dummy segments in Prεεch does increase the monetary cost of using the online service that has to transcribe more speech data than needed. Hence, in the prototype Prεεch presented in the paper, we use short segments that contain at most two non-stop words.Another weakness is related to vocabulary estimation, especially if some of the distribution-tail words are deemed to be sensitive. (2) Offline Transcription: We consider the Deep Speech architecture from Baidu [18], which is trained using Mozilla's 1 Common Voice dataset as a representative offline transcription 1 https://voice.mozilla.org/en/datasets service. Such a vocabulary would be prohibitively large for efficient and practical usage.However, note that such a definition of V is an overestimate as no real-world document would contain all possible English words. This is because dummy words (for the noisy counts) can only be added to S; removing any word from S is not feasible as this would entail in recognizing the word directly from S, which would require accurate transcription. Second, it does not partition segments at the boundaries of the identified human speech and allows 40 ms of non-speech to be included at the beginning and the end of each segment.Control Knob: Segmenting S presents with a trade-offsmaller segments result in better privacy guarantee at the expense of deteriorated transcription accuracy due to semantic context loss. Hence, using both the CSPs results in lower overall utility than just using Google's cloud service. The one-to-one VC technique gives better accuracy than many-to-one VC since it is trained for a specific predefined set of source speakers (details in Sec. 7.4.1). Although our empirical evaluation shows that the OSP has a very high accuracy for the weighted estimation of V (Sec. 7.3.2), some sensitive distribution-tail words might still be missed due to the OSP's transcription errors. Although existing offline transcribers have high WER, we found (empirically) that they can identify the set of domain words of S with high accuracy (details in Sec. }} Thus, Prεεch's two-pronged approach -1) addition of extended-vocabulary noise 2) removal of ordering information via segmentation and shuffling, proves to be effective. Thus, protecting the privacy of this word histogram is a primary focus of Prεεch, and the privacy guarantee we choose is that of differential privacy. The inputs for the mechanism are (1) S -the short segments of the speech file S, (2) the privacy parameters ε and δ and (3) N -the number of non-colluding CSPs to use. Applying these APIs to the recorded speech can significantly undermine the user's privacy.Offline and open-source transcription services, like Deep Speech [18], solve these privacy challenges as the speech files never leave the user's trust boundary. Both experiments show that prototype Prεεch is effective in sanitizing the speaker's voice and ensuring noise indistinguishability. Moreover, if the user partitions S among multiple CSP's (Sec.4.5.3), then consecutive segments would not go to the same CSP with high probability. Recall that our objective of adding noise is to obfuscate any statistical analysis built on top of the document's BoW (histogram), such as a topic modeling and stylometry analysis. Thus in Prεεch, the noise (in the form of dummy segments) can ensure differential privacy without affecting the utility. Another real-world dataset is the Supreme Court of the United States case "Carpenter v. United States" 5 We study the privacy threats that a cloud-based transcription service poses while processing private speech data. PocketSphinx is a generic system that can detect any keyword specified in runtime; it is not trained on a pre-defined list of keywords and requires no per-user training or enrollment. The distance parameter d, intuitively, connects the privacy definition in the word histogram, which is purely a formal representation, to a semantic privacy notion. For perfect voice privacy, the VC system should (1) map any voice (even if previously unseen) to the same target voice, (2) not leak any distinguishing acoustic features, and (3) operate on speech containing multiple speakers. Prεεch protects the privacy of the textual content of an input speech file S through the following three operations:Segmentation and shuffling: Prεεch breaks S into a sequence of segments, denoted by S. The results in Fig. 8 indicate that for two real-world datasets, the number of words per segment can be kept between 2 and 3 with an acceptable degradation of the WER.Voice Cloning: Voice cloning does not affect the true segments (it is only applied to dummy segments), resulting in no additional WER degradation. The first 4 steps in the above mechanism are performed in stage 3 in Prεεch (Fig. 1) while steps 5-6 are performed in stage 6. Prεεch provides the user with two broad options to satisfy this conditionvoice cloning or voice conversion.Voice cloning is a TTS system that generates speech in a target speaker voice. Although there is no formal guarantee about the adversary's ability to distinguish dummy and true segments based on their textual features, we have empirically analyzed this threat in Sec. 7.3.3 and Sec. 7.3.4. Prεεch uses a dummy segment only once per 