An adversary knows some or all of the ML system's parameters and uses this knowledge to craft training or testing samples that manipulate the decisions of the ML system according to the adversary's goal-for example, to avoid being sentenced by an ML-enhanced judge.Recent work has focused primarily on evasion attacks [4,44,17,50,35,9], which can induce a targeted misclassification on a specific sample. A commercial ML-based malware detector [1] can rely on a publicly known architecture with proprietary data collected from end hosts, and a mixture of known features (e.g. system calls of a binary), and undisclosed features (e.g. reputation scores of the binary). However, an attacker is often unable to determine the labels assigned to the poison samples in the training set -consider a case where a malware creator may provide a poison sample for the training set of an ML-based malware detector, but its malicious/benign label will be assigned by the engineers who train the detector. The labels for instances in S are assigned using an oracle -for a malware classifier, an oracle could be an antivirus service such as VirusTotal, whereas for an image classifier it might be a human annotator. The first goal is to introduce a targeted misclassification on the target by deriving a training set S from S * : h = A(S), h(t) = y d , where y d is Mallory's desired label for t. On binary classification, this translates to causing a false positive (FP) or false negative (FN). Without knowing the victim classifier A, the attacker would have to select an alternative learning algorithm A ′ and hope that the evasion or poison samples crafted for models created by A ′ transfer to models from A. Similarly, if some features are unknown (i.e., partial feature knowledge), the model used for crafting instances is an approximation of the original classifier. These questions define the spectrum for adversarial knowledge with respect to the learning algorithm: black-box access, if the information is public, gray-box, where the attacker has partial information about the algorithm class or the ensemble architecture, or white-box, for complete adversarial knowledge.The I dimension controls the overlap between the instances available to the attacker and these used by the victim. For some applications, the attacker may not be able to modify certain types of features, either because they do not control the generating process (e.g. anStudy F A I L Test Time Attacks Genetic Evasion[50] , , , † , Black-box Evasion[37] ,∅* , , ,∅* Model Stealing[46] , , , ,∅* FGSM Evasion[17] ,∅* ,∅* ∅,∅ ,∅* Carlini's Evasion[9] ,∅* , ∅,∅ ,∅* Training Time Attacks SVM Poisoning[5] ,∅* , † ∅,∅ ,∅* NN Poisoning[33] , † , , ,∅* NN Backdoor[20] 2 , † , , † , NN Trojan[29], , , , [38] , , , , Feature Squeezing [49] , , , ,Training Time Defenses RONI [34] , , , , Certified Defense [42] , , , , exploit predictor that gathers features from multiple vulnerability databases) or when the modifications would compromise the instance integrity (e.g. a watermark on images that prevents the attacker from modifying certain features). To demonstrate its utility, we evaluate a body of existing studies by means of answering two questions for each work.To categorize existing attacks, we first inspect a threat model and ask: AQ1-Are bounds for attacker limitations specified along the dimension? For example, feature squeezing [49] employs feature reduction techniques unknown to the attacker; therefore the answer is yes for the F dimension.In order to identify hardening dimensions, which attempt to limit the attack capabilities, we ask: DQ2-Is the dimension employed as a mechanism for hardening? However, RONI remains computationally inefficient as the number of trained classifiers scales linearly with the training set.Target-aware RONI (tRONI) builds on the observation that RONI fails to mitigate targeted attacks [34] because the poison instances might not individually cause a significant performance drop. We implement StingRay against four applications with distinct characteristics, each highlighting realistic constraints for the attacker. CRAFTINSTANCE creates poison images such that the distance to the target t in deep feature space is minimized and the resulting adversarial image is within τ D distance in pixel space to t. Recent observations suggest that features in the deeper layers of neural networks are not transferable [52]. The adversary aims to misclassify an Android app t. Although the problems of inducing a targeted false positive (FP) and a targeted false negative (FN) are analogous from the perspective of our definitions, in practice the adversary is likely more interested in targeted FNs, so we focus on this case in our experiments. Due to the class imbalance, we use stratified samples of 60%-40% of the data set for training and testing respectively, obtaining a 40% testing F1.The targeted attack selects a set I of vulnerabilities that are similar to t (e.g. same product or vulnerability category), have no known exploits, and gathered fewer tweets. The attacker has limited leverage and is only able to influence time series based features indirectly, by injecting information in various blacklists.We generate 2,002 attacks under two scenarios: the attacker has compromised a blacklist and is able to influence the features of many organizations, or the attacker has infiltrated a few organizations and it uses them to modify their reputation on all the blacklists. Therefore the attacker has more degrees of freedom in modifying the features of instances in I, knowing that their desired labels will be preserved.In case of the data breach predictor, the attacker utilizes organizations with no known breach and aims to poison the blacklists that measure their hygiene, or hacks them directly. We measure the PDR on holdout testing sets by considering either the average accuracy, on applications with balanced data sets, or the average F1 score (the harmonic mean between precision and recall), which is more appropriate for highly imbalanced data sets. In #5 we simulate a scenario in which the attacker only knows 70% of the victim training set, while #7-8 model an attacker with 80% of the training set available and an additional subset of instances sampled from the same distribution.These results might help us explain the contradiction with prior work. Figure 3 illustrates some images crafted by constrained adversaries.The FAIL analysis results show that the perceived PDR is generally an accurate representation of the actual value, making it easy for the adversary to assess the instance inconspicuousness and indiscriminate damage caused by the attack. By tuning the attack parameters (e.g. the layer used for comparing features or the degree of allowed perturbation) to generate poison instances that are more specific to the target, the performance drop on the victim could be further reduced at the expense of requiring more poisoning instances. For indiscriminate poisoning, a spammer can force a Bayesian filter to misclassify legitimate emails by including a large number of dictionary words in spam emails, causing the classifier to learn that all tokens are indicative of spam [3] An attacker can degrade the performance of a Twitter-based exploit predictor by posting fraudulent tweets that mimic most of the features of informative posts [40]. We show that our attack is practical for 4 classification tasks, which useAcknowledgments We thank Ciprian Baetu, Jonathan Katz, Daniel Marcu, Tom Goldstein, Michael Maynord, Ali Shafahi, W. Ronny Huang, our shepherd, Patrick McDaniel and the anonymous reviewers for their feedback. Machine learning (ML) systems are widely deployed in safety-critical domains that carry incentives for potential adversaries, such as finance [14], medicine [18], the justice system [31], cybersecurity [1], or self-driving cars [6]. Indeed, the target would be correctly classified by Alice after learning a hypothesis using a pristine training set S * (i.e. h * = A(S * ), h * (t) = y t ). Upon further analysis, we discovered that the performance drop is due to other testing instances similar to the target being misclassified as y d . We formalize the adversary's strength in the FAIL attacker model, which describes the adversary's knowledge and capabilities along 4 dimensions:• Feature knowledge R = {x i x i ∈ x, x i is readable}:the subset of features known to the adversary. In Table 2 we highlight implicit and explicit assumptions of previous defenses against poisoning and evasion attacks.Through our systematic exploration of the FAIL dimensions, we provide the first experimental comparison of the importance of these dimensions for the adversary's goals, in the context of targeted poisoning and evasion attacks. In an attempt to map the effect of adversarial perturbations on human perception, the authors of [35] found through a user study that the maximum fraction of perturbed pixels at which humans will correctly label an image is 14%. Mallory has partial knowledge of Alice's classifier and read-only access to the target's feature representation, but they do not control either t or the natural label y t , which is assigned by the oracle. We use stratified sampling and split the data set into 60%-40% folds training and testing respectively, aiming to mimic the original classifier. Therefore tRONI is only capable of identifying instances that distort the target classification significantly. StingRay is effective against 4 real-world classification tasks, even when launched by a range of weaker adversaries within the FAIL model. Could the attacker access the exact feature values? Lack of a unifying threat model to capture the dimensions of adversarial knowledge caused existing work to diverge in terms of adversary specifications. However, this is impractical because the adversary, under our threat Algorithm 1 The StingRay attack.1: procedure STINGRAY(S ′ ,Y S ′ , t, y t , y d ) 2: I = ∅ 3: h = A ′ (S ′ ) 4:repeat 5:x b = GETBASEINSTANCE(S ′ ,Y S ′ , t, y t , y d )6:x c = CRAFTINSTANCE(x b , t) 7:if GETNEGATIVEIMPACT(S ′ , x c ) < τ NI then 8:I = I ∪ {x c } 9: h = A ′ (S ′ ∪ I) 10:until (I > N min and h(t) = y d ) or I > N max 11:PDR = GETPDR(S ′ ,Y S ′ , I, y d ) 12: if h(t) ≠ y d or PDR < τ PDR then 13:return ∅ returnI 15: procedure GETBASEINSTANCE(S ′ ,Y S ′ , t, y t , y d ) 16: for x b , y b in SHUFFLE(S ′ ,Y S ′ ) do 17: if D(t, x b ) < τ D and y b = y d then 18:return x b model, does not control the labeling function. However, attacks such as StingRay could exploit this assumption to evade detection by crafting a small number of inconspicuous poison samples. The first attack subjected to the FAIL analysis is JSMA [35], a well-known targeted evasion attack Transferability of this attack has previously been studied only for an adversary with limited knowledge along the A and I dimensions [37]. By taking these constraints into account, we design StingRay, a targeted poisoning attack that is practical against 4 machine learning applications, which use 3 different learning algorithms , and can bypass 2 existing defenses. The model generalizes the transferability of attacks against ML systems, across various levels of adversarial knowledge and control. In this scenario, the attacker has an image t with true label y t (e.g. a dog) and wishes to trick the model into classifying it as a specific class y d (e.g. a cat). • We systematically explore realistic adversarial scenarios and the effect of partial adversary knowledge and control on the resilience of ML models against a test-time attack and a training-time attack. The base instances are selected using the Manhattan distance D = l 1 and each poisoning instance has a target resemblance of ¯ s = 10 features and a negative impact τ NI < 50%. To simulate weaker adversaries systematically, we formulate a questionnaire to guide the design of experiments focusing on each dimension of our model.For the F dimension, we ask: What features could be kept as a secret? For example, distillation limits adversaries along the F and A dimensions but employing a different attack strategy could bypass it [9]. While neural networks have dramatically reduced test-time errors for many applications, which suggests they are capable of generalization (e.g. by learning meaningful features from the training data), recent work [53] has shown that neural networks can also memorize randomly-labeled training data (which lack meaningful features). StingRay is a general framework for crafting poison samples.At a high level, our attack builds a set of poison instances by starting from base instances that are close to the target in the feature space but are labeled as the desired target label y d , as illustrated in the example from Figure 2. An adversary with partial leverage needs extra effort, e.g. to craft more instances (for poisoning) or to attack more of the modifiable features (for both poisoning and evasion). The classifier achieves a 60% F1 score on the testing set.In this case, the adversary plans to hack an organization t, but wants to avoid triggering an incident prediction despite the eventual blacklisting of the organization's IPs. In this section, we discuss three defenses against poisoning attacks and how StingRay exploits their limitations.The Micromodels defense was proposed for cleaning training data for network intrusion detectors [13]. Running time of StingRay.The main computational expenses of StingRay are: crafting the instances in CRAFTINSTANCE, computing the distances to the target in GETBASEINSTANCE, and measuring the negative impact of the crafted instances in GETNEGATIVEIMPACT.CRAFTINSTANCE depends on the crafting strategy and its complexity in searching for features to perturb. In table 3, we present the average results of our 11 experiments, each involving 100 attacks.For each experiment, the table reports the ∆ variation of the FAIL dimension investigated, two SR statistics: perceived (as observed by the attacker on their classifier) and potential (the effect on the victim if all attempts are triggered by the attacker) as well as the mean perturbation ¯ τ D introduced to the evasion instances. For instance, the distillation defense [38] against evasion modifies the neural network weights to make the attack more difficult; therefore the answer is yes for the A dimension.These defenses may come with inaccurate assessments for the adversarial capabilities and implicit assumptions. For applications where models are updated over time or trained in mini-batches (such as an image classifier based on neural networks), the attacker only requires control over a subset of such batches and might choose to deliver poison instances through them. In CRAFTIN-STANCE, we apply tiny perturbations to the instances (D.III) and by checking the negative impact NI of crafted poisoning instances on the classifier (D.IV) we ensure they remain individually inconspicuous.Mutating these instances with respect to the target [34] (as illustrated in Figure 1c) may still reduce the overall performance of the classifier (e.g. by causing the misclassification of additional samples similar to the target). An instance x ∈ X is an entity (e.g., a binary program ) that must receive a label y ∈ Y = {y 0 , y 1 , ..., y m } (e.g., reflecting whether the binary is malicious ). First, from the AndroidManifest XML file, which contains meta information about the app, the authors extract the permission requested, the application components and the registered system callbacks. The classifier uses 72 features from 4 categories: CVSS Score, Vulnerability Database, Twitter traffic and Twitter word features. By taking into account the goals, capabilities, and limitations of realistic adversaries, we also design StingRay, a targeted poisoning attack that can be applied in a broad range of settings 1 . The poisoning instances are mixed with the pristine ones and used to train the victim classifier from scratch. It may be easy for an attacker to craft instances with a high negative impact, but these instances may also be easy to detect using existing defenses.In practice, the cost of injecting instances in the training set can be high (e.g. controlling a network of bots in order to send fake tweets) so the attacker aims to minimize the number of poison instances (D.VI) used in the attack. Thus, it is more likely that the poison samples will become collectively inconspicuous, increasing the attack effectiveness. Feature subsets may not be publicly available (e.g. derived using a proprietary malware analysis tool, such as dynamic analysis in a contained environment), or they might be directly defined from instances 