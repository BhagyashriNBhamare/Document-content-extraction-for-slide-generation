Existing techniques, however, generate many false positives, make "closed-world" assumptions (i.e., the adversary must know in advance that the streamed video belongs to a small known set), or are not robust to noise in the network or the adversary's measurements.Further, prior work assumes that the adversary can directly observe the encrypted video stream either at the network layer (e.g., a malicious Wi-Fi access point) [11] or physical layer (e.g., a Wi-Fi sniffer) [43,46], or else that the adversary's virtual machine is co-located with the user's virtual machine [1]. We demonstrate that packet bursts in encrypted streams correspond to segment requests from the client and that burst sizes are highly correlated with the sizes of the underlying segments.Second, we demonstrate that this leak is a fingerprint for about 20% of YouTube videos because their burst patterns are highly distinct. This ensures a high Bayesian detection rate: if the adversary identifies a streamed video, then this is likely not a false positive.Third, we develop a new video identification methodology based on convolutional neural networks and evaluate it on video titles streamed by YouTube, Netflix, Amazon, and Vimeo. This attacker is much weaker than malicious ISPs and Wi-Fi access points typically considered in the traffic analysis literature.In summary, we (1) explain the root causes of burst patterns in encrypted video streams, (2) show how to exploit these patterns for video identification in an "openworld" setting, (3) develop and evaluate a noise-tolerant identification methodology based on deep learning, and (4) demonstrate how a remote attacker without direct observations of the network can identify streamed videos. We conjecture that a suffix of the vector of segment sizes, arranged in the order they are fetched from the server (which corresponds to the order of presentation), can be estimated from the observable characteristics of encrypted streaming traffic, up to a small error induced by the varying overheads of lower network layers.Example. In the last 15 seconds, the iguana reaches To demonstrate this effect more systematically, we created a 45-second "low action" scene by concatenating three copies of the 15-second footage of the resting iguana, and a 45-second "high action" scene by concatenating 15-second footage from the height of the chase. Because of the way this video was crafted, "low" and "high" action-and the correspondingly high and low burst sizes-alternate every 45 seconds (9 time seg- Figure 2.3: Burst sizes when streaming a video with alternating high-and low-bitrate periods. This is a special case of timing side channels in schedulers [16,25] that can be exploited in a variety of attack scenarios.We focus on remote attackers who can execute JavaScript in the victim's Web browser: rogue websites, advertisers, analytics services, content distribution networks, etc. For every video file that the attacker wants to identify, he constructs a detector algorithm that determines, given measurements of a stream, whether the stream is carrying this video file or not.In this paper, we use machine-learning models as detectors. For videos with an initial title sequence, this (non-unique) sequence is downloaded as part of the initial buffering; the bursts in the on-off phase correspond to the segments of unique content.We captured the network traffic of each streaming session for a certain duration (see below) using Wireshark's tshark [60]. We discarded these and only used the remaining 18 titles, with 3-minute content captures for each.We also downloaded actual 720p MP4 file video files (as opposed to their network streams) for the 3,558 titles from the crawl, using the SAVEFROM.NET Web tool. We now show that for 19% of YouTube files, this leak is actually a fingerprint: the sequence of segment sizes identifies the video with virtually no false positives.Modeling the server.We used the Bento4 MPEG-DASH toolset [4] to process our 3,558 YouTube videos (see Section 5.2) for standardized streaming, i.e., divide them into time segments and create the manifests. Let s m = mean(T S), the element-wise average over T S. Training produces α(s m ), which is the attacker's fingerprint of m.During the attack, the attacker is given the victim's trace t ∈ R k and computes its traceprint, α(t). To estimate the error for k = 40 (as will be needed later), we partition 1 t ∈ R 40 into 4 contiguous blocks of length 10 and apply the union bound on the probabilities of error in each block and the difference elements in α, i.e.,(ti − t j ) − (s m i − s m j )for (i, j) ∈ (11, 10), (21,20), (31,30). Total error is thus bounded by B with 1 With longer captures, we could have estimated this error directly.very high probability, Pr t←T m [α(t) − α(s m ) 1 ≤ B] ≥ 1 − 7 · 10 −12 ≥ 1 − 10 −11 , implying very high recall.Attacker's precision.Even if the distance between the attacker-measured "traceprint" and the video's fingerprint is small, the attacker may still misclassify the video if its fingerprint is close to another one. Since with probability 10 −11 a traceprint α(t) (of a video m with variable segment size) is B-close to the correct fingerprint α(s m ) (by the recall bound above), the probability that an attacker mistakes t's video for another one in our dataset is at most 10 −11 . Section 6 explains why DASH-based video streams are fingerprintable, but the theoretical model underestimates the capabilities of realistic attackers who can use traffic features other than burst sizes (e.g., packet timing). DNNs have proved very effective for signal recognition tasks such as speech transcription [19], image segmentation [14], image classification [28], and many others.In a neural network, each layer of neurons does some computation on its input and passes the output to the next layer (or final output)-see Figure 7.1. We exploit this in both on-path and off-path attack scenarios.Convolutional Neural Networks (CNNs) [9] are deep neural networks whose lower layers apply the same linear transformation on many windows of the input data. For comparison, we also performed training in an Ubuntu virtual machine on a commodity laptop with an i7-6600U CPU (and no GPUs) running Windows 10; in this case training was 35 times slower, but even so, the most time-consuming training (that of the Netflix classifier for 1,400 epochs) took less than 10 hours. We can use this probability as a confidence measure.Our goal is to ensure that the classifier produces no false positives, at the cost of occasionally failing to detect the match (false negatives). This connection, however, may have different network characteristics, such as bandwidth, latency, congestions and packet drops, all of which affect the collected traces.We conjecture that our classifiers learn high-level features of video streams, such as burst patterns, that are robust to reasonable differences in network characteristics and will therefore maintain high accuracy even when trained on a different network (in the absence of pathological conditions such as excessive packet loss or inadequate bandwidth for streaming). More expressive classifiers (e.g., with more hidden layers) suffer from over-fitting, but it may be solved with more data, e.g., 1000 captures per video.Finally, the low base rate potentially motivates the use of detection cascades [56] consisting of a series of classifiers, each of which is more complex (with a larger input feature space and more hidden layer activations) than the previous one. "Open world," when the attacker does not know a priori a relatively small set of possibilities for the video being streamed, is characterized by an extremely low base rate, i.e., probability P(M) that the video actually corresponds to any of the attacker's detectors. If p COL ≥ 2 10 6 , then we are likely to observe a collision in our dataset D. Under the simplifying assumption that collisions in D are independent events, 3 with overwhelming probability 1 − (1 − p COL ) (|D|−|V |)|V |+|V | 2 /2 > 0.986 there exist m V ∈ V, m D ∈ D such that m V = m D and α(s m D ) − α(s m V ) 1 ≤ 2B. An ad may include JavaScript code executing in the victim's browser, but because this code may come from a questionable source with strong commercial interest in users' data (including their viewing habits), it is confined-both by the main browser sandbox, which prevents it from issuing arbitrary requests to the OS, and by the same origin policy [51], which prevents it from accessing the content that belongs to other Web origins. JavaScript can send and receive arbitrary amounts of data to and from its colluding server to create artificial congestion on the shared link.When the shared link is congested, any attempt to use it can create observable delays in the communication between the attacker's JavaScript code and its own server. All traffic between victim-LAN (which includes the streaming client and the attack JavaScript client) and the Internet (which includes the streaming server and the attack server) thus flows through a bandwidth-constrained router.Data. 4 From the {X n } vector of message arrival times measured by the attacker's client, we compute the vector of message delays Y = (0) ((X 2 , . . . X n ) − (X 1 , . . . X n−1 )) and filter the X,Y time series for delays that exceed 8ms. Our attack relies on two assumptions: (1) the attacker can measure traffic bursts in the victim's video stream, and (2) the pattern of these bursts is similar to what the attacker observed when streaming the same title.The attack works well using only very coarse traffic features (see Section 7.3) and is therefore robust to minor noise in the stream or in the attacker's measurements. The ability of malicious JavaScript in the victim's browser to congest the network may be limited by resource-intensive processes executing on the same machine.As explained in Section 4, different encodings of the same content create different burst patterns. If the user or service re-encodes the video (e.g., at a different resolution), the attacker's previously trained detectors will no longer work.Our techniques can be automated and deployed on a reasonably large scale to detect hundreds or thousands of titles in an "open-world" setting, without assuming a priori that the video belongs to small known set. In [31], Liu et al. use aggregated traffic throughput traces (as opposed to frame-size time series) and report 1% false positive rate and 90% recall rate.These methods operate on time series resembling, and close to the granularity of, the sizes of individual frames. This approach has not been evaluated in an off-path setting, where the attacker has only noisy side-channel measurements, nor for any streaming services other than Netflix.Mass fingerprinting in [44] relies on the metadata sent by Netflix to the client at an early stage of the streaming process, namely the . The general approach of creating congestion on a shared resource (network, in our case) and using it to measure a concurrent process's consumption of that resource is used, for example, in shared-cache attacks on cryptographic computations [21,37,45]. This attack is powerful because the attacker only needs to know the user's IP address, but it cannot be deployed if the user is behind a firewall or router that discards unsolicited packets from outside the network (as many modern routers do by default). Agarwal et al. [1] show how a VM can use link congestion to infer the traffic patterns of a co-located VM.To the best of our knowledge, the ability of confined JavaScript to perform network measurements at sufficient granularity to identify concurrent video streams has never been empirically demonstrated before. Felten and Schneider [13] observed that JavaScript can infer information from the timing of cross-origin requests; Bortz and Boneh [5] demonstrated several timing-related Web attacks; Van Goethem et al. [55] proposed timing techniques that tolerate network noise and server-side mitigations. Traffic analysis was used to infer application-specific sensitive information, such as health conditions [8,33], as well as Web sources of video traffic [47]. Segmenting video files and transmitting them in bursts (which is primarily done to maximize quality of experience) reduces the granularity of the leak but does not prevent video fingerprinting.Decreasing granularity further, to minutes, will not entirely prevent the leak in longer videos, but will degrade QoE and network efficiency. Client-side-only changes are easier to deploy than changes to segmentation on the server, but devising such a regime is non-trivial even if we allow changes to both client side and server side.For example, consider a variable-size buffer that fetches equally-sized segments every X seconds (where X is fixed). We also show how an off-path adversary who merely serves a Web page or ad to a user can, via the network congestion side channel, perform the measurements needed for the attack and identify videos being streamed by the user on the same or different device. We added a random number of bytes between 0 and 2% to each burst size in the dataset and measured the accuracy of the B classifier vs. our CNNbased classifiers, which use the total burst series (see Section 5.2) and are trained for 1,400 and 700 epochs on the Netflix and YouTube data, respectively. The KNN classifier of [11] works best at the granularity of 10 seconds, and even then it only attains 0.22 accuracy.