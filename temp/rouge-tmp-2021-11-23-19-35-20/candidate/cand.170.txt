The first example exhibits jargon "rat" which means "remote access trojan"; The second example shows the jargon "blueberry" for "marijuana", while "athena" is the jargon for a kind of botnet framework in the third example.tion traces every day. Also, another prior study [31] shows that the names of illicit products (e.g., bot) automatically discovered from underground marketplaces include some jargons (e.g., "fud" which means "fully undetectable exploit"). In our research, we propose a new technique, called Cantreader, that utilizes a new neural language model to capture the inconsistencies between a phrase's semantics during its legitimate use and in underground communication. Such a neural network outputs two vectors for each input word, one for the good set and the other for the bad set, automatically making the semantic gap between the word's context measurable (Section 4.1). To control the false positives introduced by the variations in a term's legitimate use, our approach runs the new model to compare the semantics of the terms in the good set (legitimate communication traces) and their meanings in a reputable interpretative corpus (such as Wikipedia and dictionary). Running Cantreader on 375,993 communication traces collected from four underground forums across eight years, our study sheds new light on the characteristics of dark jargon, and the possible implications that they may have on criminal traces recognition and black word identification. The rich information disclosed by such communication sheds light on the adversary's strategy, tactics and techniques, and provides the landscape of the fast-evolving cybercrime.In our research, we studied the communication that took place on four infamous underground forums: Darkode (sale and trade of hacking services and tools), Hack Forums (blackhat hacking activities discussion), Nulled (data stealing tool and service) and Silk Road (illegal drug), including 375,993 traces (i.e., threads of posts) from 03/2008 to 05/2016 (4132 per month). For example, to better understand the drug trade business, Drug Enforcement Administration (DEA) intelligence program compiled a set of dark jargons to decipher forensic data and evidence or information gathered like traffickers' receipts.Due to the dynamic and fast-evolving nature of cybercrimes, the vocabulary of dark jargons continues to change, adding new terms and dropping old ones. Neural language model has been found very efficient for learning high-quality distributed representations of words (word embedding), which capture a large number of precise syntactic and semantic word relationships [28,36,37]. To counter the imbalance between the rare and frequent words, some neural language models such as the skipgram model apply a simple subsampling approach: each word w i in the training set is discarded with a probability determined as by computing P(w i ) = 1 − 񮽙 t f (w i ) where f (w i ) is the frequency of the word w i and t is a chosen threshold, typically around 10 −5 . For example, v womon − v queen ≈ v man − v king where 'woman' is the hypernym of the hyponym 'queen' and 'man' is the hypernym of the hyponym 'king'. For example, when the "popcorn" means a snack, it often comes with "eat" or "chocolate", while when it refers to marijuana, "nugz", "buds" and others would show up around the word.This observation is the key to the automated discovery and understanding of dark jargons and is fully leveraged in the design of Cantreader. It then applies the Semantics Comparison Model (Section 4.1) to calculate two semantic similarities, Sim dark,legit and Sim legit,rep , for each input word: the former between a dark forum corpus and a legitimate forum corpus, and the later between the legitimate forum corpus and a reputable interpretative corpus. Here we use Silk Road as the dark corpus (C dark ), Reddit as the legitimate corpus (C legit ), and English Wikipedia as the reputable interpretative corpus (C rep ), to demonstrate how Cantreader could discover and interpret the jargon.After preprocessing all the corpora, Cantreader first trains two Semantic Comparison Models on (C dark ,C legit ), and (C legit ,C rep ) respectively. To find out the dark semantics of the word, we leverage an public ontology [20] including the terms of cybercriminal activities and illegal products exchanged on underground markets (such as RAT and marijuana), and a projection learning model to determine whether the word has an "is-a" relation with a class under the ontology. As an unsupervised learning model, when the training is done, Word2Vec outputs the weights of the hidden layer M in the form of a |V | × |H| matrix, where |V | is the size of the input vocabulary and |H| is the size of hidden layer. As a result, for a given word, the NN produces different vectors each time when it is trained on the same corpus, which makes cross-model semantic comparison meaningless.So the key challenge here is how to make a word's vectors trained from different corpus comparable. It also gives different distributed representation for the same word from different corpora, which ensures the two corpora to be treated differently by the model.Since we double the size of the input layer, the weights of the connections between the input and the hidden layer M can now be represented as a 2|V | × |H| To analyze this architecture, we ran SCM on the Text8 corpus [1], which is a 100MB subset of Wikipedia. To this end, We randomly chose 5 words from the Text8 corpus and replaced them with 5 other words (see Table 2) to construct a new corpus Text8 syn . The test set includes a list of syntactic and semantic relations (such as capital of the country, adjective-adverb, etc.), and a number of test cases (such as Athens-Greece, Baghdad-Iraq) under each such a relation. After preprocessing these input corpora to build a shared vocabulary, it computes the cross-corpus similarities for each word by training two SCMs, one on C dark and C legit , and the other on C legit and C rep . Particularly, the embedding techniques like Word2Vec and SCM all rely on a word's context to deduce its semantics and embed it into a vector space. As a result, the same piece of text may appear on a forum repeatedly, and the words involved, though may occur across the corpus for many times, are always under the same context, whose semantics therefore cannot be effectively learned.To address this issue, we introduce a new metric, called windowed context, to measure a word's context diversity. The purpose is to compare every word's two embedded vectors (one for each corpus) by calculating their cosine similarity Sim dark,legit , for the sake of identifying those with discrepant meanings across the corpora.However, just because a word has different semantics across the dark forum and the legitimate corpus does not always mean that it is a dark jargon. For example, we found that the word "damage" on reddit.com often appear during the discussion of computer games and as a result, its context becomes very much biased towards settings in the games (such as "heal", "stun" and "dps"); on Silk Road, however, "damage" preserves its original meaning.To filter out the terms unique to the good set (C legit ), the discoverer compares every vocabulary word in the set to the same word in another legitimate corpus, in terms of their semantics. In our research, we use z = 1.65, so for each class, the discoverer simply computes the mean μ and standard deviation σ for the cosine similarities of the vector pairs as produced by an SCM for individual words and set μ − 1.65σ as the threshold for that model. Then our approach analyzes the semantics (in terms of embedded vectors) of a given jargon and all the candidates, running a classifier to find out whether any of them is a hypernym of the jargon. Figure 5 shows the example to search for the subclasses under "software", where wdt:P279 represents the subclass of relation, and wd:Q7397 describes the software entity.For each category (e.g., drug) in the seed set, we use Wikidata to find all its direct and indirect subclasses, and generate a tree rooted at the category in the seed. In our study, we ran our implementation of Cantreader on 375,993 communication traces collected from four underground forums, using an R730xd server with 40 Intel Xeon E5-2650 v3 2.3GHz, 25M Cache cores and 16 of 16GB memories. In our research, we parsed the underground forum snapshots collected by the darknet marketplace archives programs and other research projects [3,31], to get four dark corpora: the Silk Road corpus [17] consists of 195,403 traces (i.e., threads of posts) from the underground market-place Silk Road mainly discussing illicit products (such as drugs and weapons) trading; the Darkode corpus [4] includes 7,417 traces from a hacking technique forum about cybercriminal wares; the Hack forums corpus [9] has 52,670 traces from a blackhat technique forum; and the Nulled corpus [12] contains 121,499 traces from a forum talking about data stealing tools and services. Note that not all the terms appear on the two lists are actually used as dark jargons in our dark corpora because DEA's drug list includes many out-of-date and uncommon slang names for drugs, and the cybercrime marketplace product list, on the other hand, focuses mostly on illegitimate products, which are not always referred to in dark jargons. To understand the accuracy and coverage of the results, we first used the groundtruth dataset to validate our results.Among the 774 jargon words in the groundtruth set, 598 were successfully detected by Cantreader, which gives a recall of 77.2%. Performance.To understand the performance of Cantreader, we measured the time it took to process 180,899 communication traces (containing 100M total words, where 75,419 unique words are in the vocabulary) in the dark corpora and the breakdowns of the overhead at each analysis stage, the discoverer and the interpreter. Among them, 692 drug jargons are not included in the drug jargon lists reported by DEA (see Section 5), but prevalent in underground forums such as "cinderella", "pea" and "mango". The higher search interests means the higher competitiveness of a search term, indicating that it becomes more difficult for less relevant and less reputable websites to get to the top of the search results under the term through SEO. For example, we found that "chocolope", a kind of marijuana, which does not appear in C legit , frequently co-occurs with multiple drug jargons Figure 8: Trace volume of four jargons across month such as "blueberry", "diesel" and "kush" in C dark . In fact, compared to "loki" and "xtreme", "ivy" is a more popular "rat" since its release, due to its wide availability and easy-to-use features [13]. We notice a spree of the trace volume of "rat" from 02/2009 to 10/2011 due to the popularity of multiple kinds of "rat" like "loki", "xtreme" and "ivy". We conducted open domain experiments as reported in Section 3, which indicates the effectiveness of the proposed model on open domain corpora.Also, even only accepting two corpora in jargon discovery, the semantic comparison model can accept n corpora for comparison by setting the input layer to n times of the word size, where the word size is the intersection words of all n corpora. In fact, we can further optimize the performance of jargon discovery: consider the example mentioned in Section 3; we can modify the semantic comparison model to accept three corpora legit, dark 1 and dark 2 where dark 1 and dark 2 are related to the similar criminal activity such as drug trading. Examples include analyzing web privacy policies [49], generating app privacy policies [47], analyzing descriptions to infer required app permissions [40,39], detecting compromised web sites [34], identifying sensitive user input from apps [33,38], and collecting threat intelligence [35]. Our work proposes a novel NLP analysis model and identifies a novel application of NLP security, i.e., automatically identifying and understanding dark jargons in underground communication traces.One recent work that is closest to our study introduces a technique to detect search engine keywords referring to illicit products or services [46]. Our evaluation of over one million underground communication traces further reveals the prevalence and characteristics of dark jargons, which highlights the significance of this first step toward effective and automatic semantic analysis of criminal communication traces.