Detection is strongly aided by the fact that observing cache activity of co-resident processes is not possible without altering the cache state and thereby forcing evictions on the observed processes. We further show empirically that the performance of RELOAD+REFRESH on cryptographic implementations is comparable to that of other widely used cache attacks, while detection methods that rely on L3 cache events are successfully thwarted. The OS or the hypervisor in virtual environments provide strict logical isolation among processes to enable secure multi threading. Cache attacks can trespass VM boundaries to infer secret keys from neighboring processes or VMs [23,52], break security protocols [28,53] or compromise the end users privacy [47], and they can leak information from within a victim memory address space [34] when combined with other techniques.Cache and other microarchitectural attacks pose a great threat and consequently, different techniques have been proposed for their detection and/or mitigation [16]. Similarly, to the best of our knowledge, no hypervisor implements them, presumably due to the overhead they entail.As a result, the only solution that seems practical for users that want to protect themselves against this kind of threat, is to detect ongoing attacks and then react in some way. Our Contribution: We analyze the replacement policy of current Intel CPUs and identify a new strategy that allows an attacker to monitor cache set accesses without forcing evictions of the victim 's data, thereby creating a new and stealthier cache-based microarchitectural attack. Modern processors include cache memories that are hierarchically organized; low level caches (L1 and L2) are core private, smaller and closer to the processor, whereas the last level cache (LLC or L3) is bigger and shared among all the cores. The address bits are divided into offset (lowest-order bits used to locate data within a line), index (log 2 (S) consecutive bits starting from the offset bits that address the set) and tag (remaining bits which identify if the data is cached). Since the processor may stall for several cycles whenever there is a cache miss, the decision of which data is evicted and which data stays is crucial for the performance.Many replacement policies are possible including, for example, FIFO (First in First Out), LRU (Least Recently Used) or its approximations such as NRU [55] (Not Recently Used), LFU (Least Frequently Used), CLOCK [29](keeps a circular list of the elements) or even pseudo-random replacement policies. Modern high-performance processors implement approximations to LRU, because a truly LRU policy is hard to implement, as it requires complex hardware to track each access.LRU or pseudo-LRU policies have demonstrated to perform well in most situations. LIP (LRU Insertion Policy) consists in inserting each new piece of data in the LRU position whereas BIP (Bimodal Insertion Policy) most of the times places the new data in the LRU position and sometimes (in-frequently) inserts it in the MRU position. Analogously to Quereshi et al., they presented two different approaches: Static RRIP (SRRIP) which inserts each new block with an intermediate re-reference, and Bimodal RRIP (BRRIP) which inserts most blocks with a distant re-reference interval and sometimes with an intermediate re-reference interval. Their results also show that some processors include adaptive policies whereas others do not.To the best of our knowledge, our work is the first one that provides a comprehensive description of the replacement policies implemented on modern Intel processors up to the point that we are able to accurately determine which element of the set would be evicted using the information about the sequence of accesses. Whenever the pattern of memory accesses of a security-critical piece of software depends on the actual value of sensible data, such as a secret key, this sensitive data can be deduced by an attacker and will no longer be private.Traditionally, cache attacks have been grouped into three categories [16]: FLUSH+RELOAD, PRIME+PROBE and EVICT+TIME. They consist of three stages: initialization (the attacker prepares the cache somehow), waiting (the attacker waits while the victim executes) and recovering (the attacker checks the state of the cache to retrieve information about the victim). This variant recovers the information by measuring the execution time of the clflush instruction instead of the reload time, thus avoiding direct cache accesses and, as a consequence, detection. Then, he waits and finally probes the desired set looking for time variations that carry information about the victim activity.This attack was first proposed for the L1 data cache in [48] and was later expanded to the L1 instruction cache [6]. It first starts a transaction to prime the targeted set, waits and finally it may or may not receive and abort depending on whether the victim has or has not accessed this set. This kind of tools is effective before malware distribution or execution, but their effectiveness is reduced in cloud environments where the attacker does not need to infect the victim.For these reasons, we believe that the only countermeasures that an attacker may have to face when trying to retrieve information from a victim, are detection based countermeasures that can be implemented at user level. Attacks that assume a pseudo LRU eviction policy such as PRIME+PROBE or EVICT+RELOAD can benefit from detailed knowledge of the eviction policy, and can also benefit one attacker wishing to carry out a "stealthy" attack that does not cause cache misses on the victim.In order to study the eviction policy, we try to emulate the hardware in software. Input: Eviction_set, Conflicting_set Output: Accuracy of the policy hits/trials function TESTPOLICY(eviction_set, conflicting_set) hits = 0; while i ≤ num_experiments do j = 0,i++; control_array ← {}; address_array ← {}; initialize_set(); Fills address and control arrays lim = random(); while j ≤ lim do lfence; j++; next_data = eviction_set[random()]; measure time to read next_data; if time ≥ ll_threshold then LLC access update(control_array, next_data); con f _element = con f licting_set[random()]; read(con f _element);Force miss candidate=getEvictionCandidate(); if (testDataEvicted() ==candidate) then hits++; return hits/num_experiments; We have performed experiments in different machines, each of them including an Intel processor from different generations. We have extended our analysis to cover processors from fourth to eighth generation.Before conducting the experiments to disclose the eviction policy implemented in each of the used machines, we have performed some experiments intended to verify that no cached data is evicted in the event of a cache miss if there is free room in the set. Thus, in our procedure, the control bits would be -1 (line empty), 0 (line not recently used), and 1 (line recently used). To obtain the actual control sets within the slice, it is important to test the sets and slices without order, otherwise it may seem that some sets have a fixed policy and they do not.The policy we will uncover is the one implemented in the L3 cache. In fact, over 97% of the evictions have been correctly predicted in all cases 1 , and it is likely that the errors were due to noise.Although we have observed differences between generations and some machines implement set dueling, the decision of which data is going to be evicted is the same in all cases. The remaining processors (6th, 7th and 8th generations) always insert the blocks with age 2, which is equivalent to the mode 1 in the previous generations.In order to help the reader to understand how the cache works, figure 5 shows an example of how the contents of a cache set are updated with each access according to each policy. When enabled, the attacker using the RELOAD+REFRESH technique needs some reverse engineering to retrieve the address he wants to monitor, and he also needs to find an eviction set that maps to the same set as this address.We use Figure 6 to depict the stages of the attack and the possible "states" of the cache set. As a consequence, when reading (RELOAD) the target address again, the attacker will know if the victim has used the data (low reload time) or not (high reload time). The element ev W −1 is forced out of the cache, so it could be used to create a new conflict on the next iteration.When the cache policy is working in mode 2, each element is inserted with age 3. The red arrow points the eviction candidate, that is, the data that would be evicted in case of cache miss.attacker only has to access the first element (eviction candidate) to check whether the victim has or has not accessed the target data. Input: Eviction_set, Target_address Output: Reload time function RELOAD(Target_address,eviction_set) "rdtsc"; "lfence"; read(eviction_set[w − 1]); Forces a miss "lfence"; f lush(eviction_set[w − 1]); "lfence"; read(Target_address); f lush(Target_address); "lfence"; read(Target_address);Reload on first position "lfence"; "rdtsc"; read(eviction_set [0]); return time_reload; Algorithms 2 and 3 summarize the steps of the RELOAD+REFRESH attack when the insertion age is two (newest Intel generations or mode 1 in oldest generations). When initializing the set, we first fill the set, then flush the whole set and finally reload the data again to ensure the insertion order and that the cache state is known by us.In the RELOAD function it is not necessary to flush the Target_address unless it has not been used by the victim. The attacker can Input: Eviction_set Output: Refresh time function REFRESH(Eviction_set) volatile unsigned int time; asm __volatile__( " lfence \n" " rdtsc \n" " movl %%eax, %%esi \n" " movq 8(%1), %%rdi \n" Eviction_set[1] " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" " movq (%%rdi), %%rdi \n" Eviction_set[w-2] " lfence \n" " rdtsc \n" " subl %%esi, %%eax \n" Time value on %eax ); return time_re f resh;gain information about the victim by reloading the target address, and he must begin by refreshing the third element of the eviction set and finish with the first one which will evict the "noise" from the cache, so the age of all the blocks is set to 2 again. If the initial state of the cache is known, this means that at most we need 2 probes to evict the data from the cache, in the case when the access to the target happens in the middle of a probing stage. Since the accesses to the T-Tables depend on the secret key, an attacker monitoring just one line of each T-Table is able to recover the full AES key. Table 3: Mean number of samples required to retrieve each four byte group of the whole AES key when monitoring one line per encryption, and the corresponding F-Score. The reason is that in previous experiments that we have conducted, the eviction rate we achieved with the zig-zag pattern was below 80% using just one probe per measurement.Additionally, we use the rdtsc instruction to measure the time it takes to complete each encryption and show the results in Figure 8. RSA involves a public key (used for encryption) and a private key (used for decryption). As we did before, we performed the attack using our stealthy technique as well as using the FLUSH+RELOAD and PRIME+PROBE techniques.The targeted crypto library is libgcrypt version 1.5.0, which includes the aforementioned square and multiply implementation. The slight misalignment between the two traces occurs because the RSA execution timestamp is collected after each exponent bit has been processed, and the timestamp of the attack samples after the reload operation has finished.The results of our experiments are summarized in table 4. For this reason, the total amount of misses per encryption cannot be used to detect ongoing attacks, thus the cache misses have to be measured concurrently with the execution of the decryption. Figure 10 shows the section of the decryption process in which the number of misses has become stable for the different scenarios.The RELOAD+REFRESH approach (as well as the other attacks) are not synchronized with the decryption operation, as a result, there are situations in which both victim and attacker can try to access the target date simultaneously. As a consequence, detection systems cannot be arbitrarily expanded to deal with future attacks.With the aim of quantifying the effect that our proposal has on the victim, and in order to provide some insights about which counters should a detection system consider to deal with RELOAD+REFRESH, we have periodically monitored different counters when executing the attacks against AES and RSA and analyzed the outcomes. Real execution Retrieved data Figure 9: Example of a retrieved trace referred to an execution of a RSA decryption.