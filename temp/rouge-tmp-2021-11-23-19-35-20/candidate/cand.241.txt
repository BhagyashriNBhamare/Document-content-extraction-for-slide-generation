This has resulted in the surge of Machine Learning-as-a-Service (MLaaS)-cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. For example, in many real-world applications, the trained models are privacy-sensitive -a model can (a) leak sensitive information about training data [5] during/after training, and (b) can itself have commercial value or can be used in security applications that assume its secrecy (e.g., spam filters, fraud detection etc. [29,38,53]). When the label domain Y is finite (classification problem), the 0-1 loss function is frequently used:ℓ( ˆ f , z) = 0, ifˆfifˆ ifˆf (x) = y 1, otherwiseIf the label domain Y is continuous, one can use the square loss: ℓ( ˆ f , z) = ( ˆ f (x) − y) 2 . An algorithm A is a PAC passive learning algorithm for the hypothesis class F if the following holds for any D on X × Y and any ε, δ ∈ (0, 1): If A is given s A (ε, δ) i.i.d. data-points generated by D, then A outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ min f ∈F Err D ( f ) + ε with probability at least 1 − δ. A PAC passive learning algorithm A in the realizable case takes s A (ε, δ) i.i.d. instances generated by D and the corresponding labels generated using f * , and outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ ε with probability at least 1 − δ. In active learning, the learning algorithm is allowed to select a subset of unlabeled instances, query their corresponding labels from an annotator (i.e. oracle) and then use it to construct or update a model. Stream-based sampling represents scenarios where obtaining unlabeled data-points is easy and cheap, but obtaining their labels is expensive (e.g., stream of data is collected by a sensor, but the labeling requires an expert). Before describing query synthesis active learning, we wish to highlight the advantage of PAC active learning over passive PAC learning (i.e. the reduced sample complexity) for some hypothesis class through Example 1. PAC learning theory states that this can be achieved with˜Owith˜ with˜O ( f w (x) = −1 if w, x < −1 +1 otherwise R −1 −1 −1 +1 +1 +1 +1 +1w * Figure 2: Halfspace classification in dimension 1. In this scenario, the learner can request labels for any instance in the input space X, including points that the learner generates de novo, independent of the distribution D (e.g., Lcan ask for labels for those x that have zero-probability of being sampled according to D). Assume that the oracle O knows f * ∈ F and uses it as labeling function (realizable case), then the uniform error of the hypothesisˆfhypothesisˆ hypothesisˆf is defined asErr u ( ˆ f ) = Pr x∼{0,1} n [ ˆ f (x) 񮽙 = f * (x)]where x is sampled uniformly at random from the instance space {0, 1} n . Stopping criteria: this is a set of considerations used by L to decide when it must stop querying.Any system (L, O) described as above is an active learning system for F if one of the following holds:-(PAC scenario) For any D on X × Y and any ε, δ ∈ (0, 1), if L is allowed to interact with O using q L (ε, δ) queries, then L outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ min f ∈F Err D ( f ) + ε with probability at least 1 − δ. For any f * ∈ F , if L is allowed to interact with O f * using q L (ε, δ) queries, then L outputsˆfoutputsˆ outputsˆf ∈ F such that Err( ˆ f ) ≤ ε with probability at least 1 − δ.We refer to q L (ε, δ) as the query complexity of L.As we will show in the following section (in particular, refer § 3.2), the query synthesis scenario is more appropriate in casting model extraction attack as active learning when we make no assumptions about the adversary's prior knowledge.Note that, other types queries have been studied in literature. Since many existing MLaaS providers operate in a pay-per-query regime, we use query complexity as a measure of efficiency of such model extraction attacks.Formally, consider the following experiment: an adversary A, who knows the hypothesis class F , has oracle access to a proprietary model f * from F . After a few rounds, A outputs a functionˆffunctionˆ functionˆf that is the adversary's candidate approximation of f * ; the experiment considersˆfconsidersˆ considersˆf a good approximation if its error with respect to the true function f * held by the server is less then a fixed threshold ε. Given a hypothesis class F = { f : X → Y}, fix an error function Err : F → R.Let S be a MLaaS server with the knowledge of a specific f * ∈ F , denoted by S( f * ). The probability is over the randomness of A.In other words, in Definition 3 the success probability of an adversary constrained by a fixed budget for queries is explicitly lower bounded by the quantity γ.Before discussing the connection between model extraction and active learning, we provide an example of a hypothesis class that is easy to extract.Example 2 (Equation-solving attack for linear regression). Let q = q L (ε, δ) and observe thatPr[Exp ε F (S( f * ), A,q) = 1] = Pr[A outputsˆfoutputsˆ outputsˆf and Err( ˆ f ) ≤ ε] = Pr[L outputsˆfoutputsˆ outputsˆf and Err( ˆ f ) ≤ ε] ≥ 1 − δOur observation states that any active learning algorithm in the QS scenario can be used to implement a model extraction attack. Given an n-bit string x = (b 1 , · · · , b n ), b i ∈ {0, 1} as input, the decision tree defines the following computation: the computation starts at the root of the tree T . In particular, if the active learner L of [35] interacts with the oracle O T * where T * ∈ F m n,BT , then L learns g ∈ F n,BF such that Pr x∼{0,1} n [g(x) 񮽙 = T * (x)] ≤ ε with probability at least 1 − δ using a number of queries polynomial in n, m, 1 ε and log( 1 δ ). Furthermore, having a better understanding of model extraction attack and its unavoidable connection with active learning is paramount for designing MLaaS systems that are resilient to model extraction. In the feature space (the domain of Φ) the optimization problem is as follows 5 :min w,b 񮽙w񮽙 2 +C ∑ n i=1 η i such that for 1 ≤ i ≤ n y i ˆ y(x i ) ≥ 1 − η i η i ≥ 0In the formulation given above, ˆ y(x) is equal to w T Φ(x) + b. Recall that prediction of the kSVM is the sign ofˆyofˆ ofˆy(x), sô y(x) is the "pre sign" value of the prediction. Note that for some kernels (e.g. RBF) Φ is infinite dimensional, so one generally uses the "kernel trick"i.e. one solves the dual of the above problem and obtains a kernel expansion, so thatˆ y(x) = n ∑ i=1 α i K(x, x i ) + bThe vectors x 1 , · · · , x n are called support vectors. In the initial stage (t = 0) we draw r instances x 1 , · · · , x r from the uniform distribution, query their labels, and create an initial model M 0 . Round t works as follows: create h labeled instances using a strategy St T (M t−1 , h) (note that the strategy St is oracle access to the teacher, and takes as parameters model from the previous round and number of labeled instances to be generated). Ideally, St T (M t−1 , h) should be instances that the model M t−1 is least confident about or closest to the decision boundary.Tramèr et al. use line search as their strategy St T (M t−1 , h), which can lead to several queries (each step in the binary search leads to a query). Our strategy St T (M t−1 , 1) (note that we only add one labeled sample at each iteration) works as follows: we generate k random points x 1 , · · · , x k and then computê y i (x i ) for each x i (recall thatˆythatˆ thatˆy i (x i ) is the "pre sign" prediction of x i on the SVM M t−1 . Our discussion will be specialized to decision trees and random forests, but the ideas that are described are general.Let H be the hypothesis class (i.e. space of decision trees or random forests), X is the space of data, and Y is the space of labels. We also define a set S i (S 0 = / 0) recursively as follows: If the label for x i is not queried, then S i = S i−1 ; otherwise S i = S i−1 ∪ (x i , y i , p i ). Without loss of generality, let us say on x n RF n (x n ) = +1 (the case when the label is −1 is symmetric) and there are r trees in RF n (denoted by RF +1 n (x n )) such that their labels on x n are +1. The design of a good defense strategy is an open problem; we believe this is an interesting direction for future work where the ML and security communities can fruitfully collaborate.In this section, we assume that the MLaaS server S with the knowledge of f * , S( f * ), has the freedom to modify the prediction before forwarding it to the client. In the discrete case, we represent this with the notationρ D ( f * , x) = Pr[Y x 񮽙 = f * (x)],(3)where Y x is the random variable that represents the answer of the server S D ( f * ) to the query x (e.g., ˜ y ← Y x ). For any D, randomized procedure for returning labels, such that there exists f * ∈ F with ρ D ( f * ) < 1 2 , there exists an adversary that, interacting with S D ( f * ), can implement an ε-extraction attack with confidence 1 − 2δ and complexity q =8 (1−2ρ D ( f * )) 2 q(ε, δ) ln q(ε,δ) δ . An adversary implementing the Chen et al. algorithm [16] is even more efficient than the adversary˜Aadversary˜ adversary˜A defined in the proof of Proposition 1 (i.e., the total number of queries only increases by a constant multiplicative factor instead of ln q(ε, δ)). To elaborate, the algorithm learns the mean µ and the covariance Σ for a multivariate Gaussian distribution N (µ, Σ) on F d,HS such that any model drawn from N (µ, Σ) provides an accurate prediction. In particular, they use PAC active learning algorithms [9,17] (assuming that the underlying distribution D is Gaussian) to learn an approximationˆwapproximationˆ approximationˆw from queries answered in three different ways: (a) with their strategy, i.e. using a new model for each query, (b) using a fixed model to compute all labels, and (c) using a fixed model and adding independent noise to each label, i.e. y = sign(w, x + η) and η ← [−1, +1]. However, notice that this is not always the case in the multiclass setting: For example, consider the case when the answer to query x i is defined to be wrong with probability ≥ 1 2 and, when wrong, is sampled uniformly at random among the k − 1 classes that are different to the true class f * (x), then if k is large enough, y i defined via the majority vote is likely to be still correct. They rely on the server's response to incomplete queries, and the addition of node identifiers to the generated outputs to recreate the tree. We can see that while the number of queries required to launch such extraction attacks is greater than in the approach proposed 8 such a local model is seeded with uniformly random points labeled by the oracle by Tramèr et al., such an approach obtains comparable test error to the oracle. In her seminal work, Angluin [4] proposes a learning algorithm, L * , to correctly learn a regular set from any minimally adequate teacher, in polynomial time. In other domains, program synthesis using input-output example pairs (e.g., [25,58]) also follows a similar principle.If the adversary had access to a subset of the training data, or had prior knowledge of the distribution from which this data was drawn from, it could launch a different set of attacks based on the algorithms discussed below. He claims that in this setting, AL algorithms usually follow one of the following two strategies -(i) Efficient search in the hypothesis spaces (as in the algorithm proposed by Chen et al. [16], or by Cohn et al. [17]), or (ii) Exploiting clusters in the data (as in the algorithm proposed by Dasgupta et al. [22]). Most work in active learning has assumed that the correct hypothesis space for the task is already known i.e. if the model being learned is for logistic regression, or is a neural network and so on. Specifically: (1) As noted by Dasgupta [20], the label complexity of PAC active learning depends heavily on the specific target hypothesis, and can range from O(log 1 ε ) to Ω( 1 ε ). In this paper, we formalize model extraction in the context of Machine-Learning-as-a-Service (MLaaS) servers that return only prediction values (i.e., oracle access setting), and we study its relation with query synthesis active learning (Observation 1).