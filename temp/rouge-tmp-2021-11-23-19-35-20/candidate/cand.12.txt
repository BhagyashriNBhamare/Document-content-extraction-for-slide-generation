In the last decade, we have witnessed scores of adhoc approaches that have turned out to be inadequate for protecting privacy [33,23]. Differential privacy [10,9,11], which has become the gold standard privacy guarantee in the academic literature, and is gaining traction in industry and government [13,17,28], overcomes the prior issues by focusing on the privatization algorithm applied to the data, requiring that it preserves privacy in a mathematically rigorous sense under an assumption of an omnipotent adversary.There are two primary models in the differential privacy framework that define how data is to be handled by the users and data collectors: the trusted curator model and the local model.Trusted curator model: Most differentially private algorithms developed to date operate in the trusted curator model: all users' data is collected by the curator before privatization techniques are applied to it. Over the last several years, we have seen some examples of the local model beginning to be used for data collection in practice, most notably in the context of the Chrome web browser [13] and Apple's data collection [17]. Differential privacy: Formally, an algorithm A is (, δ)-differentially private [11] if and only if for all neighboring databases D and D differing in precisely one user's data, the following inequality is satisfied for all possible sets of outputs Y ⊆ Range(A):Pr[A(D) ∈ Y ] ≤ e Pr[A(D ) ∈ Y ] + δ.The definition of what it means for an algorithm to preserve differential privacy is the same for both the trusted curator model and the local model. The only distinction is in the timing of when the privacy perturbation needs to be applied -in the local model, the data needs to undergo a privacypreserving perturbation before it is sent to the aggregator, whereas in the trusted curator model the aggregator may first collect all the data, and then apply a privacy-preserving perturbation. In the trusted curator model, D represents data of all users and D represents data of all users, except values of one of the user's data may be altered.Current differential privacy literature considers the trusted curator model and the local model entirely independently. Local search revolves around the problem of how a browser maker can collect information about users' clicks as they interact with search engines in order to create the head of the search, i.e., the collection of the most popular queries and their corresponding URLs, and make it available to users locally, i.e., on their devices. Furthermore, it would not be unusual or require a significantly novel infrastructure, as plenty of data is delivered to the browser today, such as SafeBrowsing malware databases in Chrome and Firefox, Microsoft SmartScreen data in Internet Explorer, blocking lists for extensions such as AdBlock Plus, etc. An example of this is the Google trends service 3 , which has an always up-to-date list of trending topics and queries.Although trend computation is interesting, local search is a great deal harder to do well on while preserving most of the utility. Furthermore, the privatized results obtained from the opt-in group and from the clients are then "blended" in a way that takes into account the privatization algorithms used for each group, and thus, again, achieving an improved utility over a lessinformed combination of data from the two groups. Blender then aggregates all the clients' reports and, using a statistical denoising procedure, estimates both the probability for each record in the head list as well as the variance of each of the estimated probabilities based on the clients' data.For each record, Blender combines the record's probability estimates obtained from the two groups. Finally, Blender outputs the obtained records and their combined probability estimates, which can be used to drive local search, determine trends, etc.A Formal Overview of Blender: Figure 2 presents the precise algorithmic overview of each step, including key parameters.Lines 1-3 of Blender describe the treatment of data from opt-in users, line 4 -the treatment of clients, and line 5 -the process for combining the probability estimates obtained from the two groups. Among the parameters of Blender, the first four (the privacy parameters and the sets of opt-in and client users) can be viewed as given externally, whereas the following five (the number of records collected from each user and the distribution of the privacy budget among the sub-algorithms' sub-components) can be viewed as knobs the designer of Blender is at liberty to tweak in order to improve the overall utility of Blender's results. Algorithms for Head List Creation and Probability Estimation Based on Opt-in User Data ( Figures 3, 4): The opt-in users are partitioned into two sets -S, whose data will be used for initial head list creation, and T , whose data will be used to estimate the probabilities and variances of records from the formed initial head list.The initial head list creation algorithm, described in Figure 3, constructs the list in a differentially private manner using search record data from group S. The algorithm follows the strategy introduced in [26] by aggregating the records of theBlender (, δ, O, C, m O , m C , f O , f C , M )Parameters:• , δ: the differential privacy parameters. • O, C: the set of opt-in users and clients, respectively. Our algorithm differs from previous work in two ways: 1) it replaces the collection and thresholding of queries with the collection and thresholding of records (i.e., query -URL pairs) and 2) its definition of neighboring databases is that of databases differing in values of one user's records, rather than CreateHeadList(, δ, S, m O )Parameters:• , δ: the differential privacy parameters. 1: let N (r, D) = number of times an arbitrary record r appears in the given dataset D.2: for each user i ∈ S do 3:let D S,i be the database aggregating at most m O arbitrary records from i. 4: let D S be the concatenation of all D S,i databases. We introduce a wildcard record , to represent records not included in the head list, for the subsequent task of estimating their aggregate probability.For each record included in the initial head list, the algorithm described in Figure 4 uses the remaining opt-in users' data (from set T ) to differentially privately estimate their probabilities, denoted byˆpbyˆ byˆp O . We need to set m O = 1 for the privacy guarantees to hold, because we treat data at the search record rather than query level.We form the final head list from the M most frequent records inˆpinˆ inˆp O . 13: ˆ σ 2 O,q,u = ˆ p O,q,u (1− ˆ p O,q,u ) |D T |−1 + 2b 2 T |D T |·(|D T |−1)16: return HL, ˆ p O , ˆ σ 2 O . :ˆ p C,q = ˆ r C,q − 1−t k−1 t− 1−t k−1 13: ˆ σ 2 C,q = 1 t− 1−t k−1 2 ˆ r C,q (1−ˆ r C,q ) |D C |−1 14: for u ∈ HL[q] do 15:letˆrletˆ letˆr C,q,u be the fraction of records which are q, u in D C . The value for each q ∈ HL is defined as HL[q] = {u1, . . . , u l , }, representing all URLs in the head list associated with q.Body 1: let D C,i be the database aggregating at most m C records from current client i. 2: = /m C , and δ = δ/m C . 20: continue 21:report q, u. of the most frequent records) is available to each client plays a crucial role in improving the utility of the data produced by this privatization algorithm compared to the previously known algorithms operating in the local privacy model. LocalAlg is responsible for the privacy-preserving perturbation of each client's data before it gets sent to the server, and EstimateClientProbabilities is responsible for aggregating the received privatized data into a meaningful statistic. When m O = 1 the unbiased variance estimate for EstimateOptinProbabilities can be computed as:ˆ σ 2 O,q,u = |D T | |D T |−1 ˆ p O,q,u (1− ˆ p O,q,u ) |D T | + 2 b T |D T | 2 . Since changes to the probabilities may not result in ranking changes, L1 is an even less forgiving measure than NDCG.Since the purpose of Blender is to estimate probabilities of the top records, we discard the artificially added queries and URLs and rescale rel i prior to L1 and NDCG computations. Figure 8 compares their characteristics.Data analysis: To familiarize the reader with the approach we used for assessing result quality, Fig- ure 9 shows the top-10 most frequent queries in the AOL data set, with the estimates given by the different "ingredients" of Blender.The table is sorted by column 2, which contains the non-private, empirical probabilities p q for each query q from the AOL data set with 1 random record sampled from each user. We now describe and, whenever possible, motivate, our choices for these.Privacy parameters, and δ: Academic literature on differential privacy views the selection of the parameter as a "social question" [9] and thus uses in the range of 0.01 to 10 for evaluating algorithm performance (see Table 1 in [18]). Similar to the case with small opt-in percentages, having too small an makes it difficult to achieve head lists of their target size; e.g., in Figure 15a, the line for a head list of size 50 does not begin until = 3 because that size head list was not created with a smaller value.Evaluation of local search computation: Fig- ure 14 shows the NDCG measurements as a function of the opt-in percentage ranging between 1% and 10%. For example, [24,26,16,31] address the problem of publishing a subset of the data contained in a search log with differential privacy guarantees; [27] and [6] propose approaches for frequent item identification; [14] propose an approach for monitoring aggregated web browsing activities; and so on. Improvement in utility over what can be achieved in the local model comes from two sources: the hybrid privacy model lets us develop a better algorithm for client data collection and the analysis of algorithms' variances lets us smartly combine the results.In practice, a system for local search or trend computation would be run at regular intervals in order to refresh the data as well as accommodate for users being added to, removed from, or moving between the opt-in and the client groups. Using local search as a motivating application, we demonstrated that our proposed approach leads to a significant improvement in terms of utility, bridging the gap between theory and practicality.Future work: We plan to continue this work in two directions: first, to address any systems and engineering challenges to Blender's adoption in practice, including those that arise due to data changing over time; and second, to develop algorithms for other settings where the hybrid privacy model is appropriate, thus facilitating adoption of differential privacy in practice by minimizing the utility impact of privacy-preserving data collection. By the design of the algorithm, Pr[L(q HL ) = q HL ] = t and Pr[L(¯ q HL ) = q HL ] = (1−t)( 1 k−1 ), where ¯ q HL represents any query not equal to q HL . Pr[L(q) ∈ Y ] = q HL ∈Y Pr[L(q) = q HL ] = q HL ∈Y \{q,q } Pr[L(q) = q HL ] + q HL ∈Y ∩{q,q } P r[L(q) = q HL ] = q HL ∈Y \{q,q } Pr[L(q ) = q HL ] + q HL ∈Y ∩q,q Pr[L(q) = q HL ] (2) ≤ q HL ∈Y \{q,q } Pr[L(q ) = q HL ] + q HL ∈Y ∩{q,q } e Q Pr[L(q ) = q HL ] + δ Q 2 (3) ≤ e Q q HL ∈Y Pr[L(q ) = q HL ] + 2 · δ Q 2 = e Q Pr[L(q ) ∈ Y ] + δ Q ,Equality (2) stems from the fact that the probability of reporting a false query is independent of the user's true query. Privacy of URL Reporting:With t q defined as t q = exp( U )+0.5δ U (kq−1) exp( U )+kq−1, an analogous argument shows that the ( U , δ U )-differential privacy constraints hold if the original q is kept. By the design of our algorithm,r C,q = t · p q + q =q p q (1 − t) 1 k−1 = t · p q + 1−t k−1 q =q p q = t · p q + 1−t k−1 (1 − p q ). The empirical estimator for r O,q,u isˆrisˆ isˆr O,q,u = 1 |D T | |D T | j=1 X j + Y , where X j ∼ Bernoulli(p q,u ) is the random variable indicating whether report j was record q, u.The expectation of this estimator is given by E[ˆ r O,q,u ] = p q,u . The variance for this estimator isσ 2 O,q,u = V [ˆ p O,q,u ] = V 1 |D T | |D T | j=1 X j + Y = 1 |D T | 2 V |D T | j=1 X j + V [Y ] (4) = 1 |D T | 2 |D T | j=1 V [X j ] + V [Y ] (5) = 1 |D T | 2 |D T | · p q,u (1 − p q,u ) + 2 b T |D T | 2 = p q,u (1 − p q,u ) |D T | + 2 b T |D T | 2 . With the variance estimates for each algorithm fully computed, a blended estimate of p q,u is given byˆpbyˆ byˆp q,u = w q,u · ˆ p O,q,u +(1−w q,u )· ˆ p C,q,u , which has sample variancê σ 2 q,u = w 2 q,u · ˆ σ 2 O,q,u + (1 − w q,u ) 2 · ˆ σ 2 C,q,u .