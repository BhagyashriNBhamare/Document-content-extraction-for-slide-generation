In poisoning availability attacks, the attacker controls a certain amount of training data, thus influencing the trained model and ultimately the predictions at testing time on most points in testing set [4, 18, 20, 28-30, 34, 36, 41, 48]. We provide a formal definition of transferability and show that, under linearization of the loss function computed under attack, several main factors impact transferability: the intrinsic adversarial vulnerability of the target model, the complexity of the surrogate model used to optimize the attacks, and its alignment with the target model. Furthermore, we derive a new poisoning attack for logistic regression, and perform a comprehensive evaluation of both evasion and poisoning attacks on multiple datasets, confirming our theoretical analysis.In more detail, the contributions of our work are:Optimization framework for evasion and poisoning attacks. Our formal definition unveils that transferability depends on: (1) the size of input gradients of the target classifier; (2) how well the gradients of the surrogate and target models align; and (3) the variance of the loss landscape optimized to generate the attack points.Comprehensive experimental evaluation of transferability. We then formally define transferability for both evasion and poisoning attacks, and show its approximate connection with the input gradients used to craft the corresponding attack samples (Section 4). In this paper, we consider a range of adversarial models against machine learning classifiers at both training and testing time.Attackers are defined by: (i) their goal or objective in attacking the system; (ii) their knowledge of the system; (iii) their capabilities in influencing the system through manipulation of the input data. We use L(D, w) to denote the loss incurred by classifier f : X → Y (parameterized by w) on D. Typically, this is computed by averaging a loss function (y, x, w) computed on each data point, i.e., L(D, w) = 1 n ∑ n i=1 (y i , x i , w). We characterize the attacker's knowledge κ as a tuple in an abstract knowledge space K consisting of four main dimensions, respectively representing knowledge of: (k.i) the training data D; (k.ii) the feature set X ; (k.iii) the learning algorithm f , along with the objective function L minimized during training; and (k.iv) the parameters w learned after training the model. For images, this means that we do consider pixels as the input features, consistently with other recent work on black-box attacks against machine learning [32,33]. We consider the most realistic attack model in which the attacker does not have querying access to the classifier.The attacker can collect a surrogate datasetˆDdatasetˆ datasetˆD, ideally sampled from the same underlying data distribution as D, and train a surrogate modeî f on such data to approximate the target function f . These constraints can be generally accounted for in the definition of the optimal attack strategy by assuming that the initial attack sample x can only be modified according to a space of possible modifications Φ(x). Given the attacker's knowledge κ ∈ K and an attack samplex ∈ Φ(x) along with its label y, the attacker's goal can be defined in terms of an objective function A(x , y, κ) ∈ R (e.g., a loss function) which measures how effective the attack sample x is. Output: x : the adversarial example.1: Initialize the attack sample: x ← x 2: repeat 3:Store attack from previous iteration: x ← x 4:Update step: x ← Π Φ (x + η∇ x A(x,y,κ)), where the step size η is chosen with line search (bisection method), and Π Φ ensures projection on the feasible domain Φ.5: until |A(x , y, κ) − A(x,y,κ)| ≤ t 6: return x particular, as in the case of poisoning attacks, the attacker can maximize the objective by iteratively optimizing one attack point at a time [5,48]. We finally remark that non-differentiable learning algorithms, like decision trees and random forests, can be attacked with more complex strategies [17,19] or using gradient-based optimization against a differentiable surrogate learner [31,37]. The manipulation constraints Φ(x) are given in terms of: (i) a distance constraint x − x p ≤ ε, which sets a bound on the maximum input perturbation between x (i.e., the input sample) and the corresponding modified adversarial example x ; and (ii) a box constraint x lb x x ub (where u v means that each element of u has to be not greater than the corresponding element in v), which bounds the values of the attack sample x . The box constraint can be used to bound each pixel value between 0 and 255, or to ensure manipulation of only a specific region of the image. This is substantially different from crafting minimumdistance adversarial examples, as formulated in [42] and in follow-up work (e.g., [33]). In addition to starting the gradient ascent from the initial point x, for nonlinear classifiers we also consider starting the gradient ascent from the projection of a randomly-chosen point of the opposite class onto the feasible domain. Poisoning is formulated as a bilevel optimization problem in which the outer optimization maximizes the attacker's objective A (typically, a loss function L computed on untainted data), while the inner optimization amounts to learning the classifier on the poisoned training data [4,24,48]. In fact, this gradient has to capture the implicit dependency of the optimal parameter vector w (learned after training) on the poisoning point being optimized, as the classification function changes while this point is updated. Provided that the attacker function is differentiable w.r.t. w and x, the required gradient can be computed using the chain rule [4,5,24,27,48]:∇ x A = ∇ x L + ∂w ∂x ∇ w L ,(7)where the term ∂w ∂x captures the implicit dependency of the parameters w on the poisoning point x. Under some regularity conditions, this derivative can be computed by replacing the inner optimization problem with its stationarity (KarushKuhn-Tucker, KKT) conditions, i.e., with its implicit equation [24,27]. Using logistic loss as the attacker's loss, the poisoning gradient for logistic regression can be computed as:∇ x c A = − ∇ x c ∇ θ L C z c θ ∇ 2 θ L X z C C z X C ∑ n i z i −1 X(y • σ − y) y (σ − 1) C,where θ are the classifier weights (bias excluded), • is the element-wise product, z is equal to σ(1 − σ), σ is the sigmoid of the signed discriminant function (each element of that vector is therefore:σ i = 1 1+exp(−y i f i ) with f i = x i θ + b), and:∇ 2 θ L = C n ∑ i x i z i x i + I,(11)∇ x c ∇ θ L = C(I • (y c σ c − y c ) + z c θx c )(12)In the above equations, I is the identity matrix. (14)Under the same linear approximation, this corresponds to the maximization of an inner product over an ε-sized ball:max δ p ≤ε δ ∇ x (y, x, ˆ w) = ε∇ x (y, x, ˆ w) q ,(15)where q is the dual norm of p . (13), we can compute the loss increment ∆ = ˆ δ ∇ x (y, x, w) under a transfer attack in closed form; e.g., for p = 2, it is given as:∆ = ε ∇ x ˆ ∇ x ˆ 2 ∇ x ≤ ε∇ x 2 ,(16)where, for compactness, we usê = (y, x, ˆ w) and = (y, x, w). Figure 3: Size of input gradients (averaged on the test set) and test error (in the absence and presence of evasion attacks) against regularization (controlled via weight decay) for a neural network trained on MNIST89 (see Sect. 5.1.1). In Fig. 3 we report an example showing how increasing regularization (i.e., decreasing complexity) for a neural network trained on MNIST89 (see Sect. 5.1.1), by controlling its weight decay, reduces the average size of its input gradients, improving adversarial robustness to evasion. Differently from the gradient size S, gradient alignment is a pairwise metric, allowing comparisons across different surrogate models; e.g., if a surrogate SVM is better aligned with the target model than another surrogate, we can expect that attacks targeting the surrogate SVM will transfer better.R(x, y) = ∇ x ˆ ∇ x ∇ x ˆ 2 ∇ x 2 . Accordingly, if this loss landscape changes dramatically even when simply resampling the surrogate training set (which may happen, e.g., for surrogate models exhibiting a large error variance, like neural networks and decision trees), it is very likely that the local optima of the corresponding optimization problem will change, and this may in turn imply that the attacks will not transfer correctly to the target learner.We define the variability of the loss landscape simply as the variance of the loss, estimated at a given attack point x, y:V (x, y) = E D {(y, x, ˆ w) 2 } − E D {(y, x, ˆ w)} 2 ,(19)where E D is the expectation taken with respect to different (surrogate) training sets. The top row shows results for surrogates trained using only 20% of the surrogate training data, while in the bottom row surrogates are trained using all surrogate data, i.e., a training set of the same size as that of the target. Another interesting, related observation is that the adversarial examples computed against lower-complexity surrogates have to be perturbed more to evade (see Fig. 9), whereas the perturbation of the ones computed against complex models .19 .57S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R FS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F.64.55.58 .94 (a) ε = 1 .51S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R FS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R FS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F.21. We thus consider as surrogate learners: (i) linear SVMs with C = 0.01 (SVM L ) and C = 100 (SVM H ); (ii) logistic classifiers with C = 0.01 (logistic L ) and C = 10 (logistic H ); (iii) ridge classifiers with α = 100 (ridge L ) and α = 10 (ridge H ); and (iv) SVMs with RBF kernel with γ = 0.01 and C = 1 (SVM-RBF L ) and C = 100 (SVM-RBF H ). We additionally consider as target classifiers: (i) random forests with 100 base trees, each with a maximum depth of 6 for RF L , and with no limit on the maximum depth for RF H ; (ii) feed-forward neural networks with two hidden layers of 200 neurons each and ReLU activations, trained via cross-entropy loss minimization with different regularization (NN L with weight decay 0.01 and NN H with no decay); and (iii) the Convolutional Neural Network (CNN) used in [7]. In fact, from Fig. 15b it is evident that increasing V is even beneficial for SVM-based surrogates (and all these results are statistically significant according to the p-values in the sixth column of Table 1). .18 .54S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVMLS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVMLS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML.60.57.59 t r a n s fe r r a t e .14S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML.20.20.23.15.19.15.19.21.21 (b) ε = 10 t r a n s fe r r a t e .43(a) ε = 5 S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVMLS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML.62.53.64. (a) 5% poisoning t r a n s fe r r a t e .08S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C NS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N.06.10.11.11.12. The reason is that the poisoning objective function is relatively stable (i.e., it has low variance) for most classifiers, and gradient alignment between surrogate and target becomes a more important factor.Understanding attack transferability has two main implications. In particular, we have defined three metrics that impact the transferability of an attack, including the complexity of the target model, the gradient alignment between the surrogate and (a) 5% poisoning (b) 10% poisoning (c) 20% poisoning target models, and the variance of the attacker optimization objective. We would also like to thank Toyota ITC for funding this research.S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C NS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C NS V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N The authors would like to thank Neil Gong for shepherding our paper and the anonymous reviewers for their constructive feedback.