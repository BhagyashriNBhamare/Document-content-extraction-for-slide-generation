Regular expression denial of service (ReDoS) is a class of algorithmic complexity attacks where matching a regular expression against an attacker-provided input takes unexpectedly long. 1 Matching a sequence of 35 "a" characters already takes over 8 minutes, i.e., the matching time explodes exponentially.If a server implementation suffers from this kind of performance problem, then an attacker can exploit it to overwhelm the server with hard-to-match inputs. Since for some regular expressions, the worst-case complexity is much higher than the average-case complexity, an attacker can cause denial of service with a few, relatively small inputs.Even though ReDoS has been known for several years, recent developments in the web server landscape bring new and increased attention to the problem. As a result, a single request can effectively block the main thread, making the web server unresponsive to any other incoming requests and preventing it from finishing any other already established requests.Despite the importance of ReDoS in web servers, there is currently little reported knowledge about the prevalence of ReDoS vulnerabilities in real-world websites. Based on experiments with locally installed versions of the vulnerable server-side libraries, attacking these websites with crafted inputs can cause a web server to remain unresponsive for several seconds or even minutes. • A benchmark of previously unreported ReDoS vulnerabilities and ready-to-use exploits, which we make available for future research on finding, fixing, and mitigating ReDoS vulnerabilities:https://github.com/sola-da/ReDoS-vulnerabilities Regular expressions are used to check whether a given sequence of characters matches a specified pattern. Most implementations in modern programming languages address this problem by converting the regular expression into an automaton [38] and through a backtracking-based search for a sequence of transitions from the initial to an accepting state that consumes the given string. After multiple explorations the algorithm identifies the sequence of transitions s → 3 → 4 → 5 → 4 → 5 → 6 → 7 → a, which reaches the accepting state and consumes all characters of the input string. Other regular expression even have exponential complexity, e.g., because of nested repetitions, such as in /^(a*)*b$/. JavaScript is becoming more and more popular, including the server-side Node.js platform, which advocates a single-threaded, event-based execution model that uses servers, such as Apache, the single-threaded execution model compounds the problem in JavaScript. Depending on the number of available parallel processing units, the operating system, and the thread pool size, new requests can still be handled even with hundred of busy threads running. The overall goals of the methodology are to understand (i) how widespread such vulnerabilities are, (ii) whether an attacker could exploit them to affect the availability of live websites, and (iii) to what extent existing defense mechanisms address the problem. The first challenge is a technical problem: Since the server-side source code of most websites is not available, how to know what vulnerabilities a website suffers from? After removing regular expressions that contain no repetitions, and hence are immune to algorithmic complexity attacks, we obtain a total of 138,123 expressions, with mean 37.93 and median 4.00 per module.Next, we semi-automatically search for regular expression patterns that are known to be vulnerable. To this end, we focus on (i) modules included in the Express framework, (ii) middleware modules that extend this framework, and (iii) modules that manipulate HTTP request components, such as the body or a specific header. Overall, it took one of the authors only a couple of days to find 25 such vulnerabilities in widely used npm modules, showing that a skilled individual can attack real-world websites with moderate effort. If we succeed in crafting an input that takes more than five seconds, we consider the vulnerability as exploitable and consider it for the remainder of the study.To further assess the impact of the exploits, we measure how much longer it takes to process a crafted input compared to a random string of the same length. We address this challenge by experimenting on a locally installed version of the vulnerable package and by choosing input sizes that take approximately 100ms, 200ms, 500ms, 1s and 2s to respond to.Our setup allows us to assess whether a website could be exploited without actually attacking it. We perform our measurements using three different machines depending on the experiments: a ThinkPad 440s laptop with four Intel i7 CPUs and 12GB memory (Section 4.1), a third party commercial web server with 512MB memory (Section 4.3 and 4.4) and a server with 48 Intel Xeon CPUs and 64GB memory (from Section 4.6 on). 7 As explained in Section 3.3, we try to create exploits for the vulnerabilities by hypothesizing how web server implementations may use the vulnerable modules. 4 forwarded / * , * / X- Forwarded- ForThe website uses express and the "trust proxy" option is set. That is, the attacker needs to figure out which part of the website may use a markdown parser and how to provide a document that will be processed by the parser. We consider any of these eight exploits to be harmful because they may impact a website's availability (Section 4.3 and because even a non-exponential ReDoS vulnerability may aid an attacker in mounting a DoS attack (Section 5.1). To show the threat to availability posed by ReDoS exploits, we create a simple Express application with two features: it replies with a "hello world" message when called at the "/echo" path, and it calls the forwarded module with the request headers when called at the "/redos" path. The victim machine starts its requests immediately after the victim machine has triggered its requests.We vary the payload size from 0 characters to 8,000 characters in increments of 1,000 characters. This finding shows that the ReDoS payloads have a cumulative effect and even a small delay in the main loop can cause significant harm for availability.We remind the reader that the above experiment uses the smallest payload in our data set, forwarded. The strong correlation shows that the delays introduced by the network layer are relatively constant over time and that the server computation time is the dominant component in the response time measured at the client-side. Of course, the observed value depends on the chosen web server Module P1: P2: P3: P4: P5: 100ms 200ms 500ms 1s 2s fresh 12,000 17,000 27,000 37,500 53,500 forwarded 12,000 17,000 26,500 38,000 53,500 useragent 500 650 925 1,150 1,450 ua-parser-js 38 39 40 41 42 mobile-detect 10,500 15,500 25,000 36,500 50,500 platform 7,500 11,000 17,500 25,000 34,500 charset 10,500 15,500 24,000 34,000 48,000 content 8,000 11,000 18,000 25,500 35,500Figure 10: Number of characters in each payload needed to achieve a specific delay in a vulnerable module.provider and the current server load, but we can safely conclude that measuring time at the client level is a good enough estimation of the server-side computation time. In Figure 11, we plot for each of the five payload sizes the response time for malicious and random inputs. The reason is the way we dimension our payloads: Many Express instances limit the header size, and hence we cannot send large enough payloads to confirm that the sites are vulnerable. After more careful consideration, we realized that there are two more popular alternatives for parsing the Content-Header and the content package seems to be more popular among users of the hapi.js framework, which is a competitor of Express. From an attacker's perspective, the distribution of vulnerabilities is great news, because exploits are portable across websites and knowing a vulnerabilities is sufficient to attack various websites. For each point p on the horizontal axis, the vertical axis shows the number of exploitable sites with popularity rank ≤ p. For example, there are 61 vulnerable sites in the top 100,000 websites, with one site in top 1,000 and nine in top 10,000. Therefore, the current limits that the websites apply on the header size are insufficient and they do not provide adequate protection against DoS.Another interesting trend to observe in Figure 14 is that even for the most harmful exploit, useragent, for which we require payloads between 38 and 42 characters only, the number of websites that accept larger payloads decreases over time. We address this threat in multiple ways: we show that for commercial web hosting servers there is a high correlation between response time and server CPU time, we repeat measurements multiple times, and we draw conclusions only from statistically significant differences.Another potential concern is that the exploits we created are too generic and happen to cause slowdown in another regular expression than the one we created them for. As shown in Section 4.3, even the least harmful vulnerabilities we identify can be a lethal weapon when used as part of a large-scale DoS attack, because the attacker can send payloads that hang the loop for hundreds of milliseconds, several seconds, or even more, depending on the vulnerability. In contract, in an event-based system, the matching is done in the main loop and spending a few seconds matching a regular expression is equivalent to completely blocking the server for this amount of time.A large-scale ReDoS attack against Node.js-based sites is a bleak scenario for which, as we have shown, many websites are not prepared. *\ bMobile \ b ) / iThis expression from the ismobilejs module contains both lookahead and has super-linear complexity in a backtracking engine.We also recommend that Node.js augments its regular expression APIs with an additional, optional timeout parameter. An attacker may use the ReDoS attack as a hard-to-detect way to scan which sites use the vulnerable module and then attack these sites with a command injection.Server-side JavaScript Ojamaa and Düüna [27] discuss the security of Node.js and identify algorithmic complexity attacks as one of the main threats. Our work differs in three ways: (i) we analyze JavaScript ReDoS, which is more serious than Java ReDoS, (ii) we detect vulnerabilities in real-world websites whose source code is not available for analysis, and (iii) we uncover ReDoS vulnerabilities containing advanced features, e.g. lookahead, that are not supported by any of the previous work. Testing Regular Expressions The problem of generating inputs for regular expressions is also investigated from a software testing perspective [40], [24], [22], [34].