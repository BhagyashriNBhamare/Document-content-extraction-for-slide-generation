The goal of searchable encryption (SE) is to enable a client to execute searches over encrypted files stored on an untrusted server while ensuring some measure of privacy for both the encrypted files and the search queries. The goal of searchable encryption (SE) is to enable a client to perform keyword searches over encrypted files stored on an untrusted server while still guaranteeing some measure of privacy for both the files themselves as well as the client's queries. The prevailing argument is that L1 leakage is inconsequential in practice, and so represents a reasonable sacrifice for obtaining an efficient SE scheme.In truth, the ramifications of different types of leakage are poorly understood; indeed, characterizing the real-world consequences of the leakage of existing SE schemes was highlighted as an important open question in [6]. At a high level, an SE scheme allows a client to store encrypted versions of its files on a server, such that at a later point in time the client can retrieve all files containing a certain keyword (or collection of keywords). We assume an SE scheme in which searching for some keyword k is done via the following process (all efficient SE schemes work in this way): first, the client deterministically computes a token t corresponding to k and sends t to the server; using t, the server then computes and sends back the file identifiers of all files containing keyword k. (These file identifiers need not be "actual" filenames; they can instead simply be pointers to the appropriate encrypted files residing at the server.) This is reasonable to assume, even if file identifiers are chosen randomly by the client, for several reasons: (1) the server can identify the file returned based on its length (even if padding is used to mitigate this, it is impractical to pad every file to the maximum file length); (2) in SE schemes supporting updates, the server can inject a file F and then identify F with the next (encrypted) file uploaded by the client; (3) if the server can influence the queries of the client, or even if it knows some of the client's queries, then the server can use that information to identify specific injected files with particular file identifiers. The basic observation is that if the server injects a file F containing exactly half the keywords from the keyword universe K, then by observing whether the token t sent by the client matches that file (i.e., whether F is returned in response to that token), the server learns one bit of information about the keyword corresponding to t. Using a standard non-adaptive version of binary search, the server can thus use 񮽙log |K|| injected files to determine the keyword exactly. The number of injected files needed for this attack is quite reasonable; with a 10,000-keyword universe, a server who sends only one email per day to the client can inject the necessary files in just 2 weeks.Small keyword universe. This could be done either by simply not indexing files containing more than T keywords (possibly caching such files at the client), or by choosing at most T keywords to index from any file containing more than T keywords.The threshold T can be set to some reasonably small value while not significantly impacting the utility of the SE scheme. Unfortunately, as we explore in detail in the following section, the threshold countermeasure can be defeated using fewer injected files via more-sophisticated attacks.Note also that the threshold countermeasure does not affect the binary-search attack with small keyword universe K 񮽙 ⊂ K, as long as |K 񮽙 | ≤ 2T . Then, in the following section, we show attacks that reduce the number of injected files even further, but based on the assumption that the server has information about some fraction of the client's files. The attack just described is adaptive, in that it targets a particular token t and injects files whose contents depend on the results of a search using t. Given the ground truth, we recover the keyword associated with some other token t 񮽙 using the following observation: if k 񮽙 is the keyword corresponding to t 񮽙 , then the observed joint frequency f (t,t 񮽙 ) should be "close" to the estimated joint frequency f * (k, k 񮽙 ) for all pairs (t, k) in our ground-truth set, where "closeness" is determined by a parameter δ . A larger value of n means that the ground-truth set can potentially be larger, but if n is too large then there is a risk that the candidate universe K t (comprising the 2T /n keywords with estimated frequencies closest to f (t)) will not contain the true keyword corresponding to t. We do so because those keywords can be recovered correctly with higher probability, as we explain next.If the leaked files are chosen uniformly from the set of all files, then using statistical-estimation theory as above the attacker can compute a value δ such that at least 99% of the time it holds that | f * (k) − f (t)| ≤ ε · f * (k), where k denotes the (unknown) keyword corresponding to t. Thus, if the attacker sets the candidate universe K t to be the set of all keywords whose estimated frequencies are within distance ε · f * (k) of f (t), the candidate universe will include the keyword corresponding to t at least 99% of the time. In particular, then, the set of 2T /n keywords with estimated frequencies closest to f (t) (as chosen by our algorithm), will "cover" all keywords within distance ε · f * (k) of f (t) from f (t) -or, equivalently, the candidate universe will contain the true keyword k -with high probability. (We do not run any simulations for the binary-search attack described in Section 3, since this attack succeeds with probability 1, injecting a fixed number of emails.) 9: Add (t, k t ) to G.Recover the remaining tokens, let t 2 be the set of unrecovered tokens.10: for each token t 񮽙 ∈ t 2 do 11: Set its candidate universe K t 񮽙 as the set of 2T keywords with estimated frequencies f * (k) nearest to f (t 񮽙 ). for each keyword k 񮽙 ∈ K t 񮽙 do 13: for each token/keyword pair (t, k) ∈ G do 14: If | f (t,t 񮽙 ) − f * (k, k 񮽙 )| > δ · f * (k, k 񮽙 ), For our experiments we use the Enron email dataset [1], consisting of 30,109 emails from the "sent mail" folder of 150 employees of the Enron corporation that were sent between 2000-2002. We do not include error bars in our figures, but have observed that the standard deviation in our experiments is very small (less than 3% of the average). It can be observed that our attack performs quite well even with only a small fraction of leaked files, e.g., recovering the keyword about 70% of the time once only 20% of the files are leaked, and achieving 30% recovery rate even when given only 1% of the files.Neither the IKK12 attack nor the CGPR15 attack applies when the server is given the search results of only a single token. (As noted in the previous section, the CGPR15 attack inherently requires search results for multiple tokens; this explains why the results for the CGPR15 attack in Figure 7a are almost identical to the results for their attack in Figure 6.) The number of files injected in step 2 of the attack depends on both the number of unrecovered tokens (i.e., the size of t 2 ) and the average size of the candidate universe for each unrecovered token t 񮽙 (i.e., the size of K t 񮽙 ). Say 񮽙 of the files contain k and that, after keyword padding, an additional β · 񮽙 random and independently chosen files (in expectation) that do not contain k are returned in response to the search using t. (By setting parameters appropriately, this roughly encompasses both the countermeasures described above.) In fact, this is an over-estimate since if k is uniform then on average only half the injected files contain k.For the Enron dataset with |K| = 5, 000, F = 30, 109, 񮽙 = 560, and β = 0.6, and assuming half the injected files contain the keyword in question, the probability that the binary-search attack succeeds is 0.93. This is even more so the case with regard to joint frequencies, since these do not change unless two keywords are both associated with the same random file that contains neither of those keywords, something that happens with low probability.To validate our argument, we implement the padding countermeasure proposed in [4] and repeat the experiments using our attacks. The naive way to support conjunctive queries is to simply have the client issue queries for each of these keywords individually; the server can compute the set of file identifiers S i containing each keyword k i and then take their intersection to give the final result. Say the conjunctive search query involves keywords k 1 and k 2 , and we can partition the universe of keywords into two sets K 1 and K 2 with k 1 ∈ K 1 and k 2 ∈ K 2 . In contrast Let q be a conjunctive query with two keywords.Algorithm F ← Inject Files Disjoint(K 1 , K 2 ) 1: F ← Inject Files(K 1 ).2: Include all keywords in K 2 in every file in F. Algorithm F ← Inject Files Conjunctive(K)1: for i = 1, 2,. Thus, n 񮽙 > (1 + ε 4 )d log |K| with overwhelming probability.The probability that any other keyword is in all these n 񮽙 files is extremely low. It will next inject a file containing the first |K|/4 keywords.2. If this file is not in the search result for the query, the server learns that at least one keyword involved in the query has index greater than |K|/2. It will next inject a file containing the first 3|K|/4 keywords.Proceeding in this way, the server learns the lexicographically largest keyword using log |K| injected files. ,log |K i | do Inject F that contains the first b keywords in K i and all keywords in k. Let R q be the search result of query q on F. if R q = 1 then 8:b = b − |K i |/2 j . It is also worth observing that the number of injected files is essentially optimal for a deterministic attack with success probability 1, because the search results on d log |K| files contain at most d log |K| bits of information, which is roughly the entropy of a conjunctive search involving d keywords from a universe of size |K|. Second, within each injected file, the server can decide the order and number of occurrences of the keywords, can choose variants of the keywords (adding "-ed" or "-s," for example), and can freely include non-keywords ("a," "the," etc.) There are several tools (e.g., [21]) that can potentially be adapted to generate grammatically correct text from a given set of keywords by ordering keywords based on n-grams trained from leaked files and simple grammatical rules.