Whatever the future may hold for passwords, we argue that one of the most unusable and ineffective aspects of password authentication as encountered in 2016 truly does belong in the past: composition requirements and feedback derived from counts of lower-and uppercase letters, digits and symbols -LUDS for short.LUDS requirements appear in many incarnations: some sites require digits, others require the presence of at least 3 character classes, some banish certain symbols, and most set varying length minimums and maximums [21,29,52]. We argue that anything beyond a small client library with a simple interface is too costly for most would-be adopters, and that estimator accuracy is most important at low magnitudes and often not important past anywhere from 10 2 to 10 6 guesses depending on site-specific rate limiting capabilities.The Dropbox tech blog presented an early version of zxcvbn in 2012 [55]. At its core, zxcvbn checks how common a password is according to several sources -common passwords according to three leaked password sets, common names and surnames according to census data, and common words in a frequency count of Wikipedia 1-grams. We investigate two other estimators in addition to zxcvbn: the estimator of KeePass Password Safe [5] (hereafter KeePass) as it is the only other non-LUDS estimator studied in [25], 1 and NIST entropy, an influential LUDS estimator reviewed in Section 2. For example, consider our pseudocode summary of the metric NIST recommends (albeit with some disclaimers) in its most recent Special Publication 800-63-2 Electronic Authentication Guideline of August 2013 [22], commonly referred to as NIST entropy (hereafter NIST):1: function NIST_ENTROPY(p, dict) 2:e ← 4 + 2· p [2:8]. We focus on guessing resistance techniques that influence usability, as opposed to developments in cryptography, abuse detection, and other service-side precautions.The idea of a proactive password checker, a program that offers feedback and enforces policy at composition time, traces to the late 80s and early 90s with pioneering work by Nagle [45], Klein [35], Bishop [17] and Spafford [49]. We argue in Section 3 that their respective requirements to maintain and secure custom production infrastructure at scale is too costly for most would-be adopters.Dell'Amico and Filippone [26] detail a Monte Carlo sampling method that converts a password's probability as computed by any generative model into an estimate of a cracker's guessing order when running that model. KeePass (reviewed in [25]) matches several common patterns including a dictionary lookup with common transformations, then applies an optimal static entropy encoder documented in their help center [4] to search for the simplest set of candidate matches covering a password. It employs a client-server architecture that is hosted as a stand-alone site, and does not output a guess attempt estimate or equivalent, so we do not evaluate it as a candidate LUDS alternative.Ur et al.[50] and Egelman et al.[28] studied the effect of strength meters on password composition behavior. Because guessing attacks often rank low on a user's list of worries, short and memorable password choices are often driven by rational cost-benefit analysis as opposed to ignorance [31]. Alldigit and all-lowercase passwords -respectively the most common password styles in mainland China and the U.S. according to a comprehensive study [38] -should similarly be weighted instead of rejected via blanket rules. For example, if a password consists of two top-100 common words, it models an attacker who makes guesses as concatenations of two words from a 100-word dictionary, calculating 100 2 as its worst-case guess attempt estimate.To help prevent overly complex matching, zxcvbn now loosens the 2012 assumption by instead assuming the attacker knows the patterns that make up a password, but not necessarily how many or in which order. Before attempting length-|S| sequences, zxcvbn assumes that a guesser attempts lower-length pattern sequences first with a minimum of D guesses per pattern, trying a total of ∑|S|−1 l=1 D l ≈ D |S|−1 guesses for suffi- ciently large D.For example, if a password consists of the 20th most common password token t with a digit d at the end -a length-2 pattern sequence -and the attacker knows the D = 10000 most common passwords, and further, td is not in that top-10000 list (otherwise it would have been matched as a single token), the D 1 term models an attacker who iterates through those 10000 top guesses first before moving on to two-pattern guessing. If the l33t table maps @ to a and l to either i or l, it tries two additional matches by subbing [@->a, 1->i] and [@->a, 1->l], finding abalone with the second substitution.Taking a cue from KeePass, sequence matching in zxcvbn looks for sequences where each character is a fixed Unicode codepoint distance from the last. Unicode codepoint order doesn't always map directly to human-recognizable sequences; this method imperfectly matches Japanese kana sequences as one example.The repeat matcher searches for repeated blocks of one or more characters, a rewrite of the 2012 equivalent, which only matched single-character repeats. Additional layouts can be prepackaged or dynamically added.Date matching considers digit regions of 4 to 8 characters, checks a table to find possible splits, and attempts a day-month-year mapping for each split such that the year is two or four digits, the year isn't in the middle, the month is between 1 and 12 inclusive, and the day is between 1 and 31 inclusive. Two-digit years are matched as 20th-or 21st-century years, depending on whichever is closer to 2016. Green Book-style guessing space calculations then follow, but for patterns instead of random strings, where a guesser attempts simpler or more likely patterns first.For tokens, we use the frequency rank as the estimate, because an attacker guessing tokens in order of popularity would need at least that many attempts. The capitalization factor is otherwise estimated as1 2 min(U,L) ∑ i=1 񮽙 U + L i 񮽙 (2)where U and L are the number of uppercase and lowercase letters in the token. Guesses for keyboard patterns are estimated as:1 2 L ∑ i=1 min(T,i−i) ∑ j=1 񮽙 i − 1 j − 1 񮽙 SD j (3)where L is the length of the pattern, T is the number of terms, D is the average number of neighbors per key (a tilde has one neighbor on QWERTY, the 'a' key has four) and S is the number of keys on the keyboard. Repeat guess attempts are then estimated as g · n. For example, nownownow is estimated as requiring 126 guesses: now is at rank 42 in the Wiktionary set, times 3. The search considers one character of password at a time, at position k, and for each match m ending at k, evaluates whether adding m to any length-l optimal sequence ending just before m (at m.i − 1) leads to a new candidate for the optimal match sequence covering the prefix up to k:1: function SEARCH(n, S) 2:for k ∈ 0 to n − 1 3:g opt ← ∞ 4:for m ∈ S when m. j = k That is, at each index k, there are only three cases where a bruteforce match might end an optimal sequence: it might span the entire k-prefix, forming a length-1 sequence, it might extend an optimal bruteforce match ending at k − 1, or it might start as a new singlecharacter match at k. Note that given the possibility of expansion, it is always better to expand by one character than to append a new bruteforce match, because either choice would contribute equally to the Π term, but the latter would increment l.The UPDATE helper computes expression (1) Π ← Π × Π opt [m.i − 1][l − 1] 5: g ← D l−1 + l! The browser script is minified via UglifyJS2 with instructions on how to serve as gzipped data.zxcvbn works as-is on most browsers and javascript server frameworks. The PGS training data (roughly 21M unique tokens) consists of the RockYou'09 password leak (minus a randomly sampled 15k test set), Yahoo'12 leak (minus a similar 15k test set), MySpace'06 leak, 1-grams from the Google Web Corpus, and two English dictionaries. While our test data is distinct from our training data, it is by design that both include samples from the same RockYou'09 distribution; our aim is to simulate an attacker with knowledge of the distribution they are guessing. Instead of counting top passwords from the MySpace'06 leak, our estimators use the Xato'15 corpus which is over 200 times bigger.In all, we count top tokens from the PGS training portion of RockYou'09 and Yahoo'12 (test sets are excluded from the count), Xato'15, 1-grams from English Wikipedia, common words from a Wiktionary 29M-word frequency study of US television and film [10], and common names and surnames from the 1990 US Census [1]. When PGS is unable to guess a password, we exclude it from our sample set S. On each sampled password x i ∈ S, we then measure an algorithm's estimation error by computing its order-of-magnitude difference ∆ i from PGS,∆ i = log 10 g alg (x i ) g pgs (x i )(4)where g alg is the guess attempt estimate of the algorithm and g pgs is the minimum guess order of the four PGS guessing attacks. First, to give a rough sense of the shape of estimator accuracy, we show log-log scatter plots spanning from 10 0 to 10 15 guesses, with g pgs on the x axis, g alg on the y axis, and a point for every x i ∈ S. Second, we show the distribution of ∆ i as a histogram by binning values to their nearest multiple of .5, corresponding to half-orders of magnitude. Neither KeePass nor NIST formalize the type of entropy they model, so we assume that n bits of strength means guessing the password is equivalent to guessing a value of a random variable X according ton = H(X) = − ∑ i p(x i ) log 2 p(x i )(7)Assuming the guesser knows the distribution over X and tries guesses x i in decreasing order of probability p(x i ), a lower bound on the expected number of guesses E[G(X)] can be shown [41] to be:E[G(X)] ≥ 2 H(X)−2 + 1 (8)provided that H(X) ≥ 2. Hence, the greatest overestimation in both cases happens between the estimator dictionary cutoff and PGS dictionary cutoff, high- lighting the sensitivity of estimator dictionary size.The horizontal banding at fixed orders of magnitude in zxcvbn corresponds to bruteforce matches where no other pattern could be identified. Figure 4 counts and normalizes ∆ i in bin multiples of .5, demonstrating that zxcvbn is within ±.25 orders of magnitude of PGS about 50% of the time within the online range. Overall the impact is small compared to supplying additional data, but the space-and time-cost is near-zero, hence we consider these extra patterns a strict Table 1 shows the cumulative benefit of matching additional patterns. By including training and test data from the same distribution, we erred on the side of aggressiveness for our guessing simulation; however, test data aside, to the extent that PGS and zxcvbn are trained on the same or similar data with the same models, we expect similar accuracy at low magnitudes up until zxcvbn's frequency rank cutoff (given a harder guessing task, that range might span a lower percentage of the test set). We made use of the following raw data:RockYou: 32M passwords leaked in 2009 [7], excluding a random 15k test set consisting of passwords of 8 or more characters.Yahoo: 450k passwords leaked in 2012 [11], excluding a random 15k test set consisting of passwords of 8 or more characters. Past the online range, more data makes the algorithm more conservative, with progressively higher |∆| and lower ∆ + .