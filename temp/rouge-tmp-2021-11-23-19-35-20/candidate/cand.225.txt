From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. Ideally, systems for text analytics, such as topic detection and tracking (TDT) (Allan, 2002), text summarization ( Huang et al., 2003), information retrieval (IR) ( Dias et al., 2007), or question answering (QA) , could access a document representation that is aware of both topical (i.e., latent semantic content) and structural information (i.e., segmentation) in the text ( MacAvaney et al., 2018). In contrast, cities resembles a diversified domain, with high entropy (i.e., broader topics, common language, and higher word ambiguity) and will be more applicable to for example, news, risk reports, or travel reviews.We compare SECTOR to existing segmentation and classification methods based on latent Dirichlet allocation (LDA), paragraph embeddings, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Our method unifies those strongly interwoven tasks and is the first to evaluate the combined topic segmentation and classification task using a corresponding data set with long structured documents.Topic modeling is commonly applied to entire documents using probabilistic models, such as LDA ( Blei et al., 2003). Early unsupervised methods utilized lexical overlap statistics (Hearst 1997;Choi 2000), dynamic programming ( Utiyama and Isahara, 2001), Bayesian models ( Eisenstein and Barzilay, 2008), or pointwise boundary sampling ( Du et al., 2013) on raw terms.Later, supervised methods included topic models ( Riedl and Biemann, 2012) by calculating a coherence score using dense topic vectors obtained by LDA. Glavaš et al. (2016) utilized semantic relatedness of word embeddings by identifying cliques in a graph.More recently, Sehikh et al. (2017) utilized LSTM networks and showed that cohesion between bidirectional layers can be leveraged to predict topic changes. Our work takes up this idea of end-to-end training and enriches the neural model with a layer of latent topic embeddings that can be utilized for topic classification.Text classification is mostly applied at the paragraph or sentence level using machine learning methods such as support vector machines (Joachims, 1998) or, more recently, shallow and deep neural networks ( Le et al., 2018;Conneau et al., 2017). Although these data sets have been used to evaluate word-level and sentence-level segmentation ( Koshorek et al., 2018), we are not aware of any topic classification approach on this data set.Tepper et al. (2012) approached segmentation and classification in a clinical domain as supervised sequence labeling problem. For each sentence s k , we assume a distribution of local topics e k that gradually changes over the course of the document.The task is to split D into a sequence of distinct topic sections T = [T 1 , . . . , T M ], so that each predicted section T j = S j , y j contains a sequence of coherent sentences S j ⊆ S and a topic label y j that describes the common topic in these sentences. In order to use Wikipedia headlines as a source for topic labels, we contribute a normalization method to reduce the high variance of headings to a few representative labels based on the clustering of BabelNet synsets (Navigli and Ponzetto, 2012 synsets S h ⊂ S. For example, "Cognitive behavioral therapy" is assigned to synset bn:03387773n. Thus, we aim to predict coherent sections with respect to document context:p(¯ y 1 , ... , ¯ y N | D) = N k=1 p(¯ y k | s 1 , ... , s N )(1)We approach two variations of this task: For WIKISECTION-topics, we choose a single topic label y j ∈ Y out of a small number of normalized topic labels. For both tasks, we aim to maximize the log likelihood of model parameters Θ on section and sentence level:L(Θ) = M j=1 log p(y j | s 1 , ... , s N ; Θ) ¯ L(Θ) = N k=1 log p(¯ y k | s 1 , ... , s N ; Θ)(2)Our SECTOR architecture consists of four stages, shown in Figure 2: sentence encoding, topic embedding, topic classification and topic segmentation. Let I(w) ∈ {0, 1} |V| be the indicator vector, such that I(w) (i) = 1 iff w is the i-th word in the fixed vocabulary V, and let tf-idf(w) be the TF-IDF weight of w in the corpus. We use the sum of bit arrays per word as compressed Bloom embedding x bloom ∈ N m :x bloom (s) = w∈s k i=1 A hash i (w)(4)We set parameters to m = 4096 and k = 5 to achieve a compression factor of 0.2, which showed good performance in the original paper.Sentence Embeddings. We use the strategy of Arora et al. (2017) to generate a distributional sentence representation based on pre-trained word2vec embeddings ( Mikolov et al., 2013 v s = 1 |S| w∈s α α + p(w) v w x emb (s) = v s − uu T v s(5) We model the second stage in our architecture to produce a dense distributional representation of latent topics for each sentence in the document. We modify our objective given in Equation (2) accordingly with long-range depen- dencies from forward and backward layers of the LSTM:L(Θ) = N k=1 log p(¯ y k | x 1...k−1 ; Θ, Θ ) + log p(¯ y k | x k+1...N ; Θ, Θ )(6)Note that we separate network parameters Θ and Θ for forward and backward directions of the LSTM, and tie the remaining parameters Θ for the embedding and output layers. We then use a sigmoid activation function:ˆ ¯ y k = softmax(W ye e k + W ye e k + b y ) ˆ ¯ z k = sigmoid(W ze e k + W ze e k + b z )(8)Ranking Loss for Multi-Label Optimization.The multi-label objective is to maximize the likelihood of every word that appears in a heading:L(Θ) = N k=1 |Z| i=1 log p(¯ z (i) k | x 1...N ; Θ)(9)For training this model, we use a variation of the logistic pairwise ranking loss function proposed by dos Santos et al. (2015). For the negative term, we only take the most offending example y − among all incorrect class labels.score + (x) = 1 |y + | y∈y + s θ (x) (y) score − (x) = arg max y∈y − s θ (x) (y)(11)Here, s θ (x) (y) denotes the score of label y for input x. Therefore, we apply a number of transformations adapted from Laplacian-of-Gaussian edge detection on images ( Ziou and Tabbone, 1998) d k = cos(e k−1 , e k ) = e k−1 · e k e k−1 e k(12)Finally we apply the sequence d 1...N with parameters D = 16 and σ = 2.5 to locate the spots of fastest movement (see Figure 4), i.e. all k where d k−1 < d k > d k+1 ; k = 1 . . . N in our discrete case. Here, we obtain two smoothed embeddings e and e and define the bidirectional embedding deviation (bemd) as geometric mean of the forward and backward difference:d k = cos( e k−1 , e k ) · cos( e k , e k+1 ) (13)After segmentation, we assign each segment the mean class distribution of all contained sentences:ˆ y j = 1 | S j | s i ∈S j ˆ ¯ y i(14)Finally, we show in the evaluation that our SECTOR model, which was optimized for sentences ¯ y k , can be applied to the WIKISECTION task to predict coherently labeled sections T j = S j , ˆ y j . Because we are not aware of any existing method for combined segmentation and classification, we first compare all methods using given prior segmentation from newlines in the text (NL) and then additionally apply our own segmentation strategies for plain text input: maximum label (max), embedding deviation (emd) and bidirectional embedding deviation (bemd). We clearly see from NL predictions (left side of Figure 5) that SECTOR produces coherent results with sentence granularity, with topics emerging and disappearing over the course of a document. This shows that the multilabel approach fills an important gap and can even serve as an indicator for low-quality article structure.Finally, both models fail to segment the complication section near the end, because it consists of an enumeration.