CEAL stands out from existing approaches in three significant aspects: (1) eliminates the need for hand curated image generation rules by learning a generator model that imitates the style and domain of fingerprint images from a large collection of sample images , hence enabling easy customizability, (2) operates within limits of the visual discriminative ability of human perception , such that the learned fingerprint image generator avoids mapping distinct keys to images which are not distinguish-able by humans, and (3) the resulting model deterministically generates realistic fingerprint images from an input vector, where the vector components are designated to control visual properties which are either readily perceptible to a human eye, or imperceptible, yet necessary for accurately modeling the target image domain. Unlike existing visual fingerprint generators, CEAL factors in the limits of human perception, and pushes the key pay-load capacity of the images toward the limits of its generative model: We have built a generative network for nature landscape images which can reliably encode 123 bits of entropy in the fingerprint. Thus, even though such solutions use an additional source of entropy (e.g., PRNG), they have not been designed to generate humandistinguishable images.The user studies of Tan et al. [50] that compare multiple text and visual KFG solutions suggest that VKFGs can speed up the verification of key fingerprints. We note that the user does not need to memorize images, but only visually compare the two images for equality.More generally, we seek to construct a set of images, where each image can be easily and quickly distinguished from any other image in the set, by a human. Second, use the identified d to convert the weak VKFG into a strong VKFG function.In addition to the human-distinguishability of generated fingerprints, developed solutions should also (1) have a sufficiently large capacity to be resistant against preimage attacks, i.e., the number of unique and human-distinguishable generated images should be large, and (2) ensure that humans are able to quickly compare any pair of generated images. The conventional GAN consists of two competing neural networks: (1) a generator network (G) that transforms the input latent vector into an image, and (2) a discriminator network (D) that differentiates synthetic images, generated by G, from real images in a training dataset. For CEAL, we use a DC-GAN [42]-like architecture to generate images that represent a key fingerprint corresponding to an input key string.Our approach is also related to, and inspired by work on learning disentangled representations, i.e. interpretable factors of data variation [10,11,28,31], that seeks to learn a representation that captures the underlying generative structure of data. In addition, Grathwohl and Wilson [20] disentangle spatial and temporal features in videos, in an unsupervised fashion. We propose instead a disentanglement of major from minor components: decompose the latent vector into major and minor components, and train major components to encode information about human distinguishability, and the minor components to encode image realism properties. We introduce the CEAL (CrEdential Assurance Labeling) approach to build a visual key fingerprint generator that will generate realistic images and address the requirements of § 3. That is, the network takes as input a latent vector, and produces a realistic image, human-distinguishable from other images generated from latent vectors that are in Figure 3: The CEAL approach: Train a generator to convert a latent vector to a realistic image, human-distinguishable from other generator produced images. We show that it is possible to build the input mapper, thus convert the trained weak V w into a strong V s function, using an error correcting code encoder, e.g., [8,23],ECC : {0, 1} γ → {0, 1} γ , with minimum distance of d. Specifically, ∀K i , K j ∈ {0, 1} γ , K i = K j =⇒ d H (ECC(K i ), ECC(K j )) ≥ d.Then, the trained system first applies ECC to the input, then applies the V w generator to the encoded string. However, given a sufficient number of annotations, a regression predictor model HPD predict :I × I → [0, 1] may be used to approximate the HPD ratio function, E(|HPD predict (I 1 , I 2 ) − HPD ratio (I 1 , I 2 )|) < ε.We show that, even with a small number of annotated data, a very limited classification model HPD equal : I × I → {0, 1} which can detect distinguishable image pairs with high precision at the cost of low recall, P(HPD ratio > 0 | HPD equal (I 1 , I 2 ) = 1) < ε, is sufficient for training a generator which satisfies the strong V s requirement (see Equation 1 and § 3). We train CL-GAN's generator network G-CEAL, using two classifiers (see Figure 4): (1) the CL-GAN discriminator (D-CEAL) that is trained to differentiate synthetically generated images by G-CEAL from a dataset of real images, and (2) the HPD classifier, trained to estimate the likelihood that a human will label a pair of images as either identical or different.In the following, we first describe the HPD employed by CL-GAN, then detail the training process of CL-GAN. This is because the representation learned by deep neural network is distributed across different layers, where shallow layers capture low level features (e.g. Gabor-like filters in the image domain), and deeper layers capture more abstract and complicated features (e.g. the face of a cat). Experimentally investigating which representation performs best for the problem of image matching in HPD is hence a typical feature selection task in machine learning.Specifically, we employ a transfer learning approach using the Inception.v1 model [49], trained for image classification tasks on the ImageNet dataset [15] (1.2 million images from 1, 000 categories). Equation 2 shows how the weights are updated based on the weighted contrastive loss for two input samples I 1 and I 2 : 2 (2) θ denotes the model parameters (weights and biases), and Y is the actual class label of the image pair, i.e. 1 for different and 0 for identical images. r ∈ [0, 1] is the weight (importance) assigned to the positive (different) class and L(θ,Y, I 1 , I 2 ) = 1 2 (1 − r)(1 −Y )(D w 2 ) + 1 2 rY (max(0, µ − D w ))µ ∈ R, µ > 1 is a margin.After training the 3 additional layers in the twin Siamese network using contrastive loss, the network has learned to differentiate between the input image pairs, i.e. generate distant representations (O 1 and O 2 ) for dissimilar images and similar representations for similar images. To address this problem, we leverage recent successes in learning disentangled representations [10,11,16,28,31], to conjecture that we can decompose the latent vector into (1) a subset of major components, that when changed individually, produce human-perceptible changes in the generated images, and (2) the remaining, minor components, that encode relatively imperceptible characteristics of the images, see Figure 6. To achieve this, in each adversarial training epoch, we train G-CEAL using 3 steps, to generate (1) visually distinguishable images when the values of individual major components in the latent vectors are changed (flipped between 1 and -1) and (2) visually indistinguishable images when the values for minor components are flipped (see § 7.2). To force the i th component of the latent vector to be a major component, i.e., maximize the effect of the i th component on the visual characteristics of the generated images, we want the HPD classifier to classify all these image pairs (I 1 , I 2 ) as different (class 1). Then, for each seed latent vector, pick two random major components i, j ∈ R {1, 2, 3, ..., M} and i = j. Copy seed latent vector v into two other latent vectors v 1 and v 2 , then set v 1 [i] = 1 and v 2 [ j] = 1. We now describe the input mapper module, that solves the second sub-problem of Section 3: convert input keys into codes that are at Hamming distance at least d from each other.Specifically, as also illustrated in Figure 6, the input mapper takes as input a key (e.g., public key, shared key, Bitcoin address, IP address, domain name) and outputs a latent vector L of length λ, i.e., a code word at Hamming distance at least d from the code word of any other input key. We used a subset of 150,113 outdoor landscape images (mountains, ocean, forest) of 64 by 64 pixels, from the MIT Places205 dataset [38,55] to train discriminator networks (vanilla DCGAN and CL-GAN models) to differentiate between real and synthetic images. Thus, since our objective is to collect labeled data to be used for identifying the boundaries of human visual distinguishability, we generate image pairs from key fingerprints that are only different in 1 component.We followed an IRB-approved protocol to recruit 500 adult workers located in the US, to label 558 unique image pairs. We then performed a second labeling study to determine if the high error rate we observed was due to the fact that an observed "faulty" component always produces indistinguishable image pairs when its value is flipped, or this is due to other factors, e.g. the contribution of other components on the generated image.First, for each of 3 image pairs with the highest error rate in labeling study 1, we generated 99 variant image pairs as follows: Let j be the index of the component that we flipped to generate this particular image pair in study 1 (which resulted in a high error rate). In order to train the HPD to correctly classify visually similar, but random noise images, as "identical", we generated an unrealistic image dataset of 11,072 image pairs using a poorly trained vanilla DCGAN:(1) 10,048 image pairs using a vanilla DCGAN trained for only 1400 iterations, i.e., less than an epoch, and (2) 1,024 image pairs using the same vanilla DCGAN trained for 3600 iterations (slightly more than an epoch). We generated 1,024 different image pairs, each as follows: generate a random seed latent vector, copy it to v 1 and v 2 , select 10 random latent components (out of 100) uniformly and set the values of these components to 1 in v 1 and -1 in v 2 . For each of 1,000 random, vanilla DCGAN-generated images, we generated 5 images, by applying 5 enhancements, to change (1) image brightness, (2) contrast, (3) We have built CEAL in Python using Tensorflow 1.3.0 [6]. We found a significant difference between the performance (over holdout datasets) of HPD classifiers trained using the "Mixed_5c" layer features, compared to the other two layers (P-Value = 0.000 when compared to "MaxPool_4a_3x3" layer, and P-Value = 0.000, when compared to "MaxPool_5a_2x2" features). Specifically, we randomly split each synthetic dataset (except the ground truth human perception set), into training ( 80% of samples) and holdout ( 20%) sets: we used the training sets to train the HPD classifier, then tested its performance over the holdout sets. For the ground truth human perception dataset, we made sure that the number of image pairs that are labeled as identical and different, were distributed to training and test sets proportionally to their size.We hyper-tuned the architecture and parameters of the HPD classifier to find a classifier which accurately identifies samples from the "different" class (has high precision). Table 3 shows the performance of the Siamese network and the HPD networks that we trained and used in this paper.In addition, we also trained an HPD model that has the same weights as HPD_model_1 in the Siamese layers, but different weights in the fully connected layer on top of the twin networks in the HPD architecture. Further, we also experimented using CL-GAN variants with different architectures (e.g., different number of neurons in the first layer of G-CEAL, 8,192 and 16,384) and values for hyper parameters, including λ and the number of major and minor components.We also performed a grid search in the parameters of the CL-GAN including (1) the input size (λ ∈ 64, 128, 256, 512), (2) the number of major and minor components ( λ 2 , and (3) the α ∈ [25,75] with step size 5, in the loss functions of the CL-GAN generator (see Equation 3). All the pairs identified by HPD_model_1 were also identified by the HPD_attacker.We used the procedure of § 10.2 to label these pairs using 34 human workers. To ensure that major components of input vector to G-CEAL are at least in d Hamming distance of each other, we use a BCH (n=255, k=123, t=19) encoder to transform a key of length γ = 123 into the values for major components. Thus, in total we generated 123 million CEAL image pairs, organized into 123 datasets, each containing 1 million (target, attack) image pairs.We have used both the HPD_attacker and the HPD_model_-1 to compare the 123 million (target, attack) image pairs that we generated. As expected, the number of broken CEAL images decreases as the Hamming distance between the target and attack binary key fingerprints increases.We note that the KFG evaluation performed by Tan et al. [50], assumed an adversary able to perform 2 60 brute force attempts, which is similar to the effort required to control 122 of 123 bits of the input hash key, required by a (γ, 1)-attack. Vash [1] is an open source implementation for random art [41], that converts input binary strings into structured, visual fingerprints. The operation parameters (e.g., the two foci of an ellipse or the size of a flower) are chosen randomly using the PRNG.To study the ability of Vash [1] to generate humandistinguishable images, we generated 120 different Vash im-age pairs, as follows. This method provides only a lower bound estimate for the capacity of a visual key fingerprint generator function, as any estimation method fails when k >> s 2 , where k is the real population size and s is the sample size used for the estimate. To increase the entropy of the CEAL key fingerprint generator, one could design and train multiple generators (see § 9.2), then use the input key to decide which generator to use (e.g., the value of the key's first two bits to pick one out of 4 generators).