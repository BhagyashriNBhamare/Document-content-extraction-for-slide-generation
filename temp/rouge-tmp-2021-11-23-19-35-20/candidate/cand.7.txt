Our framework makes use of virtual reality (VR) systems, incorporating along the way the ability to perform animations (e.g., raising an eyebrow or smiling) of the facial model, in order to trick liveness detectors into believing that the 3D model is a real human face. 2 These attacks underscore the fact that face authentication systems require robust security features beyond mere recognition in order to foil spoofing attacks.Loosely speaking, three types of such spoofing attacks have been used in the past, to varying degrees of success: (i) still-image-based spoofing, (ii) video-based spoofing, and (iii) 3D-mask-based spoofing. Once a credible synthetic model of a user is obtained, we then employ entry-level virtual reality displays to defeat the state of the art in liveness detection.The rest of the paper is laid out as follows: §2 provides background and related work related to face authentication, exploitation of users' online photos, and 3D facial reconstruction. Before delving into the details of our approach, we first present pertinent background information needed to understanding the remainder of this paper.First, we note that given the three prominent classes of spoofing attacks mentioned earlier, it should be clear that while still-image-based attacks are the easiest to perform, they can be easily countered by detecting the 3D structure of the face. While this method does require more specific knowledge than general CAPTCHAs, Polakis et al. [42] demonstrated that facial recognition could be applied to a user's public photos to discover their social relationships and solve 22% of SA tests automatically.Given that one's online photo presence is not entirely controlled by the user alone -but by their collective social circles -many avenues exist for an attacker to uncover the facial appearance of a user, even when the user makes private their own personal photos. In the future, such an approach may help decrease the public accessibility of users' personal photos, but it is unlikely that an individual's appearance can ever be completely obfuscated from attackers across all social media sites and image stores on the Internet.Clearly, the availability of online user photos is a boon for an adversary tasked with the challenge of undermining face authentication systems. However, the constraints of the illumination model used in SFS require a relatively simple illumination setting and, therefore, cannot typically be applied to realworld photo samples, where the configuration of the light sources is unknown and often complicated.As an alternative, the structure from motion approach [12] makes use of multiple photos to triangulate spatial positions of 3D points. With this method, the reconstruction of a dense and accurate model often requires many consistent views of the surface from different angles; moreover, non-rigid variations (e.g., facial expressions) in the images can easily cause SFM methods to fail. In our scenario, these requirements make such an approach less usable: for many individuals, only a limited number of images might be publicly available online, and the dynamic nature of the face makes it difficult to find multiple images having a consistent appearance (i.e., the exact same facial expression). The basis for this type of facial reconstruction is the 3D morphable model (3DMM) of Blanz and Vetter [6,7], which learns the principal variations of face shape and appearance that occur within a population, then fits these properties to images of a specific face. For instance, the approach of Baumberger et al. [4] achieves good reconstruction quality using only two images.To make the process fully automatic, recent 3D facial reconstruction approaches have relied on a few facial landmark points instead of operating on the whole model. In this process, the main challenge is the localization of facial landmarks within the images, especially contour landmarks (along the cheekbones), which are half-occluded in non-frontal views; we introduce a new method for solving this problem when multiple input images are available.The end result of 3D reconstruction is a untextured (i.e., lacking skin color, eye color, etc.) facial surface. Once the texture is filled, we have a realistic 3D model of the user's face based on a single image.However, despite its realism, the output of stage  is still not able to fool modern face authentication systems. The 68 extracted 3D point landmarks from each of the N input images provide us with a set of coordinates s i, j ∈ R 2 , with 1 ≤ i ≤ 68, 1 ≤ j ≤ N. The projection of the 3D points S i, j ∈ R 3 on the face onto the image coordinates s i, j follows what is called the "weak perspective projection" (WPP) model [16], computed as follows:s i, j = f j PR j (S i, j + t j ) ,(1)where f j is a uniform scaling factor; P is the projectionmatrix 񮽙 1 0 0 0 1 0 񮽙; R j is a 3 × 3 rotation matrix defined by the pitch, yaw, and roll, respectively, of the face relative to the camera; and t j ∈ R 3 is the translation of the face with respect to the camera. More formally, for any given individual, the 3D coordinates S i, j on the face can be modeled asS i, j = ¯ S i + A id i α id + A exp i α exp j ,(2)where ¯ S i is the statistical average of S i, j among the individuals in the population, A id i is the set of principal axes of variation related to identity, and A exp i is the set of principal axes related to expression. However, for contour landmarks marking the edge of the face in an image, the associated 3D point on the user's facial model is pose-dependent: When the user is directly facing the camera, their jawline and cheekbones are fully in view, and the observed 2D landmarks lie on the fiducial boundary on the user's 3D facial model. Qu et al. [44] deal with contour landmarks using constraints on surface normal direction, based on the observation that points on the edge of the face in the image will have surface normals perpendicular to the viewing direction. While their approach is efficient and robust against different face angles and surface shapes, it can only handle a single image and cannot refine the reconstruction result using additional images.Our solution to the correspondence problem is to model 3D point variance for each facial landmark using a pre-trained Gaussian distribution (see Appendix A). Due to the appearance variation across social media photos, we have to achieve this by mapping the pixels in a single captured photo onto the 3D facial model, which avoids the challenges of mixing different illuminations of the face. Sadly, we cannot use their approach directly as it reconstructs a planar normalized face, instead of a 3D facial model, and so we must extend their technique to the 3D surface mesh.The idea we implemented for improving our initial textured 3D model was as follows: Starting from the single photo chosen as the main texture source, we first estimate and subsequently remove the illumination conditions present in the photo. This process is defined mathematically as∆ f = ∆g, s.t f | ∂ Ω = f 0 | ∂ Ω ,(3)where Ω is the editing region, f is the editing result, f 0 is the known original texture value, and g is the texture value in the editing region that is unknown and needs to be patched with its reflection complement. We perform the same analysis for the eye region of the average face model obtained from 3DMM [39], whose eye is looking straight towards the camera, and we similarly obtain principle color components(b std 1 , b std 2 , b std 3 ) with weight (σ std 1 , σ std 2 , σ std 3 ), σ std 1 ≥ σ std 2 ≥ σ std 3 > 0. This VR-based spoofing constitutes a fundamentally new class of attacks that exploit weaknesses in camera-based authentication systems.In the VR system, the synthetic 3D face of the user is displayed on the screen of the VR device, and as the device rotates and translates in the real world, the 3D face moves accordingly. As a result, the observed 3D facial motion will not agree with the device's inertial sensors, causing our method to fail on methods like that of Li et al. [34] that use such data for liveness detection.Fortunately, it is possible to track the 3D position of a moving smart phone using its outward-facing camera with structure from motion (see §2.3). Several computer vision approaches have been recently introduced to solve this problem accurately and in real time on mobile devices [28,46,55,56]. With their consent, we collected public photos from the users' Facebook and Google+ social media pages; we also collected any photos we could find of the users on personal or community web pages, as well as via image search on the web. Since not all textures will successfully spoof the recognition systems, we created textured reconstructions from all source images and iteratively presented them to the system (in order of what we believed to be the best reconstruction, followed by the second best, and so on) until either authentication succeeded or all reconstructions had been tested. # Tries KeyLemon 100% 85% 1.6 Mobius 100% 80% 1.5 True Key 100% 70% 1.3 BioID 100% 55% 1.7 1U App 100% 0% - Table 2: Success rate for 5 face authentication systems using a model built from (second column) an image of the user taken in an indoor environment and (third and fourth columns) images obtained on users' social media accounts. Even so, as evidenced by the second column in Table 2, our method still handily defeats the liveness detection modules of these systems given images of the user in the original illumination conditions, which suggests that all the systems we tested are vunerable to our VR-based attack.Our findings also suggest that our approach is able to successfully handle significant changes in facial expression, illumination, and for the most part, physical characteristics such as weight and facial hair. Given that it has shown to work on a varied collection of real-world data, we believe that the at-tack presented herein represents a realistic security threat model that could be exploited in the present day.Next, to gain a deeper understanding of the realism of this threat, we take a closer look at what conditions are necessary for our method to bypass the various face authentication systems we tested. To identify the robustness of the proposed system against head rotation, we first evaluate the maximum yaw angle allowed for our system to spoof baseline systems using a 9 We skip analysis of Mobius because its detection method is similar to True Key, and our method did not perform as well on True Key. For all 20 sample users, we collect multiple indoor photos with yaw angle varying from 5 degrees (approximately frontal view) to 40 degrees (significantly rotated view). Fortunately, as discussed in §3, the data consistency requirement is automatically satisfied with our virtual reality spoofing system because the 3D model rotates in tandem with the camera motion.Central to Li et al. [34]'s approach is to build a classifier that evaluates the consistency of captured video and motion sensor data. Any system using this detector will need to require multiple log-in attempts to account for the decreased recall rate; allowing multiple log-in attempts, however, allows our method more opportunties to succeed. Random projections of structured light [62], i.e., checkerboard patterns and lines, would increase the difficulty of such an attack, as the 3D-rendering system must be able to quickly and accurately render the projected illumination patterns on a model. First, our exploitation of social media photos to perform facial reconstruction underscores the notion that online privacy of one's appearance is tantamount to online privacy of other personal information, such as age and location.The ability of an adversary to recover an individual's facial characteristics through online photos is an immediate and very serious threat, albeit one that clearly cannot be completely neutralized in the age of social media. For a specific head orientation R j , the corresponding landmark points on the 3D model are found using an explicit function based on rotation angle:s i, j = f j PR j (S i 񮽙 , j + t j ) S i 񮽙 , j = ¯ S i 񮽙 + A id i 񮽙 α id + A exp i 񮽙 α exp j i 񮽙 = land(i, R j ),(6)where land(i, R j ) is the pre-calculated mapping function that computes the position of landmarks i on the 3D model when the orientation is R j . Then, the most probable parameters θ := ({ f j }, {R j }, {t j }, {α exp j }, α id ) can be estimated by minimizing the cost functionθ = argmax θ { 68 ∑ i=1 N ∑ j=1 1 (σ s i ) 2 ||s i, j − f j PR j (S i 񮽙 , j + t j )|| 2 + N ∑ j=1 (α exp j ) 񮽙 Σ −1 exp α exp j + (α id ) 񮽙 Σ −1 id α id }.