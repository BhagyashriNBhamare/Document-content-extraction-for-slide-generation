A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality) significantly improves performance. The goal of our work is to evaluate the validity of this implicit assumption in the context of PDF malware detection.Our first contribution is to evaluate feature-space evasion attack models in the context of PDF malware detection, using EvadeML as a realizable attack [44]. In practice, since P is unknown, one typically aims to find h 2 H which (approximately) minimizes empirical error on training data D.In security applications-as in others-one is not given numerical features; instead, we start with a collection of entities, such as executables, along with associated labels (we assume henceforth that these are available, as we focus here on supervised learning problems). Applying feature extractors to each entity in our dataset, and adding associated object labels, allow us to generate a dataset D to fit the conventional ML framework.In this paper we focus on PDF malware detection, where the label space is binary: either a PDF file is benign (which we can code as 񮽙1), or malicious (which we can code as +1). In an evasion attack, abstractly, one is given a learned model h(x) (e.g., a SVM or neural network) which returns a label y = h(x) (e.g., malicious or benign) for an arbitrary feature vector x 2 X (e.g., extracted from a PDF file). The attacker then transforms e into another entity, e 0 , with an associated feature vector x 0 = f(e 0 ) so as to accomplish two goals: first, that h(x 0 ) returns an erroneous label (in our running example, labels e 0 as benign based on its extracted features f(e 0 )), and second, that e 0 preserves the functionality of the original entity e-which, in our example of PDF malware detection, entails preserving malicious functionality of e. The evasion attack as just described is presumed to transform the entity itself, such as the malicious PDF file, albeit accounting for the effect of such transformation on the extracted features x 0 = f(e 0 ). In this case, the attacker is modeled as starting with a malicious feature vector x (not the malicious entity e), and directly modifying the fea-tures to produce another feature vector x 0 2 X, so as to yield erroneous predictions, i.e., y 0 = h(x 0 ) (for example, being mislabeled as benign). 1 Game-theoretic methods in general, and robust optimization in particular, are not general-purpose, as solving these directly requires special structure, such as a continuous feature space and differentiability [3,5,6], and often additional structure of the learning model, such as linearity [43] or neural network architecture and activation functions [32,42]. Since our study below involves realizable attacks (in addition to the mathematical models of attacks), non-linear SVM and, in all cases but one, binary features, iterative retraining is the sole defense that can be applied uniformly (which we require to ensure that our results are directly comparable). We have two major goals: 1) validation: to evaluate whether robust ML approaches that make use of feature-space models of evasion attacks are, indeed, robust against realrealizable-attacks, and 2) generalizability: to study generalizability of evasion defenses.We start with a conceptual model of defense and attack as a Stackelberg game between ML ("defender"), who first chooses a defense q (in our case, the learned model h(x)) and the attacker, who finds an optimal attack that reacts to the particular defense q. To be more precise, let O(h; D) be an arbitrary attack which returns evasions given a dataset D and a classifier h, and let u(h; O(h; D)) be the measure that the defender wishes to optimize (for example, accuracy on data after evasions). Now, we describe our approach to validation and generalizability evaluations.In validation, consider a model of an evasion attack, ˜ O(h; D) (e.g., a feature-space attack model), which is a proxy for a "real" (realizable) attack, O(h; D); note that each attack evades a given ML model h. For example, if we find that˜hthat˜ that˜h is ineffective against the target attack, we say that˜Othat˜ that˜O is a poor attack proxy, whereas if it remains robust, we view˜Oview˜ view˜O as a good proxy for the target attack O. The CRT indexes objects in the body, while the trailer points to the CRT.The relations between objects with cross-references can be described as a directed graph that presents their logical structure by using edges representing reference relations and nodes representing different objects.As an object can be referred to by its child node, the resulting logical structure is a directed cyclic graph. PDFRate, on the other hand, is a content-based classifier, which constructs features based on medadata and content information in the PDF file to distinguish benign and malicious instances. SL2013: SL2013 is a well-documented and open-source machine learning system using Support Vector Machines (SVM) with a radial basis function (RBF) kernel, and was shown to have state-of-the-art performance [36]. It inherits all the characteristics of SL2013 and employs structual path consolidation (SPC), a technique to consolidate features which have the same or similar semantic meaning in a PDF. The primary realizable attack in our study is EvadeML [44], which allows insertion, deletion, and swapping of objects, and is consequently a stronger attack than most other realizable attacks in the literature, which typically only allow insertion to ensure that malicious functionality is preserved. If a variant is classified as benign but displays malicious behavior, or if GP reaches the maximum number of generations, then GP terminates with the variant achieving the best fitness score and the corresponding mutation trace is stored in a pool for future population initialization. To make Mimicry consistent with our framework, we employ the Cuckoo sandbox [17] in place of WEPAWET (which was in any case discontinued) to validate maliciousness of the resulting PDF file.In addition to the original version of Mimicry, we implement an enhanced variation, Mimicry+, with two modifications. This naturally translates into the following multi-objective optimization in feature space:minimize x Q(x) = f (x) + lc(x M , x),(2)where f (x) is the score of a feature vector x, with the actual classifier (such as SVM) g(x) = sgn( f (x)), x M the malicious seed, x an evasion instance, c(x M , x) the cost of transforming x M into x, and l a parameter which determines the feature transformation cost. Since in most of our experiments features are binary, the choice of l 2 norm (as opposed to another l p norm) is not critical.As the optimization problem in Equation (2) is non-convex and variables are binary in three of the four cases we consider, we use a stochastic local search method designed for combinatorial search domains, Coordinate Greedy (alternatively known as iterative improvement), to compute a local optimum (the binary nature of the features is why we eschew gradientbased approaches) [18,23]. This plot demonstrates non-trivial effectiveness of EvadeML: the first few iterations are clearly insufficient, as re-running EvadeML creates many new evasions that cannot be correctly detected by Feature-Space Retraining Next, we experimentally evaluate the effectiveness of retraining with a feature-space model of evasion attacks in obtaining robust ML in the face of the EvadeML realizable attack. In contrast, FSR achieves a 70% evasion robustness, a significant boost over the original Hidost, to be sure, but far below the evasion robustness of RAR.Evaluating these classifiers on non-adversarial test data in terms of ROC curves (Figure 3 (right)), we can observe that RAR achieves comparable accuracy (> 99.9% AUC) with the original Hidost classifier on non-adversarial data, and provides even better True Positive Rate (TPR) when False Positive Rate (FPR) is close to zero. We trained both real-valued and binarized PDFRate (henceforth, PDFRate-R and PDFRate-B) on the same dataset as SL2013 and Hidost, and achieved > 99.9% AUC for both classifiers on test data. Observe that while RAR indeed achieves a highly robust classifier (96% robustness), FSR actually performs even better, with 100% robustness.Comparing RAR and FSR performance on non-adversarial data (Figure 4 (right)), we observe that the high robustness of FSR does incur a cost: while RAR remains exceptionally effective (>99.99% AUC), FSR achieves AUC slightly lower than 99%, although most significantly, the degradation is rather pronounced for low FPR regions (below 0.015). One of our great surprises is the robustness of the binarized PDFRate: despite the fact that the real-valued PDFRate is quite vulnerable, the same classifier using binary features was 100% robust to EvadeML (Figure 5 (left)). Feature-space retrained PDFRate-B also exhibits 100% evasion robustness, although it does require a number of iterations to converge.Considering now the performance of PDFRate-B and FSR on non-adversarial test data ( Figure 5 (right)), we can make two interesting observations. The key question is whether we can devise a simple way of anchoring feature-space attacks in the application domain to allow us to meaningfully and minimally constrain abstract attacks to reflect some of the constraints that real attacks face. The reason is that conservation is connected to the relationship between features and malicious functionality, rather than statistical properties of non-evasion data; for example, features which are strongly correlated with malicious behavior are often a consequence of attacker "laziness" (such as whether a PDF file has an author), and are easy for attackers to change. To address the third question, we learn a linear SVM classifier for SL2013 with l 1 regularization (henceforth, Linear) where we empirically adjust the SVM parameter C to perform feature reduction until the number of the features is also 8; we find that only 3 of these are conserved features (see Appendix A.6 for a more detailed analysis of the relationship between statistically useful and conserved features). As we can see in Figure 6 (left), this classifier exhibits poor robustness; thus, statistical methods are insufficient to identify good conserved features.To address the fourth question, we create a classifier using only one boolean feature which identifies the presence of JavaScript in a PDF file (henceforth, we refer to this feature as JS). We formally capture this in the new optimization problem in Equation (3), where S is the set of conserved features:minimize x Q(x) = f (x) + lc(x M , x), subject to x i = x M,i , 8i 2 S.(3)Other than this modification, we use the same Coordinate Greedy algorithm with random restarts as before to compute adversarial examples. We observe that both the CFR and CFR-JS classifiers in the PDFRate-B family achieve 100% evasion robustness against EvadeML (Figure 9 (left)), just as the RAR and FSR counterparts had.However, a close look at Figure 9 (right) demonstrates that CFR and CFR-JS achieve far better performance on nonadversarial data, with >99.9% AUC, where improvements are particularly significant for small false positive rates compared to FSR (recall Figure 5 (right)). We start by considering the Mimicry and Mimicry+ attacks for both real-valued and binarized variants of PDFRate, with the same 100 malicious seeds employed in Section 5 and 6 as attack files.The results are shown in Figures 10 and 11, and offer two noteworthy findings. Our latter observation is particularly remarkable: although the conserved features are roundly defeated by this attack, the use of these as a part of a holistic retraining approach yields a classifier that remains robust. Below we briefly describe some of the related literature on adversarial evasion or adversarial example attacks and defenses; we refer readers to Vorobeychik and Kantarcioglu [40] for a broader and more in-depth treatment of the subject of ML attacks and defenses. For example, in a bag-of-words representation for spam filtering, these could correspond to the existence of URL or file attachments, and in SQL injection attacks, these may refer to the existence of specific SQL commands, such as Select.The main limitation of our study is in the specific choices we had to make to ensure that it is tractable. We implemented a particular class of feature-space attacks, using l 2 norm to measure the attacker's cost of feature manipulations, and stochastic local search to compute evasions. However, prior work suggests that this approach yields attacks that are close to optimal [23], with the use of random restarts playing a crucial role. We define that a structural path is a dependent of another if unilateral deleting the object associated with the latter causes a flip from 1 to 0 on the feature value of the former. Note that in the case of multiple corresponding and identical objects of a structural path, all of these objects are replaced simultaneously.After structural path deletion and replacement, for each malicious PDF file x i , we can get its conserved feature set S i , non-conserved feature set O i , and dependent feature set D j for any feature j 2 S i [ O i , which could be further leveraged to design evasion-robust classifiers. We now develop an approach for transforming a collection of S i , O i , and D j i for a set of malicious seeds i into a uniform set of conserved features.Obtaining a uniform set of conserved features faces two challenges: 1) minimizing conflicts among different conserved features, as a conserved feature for one malicious instance could be a non-conserved feature for another, and 2) abiding by feature interdependence if a conserved feature should be further eliminated.To address these challenges, we propose a Forward Elimination algorithm to compute the uniform conserved feature Algorithm 1 Forward Elimination for uniform conserved feature set. Input:The set of conserved features for x i (i 2 [1, n]), S i ;The set of non-conserved features for x i (i 2 [1, n] set for a set of malicious seeds {x 1 , x 2 ,...,x n }, given the conserved feature sets, non-conserved feature sets and dependent sets for each seed. In this case, SL2013 requires 510 features, Hidost needs 154, and PDFRate-B needs 83. Since merely applying statistical approaches on training data is insufficient to discriminate between these two classes of features, as demonstrated above, we need a qualitatively different approach which relies on the nature of evasions (as implemented in EvadeML) and the sandbox (which determines whether malicious functionality is preserved) to identify features that are conserved.We use a modified version of pdfrw [27] 5 to parse the objects of PDF file and repack them to produce a new PDF file.