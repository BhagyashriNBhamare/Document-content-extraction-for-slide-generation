With the prevalence of private user data stored on web sites, the risk posed by compromised renderer processes, and the advent of transient execution attacks like Spectre and Melt-down that can leak data via microarchitectural state, it is no longer safe to render documents from different web sites in the same process. We also evaluate its practicality, finding that it incurs a total memory overhead of 9-13% in practice and increases page load latency by less than 2.25%, while sufficiently preserving compatibility with actual web content. However, current browsers allow attackers to load victim sites into the same renderer process using iframes or popups, so the browser must trust security checks in the renderer process to keep sites isolated from each other.In this paper, we move to a stronger threat model emphasizing two different types of web attackers that each aim to steal data across web site boundaries. We hope to allow some origins to opt into origin-level isolation, as discussed in Section 6.3.Cross-site subresources (e.g., JavaScript, CSS, images, media) are not protected, since the web allows documents to include them within an execution context. Across all content types, we expect this filtering will protect most sensitive data today, but there are opportunities to greatly improve this protection with headers or web platform changes [21,71,73], as discussed in Section 6.1. Due to web-visible events such as beforeunload and unload and the fact that a navigation request might complete without creating a new document (e.g., a download or an HTTP "204 No Content" response), the browser process must coordinate with both old and new renderer processes to switch at the appropriate moment: after beforeunload, after the network response has proven to be a new document, and at the point that the new process has started rendering the new page. The browser process passes it to the current Document B object in this frame, which sends the message to the corresponding Document object in Renderer Process B. Similar message routing can support other cross-origin APIs, such as focus, navigation, or closing windows.State Replication. Since a.com/4 is not contiguous with the other two a.com frames and its layout may depend on properties assigned to it by b.com/3 (e.g., CSS filters), it has a separate widget within Renderer Process A, and its surface must be composited within b.com/3's surface.Widgets are also used for input event routing, such as mouse clicks and touch interactions. Additionally, the feature must be careful to avoid leaking information to renderer processes (e.g., whether there was a match in a crosssite sibling frame), and it must be robust to renderer processes that crash or become unresponsive.These updates are required for many features that combine data across frames or that perform tasks that span multiple frames: supporting screen readers for accessibility, compositing PDFs for printing, traversing elements across frame boundaries for focus tracking, representations of the full page in developer tools, and many others. The response may nominally fail within the requested context (e.g., an HTML file would produce syntax errors in a <script> tag), but the data would be present in the renderer process, where a compromised renderer or a transient execution attack could leak it.Unfortunately, it is non-trivial to perfectly distinguish which cross-site URLs must be allowed into a renderer process and which must be blocked. These could utilize separate processes, but we choose to keep these cases inprocess as an optimization, focusing our attention on true cross-site content.Other design decisions that help reduce process count include isolating at a site granularity rather than origin, keeping cross-site images in-process, and allowing extensions to share processes with each other. The behavioral restrictions of iframe sandbox (e.g., creating new windows or dialogs, navigating other frames) and Feature-Policy are currently enforced in the renderer process, allowing compromised renderers to bypass them. For example, 5 bugs are side channel attacks that rely on timing events that work even across processes, such as a frame's onload event, to reveal information about the frame.In general, we find that Site Isolation significantly improves robustness to renderer exploit attackers, protecting users' web accounts and lowering the severity of renderer vulnerabilities. Web browser vendors have pursued three types of strategies to mitigate transient execution attacks on the web, with varying strengths and weaknesses.First, most browsers attempted to reduce the availability of precise timers that could be used for attacks [14,39,48,67]. Table 1 shows how both types of attacks are able to target data inside or outside the attacker's process, and thus both Spectre and Meltdown are relevant to consider when mitigating memory disclosure attacks.Site Isolation mitigates same-address-space attacks by avoiding putting vulnerable data in the same renderer process as a malicious principal. Meltdown-US - - - Meltdown-P - - - Meltdown-GP - - - Meltdown-NM - - - Meltdown-RW* - - - Meltdown-PK* - - - Meltdown-BR* - - -Ultimately, cross-process and user/kernel boundaries must fundamentally be preserved by the OS and hardware and cannot be left to applications to enforce. In reported metrics, we found that private memory use per renderer process decreased 51.5% (87.2 MB to 42.3 MB) at Overall, Site Isolation has a 9-13% overhead.the 50th percentile and 28.6% (from 714.2 MB to 509.7 MB) at the 99th percentile. For example, a crossprocess postMessage will take longer than a same-process postMessage, due to an extra IPC hop through the browser process; a web page could perform a timing analysis to detect whether a frame is in a different process. There are several options for protecting additional content, from using headers to protect particular responses, to expanding CORB to cover more types, to changing how browsers request subresources.First, web developers can explicitly protect sensitive resources without relying on CORB, using a Cross-Origin-Resource-Policy response header [21] or refusing to serve cross-site requests based on the Sec-Fetch-Site request header [71]. Site Isolation faces greater challenges on mobile devices due to fewer device resources (e.g., memory, CPU cores) and a different workload: there are fewer renderer processes in the working set due to proactive discarding by the mobile OS, and thus fewer opportunities for process sharing. Prior to this work, all major production browsers, including IE/Edge [76], Chrome [52], Safari [70], and Firefox [43], had multi-process architectures that rendered untrustworthy web content in sandboxed renderer processes, but they did not enforce process isolation between web security principals, and they lacked architectural support for rendering embedded content such as iframes out-of-process. Other research demonstrated a need for an architecture like Site Isolation by showing how existing browsers are vulnerable to cross-site data leaks, local file system access via sync from cloud services, and transient execution attacks [25,33,53]. We may revise this to isolate each file in the future, since this group of local files may contain less trustworthy pages saved from the web.We assign content from extensions to a separate shared principal, and we isolate all browser UI pages, such as settings or download manager, from one another. While there are a set of limitations with its current implementation, we argue that Site Isolation offers the best path to mitigating the threats posed by compromised renderer processes and transient execution attacks.In this paper, Section 2 introduces a new browser threat model covering renderer exploit attackers and memory disclosure attackers, and it discusses the current limitations of Site Isolation's protection. Note that the actual document and proxy objects live in renderer processes; the corresponding browser-side objects are stubs that track state and route IPC messages between the browser and renderer processes.For example, suppose the document in a.com/2 invokes window.parent.frames ["b"]. Alternatively, communication channels can be scoped to a site, such that a renderer process has no means to express a request for data from another site.The CORB filtering policy in Section 3.5 also requires enforcements against compromised renderers, so that a renderer exploit attacker cannot forge a request's initiator to bypass CORB. The compositing process must support many types of transforms that are possible via CSS, without leaking surface data to a cross-site renderer process.Often, many frames on a page come from the same site, and separate surfaces for each frame may be unnecessary. Thus, we recommend that web developers use this header for CORBeligible URLs that contain sensitive data, to ensure protection without relying on confirmation sniffing.If a cross-site response with one of the above confirmed content types arrives, and if it is not allowed via CORS headers [18], then CORB's logic in the network component prevents the response data from reaching the renderer process. We mask some of this latency by (1) starting the process in parallel with the network request, and (2) running the old document's unload handler in the background after the new document is created in the new process.However, in some cases (e.g., back/forward navigations) documents may load very quickly from the cache. This strategy applies whether the attack targets data inside the process or outside of it, but it has a number of weaknesses that limit its effectiveness:â€¢ It is likely incomplete: there are a wide variety of ways to build a precise timer [35,58], making it difficult to enumerate and adjust all sources of time in the platform. 1 While less powerful than renderer exploit attackers, memory disclosure attackers are not dependent on any bugs in web browser code. Data stored on the client (e.g., in localStorage) and permissions granted to a site (e.g., microphone access) are not available to processes for other sites.Potential Protections. The Site Isolation architecture should be capable of upgrading the following practices to mitigate compromised renderers as well, but our current implementation does not yet fully cover them. In the six months after Site Isolation was deployed in mid-2018, Chrome has received only 2 SOP bypass bug reports, also mitigated by Site Isolation (compared to 9 reports in the prior six months). Such additional process separation is orthogonal to Site Isolation and offers complementary benefits, such as making the browser more modular, reducing the size of the browser process, and keeping crashes in one component isolated from the rest of the browser. Origin Isolation (d) further refines sites to origins and is the most desirable principal model in the long term, but backward compatibility and performance challenges currently limit its practicality. The overhead seen in these results is significantly higher than the 9-13% overhead we reported from real-world user workloads in the previous section. These pages require vastly different permissions and privileges, and a compromise of one page (e.g., a buggy extension) should not be able to take advantage of permissions granted to a more powerful page (e.g., a download management page that can download and open files). Proxies offer cross-process support for the small set of cross-origin APIs that are permitted by the web platform, as described in [52]. The shift to multi-process browsers typically required some changes to these existing engines in order to support multiple instances of them. We did encounter and fix some bugs that allowed detection of Site Isolation, such as differing JavaScript exception behavior for in-process and outof-process frames. Dong et al's evaluation relied on sequentially browsing top Alexa sites; we additionally collect measurements from browsing workloads in the wild, providing a more realistic performance evaluation. Other research browsers isolate web applications with principals that are similarly incompatible: Tahoma [16] uses custom manifests, while SubOS [31,32] uses full URLs that include path in addition to origin. Currently, the browser process uses slow path hit testing over out-of-process iframes, i.e., asking a parent frame's process to hit-test a specific point to determine which frame should receive the event, without revealing any further details about the event itself. Rather than targeting the cache timing attack or disrupting speculation, Site Isolation assumes that transient execution attacks may be possible within a given OS process and instead attempts to move data worth stealing outside of the attacker's address space, much like kernel defenses against Meltdown-US [15,24]. Traversing the frame hierarchy must be done synchronously within the process using proxies, but interactions between documents can be handled asynchronously by routing messages. With these changes, the privileged browser process can keep most cross-site sensitive data out of a malicious document's renderer process, making it inconsequential for a web attacker to access and exfiltrate data from its address space. Within a process, however, the OS and hardware have much less visibility into where isolation is needed. Figure 4 (a) shows the total browser memory use for each site, sorted by the number of renderer processes (shown in parentheses) that each site utilizes when loaded with Site Isolation. Multi-process browsers have traditionally focused on stopping web attackers from compromising a user's computer, by rendering untrusted web content in sandboxed renderer processes, coordinated by a higher-privilege browser process [51]. Prior work has shown that such attacks can be achieved by exploiting bugs in the browser's implementation of the Same-Origin Policy (SOP) [54] (known as universal cross-site scripting bugs, or UXSS), with memory corruption, or with techniques such as data-only attacks [5,10,11,33,53,63,68]. For example, sites might adopt headers like Cross-Origin-Opener-Policy to opt into a mode that can place a top-level document in a new process by disrupting some cross-window scripting [44]. â€¢ How well does process-level isolation of web sites upgrade existing security practices to protect against compromised renderer processes? Sites that use more memory tend to have smaller relative overhead, as their memory usage outweighs the cost of extra processes. The largest and most disruptive change for Site Isolation is the requirement to load cross-site iframes in a different renderer process than their embedding page. However, many core assumptions remained intact, such as the ability to traverse all frames in a page for tasks like painting, messaging, and various browser features (e.g., find-inpage). For example, microcode updates and OS mitigations (e.g., PTI or disabling HyperThreading) may be needed for cross-process or user/kernel attacks [15,24,40,57,66]. The data in this section was collected using pseudonymous metric reporting over a two-week period starting October 1, 2018, from desktop and laptop users of Chrome (version 69) on Windows who have this reporting enabled. Most paint times improve with Site Isolation because the spare process helps mask process startup costs, which play a larger role than network latency due to the benchmark's use of recorded network traffic. Finally, while Gazelle, OP2, and IBOS have outof-process iframes, our work overcomes many challenges to support these in a production browser, such as supporting the full set of cross-process JavaScript interactions, challenges with painting and input event routing, and updating affected features (e.g., find-in-page, printing). This appendix provides additional details on how we define principals used in Site Isolation. Our experimental design and data collection were reviewed under Google's processes.) In normal execution, a render