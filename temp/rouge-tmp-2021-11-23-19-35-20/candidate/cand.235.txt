In this paper, we propose an automated analysis approach that does not only identify the root cause of a given crashing input for a binary executable, but also provides the analyst with context information on the erroneous behavior that characterizes crashing inputs. A statistical analysis of all predicates allows us to identify the predicate pinpointing the root cause, thereby not only revealing the location of the root cause, but also providing an analyst with an explanation of the misbehavior a crash exhibits at this location. Instead, the particular control flow that makes a value from type A end up in B's place is at fault.In a naive approach, an analyst could inspect stack and register values with a debugger. Furthermore, REPT and RETRACER focus on providing an interactive debugging session for an analyst to inspect manually what happened before the crash.In cases such as the type confusion above, or when debugging JIT-based software such as JavaScript engines, a single crashing input may not allow identifying the root cause without extensive manual reasoning. This also reduces the number of locations an analyst has to inspect, as AURORA only considers instructions with a plausible explanation.To enable precise identification of the root cause, we first pick one crashing input and produce a diverse set of similar inputs, some of which cause a crash while others do not. Intuitively, these predicates capture interesting runtime behavior such as whether a specific branch is taken or whether a register contains a suspiciously small value.Consider our previous type confusion example and assume that a pointer to the constructor is called at some location in the program. • We implement a prototype of AURORA and demonstrate that it can automatically and precisely identify the root cause for a diverse set of 25 software faults.To foster research on this topic, we release the implementation of AURORA at https://github.com/RUB-SysSec/ aurora. The following code snippet shows a minimized example of Ruby code that leads to a type confusion bug in the mruby interpreter [16] found by a fuzzer:1 N o t I m p l e m e n t e d E r r o r = S t r i n g 2 Module . However, as we have replaced the exception type by the string type, the layout of the underlying struct is different: At the accessed offset, the String struct stores the length of the contained string instead of a pointer as it would be the case for the exception struct. MSAN uses shadow memory to track for each bit, whether it is initialized or not, thereby preventing unintended use of uninitialized memory.Using such tools, we can identify invalid memory accesses even if they are not causing the program to crash immediately. As a consequence, sanitizers are more precise and pinpoint issues closer to the root cause of a bug.Unfortunately, this is not the case for our example: recompiling the binary with ASAN provides no new insights because the type confusion does not provoke any memory errors that can be detected by sanitizers. Still, AFL manages to find various crashing inputs by adding new mruby code unrelated to the bug.To further strengthen the analysis, a fuzzer with access to domain knowledge, such as grammar-based fuzzers [28,35,48], can be used. Still, this leaves the human analyst with an additional input to analyze, which means more time spent on debugging.Overall, this process of investigating the root cause of a given bug is not easy and-depending on the bug type and its complexity-may take a significant amount of time and domain knowledge. Intuitively, the first relevant behavior during program execution that causes the deviation is the root cause.In a first step, we create two sets of related but diverse inputs, one with crashing and one with non-crashing inputs. Amongst good explanations, we prefer these that are able to predict the crash as early as possible.On a high-level view, our design consist of three individual components: (1) input diversification to derive two diverse sets of inputs (crashing and non-crashing), (2) monitoring input behavior to track how inputs behave and (3) explanation synthesis to synthesize descriptive predicates that distinguish crashing from non-crashing inputs. A predicate is a triple consisting of a semantic description (i. e., the Boolean expression), the instruction's address at which it is evaluated and a score indicating the ability to differentiate crashes from non-crashes. Through the means of various heuristics described in Section 4.4, we filter the conditions and deduce a set of locations close to the root cause of a bug, aiding a developer in the tedious task of finding and fixing the root cause. Given a control-flow edge from x to y, the predicate has_edge_to indicates that we observed at least one transition from x to y. Contrary, always_taken_to expresses that every outgoing edge from x has been taken to y. Finally, we evaluate predicates that check if the number of successors is greater than or equal to n ∈ {0, 1, 2}. In other words, such a predicate perfectly separates crashing and non-crashing inputs.Unfortunately, there are many situations in which we cannot find a perfect predicate; consequently, we assign each predicate a probability on how well it predicts the program's behavior given the test cases. We later employ this uncertainty to rank the different predicates:ˆ θ = C f + N f C f +C t + N f + N tWe count the number of both mispredicted crashes (C f ) and mispredicted non-crashes (N f ) divided by the number of all predictions, i. e., the number of all mispredicted inputs as well as the number of all correctly predicted crashed (C t ) and non-crashes (N t ). To avoid biasing our scoring scheme towards the bigger class, we normalize each class by its size:ˆ θ = 1 2 * C f C f +C t + N f N f + N tIfˆθIfˆ Ifˆθ = 0, the predicate is perfect. Thus, true and false positives/negatives are switched, resulting in a large amount of true positives (C t = 1013) and true negatives (N t = 2000) for the inverted predicate: ¬p 1 := min(rax) ≥ 0xffTesting another predicate p 2 for the same instruction withˆθ withˆ withˆθ 2 = 0.01, we calculate the score s 2 = 2 · abs(0.01 − 0.5) = 0.98. We exploit this fact to find the best value(s) for c using only O(n * log(n)) steps where n is the number of test cases.To implement this, we proceed as follows: In a preprocessing step, we collect all values for an expression r at the given instruction and sort them. This property of our scoring scheme allows us to update the score in constant time when checking the next candidate value of c.To calculate the score for any candidate value c i , we start at the smallest candidate c 0 and calculate the predicate's score by evaluating the predicate on all inputs and counting the number of correctly predicted outcomes. This results inˆθinˆ inˆθ = 1 2 2 2+0 + 0 = 0.5 and, consequently, in a score = 2 * abs( ˆ θ − 0.5) = 0, indicating that this is not a good candidate for c. Using the next candidate c = 0x0f, we now predict that the first input is crashing. Then, we obtain the execution ranks:p 1 : 1 2 · 1 2 + 1 3 ≈ 0.41 p 2 : 1 2 · 2 + 3 3 = 1.5 p 3 : 1 2 · 2 2 + 2 3≈ 0.83 Since we sort first by score and then by execution rank, we obtain the final predicate order (p 1 , p 3 , p 2 ). Based on the prototype implementation of AURORA, we now answer the following research questions: RQ 1: Is AURORA able to identify and explain the root cause of complex and highly exploitable bug classes such as type confusions, use-after-free vulnerabilities and heap buffer overflows? As monitoring inputs with conditional breakpoints is faster than tracing an input, we empirically set the default timeout to 60 seconds. Hence, we manually inspected the root cause, identifying a reasonable location for a fix. Note that some bugs only crash in the presence of a sanitizer (as indicated by ASAN or MSAN in Table 1) and that our tracing binaries are never instrumented to avoid sanitizer artifacts disturbing our analysis. Many of these false positives occur due to insufficient code coverage achieved during crash exploration, causing the sets of crashing and non-crashing inputs to be not diverse enough.To detect such false positives during our evaluation, we employed various heuristics: First, we use the predicate's annotations to identify functions related to one of the five categories of false positives and discard them. False Positive Categories Propagations In-depth Analysis Alloc CF DS Env Error#3 Perl - - 7 - - - - #6 mruby - -38 - - - - #7 objdump - 2 - - - - - #9 Python - 1 - 2 3 - - #12 Bash 1 1 - 1 4 8 7 #13 Bash 1 1 - - 4 5 4 #14 Python - - - 3 - 15 5 #17 Python 40 - 2 - - - 1 #20 PHP - - - 21 - - - #22 mruby - 1 - - - 4 3 #23 NASM 3 - - - 2 2 2 #24 Sleuthkit - 2 - - - - -all predicates found by AURORA are strongly related to the actual root cause. On average, AURORA takes about 50 minutes for tracing, while the predicate analysis takes roughly 18 minutes and ranking four minutes. Types in mruby are implemented as enum, as visible in the following snippet of mruby's source code (mruby/value.h):112 MRB_TT_STRING , / * 16 * / 113 MRB_TT_RANGE, / * 17 * / 114 MRB_TT_EXCEPTION , / * 18 * /Our identified root cause pinpoints the location where the developers insert their fix and semantically states that the type of the presumed exception object is smaller than 17. This value is then processed further, amongst others, by an integer division where it is divided 00:10 00:03 00:02 #24 Sleuthkit < 1 min < 1 min < 1 min #25 Lua 00:11 00:07 < 1 min by 0x10, resulting in a value of 0. As no crucial data is overwritten, the program flow continues as normal unless it was compiled with ASAN, which spots the out-of-bounds write.To prevent this bug, the developers introduced a fix where they check whether the allocated memory's size is sufficient to hold the struct. More precisely, upvalues are used to store a function's local variables that have to be accessed after returning from the function [39]. Thereby, the local array is never properly initialized and MSAN aborts program execution upon the use of the uninitialized memory.When analyzing the top 50 predicates, we find that they are are all related to the bug. This is a good example to demonstrate that sometimes defining the root cause can be a hard challenge even for a human.Analyzing the top 50 predicates reported, we find that AURORA generates predicates pointing to various hotspots, which show that the label is not initialized correctly. Despite the increased number of source code lines to investigate, the information provided by AURORA is still useful: for instance, for bug #16-where 15 lines are neededmost of the lines are within the same function and only six functions are identified as candidates for containing the root Table 2: Maximum and average distance between developer fix and crashing location in both all and unique executed assembly instructions. To capture a wide array of possible explanations of a software fault's root cause, we generate three different categories of predicates, namely (1) control-flow predicates, (2) register and memory predicates, as well as (3) flag predicates. The former is required as a starting point for our analysis, while the latter serves as ground truth for the evaluation.For each target, we compile two binaries: One instrumented with AFL that is used for crash exploration and one noninstrumented binary for tracing purposes. However, establishing the ground truth would be more complex and hence we use source code strictly for evaluation purposes.For our evaluation, we resort to the well-known AFL fuzzer and run its crash exploration mode for two hours with the proof-of-concept file as seed input. Running our subsequent analysis on the best 50 predicates reported by AURORA, we manually found that all of the 50 predicates are related to the bug and provide insight into some aspects of the root cause.The line with the predicate describing the location of the developers' fix is ranked 15th. The second predicate describes the function call where the upvalue references are fetched, which are then compared for equality in the developer fix, i. e., it is located closely before the fix. This happens in the unpack_m function when unpacking a base64 encoded value from a packed string. The local char array is initialized in two states during this processing step. Often, the real cause of a crash-referred to as root cause-is not located at the point the program crashes; instead, it might be far earlier in the program's execution flow. This yields the final predicate p(r) = x < 0x400254 that will be re-evaluated on the whole dataset.We observed that if all recorded constants are either valid stack or heap addresses (i. e., pointers), we receive a high number of false positives since these addresses are too noisy for statistical analysis. Furthermore, we provide the number of predicates and source lines (SLOC) a human analyst has to examine until the location is reached where the developers applied the bug fix (denoted as Steps to Dev. Thus, we now explore different techniques and discuss their limitations. Alternatively, the actual best predicate might be more complex than predicates that could be synthesized automatically; consequently, it cannot predict all cases perfectly.To handle such instances, we model the program behavior as a noisy evaluation of the given predicate. All of our experiments are conducted within a cloud VM with 32 cores (based on Intel Xeon Silver 4114, 2.20 GHz) and 224 GiB RAM. Our predicates are Boolean expressions describing concrete program behavior, e. g., "the maximum value of rax at this position is less than 2". Using the predicates as a metric for each instruction, we can automatically pinpoint the possible root cause of crashes. However, in such a situation, we would observe control-flow transitions to these discarded addresses from addresses that are visited by inputs from both sets. The situation is worsened as fuzzing campaigns often result in a large number of crashing inputs, even if only one actual bug is found: a fuzzer can identify multiple paths to a crash, while the fault is always the same. If the target application crashes on a given input-also referred to as test case in the following-the predicate should evaluate to true. We additionally provide the number of source code lines (column SLOC) a human analyst needs to inspect before arriving at the location of the developer fix since these fixes are applied on the source code level. Since this would distort the experiment, we exclude such bugs from the comparison.Finally, to provide an intuition of how well our approach performs, we analyze the top 50 predicates (if available) produced for each target, stating whether they are related to the bug or unrelated false positives. Another predicate we identify pinpoints if the condition allows skipping the initialization steps, stating that this is a characteristic inherent to crashing inputs. Starting at the crashing location, we can manually inspect the last few instructions executed, the registers at crashing point and the call stack leading to this situation. In the second line, we call the constants function of Module. This allows for a comparative analysis of how crashes and noncrashes behave on the buggy path.To efficiently generate such inputs, we can use the crash exploration mode bundled with fuzzers such as AFL. To evaluate AURORA, we analyze 25 targets that cover a diverse set of vulnerability classes, including five use-after-free vulnerabilities, ten heap buffer overflows and two type confusion vulnerabilities that previous work fails to account for. RQ 3: How many predicates are related to the fault?To