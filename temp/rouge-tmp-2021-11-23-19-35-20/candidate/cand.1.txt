Open-source frameworks such as PredictionIO and cloud-based services offered by Amazon, Google, Microsoft, BigML, and others have arisen to broaden and simplify ML model deployment.Cloud-based ML services often allow model owners to charge others for queries to their commercially valuable models. The adversary's goal is to extract an equivalent or near-equivalent ML model, i.e., one that achieves (close to) 100% agreement on an input space of interest.We demonstrate successful model extraction attacks against a wide variety of ML model types, including decision trees, logistic regressions, SVMs, and deep neural networks, and against production ML-as-a-service (MLaaS) providers, including Amazon and BigML. For some targets employing a model type, parameters or features unknown to the attacker, we additionally show a successful preliminary attack step involving reverse-engineering these model characteristics.Our most successful attacks rely on the informationrich outputs returned by the ML prediction APIs of all cloud-based services we investigated. For example, attacks against Amazon's system indirectly leak various summary statistics about a private training set, while extraction against kernel logistic regression models [57] recovers significant information about individual training data points.The source code for our attacks is available online at https://github.com/ftramer/Steal-ML. Outputs lie in the range Y.We distinguish between categorical features, which assume one of a finite set of values (whose set size is the arity of the feature), and continuous features, which assume a value in a bounded subset of the real numbers. An ML model extraction attack arises when an adversary obtains black-box access to some target model f and attempts to learn a modeî f that closely approximates, or even matches, f (see Figure 1). Some services allow users to monetize trained models by charging others for prediction queries.To use these services, a user uploads a data set and optionally applies some data pre-processing (e.g., field removal or handling of missing values refers to the ability to download and use a trained model locally, and 'Monetize' means that a user may charge other users for black-box access to her models. We formalize "closely matching" using two different error measures:• Test error R test : This is the average error over a test set D, given byR test ( f , ˆ f ) = ∑ (x,y)∈D d( f (x), ˆ f (x))/|D|. It is widely used in many scientific fields (e.g., medical and social sciences) and is supported by all the ML services we reviewed.Formally, a LR model is defined by parameters w ∈ R d , β ∈ R, and outputs a probability f 1 (x) = σ (w · x + β ), where σ (t) = 1/(1 + e −t ). LR is a linear classifier: it defines a hyperplane in the feature space X (defined by w · x + β = 0), that separates the two classes.Given an oracle sample (x, f (x)), we get a linear equation w·x+β = σ −1 ( f 1 (x)). We consider two types of MLR models: softmax and one-vs-rest (OvR), that differ in how the c binary models are trained and combined: A softmax model fits a joint multinomial distribution to all training samples, while a OvR model trains a separate binary LR for each class, and then normalizes the class probabilities.A MLR model f is defined by parameters w ∈ R cd , β β β ∈ R c . For our attacks, MLPs and MLRs mainly differ in the number of unknowns in the system to solve. We now move to a less mainstream model class, kernel logistic regression [57], that illustrates how extraction attacks can leak private training data, when a model's outputs are directly computed as a function of that data.Kernel methods are commonly used to efficiently extend support vector machines (SVM) to nonlinear classifiers [14], but similar techniques can be applied to logistic regression [57]. We note that KLRs could easily be constructed in any ML library that supports both kernel functions and LR models.A KLR model is a softmax model, where we replace the linear components w i · x + β i by a mapping ∑ s r=1 α i,r K(x, x r ) + β i . We solve an under-determined system with 41,216 equations (using gradient descent with 200 epochs), and recover a modeî f achieving R TV test , R TV unif in the order of 10 −3 . Here, we propose attacks that exploit ML API specificities, and that apply to decision tree models used in MLaaS platforms.Our tree model, defined formally in Appendix A, allows for binary and multi-ary splits over categorical features, and binary splits over numeric features. We then search for all constraints on x that have to be satisfied to remain in that leaf, using procedures LINE SEARCH (for continuous features) and CAT SPLIT (for categorical features) described below. 1:x init ← {x 1 ,...,x d } 񮽙 random initial query 2: Q ← {x init }񮽙 Set of unprocessed queries 3: P ← {} 񮽙 Set of explored leaves with their predicates 4: while Q not empty do 5:x ← Q.POP() 6:id ← O(x) 񮽙 Call to the leaf identity oracle 7:ifid ∈ P then 񮽙 Check if leaf already visited 8: continue 9: end if 10: for 1 ≤ i ≤ d do 񮽙 Test all features 11:if IS CONTINUOUS(i) then 12:for (α, β ] ∈ LINE SEARCH(x, i, ε) do 13: if x i ∈ (α, β ] then 14: P[id]. The main issue with duplicate ids comes from the LINE SEARCH and CATEGORY SPLIT procedures: if two queries x and x 񮽙 differ in a single feature and reach different leaves with the same id, the split on that feature will be missed. To reverse-engineer the binning transformation, we use linesearches similar to those we used for decision trees: For each numeric feature, we search the feature's range in input space for thresholds (up to a granularity ε) where f 's output changes. Model hyper-parameters for instance (such as the free parameter of an RBF kernel) are typically chosen through cross-validation over a default range of values.Given a set of attack strategies with varying assumptions, A can use a generic extract-and-test approach: each attack is applied in turn, and evaluated by computing R test or R unif over a chosen set of points. Note that A needs to interact with the model f only once, to obtain responses for a chosen set of extraction samples and test samples, that can be re-used for each strategy.Our attacks on Amazon's service followed this approach: We first formulated guesses for model characteristics left unspecified by the documentation (e.g., we found no mention of one-hot-encoding, or of how missing inputs are handled). We primarily focus on settings where A can make direct queries to an API, i.e., queries for arbitrary inputs x ∈ X . A linear classifier is defined by a vector w ∈ R d and a constant β ∈ R, and classifies an instance x as positive if w · x + β > 0 and negative otherwise. Their attack uses line searches to find points arbitrarily close to f 's decision boundary (points for which w · x + β ≈ 0), and extracts w and β from these samples.This attack only works for linear binary models. Indeed, for any kernelK poly (x, x 񮽙 )=(x T · x 񮽙 + 1) d , we can derive a projection function φ (·), so that K poly (x, x 񮽙 )=φ (x) T · φ (x 񮽙 ). This transforms the kernel SVM into a linear one, since the decision boundary now becomes w F · φ (x) + β = 0 where w F = ∑ t i=1 α i φ (x i ). However, for budgets large enough to run line searches in each dimension, the Lowd-Meek attack is clearly the most efficient.For the models we trained, about 2,050 queries on average, and 5,650 at most, are needed to run the LowdMeek attack effectively. While we have not experimented with ensemble methods as targets, we suspect that they may be more resilient to extraction attacks, in the sense that attackers will only be able to obtain relatively coarse approximations of the target function. • Model extraction attacks against models that output only class labels, the obvious countermeasure against extraction attacks that rely on confidence values. We then present a more detailed threat model informed by characteristics of the aforementioned ML services.Avoiding query charges. ADD('x i ∈ S')񮽙 Values for current leaf 22:for v ∈ V do 23:Q.PUSH(x[i] ⇒ v) 񮽙New leaves to visit 24: end for 25: end if 26: end for 27: end whileThe LINE SEARCH procedure (line 12) tests continuous features. Therefore, we may need to find a large number of points close to a decision boundary in order to extract it accurately.We evaluated our attacks on the multiclass models from Table 3. Apart from PredictionIO, all of the services we examined respond to prediction queries with not only class labels, but a variety of additional information, including confidence scores (typically class probabilities) for the predicted outputs.Google and BigML allow model owners to monetize their models by charging other users for predictions. In settings where an ML model serves to detect adversarial behavior, such as identification of spam, malware classification, and network anomaly detection, model extraction can facilitate evasion attacks. The adversary's goal is to use as few queries as possible to f in order to efficiently compute an approximationˆfapproximationˆ approximationˆf that closely matches f . We then build a set S of values that lead to the current leaf, i.e., S = {R}, and a set V of values to set in x to explore other leaves (one representative per leaf). For a linear model for instance, we can trivially re-construct the model by issuing queries with a single feature specified, such as to obtain equations with a single unknown in X . This baseline strategy simply consists in sampling m points x i ∈ X uniformly at random, querying the oracle, and training a modeî f on these samples. For a budget of 100 · k, where k is the number of model parameters, we get R test = 99.16% and R unif = 98.24%, using 108,200 queries per model on average. The model inversion attack explored by Fredrikson et al. [23] uses access to a classifier f to find the input x opt that maximizes the class probability for class i, i.e., x opt = argmax x∈X f i (x). We can modify line 7 in Algorithm 1 to detect id duplicates, by checking not only whether a leaf with the current id was already visited, but also whether the current query violates that leaf's predicates. For online attacks on ML services, discussed next, this cost is trumped by the delay for the inherently adaptive prediction queries that are issued. We thus focus on evaluating the three retraining attacks we introduced, for the type of ML models we expect to find in real-world applications.We focus on softmax models here, as softmax and onevs-rest models have identical output behaviors when only class labels are provided: in both cases, the class label for an input x is given by argmax i (w i · x + β i ). These models are generated by a training algorithm T that takes as input a training set {(x i , y i )} i , where (x i , y i ) ∈ X × Y is an input with an associated (presumptively correct) class label. For exactly one feature (the root's splitting feature), the input will reach a different node. For instance, BigML reports confidences with 5 decimal places, and Amazon provides values with 16 significant digits.To understand the effects of limiting precision further, we re-evaluate equation-solving and decision tree pathfinding attacks with confidence scores rounded to a fixed decimal place. Popular models include support vector machines (SVMs), logistic regressions, neural networks, and decision trees.ML algorithms' success in the lab and in practice has led to an explosion in demand. Moreover, for services with black-box-only access (e.g., Amazon or Google), a user may abuse the service's resources to train a model over a large data set D (i.e., |D| | d), and extract it after only d + 1 predictions. These models have d + 1 parameters, and we vary the query budget as α · (d + 1), for 0.5 ≤ α ≤ 100. For softmax models for instance, the equations take the form e w i ·x+β i /(∑ c−1 j=0 e w j ·x+β j ) = f i (x). As some of our extraction attacks leak training data information (Section 4.1.3), one may ask whether DP can prevent extraction, or at least reduce the severity of the privacy violations that extraction enables.Consider na¨ıvena¨ıve application of DP to protect individual training data elements. It issues m adaptive queries to the oracle using line search techniques, to find samples close to the decision boundaries of f . Our equation-solving attacks achieved similar or better results with 100× less queries. 2 • Uniform error R unif : For a set U of vectors uniformly chosen in X , letR unif ( f , ˆ f ) = ∑ x∈U d( f (x), ˆ f (x))/|U|. For MLR models with k = c·(d + 1) parameters (c is the number of classes), k queries were sufficient to achieve perfect extraction (R test = R unif = 0, R TV test and R TV unif below 10 −7 ). Our goal is two-fold: (1) Find the predicates that x has to satisfy to end up in leaf id 2 (i.e., Size ∈ (40, 60], Color = R), and (2) create new inputs x 񮽙 to explore other paths in the tree. For example, for logistic regression, the confidence value is a simple log-linear function 1/(1+e −(w·x+β ) ) of the ddimensional input vector x. By querying d + 1 random d-dimensional inputs, an attacker can with high probability solve for the unknown d + 1 parameters w and β defining the model. Looking ahead, we will see that in some cases, significant information about training data is leaked trivially by successful model extraction, because the model itself directly incorporates training set points.Stepping stone to evasion. The models listed for Google's API are a projection based on the announced support of models in standard PMML format [25]. We show slower, but still potentially dangerous, attacks in this setting that build on prior work in learning theory. We first train a model with no categorical features, and quantile binning disabled (this is a manually tunable parameter), over the Digits data set. To illustrate our attack's success, we train a softmax regression, a OvR regression and a MLP on the Adult data set with target 'Race' (c = 5). We initialize the extracted representers to uniformly random vectors in X , as we assume A does not know the training data distribution. For attacks that require a large number of non-adaptive queries, we expect batch predictions to be faster than real-time predictions. Our new attacks extract models matching targets on >99% of the input space for a variety of model classes, but need up to 100× more queries than equation-solving attacks (specifically for multiclass linear regression and neural networks). Table 3. If A reverseengineers ex, she can query the service on samples M in input space, compute x = ex(M) locally, and extract f in feature-space using equation-solving. One would not expect, however, that this prevents model extraction, as DP is not defined to do so: consider a trivially useless learning algorithm for binary logistic regression, that discards the training data and sets w and β to 0. In such attacks, an adversary with black-box access , but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., "steal") the model. This could undermine a service's business model, should prediction fees be used to amortize the high cost of training.For each binary data set shown in Table 3, we train a LR model and extract it given d + 1 predictions. More subtly, in blackbox-only settings (e.g., Google and Amazon), a service's business model may involve amortizing up-front training costs by charging users for future predictions. We first introduce a generic equation-solving attack that applies to all logistic models (LR and neural networks). Querying partial inputs vastly improves our attack: we require far less queries (except for the Steak Survey model, where Algorithm 1 only visits a fraction of all leaves and thus achieves low success) and achieve higher accuracy for trees with duplicate leaf ids. We show how confidence values can nonetheless be exploited as pseudo-identifiers for paths in the tree, facilitating discovery of the tree's structure. An adversary may know any of a number of pieces of information about a target f : What training algorithm T generated f , the hyper-parameters used with T , the feature extraction function ex, etc. Our attacks might improve for higher budgets but it is unclear whether they would then provide any monetary advantage over using ML APIs in an honest way. Lowd and Meek [36] note that their extraction attack does not work in this setting, as A can not run line searches directly over X . In our example, we could have V = {B, G, Y} or V = {B, G, O}. A leaf-identity oracle O takes as input a query x ∈ X and returns the