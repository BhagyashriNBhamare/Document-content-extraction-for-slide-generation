Many approaches have been proposed (see Section 2), however, despite the numerous efforts in this area, a recent survey on common obfuscating transformations and deobfuscation attacks indicates that after more than two decades of research, we are still lacking reliable concepts for evaluating the resilience of code obfuscation against attacks [42]. • A model that can predict the resilience of several code obfuscating techniques against an attack based on symbolic execution, with over 90% accuracy for 80% of the programs in our dataset.The remainder of this paper is organized as follows. This taxonomy states that code obfuscation should be evaluated with respect to: potency against humanassisted attacks, resilience against automated attacks, cost (in terms of performance overhead) added by the obfuscating transformation and stealth, which measures the difficulty of identifying parts of obfuscated code in a given program. Collberg et al. [15] also proposed using several existing software features to evaluate potency, namely: program length, cyclomatic complexity, nesting complexity, data flow complexity, fan-in/-out complexity, data structure complexity and object oriented design metrics. In this paper we are chiefly concerned with predicting the effort needed to run a successful deobfuscation attack.Wu et al. [48] propose using a linear regression model over a fixed set of features, for measuring the potency of obfuscating transformations. They suggest obtaining the ground truth for training and testing a linear regression model, from security experts who manually deobfuscate the obfuscated programs and indicate the effort required for each program, which is far more expensive compared to our approach of using automated attacks. However, in many cases we can consider the effort needed to build the deobfuscator to be negligible, because an attacker needs to invest the effort to build a deobfuscator only once and can then reuse it or share it with others. Our general approach is illustrated as a work-flow in Figure 1, where ovals depict inputs and outputs of the software tools, which are represented by rectangles. Once the attack times are recorded and software features are extracted from all programs, one could directly use this information to build a regression model for predicting the time needed for deobfuscation. After the relevant features are selected, the framework uses this subset of features to build a regression model via a machine learning algorithm (step 4 in Figure 1). One can substitute the obfuscation tool in Figure 1, with any kind of software protection mechanism (e.g. code layout randomization [38]) and the deobfuscation tool by any known attack implementation corresponding to that software protection mechanism (e.g. ROPeme [27]). Since we only want to focus on the second part of this attack (i.e. symbolically executing the license checking code snippet), our C program generator produces a large number of simple programs with diverse license checking algorithms, having a variety of controland data-flows. Each generated function takes an array of primitive type (e.g. char, int) as input (i.e. in) and outputs another array of primitive type (i.e. out), as shown in Figure 2. Possible values include: arithmetic operators (addition PlusA, subtraction MinusA, multiplication Mult, division Div and modulo Mod), left shift Shiftlt, right shift Shiftrt, comparison operators (less than Lt, greater than Gt, less or equal Le, greater or equal Ge, equal Eq, different Ne) and bitwise operators (and BAnd, or BOr and xor BXor). Few inputs of the random function take this path, hence, finding such an input is equivalent to finding a valid license key.The reason why we chose to implement these features is that we suspect them to be relevant for the deobfuscation attack presented in [5], which is used in our case study presented in Section 4. Given a set of several software features (e.g. complexity metrics), it is unclear which software features one should aim to change (by applying various obfuscating transformations), such that the resulting obfuscated program is more resilient against certain automated deobfuscation attacks. One intuitive way to select relevant features, first proposed by Hall [22], is by computing the Pearson correlation [39] After computing the correlation, we sort the features by their absolute correlation values in descending order and store them in a list L. Another way of selecting relevant features from a large set of features is to first build a regression model (e.g. via random forest, support vector machines, neural networks, etc.), using all available features and record the prediction error. Rank the variables according to their average difference in prediction error, i.e. the higher the prediction error, the more important the variable is for the accuracy of the regression model.Similarly, to the previous approach based on Pearson correlation, we select those features which have the highest importance. RQ2 Which regression algorithms generate models that can predict the attack effort with the lowest error?Due to space constraints, in this paper we will focus on the deobfuscation attack based on symbolic execution presented in [5], which is equivalent to extracting a secret license key hidden inside the code of the program via obfuscation. All steps of the experiment were executed on a physical machine with a 64-bit version of Ubuntu 14.04, an Intel Xeon CPU having 3.5GHz frequency and 64 GB of RAM. • The operators allowed in expressions: RandomFunsOperators presented in Table 1 (4 values), which also describes each parameter value. • The number of statements per basic block was changed via the value of n ∈ {1, 2} from Table 1: Operator parameter values given to C code generator used for generating dataset.RandomFunsControlStructures Parameter Value (see grammar in Figure 3) win!" Table 3 shows the minimum, median, average and maximum values of various code metrics of only the original (un-obfuscated) set of programs, as computed by the Unified Code Counter (UCC) tool [37] and the total number of lines of code (LOC). In sum, we transform the path that corresponds to a successful deobfuscation attack into a SAT instance (via an SMT instance), and then compute characteristics of this formula, to be used as features for predicting the effort of deobfuscating the program.For computing source code features often used in software engineering, on both the original and obfuscated programs, we used the Unified Code Counter (UCC) tool [37]. For this purpose we performed a 10-foldcross validation with linear and random forest (RF) regression models using all combinations of 5, 10 and 15 metrics, as well as a model with all metrics. The strongest Pearson correlation of the time needed for running the deobfuscation attack is with the average size of clauses in the SAT instance (mean clause), followed by: the average number of times any one variable is used (meanvar), the standard deviation of the ratio of inter to intra community edges (sdedgeratio), the average number of intra community edges (meanintra), the average number of times a clause with the same variable (but different literals) is repeated (mean reused), the average community size (meancom), the number of unique edges (unique edges), the number of variables (vars), the standard deviation of the number of inter community edges (sdinter), the maximum number of distinct communities any one community links to (max community), the number of communities detected with the online community detection algorithm (ol coms), the maximum ratio of inter to intra community edges within any community (maxedgeratio), the maximum number of inter community edges (maxinter), the maximum number of edges in a community (max total) and finally the type of obfuscation transformation employed. The 4th most important variable in Figure 6 is the average number of inter community edges (meaninter), followed by: sdedgeratio, meancom, meanintra (see descriptions of these 3 features in Section 4.2.1), the standard deviation of community sizes (sdcom), the standard deviation of intra community edges (sdintra), the modularity of the SAT graph structure (ol q), the overall ratio of inter to intra community edges (edgeratio), the category of the McCabe cyclomatic complexity [30] (Risk), the number of outer-loops (L1.Loops), the size of the longest clause (max clause) and the number of communities that have the maximum number of inter community edges (num max inter). The Risk has four possible values depending on the value of the cyclomatic complexity (CC), i.e. low if CC ∈ [1,10], moderate if CC ∈ [11,20], high if CC ∈ [21,50] and very high if CC is above 50. To check this observation, we downloaded the Mironov-Zhang [32] and the Li-Ye [28] benchmark suites for SAT solvers, containing solvable versions of more realistic hash functions such as MD5 and SHA. However, the opaque predicate and literal encoding alone do not have such an effect.As a conclusion of this section we observe that balanced community structures translate to a high diffusion of the symbolic input to output bits, i.e. affecting any bit of the input license key will affect the result of the output. To interpret the root-mean-squared-error (RMSE) we normalize it by the range between the fastest and slowest times needed to run the deobfuscation attack on any program from our dataset. Since our dataset contains outliers (i.e. either very high and very low deobfuscation times), the normalized RMSE (NRMSE) values are very low for all algorithms, regardless of the selected feature subsets, as shown in Table 7. Table 7: The NRMSE between model prediction and ground truth (average over NRMSE of 10 models) Percentage of programs 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 Random forests (RFs) were proposed by Breiman [9] as an extension of random feature extraction, by including the idea of "bagging", i.e. computing a mean of the prediction of all random decision trees. The accuracy of this model is lower than the RF model from Figure 11, i.e. the maximum rel- 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 ative error is just below 35% for 90% of the programs, when we remove 10% of the outliers. The reason why SVM performs worse than RF is due to the bagging technique applied by RF, whereas SVM uses a single non-linear function.Again we chose to show the SVM model built using the features selected via variable importance in Fig- ure 13, because, as we can see from Figure 14, the maximum and median error rates for this model are much lower than the SVM models built using only UCC metrics or the features selected via Pearson correlation. Given the set of all code features as a set of input variables, GP [24] searches for models that combine the input variables using a given set of functions used to process and combine these variables, i.e. addition, multiplication, subtraction, logarithm, sinus and tangent in our experiments. For instance, the best GP model built using the features selected via variable importance is presented in equation 1:time = (edgeratio + cos(ol coms) + cos(cos(sdcom + num max inter) + L1.Loops)) * (sdinter * (sdedgeratio − sin(meanintra * −1.27))) * (sdedgeratio − sin(meanintra * −1.27)) * (1.03 − sin(0.04 * sdinter)) * sdedgeratio + 10.2(1)Note that only seven distinct features were selected by the GP algorithm for this model, from the subset of 15 features. Figure 19 shows the prediction error of our best RF model (trained using 10-foldcross-validation on both datasets), for the samples in the smaller dataset alone, has similar levels to the prediction error of the entire dataset.We also performed a reality check, i.e. we verified that the SAT features we identified are also relevant for the realistic hash functions from the Mironov-Zhang [32] and the Li-Ye [28] benchmark suites for SAT solvers.