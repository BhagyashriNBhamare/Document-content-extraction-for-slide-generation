For example, even for a perfect knowledge attacker, any creative way of inserting pages from the most-benign documents to the malware can be verified to keep the malicious classification.If we train classifiers to be verifiably robust against building block attacks, we can raise the bar for more sophisticated attacks to succeed. By training building block robustness properties, we eliminate simple and easy evasions, which increases the search cost for attackers.In this paper, we take the first steps towards training a PDF malware classifier with verifiable robustness properties, and we demonstrate that such classifiers also increase the attack cost even for the attackers not bounded by these properties. We use these attacks to quantify the increase in the unbounded attacker cost caused by the verifiable robust training.Using our new distance metric for the PDF tree structure, we specify two classes of robustness properties, subtree deletion properties and subtree insertion properties. Although adversarially robust training is known to achieve strong robustness against a specific type of attacker, the gradient attacker [39], our verifiably trained models can obtain superior verifiable robustness against all possible bounded attackers while keeping high test accuracies and low false positive rates.Perhaps even more importantly, we show that a verifiably robust classifier with two proposed robustness properties can already increase the cost for the unbounded attacker. The results show that training verifiably robust PDF malware classifiers even for carefully chosen simple robustness properties can effectively increase the bar for the attacker to solve the evasion problem.As defenders, making all evasion attacks on malware classifiers computationally infeasible is an extremely hard problem. For example, many PDF readers do not need the correct length field to decode the stream, and malware authors can delete the field to evade the classifier. Different methods have been used to formally verify the robustness of neural networks over input regions [20,23,24,31,34,37,44], such as abstract transformations [25], symbolic interval analysis [55,56], convex polytope approximation [58], semidefinite programming [45], mixed integer programming [50], Lagrangian relaxation [22] and relaxation with Lipschitz constant [57], which essentially solve the inner maximization problem. By using the worst case bounds derived by formal verification techniques, verifiably robust training [21,41,53,59] can obtain such verified robustness properties.The training method has been applied to image datasets to increase verifiable robustness, usually with the tradeoff of lower accuracy and higher computation and memory cost for the training. The distance d(x,x 0 ) is the cardinality of the resulting set.If the attacker inserts benign pages into the PDF malware under the /Root/Pages subtree (Figure 1b), this operation will not exceed subtree distance one, no matter how long the malicious PDF document becomes. Subtree Insertion Property (Subtree Distance 1): given a PDF malware, all possible manipulations to the PDF bounded by inserting an arbitrary subtree under the root, do not result in a benign prediction by the classifier.The attacker first chooses any one of the subtrees, and then chooses an arbitrary shape of the subtree for the insertion. Therefore, the property can overestimate the worst case behavior of the classifier.Subtree Deletion Property (Subtree Distance 1): given a PDF malware, all possible manipulations to the PDF bounded by deleting an arbitrary subtree under the root, do not result in a benign prediction by the classifier.For the PDF malware example shown in Figure 1b, this property allows deleting any one of the following: /Root/Type, /Root/Pages, and /Root/OpenAction. Next, we describe properties with larger distances.Subtree Deletion Property (Subtree Distance 2): the strongest possible attackers bounded by deletions within any two subtrees under the root, cannot make the PDF classified as benign.Subtree Insertion Property (Subtree Distance N 񮽙 1): the strongest possible attackers bounded by insertions within all but one subtree under the root, cannot make the PDF classified as benign.Monotonic Property and Subtree Insertion Property (Distance N): Incer et al. [32] have proposed to enforce the monotonic property for malware classifiers. We focus on this property for the malicious PDFs, which is a key difference from the monotonic property.Larger distances bound a larger set of evasive malware variants, which can make malicious feature vectors more similar to benign ones and affect the false positive rate. Formally, given input x 2 X and a property D k (x) bounded by distance k, the transformation T f is sound if the following condition is true: 8x 2 X, we have{ f q ( ˜ x)| ˜ x 2 D k (x)} ✓ T f (D k (x))That is, the sound analysis over-approximates all the possible neural network outputs for the property. Consequently, we adopt the same principle to train for a combined loss as below.L = L(y, f q (x))+ max˜x2D max˜ max˜x2D k (x) L(y, f q ( ˜ x))(3)In Equation 3, the left-hand side of the summation denotes the regular loss for the training data point (x, y), and the right-hand side represents the robust loss for any manipulated˜xmanipulated˜ manipulated˜x bounded by distance k satisfying a defined robustness property D k (x). The machine is configured with Intel Core i7-9700K 3.6 GHz 8-Core Processor, 64 GB physical memory, 1TB SSD, Nvidia GTX 1080 Ti GPU, and it runs a 64-bit Ubuntu 18.04 system. For brevity, we will refer to the four robustness properties as property A (subtree deletion distance one), B (subtree insertion distance one), C (subtree deletion distance two), D (subtree insertion distance 41) and E (subtree insertion distance 42). We follow previous work [28,29,46] to build a feed-forward neural network with two hidden layers, each with 200 neurons activated by ReLU, and a final layer of Softmax activation. We augment the regular training dataset with an arbitrary subtree deleted from both malicious and benign PDFs, which maintains the performance for original PDFs because they also need to be tested under multiple deletion operations. Since monotonic property is such a strong enforcement for the classifier's decision boundaries, the malware classifier in [32] has 62% temporal detection rate over a large scale dataset containing over 1.1 million binaries.Note that the ensembles and monotonic classifiers are the only models we train with properties for both malicious and benign PDFs. Although the monotonic classifiers have higher VRA in insertion properties, since Robust A+B and Robust A+B+E have strong VRA in both insertion and deletion properties, they are more robust against unrestricted attacks than the monotonic classifiers, as shown by the results in the following sections. We implement the unbounded gradient attacker that performs arbitrary insertion and deletion guided by gradients, unrestricted by all robustness Table 4 are the lower bound of ERA values.properties. By round robin, we go through the list of randomly scheduled PDFs by rounds of attacks, until all of them are evaded.We run the attack on the best baseline and robust models: Baseline NN, Adv Retrain A+B, Ensemble A+B, Monotonic Classifiers, Robust A+B, and Robust A+B+E. We use peepdf [6] static analyzer to identify the suspicious objects in the PDF malware seeds, and then inject these objects to a benign PDF. Therefore, for each test PDF, we check whether any interval representing the lower bound of all zeros and the upper bound of the original subtree can be classified as malicious.Property B, D and E. Baseline NN is the first to reach 0 ERA at L 0 = 19, whereas Robust A+B+E requires the largest L 0 distance (68) to reach 0 ERA. Training robustness properties increased the false positive rates by under 0.5% for Robust A, B, and A+B models, which are acceptable. The smallest L 0 distances to generate one evasive PDF malware variant are 49, 39, 134, and 159 for Baseline NN, Adv Retrain A+B, Ensemble A+B, and Robust A+B, respectively. We keep track of past insertion and deletion operations separately, and prioritize new insertion and deletion operations to target a different subtree.Move and Scatter Combination Attack. We train the baseline neural network model with the regular training objective (Equation 1, Section 2.3.1), using the regular training dataset (Table 2). PDF malware exploits the vulnerabilities in the PDF reader in order to transfer execution control, e.g., to run shellcode or drop additional binary. The evolutionary algorithm uses a fitness score as feedback, to guide the search in finding evasive PDF variants by mutating the seed PDF malware. Robust optimization minimizes the worst case loss for all inputs in D k (x), solving a minimax problem with two components.q = argmin q Â max˜x2D max˜ max˜x2D k (x) L(y, f q ( ˜ x))(2)• Inner Maximization Problem: find˜xfind˜ find˜x that maximizes the loss value within the robustness region D k (x), i.e., the robust loss. Our best model achieved 99.68% and 85.28% verified robust accuracy (VRA) for the insertion and deletion properties, while maintaining 99.74% accuracy and 0.56% false positive rate. Although state-of-the-art PDF malware classifiers can be trained with almost perfect test accuracy (99%) and extremely low false positive rate (under 0.1%), it has been shown that even a simple adversary can evade them. We split the dataset into 70% train set and 30% test set, summarized in Table 2. Under different whitebox attacks, Robust A+B is more robust than the monotonic classifiers.After converting the evasive feature vectors to real PDFs, none of them are still malicious, since the MILP attack deletes the exploit (Appendix A.4). All the ERA numbers are higher than the corresponding VRA in is not effective at evading this model, since the attack could have used the solution from property B to reduce the ERA for the other two properties to at least 84.6%. In this paper, we will use the following two metrics to evaluate our verifiably robust PDF malware classifier.Estimated Since it is extremely hard, if not impossible, to have a malware classifier that is robust against all possible attackers, we aim to train classifiers to be robust against building block attacks. We thoroughly evaluate the robustness against twelve baseline models, using state-of-the-art measures including estimated robust accuracy (ERA) under gradient attacks and verified robust accuracy (VRA) against any bounded adaptive attacker. We further show that such robust training can also increase the bar for state-of-the-art unbounded attackers. Specifically for PDF malware, the operations include PDF object mutation operators [61], random morpher [18] and feature insertion-only generator [30]. erties. The robust loss is computed by the worst case within the bounded region of every training data input. PDF malware authors employ various techniques to evade the detection, e.g., add content from legitimate documents, crash the PDF reader, and obfuscate the PDF content. For the deletion properties, we train with deleting one or two entire subtrees; and for the insertion properties, we train with inserting one or 41 full subtrees. Starting from the root object, a parser resolves referred objects either using the crossreference table or scanning the PDF to get the object locations.The four objects in this file are dictionaries, indicated by << and >> symbols and enclosed by obj and endobj keywords. Symbolic interval analysis uses intervals to bound the adversarial input range to the model, then propagates the range over the neural network while keeping input dependency. This gives us between 5.74% and 8.78% VRAs for the monotonic classifiers under property A. Similarly, by testing the lower bound of two subtree deletion, we verify the monotonic classifiers to have 0 VRA for property C.Verifiably Robust Models: We can increase the VRA from as low as 0% to as high as 99%, maintaining high accuracy, with under 0.6% increase in FPR in properties A and B.Training a model with one robustness property can make it obtain the same type of property under a different distance. In the whitebox setting, our robust model maintains 7% higher estimated robust accuracy (defined in Section 2.3.4) against the unrestricted gradient attack and the Mixed Integer Linear Program (MILP) attack than the baseline models. One of the most ubiquitous applications is to detect PDF malware, which is a very popular infection vector for both large-scale mass and targeted attacks. Since training property E incurs higher FPR than properties with smaller subtree distances, we plan to experiment with training insertion property with small distance for benign samples as future work. For instance, inserting pages from a benign document to the PDF malware can increase the page count feature alone to be as big as the maximal integer value, which also affects many other counts. The summation is an empirical measure of the expected loss over the entire training dataset.q = argmin q Â L(y, ˆ y)(1)In the adversarial setting, for the input x, there can be a set of all possible manipulations˜xmanipulations˜ manipulations˜x bounded by a distance metric D k within distance k, i.e. ˜ x 2 D k (x). PDFrate [47] uses a total of 202 features including counts for various keywords and certain fields in the PDF. Specifically, if two feature vectors satisfy x  x 0 , then the classifier f guarantees that f (x)  f (x 0 ). Moreover , the state-of-the-art and new adaptive evolutionary attackers need up to 10 times larger L 0 feature distance and 21 times more PDF basic mutations (e.g., inserting and deleting objects) to evade our robust model than the baselines. We train seven verifiably robust models and compare them against twelve baseline models, including neural network with regular training, adversarially robust training, ensemble classifiers, and monotonic classifiers 1 . Many prior research projects have demonstrated that machine-learning-based PDF malware classifiers can achieve almost perfect test accuracy (99%) with extremely low false positive rate (under 0.1%) [47,48]. Since the the base learner in Ensemble D needs to classify an arbitrary subtree after certain deletion and insertion operations, it is not enough to gain VRA by learning specific subtree from the training dataset.Monotonic Classifiers: All the monotonic classifier