Correlating these results allows us to draw a set of overarching conclusions: Along with the dawn of JavaScript-driven applications in the early years of the millennium, the likelihood of client-side injection vul-nerabilities has risen. What modestly started in 1991 as a mere transportation mechanism for hypertext documents is now the driving force behind the majority of today's dominating technologies. However, from a security point of view, the Web's track record is less than flattering, to a point in which a common joke under security professionals was to claim that the term Web security is actually an oxymoron.Over the years, Web technologies have given birth to a multitude of novel, Web-specific vulnerability classes, such as Cross-Site Scripting (XSS) or Clickjacking, which simply did not exist before, many of them manifesting themselves on the Web's client side. The trend is also underlined by the increase in client-side APIs in the browser: while in 2006 Firefox featured only 12 APIs, it now has support for 93 different APIs ranging from accurate timing information to an API to interact with Virtual Reality devices 1 . This allows us to gain a historical perspective on the security aspects of an emerging and constantly evolving computing platform and also foreshadows future trends.Unfortunately, the majority of Web code is commercial and, thus, not open to the public. Many of the current state-of-the-art security testing methods can be adapted to work on the archived version of the sites, enabling an automated and scalable security evaluation of the historic code.Thus, we find that the archived client-side Web code offers the unique opportunity to study the security evolution of one of the most important technology platforms during (almost) its entire existence, allowing us to conduct historical analyses of a plethora of properties of the Web. The overall goal of this activity is to enable the correlation of trends in the security area with ongoing technological shifts.Resulting Security Problems With the ever-growing complexity of the deployed Web code and the constant addition of new powerful capabilities in the Web browser in the form of novel JavaScript APIs the overall amount of potential vulnerability classes has risen as well. Section 4 documents our security testing methodology and highlights our key findings in the realm of preserved security vulnerabilites.Introduction of Dedicated Security Mechanisms To meet the new challenges of the steadily increasing security surface on the Web's client side, several dedicated mechanisms, such as security-centric HTTP headers or JavaScript APIs, have been introduced. For each year, we used the first working Internet Archive snapshot of each domain as an entry point.Unlike Lerner et al. [19], who investigated the evolution of tracking, though, we did not restrict our analysis to the start pages of the selected sites. Since the number of domains varies for each year, throughout this paper we provide fractions rather than absolute numbers for better comparability.Threats to Validity Given the nature of the data collected by the Internet Archive, our work faces certain threats to validity. On the contrary, Internet Explorer does not automatically encode any part of a URL when accessed via JavaScript, i.e., especially in the case of Client-Side Cross-Site Scripting, our results provide a lower bound of exploitable flaws.Nevertheless, we believe that the Archive gives us the 2 For a full list of domains see https://goo.gl/eXjQfs Proxy Figure 1: Infrastructure overview unique opportunity to get a glimpse into the state of Web security over a 20-year time frame. Note that apart from the regular HTTP headers, the Archive also sends the original headers of the site at the time of archiving, prefixed with X-Archive-Orig-, allowing us to collect accurate original header information. This is in part related to modern browsers nowadays switching off Flash by default, and moreover the fact that HTML5 can be used to develop interactive advertisements instead of Flash.In addition to the core technologies, we considered jQuery in our analysis, since it is one of the main drivers behind powerful JavaScript applications. Also, the graph depicts the trend of an ever-increasing number of paths, underlining the increased complexity of modern applications.These figures clearly show that modern JavaScript applications are more powerful than ever, but also incur a higher complexity due to the large code base to maintain. The trend since then is clearly pointing upwards, reaching almost 12 distinct remote origins per domain by 2016.Cross-Domain Data Access Modern Web sites are often interconnected, bringing the need for cross-domain communication and data access. There are, however, a number of security issues associated with this, such as cross-domain data leakage [18] or the Rosetta Flash attack [32]. If a snippet wants to do such an authenticated request across domains, the remote HTTP server has to specifically allow the snippet's origin in the Access-Control-Allow-Origin header; a wildcard is not sufficient to grant access. To that end, we report on the ClientSide XSS vulnerabilities we found, analyze the insecure usage of postMessages over time, outline the (in)security of cross-domain communication in Flash, and show the general pattern of including outdated third-party library versions. Although it was shown by Son and Shmatikov [31] that the existence of an origin check does not preclude a vulnerability, we present the results as an estimation over our study period.The results of our analysis are shown in Table 1: postMessage Statistics ever a site used at least one postMessage receiver without an origin check, we marked this domain as not using the origin check. Their analysis efforts, however, were mostly manual; hence, while an in-depth analysis of the discovered receivers is not feasible for our work, we leave a more automated approach to such analyses to future work.Apart from the authenticity issue of postMessages, not specifying a target origin might endanger the confidentiality of an application's data. Therefore, whenever any third-party component is vulnerable, this implies that all sites which include the flawed code will suffer from the vulnerability.To understand the risk associated with this, we used retire.js [25], a tool to detect libraries and report known vulnerabilities in them, on the versions of jQuery we collected in our study. The results are also depicted in Figure 9: it becomes clear that the majority of Web sites used outdated versions of jQuery, for which known vulnerabilities existed at the time.Although this paints a grim picture, a vulnerable library does not necessarily directly imply a site at risk. These are sent along with every request to the server, allowing a user to establish a session in the first place and for the server to correctly attribute requests to a user. We mark a domain as using HTTP-only cookies when at least one cookie was set using the httponly flag. To still allow users of these sites an unhindered view on the pages, modern browsers are very error-tolerant, i.e., they compensate for a number of mistakes which can be introduced by developers. The HTTP/1.1 standard specifically states that such sniffing should only happen when no Content-Type header is sent from the server [5]. In the specific case, Internet Explorer's algorithm was also more aggressive than RFC2616 demanded: it tried to sniff content regardless of the presence of any Content-Type headers. The main idea relies on the ability of an attacker to mask a frame pointing to a benign-but-buggy site with the opacity CSS attribute on his own site. The attacker now tries to motivate the victim to click in the area in which this hidden frame resides. For our measurements, we only counted sites which use the protective measure by either setting it to DENY, SAMEORIGIN, its alias SAMEDOMAIN, or ALLOW-FROM with a specific list of domains. In its foundation, CSP is a technique that aims to specifically whitelist sources of code with the goal of stopping any attacker-provided code from being executed. Also, by default, CSP disallows the use of inline script elements and the eval construct.CSP has many more directives, allowing Web developers to control which hosts may be contacted to retrieve images or stylesheets, specifying how the site may be framed (deprecating the X-Frame-Options header), or to report violations of the policy. Given the results from previous work, investigating the security of the policies of single sites is out of scope for our work.Initially, CSP was introduced by Firefox and WebKit-based browsers (including Chrome) with different names, i.e., X-Content-Security-Policy and X-WebKit-CSP, respectively. In that case, the cookies are transferred in any connection to the domain for which they were set, regardless of the use of HTTPS.To ensure that neither an active attacker can strip SSL nor an unknowing developer can accidentally build an insecure application, browsers implement HTTP Strict Transport Security, or HSTS for short [8]. One reasonable explanation is that Google engineers were confident that no XSS vulnerabilities were contained in their sites, and wanted to ensure that no vulnerabilities could be introduced into otherwise bug-free sites.A more recent feature for securing the client side is the sandbox feature of iframes in HTML5. The Web's Complexity is still on the Rise In our study of the Web's evolution, we found that although several technologies for client-side interaction were developed over the years, the only prevailing one is JavaScript. Along with the introduction of powerful new APIs in the browsers, which nowadays, e.g., allow for client-toclient communication that was never envisioned by the Web's server/client paradigm, we find that the general complexity of client-side Web applications is on the rise. Moreover, sites rarely update their third-party components. However, such attacks can be stopped if sites start implementing Subresource Integrity, which ensures that included JavaScript is only executed when its has the correct checksum [1]. Nevertheless, given our sample of the top 500 pages, such attacks still threaten a large fraction of the Web users and developer training should focus more on these issues.Security vs. Utility Many new technologies introduced in browsers come with security mechanisms, such as the authenticity and integrity properties provided by the postMessage API. As we observed in our work, technology which enables communication across domain boundaries, such as JSONP, Flash's ability to access remote resources, or postMessages, is often used without proper security considerations. As an example, within two years of being fully supported by the three major browsers, the X-Frame-Options header was deployed by 20% of the sites we analyzed, within four years its adoption rate even reached more than 40%. In contrast, even though deprecated by CSP, the X-Frame-Options header still shows increased usage in 2016. To understand whether there is a correlation between actual vulnerabilities and the general understanding of security concepts for the Web, we compared the set of sites vulnerable against Client-Side XSS attacks with their use of security indicators. We chose Client-Side XSS specifically, since a vulnerability can be proven, whereas it is, e.g., unclear if usage of an outdated library could actually lead to an exploit.The results of this analysis are shown in Figure 11: for each indicator we checked how many sites were vulnerable against a Client-Side XSS attack. Comparing server-and clientside vulnerabilities with respect to the use of httponly cookies, however, is an interesting alley for future work.The correlation between httponly cookies and increased fraction of vulnerabilities might be caused by several reasons: applications that use session cookies are more likely to have a larger code base and thus, more vulnerabilities. Hence, we find that the late adopters of such new technologies are more likely to introduce Client-Side XSS vulnerabilities in their sites.CSP Deployment Another insight here is the fact that not a single site using CSP had a vulnerability, even leaving out the 10% threshold discussed above. Therefore, in the following, motivated by our findings, we discuss how Web security can move forward.Ease of Use Considering the security technologies we investigated, we find that regardless of the potential benefit, a security measures adoption rate is controlled mostly by the ease of its deployment. Thus, we argue that for future techniques ease of use should be a primary design concern.Make Security Mandatory Our findings highlight that if security checks are optional, they are oftentimes not used, as evidenced, e.g., by the lack of origin checking on postMessages. In the same year, Richards et al. [27] provided the first large-scale analysis of the (mis)use of eval, showing that while it can be replaced in certain cases, removing it all-together is impossible.