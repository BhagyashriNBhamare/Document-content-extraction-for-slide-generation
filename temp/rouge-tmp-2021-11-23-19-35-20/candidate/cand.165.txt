Using a set of new techniques, we show that DIZK scales to computations of up to billions of logical gates (100× larger than prior art) at a cost of 10 µs per gate (100× faster than prior art). This bottleneck severely limits the applicability of SNARKs, and motivates a basic question: can zkSNARKs be used for circuits of much larger sizes, and at what cost? DIZK enables applications on significantly larger instance sizes, e.g., image editing on photos of 2048 by 2048 pixels.DIZK makes a significant conceptual step forward, enlarging the class of applications feasible for zkSNARKs. Also, like other zkSNARKs, DIZK requires a trusted party to run a setup procedure that uses secret randomness to sample certain public parameters; the cost of this setup grows with circuit size, which means that this party must also use a cluster, which is harder to protect against attackers than a single machine. Moreover, fast algorithms for solving these tasks, such as Fast Fourier Transforms (FFTs), are notoriously memory intensive, and rely on continuously accessing large pools of shared memory in complex patterns. zkSNARKs transform the computation of a circuit into an equivalent representation called a Quadratic Arithmetic Program [37,55]: a circuit with N wires and M gates is transformed into a satisfaction problem about O(N) polynomials of degree O(M). Different parts of a zkSNARK leverage the sparsity of the matrices above in different ways: the so-called QAP instance reduction relies on their column sparsity ( §5), while the corresponding QAP witness reduction relies on their row sparsity ( §6). Finally, we emphasize that most of the technical work described above can be re-used as the starting point to distribute many other similar proof systems. We have thus packaged these standalone components as a separate library, which we deem of independent interest.We also briefly mention that supporting billion-gate circuits required us to generate and use a pairing-friendly elliptic curve suitable for this task. Our experiments show that DIZK enables such applications to scale to much larger instance sizes than what is possible via previous (monolithic) systems.An application uses DIZK by constructing a circuit for the desired computation, and by computing values for the circuit's wires from the application inputs. We cover necessary background on zkSNARKs by providing a high-level description ( §2.1), an informal definition ( §2.2), and the protocol that we start from ( §2.3). Both keys are published as public parameters and pk F /vk F can be used to prove/verify any number of statements about F. Thus, T F is seen to be significantly larger than k.The key efficiency feature of a zkSNARK is that the verifier running time is proportional to k alone (regardless of T F ) and the proof has constant size (regardless of k, T F ). Values are in a field F of a large prime order p.An R1CS instance φ over F is parameterized by the number of inputs k, number of variables N (with k ≤ N), and number of constraints M; φ is a tuple (k, N, M, a, b, c) where a, b, c are (1 + N) × M matrices over F.An input for φ is a vector x in F k , and a witness for φ is a vector w in F N−k . On input a proving key pk (for an R1CS instance φ ), input x in F k , and witness w in F N−k , P outputs a proof π that attests to the x-satisfiability of φ . The encoding of a scalar can be efficiently computed via the double-and-add algorithm; yet (for suitable choices of G) its inverse is conjecturally hard to compute, which means that [s] hides (some) information about s. Encodings are also linearly homomorphic:[αs + βt] = α[s] + β [t] for all α, β , s,t ∈ F.Bilinear encodings involve three groups of order p:G 1 , G 2 , G 3 generated by G 1 , G 2 , G 3 respectively. Moreover,there is an efficiently computable map e :G 1 × G 2 → G 3 , called pairing, that is bilinear: for every nonzero α, β ∈ F, it holds that e ([α] 1 , [β ] 2 ) = αβ · e (G 1 , G 2 ). For example, given [s] 1 , [t] 2 , [u] 1 , one can test if st + u = 0 by testing if e ([s] 1 , [t] 2 ) + e ([u] 1 , [1] 2 ) = [0] 3 .3 Design overview of DIZK Fig. 2 shows the outline of DIZK's design. 27th USENIX Security Symposium 679 The reduction from an R1CS instance φ = (k, N, M, a, b, c) to a QAP instance Φ = (k, N, M, A, B, C, D) (in the setup) and its witness reduction (in the prover) involves arithmetic on Θ(N) polynomials of degree Θ(M); see §2.3. In light of this, our approach is the following: (i) we achieve distributed fast implementations of evaluation and interpolation, and (ii) use these to achieve distributed fast polynomial arithmetic such as multiplication and division.Recall that (multi-point) polynomial evaluation is as follows: given a polynomial P(X) = ∑ n−1 j=0 c j X j over F and elements u 1 , . . . , u n in F, compute the elements P(u 1 ), . . . , P(u n ). The structure of this algorithm can be viewed as a butterfly network since each pass requires shuffling the array according to certain memory patterns.While the Cooley-Tukey algorithm implies a fast parallel algorithm, its communication structure is not suitable for compute clusters. At each layer of the butterfly network, half of the executors are left idle and the other half have their memory consumption doubled; moreover, each such layer requires a shuffle involving the entire array.We take a different approach, suggested by Sze [65], who studies the problem of computing the product of terabit-size integers on compute clusters, via MapReduce.Sze's approach requires only a single shuffle. In addition to the expensive finite field arithmetic discussed above, the setup and prover also perform expensive group arithmetic, which we must efficiently distribute.After obtaining the evaluations of Θ(N + M) polynomials, the setup encodes these values in the groups G 1 and G 2 , performing the operations s → [s] 1 and s → [s] 2 for Θ(N + M) values of s. Given a vector of scalars s in F n and a vector of elements(P i ) n i=1 in G n , compute ∑ n i=1 s i · P i in G.For small n, both problems have simple solutions: for fixMSM, compute each element s i · P and output it; for varMSM, compute each s i · P i and output their sum.In our setting, these solutions are expensive not only because n is huge, but also because the scalars are (essentially) random in F, whose cryptographically-large prime size p has k ≈ 256 bits. Given scalars s 1 , . . . , s n and their bases P 1 , · · · , P n , Pippenger's algorithm chooses a radix 2 c , computes s 1 /2 c P 1 + · · · + s n /2 c P n , doubles it c times, and sums it to (s 1 mod 2 c )P 1 + · · · + (s n mod 2 c )P n . For the last step, the algorithm sorts the base elements into 2 c buckets according to (s 1 mod 2 c ), . . . , (s n mod 2 c ) (discarding bucket 0), sums the base elements in the remaining buckets to obtain intermediate sums Q 1 , . . . , Q 2 c −1 , and computes Q 1 +2Q 2 +· · ·+(2 c −1)Q 2 c −1 = (s 1 mod 2 c )P 1 + · · · + (s n mod 2 c )P n . As a simple example, via log |F| group operations, one can compute the table (P, 2 · P, 4 · P, . . . , 2 log |F| · P), and then compute each s i · P with only log |F|/2 group operations (on average). When running this computation, we encounter notable issues at every step: the set of joined pairs (a i, j , L j (t)) is unevenly distributed among executors, the executors take drastically differing amounts of time to perform the pair evaluations, and a small set of executors quickly exceed memory bounds from insufficient heap space.Our problems lie in that, while the matrix a is sparse, its columns are merely almost sparse: most columns are sparse, but a few are dense. Thus even dense columns will have at most values to aggregate, avoiding stragglers.DIZK identifies which columns have more than a threshold of non-zero elements and annotates them for Part 2. For all dense column indices j ∈ J a :(a) filter a by index j to obtain column a j as an RDD; (b) join the RDD (a i, j ) i, j with L j (t) for j;(c) map each pair (a i, j , L j (t)) to its product a i, j L j (t). Union (a i, j L j (t)) j∈J a with (a i, j L j (t)) j / ∈J a . Since the evaluation of A i on D equals the i-th row of a, the task that needs to be solved in a distributed way is the following.in: a ∈ F (1+N)×M and z ∈ F 1+N out: (∑ N i=0 a i, j z i ) M j=1Again, the parameters N and M are huge, so no single machine can store an array with N or M field elements.Strawman. The dense vectors depend on the constraints alone so they do not change during proving, even for different inputs x. Hence, Part 1 runs once during setup, and not again during proving (only Part 2 runs then). More precisely, the camera actually signs a commitment to the input image, and this commitment and signature also accompany the edited image, and thus can be verified separately.We benchmark our system on this application because the original PhotoProof relies on monolithic zkSNARK implementations and is thus limited to small photo sizes. The crop transformation is specified by a r × c mask and maps an input r × c image into an output r × c image by keeping or zeroing out each pixel according to the corresponding bit in the mask. Some pixels go outside the image and are thus lost, while new pixels appear and are set to zero.We follow the approach of [53], and use the method of rotation by shears [54], which uses the identity cos θ − sin θsin θ cos θ = 1 − tan(θ /2) 0 1 1 0 sin θ 1 1 − tan(θ /2) 0 1 . The blur transformation is specified by a position (x, y), height u, and width v; it maps an input r × c image into an output r × c image in which Gaussian blur has been applied to the u × v rectangle whose bottomleft corner is at (x, y). To realize this transformation as constraints, we need to verify, for each of the uv positions in the selected region and for each of the 6 directional blurs, that the new pixel is the correct (rounded) average of the 2r + 1 pixels in the old image. Computing covariance matrices is an important subroutine in classification algorithms such as Gaussian naive Bayes and linear discriminant analysis [18]. Its covariance matrix is M := 1 n−1 ∑ n i=1 (x i − ¯ x)(x i − ¯ x) T ∈ R d×d , where ¯ x := ( 1 n ∑ n i=1 x i ) ∈ R d×1is the average of the n observations.To verify M, we first check the correctness of ¯ x by individually checking each of the d entries; for each entry we use the same approach as in the case of blur (in §7.1). For example, we represent an R1CS instance φ = (k, N, M, a, b, c) via three RDDs, one for each of the three matrices a, b, c, and each record in an RDD is a tuple ( j, (i, v)) where v is the (i, j)-th entry of the matrix. For singlemachine experiments, we used one r3.large instance.For distributed experiments, we used a cluster of ten r3.8xlarge instances for up to 128 executors, and a cluster of twenty r3.8xlarge for 256 executors.We instantiate the zkSNARK via a 256-bit BarretoNaehrig curve [8], a standard choice in prior zkSNARK implementations. Fixing a number of executors on the cluster and letting the instance size increase (from several millions to over a billion), the running time of the setup and prover increases close to linearly as expected, demonstrating scalability over this range of instance sizes. We ran experiments (32 and 64 executors for all feasible instances) comparing the performance of the setup and prover with two implementations: (1) the implementation that is part of DIZK, which has optimizations described in the design sections ( §4, §5, §6); and (2) an implementation that does not employ these optimizations (e.g., uses skewjoin instead of our solution, and so on). Our data established that our techniques allow achieving instance sizes that are 10 times larger, at a cost that is 2-4 times faster in the setup and prover. These proof systems enable a weak verifier (e.g., a mobile device) to outsource an expensive computation to a powerful prover (e.g., a cloud provider). The zkSNARK setup and prover in prior implementations run on a single machine.Some recent work explores zero knowledge proofs based not on probabilistic checking techniques and do not offer constant-size proofs, but whose provers are cheaper (and need no setup). Currently-deployed zkSNARKs have relied on Secure Multi-party Computation "ceremonies" for this [12,21], and it remains to be studied if those techniques can be distributed by building on our work.Our outlook is optimistic as the area of efficient proof systems sees tremendous progress [76], not only in terms of real-world deployment [7] but also for zkSNARK constructions that, while still somewhat expensive, rely only on public randomness (no setup is needed) [13,9]. While prior systems only support circuits of up to 10-20 million gates (at a cost of 1 ms per gate in the prover), DIZK leverages the combined CPU and memory resources in a cluster to support circuits of up to billions of gates (at a cost of 10 µs per gate in the prover).