Diversity is important even within countries, because political dynamics can vary internally, and because different ISPs may implement filtering policies differently.Unfortunately, most mechanisms for measuring Internet censorship currently rely on volunteers who run measurement software deployed on their own Internetconnected devices (e.g., laptops, phones, tablets) [43,49]. Performing measurements of the scale and frequency necessary to understand the scope and evolution of Internet censorship calls for fundamentally new techniques that do not require human involvement or intervention.We aim to develop techniques that can perform widespread, longitudinal measurements of global Internet manipulation without requiring the participation of individual users in the countries of interest. To this end, our design takes steps to ensure that, to the extent possible, we only query open DNS resolvers hosted in Internet infrastructure (e.g., within Internet service providers or cloud hosting providers), in an attempt to eliminate any use of resolvers or forwarders in the home networks of individual users. Studies have also explored the employment of various censorship methods, e.g., injection of fake DNS replies [5,36], blocking of TCP/IP connections [54], and applicationlevel blocking [19,33,41]. Most of these were apparently intended to prevent citizens from reaching social media to spread unwanted information.Other studies have demonstrated that government censorship covers a broad variety of services and topics, including video portals (e.g.,youtube.com) [51], blogs (e.g., livejournal.com) [3], and news sites (e.g., bbc.com) [9]. For instance, CensMon [48] used PlanetLab nodes in different countries, and UBICA [1] aimed to increase vantage points by running censorship measurement software on home gateway devices and user desktops. A similar scan by anonymous authors [4] in 2012 showed evidence of Chinese DNS censorship affecting non-Chinese systems.Follow-on work in 2015 by Kührer et al. tackled a much larger scope: billions of lookups for 155 domain names by millions of open resolvers [34]. The study examined a broad range of potentially tampered results, which in addition to censorship included malware, phishing, domain parking, ad injection, captive portals, search redirection, and email delivery. They then identified legitimate unmanipulated answers using a number of heuristic filtering stages, such as treating a differing response as legitimate if its returned IP address lies within the same AS the ground truth IP address.We tried to use their method for conducting global measurements specifically for detecting censorship. In contrast, we frame an explicit, reproducible method for globally measuring DNS-based manipulation in an ethically responsible manner.In In this section we describe Iris, a scalable, lightweight system to detect DNS manipulation. We then describe, given a set of measurement vantage points and DNS domain names, how we characterize the results of our measurements and use them to draw conclusions about whether DNS manipulation is taking place, based on either the consistency or the independent verifiability of the responses that we receive. We aim to identify DNS manipulation, which we define as the instance of a DNS response both (1) having attributes (e.g., IP addresses, autonomous systems, web content) that are not consistent with respect to a welldefined control set; and (2) returning information that is demonstrably incorrect when compared against independent information sources (e.g., TLS certificates). Detecting DNS manipulation is conceptually simple: At a high-level, the idea entails performing DNS queries through geographically distributed DNS resolvers and analyzing the responses for activity that suggests that the responses for a DNS domain might be manipulated. Despite its apparent simplicity, however, realizing a system to scalably collect DNS data and analyze it for manipulation poses both ethical and technical challenges. §3.3 describes how Iris selects a "safe" set of open DNS resolvers; The technical challenges center around developing sound methods for detecting manipulation, which we describe in §3.4 and §3.5. With these concerns in mind, we consider the ethics of performing measurements with Iris, using the ethical guidelines of the Belmont Report [10] and Menlo Report [20] to frame our discussion.One important ethical principle is respect for persons; essentially, this principle states that an experiment should respect the rights of humans as autonomous decisionmakers. In the case of Iris, obtaining the consent of all open DNS resolver operators is impractical.In lieu of attempting to obtain informed consent, we turn to the principle of beneficence, which weighs the benefits of conducting an experiment against the risks associated with the experiment. To obtain a wide range of measurement vantage points, we use open DNS resolvers deployed around the world; such resolvers will resolve queries for any client.Measurement using open DNS resolvers is an ethically complex issue. Despite efforts to reduce both the prevalence of open resolvers and their potential impact [40], they remain commonplace.Due to these and the ethics considerations that we discussed in §3.2, we restrict the set of open resolvers that we use to the few thousand resolvers that we are reasonably certain are part of the Internet infrastructure (e.g., belonging to Internet service providers, online cloud hosting providers), as opposed to attributable to any single individual. Conceptually, the process comprises two steps: (1) scanning the Internet for open DNS resolvers; or (2) pruning the list of open DNS resolvers that we identify to limit the resolvers to a set that we can reasonably attribute to Internet infrastructure.By using DNS resolvers we do not control, we cannot differentiate between country-wide or state-mandated censorship and localized manipulation (e.g., captive portals, malware [34]) at individual resolvers. At a high level, Iris resolves each DNS domain using the global vantage points afforded by the open DNS resolvers, annotates the response IP addresses with information from both outside datasets as well as additional active probing, and uses consistency and independent verifiability metrics to identify manipulated responses. In addition to the DNS domains that we are interested in testing, we include 3 DNS domains that are under our control to help us compute our consistency metrics when identifying manipulation. Querying tens of thousands of domains across tens of thousands of resolvers required the development of a new DNS query tool, because no existing DNS measurement tool supports this scale. In practice, this rate tends to be much lower due to network latency in both reaching the resolver, as well as the time it takes the resolver to perform the recursive response. To cope with specific resolvers that are unstable or timeout frequently, the tool provides a configurable failure threshold that halts a specific resolver's set of measurements should too many queries fail.To ensure the domains we query are not overloaded, the tool randomizes the order of domains and limits the number of resolvers queried in parallel such that in the worst case no domain experiences more than 1 query per second, in expectation.Step 2: Annotating DNS responses with auxiliary information. Iris annotates each IP address returned in the set of DNS responses with additional information about each IP address's geolocation, autonomous system (AS), port 80 HTTP responses, and port 443 HTTPS X.509 certificates. As a result, when Censys retrieves certificates via port 443 (HTTPS) across the entire IPv4 address space, the certificate that Censys retrieves might differ from the certificate that the server would return in response to a query via TLS's Server Name Indication (SNI) extension. When a request returns multiple records, we check all records and consider the reply good if any response passes the appropriate tests.Additionally, unmanipulated passive DNS [6] data collected simultaneously with our experiments across a geographically diverse set of countries could enhance (or replace) our consistency metrics. To attempt to account for these discrepancies, we also check whether different IP addresses for a domain map to the same AS we see when issuing queries for the domain name through our control resolvers. We label a response as correct if the hash of the HTTPS certificate presented upon connection matches that of an IP returned via our controls. We also use 4 geographically diverse resolvers for controlled experiments; the 2 Google Public DNS servers [28], a German open resolver hosted on Amazon AWS, and a resolver that we manage at the University of California, Berkeley. We removed another 12 domains and their 72K corresponding query responses as their DNS resolutions failed an automated sanity check; resolvers across numerous countries provided the same incorrect DNS resolution for each of these domains, and the IP address returned was unique per domain (i.e., not a block page or filtering appliance). Timeouts denote connections where the resolver did not respond to our query within 15 seconds.Server failures indicate when a resolver could not recursively resolve a domain within its own pre-configured time allotment (10 seconds by default in BIND). The closer the center mass of a histogram lies to 1.0, the more effective its corresponding metric, since a larger fraction of responses are classified as correct (i.e., not manipulation) using that metric. "Same HTTP Page" is also relatively effective, as many geographically distributed deployments of the same site (such as with Points-ofPresence) have either identical content or infrastructure error characteristics (see §3.5.1). Table 8 shows the extent to which countries return private IP addresses in responses, for the top 10 countries ranked by the relative amount of DNS manipulation compared to the total number of results from that country. Iran (122) 6.02% 0.01% China (62) 4.52% 99.46% Indonesia (80) 2.74% 95.08% Iraq (7) 1.68% 1.49% New Zealand (16) 1.59% 100.00% Turkey (192) 0.84% 99.81% Romania (45) 0.77% 100.00% Kuwait (10) 0.61% 0.00% Greece (26) 0.41% 100.00% Cyprus (5) 0.40% 100.00% Heterogeneity across a country may suggest a situation where different ISPs implement filtering with different block lists; it might also indicate variability across geographic region within a country. Although no single domain experiences manipulation in more than 19 countries, several categories experience manipulation in more than 30 countries, indicating that while broad categories appear to be commonly targeted, the specific domains may vary country to country.To study how manipulated categories vary across countries, we analyzed the fraction of resolvers within each country that manipulate a particular category. Ta- ble 11 shows the most frequently manipulated categories for the top 10 countries by normalized amounts of manipulation. k Domain NameCategory # Cn # Res We also note that commonly measured sites such as The Tor Project, Google, and Twitter, experience manipulation across significantly fewer countries than some sites. Such disparity points to the need for a diverse domain dataset.China focuses its DNS manipulation not just on adult content but also major English news outlets, such as nytimes.com, online.wsj.com, and www.reuters.