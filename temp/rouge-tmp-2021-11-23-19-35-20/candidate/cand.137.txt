Third, what actions could software vendors (and the vulnerability reporters) take to systematically improve the efficiency of reproduction?Assessing Reproducibility.The biggest challenge is that reproducing a vulnerability requires almost exclusively manual efforts, and requires the "reproducer" to have highly specialized knowledge and skill sets. We carefully design a workflow so that the reproduction results reflect the value of the information in the reports, rather than the analysts' personal hacking skills.Our experiments demanded 3600 man-hours to finish, covering a dataset of 368 memory error vulnerabilities (291 CVE cases and 77 non-CVE cases) randomly sampled from those reported in the last 17 years. • Third, we conduct a user study with real-world security researchers from 10 different institutions to validate our findings, and provide suggestions on how to improve the vulnerability reproduction efficiency. Each CVE ID has a web page 2 Dataset release: https://github.com/VulnReproduction/ LinuxFlaw with a short description about the vulnerability and a list of external references. /; make; make install Table 3: Default install commands.production stage, he triggers and verifies the vulnerability by using the PoC provided in the vulnerability report.In our experiment, we restrict security analysts to follow this procedure, and use only the instructions and references tied to vulnerability reports. We prioritize compiling using the source code of the vulnerable program.If the compilation and configuration parameters are not provided, we install the package based on the default building systems specified in software package (see Table 3). The overall rate is calculated using the total number of CVE entries (291) and non-CVE entries (77) as the base respectively.The top 5 referenced websites in our dataset are: SecurityFocus, Redhat Bugzilla, ExploitDB, OpenWall, and SecurityTracker. Our case is more challenging due to the prevalence of special technical terms, symbols, and even code snippets mixed in unstructured English text.Finally, for the 77 vulnerabilities without CVE ID, the success rate is 25.6%, which is lower compared to that of all the CVE cases (combined-all). In addition, combining different information sources helps retrieve missing pieces, particularly PoC files, trigger methods, and OS information.In Table 5, we combine all the CVE and non-CVE entries and divide them into two groups: succeeded cases (202) and failed cases (166). We employ a variety of ad-hoc techniques as demanded by each case, including debugging the software and PoC files, inspecting and modifying the source code, testing the cases in multiple operating systems and versions, and searching related hints on the web. If the PoC is still missing, an analyst would have no other choices but to attempt to re-create the PoC, which requires time and in-depth knowledge of the vulnerable software.In addition, we observe that many PoC files are erroneous. The questions cover (Q4) the profession of the participant, (Q5) years of experience in software security, (Q6) first-hand experience using vulnerability reports to reproduce a vulnerability, and (Q7) their familiarity with different types of vulnerabilities. To reduce bias, we reached out to a number of independent teams from both academia and industry.In total, we received responses from 48 security professionals at 10 different institutions, including 6 academic research groups, 2 CTF teams, 2 industry research labs. If the respondent's comments do not fit in any existing categories, we list the comment at the bottom of the table.The respondents stated that the PoC files and the Trigger Method are the most necessary information, and yet those are also more likely to be missing in the original report. Overall, the survey results provide external validations to our empirical measurement results, and confirm the validity of our information recovery methods. Based on Fig- ure 3, we discuss the parts that can be potentially automated to improve the efficiency of the reproducers.First, for the report gathering step, we can potentially build automated tools to search, collect, and fuse all the available information online to generate a "reproducible" report. Most of the software we studied employ public discussion forums and mailing lists (e.g., Bugzilla) where there are back-and-forth communications between the reporters and software developers throughout the vulnerability reproduction and patching process. Most articles are completely unstructured (e.g., technical blogs), and it takes extensive manual efforts to extract the useful information. Without this information, the security analysts (reproducers) may be misled to spend a long time debugging the PoC files and trigger methods before trying the special software configuration options. In total, we collect two datasets including a primary dataset of vulnerabilities with CVE IDs, and a complementary dataset for vulnerabilities that do not yet have a CVE ID ( Table 1). Next, we describe our measurement results with a focus on the time spent on the vulnerability reproduction, the reproduction success rate, and the key contributing factors to the reproduction success. Based on our experience, the most time-consuming part is to set up the environment and compile the vulnerable software with the correct options. To validate our measurement results, we conduct a survey to examine people's perceptions towards the vulnerability reports and their usability. In addition to helping the software developers and vendors to patch the vulnerabilities, the reports can also help researchers to develop and evaluate new techniques for vulnerability detection and patching. We show that these 5 websites are among the top 10 mostly cited websites, covering 71,358 (75.0%) CVE IDs.Given a CVE entry, we follow the aforementioned workflow, and conduct 3 experiments using different information sources:• CVE Single-source. All the key steps of reproduction (e.g., reading the technical reports, installing the vulnerable software, and triggering and analyzing the crash) are different for each case, and thus cannot be automated. Given a failed case, the key question is which piece of information is problematic.Instead of picking a random information category for in-depth troubleshooting, it is helpful to prioritize certain information categories. We have compared the answers from more experienced respondents (working experience > 5 years) and those from less experienced respondents. These errors can be something small such as a syntax error (e.g., CVE-2004-2167) or a character encoding problem that affects the integrity of the PoC (e.g., CVE-2004-1293). It is often the case, however, that the PoC is not available and the reporter only describes the vulnerability, leaving the task of crafting PoCs to the community.There are other websites that often act as "external references" for the CVE pages. Most datasets are limited to less than 10 vulnerabilities [39,29,40,46,25], or at the scale of tens [55,56,27,42], due to the significant manual efforts needed to build ground truth data.We have a number of key observations. In a recent work, Li and Paxson conducted a large scale empirical study of security patches, finding that security patches have a lower footprint in code bases than non-security bug fixes [43]. Based on our experience, such vulnerabilities often require specific triggering conditions that are different from the default settings.Project Size.Counter-intuitively, vulnerabilities of simpler software (or smaller project) are not necessarily easier to reproduce. Without a PoC, the vulnerability reproduction will be regarded as a failed attempt because it is extremely difficult to infer the PoC based on the vulnerability description alone. Finally, we combine all the information contained: in the original CVE entry, in the direct references, and in the references contained within the direct references (291 experiments). They found that reporters typically do not include these information in bug reports simply due to the lack of automated tools. Based on the aggregated information and common sense knowledge, only 54.9% of the reported vulnerabilities can be reproduced.Recovering the missed information is even more challenging given the limited feedback on "why a system did not crash". The Invalid Free category includes both "Use After Free" and "Double Free". More specifically, out of the 74 cases that failed on the trigger method (Table 6), we recovered 68 cases by reading other similar vulnerability reports (16 for the same software, 52 for similar vulnerability types). In addition, during our experiments, we noticed that certain reports had made vague and seemingly unverified claims, some of which were even misleading and caused significant delays to the reproduction progress. The CVE list is supplied to the National Vulnerability Database (NVD) [14] where analysts can perform further investigations and add additional information to help the distribution and reproduction. For example, if the reporter has crafted the PoC, the website may require the reporter to fill in trigger method and the compilation options in the report. If not explicitly stated, the default OS will be a Linux system that was released in (or slightly before) the year when the vulnerability was reported. The vulnerability reproduction, as a critical early step for risk mitigation, has not been well understood.In this paper, we bridge the gap by conducting the first in-depth empirical analysis on the reproducibility of crowd-reported vulnerabilities. The analyst will need to exhaustively test possible combinations manually in a huge searching space. At the vulnerability re- Run the commands with the default shell Script program (e.g., python)Run the script with the appropriate interpreter C/C++ code Compile code with default options and run it A long string Directly input the string to the vulnerable program A malformed file (e.g., jpeg)Input the file to the vulnerable program Table 2: Default trigger method for proof-of-concept (PoC) files.Building System Default Commands automake make; make install autoconf & . However, if the PoC, trigger method, and verification method are all well-defined, it is possible for the reproducer to automate the verification process. To this end, Q3 first provides a randomized list of "techniques" that we have used and an open text box for the participants to add other techniques.We ask another 4 questions to assess the participants' background and qualification. In the following, we summarize the key existing works and highlight the uniqueness of our work.In the field of software engineering, past research explored bug fixes in general (beyond just security-related bugs). In addition, out of the 38 cases that failed on the PoC files, we recovered/fixed the PoCs for 31 cases by reading the example code from other vulnerability reports. More than 94% of succeeded cases missed the software installation and configuration instructions; 22.3% of the succeeded cases missed the information on the verification methods, and 17.3% missed the operating system information. In practice, however, the reporters may assume that the reports will be read by security professionals or software engineers, and thus certain "common sense" information can be omitted. On average, each vulnerability report for CVE cases takes about 5 hours for all the proposed tests, and each vulnerability report for non-CVE cases takes about 3 hours. For example, our findings suggest that aggregating the information across different source websites is extremely helpful when recovering missing information in individual reports. Each vulnerability report contains structured information fields (in HTML and JSON), detailed instructions on how to reproduce the vulnerability, and fully-tested PoC exploits. We first obtain a random sample of 300 CVE entries [5] on memory error vulnerabilities in Linux software (2001 to 2017). The only studies that can scale well are those which focus on the high-level information in the CVE entries without the need to perform any code analysis or vulnerability verifications [43]. This can serve as a much needed large-scale evaluation dataset for researchers.In summary, our contributions are four-fold:• First, we perform the first in-depth analysis on the reproducibility of crowd-reported security vulnerabilities. For example, prior works have used reported vulnerabilities to benchmark their vulnerability detection and patching tools. For instance, for the coreutils CVE-2013-0221/0222/0223, we found that the vulnerabilities only existed in a specific patch by SUSE: coreutils-i18n. We find that Stack Overflow vulnerabilities are most difficult to reproduce with a reproduction rate of 40% or lower. First, individual vulnerability reports from popular security forums have an extremely low success rate of reproduction (4.5% -43.8%) caused by missing information. As shown in Figure 7, the reproduction success rate shows a general upward trend (except for [2013][2014][2015], which confirms our intuition. We find that relying on a single vulnerability report from a popular security forum is generally difficult to succeed due to the incomplete information. We organize a focused group of highly experienced security researchers and conduct a series of controlled experiments to 