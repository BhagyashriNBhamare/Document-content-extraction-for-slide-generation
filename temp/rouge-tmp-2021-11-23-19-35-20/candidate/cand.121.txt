Without fine-grained taint-tracking, the analysis above, as well as our experiments, indicate that this strategy of "alternating tainting with logging" leads to substantial increases in log size.LogGC [18] develops a "garbage collection" strategy, which identifies and removes operations that have no persistent effect. Moreover, due to the use of timestamped edges, reachability changes over time, and hence the results cannot be computed once and cached for subsequent use.To overcome these computational challenges posed by timestamped graphs, we show in Section 4 how to transform them into standard graphs. Fig. 2 illustrates a few common cases where we achieve substantial reductions by combining many similar operations: -multiple reads from the same network connection (a.com, b.com) interleaved with multiple writes to files (C and L), -series of writes to and reads from pipes (E), and -series of writes to log files by multiple processes (L). Of these, a.com is a source node, i.e., an object with no parent nodes, and hence identified as the likely entry point of any attack on C.Although b.com is backward reachable from C in the standard graph-theoretic sense, it is excluded because the path from b.com to C does not always go forward in time.The set of entities impacted by the attack can be found using forward analysis [43,1,15] (a.k.a. impact analysis), typically starting from an entry point identified by backward analysis. In forensics, we are interested in reachability of a node at a given time, so we extend the above definition as follows:Definition 2 (Forward/Backward Reachability at t) • A node v is forward reachable from a node u at time t, denoted u@t −→ v, iff there is a causal path e 1 ,e 2 ,...,e n from u to v such that t ≤ end(e i ) for all i.• A node u is said to be backward reachable from v at time t, denoted u −→ v@t, iff there is a causal path e 1 ,e 2 ,...,e n from u to v such that t ≥ start(e i ) for all i.1 There can be many types of read or write events, some used on files, others used on network sockets, and so on. In Fig. 1, P@6 −→ Q, but P@11 −→ Q. Similarly, a.com −→ C@3 but b.com −→ C@3.Based on reachability, we present three dependencypreserving reductions: CD, which is close to Xu et al's full trackability, and FD and SD, two new reductions we introduce in this paper. This reduction aims to preserve forward and backward reachability at every instant of time.Definition 3 (Continuous Dependence Preservation) Let G be a dependence graph and G be a reduction of G. G is said to preserve continuous dependence iff forward and backward reachability is identical in both graphs for every pair of nodes at all times.In Fig. 3, S reads from a file F at t = 2 and t = 4, and writes to another file F at t = 3 and t = 6. Specifically, their definition ensures that reduction decisions can be made locally, e.g., by examining the edges incident on S. Thus, their criteria does not permit the combination of reads in either Fig. 3 or Fig. 4, since they share the same local structure at node S. • forward reachability from u@t to v is preserved for all t ∈ NewAnc(u), and• backward reachability of u from v@t is preserved at all t.In other words, when FD-preserving reductions are applied:• the result of backward forensic analysis from any node v will identify the exact same set of nodes before and after the reduction. To carry out this task accurately, we need to preserve only information flows from source nodes; preserving dependencies between all pairs of internal nodes is unnecessary.Definition 5 (Source Dependence (SD) Preservation) A reduction G of G is said to preserve source dependence iff for every node v and a source node u:• forward reachability from u@0 to v is preserved, and• backward reachability of u from v@t is preserved at all t.Note that SD coincides with FD applied to source nodes. The first conditions coincide as well, when we take into account that NewAnc(u) = {0} for any source node u. (A source node does not have any ancestors, but since we have defined NewAnc to always include zero, NewAnc of source nodes is always {0}.) 5 is redundant, as it is implied by the second: If u is backward reachable from a node v at t, then, by definition of backward reachability, there exists a causal path from e 1 ,e 2 ,...,e n from u to v. This mutability also means that results cannot be computed once and cached for subsequent use, unlike standard graphs, where we can determine once that v is a descendant of u and reuse this result in the future.To overcome these computational challenges posed by timestamped graph, we show how to transform them into standard graphs. F S G T 2 3 5 4 6 5 S 0 F 0 S 2 G 0 G 3 G 5 T 0 T 4 T 6 We treat the contents of the audit log as a timestamped graph G = (V,E T ). (w 0 ,w 1 ,t 1 ),(w 1 ,w 2 ,t 2 ),...,(w n−1 ,w n ,t n ) in G such that t i−1 ≤ t i for 1 ≤ i ≤ n.Note that the "only if" proof constructed a one-to-one correspondence between the paths in G and G. S 0 F 0 S 2 G 0 G 3 G 5 T 0 T 4 T 6 S 0 F 0 S 2 G 0 G 3 T 0 T 4 F 0 S 0,2 G 0,3 T 0,4 Fig. 7:The naive versioned graph from Fig. 6 (top), and the result of applying redundant edge optimization (REO) (middle) and then redundant node optimization (RNO) (bottom) to it. If not, we replace v r,s with v r,t , instead of creating a new version of v. Fig. 7 illustrates the result of applying this optimization.RNO preserves dependence for descendants of v, but it can change backward reachability of the node v itself. If a node v subject to RNO gets a child x, this child would have been added after the end timestamp of v. So, when we do a backward traversal from x, all parents of v should in fact be backward reachable.would be to search for larger cycles when a spurt in version creation is observed. Proof: We already showed that BuildVer preserves forward and backward reachability between the timestamped graph G and the naive versioned graph G. Hence it suffices to show that the edges and nodes eliminated by REO* and RNO don't change forward and backward reachability in G. Now, REO* optimization drops an edge (u,v,t) only if there is already an edge from the latest version of u to the latest or a previous version of v in G. While this is the default definition, broader definitions of source can easily be used, if an analyst considers other nodes to be possible sources of compromise.We use a direct approach to construct a versioned graph that preserves SD. Only when they determine an edge to be new, we apply the SD check based on Src sets.Theorem 9 BuildVer, together with redundant edge and redundant node optimizations and the source dependence optimization, preserves source dependence.Proof: Since full dependence preservation implies source dependence preservation, it is clear that redundant edge and redundant node optimizations preserve source dependence, so we only need to consider the effects of source dependence optimization. Now, in the induction step, note that the algorithm will either add an edge (u,v) and update Src(v) to include all of Src(u), or, discard the event because Src(v) already contains all elements of Src(u). However, we did away with many other aspects of that implementation, such as the (over-)reliance on compact, variable length encoding for events, based on techniques drawn from data compression and encoding. The back-end uses our BuildVer algorithm, together with (a) the REO, RNO, and CCO optimizations (Section 4.2) to realize FD preservation, and (b) the source dependence preservation technique described in Section 4.3. We used this capability to carry out many of our experiments, because data in CSR format can be consumed much faster than data in Linux audit log format or the OS-neutral format in which red team engagement data was provided. Table 8 shows the total number of events in the data, along with a breakdown of important event types.Since reads and writes provide finer granularity information about dependencies than open/close, we omitted open/close from our analysis and do not include them in our figures.Windows Engagement Data (Windows Desktop). Such processes typically acquire a new dependency when they make a new network connection, but subsequent operations don't add new dependencies, and hence most of them can be reduced.Our implementation of SD is on top of FD: if an edge cannot be removed by FD, then the SD criterion is tried. Edge timestamps are dropped, but nodes may be annotated with a timestamp.P 1 P 4 Q C L Enode v depends on node u if there is a (directed) path from u to v with non-decreasing edge timestamps. Versions of a node are stacked vertically in the example so as to make it easier to see the correspondence between nodes in the timestamped and versioned graphs.Note that timestamps in versioned graphs are associated with nodes (versions), not with edges. return (V,E)We intend BuildVer and its optimized versions to be online algorithms, i.e., they need to examine edges one-at-a-time, and decide immediately whether to create a new version, or to add a new edge. These constraints are motivated by our application in real-time attack detection and forensic analysis.For each entity v, an initial version v 0 is added to the graph at line 2. After 10 minutes, it is considered a new source, thus allowing us to reason about remote sites whose behavior may change over time (e.g., the site may get compromised). This is impressive, considering that it was achieved without any application-specific optimizations.While trackability equivalence [42] provides a sufficient basis for eliminating events, we show that it is far too strict, limiting reductions in many common scenarios, e.g., communication via pipes. Despite being limited to reads, writes and loads, our reduction techniques are very effective in practice, as these events typically constitute over 95% of total events.For the first requirement, our aim is to preserve the results of forward and backward forensic analysis. In order to minimize the impact of such misses, we first apply REO, RNO and CCO optimizations, and skip the edges and/or versions skipped by these optimizations. Graph databases such as OrientDB, Neo4j and Titan are designed to provide efficient support for graph queries, but experience suggests that their performance degrades dramatically on graphs that are large relative to main memory. The downside of this power is efficiency, as continuous dependence may need to examine every path in the graph before deciding which edges can be removed.Although the checking of global properties can be more time-consuming, the resulting reductions can be more powerful (i.e., achieve greater reduction). In addition, we make the following observation that readily follows from the description of BuildVer.Observation 6 For any two node versions u t and u s , there is a path from u t to u s if and only if s ≥ t.Theorem 7 Let G = (V,E) be the versioned graph constructed from G = (V,E T ). Efficient algorithms for achieving FD and SD are described in Section 4, together with a treatment of correctness and optimality. This has motivated a number of research efforts on reducing log size.Since the vast majority of I/O operations are reads, ProTracer's [22] reduction strategy is to log only the writes. For instance, a performance evaluation study on graph databases [23] found that they are unable to complete simple tasks, such as finding shortest paths on graphs with 128M edges, even when running on a computer with 256GB main memory and sufficient disk storage. Reachability in this graph is defined as follows:Definition 1 (Causal Path and Reachability) A node v is reachable from another node u if and only if there is (directed) path e 1 ,e 2 ,...,e n from u to v such that:∀1 ≤ i < n start(e i ) ≤ end(e i+1 )(1)We refer to a path satisfying this condition as a causal path.It captures the intuition that information arriving at a node through event e i can possibly flow out through the event e i+1 , i.e., successive events on this path e 1 ,e 2 ,...,e n can be causally related. Following this table is a sequence of operations, each of which correspond to the definition of an object (e.g., a file, network connection, etc.) or a forensic-relevant operation such as open, read, write, chmod, fork, execve, etc. We also prove that SD preserves the results of the most commonly used forensic analysis, which consists of running first a backward analysis to find the attacker's entry points, and then a forward analysis from these entry points to identify the full impact of the attack. For all nodes u,v and times t:• v is forward reachable from u@t iff there is a simple path in G from u ≤t to v <∞ ; and • u is backward reachable from v@t iff there is a path in G from u 0 to v ≤t . Several teams were responsible for instrumenting OSes and collecting data, while our team (and others) performed attack detection and forensic analysis using this data. Then an edge is created from the latest version of u to this new node (line 5), and another edge created to link the last version of v to this new version (line 6). However, the backward reachability condition limits us to dropping the edges from S and S , as we would otherwise change backward reachability of the source node F from F @4. Translators can easily be developed to translate CSR to standard log formats, so that standard log analyzers, or simple tools such as grep, can be used.In CSR, all subjects and objects are referenced using a numeric index. Forensic analysis requires queries over the dependence graph, e.g., finding shortest path(s) to the entry node of an attack, or a depth-first search to identify impacted nodes. We then encode these dependencies into a standard graph in order to speed up our reduction algorithms.The key challenge in this context is to minimize the size of the standard graph without dropping any existing dependency, or introducing a spurious one. In most work on forensic analysis [13,15,42], the log contents are interpreted as a dependence graph: nodes in the graph correspond to entities, while edges correspond to events. Across an enterprise with thousands of hosts, total storage requirements can easily go up to the petabyte range in a year. Section 5 summarizes a compact main-memory dependence graph and offline event log formats based on our event reductions. As a result, u t would incorrectly be included in a backward analysis result starting at the descendants of v s . This violates Xu et al.'s condition for merging edges, and hence none of these edges can be merged. It is also clear that that when the edge is discarded by the SD algorithm, it is because the edge does not change the sources that are backward reachable, and hence it is safe to drop the edge.Optimality of SD algorithm. The second column shows the size of the original data, i.e., Linux audit data for laboratory servers, and OS-neutral intermediate format for red team engagement data. With existing systems, such as Linux auditing and Windows ETW, our experience as well as that of previous researchers [42] is that the volume of audit data is in the range of gigabytes per host per day. While providing this guarantee, our techniques reduce on-disk file sizes by an average of 35× across our data sets. In Section 6.3, we evaluate the effectiveness of FD and SD in reducing the number of events, and compare it with Xu et al.'s technique (LCD). We end this discussion with examples of common scenarios where LCD reduction is permitted:• Sequence of reads without intervening writes: When an application reads a file, its read operation results in multiple read system calls, each of which is typically logged as a separate event in the audit log. Since 0 is the smallest possible timestamp, 0 ≤ end(e i ) for all i, and hence, using the causal path e 1 ,e 2 ,...,e n and the first part of Defn. The spate of APTs in recent years has fueled research on efficient collection and forensic analysis of system logs [13,14,15,9,16,17,18,22,42,30,10]. REO and RNO optimizations avoid new versions in most common scenarios that lead to an explosion of versions with naive versioning:• Output files: Typically, these files are written by a single subject, and not read until the writes are completed. In this section, we describe how to use the techniques described so far, together with others, to achieve highly compact log file and main-memory dependence graph representations. For the example in Fig. 1, our technique combines all edges between the same pair of nodes, leading to the graph shown i