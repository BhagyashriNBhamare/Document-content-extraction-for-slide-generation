In doing so, we identify a new property, which we call uncompromised distance bounding, that captures the attacker model for protecting devices such as contactless payment cards or car entry systems, which assumes that the prover being tested has not been compromised, though other provers may have been. Thieves relay signals from a victim's key fob (located inside the victim's house) to the victim's car (parked outside), which enables the thieves to unlock the car, start the engine, and drive away.Distance bounding protocols [11] use round trip times to establish an upper-bound on the distance between a "prover", e.g., a contactless payment card or key fob, and a "verifier", e.g., a payment machine or car. Defending against this kind of attack is the strongest security property needed for protocols such as MasterCard's RRP to protect contactless payment cards or NXP's proximity check when being used to protect, e.g., access to buildings.We demonstrate the applicability of our results by analysing MasterCard's RRP protocol for distance bounding of contactless EMV [20], and a distance bounding protocol from NXP [14,25]. P replication new a.P restriction let x = D in P else Q term evaluation event(M 1 , . . . , M n )an event startTimer.P timer activation stopTimer.P timer terminationS ::= systems [{P 1 , . . . , P n }] r a location newãnew˜newã.S restriction [{P 1 , . . . , P n }] r | S locationscontrols the network, so processes are not able to ensure that a particular output goes to a particular input. new id" term at the front of the process generates an arbitrary number of new process ids.Cryptography is modelled using constructors and destructors, e.g., symmetric key encryption can be modelled using a binary constructor enc(m, k) to represent the message m encrypted with the key k and a binary destructor function dec with the rewrite rule dec(enc(m, k), k) = m. Functions can be public, i.e., available for use by the attacker, or private meaning that they cay only be used by processes specified as party of the protocol. Private functions are useful, for instance, to look up private keys which should only be known to protocol participants.Functions are applied using the let statement, e.g., "let pt = dec(ct, k) in P else Q" tries to decrypt cipher text ct with key k, and acts as P if decryption succeeds and Q otherwise. We abbreviate [{P 1 , . . . , P n }] r as [P 1 , . . . , P n ] r and [{P 1 , . . . , P n }] 0 as [P 1 , . . . , P n ]. As an example we consider a verifier that sends a challenge, denoted a, to a prover, to which the prover replies with a function f applied to this and some other value b: Figure 2 Operational semantics for our timer locations calculusP V P P a timed f (a, b) P V P PE, L ∪ { [P ∪ {! P P 0 } → * E, { stopTimer.P V 1 P P 0 } → E, { P V 0 P P 0 }Note that the message received by P V uses the name p rather than the challenge name a, hence, when using preemption there is no way in which the answer to the response to a timed challenge can be based on the message outputted as part of that challenge.Rule (ASYNC) defines asynchronous communication, which prevents processes from blocking when they are ready to output. Q | A, where Q outputs bound and free names of Q (including names iñ n, which are otherwise hidden from the attacker) and the results of any private function applications in Q, and A is the context hole.The process DP-A(id) reveals all the secret values of a normal prover to the attacker, which captures a dishonest prover attacker.Example 11. Intuitively, the nonce c2 is only sent when the timer is running, so the attacker can never return this in time if not co-located with the verifier.Terrorist provers are less powerful than dishonest provers, because they will not send their secret values to a third party. Q | A, where Q is the process that acts as an oracle with all relevant functions for all bound and free names and private function applications in Q, and A is the context hole.The process T P-A will perform operations on behalf of the attacker, e.g., signing, encrypting and decrypting any values the attacker wishes, but it will not reveal secret values.Example 12. out(enc(x, k))) | AThis process can receive the encrypted challenge from the verifier, decrypt it, and send the resulting plaintext to an attacker process co-located with the verifier, all before the timer is started. The second, more secure, protocol in Example 2 can be modelled in our calculus as (V 2, P2, ) where:P2(id) = ! These scenarios consider the following terms:• V | A, a verifier co-located with a basic attacker (relay fraud and terrorist fraud); • V , a verifier in isolation (distance fraud);• V | P(id ), a verifier co-located with honest provers (distance hijacking); • V | DP-A(id ), a verifier co-located with dishonest provers (assisted distance fraud);• P(id) | A, remote provers co-located with an attacker (relay fraud); • DP-A(id), remote dishonest provers in isolation (distance fraud and distance hijacking); and • T P-A(id), remote terrorist provers in isolation (terrorist fraud and assisted distance fraud). This gives us 2 4 · (2 3 − 1) · 2 4 = 1 792 scenarios to consider, significantly more than the five scenarios that have been identified in the literature.We can reduce the number of scenarios we need to consider by observing that there is a strict order on the capabilities of the different attacker processes: Lemma 1. Therefore, the strongest property that a distance bounding protocol can have is protection from both distance hijacking and assisted distance fraud.To separate many of the distance bounding properties we need to consider a verifier that will verify any process that sends it a secret key. L n , where L 1 , . . . , L n are linear processes.Linear processes allow us to express all distance bounding protocols from the literature, so they do not reduce the usefulness of our method.Using linear processes, we introduce a technique to simplify the detection of vulnerabilities and define a compiler that allows us to take advantage of that technique. V L | L v | A] | [L p | A], sets of names E and name id, such that V L , L v and L p are linear processes and only V L contains a timer, we have:verified(id):newñnew˜newñ. Thus, compilation encodes timers as phases.Encoding the activation and deactivation of timers as phases is straightforward, indeed, we merely replace startTimer.P with 1 : P and stopTimer.Q with 2 : Q. But, encoding the advancement of other processes at the same location as the timer from phase 0 to phase 1 is problematic, as is advancing processes at different locations from phase 0 to phase 2, because we cannot know when processes should advance. It suffices to consider advancements just before input operations, because processes ready to output can be reduced by an attacker that receives those outputs before an advancement and replays the messages received afterwards, and other processes do not produce communications, so it does not matter whether they happen before or after an advancement. P n where {P 1 , . . . , P n } = phasesSet(P , ds), P equals P with every in(x) replaced with in(c,x) and every out(M) replaced with out(c,M) and function phasesSet is defined as follows:phasesSet(P, [d]) = {C[d : in(M, x). P ] ∧ P ∈ phasesSet(P , d 2 :: ds)} ∪ phasesSet(P, d 2 :: ds)Using function phases, we define our compiler, first for systems with verifiers co-located with attackers and then for systems with remote attackers.Definition 9. new id.phases(P L , [2]) | phases(L p , [2]))where tToPh(L) is L after replacing startTimer.P with 1 : P and stopTimer.Q with 2 : Q and every in(x) replaced with in(c,x) and every out(M) replaced with out(c,M)Timers limit communication between locations. out(priv, x), which allows messages sent on private channel priv to be buffered, i.e., received and relayed.This final process ensures that any reduction by the (ASYNC) rule on private channel priv in our timer location calculus can be mapped to a reduction in the applied pi-calculus, which has no such rule (a similar processes isn't required for reductions by the (ASYNC) rule on public channel c, because the attacker process can simulate such reductions). P j } → E j ∪ {b j }, P i ∪ {P j {b j /a j }}The following theorem tells us that we can check the compiled system in the applied pi-calculus and concluded security results about the system with locations:Theorem 1. P L | L p | A], and a name id, we have not ev(verify(id)), {c} : compile(id, S) ⇒ ¬ verified({c}, id):S We have implemented the compiler introduced in the previous section. The EMV protocol comprises of an exchange of transaction data and then the card generates a MAC (called the Application Cryptogram or AC) using a session key based on a key shared between the smart card and the card issuer and the Application Transaction Counter (ATC), which equals the number of times the card has been used and will provide freshness to the transaction. We describe these in detail in the next section.In this paper, we extend the applied pi-calculus [2] to distinguish between co-located processes and processes at distinct locations, and we restrict communication between locations using timers. These protocols need not defend against at-tacks requiring dishonest provers, because if an attacker gets access to the secret keys, they can clone the cards or key fobs, and make payments or gain access without a need to relay the original device, i.e., protection is only needed for an uncompromised device.However, we expect some devices (e.g., EMV cards or car fobs) may be compromised at some point, and we would like to ensure that the compromise of a particular prover would not lead to an attacker being able to successfully attack other provers. We assume that the attacker Figure 1 The timer location calculus syntaxM, N ::= terms x, y, z variables a, b, c, k names f (M 1 , . . . , M n ) constructor application D ::= g(M 1 , . . . , M n ) destructor application P, Q ::= processes 0 nil out(N). We require that no further events are used in either process and the only free names (i.e., names not declared as new or bound by an input) used are those iñ n and the public channel c.Process Q models a single run of a prover with the identity id and P(id) represents arbitrarily many distinct provers, each of which can run arbitrarily many times. For any distance bounding protocol specification (P(id),V, ˜ n), from which we derive DP-A and T P-A, and for all system contexts C, sets of names E and names x ∈ {id, id }, we haveverified(id):C[A | P(x)] ⇒ verified(id):C[T P-A(x)] ⇒ verified(id):C[DP-A(x)]Moreover, no reverse implication holds.By filling a context's hole with a process containing a hole (as above), we derive a context (which is required by the verified predicate). However, this property is very strong; industrial distance bounding protocols such as MasterCard's RRP or NXP's proximity check do not have this property nor do they need it: If a bank card or key fob has been fully compromised, then an attacker may send all key information from this device to the same location as the verifier and so pass the verification.The lines which dissect Figure 3 each represent different possible attacker models, and each area is dominated by a single property, which, if checked, will prove security for that particular attacker model. out(resp)Events are used to annotate the protocol for automated checking. Mauw et al. [28] improves on the framework of [34] looking at causality between actions to make a framework for automatically testing distance fraud and terrorist fraud.None of the previous papers on symbolic checking of distance bounding protocols consider the full range of distance bounding properties or makes comparisons between them.A recent survey [3] gives many examples of distance bounding protocols and attacks. This attacker corresponds to, for instance, a relay attack against an EMV card, which uses another, different EMV card at the verifier's location. L 1 , where blind(L 1 ) is L 1 after removing timer actions (startTimer and stopTimer) and events, hence, it suffices to isolate timers and events to a single instance of L 1 . In summary, our ordering tells us that:• If the protocol is aiming to defend against terrorist fraud attackers, then it should be checked against assisted distance fraud. As with any protocol on top of ISO/IEC 14443, the session starts with the reader sending a SE-LECT command to the card and the card responding with its ID. P}] r } → E, L ∪ { [P ∪ {P}] r } (EVENT)We can write these roles as processes:P V = startTimer.out(a). The protocol informally described in Example 1 can be modelled as specification (ProverE(id), VerifierE, k), where ProverE(id) and VerifierE are as described above, and k is the global shared key.Since attackers can be present at a number of different locations, we introduce system contexts as systems with "holes," in which a process may be placed. For any distance bounding protocol specification (P(id),V, ˜ n), from which we derive DP-A and T P-A, and for any system contexts C[ ], sets of names E, names id and id , and X ∈ {P, DP-A, T P-A}, we have:verified(id):C[X(id ) | X(id)] ⇔ verified(id):C[X(id)]It follows from Lemma 2 that if process X(id) is present, then it is not necessary to consider the corresponding X(id ) process as well.When there is a dishonest prover at a different location to a basic attacker process, the dishonest prover could send all of its secrets to the basic attacker process enabling it to also act as a dishonest prover:Lemma 3. let (chal, resp) = dec(x, k) in out(ready) . out(resp, x), this process can receive the challenge information from the terrorist prover process, and then use it to complete the verifier's challenge. [P | DP-A(x)] | [A | Q]Our observations reduce the number of interesting, distinct, system contexts to 27, each of which models a different distance bounding attack scenario, and protection against which offers a distinct security property. To do this, we replace the public channel "c" with a private channel "priv" between phase 1 and 2 (i.e., whilst the timer is active), thereby denying the attacker access to the communication channel. These holes denote the locations in a system where the attacker can act, and we write them as A. E.g., the system contextC 1 = new k.[Veri f ierE | A ] | [ ProverE(id) | A ]represents a scenario in which the attacker can be co-located with the verifier V , and co-located with the prover Q, whereasC 2 = new k.[ Veri f ierE ] | [ ProverE(id) | A ]represents a scenario in which the attacker 