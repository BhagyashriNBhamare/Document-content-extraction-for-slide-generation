We focus on the text-totext setup, which we find to be more challenging and more natural.We begin with vanilla SEQ2SEQ models with attention ( Bahdanau et al., 2015) and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of Narayan et al. (2017) and approaching their best RDF-aware method. Digging further, we find that 99% of the simple sentences (more than 89% of the unique ones) in the validation and test sets also appear in the training set, which-coupled with the good memorization capabilities of SEQ2SEQ models and the relatively small number of distinct simple sentences-helps to explain the high BLEU score.To aid further research on the task, we propose a more challenging split of the data. We also establish a stronger baseline by extending the SEQ2SEQ approach with a copy mechanism, which was shown to be helpful in similar tasks ( Gu et al., 2016;Merity et al., 2017;See et al., 2017 In parallel to our work, an updated version of the dataset was released (v1.0), which is larger and features a train/test split protocol which is similar to our proposal. 2 We split each prediction to 1 https://github.com/biu-nlp/ sprp-acl2018 2 Note that this differs from "normal" multi-reference BLEU (as implemented in multi-bleu.pl) since the number of references differs among the instances in the test- Analysis We begin analyzing the results by manually inspecting the model's predictions on the validation set. Inspecting the attention weights (Figure 1) reveals a worrying trend: throughout the prediction, the model focuses heavily on the first word in of the first entity ("A wizard of Mars") while paying little attention to other cues like "hardcover", "Diane" and references of a specific complex sentence, and then average these numbers. Table 1 gives more detailed statistics on the WEBSPLIT dataset.To further illustrate the model's recognize-andspit strategy, we compose inputs containing an entity string which is duplicated three times, as shown in the bottom two rows of Table 3. Every RDF triplet (a complete fact) is represented only in one of the splits.While the set of complex sentences is still divided roughly to 80%/10%/10% as in the original split, now there are nearly no simple sentences in Table 4: Statistics for the RDF-based data split the development and test sets that appear verbatim in the train-set.