In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. Optimizing the parameters results in what we call the Optimized Unary Encoding (OUE) protocol, which has significantly better accuracy.Protocols based on unary encoding require Θ(d) com-munication cost, where d is the number of possible input values, and can be very large (or even unbounded) for some applications. We then present our framework for pure LDP protocols in Section 3, apply it to study LDP protocols in Section 4, and compare different LDP protocols in Section 5. The notion of differential privacy was originally introduced for the setting where there is a trusted data curator, who gathers data from individual users, processes the data in a way that satisfies DP, and then publishes the results. • Aggregate is executed by the aggregator; it takes all the reported values, and outputs aggregated information.Definition 2 (Local Differential Privacy) An algorithm A satisfies ε-local differential privacy (ε-LDP), where ε ≥ 0, if and only if for any input v 1 and v 2 , we have∀y ∈ Range(A) : Pr [A(v 1 ) = y] ≤ e ε Pr [A(v 2 ) = y] ,where Range(A) denotes the set of all possible outputs of the algorithm A.This notion is related to randomized response [24], which is a decades-old technique in social science to collect statistical information about embarrassing or illegal behavior. Perturb(B 0 ) consists of two steps:Step 1: Permanent randomized response: Generate B 1 such that:Pr [B 1 [i] = 1] = { 1 − 1 2 f , if B 0 [i] = 1, 1 2 f , if B 0 [i] = 0. Ignoring the Instantaneous randomized response step, to estimate the number of times i occurs, the aggregator computes:˜ c(i) = ∑ j 1 {i|B j [i]=1} (i) − 1 2 f n 1 − fThat is, the aggregator first counts how many time i is reported by computing ∑ j 1 {i|B j [i]=1} (i), which counts how many reported vectors have the i'th bit being 1, and then corrects for the effect of randomization. Encoding uses a set of m hash functionsH = {H 1 , H 2 , . . . , H m }, each of which outputs an integer in [k] = {0, 1, . . . , k − 1}. The effect is that each user with input value i contributes c tõ c(i) with probability p, and −c with probability q; thus the expected contribution is(p − q) · c = ( e ε e ε + 1 − 1 e ε + 1 ) · e ε + 1 e ε − 1 = 1. Definition 3 (Pure LDP Protocols) A protocol given by PE and Support is pure if and only if there exist two probability values p * > q * such that for all v 1 ,Pr [PE(v 1 ) ∈ {y | v 1 ∈ Support(y)}] = p * , ∀ v 2 ̸ =v 1 Pr [PE(v 2 ) ∈ {y | v 1 ∈ Support(y)}] = q * . Assuming the use of two hash functions, ifv 1 is mapped to [1, 1, 0, 0], v 2 is mapped to [1, 0, 1, 0], and v 3 is mapped to [0, 0, 1, 1], then because [1, 1, 0, 0] differs from [1, 0, 1, 0] by only two bits, and from [0, 0, 1, 1] by four bits, the probability that v 2 is mapped to v 1 's support set is higher than that of v 3 being mapped to v 1 's support set.For a pure protocol, let y j denote the submitted value by user j, a simple aggregation technique to estimate the number of times that i occurs is as follows:˜ c(i) = ∑ j 1 Support(y j ) (i) − nq * p * − q * (1)The intuition is that each output that supports i gives an count of 1 for i. However, this needs to be normalized, because even if every input is i, we only expect to see n · p * outputs that support i, and even if input i never occurs, we expect to see n · q * supports for it. Casting these protocols into the framework of pure protocols enables us to derive their variances and understand how each method's accuracy is affected by parameters such as domain size, ε, etc. We consider two aggregation techniques, SHE and THE.-Summation with Histogram Encoding (SHE) simply sums up the reported noisy histograms from all users.-Thresholding with Histogram Encoding (THE) is parameterized by a value θ ; it interprets each noisy count above a threshold θ as a 1, and each count below θ as a 0. Here two key parameters in perturbation are p, the probability that 1 remains 1 after perturbation, and q, the probability that 0 is perturbed into 1. Encode DE (v) = v, and Perturb is defined as follows.Pr [Perturb DE (x) = i] = { p = e ε e ε +d−1 , if i = x q = 1−p d−1 = 1 e ε +d−1 , if i ̸ = x Theorem 3 (Privacy of DE) The Direct Encoding (DE) Protocol satisfies ε-LDP. Then this protocol is pure, with p * = p and q * = q. Plugging these values into (4), we haveVar * [ ˜ c DE (i)] = n · d − 2 + e ε (e ε − 1) 2Note that the variance given above is linear in nd. Perturb(B) outputs B ′ as follows:Pr [ B ′ [i] = 1 ] = { p, if B[i] = 1 q, if B[i] = 0Theorem 6 (Privacy of UE) The Unary Encoding protocol satisfies ε-LDP forε = ln ( p(1 − q) (1 − p)q )(5)Proof 6 For any inputs v 1 , v 2 , and output B, we havePr [B|v 1 ] Pr [B|v 2 ] = ∏ i∈[d] Pr [B[i]|v 1 ] ∏ i∈[d] Pr [B[i]|v 2 ](6)≤ Pr [B[v 1 ] = 1|v 1 ] Pr [B[v 2 ] = 0|v 1 ] Pr [B[v 1 ] = 1|v 2 ] Pr [B[v 2 ] = 0|v 2 ](7)= p q · 1 − q 1 − p = e ε (6)is because each bit is flipped independently, and (7) is because v 1 and v 2 result in bit vectors that differ only in locations v 1 and v 2 , and a vector with position v 1 being 1 and position v 2 being 0 maximizes the ratio. (5) into (4), we haveVar * [ ˜ c UE (i)] = nq(1 − q) (p − q) 2 = nq(1 − q) ( e ε q 1−q+e ε q − q) 2 = n · ((e ε − 1)q + 1) 2 (e ε − 1) 2 (1 − q)q . Take the partial derivative of (8) with respect to q, and solving q to make the result 0, we get:∂ [ ((e ε −1)q+1) 2 (e ε −1) 2 (1−q)q ] ∂ q = ∂ [ 1 (e ε −1) 2 · ( (e ε −1) 2 q 1−q + 2(e ε −1) 1−q + 1 q(1−q) )] ∂ q = ∂ [ 1 (e ε −1) 2 · ( −(e ε − 1) 2 + e 2ε 1−q + 1 q )] ∂ q = 1 (e ε − 1) 2 ( e 2ε (1 − q) 2 − 1 q 2 ) = 0 =⇒ 1 − q q = e ε , i.e., q = 1 e ε + 1 and p = 1 2 Plugging p = 1 2 and q= 1 e ε +1 into (8), we get Var * [ ˜ c OUE (i)] = n 4e ε (e ε − 1) 2(9)The reason why setting p = 1 2 and q = 1 e ε +1 is optimal when the true frequencies are small may be unclear at first glance; however, there is an intuition behind it. We call this the local hashing approach.The random matrix-base protocol in [6] (described in Section 2.4), in its very essence, uses a local hashing encoding that maps an input value to a single bit, which is then transmitted using randomized response. Below is the Binary Local Hashing (BLH) protocol, which is logically equivalent to the one in Section 2.4, but is simpler and, we hope, better illustrates the essence of the idea.Let H be a universal hash function family, such that each hash function H ∈ H hashes an input in [d] into one bit. Perturb BLH (⟨H, b⟩) = ⟨H, b ′ ⟩ such that Pr [ b ′ = 1 ] = { p = e ε e ε +1 , if b = 1 q = 1 e ε +1 , if b = 0 Aggregation. Perturb(⟨H, x⟩) = (⟨H, y⟩), where∀ i∈[g] Pr [y = i] = { p = e ε e ε +g−1 , if x = i q = 1e ε +g−1 , if x ̸ = i Theorem 7 (Privacy of LH) The Local Hashing (LH) Protocol satisfies ε-LDP Proof 7 For any two possible input values v 1 , v 2 and any output ⟨H, y⟩, we have,Pr [⟨H, y⟩|v 1 ] Pr [⟨H, y⟩|v 2 ] = Pr [Perturb(H(v 1 )) = y] Pr [Perturb(H(v 2 )) = y] ≤ p q = e ε Aggregation. The fact that optimizing two apparently different encoding approaches, namely, unary encoding and local hashing, results in conceptually equivalent protocol, seems to suggest that this may be optimal (at least when d is large). All protocols other than DE have O(n · d) computation cost to estimate frequency of all values.Numerical values of the approximate variances using (4) for all protocols are given in Table 2 and Figure 1 (n = 10, 000). • When d is small, more precisely, when d < 3e ε + 2, DE is the best among all approaches. Adding Laplacian noises to a histogram is typically used in a setting with a trusted data curator, who first computes the histogram from all users' data and then adds the noise. Following the setting of Erlingsson et al. [13], we use a 128-bit Bloom filter, 2 hash functions and 8/16 cohorts in RAPPOR. After the estimation, all estimations above the threshold are kept, and those below the threshold T s are discarded.T s = Φ −1 ( 1 − α d ) √ Var * ,where d is the domain size, Φ −1 is the inverse of the cumulative density function of standard normal distribution, and the term inside the square root is the variance of the protocol. We run OLH, BLH and RAPPOR on the Kosarak dataset.As we can see in Figure 5(a), fixing a threshold, OLH and BLH performs similarly in identifying true positives, which is as expected, because frequent values are rare, and variance does not change much the probability it is identified. RAPPOR performs slightly worse because of the Bloom filter collision.As for the false positives, as shown in Figure 5(b), different protocols perform quite differently in eliminating false positives. Google deployed RAPPOR [13] in Chrome, and Apple [1] also uses similar methods to help with predictions of spelling and other things.State of the art protocols for frequency estimation under LDP are RAPPOR by Erlingsson et al. [13] and Random Matrix Projection (BLH) by Bassily and Smith [6], which we have presented in Section 2 and compared with in detail in the paper. Kairouz et al. [18] study the problem of finding the optimal LDP protocol for two goals: (1) hypothesis testing, i.e., telling whether the users' inputs are drawn from distribution P 0 or P 1 , and (2) maximize mutual information between input and output. In OLH, the variance is σ 2 = Var * [ ˜ c OLH (i)/n] = 4e ε (e ε −1σ 2 2 − σ 2 1 = 4 n ( e ε/Q ( e ε/Q − 1 ) 2 − Qe ε (e ε − 1) 2 ) = 4e ε/Q n ( e ε/Q − 1 ) 2 (e ε − 1) 2 · [ (e ε − 1) 2 − Qe ε−ε/Q ( e ε/Q − 1 ) 2 ]The first term is always greater than zero since ε > 0. For the second term, we define e ε/Q = z, and write it as:(z Q − 1) 2 − Qz Q−1 (z − 1) 2 =(z − 1) 2 · [ (z Q−1 + z Q−2 + . . . + 1) 2 − Qz Q−1 ] > 0Therefore, σ 2 1 is always smaller than σ 2 2 . We generate 10 million points following a normal distribution (rounded to integers, with mean 500 and standard deviation 10) and a Zipf's distribution (with parameter 1.5). It can be seen that different methods perform similarly in different distributions.